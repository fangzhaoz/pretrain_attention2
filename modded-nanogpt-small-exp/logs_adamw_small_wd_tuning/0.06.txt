import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:43:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:80ms step_avg:80.09ms
step:2/2330 train_time:171ms step_avg:85.60ms
step:3/2330 train_time:189ms step_avg:63.03ms
step:4/2330 train_time:208ms step_avg:52.02ms
step:5/2330 train_time:262ms step_avg:52.39ms
step:6/2330 train_time:320ms step_avg:53.33ms
step:7/2330 train_time:375ms step_avg:53.57ms
step:8/2330 train_time:434ms step_avg:54.22ms
step:9/2330 train_time:489ms step_avg:54.34ms
step:10/2330 train_time:548ms step_avg:54.77ms
step:11/2330 train_time:603ms step_avg:54.85ms
step:12/2330 train_time:662ms step_avg:55.13ms
step:13/2330 train_time:718ms step_avg:55.20ms
step:14/2330 train_time:776ms step_avg:55.40ms
step:15/2330 train_time:830ms step_avg:55.36ms
step:16/2330 train_time:889ms step_avg:55.58ms
step:17/2330 train_time:945ms step_avg:55.58ms
step:18/2330 train_time:1004ms step_avg:55.80ms
step:19/2330 train_time:1063ms step_avg:55.94ms
step:20/2330 train_time:1124ms step_avg:56.18ms
step:21/2330 train_time:1181ms step_avg:56.24ms
step:22/2330 train_time:1241ms step_avg:56.42ms
step:23/2330 train_time:1297ms step_avg:56.40ms
step:24/2330 train_time:1356ms step_avg:56.51ms
step:25/2330 train_time:1412ms step_avg:56.50ms
step:26/2330 train_time:1470ms step_avg:56.56ms
step:27/2330 train_time:1526ms step_avg:56.51ms
step:28/2330 train_time:1585ms step_avg:56.60ms
step:29/2330 train_time:1640ms step_avg:56.55ms
step:30/2330 train_time:1699ms step_avg:56.62ms
step:31/2330 train_time:1754ms step_avg:56.58ms
step:32/2330 train_time:1813ms step_avg:56.65ms
step:33/2330 train_time:1868ms step_avg:56.61ms
step:34/2330 train_time:1927ms step_avg:56.66ms
step:35/2330 train_time:1983ms step_avg:56.66ms
step:36/2330 train_time:2042ms step_avg:56.73ms
step:37/2330 train_time:2099ms step_avg:56.72ms
step:38/2330 train_time:2160ms step_avg:56.84ms
step:39/2330 train_time:2216ms step_avg:56.82ms
step:40/2330 train_time:2276ms step_avg:56.89ms
step:41/2330 train_time:2332ms step_avg:56.87ms
step:42/2330 train_time:2391ms step_avg:56.94ms
step:43/2330 train_time:2447ms step_avg:56.90ms
step:44/2330 train_time:2506ms step_avg:56.95ms
step:45/2330 train_time:2561ms step_avg:56.91ms
step:46/2330 train_time:2620ms step_avg:56.95ms
step:47/2330 train_time:2675ms step_avg:56.91ms
step:48/2330 train_time:2734ms step_avg:56.95ms
step:49/2330 train_time:2789ms step_avg:56.92ms
step:50/2330 train_time:2848ms step_avg:56.95ms
step:51/2330 train_time:2903ms step_avg:56.93ms
step:52/2330 train_time:2962ms step_avg:56.96ms
step:53/2330 train_time:3018ms step_avg:56.94ms
step:54/2330 train_time:3077ms step_avg:56.98ms
step:55/2330 train_time:3133ms step_avg:56.96ms
step:56/2330 train_time:3193ms step_avg:57.01ms
step:57/2330 train_time:3249ms step_avg:57.00ms
step:58/2330 train_time:3310ms step_avg:57.07ms
step:59/2330 train_time:3365ms step_avg:57.04ms
step:60/2330 train_time:3424ms step_avg:57.07ms
step:61/2330 train_time:3480ms step_avg:57.05ms
step:62/2330 train_time:3540ms step_avg:57.10ms
step:63/2330 train_time:3595ms step_avg:57.07ms
step:64/2330 train_time:3655ms step_avg:57.11ms
step:65/2330 train_time:3710ms step_avg:57.08ms
step:66/2330 train_time:3769ms step_avg:57.11ms
step:67/2330 train_time:3825ms step_avg:57.08ms
step:68/2330 train_time:3884ms step_avg:57.12ms
step:69/2330 train_time:3940ms step_avg:57.10ms
step:70/2330 train_time:3999ms step_avg:57.13ms
step:71/2330 train_time:4055ms step_avg:57.11ms
step:72/2330 train_time:4114ms step_avg:57.14ms
step:73/2330 train_time:4170ms step_avg:57.12ms
step:74/2330 train_time:4230ms step_avg:57.16ms
step:75/2330 train_time:4286ms step_avg:57.15ms
step:76/2330 train_time:4345ms step_avg:57.17ms
step:77/2330 train_time:4401ms step_avg:57.16ms
step:78/2330 train_time:4460ms step_avg:57.18ms
step:79/2330 train_time:4516ms step_avg:57.16ms
step:80/2330 train_time:4574ms step_avg:57.18ms
step:81/2330 train_time:4630ms step_avg:57.16ms
step:82/2330 train_time:4688ms step_avg:57.18ms
step:83/2330 train_time:4744ms step_avg:57.15ms
step:84/2330 train_time:4803ms step_avg:57.18ms
step:85/2330 train_time:4859ms step_avg:57.16ms
step:86/2330 train_time:4917ms step_avg:57.17ms
step:87/2330 train_time:4972ms step_avg:57.15ms
step:88/2330 train_time:5032ms step_avg:57.18ms
step:89/2330 train_time:5087ms step_avg:57.16ms
step:90/2330 train_time:5147ms step_avg:57.19ms
step:91/2330 train_time:5203ms step_avg:57.17ms
step:92/2330 train_time:5263ms step_avg:57.21ms
step:93/2330 train_time:5319ms step_avg:57.19ms
step:94/2330 train_time:5379ms step_avg:57.22ms
step:95/2330 train_time:5434ms step_avg:57.20ms
step:96/2330 train_time:5493ms step_avg:57.22ms
step:97/2330 train_time:5548ms step_avg:57.20ms
step:98/2330 train_time:5607ms step_avg:57.21ms
step:99/2330 train_time:5663ms step_avg:57.20ms
step:100/2330 train_time:5722ms step_avg:57.22ms
step:101/2330 train_time:5778ms step_avg:57.21ms
step:102/2330 train_time:5837ms step_avg:57.22ms
step:103/2330 train_time:5892ms step_avg:57.21ms
step:104/2330 train_time:5951ms step_avg:57.22ms
step:105/2330 train_time:6007ms step_avg:57.21ms
step:106/2330 train_time:6066ms step_avg:57.23ms
step:107/2330 train_time:6123ms step_avg:57.22ms
step:108/2330 train_time:6182ms step_avg:57.24ms
step:109/2330 train_time:6237ms step_avg:57.22ms
step:110/2330 train_time:6297ms step_avg:57.25ms
step:111/2330 train_time:6353ms step_avg:57.23ms
step:112/2330 train_time:6413ms step_avg:57.26ms
step:113/2330 train_time:6468ms step_avg:57.24ms
step:114/2330 train_time:6528ms step_avg:57.26ms
step:115/2330 train_time:6583ms step_avg:57.25ms
step:116/2330 train_time:6642ms step_avg:57.26ms
step:117/2330 train_time:6699ms step_avg:57.26ms
step:118/2330 train_time:6758ms step_avg:57.27ms
step:119/2330 train_time:6813ms step_avg:57.25ms
step:120/2330 train_time:6872ms step_avg:57.26ms
step:121/2330 train_time:6927ms step_avg:57.25ms
step:122/2330 train_time:6987ms step_avg:57.27ms
step:123/2330 train_time:7044ms step_avg:57.26ms
step:124/2330 train_time:7103ms step_avg:57.28ms
step:125/2330 train_time:7159ms step_avg:57.27ms
step:126/2330 train_time:7218ms step_avg:57.29ms
step:127/2330 train_time:7274ms step_avg:57.27ms
step:128/2330 train_time:7333ms step_avg:57.29ms
step:129/2330 train_time:7389ms step_avg:57.28ms
step:130/2330 train_time:7449ms step_avg:57.30ms
step:131/2330 train_time:7505ms step_avg:57.29ms
step:132/2330 train_time:7564ms step_avg:57.30ms
step:133/2330 train_time:7619ms step_avg:57.29ms
step:134/2330 train_time:7678ms step_avg:57.30ms
step:135/2330 train_time:7734ms step_avg:57.29ms
step:136/2330 train_time:7792ms step_avg:57.29ms
step:137/2330 train_time:7847ms step_avg:57.28ms
step:138/2330 train_time:7907ms step_avg:57.30ms
step:139/2330 train_time:7963ms step_avg:57.29ms
step:140/2330 train_time:8022ms step_avg:57.30ms
step:141/2330 train_time:8079ms step_avg:57.30ms
step:142/2330 train_time:8137ms step_avg:57.30ms
step:143/2330 train_time:8194ms step_avg:57.30ms
step:144/2330 train_time:8252ms step_avg:57.31ms
step:145/2330 train_time:8308ms step_avg:57.30ms
step:146/2330 train_time:8367ms step_avg:57.31ms
step:147/2330 train_time:8423ms step_avg:57.30ms
step:148/2330 train_time:8482ms step_avg:57.31ms
step:149/2330 train_time:8537ms step_avg:57.30ms
step:150/2330 train_time:8597ms step_avg:57.32ms
step:151/2330 train_time:8653ms step_avg:57.30ms
step:152/2330 train_time:8712ms step_avg:57.32ms
step:153/2330 train_time:8768ms step_avg:57.31ms
step:154/2330 train_time:8827ms step_avg:57.32ms
step:155/2330 train_time:8884ms step_avg:57.31ms
step:156/2330 train_time:8942ms step_avg:57.32ms
step:157/2330 train_time:8998ms step_avg:57.31ms
step:158/2330 train_time:9057ms step_avg:57.32ms
step:159/2330 train_time:9113ms step_avg:57.31ms
step:160/2330 train_time:9172ms step_avg:57.32ms
step:161/2330 train_time:9227ms step_avg:57.31ms
step:162/2330 train_time:9287ms step_avg:57.33ms
step:163/2330 train_time:9344ms step_avg:57.33ms
step:164/2330 train_time:9403ms step_avg:57.34ms
step:165/2330 train_time:9459ms step_avg:57.33ms
step:166/2330 train_time:9518ms step_avg:57.34ms
step:167/2330 train_time:9574ms step_avg:57.33ms
step:168/2330 train_time:9634ms step_avg:57.35ms
step:169/2330 train_time:9690ms step_avg:57.34ms
step:170/2330 train_time:9749ms step_avg:57.35ms
step:171/2330 train_time:9805ms step_avg:57.34ms
step:172/2330 train_time:9864ms step_avg:57.35ms
step:173/2330 train_time:9920ms step_avg:57.34ms
step:174/2330 train_time:9979ms step_avg:57.35ms
step:175/2330 train_time:10035ms step_avg:57.34ms
step:176/2330 train_time:10094ms step_avg:57.35ms
step:177/2330 train_time:10150ms step_avg:57.34ms
step:178/2330 train_time:10208ms step_avg:57.35ms
step:179/2330 train_time:10265ms step_avg:57.35ms
step:180/2330 train_time:10324ms step_avg:57.35ms
step:181/2330 train_time:10380ms step_avg:57.35ms
step:182/2330 train_time:10438ms step_avg:57.35ms
step:183/2330 train_time:10494ms step_avg:57.34ms
step:184/2330 train_time:10554ms step_avg:57.36ms
step:185/2330 train_time:10610ms step_avg:57.35ms
step:186/2330 train_time:10669ms step_avg:57.36ms
step:187/2330 train_time:10725ms step_avg:57.35ms
step:188/2330 train_time:10784ms step_avg:57.36ms
step:189/2330 train_time:10840ms step_avg:57.36ms
step:190/2330 train_time:10899ms step_avg:57.37ms
step:191/2330 train_time:10955ms step_avg:57.36ms
step:192/2330 train_time:11014ms step_avg:57.37ms
step:193/2330 train_time:11070ms step_avg:57.36ms
step:194/2330 train_time:11129ms step_avg:57.36ms
step:195/2330 train_time:11185ms step_avg:57.36ms
step:196/2330 train_time:11244ms step_avg:57.37ms
step:197/2330 train_time:11300ms step_avg:57.36ms
step:198/2330 train_time:11359ms step_avg:57.37ms
step:199/2330 train_time:11414ms step_avg:57.36ms
step:200/2330 train_time:11474ms step_avg:57.37ms
step:201/2330 train_time:11529ms step_avg:57.36ms
step:202/2330 train_time:11589ms step_avg:57.37ms
step:203/2330 train_time:11645ms step_avg:57.36ms
step:204/2330 train_time:11704ms step_avg:57.37ms
step:205/2330 train_time:11760ms step_avg:57.37ms
step:206/2330 train_time:11820ms step_avg:57.38ms
step:207/2330 train_time:11875ms step_avg:57.37ms
step:208/2330 train_time:11934ms step_avg:57.38ms
step:209/2330 train_time:11990ms step_avg:57.37ms
step:210/2330 train_time:12049ms step_avg:57.38ms
step:211/2330 train_time:12105ms step_avg:57.37ms
step:212/2330 train_time:12164ms step_avg:57.38ms
step:213/2330 train_time:12220ms step_avg:57.37ms
step:214/2330 train_time:12279ms step_avg:57.38ms
step:215/2330 train_time:12335ms step_avg:57.37ms
step:216/2330 train_time:12394ms step_avg:57.38ms
step:217/2330 train_time:12449ms step_avg:57.37ms
step:218/2330 train_time:12508ms step_avg:57.38ms
step:219/2330 train_time:12564ms step_avg:57.37ms
step:220/2330 train_time:12623ms step_avg:57.38ms
step:221/2330 train_time:12680ms step_avg:57.38ms
step:222/2330 train_time:12738ms step_avg:57.38ms
step:223/2330 train_time:12795ms step_avg:57.37ms
step:224/2330 train_time:12853ms step_avg:57.38ms
step:225/2330 train_time:12909ms step_avg:57.37ms
step:226/2330 train_time:12968ms step_avg:57.38ms
step:227/2330 train_time:13024ms step_avg:57.37ms
step:228/2330 train_time:13083ms step_avg:57.38ms
step:229/2330 train_time:13138ms step_avg:57.37ms
step:230/2330 train_time:13198ms step_avg:57.38ms
step:231/2330 train_time:13255ms step_avg:57.38ms
step:232/2330 train_time:13314ms step_avg:57.39ms
step:233/2330 train_time:13370ms step_avg:57.38ms
step:234/2330 train_time:13428ms step_avg:57.39ms
step:235/2330 train_time:13484ms step_avg:57.38ms
step:236/2330 train_time:13544ms step_avg:57.39ms
step:237/2330 train_time:13600ms step_avg:57.39ms
step:238/2330 train_time:13659ms step_avg:57.39ms
step:239/2330 train_time:13715ms step_avg:57.38ms
step:240/2330 train_time:13774ms step_avg:57.39ms
step:241/2330 train_time:13830ms step_avg:57.38ms
step:242/2330 train_time:13889ms step_avg:57.39ms
step:243/2330 train_time:13945ms step_avg:57.39ms
step:244/2330 train_time:14004ms step_avg:57.39ms
step:245/2330 train_time:14060ms step_avg:57.39ms
step:246/2330 train_time:14120ms step_avg:57.40ms
step:247/2330 train_time:14176ms step_avg:57.39ms
step:248/2330 train_time:14234ms step_avg:57.39ms
step:249/2330 train_time:14290ms step_avg:57.39ms
step:250/2330 train_time:14349ms step_avg:57.40ms
step:250/2330 val_loss:4.8854 train_time:14428ms step_avg:57.71ms
step:251/2330 train_time:14446ms step_avg:57.55ms
step:252/2330 train_time:14466ms step_avg:57.40ms
step:253/2330 train_time:14523ms step_avg:57.40ms
step:254/2330 train_time:14588ms step_avg:57.43ms
step:255/2330 train_time:14644ms step_avg:57.43ms
step:256/2330 train_time:14706ms step_avg:57.44ms
step:257/2330 train_time:14761ms step_avg:57.43ms
step:258/2330 train_time:14820ms step_avg:57.44ms
step:259/2330 train_time:14876ms step_avg:57.44ms
step:260/2330 train_time:14935ms step_avg:57.44ms
step:261/2330 train_time:14990ms step_avg:57.43ms
step:262/2330 train_time:15048ms step_avg:57.44ms
step:263/2330 train_time:15103ms step_avg:57.43ms
step:264/2330 train_time:15162ms step_avg:57.43ms
step:265/2330 train_time:15217ms step_avg:57.42ms
step:266/2330 train_time:15276ms step_avg:57.43ms
step:267/2330 train_time:15331ms step_avg:57.42ms
step:268/2330 train_time:15390ms step_avg:57.43ms
step:269/2330 train_time:15447ms step_avg:57.42ms
step:270/2330 train_time:15507ms step_avg:57.43ms
step:271/2330 train_time:15564ms step_avg:57.43ms
step:272/2330 train_time:15624ms step_avg:57.44ms
step:273/2330 train_time:15681ms step_avg:57.44ms
step:274/2330 train_time:15741ms step_avg:57.45ms
step:275/2330 train_time:15797ms step_avg:57.44ms
step:276/2330 train_time:15855ms step_avg:57.45ms
step:277/2330 train_time:15911ms step_avg:57.44ms
step:278/2330 train_time:15970ms step_avg:57.45ms
step:279/2330 train_time:16026ms step_avg:57.44ms
step:280/2330 train_time:16084ms step_avg:57.44ms
step:281/2330 train_time:16139ms step_avg:57.43ms
step:282/2330 train_time:16198ms step_avg:57.44ms
step:283/2330 train_time:16253ms step_avg:57.43ms
step:284/2330 train_time:16312ms step_avg:57.44ms
step:285/2330 train_time:16367ms step_avg:57.43ms
step:286/2330 train_time:16427ms step_avg:57.44ms
step:287/2330 train_time:16483ms step_avg:57.43ms
step:288/2330 train_time:16543ms step_avg:57.44ms
step:289/2330 train_time:16599ms step_avg:57.44ms
step:290/2330 train_time:16659ms step_avg:57.45ms
step:291/2330 train_time:16716ms step_avg:57.44ms
step:292/2330 train_time:16775ms step_avg:57.45ms
step:293/2330 train_time:16831ms step_avg:57.44ms
step:294/2330 train_time:16891ms step_avg:57.45ms
step:295/2330 train_time:16947ms step_avg:57.45ms
step:296/2330 train_time:17006ms step_avg:57.45ms
step:297/2330 train_time:17061ms step_avg:57.45ms
step:298/2330 train_time:17121ms step_avg:57.45ms
step:299/2330 train_time:17177ms step_avg:57.45ms
step:300/2330 train_time:17235ms step_avg:57.45ms
step:301/2330 train_time:17290ms step_avg:57.44ms
step:302/2330 train_time:17350ms step_avg:57.45ms
step:303/2330 train_time:17406ms step_avg:57.44ms
step:304/2330 train_time:17465ms step_avg:57.45ms
step:305/2330 train_time:17521ms step_avg:57.45ms
step:306/2330 train_time:17580ms step_avg:57.45ms
step:307/2330 train_time:17637ms step_avg:57.45ms
step:308/2330 train_time:17697ms step_avg:57.46ms
step:309/2330 train_time:17752ms step_avg:57.45ms
step:310/2330 train_time:17812ms step_avg:57.46ms
step:311/2330 train_time:17868ms step_avg:57.45ms
step:312/2330 train_time:17926ms step_avg:57.46ms
step:313/2330 train_time:17982ms step_avg:57.45ms
step:314/2330 train_time:18041ms step_avg:57.46ms
step:315/2330 train_time:18097ms step_avg:57.45ms
step:316/2330 train_time:18156ms step_avg:57.46ms
step:317/2330 train_time:18212ms step_avg:57.45ms
step:318/2330 train_time:18271ms step_avg:57.45ms
step:319/2330 train_time:18326ms step_avg:57.45ms
step:320/2330 train_time:18386ms step_avg:57.46ms
step:321/2330 train_time:18442ms step_avg:57.45ms
step:322/2330 train_time:18500ms step_avg:57.45ms
step:323/2330 train_time:18557ms step_avg:57.45ms
step:324/2330 train_time:18617ms step_avg:57.46ms
step:325/2330 train_time:18673ms step_avg:57.46ms
step:326/2330 train_time:18732ms step_avg:57.46ms
step:327/2330 train_time:18789ms step_avg:57.46ms
step:328/2330 train_time:18848ms step_avg:57.46ms
step:329/2330 train_time:18904ms step_avg:57.46ms
step:330/2330 train_time:18964ms step_avg:57.47ms
step:331/2330 train_time:19019ms step_avg:57.46ms
step:332/2330 train_time:19079ms step_avg:57.47ms
step:333/2330 train_time:19136ms step_avg:57.46ms
step:334/2330 train_time:19194ms step_avg:57.47ms
step:335/2330 train_time:19250ms step_avg:57.46ms
step:336/2330 train_time:19309ms step_avg:57.47ms
step:337/2330 train_time:19365ms step_avg:57.46ms
step:338/2330 train_time:19424ms step_avg:57.47ms
step:339/2330 train_time:19480ms step_avg:57.46ms
step:340/2330 train_time:19539ms step_avg:57.47ms
step:341/2330 train_time:19596ms step_avg:57.46ms
step:342/2330 train_time:19655ms step_avg:57.47ms
step:343/2330 train_time:19711ms step_avg:57.47ms
step:344/2330 train_time:19770ms step_avg:57.47ms
step:345/2330 train_time:19826ms step_avg:57.47ms
step:346/2330 train_time:19886ms step_avg:57.47ms
step:347/2330 train_time:19941ms step_avg:57.47ms
step:348/2330 train_time:20001ms step_avg:57.47ms
step:349/2330 train_time:20057ms step_avg:57.47ms
step:350/2330 train_time:20116ms step_avg:57.47ms
step:351/2330 train_time:20172ms step_avg:57.47ms
step:352/2330 train_time:20231ms step_avg:57.48ms
step:353/2330 train_time:20287ms step_avg:57.47ms
step:354/2330 train_time:20346ms step_avg:57.47ms
step:355/2330 train_time:20401ms step_avg:57.47ms
step:356/2330 train_time:20460ms step_avg:57.47ms
step:357/2330 train_time:20517ms step_avg:57.47ms
step:358/2330 train_time:20576ms step_avg:57.48ms
step:359/2330 train_time:20632ms step_avg:57.47ms
step:360/2330 train_time:20692ms step_avg:57.48ms
step:361/2330 train_time:20748ms step_avg:57.47ms
step:362/2330 train_time:20808ms step_avg:57.48ms
step:363/2330 train_time:20864ms step_avg:57.48ms
step:364/2330 train_time:20923ms step_avg:57.48ms
step:365/2330 train_time:20979ms step_avg:57.48ms
step:366/2330 train_time:21038ms step_avg:57.48ms
step:367/2330 train_time:21094ms step_avg:57.48ms
step:368/2330 train_time:21153ms step_avg:57.48ms
step:369/2330 train_time:21209ms step_avg:57.48ms
step:370/2330 train_time:21268ms step_avg:57.48ms
step:371/2330 train_time:21323ms step_avg:57.48ms
step:372/2330 train_time:21384ms step_avg:57.48ms
step:373/2330 train_time:21439ms step_avg:57.48ms
step:374/2330 train_time:21498ms step_avg:57.48ms
step:375/2330 train_time:21555ms step_avg:57.48ms
step:376/2330 train_time:21614ms step_avg:57.48ms
step:377/2330 train_time:21670ms step_avg:57.48ms
step:378/2330 train_time:21729ms step_avg:57.49ms
step:379/2330 train_time:21785ms step_avg:57.48ms
step:380/2330 train_time:21845ms step_avg:57.49ms
step:381/2330 train_time:21900ms step_avg:57.48ms
step:382/2330 train_time:21961ms step_avg:57.49ms
step:383/2330 train_time:22016ms step_avg:57.48ms
step:384/2330 train_time:22076ms step_avg:57.49ms
step:385/2330 train_time:22132ms step_avg:57.49ms
step:386/2330 train_time:22191ms step_avg:57.49ms
step:387/2330 train_time:22247ms step_avg:57.49ms
step:388/2330 train_time:22307ms step_avg:57.49ms
step:389/2330 train_time:22362ms step_avg:57.49ms
step:390/2330 train_time:22422ms step_avg:57.49ms
step:391/2330 train_time:22477ms step_avg:57.49ms
step:392/2330 train_time:22537ms step_avg:57.49ms
step:393/2330 train_time:22593ms step_avg:57.49ms
step:394/2330 train_time:22653ms step_avg:57.49ms
step:395/2330 train_time:22709ms step_avg:57.49ms
step:396/2330 train_time:22768ms step_avg:57.49ms
step:397/2330 train_time:22824ms step_avg:57.49ms
step:398/2330 train_time:22883ms step_avg:57.50ms
step:399/2330 train_time:22939ms step_avg:57.49ms
step:400/2330 train_time:22998ms step_avg:57.49ms
step:401/2330 train_time:23054ms step_avg:57.49ms
step:402/2330 train_time:23113ms step_avg:57.50ms
step:403/2330 train_time:23170ms step_avg:57.49ms
step:404/2330 train_time:23228ms step_avg:57.50ms
step:405/2330 train_time:23284ms step_avg:57.49ms
step:406/2330 train_time:23343ms step_avg:57.50ms
step:407/2330 train_time:23399ms step_avg:57.49ms
step:408/2330 train_time:23459ms step_avg:57.50ms
step:409/2330 train_time:23514ms step_avg:57.49ms
step:410/2330 train_time:23574ms step_avg:57.50ms
step:411/2330 train_time:23630ms step_avg:57.49ms
step:412/2330 train_time:23689ms step_avg:57.50ms
step:413/2330 train_time:23745ms step_avg:57.49ms
step:414/2330 train_time:23804ms step_avg:57.50ms
step:415/2330 train_time:23860ms step_avg:57.49ms
step:416/2330 train_time:23919ms step_avg:57.50ms
step:417/2330 train_time:23976ms step_avg:57.50ms
step:418/2330 train_time:24035ms step_avg:57.50ms
step:419/2330 train_time:24092ms step_avg:57.50ms
step:420/2330 train_time:24151ms step_avg:57.50ms
step:421/2330 train_time:24207ms step_avg:57.50ms
step:422/2330 train_time:24265ms step_avg:57.50ms
step:423/2330 train_time:24321ms step_avg:57.50ms
step:424/2330 train_time:24381ms step_avg:57.50ms
step:425/2330 train_time:24438ms step_avg:57.50ms
step:426/2330 train_time:24497ms step_avg:57.50ms
step:427/2330 train_time:24553ms step_avg:57.50ms
step:428/2330 train_time:24611ms step_avg:57.50ms
step:429/2330 train_time:24667ms step_avg:57.50ms
step:430/2330 train_time:24728ms step_avg:57.51ms
step:431/2330 train_time:24784ms step_avg:57.50ms
step:432/2330 train_time:24844ms step_avg:57.51ms
step:433/2330 train_time:24899ms step_avg:57.50ms
step:434/2330 train_time:24959ms step_avg:57.51ms
step:435/2330 train_time:25015ms step_avg:57.51ms
step:436/2330 train_time:25075ms step_avg:57.51ms
step:437/2330 train_time:25131ms step_avg:57.51ms
step:438/2330 train_time:25190ms step_avg:57.51ms
step:439/2330 train_time:25246ms step_avg:57.51ms
step:440/2330 train_time:25304ms step_avg:57.51ms
step:441/2330 train_time:25360ms step_avg:57.51ms
step:442/2330 train_time:25419ms step_avg:57.51ms
step:443/2330 train_time:25476ms step_avg:57.51ms
step:444/2330 train_time:25535ms step_avg:57.51ms
step:445/2330 train_time:25591ms step_avg:57.51ms
step:446/2330 train_time:25651ms step_avg:57.51ms
step:447/2330 train_time:25706ms step_avg:57.51ms
step:448/2330 train_time:25766ms step_avg:57.51ms
step:449/2330 train_time:25822ms step_avg:57.51ms
step:450/2330 train_time:25882ms step_avg:57.52ms
step:451/2330 train_time:25938ms step_avg:57.51ms
step:452/2330 train_time:25999ms step_avg:57.52ms
step:453/2330 train_time:26055ms step_avg:57.52ms
step:454/2330 train_time:26114ms step_avg:57.52ms
step:455/2330 train_time:26170ms step_avg:57.52ms
step:456/2330 train_time:26229ms step_avg:57.52ms
step:457/2330 train_time:26285ms step_avg:57.52ms
step:458/2330 train_time:26345ms step_avg:57.52ms
step:459/2330 train_time:26400ms step_avg:57.52ms
step:460/2330 train_time:26460ms step_avg:57.52ms
step:461/2330 train_time:26517ms step_avg:57.52ms
step:462/2330 train_time:26577ms step_avg:57.53ms
step:463/2330 train_time:26633ms step_avg:57.52ms
step:464/2330 train_time:26692ms step_avg:57.53ms
step:465/2330 train_time:26749ms step_avg:57.52ms
step:466/2330 train_time:26809ms step_avg:57.53ms
step:467/2330 train_time:26864ms step_avg:57.52ms
step:468/2330 train_time:26924ms step_avg:57.53ms
step:469/2330 train_time:26980ms step_avg:57.53ms
step:470/2330 train_time:27040ms step_avg:57.53ms
step:471/2330 train_time:27097ms step_avg:57.53ms
step:472/2330 train_time:27156ms step_avg:57.53ms
step:473/2330 train_time:27212ms step_avg:57.53ms
step:474/2330 train_time:27271ms step_avg:57.53ms
step:475/2330 train_time:27326ms step_avg:57.53ms
step:476/2330 train_time:27386ms step_avg:57.53ms
step:477/2330 train_time:27441ms step_avg:57.53ms
step:478/2330 train_time:27500ms step_avg:57.53ms
step:479/2330 train_time:27557ms step_avg:57.53ms
step:480/2330 train_time:27616ms step_avg:57.53ms
step:481/2330 train_time:27673ms step_avg:57.53ms
step:482/2330 train_time:27732ms step_avg:57.53ms
step:483/2330 train_time:27788ms step_avg:57.53ms
step:484/2330 train_time:27847ms step_avg:57.54ms
step:485/2330 train_time:27903ms step_avg:57.53ms
step:486/2330 train_time:27964ms step_avg:57.54ms
step:487/2330 train_time:28019ms step_avg:57.53ms
step:488/2330 train_time:28079ms step_avg:57.54ms
step:489/2330 train_time:28135ms step_avg:57.54ms
step:490/2330 train_time:28194ms step_avg:57.54ms
step:491/2330 train_time:28250ms step_avg:57.54ms
step:492/2330 train_time:28310ms step_avg:57.54ms
step:493/2330 train_time:28366ms step_avg:57.54ms
step:494/2330 train_time:28426ms step_avg:57.54ms
step:495/2330 train_time:28481ms step_avg:57.54ms
step:496/2330 train_time:28541ms step_avg:57.54ms
step:497/2330 train_time:28596ms step_avg:57.54ms
step:498/2330 train_time:28656ms step_avg:57.54ms
step:499/2330 train_time:28713ms step_avg:57.54ms
step:500/2330 train_time:28771ms step_avg:57.54ms
step:500/2330 val_loss:4.4003 train_time:28851ms step_avg:57.70ms
step:501/2330 train_time:28869ms step_avg:57.62ms
step:502/2330 train_time:28890ms step_avg:57.55ms
step:503/2330 train_time:28948ms step_avg:57.55ms
step:504/2330 train_time:29013ms step_avg:57.56ms
step:505/2330 train_time:29070ms step_avg:57.57ms
step:506/2330 train_time:29130ms step_avg:57.57ms
step:507/2330 train_time:29185ms step_avg:57.56ms
step:508/2330 train_time:29244ms step_avg:57.57ms
step:509/2330 train_time:29300ms step_avg:57.56ms
step:510/2330 train_time:29358ms step_avg:57.56ms
step:511/2330 train_time:29413ms step_avg:57.56ms
step:512/2330 train_time:29472ms step_avg:57.56ms
step:513/2330 train_time:29527ms step_avg:57.56ms
step:514/2330 train_time:29585ms step_avg:57.56ms
step:515/2330 train_time:29641ms step_avg:57.56ms
step:516/2330 train_time:29699ms step_avg:57.56ms
step:517/2330 train_time:29755ms step_avg:57.55ms
step:518/2330 train_time:29814ms step_avg:57.56ms
step:519/2330 train_time:29869ms step_avg:57.55ms
step:520/2330 train_time:29932ms step_avg:57.56ms
step:521/2330 train_time:29989ms step_avg:57.56ms
step:522/2330 train_time:30050ms step_avg:57.57ms
step:523/2330 train_time:30106ms step_avg:57.56ms
step:524/2330 train_time:30166ms step_avg:57.57ms
step:525/2330 train_time:30221ms step_avg:57.56ms
step:526/2330 train_time:30281ms step_avg:57.57ms
step:527/2330 train_time:30338ms step_avg:57.57ms
step:528/2330 train_time:30396ms step_avg:57.57ms
step:529/2330 train_time:30451ms step_avg:57.56ms
step:530/2330 train_time:30510ms step_avg:57.57ms
step:531/2330 train_time:30565ms step_avg:57.56ms
step:532/2330 train_time:30624ms step_avg:57.56ms
step:533/2330 train_time:30680ms step_avg:57.56ms
step:534/2330 train_time:30739ms step_avg:57.56ms
step:535/2330 train_time:30794ms step_avg:57.56ms
step:536/2330 train_time:30854ms step_avg:57.56ms
step:537/2330 train_time:30910ms step_avg:57.56ms
step:538/2330 train_time:30971ms step_avg:57.57ms
step:539/2330 train_time:31027ms step_avg:57.56ms
step:540/2330 train_time:31088ms step_avg:57.57ms
step:541/2330 train_time:31145ms step_avg:57.57ms
step:542/2330 train_time:31204ms step_avg:57.57ms
step:543/2330 train_time:31260ms step_avg:57.57ms
step:544/2330 train_time:31319ms step_avg:57.57ms
step:545/2330 train_time:31375ms step_avg:57.57ms
step:546/2330 train_time:31434ms step_avg:57.57ms
step:547/2330 train_time:31490ms step_avg:57.57ms
step:548/2330 train_time:31548ms step_avg:57.57ms
step:549/2330 train_time:31604ms step_avg:57.57ms
step:550/2330 train_time:31663ms step_avg:57.57ms
step:551/2330 train_time:31719ms step_avg:57.57ms
step:552/2330 train_time:31778ms step_avg:57.57ms
step:553/2330 train_time:31834ms step_avg:57.57ms
step:554/2330 train_time:31896ms step_avg:57.57ms
step:555/2330 train_time:31952ms step_avg:57.57ms
step:556/2330 train_time:32014ms step_avg:57.58ms
step:557/2330 train_time:32070ms step_avg:57.58ms
step:558/2330 train_time:32130ms step_avg:57.58ms
step:559/2330 train_time:32185ms step_avg:57.58ms
step:560/2330 train_time:32244ms step_avg:57.58ms
step:561/2330 train_time:32300ms step_avg:57.58ms
step:562/2330 train_time:32360ms step_avg:57.58ms
step:563/2330 train_time:32416ms step_avg:57.58ms
step:564/2330 train_time:32475ms step_avg:57.58ms
step:565/2330 train_time:32531ms step_avg:57.58ms
step:566/2330 train_time:32590ms step_avg:57.58ms
step:567/2330 train_time:32646ms step_avg:57.58ms
step:568/2330 train_time:32704ms step_avg:57.58ms
step:569/2330 train_time:32760ms step_avg:57.57ms
step:570/2330 train_time:32819ms step_avg:57.58ms
step:571/2330 train_time:32875ms step_avg:57.57ms
step:572/2330 train_time:32936ms step_avg:57.58ms
step:573/2330 train_time:32991ms step_avg:57.58ms
step:574/2330 train_time:33051ms step_avg:57.58ms
step:575/2330 train_time:33107ms step_avg:57.58ms
step:576/2330 train_time:33168ms step_avg:57.58ms
step:577/2330 train_time:33224ms step_avg:57.58ms
step:578/2330 train_time:33283ms step_avg:57.58ms
step:579/2330 train_time:33339ms step_avg:57.58ms
step:580/2330 train_time:33398ms step_avg:57.58ms
step:581/2330 train_time:33454ms step_avg:57.58ms
step:582/2330 train_time:33513ms step_avg:57.58ms
step:583/2330 train_time:33568ms step_avg:57.58ms
step:584/2330 train_time:33628ms step_avg:57.58ms
step:585/2330 train_time:33683ms step_avg:57.58ms
step:586/2330 train_time:33744ms step_avg:57.58ms
step:587/2330 train_time:33800ms step_avg:57.58ms
step:588/2330 train_time:33860ms step_avg:57.59ms
step:589/2330 train_time:33916ms step_avg:57.58ms
step:590/2330 train_time:33976ms step_avg:57.59ms
step:591/2330 train_time:34032ms step_avg:57.58ms
step:592/2330 train_time:34092ms step_avg:57.59ms
step:593/2330 train_time:34148ms step_avg:57.58ms
step:594/2330 train_time:34208ms step_avg:57.59ms
step:595/2330 train_time:34264ms step_avg:57.59ms
step:596/2330 train_time:34324ms step_avg:57.59ms
step:597/2330 train_time:34380ms step_avg:57.59ms
step:598/2330 train_time:34439ms step_avg:57.59ms
step:599/2330 train_time:34495ms step_avg:57.59ms
step:600/2330 train_time:34554ms step_avg:57.59ms
step:601/2330 train_time:34610ms step_avg:57.59ms
step:602/2330 train_time:34670ms step_avg:57.59ms
step:603/2330 train_time:34726ms step_avg:57.59ms
step:604/2330 train_time:34785ms step_avg:57.59ms
step:605/2330 train_time:34841ms step_avg:57.59ms
step:606/2330 train_time:34900ms step_avg:57.59ms
step:607/2330 train_time:34957ms step_avg:57.59ms
step:608/2330 train_time:35016ms step_avg:57.59ms
step:609/2330 train_time:35073ms step_avg:57.59ms
step:610/2330 train_time:35132ms step_avg:57.59ms
step:611/2330 train_time:35188ms step_avg:57.59ms
step:612/2330 train_time:35247ms step_avg:57.59ms
step:613/2330 train_time:35303ms step_avg:57.59ms
step:614/2330 train_time:35363ms step_avg:57.59ms
step:615/2330 train_time:35419ms step_avg:57.59ms
step:616/2330 train_time:35478ms step_avg:57.59ms
step:617/2330 train_time:35534ms step_avg:57.59ms
step:618/2330 train_time:35593ms step_avg:57.59ms
step:619/2330 train_time:35649ms step_avg:57.59ms
step:620/2330 train_time:35708ms step_avg:57.59ms
step:621/2330 train_time:35764ms step_avg:57.59ms
step:622/2330 train_time:35825ms step_avg:57.60ms
step:623/2330 train_time:35882ms step_avg:57.60ms
step:624/2330 train_time:35941ms step_avg:57.60ms
step:625/2330 train_time:35997ms step_avg:57.59ms
step:626/2330 train_time:36057ms step_avg:57.60ms
step:627/2330 train_time:36112ms step_avg:57.60ms
step:628/2330 train_time:36173ms step_avg:57.60ms
step:629/2330 train_time:36229ms step_avg:57.60ms
step:630/2330 train_time:36288ms step_avg:57.60ms
step:631/2330 train_time:36344ms step_avg:57.60ms
step:632/2330 train_time:36403ms step_avg:57.60ms
step:633/2330 train_time:36460ms step_avg:57.60ms
step:634/2330 train_time:36519ms step_avg:57.60ms
step:635/2330 train_time:36575ms step_avg:57.60ms
step:636/2330 train_time:36634ms step_avg:57.60ms
step:637/2330 train_time:36689ms step_avg:57.60ms
step:638/2330 train_time:36749ms step_avg:57.60ms
step:639/2330 train_time:36805ms step_avg:57.60ms
step:640/2330 train_time:36866ms step_avg:57.60ms
step:641/2330 train_time:36924ms step_avg:57.60ms
step:642/2330 train_time:36983ms step_avg:57.61ms
step:643/2330 train_time:37040ms step_avg:57.60ms
step:644/2330 train_time:37098ms step_avg:57.61ms
step:645/2330 train_time:37154ms step_avg:57.60ms
step:646/2330 train_time:37214ms step_avg:57.61ms
step:647/2330 train_time:37270ms step_avg:57.60ms
step:648/2330 train_time:37330ms step_avg:57.61ms
step:649/2330 train_time:37385ms step_avg:57.60ms
step:650/2330 train_time:37445ms step_avg:57.61ms
step:651/2330 train_time:37501ms step_avg:57.60ms
step:652/2330 train_time:37560ms step_avg:57.61ms
step:653/2330 train_time:37616ms step_avg:57.60ms
step:654/2330 train_time:37676ms step_avg:57.61ms
step:655/2330 train_time:37732ms step_avg:57.61ms
step:656/2330 train_time:37791ms step_avg:57.61ms
step:657/2330 train_time:37846ms step_avg:57.60ms
step:658/2330 train_time:37907ms step_avg:57.61ms
step:659/2330 train_time:37963ms step_avg:57.61ms
step:660/2330 train_time:38024ms step_avg:57.61ms
step:661/2330 train_time:38081ms step_avg:57.61ms
step:662/2330 train_time:38140ms step_avg:57.61ms
step:663/2330 train_time:38197ms step_avg:57.61ms
step:664/2330 train_time:38255ms step_avg:57.61ms
step:665/2330 train_time:38312ms step_avg:57.61ms
step:666/2330 train_time:38371ms step_avg:57.61ms
step:667/2330 train_time:38427ms step_avg:57.61ms
step:668/2330 train_time:38487ms step_avg:57.62ms
step:669/2330 train_time:38543ms step_avg:57.61ms
step:670/2330 train_time:38602ms step_avg:57.61ms
step:671/2330 train_time:38658ms step_avg:57.61ms
step:672/2330 train_time:38717ms step_avg:57.61ms
step:673/2330 train_time:38773ms step_avg:57.61ms
step:674/2330 train_time:38833ms step_avg:57.62ms
step:675/2330 train_time:38888ms step_avg:57.61ms
step:676/2330 train_time:38948ms step_avg:57.62ms
step:677/2330 train_time:39004ms step_avg:57.61ms
step:678/2330 train_time:39063ms step_avg:57.62ms
step:679/2330 train_time:39120ms step_avg:57.61ms
step:680/2330 train_time:39179ms step_avg:57.62ms
step:681/2330 train_time:39236ms step_avg:57.62ms
step:682/2330 train_time:39295ms step_avg:57.62ms
step:683/2330 train_time:39351ms step_avg:57.62ms
step:684/2330 train_time:39411ms step_avg:57.62ms
step:685/2330 train_time:39467ms step_avg:57.62ms
step:686/2330 train_time:39526ms step_avg:57.62ms
step:687/2330 train_time:39583ms step_avg:57.62ms
step:688/2330 train_time:39641ms step_avg:57.62ms
step:689/2330 train_time:39698ms step_avg:57.62ms
step:690/2330 train_time:39757ms step_avg:57.62ms
step:691/2330 train_time:39813ms step_avg:57.62ms
step:692/2330 train_time:39873ms step_avg:57.62ms
step:693/2330 train_time:39929ms step_avg:57.62ms
step:694/2330 train_time:39988ms step_avg:57.62ms
step:695/2330 train_time:40044ms step_avg:57.62ms
step:696/2330 train_time:40104ms step_avg:57.62ms
step:697/2330 train_time:40160ms step_avg:57.62ms
step:698/2330 train_time:40218ms step_avg:57.62ms
step:699/2330 train_time:40275ms step_avg:57.62ms
step:700/2330 train_time:40335ms step_avg:57.62ms
step:701/2330 train_time:40390ms step_avg:57.62ms
step:702/2330 train_time:40450ms step_avg:57.62ms
step:703/2330 train_time:40505ms step_avg:57.62ms
step:704/2330 train_time:40565ms step_avg:57.62ms
step:705/2330 train_time:40622ms step_avg:57.62ms
step:706/2330 train_time:40681ms step_avg:57.62ms
step:707/2330 train_time:40738ms step_avg:57.62ms
step:708/2330 train_time:40797ms step_avg:57.62ms
step:709/2330 train_time:40853ms step_avg:57.62ms
step:710/2330 train_time:40912ms step_avg:57.62ms
step:711/2330 train_time:40967ms step_avg:57.62ms
step:712/2330 train_time:41028ms step_avg:57.62ms
step:713/2330 train_time:41084ms step_avg:57.62ms
step:714/2330 train_time:41143ms step_avg:57.62ms
step:715/2330 train_time:41199ms step_avg:57.62ms
step:716/2330 train_time:41258ms step_avg:57.62ms
step:717/2330 train_time:41314ms step_avg:57.62ms
step:718/2330 train_time:41373ms step_avg:57.62ms
step:719/2330 train_time:41429ms step_avg:57.62ms
step:720/2330 train_time:41488ms step_avg:57.62ms
step:721/2330 train_time:41544ms step_avg:57.62ms
step:722/2330 train_time:41604ms step_avg:57.62ms
step:723/2330 train_time:41661ms step_avg:57.62ms
step:724/2330 train_time:41721ms step_avg:57.63ms
step:725/2330 train_time:41777ms step_avg:57.62ms
step:726/2330 train_time:41836ms step_avg:57.63ms
step:727/2330 train_time:41893ms step_avg:57.62ms
step:728/2330 train_time:41951ms step_avg:57.63ms
step:729/2330 train_time:42007ms step_avg:57.62ms
step:730/2330 train_time:42067ms step_avg:57.63ms
step:731/2330 train_time:42123ms step_avg:57.62ms
step:732/2330 train_time:42183ms step_avg:57.63ms
step:733/2330 train_time:42239ms step_avg:57.62ms
step:734/2330 train_time:42298ms step_avg:57.63ms
step:735/2330 train_time:42354ms step_avg:57.62ms
step:736/2330 train_time:42414ms step_avg:57.63ms
step:737/2330 train_time:42470ms step_avg:57.63ms
step:738/2330 train_time:42530ms step_avg:57.63ms
step:739/2330 train_time:42586ms step_avg:57.63ms
step:740/2330 train_time:42646ms step_avg:57.63ms
step:741/2330 train_time:42702ms step_avg:57.63ms
step:742/2330 train_time:42762ms step_avg:57.63ms
step:743/2330 train_time:42818ms step_avg:57.63ms
step:744/2330 train_time:42877ms step_avg:57.63ms
step:745/2330 train_time:42933ms step_avg:57.63ms
step:746/2330 train_time:42993ms step_avg:57.63ms
step:747/2330 train_time:43049ms step_avg:57.63ms
step:748/2330 train_time:43108ms step_avg:57.63ms
step:749/2330 train_time:43164ms step_avg:57.63ms
step:750/2330 train_time:43226ms step_avg:57.63ms
step:750/2330 val_loss:4.2129 train_time:43306ms step_avg:57.74ms
step:751/2330 train_time:43325ms step_avg:57.69ms
step:752/2330 train_time:43345ms step_avg:57.64ms
step:753/2330 train_time:43401ms step_avg:57.64ms
step:754/2330 train_time:43464ms step_avg:57.64ms
step:755/2330 train_time:43521ms step_avg:57.64ms
step:756/2330 train_time:43583ms step_avg:57.65ms
step:757/2330 train_time:43639ms step_avg:57.65ms
step:758/2330 train_time:43697ms step_avg:57.65ms
step:759/2330 train_time:43753ms step_avg:57.65ms
step:760/2330 train_time:43812ms step_avg:57.65ms
step:761/2330 train_time:43867ms step_avg:57.64ms
step:762/2330 train_time:43926ms step_avg:57.65ms
step:763/2330 train_time:43982ms step_avg:57.64ms
step:764/2330 train_time:44040ms step_avg:57.64ms
step:765/2330 train_time:44097ms step_avg:57.64ms
step:766/2330 train_time:44155ms step_avg:57.64ms
step:767/2330 train_time:44212ms step_avg:57.64ms
step:768/2330 train_time:44273ms step_avg:57.65ms
step:769/2330 train_time:44330ms step_avg:57.65ms
step:770/2330 train_time:44392ms step_avg:57.65ms
step:771/2330 train_time:44449ms step_avg:57.65ms
step:772/2330 train_time:44512ms step_avg:57.66ms
step:773/2330 train_time:44568ms step_avg:57.66ms
step:774/2330 train_time:44630ms step_avg:57.66ms
step:775/2330 train_time:44686ms step_avg:57.66ms
step:776/2330 train_time:44747ms step_avg:57.66ms
step:777/2330 train_time:44803ms step_avg:57.66ms
step:778/2330 train_time:44863ms step_avg:57.66ms
step:779/2330 train_time:44919ms step_avg:57.66ms
step:780/2330 train_time:44978ms step_avg:57.66ms
step:781/2330 train_time:45035ms step_avg:57.66ms
step:782/2330 train_time:45094ms step_avg:57.67ms
step:783/2330 train_time:45152ms step_avg:57.67ms
step:784/2330 train_time:45211ms step_avg:57.67ms
step:785/2330 train_time:45269ms step_avg:57.67ms
step:786/2330 train_time:45329ms step_avg:57.67ms
step:787/2330 train_time:45387ms step_avg:57.67ms
step:788/2330 train_time:45447ms step_avg:57.67ms
step:789/2330 train_time:45505ms step_avg:57.67ms
step:790/2330 train_time:45566ms step_avg:57.68ms
step:791/2330 train_time:45623ms step_avg:57.68ms
step:792/2330 train_time:45683ms step_avg:57.68ms
step:793/2330 train_time:45739ms step_avg:57.68ms
step:794/2330 train_time:45799ms step_avg:57.68ms
step:795/2330 train_time:45856ms step_avg:57.68ms
step:796/2330 train_time:45915ms step_avg:57.68ms
step:797/2330 train_time:45972ms step_avg:57.68ms
step:798/2330 train_time:46031ms step_avg:57.68ms
step:799/2330 train_time:46087ms step_avg:57.68ms
step:800/2330 train_time:46148ms step_avg:57.68ms
step:801/2330 train_time:46205ms step_avg:57.68ms
step:802/2330 train_time:46264ms step_avg:57.69ms
step:803/2330 train_time:46323ms step_avg:57.69ms
step:804/2330 train_time:46382ms step_avg:57.69ms
step:805/2330 train_time:46440ms step_avg:57.69ms
step:806/2330 train_time:46500ms step_avg:57.69ms
step:807/2330 train_time:46558ms step_avg:57.69ms
step:808/2330 train_time:46618ms step_avg:57.70ms
step:809/2330 train_time:46675ms step_avg:57.70ms
step:810/2330 train_time:46734ms step_avg:57.70ms
step:811/2330 train_time:46791ms step_avg:57.70ms
step:812/2330 train_time:46851ms step_avg:57.70ms
step:813/2330 train_time:46907ms step_avg:57.70ms
step:814/2330 train_time:46967ms step_avg:57.70ms
step:815/2330 train_time:47024ms step_avg:57.70ms
step:816/2330 train_time:47083ms step_avg:57.70ms
step:817/2330 train_time:47140ms step_avg:57.70ms
step:818/2330 train_time:47200ms step_avg:57.70ms
step:819/2330 train_time:47258ms step_avg:57.70ms
step:820/2330 train_time:47317ms step_avg:57.70ms
step:821/2330 train_time:47376ms step_avg:57.70ms
step:822/2330 train_time:47435ms step_avg:57.71ms
step:823/2330 train_time:47492ms step_avg:57.71ms
step:824/2330 train_time:47553ms step_avg:57.71ms
step:825/2330 train_time:47611ms step_avg:57.71ms
step:826/2330 train_time:47670ms step_avg:57.71ms
step:827/2330 train_time:47727ms step_avg:57.71ms
step:828/2330 train_time:47787ms step_avg:57.71ms
step:829/2330 train_time:47844ms step_avg:57.71ms
step:830/2330 train_time:47903ms step_avg:57.71ms
step:831/2330 train_time:47960ms step_avg:57.71ms
step:832/2330 train_time:48019ms step_avg:57.72ms
step:833/2330 train_time:48077ms step_avg:57.71ms
step:834/2330 train_time:48135ms step_avg:57.72ms
step:835/2330 train_time:48192ms step_avg:57.71ms
step:836/2330 train_time:48252ms step_avg:57.72ms
step:837/2330 train_time:48309ms step_avg:57.72ms
step:838/2330 train_time:48369ms step_avg:57.72ms
step:839/2330 train_time:48427ms step_avg:57.72ms
step:840/2330 train_time:48488ms step_avg:57.72ms
step:841/2330 train_time:48545ms step_avg:57.72ms
step:842/2330 train_time:48606ms step_avg:57.73ms
step:843/2330 train_time:48663ms step_avg:57.73ms
step:844/2330 train_time:48722ms step_avg:57.73ms
step:845/2330 train_time:48779ms step_avg:57.73ms
step:846/2330 train_time:48838ms step_avg:57.73ms
step:847/2330 train_time:48895ms step_avg:57.73ms
step:848/2330 train_time:48955ms step_avg:57.73ms
step:849/2330 train_time:49012ms step_avg:57.73ms
step:850/2330 train_time:49071ms step_avg:57.73ms
step:851/2330 train_time:49128ms step_avg:57.73ms
step:852/2330 train_time:49188ms step_avg:57.73ms
step:853/2330 train_time:49245ms step_avg:57.73ms
step:854/2330 train_time:49305ms step_avg:57.73ms
step:855/2330 train_time:49362ms step_avg:57.73ms
step:856/2330 train_time:49422ms step_avg:57.74ms
step:857/2330 train_time:49480ms step_avg:57.74ms
step:858/2330 train_time:49540ms step_avg:57.74ms
step:859/2330 train_time:49598ms step_avg:57.74ms
step:860/2330 train_time:49658ms step_avg:57.74ms
step:861/2330 train_time:49716ms step_avg:57.74ms
step:862/2330 train_time:49776ms step_avg:57.74ms
step:863/2330 train_time:49833ms step_avg:57.74ms
step:864/2330 train_time:49892ms step_avg:57.75ms
step:865/2330 train_time:49950ms step_avg:57.75ms
step:866/2330 train_time:50009ms step_avg:57.75ms
step:867/2330 train_time:50066ms step_avg:57.75ms
step:868/2330 train_time:50125ms step_avg:57.75ms
step:869/2330 train_time:50182ms step_avg:57.75ms
step:870/2330 train_time:50242ms step_avg:57.75ms
step:871/2330 train_time:50299ms step_avg:57.75ms
step:872/2330 train_time:50359ms step_avg:57.75ms
step:873/2330 train_time:50416ms step_avg:57.75ms
step:874/2330 train_time:50476ms step_avg:57.75ms
step:875/2330 train_time:50533ms step_avg:57.75ms
step:876/2330 train_time:50593ms step_avg:57.75ms
step:877/2330 train_time:50650ms step_avg:57.75ms
step:878/2330 train_time:50710ms step_avg:57.76ms
step:879/2330 train_time:50767ms step_avg:57.75ms
step:880/2330 train_time:50827ms step_avg:57.76ms
step:881/2330 train_time:50884ms step_avg:57.76ms
step:882/2330 train_time:50944ms step_avg:57.76ms
step:883/2330 train_time:51001ms step_avg:57.76ms
step:884/2330 train_time:51061ms step_avg:57.76ms
step:885/2330 train_time:51119ms step_avg:57.76ms
step:886/2330 train_time:51178ms step_avg:57.76ms
step:887/2330 train_time:51235ms step_avg:57.76ms
step:888/2330 train_time:51294ms step_avg:57.76ms
step:889/2330 train_time:51351ms step_avg:57.76ms
step:890/2330 train_time:51411ms step_avg:57.77ms
step:891/2330 train_time:51468ms step_avg:57.76ms
step:892/2330 train_time:51528ms step_avg:57.77ms
step:893/2330 train_time:51585ms step_avg:57.77ms
step:894/2330 train_time:51645ms step_avg:57.77ms
step:895/2330 train_time:51702ms step_avg:57.77ms
step:896/2330 train_time:51763ms step_avg:57.77ms
step:897/2330 train_time:51820ms step_avg:57.77ms
step:898/2330 train_time:51880ms step_avg:57.77ms
step:899/2330 train_time:51937ms step_avg:57.77ms
step:900/2330 train_time:51997ms step_avg:57.77ms
step:901/2330 train_time:52054ms step_avg:57.77ms
step:902/2330 train_time:52113ms step_avg:57.78ms
step:903/2330 train_time:52171ms step_avg:57.78ms
step:904/2330 train_time:52230ms step_avg:57.78ms
step:905/2330 train_time:52287ms step_avg:57.78ms
step:906/2330 train_time:52347ms step_avg:57.78ms
step:907/2330 train_time:52404ms step_avg:57.78ms
step:908/2330 train_time:52464ms step_avg:57.78ms
step:909/2330 train_time:52522ms step_avg:57.78ms
step:910/2330 train_time:52581ms step_avg:57.78ms
step:911/2330 train_time:52639ms step_avg:57.78ms
step:912/2330 train_time:52699ms step_avg:57.78ms
step:913/2330 train_time:52756ms step_avg:57.78ms
step:914/2330 train_time:52815ms step_avg:57.78ms
step:915/2330 train_time:52873ms step_avg:57.78ms
step:916/2330 train_time:52932ms step_avg:57.79ms
step:917/2330 train_time:52989ms step_avg:57.78ms
step:918/2330 train_time:53049ms step_avg:57.79ms
step:919/2330 train_time:53105ms step_avg:57.79ms
step:920/2330 train_time:53166ms step_avg:57.79ms
step:921/2330 train_time:53223ms step_avg:57.79ms
step:922/2330 train_time:53282ms step_avg:57.79ms
step:923/2330 train_time:53339ms step_avg:57.79ms
step:924/2330 train_time:53398ms step_avg:57.79ms
step:925/2330 train_time:53455ms step_avg:57.79ms
step:926/2330 train_time:53516ms step_avg:57.79ms
step:927/2330 train_time:53573ms step_avg:57.79ms
step:928/2330 train_time:53633ms step_avg:57.79ms
step:929/2330 train_time:53689ms step_avg:57.79ms
step:930/2330 train_time:53751ms step_avg:57.80ms
step:931/2330 train_time:53808ms step_avg:57.80ms
step:932/2330 train_time:53868ms step_avg:57.80ms
step:933/2330 train_time:53924ms step_avg:57.80ms
step:934/2330 train_time:53984ms step_avg:57.80ms
step:935/2330 train_time:54041ms step_avg:57.80ms
step:936/2330 train_time:54101ms step_avg:57.80ms
step:937/2330 train_time:54158ms step_avg:57.80ms
step:938/2330 train_time:54216ms step_avg:57.80ms
step:939/2330 train_time:54273ms step_avg:57.80ms
step:940/2330 train_time:54333ms step_avg:57.80ms
step:941/2330 train_time:54390ms step_avg:57.80ms
step:942/2330 train_time:54450ms step_avg:57.80ms
step:943/2330 train_time:54507ms step_avg:57.80ms
step:944/2330 train_time:54568ms step_avg:57.81ms
step:945/2330 train_time:54625ms step_avg:57.80ms
step:946/2330 train_time:54685ms step_avg:57.81ms
step:947/2330 train_time:54743ms step_avg:57.81ms
step:948/2330 train_time:54803ms step_avg:57.81ms
step:949/2330 train_time:54861ms step_avg:57.81ms
step:950/2330 train_time:54920ms step_avg:57.81ms
step:951/2330 train_time:54977ms step_avg:57.81ms
step:952/2330 train_time:55037ms step_avg:57.81ms
step:953/2330 train_time:55094ms step_avg:57.81ms
step:954/2330 train_time:55153ms step_avg:57.81ms
step:955/2330 train_time:55210ms step_avg:57.81ms
step:956/2330 train_time:55270ms step_avg:57.81ms
step:957/2330 train_time:55326ms step_avg:57.81ms
step:958/2330 train_time:55387ms step_avg:57.82ms
step:959/2330 train_time:55444ms step_avg:57.81ms
step:960/2330 train_time:55505ms step_avg:57.82ms
step:961/2330 train_time:55562ms step_avg:57.82ms
step:962/2330 train_time:55622ms step_avg:57.82ms
step:963/2330 train_time:55680ms step_avg:57.82ms
step:964/2330 train_time:55739ms step_avg:57.82ms
step:965/2330 train_time:55797ms step_avg:57.82ms
step:966/2330 train_time:55857ms step_avg:57.82ms
step:967/2330 train_time:55913ms step_avg:57.82ms
step:968/2330 train_time:55974ms step_avg:57.82ms
step:969/2330 train_time:56031ms step_avg:57.82ms
step:970/2330 train_time:56091ms step_avg:57.83ms
step:971/2330 train_time:56148ms step_avg:57.82ms
step:972/2330 train_time:56207ms step_avg:57.83ms
step:973/2330 train_time:56265ms step_avg:57.83ms
step:974/2330 train_time:56324ms step_avg:57.83ms
step:975/2330 train_time:56380ms step_avg:57.83ms
step:976/2330 train_time:56441ms step_avg:57.83ms
step:977/2330 train_time:56498ms step_avg:57.83ms
step:978/2330 train_time:56557ms step_avg:57.83ms
step:979/2330 train_time:56614ms step_avg:57.83ms
step:980/2330 train_time:56675ms step_avg:57.83ms
step:981/2330 train_time:56731ms step_avg:57.83ms
step:982/2330 train_time:56791ms step_avg:57.83ms
step:983/2330 train_time:56849ms step_avg:57.83ms
step:984/2330 train_time:56909ms step_avg:57.83ms
step:985/2330 train_time:56966ms step_avg:57.83ms
step:986/2330 train_time:57025ms step_avg:57.84ms
step:987/2330 train_time:57082ms step_avg:57.83ms
step:988/2330 train_time:57142ms step_avg:57.84ms
step:989/2330 train_time:57200ms step_avg:57.84ms
step:990/2330 train_time:57260ms step_avg:57.84ms
step:991/2330 train_time:57318ms step_avg:57.84ms
step:992/2330 train_time:57377ms step_avg:57.84ms
step:993/2330 train_time:57434ms step_avg:57.84ms
step:994/2330 train_time:57494ms step_avg:57.84ms
step:995/2330 train_time:57551ms step_avg:57.84ms
step:996/2330 train_time:57612ms step_avg:57.84ms
step:997/2330 train_time:57669ms step_avg:57.84ms
step:998/2330 train_time:57728ms step_avg:57.84ms
step:999/2330 train_time:57785ms step_avg:57.84ms
step:1000/2330 train_time:57845ms step_avg:57.84ms
step:1000/2330 val_loss:4.0644 train_time:57925ms step_avg:57.93ms
step:1001/2330 train_time:57945ms step_avg:57.89ms
step:1002/2330 train_time:57966ms step_avg:57.85ms
step:1003/2330 train_time:58018ms step_avg:57.84ms
step:1004/2330 train_time:58084ms step_avg:57.85ms
step:1005/2330 train_time:58139ms step_avg:57.85ms
step:1006/2330 train_time:58203ms step_avg:57.86ms
step:1007/2330 train_time:58260ms step_avg:57.85ms
step:1008/2330 train_time:58320ms step_avg:57.86ms
step:1009/2330 train_time:58376ms step_avg:57.86ms
step:1010/2330 train_time:58436ms step_avg:57.86ms
step:1011/2330 train_time:58492ms step_avg:57.86ms
step:1012/2330 train_time:58553ms step_avg:57.86ms
step:1013/2330 train_time:58609ms step_avg:57.86ms
step:1014/2330 train_time:58669ms step_avg:57.86ms
step:1015/2330 train_time:58725ms step_avg:57.86ms
step:1016/2330 train_time:58785ms step_avg:57.86ms
step:1017/2330 train_time:58844ms step_avg:57.86ms
step:1018/2330 train_time:58907ms step_avg:57.87ms
step:1019/2330 train_time:58966ms step_avg:57.87ms
step:1020/2330 train_time:59026ms step_avg:57.87ms
step:1021/2330 train_time:59085ms step_avg:57.87ms
step:1022/2330 train_time:59144ms step_avg:57.87ms
step:1023/2330 train_time:59202ms step_avg:57.87ms
step:1024/2330 train_time:59262ms step_avg:57.87ms
step:1025/2330 train_time:59319ms step_avg:57.87ms
step:1026/2330 train_time:59378ms step_avg:57.87ms
step:1027/2330 train_time:59434ms step_avg:57.87ms
step:1028/2330 train_time:59496ms step_avg:57.88ms
step:1029/2330 train_time:59552ms step_avg:57.87ms
step:1030/2330 train_time:59612ms step_avg:57.88ms
step:1031/2330 train_time:59668ms step_avg:57.87ms
step:1032/2330 train_time:59727ms step_avg:57.88ms
step:1033/2330 train_time:59784ms step_avg:57.87ms
step:1034/2330 train_time:59845ms step_avg:57.88ms
step:1035/2330 train_time:59903ms step_avg:57.88ms
step:1036/2330 train_time:59964ms step_avg:57.88ms
step:1037/2330 train_time:60021ms step_avg:57.88ms
step:1038/2330 train_time:60082ms step_avg:57.88ms
step:1039/2330 train_time:60140ms step_avg:57.88ms
step:1040/2330 train_time:60200ms step_avg:57.88ms
step:1041/2330 train_time:60257ms step_avg:57.88ms
step:1042/2330 train_time:60317ms step_avg:57.89ms
step:1043/2330 train_time:60373ms step_avg:57.88ms
step:1044/2330 train_time:60433ms step_avg:57.89ms
step:1045/2330 train_time:60489ms step_avg:57.88ms
step:1046/2330 train_time:60551ms step_avg:57.89ms
step:1047/2330 train_time:60607ms step_avg:57.89ms
step:1048/2330 train_time:60667ms step_avg:57.89ms
step:1049/2330 train_time:60724ms step_avg:57.89ms
step:1050/2330 train_time:60783ms step_avg:57.89ms
step:1051/2330 train_time:60841ms step_avg:57.89ms
step:1052/2330 train_time:60901ms step_avg:57.89ms
step:1053/2330 train_time:60958ms step_avg:57.89ms
step:1054/2330 train_time:61018ms step_avg:57.89ms
step:1055/2330 train_time:61075ms step_avg:57.89ms
step:1056/2330 train_time:61136ms step_avg:57.89ms
step:1057/2330 train_time:61192ms step_avg:57.89ms
step:1058/2330 train_time:61254ms step_avg:57.90ms
step:1059/2330 train_time:61311ms step_avg:57.89ms
step:1060/2330 train_time:61370ms step_avg:57.90ms
step:1061/2330 train_time:61427ms step_avg:57.90ms
step:1062/2330 train_time:61488ms step_avg:57.90ms
step:1063/2330 train_time:61544ms step_avg:57.90ms
step:1064/2330 train_time:61604ms step_avg:57.90ms
step:1065/2330 train_time:61661ms step_avg:57.90ms
step:1066/2330 train_time:61721ms step_avg:57.90ms
step:1067/2330 train_time:61777ms step_avg:57.90ms
step:1068/2330 train_time:61838ms step_avg:57.90ms
step:1069/2330 train_time:61894ms step_avg:57.90ms
step:1070/2330 train_time:61954ms step_avg:57.90ms
step:1071/2330 train_time:62011ms step_avg:57.90ms
step:1072/2330 train_time:62071ms step_avg:57.90ms
step:1073/2330 train_time:62128ms step_avg:57.90ms
step:1074/2330 train_time:62189ms step_avg:57.90ms
step:1075/2330 train_time:62246ms step_avg:57.90ms
step:1076/2330 train_time:62306ms step_avg:57.91ms
step:1077/2330 train_time:62364ms step_avg:57.91ms
step:1078/2330 train_time:62423ms step_avg:57.91ms
step:1079/2330 train_time:62481ms step_avg:57.91ms
step:1080/2330 train_time:62540ms step_avg:57.91ms
step:1081/2330 train_time:62597ms step_avg:57.91ms
step:1082/2330 train_time:62657ms step_avg:57.91ms
step:1083/2330 train_time:62714ms step_avg:57.91ms
step:1084/2330 train_time:62774ms step_avg:57.91ms
step:1085/2330 train_time:62830ms step_avg:57.91ms
step:1086/2330 train_time:62890ms step_avg:57.91ms
step:1087/2330 train_time:62947ms step_avg:57.91ms
step:1088/2330 train_time:63007ms step_avg:57.91ms
step:1089/2330 train_time:63064ms step_avg:57.91ms
step:1090/2330 train_time:63125ms step_avg:57.91ms
step:1091/2330 train_time:63182ms step_avg:57.91ms
step:1092/2330 train_time:63242ms step_avg:57.91ms
step:1093/2330 train_time:63299ms step_avg:57.91ms
step:1094/2330 train_time:63360ms step_avg:57.92ms
step:1095/2330 train_time:63416ms step_avg:57.91ms
step:1096/2330 train_time:63478ms step_avg:57.92ms
step:1097/2330 train_time:63535ms step_avg:57.92ms
step:1098/2330 train_time:63595ms step_avg:57.92ms
step:1099/2330 train_time:63651ms step_avg:57.92ms
step:1100/2330 train_time:63711ms step_avg:57.92ms
step:1101/2330 train_time:63768ms step_avg:57.92ms
step:1102/2330 train_time:63828ms step_avg:57.92ms
step:1103/2330 train_time:63885ms step_avg:57.92ms
step:1104/2330 train_time:63945ms step_avg:57.92ms
step:1105/2330 train_time:64002ms step_avg:57.92ms
step:1106/2330 train_time:64062ms step_avg:57.92ms
step:1107/2330 train_time:64120ms step_avg:57.92ms
step:1108/2330 train_time:64179ms step_avg:57.92ms
step:1109/2330 train_time:64237ms step_avg:57.92ms
step:1110/2330 train_time:64298ms step_avg:57.93ms
step:1111/2330 train_time:64355ms step_avg:57.93ms
step:1112/2330 train_time:64415ms step_avg:57.93ms
step:1113/2330 train_time:64472ms step_avg:57.93ms
step:1114/2330 train_time:64532ms step_avg:57.93ms
step:1115/2330 train_time:64588ms step_avg:57.93ms
step:1116/2330 train_time:64648ms step_avg:57.93ms
step:1117/2330 train_time:64705ms step_avg:57.93ms
step:1118/2330 train_time:64765ms step_avg:57.93ms
step:1119/2330 train_time:64822ms step_avg:57.93ms
step:1120/2330 train_time:64883ms step_avg:57.93ms
step:1121/2330 train_time:64939ms step_avg:57.93ms
step:1122/2330 train_time:64999ms step_avg:57.93ms
step:1123/2330 train_time:65056ms step_avg:57.93ms
step:1124/2330 train_time:65117ms step_avg:57.93ms
step:1125/2330 train_time:65174ms step_avg:57.93ms
step:1126/2330 train_time:65234ms step_avg:57.93ms
step:1127/2330 train_time:65291ms step_avg:57.93ms
step:1128/2330 train_time:65351ms step_avg:57.94ms
step:1129/2330 train_time:65408ms step_avg:57.93ms
step:1130/2330 train_time:65469ms step_avg:57.94ms
step:1131/2330 train_time:65526ms step_avg:57.94ms
step:1132/2330 train_time:65585ms step_avg:57.94ms
step:1133/2330 train_time:65642ms step_avg:57.94ms
step:1134/2330 train_time:65702ms step_avg:57.94ms
step:1135/2330 train_time:65759ms step_avg:57.94ms
step:1136/2330 train_time:65819ms step_avg:57.94ms
step:1137/2330 train_time:65876ms step_avg:57.94ms
step:1138/2330 train_time:65936ms step_avg:57.94ms
step:1139/2330 train_time:65992ms step_avg:57.94ms
step:1140/2330 train_time:66053ms step_avg:57.94ms
step:1141/2330 train_time:66110ms step_avg:57.94ms
step:1142/2330 train_time:66170ms step_avg:57.94ms
step:1143/2330 train_time:66227ms step_avg:57.94ms
step:1144/2330 train_time:66287ms step_avg:57.94ms
step:1145/2330 train_time:66344ms step_avg:57.94ms
step:1146/2330 train_time:66404ms step_avg:57.94ms
step:1147/2330 train_time:66461ms step_avg:57.94ms
step:1148/2330 train_time:66522ms step_avg:57.95ms
step:1149/2330 train_time:66579ms step_avg:57.94ms
step:1150/2330 train_time:66639ms step_avg:57.95ms
step:1151/2330 train_time:66695ms step_avg:57.95ms
step:1152/2330 train_time:66755ms step_avg:57.95ms
step:1153/2330 train_time:66812ms step_avg:57.95ms
step:1154/2330 train_time:66872ms step_avg:57.95ms
step:1155/2330 train_time:66928ms step_avg:57.95ms
step:1156/2330 train_time:66989ms step_avg:57.95ms
step:1157/2330 train_time:67047ms step_avg:57.95ms
step:1158/2330 train_time:67107ms step_avg:57.95ms
step:1159/2330 train_time:67164ms step_avg:57.95ms
step:1160/2330 train_time:67223ms step_avg:57.95ms
step:1161/2330 train_time:67281ms step_avg:57.95ms
step:1162/2330 train_time:67341ms step_avg:57.95ms
step:1163/2330 train_time:67398ms step_avg:57.95ms
step:1164/2330 train_time:67457ms step_avg:57.95ms
step:1165/2330 train_time:67514ms step_avg:57.95ms
step:1166/2330 train_time:67576ms step_avg:57.96ms
step:1167/2330 train_time:67632ms step_avg:57.95ms
step:1168/2330 train_time:67693ms step_avg:57.96ms
step:1169/2330 train_time:67750ms step_avg:57.96ms
step:1170/2330 train_time:67810ms step_avg:57.96ms
step:1171/2330 train_time:67866ms step_avg:57.96ms
step:1172/2330 train_time:67926ms step_avg:57.96ms
step:1173/2330 train_time:67984ms step_avg:57.96ms
step:1174/2330 train_time:68043ms step_avg:57.96ms
step:1175/2330 train_time:68101ms step_avg:57.96ms
step:1176/2330 train_time:68161ms step_avg:57.96ms
step:1177/2330 train_time:68218ms step_avg:57.96ms
step:1178/2330 train_time:68278ms step_avg:57.96ms
step:1179/2330 train_time:68335ms step_avg:57.96ms
step:1180/2330 train_time:68396ms step_avg:57.96ms
step:1181/2330 train_time:68453ms step_avg:57.96ms
step:1182/2330 train_time:68513ms step_avg:57.96ms
step:1183/2330 train_time:68570ms step_avg:57.96ms
step:1184/2330 train_time:68630ms step_avg:57.96ms
step:1185/2330 train_time:68688ms step_avg:57.96ms
step:1186/2330 train_time:68748ms step_avg:57.97ms
step:1187/2330 train_time:68805ms step_avg:57.97ms
step:1188/2330 train_time:68864ms step_avg:57.97ms
step:1189/2330 train_time:68921ms step_avg:57.97ms
step:1190/2330 train_time:68980ms step_avg:57.97ms
step:1191/2330 train_time:69036ms step_avg:57.96ms
step:1192/2330 train_time:69098ms step_avg:57.97ms
step:1193/2330 train_time:69154ms step_avg:57.97ms
step:1194/2330 train_time:69215ms step_avg:57.97ms
step:1195/2330 train_time:69271ms step_avg:57.97ms
step:1196/2330 train_time:69332ms step_avg:57.97ms
step:1197/2330 train_time:69389ms step_avg:57.97ms
step:1198/2330 train_time:69450ms step_avg:57.97ms
step:1199/2330 train_time:69508ms step_avg:57.97ms
step:1200/2330 train_time:69568ms step_avg:57.97ms
step:1201/2330 train_time:69625ms step_avg:57.97ms
step:1202/2330 train_time:69685ms step_avg:57.97ms
step:1203/2330 train_time:69742ms step_avg:57.97ms
step:1204/2330 train_time:69801ms step_avg:57.97ms
step:1205/2330 train_time:69858ms step_avg:57.97ms
step:1206/2330 train_time:69918ms step_avg:57.98ms
step:1207/2330 train_time:69976ms step_avg:57.97ms
step:1208/2330 train_time:70036ms step_avg:57.98ms
step:1209/2330 train_time:70093ms step_avg:57.98ms
step:1210/2330 train_time:70153ms step_avg:57.98ms
step:1211/2330 train_time:70210ms step_avg:57.98ms
step:1212/2330 train_time:70270ms step_avg:57.98ms
step:1213/2330 train_time:70327ms step_avg:57.98ms
step:1214/2330 train_time:70388ms step_avg:57.98ms
step:1215/2330 train_time:70445ms step_avg:57.98ms
step:1216/2330 train_time:70505ms step_avg:57.98ms
step:1217/2330 train_time:70563ms step_avg:57.98ms
step:1218/2330 train_time:70623ms step_avg:57.98ms
step:1219/2330 train_time:70681ms step_avg:57.98ms
step:1220/2330 train_time:70740ms step_avg:57.98ms
step:1221/2330 train_time:70797ms step_avg:57.98ms
step:1222/2330 train_time:70857ms step_avg:57.98ms
step:1223/2330 train_time:70913ms step_avg:57.98ms
step:1224/2330 train_time:70974ms step_avg:57.99ms
step:1225/2330 train_time:71031ms step_avg:57.98ms
step:1226/2330 train_time:71092ms step_avg:57.99ms
step:1227/2330 train_time:71148ms step_avg:57.99ms
step:1228/2330 train_time:71209ms step_avg:57.99ms
step:1229/2330 train_time:71266ms step_avg:57.99ms
step:1230/2330 train_time:71326ms step_avg:57.99ms
step:1231/2330 train_time:71383ms step_avg:57.99ms
step:1232/2330 train_time:71443ms step_avg:57.99ms
step:1233/2330 train_time:71501ms step_avg:57.99ms
step:1234/2330 train_time:71561ms step_avg:57.99ms
step:1235/2330 train_time:71618ms step_avg:57.99ms
step:1236/2330 train_time:71680ms step_avg:57.99ms
step:1237/2330 train_time:71736ms step_avg:57.99ms
step:1238/2330 train_time:71798ms step_avg:57.99ms
step:1239/2330 train_time:71854ms step_avg:57.99ms
step:1240/2330 train_time:71914ms step_avg:58.00ms
step:1241/2330 train_time:71971ms step_avg:57.99ms
step:1242/2330 train_time:72031ms step_avg:58.00ms
step:1243/2330 train_time:72088ms step_avg:58.00ms
step:1244/2330 train_time:72148ms step_avg:58.00ms
step:1245/2330 train_time:72205ms step_avg:58.00ms
step:1246/2330 train_time:72266ms step_avg:58.00ms
step:1247/2330 train_time:72323ms step_avg:58.00ms
step:1248/2330 train_time:72383ms step_avg:58.00ms
step:1249/2330 train_time:72440ms step_avg:58.00ms
step:1250/2330 train_time:72500ms step_avg:58.00ms
step:1250/2330 val_loss:3.9859 train_time:72582ms step_avg:58.07ms
step:1251/2330 train_time:72601ms step_avg:58.03ms
step:1252/2330 train_time:72622ms step_avg:58.00ms
step:1253/2330 train_time:72678ms step_avg:58.00ms
step:1254/2330 train_time:72745ms step_avg:58.01ms
step:1255/2330 train_time:72802ms step_avg:58.01ms
step:1256/2330 train_time:72864ms step_avg:58.01ms
step:1257/2330 train_time:72920ms step_avg:58.01ms
step:1258/2330 train_time:72980ms step_avg:58.01ms
step:1259/2330 train_time:73037ms step_avg:58.01ms
step:1260/2330 train_time:73097ms step_avg:58.01ms
step:1261/2330 train_time:73153ms step_avg:58.01ms
step:1262/2330 train_time:73212ms step_avg:58.01ms
step:1263/2330 train_time:73268ms step_avg:58.01ms
step:1264/2330 train_time:73328ms step_avg:58.01ms
step:1265/2330 train_time:73384ms step_avg:58.01ms
step:1266/2330 train_time:73443ms step_avg:58.01ms
step:1267/2330 train_time:73500ms step_avg:58.01ms
step:1268/2330 train_time:73560ms step_avg:58.01ms
step:1269/2330 train_time:73618ms step_avg:58.01ms
step:1270/2330 train_time:73681ms step_avg:58.02ms
step:1271/2330 train_time:73738ms step_avg:58.02ms
step:1272/2330 train_time:73800ms step_avg:58.02ms
step:1273/2330 train_time:73857ms step_avg:58.02ms
step:1274/2330 train_time:73918ms step_avg:58.02ms
step:1275/2330 train_time:73974ms step_avg:58.02ms
step:1276/2330 train_time:74035ms step_avg:58.02ms
step:1277/2330 train_time:74092ms step_avg:58.02ms
step:1278/2330 train_time:74151ms step_avg:58.02ms
step:1279/2330 train_time:74207ms step_avg:58.02ms
step:1280/2330 train_time:74267ms step_avg:58.02ms
step:1281/2330 train_time:74323ms step_avg:58.02ms
step:1282/2330 train_time:74383ms step_avg:58.02ms
step:1283/2330 train_time:74441ms step_avg:58.02ms
step:1284/2330 train_time:74500ms step_avg:58.02ms
step:1285/2330 train_time:74556ms step_avg:58.02ms
step:1286/2330 train_time:74618ms step_avg:58.02ms
step:1287/2330 train_time:74675ms step_avg:58.02ms
step:1288/2330 train_time:74739ms step_avg:58.03ms
step:1289/2330 train_time:74796ms step_avg:58.03ms
step:1290/2330 train_time:74856ms step_avg:58.03ms
step:1291/2330 train_time:74913ms step_avg:58.03ms
step:1292/2330 train_time:74974ms step_avg:58.03ms
step:1293/2330 train_time:75031ms step_avg:58.03ms
step:1294/2330 train_time:75091ms step_avg:58.03ms
step:1295/2330 train_time:75147ms step_avg:58.03ms
step:1296/2330 train_time:75207ms step_avg:58.03ms
step:1297/2330 train_time:75263ms step_avg:58.03ms
step:1298/2330 train_time:75322ms step_avg:58.03ms
step:1299/2330 train_time:75380ms step_avg:58.03ms
step:1300/2330 train_time:75439ms step_avg:58.03ms
step:1301/2330 train_time:75496ms step_avg:58.03ms
step:1302/2330 train_time:75556ms step_avg:58.03ms
step:1303/2330 train_time:75613ms step_avg:58.03ms
step:1304/2330 train_time:75674ms step_avg:58.03ms
step:1305/2330 train_time:75732ms step_avg:58.03ms
step:1306/2330 train_time:75792ms step_avg:58.03ms
step:1307/2330 train_time:75851ms step_avg:58.03ms
step:1308/2330 train_time:75910ms step_avg:58.04ms
step:1309/2330 train_time:75967ms step_avg:58.03ms
step:1310/2330 train_time:76028ms step_avg:58.04ms
step:1311/2330 train_time:76084ms step_avg:58.04ms
step:1312/2330 train_time:76146ms step_avg:58.04ms
step:1313/2330 train_time:76201ms step_avg:58.04ms
step:1314/2330 train_time:76262ms step_avg:58.04ms
step:1315/2330 train_time:76318ms step_avg:58.04ms
step:1316/2330 train_time:76379ms step_avg:58.04ms
step:1317/2330 train_time:76435ms step_avg:58.04ms
step:1318/2330 train_time:76495ms step_avg:58.04ms
step:1319/2330 train_time:76552ms step_avg:58.04ms
step:1320/2330 train_time:76612ms step_avg:58.04ms
step:1321/2330 train_time:76669ms step_avg:58.04ms
step:1322/2330 train_time:76731ms step_avg:58.04ms
step:1323/2330 train_time:76788ms step_avg:58.04ms
step:1324/2330 train_time:76849ms step_avg:58.04ms
step:1325/2330 train_time:76907ms step_avg:58.04ms
step:1326/2330 train_time:76967ms step_avg:58.04ms
step:1327/2330 train_time:77024ms step_avg:58.04ms
step:1328/2330 train_time:77085ms step_avg:58.05ms
step:1329/2330 train_time:77142ms step_avg:58.05ms
step:1330/2330 train_time:77202ms step_avg:58.05ms
step:1331/2330 train_time:77259ms step_avg:58.05ms
step:1332/2330 train_time:77319ms step_avg:58.05ms
step:1333/2330 train_time:77375ms step_avg:58.05ms
step:1334/2330 train_time:77435ms step_avg:58.05ms
step:1335/2330 train_time:77492ms step_avg:58.05ms
step:1336/2330 train_time:77551ms step_avg:58.05ms
step:1337/2330 train_time:77609ms step_avg:58.05ms
step:1338/2330 train_time:77669ms step_avg:58.05ms
step:1339/2330 train_time:77727ms step_avg:58.05ms
step:1340/2330 train_time:77787ms step_avg:58.05ms
step:1341/2330 train_time:77844ms step_avg:58.05ms
step:1342/2330 train_time:77905ms step_avg:58.05ms
step:1343/2330 train_time:77962ms step_avg:58.05ms
step:1344/2330 train_time:78023ms step_avg:58.05ms
step:1345/2330 train_time:78079ms step_avg:58.05ms
step:1346/2330 train_time:78139ms step_avg:58.05ms
step:1347/2330 train_time:78196ms step_avg:58.05ms
step:1348/2330 train_time:78256ms step_avg:58.05ms
step:1349/2330 train_time:78313ms step_avg:58.05ms
step:1350/2330 train_time:78373ms step_avg:58.05ms
step:1351/2330 train_time:78430ms step_avg:58.05ms
step:1352/2330 train_time:78489ms step_avg:58.05ms
step:1353/2330 train_time:78546ms step_avg:58.05ms
step:1354/2330 train_time:78606ms step_avg:58.05ms
step:1355/2330 train_time:78662ms step_avg:58.05ms
step:1356/2330 train_time:78724ms step_avg:58.06ms
step:1357/2330 train_time:78781ms step_avg:58.06ms
step:1358/2330 train_time:78841ms step_avg:58.06ms
step:1359/2330 train_time:78897ms step_avg:58.06ms
step:1360/2330 train_time:78958ms step_avg:58.06ms
step:1361/2330 train_time:79015ms step_avg:58.06ms
step:1362/2330 train_time:79077ms step_avg:58.06ms
step:1363/2330 train_time:79134ms step_avg:58.06ms
step:1364/2330 train_time:79195ms step_avg:58.06ms
step:1365/2330 train_time:79252ms step_avg:58.06ms
step:1366/2330 train_time:79311ms step_avg:58.06ms
step:1367/2330 train_time:79368ms step_avg:58.06ms
step:1368/2330 train_time:79428ms step_avg:58.06ms
step:1369/2330 train_time:79486ms step_avg:58.06ms
step:1370/2330 train_time:79545ms step_avg:58.06ms
step:1371/2330 train_time:79602ms step_avg:58.06ms
step:1372/2330 train_time:79662ms step_avg:58.06ms
step:1373/2330 train_time:79719ms step_avg:58.06ms
step:1374/2330 train_time:79780ms step_avg:58.06ms
step:1375/2330 train_time:79836ms step_avg:58.06ms
step:1376/2330 train_time:79897ms step_avg:58.06ms
step:1377/2330 train_time:79954ms step_avg:58.06ms
step:1378/2330 train_time:80014ms step_avg:58.07ms
step:1379/2330 train_time:80071ms step_avg:58.06ms
step:1380/2330 train_time:80132ms step_avg:58.07ms
step:1381/2330 train_time:80189ms step_avg:58.07ms
step:1382/2330 train_time:80250ms step_avg:58.07ms
step:1383/2330 train_time:80307ms step_avg:58.07ms
step:1384/2330 train_time:80367ms step_avg:58.07ms
step:1385/2330 train_time:80424ms step_avg:58.07ms
step:1386/2330 train_time:80484ms step_avg:58.07ms
step:1387/2330 train_time:80540ms step_avg:58.07ms
step:1388/2330 train_time:80601ms step_avg:58.07ms
step:1389/2330 train_time:80657ms step_avg:58.07ms
step:1390/2330 train_time:80718ms step_avg:58.07ms
step:1391/2330 train_time:80775ms step_avg:58.07ms
step:1392/2330 train_time:80835ms step_avg:58.07ms
step:1393/2330 train_time:80892ms step_avg:58.07ms
step:1394/2330 train_time:80952ms step_avg:58.07ms
step:1395/2330 train_time:81009ms step_avg:58.07ms
step:1396/2330 train_time:81068ms step_avg:58.07ms
step:1397/2330 train_time:81125ms step_avg:58.07ms
step:1398/2330 train_time:81187ms step_avg:58.07ms
step:1399/2330 train_time:81244ms step_avg:58.07ms
step:1400/2330 train_time:81304ms step_avg:58.07ms
step:1401/2330 train_time:81361ms step_avg:58.07ms
step:1402/2330 train_time:81421ms step_avg:58.08ms
step:1403/2330 train_time:81478ms step_avg:58.07ms
step:1404/2330 train_time:81538ms step_avg:58.08ms
step:1405/2330 train_time:81595ms step_avg:58.07ms
step:1406/2330 train_time:81655ms step_avg:58.08ms
step:1407/2330 train_time:81712ms step_avg:58.08ms
step:1408/2330 train_time:81772ms step_avg:58.08ms
step:1409/2330 train_time:81829ms step_avg:58.08ms
step:1410/2330 train_time:81889ms step_avg:58.08ms
step:1411/2330 train_time:81947ms step_avg:58.08ms
step:1412/2330 train_time:82007ms step_avg:58.08ms
step:1413/2330 train_time:82064ms step_avg:58.08ms
step:1414/2330 train_time:82125ms step_avg:58.08ms
step:1415/2330 train_time:82182ms step_avg:58.08ms
step:1416/2330 train_time:82243ms step_avg:58.08ms
step:1417/2330 train_time:82300ms step_avg:58.08ms
step:1418/2330 train_time:82361ms step_avg:58.08ms
step:1419/2330 train_time:82417ms step_avg:58.08ms
step:1420/2330 train_time:82478ms step_avg:58.08ms
step:1421/2330 train_time:82534ms step_avg:58.08ms
step:1422/2330 train_time:82594ms step_avg:58.08ms
step:1423/2330 train_time:82651ms step_avg:58.08ms
step:1424/2330 train_time:82711ms step_avg:58.08ms
step:1425/2330 train_time:82768ms step_avg:58.08ms
step:1426/2330 train_time:82828ms step_avg:58.08ms
step:1427/2330 train_time:82885ms step_avg:58.08ms
step:1428/2330 train_time:82945ms step_avg:58.08ms
step:1429/2330 train_time:83002ms step_avg:58.08ms
step:1430/2330 train_time:83063ms step_avg:58.09ms
step:1431/2330 train_time:83121ms step_avg:58.09ms
step:1432/2330 train_time:83181ms step_avg:58.09ms
step:1433/2330 train_time:83238ms step_avg:58.09ms
step:1434/2330 train_time:83298ms step_avg:58.09ms
step:1435/2330 train_time:83355ms step_avg:58.09ms
step:1436/2330 train_time:83416ms step_avg:58.09ms
step:1437/2330 train_time:83473ms step_avg:58.09ms
step:1438/2330 train_time:83533ms step_avg:58.09ms
step:1439/2330 train_time:83590ms step_avg:58.09ms
step:1440/2330 train_time:83650ms step_avg:58.09ms
step:1441/2330 train_time:83706ms step_avg:58.09ms
step:1442/2330 train_time:83766ms step_avg:58.09ms
step:1443/2330 train_time:83822ms step_avg:58.09ms
step:1444/2330 train_time:83884ms step_avg:58.09ms
step:1445/2330 train_time:83941ms step_avg:58.09ms
step:1446/2330 train_time:84001ms step_avg:58.09ms
step:1447/2330 train_time:84058ms step_avg:58.09ms
step:1448/2330 train_time:84118ms step_avg:58.09ms
step:1449/2330 train_time:84176ms step_avg:58.09ms
step:1450/2330 train_time:84236ms step_avg:58.09ms
step:1451/2330 train_time:84293ms step_avg:58.09ms
step:1452/2330 train_time:84353ms step_avg:58.09ms
step:1453/2330 train_time:84410ms step_avg:58.09ms
step:1454/2330 train_time:84470ms step_avg:58.09ms
step:1455/2330 train_time:84526ms step_avg:58.09ms
step:1456/2330 train_time:84587ms step_avg:58.10ms
step:1457/2330 train_time:84644ms step_avg:58.09ms
step:1458/2330 train_time:84705ms step_avg:58.10ms
step:1459/2330 train_time:84761ms step_avg:58.10ms
step:1460/2330 train_time:84822ms step_avg:58.10ms
step:1461/2330 train_time:84879ms step_avg:58.10ms
step:1462/2330 train_time:84938ms step_avg:58.10ms
step:1463/2330 train_time:84995ms step_avg:58.10ms
step:1464/2330 train_time:85055ms step_avg:58.10ms
step:1465/2330 train_time:85113ms step_avg:58.10ms
step:1466/2330 train_time:85174ms step_avg:58.10ms
step:1467/2330 train_time:85231ms step_avg:58.10ms
step:1468/2330 train_time:85290ms step_avg:58.10ms
step:1469/2330 train_time:85348ms step_avg:58.10ms
step:1470/2330 train_time:85408ms step_avg:58.10ms
step:1471/2330 train_time:85465ms step_avg:58.10ms
step:1472/2330 train_time:85525ms step_avg:58.10ms
step:1473/2330 train_time:85582ms step_avg:58.10ms
step:1474/2330 train_time:85642ms step_avg:58.10ms
step:1475/2330 train_time:85699ms step_avg:58.10ms
step:1476/2330 train_time:85759ms step_avg:58.10ms
step:1477/2330 train_time:85815ms step_avg:58.10ms
step:1478/2330 train_time:85876ms step_avg:58.10ms
step:1479/2330 train_time:85932ms step_avg:58.10ms
step:1480/2330 train_time:85993ms step_avg:58.10ms
step:1481/2330 train_time:86050ms step_avg:58.10ms
step:1482/2330 train_time:86110ms step_avg:58.10ms
step:1483/2330 train_time:86167ms step_avg:58.10ms
step:1484/2330 train_time:86227ms step_avg:58.10ms
step:1485/2330 train_time:86284ms step_avg:58.10ms
step:1486/2330 train_time:86345ms step_avg:58.11ms
step:1487/2330 train_time:86401ms step_avg:58.10ms
step:1488/2330 train_time:86462ms step_avg:58.11ms
step:1489/2330 train_time:86519ms step_avg:58.11ms
step:1490/2330 train_time:86580ms step_avg:58.11ms
step:1491/2330 train_time:86638ms step_avg:58.11ms
step:1492/2330 train_time:86697ms step_avg:58.11ms
step:1493/2330 train_time:86754ms step_avg:58.11ms
step:1494/2330 train_time:86814ms step_avg:58.11ms
step:1495/2330 train_time:86871ms step_avg:58.11ms
step:1496/2330 train_time:86930ms step_avg:58.11ms
step:1497/2330 train_time:86988ms step_avg:58.11ms
step:1498/2330 train_time:87047ms step_avg:58.11ms
step:1499/2330 train_time:87105ms step_avg:58.11ms
step:1500/2330 train_time:87165ms step_avg:58.11ms
step:1500/2330 val_loss:3.9053 train_time:87246ms step_avg:58.16ms
step:1501/2330 train_time:87266ms step_avg:58.14ms
step:1502/2330 train_time:87287ms step_avg:58.11ms
step:1503/2330 train_time:87347ms step_avg:58.12ms
step:1504/2330 train_time:87411ms step_avg:58.12ms
step:1505/2330 train_time:87470ms step_avg:58.12ms
step:1506/2330 train_time:87530ms step_avg:58.12ms
step:1507/2330 train_time:87587ms step_avg:58.12ms
step:1508/2330 train_time:87646ms step_avg:58.12ms
step:1509/2330 train_time:87703ms step_avg:58.12ms
step:1510/2330 train_time:87762ms step_avg:58.12ms
step:1511/2330 train_time:87818ms step_avg:58.12ms
step:1512/2330 train_time:87878ms step_avg:58.12ms
step:1513/2330 train_time:87934ms step_avg:58.12ms
step:1514/2330 train_time:87994ms step_avg:58.12ms
step:1515/2330 train_time:88051ms step_avg:58.12ms
step:1516/2330 train_time:88110ms step_avg:58.12ms
step:1517/2330 train_time:88167ms step_avg:58.12ms
step:1518/2330 train_time:88229ms step_avg:58.12ms
step:1519/2330 train_time:88286ms step_avg:58.12ms
step:1520/2330 train_time:88351ms step_avg:58.13ms
step:1521/2330 train_time:88409ms step_avg:58.13ms
step:1522/2330 train_time:88472ms step_avg:58.13ms
step:1523/2330 train_time:88529ms step_avg:58.13ms
step:1524/2330 train_time:88589ms step_avg:58.13ms
step:1525/2330 train_time:88647ms step_avg:58.13ms
step:1526/2330 train_time:88706ms step_avg:58.13ms
step:1527/2330 train_time:88763ms step_avg:58.13ms
step:1528/2330 train_time:88822ms step_avg:58.13ms
step:1529/2330 train_time:88879ms step_avg:58.13ms
step:1530/2330 train_time:88938ms step_avg:58.13ms
step:1531/2330 train_time:88994ms step_avg:58.13ms
step:1532/2330 train_time:89055ms step_avg:58.13ms
step:1533/2330 train_time:89112ms step_avg:58.13ms
step:1534/2330 train_time:89172ms step_avg:58.13ms
step:1535/2330 train_time:89231ms step_avg:58.13ms
step:1536/2330 train_time:89293ms step_avg:58.13ms
step:1537/2330 train_time:89351ms step_avg:58.13ms
step:1538/2330 train_time:89413ms step_avg:58.14ms
step:1539/2330 train_time:89471ms step_avg:58.14ms
step:1540/2330 train_time:89532ms step_avg:58.14ms
step:1541/2330 train_time:89590ms step_avg:58.14ms
step:1542/2330 train_time:89650ms step_avg:58.14ms
step:1543/2330 train_time:89708ms step_avg:58.14ms
step:1544/2330 train_time:89769ms step_avg:58.14ms
step:1545/2330 train_time:89826ms step_avg:58.14ms
step:1546/2330 train_time:89887ms step_avg:58.14ms
step:1547/2330 train_time:89943ms step_avg:58.14ms
step:1548/2330 train_time:90004ms step_avg:58.14ms
step:1549/2330 train_time:90061ms step_avg:58.14ms
step:1550/2330 train_time:90123ms step_avg:58.14ms
step:1551/2330 train_time:90180ms step_avg:58.14ms
step:1552/2330 train_time:90243ms step_avg:58.15ms
step:1553/2330 train_time:90300ms step_avg:58.15ms
step:1554/2330 train_time:90363ms step_avg:58.15ms
step:1555/2330 train_time:90420ms step_avg:58.15ms
step:1556/2330 train_time:90482ms step_avg:58.15ms
step:1557/2330 train_time:90538ms step_avg:58.15ms
step:1558/2330 train_time:90601ms step_avg:58.15ms
step:1559/2330 train_time:90657ms step_avg:58.15ms
step:1560/2330 train_time:90718ms step_avg:58.15ms
step:1561/2330 train_time:90775ms step_avg:58.15ms
step:1562/2330 train_time:90837ms step_avg:58.15ms
step:1563/2330 train_time:90895ms step_avg:58.15ms
step:1564/2330 train_time:90955ms step_avg:58.16ms
step:1565/2330 train_time:91013ms step_avg:58.16ms
step:1566/2330 train_time:91073ms step_avg:58.16ms
step:1567/2330 train_time:91131ms step_avg:58.16ms
step:1568/2330 train_time:91191ms step_avg:58.16ms
step:1569/2330 train_time:91248ms step_avg:58.16ms
step:1570/2330 train_time:91310ms step_avg:58.16ms
step:1571/2330 train_time:91369ms step_avg:58.16ms
step:1572/2330 train_time:91429ms step_avg:58.16ms
step:1573/2330 train_time:91487ms step_avg:58.16ms
step:1574/2330 train_time:91548ms step_avg:58.16ms
step:1575/2330 train_time:91606ms step_avg:58.16ms
step:1576/2330 train_time:91667ms step_avg:58.16ms
step:1577/2330 train_time:91725ms step_avg:58.16ms
step:1578/2330 train_time:91786ms step_avg:58.17ms
step:1579/2330 train_time:91843ms step_avg:58.17ms
step:1580/2330 train_time:91903ms step_avg:58.17ms
step:1581/2330 train_time:91960ms step_avg:58.17ms
step:1582/2330 train_time:92021ms step_avg:58.17ms
step:1583/2330 train_time:92077ms step_avg:58.17ms
step:1584/2330 train_time:92140ms step_avg:58.17ms
step:1585/2330 train_time:92197ms step_avg:58.17ms
step:1586/2330 train_time:92258ms step_avg:58.17ms
step:1587/2330 train_time:92315ms step_avg:58.17ms
step:1588/2330 train_time:92376ms step_avg:58.17ms
step:1589/2330 train_time:92434ms step_avg:58.17ms
step:1590/2330 train_time:92496ms step_avg:58.17ms
step:1591/2330 train_time:92554ms step_avg:58.17ms
step:1592/2330 train_time:92614ms step_avg:58.17ms
step:1593/2330 train_time:92672ms step_avg:58.17ms
step:1594/2330 train_time:92734ms step_avg:58.18ms
step:1595/2330 train_time:92792ms step_avg:58.18ms
step:1596/2330 train_time:92853ms step_avg:58.18ms
step:1597/2330 train_time:92911ms step_avg:58.18ms
step:1598/2330 train_time:92971ms step_avg:58.18ms
step:1599/2330 train_time:93029ms step_avg:58.18ms
step:1600/2330 train_time:93090ms step_avg:58.18ms
step:1601/2330 train_time:93148ms step_avg:58.18ms
step:1602/2330 train_time:93208ms step_avg:58.18ms
step:1603/2330 train_time:93265ms step_avg:58.18ms
step:1604/2330 train_time:93327ms step_avg:58.18ms
step:1605/2330 train_time:93385ms step_avg:58.18ms
step:1606/2330 train_time:93446ms step_avg:58.19ms
step:1607/2330 train_time:93503ms step_avg:58.19ms
step:1608/2330 train_time:93565ms step_avg:58.19ms
step:1609/2330 train_time:93622ms step_avg:58.19ms
step:1610/2330 train_time:93684ms step_avg:58.19ms
step:1611/2330 train_time:93741ms step_avg:58.19ms
step:1612/2330 train_time:93803ms step_avg:58.19ms
step:1613/2330 train_time:93859ms step_avg:58.19ms
step:1614/2330 train_time:93921ms step_avg:58.19ms
step:1615/2330 train_time:93978ms step_avg:58.19ms
step:1616/2330 train_time:94040ms step_avg:58.19ms
step:1617/2330 train_time:94096ms step_avg:58.19ms
step:1618/2330 train_time:94159ms step_avg:58.19ms
step:1619/2330 train_time:94216ms step_avg:58.19ms
step:1620/2330 train_time:94277ms step_avg:58.20ms
step:1621/2330 train_time:94334ms step_avg:58.20ms
step:1622/2330 train_time:94395ms step_avg:58.20ms
step:1623/2330 train_time:94454ms step_avg:58.20ms
step:1624/2330 train_time:94514ms step_avg:58.20ms
step:1625/2330 train_time:94571ms step_avg:58.20ms
step:1626/2330 train_time:94631ms step_avg:58.20ms
step:1627/2330 train_time:94690ms step_avg:58.20ms
step:1628/2330 train_time:94750ms step_avg:58.20ms
step:1629/2330 train_time:94809ms step_avg:58.20ms
step:1630/2330 train_time:94870ms step_avg:58.20ms
step:1631/2330 train_time:94926ms step_avg:58.20ms
step:1632/2330 train_time:94988ms step_avg:58.20ms
step:1633/2330 train_time:95045ms step_avg:58.20ms
step:1634/2330 train_time:95106ms step_avg:58.20ms
step:1635/2330 train_time:95164ms step_avg:58.20ms
step:1636/2330 train_time:95225ms step_avg:58.21ms
step:1637/2330 train_time:95281ms step_avg:58.20ms
step:1638/2330 train_time:95344ms step_avg:58.21ms
step:1639/2330 train_time:95400ms step_avg:58.21ms
step:1640/2330 train_time:95463ms step_avg:58.21ms
step:1641/2330 train_time:95519ms step_avg:58.21ms
step:1642/2330 train_time:95581ms step_avg:58.21ms
step:1643/2330 train_time:95637ms step_avg:58.21ms
step:1644/2330 train_time:95699ms step_avg:58.21ms
step:1645/2330 train_time:95757ms step_avg:58.21ms
step:1646/2330 train_time:95818ms step_avg:58.21ms
step:1647/2330 train_time:95876ms step_avg:58.21ms
step:1648/2330 train_time:95937ms step_avg:58.21ms
step:1649/2330 train_time:95994ms step_avg:58.21ms
step:1650/2330 train_time:96055ms step_avg:58.22ms
step:1651/2330 train_time:96113ms step_avg:58.22ms
step:1652/2330 train_time:96174ms step_avg:58.22ms
step:1653/2330 train_time:96233ms step_avg:58.22ms
step:1654/2330 train_time:96293ms step_avg:58.22ms
step:1655/2330 train_time:96351ms step_avg:58.22ms
step:1656/2330 train_time:96411ms step_avg:58.22ms
step:1657/2330 train_time:96470ms step_avg:58.22ms
step:1658/2330 train_time:96530ms step_avg:58.22ms
step:1659/2330 train_time:96587ms step_avg:58.22ms
step:1660/2330 train_time:96647ms step_avg:58.22ms
step:1661/2330 train_time:96704ms step_avg:58.22ms
step:1662/2330 train_time:96766ms step_avg:58.22ms
step:1663/2330 train_time:96822ms step_avg:58.22ms
step:1664/2330 train_time:96884ms step_avg:58.22ms
step:1665/2330 train_time:96941ms step_avg:58.22ms
step:1666/2330 train_time:97004ms step_avg:58.23ms
step:1667/2330 train_time:97061ms step_avg:58.23ms
step:1668/2330 train_time:97122ms step_avg:58.23ms
step:1669/2330 train_time:97179ms step_avg:58.23ms
step:1670/2330 train_time:97241ms step_avg:58.23ms
step:1671/2330 train_time:97298ms step_avg:58.23ms
step:1672/2330 train_time:97359ms step_avg:58.23ms
step:1673/2330 train_time:97416ms step_avg:58.23ms
step:1674/2330 train_time:97477ms step_avg:58.23ms
step:1675/2330 train_time:97534ms step_avg:58.23ms
step:1676/2330 train_time:97595ms step_avg:58.23ms
step:1677/2330 train_time:97653ms step_avg:58.23ms
step:1678/2330 train_time:97713ms step_avg:58.23ms
step:1679/2330 train_time:97772ms step_avg:58.23ms
step:1680/2330 train_time:97832ms step_avg:58.23ms
step:1681/2330 train_time:97891ms step_avg:58.23ms
step:1682/2330 train_time:97952ms step_avg:58.24ms
step:1683/2330 train_time:98009ms step_avg:58.23ms
step:1684/2330 train_time:98070ms step_avg:58.24ms
step:1685/2330 train_time:98128ms step_avg:58.24ms
step:1686/2330 train_time:98189ms step_avg:58.24ms
step:1687/2330 train_time:98247ms step_avg:58.24ms
step:1688/2330 train_time:98308ms step_avg:58.24ms
step:1689/2330 train_time:98365ms step_avg:58.24ms
step:1690/2330 train_time:98427ms step_avg:58.24ms
step:1691/2330 train_time:98484ms step_avg:58.24ms
step:1692/2330 train_time:98545ms step_avg:58.24ms
step:1693/2330 train_time:98603ms step_avg:58.24ms
step:1694/2330 train_time:98664ms step_avg:58.24ms
step:1695/2330 train_time:98721ms step_avg:58.24ms
step:1696/2330 train_time:98783ms step_avg:58.24ms
step:1697/2330 train_time:98840ms step_avg:58.24ms
step:1698/2330 train_time:98902ms step_avg:58.25ms
step:1699/2330 train_time:98959ms step_avg:58.25ms
step:1700/2330 train_time:99020ms step_avg:58.25ms
step:1701/2330 train_time:99076ms step_avg:58.25ms
step:1702/2330 train_time:99138ms step_avg:58.25ms
step:1703/2330 train_time:99195ms step_avg:58.25ms
step:1704/2330 train_time:99257ms step_avg:58.25ms
step:1705/2330 train_time:99315ms step_avg:58.25ms
step:1706/2330 train_time:99375ms step_avg:58.25ms
step:1707/2330 train_time:99433ms step_avg:58.25ms
step:1708/2330 train_time:99494ms step_avg:58.25ms
step:1709/2330 train_time:99552ms step_avg:58.25ms
step:1710/2330 train_time:99613ms step_avg:58.25ms
step:1711/2330 train_time:99671ms step_avg:58.25ms
step:1712/2330 train_time:99732ms step_avg:58.25ms
step:1713/2330 train_time:99790ms step_avg:58.25ms
step:1714/2330 train_time:99849ms step_avg:58.26ms
step:1715/2330 train_time:99907ms step_avg:58.25ms
step:1716/2330 train_time:99968ms step_avg:58.26ms
step:1717/2330 train_time:100025ms step_avg:58.26ms
step:1718/2330 train_time:100086ms step_avg:58.26ms
step:1719/2330 train_time:100143ms step_avg:58.26ms
step:1720/2330 train_time:100205ms step_avg:58.26ms
step:1721/2330 train_time:100261ms step_avg:58.26ms
step:1722/2330 train_time:100323ms step_avg:58.26ms
step:1723/2330 train_time:100379ms step_avg:58.26ms
step:1724/2330 train_time:100442ms step_avg:58.26ms
step:1725/2330 train_time:100499ms step_avg:58.26ms
step:1726/2330 train_time:100562ms step_avg:58.26ms
step:1727/2330 train_time:100618ms step_avg:58.26ms
step:1728/2330 train_time:100679ms step_avg:58.26ms
step:1729/2330 train_time:100736ms step_avg:58.26ms
step:1730/2330 train_time:100798ms step_avg:58.26ms
step:1731/2330 train_time:100855ms step_avg:58.26ms
step:1732/2330 train_time:100916ms step_avg:58.27ms
step:1733/2330 train_time:100974ms step_avg:58.27ms
step:1734/2330 train_time:101035ms step_avg:58.27ms
step:1735/2330 train_time:101093ms step_avg:58.27ms
step:1736/2330 train_time:101155ms step_avg:58.27ms
step:1737/2330 train_time:101213ms step_avg:58.27ms
step:1738/2330 train_time:101273ms step_avg:58.27ms
step:1739/2330 train_time:101331ms step_avg:58.27ms
step:1740/2330 train_time:101392ms step_avg:58.27ms
step:1741/2330 train_time:101450ms step_avg:58.27ms
step:1742/2330 train_time:101510ms step_avg:58.27ms
step:1743/2330 train_time:101568ms step_avg:58.27ms
step:1744/2330 train_time:101628ms step_avg:58.27ms
step:1745/2330 train_time:101685ms step_avg:58.27ms
step:1746/2330 train_time:101746ms step_avg:58.27ms
step:1747/2330 train_time:101803ms step_avg:58.27ms
step:1748/2330 train_time:101864ms step_avg:58.27ms
step:1749/2330 train_time:101921ms step_avg:58.27ms
step:1750/2330 train_time:101983ms step_avg:58.28ms
step:1750/2330 val_loss:3.8201 train_time:102065ms step_avg:58.32ms
step:1751/2330 train_time:102085ms step_avg:58.30ms
step:1752/2330 train_time:102106ms step_avg:58.28ms
step:1753/2330 train_time:102159ms step_avg:58.28ms
step:1754/2330 train_time:102226ms step_avg:58.28ms
step:1755/2330 train_time:102282ms step_avg:58.28ms
step:1756/2330 train_time:102347ms step_avg:58.28ms
step:1757/2330 train_time:102404ms step_avg:58.28ms
step:1758/2330 train_time:102463ms step_avg:58.28ms
step:1759/2330 train_time:102520ms step_avg:58.28ms
step:1760/2330 train_time:102579ms step_avg:58.28ms
step:1761/2330 train_time:102636ms step_avg:58.28ms
step:1762/2330 train_time:102696ms step_avg:58.28ms
step:1763/2330 train_time:102752ms step_avg:58.28ms
step:1764/2330 train_time:102812ms step_avg:58.28ms
step:1765/2330 train_time:102869ms step_avg:58.28ms
step:1766/2330 train_time:102928ms step_avg:58.28ms
step:1767/2330 train_time:102987ms step_avg:58.28ms
step:1768/2330 train_time:103054ms step_avg:58.29ms
step:1769/2330 train_time:103112ms step_avg:58.29ms
step:1770/2330 train_time:103173ms step_avg:58.29ms
step:1771/2330 train_time:103231ms step_avg:58.29ms
step:1772/2330 train_time:103293ms step_avg:58.29ms
step:1773/2330 train_time:103350ms step_avg:58.29ms
step:1774/2330 train_time:103412ms step_avg:58.29ms
step:1775/2330 train_time:103468ms step_avg:58.29ms
step:1776/2330 train_time:103529ms step_avg:58.29ms
step:1777/2330 train_time:103585ms step_avg:58.29ms
step:1778/2330 train_time:103647ms step_avg:58.29ms
step:1779/2330 train_time:103704ms step_avg:58.29ms
step:1780/2330 train_time:103763ms step_avg:58.29ms
step:1781/2330 train_time:103821ms step_avg:58.29ms
step:1782/2330 train_time:103880ms step_avg:58.29ms
step:1783/2330 train_time:103938ms step_avg:58.29ms
step:1784/2330 train_time:104001ms step_avg:58.30ms
step:1785/2330 train_time:104061ms step_avg:58.30ms
step:1786/2330 train_time:104122ms step_avg:58.30ms
step:1787/2330 train_time:104180ms step_avg:58.30ms
step:1788/2330 train_time:104242ms step_avg:58.30ms
step:1789/2330 train_time:104299ms step_avg:58.30ms
step:1790/2330 train_time:104360ms step_avg:58.30ms
step:1791/2330 train_time:104418ms step_avg:58.30ms
step:1792/2330 train_time:104479ms step_avg:58.30ms
step:1793/2330 train_time:104536ms step_avg:58.30ms
step:1794/2330 train_time:104596ms step_avg:58.30ms
step:1795/2330 train_time:104653ms step_avg:58.30ms
step:1796/2330 train_time:104712ms step_avg:58.30ms
step:1797/2330 train_time:104768ms step_avg:58.30ms
step:1798/2330 train_time:104829ms step_avg:58.30ms
step:1799/2330 train_time:104886ms step_avg:58.30ms
step:1800/2330 train_time:104947ms step_avg:58.30ms
step:1801/2330 train_time:105005ms step_avg:58.30ms
step:1802/2330 train_time:105066ms step_avg:58.31ms
step:1803/2330 train_time:105124ms step_avg:58.30ms
step:1804/2330 train_time:105184ms step_avg:58.31ms
step:1805/2330 train_time:105242ms step_avg:58.31ms
step:1806/2330 train_time:105303ms step_avg:58.31ms
step:1807/2330 train_time:105361ms step_avg:58.31ms
step:1808/2330 train_time:105421ms step_avg:58.31ms
step:1809/2330 train_time:105478ms step_avg:58.31ms
step:1810/2330 train_time:105540ms step_avg:58.31ms
step:1811/2330 train_time:105597ms step_avg:58.31ms
step:1812/2330 train_time:105658ms step_avg:58.31ms
step:1813/2330 train_time:105716ms step_avg:58.31ms
step:1814/2330 train_time:105776ms step_avg:58.31ms
step:1815/2330 train_time:105833ms step_avg:58.31ms
step:1816/2330 train_time:105893ms step_avg:58.31ms
step:1817/2330 train_time:105951ms step_avg:58.31ms
step:1818/2330 train_time:106012ms step_avg:58.31ms
step:1819/2330 train_time:106070ms step_avg:58.31ms
step:1820/2330 train_time:106131ms step_avg:58.31ms
step:1821/2330 train_time:106187ms step_avg:58.31ms
step:1822/2330 train_time:106249ms step_avg:58.31ms
step:1823/2330 train_time:106307ms step_avg:58.31ms
step:1824/2330 train_time:106370ms step_avg:58.32ms
step:1825/2330 train_time:106426ms step_avg:58.32ms
step:1826/2330 train_time:106488ms step_avg:58.32ms
step:1827/2330 train_time:106544ms step_avg:58.32ms
step:1828/2330 train_time:106606ms step_avg:58.32ms
step:1829/2330 train_time:106662ms step_avg:58.32ms
step:1830/2330 train_time:106723ms step_avg:58.32ms
step:1831/2330 train_time:106780ms step_avg:58.32ms
step:1832/2330 train_time:106841ms step_avg:58.32ms
step:1833/2330 train_time:106898ms step_avg:58.32ms
step:1834/2330 train_time:106960ms step_avg:58.32ms
step:1835/2330 train_time:107019ms step_avg:58.32ms
step:1836/2330 train_time:107080ms step_avg:58.32ms
step:1837/2330 train_time:107139ms step_avg:58.32ms
step:1838/2330 train_time:107199ms step_avg:58.32ms
step:1839/2330 train_time:107258ms step_avg:58.32ms
step:1840/2330 train_time:107318ms step_avg:58.33ms
step:1841/2330 train_time:107377ms step_avg:58.33ms
step:1842/2330 train_time:107438ms step_avg:58.33ms
step:1843/2330 train_time:107495ms step_avg:58.33ms
step:1844/2330 train_time:107555ms step_avg:58.33ms
step:1845/2330 train_time:107613ms step_avg:58.33ms
step:1846/2330 train_time:107673ms step_avg:58.33ms
step:1847/2330 train_time:107730ms step_avg:58.33ms
step:1848/2330 train_time:107790ms step_avg:58.33ms
step:1849/2330 train_time:107846ms step_avg:58.33ms
step:1850/2330 train_time:107907ms step_avg:58.33ms
step:1851/2330 train_time:107965ms step_avg:58.33ms
step:1852/2330 train_time:108025ms step_avg:58.33ms
step:1853/2330 train_time:108082ms step_avg:58.33ms
step:1854/2330 train_time:108143ms step_avg:58.33ms
step:1855/2330 train_time:108201ms step_avg:58.33ms
step:1856/2330 train_time:108263ms step_avg:58.33ms
step:1857/2330 train_time:108321ms step_avg:58.33ms
step:1858/2330 train_time:108382ms step_avg:58.33ms
step:1859/2330 train_time:108441ms step_avg:58.33ms
step:1860/2330 train_time:108501ms step_avg:58.33ms
step:1861/2330 train_time:108560ms step_avg:58.33ms
step:1862/2330 train_time:108621ms step_avg:58.34ms
step:1863/2330 train_time:108678ms step_avg:58.33ms
step:1864/2330 train_time:108738ms step_avg:58.34ms
step:1865/2330 train_time:108795ms step_avg:58.34ms
step:1866/2330 train_time:108855ms step_avg:58.34ms
step:1867/2330 train_time:108913ms step_avg:58.34ms
step:1868/2330 train_time:108973ms step_avg:58.34ms
step:1869/2330 train_time:109030ms step_avg:58.34ms
step:1870/2330 train_time:109091ms step_avg:58.34ms
step:1871/2330 train_time:109148ms step_avg:58.34ms
step:1872/2330 train_time:109210ms step_avg:58.34ms
step:1873/2330 train_time:109267ms step_avg:58.34ms
step:1874/2330 train_time:109329ms step_avg:58.34ms
step:1875/2330 train_time:109386ms step_avg:58.34ms
step:1876/2330 train_time:109447ms step_avg:58.34ms
step:1877/2330 train_time:109504ms step_avg:58.34ms
step:1878/2330 train_time:109566ms step_avg:58.34ms
step:1879/2330 train_time:109623ms step_avg:58.34ms
step:1880/2330 train_time:109684ms step_avg:58.34ms
step:1881/2330 train_time:109742ms step_avg:58.34ms
step:1882/2330 train_time:109802ms step_avg:58.34ms
step:1883/2330 train_time:109860ms step_avg:58.34ms
step:1884/2330 train_time:109921ms step_avg:58.34ms
step:1885/2330 train_time:109979ms step_avg:58.34ms
step:1886/2330 train_time:110040ms step_avg:58.35ms
step:1887/2330 train_time:110099ms step_avg:58.35ms
step:1888/2330 train_time:110159ms step_avg:58.35ms
step:1889/2330 train_time:110218ms step_avg:58.35ms
step:1890/2330 train_time:110278ms step_avg:58.35ms
step:1891/2330 train_time:110336ms step_avg:58.35ms
step:1892/2330 train_time:110396ms step_avg:58.35ms
step:1893/2330 train_time:110453ms step_avg:58.35ms
step:1894/2330 train_time:110513ms step_avg:58.35ms
step:1895/2330 train_time:110570ms step_avg:58.35ms
step:1896/2330 train_time:110631ms step_avg:58.35ms
step:1897/2330 train_time:110688ms step_avg:58.35ms
step:1898/2330 train_time:110750ms step_avg:58.35ms
step:1899/2330 train_time:110808ms step_avg:58.35ms
step:1900/2330 train_time:110868ms step_avg:58.35ms
step:1901/2330 train_time:110925ms step_avg:58.35ms
step:1902/2330 train_time:110986ms step_avg:58.35ms
step:1903/2330 train_time:111043ms step_avg:58.35ms
step:1904/2330 train_time:111105ms step_avg:58.35ms
step:1905/2330 train_time:111163ms step_avg:58.35ms
step:1906/2330 train_time:111223ms step_avg:58.35ms
step:1907/2330 train_time:111280ms step_avg:58.35ms
step:1908/2330 train_time:111342ms step_avg:58.36ms
step:1909/2330 train_time:111399ms step_avg:58.35ms
step:1910/2330 train_time:111460ms step_avg:58.36ms
step:1911/2330 train_time:111517ms step_avg:58.36ms
step:1912/2330 train_time:111578ms step_avg:58.36ms
step:1913/2330 train_time:111636ms step_avg:58.36ms
step:1914/2330 train_time:111697ms step_avg:58.36ms
step:1915/2330 train_time:111755ms step_avg:58.36ms
step:1916/2330 train_time:111815ms step_avg:58.36ms
step:1917/2330 train_time:111873ms step_avg:58.36ms
step:1918/2330 train_time:111933ms step_avg:58.36ms
step:1919/2330 train_time:111990ms step_avg:58.36ms
step:1920/2330 train_time:112052ms step_avg:58.36ms
step:1921/2330 train_time:112109ms step_avg:58.36ms
step:1922/2330 train_time:112170ms step_avg:58.36ms
step:1923/2330 train_time:112227ms step_avg:58.36ms
step:1924/2330 train_time:112288ms step_avg:58.36ms
step:1925/2330 train_time:112345ms step_avg:58.36ms
step:1926/2330 train_time:112407ms step_avg:58.36ms
step:1927/2330 train_time:112463ms step_avg:58.36ms
step:1928/2330 train_time:112524ms step_avg:58.36ms
step:1929/2330 train_time:112582ms step_avg:58.36ms
step:1930/2330 train_time:112642ms step_avg:58.36ms
step:1931/2330 train_time:112700ms step_avg:58.36ms
step:1932/2330 train_time:112763ms step_avg:58.37ms
step:1933/2330 train_time:112821ms step_avg:58.37ms
step:1934/2330 train_time:112881ms step_avg:58.37ms
step:1935/2330 train_time:112938ms step_avg:58.37ms
step:1936/2330 train_time:113001ms step_avg:58.37ms
step:1937/2330 train_time:113059ms step_avg:58.37ms
step:1938/2330 train_time:113119ms step_avg:58.37ms
step:1939/2330 train_time:113177ms step_avg:58.37ms
step:1940/2330 train_time:113237ms step_avg:58.37ms
step:1941/2330 train_time:113295ms step_avg:58.37ms
step:1942/2330 train_time:113356ms step_avg:58.37ms
step:1943/2330 train_time:113413ms step_avg:58.37ms
step:1944/2330 train_time:113474ms step_avg:58.37ms
step:1945/2330 train_time:113531ms step_avg:58.37ms
step:1946/2330 train_time:113592ms step_avg:58.37ms
step:1947/2330 train_time:113649ms step_avg:58.37ms
step:1948/2330 train_time:113710ms step_avg:58.37ms
step:1949/2330 train_time:113767ms step_avg:58.37ms
step:1950/2330 train_time:113828ms step_avg:58.37ms
step:1951/2330 train_time:113884ms step_avg:58.37ms
step:1952/2330 train_time:113946ms step_avg:58.37ms
step:1953/2330 train_time:114004ms step_avg:58.37ms
step:1954/2330 train_time:114064ms step_avg:58.37ms
step:1955/2330 train_time:114121ms step_avg:58.37ms
step:1956/2330 train_time:114182ms step_avg:58.38ms
step:1957/2330 train_time:114239ms step_avg:58.37ms
step:1958/2330 train_time:114301ms step_avg:58.38ms
step:1959/2330 train_time:114358ms step_avg:58.38ms
step:1960/2330 train_time:114421ms step_avg:58.38ms
step:1961/2330 train_time:114479ms step_avg:58.38ms
step:1962/2330 train_time:114538ms step_avg:58.38ms
step:1963/2330 train_time:114596ms step_avg:58.38ms
step:1964/2330 train_time:114657ms step_avg:58.38ms
step:1965/2330 train_time:114714ms step_avg:58.38ms
step:1966/2330 train_time:114774ms step_avg:58.38ms
step:1967/2330 train_time:114832ms step_avg:58.38ms
step:1968/2330 train_time:114892ms step_avg:58.38ms
step:1969/2330 train_time:114949ms step_avg:58.38ms
step:1970/2330 train_time:115011ms step_avg:58.38ms
step:1971/2330 train_time:115068ms step_avg:58.38ms
step:1972/2330 train_time:115129ms step_avg:58.38ms
step:1973/2330 train_time:115186ms step_avg:58.38ms
step:1974/2330 train_time:115249ms step_avg:58.38ms
step:1975/2330 train_time:115306ms step_avg:58.38ms
step:1976/2330 train_time:115367ms step_avg:58.38ms
step:1977/2330 train_time:115425ms step_avg:58.38ms
step:1978/2330 train_time:115485ms step_avg:58.38ms
step:1979/2330 train_time:115544ms step_avg:58.38ms
step:1980/2330 train_time:115604ms step_avg:58.39ms
step:1981/2330 train_time:115661ms step_avg:58.39ms
step:1982/2330 train_time:115722ms step_avg:58.39ms
step:1983/2330 train_time:115781ms step_avg:58.39ms
step:1984/2330 train_time:115840ms step_avg:58.39ms
step:1985/2330 train_time:115898ms step_avg:58.39ms
step:1986/2330 train_time:115959ms step_avg:58.39ms
step:1987/2330 train_time:116016ms step_avg:58.39ms
step:1988/2330 train_time:116076ms step_avg:58.39ms
step:1989/2330 train_time:116133ms step_avg:58.39ms
step:1990/2330 train_time:116194ms step_avg:58.39ms
step:1991/2330 train_time:116251ms step_avg:58.39ms
step:1992/2330 train_time:116312ms step_avg:58.39ms
step:1993/2330 train_time:116370ms step_avg:58.39ms
step:1994/2330 train_time:116431ms step_avg:58.39ms
step:1995/2330 train_time:116487ms step_avg:58.39ms
step:1996/2330 train_time:116550ms step_avg:58.39ms
step:1997/2330 train_time:116607ms step_avg:58.39ms
step:1998/2330 train_time:116668ms step_avg:58.39ms
step:1999/2330 train_time:116725ms step_avg:58.39ms
step:2000/2330 train_time:116787ms step_avg:58.39ms
step:2000/2330 val_loss:3.7572 train_time:116868ms step_avg:58.43ms
step:2001/2330 train_time:116888ms step_avg:58.41ms
step:2002/2330 train_time:116910ms step_avg:58.40ms
step:2003/2330 train_time:116971ms step_avg:58.40ms
step:2004/2330 train_time:117033ms step_avg:58.40ms
step:2005/2330 train_time:117090ms step_avg:58.40ms
step:2006/2330 train_time:117151ms step_avg:58.40ms
step:2007/2330 train_time:117208ms step_avg:58.40ms
step:2008/2330 train_time:117270ms step_avg:58.40ms
step:2009/2330 train_time:117327ms step_avg:58.40ms
step:2010/2330 train_time:117389ms step_avg:58.40ms
step:2011/2330 train_time:117447ms step_avg:58.40ms
step:2012/2330 train_time:117507ms step_avg:58.40ms
step:2013/2330 train_time:117564ms step_avg:58.40ms
step:2014/2330 train_time:117624ms step_avg:58.40ms
step:2015/2330 train_time:117681ms step_avg:58.40ms
step:2016/2330 train_time:117740ms step_avg:58.40ms
step:2017/2330 train_time:117798ms step_avg:58.40ms
step:2018/2330 train_time:117860ms step_avg:58.40ms
step:2019/2330 train_time:117919ms step_avg:58.40ms
step:2020/2330 train_time:117981ms step_avg:58.41ms
step:2021/2330 train_time:118039ms step_avg:58.41ms
step:2022/2330 train_time:118100ms step_avg:58.41ms
step:2023/2330 train_time:118157ms step_avg:58.41ms
step:2024/2330 train_time:118220ms step_avg:58.41ms
step:2025/2330 train_time:118276ms step_avg:58.41ms
step:2026/2330 train_time:118338ms step_avg:58.41ms
step:2027/2330 train_time:118395ms step_avg:58.41ms
step:2028/2330 train_time:118456ms step_avg:58.41ms
step:2029/2330 train_time:118512ms step_avg:58.41ms
step:2030/2330 train_time:118573ms step_avg:58.41ms
step:2031/2330 train_time:118629ms step_avg:58.41ms
step:2032/2330 train_time:118689ms step_avg:58.41ms
step:2033/2330 train_time:118746ms step_avg:58.41ms
step:2034/2330 train_time:118807ms step_avg:58.41ms
step:2035/2330 train_time:118865ms step_avg:58.41ms
step:2036/2330 train_time:118927ms step_avg:58.41ms
step:2037/2330 train_time:118985ms step_avg:58.41ms
step:2038/2330 train_time:119047ms step_avg:58.41ms
step:2039/2330 train_time:119105ms step_avg:58.41ms
step:2040/2330 train_time:119166ms step_avg:58.41ms
step:2041/2330 train_time:119224ms step_avg:58.41ms
step:2042/2330 train_time:119286ms step_avg:58.42ms
step:2043/2330 train_time:119345ms step_avg:58.42ms
step:2044/2330 train_time:119405ms step_avg:58.42ms
step:2045/2330 train_time:119463ms step_avg:58.42ms
step:2046/2330 train_time:119524ms step_avg:58.42ms
step:2047/2330 train_time:119582ms step_avg:58.42ms
step:2048/2330 train_time:119642ms step_avg:58.42ms
step:2049/2330 train_time:119699ms step_avg:58.42ms
step:2050/2330 train_time:119759ms step_avg:58.42ms
step:2051/2330 train_time:119816ms step_avg:58.42ms
step:2052/2330 train_time:119877ms step_avg:58.42ms
step:2053/2330 train_time:119935ms step_avg:58.42ms
step:2054/2330 train_time:119996ms step_avg:58.42ms
step:2055/2330 train_time:120054ms step_avg:58.42ms
step:2056/2330 train_time:120115ms step_avg:58.42ms
step:2057/2330 train_time:120173ms step_avg:58.42ms
step:2058/2330 train_time:120234ms step_avg:58.42ms
step:2059/2330 train_time:120291ms step_avg:58.42ms
step:2060/2330 train_time:120353ms step_avg:58.42ms
step:2061/2330 train_time:120409ms step_avg:58.42ms
step:2062/2330 train_time:120471ms step_avg:58.42ms
step:2063/2330 train_time:120528ms step_avg:58.42ms
step:2064/2330 train_time:120590ms step_avg:58.43ms
step:2065/2330 train_time:120647ms step_avg:58.42ms
step:2066/2330 train_time:120707ms step_avg:58.43ms
step:2067/2330 train_time:120764ms step_avg:58.42ms
step:2068/2330 train_time:120826ms step_avg:58.43ms
step:2069/2330 train_time:120883ms step_avg:58.43ms
step:2070/2330 train_time:120944ms step_avg:58.43ms
step:2071/2330 train_time:121003ms step_avg:58.43ms
step:2072/2330 train_time:121064ms step_avg:58.43ms
step:2073/2330 train_time:121123ms step_avg:58.43ms
step:2074/2330 train_time:121185ms step_avg:58.43ms
step:2075/2330 train_time:121244ms step_avg:58.43ms
step:2076/2330 train_time:121304ms step_avg:58.43ms
step:2077/2330 train_time:121363ms step_avg:58.43ms
step:2078/2330 train_time:121423ms step_avg:58.43ms
step:2079/2330 train_time:121480ms step_avg:58.43ms
step:2080/2330 train_time:121541ms step_avg:58.43ms
step:2081/2330 train_time:121598ms step_avg:58.43ms
step:2082/2330 train_time:121659ms step_avg:58.43ms
step:2083/2330 train_time:121715ms step_avg:58.43ms
step:2084/2330 train_time:121777ms step_avg:58.43ms
step:2085/2330 train_time:121833ms step_avg:58.43ms
step:2086/2330 train_time:121894ms step_avg:58.43ms
step:2087/2330 train_time:121950ms step_avg:58.43ms
step:2088/2330 train_time:122011ms step_avg:58.43ms
step:2089/2330 train_time:122069ms step_avg:58.43ms
step:2090/2330 train_time:122130ms step_avg:58.44ms
step:2091/2330 train_time:122187ms step_avg:58.43ms
step:2092/2330 train_time:122248ms step_avg:58.44ms
step:2093/2330 train_time:122307ms step_avg:58.44ms
step:2094/2330 train_time:122368ms step_avg:58.44ms
step:2095/2330 train_time:122426ms step_avg:58.44ms
step:2096/2330 train_time:122486ms step_avg:58.44ms
step:2097/2330 train_time:122545ms step_avg:58.44ms
step:2098/2330 train_time:122606ms step_avg:58.44ms
step:2099/2330 train_time:122663ms step_avg:58.44ms
step:2100/2330 train_time:122724ms step_avg:58.44ms
step:2101/2330 train_time:122782ms step_avg:58.44ms
step:2102/2330 train_time:122842ms step_avg:58.44ms
step:2103/2330 train_time:122899ms step_avg:58.44ms
step:2104/2330 train_time:122961ms step_avg:58.44ms
step:2105/2330 train_time:123018ms step_avg:58.44ms
step:2106/2330 train_time:123079ms step_avg:58.44ms
step:2107/2330 train_time:123136ms step_avg:58.44ms
step:2108/2330 train_time:123197ms step_avg:58.44ms
step:2109/2330 train_time:123255ms step_avg:58.44ms
step:2110/2330 train_time:123315ms step_avg:58.44ms
step:2111/2330 train_time:123373ms step_avg:58.44ms
step:2112/2330 train_time:123433ms step_avg:58.44ms
step:2113/2330 train_time:123491ms step_avg:58.44ms
step:2114/2330 train_time:123552ms step_avg:58.44ms
step:2115/2330 train_time:123609ms step_avg:58.44ms
step:2116/2330 train_time:123669ms step_avg:58.44ms
step:2117/2330 train_time:123727ms step_avg:58.44ms
step:2118/2330 train_time:123788ms step_avg:58.45ms
step:2119/2330 train_time:123846ms step_avg:58.45ms
step:2120/2330 train_time:123907ms step_avg:58.45ms
step:2121/2330 train_time:123964ms step_avg:58.45ms
step:2122/2330 train_time:124025ms step_avg:58.45ms
step:2123/2330 train_time:124084ms step_avg:58.45ms
step:2124/2330 train_time:124145ms step_avg:58.45ms
step:2125/2330 train_time:124204ms step_avg:58.45ms
step:2126/2330 train_time:124264ms step_avg:58.45ms
step:2127/2330 train_time:124321ms step_avg:58.45ms
step:2128/2330 train_time:124381ms step_avg:58.45ms
step:2129/2330 train_time:124439ms step_avg:58.45ms
step:2130/2330 train_time:124500ms step_avg:58.45ms
step:2131/2330 train_time:124558ms step_avg:58.45ms
step:2132/2330 train_time:124620ms step_avg:58.45ms
step:2133/2330 train_time:124677ms step_avg:58.45ms
step:2134/2330 train_time:124738ms step_avg:58.45ms
step:2135/2330 train_time:124794ms step_avg:58.45ms
step:2136/2330 train_time:124856ms step_avg:58.45ms
step:2137/2330 train_time:124912ms step_avg:58.45ms
step:2138/2330 train_time:124974ms step_avg:58.45ms
step:2139/2330 train_time:125031ms step_avg:58.45ms
step:2140/2330 train_time:125093ms step_avg:58.45ms
step:2141/2330 train_time:125150ms step_avg:58.45ms
step:2142/2330 train_time:125211ms step_avg:58.46ms
step:2143/2330 train_time:125268ms step_avg:58.45ms
step:2144/2330 train_time:125330ms step_avg:58.46ms
step:2145/2330 train_time:125388ms step_avg:58.46ms
step:2146/2330 train_time:125448ms step_avg:58.46ms
step:2147/2330 train_time:125507ms step_avg:58.46ms
step:2148/2330 train_time:125567ms step_avg:58.46ms
step:2149/2330 train_time:125625ms step_avg:58.46ms
step:2150/2330 train_time:125686ms step_avg:58.46ms
step:2151/2330 train_time:125744ms step_avg:58.46ms
step:2152/2330 train_time:125805ms step_avg:58.46ms
step:2153/2330 train_time:125863ms step_avg:58.46ms
step:2154/2330 train_time:125924ms step_avg:58.46ms
step:2155/2330 train_time:125981ms step_avg:58.46ms
step:2156/2330 train_time:126042ms step_avg:58.46ms
step:2157/2330 train_time:126100ms step_avg:58.46ms
step:2158/2330 train_time:126161ms step_avg:58.46ms
step:2159/2330 train_time:126218ms step_avg:58.46ms
step:2160/2330 train_time:126279ms step_avg:58.46ms
step:2161/2330 train_time:126336ms step_avg:58.46ms
step:2162/2330 train_time:126398ms step_avg:58.46ms
step:2163/2330 train_time:126455ms step_avg:58.46ms
step:2164/2330 train_time:126516ms step_avg:58.46ms
step:2165/2330 train_time:126573ms step_avg:58.46ms
step:2166/2330 train_time:126635ms step_avg:58.46ms
step:2167/2330 train_time:126692ms step_avg:58.46ms
step:2168/2330 train_time:126753ms step_avg:58.47ms
step:2169/2330 train_time:126810ms step_avg:58.46ms
step:2170/2330 train_time:126871ms step_avg:58.47ms
step:2171/2330 train_time:126929ms step_avg:58.47ms
step:2172/2330 train_time:126991ms step_avg:58.47ms
step:2173/2330 train_time:127048ms step_avg:58.47ms
step:2174/2330 train_time:127109ms step_avg:58.47ms
step:2175/2330 train_time:127168ms step_avg:58.47ms
step:2176/2330 train_time:127229ms step_avg:58.47ms
step:2177/2330 train_time:127287ms step_avg:58.47ms
step:2178/2330 train_time:127348ms step_avg:58.47ms
step:2179/2330 train_time:127405ms step_avg:58.47ms
step:2180/2330 train_time:127467ms step_avg:58.47ms
step:2181/2330 train_time:127525ms step_avg:58.47ms
step:2182/2330 train_time:127586ms step_avg:58.47ms
step:2183/2330 train_time:127645ms step_avg:58.47ms
step:2184/2330 train_time:127705ms step_avg:58.47ms
step:2185/2330 train_time:127762ms step_avg:58.47ms
step:2186/2330 train_time:127822ms step_avg:58.47ms
step:2187/2330 train_time:127880ms step_avg:58.47ms
step:2188/2330 train_time:127941ms step_avg:58.47ms
step:2189/2330 train_time:127998ms step_avg:58.47ms
step:2190/2330 train_time:128059ms step_avg:58.47ms
step:2191/2330 train_time:128116ms step_avg:58.47ms
step:2192/2330 train_time:128177ms step_avg:58.47ms
step:2193/2330 train_time:128234ms step_avg:58.47ms
step:2194/2330 train_time:128295ms step_avg:58.48ms
step:2195/2330 train_time:128352ms step_avg:58.47ms
step:2196/2330 train_time:128413ms step_avg:58.48ms
step:2197/2330 train_time:128471ms step_avg:58.48ms
step:2198/2330 train_time:128531ms step_avg:58.48ms
step:2199/2330 train_time:128589ms step_avg:58.48ms
step:2200/2330 train_time:128650ms step_avg:58.48ms
step:2201/2330 train_time:128706ms step_avg:58.48ms
step:2202/2330 train_time:128768ms step_avg:58.48ms
step:2203/2330 train_time:128825ms step_avg:58.48ms
step:2204/2330 train_time:128887ms step_avg:58.48ms
step:2205/2330 train_time:128944ms step_avg:58.48ms
step:2206/2330 train_time:129005ms step_avg:58.48ms
step:2207/2330 train_time:129063ms step_avg:58.48ms
step:2208/2330 train_time:129123ms step_avg:58.48ms
step:2209/2330 train_time:129182ms step_avg:58.48ms
step:2210/2330 train_time:129243ms step_avg:58.48ms
step:2211/2330 train_time:129302ms step_avg:58.48ms
step:2212/2330 train_time:129362ms step_avg:58.48ms
step:2213/2330 train_time:129420ms step_avg:58.48ms
step:2214/2330 train_time:129480ms step_avg:58.48ms
step:2215/2330 train_time:129537ms step_avg:58.48ms
step:2216/2330 train_time:129599ms step_avg:58.48ms
step:2217/2330 train_time:129656ms step_avg:58.48ms
step:2218/2330 train_time:129717ms step_avg:58.48ms
step:2219/2330 train_time:129774ms step_avg:58.48ms
step:2220/2330 train_time:129836ms step_avg:58.48ms
step:2221/2330 train_time:129892ms step_avg:58.48ms
step:2222/2330 train_time:129954ms step_avg:58.49ms
step:2223/2330 train_time:130010ms step_avg:58.48ms
step:2224/2330 train_time:130072ms step_avg:58.49ms
step:2225/2330 train_time:130129ms step_avg:58.48ms
step:2226/2330 train_time:130191ms step_avg:58.49ms
step:2227/2330 train_time:130248ms step_avg:58.49ms
step:2228/2330 train_time:130310ms step_avg:58.49ms
step:2229/2330 train_time:130367ms step_avg:58.49ms
step:2230/2330 train_time:130428ms step_avg:58.49ms
step:2231/2330 train_time:130487ms step_avg:58.49ms
step:2232/2330 train_time:130547ms step_avg:58.49ms
step:2233/2330 train_time:130605ms step_avg:58.49ms
step:2234/2330 train_time:130666ms step_avg:58.49ms
step:2235/2330 train_time:130724ms step_avg:58.49ms
step:2236/2330 train_time:130786ms step_avg:58.49ms
step:2237/2330 train_time:130843ms step_avg:58.49ms
step:2238/2330 train_time:130904ms step_avg:58.49ms
step:2239/2330 train_time:130961ms step_avg:58.49ms
step:2240/2330 train_time:131021ms step_avg:58.49ms
step:2241/2330 train_time:131080ms step_avg:58.49ms
step:2242/2330 train_time:131140ms step_avg:58.49ms
step:2243/2330 train_time:131198ms step_avg:58.49ms
step:2244/2330 train_time:131258ms step_avg:58.49ms
step:2245/2330 train_time:131315ms step_avg:58.49ms
step:2246/2330 train_time:131376ms step_avg:58.49ms
step:2247/2330 train_time:131433ms step_avg:58.49ms
step:2248/2330 train_time:131495ms step_avg:58.49ms
step:2249/2330 train_time:131552ms step_avg:58.49ms
step:2250/2330 train_time:131613ms step_avg:58.49ms
step:2250/2330 val_loss:3.7083 train_time:131696ms step_avg:58.53ms
step:2251/2330 train_time:131715ms step_avg:58.51ms
step:2252/2330 train_time:131735ms step_avg:58.50ms
step:2253/2330 train_time:131797ms step_avg:58.50ms
step:2254/2330 train_time:131862ms step_avg:58.50ms
step:2255/2330 train_time:131921ms step_avg:58.50ms
step:2256/2330 train_time:131982ms step_avg:58.50ms
step:2257/2330 train_time:132040ms step_avg:58.50ms
step:2258/2330 train_time:132100ms step_avg:58.50ms
step:2259/2330 train_time:132157ms step_avg:58.50ms
step:2260/2330 train_time:132216ms step_avg:58.50ms
step:2261/2330 train_time:132273ms step_avg:58.50ms
step:2262/2330 train_time:132333ms step_avg:58.50ms
step:2263/2330 train_time:132390ms step_avg:58.50ms
step:2264/2330 train_time:132450ms step_avg:58.50ms
step:2265/2330 train_time:132507ms step_avg:58.50ms
step:2266/2330 train_time:132566ms step_avg:58.50ms
step:2267/2330 train_time:132623ms step_avg:58.50ms
step:2268/2330 train_time:132685ms step_avg:58.50ms
step:2269/2330 train_time:132743ms step_avg:58.50ms
step:2270/2330 train_time:132808ms step_avg:58.51ms
step:2271/2330 train_time:132866ms step_avg:58.51ms
step:2272/2330 train_time:132928ms step_avg:58.51ms
step:2273/2330 train_time:132985ms step_avg:58.51ms
step:2274/2330 train_time:133049ms step_avg:58.51ms
step:2275/2330 train_time:133105ms step_avg:58.51ms
step:2276/2330 train_time:133166ms step_avg:58.51ms
step:2277/2330 train_time:133222ms step_avg:58.51ms
step:2278/2330 train_time:133283ms step_avg:58.51ms
step:2279/2330 train_time:133340ms step_avg:58.51ms
step:2280/2330 train_time:133400ms step_avg:58.51ms
step:2281/2330 train_time:133457ms step_avg:58.51ms
step:2282/2330 train_time:133518ms step_avg:58.51ms
step:2283/2330 train_time:133575ms step_avg:58.51ms
step:2284/2330 train_time:133636ms step_avg:58.51ms
step:2285/2330 train_time:133695ms step_avg:58.51ms
step:2286/2330 train_time:133756ms step_avg:58.51ms
step:2287/2330 train_time:133815ms step_avg:58.51ms
step:2288/2330 train_time:133876ms step_avg:58.51ms
step:2289/2330 train_time:133934ms step_avg:58.51ms
step:2290/2330 train_time:133996ms step_avg:58.51ms
step:2291/2330 train_time:134054ms step_avg:58.51ms
step:2292/2330 train_time:134114ms step_avg:58.51ms
step:2293/2330 train_time:134172ms step_avg:58.51ms
step:2294/2330 train_time:134232ms step_avg:58.51ms
step:2295/2330 train_time:134290ms step_avg:58.51ms
step:2296/2330 train_time:134349ms step_avg:58.51ms
step:2297/2330 train_time:134406ms step_avg:58.51ms
step:2298/2330 train_time:134466ms step_avg:58.51ms
step:2299/2330 train_time:134523ms step_avg:58.51ms
step:2300/2330 train_time:134583ms step_avg:58.51ms
step:2301/2330 train_time:134641ms step_avg:58.51ms
step:2302/2330 train_time:134702ms step_avg:58.52ms
step:2303/2330 train_time:134761ms step_avg:58.52ms
step:2304/2330 train_time:134822ms step_avg:58.52ms
step:2305/2330 train_time:134879ms step_avg:58.52ms
step:2306/2330 train_time:134941ms step_avg:58.52ms
step:2307/2330 train_time:135000ms step_avg:58.52ms
step:2308/2330 train_time:135060ms step_avg:58.52ms
step:2309/2330 train_time:135119ms step_avg:58.52ms
step:2310/2330 train_time:135180ms step_avg:58.52ms
step:2311/2330 train_time:135237ms step_avg:58.52ms
step:2312/2330 train_time:135298ms step_avg:58.52ms
step:2313/2330 train_time:135355ms step_avg:58.52ms
step:2314/2330 train_time:135416ms step_avg:58.52ms
step:2315/2330 train_time:135473ms step_avg:58.52ms
step:2316/2330 train_time:135533ms step_avg:58.52ms
step:2317/2330 train_time:135590ms step_avg:58.52ms
step:2318/2330 train_time:135651ms step_avg:58.52ms
step:2319/2330 train_time:135707ms step_avg:58.52ms
step:2320/2330 train_time:135770ms step_avg:58.52ms
step:2321/2330 train_time:135827ms step_avg:58.52ms
step:2322/2330 train_time:135888ms step_avg:58.52ms
step:2323/2330 train_time:135946ms step_avg:58.52ms
step:2324/2330 train_time:136009ms step_avg:58.52ms
step:2325/2330 train_time:136065ms step_avg:58.52ms
step:2326/2330 train_time:136128ms step_avg:58.52ms
step:2327/2330 train_time:136185ms step_avg:58.52ms
step:2328/2330 train_time:136247ms step_avg:58.53ms
step:2329/2330 train_time:136304ms step_avg:58.52ms
step:2330/2330 train_time:136364ms step_avg:58.53ms
step:2330/2330 val_loss:3.6928 train_time:136446ms step_avg:58.56ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
