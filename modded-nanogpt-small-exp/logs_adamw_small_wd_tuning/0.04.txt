import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:33:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:92ms step_avg:92.39ms
step:2/2330 train_time:184ms step_avg:92.08ms
step:3/2330 train_time:202ms step_avg:67.42ms
step:4/2330 train_time:221ms step_avg:55.26ms
step:5/2330 train_time:275ms step_avg:54.91ms
step:6/2330 train_time:333ms step_avg:55.52ms
step:7/2330 train_time:389ms step_avg:55.50ms
step:8/2330 train_time:447ms step_avg:55.91ms
step:9/2330 train_time:503ms step_avg:55.87ms
step:10/2330 train_time:561ms step_avg:56.09ms
step:11/2330 train_time:617ms step_avg:56.07ms
step:12/2330 train_time:675ms step_avg:56.23ms
step:13/2330 train_time:730ms step_avg:56.14ms
step:14/2330 train_time:789ms step_avg:56.34ms
step:15/2330 train_time:844ms step_avg:56.26ms
step:16/2330 train_time:903ms step_avg:56.44ms
step:17/2330 train_time:958ms step_avg:56.35ms
step:18/2330 train_time:1017ms step_avg:56.52ms
step:19/2330 train_time:1075ms step_avg:56.55ms
step:20/2330 train_time:1137ms step_avg:56.84ms
step:21/2330 train_time:1195ms step_avg:56.90ms
step:22/2330 train_time:1256ms step_avg:57.11ms
step:23/2330 train_time:1312ms step_avg:57.04ms
step:24/2330 train_time:1373ms step_avg:57.20ms
step:25/2330 train_time:1428ms step_avg:57.13ms
step:26/2330 train_time:1487ms step_avg:57.20ms
step:27/2330 train_time:1543ms step_avg:57.13ms
step:28/2330 train_time:1602ms step_avg:57.20ms
step:29/2330 train_time:1658ms step_avg:57.16ms
step:30/2330 train_time:1716ms step_avg:57.21ms
step:31/2330 train_time:1771ms step_avg:57.14ms
step:32/2330 train_time:1829ms step_avg:57.17ms
step:33/2330 train_time:1885ms step_avg:57.11ms
step:34/2330 train_time:1944ms step_avg:57.17ms
step:35/2330 train_time:1999ms step_avg:57.12ms
step:36/2330 train_time:2059ms step_avg:57.18ms
step:37/2330 train_time:2115ms step_avg:57.17ms
step:38/2330 train_time:2176ms step_avg:57.26ms
step:39/2330 train_time:2232ms step_avg:57.24ms
step:40/2330 train_time:2293ms step_avg:57.33ms
step:41/2330 train_time:2349ms step_avg:57.30ms
step:42/2330 train_time:2408ms step_avg:57.34ms
step:43/2330 train_time:2464ms step_avg:57.29ms
step:44/2330 train_time:2522ms step_avg:57.33ms
step:45/2330 train_time:2578ms step_avg:57.29ms
step:46/2330 train_time:2637ms step_avg:57.33ms
step:47/2330 train_time:2693ms step_avg:57.30ms
step:48/2330 train_time:2752ms step_avg:57.33ms
step:49/2330 train_time:2807ms step_avg:57.29ms
step:50/2330 train_time:2866ms step_avg:57.32ms
step:51/2330 train_time:2922ms step_avg:57.29ms
step:52/2330 train_time:2981ms step_avg:57.32ms
step:53/2330 train_time:3038ms step_avg:57.31ms
step:54/2330 train_time:3097ms step_avg:57.35ms
step:55/2330 train_time:3154ms step_avg:57.34ms
step:56/2330 train_time:3213ms step_avg:57.38ms
step:57/2330 train_time:3270ms step_avg:57.36ms
step:58/2330 train_time:3329ms step_avg:57.40ms
step:59/2330 train_time:3385ms step_avg:57.37ms
step:60/2330 train_time:3445ms step_avg:57.41ms
step:61/2330 train_time:3500ms step_avg:57.38ms
step:62/2330 train_time:3560ms step_avg:57.41ms
step:63/2330 train_time:3615ms step_avg:57.38ms
step:64/2330 train_time:3675ms step_avg:57.42ms
step:65/2330 train_time:3730ms step_avg:57.39ms
step:66/2330 train_time:3788ms step_avg:57.40ms
step:67/2330 train_time:3844ms step_avg:57.37ms
step:68/2330 train_time:3903ms step_avg:57.39ms
step:69/2330 train_time:3958ms step_avg:57.37ms
step:70/2330 train_time:4018ms step_avg:57.39ms
step:71/2330 train_time:4074ms step_avg:57.37ms
step:72/2330 train_time:4133ms step_avg:57.40ms
step:73/2330 train_time:4190ms step_avg:57.40ms
step:74/2330 train_time:4249ms step_avg:57.42ms
step:75/2330 train_time:4305ms step_avg:57.41ms
step:76/2330 train_time:4365ms step_avg:57.43ms
step:77/2330 train_time:4421ms step_avg:57.41ms
step:78/2330 train_time:4480ms step_avg:57.43ms
step:79/2330 train_time:4535ms step_avg:57.41ms
step:80/2330 train_time:4595ms step_avg:57.43ms
step:81/2330 train_time:4651ms step_avg:57.42ms
step:82/2330 train_time:4709ms step_avg:57.43ms
step:83/2330 train_time:4765ms step_avg:57.40ms
step:84/2330 train_time:4823ms step_avg:57.42ms
step:85/2330 train_time:4879ms step_avg:57.40ms
step:86/2330 train_time:4938ms step_avg:57.42ms
step:87/2330 train_time:4994ms step_avg:57.40ms
step:88/2330 train_time:5053ms step_avg:57.42ms
step:89/2330 train_time:5109ms step_avg:57.41ms
step:90/2330 train_time:5168ms step_avg:57.42ms
step:91/2330 train_time:5224ms step_avg:57.40ms
step:92/2330 train_time:5283ms step_avg:57.42ms
step:93/2330 train_time:5339ms step_avg:57.41ms
step:94/2330 train_time:5398ms step_avg:57.43ms
step:95/2330 train_time:5454ms step_avg:57.41ms
step:96/2330 train_time:5515ms step_avg:57.44ms
step:97/2330 train_time:5570ms step_avg:57.42ms
step:98/2330 train_time:5629ms step_avg:57.44ms
step:99/2330 train_time:5684ms step_avg:57.42ms
step:100/2330 train_time:5744ms step_avg:57.44ms
step:101/2330 train_time:5800ms step_avg:57.42ms
step:102/2330 train_time:5859ms step_avg:57.44ms
step:103/2330 train_time:5915ms step_avg:57.43ms
step:104/2330 train_time:5974ms step_avg:57.45ms
step:105/2330 train_time:6030ms step_avg:57.43ms
step:106/2330 train_time:6089ms step_avg:57.45ms
step:107/2330 train_time:6145ms step_avg:57.43ms
step:108/2330 train_time:6205ms step_avg:57.45ms
step:109/2330 train_time:6261ms step_avg:57.44ms
step:110/2330 train_time:6320ms step_avg:57.46ms
step:111/2330 train_time:6377ms step_avg:57.45ms
step:112/2330 train_time:6436ms step_avg:57.46ms
step:113/2330 train_time:6492ms step_avg:57.45ms
step:114/2330 train_time:6552ms step_avg:57.47ms
step:115/2330 train_time:6607ms step_avg:57.45ms
step:116/2330 train_time:6667ms step_avg:57.47ms
step:117/2330 train_time:6723ms step_avg:57.46ms
step:118/2330 train_time:6782ms step_avg:57.48ms
step:119/2330 train_time:6838ms step_avg:57.46ms
step:120/2330 train_time:6897ms step_avg:57.48ms
step:121/2330 train_time:6953ms step_avg:57.46ms
step:122/2330 train_time:7012ms step_avg:57.48ms
step:123/2330 train_time:7068ms step_avg:57.46ms
step:124/2330 train_time:7127ms step_avg:57.48ms
step:125/2330 train_time:7183ms step_avg:57.47ms
step:126/2330 train_time:7243ms step_avg:57.48ms
step:127/2330 train_time:7299ms step_avg:57.48ms
step:128/2330 train_time:7359ms step_avg:57.49ms
step:129/2330 train_time:7415ms step_avg:57.48ms
step:130/2330 train_time:7475ms step_avg:57.50ms
step:131/2330 train_time:7531ms step_avg:57.49ms
step:132/2330 train_time:7590ms step_avg:57.50ms
step:133/2330 train_time:7646ms step_avg:57.49ms
step:134/2330 train_time:7704ms step_avg:57.50ms
step:135/2330 train_time:7760ms step_avg:57.48ms
step:136/2330 train_time:7819ms step_avg:57.49ms
step:137/2330 train_time:7874ms step_avg:57.48ms
step:138/2330 train_time:7934ms step_avg:57.49ms
step:139/2330 train_time:7989ms step_avg:57.48ms
step:140/2330 train_time:8048ms step_avg:57.49ms
step:141/2330 train_time:8104ms step_avg:57.47ms
step:142/2330 train_time:8164ms step_avg:57.49ms
step:143/2330 train_time:8220ms step_avg:57.48ms
step:144/2330 train_time:8279ms step_avg:57.49ms
step:145/2330 train_time:8334ms step_avg:57.48ms
step:146/2330 train_time:8393ms step_avg:57.49ms
step:147/2330 train_time:8450ms step_avg:57.48ms
step:148/2330 train_time:8509ms step_avg:57.49ms
step:149/2330 train_time:8564ms step_avg:57.48ms
step:150/2330 train_time:8624ms step_avg:57.50ms
step:151/2330 train_time:8680ms step_avg:57.48ms
step:152/2330 train_time:8739ms step_avg:57.49ms
step:153/2330 train_time:8794ms step_avg:57.48ms
step:154/2330 train_time:8854ms step_avg:57.49ms
step:155/2330 train_time:8910ms step_avg:57.48ms
step:156/2330 train_time:8969ms step_avg:57.49ms
step:157/2330 train_time:9024ms step_avg:57.48ms
step:158/2330 train_time:9084ms step_avg:57.49ms
step:159/2330 train_time:9139ms step_avg:57.48ms
step:160/2330 train_time:9198ms step_avg:57.49ms
step:161/2330 train_time:9254ms step_avg:57.48ms
step:162/2330 train_time:9314ms step_avg:57.49ms
step:163/2330 train_time:9370ms step_avg:57.48ms
step:164/2330 train_time:9428ms step_avg:57.49ms
step:165/2330 train_time:9484ms step_avg:57.48ms
step:166/2330 train_time:9544ms step_avg:57.49ms
step:167/2330 train_time:9602ms step_avg:57.49ms
step:168/2330 train_time:9660ms step_avg:57.50ms
step:169/2330 train_time:9717ms step_avg:57.50ms
step:170/2330 train_time:9776ms step_avg:57.50ms
step:171/2330 train_time:9832ms step_avg:57.50ms
step:172/2330 train_time:9891ms step_avg:57.50ms
step:173/2330 train_time:9946ms step_avg:57.49ms
step:174/2330 train_time:10005ms step_avg:57.50ms
step:175/2330 train_time:10061ms step_avg:57.49ms
step:176/2330 train_time:10120ms step_avg:57.50ms
step:177/2330 train_time:10176ms step_avg:57.49ms
step:178/2330 train_time:10235ms step_avg:57.50ms
step:179/2330 train_time:10291ms step_avg:57.49ms
step:180/2330 train_time:10349ms step_avg:57.50ms
step:181/2330 train_time:10405ms step_avg:57.49ms
step:182/2330 train_time:10464ms step_avg:57.49ms
step:183/2330 train_time:10520ms step_avg:57.49ms
step:184/2330 train_time:10579ms step_avg:57.49ms
step:185/2330 train_time:10635ms step_avg:57.49ms
step:186/2330 train_time:10694ms step_avg:57.49ms
step:187/2330 train_time:10750ms step_avg:57.49ms
step:188/2330 train_time:10810ms step_avg:57.50ms
step:189/2330 train_time:10865ms step_avg:57.49ms
step:190/2330 train_time:10924ms step_avg:57.50ms
step:191/2330 train_time:10980ms step_avg:57.49ms
step:192/2330 train_time:11039ms step_avg:57.49ms
step:193/2330 train_time:11094ms step_avg:57.48ms
step:194/2330 train_time:11154ms step_avg:57.50ms
step:195/2330 train_time:11210ms step_avg:57.49ms
step:196/2330 train_time:11269ms step_avg:57.50ms
step:197/2330 train_time:11325ms step_avg:57.49ms
step:198/2330 train_time:11384ms step_avg:57.50ms
step:199/2330 train_time:11440ms step_avg:57.49ms
step:200/2330 train_time:11500ms step_avg:57.50ms
step:201/2330 train_time:11556ms step_avg:57.49ms
step:202/2330 train_time:11616ms step_avg:57.50ms
step:203/2330 train_time:11671ms step_avg:57.49ms
step:204/2330 train_time:11731ms step_avg:57.50ms
step:205/2330 train_time:11787ms step_avg:57.50ms
step:206/2330 train_time:11846ms step_avg:57.50ms
step:207/2330 train_time:11902ms step_avg:57.50ms
step:208/2330 train_time:11961ms step_avg:57.50ms
step:209/2330 train_time:12017ms step_avg:57.50ms
step:210/2330 train_time:12076ms step_avg:57.50ms
step:211/2330 train_time:12132ms step_avg:57.50ms
step:212/2330 train_time:12190ms step_avg:57.50ms
step:213/2330 train_time:12246ms step_avg:57.49ms
step:214/2330 train_time:12306ms step_avg:57.50ms
step:215/2330 train_time:12362ms step_avg:57.50ms
step:216/2330 train_time:12422ms step_avg:57.51ms
step:217/2330 train_time:12478ms step_avg:57.50ms
step:218/2330 train_time:12537ms step_avg:57.51ms
step:219/2330 train_time:12593ms step_avg:57.50ms
step:220/2330 train_time:12652ms step_avg:57.51ms
step:221/2330 train_time:12708ms step_avg:57.50ms
step:222/2330 train_time:12767ms step_avg:57.51ms
step:223/2330 train_time:12823ms step_avg:57.50ms
step:224/2330 train_time:12882ms step_avg:57.51ms
step:225/2330 train_time:12937ms step_avg:57.50ms
step:226/2330 train_time:12997ms step_avg:57.51ms
step:227/2330 train_time:13053ms step_avg:57.50ms
step:228/2330 train_time:13113ms step_avg:57.51ms
step:229/2330 train_time:13168ms step_avg:57.50ms
step:230/2330 train_time:13227ms step_avg:57.51ms
step:231/2330 train_time:13282ms step_avg:57.50ms
step:232/2330 train_time:13342ms step_avg:57.51ms
step:233/2330 train_time:13398ms step_avg:57.50ms
step:234/2330 train_time:13458ms step_avg:57.51ms
step:235/2330 train_time:13514ms step_avg:57.50ms
step:236/2330 train_time:13573ms step_avg:57.51ms
step:237/2330 train_time:13629ms step_avg:57.50ms
step:238/2330 train_time:13688ms step_avg:57.51ms
step:239/2330 train_time:13744ms step_avg:57.51ms
step:240/2330 train_time:13803ms step_avg:57.51ms
step:241/2330 train_time:13860ms step_avg:57.51ms
step:242/2330 train_time:13919ms step_avg:57.51ms
step:243/2330 train_time:13974ms step_avg:57.51ms
step:244/2330 train_time:14034ms step_avg:57.51ms
step:245/2330 train_time:14089ms step_avg:57.51ms
step:246/2330 train_time:14148ms step_avg:57.51ms
step:247/2330 train_time:14204ms step_avg:57.51ms
step:248/2330 train_time:14263ms step_avg:57.51ms
step:249/2330 train_time:14319ms step_avg:57.50ms
step:250/2330 train_time:14378ms step_avg:57.51ms
step:250/2330 val_loss:4.9036 train_time:14457ms step_avg:57.83ms
step:251/2330 train_time:14474ms step_avg:57.67ms
step:252/2330 train_time:14495ms step_avg:57.52ms
step:253/2330 train_time:14552ms step_avg:57.52ms
step:254/2330 train_time:14616ms step_avg:57.55ms
step:255/2330 train_time:14675ms step_avg:57.55ms
step:256/2330 train_time:14737ms step_avg:57.57ms
step:257/2330 train_time:14793ms step_avg:57.56ms
step:258/2330 train_time:14853ms step_avg:57.57ms
step:259/2330 train_time:14908ms step_avg:57.56ms
step:260/2330 train_time:14967ms step_avg:57.57ms
step:261/2330 train_time:15023ms step_avg:57.56ms
step:262/2330 train_time:15081ms step_avg:57.56ms
step:263/2330 train_time:15136ms step_avg:57.55ms
step:264/2330 train_time:15194ms step_avg:57.55ms
step:265/2330 train_time:15250ms step_avg:57.55ms
step:266/2330 train_time:15308ms step_avg:57.55ms
step:267/2330 train_time:15363ms step_avg:57.54ms
step:268/2330 train_time:15422ms step_avg:57.54ms
step:269/2330 train_time:15478ms step_avg:57.54ms
step:270/2330 train_time:15538ms step_avg:57.55ms
step:271/2330 train_time:15595ms step_avg:57.55ms
step:272/2330 train_time:15656ms step_avg:57.56ms
step:273/2330 train_time:15713ms step_avg:57.56ms
step:274/2330 train_time:15774ms step_avg:57.57ms
step:275/2330 train_time:15830ms step_avg:57.56ms
step:276/2330 train_time:15889ms step_avg:57.57ms
step:277/2330 train_time:15946ms step_avg:57.57ms
step:278/2330 train_time:16004ms step_avg:57.57ms
step:279/2330 train_time:16059ms step_avg:57.56ms
step:280/2330 train_time:16118ms step_avg:57.57ms
step:281/2330 train_time:16175ms step_avg:57.56ms
step:282/2330 train_time:16234ms step_avg:57.57ms
step:283/2330 train_time:16290ms step_avg:57.56ms
step:284/2330 train_time:16348ms step_avg:57.56ms
step:285/2330 train_time:16404ms step_avg:57.56ms
step:286/2330 train_time:16463ms step_avg:57.56ms
step:287/2330 train_time:16519ms step_avg:57.56ms
step:288/2330 train_time:16578ms step_avg:57.56ms
step:289/2330 train_time:16635ms step_avg:57.56ms
step:290/2330 train_time:16695ms step_avg:57.57ms
step:291/2330 train_time:16751ms step_avg:57.57ms
step:292/2330 train_time:16811ms step_avg:57.57ms
step:293/2330 train_time:16867ms step_avg:57.57ms
step:294/2330 train_time:16928ms step_avg:57.58ms
step:295/2330 train_time:16983ms step_avg:57.57ms
step:296/2330 train_time:17042ms step_avg:57.58ms
step:297/2330 train_time:17098ms step_avg:57.57ms
step:298/2330 train_time:17157ms step_avg:57.58ms
step:299/2330 train_time:17213ms step_avg:57.57ms
step:300/2330 train_time:17272ms step_avg:57.57ms
step:301/2330 train_time:17328ms step_avg:57.57ms
step:302/2330 train_time:17387ms step_avg:57.57ms
step:303/2330 train_time:17442ms step_avg:57.56ms
step:304/2330 train_time:17501ms step_avg:57.57ms
step:305/2330 train_time:17557ms step_avg:57.57ms
step:306/2330 train_time:17616ms step_avg:57.57ms
step:307/2330 train_time:17673ms step_avg:57.57ms
step:308/2330 train_time:17733ms step_avg:57.57ms
step:309/2330 train_time:17789ms step_avg:57.57ms
step:310/2330 train_time:17848ms step_avg:57.58ms
step:311/2330 train_time:17905ms step_avg:57.57ms
step:312/2330 train_time:17964ms step_avg:57.58ms
step:313/2330 train_time:18020ms step_avg:57.57ms
step:314/2330 train_time:18079ms step_avg:57.58ms
step:315/2330 train_time:18135ms step_avg:57.57ms
step:316/2330 train_time:18194ms step_avg:57.58ms
step:317/2330 train_time:18251ms step_avg:57.57ms
step:318/2330 train_time:18309ms step_avg:57.57ms
step:319/2330 train_time:18364ms step_avg:57.57ms
step:320/2330 train_time:18424ms step_avg:57.58ms
step:321/2330 train_time:18480ms step_avg:57.57ms
step:322/2330 train_time:18539ms step_avg:57.57ms
step:323/2330 train_time:18595ms step_avg:57.57ms
step:324/2330 train_time:18655ms step_avg:57.58ms
step:325/2330 train_time:18710ms step_avg:57.57ms
step:326/2330 train_time:18771ms step_avg:57.58ms
step:327/2330 train_time:18827ms step_avg:57.57ms
step:328/2330 train_time:18887ms step_avg:57.58ms
step:329/2330 train_time:18944ms step_avg:57.58ms
step:330/2330 train_time:19002ms step_avg:57.58ms
step:331/2330 train_time:19058ms step_avg:57.58ms
step:332/2330 train_time:19117ms step_avg:57.58ms
step:333/2330 train_time:19173ms step_avg:57.58ms
step:334/2330 train_time:19233ms step_avg:57.58ms
step:335/2330 train_time:19288ms step_avg:57.58ms
step:336/2330 train_time:19347ms step_avg:57.58ms
step:337/2330 train_time:19402ms step_avg:57.57ms
step:338/2330 train_time:19461ms step_avg:57.58ms
step:339/2330 train_time:19517ms step_avg:57.57ms
step:340/2330 train_time:19577ms step_avg:57.58ms
step:341/2330 train_time:19634ms step_avg:57.58ms
step:342/2330 train_time:19693ms step_avg:57.58ms
step:343/2330 train_time:19749ms step_avg:57.58ms
step:344/2330 train_time:19808ms step_avg:57.58ms
step:345/2330 train_time:19864ms step_avg:57.58ms
step:346/2330 train_time:19923ms step_avg:57.58ms
step:347/2330 train_time:19980ms step_avg:57.58ms
step:348/2330 train_time:20038ms step_avg:57.58ms
step:349/2330 train_time:20094ms step_avg:57.58ms
step:350/2330 train_time:20153ms step_avg:57.58ms
step:351/2330 train_time:20209ms step_avg:57.57ms
step:352/2330 train_time:20267ms step_avg:57.58ms
step:353/2330 train_time:20323ms step_avg:57.57ms
step:354/2330 train_time:20382ms step_avg:57.58ms
step:355/2330 train_time:20438ms step_avg:57.57ms
step:356/2330 train_time:20497ms step_avg:57.58ms
step:357/2330 train_time:20553ms step_avg:57.57ms
step:358/2330 train_time:20612ms step_avg:57.58ms
step:359/2330 train_time:20669ms step_avg:57.57ms
step:360/2330 train_time:20728ms step_avg:57.58ms
step:361/2330 train_time:20784ms step_avg:57.57ms
step:362/2330 train_time:20843ms step_avg:57.58ms
step:363/2330 train_time:20899ms step_avg:57.57ms
step:364/2330 train_time:20958ms step_avg:57.58ms
step:365/2330 train_time:21014ms step_avg:57.57ms
step:366/2330 train_time:21073ms step_avg:57.58ms
step:367/2330 train_time:21129ms step_avg:57.57ms
step:368/2330 train_time:21189ms step_avg:57.58ms
step:369/2330 train_time:21244ms step_avg:57.57ms
step:370/2330 train_time:21305ms step_avg:57.58ms
step:371/2330 train_time:21360ms step_avg:57.57ms
step:372/2330 train_time:21420ms step_avg:57.58ms
step:373/2330 train_time:21475ms step_avg:57.57ms
step:374/2330 train_time:21534ms step_avg:57.58ms
step:375/2330 train_time:21590ms step_avg:57.57ms
step:376/2330 train_time:21650ms step_avg:57.58ms
step:377/2330 train_time:21705ms step_avg:57.57ms
step:378/2330 train_time:21766ms step_avg:57.58ms
step:379/2330 train_time:21822ms step_avg:57.58ms
step:380/2330 train_time:21881ms step_avg:57.58ms
step:381/2330 train_time:21937ms step_avg:57.58ms
step:382/2330 train_time:21996ms step_avg:57.58ms
step:383/2330 train_time:22052ms step_avg:57.58ms
step:384/2330 train_time:22112ms step_avg:57.58ms
step:385/2330 train_time:22168ms step_avg:57.58ms
step:386/2330 train_time:22227ms step_avg:57.58ms
step:387/2330 train_time:22283ms step_avg:57.58ms
step:388/2330 train_time:22343ms step_avg:57.59ms
step:389/2330 train_time:22399ms step_avg:57.58ms
step:390/2330 train_time:22458ms step_avg:57.59ms
step:391/2330 train_time:22514ms step_avg:57.58ms
step:392/2330 train_time:22573ms step_avg:57.58ms
step:393/2330 train_time:22629ms step_avg:57.58ms
step:394/2330 train_time:22689ms step_avg:57.59ms
step:395/2330 train_time:22745ms step_avg:57.58ms
step:396/2330 train_time:22804ms step_avg:57.59ms
step:397/2330 train_time:22860ms step_avg:57.58ms
step:398/2330 train_time:22919ms step_avg:57.59ms
step:399/2330 train_time:22976ms step_avg:57.58ms
step:400/2330 train_time:23035ms step_avg:57.59ms
step:401/2330 train_time:23092ms step_avg:57.59ms
step:402/2330 train_time:23151ms step_avg:57.59ms
step:403/2330 train_time:23208ms step_avg:57.59ms
step:404/2330 train_time:23267ms step_avg:57.59ms
step:405/2330 train_time:23322ms step_avg:57.59ms
step:406/2330 train_time:23382ms step_avg:57.59ms
step:407/2330 train_time:23437ms step_avg:57.59ms
step:408/2330 train_time:23498ms step_avg:57.59ms
step:409/2330 train_time:23553ms step_avg:57.59ms
step:410/2330 train_time:23614ms step_avg:57.59ms
step:411/2330 train_time:23670ms step_avg:57.59ms
step:412/2330 train_time:23728ms step_avg:57.59ms
step:413/2330 train_time:23785ms step_avg:57.59ms
step:414/2330 train_time:23843ms step_avg:57.59ms
step:415/2330 train_time:23899ms step_avg:57.59ms
step:416/2330 train_time:23958ms step_avg:57.59ms
step:417/2330 train_time:24015ms step_avg:57.59ms
step:418/2330 train_time:24074ms step_avg:57.59ms
step:419/2330 train_time:24131ms step_avg:57.59ms
step:420/2330 train_time:24190ms step_avg:57.59ms
step:421/2330 train_time:24246ms step_avg:57.59ms
step:422/2330 train_time:24304ms step_avg:57.59ms
step:423/2330 train_time:24361ms step_avg:57.59ms
step:424/2330 train_time:24419ms step_avg:57.59ms
step:425/2330 train_time:24475ms step_avg:57.59ms
step:426/2330 train_time:24535ms step_avg:57.59ms
step:427/2330 train_time:24592ms step_avg:57.59ms
step:428/2330 train_time:24651ms step_avg:57.59ms
step:429/2330 train_time:24706ms step_avg:57.59ms
step:430/2330 train_time:24766ms step_avg:57.59ms
step:431/2330 train_time:24822ms step_avg:57.59ms
step:432/2330 train_time:24882ms step_avg:57.60ms
step:433/2330 train_time:24937ms step_avg:57.59ms
step:434/2330 train_time:24997ms step_avg:57.60ms
step:435/2330 train_time:25054ms step_avg:57.59ms
step:436/2330 train_time:25113ms step_avg:57.60ms
step:437/2330 train_time:25170ms step_avg:57.60ms
step:438/2330 train_time:25229ms step_avg:57.60ms
step:439/2330 train_time:25286ms step_avg:57.60ms
step:440/2330 train_time:25345ms step_avg:57.60ms
step:441/2330 train_time:25401ms step_avg:57.60ms
step:442/2330 train_time:25460ms step_avg:57.60ms
step:443/2330 train_time:25516ms step_avg:57.60ms
step:444/2330 train_time:25577ms step_avg:57.61ms
step:445/2330 train_time:25633ms step_avg:57.60ms
step:446/2330 train_time:25692ms step_avg:57.61ms
step:447/2330 train_time:25748ms step_avg:57.60ms
step:448/2330 train_time:25808ms step_avg:57.61ms
step:449/2330 train_time:25864ms step_avg:57.60ms
step:450/2330 train_time:25924ms step_avg:57.61ms
step:451/2330 train_time:25979ms step_avg:57.60ms
step:452/2330 train_time:26039ms step_avg:57.61ms
step:453/2330 train_time:26095ms step_avg:57.60ms
step:454/2330 train_time:26155ms step_avg:57.61ms
step:455/2330 train_time:26211ms step_avg:57.61ms
step:456/2330 train_time:26270ms step_avg:57.61ms
step:457/2330 train_time:26326ms step_avg:57.61ms
step:458/2330 train_time:26385ms step_avg:57.61ms
step:459/2330 train_time:26441ms step_avg:57.61ms
step:460/2330 train_time:26499ms step_avg:57.61ms
step:461/2330 train_time:26556ms step_avg:57.61ms
step:462/2330 train_time:26616ms step_avg:57.61ms
step:463/2330 train_time:26672ms step_avg:57.61ms
step:464/2330 train_time:26732ms step_avg:57.61ms
step:465/2330 train_time:26788ms step_avg:57.61ms
step:466/2330 train_time:26847ms step_avg:57.61ms
step:467/2330 train_time:26902ms step_avg:57.61ms
step:468/2330 train_time:26963ms step_avg:57.61ms
step:469/2330 train_time:27019ms step_avg:57.61ms
step:470/2330 train_time:27079ms step_avg:57.61ms
step:471/2330 train_time:27135ms step_avg:57.61ms
step:472/2330 train_time:27194ms step_avg:57.61ms
step:473/2330 train_time:27250ms step_avg:57.61ms
step:474/2330 train_time:27309ms step_avg:57.61ms
step:475/2330 train_time:27365ms step_avg:57.61ms
step:476/2330 train_time:27425ms step_avg:57.62ms
step:477/2330 train_time:27481ms step_avg:57.61ms
step:478/2330 train_time:27540ms step_avg:57.62ms
step:479/2330 train_time:27596ms step_avg:57.61ms
step:480/2330 train_time:27656ms step_avg:57.62ms
step:481/2330 train_time:27712ms step_avg:57.61ms
step:482/2330 train_time:27772ms step_avg:57.62ms
step:483/2330 train_time:27829ms step_avg:57.62ms
step:484/2330 train_time:27887ms step_avg:57.62ms
step:485/2330 train_time:27944ms step_avg:57.62ms
step:486/2330 train_time:28003ms step_avg:57.62ms
step:487/2330 train_time:28059ms step_avg:57.62ms
step:488/2330 train_time:28118ms step_avg:57.62ms
step:489/2330 train_time:28174ms step_avg:57.62ms
step:490/2330 train_time:28234ms step_avg:57.62ms
step:491/2330 train_time:28291ms step_avg:57.62ms
step:492/2330 train_time:28349ms step_avg:57.62ms
step:493/2330 train_time:28405ms step_avg:57.62ms
step:494/2330 train_time:28465ms step_avg:57.62ms
step:495/2330 train_time:28521ms step_avg:57.62ms
step:496/2330 train_time:28580ms step_avg:57.62ms
step:497/2330 train_time:28636ms step_avg:57.62ms
step:498/2330 train_time:28695ms step_avg:57.62ms
step:499/2330 train_time:28751ms step_avg:57.62ms
step:500/2330 train_time:28811ms step_avg:57.62ms
step:500/2330 val_loss:4.4284 train_time:28891ms step_avg:57.78ms
step:501/2330 train_time:28909ms step_avg:57.70ms
step:502/2330 train_time:28931ms step_avg:57.63ms
step:503/2330 train_time:28987ms step_avg:57.63ms
step:504/2330 train_time:29050ms step_avg:57.64ms
step:505/2330 train_time:29106ms step_avg:57.64ms
step:506/2330 train_time:29169ms step_avg:57.65ms
step:507/2330 train_time:29225ms step_avg:57.64ms
step:508/2330 train_time:29283ms step_avg:57.64ms
step:509/2330 train_time:29339ms step_avg:57.64ms
step:510/2330 train_time:29398ms step_avg:57.64ms
step:511/2330 train_time:29453ms step_avg:57.64ms
step:512/2330 train_time:29511ms step_avg:57.64ms
step:513/2330 train_time:29567ms step_avg:57.64ms
step:514/2330 train_time:29625ms step_avg:57.64ms
step:515/2330 train_time:29681ms step_avg:57.63ms
step:516/2330 train_time:29740ms step_avg:57.64ms
step:517/2330 train_time:29796ms step_avg:57.63ms
step:518/2330 train_time:29855ms step_avg:57.64ms
step:519/2330 train_time:29912ms step_avg:57.63ms
step:520/2330 train_time:29975ms step_avg:57.64ms
step:521/2330 train_time:30031ms step_avg:57.64ms
step:522/2330 train_time:30093ms step_avg:57.65ms
step:523/2330 train_time:30150ms step_avg:57.65ms
step:524/2330 train_time:30210ms step_avg:57.65ms
step:525/2330 train_time:30267ms step_avg:57.65ms
step:526/2330 train_time:30325ms step_avg:57.65ms
step:527/2330 train_time:30380ms step_avg:57.65ms
step:528/2330 train_time:30439ms step_avg:57.65ms
step:529/2330 train_time:30495ms step_avg:57.65ms
step:530/2330 train_time:30554ms step_avg:57.65ms
step:531/2330 train_time:30610ms step_avg:57.65ms
step:532/2330 train_time:30668ms step_avg:57.65ms
step:533/2330 train_time:30724ms step_avg:57.64ms
step:534/2330 train_time:30783ms step_avg:57.65ms
step:535/2330 train_time:30839ms step_avg:57.64ms
step:536/2330 train_time:30899ms step_avg:57.65ms
step:537/2330 train_time:30956ms step_avg:57.65ms
step:538/2330 train_time:31016ms step_avg:57.65ms
step:539/2330 train_time:31073ms step_avg:57.65ms
step:540/2330 train_time:31132ms step_avg:57.65ms
step:541/2330 train_time:31189ms step_avg:57.65ms
step:542/2330 train_time:31249ms step_avg:57.66ms
step:543/2330 train_time:31305ms step_avg:57.65ms
step:544/2330 train_time:31364ms step_avg:57.66ms
step:545/2330 train_time:31420ms step_avg:57.65ms
step:546/2330 train_time:31478ms step_avg:57.65ms
step:547/2330 train_time:31534ms step_avg:57.65ms
step:548/2330 train_time:31593ms step_avg:57.65ms
step:549/2330 train_time:31648ms step_avg:57.65ms
step:550/2330 train_time:31707ms step_avg:57.65ms
step:551/2330 train_time:31763ms step_avg:57.65ms
step:552/2330 train_time:31822ms step_avg:57.65ms
step:553/2330 train_time:31879ms step_avg:57.65ms
step:554/2330 train_time:31938ms step_avg:57.65ms
step:555/2330 train_time:31995ms step_avg:57.65ms
step:556/2330 train_time:32055ms step_avg:57.65ms
step:557/2330 train_time:32112ms step_avg:57.65ms
step:558/2330 train_time:32171ms step_avg:57.65ms
step:559/2330 train_time:32227ms step_avg:57.65ms
step:560/2330 train_time:32287ms step_avg:57.65ms
step:561/2330 train_time:32342ms step_avg:57.65ms
step:562/2330 train_time:32402ms step_avg:57.65ms
step:563/2330 train_time:32458ms step_avg:57.65ms
step:564/2330 train_time:32517ms step_avg:57.66ms
step:565/2330 train_time:32574ms step_avg:57.65ms
step:566/2330 train_time:32632ms step_avg:57.65ms
step:567/2330 train_time:32688ms step_avg:57.65ms
step:568/2330 train_time:32748ms step_avg:57.65ms
step:569/2330 train_time:32803ms step_avg:57.65ms
step:570/2330 train_time:32863ms step_avg:57.66ms
step:571/2330 train_time:32919ms step_avg:57.65ms
step:572/2330 train_time:32979ms step_avg:57.66ms
step:573/2330 train_time:33036ms step_avg:57.65ms
step:574/2330 train_time:33096ms step_avg:57.66ms
step:575/2330 train_time:33152ms step_avg:57.66ms
step:576/2330 train_time:33211ms step_avg:57.66ms
step:577/2330 train_time:33267ms step_avg:57.66ms
step:578/2330 train_time:33327ms step_avg:57.66ms
step:579/2330 train_time:33383ms step_avg:57.66ms
step:580/2330 train_time:33443ms step_avg:57.66ms
step:581/2330 train_time:33499ms step_avg:57.66ms
step:582/2330 train_time:33558ms step_avg:57.66ms
step:583/2330 train_time:33615ms step_avg:57.66ms
step:584/2330 train_time:33674ms step_avg:57.66ms
step:585/2330 train_time:33730ms step_avg:57.66ms
step:586/2330 train_time:33789ms step_avg:57.66ms
step:587/2330 train_time:33844ms step_avg:57.66ms
step:588/2330 train_time:33904ms step_avg:57.66ms
step:589/2330 train_time:33960ms step_avg:57.66ms
step:590/2330 train_time:34021ms step_avg:57.66ms
step:591/2330 train_time:34079ms step_avg:57.66ms
step:592/2330 train_time:34139ms step_avg:57.67ms
step:593/2330 train_time:34195ms step_avg:57.66ms
step:594/2330 train_time:34254ms step_avg:57.67ms
step:595/2330 train_time:34311ms step_avg:57.67ms
step:596/2330 train_time:34369ms step_avg:57.67ms
step:597/2330 train_time:34425ms step_avg:57.66ms
step:598/2330 train_time:34484ms step_avg:57.67ms
step:599/2330 train_time:34539ms step_avg:57.66ms
step:600/2330 train_time:34599ms step_avg:57.67ms
step:601/2330 train_time:34656ms step_avg:57.66ms
step:602/2330 train_time:34716ms step_avg:57.67ms
step:603/2330 train_time:34772ms step_avg:57.66ms
step:604/2330 train_time:34831ms step_avg:57.67ms
step:605/2330 train_time:34886ms step_avg:57.66ms
step:606/2330 train_time:34946ms step_avg:57.67ms
step:607/2330 train_time:35001ms step_avg:57.66ms
step:608/2330 train_time:35062ms step_avg:57.67ms
step:609/2330 train_time:35118ms step_avg:57.67ms
step:610/2330 train_time:35178ms step_avg:57.67ms
step:611/2330 train_time:35234ms step_avg:57.67ms
step:612/2330 train_time:35294ms step_avg:57.67ms
step:613/2330 train_time:35350ms step_avg:57.67ms
step:614/2330 train_time:35409ms step_avg:57.67ms
step:615/2330 train_time:35465ms step_avg:57.67ms
step:616/2330 train_time:35525ms step_avg:57.67ms
step:617/2330 train_time:35580ms step_avg:57.67ms
step:618/2330 train_time:35640ms step_avg:57.67ms
step:619/2330 train_time:35696ms step_avg:57.67ms
step:620/2330 train_time:35755ms step_avg:57.67ms
step:621/2330 train_time:35812ms step_avg:57.67ms
step:622/2330 train_time:35872ms step_avg:57.67ms
step:623/2330 train_time:35928ms step_avg:57.67ms
step:624/2330 train_time:35987ms step_avg:57.67ms
step:625/2330 train_time:36043ms step_avg:57.67ms
step:626/2330 train_time:36103ms step_avg:57.67ms
step:627/2330 train_time:36159ms step_avg:57.67ms
step:628/2330 train_time:36219ms step_avg:57.67ms
step:629/2330 train_time:36276ms step_avg:57.67ms
step:630/2330 train_time:36335ms step_avg:57.67ms
step:631/2330 train_time:36391ms step_avg:57.67ms
step:632/2330 train_time:36450ms step_avg:57.67ms
step:633/2330 train_time:36506ms step_avg:57.67ms
step:634/2330 train_time:36566ms step_avg:57.67ms
step:635/2330 train_time:36621ms step_avg:57.67ms
step:636/2330 train_time:36682ms step_avg:57.68ms
step:637/2330 train_time:36738ms step_avg:57.67ms
step:638/2330 train_time:36798ms step_avg:57.68ms
step:639/2330 train_time:36854ms step_avg:57.67ms
step:640/2330 train_time:36913ms step_avg:57.68ms
step:641/2330 train_time:36968ms step_avg:57.67ms
step:642/2330 train_time:37029ms step_avg:57.68ms
step:643/2330 train_time:37084ms step_avg:57.67ms
step:644/2330 train_time:37144ms step_avg:57.68ms
step:645/2330 train_time:37200ms step_avg:57.67ms
step:646/2330 train_time:37260ms step_avg:57.68ms
step:647/2330 train_time:37317ms step_avg:57.68ms
step:648/2330 train_time:37376ms step_avg:57.68ms
step:649/2330 train_time:37432ms step_avg:57.68ms
step:650/2330 train_time:37490ms step_avg:57.68ms
step:651/2330 train_time:37546ms step_avg:57.67ms
step:652/2330 train_time:37605ms step_avg:57.68ms
step:653/2330 train_time:37661ms step_avg:57.67ms
step:654/2330 train_time:37720ms step_avg:57.68ms
step:655/2330 train_time:37776ms step_avg:57.67ms
step:656/2330 train_time:37836ms step_avg:57.68ms
step:657/2330 train_time:37891ms step_avg:57.67ms
step:658/2330 train_time:37951ms step_avg:57.68ms
step:659/2330 train_time:38006ms step_avg:57.67ms
step:660/2330 train_time:38066ms step_avg:57.68ms
step:661/2330 train_time:38122ms step_avg:57.67ms
step:662/2330 train_time:38181ms step_avg:57.67ms
step:663/2330 train_time:38238ms step_avg:57.67ms
step:664/2330 train_time:38296ms step_avg:57.68ms
step:665/2330 train_time:38352ms step_avg:57.67ms
step:666/2330 train_time:38412ms step_avg:57.68ms
step:667/2330 train_time:38468ms step_avg:57.67ms
step:668/2330 train_time:38527ms step_avg:57.68ms
step:669/2330 train_time:38582ms step_avg:57.67ms
step:670/2330 train_time:38642ms step_avg:57.68ms
step:671/2330 train_time:38700ms step_avg:57.67ms
step:672/2330 train_time:38759ms step_avg:57.68ms
step:673/2330 train_time:38815ms step_avg:57.68ms
step:674/2330 train_time:38874ms step_avg:57.68ms
step:675/2330 train_time:38930ms step_avg:57.67ms
step:676/2330 train_time:38990ms step_avg:57.68ms
step:677/2330 train_time:39046ms step_avg:57.67ms
step:678/2330 train_time:39105ms step_avg:57.68ms
step:679/2330 train_time:39161ms step_avg:57.67ms
step:680/2330 train_time:39220ms step_avg:57.68ms
step:681/2330 train_time:39277ms step_avg:57.67ms
step:682/2330 train_time:39337ms step_avg:57.68ms
step:683/2330 train_time:39393ms step_avg:57.68ms
step:684/2330 train_time:39452ms step_avg:57.68ms
step:685/2330 train_time:39508ms step_avg:57.68ms
step:686/2330 train_time:39568ms step_avg:57.68ms
step:687/2330 train_time:39624ms step_avg:57.68ms
step:688/2330 train_time:39683ms step_avg:57.68ms
step:689/2330 train_time:39739ms step_avg:57.68ms
step:690/2330 train_time:39799ms step_avg:57.68ms
step:691/2330 train_time:39855ms step_avg:57.68ms
step:692/2330 train_time:39913ms step_avg:57.68ms
step:693/2330 train_time:39970ms step_avg:57.68ms
step:694/2330 train_time:40029ms step_avg:57.68ms
step:695/2330 train_time:40086ms step_avg:57.68ms
step:696/2330 train_time:40145ms step_avg:57.68ms
step:697/2330 train_time:40201ms step_avg:57.68ms
step:698/2330 train_time:40260ms step_avg:57.68ms
step:699/2330 train_time:40317ms step_avg:57.68ms
step:700/2330 train_time:40376ms step_avg:57.68ms
step:701/2330 train_time:40433ms step_avg:57.68ms
step:702/2330 train_time:40492ms step_avg:57.68ms
step:703/2330 train_time:40548ms step_avg:57.68ms
step:704/2330 train_time:40608ms step_avg:57.68ms
step:705/2330 train_time:40664ms step_avg:57.68ms
step:706/2330 train_time:40724ms step_avg:57.68ms
step:707/2330 train_time:40781ms step_avg:57.68ms
step:708/2330 train_time:40840ms step_avg:57.68ms
step:709/2330 train_time:40896ms step_avg:57.68ms
step:710/2330 train_time:40955ms step_avg:57.68ms
step:711/2330 train_time:41011ms step_avg:57.68ms
step:712/2330 train_time:41070ms step_avg:57.68ms
step:713/2330 train_time:41126ms step_avg:57.68ms
step:714/2330 train_time:41185ms step_avg:57.68ms
step:715/2330 train_time:41241ms step_avg:57.68ms
step:716/2330 train_time:41301ms step_avg:57.68ms
step:717/2330 train_time:41358ms step_avg:57.68ms
step:718/2330 train_time:41417ms step_avg:57.68ms
step:719/2330 train_time:41474ms step_avg:57.68ms
step:720/2330 train_time:41533ms step_avg:57.69ms
step:721/2330 train_time:41590ms step_avg:57.68ms
step:722/2330 train_time:41649ms step_avg:57.69ms
step:723/2330 train_time:41706ms step_avg:57.68ms
step:724/2330 train_time:41765ms step_avg:57.69ms
step:725/2330 train_time:41820ms step_avg:57.68ms
step:726/2330 train_time:41880ms step_avg:57.69ms
step:727/2330 train_time:41936ms step_avg:57.68ms
step:728/2330 train_time:41995ms step_avg:57.68ms
step:729/2330 train_time:42051ms step_avg:57.68ms
step:730/2330 train_time:42110ms step_avg:57.68ms
step:731/2330 train_time:42166ms step_avg:57.68ms
step:732/2330 train_time:42225ms step_avg:57.68ms
step:733/2330 train_time:42280ms step_avg:57.68ms
step:734/2330 train_time:42342ms step_avg:57.69ms
step:735/2330 train_time:42398ms step_avg:57.68ms
step:736/2330 train_time:42459ms step_avg:57.69ms
step:737/2330 train_time:42515ms step_avg:57.69ms
step:738/2330 train_time:42575ms step_avg:57.69ms
step:739/2330 train_time:42632ms step_avg:57.69ms
step:740/2330 train_time:42691ms step_avg:57.69ms
step:741/2330 train_time:42747ms step_avg:57.69ms
step:742/2330 train_time:42807ms step_avg:57.69ms
step:743/2330 train_time:42863ms step_avg:57.69ms
step:744/2330 train_time:42922ms step_avg:57.69ms
step:745/2330 train_time:42978ms step_avg:57.69ms
step:746/2330 train_time:43038ms step_avg:57.69ms
step:747/2330 train_time:43094ms step_avg:57.69ms
step:748/2330 train_time:43153ms step_avg:57.69ms
step:749/2330 train_time:43209ms step_avg:57.69ms
step:750/2330 train_time:43269ms step_avg:57.69ms
step:750/2330 val_loss:4.2137 train_time:43349ms step_avg:57.80ms
step:751/2330 train_time:43367ms step_avg:57.75ms
step:752/2330 train_time:43387ms step_avg:57.70ms
step:753/2330 train_time:43445ms step_avg:57.70ms
step:754/2330 train_time:43508ms step_avg:57.70ms
step:755/2330 train_time:43564ms step_avg:57.70ms
step:756/2330 train_time:43626ms step_avg:57.71ms
step:757/2330 train_time:43682ms step_avg:57.70ms
step:758/2330 train_time:43741ms step_avg:57.71ms
step:759/2330 train_time:43796ms step_avg:57.70ms
step:760/2330 train_time:43856ms step_avg:57.71ms
step:761/2330 train_time:43912ms step_avg:57.70ms
step:762/2330 train_time:43970ms step_avg:57.70ms
step:763/2330 train_time:44025ms step_avg:57.70ms
step:764/2330 train_time:44084ms step_avg:57.70ms
step:765/2330 train_time:44141ms step_avg:57.70ms
step:766/2330 train_time:44200ms step_avg:57.70ms
step:767/2330 train_time:44257ms step_avg:57.70ms
step:768/2330 train_time:44318ms step_avg:57.71ms
step:769/2330 train_time:44376ms step_avg:57.71ms
step:770/2330 train_time:44438ms step_avg:57.71ms
step:771/2330 train_time:44496ms step_avg:57.71ms
step:772/2330 train_time:44558ms step_avg:57.72ms
step:773/2330 train_time:44615ms step_avg:57.72ms
step:774/2330 train_time:44675ms step_avg:57.72ms
step:775/2330 train_time:44732ms step_avg:57.72ms
step:776/2330 train_time:44792ms step_avg:57.72ms
step:777/2330 train_time:44849ms step_avg:57.72ms
step:778/2330 train_time:44910ms step_avg:57.72ms
step:779/2330 train_time:44966ms step_avg:57.72ms
step:780/2330 train_time:45026ms step_avg:57.73ms
step:781/2330 train_time:45082ms step_avg:57.72ms
step:782/2330 train_time:45142ms step_avg:57.73ms
step:783/2330 train_time:45198ms step_avg:57.72ms
step:784/2330 train_time:45258ms step_avg:57.73ms
step:785/2330 train_time:45316ms step_avg:57.73ms
step:786/2330 train_time:45376ms step_avg:57.73ms
step:787/2330 train_time:45433ms step_avg:57.73ms
step:788/2330 train_time:45495ms step_avg:57.73ms
step:789/2330 train_time:45553ms step_avg:57.74ms
step:790/2330 train_time:45613ms step_avg:57.74ms
step:791/2330 train_time:45671ms step_avg:57.74ms
step:792/2330 train_time:45730ms step_avg:57.74ms
step:793/2330 train_time:45787ms step_avg:57.74ms
step:794/2330 train_time:45848ms step_avg:57.74ms
step:795/2330 train_time:45905ms step_avg:57.74ms
step:796/2330 train_time:45964ms step_avg:57.74ms
step:797/2330 train_time:46021ms step_avg:57.74ms
step:798/2330 train_time:46082ms step_avg:57.75ms
step:799/2330 train_time:46137ms step_avg:57.74ms
step:800/2330 train_time:46198ms step_avg:57.75ms
step:801/2330 train_time:46256ms step_avg:57.75ms
step:802/2330 train_time:46315ms step_avg:57.75ms
step:803/2330 train_time:46373ms step_avg:57.75ms
step:804/2330 train_time:46433ms step_avg:57.75ms
step:805/2330 train_time:46490ms step_avg:57.75ms
step:806/2330 train_time:46550ms step_avg:57.75ms
step:807/2330 train_time:46608ms step_avg:57.75ms
step:808/2330 train_time:46668ms step_avg:57.76ms
step:809/2330 train_time:46725ms step_avg:57.76ms
step:810/2330 train_time:46785ms step_avg:57.76ms
step:811/2330 train_time:46843ms step_avg:57.76ms
step:812/2330 train_time:46902ms step_avg:57.76ms
step:813/2330 train_time:46959ms step_avg:57.76ms
step:814/2330 train_time:47019ms step_avg:57.76ms
step:815/2330 train_time:47076ms step_avg:57.76ms
step:816/2330 train_time:47135ms step_avg:57.76ms
step:817/2330 train_time:47192ms step_avg:57.76ms
step:818/2330 train_time:47252ms step_avg:57.77ms
step:819/2330 train_time:47309ms step_avg:57.76ms
step:820/2330 train_time:47369ms step_avg:57.77ms
step:821/2330 train_time:47427ms step_avg:57.77ms
step:822/2330 train_time:47486ms step_avg:57.77ms
step:823/2330 train_time:47543ms step_avg:57.77ms
step:824/2330 train_time:47603ms step_avg:57.77ms
step:825/2330 train_time:47660ms step_avg:57.77ms
step:826/2330 train_time:47721ms step_avg:57.77ms
step:827/2330 train_time:47778ms step_avg:57.77ms
step:828/2330 train_time:47839ms step_avg:57.78ms
step:829/2330 train_time:47896ms step_avg:57.78ms
step:830/2330 train_time:47956ms step_avg:57.78ms
step:831/2330 train_time:48014ms step_avg:57.78ms
step:832/2330 train_time:48073ms step_avg:57.78ms
step:833/2330 train_time:48130ms step_avg:57.78ms
step:834/2330 train_time:48191ms step_avg:57.78ms
step:835/2330 train_time:48247ms step_avg:57.78ms
step:836/2330 train_time:48309ms step_avg:57.79ms
step:837/2330 train_time:48366ms step_avg:57.78ms
step:838/2330 train_time:48427ms step_avg:57.79ms
step:839/2330 train_time:48484ms step_avg:57.79ms
step:840/2330 train_time:48544ms step_avg:57.79ms
step:841/2330 train_time:48601ms step_avg:57.79ms
step:842/2330 train_time:48662ms step_avg:57.79ms
step:843/2330 train_time:48718ms step_avg:57.79ms
step:844/2330 train_time:48779ms step_avg:57.80ms
step:845/2330 train_time:48836ms step_avg:57.79ms
step:846/2330 train_time:48896ms step_avg:57.80ms
step:847/2330 train_time:48953ms step_avg:57.80ms
step:848/2330 train_time:49013ms step_avg:57.80ms
step:849/2330 train_time:49071ms step_avg:57.80ms
step:850/2330 train_time:49131ms step_avg:57.80ms
step:851/2330 train_time:49188ms step_avg:57.80ms
step:852/2330 train_time:49247ms step_avg:57.80ms
step:853/2330 train_time:49304ms step_avg:57.80ms
step:854/2330 train_time:49364ms step_avg:57.80ms
step:855/2330 train_time:49421ms step_avg:57.80ms
step:856/2330 train_time:49482ms step_avg:57.81ms
step:857/2330 train_time:49539ms step_avg:57.80ms
step:858/2330 train_time:49599ms step_avg:57.81ms
step:859/2330 train_time:49656ms step_avg:57.81ms
step:860/2330 train_time:49716ms step_avg:57.81ms
step:861/2330 train_time:49774ms step_avg:57.81ms
step:862/2330 train_time:49834ms step_avg:57.81ms
step:863/2330 train_time:49892ms step_avg:57.81ms
step:864/2330 train_time:49951ms step_avg:57.81ms
step:865/2330 train_time:50008ms step_avg:57.81ms
step:866/2330 train_time:50067ms step_avg:57.81ms
step:867/2330 train_time:50124ms step_avg:57.81ms
step:868/2330 train_time:50185ms step_avg:57.82ms
step:869/2330 train_time:50241ms step_avg:57.82ms
step:870/2330 train_time:50301ms step_avg:57.82ms
step:871/2330 train_time:50358ms step_avg:57.82ms
step:872/2330 train_time:50418ms step_avg:57.82ms
step:873/2330 train_time:50475ms step_avg:57.82ms
step:874/2330 train_time:50535ms step_avg:57.82ms
step:875/2330 train_time:50592ms step_avg:57.82ms
step:876/2330 train_time:50654ms step_avg:57.82ms
step:877/2330 train_time:50711ms step_avg:57.82ms
step:878/2330 train_time:50771ms step_avg:57.83ms
step:879/2330 train_time:50827ms step_avg:57.82ms
step:880/2330 train_time:50889ms step_avg:57.83ms
step:881/2330 train_time:50946ms step_avg:57.83ms
step:882/2330 train_time:51006ms step_avg:57.83ms
step:883/2330 train_time:51063ms step_avg:57.83ms
step:884/2330 train_time:51123ms step_avg:57.83ms
step:885/2330 train_time:51179ms step_avg:57.83ms
step:886/2330 train_time:51240ms step_avg:57.83ms
step:887/2330 train_time:51296ms step_avg:57.83ms
step:888/2330 train_time:51357ms step_avg:57.84ms
step:889/2330 train_time:51415ms step_avg:57.83ms
step:890/2330 train_time:51475ms step_avg:57.84ms
step:891/2330 train_time:51532ms step_avg:57.84ms
step:892/2330 train_time:51592ms step_avg:57.84ms
step:893/2330 train_time:51649ms step_avg:57.84ms
step:894/2330 train_time:51709ms step_avg:57.84ms
step:895/2330 train_time:51766ms step_avg:57.84ms
step:896/2330 train_time:51826ms step_avg:57.84ms
step:897/2330 train_time:51883ms step_avg:57.84ms
step:898/2330 train_time:51942ms step_avg:57.84ms
step:899/2330 train_time:52000ms step_avg:57.84ms
step:900/2330 train_time:52060ms step_avg:57.84ms
step:901/2330 train_time:52117ms step_avg:57.84ms
step:902/2330 train_time:52176ms step_avg:57.85ms
step:903/2330 train_time:52234ms step_avg:57.84ms
step:904/2330 train_time:52294ms step_avg:57.85ms
step:905/2330 train_time:52350ms step_avg:57.85ms
step:906/2330 train_time:52411ms step_avg:57.85ms
step:907/2330 train_time:52467ms step_avg:57.85ms
step:908/2330 train_time:52528ms step_avg:57.85ms
step:909/2330 train_time:52585ms step_avg:57.85ms
step:910/2330 train_time:52646ms step_avg:57.85ms
step:911/2330 train_time:52703ms step_avg:57.85ms
step:912/2330 train_time:52763ms step_avg:57.85ms
step:913/2330 train_time:52819ms step_avg:57.85ms
step:914/2330 train_time:52879ms step_avg:57.85ms
step:915/2330 train_time:52936ms step_avg:57.85ms
step:916/2330 train_time:52997ms step_avg:57.86ms
step:917/2330 train_time:53054ms step_avg:57.86ms
step:918/2330 train_time:53115ms step_avg:57.86ms
step:919/2330 train_time:53171ms step_avg:57.86ms
step:920/2330 train_time:53232ms step_avg:57.86ms
step:921/2330 train_time:53289ms step_avg:57.86ms
step:922/2330 train_time:53349ms step_avg:57.86ms
step:923/2330 train_time:53405ms step_avg:57.86ms
step:924/2330 train_time:53466ms step_avg:57.86ms
step:925/2330 train_time:53523ms step_avg:57.86ms
step:926/2330 train_time:53583ms step_avg:57.86ms
step:927/2330 train_time:53639ms step_avg:57.86ms
step:928/2330 train_time:53701ms step_avg:57.87ms
step:929/2330 train_time:53757ms step_avg:57.87ms
step:930/2330 train_time:53817ms step_avg:57.87ms
step:931/2330 train_time:53875ms step_avg:57.87ms
step:932/2330 train_time:53935ms step_avg:57.87ms
step:933/2330 train_time:53992ms step_avg:57.87ms
step:934/2330 train_time:54052ms step_avg:57.87ms
step:935/2330 train_time:54109ms step_avg:57.87ms
step:936/2330 train_time:54169ms step_avg:57.87ms
step:937/2330 train_time:54226ms step_avg:57.87ms
step:938/2330 train_time:54286ms step_avg:57.87ms
step:939/2330 train_time:54343ms step_avg:57.87ms
step:940/2330 train_time:54403ms step_avg:57.88ms
step:941/2330 train_time:54460ms step_avg:57.87ms
step:942/2330 train_time:54520ms step_avg:57.88ms
step:943/2330 train_time:54577ms step_avg:57.88ms
step:944/2330 train_time:54637ms step_avg:57.88ms
step:945/2330 train_time:54693ms step_avg:57.88ms
step:946/2330 train_time:54754ms step_avg:57.88ms
step:947/2330 train_time:54812ms step_avg:57.88ms
step:948/2330 train_time:54872ms step_avg:57.88ms
step:949/2330 train_time:54928ms step_avg:57.88ms
step:950/2330 train_time:54989ms step_avg:57.88ms
step:951/2330 train_time:55045ms step_avg:57.88ms
step:952/2330 train_time:55106ms step_avg:57.88ms
step:953/2330 train_time:55163ms step_avg:57.88ms
step:954/2330 train_time:55222ms step_avg:57.88ms
step:955/2330 train_time:55278ms step_avg:57.88ms
step:956/2330 train_time:55340ms step_avg:57.89ms
step:957/2330 train_time:55397ms step_avg:57.89ms
step:958/2330 train_time:55458ms step_avg:57.89ms
step:959/2330 train_time:55517ms step_avg:57.89ms
step:960/2330 train_time:55577ms step_avg:57.89ms
step:961/2330 train_time:55634ms step_avg:57.89ms
step:962/2330 train_time:55693ms step_avg:57.89ms
step:963/2330 train_time:55750ms step_avg:57.89ms
step:964/2330 train_time:55810ms step_avg:57.89ms
step:965/2330 train_time:55869ms step_avg:57.89ms
step:966/2330 train_time:55928ms step_avg:57.90ms
step:967/2330 train_time:55985ms step_avg:57.90ms
step:968/2330 train_time:56045ms step_avg:57.90ms
step:969/2330 train_time:56102ms step_avg:57.90ms
step:970/2330 train_time:56163ms step_avg:57.90ms
step:971/2330 train_time:56219ms step_avg:57.90ms
step:972/2330 train_time:56280ms step_avg:57.90ms
step:973/2330 train_time:56337ms step_avg:57.90ms
step:974/2330 train_time:56397ms step_avg:57.90ms
step:975/2330 train_time:56454ms step_avg:57.90ms
step:976/2330 train_time:56514ms step_avg:57.90ms
step:977/2330 train_time:56572ms step_avg:57.90ms
step:978/2330 train_time:56632ms step_avg:57.91ms
step:979/2330 train_time:56688ms step_avg:57.90ms
step:980/2330 train_time:56748ms step_avg:57.91ms
step:981/2330 train_time:56804ms step_avg:57.90ms
step:982/2330 train_time:56866ms step_avg:57.91ms
step:983/2330 train_time:56922ms step_avg:57.91ms
step:984/2330 train_time:56982ms step_avg:57.91ms
step:985/2330 train_time:57039ms step_avg:57.91ms
step:986/2330 train_time:57099ms step_avg:57.91ms
step:987/2330 train_time:57156ms step_avg:57.91ms
step:988/2330 train_time:57216ms step_avg:57.91ms
step:989/2330 train_time:57273ms step_avg:57.91ms
step:990/2330 train_time:57333ms step_avg:57.91ms
step:991/2330 train_time:57391ms step_avg:57.91ms
step:992/2330 train_time:57451ms step_avg:57.91ms
step:993/2330 train_time:57508ms step_avg:57.91ms
step:994/2330 train_time:57569ms step_avg:57.92ms
step:995/2330 train_time:57626ms step_avg:57.92ms
step:996/2330 train_time:57686ms step_avg:57.92ms
step:997/2330 train_time:57743ms step_avg:57.92ms
step:998/2330 train_time:57803ms step_avg:57.92ms
step:999/2330 train_time:57860ms step_avg:57.92ms
step:1000/2330 train_time:57920ms step_avg:57.92ms
step:1000/2330 val_loss:4.0687 train_time:58001ms step_avg:58.00ms
step:1001/2330 train_time:58020ms step_avg:57.96ms
step:1002/2330 train_time:58039ms step_avg:57.92ms
step:1003/2330 train_time:58094ms step_avg:57.92ms
step:1004/2330 train_time:58163ms step_avg:57.93ms
step:1005/2330 train_time:58219ms step_avg:57.93ms
step:1006/2330 train_time:58282ms step_avg:57.93ms
step:1007/2330 train_time:58339ms step_avg:57.93ms
step:1008/2330 train_time:58399ms step_avg:57.94ms
step:1009/2330 train_time:58456ms step_avg:57.93ms
step:1010/2330 train_time:58515ms step_avg:57.94ms
step:1011/2330 train_time:58572ms step_avg:57.93ms
step:1012/2330 train_time:58631ms step_avg:57.94ms
step:1013/2330 train_time:58687ms step_avg:57.93ms
step:1014/2330 train_time:58746ms step_avg:57.93ms
step:1015/2330 train_time:58802ms step_avg:57.93ms
step:1016/2330 train_time:58862ms step_avg:57.93ms
step:1017/2330 train_time:58923ms step_avg:57.94ms
step:1018/2330 train_time:58984ms step_avg:57.94ms
step:1019/2330 train_time:59042ms step_avg:57.94ms
step:1020/2330 train_time:59104ms step_avg:57.95ms
step:1021/2330 train_time:59162ms step_avg:57.95ms
step:1022/2330 train_time:59225ms step_avg:57.95ms
step:1023/2330 train_time:59281ms step_avg:57.95ms
step:1024/2330 train_time:59342ms step_avg:57.95ms
step:1025/2330 train_time:59399ms step_avg:57.95ms
step:1026/2330 train_time:59460ms step_avg:57.95ms
step:1027/2330 train_time:59517ms step_avg:57.95ms
step:1028/2330 train_time:59576ms step_avg:57.95ms
step:1029/2330 train_time:59632ms step_avg:57.95ms
step:1030/2330 train_time:59691ms step_avg:57.95ms
step:1031/2330 train_time:59748ms step_avg:57.95ms
step:1032/2330 train_time:59808ms step_avg:57.95ms
step:1033/2330 train_time:59866ms step_avg:57.95ms
step:1034/2330 train_time:59926ms step_avg:57.96ms
step:1035/2330 train_time:59983ms step_avg:57.95ms
step:1036/2330 train_time:60044ms step_avg:57.96ms
step:1037/2330 train_time:60102ms step_avg:57.96ms
step:1038/2330 train_time:60164ms step_avg:57.96ms
step:1039/2330 train_time:60221ms step_avg:57.96ms
step:1040/2330 train_time:60282ms step_avg:57.96ms
step:1041/2330 train_time:60339ms step_avg:57.96ms
step:1042/2330 train_time:60399ms step_avg:57.96ms
step:1043/2330 train_time:60456ms step_avg:57.96ms
step:1044/2330 train_time:60516ms step_avg:57.97ms
step:1045/2330 train_time:60573ms step_avg:57.96ms
step:1046/2330 train_time:60633ms step_avg:57.97ms
step:1047/2330 train_time:60689ms step_avg:57.97ms
step:1048/2330 train_time:60748ms step_avg:57.97ms
step:1049/2330 train_time:60807ms step_avg:57.97ms
step:1050/2330 train_time:60866ms step_avg:57.97ms
step:1051/2330 train_time:60923ms step_avg:57.97ms
step:1052/2330 train_time:60983ms step_avg:57.97ms
step:1053/2330 train_time:61041ms step_avg:57.97ms
step:1054/2330 train_time:61102ms step_avg:57.97ms
step:1055/2330 train_time:61159ms step_avg:57.97ms
step:1056/2330 train_time:61220ms step_avg:57.97ms
step:1057/2330 train_time:61276ms step_avg:57.97ms
step:1058/2330 train_time:61338ms step_avg:57.98ms
step:1059/2330 train_time:61394ms step_avg:57.97ms
step:1060/2330 train_time:61455ms step_avg:57.98ms
step:1061/2330 train_time:61511ms step_avg:57.97ms
step:1062/2330 train_time:61571ms step_avg:57.98ms
step:1063/2330 train_time:61628ms step_avg:57.98ms
step:1064/2330 train_time:61687ms step_avg:57.98ms
step:1065/2330 train_time:61744ms step_avg:57.98ms
step:1066/2330 train_time:61803ms step_avg:57.98ms
step:1067/2330 train_time:61860ms step_avg:57.98ms
step:1068/2330 train_time:61921ms step_avg:57.98ms
step:1069/2330 train_time:61978ms step_avg:57.98ms
step:1070/2330 train_time:62039ms step_avg:57.98ms
step:1071/2330 train_time:62096ms step_avg:57.98ms
step:1072/2330 train_time:62156ms step_avg:57.98ms
step:1073/2330 train_time:62213ms step_avg:57.98ms
step:1074/2330 train_time:62274ms step_avg:57.98ms
step:1075/2330 train_time:62331ms step_avg:57.98ms
step:1076/2330 train_time:62392ms step_avg:57.99ms
step:1077/2330 train_time:62449ms step_avg:57.98ms
step:1078/2330 train_time:62509ms step_avg:57.99ms
step:1079/2330 train_time:62565ms step_avg:57.98ms
step:1080/2330 train_time:62625ms step_avg:57.99ms
step:1081/2330 train_time:62681ms step_avg:57.98ms
step:1082/2330 train_time:62742ms step_avg:57.99ms
step:1083/2330 train_time:62799ms step_avg:57.99ms
step:1084/2330 train_time:62860ms step_avg:57.99ms
step:1085/2330 train_time:62917ms step_avg:57.99ms
step:1086/2330 train_time:62977ms step_avg:57.99ms
step:1087/2330 train_time:63034ms step_avg:57.99ms
step:1088/2330 train_time:63094ms step_avg:57.99ms
step:1089/2330 train_time:63151ms step_avg:57.99ms
step:1090/2330 train_time:63212ms step_avg:57.99ms
step:1091/2330 train_time:63269ms step_avg:57.99ms
step:1092/2330 train_time:63330ms step_avg:57.99ms
step:1093/2330 train_time:63387ms step_avg:57.99ms
step:1094/2330 train_time:63448ms step_avg:58.00ms
step:1095/2330 train_time:63505ms step_avg:58.00ms
step:1096/2330 train_time:63565ms step_avg:58.00ms
step:1097/2330 train_time:63622ms step_avg:58.00ms
step:1098/2330 train_time:63682ms step_avg:58.00ms
step:1099/2330 train_time:63739ms step_avg:58.00ms
step:1100/2330 train_time:63798ms step_avg:58.00ms
step:1101/2330 train_time:63856ms step_avg:58.00ms
step:1102/2330 train_time:63916ms step_avg:58.00ms
step:1103/2330 train_time:63974ms step_avg:58.00ms
step:1104/2330 train_time:64034ms step_avg:58.00ms
step:1105/2330 train_time:64091ms step_avg:58.00ms
step:1106/2330 train_time:64151ms step_avg:58.00ms
step:1107/2330 train_time:64209ms step_avg:58.00ms
step:1108/2330 train_time:64269ms step_avg:58.00ms
step:1109/2330 train_time:64326ms step_avg:58.00ms
step:1110/2330 train_time:64387ms step_avg:58.01ms
step:1111/2330 train_time:64444ms step_avg:58.01ms
step:1112/2330 train_time:64504ms step_avg:58.01ms
step:1113/2330 train_time:64561ms step_avg:58.01ms
step:1114/2330 train_time:64621ms step_avg:58.01ms
step:1115/2330 train_time:64678ms step_avg:58.01ms
step:1116/2330 train_time:64738ms step_avg:58.01ms
step:1117/2330 train_time:64794ms step_avg:58.01ms
step:1118/2330 train_time:64855ms step_avg:58.01ms
step:1119/2330 train_time:64912ms step_avg:58.01ms
step:1120/2330 train_time:64972ms step_avg:58.01ms
step:1121/2330 train_time:65029ms step_avg:58.01ms
step:1122/2330 train_time:65089ms step_avg:58.01ms
step:1123/2330 train_time:65147ms step_avg:58.01ms
step:1124/2330 train_time:65207ms step_avg:58.01ms
step:1125/2330 train_time:65265ms step_avg:58.01ms
step:1126/2330 train_time:65324ms step_avg:58.01ms
step:1127/2330 train_time:65382ms step_avg:58.01ms
step:1128/2330 train_time:65442ms step_avg:58.02ms
step:1129/2330 train_time:65499ms step_avg:58.02ms
step:1130/2330 train_time:65559ms step_avg:58.02ms
step:1131/2330 train_time:65616ms step_avg:58.02ms
step:1132/2330 train_time:65676ms step_avg:58.02ms
step:1133/2330 train_time:65733ms step_avg:58.02ms
step:1134/2330 train_time:65792ms step_avg:58.02ms
step:1135/2330 train_time:65849ms step_avg:58.02ms
step:1136/2330 train_time:65909ms step_avg:58.02ms
step:1137/2330 train_time:65966ms step_avg:58.02ms
step:1138/2330 train_time:66026ms step_avg:58.02ms
step:1139/2330 train_time:66082ms step_avg:58.02ms
step:1140/2330 train_time:66144ms step_avg:58.02ms
step:1141/2330 train_time:66201ms step_avg:58.02ms
step:1142/2330 train_time:66261ms step_avg:58.02ms
step:1143/2330 train_time:66319ms step_avg:58.02ms
step:1144/2330 train_time:66378ms step_avg:58.02ms
step:1145/2330 train_time:66435ms step_avg:58.02ms
step:1146/2330 train_time:66495ms step_avg:58.02ms
step:1147/2330 train_time:66552ms step_avg:58.02ms
step:1148/2330 train_time:66612ms step_avg:58.02ms
step:1149/2330 train_time:66669ms step_avg:58.02ms
step:1150/2330 train_time:66729ms step_avg:58.03ms
step:1151/2330 train_time:66786ms step_avg:58.02ms
step:1152/2330 train_time:66846ms step_avg:58.03ms
step:1153/2330 train_time:66902ms step_avg:58.02ms
step:1154/2330 train_time:66963ms step_avg:58.03ms
step:1155/2330 train_time:67020ms step_avg:58.03ms
step:1156/2330 train_time:67080ms step_avg:58.03ms
step:1157/2330 train_time:67138ms step_avg:58.03ms
step:1158/2330 train_time:67197ms step_avg:58.03ms
step:1159/2330 train_time:67255ms step_avg:58.03ms
step:1160/2330 train_time:67315ms step_avg:58.03ms
step:1161/2330 train_time:67372ms step_avg:58.03ms
step:1162/2330 train_time:67431ms step_avg:58.03ms
step:1163/2330 train_time:67488ms step_avg:58.03ms
step:1164/2330 train_time:67548ms step_avg:58.03ms
step:1165/2330 train_time:67604ms step_avg:58.03ms
step:1166/2330 train_time:67665ms step_avg:58.03ms
step:1167/2330 train_time:67721ms step_avg:58.03ms
step:1168/2330 train_time:67782ms step_avg:58.03ms
step:1169/2330 train_time:67840ms step_avg:58.03ms
step:1170/2330 train_time:67900ms step_avg:58.03ms
step:1171/2330 train_time:67957ms step_avg:58.03ms
step:1172/2330 train_time:68018ms step_avg:58.04ms
step:1173/2330 train_time:68075ms step_avg:58.04ms
step:1174/2330 train_time:68135ms step_avg:58.04ms
step:1175/2330 train_time:68193ms step_avg:58.04ms
step:1176/2330 train_time:68253ms step_avg:58.04ms
step:1177/2330 train_time:68310ms step_avg:58.04ms
step:1178/2330 train_time:68371ms step_avg:58.04ms
step:1179/2330 train_time:68428ms step_avg:58.04ms
step:1180/2330 train_time:68487ms step_avg:58.04ms
step:1181/2330 train_time:68544ms step_avg:58.04ms
step:1182/2330 train_time:68604ms step_avg:58.04ms
step:1183/2330 train_time:68660ms step_avg:58.04ms
step:1184/2330 train_time:68720ms step_avg:58.04ms
step:1185/2330 train_time:68777ms step_avg:58.04ms
step:1186/2330 train_time:68838ms step_avg:58.04ms
step:1187/2330 train_time:68895ms step_avg:58.04ms
step:1188/2330 train_time:68955ms step_avg:58.04ms
step:1189/2330 train_time:69013ms step_avg:58.04ms
step:1190/2330 train_time:69072ms step_avg:58.04ms
step:1191/2330 train_time:69129ms step_avg:58.04ms
step:1192/2330 train_time:69188ms step_avg:58.04ms
step:1193/2330 train_time:69245ms step_avg:58.04ms
step:1194/2330 train_time:69307ms step_avg:58.05ms
step:1195/2330 train_time:69364ms step_avg:58.04ms
step:1196/2330 train_time:69423ms step_avg:58.05ms
step:1197/2330 train_time:69480ms step_avg:58.04ms
step:1198/2330 train_time:69540ms step_avg:58.05ms
step:1199/2330 train_time:69597ms step_avg:58.05ms
step:1200/2330 train_time:69657ms step_avg:58.05ms
step:1201/2330 train_time:69714ms step_avg:58.05ms
step:1202/2330 train_time:69774ms step_avg:58.05ms
step:1203/2330 train_time:69831ms step_avg:58.05ms
step:1204/2330 train_time:69890ms step_avg:58.05ms
step:1205/2330 train_time:69947ms step_avg:58.05ms
step:1206/2330 train_time:70008ms step_avg:58.05ms
step:1207/2330 train_time:70065ms step_avg:58.05ms
step:1208/2330 train_time:70124ms step_avg:58.05ms
step:1209/2330 train_time:70181ms step_avg:58.05ms
step:1210/2330 train_time:70242ms step_avg:58.05ms
step:1211/2330 train_time:70299ms step_avg:58.05ms
step:1212/2330 train_time:70359ms step_avg:58.05ms
step:1213/2330 train_time:70416ms step_avg:58.05ms
step:1214/2330 train_time:70477ms step_avg:58.05ms
step:1215/2330 train_time:70534ms step_avg:58.05ms
step:1216/2330 train_time:70594ms step_avg:58.05ms
step:1217/2330 train_time:70652ms step_avg:58.05ms
step:1218/2330 train_time:70711ms step_avg:58.06ms
step:1219/2330 train_time:70769ms step_avg:58.05ms
step:1220/2330 train_time:70828ms step_avg:58.06ms
step:1221/2330 train_time:70884ms step_avg:58.05ms
step:1222/2330 train_time:70945ms step_avg:58.06ms
step:1223/2330 train_time:71002ms step_avg:58.06ms
step:1224/2330 train_time:71063ms step_avg:58.06ms
step:1225/2330 train_time:71120ms step_avg:58.06ms
step:1226/2330 train_time:71180ms step_avg:58.06ms
step:1227/2330 train_time:71237ms step_avg:58.06ms
step:1228/2330 train_time:71297ms step_avg:58.06ms
step:1229/2330 train_time:71354ms step_avg:58.06ms
step:1230/2330 train_time:71414ms step_avg:58.06ms
step:1231/2330 train_time:71472ms step_avg:58.06ms
step:1232/2330 train_time:71531ms step_avg:58.06ms
step:1233/2330 train_time:71588ms step_avg:58.06ms
step:1234/2330 train_time:71648ms step_avg:58.06ms
step:1235/2330 train_time:71704ms step_avg:58.06ms
step:1236/2330 train_time:71765ms step_avg:58.06ms
step:1237/2330 train_time:71822ms step_avg:58.06ms
step:1238/2330 train_time:71881ms step_avg:58.06ms
step:1239/2330 train_time:71939ms step_avg:58.06ms
step:1240/2330 train_time:71999ms step_avg:58.06ms
step:1241/2330 train_time:72058ms step_avg:58.06ms
step:1242/2330 train_time:72117ms step_avg:58.07ms
step:1243/2330 train_time:72175ms step_avg:58.06ms
step:1244/2330 train_time:72234ms step_avg:58.07ms
step:1245/2330 train_time:72290ms step_avg:58.06ms
step:1246/2330 train_time:72351ms step_avg:58.07ms
step:1247/2330 train_time:72407ms step_avg:58.07ms
step:1248/2330 train_time:72468ms step_avg:58.07ms
step:1249/2330 train_time:72524ms step_avg:58.07ms
step:1250/2330 train_time:72584ms step_avg:58.07ms
step:1250/2330 val_loss:3.9876 train_time:72666ms step_avg:58.13ms
step:1251/2330 train_time:72686ms step_avg:58.10ms
step:1252/2330 train_time:72706ms step_avg:58.07ms
step:1253/2330 train_time:72764ms step_avg:58.07ms
step:1254/2330 train_time:72829ms step_avg:58.08ms
step:1255/2330 train_time:72888ms step_avg:58.08ms
step:1256/2330 train_time:72950ms step_avg:58.08ms
step:1257/2330 train_time:73006ms step_avg:58.08ms
step:1258/2330 train_time:73066ms step_avg:58.08ms
step:1259/2330 train_time:73123ms step_avg:58.08ms
step:1260/2330 train_time:73183ms step_avg:58.08ms
step:1261/2330 train_time:73240ms step_avg:58.08ms
step:1262/2330 train_time:73299ms step_avg:58.08ms
step:1263/2330 train_time:73356ms step_avg:58.08ms
step:1264/2330 train_time:73414ms step_avg:58.08ms
step:1265/2330 train_time:73470ms step_avg:58.08ms
step:1266/2330 train_time:73530ms step_avg:58.08ms
step:1267/2330 train_time:73586ms step_avg:58.08ms
step:1268/2330 train_time:73648ms step_avg:58.08ms
step:1269/2330 train_time:73707ms step_avg:58.08ms
step:1270/2330 train_time:73769ms step_avg:58.09ms
step:1271/2330 train_time:73826ms step_avg:58.09ms
step:1272/2330 train_time:73889ms step_avg:58.09ms
step:1273/2330 train_time:73946ms step_avg:58.09ms
step:1274/2330 train_time:74007ms step_avg:58.09ms
step:1275/2330 train_time:74064ms step_avg:58.09ms
step:1276/2330 train_time:74125ms step_avg:58.09ms
step:1277/2330 train_time:74182ms step_avg:58.09ms
step:1278/2330 train_time:74242ms step_avg:58.09ms
step:1279/2330 train_time:74298ms step_avg:58.09ms
step:1280/2330 train_time:74357ms step_avg:58.09ms
step:1281/2330 train_time:74414ms step_avg:58.09ms
step:1282/2330 train_time:74474ms step_avg:58.09ms
step:1283/2330 train_time:74532ms step_avg:58.09ms
step:1284/2330 train_time:74591ms step_avg:58.09ms
step:1285/2330 train_time:74648ms step_avg:58.09ms
step:1286/2330 train_time:74708ms step_avg:58.09ms
step:1287/2330 train_time:74766ms step_avg:58.09ms
step:1288/2330 train_time:74827ms step_avg:58.10ms
step:1289/2330 train_time:74885ms step_avg:58.10ms
step:1290/2330 train_time:74945ms step_avg:58.10ms
step:1291/2330 train_time:75003ms step_avg:58.10ms
step:1292/2330 train_time:75596ms step_avg:58.51ms
step:1293/2330 train_time:75615ms step_avg:58.48ms
step:1294/2330 train_time:75658ms step_avg:58.47ms
step:1295/2330 train_time:75714ms step_avg:58.47ms
step:1296/2330 train_time:75773ms step_avg:58.47ms
step:1297/2330 train_time:75830ms step_avg:58.47ms
step:1298/2330 train_time:75889ms step_avg:58.47ms
step:1299/2330 train_time:75945ms step_avg:58.46ms
step:1300/2330 train_time:76005ms step_avg:58.47ms
step:1301/2330 train_time:76061ms step_avg:58.46ms
step:1302/2330 train_time:76120ms step_avg:58.46ms
step:1303/2330 train_time:76177ms step_avg:58.46ms
step:1304/2330 train_time:76236ms step_avg:58.46ms
step:1305/2330 train_time:76292ms step_avg:58.46ms
step:1306/2330 train_time:76351ms step_avg:58.46ms
step:1307/2330 train_time:76407ms step_avg:58.46ms
step:1308/2330 train_time:76467ms step_avg:58.46ms
step:1309/2330 train_time:76529ms step_avg:58.46ms
step:1310/2330 train_time:76593ms step_avg:58.47ms
step:1311/2330 train_time:76651ms step_avg:58.47ms
step:1312/2330 train_time:76712ms step_avg:58.47ms
step:1313/2330 train_time:76768ms step_avg:58.47ms
step:1314/2330 train_time:76829ms step_avg:58.47ms
step:1315/2330 train_time:76886ms step_avg:58.47ms
step:1316/2330 train_time:76946ms step_avg:58.47ms
step:1317/2330 train_time:77002ms step_avg:58.47ms
step:1318/2330 train_time:77062ms step_avg:58.47ms
step:1319/2330 train_time:77119ms step_avg:58.47ms
step:1320/2330 train_time:77178ms step_avg:58.47ms
step:1321/2330 train_time:77234ms step_avg:58.47ms
step:1322/2330 train_time:77294ms step_avg:58.47ms
step:1323/2330 train_time:77350ms step_avg:58.47ms
step:1324/2330 train_time:77410ms step_avg:58.47ms
step:1325/2330 train_time:77468ms step_avg:58.47ms
step:1326/2330 train_time:77530ms step_avg:58.47ms
step:1327/2330 train_time:77589ms step_avg:58.47ms
step:1328/2330 train_time:77649ms step_avg:58.47ms
step:1329/2330 train_time:77707ms step_avg:58.47ms
step:1330/2330 train_time:77768ms step_avg:58.47ms
step:1331/2330 train_time:77825ms step_avg:58.47ms
step:1332/2330 train_time:77886ms step_avg:58.47ms
step:1333/2330 train_time:77944ms step_avg:58.47ms
step:1334/2330 train_time:78003ms step_avg:58.47ms
step:1335/2330 train_time:78060ms step_avg:58.47ms
step:1336/2330 train_time:78119ms step_avg:58.47ms
step:1337/2330 train_time:78176ms step_avg:58.47ms
step:1338/2330 train_time:78236ms step_avg:58.47ms
step:1339/2330 train_time:78292ms step_avg:58.47ms
step:1340/2330 train_time:78352ms step_avg:58.47ms
step:1341/2330 train_time:78409ms step_avg:58.47ms
step:1342/2330 train_time:78469ms step_avg:58.47ms
step:1343/2330 train_time:78527ms step_avg:58.47ms
step:1344/2330 train_time:78588ms step_avg:58.47ms
step:1345/2330 train_time:78646ms step_avg:58.47ms
step:1346/2330 train_time:78706ms step_avg:58.47ms
step:1347/2330 train_time:78763ms step_avg:58.47ms
step:1348/2330 train_time:78824ms step_avg:58.47ms
step:1349/2330 train_time:78881ms step_avg:58.47ms
step:1350/2330 train_time:78942ms step_avg:58.48ms
step:1351/2330 train_time:78999ms step_avg:58.47ms
step:1352/2330 train_time:79058ms step_avg:58.48ms
step:1353/2330 train_time:79115ms step_avg:58.47ms
step:1354/2330 train_time:79175ms step_avg:58.47ms
step:1355/2330 train_time:79232ms step_avg:58.47ms
step:1356/2330 train_time:79291ms step_avg:58.47ms
step:1357/2330 train_time:79348ms step_avg:58.47ms
step:1358/2330 train_time:79408ms step_avg:58.47ms
step:1359/2330 train_time:79466ms step_avg:58.47ms
step:1360/2330 train_time:79526ms step_avg:58.48ms
step:1361/2330 train_time:79583ms step_avg:58.47ms
step:1362/2330 train_time:79644ms step_avg:58.48ms
step:1363/2330 train_time:79702ms step_avg:58.48ms
step:1364/2330 train_time:79763ms step_avg:58.48ms
step:1365/2330 train_time:79820ms step_avg:58.48ms
step:1366/2330 train_time:79880ms step_avg:58.48ms
step:1367/2330 train_time:79938ms step_avg:58.48ms
step:1368/2330 train_time:79997ms step_avg:58.48ms
step:1369/2330 train_time:80054ms step_avg:58.48ms
step:1370/2330 train_time:80114ms step_avg:58.48ms
step:1371/2330 train_time:80171ms step_avg:58.48ms
step:1372/2330 train_time:80231ms step_avg:58.48ms
step:1373/2330 train_time:80287ms step_avg:58.48ms
step:1374/2330 train_time:80347ms step_avg:58.48ms
step:1375/2330 train_time:80404ms step_avg:58.48ms
step:1376/2330 train_time:80465ms step_avg:58.48ms
step:1377/2330 train_time:80522ms step_avg:58.48ms
step:1378/2330 train_time:80582ms step_avg:58.48ms
step:1379/2330 train_time:80640ms step_avg:58.48ms
step:1380/2330 train_time:80700ms step_avg:58.48ms
step:1381/2330 train_time:80757ms step_avg:58.48ms
step:1382/2330 train_time:80818ms step_avg:58.48ms
step:1383/2330 train_time:80876ms step_avg:58.48ms
step:1384/2330 train_time:80936ms step_avg:58.48ms
step:1385/2330 train_time:80993ms step_avg:58.48ms
step:1386/2330 train_time:81053ms step_avg:58.48ms
step:1387/2330 train_time:81109ms step_avg:58.48ms
step:1388/2330 train_time:81170ms step_avg:58.48ms
step:1389/2330 train_time:81227ms step_avg:58.48ms
step:1390/2330 train_time:81286ms step_avg:58.48ms
step:1391/2330 train_time:81343ms step_avg:58.48ms
step:1392/2330 train_time:81402ms step_avg:58.48ms
step:1393/2330 train_time:81460ms step_avg:58.48ms
step:1394/2330 train_time:81520ms step_avg:58.48ms
step:1395/2330 train_time:81577ms step_avg:58.48ms
step:1396/2330 train_time:81638ms step_avg:58.48ms
step:1397/2330 train_time:81694ms step_avg:58.48ms
step:1398/2330 train_time:81756ms step_avg:58.48ms
step:1399/2330 train_time:81812ms step_avg:58.48ms
step:1400/2330 train_time:81874ms step_avg:58.48ms
step:1401/2330 train_time:81930ms step_avg:58.48ms
step:1402/2330 train_time:81991ms step_avg:58.48ms
step:1403/2330 train_time:82049ms step_avg:58.48ms
step:1404/2330 train_time:82109ms step_avg:58.48ms
step:1405/2330 train_time:82165ms step_avg:58.48ms
step:1406/2330 train_time:82226ms step_avg:58.48ms
step:1407/2330 train_time:82282ms step_avg:58.48ms
step:1408/2330 train_time:82343ms step_avg:58.48ms
step:1409/2330 train_time:82400ms step_avg:58.48ms
step:1410/2330 train_time:82461ms step_avg:58.48ms
step:1411/2330 train_time:82518ms step_avg:58.48ms
step:1412/2330 train_time:82577ms step_avg:58.48ms
step:1413/2330 train_time:82635ms step_avg:58.48ms
step:1414/2330 train_time:82695ms step_avg:58.48ms
step:1415/2330 train_time:82752ms step_avg:58.48ms
step:1416/2330 train_time:82812ms step_avg:58.48ms
step:1417/2330 train_time:82868ms step_avg:58.48ms
step:1418/2330 train_time:82929ms step_avg:58.48ms
step:1419/2330 train_time:82987ms step_avg:58.48ms
step:1420/2330 train_time:83047ms step_avg:58.48ms
step:1421/2330 train_time:83104ms step_avg:58.48ms
step:1422/2330 train_time:83164ms step_avg:58.48ms
step:1423/2330 train_time:83221ms step_avg:58.48ms
step:1424/2330 train_time:83281ms step_avg:58.48ms
step:1425/2330 train_time:83338ms step_avg:58.48ms
step:1426/2330 train_time:83398ms step_avg:58.48ms
step:1427/2330 train_time:83455ms step_avg:58.48ms
step:1428/2330 train_time:83516ms step_avg:58.48ms
step:1429/2330 train_time:83574ms step_avg:58.48ms
step:1430/2330 train_time:83633ms step_avg:58.48ms
step:1431/2330 train_time:83691ms step_avg:58.48ms
step:1432/2330 train_time:83751ms step_avg:58.49ms
step:1433/2330 train_time:83807ms step_avg:58.48ms
step:1434/2330 train_time:83869ms step_avg:58.49ms
step:1435/2330 train_time:83925ms step_avg:58.48ms
step:1436/2330 train_time:83987ms step_avg:58.49ms
step:1437/2330 train_time:84044ms step_avg:58.49ms
step:1438/2330 train_time:84103ms step_avg:58.49ms
step:1439/2330 train_time:84161ms step_avg:58.49ms
step:1440/2330 train_time:84220ms step_avg:58.49ms
step:1441/2330 train_time:84277ms step_avg:58.49ms
step:1442/2330 train_time:84337ms step_avg:58.49ms
step:1443/2330 train_time:84394ms step_avg:58.48ms
step:1444/2330 train_time:84454ms step_avg:58.49ms
step:1445/2330 train_time:84511ms step_avg:58.49ms
step:1446/2330 train_time:84571ms step_avg:58.49ms
step:1447/2330 train_time:84628ms step_avg:58.49ms
step:1448/2330 train_time:84690ms step_avg:58.49ms
step:1449/2330 train_time:84746ms step_avg:58.49ms
step:1450/2330 train_time:84806ms step_avg:58.49ms
step:1451/2330 train_time:84863ms step_avg:58.49ms
step:1452/2330 train_time:84924ms step_avg:58.49ms
step:1453/2330 train_time:84982ms step_avg:58.49ms
step:1454/2330 train_time:85042ms step_avg:58.49ms
step:1455/2330 train_time:85099ms step_avg:58.49ms
step:1456/2330 train_time:85160ms step_avg:58.49ms
step:1457/2330 train_time:85217ms step_avg:58.49ms
step:1458/2330 train_time:85277ms step_avg:58.49ms
step:1459/2330 train_time:85333ms step_avg:58.49ms
step:1460/2330 train_time:85394ms step_avg:58.49ms
step:1461/2330 train_time:85450ms step_avg:58.49ms
step:1462/2330 train_time:85511ms step_avg:58.49ms
step:1463/2330 train_time:85567ms step_avg:58.49ms
step:1464/2330 train_time:85628ms step_avg:58.49ms
step:1465/2330 train_time:85685ms step_avg:58.49ms
step:1466/2330 train_time:85746ms step_avg:58.49ms
step:1467/2330 train_time:85803ms step_avg:58.49ms
step:1468/2330 train_time:85864ms step_avg:58.49ms
step:1469/2330 train_time:85921ms step_avg:58.49ms
step:1470/2330 train_time:85981ms step_avg:58.49ms
step:1471/2330 train_time:86038ms step_avg:58.49ms
step:1472/2330 train_time:86098ms step_avg:58.49ms
step:1473/2330 train_time:86155ms step_avg:58.49ms
step:1474/2330 train_time:86216ms step_avg:58.49ms
step:1475/2330 train_time:86274ms step_avg:58.49ms
step:1476/2330 train_time:86333ms step_avg:58.49ms
step:1477/2330 train_time:86390ms step_avg:58.49ms
step:1478/2330 train_time:86451ms step_avg:58.49ms
step:1479/2330 train_time:86507ms step_avg:58.49ms
step:1480/2330 train_time:86568ms step_avg:58.49ms
step:1481/2330 train_time:86625ms step_avg:58.49ms
step:1482/2330 train_time:86686ms step_avg:58.49ms
step:1483/2330 train_time:86743ms step_avg:58.49ms
step:1484/2330 train_time:86803ms step_avg:58.49ms
step:1485/2330 train_time:86859ms step_avg:58.49ms
step:1486/2330 train_time:86920ms step_avg:58.49ms
step:1487/2330 train_time:86977ms step_avg:58.49ms
step:1488/2330 train_time:87037ms step_avg:58.49ms
step:1489/2330 train_time:87094ms step_avg:58.49ms
step:1490/2330 train_time:87154ms step_avg:58.49ms
step:1491/2330 train_time:87211ms step_avg:58.49ms
step:1492/2330 train_time:87271ms step_avg:58.49ms
step:1493/2330 train_time:87327ms step_avg:58.49ms
step:1494/2330 train_time:87387ms step_avg:58.49ms
step:1495/2330 train_time:87444ms step_avg:58.49ms
step:1496/2330 train_time:87505ms step_avg:58.49ms
step:1497/2330 train_time:87562ms step_avg:58.49ms
step:1498/2330 train_time:87622ms step_avg:58.49ms
step:1499/2330 train_time:87680ms step_avg:58.49ms
step:1500/2330 train_time:87740ms step_avg:58.49ms
step:1500/2330 val_loss:3.9045 train_time:87821ms step_avg:58.55ms
step:1501/2330 train_time:87841ms step_avg:58.52ms
step:1502/2330 train_time:87862ms step_avg:58.50ms
step:1503/2330 train_time:87918ms step_avg:58.49ms
step:1504/2330 train_time:87985ms step_avg:58.50ms
step:1505/2330 train_time:88041ms step_avg:58.50ms
step:1506/2330 train_time:88105ms step_avg:58.50ms
step:1507/2330 train_time:88161ms step_avg:58.50ms
step:1508/2330 train_time:88221ms step_avg:58.50ms
step:1509/2330 train_time:88278ms step_avg:58.50ms
step:1510/2330 train_time:88337ms step_avg:58.50ms
step:1511/2330 train_time:88393ms step_avg:58.50ms
step:1512/2330 train_time:88453ms step_avg:58.50ms
step:1513/2330 train_time:88509ms step_avg:58.50ms
step:1514/2330 train_time:88568ms step_avg:58.50ms
step:1515/2330 train_time:88624ms step_avg:58.50ms
step:1516/2330 train_time:88684ms step_avg:58.50ms
step:1517/2330 train_time:88740ms step_avg:58.50ms
step:1518/2330 train_time:88802ms step_avg:58.50ms
step:1519/2330 train_time:88860ms step_avg:58.50ms
step:1520/2330 train_time:88924ms step_avg:58.50ms
step:1521/2330 train_time:88982ms step_avg:58.50ms
step:1522/2330 train_time:89043ms step_avg:58.50ms
step:1523/2330 train_time:89100ms step_avg:58.50ms
step:1524/2330 train_time:89161ms step_avg:58.50ms
step:1525/2330 train_time:89218ms step_avg:58.50ms
step:1526/2330 train_time:89277ms step_avg:58.50ms
step:1527/2330 train_time:89334ms step_avg:58.50ms
step:1528/2330 train_time:89393ms step_avg:58.50ms
step:1529/2330 train_time:89452ms step_avg:58.50ms
step:1530/2330 train_time:89510ms step_avg:58.50ms
step:1531/2330 train_time:89568ms step_avg:58.50ms
step:1532/2330 train_time:89628ms step_avg:58.50ms
step:1533/2330 train_time:89686ms step_avg:58.50ms
step:1534/2330 train_time:89746ms step_avg:58.50ms
step:1535/2330 train_time:89804ms step_avg:58.50ms
step:1536/2330 train_time:89866ms step_avg:58.51ms
step:1537/2330 train_time:89924ms step_avg:58.51ms
step:1538/2330 train_time:89987ms step_avg:58.51ms
step:1539/2330 train_time:90044ms step_avg:58.51ms
step:1540/2330 train_time:90108ms step_avg:58.51ms
step:1541/2330 train_time:90165ms step_avg:58.51ms
step:1542/2330 train_time:90226ms step_avg:58.51ms
step:1543/2330 train_time:90282ms step_avg:58.51ms
step:1544/2330 train_time:90344ms step_avg:58.51ms
step:1545/2330 train_time:90400ms step_avg:58.51ms
step:1546/2330 train_time:90461ms step_avg:58.51ms
step:1547/2330 train_time:90518ms step_avg:58.51ms
step:1548/2330 train_time:90578ms step_avg:58.51ms
step:1549/2330 train_time:90634ms step_avg:58.51ms
step:1550/2330 train_time:90697ms step_avg:58.51ms
step:1551/2330 train_time:90755ms step_avg:58.51ms
step:1552/2330 train_time:90816ms step_avg:58.52ms
step:1553/2330 train_time:90875ms step_avg:58.52ms
step:1554/2330 train_time:90935ms step_avg:58.52ms
step:1555/2330 train_time:90993ms step_avg:58.52ms
step:1556/2330 train_time:91056ms step_avg:58.52ms
step:1557/2330 train_time:91114ms step_avg:58.52ms
step:1558/2330 train_time:91175ms step_avg:58.52ms
step:1559/2330 train_time:91234ms step_avg:58.52ms
step:1560/2330 train_time:91294ms step_avg:58.52ms
step:1561/2330 train_time:91352ms step_avg:58.52ms
step:1562/2330 train_time:91413ms step_avg:58.52ms
step:1563/2330 train_time:91471ms step_avg:58.52ms
step:1564/2330 train_time:91531ms step_avg:58.52ms
step:1565/2330 train_time:91588ms step_avg:58.52ms
step:1566/2330 train_time:91649ms step_avg:58.52ms
step:1567/2330 train_time:91707ms step_avg:58.52ms
step:1568/2330 train_time:91768ms step_avg:58.53ms
step:1569/2330 train_time:91825ms step_avg:58.52ms
step:1570/2330 train_time:91887ms step_avg:58.53ms
step:1571/2330 train_time:91945ms step_avg:58.53ms
step:1572/2330 train_time:92007ms step_avg:58.53ms
step:1573/2330 train_time:92065ms step_avg:58.53ms
step:1574/2330 train_time:92127ms step_avg:58.53ms
step:1575/2330 train_time:92184ms step_avg:58.53ms
step:1576/2330 train_time:92247ms step_avg:58.53ms
step:1577/2330 train_time:92304ms step_avg:58.53ms
step:1578/2330 train_time:92365ms step_avg:58.53ms
step:1579/2330 train_time:92422ms step_avg:58.53ms
step:1580/2330 train_time:92483ms step_avg:58.53ms
step:1581/2330 train_time:92539ms step_avg:58.53ms
step:1582/2330 train_time:92600ms step_avg:58.53ms
step:1583/2330 train_time:92657ms step_avg:58.53ms
step:1584/2330 train_time:92720ms step_avg:58.54ms
step:1585/2330 train_time:92776ms step_avg:58.53ms
step:1586/2330 train_time:92837ms step_avg:58.54ms
step:1587/2330 train_time:92894ms step_avg:58.53ms
step:1588/2330 train_time:92956ms step_avg:58.54ms
step:1589/2330 train_time:93014ms step_avg:58.54ms
step:1590/2330 train_time:93075ms step_avg:58.54ms
step:1591/2330 train_time:93133ms step_avg:58.54ms
step:1592/2330 train_time:93195ms step_avg:58.54ms
step:1593/2330 train_time:93253ms step_avg:58.54ms
step:1594/2330 train_time:93314ms step_avg:58.54ms
step:1595/2330 train_time:93373ms step_avg:58.54ms
step:1596/2330 train_time:93433ms step_avg:58.54ms
step:1597/2330 train_time:93491ms step_avg:58.54ms
step:1598/2330 train_time:93551ms step_avg:58.54ms
step:1599/2330 train_time:93608ms step_avg:58.54ms
step:1600/2330 train_time:93670ms step_avg:58.54ms
step:1601/2330 train_time:93728ms step_avg:58.54ms
step:1602/2330 train_time:93789ms step_avg:58.54ms
step:1603/2330 train_time:93846ms step_avg:58.54ms
step:1604/2330 train_time:93907ms step_avg:58.55ms
step:1605/2330 train_time:93964ms step_avg:58.54ms
step:1606/2330 train_time:94025ms step_avg:58.55ms
step:1607/2330 train_time:94082ms step_avg:58.55ms
step:1608/2330 train_time:94144ms step_avg:58.55ms
step:1609/2330 train_time:94201ms step_avg:58.55ms
step:1610/2330 train_time:94263ms step_avg:58.55ms
step:1611/2330 train_time:94320ms step_avg:58.55ms
step:1612/2330 train_time:94381ms step_avg:58.55ms
step:1613/2330 train_time:94438ms step_avg:58.55ms
step:1614/2330 train_time:94499ms step_avg:58.55ms
step:1615/2330 train_time:94556ms step_avg:58.55ms
step:1616/2330 train_time:94618ms step_avg:58.55ms
step:1617/2330 train_time:94675ms step_avg:58.55ms
step:1618/2330 train_time:94737ms step_avg:58.55ms
step:1619/2330 train_time:94794ms step_avg:58.55ms
step:1620/2330 train_time:94855ms step_avg:58.55ms
step:1621/2330 train_time:94913ms step_avg:58.55ms
step:1622/2330 train_time:94973ms step_avg:58.55ms
step:1623/2330 train_time:95031ms step_avg:58.55ms
step:1624/2330 train_time:95092ms step_avg:58.55ms
step:1625/2330 train_time:95150ms step_avg:58.55ms
step:1626/2330 train_time:95211ms step_avg:58.56ms
step:1627/2330 train_time:95270ms step_avg:58.56ms
step:1628/2330 train_time:95330ms step_avg:58.56ms
step:1629/2330 train_time:95388ms step_avg:58.56ms
step:1630/2330 train_time:95448ms step_avg:58.56ms
step:1631/2330 train_time:95505ms step_avg:58.56ms
step:1632/2330 train_time:95566ms step_avg:58.56ms
step:1633/2330 train_time:95623ms step_avg:58.56ms
step:1634/2330 train_time:95685ms step_avg:58.56ms
step:1635/2330 train_time:95741ms step_avg:58.56ms
step:1636/2330 train_time:95803ms step_avg:58.56ms
step:1637/2330 train_time:95860ms step_avg:58.56ms
step:1638/2330 train_time:95923ms step_avg:58.56ms
step:1639/2330 train_time:95979ms step_avg:58.56ms
step:1640/2330 train_time:96042ms step_avg:58.56ms
step:1641/2330 train_time:96098ms step_avg:58.56ms
step:1642/2330 train_time:96160ms step_avg:58.56ms
step:1643/2330 train_time:96217ms step_avg:58.56ms
step:1644/2330 train_time:96278ms step_avg:58.56ms
step:1645/2330 train_time:96334ms step_avg:58.56ms
step:1646/2330 train_time:96396ms step_avg:58.56ms
step:1647/2330 train_time:96453ms step_avg:58.56ms
step:1648/2330 train_time:96516ms step_avg:58.57ms
step:1649/2330 train_time:96573ms step_avg:58.56ms
step:1650/2330 train_time:96634ms step_avg:58.57ms
step:1651/2330 train_time:96691ms step_avg:58.57ms
step:1652/2330 train_time:96752ms step_avg:58.57ms
step:1653/2330 train_time:96810ms step_avg:58.57ms
step:1654/2330 train_time:96872ms step_avg:58.57ms
step:1655/2330 train_time:96930ms step_avg:58.57ms
step:1656/2330 train_time:96990ms step_avg:58.57ms
step:1657/2330 train_time:97048ms step_avg:58.57ms
step:1658/2330 train_time:97109ms step_avg:58.57ms
step:1659/2330 train_time:97167ms step_avg:58.57ms
step:1660/2330 train_time:97227ms step_avg:58.57ms
step:1661/2330 train_time:97285ms step_avg:58.57ms
step:1662/2330 train_time:97345ms step_avg:58.57ms
step:1663/2330 train_time:97402ms step_avg:58.57ms
step:1664/2330 train_time:97465ms step_avg:58.57ms
step:1665/2330 train_time:97521ms step_avg:58.57ms
step:1666/2330 train_time:97583ms step_avg:58.57ms
step:1667/2330 train_time:97640ms step_avg:58.57ms
step:1668/2330 train_time:97701ms step_avg:58.57ms
step:1669/2330 train_time:97758ms step_avg:58.57ms
step:1670/2330 train_time:97820ms step_avg:58.57ms
step:1671/2330 train_time:97877ms step_avg:58.57ms
step:1672/2330 train_time:97938ms step_avg:58.58ms
step:1673/2330 train_time:97995ms step_avg:58.57ms
step:1674/2330 train_time:98058ms step_avg:58.58ms
step:1675/2330 train_time:98115ms step_avg:58.58ms
step:1676/2330 train_time:98176ms step_avg:58.58ms
step:1677/2330 train_time:98234ms step_avg:58.58ms
step:1678/2330 train_time:98295ms step_avg:58.58ms
step:1679/2330 train_time:98353ms step_avg:58.58ms
step:1680/2330 train_time:98415ms step_avg:58.58ms
step:1681/2330 train_time:98474ms step_avg:58.58ms
step:1682/2330 train_time:98534ms step_avg:58.58ms
step:1683/2330 train_time:98592ms step_avg:58.58ms
step:1684/2330 train_time:98652ms step_avg:58.58ms
step:1685/2330 train_time:98710ms step_avg:58.58ms
step:1686/2330 train_time:98771ms step_avg:58.58ms
step:1687/2330 train_time:98829ms step_avg:58.58ms
step:1688/2330 train_time:98890ms step_avg:58.58ms
step:1689/2330 train_time:98948ms step_avg:58.58ms
step:1690/2330 train_time:99008ms step_avg:58.58ms
step:1691/2330 train_time:99066ms step_avg:58.58ms
step:1692/2330 train_time:99127ms step_avg:58.59ms
step:1693/2330 train_time:99185ms step_avg:58.59ms
step:1694/2330 train_time:99246ms step_avg:58.59ms
step:1695/2330 train_time:99304ms step_avg:58.59ms
step:1696/2330 train_time:99365ms step_avg:58.59ms
step:1697/2330 train_time:99423ms step_avg:58.59ms
step:1698/2330 train_time:99485ms step_avg:58.59ms
step:1699/2330 train_time:99542ms step_avg:58.59ms
step:1700/2330 train_time:99602ms step_avg:58.59ms
step:1701/2330 train_time:99659ms step_avg:58.59ms
step:1702/2330 train_time:99721ms step_avg:58.59ms
step:1703/2330 train_time:99778ms step_avg:58.59ms
step:1704/2330 train_time:99839ms step_avg:58.59ms
step:1705/2330 train_time:99895ms step_avg:58.59ms
step:1706/2330 train_time:99958ms step_avg:58.59ms
step:1707/2330 train_time:100015ms step_avg:58.59ms
step:1708/2330 train_time:100078ms step_avg:58.59ms
step:1709/2330 train_time:100135ms step_avg:58.59ms
step:1710/2330 train_time:100196ms step_avg:58.59ms
step:1711/2330 train_time:100254ms step_avg:58.59ms
step:1712/2330 train_time:100316ms step_avg:58.60ms
step:1713/2330 train_time:100374ms step_avg:58.60ms
step:1714/2330 train_time:100434ms step_avg:58.60ms
step:1715/2330 train_time:100491ms step_avg:58.60ms
step:1716/2330 train_time:100553ms step_avg:58.60ms
step:1717/2330 train_time:100612ms step_avg:58.60ms
step:1718/2330 train_time:100672ms step_avg:58.60ms
step:1719/2330 train_time:100729ms step_avg:58.60ms
step:1720/2330 train_time:100790ms step_avg:58.60ms
step:1721/2330 train_time:100848ms step_avg:58.60ms
step:1722/2330 train_time:100909ms step_avg:58.60ms
step:1723/2330 train_time:100967ms step_avg:58.60ms
step:1724/2330 train_time:101028ms step_avg:58.60ms
step:1725/2330 train_time:101085ms step_avg:58.60ms
step:1726/2330 train_time:101147ms step_avg:58.60ms
step:1727/2330 train_time:101205ms step_avg:58.60ms
step:1728/2330 train_time:101267ms step_avg:58.60ms
step:1729/2330 train_time:101324ms step_avg:58.60ms
step:1730/2330 train_time:101386ms step_avg:58.60ms
step:1731/2330 train_time:101443ms step_avg:58.60ms
step:1732/2330 train_time:101505ms step_avg:58.61ms
step:1733/2330 train_time:101562ms step_avg:58.60ms
step:1734/2330 train_time:101623ms step_avg:58.61ms
step:1735/2330 train_time:101680ms step_avg:58.61ms
step:1736/2330 train_time:101742ms step_avg:58.61ms
step:1737/2330 train_time:101799ms step_avg:58.61ms
step:1738/2330 train_time:101860ms step_avg:58.61ms
step:1739/2330 train_time:101917ms step_avg:58.61ms
step:1740/2330 train_time:101979ms step_avg:58.61ms
step:1741/2330 train_time:102035ms step_avg:58.61ms
step:1742/2330 train_time:102097ms step_avg:58.61ms
step:1743/2330 train_time:102154ms step_avg:58.61ms
step:1744/2330 train_time:102217ms step_avg:58.61ms
step:1745/2330 train_time:102274ms step_avg:58.61ms
step:1746/2330 train_time:102335ms step_avg:58.61ms
step:1747/2330 train_time:102392ms step_avg:58.61ms
step:1748/2330 train_time:102453ms step_avg:58.61ms
step:1749/2330 train_time:102512ms step_avg:58.61ms
step:1750/2330 train_time:102573ms step_avg:58.61ms
step:1750/2330 val_loss:3.8187 train_time:102654ms step_avg:58.66ms
step:1751/2330 train_time:102674ms step_avg:58.64ms
step:1752/2330 train_time:102696ms step_avg:58.62ms
step:1753/2330 train_time:102748ms step_avg:58.61ms
step:1754/2330 train_time:102814ms step_avg:58.62ms
step:1755/2330 train_time:102870ms step_avg:58.62ms
step:1756/2330 train_time:102933ms step_avg:58.62ms
step:1757/2330 train_time:102990ms step_avg:58.62ms
step:1758/2330 train_time:103051ms step_avg:58.62ms
step:1759/2330 train_time:103107ms step_avg:58.62ms
step:1760/2330 train_time:103167ms step_avg:58.62ms
step:1761/2330 train_time:103224ms step_avg:58.62ms
step:1762/2330 train_time:103284ms step_avg:58.62ms
step:1763/2330 train_time:103341ms step_avg:58.62ms
step:1764/2330 train_time:103400ms step_avg:58.62ms
step:1765/2330 train_time:103457ms step_avg:58.62ms
step:1766/2330 train_time:103516ms step_avg:58.62ms
step:1767/2330 train_time:103576ms step_avg:58.62ms
step:1768/2330 train_time:103641ms step_avg:58.62ms
step:1769/2330 train_time:103701ms step_avg:58.62ms
step:1770/2330 train_time:103761ms step_avg:58.62ms
step:1771/2330 train_time:103819ms step_avg:58.62ms
step:1772/2330 train_time:103879ms step_avg:58.62ms
step:1773/2330 train_time:103936ms step_avg:58.62ms
step:1774/2330 train_time:103996ms step_avg:58.62ms
step:1775/2330 train_time:104053ms step_avg:58.62ms
step:1776/2330 train_time:104114ms step_avg:58.62ms
step:1777/2330 train_time:104171ms step_avg:58.62ms
step:1778/2330 train_time:104232ms step_avg:58.62ms
step:1779/2330 train_time:104289ms step_avg:58.62ms
step:1780/2330 train_time:104349ms step_avg:58.62ms
step:1781/2330 train_time:104407ms step_avg:58.62ms
step:1782/2330 train_time:104466ms step_avg:58.62ms
step:1783/2330 train_time:104525ms step_avg:58.62ms
step:1784/2330 train_time:104587ms step_avg:58.63ms
step:1785/2330 train_time:104647ms step_avg:58.63ms
step:1786/2330 train_time:104709ms step_avg:58.63ms
step:1787/2330 train_time:104768ms step_avg:58.63ms
step:1788/2330 train_time:104829ms step_avg:58.63ms
step:1789/2330 train_time:104886ms step_avg:58.63ms
step:1790/2330 train_time:104947ms step_avg:58.63ms
step:1791/2330 train_time:105004ms step_avg:58.63ms
step:1792/2330 train_time:105065ms step_avg:58.63ms
step:1793/2330 train_time:105122ms step_avg:58.63ms
step:1794/2330 train_time:105182ms step_avg:58.63ms
step:1795/2330 train_time:105238ms step_avg:58.63ms
step:1796/2330 train_time:105300ms step_avg:58.63ms
step:1797/2330 train_time:105356ms step_avg:58.63ms
step:1798/2330 train_time:105417ms step_avg:58.63ms
step:1799/2330 train_time:105474ms step_avg:58.63ms
step:1800/2330 train_time:105536ms step_avg:58.63ms
step:1801/2330 train_time:105594ms step_avg:58.63ms
step:1802/2330 train_time:105656ms step_avg:58.63ms
step:1803/2330 train_time:105714ms step_avg:58.63ms
step:1804/2330 train_time:105775ms step_avg:58.63ms
step:1805/2330 train_time:105832ms step_avg:58.63ms
step:1806/2330 train_time:105894ms step_avg:58.63ms
step:1807/2330 train_time:105950ms step_avg:58.63ms
step:1808/2330 train_time:106012ms step_avg:58.63ms
step:1809/2330 train_time:106068ms step_avg:58.63ms
step:1810/2330 train_time:106129ms step_avg:58.63ms
step:1811/2330 train_time:106186ms step_avg:58.63ms
step:1812/2330 train_time:106247ms step_avg:58.64ms
step:1813/2330 train_time:106304ms step_avg:58.63ms
step:1814/2330 train_time:106365ms step_avg:58.64ms
step:1815/2330 train_time:106422ms step_avg:58.63ms
step:1816/2330 train_time:106483ms step_avg:58.64ms
step:1817/2330 train_time:106540ms step_avg:58.64ms
step:1818/2330 train_time:106602ms step_avg:58.64ms
step:1819/2330 train_time:106660ms step_avg:58.64ms
step:1820/2330 train_time:106722ms step_avg:58.64ms
step:1821/2330 train_time:106779ms step_avg:58.64ms
step:1822/2330 train_time:106841ms step_avg:58.64ms
step:1823/2330 train_time:106898ms step_avg:58.64ms
step:1824/2330 train_time:106959ms step_avg:58.64ms
step:1825/2330 train_time:107017ms step_avg:58.64ms
step:1826/2330 train_time:107077ms step_avg:58.64ms
step:1827/2330 train_time:107134ms step_avg:58.64ms
step:1828/2330 train_time:107195ms step_avg:58.64ms
step:1829/2330 train_time:107252ms step_avg:58.64ms
step:1830/2330 train_time:107312ms step_avg:58.64ms
step:1831/2330 train_time:107369ms step_avg:58.64ms
step:1832/2330 train_time:107430ms step_avg:58.64ms
step:1833/2330 train_time:107489ms step_avg:58.64ms
step:1834/2330 train_time:107548ms step_avg:58.64ms
step:1835/2330 train_time:107607ms step_avg:58.64ms
step:1836/2330 train_time:107668ms step_avg:58.64ms
step:1837/2330 train_time:107727ms step_avg:58.64ms
step:1838/2330 train_time:107788ms step_avg:58.64ms
step:1839/2330 train_time:107847ms step_avg:58.64ms
step:1840/2330 train_time:107907ms step_avg:58.65ms
step:1841/2330 train_time:107966ms step_avg:58.65ms
step:1842/2330 train_time:108027ms step_avg:58.65ms
step:1843/2330 train_time:108084ms step_avg:58.65ms
step:1844/2330 train_time:108144ms step_avg:58.65ms
step:1845/2330 train_time:108201ms step_avg:58.65ms
step:1846/2330 train_time:108262ms step_avg:58.65ms
step:1847/2330 train_time:108319ms step_avg:58.65ms
step:1848/2330 train_time:108379ms step_avg:58.65ms
step:1849/2330 train_time:108436ms step_avg:58.65ms
step:1850/2330 train_time:108497ms step_avg:58.65ms
step:1851/2330 train_time:108554ms step_avg:58.65ms
step:1852/2330 train_time:108615ms step_avg:58.65ms
step:1853/2330 train_time:108672ms step_avg:58.65ms
step:1854/2330 train_time:108734ms step_avg:58.65ms
step:1855/2330 train_time:108792ms step_avg:58.65ms
step:1856/2330 train_time:108855ms step_avg:58.65ms
step:1857/2330 train_time:108913ms step_avg:58.65ms
step:1858/2330 train_time:108974ms step_avg:58.65ms
step:1859/2330 train_time:109031ms step_avg:58.65ms
step:1860/2330 train_time:109092ms step_avg:58.65ms
step:1861/2330 train_time:109150ms step_avg:58.65ms
step:1862/2330 train_time:109210ms step_avg:58.65ms
step:1863/2330 train_time:109268ms step_avg:58.65ms
step:1864/2330 train_time:109329ms step_avg:58.65ms
step:1865/2330 train_time:109387ms step_avg:58.65ms
step:1866/2330 train_time:109447ms step_avg:58.65ms
step:1867/2330 train_time:109504ms step_avg:58.65ms
step:1868/2330 train_time:109565ms step_avg:58.65ms
step:1869/2330 train_time:109623ms step_avg:58.65ms
step:1870/2330 train_time:109684ms step_avg:58.65ms
step:1871/2330 train_time:109742ms step_avg:58.65ms
step:1872/2330 train_time:109803ms step_avg:58.66ms
step:1873/2330 train_time:109861ms step_avg:58.65ms
step:1874/2330 train_time:109922ms step_avg:58.66ms
step:1875/2330 train_time:109979ms step_avg:58.66ms
step:1876/2330 train_time:110041ms step_avg:58.66ms
step:1877/2330 train_time:110098ms step_avg:58.66ms
step:1878/2330 train_time:110159ms step_avg:58.66ms
step:1879/2330 train_time:110216ms step_avg:58.66ms
step:1880/2330 train_time:110276ms step_avg:58.66ms
step:1881/2330 train_time:110334ms step_avg:58.66ms
step:1882/2330 train_time:110394ms step_avg:58.66ms
step:1883/2330 train_time:110452ms step_avg:58.66ms
step:1884/2330 train_time:110514ms step_avg:58.66ms
step:1885/2330 train_time:110571ms step_avg:58.66ms
step:1886/2330 train_time:110632ms step_avg:58.66ms
step:1887/2330 train_time:110690ms step_avg:58.66ms
step:1888/2330 train_time:110751ms step_avg:58.66ms
step:1889/2330 train_time:110809ms step_avg:58.66ms
step:1890/2330 train_time:110871ms step_avg:58.66ms
step:1891/2330 train_time:110929ms step_avg:58.66ms
step:1892/2330 train_time:110991ms step_avg:58.66ms
step:1893/2330 train_time:111048ms step_avg:58.66ms
step:1894/2330 train_time:111110ms step_avg:58.66ms
step:1895/2330 train_time:111168ms step_avg:58.66ms
step:1896/2330 train_time:111228ms step_avg:58.66ms
step:1897/2330 train_time:111287ms step_avg:58.66ms
step:1898/2330 train_time:111347ms step_avg:58.67ms
step:1899/2330 train_time:111404ms step_avg:58.66ms
step:1900/2330 train_time:111465ms step_avg:58.67ms
step:1901/2330 train_time:111523ms step_avg:58.67ms
step:1902/2330 train_time:111583ms step_avg:58.67ms
step:1903/2330 train_time:111640ms step_avg:58.67ms
step:1904/2330 train_time:111700ms step_avg:58.67ms
step:1905/2330 train_time:111757ms step_avg:58.66ms
step:1906/2330 train_time:111818ms step_avg:58.67ms
step:1907/2330 train_time:111876ms step_avg:58.67ms
step:1908/2330 train_time:111936ms step_avg:58.67ms
step:1909/2330 train_time:111993ms step_avg:58.67ms
step:1910/2330 train_time:112054ms step_avg:58.67ms
step:1911/2330 train_time:112111ms step_avg:58.67ms
step:1912/2330 train_time:112171ms step_avg:58.67ms
step:1913/2330 train_time:112229ms step_avg:58.67ms
step:1914/2330 train_time:112290ms step_avg:58.67ms
step:1915/2330 train_time:112349ms step_avg:58.67ms
step:1916/2330 train_time:112408ms step_avg:58.67ms
step:1917/2330 train_time:112466ms step_avg:58.67ms
step:1918/2330 train_time:112527ms step_avg:58.67ms
step:1919/2330 train_time:112586ms step_avg:58.67ms
step:1920/2330 train_time:112646ms step_avg:58.67ms
step:1921/2330 train_time:112704ms step_avg:58.67ms
step:1922/2330 train_time:112764ms step_avg:58.67ms
step:1923/2330 train_time:112823ms step_avg:58.67ms
step:1924/2330 train_time:112884ms step_avg:58.67ms
step:1925/2330 train_time:112941ms step_avg:58.67ms
step:1926/2330 train_time:113001ms step_avg:58.67ms
step:1927/2330 train_time:113058ms step_avg:58.67ms
step:1928/2330 train_time:113120ms step_avg:58.67ms
step:1929/2330 train_time:113177ms step_avg:58.67ms
step:1930/2330 train_time:113239ms step_avg:58.67ms
step:1931/2330 train_time:113296ms step_avg:58.67ms
step:1932/2330 train_time:113356ms step_avg:58.67ms
step:1933/2330 train_time:113412ms step_avg:58.67ms
step:1934/2330 train_time:113475ms step_avg:58.67ms
step:1935/2330 train_time:113531ms step_avg:58.67ms
step:1936/2330 train_time:113594ms step_avg:58.67ms
step:1937/2330 train_time:113651ms step_avg:58.67ms
step:1938/2330 train_time:113713ms step_avg:58.68ms
step:1939/2330 train_time:113770ms step_avg:58.67ms
step:1940/2330 train_time:113831ms step_avg:58.68ms
step:1941/2330 train_time:113890ms step_avg:58.68ms
step:1942/2330 train_time:113951ms step_avg:58.68ms
step:1943/2330 train_time:114010ms step_avg:58.68ms
step:1944/2330 train_time:114069ms step_avg:58.68ms
step:1945/2330 train_time:114127ms step_avg:58.68ms
step:1946/2330 train_time:114188ms step_avg:58.68ms
step:1947/2330 train_time:114247ms step_avg:58.68ms
step:1948/2330 train_time:114307ms step_avg:58.68ms
step:1949/2330 train_time:114365ms step_avg:58.68ms
step:1950/2330 train_time:114425ms step_avg:58.68ms
step:1951/2330 train_time:114482ms step_avg:58.68ms
step:1952/2330 train_time:114545ms step_avg:58.68ms
step:1953/2330 train_time:114602ms step_avg:58.68ms
step:1954/2330 train_time:114663ms step_avg:58.68ms
step:1955/2330 train_time:114720ms step_avg:58.68ms
step:1956/2330 train_time:114780ms step_avg:58.68ms
step:1957/2330 train_time:114838ms step_avg:58.68ms
step:1958/2330 train_time:114899ms step_avg:58.68ms
step:1959/2330 train_time:114957ms step_avg:58.68ms
step:1960/2330 train_time:115017ms step_avg:58.68ms
step:1961/2330 train_time:115075ms step_avg:58.68ms
step:1962/2330 train_time:115134ms step_avg:58.68ms
step:1963/2330 train_time:115192ms step_avg:58.68ms
step:1964/2330 train_time:115252ms step_avg:58.68ms
step:1965/2330 train_time:115309ms step_avg:58.68ms
step:1966/2330 train_time:115370ms step_avg:58.68ms
step:1967/2330 train_time:115428ms step_avg:58.68ms
step:1968/2330 train_time:115491ms step_avg:58.68ms
step:1969/2330 train_time:115548ms step_avg:58.68ms
step:1970/2330 train_time:115610ms step_avg:58.69ms
step:1971/2330 train_time:115668ms step_avg:58.68ms
step:1972/2330 train_time:115728ms step_avg:58.69ms
step:1973/2330 train_time:115786ms step_avg:58.69ms
step:1974/2330 train_time:115847ms step_avg:58.69ms
step:1975/2330 train_time:115904ms step_avg:58.69ms
step:1976/2330 train_time:115964ms step_avg:58.69ms
step:1977/2330 train_time:116022ms step_avg:58.69ms
step:1978/2330 train_time:116082ms step_avg:58.69ms
step:1979/2330 train_time:116140ms step_avg:58.69ms
step:1980/2330 train_time:116202ms step_avg:58.69ms
step:1981/2330 train_time:116259ms step_avg:58.69ms
step:1982/2330 train_time:116320ms step_avg:58.69ms
step:1983/2330 train_time:116378ms step_avg:58.69ms
step:1984/2330 train_time:116438ms step_avg:58.69ms
step:1985/2330 train_time:116495ms step_avg:58.69ms
step:1986/2330 train_time:116556ms step_avg:58.69ms
step:1987/2330 train_time:116613ms step_avg:58.69ms
step:1988/2330 train_time:116675ms step_avg:58.69ms
step:1989/2330 train_time:116732ms step_avg:58.69ms
step:1990/2330 train_time:116794ms step_avg:58.69ms
step:1991/2330 train_time:116851ms step_avg:58.69ms
step:1992/2330 train_time:116912ms step_avg:58.69ms
step:1993/2330 train_time:116970ms step_avg:58.69ms
step:1994/2330 train_time:117030ms step_avg:58.69ms
step:1995/2330 train_time:117088ms step_avg:58.69ms
step:1996/2330 train_time:117149ms step_avg:58.69ms
step:1997/2330 train_time:117206ms step_avg:58.69ms
step:1998/2330 train_time:117267ms step_avg:58.69ms
step:1999/2330 train_time:117325ms step_avg:58.69ms
step:2000/2330 train_time:117386ms step_avg:58.69ms
step:2000/2330 val_loss:3.7559 train_time:117467ms step_avg:58.73ms
step:2001/2330 train_time:117486ms step_avg:58.71ms
step:2002/2330 train_time:117507ms step_avg:58.69ms
step:2003/2330 train_time:117571ms step_avg:58.70ms
step:2004/2330 train_time:117636ms step_avg:58.70ms
step:2005/2330 train_time:117695ms step_avg:58.70ms
step:2006/2330 train_time:117755ms step_avg:58.70ms
step:2007/2330 train_time:117812ms step_avg:58.70ms
step:2008/2330 train_time:117872ms step_avg:58.70ms
step:2009/2330 train_time:117927ms step_avg:58.70ms
step:2010/2330 train_time:117990ms step_avg:58.70ms
step:2011/2330 train_time:118046ms step_avg:58.70ms
step:2012/2330 train_time:118107ms step_avg:58.70ms
step:2013/2330 train_time:118164ms step_avg:58.70ms
step:2014/2330 train_time:118224ms step_avg:58.70ms
step:2015/2330 train_time:118281ms step_avg:58.70ms
step:2016/2330 train_time:118341ms step_avg:58.70ms
step:2017/2330 train_time:118397ms step_avg:58.70ms
step:2018/2330 train_time:118457ms step_avg:58.70ms
step:2019/2330 train_time:118516ms step_avg:58.70ms
step:2020/2330 train_time:118580ms step_avg:58.70ms
step:2021/2330 train_time:118638ms step_avg:58.70ms
step:2022/2330 train_time:118701ms step_avg:58.70ms
step:2023/2330 train_time:118760ms step_avg:58.70ms
step:2024/2330 train_time:118822ms step_avg:58.71ms
step:2025/2330 train_time:118879ms step_avg:58.71ms
step:2026/2330 train_time:118941ms step_avg:58.71ms
step:2027/2330 train_time:118997ms step_avg:58.71ms
step:2028/2330 train_time:119058ms step_avg:58.71ms
step:2029/2330 train_time:119115ms step_avg:58.71ms
step:2030/2330 train_time:119174ms step_avg:58.71ms
step:2031/2330 train_time:119231ms step_avg:58.71ms
step:2032/2330 train_time:119291ms step_avg:58.71ms
step:2033/2330 train_time:119347ms step_avg:58.71ms
step:2034/2330 train_time:119408ms step_avg:58.71ms
step:2035/2330 train_time:119465ms step_avg:58.71ms
step:2036/2330 train_time:119528ms step_avg:58.71ms
step:2037/2330 train_time:119587ms step_avg:58.71ms
step:2038/2330 train_time:119650ms step_avg:58.71ms
step:2039/2330 train_time:119708ms step_avg:58.71ms
step:2040/2330 train_time:119771ms step_avg:58.71ms
step:2041/2330 train_time:119828ms step_avg:58.71ms
step:2042/2330 train_time:119889ms step_avg:58.71ms
step:2043/2330 train_time:119945ms step_avg:58.71ms
step:2044/2330 train_time:120008ms step_avg:58.71ms
step:2045/2330 train_time:120065ms step_avg:58.71ms
step:2046/2330 train_time:120127ms step_avg:58.71ms
step:2047/2330 train_time:120184ms step_avg:58.71ms
step:2048/2330 train_time:120244ms step_avg:58.71ms
step:2049/2330 train_time:120301ms step_avg:58.71ms
step:2050/2330 train_time:120361ms step_avg:58.71ms
step:2051/2330 train_time:120419ms step_avg:58.71ms
step:2052/2330 train_time:120479ms step_avg:58.71ms
step:2053/2330 train_time:120537ms step_avg:58.71ms
step:2054/2330 train_time:120598ms step_avg:58.71ms
step:2055/2330 train_time:120656ms step_avg:58.71ms
step:2056/2330 train_time:120719ms step_avg:58.72ms
step:2057/2330 train_time:120776ms step_avg:58.71ms
step:2058/2330 train_time:120838ms step_avg:58.72ms
step:2059/2330 train_time:120894ms step_avg:58.72ms
step:2060/2330 train_time:120957ms step_avg:58.72ms
step:2061/2330 train_time:121013ms step_avg:58.72ms
step:2062/2330 train_time:121074ms step_avg:58.72ms
step:2063/2330 train_time:121131ms step_avg:58.72ms
step:2064/2330 train_time:121191ms step_avg:58.72ms
step:2065/2330 train_time:121247ms step_avg:58.72ms
step:2066/2330 train_time:121308ms step_avg:58.72ms
step:2067/2330 train_time:121365ms step_avg:58.72ms
step:2068/2330 train_time:121426ms step_avg:58.72ms
step:2069/2330 train_time:121485ms step_avg:58.72ms
step:2070/2330 train_time:121546ms step_avg:58.72ms
step:2071/2330 train_time:121605ms step_avg:58.72ms
step:2072/2330 train_time:121666ms step_avg:58.72ms
step:2073/2330 train_time:121724ms step_avg:58.72ms
step:2074/2330 train_time:121784ms step_avg:58.72ms
step:2075/2330 train_time:121843ms step_avg:58.72ms
step:2076/2330 train_time:121904ms step_avg:58.72ms
step:2077/2330 train_time:121963ms step_avg:58.72ms
step:2078/2330 train_time:122023ms step_avg:58.72ms
step:2079/2330 train_time:122081ms step_avg:58.72ms
step:2080/2330 train_time:122140ms step_avg:58.72ms
step:2081/2330 train_time:122197ms step_avg:58.72ms
step:2082/2330 train_time:122259ms step_avg:58.72ms
step:2083/2330 train_time:122316ms step_avg:58.72ms
step:2084/2330 train_time:122376ms step_avg:58.72ms
step:2085/2330 train_time:122433ms step_avg:58.72ms
step:2086/2330 train_time:122494ms step_avg:58.72ms
step:2087/2330 train_time:122552ms step_avg:58.72ms
step:2088/2330 train_time:122613ms step_avg:58.72ms
step:2089/2330 train_time:122671ms step_avg:58.72ms
step:2090/2330 train_time:122731ms step_avg:58.72ms
step:2091/2330 train_time:122789ms step_avg:58.72ms
step:2092/2330 train_time:122851ms step_avg:58.72ms
step:2093/2330 train_time:122909ms step_avg:58.72ms
step:2094/2330 train_time:122969ms step_avg:58.72ms
step:2095/2330 train_time:123027ms step_avg:58.72ms
step:2096/2330 train_time:123087ms step_avg:58.72ms
step:2097/2330 train_time:123144ms step_avg:58.72ms
step:2098/2330 train_time:123206ms step_avg:58.73ms
step:2099/2330 train_time:123263ms step_avg:58.72ms
step:2100/2330 train_time:123324ms step_avg:58.73ms
step:2101/2330 train_time:123381ms step_avg:58.72ms
step:2102/2330 train_time:123443ms step_avg:58.73ms
step:2103/2330 train_time:123501ms step_avg:58.73ms
step:2104/2330 train_time:123562ms step_avg:58.73ms
step:2105/2330 train_time:123620ms step_avg:58.73ms
step:2106/2330 train_time:123680ms step_avg:58.73ms
step:2107/2330 train_time:123737ms step_avg:58.73ms
step:2108/2330 train_time:123799ms step_avg:58.73ms
step:2109/2330 train_time:123856ms step_avg:58.73ms
step:2110/2330 train_time:123918ms step_avg:58.73ms
step:2111/2330 train_time:123976ms step_avg:58.73ms
step:2112/2330 train_time:124036ms step_avg:58.73ms
step:2113/2330 train_time:124093ms step_avg:58.73ms
step:2114/2330 train_time:124154ms step_avg:58.73ms
step:2115/2330 train_time:124211ms step_avg:58.73ms
step:2116/2330 train_time:124271ms step_avg:58.73ms
step:2117/2330 train_time:124328ms step_avg:58.73ms
step:2118/2330 train_time:124389ms step_avg:58.73ms
step:2119/2330 train_time:124447ms step_avg:58.73ms
step:2120/2330 train_time:124509ms step_avg:58.73ms
step:2121/2330 train_time:124567ms step_avg:58.73ms
step:2122/2330 train_time:124628ms step_avg:58.73ms
step:2123/2330 train_time:124686ms step_avg:58.73ms
step:2124/2330 train_time:124748ms step_avg:58.73ms
step:2125/2330 train_time:124805ms step_avg:58.73ms
step:2126/2330 train_time:124867ms step_avg:58.73ms
step:2127/2330 train_time:124924ms step_avg:58.73ms
step:2128/2330 train_time:124986ms step_avg:58.73ms
step:2129/2330 train_time:125043ms step_avg:58.73ms
step:2130/2330 train_time:125104ms step_avg:58.73ms
step:2131/2330 train_time:125163ms step_avg:58.73ms
step:2132/2330 train_time:125223ms step_avg:58.73ms
step:2133/2330 train_time:125280ms step_avg:58.73ms
step:2134/2330 train_time:125341ms step_avg:58.74ms
step:2135/2330 train_time:125398ms step_avg:58.73ms
step:2136/2330 train_time:125459ms step_avg:58.74ms
step:2137/2330 train_time:125516ms step_avg:58.73ms
step:2138/2330 train_time:125578ms step_avg:58.74ms
step:2139/2330 train_time:125635ms step_avg:58.74ms
step:2140/2330 train_time:125696ms step_avg:58.74ms
step:2141/2330 train_time:125753ms step_avg:58.74ms
step:2142/2330 train_time:125815ms step_avg:58.74ms
step:2143/2330 train_time:125872ms step_avg:58.74ms
step:2144/2330 train_time:125933ms step_avg:58.74ms
step:2145/2330 train_time:125990ms step_avg:58.74ms
step:2146/2330 train_time:126052ms step_avg:58.74ms
step:2147/2330 train_time:126109ms step_avg:58.74ms
step:2148/2330 train_time:126170ms step_avg:58.74ms
step:2149/2330 train_time:126228ms step_avg:58.74ms
step:2150/2330 train_time:126289ms step_avg:58.74ms
step:2151/2330 train_time:126347ms step_avg:58.74ms
step:2152/2330 train_time:126409ms step_avg:58.74ms
step:2153/2330 train_time:126468ms step_avg:58.74ms
step:2154/2330 train_time:126527ms step_avg:58.74ms
step:2155/2330 train_time:126585ms step_avg:58.74ms
step:2156/2330 train_time:126648ms step_avg:58.74ms
step:2157/2330 train_time:126706ms step_avg:58.74ms
step:2158/2330 train_time:126767ms step_avg:58.74ms
step:2159/2330 train_time:126825ms step_avg:58.74ms
step:2160/2330 train_time:126886ms step_avg:58.74ms
step:2161/2330 train_time:126943ms step_avg:58.74ms
step:2162/2330 train_time:127004ms step_avg:58.74ms
step:2163/2330 train_time:127062ms step_avg:58.74ms
step:2164/2330 train_time:127122ms step_avg:58.74ms
step:2165/2330 train_time:127181ms step_avg:58.74ms
step:2166/2330 train_time:127241ms step_avg:58.74ms
step:2167/2330 train_time:127298ms step_avg:58.74ms
step:2168/2330 train_time:127359ms step_avg:58.74ms
step:2169/2330 train_time:127416ms step_avg:58.74ms
step:2170/2330 train_time:127477ms step_avg:58.75ms
step:2171/2330 train_time:127534ms step_avg:58.74ms
step:2172/2330 train_time:127594ms step_avg:58.75ms
step:2173/2330 train_time:127652ms step_avg:58.74ms
step:2174/2330 train_time:127713ms step_avg:58.75ms
step:2175/2330 train_time:127771ms step_avg:58.75ms
step:2176/2330 train_time:127832ms step_avg:58.75ms
step:2177/2330 train_time:127889ms step_avg:58.75ms
step:2178/2330 train_time:127952ms step_avg:58.75ms
step:2179/2330 train_time:128008ms step_avg:58.75ms
step:2180/2330 train_time:128072ms step_avg:58.75ms
step:2181/2330 train_time:128129ms step_avg:58.75ms
step:2182/2330 train_time:128189ms step_avg:58.75ms
step:2183/2330 train_time:128247ms step_avg:58.75ms
step:2184/2330 train_time:128309ms step_avg:58.75ms
step:2185/2330 train_time:128368ms step_avg:58.75ms
step:2186/2330 train_time:128429ms step_avg:58.75ms
step:2187/2330 train_time:128486ms step_avg:58.75ms
step:2188/2330 train_time:128548ms step_avg:58.75ms
step:2189/2330 train_time:128605ms step_avg:58.75ms
step:2190/2330 train_time:128666ms step_avg:58.75ms
step:2191/2330 train_time:128724ms step_avg:58.75ms
step:2192/2330 train_time:128785ms step_avg:58.75ms
step:2193/2330 train_time:128843ms step_avg:58.75ms
step:2194/2330 train_time:128904ms step_avg:58.75ms
step:2195/2330 train_time:128961ms step_avg:58.75ms
step:2196/2330 train_time:129022ms step_avg:58.75ms
step:2197/2330 train_time:129079ms step_avg:58.75ms
step:2198/2330 train_time:129140ms step_avg:58.75ms
step:2199/2330 train_time:129197ms step_avg:58.75ms
step:2200/2330 train_time:129258ms step_avg:58.75ms
step:2201/2330 train_time:129316ms step_avg:58.75ms
step:2202/2330 train_time:129376ms step_avg:58.75ms
step:2203/2330 train_time:129433ms step_avg:58.75ms
step:2204/2330 train_time:129494ms step_avg:58.75ms
step:2205/2330 train_time:129551ms step_avg:58.75ms
step:2206/2330 train_time:129611ms step_avg:58.75ms
step:2207/2330 train_time:129669ms step_avg:58.75ms
step:2208/2330 train_time:129730ms step_avg:58.75ms
step:2209/2330 train_time:129787ms step_avg:58.75ms
step:2210/2330 train_time:129848ms step_avg:58.75ms
step:2211/2330 train_time:129906ms step_avg:58.75ms
step:2212/2330 train_time:129967ms step_avg:58.76ms
step:2213/2330 train_time:130025ms step_avg:58.76ms
step:2214/2330 train_time:130087ms step_avg:58.76ms
step:2215/2330 train_time:130144ms step_avg:58.76ms
step:2216/2330 train_time:130206ms step_avg:58.76ms
step:2217/2330 train_time:130264ms step_avg:58.76ms
step:2218/2330 train_time:130325ms step_avg:58.76ms
step:2219/2330 train_time:130383ms step_avg:58.76ms
step:2220/2330 train_time:130444ms step_avg:58.76ms
step:2221/2330 train_time:130502ms step_avg:58.76ms
step:2222/2330 train_time:130562ms step_avg:58.76ms
step:2223/2330 train_time:130620ms step_avg:58.76ms
step:2224/2330 train_time:130680ms step_avg:58.76ms
step:2225/2330 train_time:130737ms step_avg:58.76ms
step:2226/2330 train_time:130798ms step_avg:58.76ms
step:2227/2330 train_time:130855ms step_avg:58.76ms
step:2228/2330 train_time:130917ms step_avg:58.76ms
step:2229/2330 train_time:130974ms step_avg:58.76ms
step:2230/2330 train_time:131035ms step_avg:58.76ms
step:2231/2330 train_time:131091ms step_avg:58.76ms
step:2232/2330 train_time:131153ms step_avg:58.76ms
step:2233/2330 train_time:131210ms step_avg:58.76ms
step:2234/2330 train_time:131271ms step_avg:58.76ms
step:2235/2330 train_time:131328ms step_avg:58.76ms
step:2236/2330 train_time:131390ms step_avg:58.76ms
step:2237/2330 train_time:131447ms step_avg:58.76ms
step:2238/2330 train_time:131510ms step_avg:58.76ms
step:2239/2330 train_time:131567ms step_avg:58.76ms
step:2240/2330 train_time:131628ms step_avg:58.76ms
step:2241/2330 train_time:131686ms step_avg:58.76ms
step:2242/2330 train_time:131746ms step_avg:58.76ms
step:2243/2330 train_time:131804ms step_avg:58.76ms
step:2244/2330 train_time:131866ms step_avg:58.76ms
step:2245/2330 train_time:131924ms step_avg:58.76ms
step:2246/2330 train_time:131984ms step_avg:58.76ms
step:2247/2330 train_time:132042ms step_avg:58.76ms
step:2248/2330 train_time:132103ms step_avg:58.76ms
step:2249/2330 train_time:132159ms step_avg:58.76ms
step:2250/2330 train_time:132221ms step_avg:58.76ms
step:2250/2330 val_loss:3.7067 train_time:132302ms step_avg:58.80ms
step:2251/2330 train_time:132319ms step_avg:58.78ms
step:2252/2330 train_time:132342ms step_avg:58.77ms
step:2253/2330 train_time:132402ms step_avg:58.77ms
step:2254/2330 train_time:132465ms step_avg:58.77ms
step:2255/2330 train_time:132523ms step_avg:58.77ms
step:2256/2330 train_time:132584ms step_avg:58.77ms
step:2257/2330 train_time:132641ms step_avg:58.77ms
step:2258/2330 train_time:132701ms step_avg:58.77ms
step:2259/2330 train_time:132758ms step_avg:58.77ms
step:2260/2330 train_time:132818ms step_avg:58.77ms
step:2261/2330 train_time:132874ms step_avg:58.77ms
step:2262/2330 train_time:132934ms step_avg:58.77ms
step:2263/2330 train_time:132990ms step_avg:58.77ms
step:2264/2330 train_time:133051ms step_avg:58.77ms
step:2265/2330 train_time:133108ms step_avg:58.77ms
step:2266/2330 train_time:133168ms step_avg:58.77ms
step:2267/2330 train_time:133226ms step_avg:58.77ms
step:2268/2330 train_time:133289ms step_avg:58.77ms
step:2269/2330 train_time:133347ms step_avg:58.77ms
step:2270/2330 train_time:133412ms step_avg:58.77ms
step:2271/2330 train_time:133470ms step_avg:58.77ms
step:2272/2330 train_time:133534ms step_avg:58.77ms
step:2273/2330 train_time:133590ms step_avg:58.77ms
step:2274/2330 train_time:133653ms step_avg:58.77ms
step:2275/2330 train_time:133710ms step_avg:58.77ms
step:2276/2330 train_time:133770ms step_avg:58.77ms
step:2277/2330 train_time:133826ms step_avg:58.77ms
step:2278/2330 train_time:133887ms step_avg:58.77ms
step:2279/2330 train_time:133944ms step_avg:58.77ms
step:2280/2330 train_time:134004ms step_avg:58.77ms
step:2281/2330 train_time:134061ms step_avg:58.77ms
step:2282/2330 train_time:134120ms step_avg:58.77ms
step:2283/2330 train_time:134178ms step_avg:58.77ms
step:2284/2330 train_time:134240ms step_avg:58.77ms
step:2285/2330 train_time:134297ms step_avg:58.77ms
step:2286/2330 train_time:134359ms step_avg:58.77ms
step:2287/2330 train_time:134416ms step_avg:58.77ms
step:2288/2330 train_time:134480ms step_avg:58.78ms
step:2289/2330 train_time:134537ms step_avg:58.78ms
step:2290/2330 train_time:134600ms step_avg:58.78ms
step:2291/2330 train_time:134657ms step_avg:58.78ms
step:2292/2330 train_time:134718ms step_avg:58.78ms
step:2293/2330 train_time:134775ms step_avg:58.78ms
step:2294/2330 train_time:134836ms step_avg:58.78ms
step:2295/2330 train_time:134893ms step_avg:58.78ms
step:2296/2330 train_time:134953ms step_avg:58.78ms
step:2297/2330 train_time:135010ms step_avg:58.78ms
step:2298/2330 train_time:135071ms step_avg:58.78ms
step:2299/2330 train_time:135128ms step_avg:58.78ms
step:2300/2330 train_time:135189ms step_avg:58.78ms
step:2301/2330 train_time:135247ms step_avg:58.78ms
step:2302/2330 train_time:135307ms step_avg:58.78ms
step:2303/2330 train_time:135365ms step_avg:58.78ms
step:2304/2330 train_time:135429ms step_avg:58.78ms
step:2305/2330 train_time:135488ms step_avg:58.78ms
step:2306/2330 train_time:135548ms step_avg:58.78ms
step:2307/2330 train_time:135606ms step_avg:58.78ms
step:2308/2330 train_time:135668ms step_avg:58.78ms
step:2309/2330 train_time:135726ms step_avg:58.78ms
step:2310/2330 train_time:135787ms step_avg:58.78ms
step:2311/2330 train_time:135845ms step_avg:58.78ms
step:2312/2330 train_time:135904ms step_avg:58.78ms
step:2313/2330 train_time:135961ms step_avg:58.78ms
step:2314/2330 train_time:136022ms step_avg:58.78ms
step:2315/2330 train_time:136079ms step_avg:58.78ms
step:2316/2330 train_time:136139ms step_avg:58.78ms
step:2317/2330 train_time:136196ms step_avg:58.78ms
step:2318/2330 train_time:136257ms step_avg:58.78ms
step:2319/2330 train_time:136313ms step_avg:58.78ms
step:2320/2330 train_time:136376ms step_avg:58.78ms
step:2321/2330 train_time:136434ms step_avg:58.78ms
step:2322/2330 train_time:136494ms step_avg:58.78ms
step:2323/2330 train_time:136551ms step_avg:58.78ms
step:2324/2330 train_time:136613ms step_avg:58.78ms
step:2325/2330 train_time:136670ms step_avg:58.78ms
step:2326/2330 train_time:136732ms step_avg:58.78ms
step:2327/2330 train_time:136789ms step_avg:58.78ms
step:2328/2330 train_time:136850ms step_avg:58.78ms
step:2329/2330 train_time:136906ms step_avg:58.78ms
step:2330/2330 train_time:136969ms step_avg:58.78ms
step:2330/2330 val_loss:3.6908 train_time:137051ms step_avg:58.82ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
