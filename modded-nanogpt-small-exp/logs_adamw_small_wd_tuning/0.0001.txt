import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:55:37 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   31C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:86.69ms
step:2/2330 train_time:179ms step_avg:89.45ms
step:3/2330 train_time:197ms step_avg:65.78ms
step:4/2330 train_time:216ms step_avg:54.04ms
step:5/2330 train_time:270ms step_avg:53.98ms
step:6/2330 train_time:328ms step_avg:54.68ms
step:7/2330 train_time:383ms step_avg:54.75ms
step:8/2330 train_time:441ms step_avg:55.18ms
step:9/2330 train_time:496ms step_avg:55.16ms
step:10/2330 train_time:555ms step_avg:55.54ms
step:11/2330 train_time:611ms step_avg:55.57ms
step:12/2330 train_time:670ms step_avg:55.81ms
step:13/2330 train_time:725ms step_avg:55.80ms
step:14/2330 train_time:783ms step_avg:55.96ms
step:15/2330 train_time:839ms step_avg:55.92ms
step:16/2330 train_time:897ms step_avg:56.03ms
step:17/2330 train_time:952ms step_avg:56.02ms
step:18/2330 train_time:1012ms step_avg:56.24ms
step:19/2330 train_time:1072ms step_avg:56.40ms
step:20/2330 train_time:1133ms step_avg:56.64ms
step:21/2330 train_time:1190ms step_avg:56.68ms
step:22/2330 train_time:1251ms step_avg:56.85ms
step:23/2330 train_time:1307ms step_avg:56.81ms
step:24/2330 train_time:1367ms step_avg:56.95ms
step:25/2330 train_time:1423ms step_avg:56.92ms
step:26/2330 train_time:1481ms step_avg:56.96ms
step:27/2330 train_time:1536ms step_avg:56.90ms
step:28/2330 train_time:1595ms step_avg:56.97ms
step:29/2330 train_time:1651ms step_avg:56.92ms
step:30/2330 train_time:1709ms step_avg:56.98ms
step:31/2330 train_time:1766ms step_avg:56.96ms
step:32/2330 train_time:1824ms step_avg:57.01ms
step:33/2330 train_time:1880ms step_avg:56.96ms
step:34/2330 train_time:1938ms step_avg:57.00ms
step:35/2330 train_time:1994ms step_avg:56.97ms
step:36/2330 train_time:2054ms step_avg:57.05ms
step:37/2330 train_time:2111ms step_avg:57.07ms
step:38/2330 train_time:2171ms step_avg:57.12ms
step:39/2330 train_time:2227ms step_avg:57.11ms
step:40/2330 train_time:2288ms step_avg:57.19ms
step:41/2330 train_time:2344ms step_avg:57.17ms
step:42/2330 train_time:2403ms step_avg:57.21ms
step:43/2330 train_time:2458ms step_avg:57.17ms
step:44/2330 train_time:2518ms step_avg:57.22ms
step:45/2330 train_time:2573ms step_avg:57.19ms
step:46/2330 train_time:2633ms step_avg:57.23ms
step:47/2330 train_time:2688ms step_avg:57.19ms
step:48/2330 train_time:2747ms step_avg:57.22ms
step:49/2330 train_time:2802ms step_avg:57.19ms
step:50/2330 train_time:2861ms step_avg:57.22ms
step:51/2330 train_time:2917ms step_avg:57.19ms
step:52/2330 train_time:2976ms step_avg:57.22ms
step:53/2330 train_time:3033ms step_avg:57.22ms
step:54/2330 train_time:3092ms step_avg:57.26ms
step:55/2330 train_time:3149ms step_avg:57.25ms
step:56/2330 train_time:3208ms step_avg:57.28ms
step:57/2330 train_time:3264ms step_avg:57.26ms
step:58/2330 train_time:3323ms step_avg:57.30ms
step:59/2330 train_time:3380ms step_avg:57.29ms
step:60/2330 train_time:3439ms step_avg:57.31ms
step:61/2330 train_time:3495ms step_avg:57.29ms
step:62/2330 train_time:3553ms step_avg:57.31ms
step:63/2330 train_time:3609ms step_avg:57.29ms
step:64/2330 train_time:3668ms step_avg:57.31ms
step:65/2330 train_time:3723ms step_avg:57.28ms
step:66/2330 train_time:3783ms step_avg:57.32ms
step:67/2330 train_time:3839ms step_avg:57.29ms
step:68/2330 train_time:3897ms step_avg:57.31ms
step:69/2330 train_time:3953ms step_avg:57.28ms
step:70/2330 train_time:4012ms step_avg:57.32ms
step:71/2330 train_time:4068ms step_avg:57.30ms
step:72/2330 train_time:4128ms step_avg:57.33ms
step:73/2330 train_time:4184ms step_avg:57.32ms
step:74/2330 train_time:4243ms step_avg:57.34ms
step:75/2330 train_time:4299ms step_avg:57.32ms
step:76/2330 train_time:4359ms step_avg:57.35ms
step:77/2330 train_time:4415ms step_avg:57.33ms
step:78/2330 train_time:4474ms step_avg:57.35ms
step:79/2330 train_time:4530ms step_avg:57.34ms
step:80/2330 train_time:4589ms step_avg:57.36ms
step:81/2330 train_time:4645ms step_avg:57.34ms
step:82/2330 train_time:4703ms step_avg:57.36ms
step:83/2330 train_time:4760ms step_avg:57.35ms
step:84/2330 train_time:4818ms step_avg:57.36ms
step:85/2330 train_time:4873ms step_avg:57.33ms
step:86/2330 train_time:4933ms step_avg:57.36ms
step:87/2330 train_time:4989ms step_avg:57.34ms
step:88/2330 train_time:5048ms step_avg:57.36ms
step:89/2330 train_time:5103ms step_avg:57.34ms
step:90/2330 train_time:5164ms step_avg:57.38ms
step:91/2330 train_time:5220ms step_avg:57.36ms
step:92/2330 train_time:5279ms step_avg:57.38ms
step:93/2330 train_time:5335ms step_avg:57.37ms
step:94/2330 train_time:5394ms step_avg:57.38ms
step:95/2330 train_time:5450ms step_avg:57.37ms
step:96/2330 train_time:5509ms step_avg:57.39ms
step:97/2330 train_time:5565ms step_avg:57.37ms
step:98/2330 train_time:5625ms step_avg:57.39ms
step:99/2330 train_time:5681ms step_avg:57.38ms
step:100/2330 train_time:5740ms step_avg:57.40ms
step:101/2330 train_time:5795ms step_avg:57.38ms
step:102/2330 train_time:5854ms step_avg:57.39ms
step:103/2330 train_time:5910ms step_avg:57.38ms
step:104/2330 train_time:5968ms step_avg:57.39ms
step:105/2330 train_time:6024ms step_avg:57.37ms
step:106/2330 train_time:6083ms step_avg:57.39ms
step:107/2330 train_time:6139ms step_avg:57.38ms
step:108/2330 train_time:6198ms step_avg:57.39ms
step:109/2330 train_time:6254ms step_avg:57.38ms
step:110/2330 train_time:6313ms step_avg:57.39ms
step:111/2330 train_time:6371ms step_avg:57.39ms
step:112/2330 train_time:6430ms step_avg:57.41ms
step:113/2330 train_time:6486ms step_avg:57.39ms
step:114/2330 train_time:6545ms step_avg:57.41ms
step:115/2330 train_time:6601ms step_avg:57.40ms
step:116/2330 train_time:6660ms step_avg:57.41ms
step:117/2330 train_time:6716ms step_avg:57.40ms
step:118/2330 train_time:6775ms step_avg:57.41ms
step:119/2330 train_time:6831ms step_avg:57.40ms
step:120/2330 train_time:6889ms step_avg:57.41ms
step:121/2330 train_time:6946ms step_avg:57.40ms
step:122/2330 train_time:7005ms step_avg:57.42ms
step:123/2330 train_time:7061ms step_avg:57.41ms
step:124/2330 train_time:7120ms step_avg:57.42ms
step:125/2330 train_time:7176ms step_avg:57.41ms
step:126/2330 train_time:7234ms step_avg:57.41ms
step:127/2330 train_time:7290ms step_avg:57.40ms
step:128/2330 train_time:7349ms step_avg:57.42ms
step:129/2330 train_time:7406ms step_avg:57.41ms
step:130/2330 train_time:7465ms step_avg:57.42ms
step:131/2330 train_time:7521ms step_avg:57.41ms
step:132/2330 train_time:7579ms step_avg:57.42ms
step:133/2330 train_time:7635ms step_avg:57.40ms
step:134/2330 train_time:7694ms step_avg:57.42ms
step:135/2330 train_time:7750ms step_avg:57.41ms
step:136/2330 train_time:7809ms step_avg:57.42ms
step:137/2330 train_time:7865ms step_avg:57.41ms
step:138/2330 train_time:7925ms step_avg:57.42ms
step:139/2330 train_time:7981ms step_avg:57.41ms
step:140/2330 train_time:8040ms step_avg:57.43ms
step:141/2330 train_time:8096ms step_avg:57.42ms
step:142/2330 train_time:8154ms step_avg:57.42ms
step:143/2330 train_time:8210ms step_avg:57.41ms
step:144/2330 train_time:8269ms step_avg:57.42ms
step:145/2330 train_time:8325ms step_avg:57.41ms
step:146/2330 train_time:8384ms step_avg:57.42ms
step:147/2330 train_time:8440ms step_avg:57.41ms
step:148/2330 train_time:8499ms step_avg:57.42ms
step:149/2330 train_time:8554ms step_avg:57.41ms
step:150/2330 train_time:8614ms step_avg:57.43ms
step:151/2330 train_time:8670ms step_avg:57.42ms
step:152/2330 train_time:8729ms step_avg:57.43ms
step:153/2330 train_time:8784ms step_avg:57.41ms
step:154/2330 train_time:8844ms step_avg:57.43ms
step:155/2330 train_time:8900ms step_avg:57.42ms
step:156/2330 train_time:8960ms step_avg:57.44ms
step:157/2330 train_time:9016ms step_avg:57.43ms
step:158/2330 train_time:9075ms step_avg:57.44ms
step:159/2330 train_time:9131ms step_avg:57.43ms
step:160/2330 train_time:9190ms step_avg:57.44ms
step:161/2330 train_time:9246ms step_avg:57.43ms
step:162/2330 train_time:9305ms step_avg:57.44ms
step:163/2330 train_time:9362ms step_avg:57.43ms
step:164/2330 train_time:9421ms step_avg:57.44ms
step:165/2330 train_time:9477ms step_avg:57.43ms
step:166/2330 train_time:9535ms step_avg:57.44ms
step:167/2330 train_time:9591ms step_avg:57.43ms
step:168/2330 train_time:9650ms step_avg:57.44ms
step:169/2330 train_time:9706ms step_avg:57.43ms
step:170/2330 train_time:9765ms step_avg:57.44ms
step:171/2330 train_time:9821ms step_avg:57.43ms
step:172/2330 train_time:9880ms step_avg:57.44ms
step:173/2330 train_time:9936ms step_avg:57.43ms
step:174/2330 train_time:9995ms step_avg:57.44ms
step:175/2330 train_time:10051ms step_avg:57.43ms
step:176/2330 train_time:10111ms step_avg:57.45ms
step:177/2330 train_time:10167ms step_avg:57.44ms
step:178/2330 train_time:10226ms step_avg:57.45ms
step:179/2330 train_time:10282ms step_avg:57.44ms
step:180/2330 train_time:10341ms step_avg:57.45ms
step:181/2330 train_time:10398ms step_avg:57.45ms
step:182/2330 train_time:10457ms step_avg:57.45ms
step:183/2330 train_time:10513ms step_avg:57.45ms
step:184/2330 train_time:10572ms step_avg:57.46ms
step:185/2330 train_time:10628ms step_avg:57.45ms
step:186/2330 train_time:10688ms step_avg:57.46ms
step:187/2330 train_time:10743ms step_avg:57.45ms
step:188/2330 train_time:10804ms step_avg:57.47ms
step:189/2330 train_time:10859ms step_avg:57.46ms
step:190/2330 train_time:10919ms step_avg:57.47ms
step:191/2330 train_time:10974ms step_avg:57.46ms
step:192/2330 train_time:11033ms step_avg:57.46ms
step:193/2330 train_time:11090ms step_avg:57.46ms
step:194/2330 train_time:11149ms step_avg:57.47ms
step:195/2330 train_time:11206ms step_avg:57.47ms
step:196/2330 train_time:11265ms step_avg:57.47ms
step:197/2330 train_time:11321ms step_avg:57.47ms
step:198/2330 train_time:11379ms step_avg:57.47ms
step:199/2330 train_time:11435ms step_avg:57.46ms
step:200/2330 train_time:11494ms step_avg:57.47ms
step:201/2330 train_time:11550ms step_avg:57.46ms
step:202/2330 train_time:11609ms step_avg:57.47ms
step:203/2330 train_time:11665ms step_avg:57.47ms
step:204/2330 train_time:11725ms step_avg:57.47ms
step:205/2330 train_time:11781ms step_avg:57.47ms
step:206/2330 train_time:11839ms step_avg:57.47ms
step:207/2330 train_time:11895ms step_avg:57.46ms
step:208/2330 train_time:11954ms step_avg:57.47ms
step:209/2330 train_time:12010ms step_avg:57.46ms
step:210/2330 train_time:12069ms step_avg:57.47ms
step:211/2330 train_time:12125ms step_avg:57.47ms
step:212/2330 train_time:12184ms step_avg:57.47ms
step:213/2330 train_time:12240ms step_avg:57.47ms
step:214/2330 train_time:12299ms step_avg:57.47ms
step:215/2330 train_time:12356ms step_avg:57.47ms
step:216/2330 train_time:12414ms step_avg:57.47ms
step:217/2330 train_time:12471ms step_avg:57.47ms
step:218/2330 train_time:12530ms step_avg:57.48ms
step:219/2330 train_time:12586ms step_avg:57.47ms
step:220/2330 train_time:12645ms step_avg:57.48ms
step:221/2330 train_time:12701ms step_avg:57.47ms
step:222/2330 train_time:12760ms step_avg:57.48ms
step:223/2330 train_time:12817ms step_avg:57.47ms
step:224/2330 train_time:12875ms step_avg:57.48ms
step:225/2330 train_time:12931ms step_avg:57.47ms
step:226/2330 train_time:12991ms step_avg:57.48ms
step:227/2330 train_time:13047ms step_avg:57.48ms
step:228/2330 train_time:13106ms step_avg:57.48ms
step:229/2330 train_time:13162ms step_avg:57.48ms
step:230/2330 train_time:13221ms step_avg:57.48ms
step:231/2330 train_time:13278ms step_avg:57.48ms
step:232/2330 train_time:13336ms step_avg:57.48ms
step:233/2330 train_time:13392ms step_avg:57.47ms
step:234/2330 train_time:13451ms step_avg:57.48ms
step:235/2330 train_time:13507ms step_avg:57.48ms
step:236/2330 train_time:13566ms step_avg:57.48ms
step:237/2330 train_time:13622ms step_avg:57.48ms
step:238/2330 train_time:13681ms step_avg:57.48ms
step:239/2330 train_time:13737ms step_avg:57.48ms
step:240/2330 train_time:13795ms step_avg:57.48ms
step:241/2330 train_time:13851ms step_avg:57.47ms
step:242/2330 train_time:13910ms step_avg:57.48ms
step:243/2330 train_time:13967ms step_avg:57.48ms
step:244/2330 train_time:14026ms step_avg:57.49ms
step:245/2330 train_time:14083ms step_avg:57.48ms
step:246/2330 train_time:14141ms step_avg:57.48ms
step:247/2330 train_time:14197ms step_avg:57.48ms
step:248/2330 train_time:14257ms step_avg:57.49ms
step:249/2330 train_time:14313ms step_avg:57.48ms
step:250/2330 train_time:14372ms step_avg:57.49ms
step:250/2330 val_loss:4.9084 train_time:14450ms step_avg:57.80ms
step:251/2330 train_time:14469ms step_avg:57.65ms
step:252/2330 train_time:14489ms step_avg:57.50ms
step:253/2330 train_time:14545ms step_avg:57.49ms
step:254/2330 train_time:14611ms step_avg:57.53ms
step:255/2330 train_time:14667ms step_avg:57.52ms
step:256/2330 train_time:14730ms step_avg:57.54ms
step:257/2330 train_time:14786ms step_avg:57.53ms
step:258/2330 train_time:14847ms step_avg:57.55ms
step:259/2330 train_time:14903ms step_avg:57.54ms
step:260/2330 train_time:14961ms step_avg:57.54ms
step:261/2330 train_time:15017ms step_avg:57.54ms
step:262/2330 train_time:15075ms step_avg:57.54ms
step:263/2330 train_time:15130ms step_avg:57.53ms
step:264/2330 train_time:15189ms step_avg:57.53ms
step:265/2330 train_time:15245ms step_avg:57.53ms
step:266/2330 train_time:15303ms step_avg:57.53ms
step:267/2330 train_time:15359ms step_avg:57.52ms
step:268/2330 train_time:15418ms step_avg:57.53ms
step:269/2330 train_time:15476ms step_avg:57.53ms
step:270/2330 train_time:15535ms step_avg:57.54ms
step:271/2330 train_time:15592ms step_avg:57.53ms
step:272/2330 train_time:15653ms step_avg:57.55ms
step:273/2330 train_time:15709ms step_avg:57.54ms
step:274/2330 train_time:15769ms step_avg:57.55ms
step:275/2330 train_time:15825ms step_avg:57.55ms
step:276/2330 train_time:15885ms step_avg:57.56ms
step:277/2330 train_time:15941ms step_avg:57.55ms
step:278/2330 train_time:16000ms step_avg:57.55ms
step:279/2330 train_time:16056ms step_avg:57.55ms
step:280/2330 train_time:16114ms step_avg:57.55ms
step:281/2330 train_time:16169ms step_avg:57.54ms
step:282/2330 train_time:16228ms step_avg:57.55ms
step:283/2330 train_time:16284ms step_avg:57.54ms
step:284/2330 train_time:16343ms step_avg:57.55ms
step:285/2330 train_time:16400ms step_avg:57.54ms
step:286/2330 train_time:16458ms step_avg:57.55ms
step:287/2330 train_time:16515ms step_avg:57.54ms
step:288/2330 train_time:16574ms step_avg:57.55ms
step:289/2330 train_time:16631ms step_avg:57.55ms
step:290/2330 train_time:16690ms step_avg:57.55ms
step:291/2330 train_time:16746ms step_avg:57.55ms
step:292/2330 train_time:16805ms step_avg:57.55ms
step:293/2330 train_time:16861ms step_avg:57.55ms
step:294/2330 train_time:16922ms step_avg:57.56ms
step:295/2330 train_time:16978ms step_avg:57.55ms
step:296/2330 train_time:17037ms step_avg:57.56ms
step:297/2330 train_time:17093ms step_avg:57.55ms
step:298/2330 train_time:17151ms step_avg:57.55ms
step:299/2330 train_time:17206ms step_avg:57.55ms
step:300/2330 train_time:17265ms step_avg:57.55ms
step:301/2330 train_time:17322ms step_avg:57.55ms
step:302/2330 train_time:17381ms step_avg:57.55ms
step:303/2330 train_time:17437ms step_avg:57.55ms
step:304/2330 train_time:17497ms step_avg:57.56ms
step:305/2330 train_time:17553ms step_avg:57.55ms
step:306/2330 train_time:17612ms step_avg:57.56ms
step:307/2330 train_time:17668ms step_avg:57.55ms
step:308/2330 train_time:17729ms step_avg:57.56ms
step:309/2330 train_time:17785ms step_avg:57.56ms
step:310/2330 train_time:17846ms step_avg:57.57ms
step:311/2330 train_time:17901ms step_avg:57.56ms
step:312/2330 train_time:17961ms step_avg:57.57ms
step:313/2330 train_time:18017ms step_avg:57.56ms
step:314/2330 train_time:18076ms step_avg:57.57ms
step:315/2330 train_time:18132ms step_avg:57.56ms
step:316/2330 train_time:18191ms step_avg:57.57ms
step:317/2330 train_time:18247ms step_avg:57.56ms
step:318/2330 train_time:18307ms step_avg:57.57ms
step:319/2330 train_time:18363ms step_avg:57.56ms
step:320/2330 train_time:18422ms step_avg:57.57ms
step:321/2330 train_time:18478ms step_avg:57.57ms
step:322/2330 train_time:18538ms step_avg:57.57ms
step:323/2330 train_time:18594ms step_avg:57.57ms
step:324/2330 train_time:18652ms step_avg:57.57ms
step:325/2330 train_time:18708ms step_avg:57.56ms
step:326/2330 train_time:18767ms step_avg:57.57ms
step:327/2330 train_time:18824ms step_avg:57.57ms
step:328/2330 train_time:18884ms step_avg:57.57ms
step:329/2330 train_time:18940ms step_avg:57.57ms
step:330/2330 train_time:19000ms step_avg:57.58ms
step:331/2330 train_time:19056ms step_avg:57.57ms
step:332/2330 train_time:19114ms step_avg:57.57ms
step:333/2330 train_time:19170ms step_avg:57.57ms
step:334/2330 train_time:19228ms step_avg:57.57ms
step:335/2330 train_time:19284ms step_avg:57.56ms
step:336/2330 train_time:19343ms step_avg:57.57ms
step:337/2330 train_time:19399ms step_avg:57.56ms
step:338/2330 train_time:19459ms step_avg:57.57ms
step:339/2330 train_time:19515ms step_avg:57.57ms
step:340/2330 train_time:19575ms step_avg:57.57ms
step:341/2330 train_time:19631ms step_avg:57.57ms
step:342/2330 train_time:19690ms step_avg:57.57ms
step:343/2330 train_time:19746ms step_avg:57.57ms
step:344/2330 train_time:19804ms step_avg:57.57ms
step:345/2330 train_time:19860ms step_avg:57.57ms
step:346/2330 train_time:19920ms step_avg:57.57ms
step:347/2330 train_time:19976ms step_avg:57.57ms
step:348/2330 train_time:20036ms step_avg:57.58ms
step:349/2330 train_time:20092ms step_avg:57.57ms
step:350/2330 train_time:20151ms step_avg:57.57ms
step:351/2330 train_time:20206ms step_avg:57.57ms
step:352/2330 train_time:20266ms step_avg:57.57ms
step:353/2330 train_time:20322ms step_avg:57.57ms
step:354/2330 train_time:20382ms step_avg:57.58ms
step:355/2330 train_time:20438ms step_avg:57.57ms
step:356/2330 train_time:20497ms step_avg:57.58ms
step:357/2330 train_time:20554ms step_avg:57.57ms
step:358/2330 train_time:20613ms step_avg:57.58ms
step:359/2330 train_time:20668ms step_avg:57.57ms
step:360/2330 train_time:20728ms step_avg:57.58ms
step:361/2330 train_time:20783ms step_avg:57.57ms
step:362/2330 train_time:20843ms step_avg:57.58ms
step:363/2330 train_time:20899ms step_avg:57.57ms
step:364/2330 train_time:20960ms step_avg:57.58ms
step:365/2330 train_time:21015ms step_avg:57.58ms
step:366/2330 train_time:21076ms step_avg:57.58ms
step:367/2330 train_time:21132ms step_avg:57.58ms
step:368/2330 train_time:21191ms step_avg:57.58ms
step:369/2330 train_time:21247ms step_avg:57.58ms
step:370/2330 train_time:21306ms step_avg:57.58ms
step:371/2330 train_time:21362ms step_avg:57.58ms
step:372/2330 train_time:21421ms step_avg:57.58ms
step:373/2330 train_time:21477ms step_avg:57.58ms
step:374/2330 train_time:21536ms step_avg:57.58ms
step:375/2330 train_time:21593ms step_avg:57.58ms
step:376/2330 train_time:21651ms step_avg:57.58ms
step:377/2330 train_time:21708ms step_avg:57.58ms
step:378/2330 train_time:21767ms step_avg:57.58ms
step:379/2330 train_time:21824ms step_avg:57.58ms
step:380/2330 train_time:21883ms step_avg:57.59ms
step:381/2330 train_time:21939ms step_avg:57.58ms
step:382/2330 train_time:21999ms step_avg:57.59ms
step:383/2330 train_time:22054ms step_avg:57.58ms
step:384/2330 train_time:22114ms step_avg:57.59ms
step:385/2330 train_time:22169ms step_avg:57.58ms
step:386/2330 train_time:22229ms step_avg:57.59ms
step:387/2330 train_time:22285ms step_avg:57.59ms
step:388/2330 train_time:22344ms step_avg:57.59ms
step:389/2330 train_time:22400ms step_avg:57.58ms
step:390/2330 train_time:22459ms step_avg:57.59ms
step:391/2330 train_time:22515ms step_avg:57.58ms
step:392/2330 train_time:22575ms step_avg:57.59ms
step:393/2330 train_time:22631ms step_avg:57.58ms
step:394/2330 train_time:22690ms step_avg:57.59ms
step:395/2330 train_time:22746ms step_avg:57.58ms
step:396/2330 train_time:22806ms step_avg:57.59ms
step:397/2330 train_time:22862ms step_avg:57.59ms
step:398/2330 train_time:22922ms step_avg:57.59ms
step:399/2330 train_time:22979ms step_avg:57.59ms
step:400/2330 train_time:23037ms step_avg:57.59ms
step:401/2330 train_time:23094ms step_avg:57.59ms
step:402/2330 train_time:23153ms step_avg:57.59ms
step:403/2330 train_time:23209ms step_avg:57.59ms
step:404/2330 train_time:23269ms step_avg:57.60ms
step:405/2330 train_time:23325ms step_avg:57.59ms
step:406/2330 train_time:23384ms step_avg:57.60ms
step:407/2330 train_time:23441ms step_avg:57.59ms
step:408/2330 train_time:23501ms step_avg:57.60ms
step:409/2330 train_time:23557ms step_avg:57.60ms
step:410/2330 train_time:23616ms step_avg:57.60ms
step:411/2330 train_time:23672ms step_avg:57.60ms
step:412/2330 train_time:23731ms step_avg:57.60ms
step:413/2330 train_time:23787ms step_avg:57.60ms
step:414/2330 train_time:23848ms step_avg:57.60ms
step:415/2330 train_time:23904ms step_avg:57.60ms
step:416/2330 train_time:23964ms step_avg:57.61ms
step:417/2330 train_time:24020ms step_avg:57.60ms
step:418/2330 train_time:24079ms step_avg:57.61ms
step:419/2330 train_time:24135ms step_avg:57.60ms
step:420/2330 train_time:24195ms step_avg:57.61ms
step:421/2330 train_time:24250ms step_avg:57.60ms
step:422/2330 train_time:24309ms step_avg:57.60ms
step:423/2330 train_time:24365ms step_avg:57.60ms
step:424/2330 train_time:24425ms step_avg:57.61ms
step:425/2330 train_time:24481ms step_avg:57.60ms
step:426/2330 train_time:24540ms step_avg:57.61ms
step:427/2330 train_time:24597ms step_avg:57.60ms
step:428/2330 train_time:24656ms step_avg:57.61ms
step:429/2330 train_time:24711ms step_avg:57.60ms
step:430/2330 train_time:24770ms step_avg:57.61ms
step:431/2330 train_time:24827ms step_avg:57.60ms
step:432/2330 train_time:24885ms step_avg:57.61ms
step:433/2330 train_time:24942ms step_avg:57.60ms
step:434/2330 train_time:25002ms step_avg:57.61ms
step:435/2330 train_time:25057ms step_avg:57.60ms
step:436/2330 train_time:25117ms step_avg:57.61ms
step:437/2330 train_time:25173ms step_avg:57.60ms
step:438/2330 train_time:25231ms step_avg:57.60ms
step:439/2330 train_time:25287ms step_avg:57.60ms
step:440/2330 train_time:25346ms step_avg:57.60ms
step:441/2330 train_time:25403ms step_avg:57.60ms
step:442/2330 train_time:25462ms step_avg:57.61ms
step:443/2330 train_time:25517ms step_avg:57.60ms
step:444/2330 train_time:25576ms step_avg:57.60ms
step:445/2330 train_time:25632ms step_avg:57.60ms
step:446/2330 train_time:25691ms step_avg:57.60ms
step:447/2330 train_time:25747ms step_avg:57.60ms
step:448/2330 train_time:25807ms step_avg:57.61ms
step:449/2330 train_time:25863ms step_avg:57.60ms
step:450/2330 train_time:25924ms step_avg:57.61ms
step:451/2330 train_time:25981ms step_avg:57.61ms
step:452/2330 train_time:26040ms step_avg:57.61ms
step:453/2330 train_time:26096ms step_avg:57.61ms
step:454/2330 train_time:26156ms step_avg:57.61ms
step:455/2330 train_time:26212ms step_avg:57.61ms
step:456/2330 train_time:26271ms step_avg:57.61ms
step:457/2330 train_time:26327ms step_avg:57.61ms
step:458/2330 train_time:26386ms step_avg:57.61ms
step:459/2330 train_time:26443ms step_avg:57.61ms
step:460/2330 train_time:26502ms step_avg:57.61ms
step:461/2330 train_time:26558ms step_avg:57.61ms
step:462/2330 train_time:26617ms step_avg:57.61ms
step:463/2330 train_time:26673ms step_avg:57.61ms
step:464/2330 train_time:26732ms step_avg:57.61ms
step:465/2330 train_time:26787ms step_avg:57.61ms
step:466/2330 train_time:26848ms step_avg:57.61ms
step:467/2330 train_time:26904ms step_avg:57.61ms
step:468/2330 train_time:26964ms step_avg:57.62ms
step:469/2330 train_time:27020ms step_avg:57.61ms
step:470/2330 train_time:27080ms step_avg:57.62ms
step:471/2330 train_time:27137ms step_avg:57.62ms
step:472/2330 train_time:27196ms step_avg:57.62ms
step:473/2330 train_time:27252ms step_avg:57.62ms
step:474/2330 train_time:27311ms step_avg:57.62ms
step:475/2330 train_time:27367ms step_avg:57.61ms
step:476/2330 train_time:27426ms step_avg:57.62ms
step:477/2330 train_time:27483ms step_avg:57.62ms
step:478/2330 train_time:27543ms step_avg:57.62ms
step:479/2330 train_time:27600ms step_avg:57.62ms
step:480/2330 train_time:27658ms step_avg:57.62ms
step:481/2330 train_time:27714ms step_avg:57.62ms
step:482/2330 train_time:27772ms step_avg:57.62ms
step:483/2330 train_time:27828ms step_avg:57.62ms
step:484/2330 train_time:27888ms step_avg:57.62ms
step:485/2330 train_time:27945ms step_avg:57.62ms
step:486/2330 train_time:28004ms step_avg:57.62ms
step:487/2330 train_time:28060ms step_avg:57.62ms
step:488/2330 train_time:28119ms step_avg:57.62ms
step:489/2330 train_time:28175ms step_avg:57.62ms
step:490/2330 train_time:28235ms step_avg:57.62ms
step:491/2330 train_time:28291ms step_avg:57.62ms
step:492/2330 train_time:28350ms step_avg:57.62ms
step:493/2330 train_time:28406ms step_avg:57.62ms
step:494/2330 train_time:28465ms step_avg:57.62ms
step:495/2330 train_time:28521ms step_avg:57.62ms
step:496/2330 train_time:28581ms step_avg:57.62ms
step:497/2330 train_time:28637ms step_avg:57.62ms
step:498/2330 train_time:28697ms step_avg:57.62ms
step:499/2330 train_time:28753ms step_avg:57.62ms
step:500/2330 train_time:28811ms step_avg:57.62ms
step:500/2330 val_loss:4.4176 train_time:28891ms step_avg:57.78ms
step:501/2330 train_time:28909ms step_avg:57.70ms
step:502/2330 train_time:28929ms step_avg:57.63ms
step:503/2330 train_time:28985ms step_avg:57.62ms
step:504/2330 train_time:29049ms step_avg:57.64ms
step:505/2330 train_time:29106ms step_avg:57.63ms
step:506/2330 train_time:29166ms step_avg:57.64ms
step:507/2330 train_time:29222ms step_avg:57.64ms
step:508/2330 train_time:29282ms step_avg:57.64ms
step:509/2330 train_time:29339ms step_avg:57.64ms
step:510/2330 train_time:29397ms step_avg:57.64ms
step:511/2330 train_time:29452ms step_avg:57.64ms
step:512/2330 train_time:29511ms step_avg:57.64ms
step:513/2330 train_time:29566ms step_avg:57.63ms
step:514/2330 train_time:29625ms step_avg:57.64ms
step:515/2330 train_time:29681ms step_avg:57.63ms
step:516/2330 train_time:29740ms step_avg:57.64ms
step:517/2330 train_time:29796ms step_avg:57.63ms
step:518/2330 train_time:29857ms step_avg:57.64ms
step:519/2330 train_time:29913ms step_avg:57.64ms
step:520/2330 train_time:29974ms step_avg:57.64ms
step:521/2330 train_time:30030ms step_avg:57.64ms
step:522/2330 train_time:30091ms step_avg:57.65ms
step:523/2330 train_time:30146ms step_avg:57.64ms
step:524/2330 train_time:30207ms step_avg:57.65ms
step:525/2330 train_time:30263ms step_avg:57.64ms
step:526/2330 train_time:30323ms step_avg:57.65ms
step:527/2330 train_time:30380ms step_avg:57.65ms
step:528/2330 train_time:30439ms step_avg:57.65ms
step:529/2330 train_time:30494ms step_avg:57.65ms
step:530/2330 train_time:30553ms step_avg:57.65ms
step:531/2330 train_time:30609ms step_avg:57.64ms
step:532/2330 train_time:30668ms step_avg:57.65ms
step:533/2330 train_time:30725ms step_avg:57.65ms
step:534/2330 train_time:30784ms step_avg:57.65ms
step:535/2330 train_time:30841ms step_avg:57.65ms
step:536/2330 train_time:30900ms step_avg:57.65ms
step:537/2330 train_time:30957ms step_avg:57.65ms
step:538/2330 train_time:31016ms step_avg:57.65ms
step:539/2330 train_time:31072ms step_avg:57.65ms
step:540/2330 train_time:31133ms step_avg:57.65ms
step:541/2330 train_time:31189ms step_avg:57.65ms
step:542/2330 train_time:31248ms step_avg:57.65ms
step:543/2330 train_time:31303ms step_avg:57.65ms
step:544/2330 train_time:31364ms step_avg:57.65ms
step:545/2330 train_time:31421ms step_avg:57.65ms
step:546/2330 train_time:31479ms step_avg:57.65ms
step:547/2330 train_time:31535ms step_avg:57.65ms
step:548/2330 train_time:31594ms step_avg:57.65ms
step:549/2330 train_time:31650ms step_avg:57.65ms
step:550/2330 train_time:31708ms step_avg:57.65ms
step:551/2330 train_time:31765ms step_avg:57.65ms
step:552/2330 train_time:31824ms step_avg:57.65ms
step:553/2330 train_time:31880ms step_avg:57.65ms
step:554/2330 train_time:31940ms step_avg:57.65ms
step:555/2330 train_time:31996ms step_avg:57.65ms
step:556/2330 train_time:32056ms step_avg:57.65ms
step:557/2330 train_time:32112ms step_avg:57.65ms
step:558/2330 train_time:32172ms step_avg:57.66ms
step:559/2330 train_time:32228ms step_avg:57.65ms
step:560/2330 train_time:32287ms step_avg:57.66ms
step:561/2330 train_time:32343ms step_avg:57.65ms
step:562/2330 train_time:32403ms step_avg:57.66ms
step:563/2330 train_time:32459ms step_avg:57.65ms
step:564/2330 train_time:32518ms step_avg:57.66ms
step:565/2330 train_time:32574ms step_avg:57.65ms
step:566/2330 train_time:32633ms step_avg:57.66ms
step:567/2330 train_time:32689ms step_avg:57.65ms
step:568/2330 train_time:32748ms step_avg:57.66ms
step:569/2330 train_time:32804ms step_avg:57.65ms
step:570/2330 train_time:32863ms step_avg:57.65ms
step:571/2330 train_time:32920ms step_avg:57.65ms
step:572/2330 train_time:32980ms step_avg:57.66ms
step:573/2330 train_time:33037ms step_avg:57.66ms
step:574/2330 train_time:33096ms step_avg:57.66ms
step:575/2330 train_time:33152ms step_avg:57.66ms
step:576/2330 train_time:33212ms step_avg:57.66ms
step:577/2330 train_time:33268ms step_avg:57.66ms
step:578/2330 train_time:33327ms step_avg:57.66ms
step:579/2330 train_time:33383ms step_avg:57.66ms
step:580/2330 train_time:33442ms step_avg:57.66ms
step:581/2330 train_time:33498ms step_avg:57.66ms
step:582/2330 train_time:33558ms step_avg:57.66ms
step:583/2330 train_time:33614ms step_avg:57.66ms
step:584/2330 train_time:33673ms step_avg:57.66ms
step:585/2330 train_time:33729ms step_avg:57.66ms
step:586/2330 train_time:33788ms step_avg:57.66ms
step:587/2330 train_time:33845ms step_avg:57.66ms
step:588/2330 train_time:33904ms step_avg:57.66ms
step:589/2330 train_time:33961ms step_avg:57.66ms
step:590/2330 train_time:34020ms step_avg:57.66ms
step:591/2330 train_time:34077ms step_avg:57.66ms
step:592/2330 train_time:34136ms step_avg:57.66ms
step:593/2330 train_time:34193ms step_avg:57.66ms
step:594/2330 train_time:34252ms step_avg:57.66ms
step:595/2330 train_time:34308ms step_avg:57.66ms
step:596/2330 train_time:34367ms step_avg:57.66ms
step:597/2330 train_time:34423ms step_avg:57.66ms
step:598/2330 train_time:34482ms step_avg:57.66ms
step:599/2330 train_time:34538ms step_avg:57.66ms
step:600/2330 train_time:34598ms step_avg:57.66ms
step:601/2330 train_time:34654ms step_avg:57.66ms
step:602/2330 train_time:34713ms step_avg:57.66ms
step:603/2330 train_time:34770ms step_avg:57.66ms
step:604/2330 train_time:34828ms step_avg:57.66ms
step:605/2330 train_time:34885ms step_avg:57.66ms
step:606/2330 train_time:34944ms step_avg:57.66ms
step:607/2330 train_time:35000ms step_avg:57.66ms
step:608/2330 train_time:35060ms step_avg:57.66ms
step:609/2330 train_time:35117ms step_avg:57.66ms
step:610/2330 train_time:35176ms step_avg:57.67ms
step:611/2330 train_time:35232ms step_avg:57.66ms
step:612/2330 train_time:35292ms step_avg:57.67ms
step:613/2330 train_time:35347ms step_avg:57.66ms
step:614/2330 train_time:35406ms step_avg:57.66ms
step:615/2330 train_time:35462ms step_avg:57.66ms
step:616/2330 train_time:35522ms step_avg:57.67ms
step:617/2330 train_time:35579ms step_avg:57.66ms
step:618/2330 train_time:35637ms step_avg:57.67ms
step:619/2330 train_time:35693ms step_avg:57.66ms
step:620/2330 train_time:35753ms step_avg:57.67ms
step:621/2330 train_time:35809ms step_avg:57.66ms
step:622/2330 train_time:35868ms step_avg:57.67ms
step:623/2330 train_time:35924ms step_avg:57.66ms
step:624/2330 train_time:35983ms step_avg:57.66ms
step:625/2330 train_time:36040ms step_avg:57.66ms
step:626/2330 train_time:36100ms step_avg:57.67ms
step:627/2330 train_time:36157ms step_avg:57.67ms
step:628/2330 train_time:36216ms step_avg:57.67ms
step:629/2330 train_time:36273ms step_avg:57.67ms
step:630/2330 train_time:36331ms step_avg:57.67ms
step:631/2330 train_time:36387ms step_avg:57.67ms
step:632/2330 train_time:36446ms step_avg:57.67ms
step:633/2330 train_time:36503ms step_avg:57.67ms
step:634/2330 train_time:36564ms step_avg:57.67ms
step:635/2330 train_time:36620ms step_avg:57.67ms
step:636/2330 train_time:36679ms step_avg:57.67ms
step:637/2330 train_time:36736ms step_avg:57.67ms
step:638/2330 train_time:36795ms step_avg:57.67ms
step:639/2330 train_time:36850ms step_avg:57.67ms
step:640/2330 train_time:36909ms step_avg:57.67ms
step:641/2330 train_time:36965ms step_avg:57.67ms
step:642/2330 train_time:37026ms step_avg:57.67ms
step:643/2330 train_time:37082ms step_avg:57.67ms
step:644/2330 train_time:37142ms step_avg:57.67ms
step:645/2330 train_time:37199ms step_avg:57.67ms
step:646/2330 train_time:37258ms step_avg:57.67ms
step:647/2330 train_time:37314ms step_avg:57.67ms
step:648/2330 train_time:37373ms step_avg:57.67ms
step:649/2330 train_time:37429ms step_avg:57.67ms
step:650/2330 train_time:37489ms step_avg:57.67ms
step:651/2330 train_time:37545ms step_avg:57.67ms
step:652/2330 train_time:37604ms step_avg:57.68ms
step:653/2330 train_time:37661ms step_avg:57.67ms
step:654/2330 train_time:37721ms step_avg:57.68ms
step:655/2330 train_time:37777ms step_avg:57.68ms
step:656/2330 train_time:37836ms step_avg:57.68ms
step:657/2330 train_time:37892ms step_avg:57.67ms
step:658/2330 train_time:37950ms step_avg:57.68ms
step:659/2330 train_time:38007ms step_avg:57.67ms
step:660/2330 train_time:38066ms step_avg:57.68ms
step:661/2330 train_time:38123ms step_avg:57.67ms
step:662/2330 train_time:38183ms step_avg:57.68ms
step:663/2330 train_time:38239ms step_avg:57.68ms
step:664/2330 train_time:38299ms step_avg:57.68ms
step:665/2330 train_time:38355ms step_avg:57.68ms
step:666/2330 train_time:38415ms step_avg:57.68ms
step:667/2330 train_time:38471ms step_avg:57.68ms
step:668/2330 train_time:38529ms step_avg:57.68ms
step:669/2330 train_time:38585ms step_avg:57.68ms
step:670/2330 train_time:38645ms step_avg:57.68ms
step:671/2330 train_time:38702ms step_avg:57.68ms
step:672/2330 train_time:38760ms step_avg:57.68ms
step:673/2330 train_time:38817ms step_avg:57.68ms
step:674/2330 train_time:38876ms step_avg:57.68ms
step:675/2330 train_time:38932ms step_avg:57.68ms
step:676/2330 train_time:38992ms step_avg:57.68ms
step:677/2330 train_time:39047ms step_avg:57.68ms
step:678/2330 train_time:39107ms step_avg:57.68ms
step:679/2330 train_time:39163ms step_avg:57.68ms
step:680/2330 train_time:39223ms step_avg:57.68ms
step:681/2330 train_time:39279ms step_avg:57.68ms
step:682/2330 train_time:39338ms step_avg:57.68ms
step:683/2330 train_time:39394ms step_avg:57.68ms
step:684/2330 train_time:39454ms step_avg:57.68ms
step:685/2330 train_time:39511ms step_avg:57.68ms
step:686/2330 train_time:39569ms step_avg:57.68ms
step:687/2330 train_time:39625ms step_avg:57.68ms
step:688/2330 train_time:39684ms step_avg:57.68ms
step:689/2330 train_time:39740ms step_avg:57.68ms
step:690/2330 train_time:39800ms step_avg:57.68ms
step:691/2330 train_time:39856ms step_avg:57.68ms
step:692/2330 train_time:39916ms step_avg:57.68ms
step:693/2330 train_time:39972ms step_avg:57.68ms
step:694/2330 train_time:40032ms step_avg:57.68ms
step:695/2330 train_time:40088ms step_avg:57.68ms
step:696/2330 train_time:40147ms step_avg:57.68ms
step:697/2330 train_time:40204ms step_avg:57.68ms
step:698/2330 train_time:40264ms step_avg:57.68ms
step:699/2330 train_time:40320ms step_avg:57.68ms
step:700/2330 train_time:40380ms step_avg:57.69ms
step:701/2330 train_time:40437ms step_avg:57.68ms
step:702/2330 train_time:40496ms step_avg:57.69ms
step:703/2330 train_time:40551ms step_avg:57.68ms
step:704/2330 train_time:40611ms step_avg:57.69ms
step:705/2330 train_time:40666ms step_avg:57.68ms
step:706/2330 train_time:40725ms step_avg:57.68ms
step:707/2330 train_time:40782ms step_avg:57.68ms
step:708/2330 train_time:40842ms step_avg:57.69ms
step:709/2330 train_time:40899ms step_avg:57.69ms
step:710/2330 train_time:40959ms step_avg:57.69ms
step:711/2330 train_time:41015ms step_avg:57.69ms
step:712/2330 train_time:41074ms step_avg:57.69ms
step:713/2330 train_time:41130ms step_avg:57.69ms
step:714/2330 train_time:41189ms step_avg:57.69ms
step:715/2330 train_time:41245ms step_avg:57.69ms
step:716/2330 train_time:41304ms step_avg:57.69ms
step:717/2330 train_time:41360ms step_avg:57.68ms
step:718/2330 train_time:41420ms step_avg:57.69ms
step:719/2330 train_time:41476ms step_avg:57.69ms
step:720/2330 train_time:41534ms step_avg:57.69ms
step:721/2330 train_time:41591ms step_avg:57.68ms
step:722/2330 train_time:41649ms step_avg:57.69ms
step:723/2330 train_time:41706ms step_avg:57.68ms
step:724/2330 train_time:41765ms step_avg:57.69ms
step:725/2330 train_time:41821ms step_avg:57.68ms
step:726/2330 train_time:41881ms step_avg:57.69ms
step:727/2330 train_time:41938ms step_avg:57.69ms
step:728/2330 train_time:41997ms step_avg:57.69ms
step:729/2330 train_time:42052ms step_avg:57.68ms
step:730/2330 train_time:42111ms step_avg:57.69ms
step:731/2330 train_time:42167ms step_avg:57.68ms
step:732/2330 train_time:42226ms step_avg:57.69ms
step:733/2330 train_time:42283ms step_avg:57.68ms
step:734/2330 train_time:42342ms step_avg:57.69ms
step:735/2330 train_time:42399ms step_avg:57.69ms
step:736/2330 train_time:42458ms step_avg:57.69ms
step:737/2330 train_time:42514ms step_avg:57.68ms
step:738/2330 train_time:42572ms step_avg:57.69ms
step:739/2330 train_time:42628ms step_avg:57.68ms
step:740/2330 train_time:42687ms step_avg:57.69ms
step:741/2330 train_time:42743ms step_avg:57.68ms
step:742/2330 train_time:42803ms step_avg:57.69ms
step:743/2330 train_time:42860ms step_avg:57.68ms
step:744/2330 train_time:42919ms step_avg:57.69ms
step:745/2330 train_time:42976ms step_avg:57.69ms
step:746/2330 train_time:43035ms step_avg:57.69ms
step:747/2330 train_time:43092ms step_avg:57.69ms
step:748/2330 train_time:43150ms step_avg:57.69ms
step:749/2330 train_time:43206ms step_avg:57.68ms
step:750/2330 train_time:43265ms step_avg:57.69ms
step:750/2330 val_loss:4.2145 train_time:43345ms step_avg:57.79ms
step:751/2330 train_time:43363ms step_avg:57.74ms
step:752/2330 train_time:43384ms step_avg:57.69ms
step:753/2330 train_time:43440ms step_avg:57.69ms
step:754/2330 train_time:43503ms step_avg:57.70ms
step:755/2330 train_time:43559ms step_avg:57.69ms
step:756/2330 train_time:43621ms step_avg:57.70ms
step:757/2330 train_time:43677ms step_avg:57.70ms
step:758/2330 train_time:43737ms step_avg:57.70ms
step:759/2330 train_time:43792ms step_avg:57.70ms
step:760/2330 train_time:43851ms step_avg:57.70ms
step:761/2330 train_time:43907ms step_avg:57.70ms
step:762/2330 train_time:43965ms step_avg:57.70ms
step:763/2330 train_time:44021ms step_avg:57.69ms
step:764/2330 train_time:44080ms step_avg:57.70ms
step:765/2330 train_time:44137ms step_avg:57.70ms
step:766/2330 train_time:44195ms step_avg:57.70ms
step:767/2330 train_time:44251ms step_avg:57.69ms
step:768/2330 train_time:44311ms step_avg:57.70ms
step:769/2330 train_time:44370ms step_avg:57.70ms
step:770/2330 train_time:44431ms step_avg:57.70ms
step:771/2330 train_time:44490ms step_avg:57.70ms
step:772/2330 train_time:44551ms step_avg:57.71ms
step:773/2330 train_time:44608ms step_avg:57.71ms
step:774/2330 train_time:44668ms step_avg:57.71ms
step:775/2330 train_time:44725ms step_avg:57.71ms
step:776/2330 train_time:44786ms step_avg:57.71ms
step:777/2330 train_time:44843ms step_avg:57.71ms
step:778/2330 train_time:44902ms step_avg:57.72ms
step:779/2330 train_time:44959ms step_avg:57.71ms
step:780/2330 train_time:45018ms step_avg:57.72ms
step:781/2330 train_time:45075ms step_avg:57.71ms
step:782/2330 train_time:45134ms step_avg:57.72ms
step:783/2330 train_time:45190ms step_avg:57.71ms
step:784/2330 train_time:45250ms step_avg:57.72ms
step:785/2330 train_time:45307ms step_avg:57.72ms
step:786/2330 train_time:45368ms step_avg:57.72ms
step:787/2330 train_time:45425ms step_avg:57.72ms
step:788/2330 train_time:45487ms step_avg:57.72ms
step:789/2330 train_time:45544ms step_avg:57.72ms
step:790/2330 train_time:45604ms step_avg:57.73ms
step:791/2330 train_time:45662ms step_avg:57.73ms
step:792/2330 train_time:45722ms step_avg:57.73ms
step:793/2330 train_time:45779ms step_avg:57.73ms
step:794/2330 train_time:45839ms step_avg:57.73ms
step:795/2330 train_time:45896ms step_avg:57.73ms
step:796/2330 train_time:45955ms step_avg:57.73ms
step:797/2330 train_time:46011ms step_avg:57.73ms
step:798/2330 train_time:46072ms step_avg:57.73ms
step:799/2330 train_time:46129ms step_avg:57.73ms
step:800/2330 train_time:46188ms step_avg:57.74ms
step:801/2330 train_time:46245ms step_avg:57.73ms
step:802/2330 train_time:46306ms step_avg:57.74ms
step:803/2330 train_time:46363ms step_avg:57.74ms
step:804/2330 train_time:46423ms step_avg:57.74ms
step:805/2330 train_time:46480ms step_avg:57.74ms
step:806/2330 train_time:46541ms step_avg:57.74ms
step:807/2330 train_time:46598ms step_avg:57.74ms
step:808/2330 train_time:46658ms step_avg:57.74ms
step:809/2330 train_time:46715ms step_avg:57.74ms
step:810/2330 train_time:46775ms step_avg:57.75ms
step:811/2330 train_time:46832ms step_avg:57.75ms
step:812/2330 train_time:46892ms step_avg:57.75ms
step:813/2330 train_time:46949ms step_avg:57.75ms
step:814/2330 train_time:47008ms step_avg:57.75ms
step:815/2330 train_time:47066ms step_avg:57.75ms
step:816/2330 train_time:47126ms step_avg:57.75ms
step:817/2330 train_time:47182ms step_avg:57.75ms
step:818/2330 train_time:47242ms step_avg:57.75ms
step:819/2330 train_time:47299ms step_avg:57.75ms
step:820/2330 train_time:47358ms step_avg:57.75ms
step:821/2330 train_time:47415ms step_avg:57.75ms
step:822/2330 train_time:47475ms step_avg:57.76ms
step:823/2330 train_time:47532ms step_avg:57.75ms
step:824/2330 train_time:47593ms step_avg:57.76ms
step:825/2330 train_time:47651ms step_avg:57.76ms
step:826/2330 train_time:47711ms step_avg:57.76ms
step:827/2330 train_time:47769ms step_avg:57.76ms
step:828/2330 train_time:47829ms step_avg:57.76ms
step:829/2330 train_time:47886ms step_avg:57.76ms
step:830/2330 train_time:47946ms step_avg:57.77ms
step:831/2330 train_time:48003ms step_avg:57.77ms
step:832/2330 train_time:48062ms step_avg:57.77ms
step:833/2330 train_time:48119ms step_avg:57.77ms
step:834/2330 train_time:48180ms step_avg:57.77ms
step:835/2330 train_time:48237ms step_avg:57.77ms
step:836/2330 train_time:48296ms step_avg:57.77ms
step:837/2330 train_time:48353ms step_avg:57.77ms
step:838/2330 train_time:48413ms step_avg:57.77ms
step:839/2330 train_time:48471ms step_avg:57.77ms
step:840/2330 train_time:48530ms step_avg:57.77ms
step:841/2330 train_time:48588ms step_avg:57.77ms
step:842/2330 train_time:48648ms step_avg:57.78ms
step:843/2330 train_time:48705ms step_avg:57.78ms
step:844/2330 train_time:48765ms step_avg:57.78ms
step:845/2330 train_time:48823ms step_avg:57.78ms
step:846/2330 train_time:48883ms step_avg:57.78ms
step:847/2330 train_time:48940ms step_avg:57.78ms
step:848/2330 train_time:49000ms step_avg:57.78ms
step:849/2330 train_time:49056ms step_avg:57.78ms
step:850/2330 train_time:49118ms step_avg:57.79ms
step:851/2330 train_time:49174ms step_avg:57.78ms
step:852/2330 train_time:49235ms step_avg:57.79ms
step:853/2330 train_time:49291ms step_avg:57.79ms
step:854/2330 train_time:49352ms step_avg:57.79ms
step:855/2330 train_time:49409ms step_avg:57.79ms
step:856/2330 train_time:49469ms step_avg:57.79ms
step:857/2330 train_time:49527ms step_avg:57.79ms
step:858/2330 train_time:49587ms step_avg:57.79ms
step:859/2330 train_time:49645ms step_avg:57.79ms
step:860/2330 train_time:49705ms step_avg:57.80ms
step:861/2330 train_time:49762ms step_avg:57.80ms
step:862/2330 train_time:49822ms step_avg:57.80ms
step:863/2330 train_time:49879ms step_avg:57.80ms
step:864/2330 train_time:49939ms step_avg:57.80ms
step:865/2330 train_time:49996ms step_avg:57.80ms
step:866/2330 train_time:50055ms step_avg:57.80ms
step:867/2330 train_time:50112ms step_avg:57.80ms
step:868/2330 train_time:50172ms step_avg:57.80ms
step:869/2330 train_time:50229ms step_avg:57.80ms
step:870/2330 train_time:50288ms step_avg:57.80ms
step:871/2330 train_time:50345ms step_avg:57.80ms
step:872/2330 train_time:50406ms step_avg:57.80ms
step:873/2330 train_time:50463ms step_avg:57.80ms
step:874/2330 train_time:50523ms step_avg:57.81ms
step:875/2330 train_time:50580ms step_avg:57.81ms
step:876/2330 train_time:50640ms step_avg:57.81ms
step:877/2330 train_time:50697ms step_avg:57.81ms
step:878/2330 train_time:50757ms step_avg:57.81ms
step:879/2330 train_time:50815ms step_avg:57.81ms
step:880/2330 train_time:50875ms step_avg:57.81ms
step:881/2330 train_time:50932ms step_avg:57.81ms
step:882/2330 train_time:50991ms step_avg:57.81ms
step:883/2330 train_time:51048ms step_avg:57.81ms
step:884/2330 train_time:51108ms step_avg:57.81ms
step:885/2330 train_time:51165ms step_avg:57.81ms
step:886/2330 train_time:51226ms step_avg:57.82ms
step:887/2330 train_time:51283ms step_avg:57.82ms
step:888/2330 train_time:51343ms step_avg:57.82ms
step:889/2330 train_time:51399ms step_avg:57.82ms
step:890/2330 train_time:51459ms step_avg:57.82ms
step:891/2330 train_time:51516ms step_avg:57.82ms
step:892/2330 train_time:51577ms step_avg:57.82ms
step:893/2330 train_time:51633ms step_avg:57.82ms
step:894/2330 train_time:51694ms step_avg:57.82ms
step:895/2330 train_time:51751ms step_avg:57.82ms
step:896/2330 train_time:51812ms step_avg:57.83ms
step:897/2330 train_time:51869ms step_avg:57.82ms
step:898/2330 train_time:51930ms step_avg:57.83ms
step:899/2330 train_time:51987ms step_avg:57.83ms
step:900/2330 train_time:52047ms step_avg:57.83ms
step:901/2330 train_time:52105ms step_avg:57.83ms
step:902/2330 train_time:52164ms step_avg:57.83ms
step:903/2330 train_time:52222ms step_avg:57.83ms
step:904/2330 train_time:52281ms step_avg:57.83ms
step:905/2330 train_time:52339ms step_avg:57.83ms
step:906/2330 train_time:52398ms step_avg:57.83ms
step:907/2330 train_time:52455ms step_avg:57.83ms
step:908/2330 train_time:52514ms step_avg:57.84ms
step:909/2330 train_time:52571ms step_avg:57.83ms
step:910/2330 train_time:52631ms step_avg:57.84ms
step:911/2330 train_time:52689ms step_avg:57.84ms
step:912/2330 train_time:52749ms step_avg:57.84ms
step:913/2330 train_time:52807ms step_avg:57.84ms
step:914/2330 train_time:52866ms step_avg:57.84ms
step:915/2330 train_time:52924ms step_avg:57.84ms
step:916/2330 train_time:52984ms step_avg:57.84ms
step:917/2330 train_time:53040ms step_avg:57.84ms
step:918/2330 train_time:53101ms step_avg:57.84ms
step:919/2330 train_time:53158ms step_avg:57.84ms
step:920/2330 train_time:53219ms step_avg:57.85ms
step:921/2330 train_time:53276ms step_avg:57.85ms
step:922/2330 train_time:53335ms step_avg:57.85ms
step:923/2330 train_time:53392ms step_avg:57.85ms
step:924/2330 train_time:53453ms step_avg:57.85ms
step:925/2330 train_time:53510ms step_avg:57.85ms
step:926/2330 train_time:53570ms step_avg:57.85ms
step:927/2330 train_time:53628ms step_avg:57.85ms
step:928/2330 train_time:53688ms step_avg:57.85ms
step:929/2330 train_time:53745ms step_avg:57.85ms
step:930/2330 train_time:53804ms step_avg:57.85ms
step:931/2330 train_time:53861ms step_avg:57.85ms
step:932/2330 train_time:53922ms step_avg:57.86ms
step:933/2330 train_time:53978ms step_avg:57.85ms
step:934/2330 train_time:54039ms step_avg:57.86ms
step:935/2330 train_time:54096ms step_avg:57.86ms
step:936/2330 train_time:54155ms step_avg:57.86ms
step:937/2330 train_time:54213ms step_avg:57.86ms
step:938/2330 train_time:54273ms step_avg:57.86ms
step:939/2330 train_time:54331ms step_avg:57.86ms
step:940/2330 train_time:54391ms step_avg:57.86ms
step:941/2330 train_time:54448ms step_avg:57.86ms
step:942/2330 train_time:54507ms step_avg:57.86ms
step:943/2330 train_time:54565ms step_avg:57.86ms
step:944/2330 train_time:54625ms step_avg:57.87ms
step:945/2330 train_time:54683ms step_avg:57.87ms
step:946/2330 train_time:54742ms step_avg:57.87ms
step:947/2330 train_time:54799ms step_avg:57.87ms
step:948/2330 train_time:54859ms step_avg:57.87ms
step:949/2330 train_time:54916ms step_avg:57.87ms
step:950/2330 train_time:54976ms step_avg:57.87ms
step:951/2330 train_time:55033ms step_avg:57.87ms
step:952/2330 train_time:55093ms step_avg:57.87ms
step:953/2330 train_time:55150ms step_avg:57.87ms
step:954/2330 train_time:55211ms step_avg:57.87ms
step:955/2330 train_time:55268ms step_avg:57.87ms
step:956/2330 train_time:55328ms step_avg:57.87ms
step:957/2330 train_time:55385ms step_avg:57.87ms
step:958/2330 train_time:55445ms step_avg:57.88ms
step:959/2330 train_time:55503ms step_avg:57.88ms
step:960/2330 train_time:55562ms step_avg:57.88ms
step:961/2330 train_time:55618ms step_avg:57.88ms
step:962/2330 train_time:55680ms step_avg:57.88ms
step:963/2330 train_time:55737ms step_avg:57.88ms
step:964/2330 train_time:55797ms step_avg:57.88ms
step:965/2330 train_time:55854ms step_avg:57.88ms
step:966/2330 train_time:55914ms step_avg:57.88ms
step:967/2330 train_time:55972ms step_avg:57.88ms
step:968/2330 train_time:56032ms step_avg:57.88ms
step:969/2330 train_time:56089ms step_avg:57.88ms
step:970/2330 train_time:56149ms step_avg:57.89ms
step:971/2330 train_time:56206ms step_avg:57.88ms
step:972/2330 train_time:56266ms step_avg:57.89ms
step:973/2330 train_time:56322ms step_avg:57.89ms
step:974/2330 train_time:56383ms step_avg:57.89ms
step:975/2330 train_time:56440ms step_avg:57.89ms
step:976/2330 train_time:56499ms step_avg:57.89ms
step:977/2330 train_time:56557ms step_avg:57.89ms
step:978/2330 train_time:56616ms step_avg:57.89ms
step:979/2330 train_time:56673ms step_avg:57.89ms
step:980/2330 train_time:56734ms step_avg:57.89ms
step:981/2330 train_time:56790ms step_avg:57.89ms
step:982/2330 train_time:56850ms step_avg:57.89ms
step:983/2330 train_time:56908ms step_avg:57.89ms
step:984/2330 train_time:56969ms step_avg:57.89ms
step:985/2330 train_time:57026ms step_avg:57.89ms
step:986/2330 train_time:57086ms step_avg:57.90ms
step:987/2330 train_time:57143ms step_avg:57.90ms
step:988/2330 train_time:57203ms step_avg:57.90ms
step:989/2330 train_time:57260ms step_avg:57.90ms
step:990/2330 train_time:57320ms step_avg:57.90ms
step:991/2330 train_time:57378ms step_avg:57.90ms
step:992/2330 train_time:57437ms step_avg:57.90ms
step:993/2330 train_time:57494ms step_avg:57.90ms
step:994/2330 train_time:57554ms step_avg:57.90ms
step:995/2330 train_time:57611ms step_avg:57.90ms
step:996/2330 train_time:57672ms step_avg:57.90ms
step:997/2330 train_time:57729ms step_avg:57.90ms
step:998/2330 train_time:57789ms step_avg:57.90ms
step:999/2330 train_time:57846ms step_avg:57.90ms
step:1000/2330 train_time:57906ms step_avg:57.91ms
step:1000/2330 val_loss:4.0625 train_time:57986ms step_avg:57.99ms
step:1001/2330 train_time:58005ms step_avg:57.95ms
step:1002/2330 train_time:58026ms step_avg:57.91ms
step:1003/2330 train_time:58079ms step_avg:57.91ms
step:1004/2330 train_time:58149ms step_avg:57.92ms
step:1005/2330 train_time:58205ms step_avg:57.92ms
step:1006/2330 train_time:58270ms step_avg:57.92ms
step:1007/2330 train_time:58326ms step_avg:57.92ms
step:1008/2330 train_time:58386ms step_avg:57.92ms
step:1009/2330 train_time:58442ms step_avg:57.92ms
step:1010/2330 train_time:58502ms step_avg:57.92ms
step:1011/2330 train_time:58558ms step_avg:57.92ms
step:1012/2330 train_time:58617ms step_avg:57.92ms
step:1013/2330 train_time:58673ms step_avg:57.92ms
step:1014/2330 train_time:58732ms step_avg:57.92ms
step:1015/2330 train_time:58788ms step_avg:57.92ms
step:1016/2330 train_time:58847ms step_avg:57.92ms
step:1017/2330 train_time:58905ms step_avg:57.92ms
step:1018/2330 train_time:58968ms step_avg:57.93ms
step:1019/2330 train_time:59027ms step_avg:57.93ms
step:1020/2330 train_time:59088ms step_avg:57.93ms
step:1021/2330 train_time:59146ms step_avg:57.93ms
step:1022/2330 train_time:59206ms step_avg:57.93ms
step:1023/2330 train_time:59263ms step_avg:57.93ms
step:1024/2330 train_time:59324ms step_avg:57.93ms
step:1025/2330 train_time:59380ms step_avg:57.93ms
step:1026/2330 train_time:59440ms step_avg:57.93ms
step:1027/2330 train_time:59497ms step_avg:57.93ms
step:1028/2330 train_time:59556ms step_avg:57.93ms
step:1029/2330 train_time:59613ms step_avg:57.93ms
step:1030/2330 train_time:59672ms step_avg:57.93ms
step:1031/2330 train_time:59728ms step_avg:57.93ms
step:1032/2330 train_time:59787ms step_avg:57.93ms
step:1033/2330 train_time:59845ms step_avg:57.93ms
step:1034/2330 train_time:59905ms step_avg:57.94ms
step:1035/2330 train_time:59962ms step_avg:57.93ms
step:1036/2330 train_time:60025ms step_avg:57.94ms
step:1037/2330 train_time:60082ms step_avg:57.94ms
step:1038/2330 train_time:60143ms step_avg:57.94ms
step:1039/2330 train_time:60202ms step_avg:57.94ms
step:1040/2330 train_time:60262ms step_avg:57.94ms
step:1041/2330 train_time:60318ms step_avg:57.94ms
step:1042/2330 train_time:60379ms step_avg:57.95ms
step:1043/2330 train_time:60436ms step_avg:57.94ms
step:1044/2330 train_time:60497ms step_avg:57.95ms
step:1045/2330 train_time:60553ms step_avg:57.95ms
step:1046/2330 train_time:60613ms step_avg:57.95ms
step:1047/2330 train_time:60669ms step_avg:57.95ms
step:1048/2330 train_time:60729ms step_avg:57.95ms
step:1049/2330 train_time:60785ms step_avg:57.95ms
step:1050/2330 train_time:60845ms step_avg:57.95ms
step:1051/2330 train_time:60902ms step_avg:57.95ms
step:1052/2330 train_time:60962ms step_avg:57.95ms
step:1053/2330 train_time:61020ms step_avg:57.95ms
step:1054/2330 train_time:61081ms step_avg:57.95ms
step:1055/2330 train_time:61139ms step_avg:57.95ms
step:1056/2330 train_time:61200ms step_avg:57.95ms
step:1057/2330 train_time:61257ms step_avg:57.95ms
step:1058/2330 train_time:61317ms step_avg:57.96ms
step:1059/2330 train_time:61374ms step_avg:57.95ms
step:1060/2330 train_time:61434ms step_avg:57.96ms
step:1061/2330 train_time:61490ms step_avg:57.96ms
step:1062/2330 train_time:61550ms step_avg:57.96ms
step:1063/2330 train_time:61607ms step_avg:57.96ms
step:1064/2330 train_time:61666ms step_avg:57.96ms
step:1065/2330 train_time:61722ms step_avg:57.96ms
step:1066/2330 train_time:61783ms step_avg:57.96ms
step:1067/2330 train_time:61839ms step_avg:57.96ms
step:1068/2330 train_time:61901ms step_avg:57.96ms
step:1069/2330 train_time:61958ms step_avg:57.96ms
step:1070/2330 train_time:62018ms step_avg:57.96ms
step:1071/2330 train_time:62076ms step_avg:57.96ms
step:1072/2330 train_time:62135ms step_avg:57.96ms
step:1073/2330 train_time:62193ms step_avg:57.96ms
step:1074/2330 train_time:62253ms step_avg:57.96ms
step:1075/2330 train_time:62310ms step_avg:57.96ms
step:1076/2330 train_time:62370ms step_avg:57.96ms
step:1077/2330 train_time:62427ms step_avg:57.96ms
step:1078/2330 train_time:62486ms step_avg:57.97ms
step:1079/2330 train_time:62543ms step_avg:57.96ms
step:1080/2330 train_time:62603ms step_avg:57.97ms
step:1081/2330 train_time:62660ms step_avg:57.96ms
step:1082/2330 train_time:62720ms step_avg:57.97ms
step:1083/2330 train_time:62777ms step_avg:57.97ms
step:1084/2330 train_time:62838ms step_avg:57.97ms
step:1085/2330 train_time:62895ms step_avg:57.97ms
step:1086/2330 train_time:62955ms step_avg:57.97ms
step:1087/2330 train_time:63011ms step_avg:57.97ms
step:1088/2330 train_time:63071ms step_avg:57.97ms
step:1089/2330 train_time:63129ms step_avg:57.97ms
step:1090/2330 train_time:63189ms step_avg:57.97ms
step:1091/2330 train_time:63246ms step_avg:57.97ms
step:1092/2330 train_time:63305ms step_avg:57.97ms
step:1093/2330 train_time:63363ms step_avg:57.97ms
step:1094/2330 train_time:63423ms step_avg:57.97ms
step:1095/2330 train_time:63480ms step_avg:57.97ms
step:1096/2330 train_time:63541ms step_avg:57.98ms
step:1097/2330 train_time:63598ms step_avg:57.97ms
step:1098/2330 train_time:63658ms step_avg:57.98ms
step:1099/2330 train_time:63715ms step_avg:57.98ms
step:1100/2330 train_time:63775ms step_avg:57.98ms
step:1101/2330 train_time:63832ms step_avg:57.98ms
step:1102/2330 train_time:63891ms step_avg:57.98ms
step:1103/2330 train_time:63948ms step_avg:57.98ms
step:1104/2330 train_time:64008ms step_avg:57.98ms
step:1105/2330 train_time:64065ms step_avg:57.98ms
step:1106/2330 train_time:64125ms step_avg:57.98ms
step:1107/2330 train_time:64182ms step_avg:57.98ms
step:1108/2330 train_time:64243ms step_avg:57.98ms
step:1109/2330 train_time:64300ms step_avg:57.98ms
step:1110/2330 train_time:64360ms step_avg:57.98ms
step:1111/2330 train_time:64418ms step_avg:57.98ms
step:1112/2330 train_time:64478ms step_avg:57.98ms
step:1113/2330 train_time:64536ms step_avg:57.98ms
step:1114/2330 train_time:64595ms step_avg:57.99ms
step:1115/2330 train_time:64653ms step_avg:57.98ms
step:1116/2330 train_time:64713ms step_avg:57.99ms
step:1117/2330 train_time:64770ms step_avg:57.99ms
step:1118/2330 train_time:64830ms step_avg:57.99ms
step:1119/2330 train_time:64887ms step_avg:57.99ms
step:1120/2330 train_time:64946ms step_avg:57.99ms
step:1121/2330 train_time:65003ms step_avg:57.99ms
step:1122/2330 train_time:65063ms step_avg:57.99ms
step:1123/2330 train_time:65120ms step_avg:57.99ms
step:1124/2330 train_time:65181ms step_avg:57.99ms
step:1125/2330 train_time:65238ms step_avg:57.99ms
step:1126/2330 train_time:65298ms step_avg:57.99ms
step:1127/2330 train_time:65355ms step_avg:57.99ms
step:1128/2330 train_time:65416ms step_avg:57.99ms
step:1129/2330 train_time:65474ms step_avg:57.99ms
step:1130/2330 train_time:65534ms step_avg:57.99ms
step:1131/2330 train_time:65591ms step_avg:57.99ms
step:1132/2330 train_time:65651ms step_avg:58.00ms
step:1133/2330 train_time:65707ms step_avg:57.99ms
step:1134/2330 train_time:65768ms step_avg:58.00ms
step:1135/2330 train_time:65825ms step_avg:58.00ms
step:1136/2330 train_time:65885ms step_avg:58.00ms
step:1137/2330 train_time:65941ms step_avg:58.00ms
step:1138/2330 train_time:66002ms step_avg:58.00ms
step:1139/2330 train_time:66059ms step_avg:58.00ms
step:1140/2330 train_time:66119ms step_avg:58.00ms
step:1141/2330 train_time:66176ms step_avg:58.00ms
step:1142/2330 train_time:66236ms step_avg:58.00ms
step:1143/2330 train_time:66293ms step_avg:58.00ms
step:1144/2330 train_time:66353ms step_avg:58.00ms
step:1145/2330 train_time:66411ms step_avg:58.00ms
step:1146/2330 train_time:66470ms step_avg:58.00ms
step:1147/2330 train_time:66526ms step_avg:58.00ms
step:1148/2330 train_time:66588ms step_avg:58.00ms
step:1149/2330 train_time:66645ms step_avg:58.00ms
step:1150/2330 train_time:66705ms step_avg:58.00ms
step:1151/2330 train_time:66762ms step_avg:58.00ms
step:1152/2330 train_time:66822ms step_avg:58.01ms
step:1153/2330 train_time:66879ms step_avg:58.00ms
step:1154/2330 train_time:66941ms step_avg:58.01ms
step:1155/2330 train_time:66998ms step_avg:58.01ms
step:1156/2330 train_time:67057ms step_avg:58.01ms
step:1157/2330 train_time:67115ms step_avg:58.01ms
step:1158/2330 train_time:67175ms step_avg:58.01ms
step:1159/2330 train_time:67232ms step_avg:58.01ms
step:1160/2330 train_time:67291ms step_avg:58.01ms
step:1161/2330 train_time:67349ms step_avg:58.01ms
step:1162/2330 train_time:67410ms step_avg:58.01ms
step:1163/2330 train_time:67466ms step_avg:58.01ms
step:1164/2330 train_time:67526ms step_avg:58.01ms
step:1165/2330 train_time:67583ms step_avg:58.01ms
step:1166/2330 train_time:67644ms step_avg:58.01ms
step:1167/2330 train_time:67701ms step_avg:58.01ms
step:1168/2330 train_time:67760ms step_avg:58.01ms
step:1169/2330 train_time:67817ms step_avg:58.01ms
step:1170/2330 train_time:67879ms step_avg:58.02ms
step:1171/2330 train_time:67936ms step_avg:58.02ms
step:1172/2330 train_time:67996ms step_avg:58.02ms
step:1173/2330 train_time:68054ms step_avg:58.02ms
step:1174/2330 train_time:68113ms step_avg:58.02ms
step:1175/2330 train_time:68170ms step_avg:58.02ms
step:1176/2330 train_time:68230ms step_avg:58.02ms
step:1177/2330 train_time:68287ms step_avg:58.02ms
step:1178/2330 train_time:68346ms step_avg:58.02ms
step:1179/2330 train_time:68403ms step_avg:58.02ms
step:1180/2330 train_time:68464ms step_avg:58.02ms
step:1181/2330 train_time:68520ms step_avg:58.02ms
step:1182/2330 train_time:68582ms step_avg:58.02ms
step:1183/2330 train_time:68638ms step_avg:58.02ms
step:1184/2330 train_time:68700ms step_avg:58.02ms
step:1185/2330 train_time:68757ms step_avg:58.02ms
step:1186/2330 train_time:68817ms step_avg:58.02ms
step:1187/2330 train_time:68874ms step_avg:58.02ms
step:1188/2330 train_time:68934ms step_avg:58.03ms
step:1189/2330 train_time:68991ms step_avg:58.02ms
step:1190/2330 train_time:69051ms step_avg:58.03ms
step:1191/2330 train_time:69108ms step_avg:58.02ms
step:1192/2330 train_time:69168ms step_avg:58.03ms
step:1193/2330 train_time:69225ms step_avg:58.03ms
step:1194/2330 train_time:69284ms step_avg:58.03ms
step:1195/2330 train_time:69341ms step_avg:58.03ms
step:1196/2330 train_time:69402ms step_avg:58.03ms
step:1197/2330 train_time:69459ms step_avg:58.03ms
step:1198/2330 train_time:69519ms step_avg:58.03ms
step:1199/2330 train_time:69576ms step_avg:58.03ms
step:1200/2330 train_time:69637ms step_avg:58.03ms
step:1201/2330 train_time:69694ms step_avg:58.03ms
step:1202/2330 train_time:69754ms step_avg:58.03ms
step:1203/2330 train_time:69812ms step_avg:58.03ms
step:1204/2330 train_time:69872ms step_avg:58.03ms
step:1205/2330 train_time:69929ms step_avg:58.03ms
step:1206/2330 train_time:69989ms step_avg:58.03ms
step:1207/2330 train_time:70046ms step_avg:58.03ms
step:1208/2330 train_time:70106ms step_avg:58.03ms
step:1209/2330 train_time:70162ms step_avg:58.03ms
step:1210/2330 train_time:70222ms step_avg:58.03ms
step:1211/2330 train_time:70279ms step_avg:58.03ms
step:1212/2330 train_time:70341ms step_avg:58.04ms
step:1213/2330 train_time:70398ms step_avg:58.04ms
step:1214/2330 train_time:70458ms step_avg:58.04ms
step:1215/2330 train_time:70515ms step_avg:58.04ms
step:1216/2330 train_time:70575ms step_avg:58.04ms
step:1217/2330 train_time:70632ms step_avg:58.04ms
step:1218/2330 train_time:70692ms step_avg:58.04ms
step:1219/2330 train_time:70749ms step_avg:58.04ms
step:1220/2330 train_time:70809ms step_avg:58.04ms
step:1221/2330 train_time:70866ms step_avg:58.04ms
step:1222/2330 train_time:70925ms step_avg:58.04ms
step:1223/2330 train_time:70982ms step_avg:58.04ms
step:1224/2330 train_time:71043ms step_avg:58.04ms
step:1225/2330 train_time:71100ms step_avg:58.04ms
step:1226/2330 train_time:71161ms step_avg:58.04ms
step:1227/2330 train_time:71218ms step_avg:58.04ms
step:1228/2330 train_time:71278ms step_avg:58.04ms
step:1229/2330 train_time:71335ms step_avg:58.04ms
step:1230/2330 train_time:71395ms step_avg:58.04ms
step:1231/2330 train_time:71452ms step_avg:58.04ms
step:1232/2330 train_time:71512ms step_avg:58.05ms
step:1233/2330 train_time:71569ms step_avg:58.04ms
step:1234/2330 train_time:71629ms step_avg:58.05ms
step:1235/2330 train_time:71686ms step_avg:58.05ms
step:1236/2330 train_time:71745ms step_avg:58.05ms
step:1237/2330 train_time:71803ms step_avg:58.05ms
step:1238/2330 train_time:71863ms step_avg:58.05ms
step:1239/2330 train_time:71920ms step_avg:58.05ms
step:1240/2330 train_time:71980ms step_avg:58.05ms
step:1241/2330 train_time:72038ms step_avg:58.05ms
step:1242/2330 train_time:72098ms step_avg:58.05ms
step:1243/2330 train_time:72155ms step_avg:58.05ms
step:1244/2330 train_time:72215ms step_avg:58.05ms
step:1245/2330 train_time:72272ms step_avg:58.05ms
step:1246/2330 train_time:72332ms step_avg:58.05ms
step:1247/2330 train_time:72390ms step_avg:58.05ms
step:1248/2330 train_time:72449ms step_avg:58.05ms
step:1249/2330 train_time:72506ms step_avg:58.05ms
step:1250/2330 train_time:72565ms step_avg:58.05ms
step:1250/2330 val_loss:3.9842 train_time:72647ms step_avg:58.12ms
step:1251/2330 train_time:72666ms step_avg:58.09ms
step:1252/2330 train_time:72687ms step_avg:58.06ms
step:1253/2330 train_time:72746ms step_avg:58.06ms
step:1254/2330 train_time:72810ms step_avg:58.06ms
step:1255/2330 train_time:72867ms step_avg:58.06ms
step:1256/2330 train_time:72928ms step_avg:58.06ms
step:1257/2330 train_time:72984ms step_avg:58.06ms
step:1258/2330 train_time:73045ms step_avg:58.06ms
step:1259/2330 train_time:73101ms step_avg:58.06ms
step:1260/2330 train_time:73160ms step_avg:58.06ms
step:1261/2330 train_time:73216ms step_avg:58.06ms
step:1262/2330 train_time:73276ms step_avg:58.06ms
step:1263/2330 train_time:73333ms step_avg:58.06ms
step:1264/2330 train_time:73392ms step_avg:58.06ms
step:1265/2330 train_time:73448ms step_avg:58.06ms
step:1266/2330 train_time:73507ms step_avg:58.06ms
step:1267/2330 train_time:73563ms step_avg:58.06ms
step:1268/2330 train_time:73624ms step_avg:58.06ms
step:1269/2330 train_time:73683ms step_avg:58.06ms
step:1270/2330 train_time:73745ms step_avg:58.07ms
step:1271/2330 train_time:73803ms step_avg:58.07ms
step:1272/2330 train_time:73865ms step_avg:58.07ms
step:1273/2330 train_time:73922ms step_avg:58.07ms
step:1274/2330 train_time:73983ms step_avg:58.07ms
step:1275/2330 train_time:74039ms step_avg:58.07ms
step:1276/2330 train_time:74099ms step_avg:58.07ms
step:1277/2330 train_time:74155ms step_avg:58.07ms
step:1278/2330 train_time:74216ms step_avg:58.07ms
step:1279/2330 train_time:74272ms step_avg:58.07ms
step:1280/2330 train_time:74332ms step_avg:58.07ms
step:1281/2330 train_time:74388ms step_avg:58.07ms
step:1282/2330 train_time:74449ms step_avg:58.07ms
step:1283/2330 train_time:74505ms step_avg:58.07ms
step:1284/2330 train_time:74565ms step_avg:58.07ms
step:1285/2330 train_time:74621ms step_avg:58.07ms
step:1286/2330 train_time:74683ms step_avg:58.07ms
step:1287/2330 train_time:74740ms step_avg:58.07ms
step:1288/2330 train_time:74802ms step_avg:58.08ms
step:1289/2330 train_time:74859ms step_avg:58.08ms
step:1290/2330 train_time:74921ms step_avg:58.08ms
step:1291/2330 train_time:74978ms step_avg:58.08ms
step:1292/2330 train_time:75039ms step_avg:58.08ms
step:1293/2330 train_time:75094ms step_avg:58.08ms
step:1294/2330 train_time:75155ms step_avg:58.08ms
step:1295/2330 train_time:75211ms step_avg:58.08ms
step:1296/2330 train_time:75271ms step_avg:58.08ms
step:1297/2330 train_time:75327ms step_avg:58.08ms
step:1298/2330 train_time:75387ms step_avg:58.08ms
step:1299/2330 train_time:75443ms step_avg:58.08ms
step:1300/2330 train_time:75503ms step_avg:58.08ms
step:1301/2330 train_time:75559ms step_avg:58.08ms
step:1302/2330 train_time:75620ms step_avg:58.08ms
step:1303/2330 train_time:75677ms step_avg:58.08ms
step:1304/2330 train_time:75739ms step_avg:58.08ms
step:1305/2330 train_time:75796ms step_avg:58.08ms
step:1306/2330 train_time:75857ms step_avg:58.08ms
step:1307/2330 train_time:75915ms step_avg:58.08ms
step:1308/2330 train_time:75976ms step_avg:58.09ms
step:1309/2330 train_time:76032ms step_avg:58.08ms
step:1310/2330 train_time:76093ms step_avg:58.09ms
step:1311/2330 train_time:76149ms step_avg:58.08ms
step:1312/2330 train_time:76211ms step_avg:58.09ms
step:1313/2330 train_time:76266ms step_avg:58.09ms
step:1314/2330 train_time:76328ms step_avg:58.09ms
step:1315/2330 train_time:76385ms step_avg:58.09ms
step:1316/2330 train_time:76445ms step_avg:58.09ms
step:1317/2330 train_time:76502ms step_avg:58.09ms
step:1318/2330 train_time:76562ms step_avg:58.09ms
step:1319/2330 train_time:76618ms step_avg:58.09ms
step:1320/2330 train_time:76679ms step_avg:58.09ms
step:1321/2330 train_time:76736ms step_avg:58.09ms
step:1322/2330 train_time:76797ms step_avg:58.09ms
step:1323/2330 train_time:76855ms step_avg:58.09ms
step:1324/2330 train_time:76916ms step_avg:58.09ms
step:1325/2330 train_time:76973ms step_avg:58.09ms
step:1326/2330 train_time:77034ms step_avg:58.09ms
step:1327/2330 train_time:77091ms step_avg:58.09ms
step:1328/2330 train_time:77151ms step_avg:58.10ms
step:1329/2330 train_time:77208ms step_avg:58.09ms
step:1330/2330 train_time:77268ms step_avg:58.10ms
step:1331/2330 train_time:77325ms step_avg:58.10ms
step:1332/2330 train_time:77385ms step_avg:58.10ms
step:1333/2330 train_time:77441ms step_avg:58.10ms
step:1334/2330 train_time:77501ms step_avg:58.10ms
step:1335/2330 train_time:77558ms step_avg:58.10ms
step:1336/2330 train_time:77619ms step_avg:58.10ms
step:1337/2330 train_time:77675ms step_avg:58.10ms
step:1338/2330 train_time:77737ms step_avg:58.10ms
step:1339/2330 train_time:77793ms step_avg:58.10ms
step:1340/2330 train_time:77854ms step_avg:58.10ms
step:1341/2330 train_time:77910ms step_avg:58.10ms
step:1342/2330 train_time:77973ms step_avg:58.10ms
step:1343/2330 train_time:78029ms step_avg:58.10ms
step:1344/2330 train_time:78091ms step_avg:58.10ms
step:1345/2330 train_time:78147ms step_avg:58.10ms
step:1346/2330 train_time:78207ms step_avg:58.10ms
step:1347/2330 train_time:78264ms step_avg:58.10ms
step:1348/2330 train_time:78323ms step_avg:58.10ms
step:1349/2330 train_time:78379ms step_avg:58.10ms
step:1350/2330 train_time:78440ms step_avg:58.10ms
step:1351/2330 train_time:78496ms step_avg:58.10ms
step:1352/2330 train_time:78557ms step_avg:58.10ms
step:1353/2330 train_time:78613ms step_avg:58.10ms
step:1354/2330 train_time:78674ms step_avg:58.10ms
step:1355/2330 train_time:78731ms step_avg:58.10ms
step:1356/2330 train_time:78792ms step_avg:58.11ms
step:1357/2330 train_time:78849ms step_avg:58.11ms
step:1358/2330 train_time:78910ms step_avg:58.11ms
step:1359/2330 train_time:78967ms step_avg:58.11ms
step:1360/2330 train_time:79028ms step_avg:58.11ms
step:1361/2330 train_time:79085ms step_avg:58.11ms
step:1362/2330 train_time:79146ms step_avg:58.11ms
step:1363/2330 train_time:79202ms step_avg:58.11ms
step:1364/2330 train_time:79264ms step_avg:58.11ms
step:1365/2330 train_time:79321ms step_avg:58.11ms
step:1366/2330 train_time:79380ms step_avg:58.11ms
step:1367/2330 train_time:79436ms step_avg:58.11ms
step:1368/2330 train_time:79497ms step_avg:58.11ms
step:1369/2330 train_time:79554ms step_avg:58.11ms
step:1370/2330 train_time:79614ms step_avg:58.11ms
step:1371/2330 train_time:79670ms step_avg:58.11ms
step:1372/2330 train_time:79732ms step_avg:58.11ms
step:1373/2330 train_time:79789ms step_avg:58.11ms
step:1374/2330 train_time:79849ms step_avg:58.11ms
step:1375/2330 train_time:79906ms step_avg:58.11ms
step:1376/2330 train_time:79967ms step_avg:58.12ms
step:1377/2330 train_time:80023ms step_avg:58.11ms
step:1378/2330 train_time:80085ms step_avg:58.12ms
step:1379/2330 train_time:80141ms step_avg:58.12ms
step:1380/2330 train_time:80201ms step_avg:58.12ms
step:1381/2330 train_time:80258ms step_avg:58.12ms
step:1382/2330 train_time:80319ms step_avg:58.12ms
step:1383/2330 train_time:80375ms step_avg:58.12ms
step:1384/2330 train_time:80436ms step_avg:58.12ms
step:1385/2330 train_time:80492ms step_avg:58.12ms
step:1386/2330 train_time:80552ms step_avg:58.12ms
step:1387/2330 train_time:80609ms step_avg:58.12ms
step:1388/2330 train_time:80669ms step_avg:58.12ms
step:1389/2330 train_time:80725ms step_avg:58.12ms
step:1390/2330 train_time:80786ms step_avg:58.12ms
step:1391/2330 train_time:80842ms step_avg:58.12ms
step:1392/2330 train_time:80903ms step_avg:58.12ms
step:1393/2330 train_time:80959ms step_avg:58.12ms
step:1394/2330 train_time:81021ms step_avg:58.12ms
step:1395/2330 train_time:81077ms step_avg:58.12ms
step:1396/2330 train_time:81138ms step_avg:58.12ms
step:1397/2330 train_time:81194ms step_avg:58.12ms
step:1398/2330 train_time:81255ms step_avg:58.12ms
step:1399/2330 train_time:81312ms step_avg:58.12ms
step:1400/2330 train_time:81372ms step_avg:58.12ms
step:1401/2330 train_time:81429ms step_avg:58.12ms
step:1402/2330 train_time:81488ms step_avg:58.12ms
step:1403/2330 train_time:81545ms step_avg:58.12ms
step:1404/2330 train_time:81605ms step_avg:58.12ms
step:1405/2330 train_time:81661ms step_avg:58.12ms
step:1406/2330 train_time:81722ms step_avg:58.12ms
step:1407/2330 train_time:81779ms step_avg:58.12ms
step:1408/2330 train_time:81840ms step_avg:58.12ms
step:1409/2330 train_time:81896ms step_avg:58.12ms
step:1410/2330 train_time:81957ms step_avg:58.13ms
step:1411/2330 train_time:82014ms step_avg:58.12ms
step:1412/2330 train_time:82075ms step_avg:58.13ms
step:1413/2330 train_time:82132ms step_avg:58.13ms
step:1414/2330 train_time:82193ms step_avg:58.13ms
step:1415/2330 train_time:82249ms step_avg:58.13ms
step:1416/2330 train_time:82311ms step_avg:58.13ms
step:1417/2330 train_time:82368ms step_avg:58.13ms
step:1418/2330 train_time:82428ms step_avg:58.13ms
step:1419/2330 train_time:82484ms step_avg:58.13ms
step:1420/2330 train_time:82545ms step_avg:58.13ms
step:1421/2330 train_time:82601ms step_avg:58.13ms
step:1422/2330 train_time:82661ms step_avg:58.13ms
step:1423/2330 train_time:82717ms step_avg:58.13ms
step:1424/2330 train_time:82778ms step_avg:58.13ms
step:1425/2330 train_time:82835ms step_avg:58.13ms
step:1426/2330 train_time:82895ms step_avg:58.13ms
step:1427/2330 train_time:82951ms step_avg:58.13ms
step:1428/2330 train_time:83014ms step_avg:58.13ms
step:1429/2330 train_time:83071ms step_avg:58.13ms
step:1430/2330 train_time:83133ms step_avg:58.13ms
step:1431/2330 train_time:83189ms step_avg:58.13ms
step:1432/2330 train_time:83250ms step_avg:58.14ms
step:1433/2330 train_time:83307ms step_avg:58.13ms
step:1434/2330 train_time:83368ms step_avg:58.14ms
step:1435/2330 train_time:83424ms step_avg:58.14ms
step:1436/2330 train_time:83485ms step_avg:58.14ms
step:1437/2330 train_time:83541ms step_avg:58.14ms
step:1438/2330 train_time:83602ms step_avg:58.14ms
step:1439/2330 train_time:83659ms step_avg:58.14ms
step:1440/2330 train_time:83719ms step_avg:58.14ms
step:1441/2330 train_time:83775ms step_avg:58.14ms
step:1442/2330 train_time:83836ms step_avg:58.14ms
step:1443/2330 train_time:83893ms step_avg:58.14ms
step:1444/2330 train_time:83953ms step_avg:58.14ms
step:1445/2330 train_time:84010ms step_avg:58.14ms
step:1446/2330 train_time:84071ms step_avg:58.14ms
step:1447/2330 train_time:84127ms step_avg:58.14ms
step:1448/2330 train_time:84188ms step_avg:58.14ms
step:1449/2330 train_time:84245ms step_avg:58.14ms
step:1450/2330 train_time:84305ms step_avg:58.14ms
step:1451/2330 train_time:84362ms step_avg:58.14ms
step:1452/2330 train_time:84422ms step_avg:58.14ms
step:1453/2330 train_time:84479ms step_avg:58.14ms
step:1454/2330 train_time:84539ms step_avg:58.14ms
step:1455/2330 train_time:84595ms step_avg:58.14ms
step:1456/2330 train_time:84657ms step_avg:58.14ms
step:1457/2330 train_time:84713ms step_avg:58.14ms
step:1458/2330 train_time:84774ms step_avg:58.14ms
step:1459/2330 train_time:84831ms step_avg:58.14ms
step:1460/2330 train_time:84891ms step_avg:58.14ms
step:1461/2330 train_time:84948ms step_avg:58.14ms
step:1462/2330 train_time:85007ms step_avg:58.14ms
step:1463/2330 train_time:85063ms step_avg:58.14ms
step:1464/2330 train_time:85125ms step_avg:58.15ms
step:1465/2330 train_time:85181ms step_avg:58.14ms
step:1466/2330 train_time:85243ms step_avg:58.15ms
step:1467/2330 train_time:85299ms step_avg:58.15ms
step:1468/2330 train_time:85360ms step_avg:58.15ms
step:1469/2330 train_time:85416ms step_avg:58.15ms
step:1470/2330 train_time:85477ms step_avg:58.15ms
step:1471/2330 train_time:85534ms step_avg:58.15ms
step:1472/2330 train_time:85594ms step_avg:58.15ms
step:1473/2330 train_time:85651ms step_avg:58.15ms
step:1474/2330 train_time:85712ms step_avg:58.15ms
step:1475/2330 train_time:85768ms step_avg:58.15ms
step:1476/2330 train_time:85830ms step_avg:58.15ms
step:1477/2330 train_time:85886ms step_avg:58.15ms
step:1478/2330 train_time:85947ms step_avg:58.15ms
step:1479/2330 train_time:86004ms step_avg:58.15ms
step:1480/2330 train_time:86064ms step_avg:58.15ms
step:1481/2330 train_time:86121ms step_avg:58.15ms
step:1482/2330 train_time:86183ms step_avg:58.15ms
step:1483/2330 train_time:86239ms step_avg:58.15ms
step:1484/2330 train_time:86300ms step_avg:58.15ms
step:1485/2330 train_time:86356ms step_avg:58.15ms
step:1486/2330 train_time:86418ms step_avg:58.15ms
step:1487/2330 train_time:86474ms step_avg:58.15ms
step:1488/2330 train_time:86535ms step_avg:58.16ms
step:1489/2330 train_time:86591ms step_avg:58.15ms
step:1490/2330 train_time:86652ms step_avg:58.16ms
step:1491/2330 train_time:86708ms step_avg:58.15ms
step:1492/2330 train_time:86768ms step_avg:58.16ms
step:1493/2330 train_time:86825ms step_avg:58.15ms
step:1494/2330 train_time:86886ms step_avg:58.16ms
step:1495/2330 train_time:86943ms step_avg:58.16ms
step:1496/2330 train_time:87003ms step_avg:58.16ms
step:1497/2330 train_time:87060ms step_avg:58.16ms
step:1498/2330 train_time:87120ms step_avg:58.16ms
step:1499/2330 train_time:87177ms step_avg:58.16ms
step:1500/2330 train_time:87237ms step_avg:58.16ms
step:1500/2330 val_loss:3.9031 train_time:87318ms step_avg:58.21ms
step:1501/2330 train_time:87337ms step_avg:58.19ms
step:1502/2330 train_time:87357ms step_avg:58.16ms
step:1503/2330 train_time:87415ms step_avg:58.16ms
step:1504/2330 train_time:87482ms step_avg:58.17ms
step:1505/2330 train_time:87538ms step_avg:58.16ms
step:1506/2330 train_time:87601ms step_avg:58.17ms
step:1507/2330 train_time:87657ms step_avg:58.17ms
step:1508/2330 train_time:87717ms step_avg:58.17ms
step:1509/2330 train_time:87773ms step_avg:58.17ms
step:1510/2330 train_time:87834ms step_avg:58.17ms
step:1511/2330 train_time:87890ms step_avg:58.17ms
step:1512/2330 train_time:87950ms step_avg:58.17ms
step:1513/2330 train_time:88006ms step_avg:58.17ms
step:1514/2330 train_time:88065ms step_avg:58.17ms
step:1515/2330 train_time:88122ms step_avg:58.17ms
step:1516/2330 train_time:88181ms step_avg:58.17ms
step:1517/2330 train_time:88238ms step_avg:58.17ms
step:1518/2330 train_time:88298ms step_avg:58.17ms
step:1519/2330 train_time:88357ms step_avg:58.17ms
step:1520/2330 train_time:88418ms step_avg:58.17ms
step:1521/2330 train_time:88475ms step_avg:58.17ms
step:1522/2330 train_time:88539ms step_avg:58.17ms
step:1523/2330 train_time:88596ms step_avg:58.17ms
step:1524/2330 train_time:88658ms step_avg:58.17ms
step:1525/2330 train_time:88714ms step_avg:58.17ms
step:1526/2330 train_time:88776ms step_avg:58.18ms
step:1527/2330 train_time:88832ms step_avg:58.17ms
step:1528/2330 train_time:88893ms step_avg:58.18ms
step:1529/2330 train_time:88951ms step_avg:58.18ms
step:1530/2330 train_time:89009ms step_avg:58.18ms
step:1531/2330 train_time:89066ms step_avg:58.17ms
step:1532/2330 train_time:89126ms step_avg:58.18ms
step:1533/2330 train_time:89183ms step_avg:58.18ms
step:1534/2330 train_time:89243ms step_avg:58.18ms
step:1535/2330 train_time:89300ms step_avg:58.18ms
step:1536/2330 train_time:89362ms step_avg:58.18ms
step:1537/2330 train_time:89419ms step_avg:58.18ms
step:1538/2330 train_time:89482ms step_avg:58.18ms
step:1539/2330 train_time:89541ms step_avg:58.18ms
step:1540/2330 train_time:89603ms step_avg:58.18ms
step:1541/2330 train_time:89662ms step_avg:58.18ms
step:1542/2330 train_time:89722ms step_avg:58.19ms
step:1543/2330 train_time:89780ms step_avg:58.19ms
step:1544/2330 train_time:89840ms step_avg:58.19ms
step:1545/2330 train_time:89897ms step_avg:58.19ms
step:1546/2330 train_time:89958ms step_avg:58.19ms
step:1547/2330 train_time:90015ms step_avg:58.19ms
step:1548/2330 train_time:90077ms step_avg:58.19ms
step:1549/2330 train_time:90134ms step_avg:58.19ms
step:1550/2330 train_time:90195ms step_avg:58.19ms
step:1551/2330 train_time:90252ms step_avg:58.19ms
step:1552/2330 train_time:90313ms step_avg:58.19ms
step:1553/2330 train_time:90370ms step_avg:58.19ms
step:1554/2330 train_time:90431ms step_avg:58.19ms
step:1555/2330 train_time:90488ms step_avg:58.19ms
step:1556/2330 train_time:90551ms step_avg:58.19ms
step:1557/2330 train_time:90608ms step_avg:58.19ms
step:1558/2330 train_time:90670ms step_avg:58.20ms
step:1559/2330 train_time:90727ms step_avg:58.20ms
step:1560/2330 train_time:90788ms step_avg:58.20ms
step:1561/2330 train_time:90845ms step_avg:58.20ms
step:1562/2330 train_time:90906ms step_avg:58.20ms
step:1563/2330 train_time:90963ms step_avg:58.20ms
step:1564/2330 train_time:91024ms step_avg:58.20ms
step:1565/2330 train_time:91082ms step_avg:58.20ms
step:1566/2330 train_time:91141ms step_avg:58.20ms
step:1567/2330 train_time:91199ms step_avg:58.20ms
step:1568/2330 train_time:91260ms step_avg:58.20ms
step:1569/2330 train_time:91317ms step_avg:58.20ms
step:1570/2330 train_time:91378ms step_avg:58.20ms
step:1571/2330 train_time:91436ms step_avg:58.20ms
step:1572/2330 train_time:91496ms step_avg:58.20ms
step:1573/2330 train_time:91554ms step_avg:58.20ms
step:1574/2330 train_time:91616ms step_avg:58.21ms
step:1575/2330 train_time:91673ms step_avg:58.21ms
step:1576/2330 train_time:91735ms step_avg:58.21ms
step:1577/2330 train_time:91792ms step_avg:58.21ms
step:1578/2330 train_time:91854ms step_avg:58.21ms
step:1579/2330 train_time:91910ms step_avg:58.21ms
step:1580/2330 train_time:91971ms step_avg:58.21ms
step:1581/2330 train_time:92028ms step_avg:58.21ms
step:1582/2330 train_time:92088ms step_avg:58.21ms
step:1583/2330 train_time:92145ms step_avg:58.21ms
step:1584/2330 train_time:92207ms step_avg:58.21ms
step:1585/2330 train_time:92263ms step_avg:58.21ms
step:1586/2330 train_time:92325ms step_avg:58.21ms
step:1587/2330 train_time:92382ms step_avg:58.21ms
step:1588/2330 train_time:92443ms step_avg:58.21ms
step:1589/2330 train_time:92501ms step_avg:58.21ms
step:1590/2330 train_time:92564ms step_avg:58.22ms
step:1591/2330 train_time:92623ms step_avg:58.22ms
step:1592/2330 train_time:92683ms step_avg:58.22ms
step:1593/2330 train_time:92741ms step_avg:58.22ms
step:1594/2330 train_time:92802ms step_avg:58.22ms
step:1595/2330 train_time:92861ms step_avg:58.22ms
step:1596/2330 train_time:92921ms step_avg:58.22ms
step:1597/2330 train_time:92980ms step_avg:58.22ms
step:1598/2330 train_time:93039ms step_avg:58.22ms
step:1599/2330 train_time:93096ms step_avg:58.22ms
step:1600/2330 train_time:93157ms step_avg:58.22ms
step:1601/2330 train_time:93214ms step_avg:58.22ms
step:1602/2330 train_time:93275ms step_avg:58.22ms
step:1603/2330 train_time:93332ms step_avg:58.22ms
step:1604/2330 train_time:93393ms step_avg:58.23ms
step:1605/2330 train_time:93450ms step_avg:58.22ms
step:1606/2330 train_time:93511ms step_avg:58.23ms
step:1607/2330 train_time:93568ms step_avg:58.23ms
step:1608/2330 train_time:93629ms step_avg:58.23ms
step:1609/2330 train_time:93686ms step_avg:58.23ms
step:1610/2330 train_time:93748ms step_avg:58.23ms
step:1611/2330 train_time:93805ms step_avg:58.23ms
step:1612/2330 train_time:93866ms step_avg:58.23ms
step:1613/2330 train_time:93923ms step_avg:58.23ms
step:1614/2330 train_time:93984ms step_avg:58.23ms
step:1615/2330 train_time:94042ms step_avg:58.23ms
step:1616/2330 train_time:94103ms step_avg:58.23ms
step:1617/2330 train_time:94161ms step_avg:58.23ms
step:1618/2330 train_time:94222ms step_avg:58.23ms
step:1619/2330 train_time:94280ms step_avg:58.23ms
step:1620/2330 train_time:94340ms step_avg:58.23ms
step:1621/2330 train_time:94398ms step_avg:58.23ms
step:1622/2330 train_time:94458ms step_avg:58.24ms
step:1623/2330 train_time:94516ms step_avg:58.24ms
step:1624/2330 train_time:94578ms step_avg:58.24ms
step:1625/2330 train_time:94635ms step_avg:58.24ms
step:1626/2330 train_time:94696ms step_avg:58.24ms
step:1627/2330 train_time:94753ms step_avg:58.24ms
step:1628/2330 train_time:94813ms step_avg:58.24ms
step:1629/2330 train_time:94871ms step_avg:58.24ms
step:1630/2330 train_time:94931ms step_avg:58.24ms
step:1631/2330 train_time:94987ms step_avg:58.24ms
step:1632/2330 train_time:95050ms step_avg:58.24ms
step:1633/2330 train_time:95106ms step_avg:58.24ms
step:1634/2330 train_time:95169ms step_avg:58.24ms
step:1635/2330 train_time:95226ms step_avg:58.24ms
step:1636/2330 train_time:95287ms step_avg:58.24ms
step:1637/2330 train_time:95344ms step_avg:58.24ms
step:1638/2330 train_time:95406ms step_avg:58.25ms
step:1639/2330 train_time:95464ms step_avg:58.25ms
step:1640/2330 train_time:95526ms step_avg:58.25ms
step:1641/2330 train_time:95584ms step_avg:58.25ms
step:1642/2330 train_time:95644ms step_avg:58.25ms
step:1643/2330 train_time:95702ms step_avg:58.25ms
step:1644/2330 train_time:95763ms step_avg:58.25ms
step:1645/2330 train_time:95821ms step_avg:58.25ms
step:1646/2330 train_time:95881ms step_avg:58.25ms
step:1647/2330 train_time:95939ms step_avg:58.25ms
step:1648/2330 train_time:96000ms step_avg:58.25ms
step:1649/2330 train_time:96058ms step_avg:58.25ms
step:1650/2330 train_time:96118ms step_avg:58.25ms
step:1651/2330 train_time:96176ms step_avg:58.25ms
step:1652/2330 train_time:96237ms step_avg:58.25ms
step:1653/2330 train_time:96293ms step_avg:58.25ms
step:1654/2330 train_time:96354ms step_avg:58.26ms
step:1655/2330 train_time:96411ms step_avg:58.25ms
step:1656/2330 train_time:96472ms step_avg:58.26ms
step:1657/2330 train_time:96529ms step_avg:58.26ms
step:1658/2330 train_time:96590ms step_avg:58.26ms
step:1659/2330 train_time:96647ms step_avg:58.26ms
step:1660/2330 train_time:96709ms step_avg:58.26ms
step:1661/2330 train_time:96765ms step_avg:58.26ms
step:1662/2330 train_time:96827ms step_avg:58.26ms
step:1663/2330 train_time:96884ms step_avg:58.26ms
step:1664/2330 train_time:96945ms step_avg:58.26ms
step:1665/2330 train_time:97002ms step_avg:58.26ms
step:1666/2330 train_time:97064ms step_avg:58.26ms
step:1667/2330 train_time:97121ms step_avg:58.26ms
step:1668/2330 train_time:97183ms step_avg:58.26ms
step:1669/2330 train_time:97241ms step_avg:58.26ms
step:1670/2330 train_time:97302ms step_avg:58.26ms
step:1671/2330 train_time:97360ms step_avg:58.26ms
step:1672/2330 train_time:97421ms step_avg:58.27ms
step:1673/2330 train_time:97479ms step_avg:58.27ms
step:1674/2330 train_time:97540ms step_avg:58.27ms
step:1675/2330 train_time:97598ms step_avg:58.27ms
step:1676/2330 train_time:97658ms step_avg:58.27ms
step:1677/2330 train_time:97716ms step_avg:58.27ms
step:1678/2330 train_time:97776ms step_avg:58.27ms
step:1679/2330 train_time:97833ms step_avg:58.27ms
step:1680/2330 train_time:97893ms step_avg:58.27ms
step:1681/2330 train_time:97951ms step_avg:58.27ms
step:1682/2330 train_time:98011ms step_avg:58.27ms
step:1683/2330 train_time:98068ms step_avg:58.27ms
step:1684/2330 train_time:98129ms step_avg:58.27ms
step:1685/2330 train_time:98186ms step_avg:58.27ms
step:1686/2330 train_time:98249ms step_avg:58.27ms
step:1687/2330 train_time:98306ms step_avg:58.27ms
step:1688/2330 train_time:98367ms step_avg:58.27ms
step:1689/2330 train_time:98425ms step_avg:58.27ms
step:1690/2330 train_time:98485ms step_avg:58.28ms
step:1691/2330 train_time:98542ms step_avg:58.27ms
step:1692/2330 train_time:98604ms step_avg:58.28ms
step:1693/2330 train_time:98663ms step_avg:58.28ms
step:1694/2330 train_time:98722ms step_avg:58.28ms
step:1695/2330 train_time:98780ms step_avg:58.28ms
step:1696/2330 train_time:98840ms step_avg:58.28ms
step:1697/2330 train_time:98898ms step_avg:58.28ms
step:1698/2330 train_time:98958ms step_avg:58.28ms
step:1699/2330 train_time:99015ms step_avg:58.28ms
step:1700/2330 train_time:99076ms step_avg:58.28ms
step:1701/2330 train_time:99133ms step_avg:58.28ms
step:1702/2330 train_time:99195ms step_avg:58.28ms
step:1703/2330 train_time:99253ms step_avg:58.28ms
step:1704/2330 train_time:99313ms step_avg:58.28ms
step:1705/2330 train_time:99370ms step_avg:58.28ms
step:1706/2330 train_time:99432ms step_avg:58.28ms
step:1707/2330 train_time:99488ms step_avg:58.28ms
step:1708/2330 train_time:99551ms step_avg:58.29ms
step:1709/2330 train_time:99608ms step_avg:58.28ms
step:1710/2330 train_time:99670ms step_avg:58.29ms
step:1711/2330 train_time:99726ms step_avg:58.29ms
step:1712/2330 train_time:99787ms step_avg:58.29ms
step:1713/2330 train_time:99844ms step_avg:58.29ms
step:1714/2330 train_time:99905ms step_avg:58.29ms
step:1715/2330 train_time:99963ms step_avg:58.29ms
step:1716/2330 train_time:100025ms step_avg:58.29ms
step:1717/2330 train_time:100083ms step_avg:58.29ms
step:1718/2330 train_time:100144ms step_avg:58.29ms
step:1719/2330 train_time:100202ms step_avg:58.29ms
step:1720/2330 train_time:100263ms step_avg:58.29ms
step:1721/2330 train_time:100321ms step_avg:58.29ms
step:1722/2330 train_time:100383ms step_avg:58.29ms
step:1723/2330 train_time:100440ms step_avg:58.29ms
step:1724/2330 train_time:100503ms step_avg:58.30ms
step:1725/2330 train_time:100560ms step_avg:58.30ms
step:1726/2330 train_time:100623ms step_avg:58.30ms
step:1727/2330 train_time:100680ms step_avg:58.30ms
step:1728/2330 train_time:100740ms step_avg:58.30ms
step:1729/2330 train_time:100798ms step_avg:58.30ms
step:1730/2330 train_time:100858ms step_avg:58.30ms
step:1731/2330 train_time:100915ms step_avg:58.30ms
step:1732/2330 train_time:100975ms step_avg:58.30ms
step:1733/2330 train_time:101033ms step_avg:58.30ms
step:1734/2330 train_time:101093ms step_avg:58.30ms
step:1735/2330 train_time:101150ms step_avg:58.30ms
step:1736/2330 train_time:101211ms step_avg:58.30ms
step:1737/2330 train_time:101268ms step_avg:58.30ms
step:1738/2330 train_time:101328ms step_avg:58.30ms
step:1739/2330 train_time:101385ms step_avg:58.30ms
step:1740/2330 train_time:101446ms step_avg:58.30ms
step:1741/2330 train_time:101503ms step_avg:58.30ms
step:1742/2330 train_time:101565ms step_avg:58.30ms
step:1743/2330 train_time:101622ms step_avg:58.30ms
step:1744/2330 train_time:101684ms step_avg:58.30ms
step:1745/2330 train_time:101742ms step_avg:58.30ms
step:1746/2330 train_time:101802ms step_avg:58.31ms
step:1747/2330 train_time:101860ms step_avg:58.31ms
step:1748/2330 train_time:101921ms step_avg:58.31ms
step:1749/2330 train_time:101980ms step_avg:58.31ms
step:1750/2330 train_time:102040ms step_avg:58.31ms
step:1750/2330 val_loss:3.8184 train_time:102122ms step_avg:58.36ms
step:1751/2330 train_time:102141ms step_avg:58.33ms
step:1752/2330 train_time:102160ms step_avg:58.31ms
step:1753/2330 train_time:102215ms step_avg:58.31ms
step:1754/2330 train_time:102280ms step_avg:58.31ms
step:1755/2330 train_time:102336ms step_avg:58.31ms
step:1756/2330 train_time:102403ms step_avg:58.32ms
step:1757/2330 train_time:102460ms step_avg:58.32ms
step:1758/2330 train_time:102521ms step_avg:58.32ms
step:1759/2330 train_time:102578ms step_avg:58.32ms
step:1760/2330 train_time:102639ms step_avg:58.32ms
step:1761/2330 train_time:102695ms step_avg:58.32ms
step:1762/2330 train_time:102755ms step_avg:58.32ms
step:1763/2330 train_time:102812ms step_avg:58.32ms
step:1764/2330 train_time:102870ms step_avg:58.32ms
step:1765/2330 train_time:102927ms step_avg:58.32ms
step:1766/2330 train_time:102987ms step_avg:58.32ms
step:1767/2330 train_time:103049ms step_avg:58.32ms
step:1768/2330 train_time:103112ms step_avg:58.32ms
step:1769/2330 train_time:103171ms step_avg:58.32ms
step:1770/2330 train_time:103231ms step_avg:58.32ms
step:1771/2330 train_time:103289ms step_avg:58.32ms
step:1772/2330 train_time:103350ms step_avg:58.32ms
step:1773/2330 train_time:103407ms step_avg:58.32ms
step:1774/2330 train_time:103469ms step_avg:58.33ms
step:1775/2330 train_time:103525ms step_avg:58.32ms
step:1776/2330 train_time:103586ms step_avg:58.33ms
step:1777/2330 train_time:103644ms step_avg:58.33ms
step:1778/2330 train_time:103705ms step_avg:58.33ms
step:1779/2330 train_time:103762ms step_avg:58.33ms
step:1780/2330 train_time:103823ms step_avg:58.33ms
step:1781/2330 train_time:103879ms step_avg:58.33ms
step:1782/2330 train_time:103939ms step_avg:58.33ms
step:1783/2330 train_time:103998ms step_avg:58.33ms
step:1784/2330 train_time:104060ms step_avg:58.33ms
step:1785/2330 train_time:104120ms step_avg:58.33ms
step:1786/2330 train_time:104181ms step_avg:58.33ms
step:1787/2330 train_time:104238ms step_avg:58.33ms
step:1788/2330 train_time:104301ms step_avg:58.33ms
step:1789/2330 train_time:104360ms step_avg:58.33ms
step:1790/2330 train_time:104421ms step_avg:58.34ms
step:1791/2330 train_time:104478ms step_avg:58.34ms
step:1792/2330 train_time:104538ms step_avg:58.34ms
step:1793/2330 train_time:104595ms step_avg:58.34ms
step:1794/2330 train_time:104656ms step_avg:58.34ms
step:1795/2330 train_time:104713ms step_avg:58.34ms
step:1796/2330 train_time:104772ms step_avg:58.34ms
step:1797/2330 train_time:104829ms step_avg:58.34ms
step:1798/2330 train_time:104889ms step_avg:58.34ms
step:1799/2330 train_time:104946ms step_avg:58.34ms
step:1800/2330 train_time:105007ms step_avg:58.34ms
step:1801/2330 train_time:105066ms step_avg:58.34ms
step:1802/2330 train_time:105127ms step_avg:58.34ms
step:1803/2330 train_time:105186ms step_avg:58.34ms
step:1804/2330 train_time:105247ms step_avg:58.34ms
step:1805/2330 train_time:105305ms step_avg:58.34ms
step:1806/2330 train_time:105365ms step_avg:58.34ms
step:1807/2330 train_time:105423ms step_avg:58.34ms
step:1808/2330 train_time:105484ms step_avg:58.34ms
step:1809/2330 train_time:105542ms step_avg:58.34ms
step:1810/2330 train_time:105602ms step_avg:58.34ms
step:1811/2330 train_time:105659ms step_avg:58.34ms
step:1812/2330 train_time:105720ms step_avg:58.34ms
step:1813/2330 train_time:105777ms step_avg:58.34ms
step:1814/2330 train_time:105837ms step_avg:58.34ms
step:1815/2330 train_time:105894ms step_avg:58.34ms
step:1816/2330 train_time:105954ms step_avg:58.34ms
step:1817/2330 train_time:106012ms step_avg:58.34ms
step:1818/2330 train_time:106073ms step_avg:58.35ms
step:1819/2330 train_time:106130ms step_avg:58.35ms
step:1820/2330 train_time:106191ms step_avg:58.35ms
step:1821/2330 train_time:106249ms step_avg:58.35ms
step:1822/2330 train_time:106309ms step_avg:58.35ms
step:1823/2330 train_time:106366ms step_avg:58.35ms
step:1824/2330 train_time:106427ms step_avg:58.35ms
step:1825/2330 train_time:106484ms step_avg:58.35ms
step:1826/2330 train_time:106545ms step_avg:58.35ms
step:1827/2330 train_time:106602ms step_avg:58.35ms
step:1828/2330 train_time:106664ms step_avg:58.35ms
step:1829/2330 train_time:106721ms step_avg:58.35ms
step:1830/2330 train_time:106782ms step_avg:58.35ms
step:1831/2330 train_time:106839ms step_avg:58.35ms
step:1832/2330 train_time:106900ms step_avg:58.35ms
step:1833/2330 train_time:106958ms step_avg:58.35ms
step:1834/2330 train_time:107019ms step_avg:58.35ms
step:1835/2330 train_time:107076ms step_avg:58.35ms
step:1836/2330 train_time:107137ms step_avg:58.35ms
step:1837/2330 train_time:107196ms step_avg:58.35ms
step:1838/2330 train_time:107257ms step_avg:58.36ms
step:1839/2330 train_time:107314ms step_avg:58.35ms
step:1840/2330 train_time:107376ms step_avg:58.36ms
step:1841/2330 train_time:107433ms step_avg:58.36ms
step:1842/2330 train_time:107495ms step_avg:58.36ms
step:1843/2330 train_time:107552ms step_avg:58.36ms
step:1844/2330 train_time:107612ms step_avg:58.36ms
step:1845/2330 train_time:107670ms step_avg:58.36ms
step:1846/2330 train_time:107731ms step_avg:58.36ms
step:1847/2330 train_time:107788ms step_avg:58.36ms
step:1848/2330 train_time:107849ms step_avg:58.36ms
step:1849/2330 train_time:107905ms step_avg:58.36ms
step:1850/2330 train_time:107966ms step_avg:58.36ms
step:1851/2330 train_time:108024ms step_avg:58.36ms
step:1852/2330 train_time:108086ms step_avg:58.36ms
step:1853/2330 train_time:108145ms step_avg:58.36ms
step:1854/2330 train_time:108206ms step_avg:58.36ms
step:1855/2330 train_time:108264ms step_avg:58.36ms
step:1856/2330 train_time:108325ms step_avg:58.36ms
step:1857/2330 train_time:108383ms step_avg:58.36ms
step:1858/2330 train_time:108445ms step_avg:58.37ms
step:1859/2330 train_time:108503ms step_avg:58.37ms
step:1860/2330 train_time:108564ms step_avg:58.37ms
step:1861/2330 train_time:108621ms step_avg:58.37ms
step:1862/2330 train_time:108682ms step_avg:58.37ms
step:1863/2330 train_time:108740ms step_avg:58.37ms
step:1864/2330 train_time:108800ms step_avg:58.37ms
step:1865/2330 train_time:108857ms step_avg:58.37ms
step:1866/2330 train_time:108917ms step_avg:58.37ms
step:1867/2330 train_time:108974ms step_avg:58.37ms
step:1868/2330 train_time:109035ms step_avg:58.37ms
step:1869/2330 train_time:109092ms step_avg:58.37ms
step:1870/2330 train_time:109153ms step_avg:58.37ms
step:1871/2330 train_time:109210ms step_avg:58.37ms
step:1872/2330 train_time:109270ms step_avg:58.37ms
step:1873/2330 train_time:109328ms step_avg:58.37ms
step:1874/2330 train_time:109389ms step_avg:58.37ms
step:1875/2330 train_time:109447ms step_avg:58.37ms
step:1876/2330 train_time:109508ms step_avg:58.37ms
step:1877/2330 train_time:109566ms step_avg:58.37ms
step:1878/2330 train_time:109626ms step_avg:58.37ms
step:1879/2330 train_time:109684ms step_avg:58.37ms
step:1880/2330 train_time:109744ms step_avg:58.37ms
step:1881/2330 train_time:109802ms step_avg:58.37ms
step:1882/2330 train_time:109863ms step_avg:58.38ms
step:1883/2330 train_time:109920ms step_avg:58.37ms
step:1884/2330 train_time:109982ms step_avg:58.38ms
step:1885/2330 train_time:110041ms step_avg:58.38ms
step:1886/2330 train_time:110102ms step_avg:58.38ms
step:1887/2330 train_time:110159ms step_avg:58.38ms
step:1888/2330 train_time:110220ms step_avg:58.38ms
step:1889/2330 train_time:110278ms step_avg:58.38ms
step:1890/2330 train_time:110338ms step_avg:58.38ms
step:1891/2330 train_time:110395ms step_avg:58.38ms
step:1892/2330 train_time:110457ms step_avg:58.38ms
step:1893/2330 train_time:110514ms step_avg:58.38ms
step:1894/2330 train_time:110577ms step_avg:58.38ms
step:1895/2330 train_time:110634ms step_avg:58.38ms
step:1896/2330 train_time:110694ms step_avg:58.38ms
step:1897/2330 train_time:110751ms step_avg:58.38ms
step:1898/2330 train_time:110812ms step_avg:58.38ms
step:1899/2330 train_time:110868ms step_avg:58.38ms
step:1900/2330 train_time:110929ms step_avg:58.38ms
step:1901/2330 train_time:110987ms step_avg:58.38ms
step:1902/2330 train_time:111047ms step_avg:58.38ms
step:1903/2330 train_time:111104ms step_avg:58.38ms
step:1904/2330 train_time:111165ms step_avg:58.39ms
step:1905/2330 train_time:111223ms step_avg:58.38ms
step:1906/2330 train_time:111284ms step_avg:58.39ms
step:1907/2330 train_time:111342ms step_avg:58.39ms
step:1908/2330 train_time:111403ms step_avg:58.39ms
step:1909/2330 train_time:111461ms step_avg:58.39ms
step:1910/2330 train_time:111522ms step_avg:58.39ms
step:1911/2330 train_time:111581ms step_avg:58.39ms
step:1912/2330 train_time:111642ms step_avg:58.39ms
step:1913/2330 train_time:111700ms step_avg:58.39ms
step:1914/2330 train_time:111760ms step_avg:58.39ms
step:1915/2330 train_time:111819ms step_avg:58.39ms
step:1916/2330 train_time:111879ms step_avg:58.39ms
step:1917/2330 train_time:111938ms step_avg:58.39ms
step:1918/2330 train_time:111998ms step_avg:58.39ms
step:1919/2330 train_time:112055ms step_avg:58.39ms
step:1920/2330 train_time:112116ms step_avg:58.39ms
step:1921/2330 train_time:112173ms step_avg:58.39ms
step:1922/2330 train_time:112234ms step_avg:58.39ms
step:1923/2330 train_time:112291ms step_avg:58.39ms
step:1924/2330 train_time:112352ms step_avg:58.40ms
step:1925/2330 train_time:112409ms step_avg:58.39ms
step:1926/2330 train_time:112470ms step_avg:58.40ms
step:1927/2330 train_time:112527ms step_avg:58.39ms
step:1928/2330 train_time:112589ms step_avg:58.40ms
step:1929/2330 train_time:112647ms step_avg:58.40ms
step:1930/2330 train_time:112707ms step_avg:58.40ms
step:1931/2330 train_time:112764ms step_avg:58.40ms
step:1932/2330 train_time:112825ms step_avg:58.40ms
step:1933/2330 train_time:112883ms step_avg:58.40ms
step:1934/2330 train_time:112944ms step_avg:58.40ms
step:1935/2330 train_time:113002ms step_avg:58.40ms
step:1936/2330 train_time:113063ms step_avg:58.40ms
step:1937/2330 train_time:113122ms step_avg:58.40ms
step:1938/2330 train_time:113182ms step_avg:58.40ms
step:1939/2330 train_time:113240ms step_avg:58.40ms
step:1940/2330 train_time:113301ms step_avg:58.40ms
step:1941/2330 train_time:113360ms step_avg:58.40ms
step:1942/2330 train_time:113420ms step_avg:58.40ms
step:1943/2330 train_time:113477ms step_avg:58.40ms
step:1944/2330 train_time:113537ms step_avg:58.40ms
step:1945/2330 train_time:113594ms step_avg:58.40ms
step:1946/2330 train_time:113655ms step_avg:58.40ms
step:1947/2330 train_time:113712ms step_avg:58.40ms
step:1948/2330 train_time:113773ms step_avg:58.41ms
step:1949/2330 train_time:113830ms step_avg:58.40ms
step:1950/2330 train_time:113891ms step_avg:58.41ms
step:1951/2330 train_time:113948ms step_avg:58.41ms
step:1952/2330 train_time:114009ms step_avg:58.41ms
step:1953/2330 train_time:114067ms step_avg:58.41ms
step:1954/2330 train_time:114128ms step_avg:58.41ms
step:1955/2330 train_time:114186ms step_avg:58.41ms
step:1956/2330 train_time:114247ms step_avg:58.41ms
step:1957/2330 train_time:114305ms step_avg:58.41ms
step:1958/2330 train_time:114365ms step_avg:58.41ms
step:1959/2330 train_time:114423ms step_avg:58.41ms
step:1960/2330 train_time:114483ms step_avg:58.41ms
step:1961/2330 train_time:114541ms step_avg:58.41ms
step:1962/2330 train_time:114602ms step_avg:58.41ms
step:1963/2330 train_time:114661ms step_avg:58.41ms
step:1964/2330 train_time:114722ms step_avg:58.41ms
step:1965/2330 train_time:114779ms step_avg:58.41ms
step:1966/2330 train_time:114840ms step_avg:58.41ms
step:1967/2330 train_time:114897ms step_avg:58.41ms
step:1968/2330 train_time:114959ms step_avg:58.41ms
step:1969/2330 train_time:115017ms step_avg:58.41ms
step:1970/2330 train_time:115077ms step_avg:58.41ms
step:1971/2330 train_time:115134ms step_avg:58.41ms
step:1972/2330 train_time:115195ms step_avg:58.42ms
step:1973/2330 train_time:115252ms step_avg:58.41ms
step:1974/2330 train_time:115314ms step_avg:58.42ms
step:1975/2330 train_time:115371ms step_avg:58.42ms
step:1976/2330 train_time:115432ms step_avg:58.42ms
step:1977/2330 train_time:115489ms step_avg:58.42ms
step:1978/2330 train_time:115549ms step_avg:58.42ms
step:1979/2330 train_time:115606ms step_avg:58.42ms
step:1980/2330 train_time:115667ms step_avg:58.42ms
step:1981/2330 train_time:115725ms step_avg:58.42ms
step:1982/2330 train_time:115785ms step_avg:58.42ms
step:1983/2330 train_time:115844ms step_avg:58.42ms
step:1984/2330 train_time:115905ms step_avg:58.42ms
step:1985/2330 train_time:115962ms step_avg:58.42ms
step:1986/2330 train_time:116023ms step_avg:58.42ms
step:1987/2330 train_time:116080ms step_avg:58.42ms
step:1988/2330 train_time:116142ms step_avg:58.42ms
step:1989/2330 train_time:116200ms step_avg:58.42ms
step:1990/2330 train_time:116261ms step_avg:58.42ms
step:1991/2330 train_time:116319ms step_avg:58.42ms
step:1992/2330 train_time:116380ms step_avg:58.42ms
step:1993/2330 train_time:116437ms step_avg:58.42ms
step:1994/2330 train_time:116498ms step_avg:58.42ms
step:1995/2330 train_time:116555ms step_avg:58.42ms
step:1996/2330 train_time:116616ms step_avg:58.43ms
step:1997/2330 train_time:116674ms step_avg:58.42ms
step:1998/2330 train_time:116734ms step_avg:58.43ms
step:1999/2330 train_time:116791ms step_avg:58.42ms
step:2000/2330 train_time:116854ms step_avg:58.43ms
step:2000/2330 val_loss:3.7569 train_time:116934ms step_avg:58.47ms
step:2001/2330 train_time:116953ms step_avg:58.45ms
step:2002/2330 train_time:116973ms step_avg:58.43ms
step:2003/2330 train_time:117030ms step_avg:58.43ms
step:2004/2330 train_time:117097ms step_avg:58.43ms
step:2005/2330 train_time:117155ms step_avg:58.43ms
step:2006/2330 train_time:117216ms step_avg:58.43ms
step:2007/2330 train_time:117273ms step_avg:58.43ms
step:2008/2330 train_time:117333ms step_avg:58.43ms
step:2009/2330 train_time:117391ms step_avg:58.43ms
step:2010/2330 train_time:117450ms step_avg:58.43ms
step:2011/2330 train_time:117507ms step_avg:58.43ms
step:2012/2330 train_time:117567ms step_avg:58.43ms
step:2013/2330 train_time:117623ms step_avg:58.43ms
step:2014/2330 train_time:117683ms step_avg:58.43ms
step:2015/2330 train_time:117740ms step_avg:58.43ms
step:2016/2330 train_time:117800ms step_avg:58.43ms
step:2017/2330 train_time:117858ms step_avg:58.43ms
step:2018/2330 train_time:117918ms step_avg:58.43ms
step:2019/2330 train_time:117977ms step_avg:58.43ms
step:2020/2330 train_time:118040ms step_avg:58.44ms
step:2021/2330 train_time:118098ms step_avg:58.44ms
step:2022/2330 train_time:118160ms step_avg:58.44ms
step:2023/2330 train_time:118218ms step_avg:58.44ms
step:2024/2330 train_time:118279ms step_avg:58.44ms
step:2025/2330 train_time:118336ms step_avg:58.44ms
step:2026/2330 train_time:118398ms step_avg:58.44ms
step:2027/2330 train_time:118456ms step_avg:58.44ms
step:2028/2330 train_time:118515ms step_avg:58.44ms
step:2029/2330 train_time:118573ms step_avg:58.44ms
step:2030/2330 train_time:118633ms step_avg:58.44ms
step:2031/2330 train_time:118689ms step_avg:58.44ms
step:2032/2330 train_time:118749ms step_avg:58.44ms
step:2033/2330 train_time:118806ms step_avg:58.44ms
step:2034/2330 train_time:118867ms step_avg:58.44ms
step:2035/2330 train_time:118924ms step_avg:58.44ms
step:2036/2330 train_time:118988ms step_avg:58.44ms
step:2037/2330 train_time:119045ms step_avg:58.44ms
step:2038/2330 train_time:119108ms step_avg:58.44ms
step:2039/2330 train_time:119165ms step_avg:58.44ms
step:2040/2330 train_time:119226ms step_avg:58.44ms
step:2041/2330 train_time:119283ms step_avg:58.44ms
step:2042/2330 train_time:119344ms step_avg:58.44ms
step:2043/2330 train_time:119401ms step_avg:58.44ms
step:2044/2330 train_time:119461ms step_avg:58.44ms
step:2045/2330 train_time:119518ms step_avg:58.44ms
step:2046/2330 train_time:119578ms step_avg:58.44ms
step:2047/2330 train_time:119636ms step_avg:58.44ms
step:2048/2330 train_time:119696ms step_avg:58.45ms
step:2049/2330 train_time:119753ms step_avg:58.44ms
step:2050/2330 train_time:119814ms step_avg:58.45ms
step:2051/2330 train_time:119873ms step_avg:58.45ms
step:2052/2330 train_time:119934ms step_avg:58.45ms
step:2053/2330 train_time:119992ms step_avg:58.45ms
step:2054/2330 train_time:120053ms step_avg:58.45ms
step:2055/2330 train_time:120110ms step_avg:58.45ms
step:2056/2330 train_time:120173ms step_avg:58.45ms
step:2057/2330 train_time:120230ms step_avg:58.45ms
step:2058/2330 train_time:120291ms step_avg:58.45ms
step:2059/2330 train_time:120348ms step_avg:58.45ms
step:2060/2330 train_time:120410ms step_avg:58.45ms
step:2061/2330 train_time:120466ms step_avg:58.45ms
step:2062/2330 train_time:120527ms step_avg:58.45ms
step:2063/2330 train_time:120584ms step_avg:58.45ms
step:2064/2330 train_time:120644ms step_avg:58.45ms
step:2065/2330 train_time:120701ms step_avg:58.45ms
step:2066/2330 train_time:120761ms step_avg:58.45ms
step:2067/2330 train_time:120819ms step_avg:58.45ms
step:2068/2330 train_time:120880ms step_avg:58.45ms
step:2069/2330 train_time:120938ms step_avg:58.45ms
step:2070/2330 train_time:121000ms step_avg:58.45ms
step:2071/2330 train_time:121059ms step_avg:58.45ms
step:2072/2330 train_time:121119ms step_avg:58.46ms
step:2073/2330 train_time:121177ms step_avg:58.46ms
step:2074/2330 train_time:121238ms step_avg:58.46ms
step:2075/2330 train_time:121295ms step_avg:58.46ms
step:2076/2330 train_time:121356ms step_avg:58.46ms
step:2077/2330 train_time:121414ms step_avg:58.46ms
step:2078/2330 train_time:121475ms step_avg:58.46ms
step:2079/2330 train_time:121532ms step_avg:58.46ms
step:2080/2330 train_time:121593ms step_avg:58.46ms
step:2081/2330 train_time:121649ms step_avg:58.46ms
step:2082/2330 train_time:121710ms step_avg:58.46ms
step:2083/2330 train_time:121767ms step_avg:58.46ms
step:2084/2330 train_time:121828ms step_avg:58.46ms
step:2085/2330 train_time:121885ms step_avg:58.46ms
step:2086/2330 train_time:121946ms step_avg:58.46ms
step:2087/2330 train_time:122003ms step_avg:58.46ms
step:2088/2330 train_time:122063ms step_avg:58.46ms
step:2089/2330 train_time:122120ms step_avg:58.46ms
step:2090/2330 train_time:122181ms step_avg:58.46ms
step:2091/2330 train_time:122239ms step_avg:58.46ms
step:2092/2330 train_time:122300ms step_avg:58.46ms
step:2093/2330 train_time:122358ms step_avg:58.46ms
step:2094/2330 train_time:122419ms step_avg:58.46ms
step:2095/2330 train_time:122477ms step_avg:58.46ms
step:2096/2330 train_time:122537ms step_avg:58.46ms
step:2097/2330 train_time:122595ms step_avg:58.46ms
step:2098/2330 train_time:122655ms step_avg:58.46ms
step:2099/2330 train_time:122714ms step_avg:58.46ms
step:2100/2330 train_time:122775ms step_avg:58.46ms
step:2101/2330 train_time:122832ms step_avg:58.46ms
step:2102/2330 train_time:122892ms step_avg:58.46ms
step:2103/2330 train_time:122950ms step_avg:58.46ms
step:2104/2330 train_time:123011ms step_avg:58.47ms
step:2105/2330 train_time:123069ms step_avg:58.46ms
step:2106/2330 train_time:123129ms step_avg:58.47ms
step:2107/2330 train_time:123185ms step_avg:58.46ms
step:2108/2330 train_time:123248ms step_avg:58.47ms
step:2109/2330 train_time:123305ms step_avg:58.47ms
step:2110/2330 train_time:123366ms step_avg:58.47ms
step:2111/2330 train_time:123423ms step_avg:58.47ms
step:2112/2330 train_time:123484ms step_avg:58.47ms
step:2113/2330 train_time:123541ms step_avg:58.47ms
step:2114/2330 train_time:123602ms step_avg:58.47ms
step:2115/2330 train_time:123659ms step_avg:58.47ms
step:2116/2330 train_time:123721ms step_avg:58.47ms
step:2117/2330 train_time:123778ms step_avg:58.47ms
step:2118/2330 train_time:123839ms step_avg:58.47ms
step:2119/2330 train_time:123897ms step_avg:58.47ms
step:2120/2330 train_time:123958ms step_avg:58.47ms
step:2121/2330 train_time:124015ms step_avg:58.47ms
step:2122/2330 train_time:124076ms step_avg:58.47ms
step:2123/2330 train_time:124134ms step_avg:58.47ms
step:2124/2330 train_time:124195ms step_avg:58.47ms
step:2125/2330 train_time:124252ms step_avg:58.47ms
step:2126/2330 train_time:124314ms step_avg:58.47ms
step:2127/2330 train_time:124371ms step_avg:58.47ms
step:2128/2330 train_time:124433ms step_avg:58.47ms
step:2129/2330 train_time:124490ms step_avg:58.47ms
step:2130/2330 train_time:124552ms step_avg:58.48ms
step:2131/2330 train_time:124609ms step_avg:58.47ms
step:2132/2330 train_time:124670ms step_avg:58.48ms
step:2133/2330 train_time:124727ms step_avg:58.48ms
step:2134/2330 train_time:124787ms step_avg:58.48ms
step:2135/2330 train_time:124844ms step_avg:58.47ms
step:2136/2330 train_time:124905ms step_avg:58.48ms
step:2137/2330 train_time:124961ms step_avg:58.48ms
step:2138/2330 train_time:125022ms step_avg:58.48ms
step:2139/2330 train_time:125079ms step_avg:58.48ms
step:2140/2330 train_time:125141ms step_avg:58.48ms
step:2141/2330 train_time:125198ms step_avg:58.48ms
step:2142/2330 train_time:125260ms step_avg:58.48ms
step:2143/2330 train_time:125318ms step_avg:58.48ms
step:2144/2330 train_time:125378ms step_avg:58.48ms
step:2145/2330 train_time:125436ms step_avg:58.48ms
step:2146/2330 train_time:125496ms step_avg:58.48ms
step:2147/2330 train_time:125554ms step_avg:58.48ms
step:2148/2330 train_time:125616ms step_avg:58.48ms
step:2149/2330 train_time:125673ms step_avg:58.48ms
step:2150/2330 train_time:125733ms step_avg:58.48ms
step:2151/2330 train_time:125791ms step_avg:58.48ms
step:2152/2330 train_time:125851ms step_avg:58.48ms
step:2153/2330 train_time:125908ms step_avg:58.48ms
step:2154/2330 train_time:125969ms step_avg:58.48ms
step:2155/2330 train_time:126026ms step_avg:58.48ms
step:2156/2330 train_time:126087ms step_avg:58.48ms
step:2157/2330 train_time:126143ms step_avg:58.48ms
step:2158/2330 train_time:126206ms step_avg:58.48ms
step:2159/2330 train_time:126263ms step_avg:58.48ms
step:2160/2330 train_time:126324ms step_avg:58.48ms
step:2161/2330 train_time:126381ms step_avg:58.48ms
step:2162/2330 train_time:126443ms step_avg:58.48ms
step:2163/2330 train_time:126499ms step_avg:58.48ms
step:2164/2330 train_time:126561ms step_avg:58.48ms
step:2165/2330 train_time:126619ms step_avg:58.48ms
step:2166/2330 train_time:126679ms step_avg:58.49ms
step:2167/2330 train_time:126736ms step_avg:58.48ms
step:2168/2330 train_time:126797ms step_avg:58.49ms
step:2169/2330 train_time:126855ms step_avg:58.49ms
step:2170/2330 train_time:126916ms step_avg:58.49ms
step:2171/2330 train_time:126974ms step_avg:58.49ms
step:2172/2330 train_time:127035ms step_avg:58.49ms
step:2173/2330 train_time:127093ms step_avg:58.49ms
step:2174/2330 train_time:127155ms step_avg:58.49ms
step:2175/2330 train_time:127213ms step_avg:58.49ms
step:2176/2330 train_time:127274ms step_avg:58.49ms
step:2177/2330 train_time:127331ms step_avg:58.49ms
step:2178/2330 train_time:127393ms step_avg:58.49ms
step:2179/2330 train_time:127450ms step_avg:58.49ms
step:2180/2330 train_time:127512ms step_avg:58.49ms
step:2181/2330 train_time:127569ms step_avg:58.49ms
step:2182/2330 train_time:127629ms step_avg:58.49ms
step:2183/2330 train_time:127686ms step_avg:58.49ms
step:2184/2330 train_time:127747ms step_avg:58.49ms
step:2185/2330 train_time:127804ms step_avg:58.49ms
step:2186/2330 train_time:127864ms step_avg:58.49ms
step:2187/2330 train_time:127921ms step_avg:58.49ms
step:2188/2330 train_time:127983ms step_avg:58.49ms
step:2189/2330 train_time:128040ms step_avg:58.49ms
step:2190/2330 train_time:128100ms step_avg:58.49ms
step:2191/2330 train_time:128159ms step_avg:58.49ms
step:2192/2330 train_time:128219ms step_avg:58.49ms
step:2193/2330 train_time:128276ms step_avg:58.49ms
step:2194/2330 train_time:128338ms step_avg:58.50ms
step:2195/2330 train_time:128395ms step_avg:58.49ms
step:2196/2330 train_time:128456ms step_avg:58.50ms
step:2197/2330 train_time:128513ms step_avg:58.49ms
step:2198/2330 train_time:128574ms step_avg:58.50ms
step:2199/2330 train_time:128633ms step_avg:58.50ms
step:2200/2330 train_time:128693ms step_avg:58.50ms
step:2201/2330 train_time:128752ms step_avg:58.50ms
step:2202/2330 train_time:128812ms step_avg:58.50ms
step:2203/2330 train_time:128869ms step_avg:58.50ms
step:2204/2330 train_time:128930ms step_avg:58.50ms
step:2205/2330 train_time:128987ms step_avg:58.50ms
step:2206/2330 train_time:129048ms step_avg:58.50ms
step:2207/2330 train_time:129104ms step_avg:58.50ms
step:2208/2330 train_time:129167ms step_avg:58.50ms
step:2209/2330 train_time:129223ms step_avg:58.50ms
step:2210/2330 train_time:129284ms step_avg:58.50ms
step:2211/2330 train_time:129341ms step_avg:58.50ms
step:2212/2330 train_time:129403ms step_avg:58.50ms
step:2213/2330 train_time:129460ms step_avg:58.50ms
step:2214/2330 train_time:129520ms step_avg:58.50ms
step:2215/2330 train_time:129577ms step_avg:58.50ms
step:2216/2330 train_time:129638ms step_avg:58.50ms
step:2217/2330 train_time:129695ms step_avg:58.50ms
step:2218/2330 train_time:129757ms step_avg:58.50ms
step:2219/2330 train_time:129815ms step_avg:58.50ms
step:2220/2330 train_time:129875ms step_avg:58.50ms
step:2221/2330 train_time:129932ms step_avg:58.50ms
step:2222/2330 train_time:129994ms step_avg:58.50ms
step:2223/2330 train_time:130052ms step_avg:58.50ms
step:2224/2330 train_time:130112ms step_avg:58.50ms
step:2225/2330 train_time:130169ms step_avg:58.50ms
step:2226/2330 train_time:130231ms step_avg:58.50ms
step:2227/2330 train_time:130288ms step_avg:58.50ms
step:2228/2330 train_time:130350ms step_avg:58.51ms
step:2229/2330 train_time:130407ms step_avg:58.50ms
step:2230/2330 train_time:130468ms step_avg:58.51ms
step:2231/2330 train_time:130525ms step_avg:58.50ms
step:2232/2330 train_time:130586ms step_avg:58.51ms
step:2233/2330 train_time:130643ms step_avg:58.51ms
step:2234/2330 train_time:130703ms step_avg:58.51ms
step:2235/2330 train_time:130760ms step_avg:58.51ms
step:2236/2330 train_time:130821ms step_avg:58.51ms
step:2237/2330 train_time:130878ms step_avg:58.51ms
step:2238/2330 train_time:130940ms step_avg:58.51ms
step:2239/2330 train_time:130997ms step_avg:58.51ms
step:2240/2330 train_time:131059ms step_avg:58.51ms
step:2241/2330 train_time:131117ms step_avg:58.51ms
step:2242/2330 train_time:131178ms step_avg:58.51ms
step:2243/2330 train_time:131235ms step_avg:58.51ms
step:2244/2330 train_time:131297ms step_avg:58.51ms
step:2245/2330 train_time:131354ms step_avg:58.51ms
step:2246/2330 train_time:131416ms step_avg:58.51ms
step:2247/2330 train_time:131473ms step_avg:58.51ms
step:2248/2330 train_time:131534ms step_avg:58.51ms
step:2249/2330 train_time:131591ms step_avg:58.51ms
step:2250/2330 train_time:131653ms step_avg:58.51ms
step:2250/2330 val_loss:3.7086 train_time:131735ms step_avg:58.55ms
step:2251/2330 train_time:131753ms step_avg:58.53ms
step:2252/2330 train_time:131774ms step_avg:58.51ms
step:2253/2330 train_time:131832ms step_avg:58.51ms
step:2254/2330 train_time:131900ms step_avg:58.52ms
step:2255/2330 train_time:131957ms step_avg:58.52ms
step:2256/2330 train_time:132020ms step_avg:58.52ms
step:2257/2330 train_time:132078ms step_avg:58.52ms
step:2258/2330 train_time:132138ms step_avg:58.52ms
step:2259/2330 train_time:132196ms step_avg:58.52ms
step:2260/2330 train_time:132255ms step_avg:58.52ms
step:2261/2330 train_time:132312ms step_avg:58.52ms
step:2262/2330 train_time:132372ms step_avg:58.52ms
step:2263/2330 train_time:132428ms step_avg:58.52ms
step:2264/2330 train_time:132490ms step_avg:58.52ms
step:2265/2330 train_time:132546ms step_avg:58.52ms
step:2266/2330 train_time:132608ms step_avg:58.52ms
step:2267/2330 train_time:132665ms step_avg:58.52ms
step:2268/2330 train_time:132726ms step_avg:58.52ms
step:2269/2330 train_time:132784ms step_avg:58.52ms
step:2270/2330 train_time:132846ms step_avg:58.52ms
step:2271/2330 train_time:132903ms step_avg:58.52ms
step:2272/2330 train_time:132966ms step_avg:58.52ms
step:2273/2330 train_time:133023ms step_avg:58.52ms
step:2274/2330 train_time:133084ms step_avg:58.52ms
step:2275/2330 train_time:133141ms step_avg:58.52ms
step:2276/2330 train_time:133201ms step_avg:58.52ms
step:2277/2330 train_time:133258ms step_avg:58.52ms
step:2278/2330 train_time:133318ms step_avg:58.52ms
step:2279/2330 train_time:133375ms step_avg:58.52ms
step:2280/2330 train_time:133436ms step_avg:58.52ms
step:2281/2330 train_time:133494ms step_avg:58.52ms
step:2282/2330 train_time:133554ms step_avg:58.52ms
step:2283/2330 train_time:133611ms step_avg:58.52ms
step:2284/2330 train_time:133671ms step_avg:58.53ms
step:2285/2330 train_time:133730ms step_avg:58.53ms
step:2286/2330 train_time:133791ms step_avg:58.53ms
step:2287/2330 train_time:133849ms step_avg:58.53ms
step:2288/2330 train_time:133912ms step_avg:58.53ms
step:2289/2330 train_time:133968ms step_avg:58.53ms
step:2290/2330 train_time:134033ms step_avg:58.53ms
step:2291/2330 train_time:134090ms step_avg:58.53ms
step:2292/2330 train_time:134152ms step_avg:58.53ms
step:2293/2330 train_time:134208ms step_avg:58.53ms
step:2294/2330 train_time:134270ms step_avg:58.53ms
step:2295/2330 train_time:134327ms step_avg:58.53ms
step:2296/2330 train_time:134387ms step_avg:58.53ms
step:2297/2330 train_time:134444ms step_avg:58.53ms
step:2298/2330 train_time:134504ms step_avg:58.53ms
step:2299/2330 train_time:134561ms step_avg:58.53ms
step:2300/2330 train_time:134621ms step_avg:58.53ms
step:2301/2330 train_time:134679ms step_avg:58.53ms
step:2302/2330 train_time:134740ms step_avg:58.53ms
step:2303/2330 train_time:134799ms step_avg:58.53ms
step:2304/2330 train_time:134859ms step_avg:58.53ms
step:2305/2330 train_time:134917ms step_avg:58.53ms
step:2306/2330 train_time:134978ms step_avg:58.53ms
step:2307/2330 train_time:135037ms step_avg:58.53ms
step:2308/2330 train_time:135097ms step_avg:58.53ms
step:2309/2330 train_time:135155ms step_avg:58.53ms
step:2310/2330 train_time:135216ms step_avg:58.54ms
step:2311/2330 train_time:135274ms step_avg:58.53ms
step:2312/2330 train_time:135335ms step_avg:58.54ms
step:2313/2330 train_time:135392ms step_avg:58.54ms
step:2314/2330 train_time:135453ms step_avg:58.54ms
step:2315/2330 train_time:135511ms step_avg:58.54ms
step:2316/2330 train_time:135571ms step_avg:58.54ms
step:2317/2330 train_time:135627ms step_avg:58.54ms
step:2318/2330 train_time:135690ms step_avg:58.54ms
step:2319/2330 train_time:135747ms step_avg:58.54ms
step:2320/2330 train_time:135808ms step_avg:58.54ms
step:2321/2330 train_time:135866ms step_avg:58.54ms
step:2322/2330 train_time:135926ms step_avg:58.54ms
step:2323/2330 train_time:135983ms step_avg:58.54ms
step:2324/2330 train_time:136044ms step_avg:58.54ms
step:2325/2330 train_time:136101ms step_avg:58.54ms
step:2326/2330 train_time:136163ms step_avg:58.54ms
step:2327/2330 train_time:136220ms step_avg:58.54ms
step:2328/2330 train_time:136281ms step_avg:58.54ms
step:2329/2330 train_time:136339ms step_avg:58.54ms
step:2330/2330 train_time:136399ms step_avg:58.54ms
step:2330/2330 val_loss:3.6928 train_time:136481ms step_avg:58.58ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
