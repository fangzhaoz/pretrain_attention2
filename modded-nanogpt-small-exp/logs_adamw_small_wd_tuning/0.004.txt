import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:49:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:80ms step_avg:80.05ms
step:2/2330 train_time:172ms step_avg:85.95ms
step:3/2330 train_time:190ms step_avg:63.17ms
step:4/2330 train_time:209ms step_avg:52.19ms
step:5/2330 train_time:263ms step_avg:52.54ms
step:6/2330 train_time:322ms step_avg:53.59ms
step:7/2330 train_time:377ms step_avg:53.89ms
step:8/2330 train_time:436ms step_avg:54.45ms
step:9/2330 train_time:491ms step_avg:54.57ms
step:10/2330 train_time:549ms step_avg:54.92ms
step:11/2330 train_time:604ms step_avg:54.93ms
step:12/2330 train_time:663ms step_avg:55.21ms
step:13/2330 train_time:718ms step_avg:55.23ms
step:14/2330 train_time:776ms step_avg:55.46ms
step:15/2330 train_time:832ms step_avg:55.48ms
step:16/2330 train_time:890ms step_avg:55.64ms
step:17/2330 train_time:946ms step_avg:55.62ms
step:18/2330 train_time:1005ms step_avg:55.82ms
step:19/2330 train_time:1062ms step_avg:55.91ms
step:20/2330 train_time:1124ms step_avg:56.22ms
step:21/2330 train_time:1182ms step_avg:56.27ms
step:22/2330 train_time:1242ms step_avg:56.45ms
step:23/2330 train_time:1298ms step_avg:56.42ms
step:24/2330 train_time:1356ms step_avg:56.51ms
step:25/2330 train_time:1412ms step_avg:56.50ms
step:26/2330 train_time:1471ms step_avg:56.57ms
step:27/2330 train_time:1526ms step_avg:56.53ms
step:28/2330 train_time:1585ms step_avg:56.59ms
step:29/2330 train_time:1640ms step_avg:56.56ms
step:30/2330 train_time:1699ms step_avg:56.63ms
step:31/2330 train_time:1754ms step_avg:56.59ms
step:32/2330 train_time:1812ms step_avg:56.62ms
step:33/2330 train_time:1867ms step_avg:56.57ms
step:34/2330 train_time:1928ms step_avg:56.70ms
step:35/2330 train_time:1984ms step_avg:56.67ms
step:36/2330 train_time:2045ms step_avg:56.80ms
step:37/2330 train_time:2101ms step_avg:56.79ms
step:38/2330 train_time:2161ms step_avg:56.87ms
step:39/2330 train_time:2217ms step_avg:56.85ms
step:40/2330 train_time:2278ms step_avg:56.94ms
step:41/2330 train_time:2334ms step_avg:56.92ms
step:42/2330 train_time:2393ms step_avg:56.97ms
step:43/2330 train_time:2450ms step_avg:56.97ms
step:44/2330 train_time:2508ms step_avg:57.00ms
step:45/2330 train_time:2564ms step_avg:56.97ms
step:46/2330 train_time:2622ms step_avg:57.01ms
step:47/2330 train_time:2678ms step_avg:56.98ms
step:48/2330 train_time:2737ms step_avg:57.02ms
step:49/2330 train_time:2793ms step_avg:57.00ms
step:50/2330 train_time:2851ms step_avg:57.02ms
step:51/2330 train_time:2907ms step_avg:56.99ms
step:52/2330 train_time:2966ms step_avg:57.04ms
step:53/2330 train_time:3022ms step_avg:57.02ms
step:54/2330 train_time:3082ms step_avg:57.08ms
step:55/2330 train_time:3138ms step_avg:57.05ms
step:56/2330 train_time:3198ms step_avg:57.11ms
step:57/2330 train_time:3254ms step_avg:57.09ms
step:58/2330 train_time:3315ms step_avg:57.15ms
step:59/2330 train_time:3371ms step_avg:57.13ms
step:60/2330 train_time:3429ms step_avg:57.16ms
step:61/2330 train_time:3485ms step_avg:57.13ms
step:62/2330 train_time:3544ms step_avg:57.17ms
step:63/2330 train_time:3600ms step_avg:57.14ms
step:64/2330 train_time:3658ms step_avg:57.16ms
step:65/2330 train_time:3714ms step_avg:57.14ms
step:66/2330 train_time:3772ms step_avg:57.16ms
step:67/2330 train_time:3828ms step_avg:57.14ms
step:68/2330 train_time:3887ms step_avg:57.16ms
step:69/2330 train_time:3943ms step_avg:57.14ms
step:70/2330 train_time:4003ms step_avg:57.18ms
step:71/2330 train_time:4058ms step_avg:57.16ms
step:72/2330 train_time:4118ms step_avg:57.19ms
step:73/2330 train_time:4174ms step_avg:57.17ms
step:74/2330 train_time:4234ms step_avg:57.21ms
step:75/2330 train_time:4290ms step_avg:57.20ms
step:76/2330 train_time:4349ms step_avg:57.22ms
step:77/2330 train_time:4405ms step_avg:57.21ms
step:78/2330 train_time:4464ms step_avg:57.23ms
step:79/2330 train_time:4520ms step_avg:57.21ms
step:80/2330 train_time:4579ms step_avg:57.23ms
step:81/2330 train_time:4634ms step_avg:57.21ms
step:82/2330 train_time:4693ms step_avg:57.24ms
step:83/2330 train_time:4749ms step_avg:57.22ms
step:84/2330 train_time:4808ms step_avg:57.23ms
step:85/2330 train_time:4863ms step_avg:57.22ms
step:86/2330 train_time:4922ms step_avg:57.23ms
step:87/2330 train_time:4978ms step_avg:57.22ms
step:88/2330 train_time:5038ms step_avg:57.25ms
step:89/2330 train_time:5094ms step_avg:57.24ms
step:90/2330 train_time:5154ms step_avg:57.26ms
step:91/2330 train_time:5210ms step_avg:57.25ms
step:92/2330 train_time:5269ms step_avg:57.27ms
step:93/2330 train_time:5325ms step_avg:57.26ms
step:94/2330 train_time:5384ms step_avg:57.28ms
step:95/2330 train_time:5440ms step_avg:57.26ms
step:96/2330 train_time:5499ms step_avg:57.28ms
step:97/2330 train_time:5554ms step_avg:57.26ms
step:98/2330 train_time:5613ms step_avg:57.28ms
step:99/2330 train_time:5669ms step_avg:57.27ms
step:100/2330 train_time:5728ms step_avg:57.28ms
step:101/2330 train_time:5783ms step_avg:57.26ms
step:102/2330 train_time:5842ms step_avg:57.28ms
step:103/2330 train_time:5898ms step_avg:57.26ms
step:104/2330 train_time:5956ms step_avg:57.27ms
step:105/2330 train_time:6012ms step_avg:57.26ms
step:106/2330 train_time:6072ms step_avg:57.28ms
step:107/2330 train_time:6128ms step_avg:57.27ms
step:108/2330 train_time:6187ms step_avg:57.29ms
step:109/2330 train_time:6243ms step_avg:57.28ms
step:110/2330 train_time:6302ms step_avg:57.29ms
step:111/2330 train_time:6358ms step_avg:57.28ms
step:112/2330 train_time:6417ms step_avg:57.30ms
step:113/2330 train_time:6473ms step_avg:57.28ms
step:114/2330 train_time:6532ms step_avg:57.30ms
step:115/2330 train_time:6587ms step_avg:57.28ms
step:116/2330 train_time:6647ms step_avg:57.30ms
step:117/2330 train_time:6702ms step_avg:57.28ms
step:118/2330 train_time:6761ms step_avg:57.30ms
step:119/2330 train_time:6817ms step_avg:57.29ms
step:120/2330 train_time:6875ms step_avg:57.29ms
step:121/2330 train_time:6931ms step_avg:57.28ms
step:122/2330 train_time:6991ms step_avg:57.30ms
step:123/2330 train_time:7046ms step_avg:57.28ms
step:124/2330 train_time:7106ms step_avg:57.31ms
step:125/2330 train_time:7162ms step_avg:57.30ms
step:126/2330 train_time:7222ms step_avg:57.31ms
step:127/2330 train_time:7277ms step_avg:57.30ms
step:128/2330 train_time:7338ms step_avg:57.33ms
step:129/2330 train_time:7394ms step_avg:57.32ms
step:130/2330 train_time:7453ms step_avg:57.33ms
step:131/2330 train_time:7509ms step_avg:57.32ms
step:132/2330 train_time:7568ms step_avg:57.33ms
step:133/2330 train_time:7624ms step_avg:57.32ms
step:134/2330 train_time:7682ms step_avg:57.33ms
step:135/2330 train_time:7738ms step_avg:57.32ms
step:136/2330 train_time:7797ms step_avg:57.33ms
step:137/2330 train_time:7853ms step_avg:57.32ms
step:138/2330 train_time:7912ms step_avg:57.33ms
step:139/2330 train_time:7967ms step_avg:57.32ms
step:140/2330 train_time:8026ms step_avg:57.33ms
step:141/2330 train_time:8082ms step_avg:57.32ms
step:142/2330 train_time:8141ms step_avg:57.33ms
step:143/2330 train_time:8197ms step_avg:57.32ms
step:144/2330 train_time:8256ms step_avg:57.33ms
step:145/2330 train_time:8312ms step_avg:57.32ms
step:146/2330 train_time:8371ms step_avg:57.34ms
step:147/2330 train_time:8428ms step_avg:57.33ms
step:148/2330 train_time:8487ms step_avg:57.34ms
step:149/2330 train_time:8543ms step_avg:57.34ms
step:150/2330 train_time:8601ms step_avg:57.34ms
step:151/2330 train_time:8657ms step_avg:57.33ms
step:152/2330 train_time:8717ms step_avg:57.35ms
step:153/2330 train_time:8773ms step_avg:57.34ms
step:154/2330 train_time:8832ms step_avg:57.35ms
step:155/2330 train_time:8888ms step_avg:57.34ms
step:156/2330 train_time:8946ms step_avg:57.35ms
step:157/2330 train_time:9002ms step_avg:57.34ms
step:158/2330 train_time:9061ms step_avg:57.35ms
step:159/2330 train_time:9117ms step_avg:57.34ms
step:160/2330 train_time:9176ms step_avg:57.35ms
step:161/2330 train_time:9232ms step_avg:57.34ms
step:162/2330 train_time:9291ms step_avg:57.35ms
step:163/2330 train_time:9346ms step_avg:57.34ms
step:164/2330 train_time:9406ms step_avg:57.35ms
step:165/2330 train_time:9461ms step_avg:57.34ms
step:166/2330 train_time:9520ms step_avg:57.35ms
step:167/2330 train_time:9575ms step_avg:57.34ms
step:168/2330 train_time:9635ms step_avg:57.35ms
step:169/2330 train_time:9691ms step_avg:57.34ms
step:170/2330 train_time:9750ms step_avg:57.36ms
step:171/2330 train_time:9806ms step_avg:57.34ms
step:172/2330 train_time:9865ms step_avg:57.36ms
step:173/2330 train_time:9921ms step_avg:57.35ms
step:174/2330 train_time:9980ms step_avg:57.36ms
step:175/2330 train_time:10036ms step_avg:57.35ms
step:176/2330 train_time:10095ms step_avg:57.36ms
step:177/2330 train_time:10152ms step_avg:57.36ms
step:178/2330 train_time:10211ms step_avg:57.36ms
step:179/2330 train_time:10266ms step_avg:57.35ms
step:180/2330 train_time:10325ms step_avg:57.36ms
step:181/2330 train_time:10381ms step_avg:57.35ms
step:182/2330 train_time:10440ms step_avg:57.36ms
step:183/2330 train_time:10495ms step_avg:57.35ms
step:184/2330 train_time:10555ms step_avg:57.36ms
step:185/2330 train_time:10610ms step_avg:57.35ms
step:186/2330 train_time:10669ms step_avg:57.36ms
step:187/2330 train_time:10726ms step_avg:57.36ms
step:188/2330 train_time:10784ms step_avg:57.36ms
step:189/2330 train_time:10840ms step_avg:57.36ms
step:190/2330 train_time:10899ms step_avg:57.36ms
step:191/2330 train_time:10954ms step_avg:57.35ms
step:192/2330 train_time:11013ms step_avg:57.36ms
step:193/2330 train_time:11070ms step_avg:57.36ms
step:194/2330 train_time:11129ms step_avg:57.36ms
step:195/2330 train_time:11185ms step_avg:57.36ms
step:196/2330 train_time:11245ms step_avg:57.37ms
step:197/2330 train_time:11301ms step_avg:57.37ms
step:198/2330 train_time:11360ms step_avg:57.37ms
step:199/2330 train_time:11416ms step_avg:57.37ms
step:200/2330 train_time:11474ms step_avg:57.37ms
step:201/2330 train_time:11530ms step_avg:57.36ms
step:202/2330 train_time:11589ms step_avg:57.37ms
step:203/2330 train_time:11645ms step_avg:57.37ms
step:204/2330 train_time:11704ms step_avg:57.37ms
step:205/2330 train_time:11759ms step_avg:57.36ms
step:206/2330 train_time:11818ms step_avg:57.37ms
step:207/2330 train_time:11874ms step_avg:57.36ms
step:208/2330 train_time:11933ms step_avg:57.37ms
step:209/2330 train_time:11989ms step_avg:57.36ms
step:210/2330 train_time:12048ms step_avg:57.37ms
step:211/2330 train_time:12104ms step_avg:57.36ms
step:212/2330 train_time:12163ms step_avg:57.37ms
step:213/2330 train_time:12219ms step_avg:57.37ms
step:214/2330 train_time:12278ms step_avg:57.38ms
step:215/2330 train_time:12334ms step_avg:57.37ms
step:216/2330 train_time:12393ms step_avg:57.38ms
step:217/2330 train_time:12449ms step_avg:57.37ms
step:218/2330 train_time:12509ms step_avg:57.38ms
step:219/2330 train_time:12565ms step_avg:57.37ms
step:220/2330 train_time:12624ms step_avg:57.38ms
step:221/2330 train_time:12680ms step_avg:57.38ms
step:222/2330 train_time:12740ms step_avg:57.39ms
step:223/2330 train_time:12796ms step_avg:57.38ms
step:224/2330 train_time:12854ms step_avg:57.39ms
step:225/2330 train_time:12910ms step_avg:57.38ms
step:226/2330 train_time:12969ms step_avg:57.39ms
step:227/2330 train_time:13025ms step_avg:57.38ms
step:228/2330 train_time:13084ms step_avg:57.38ms
step:229/2330 train_time:13140ms step_avg:57.38ms
step:230/2330 train_time:13199ms step_avg:57.39ms
step:231/2330 train_time:13255ms step_avg:57.38ms
step:232/2330 train_time:13313ms step_avg:57.39ms
step:233/2330 train_time:13369ms step_avg:57.38ms
step:234/2330 train_time:13429ms step_avg:57.39ms
step:235/2330 train_time:13484ms step_avg:57.38ms
step:236/2330 train_time:13545ms step_avg:57.39ms
step:237/2330 train_time:13600ms step_avg:57.38ms
step:238/2330 train_time:13660ms step_avg:57.40ms
step:239/2330 train_time:13716ms step_avg:57.39ms
step:240/2330 train_time:13774ms step_avg:57.39ms
step:241/2330 train_time:13830ms step_avg:57.39ms
step:242/2330 train_time:13889ms step_avg:57.39ms
step:243/2330 train_time:13945ms step_avg:57.39ms
step:244/2330 train_time:14005ms step_avg:57.40ms
step:245/2330 train_time:14060ms step_avg:57.39ms
step:246/2330 train_time:14120ms step_avg:57.40ms
step:247/2330 train_time:14176ms step_avg:57.39ms
step:248/2330 train_time:14235ms step_avg:57.40ms
step:249/2330 train_time:14290ms step_avg:57.39ms
step:250/2330 train_time:14350ms step_avg:57.40ms
step:250/2330 val_loss:4.8967 train_time:14429ms step_avg:57.72ms
step:251/2330 train_time:14447ms step_avg:57.56ms
step:252/2330 train_time:14467ms step_avg:57.41ms
step:253/2330 train_time:14522ms step_avg:57.40ms
step:254/2330 train_time:14586ms step_avg:57.42ms
step:255/2330 train_time:14642ms step_avg:57.42ms
step:256/2330 train_time:14704ms step_avg:57.44ms
step:257/2330 train_time:14760ms step_avg:57.43ms
step:258/2330 train_time:14821ms step_avg:57.45ms
step:259/2330 train_time:14876ms step_avg:57.44ms
step:260/2330 train_time:14936ms step_avg:57.45ms
step:261/2330 train_time:14991ms step_avg:57.44ms
step:262/2330 train_time:15049ms step_avg:57.44ms
step:263/2330 train_time:15105ms step_avg:57.43ms
step:264/2330 train_time:15163ms step_avg:57.44ms
step:265/2330 train_time:15218ms step_avg:57.43ms
step:266/2330 train_time:15277ms step_avg:57.43ms
step:267/2330 train_time:15333ms step_avg:57.43ms
step:268/2330 train_time:15393ms step_avg:57.44ms
step:269/2330 train_time:15449ms step_avg:57.43ms
step:270/2330 train_time:15508ms step_avg:57.44ms
step:271/2330 train_time:15565ms step_avg:57.44ms
step:272/2330 train_time:15626ms step_avg:57.45ms
step:273/2330 train_time:15681ms step_avg:57.44ms
step:274/2330 train_time:15742ms step_avg:57.45ms
step:275/2330 train_time:15797ms step_avg:57.44ms
step:276/2330 train_time:15858ms step_avg:57.46ms
step:277/2330 train_time:15914ms step_avg:57.45ms
step:278/2330 train_time:15973ms step_avg:57.46ms
step:279/2330 train_time:16028ms step_avg:57.45ms
step:280/2330 train_time:16087ms step_avg:57.45ms
step:281/2330 train_time:16142ms step_avg:57.45ms
step:282/2330 train_time:16202ms step_avg:57.45ms
step:283/2330 train_time:16257ms step_avg:57.45ms
step:284/2330 train_time:16316ms step_avg:57.45ms
step:285/2330 train_time:16372ms step_avg:57.45ms
step:286/2330 train_time:16431ms step_avg:57.45ms
step:287/2330 train_time:16488ms step_avg:57.45ms
step:288/2330 train_time:16548ms step_avg:57.46ms
step:289/2330 train_time:16604ms step_avg:57.45ms
step:290/2330 train_time:16664ms step_avg:57.46ms
step:291/2330 train_time:16720ms step_avg:57.46ms
step:292/2330 train_time:16779ms step_avg:57.46ms
step:293/2330 train_time:16835ms step_avg:57.46ms
step:294/2330 train_time:16894ms step_avg:57.46ms
step:295/2330 train_time:16949ms step_avg:57.46ms
step:296/2330 train_time:17009ms step_avg:57.46ms
step:297/2330 train_time:17065ms step_avg:57.46ms
step:298/2330 train_time:17123ms step_avg:57.46ms
step:299/2330 train_time:17179ms step_avg:57.45ms
step:300/2330 train_time:17237ms step_avg:57.46ms
step:301/2330 train_time:17293ms step_avg:57.45ms
step:302/2330 train_time:17352ms step_avg:57.46ms
step:303/2330 train_time:17408ms step_avg:57.45ms
step:304/2330 train_time:17467ms step_avg:57.46ms
step:305/2330 train_time:17523ms step_avg:57.45ms
step:306/2330 train_time:17582ms step_avg:57.46ms
step:307/2330 train_time:17638ms step_avg:57.45ms
step:308/2330 train_time:17698ms step_avg:57.46ms
step:309/2330 train_time:17754ms step_avg:57.46ms
step:310/2330 train_time:17813ms step_avg:57.46ms
step:311/2330 train_time:17870ms step_avg:57.46ms
step:312/2330 train_time:17929ms step_avg:57.46ms
step:313/2330 train_time:17985ms step_avg:57.46ms
step:314/2330 train_time:18044ms step_avg:57.47ms
step:315/2330 train_time:18100ms step_avg:57.46ms
step:316/2330 train_time:18158ms step_avg:57.46ms
step:317/2330 train_time:18214ms step_avg:57.46ms
step:318/2330 train_time:18274ms step_avg:57.46ms
step:319/2330 train_time:18329ms step_avg:57.46ms
step:320/2330 train_time:18388ms step_avg:57.46ms
step:321/2330 train_time:18444ms step_avg:57.46ms
step:322/2330 train_time:18503ms step_avg:57.46ms
step:323/2330 train_time:18559ms step_avg:57.46ms
step:324/2330 train_time:18619ms step_avg:57.46ms
step:325/2330 train_time:18675ms step_avg:57.46ms
step:326/2330 train_time:18734ms step_avg:57.47ms
step:327/2330 train_time:18790ms step_avg:57.46ms
step:328/2330 train_time:18849ms step_avg:57.47ms
step:329/2330 train_time:18905ms step_avg:57.46ms
step:330/2330 train_time:18964ms step_avg:57.47ms
step:331/2330 train_time:19020ms step_avg:57.46ms
step:332/2330 train_time:19079ms step_avg:57.47ms
step:333/2330 train_time:19135ms step_avg:57.46ms
step:334/2330 train_time:19194ms step_avg:57.47ms
step:335/2330 train_time:19250ms step_avg:57.46ms
step:336/2330 train_time:19308ms step_avg:57.46ms
step:337/2330 train_time:19363ms step_avg:57.46ms
step:338/2330 train_time:19424ms step_avg:57.47ms
step:339/2330 train_time:19480ms step_avg:57.46ms
step:340/2330 train_time:19538ms step_avg:57.47ms
step:341/2330 train_time:19594ms step_avg:57.46ms
step:342/2330 train_time:19653ms step_avg:57.47ms
step:343/2330 train_time:19710ms step_avg:57.46ms
step:344/2330 train_time:19770ms step_avg:57.47ms
step:345/2330 train_time:19826ms step_avg:57.47ms
step:346/2330 train_time:19886ms step_avg:57.47ms
step:347/2330 train_time:19941ms step_avg:57.47ms
step:348/2330 train_time:20000ms step_avg:57.47ms
step:349/2330 train_time:20055ms step_avg:57.46ms
step:350/2330 train_time:20115ms step_avg:57.47ms
step:351/2330 train_time:20171ms step_avg:57.47ms
step:352/2330 train_time:20230ms step_avg:57.47ms
step:353/2330 train_time:20286ms step_avg:57.47ms
step:354/2330 train_time:20345ms step_avg:57.47ms
step:355/2330 train_time:20401ms step_avg:57.47ms
step:356/2330 train_time:20460ms step_avg:57.47ms
step:357/2330 train_time:20515ms step_avg:57.47ms
step:358/2330 train_time:20575ms step_avg:57.47ms
step:359/2330 train_time:20631ms step_avg:57.47ms
step:360/2330 train_time:20691ms step_avg:57.47ms
step:361/2330 train_time:20747ms step_avg:57.47ms
step:362/2330 train_time:20806ms step_avg:57.48ms
step:363/2330 train_time:20862ms step_avg:57.47ms
step:364/2330 train_time:20921ms step_avg:57.48ms
step:365/2330 train_time:20978ms step_avg:57.47ms
step:366/2330 train_time:21036ms step_avg:57.47ms
step:367/2330 train_time:21091ms step_avg:57.47ms
step:368/2330 train_time:21151ms step_avg:57.48ms
step:369/2330 train_time:21207ms step_avg:57.47ms
step:370/2330 train_time:21266ms step_avg:57.48ms
step:371/2330 train_time:21322ms step_avg:57.47ms
step:372/2330 train_time:21382ms step_avg:57.48ms
step:373/2330 train_time:21437ms step_avg:57.47ms
step:374/2330 train_time:21497ms step_avg:57.48ms
step:375/2330 train_time:21552ms step_avg:57.47ms
step:376/2330 train_time:21612ms step_avg:57.48ms
step:377/2330 train_time:21668ms step_avg:57.48ms
step:378/2330 train_time:21728ms step_avg:57.48ms
step:379/2330 train_time:21783ms step_avg:57.48ms
step:380/2330 train_time:21843ms step_avg:57.48ms
step:381/2330 train_time:21899ms step_avg:57.48ms
step:382/2330 train_time:21959ms step_avg:57.48ms
step:383/2330 train_time:22014ms step_avg:57.48ms
step:384/2330 train_time:22073ms step_avg:57.48ms
step:385/2330 train_time:22129ms step_avg:57.48ms
step:386/2330 train_time:22189ms step_avg:57.48ms
step:387/2330 train_time:22245ms step_avg:57.48ms
step:388/2330 train_time:22304ms step_avg:57.48ms
step:389/2330 train_time:22360ms step_avg:57.48ms
step:390/2330 train_time:22420ms step_avg:57.49ms
step:391/2330 train_time:22475ms step_avg:57.48ms
step:392/2330 train_time:22534ms step_avg:57.49ms
step:393/2330 train_time:22592ms step_avg:57.48ms
step:394/2330 train_time:22651ms step_avg:57.49ms
step:395/2330 train_time:22707ms step_avg:57.49ms
step:396/2330 train_time:22765ms step_avg:57.49ms
step:397/2330 train_time:22822ms step_avg:57.49ms
step:398/2330 train_time:22881ms step_avg:57.49ms
step:399/2330 train_time:22936ms step_avg:57.48ms
step:400/2330 train_time:22995ms step_avg:57.49ms
step:401/2330 train_time:23052ms step_avg:57.49ms
step:402/2330 train_time:23111ms step_avg:57.49ms
step:403/2330 train_time:23166ms step_avg:57.48ms
step:404/2330 train_time:23226ms step_avg:57.49ms
step:405/2330 train_time:23281ms step_avg:57.48ms
step:406/2330 train_time:23340ms step_avg:57.49ms
step:407/2330 train_time:23396ms step_avg:57.48ms
step:408/2330 train_time:23456ms step_avg:57.49ms
step:409/2330 train_time:23513ms step_avg:57.49ms
step:410/2330 train_time:23572ms step_avg:57.49ms
step:411/2330 train_time:23627ms step_avg:57.49ms
step:412/2330 train_time:23686ms step_avg:57.49ms
step:413/2330 train_time:23742ms step_avg:57.49ms
step:414/2330 train_time:23801ms step_avg:57.49ms
step:415/2330 train_time:23857ms step_avg:57.49ms
step:416/2330 train_time:23916ms step_avg:57.49ms
step:417/2330 train_time:23972ms step_avg:57.49ms
step:418/2330 train_time:24032ms step_avg:57.49ms
step:419/2330 train_time:24088ms step_avg:57.49ms
step:420/2330 train_time:24148ms step_avg:57.50ms
step:421/2330 train_time:24204ms step_avg:57.49ms
step:422/2330 train_time:24263ms step_avg:57.50ms
step:423/2330 train_time:24319ms step_avg:57.49ms
step:424/2330 train_time:24377ms step_avg:57.49ms
step:425/2330 train_time:24433ms step_avg:57.49ms
step:426/2330 train_time:24493ms step_avg:57.50ms
step:427/2330 train_time:24549ms step_avg:57.49ms
step:428/2330 train_time:24608ms step_avg:57.50ms
step:429/2330 train_time:24664ms step_avg:57.49ms
step:430/2330 train_time:24724ms step_avg:57.50ms
step:431/2330 train_time:24780ms step_avg:57.49ms
step:432/2330 train_time:24839ms step_avg:57.50ms
step:433/2330 train_time:24895ms step_avg:57.49ms
step:434/2330 train_time:24955ms step_avg:57.50ms
step:435/2330 train_time:25011ms step_avg:57.50ms
step:436/2330 train_time:25070ms step_avg:57.50ms
step:437/2330 train_time:25126ms step_avg:57.50ms
step:438/2330 train_time:25185ms step_avg:57.50ms
step:439/2330 train_time:25241ms step_avg:57.50ms
step:440/2330 train_time:25300ms step_avg:57.50ms
step:441/2330 train_time:25356ms step_avg:57.50ms
step:442/2330 train_time:25415ms step_avg:57.50ms
step:443/2330 train_time:25472ms step_avg:57.50ms
step:444/2330 train_time:25531ms step_avg:57.50ms
step:445/2330 train_time:25587ms step_avg:57.50ms
step:446/2330 train_time:25646ms step_avg:57.50ms
step:447/2330 train_time:25701ms step_avg:57.50ms
step:448/2330 train_time:25761ms step_avg:57.50ms
step:449/2330 train_time:25816ms step_avg:57.50ms
step:450/2330 train_time:25876ms step_avg:57.50ms
step:451/2330 train_time:25932ms step_avg:57.50ms
step:452/2330 train_time:25991ms step_avg:57.50ms
step:453/2330 train_time:26047ms step_avg:57.50ms
step:454/2330 train_time:26107ms step_avg:57.50ms
step:455/2330 train_time:26163ms step_avg:57.50ms
step:456/2330 train_time:26222ms step_avg:57.50ms
step:457/2330 train_time:26278ms step_avg:57.50ms
step:458/2330 train_time:26337ms step_avg:57.50ms
step:459/2330 train_time:26393ms step_avg:57.50ms
step:460/2330 train_time:26452ms step_avg:57.50ms
step:461/2330 train_time:26508ms step_avg:57.50ms
step:462/2330 train_time:26567ms step_avg:57.50ms
step:463/2330 train_time:26624ms step_avg:57.50ms
step:464/2330 train_time:26683ms step_avg:57.51ms
step:465/2330 train_time:26738ms step_avg:57.50ms
step:466/2330 train_time:26798ms step_avg:57.51ms
step:467/2330 train_time:26854ms step_avg:57.50ms
step:468/2330 train_time:26915ms step_avg:57.51ms
step:469/2330 train_time:26972ms step_avg:57.51ms
step:470/2330 train_time:27031ms step_avg:57.51ms
step:471/2330 train_time:27087ms step_avg:57.51ms
step:472/2330 train_time:27145ms step_avg:57.51ms
step:473/2330 train_time:27201ms step_avg:57.51ms
step:474/2330 train_time:27260ms step_avg:57.51ms
step:475/2330 train_time:27315ms step_avg:57.51ms
step:476/2330 train_time:27376ms step_avg:57.51ms
step:477/2330 train_time:27432ms step_avg:57.51ms
step:478/2330 train_time:27491ms step_avg:57.51ms
step:479/2330 train_time:27548ms step_avg:57.51ms
step:480/2330 train_time:27607ms step_avg:57.51ms
step:481/2330 train_time:27663ms step_avg:57.51ms
step:482/2330 train_time:27722ms step_avg:57.51ms
step:483/2330 train_time:27777ms step_avg:57.51ms
step:484/2330 train_time:27837ms step_avg:57.51ms
step:485/2330 train_time:27893ms step_avg:57.51ms
step:486/2330 train_time:27952ms step_avg:57.52ms
step:487/2330 train_time:28009ms step_avg:57.51ms
step:488/2330 train_time:28068ms step_avg:57.52ms
step:489/2330 train_time:28124ms step_avg:57.51ms
step:490/2330 train_time:28183ms step_avg:57.52ms
step:491/2330 train_time:28239ms step_avg:57.51ms
step:492/2330 train_time:28299ms step_avg:57.52ms
step:493/2330 train_time:28355ms step_avg:57.51ms
step:494/2330 train_time:28414ms step_avg:57.52ms
step:495/2330 train_time:28471ms step_avg:57.52ms
step:496/2330 train_time:28530ms step_avg:57.52ms
step:497/2330 train_time:28587ms step_avg:57.52ms
step:498/2330 train_time:28645ms step_avg:57.52ms
step:499/2330 train_time:28701ms step_avg:57.52ms
step:500/2330 train_time:28760ms step_avg:57.52ms
step:500/2330 val_loss:4.4022 train_time:28840ms step_avg:57.68ms
step:501/2330 train_time:28857ms step_avg:57.60ms
step:502/2330 train_time:28878ms step_avg:57.53ms
step:503/2330 train_time:28936ms step_avg:57.53ms
step:504/2330 train_time:29001ms step_avg:57.54ms
step:505/2330 train_time:29058ms step_avg:57.54ms
step:506/2330 train_time:29117ms step_avg:57.54ms
step:507/2330 train_time:29173ms step_avg:57.54ms
step:508/2330 train_time:29233ms step_avg:57.55ms
step:509/2330 train_time:29288ms step_avg:57.54ms
step:510/2330 train_time:29348ms step_avg:57.54ms
step:511/2330 train_time:29403ms step_avg:57.54ms
step:512/2330 train_time:29462ms step_avg:57.54ms
step:513/2330 train_time:29517ms step_avg:57.54ms
step:514/2330 train_time:29576ms step_avg:57.54ms
step:515/2330 train_time:29631ms step_avg:57.54ms
step:516/2330 train_time:29689ms step_avg:57.54ms
step:517/2330 train_time:29745ms step_avg:57.53ms
step:518/2330 train_time:29804ms step_avg:57.54ms
step:519/2330 train_time:29861ms step_avg:57.54ms
step:520/2330 train_time:29922ms step_avg:57.54ms
step:521/2330 train_time:29980ms step_avg:57.54ms
step:522/2330 train_time:30039ms step_avg:57.55ms
step:523/2330 train_time:30095ms step_avg:57.54ms
step:524/2330 train_time:30156ms step_avg:57.55ms
step:525/2330 train_time:30212ms step_avg:57.55ms
step:526/2330 train_time:30272ms step_avg:57.55ms
step:527/2330 train_time:30327ms step_avg:57.55ms
step:528/2330 train_time:30386ms step_avg:57.55ms
step:529/2330 train_time:30441ms step_avg:57.54ms
step:530/2330 train_time:30500ms step_avg:57.55ms
step:531/2330 train_time:30556ms step_avg:57.54ms
step:532/2330 train_time:30615ms step_avg:57.55ms
step:533/2330 train_time:30671ms step_avg:57.54ms
step:534/2330 train_time:30729ms step_avg:57.54ms
step:535/2330 train_time:30785ms step_avg:57.54ms
step:536/2330 train_time:30844ms step_avg:57.54ms
step:537/2330 train_time:30900ms step_avg:57.54ms
step:538/2330 train_time:30961ms step_avg:57.55ms
step:539/2330 train_time:31018ms step_avg:57.55ms
step:540/2330 train_time:31077ms step_avg:57.55ms
step:541/2330 train_time:31135ms step_avg:57.55ms
step:542/2330 train_time:31194ms step_avg:57.55ms
step:543/2330 train_time:31250ms step_avg:57.55ms
step:544/2330 train_time:31311ms step_avg:57.56ms
step:545/2330 train_time:31367ms step_avg:57.55ms
step:546/2330 train_time:31425ms step_avg:57.56ms
step:547/2330 train_time:31481ms step_avg:57.55ms
step:548/2330 train_time:31540ms step_avg:57.55ms
step:549/2330 train_time:31596ms step_avg:57.55ms
step:550/2330 train_time:31655ms step_avg:57.55ms
step:551/2330 train_time:31711ms step_avg:57.55ms
step:552/2330 train_time:31770ms step_avg:57.55ms
step:553/2330 train_time:31826ms step_avg:57.55ms
step:554/2330 train_time:31885ms step_avg:57.55ms
step:555/2330 train_time:31941ms step_avg:57.55ms
step:556/2330 train_time:32003ms step_avg:57.56ms
step:557/2330 train_time:32060ms step_avg:57.56ms
step:558/2330 train_time:32119ms step_avg:57.56ms
step:559/2330 train_time:32176ms step_avg:57.56ms
step:560/2330 train_time:32235ms step_avg:57.56ms
step:561/2330 train_time:32292ms step_avg:57.56ms
step:562/2330 train_time:32351ms step_avg:57.56ms
step:563/2330 train_time:32407ms step_avg:57.56ms
step:564/2330 train_time:32466ms step_avg:57.56ms
step:565/2330 train_time:32521ms step_avg:57.56ms
step:566/2330 train_time:32580ms step_avg:57.56ms
step:567/2330 train_time:32636ms step_avg:57.56ms
step:568/2330 train_time:32696ms step_avg:57.56ms
step:569/2330 train_time:32752ms step_avg:57.56ms
step:570/2330 train_time:32812ms step_avg:57.56ms
step:571/2330 train_time:32867ms step_avg:57.56ms
step:572/2330 train_time:32927ms step_avg:57.56ms
step:573/2330 train_time:32983ms step_avg:57.56ms
step:574/2330 train_time:33043ms step_avg:57.57ms
step:575/2330 train_time:33098ms step_avg:57.56ms
step:576/2330 train_time:33158ms step_avg:57.57ms
step:577/2330 train_time:33214ms step_avg:57.56ms
step:578/2330 train_time:33273ms step_avg:57.57ms
step:579/2330 train_time:33329ms step_avg:57.56ms
step:580/2330 train_time:33389ms step_avg:57.57ms
step:581/2330 train_time:33445ms step_avg:57.56ms
step:582/2330 train_time:33504ms step_avg:57.57ms
step:583/2330 train_time:33560ms step_avg:57.56ms
step:584/2330 train_time:33619ms step_avg:57.57ms
step:585/2330 train_time:33675ms step_avg:57.56ms
step:586/2330 train_time:33734ms step_avg:57.57ms
step:587/2330 train_time:33790ms step_avg:57.56ms
step:588/2330 train_time:33850ms step_avg:57.57ms
step:589/2330 train_time:33906ms step_avg:57.56ms
step:590/2330 train_time:33966ms step_avg:57.57ms
step:591/2330 train_time:34022ms step_avg:57.57ms
step:592/2330 train_time:34081ms step_avg:57.57ms
step:593/2330 train_time:34137ms step_avg:57.57ms
step:594/2330 train_time:34196ms step_avg:57.57ms
step:595/2330 train_time:34253ms step_avg:57.57ms
step:596/2330 train_time:34312ms step_avg:57.57ms
step:597/2330 train_time:34368ms step_avg:57.57ms
step:598/2330 train_time:34428ms step_avg:57.57ms
step:599/2330 train_time:34484ms step_avg:57.57ms
step:600/2330 train_time:34544ms step_avg:57.57ms
step:601/2330 train_time:34599ms step_avg:57.57ms
step:602/2330 train_time:34659ms step_avg:57.57ms
step:603/2330 train_time:34716ms step_avg:57.57ms
step:604/2330 train_time:34775ms step_avg:57.57ms
step:605/2330 train_time:34830ms step_avg:57.57ms
step:606/2330 train_time:34890ms step_avg:57.57ms
step:607/2330 train_time:34946ms step_avg:57.57ms
step:608/2330 train_time:35005ms step_avg:57.57ms
step:609/2330 train_time:35061ms step_avg:57.57ms
step:610/2330 train_time:35120ms step_avg:57.57ms
step:611/2330 train_time:35177ms step_avg:57.57ms
step:612/2330 train_time:35236ms step_avg:57.57ms
step:613/2330 train_time:35292ms step_avg:57.57ms
step:614/2330 train_time:35352ms step_avg:57.58ms
step:615/2330 train_time:35407ms step_avg:57.57ms
step:616/2330 train_time:35467ms step_avg:57.58ms
step:617/2330 train_time:35523ms step_avg:57.57ms
step:618/2330 train_time:35583ms step_avg:57.58ms
step:619/2330 train_time:35638ms step_avg:57.57ms
step:620/2330 train_time:35697ms step_avg:57.58ms
step:621/2330 train_time:35754ms step_avg:57.57ms
step:622/2330 train_time:35813ms step_avg:57.58ms
step:623/2330 train_time:35870ms step_avg:57.58ms
step:624/2330 train_time:35930ms step_avg:57.58ms
step:625/2330 train_time:35986ms step_avg:57.58ms
step:626/2330 train_time:36046ms step_avg:57.58ms
step:627/2330 train_time:36101ms step_avg:57.58ms
step:628/2330 train_time:36162ms step_avg:57.58ms
step:629/2330 train_time:36218ms step_avg:57.58ms
step:630/2330 train_time:36277ms step_avg:57.58ms
step:631/2330 train_time:36333ms step_avg:57.58ms
step:632/2330 train_time:36393ms step_avg:57.58ms
step:633/2330 train_time:36449ms step_avg:57.58ms
step:634/2330 train_time:36509ms step_avg:57.59ms
step:635/2330 train_time:36565ms step_avg:57.58ms
step:636/2330 train_time:36624ms step_avg:57.59ms
step:637/2330 train_time:36680ms step_avg:57.58ms
step:638/2330 train_time:36741ms step_avg:57.59ms
step:639/2330 train_time:36797ms step_avg:57.59ms
step:640/2330 train_time:36856ms step_avg:57.59ms
step:641/2330 train_time:36912ms step_avg:57.59ms
step:642/2330 train_time:36972ms step_avg:57.59ms
step:643/2330 train_time:37028ms step_avg:57.59ms
step:644/2330 train_time:37087ms step_avg:57.59ms
step:645/2330 train_time:37143ms step_avg:57.59ms
step:646/2330 train_time:37202ms step_avg:57.59ms
step:647/2330 train_time:37258ms step_avg:57.59ms
step:648/2330 train_time:37318ms step_avg:57.59ms
step:649/2330 train_time:37375ms step_avg:57.59ms
step:650/2330 train_time:37434ms step_avg:57.59ms
step:651/2330 train_time:37490ms step_avg:57.59ms
step:652/2330 train_time:37550ms step_avg:57.59ms
step:653/2330 train_time:37606ms step_avg:57.59ms
step:654/2330 train_time:37665ms step_avg:57.59ms
step:655/2330 train_time:37720ms step_avg:57.59ms
step:656/2330 train_time:37780ms step_avg:57.59ms
step:657/2330 train_time:37836ms step_avg:57.59ms
step:658/2330 train_time:37896ms step_avg:57.59ms
step:659/2330 train_time:37951ms step_avg:57.59ms
step:660/2330 train_time:38011ms step_avg:57.59ms
step:661/2330 train_time:38066ms step_avg:57.59ms
step:662/2330 train_time:38126ms step_avg:57.59ms
step:663/2330 train_time:38182ms step_avg:57.59ms
step:664/2330 train_time:38242ms step_avg:57.59ms
step:665/2330 train_time:38298ms step_avg:57.59ms
step:666/2330 train_time:38357ms step_avg:57.59ms
step:667/2330 train_time:38413ms step_avg:57.59ms
step:668/2330 train_time:38473ms step_avg:57.59ms
step:669/2330 train_time:38529ms step_avg:57.59ms
step:670/2330 train_time:38588ms step_avg:57.59ms
step:671/2330 train_time:38644ms step_avg:57.59ms
step:672/2330 train_time:38704ms step_avg:57.60ms
step:673/2330 train_time:38760ms step_avg:57.59ms
step:674/2330 train_time:38819ms step_avg:57.60ms
step:675/2330 train_time:38876ms step_avg:57.59ms
step:676/2330 train_time:38935ms step_avg:57.60ms
step:677/2330 train_time:38991ms step_avg:57.59ms
step:678/2330 train_time:39051ms step_avg:57.60ms
step:679/2330 train_time:39106ms step_avg:57.59ms
step:680/2330 train_time:39167ms step_avg:57.60ms
step:681/2330 train_time:39222ms step_avg:57.60ms
step:682/2330 train_time:39282ms step_avg:57.60ms
step:683/2330 train_time:39338ms step_avg:57.60ms
step:684/2330 train_time:39398ms step_avg:57.60ms
step:685/2330 train_time:39455ms step_avg:57.60ms
step:686/2330 train_time:39514ms step_avg:57.60ms
step:687/2330 train_time:39570ms step_avg:57.60ms
step:688/2330 train_time:39629ms step_avg:57.60ms
step:689/2330 train_time:39684ms step_avg:57.60ms
step:690/2330 train_time:39744ms step_avg:57.60ms
step:691/2330 train_time:39800ms step_avg:57.60ms
step:692/2330 train_time:39861ms step_avg:57.60ms
step:693/2330 train_time:39918ms step_avg:57.60ms
step:694/2330 train_time:39976ms step_avg:57.60ms
step:695/2330 train_time:40033ms step_avg:57.60ms
step:696/2330 train_time:40092ms step_avg:57.60ms
step:697/2330 train_time:40148ms step_avg:57.60ms
step:698/2330 train_time:40207ms step_avg:57.60ms
step:699/2330 train_time:40263ms step_avg:57.60ms
step:700/2330 train_time:40322ms step_avg:57.60ms
step:701/2330 train_time:40378ms step_avg:57.60ms
step:702/2330 train_time:40437ms step_avg:57.60ms
step:703/2330 train_time:40494ms step_avg:57.60ms
step:704/2330 train_time:40553ms step_avg:57.60ms
step:705/2330 train_time:40610ms step_avg:57.60ms
step:706/2330 train_time:40669ms step_avg:57.61ms
step:707/2330 train_time:40725ms step_avg:57.60ms
step:708/2330 train_time:40784ms step_avg:57.61ms
step:709/2330 train_time:40840ms step_avg:57.60ms
step:710/2330 train_time:40901ms step_avg:57.61ms
step:711/2330 train_time:40958ms step_avg:57.61ms
step:712/2330 train_time:41017ms step_avg:57.61ms
step:713/2330 train_time:41073ms step_avg:57.61ms
step:714/2330 train_time:41134ms step_avg:57.61ms
step:715/2330 train_time:41190ms step_avg:57.61ms
step:716/2330 train_time:41249ms step_avg:57.61ms
step:717/2330 train_time:41305ms step_avg:57.61ms
step:718/2330 train_time:41366ms step_avg:57.61ms
step:719/2330 train_time:41421ms step_avg:57.61ms
step:720/2330 train_time:41482ms step_avg:57.61ms
step:721/2330 train_time:41538ms step_avg:57.61ms
step:722/2330 train_time:41598ms step_avg:57.61ms
step:723/2330 train_time:41654ms step_avg:57.61ms
step:724/2330 train_time:41713ms step_avg:57.61ms
step:725/2330 train_time:41769ms step_avg:57.61ms
step:726/2330 train_time:41829ms step_avg:57.62ms
step:727/2330 train_time:41885ms step_avg:57.61ms
step:728/2330 train_time:41944ms step_avg:57.62ms
step:729/2330 train_time:42000ms step_avg:57.61ms
step:730/2330 train_time:42059ms step_avg:57.62ms
step:731/2330 train_time:42116ms step_avg:57.61ms
step:732/2330 train_time:42175ms step_avg:57.62ms
step:733/2330 train_time:42231ms step_avg:57.61ms
step:734/2330 train_time:42290ms step_avg:57.62ms
step:735/2330 train_time:42347ms step_avg:57.61ms
step:736/2330 train_time:42406ms step_avg:57.62ms
step:737/2330 train_time:42461ms step_avg:57.61ms
step:738/2330 train_time:42520ms step_avg:57.62ms
step:739/2330 train_time:42576ms step_avg:57.61ms
step:740/2330 train_time:42636ms step_avg:57.62ms
step:741/2330 train_time:42692ms step_avg:57.61ms
step:742/2330 train_time:42752ms step_avg:57.62ms
step:743/2330 train_time:42808ms step_avg:57.61ms
step:744/2330 train_time:42868ms step_avg:57.62ms
step:745/2330 train_time:42924ms step_avg:57.62ms
step:746/2330 train_time:42983ms step_avg:57.62ms
step:747/2330 train_time:43039ms step_avg:57.62ms
step:748/2330 train_time:43099ms step_avg:57.62ms
step:749/2330 train_time:43156ms step_avg:57.62ms
step:750/2330 train_time:43215ms step_avg:57.62ms
step:750/2330 val_loss:4.2059 train_time:43294ms step_avg:57.73ms
step:751/2330 train_time:43312ms step_avg:57.67ms
step:752/2330 train_time:43334ms step_avg:57.62ms
step:753/2330 train_time:43390ms step_avg:57.62ms
step:754/2330 train_time:43456ms step_avg:57.63ms
step:755/2330 train_time:43513ms step_avg:57.63ms
step:756/2330 train_time:43573ms step_avg:57.64ms
step:757/2330 train_time:43630ms step_avg:57.63ms
step:758/2330 train_time:43689ms step_avg:57.64ms
step:759/2330 train_time:43745ms step_avg:57.64ms
step:760/2330 train_time:43804ms step_avg:57.64ms
step:761/2330 train_time:43859ms step_avg:57.63ms
step:762/2330 train_time:43918ms step_avg:57.64ms
step:763/2330 train_time:43974ms step_avg:57.63ms
step:764/2330 train_time:44033ms step_avg:57.63ms
step:765/2330 train_time:44090ms step_avg:57.63ms
step:766/2330 train_time:44148ms step_avg:57.63ms
step:767/2330 train_time:44204ms step_avg:57.63ms
step:768/2330 train_time:44264ms step_avg:57.63ms
step:769/2330 train_time:44321ms step_avg:57.63ms
step:770/2330 train_time:44384ms step_avg:57.64ms
step:771/2330 train_time:44441ms step_avg:57.64ms
step:772/2330 train_time:44505ms step_avg:57.65ms
step:773/2330 train_time:44562ms step_avg:57.65ms
step:774/2330 train_time:44623ms step_avg:57.65ms
step:775/2330 train_time:44680ms step_avg:57.65ms
step:776/2330 train_time:44739ms step_avg:57.65ms
step:777/2330 train_time:44796ms step_avg:57.65ms
step:778/2330 train_time:44856ms step_avg:57.66ms
step:779/2330 train_time:44912ms step_avg:57.65ms
step:780/2330 train_time:44971ms step_avg:57.66ms
step:781/2330 train_time:45028ms step_avg:57.65ms
step:782/2330 train_time:45087ms step_avg:57.66ms
step:783/2330 train_time:45143ms step_avg:57.65ms
step:784/2330 train_time:45203ms step_avg:57.66ms
step:785/2330 train_time:45260ms step_avg:57.66ms
step:786/2330 train_time:45320ms step_avg:57.66ms
step:787/2330 train_time:45378ms step_avg:57.66ms
step:788/2330 train_time:45439ms step_avg:57.66ms
step:789/2330 train_time:45497ms step_avg:57.66ms
step:790/2330 train_time:45557ms step_avg:57.67ms
step:791/2330 train_time:45614ms step_avg:57.67ms
step:792/2330 train_time:45674ms step_avg:57.67ms
step:793/2330 train_time:45730ms step_avg:57.67ms
step:794/2330 train_time:45790ms step_avg:57.67ms
step:795/2330 train_time:45847ms step_avg:57.67ms
step:796/2330 train_time:45907ms step_avg:57.67ms
step:797/2330 train_time:45963ms step_avg:57.67ms
step:798/2330 train_time:46023ms step_avg:57.67ms
step:799/2330 train_time:46080ms step_avg:57.67ms
step:800/2330 train_time:46139ms step_avg:57.67ms
step:801/2330 train_time:46196ms step_avg:57.67ms
step:802/2330 train_time:46256ms step_avg:57.68ms
step:803/2330 train_time:46314ms step_avg:57.68ms
step:804/2330 train_time:46374ms step_avg:57.68ms
step:805/2330 train_time:46432ms step_avg:57.68ms
step:806/2330 train_time:46493ms step_avg:57.68ms
step:807/2330 train_time:46550ms step_avg:57.68ms
step:808/2330 train_time:46612ms step_avg:57.69ms
step:809/2330 train_time:46669ms step_avg:57.69ms
step:810/2330 train_time:46729ms step_avg:57.69ms
step:811/2330 train_time:46786ms step_avg:57.69ms
step:812/2330 train_time:46846ms step_avg:57.69ms
step:813/2330 train_time:46903ms step_avg:57.69ms
step:814/2330 train_time:46963ms step_avg:57.69ms
step:815/2330 train_time:47019ms step_avg:57.69ms
step:816/2330 train_time:47079ms step_avg:57.69ms
step:817/2330 train_time:47135ms step_avg:57.69ms
step:818/2330 train_time:47195ms step_avg:57.70ms
step:819/2330 train_time:47252ms step_avg:57.69ms
step:820/2330 train_time:47311ms step_avg:57.70ms
step:821/2330 train_time:47369ms step_avg:57.70ms
step:822/2330 train_time:47430ms step_avg:57.70ms
step:823/2330 train_time:47487ms step_avg:57.70ms
step:824/2330 train_time:47546ms step_avg:57.70ms
step:825/2330 train_time:47604ms step_avg:57.70ms
step:826/2330 train_time:47664ms step_avg:57.70ms
step:827/2330 train_time:47721ms step_avg:57.70ms
step:828/2330 train_time:47781ms step_avg:57.71ms
step:829/2330 train_time:47838ms step_avg:57.71ms
step:830/2330 train_time:47899ms step_avg:57.71ms
step:831/2330 train_time:47956ms step_avg:57.71ms
step:832/2330 train_time:48015ms step_avg:57.71ms
step:833/2330 train_time:48071ms step_avg:57.71ms
step:834/2330 train_time:48131ms step_avg:57.71ms
step:835/2330 train_time:48188ms step_avg:57.71ms
step:836/2330 train_time:48248ms step_avg:57.71ms
step:837/2330 train_time:48305ms step_avg:57.71ms
step:838/2330 train_time:48365ms step_avg:57.71ms
step:839/2330 train_time:48421ms step_avg:57.71ms
step:840/2330 train_time:48482ms step_avg:57.72ms
step:841/2330 train_time:48539ms step_avg:57.72ms
step:842/2330 train_time:48600ms step_avg:57.72ms
step:843/2330 train_time:48658ms step_avg:57.72ms
step:844/2330 train_time:48718ms step_avg:57.72ms
step:845/2330 train_time:48774ms step_avg:57.72ms
step:846/2330 train_time:48835ms step_avg:57.72ms
step:847/2330 train_time:48892ms step_avg:57.72ms
step:848/2330 train_time:48952ms step_avg:57.73ms
step:849/2330 train_time:49009ms step_avg:57.73ms
step:850/2330 train_time:49068ms step_avg:57.73ms
step:851/2330 train_time:49125ms step_avg:57.73ms
step:852/2330 train_time:49185ms step_avg:57.73ms
step:853/2330 train_time:49241ms step_avg:57.73ms
step:854/2330 train_time:49303ms step_avg:57.73ms
step:855/2330 train_time:49360ms step_avg:57.73ms
step:856/2330 train_time:49419ms step_avg:57.73ms
step:857/2330 train_time:49476ms step_avg:57.73ms
step:858/2330 train_time:49536ms step_avg:57.73ms
step:859/2330 train_time:49594ms step_avg:57.73ms
step:860/2330 train_time:49655ms step_avg:57.74ms
step:861/2330 train_time:49712ms step_avg:57.74ms
step:862/2330 train_time:49772ms step_avg:57.74ms
step:863/2330 train_time:49828ms step_avg:57.74ms
step:864/2330 train_time:49888ms step_avg:57.74ms
step:865/2330 train_time:49945ms step_avg:57.74ms
step:866/2330 train_time:50005ms step_avg:57.74ms
step:867/2330 train_time:50062ms step_avg:57.74ms
step:868/2330 train_time:50122ms step_avg:57.74ms
step:869/2330 train_time:50179ms step_avg:57.74ms
step:870/2330 train_time:50239ms step_avg:57.75ms
step:871/2330 train_time:50296ms step_avg:57.75ms
step:872/2330 train_time:50356ms step_avg:57.75ms
step:873/2330 train_time:50413ms step_avg:57.75ms
step:874/2330 train_time:50473ms step_avg:57.75ms
step:875/2330 train_time:50529ms step_avg:57.75ms
step:876/2330 train_time:50590ms step_avg:57.75ms
step:877/2330 train_time:50647ms step_avg:57.75ms
step:878/2330 train_time:50709ms step_avg:57.75ms
step:879/2330 train_time:50765ms step_avg:57.75ms
step:880/2330 train_time:50825ms step_avg:57.76ms
step:881/2330 train_time:50882ms step_avg:57.75ms
step:882/2330 train_time:50943ms step_avg:57.76ms
step:883/2330 train_time:51000ms step_avg:57.76ms
step:884/2330 train_time:51060ms step_avg:57.76ms
step:885/2330 train_time:51117ms step_avg:57.76ms
step:886/2330 train_time:51178ms step_avg:57.76ms
step:887/2330 train_time:51235ms step_avg:57.76ms
step:888/2330 train_time:51295ms step_avg:57.76ms
step:889/2330 train_time:51352ms step_avg:57.76ms
step:890/2330 train_time:51412ms step_avg:57.77ms
step:891/2330 train_time:51468ms step_avg:57.76ms
step:892/2330 train_time:51528ms step_avg:57.77ms
step:893/2330 train_time:51586ms step_avg:57.77ms
step:894/2330 train_time:51646ms step_avg:57.77ms
step:895/2330 train_time:51703ms step_avg:57.77ms
step:896/2330 train_time:51764ms step_avg:57.77ms
step:897/2330 train_time:51820ms step_avg:57.77ms
step:898/2330 train_time:51880ms step_avg:57.77ms
step:899/2330 train_time:51937ms step_avg:57.77ms
step:900/2330 train_time:51997ms step_avg:57.77ms
step:901/2330 train_time:52054ms step_avg:57.77ms
step:902/2330 train_time:52114ms step_avg:57.78ms
step:903/2330 train_time:52171ms step_avg:57.77ms
step:904/2330 train_time:52231ms step_avg:57.78ms
step:905/2330 train_time:52289ms step_avg:57.78ms
step:906/2330 train_time:52348ms step_avg:57.78ms
step:907/2330 train_time:52406ms step_avg:57.78ms
step:908/2330 train_time:52465ms step_avg:57.78ms
step:909/2330 train_time:52522ms step_avg:57.78ms
step:910/2330 train_time:52582ms step_avg:57.78ms
step:911/2330 train_time:52640ms step_avg:57.78ms
step:912/2330 train_time:52699ms step_avg:57.78ms
step:913/2330 train_time:52757ms step_avg:57.78ms
step:914/2330 train_time:52817ms step_avg:57.79ms
step:915/2330 train_time:52875ms step_avg:57.79ms
step:916/2330 train_time:52935ms step_avg:57.79ms
step:917/2330 train_time:52992ms step_avg:57.79ms
step:918/2330 train_time:53052ms step_avg:57.79ms
step:919/2330 train_time:53108ms step_avg:57.79ms
step:920/2330 train_time:53169ms step_avg:57.79ms
step:921/2330 train_time:53225ms step_avg:57.79ms
step:922/2330 train_time:53286ms step_avg:57.79ms
step:923/2330 train_time:53343ms step_avg:57.79ms
step:924/2330 train_time:53403ms step_avg:57.80ms
step:925/2330 train_time:53460ms step_avg:57.79ms
step:926/2330 train_time:53521ms step_avg:57.80ms
step:927/2330 train_time:53578ms step_avg:57.80ms
step:928/2330 train_time:53638ms step_avg:57.80ms
step:929/2330 train_time:53695ms step_avg:57.80ms
step:930/2330 train_time:53756ms step_avg:57.80ms
step:931/2330 train_time:53813ms step_avg:57.80ms
step:932/2330 train_time:53874ms step_avg:57.80ms
step:933/2330 train_time:53931ms step_avg:57.80ms
step:934/2330 train_time:53991ms step_avg:57.81ms
step:935/2330 train_time:54048ms step_avg:57.81ms
step:936/2330 train_time:54109ms step_avg:57.81ms
step:937/2330 train_time:54165ms step_avg:57.81ms
step:938/2330 train_time:54225ms step_avg:57.81ms
step:939/2330 train_time:54282ms step_avg:57.81ms
step:940/2330 train_time:54343ms step_avg:57.81ms
step:941/2330 train_time:54399ms step_avg:57.81ms
step:942/2330 train_time:54460ms step_avg:57.81ms
step:943/2330 train_time:54518ms step_avg:57.81ms
step:944/2330 train_time:54577ms step_avg:57.81ms
step:945/2330 train_time:54634ms step_avg:57.81ms
step:946/2330 train_time:54695ms step_avg:57.82ms
step:947/2330 train_time:54752ms step_avg:57.82ms
step:948/2330 train_time:54812ms step_avg:57.82ms
step:949/2330 train_time:54868ms step_avg:57.82ms
step:950/2330 train_time:54928ms step_avg:57.82ms
step:951/2330 train_time:54985ms step_avg:57.82ms
step:952/2330 train_time:55046ms step_avg:57.82ms
step:953/2330 train_time:55103ms step_avg:57.82ms
step:954/2330 train_time:55164ms step_avg:57.82ms
step:955/2330 train_time:55220ms step_avg:57.82ms
step:956/2330 train_time:55280ms step_avg:57.82ms
step:957/2330 train_time:55337ms step_avg:57.82ms
step:958/2330 train_time:55398ms step_avg:57.83ms
step:959/2330 train_time:55455ms step_avg:57.83ms
step:960/2330 train_time:55515ms step_avg:57.83ms
step:961/2330 train_time:55573ms step_avg:57.83ms
step:962/2330 train_time:55633ms step_avg:57.83ms
step:963/2330 train_time:55690ms step_avg:57.83ms
step:964/2330 train_time:55751ms step_avg:57.83ms
step:965/2330 train_time:55808ms step_avg:57.83ms
step:966/2330 train_time:55868ms step_avg:57.83ms
step:967/2330 train_time:55926ms step_avg:57.83ms
step:968/2330 train_time:55985ms step_avg:57.84ms
step:969/2330 train_time:56042ms step_avg:57.83ms
step:970/2330 train_time:56103ms step_avg:57.84ms
step:971/2330 train_time:56160ms step_avg:57.84ms
step:972/2330 train_time:56220ms step_avg:57.84ms
step:973/2330 train_time:56278ms step_avg:57.84ms
step:974/2330 train_time:56338ms step_avg:57.84ms
step:975/2330 train_time:56394ms step_avg:57.84ms
step:976/2330 train_time:56454ms step_avg:57.84ms
step:977/2330 train_time:56511ms step_avg:57.84ms
step:978/2330 train_time:56571ms step_avg:57.84ms
step:979/2330 train_time:56628ms step_avg:57.84ms
step:980/2330 train_time:56689ms step_avg:57.85ms
step:981/2330 train_time:56745ms step_avg:57.84ms
step:982/2330 train_time:56807ms step_avg:57.85ms
step:983/2330 train_time:56864ms step_avg:57.85ms
step:984/2330 train_time:56924ms step_avg:57.85ms
step:985/2330 train_time:56981ms step_avg:57.85ms
step:986/2330 train_time:57041ms step_avg:57.85ms
step:987/2330 train_time:57098ms step_avg:57.85ms
step:988/2330 train_time:57157ms step_avg:57.85ms
step:989/2330 train_time:57215ms step_avg:57.85ms
step:990/2330 train_time:57275ms step_avg:57.85ms
step:991/2330 train_time:57332ms step_avg:57.85ms
step:992/2330 train_time:57391ms step_avg:57.85ms
step:993/2330 train_time:57449ms step_avg:57.85ms
step:994/2330 train_time:57508ms step_avg:57.86ms
step:995/2330 train_time:57566ms step_avg:57.85ms
step:996/2330 train_time:57625ms step_avg:57.86ms
step:997/2330 train_time:57682ms step_avg:57.86ms
step:998/2330 train_time:57742ms step_avg:57.86ms
step:999/2330 train_time:57801ms step_avg:57.86ms
step:1000/2330 train_time:57860ms step_avg:57.86ms
step:1000/2330 val_loss:4.0643 train_time:57942ms step_avg:57.94ms
step:1001/2330 train_time:57962ms step_avg:57.90ms
step:1002/2330 train_time:57983ms step_avg:57.87ms
step:1003/2330 train_time:58034ms step_avg:57.86ms
step:1004/2330 train_time:58106ms step_avg:57.87ms
step:1005/2330 train_time:58162ms step_avg:57.87ms
step:1006/2330 train_time:58225ms step_avg:57.88ms
step:1007/2330 train_time:58281ms step_avg:57.88ms
step:1008/2330 train_time:58342ms step_avg:57.88ms
step:1009/2330 train_time:58398ms step_avg:57.88ms
step:1010/2330 train_time:58457ms step_avg:57.88ms
step:1011/2330 train_time:58514ms step_avg:57.88ms
step:1012/2330 train_time:58573ms step_avg:57.88ms
step:1013/2330 train_time:58629ms step_avg:57.88ms
step:1014/2330 train_time:58688ms step_avg:57.88ms
step:1015/2330 train_time:58744ms step_avg:57.88ms
step:1016/2330 train_time:58804ms step_avg:57.88ms
step:1017/2330 train_time:58865ms step_avg:57.88ms
step:1018/2330 train_time:58927ms step_avg:57.89ms
step:1019/2330 train_time:58986ms step_avg:57.89ms
step:1020/2330 train_time:59046ms step_avg:57.89ms
step:1021/2330 train_time:59103ms step_avg:57.89ms
step:1022/2330 train_time:59164ms step_avg:57.89ms
step:1023/2330 train_time:59221ms step_avg:57.89ms
step:1024/2330 train_time:59282ms step_avg:57.89ms
step:1025/2330 train_time:59339ms step_avg:57.89ms
step:1026/2330 train_time:59398ms step_avg:57.89ms
step:1027/2330 train_time:59455ms step_avg:57.89ms
step:1028/2330 train_time:59514ms step_avg:57.89ms
step:1029/2330 train_time:59570ms step_avg:57.89ms
step:1030/2330 train_time:59630ms step_avg:57.89ms
step:1031/2330 train_time:59687ms step_avg:57.89ms
step:1032/2330 train_time:59746ms step_avg:57.89ms
step:1033/2330 train_time:59803ms step_avg:57.89ms
step:1034/2330 train_time:59864ms step_avg:57.90ms
step:1035/2330 train_time:59923ms step_avg:57.90ms
step:1036/2330 train_time:59984ms step_avg:57.90ms
step:1037/2330 train_time:60042ms step_avg:57.90ms
step:1038/2330 train_time:60102ms step_avg:57.90ms
step:1039/2330 train_time:60160ms step_avg:57.90ms
step:1040/2330 train_time:60220ms step_avg:57.90ms
step:1041/2330 train_time:60277ms step_avg:57.90ms
step:1042/2330 train_time:60336ms step_avg:57.90ms
step:1043/2330 train_time:60393ms step_avg:57.90ms
step:1044/2330 train_time:60453ms step_avg:57.91ms
step:1045/2330 train_time:60509ms step_avg:57.90ms
step:1046/2330 train_time:60569ms step_avg:57.91ms
step:1047/2330 train_time:60626ms step_avg:57.90ms
step:1048/2330 train_time:60685ms step_avg:57.91ms
step:1049/2330 train_time:60741ms step_avg:57.90ms
step:1050/2330 train_time:60801ms step_avg:57.91ms
step:1051/2330 train_time:60859ms step_avg:57.91ms
step:1052/2330 train_time:60920ms step_avg:57.91ms
step:1053/2330 train_time:60978ms step_avg:57.91ms
step:1054/2330 train_time:61038ms step_avg:57.91ms
step:1055/2330 train_time:61097ms step_avg:57.91ms
step:1056/2330 train_time:61157ms step_avg:57.91ms
step:1057/2330 train_time:61215ms step_avg:57.91ms
step:1058/2330 train_time:61274ms step_avg:57.92ms
step:1059/2330 train_time:61331ms step_avg:57.91ms
step:1060/2330 train_time:61391ms step_avg:57.92ms
step:1061/2330 train_time:61447ms step_avg:57.91ms
step:1062/2330 train_time:61507ms step_avg:57.92ms
step:1063/2330 train_time:61564ms step_avg:57.92ms
step:1064/2330 train_time:61623ms step_avg:57.92ms
step:1065/2330 train_time:61680ms step_avg:57.92ms
step:1066/2330 train_time:61740ms step_avg:57.92ms
step:1067/2330 train_time:61797ms step_avg:57.92ms
step:1068/2330 train_time:61858ms step_avg:57.92ms
step:1069/2330 train_time:61915ms step_avg:57.92ms
step:1070/2330 train_time:61975ms step_avg:57.92ms
step:1071/2330 train_time:62033ms step_avg:57.92ms
step:1072/2330 train_time:62093ms step_avg:57.92ms
step:1073/2330 train_time:62150ms step_avg:57.92ms
step:1074/2330 train_time:62212ms step_avg:57.93ms
step:1075/2330 train_time:62269ms step_avg:57.92ms
step:1076/2330 train_time:62329ms step_avg:57.93ms
step:1077/2330 train_time:62386ms step_avg:57.93ms
step:1078/2330 train_time:62446ms step_avg:57.93ms
step:1079/2330 train_time:62502ms step_avg:57.93ms
step:1080/2330 train_time:62563ms step_avg:57.93ms
step:1081/2330 train_time:62619ms step_avg:57.93ms
step:1082/2330 train_time:62680ms step_avg:57.93ms
step:1083/2330 train_time:62737ms step_avg:57.93ms
step:1084/2330 train_time:62797ms step_avg:57.93ms
step:1085/2330 train_time:62854ms step_avg:57.93ms
step:1086/2330 train_time:62915ms step_avg:57.93ms
step:1087/2330 train_time:62973ms step_avg:57.93ms
step:1088/2330 train_time:63032ms step_avg:57.93ms
step:1089/2330 train_time:63089ms step_avg:57.93ms
step:1090/2330 train_time:63150ms step_avg:57.94ms
step:1091/2330 train_time:63207ms step_avg:57.93ms
step:1092/2330 train_time:63266ms step_avg:57.94ms
step:1093/2330 train_time:63324ms step_avg:57.94ms
step:1094/2330 train_time:63384ms step_avg:57.94ms
step:1095/2330 train_time:63441ms step_avg:57.94ms
step:1096/2330 train_time:63501ms step_avg:57.94ms
step:1097/2330 train_time:63557ms step_avg:57.94ms
step:1098/2330 train_time:63617ms step_avg:57.94ms
step:1099/2330 train_time:63674ms step_avg:57.94ms
step:1100/2330 train_time:63734ms step_avg:57.94ms
step:1101/2330 train_time:63791ms step_avg:57.94ms
step:1102/2330 train_time:63851ms step_avg:57.94ms
step:1103/2330 train_time:63908ms step_avg:57.94ms
step:1104/2330 train_time:63967ms step_avg:57.94ms
step:1105/2330 train_time:64024ms step_avg:57.94ms
step:1106/2330 train_time:64086ms step_avg:57.94ms
step:1107/2330 train_time:64143ms step_avg:57.94ms
step:1108/2330 train_time:64203ms step_avg:57.95ms
step:1109/2330 train_time:64260ms step_avg:57.94ms
step:1110/2330 train_time:64320ms step_avg:57.95ms
step:1111/2330 train_time:64378ms step_avg:57.95ms
step:1112/2330 train_time:64438ms step_avg:57.95ms
step:1113/2330 train_time:64495ms step_avg:57.95ms
step:1114/2330 train_time:64554ms step_avg:57.95ms
step:1115/2330 train_time:64611ms step_avg:57.95ms
step:1116/2330 train_time:64671ms step_avg:57.95ms
step:1117/2330 train_time:64728ms step_avg:57.95ms
step:1118/2330 train_time:64789ms step_avg:57.95ms
step:1119/2330 train_time:64845ms step_avg:57.95ms
step:1120/2330 train_time:64905ms step_avg:57.95ms
step:1121/2330 train_time:64963ms step_avg:57.95ms
step:1122/2330 train_time:65024ms step_avg:57.95ms
step:1123/2330 train_time:65081ms step_avg:57.95ms
step:1124/2330 train_time:65141ms step_avg:57.95ms
step:1125/2330 train_time:65199ms step_avg:57.95ms
step:1126/2330 train_time:65259ms step_avg:57.96ms
step:1127/2330 train_time:65317ms step_avg:57.96ms
step:1128/2330 train_time:65377ms step_avg:57.96ms
step:1129/2330 train_time:65434ms step_avg:57.96ms
step:1130/2330 train_time:65494ms step_avg:57.96ms
step:1131/2330 train_time:65550ms step_avg:57.96ms
step:1132/2330 train_time:65611ms step_avg:57.96ms
step:1133/2330 train_time:65667ms step_avg:57.96ms
step:1134/2330 train_time:65728ms step_avg:57.96ms
step:1135/2330 train_time:65785ms step_avg:57.96ms
step:1136/2330 train_time:65845ms step_avg:57.96ms
step:1137/2330 train_time:65902ms step_avg:57.96ms
step:1138/2330 train_time:65961ms step_avg:57.96ms
step:1139/2330 train_time:66019ms step_avg:57.96ms
step:1140/2330 train_time:66079ms step_avg:57.96ms
step:1141/2330 train_time:66137ms step_avg:57.96ms
step:1142/2330 train_time:66197ms step_avg:57.97ms
step:1143/2330 train_time:66254ms step_avg:57.97ms
step:1144/2330 train_time:66314ms step_avg:57.97ms
step:1145/2330 train_time:66371ms step_avg:57.97ms
step:1146/2330 train_time:66430ms step_avg:57.97ms
step:1147/2330 train_time:66488ms step_avg:57.97ms
step:1148/2330 train_time:66547ms step_avg:57.97ms
step:1149/2330 train_time:66604ms step_avg:57.97ms
step:1150/2330 train_time:66665ms step_avg:57.97ms
step:1151/2330 train_time:66722ms step_avg:57.97ms
step:1152/2330 train_time:66782ms step_avg:57.97ms
step:1153/2330 train_time:66839ms step_avg:57.97ms
step:1154/2330 train_time:66899ms step_avg:57.97ms
step:1155/2330 train_time:66956ms step_avg:57.97ms
step:1156/2330 train_time:67015ms step_avg:57.97ms
step:1157/2330 train_time:67073ms step_avg:57.97ms
step:1158/2330 train_time:67133ms step_avg:57.97ms
step:1159/2330 train_time:67190ms step_avg:57.97ms
step:1160/2330 train_time:67249ms step_avg:57.97ms
step:1161/2330 train_time:67306ms step_avg:57.97ms
step:1162/2330 train_time:67367ms step_avg:57.98ms
step:1163/2330 train_time:67424ms step_avg:57.97ms
step:1164/2330 train_time:67485ms step_avg:57.98ms
step:1165/2330 train_time:67542ms step_avg:57.98ms
step:1166/2330 train_time:67602ms step_avg:57.98ms
step:1167/2330 train_time:67659ms step_avg:57.98ms
step:1168/2330 train_time:67719ms step_avg:57.98ms
step:1169/2330 train_time:67776ms step_avg:57.98ms
step:1170/2330 train_time:67837ms step_avg:57.98ms
step:1171/2330 train_time:67894ms step_avg:57.98ms
step:1172/2330 train_time:67954ms step_avg:57.98ms
step:1173/2330 train_time:68011ms step_avg:57.98ms
step:1174/2330 train_time:68070ms step_avg:57.98ms
step:1175/2330 train_time:68128ms step_avg:57.98ms
step:1176/2330 train_time:68187ms step_avg:57.98ms
step:1177/2330 train_time:68244ms step_avg:57.98ms
step:1178/2330 train_time:68305ms step_avg:57.98ms
step:1179/2330 train_time:68363ms step_avg:57.98ms
step:1180/2330 train_time:68423ms step_avg:57.99ms
step:1181/2330 train_time:68480ms step_avg:57.98ms
step:1182/2330 train_time:68540ms step_avg:57.99ms
step:1183/2330 train_time:68597ms step_avg:57.99ms
step:1184/2330 train_time:68658ms step_avg:57.99ms
step:1185/2330 train_time:68715ms step_avg:57.99ms
step:1186/2330 train_time:68776ms step_avg:57.99ms
step:1187/2330 train_time:68833ms step_avg:57.99ms
step:1188/2330 train_time:68894ms step_avg:57.99ms
step:1189/2330 train_time:68950ms step_avg:57.99ms
step:1190/2330 train_time:69010ms step_avg:57.99ms
step:1191/2330 train_time:69067ms step_avg:57.99ms
step:1192/2330 train_time:69129ms step_avg:57.99ms
step:1193/2330 train_time:69186ms step_avg:57.99ms
step:1194/2330 train_time:69246ms step_avg:57.99ms
step:1195/2330 train_time:69303ms step_avg:57.99ms
step:1196/2330 train_time:69363ms step_avg:58.00ms
step:1197/2330 train_time:69421ms step_avg:58.00ms
step:1198/2330 train_time:69480ms step_avg:58.00ms
step:1199/2330 train_time:69537ms step_avg:58.00ms
step:1200/2330 train_time:69599ms step_avg:58.00ms
step:1201/2330 train_time:69655ms step_avg:58.00ms
step:1202/2330 train_time:69716ms step_avg:58.00ms
step:1203/2330 train_time:69773ms step_avg:58.00ms
step:1204/2330 train_time:69833ms step_avg:58.00ms
step:1205/2330 train_time:69889ms step_avg:58.00ms
step:1206/2330 train_time:69950ms step_avg:58.00ms
step:1207/2330 train_time:70007ms step_avg:58.00ms
step:1208/2330 train_time:70068ms step_avg:58.00ms
step:1209/2330 train_time:70125ms step_avg:58.00ms
step:1210/2330 train_time:70185ms step_avg:58.00ms
step:1211/2330 train_time:70242ms step_avg:58.00ms
step:1212/2330 train_time:70302ms step_avg:58.00ms
step:1213/2330 train_time:70359ms step_avg:58.00ms
step:1214/2330 train_time:70419ms step_avg:58.01ms
step:1215/2330 train_time:70476ms step_avg:58.00ms
step:1216/2330 train_time:70537ms step_avg:58.01ms
step:1217/2330 train_time:70593ms step_avg:58.01ms
step:1218/2330 train_time:70655ms step_avg:58.01ms
step:1219/2330 train_time:70712ms step_avg:58.01ms
step:1220/2330 train_time:70772ms step_avg:58.01ms
step:1221/2330 train_time:70830ms step_avg:58.01ms
step:1222/2330 train_time:70890ms step_avg:58.01ms
step:1223/2330 train_time:70947ms step_avg:58.01ms
step:1224/2330 train_time:71008ms step_avg:58.01ms
step:1225/2330 train_time:71065ms step_avg:58.01ms
step:1226/2330 train_time:71126ms step_avg:58.01ms
step:1227/2330 train_time:71183ms step_avg:58.01ms
step:1228/2330 train_time:71242ms step_avg:58.01ms
step:1229/2330 train_time:71299ms step_avg:58.01ms
step:1230/2330 train_time:71359ms step_avg:58.02ms
step:1231/2330 train_time:71416ms step_avg:58.01ms
step:1232/2330 train_time:71476ms step_avg:58.02ms
step:1233/2330 train_time:71533ms step_avg:58.02ms
step:1234/2330 train_time:71592ms step_avg:58.02ms
step:1235/2330 train_time:71649ms step_avg:58.02ms
step:1236/2330 train_time:71710ms step_avg:58.02ms
step:1237/2330 train_time:71767ms step_avg:58.02ms
step:1238/2330 train_time:71828ms step_avg:58.02ms
step:1239/2330 train_time:71885ms step_avg:58.02ms
step:1240/2330 train_time:71945ms step_avg:58.02ms
step:1241/2330 train_time:72001ms step_avg:58.02ms
step:1242/2330 train_time:72062ms step_avg:58.02ms
step:1243/2330 train_time:72119ms step_avg:58.02ms
step:1244/2330 train_time:72179ms step_avg:58.02ms
step:1245/2330 train_time:72236ms step_avg:58.02ms
step:1246/2330 train_time:72297ms step_avg:58.02ms
step:1247/2330 train_time:72354ms step_avg:58.02ms
step:1248/2330 train_time:72413ms step_avg:58.02ms
step:1249/2330 train_time:72470ms step_avg:58.02ms
step:1250/2330 train_time:72531ms step_avg:58.02ms
step:1250/2330 val_loss:3.9842 train_time:72611ms step_avg:58.09ms
step:1251/2330 train_time:72631ms step_avg:58.06ms
step:1252/2330 train_time:72652ms step_avg:58.03ms
step:1253/2330 train_time:72712ms step_avg:58.03ms
step:1254/2330 train_time:72774ms step_avg:58.03ms
step:1255/2330 train_time:72832ms step_avg:58.03ms
step:1256/2330 train_time:72893ms step_avg:58.04ms
step:1257/2330 train_time:72949ms step_avg:58.03ms
step:1258/2330 train_time:73009ms step_avg:58.04ms
step:1259/2330 train_time:73066ms step_avg:58.03ms
step:1260/2330 train_time:73126ms step_avg:58.04ms
step:1261/2330 train_time:73182ms step_avg:58.03ms
step:1262/2330 train_time:73241ms step_avg:58.04ms
step:1263/2330 train_time:73297ms step_avg:58.03ms
step:1264/2330 train_time:73357ms step_avg:58.04ms
step:1265/2330 train_time:73413ms step_avg:58.03ms
step:1266/2330 train_time:73472ms step_avg:58.03ms
step:1267/2330 train_time:73529ms step_avg:58.03ms
step:1268/2330 train_time:73590ms step_avg:58.04ms
step:1269/2330 train_time:73651ms step_avg:58.04ms
step:1270/2330 train_time:73711ms step_avg:58.04ms
step:1271/2330 train_time:73768ms step_avg:58.04ms
step:1272/2330 train_time:73830ms step_avg:58.04ms
step:1273/2330 train_time:73887ms step_avg:58.04ms
step:1274/2330 train_time:73947ms step_avg:58.04ms
step:1275/2330 train_time:74003ms step_avg:58.04ms
step:1276/2330 train_time:74063ms step_avg:58.04ms
step:1277/2330 train_time:74120ms step_avg:58.04ms
step:1278/2330 train_time:74179ms step_avg:58.04ms
step:1279/2330 train_time:74236ms step_avg:58.04ms
step:1280/2330 train_time:74297ms step_avg:58.04ms
step:1281/2330 train_time:74353ms step_avg:58.04ms
step:1282/2330 train_time:74413ms step_avg:58.04ms
step:1283/2330 train_time:74470ms step_avg:58.04ms
step:1284/2330 train_time:74530ms step_avg:58.05ms
step:1285/2330 train_time:74588ms step_avg:58.05ms
step:1286/2330 train_time:74648ms step_avg:58.05ms
step:1287/2330 train_time:74706ms step_avg:58.05ms
step:1288/2330 train_time:74767ms step_avg:58.05ms
step:1289/2330 train_time:74825ms step_avg:58.05ms
step:1290/2330 train_time:74885ms step_avg:58.05ms
step:1291/2330 train_time:74942ms step_avg:58.05ms
step:1292/2330 train_time:75003ms step_avg:58.05ms
step:1293/2330 train_time:75060ms step_avg:58.05ms
step:1294/2330 train_time:75120ms step_avg:58.05ms
step:1295/2330 train_time:75176ms step_avg:58.05ms
step:1296/2330 train_time:75236ms step_avg:58.05ms
step:1297/2330 train_time:75292ms step_avg:58.05ms
step:1298/2330 train_time:75352ms step_avg:58.05ms
step:1299/2330 train_time:75409ms step_avg:58.05ms
step:1300/2330 train_time:75469ms step_avg:58.05ms
step:1301/2330 train_time:75526ms step_avg:58.05ms
step:1302/2330 train_time:75587ms step_avg:58.05ms
step:1303/2330 train_time:75645ms step_avg:58.05ms
step:1304/2330 train_time:75705ms step_avg:58.06ms
step:1305/2330 train_time:75763ms step_avg:58.06ms
step:1306/2330 train_time:75823ms step_avg:58.06ms
step:1307/2330 train_time:75880ms step_avg:58.06ms
step:1308/2330 train_time:75941ms step_avg:58.06ms
step:1309/2330 train_time:75997ms step_avg:58.06ms
step:1310/2330 train_time:76057ms step_avg:58.06ms
step:1311/2330 train_time:76115ms step_avg:58.06ms
step:1312/2330 train_time:76174ms step_avg:58.06ms
step:1313/2330 train_time:76231ms step_avg:58.06ms
step:1314/2330 train_time:76291ms step_avg:58.06ms
step:1315/2330 train_time:76347ms step_avg:58.06ms
step:1316/2330 train_time:76407ms step_avg:58.06ms
step:1317/2330 train_time:76464ms step_avg:58.06ms
step:1318/2330 train_time:76524ms step_avg:58.06ms
step:1319/2330 train_time:76581ms step_avg:58.06ms
step:1320/2330 train_time:76642ms step_avg:58.06ms
step:1321/2330 train_time:76699ms step_avg:58.06ms
step:1322/2330 train_time:76759ms step_avg:58.06ms
step:1323/2330 train_time:76816ms step_avg:58.06ms
step:1324/2330 train_time:76877ms step_avg:58.06ms
step:1325/2330 train_time:76934ms step_avg:58.06ms
step:1326/2330 train_time:76995ms step_avg:58.07ms
step:1327/2330 train_time:77052ms step_avg:58.06ms
step:1328/2330 train_time:77112ms step_avg:58.07ms
step:1329/2330 train_time:77170ms step_avg:58.07ms
step:1330/2330 train_time:77229ms step_avg:58.07ms
step:1331/2330 train_time:77286ms step_avg:58.07ms
step:1332/2330 train_time:77346ms step_avg:58.07ms
step:1333/2330 train_time:77403ms step_avg:58.07ms
step:1334/2330 train_time:77462ms step_avg:58.07ms
step:1335/2330 train_time:77519ms step_avg:58.07ms
step:1336/2330 train_time:77580ms step_avg:58.07ms
step:1337/2330 train_time:77637ms step_avg:58.07ms
step:1338/2330 train_time:77698ms step_avg:58.07ms
step:1339/2330 train_time:77755ms step_avg:58.07ms
step:1340/2330 train_time:77815ms step_avg:58.07ms
step:1341/2330 train_time:77872ms step_avg:58.07ms
step:1342/2330 train_time:77932ms step_avg:58.07ms
step:1343/2330 train_time:77989ms step_avg:58.07ms
step:1344/2330 train_time:78050ms step_avg:58.07ms
step:1345/2330 train_time:78108ms step_avg:58.07ms
step:1346/2330 train_time:78168ms step_avg:58.07ms
step:1347/2330 train_time:78224ms step_avg:58.07ms
step:1348/2330 train_time:78285ms step_avg:58.07ms
step:1349/2330 train_time:78342ms step_avg:58.07ms
step:1350/2330 train_time:78402ms step_avg:58.08ms
step:1351/2330 train_time:78459ms step_avg:58.07ms
step:1352/2330 train_time:78518ms step_avg:58.08ms
step:1353/2330 train_time:78575ms step_avg:58.07ms
step:1354/2330 train_time:78636ms step_avg:58.08ms
step:1355/2330 train_time:78693ms step_avg:58.08ms
step:1356/2330 train_time:78754ms step_avg:58.08ms
step:1357/2330 train_time:78811ms step_avg:58.08ms
step:1358/2330 train_time:78871ms step_avg:58.08ms
step:1359/2330 train_time:78928ms step_avg:58.08ms
step:1360/2330 train_time:78988ms step_avg:58.08ms
step:1361/2330 train_time:79045ms step_avg:58.08ms
step:1362/2330 train_time:79105ms step_avg:58.08ms
step:1363/2330 train_time:79163ms step_avg:58.08ms
step:1364/2330 train_time:79223ms step_avg:58.08ms
step:1365/2330 train_time:79280ms step_avg:58.08ms
step:1366/2330 train_time:79339ms step_avg:58.08ms
step:1367/2330 train_time:79397ms step_avg:58.08ms
step:1368/2330 train_time:79457ms step_avg:58.08ms
step:1369/2330 train_time:79514ms step_avg:58.08ms
step:1370/2330 train_time:79574ms step_avg:58.08ms
step:1371/2330 train_time:79631ms step_avg:58.08ms
step:1372/2330 train_time:79692ms step_avg:58.08ms
step:1373/2330 train_time:79749ms step_avg:58.08ms
step:1374/2330 train_time:79808ms step_avg:58.08ms
step:1375/2330 train_time:79866ms step_avg:58.08ms
step:1376/2330 train_time:79926ms step_avg:58.09ms
step:1377/2330 train_time:79983ms step_avg:58.08ms
step:1378/2330 train_time:80044ms step_avg:58.09ms
step:1379/2330 train_time:80100ms step_avg:58.09ms
step:1380/2330 train_time:80161ms step_avg:58.09ms
step:1381/2330 train_time:80218ms step_avg:58.09ms
step:1382/2330 train_time:80278ms step_avg:58.09ms
step:1383/2330 train_time:80334ms step_avg:58.09ms
step:1384/2330 train_time:80395ms step_avg:58.09ms
step:1385/2330 train_time:80452ms step_avg:58.09ms
step:1386/2330 train_time:80511ms step_avg:58.09ms
step:1387/2330 train_time:80569ms step_avg:58.09ms
step:1388/2330 train_time:80629ms step_avg:58.09ms
step:1389/2330 train_time:80686ms step_avg:58.09ms
step:1390/2330 train_time:80746ms step_avg:58.09ms
step:1391/2330 train_time:80803ms step_avg:58.09ms
step:1392/2330 train_time:80864ms step_avg:58.09ms
step:1393/2330 train_time:80921ms step_avg:58.09ms
step:1394/2330 train_time:80982ms step_avg:58.09ms
step:1395/2330 train_time:81039ms step_avg:58.09ms
step:1396/2330 train_time:81099ms step_avg:58.09ms
step:1397/2330 train_time:81156ms step_avg:58.09ms
step:1398/2330 train_time:81216ms step_avg:58.09ms
step:1399/2330 train_time:81274ms step_avg:58.09ms
step:1400/2330 train_time:81334ms step_avg:58.10ms
step:1401/2330 train_time:81391ms step_avg:58.09ms
step:1402/2330 train_time:81450ms step_avg:58.10ms
step:1403/2330 train_time:81508ms step_avg:58.10ms
step:1404/2330 train_time:81568ms step_avg:58.10ms
step:1405/2330 train_time:81626ms step_avg:58.10ms
step:1406/2330 train_time:81685ms step_avg:58.10ms
step:1407/2330 train_time:81742ms step_avg:58.10ms
step:1408/2330 train_time:81802ms step_avg:58.10ms
step:1409/2330 train_time:81859ms step_avg:58.10ms
step:1410/2330 train_time:81919ms step_avg:58.10ms
step:1411/2330 train_time:81976ms step_avg:58.10ms
step:1412/2330 train_time:82036ms step_avg:58.10ms
step:1413/2330 train_time:82093ms step_avg:58.10ms
step:1414/2330 train_time:82154ms step_avg:58.10ms
step:1415/2330 train_time:82211ms step_avg:58.10ms
step:1416/2330 train_time:82271ms step_avg:58.10ms
step:1417/2330 train_time:82328ms step_avg:58.10ms
step:1418/2330 train_time:82387ms step_avg:58.10ms
step:1419/2330 train_time:82444ms step_avg:58.10ms
step:1420/2330 train_time:82504ms step_avg:58.10ms
step:1421/2330 train_time:82560ms step_avg:58.10ms
step:1422/2330 train_time:82621ms step_avg:58.10ms
step:1423/2330 train_time:82677ms step_avg:58.10ms
step:1424/2330 train_time:82738ms step_avg:58.10ms
step:1425/2330 train_time:82795ms step_avg:58.10ms
step:1426/2330 train_time:82855ms step_avg:58.10ms
step:1427/2330 train_time:82912ms step_avg:58.10ms
step:1428/2330 train_time:82972ms step_avg:58.10ms
step:1429/2330 train_time:83029ms step_avg:58.10ms
step:1430/2330 train_time:83090ms step_avg:58.10ms
step:1431/2330 train_time:83146ms step_avg:58.10ms
step:1432/2330 train_time:83207ms step_avg:58.11ms
step:1433/2330 train_time:83263ms step_avg:58.10ms
step:1434/2330 train_time:83324ms step_avg:58.11ms
step:1435/2330 train_time:83381ms step_avg:58.11ms
step:1436/2330 train_time:83442ms step_avg:58.11ms
step:1437/2330 train_time:83499ms step_avg:58.11ms
step:1438/2330 train_time:83559ms step_avg:58.11ms
step:1439/2330 train_time:83616ms step_avg:58.11ms
step:1440/2330 train_time:83676ms step_avg:58.11ms
step:1441/2330 train_time:83733ms step_avg:58.11ms
step:1442/2330 train_time:83793ms step_avg:58.11ms
step:1443/2330 train_time:83851ms step_avg:58.11ms
step:1444/2330 train_time:83910ms step_avg:58.11ms
step:1445/2330 train_time:83967ms step_avg:58.11ms
step:1446/2330 train_time:84027ms step_avg:58.11ms
step:1447/2330 train_time:84084ms step_avg:58.11ms
step:1448/2330 train_time:84145ms step_avg:58.11ms
step:1449/2330 train_time:84202ms step_avg:58.11ms
step:1450/2330 train_time:84263ms step_avg:58.11ms
step:1451/2330 train_time:84319ms step_avg:58.11ms
step:1452/2330 train_time:84380ms step_avg:58.11ms
step:1453/2330 train_time:84437ms step_avg:58.11ms
step:1454/2330 train_time:84498ms step_avg:58.11ms
step:1455/2330 train_time:84554ms step_avg:58.11ms
step:1456/2330 train_time:84615ms step_avg:58.11ms
step:1457/2330 train_time:84672ms step_avg:58.11ms
step:1458/2330 train_time:84732ms step_avg:58.12ms
step:1459/2330 train_time:84789ms step_avg:58.11ms
step:1460/2330 train_time:84849ms step_avg:58.12ms
step:1461/2330 train_time:84906ms step_avg:58.11ms
step:1462/2330 train_time:84966ms step_avg:58.12ms
step:1463/2330 train_time:85023ms step_avg:58.12ms
step:1464/2330 train_time:85083ms step_avg:58.12ms
step:1465/2330 train_time:85141ms step_avg:58.12ms
step:1466/2330 train_time:85201ms step_avg:58.12ms
step:1467/2330 train_time:85258ms step_avg:58.12ms
step:1468/2330 train_time:85318ms step_avg:58.12ms
step:1469/2330 train_time:85374ms step_avg:58.12ms
step:1470/2330 train_time:85436ms step_avg:58.12ms
step:1471/2330 train_time:85493ms step_avg:58.12ms
step:1472/2330 train_time:85553ms step_avg:58.12ms
step:1473/2330 train_time:85611ms step_avg:58.12ms
step:1474/2330 train_time:85670ms step_avg:58.12ms
step:1475/2330 train_time:85727ms step_avg:58.12ms
step:1476/2330 train_time:85787ms step_avg:58.12ms
step:1477/2330 train_time:85845ms step_avg:58.12ms
step:1478/2330 train_time:85905ms step_avg:58.12ms
step:1479/2330 train_time:85962ms step_avg:58.12ms
step:1480/2330 train_time:86022ms step_avg:58.12ms
step:1481/2330 train_time:86079ms step_avg:58.12ms
step:1482/2330 train_time:86139ms step_avg:58.12ms
step:1483/2330 train_time:86197ms step_avg:58.12ms
step:1484/2330 train_time:86257ms step_avg:58.12ms
step:1485/2330 train_time:86314ms step_avg:58.12ms
step:1486/2330 train_time:86375ms step_avg:58.13ms
step:1487/2330 train_time:86431ms step_avg:58.12ms
step:1488/2330 train_time:86492ms step_avg:58.13ms
step:1489/2330 train_time:86549ms step_avg:58.13ms
step:1490/2330 train_time:86610ms step_avg:58.13ms
step:1491/2330 train_time:86666ms step_avg:58.13ms
step:1492/2330 train_time:86727ms step_avg:58.13ms
step:1493/2330 train_time:86783ms step_avg:58.13ms
step:1494/2330 train_time:86845ms step_avg:58.13ms
step:1495/2330 train_time:86902ms step_avg:58.13ms
step:1496/2330 train_time:86963ms step_avg:58.13ms
step:1497/2330 train_time:87019ms step_avg:58.13ms
step:1498/2330 train_time:87080ms step_avg:58.13ms
step:1499/2330 train_time:87137ms step_avg:58.13ms
step:1500/2330 train_time:87197ms step_avg:58.13ms
step:1500/2330 val_loss:3.9042 train_time:87277ms step_avg:58.18ms
step:1501/2330 train_time:87297ms step_avg:58.16ms
step:1502/2330 train_time:87318ms step_avg:58.13ms
step:1503/2330 train_time:87380ms step_avg:58.14ms
step:1504/2330 train_time:87443ms step_avg:58.14ms
step:1505/2330 train_time:87500ms step_avg:58.14ms
step:1506/2330 train_time:87560ms step_avg:58.14ms
step:1507/2330 train_time:87617ms step_avg:58.14ms
step:1508/2330 train_time:87676ms step_avg:58.14ms
step:1509/2330 train_time:87733ms step_avg:58.14ms
step:1510/2330 train_time:87792ms step_avg:58.14ms
step:1511/2330 train_time:87849ms step_avg:58.14ms
step:1512/2330 train_time:87908ms step_avg:58.14ms
step:1513/2330 train_time:87965ms step_avg:58.14ms
step:1514/2330 train_time:88023ms step_avg:58.14ms
step:1515/2330 train_time:88079ms step_avg:58.14ms
step:1516/2330 train_time:88139ms step_avg:58.14ms
step:1517/2330 train_time:88195ms step_avg:58.14ms
step:1518/2330 train_time:88257ms step_avg:58.14ms
step:1519/2330 train_time:88315ms step_avg:58.14ms
step:1520/2330 train_time:88377ms step_avg:58.14ms
step:1521/2330 train_time:88436ms step_avg:58.14ms
step:1522/2330 train_time:88497ms step_avg:58.14ms
step:1523/2330 train_time:88555ms step_avg:58.14ms
step:1524/2330 train_time:88614ms step_avg:58.15ms
step:1525/2330 train_time:88672ms step_avg:58.15ms
step:1526/2330 train_time:88731ms step_avg:58.15ms
step:1527/2330 train_time:88788ms step_avg:58.15ms
step:1528/2330 train_time:88848ms step_avg:58.15ms
step:1529/2330 train_time:88906ms step_avg:58.15ms
step:1530/2330 train_time:88965ms step_avg:58.15ms
step:1531/2330 train_time:89022ms step_avg:58.15ms
step:1532/2330 train_time:89082ms step_avg:58.15ms
step:1533/2330 train_time:89138ms step_avg:58.15ms
step:1534/2330 train_time:89199ms step_avg:58.15ms
step:1535/2330 train_time:89257ms step_avg:58.15ms
step:1536/2330 train_time:89318ms step_avg:58.15ms
step:1537/2330 train_time:89377ms step_avg:58.15ms
step:1538/2330 train_time:89438ms step_avg:58.15ms
step:1539/2330 train_time:89497ms step_avg:58.15ms
step:1540/2330 train_time:89557ms step_avg:58.15ms
step:1541/2330 train_time:89615ms step_avg:58.15ms
step:1542/2330 train_time:89675ms step_avg:58.16ms
step:1543/2330 train_time:89734ms step_avg:58.16ms
step:1544/2330 train_time:89794ms step_avg:58.16ms
step:1545/2330 train_time:89851ms step_avg:58.16ms
step:1546/2330 train_time:89912ms step_avg:58.16ms
step:1547/2330 train_time:89970ms step_avg:58.16ms
step:1548/2330 train_time:90029ms step_avg:58.16ms
step:1549/2330 train_time:90087ms step_avg:58.16ms
step:1550/2330 train_time:90147ms step_avg:58.16ms
step:1551/2330 train_time:90204ms step_avg:58.16ms
step:1552/2330 train_time:90266ms step_avg:58.16ms
step:1553/2330 train_time:90324ms step_avg:58.16ms
step:1554/2330 train_time:90385ms step_avg:58.16ms
step:1555/2330 train_time:90442ms step_avg:58.16ms
step:1556/2330 train_time:90505ms step_avg:58.16ms
step:1557/2330 train_time:90562ms step_avg:58.16ms
step:1558/2330 train_time:90624ms step_avg:58.17ms
step:1559/2330 train_time:90681ms step_avg:58.17ms
step:1560/2330 train_time:90743ms step_avg:58.17ms
step:1561/2330 train_time:90800ms step_avg:58.17ms
step:1562/2330 train_time:90862ms step_avg:58.17ms
step:1563/2330 train_time:90920ms step_avg:58.17ms
step:1564/2330 train_time:90980ms step_avg:58.17ms
step:1565/2330 train_time:91038ms step_avg:58.17ms
step:1566/2330 train_time:91097ms step_avg:58.17ms
step:1567/2330 train_time:91155ms step_avg:58.17ms
step:1568/2330 train_time:91216ms step_avg:58.17ms
step:1569/2330 train_time:91273ms step_avg:58.17ms
step:1570/2330 train_time:91334ms step_avg:58.17ms
step:1571/2330 train_time:91393ms step_avg:58.18ms
step:1572/2330 train_time:91454ms step_avg:58.18ms
step:1573/2330 train_time:91513ms step_avg:58.18ms
step:1574/2330 train_time:91574ms step_avg:58.18ms
step:1575/2330 train_time:91632ms step_avg:58.18ms
step:1576/2330 train_time:91693ms step_avg:58.18ms
step:1577/2330 train_time:91750ms step_avg:58.18ms
step:1578/2330 train_time:91811ms step_avg:58.18ms
step:1579/2330 train_time:91868ms step_avg:58.18ms
step:1580/2330 train_time:91930ms step_avg:58.18ms
step:1581/2330 train_time:91987ms step_avg:58.18ms
step:1582/2330 train_time:92048ms step_avg:58.18ms
step:1583/2330 train_time:92104ms step_avg:58.18ms
step:1584/2330 train_time:92172ms step_avg:58.19ms
step:1585/2330 train_time:92223ms step_avg:58.19ms
step:1586/2330 train_time:92284ms step_avg:58.19ms
step:1587/2330 train_time:92341ms step_avg:58.19ms
step:1588/2330 train_time:92403ms step_avg:58.19ms
step:1589/2330 train_time:92460ms step_avg:58.19ms
step:1590/2330 train_time:92523ms step_avg:58.19ms
step:1591/2330 train_time:92581ms step_avg:58.19ms
step:1592/2330 train_time:92641ms step_avg:58.19ms
step:1593/2330 train_time:92698ms step_avg:58.19ms
step:1594/2330 train_time:92761ms step_avg:58.19ms
step:1595/2330 train_time:92818ms step_avg:58.19ms
step:1596/2330 train_time:92879ms step_avg:58.19ms
step:1597/2330 train_time:92938ms step_avg:58.20ms
step:1598/2330 train_time:92997ms step_avg:58.20ms
step:1599/2330 train_time:93055ms step_avg:58.20ms
step:1600/2330 train_time:93116ms step_avg:58.20ms
step:1601/2330 train_time:93175ms step_avg:58.20ms
step:1602/2330 train_time:93235ms step_avg:58.20ms
step:1603/2330 train_time:93293ms step_avg:58.20ms
step:1604/2330 train_time:93354ms step_avg:58.20ms
step:1605/2330 train_time:93411ms step_avg:58.20ms
step:1606/2330 train_time:93472ms step_avg:58.20ms
step:1607/2330 train_time:93529ms step_avg:58.20ms
step:1608/2330 train_time:93591ms step_avg:58.20ms
step:1609/2330 train_time:93647ms step_avg:58.20ms
step:1610/2330 train_time:93709ms step_avg:58.20ms
step:1611/2330 train_time:93766ms step_avg:58.20ms
step:1612/2330 train_time:93828ms step_avg:58.21ms
step:1613/2330 train_time:93886ms step_avg:58.21ms
step:1614/2330 train_time:93946ms step_avg:58.21ms
step:1615/2330 train_time:94003ms step_avg:58.21ms
step:1616/2330 train_time:94064ms step_avg:58.21ms
step:1617/2330 train_time:94121ms step_avg:58.21ms
step:1618/2330 train_time:94183ms step_avg:58.21ms
step:1619/2330 train_time:94240ms step_avg:58.21ms
step:1620/2330 train_time:94301ms step_avg:58.21ms
step:1621/2330 train_time:94358ms step_avg:58.21ms
step:1622/2330 train_time:94418ms step_avg:58.21ms
step:1623/2330 train_time:94477ms step_avg:58.21ms
step:1624/2330 train_time:94537ms step_avg:58.21ms
step:1625/2330 train_time:94595ms step_avg:58.21ms
step:1626/2330 train_time:94656ms step_avg:58.21ms
step:1627/2330 train_time:94713ms step_avg:58.21ms
step:1628/2330 train_time:94774ms step_avg:58.22ms
step:1629/2330 train_time:94833ms step_avg:58.22ms
step:1630/2330 train_time:94893ms step_avg:58.22ms
step:1631/2330 train_time:94951ms step_avg:58.22ms
step:1632/2330 train_time:95012ms step_avg:58.22ms
step:1633/2330 train_time:95070ms step_avg:58.22ms
step:1634/2330 train_time:95131ms step_avg:58.22ms
step:1635/2330 train_time:95189ms step_avg:58.22ms
step:1636/2330 train_time:95249ms step_avg:58.22ms
step:1637/2330 train_time:95307ms step_avg:58.22ms
step:1638/2330 train_time:95368ms step_avg:58.22ms
step:1639/2330 train_time:95424ms step_avg:58.22ms
step:1640/2330 train_time:95486ms step_avg:58.22ms
step:1641/2330 train_time:95543ms step_avg:58.22ms
step:1642/2330 train_time:95604ms step_avg:58.22ms
step:1643/2330 train_time:95662ms step_avg:58.22ms
step:1644/2330 train_time:95724ms step_avg:58.23ms
step:1645/2330 train_time:95781ms step_avg:58.23ms
step:1646/2330 train_time:95843ms step_avg:58.23ms
step:1647/2330 train_time:95900ms step_avg:58.23ms
step:1648/2330 train_time:95962ms step_avg:58.23ms
step:1649/2330 train_time:96019ms step_avg:58.23ms
step:1650/2330 train_time:96079ms step_avg:58.23ms
step:1651/2330 train_time:96138ms step_avg:58.23ms
step:1652/2330 train_time:96198ms step_avg:58.23ms
step:1653/2330 train_time:96256ms step_avg:58.23ms
step:1654/2330 train_time:96317ms step_avg:58.23ms
step:1655/2330 train_time:96374ms step_avg:58.23ms
step:1656/2330 train_time:96435ms step_avg:58.23ms
step:1657/2330 train_time:96494ms step_avg:58.23ms
step:1658/2330 train_time:96555ms step_avg:58.24ms
step:1659/2330 train_time:96613ms step_avg:58.24ms
step:1660/2330 train_time:96673ms step_avg:58.24ms
step:1661/2330 train_time:96730ms step_avg:58.24ms
step:1662/2330 train_time:96792ms step_avg:58.24ms
step:1663/2330 train_time:96849ms step_avg:58.24ms
step:1664/2330 train_time:96910ms step_avg:58.24ms
step:1665/2330 train_time:96966ms step_avg:58.24ms
step:1666/2330 train_time:97029ms step_avg:58.24ms
step:1667/2330 train_time:97086ms step_avg:58.24ms
step:1668/2330 train_time:97146ms step_avg:58.24ms
step:1669/2330 train_time:97203ms step_avg:58.24ms
step:1670/2330 train_time:97265ms step_avg:58.24ms
step:1671/2330 train_time:97322ms step_avg:58.24ms
step:1672/2330 train_time:97384ms step_avg:58.24ms
step:1673/2330 train_time:97441ms step_avg:58.24ms
step:1674/2330 train_time:97502ms step_avg:58.24ms
step:1675/2330 train_time:97559ms step_avg:58.24ms
step:1676/2330 train_time:97620ms step_avg:58.25ms
step:1677/2330 train_time:97677ms step_avg:58.24ms
step:1678/2330 train_time:97738ms step_avg:58.25ms
step:1679/2330 train_time:97797ms step_avg:58.25ms
step:1680/2330 train_time:97857ms step_avg:58.25ms
step:1681/2330 train_time:97915ms step_avg:58.25ms
step:1682/2330 train_time:97976ms step_avg:58.25ms
step:1683/2330 train_time:98035ms step_avg:58.25ms
step:1684/2330 train_time:98096ms step_avg:58.25ms
step:1685/2330 train_time:98155ms step_avg:58.25ms
step:1686/2330 train_time:98215ms step_avg:58.25ms
step:1687/2330 train_time:98273ms step_avg:58.25ms
step:1688/2330 train_time:98334ms step_avg:58.25ms
step:1689/2330 train_time:98392ms step_avg:58.25ms
step:1690/2330 train_time:98451ms step_avg:58.26ms
step:1691/2330 train_time:98509ms step_avg:58.25ms
step:1692/2330 train_time:98570ms step_avg:58.26ms
step:1693/2330 train_time:98627ms step_avg:58.26ms
step:1694/2330 train_time:98688ms step_avg:58.26ms
step:1695/2330 train_time:98746ms step_avg:58.26ms
step:1696/2330 train_time:98806ms step_avg:58.26ms
step:1697/2330 train_time:98863ms step_avg:58.26ms
step:1698/2330 train_time:98925ms step_avg:58.26ms
step:1699/2330 train_time:98983ms step_avg:58.26ms
step:1700/2330 train_time:99043ms step_avg:58.26ms
step:1701/2330 train_time:99100ms step_avg:58.26ms
step:1702/2330 train_time:99163ms step_avg:58.26ms
step:1703/2330 train_time:99220ms step_avg:58.26ms
step:1704/2330 train_time:99283ms step_avg:58.26ms
step:1705/2330 train_time:99339ms step_avg:58.26ms
step:1706/2330 train_time:99401ms step_avg:58.27ms
step:1707/2330 train_time:99459ms step_avg:58.27ms
step:1708/2330 train_time:99520ms step_avg:58.27ms
step:1709/2330 train_time:99578ms step_avg:58.27ms
step:1710/2330 train_time:99639ms step_avg:58.27ms
step:1711/2330 train_time:99697ms step_avg:58.27ms
step:1712/2330 train_time:99757ms step_avg:58.27ms
step:1713/2330 train_time:99814ms step_avg:58.27ms
step:1714/2330 train_time:99876ms step_avg:58.27ms
step:1715/2330 train_time:99934ms step_avg:58.27ms
step:1716/2330 train_time:99995ms step_avg:58.27ms
step:1717/2330 train_time:100054ms step_avg:58.27ms
step:1718/2330 train_time:100114ms step_avg:58.27ms
step:1719/2330 train_time:100173ms step_avg:58.27ms
step:1720/2330 train_time:100233ms step_avg:58.28ms
step:1721/2330 train_time:100291ms step_avg:58.27ms
step:1722/2330 train_time:100351ms step_avg:58.28ms
step:1723/2330 train_time:100408ms step_avg:58.28ms
step:1724/2330 train_time:100470ms step_avg:58.28ms
step:1725/2330 train_time:100527ms step_avg:58.28ms
step:1726/2330 train_time:100590ms step_avg:58.28ms
step:1727/2330 train_time:100648ms step_avg:58.28ms
step:1728/2330 train_time:100708ms step_avg:58.28ms
step:1729/2330 train_time:100766ms step_avg:58.28ms
step:1730/2330 train_time:100827ms step_avg:58.28ms
step:1731/2330 train_time:100884ms step_avg:58.28ms
step:1732/2330 train_time:100946ms step_avg:58.28ms
step:1733/2330 train_time:101003ms step_avg:58.28ms
step:1734/2330 train_time:101065ms step_avg:58.28ms
step:1735/2330 train_time:101123ms step_avg:58.28ms
step:1736/2330 train_time:101184ms step_avg:58.29ms
step:1737/2330 train_time:101241ms step_avg:58.28ms
step:1738/2330 train_time:101302ms step_avg:58.29ms
step:1739/2330 train_time:101359ms step_avg:58.29ms
step:1740/2330 train_time:101420ms step_avg:58.29ms
step:1741/2330 train_time:101477ms step_avg:58.29ms
step:1742/2330 train_time:101538ms step_avg:58.29ms
step:1743/2330 train_time:101596ms step_avg:58.29ms
step:1744/2330 train_time:101656ms step_avg:58.29ms
step:1745/2330 train_time:101715ms step_avg:58.29ms
step:1746/2330 train_time:101775ms step_avg:58.29ms
step:1747/2330 train_time:101833ms step_avg:58.29ms
step:1748/2330 train_time:101894ms step_avg:58.29ms
step:1749/2330 train_time:101953ms step_avg:58.29ms
step:1750/2330 train_time:102014ms step_avg:58.29ms
step:1750/2330 val_loss:3.8197 train_time:102095ms step_avg:58.34ms
step:1751/2330 train_time:102116ms step_avg:58.32ms
step:1752/2330 train_time:102136ms step_avg:58.30ms
step:1753/2330 train_time:102189ms step_avg:58.29ms
step:1754/2330 train_time:102255ms step_avg:58.30ms
step:1755/2330 train_time:102311ms step_avg:58.30ms
step:1756/2330 train_time:102375ms step_avg:58.30ms
step:1757/2330 train_time:102432ms step_avg:58.30ms
step:1758/2330 train_time:102492ms step_avg:58.30ms
step:1759/2330 train_time:102548ms step_avg:58.30ms
step:1760/2330 train_time:102607ms step_avg:58.30ms
step:1761/2330 train_time:102664ms step_avg:58.30ms
step:1762/2330 train_time:102724ms step_avg:58.30ms
step:1763/2330 train_time:102780ms step_avg:58.30ms
step:1764/2330 train_time:102840ms step_avg:58.30ms
step:1765/2330 train_time:102897ms step_avg:58.30ms
step:1766/2330 train_time:102956ms step_avg:58.30ms
step:1767/2330 train_time:103017ms step_avg:58.30ms
step:1768/2330 train_time:103083ms step_avg:58.30ms
step:1769/2330 train_time:103142ms step_avg:58.31ms
step:1770/2330 train_time:103202ms step_avg:58.31ms
step:1771/2330 train_time:103261ms step_avg:58.31ms
step:1772/2330 train_time:103321ms step_avg:58.31ms
step:1773/2330 train_time:103377ms step_avg:58.31ms
step:1774/2330 train_time:103439ms step_avg:58.31ms
step:1775/2330 train_time:103496ms step_avg:58.31ms
step:1776/2330 train_time:103556ms step_avg:58.31ms
step:1777/2330 train_time:103613ms step_avg:58.31ms
step:1778/2330 train_time:103674ms step_avg:58.31ms
step:1779/2330 train_time:103731ms step_avg:58.31ms
step:1780/2330 train_time:103791ms step_avg:58.31ms
step:1781/2330 train_time:103848ms step_avg:58.31ms
step:1782/2330 train_time:103907ms step_avg:58.31ms
step:1783/2330 train_time:103964ms step_avg:58.31ms
step:1784/2330 train_time:104027ms step_avg:58.31ms
step:1785/2330 train_time:104085ms step_avg:58.31ms
step:1786/2330 train_time:104147ms step_avg:58.31ms
step:1787/2330 train_time:104204ms step_avg:58.31ms
step:1788/2330 train_time:104267ms step_avg:58.31ms
step:1789/2330 train_time:104325ms step_avg:58.31ms
step:1790/2330 train_time:104386ms step_avg:58.32ms
step:1791/2330 train_time:104442ms step_avg:58.32ms
step:1792/2330 train_time:104503ms step_avg:58.32ms
step:1793/2330 train_time:104560ms step_avg:58.32ms
step:1794/2330 train_time:104621ms step_avg:58.32ms
step:1795/2330 train_time:104678ms step_avg:58.32ms
step:1796/2330 train_time:104739ms step_avg:58.32ms
step:1797/2330 train_time:104797ms step_avg:58.32ms
step:1798/2330 train_time:104857ms step_avg:58.32ms
step:1799/2330 train_time:104914ms step_avg:58.32ms
step:1800/2330 train_time:104976ms step_avg:58.32ms
step:1801/2330 train_time:105034ms step_avg:58.32ms
step:1802/2330 train_time:105096ms step_avg:58.32ms
step:1803/2330 train_time:105154ms step_avg:58.32ms
step:1804/2330 train_time:105218ms step_avg:58.32ms
step:1805/2330 train_time:105276ms step_avg:58.32ms
step:1806/2330 train_time:105336ms step_avg:58.33ms
step:1807/2330 train_time:105395ms step_avg:58.33ms
step:1808/2330 train_time:105455ms step_avg:58.33ms
step:1809/2330 train_time:105512ms step_avg:58.33ms
step:1810/2330 train_time:105572ms step_avg:58.33ms
step:1811/2330 train_time:105629ms step_avg:58.33ms
step:1812/2330 train_time:105690ms step_avg:58.33ms
step:1813/2330 train_time:105747ms step_avg:58.33ms
step:1814/2330 train_time:105807ms step_avg:58.33ms
step:1815/2330 train_time:105864ms step_avg:58.33ms
step:1816/2330 train_time:105925ms step_avg:58.33ms
step:1817/2330 train_time:105982ms step_avg:58.33ms
step:1818/2330 train_time:106045ms step_avg:58.33ms
step:1819/2330 train_time:106102ms step_avg:58.33ms
step:1820/2330 train_time:106163ms step_avg:58.33ms
step:1821/2330 train_time:106221ms step_avg:58.33ms
step:1822/2330 train_time:106282ms step_avg:58.33ms
step:1823/2330 train_time:106340ms step_avg:58.33ms
step:1824/2330 train_time:106400ms step_avg:58.33ms
step:1825/2330 train_time:106458ms step_avg:58.33ms
step:1826/2330 train_time:106519ms step_avg:58.33ms
step:1827/2330 train_time:106576ms step_avg:58.33ms
step:1828/2330 train_time:106636ms step_avg:58.33ms
step:1829/2330 train_time:106695ms step_avg:58.33ms
step:1830/2330 train_time:106754ms step_avg:58.34ms
step:1831/2330 train_time:106811ms step_avg:58.33ms
step:1832/2330 train_time:106872ms step_avg:58.34ms
step:1833/2330 train_time:106930ms step_avg:58.34ms
step:1834/2330 train_time:106992ms step_avg:58.34ms
step:1835/2330 train_time:107050ms step_avg:58.34ms
step:1836/2330 train_time:107112ms step_avg:58.34ms
step:1837/2330 train_time:107170ms step_avg:58.34ms
step:1838/2330 train_time:107231ms step_avg:58.34ms
step:1839/2330 train_time:107287ms step_avg:58.34ms
step:1840/2330 train_time:107350ms step_avg:58.34ms
step:1841/2330 train_time:107406ms step_avg:58.34ms
step:1842/2330 train_time:107469ms step_avg:58.34ms
step:1843/2330 train_time:107526ms step_avg:58.34ms
step:1844/2330 train_time:107587ms step_avg:58.34ms
step:1845/2330 train_time:107643ms step_avg:58.34ms
step:1846/2330 train_time:107704ms step_avg:58.34ms
step:1847/2330 train_time:107762ms step_avg:58.34ms
step:1848/2330 train_time:107822ms step_avg:58.35ms
step:1849/2330 train_time:107879ms step_avg:58.34ms
step:1850/2330 train_time:107941ms step_avg:58.35ms
step:1851/2330 train_time:107998ms step_avg:58.35ms
step:1852/2330 train_time:108059ms step_avg:58.35ms
step:1853/2330 train_time:108117ms step_avg:58.35ms
step:1854/2330 train_time:108179ms step_avg:58.35ms
step:1855/2330 train_time:108237ms step_avg:58.35ms
step:1856/2330 train_time:108298ms step_avg:58.35ms
step:1857/2330 train_time:108357ms step_avg:58.35ms
step:1858/2330 train_time:108417ms step_avg:58.35ms
step:1859/2330 train_time:108475ms step_avg:58.35ms
step:1860/2330 train_time:108536ms step_avg:58.35ms
step:1861/2330 train_time:108594ms step_avg:58.35ms
step:1862/2330 train_time:108654ms step_avg:58.35ms
step:1863/2330 train_time:108712ms step_avg:58.35ms
step:1864/2330 train_time:108771ms step_avg:58.35ms
step:1865/2330 train_time:108829ms step_avg:58.35ms
step:1866/2330 train_time:108890ms step_avg:58.35ms
step:1867/2330 train_time:108947ms step_avg:58.35ms
step:1868/2330 train_time:109007ms step_avg:58.36ms
step:1869/2330 train_time:109065ms step_avg:58.35ms
step:1870/2330 train_time:109126ms step_avg:58.36ms
step:1871/2330 train_time:109183ms step_avg:58.36ms
step:1872/2330 train_time:109244ms step_avg:58.36ms
step:1873/2330 train_time:109301ms step_avg:58.36ms
step:1874/2330 train_time:109363ms step_avg:58.36ms
step:1875/2330 train_time:109420ms step_avg:58.36ms
step:1876/2330 train_time:109482ms step_avg:58.36ms
step:1877/2330 train_time:109540ms step_avg:58.36ms
step:1878/2330 train_time:109600ms step_avg:58.36ms
step:1879/2330 train_time:109658ms step_avg:58.36ms
step:1880/2330 train_time:109719ms step_avg:58.36ms
step:1881/2330 train_time:109777ms step_avg:58.36ms
step:1882/2330 train_time:109838ms step_avg:58.36ms
step:1883/2330 train_time:109896ms step_avg:58.36ms
step:1884/2330 train_time:109956ms step_avg:58.36ms
step:1885/2330 train_time:110014ms step_avg:58.36ms
step:1886/2330 train_time:110074ms step_avg:58.36ms
step:1887/2330 train_time:110133ms step_avg:58.36ms
step:1888/2330 train_time:110193ms step_avg:58.37ms
step:1889/2330 train_time:110251ms step_avg:58.36ms
step:1890/2330 train_time:110312ms step_avg:58.37ms
step:1891/2330 train_time:110369ms step_avg:58.37ms
step:1892/2330 train_time:110431ms step_avg:58.37ms
step:1893/2330 train_time:110488ms step_avg:58.37ms
step:1894/2330 train_time:110550ms step_avg:58.37ms
step:1895/2330 train_time:110606ms step_avg:58.37ms
step:1896/2330 train_time:110668ms step_avg:58.37ms
step:1897/2330 train_time:110725ms step_avg:58.37ms
step:1898/2330 train_time:110786ms step_avg:58.37ms
step:1899/2330 train_time:110843ms step_avg:58.37ms
step:1900/2330 train_time:110904ms step_avg:58.37ms
step:1901/2330 train_time:110962ms step_avg:58.37ms
step:1902/2330 train_time:111023ms step_avg:58.37ms
step:1903/2330 train_time:111080ms step_avg:58.37ms
step:1904/2330 train_time:111141ms step_avg:58.37ms
step:1905/2330 train_time:111198ms step_avg:58.37ms
step:1906/2330 train_time:111259ms step_avg:58.37ms
step:1907/2330 train_time:111317ms step_avg:58.37ms
step:1908/2330 train_time:111378ms step_avg:58.37ms
step:1909/2330 train_time:111437ms step_avg:58.37ms
step:1910/2330 train_time:111497ms step_avg:58.38ms
step:1911/2330 train_time:111556ms step_avg:58.38ms
step:1912/2330 train_time:111617ms step_avg:58.38ms
step:1913/2330 train_time:111675ms step_avg:58.38ms
step:1914/2330 train_time:111736ms step_avg:58.38ms
step:1915/2330 train_time:111793ms step_avg:58.38ms
step:1916/2330 train_time:111855ms step_avg:58.38ms
step:1917/2330 train_time:111912ms step_avg:58.38ms
step:1918/2330 train_time:111973ms step_avg:58.38ms
step:1919/2330 train_time:112030ms step_avg:58.38ms
step:1920/2330 train_time:112091ms step_avg:58.38ms
step:1921/2330 train_time:112148ms step_avg:58.38ms
step:1922/2330 train_time:112209ms step_avg:58.38ms
step:1923/2330 train_time:112267ms step_avg:58.38ms
step:1924/2330 train_time:112329ms step_avg:58.38ms
step:1925/2330 train_time:112387ms step_avg:58.38ms
step:1926/2330 train_time:112448ms step_avg:58.38ms
step:1927/2330 train_time:112505ms step_avg:58.38ms
step:1928/2330 train_time:112566ms step_avg:58.38ms
step:1929/2330 train_time:112622ms step_avg:58.38ms
step:1930/2330 train_time:112684ms step_avg:58.39ms
step:1931/2330 train_time:112741ms step_avg:58.38ms
step:1932/2330 train_time:112802ms step_avg:58.39ms
step:1933/2330 train_time:112859ms step_avg:58.39ms
step:1934/2330 train_time:112919ms step_avg:58.39ms
step:1935/2330 train_time:112977ms step_avg:58.39ms
step:1936/2330 train_time:113038ms step_avg:58.39ms
step:1937/2330 train_time:113096ms step_avg:58.39ms
step:1938/2330 train_time:113157ms step_avg:58.39ms
step:1939/2330 train_time:113215ms step_avg:58.39ms
step:1940/2330 train_time:113276ms step_avg:58.39ms
step:1941/2330 train_time:113335ms step_avg:58.39ms
step:1942/2330 train_time:113396ms step_avg:58.39ms
step:1943/2330 train_time:113454ms step_avg:58.39ms
step:1944/2330 train_time:113514ms step_avg:58.39ms
step:1945/2330 train_time:113571ms step_avg:58.39ms
step:1946/2330 train_time:113634ms step_avg:58.39ms
step:1947/2330 train_time:113691ms step_avg:58.39ms
step:1948/2330 train_time:113752ms step_avg:58.39ms
step:1949/2330 train_time:113808ms step_avg:58.39ms
step:1950/2330 train_time:113868ms step_avg:58.39ms
step:1951/2330 train_time:113925ms step_avg:58.39ms
step:1952/2330 train_time:113988ms step_avg:58.40ms
step:1953/2330 train_time:114045ms step_avg:58.39ms
step:1954/2330 train_time:114106ms step_avg:58.40ms
step:1955/2330 train_time:114163ms step_avg:58.40ms
step:1956/2330 train_time:114226ms step_avg:58.40ms
step:1957/2330 train_time:114284ms step_avg:58.40ms
step:1958/2330 train_time:114344ms step_avg:58.40ms
step:1959/2330 train_time:114401ms step_avg:58.40ms
step:1960/2330 train_time:114463ms step_avg:58.40ms
step:1961/2330 train_time:114520ms step_avg:58.40ms
step:1962/2330 train_time:114581ms step_avg:58.40ms
step:1963/2330 train_time:114639ms step_avg:58.40ms
step:1964/2330 train_time:114699ms step_avg:58.40ms
step:1965/2330 train_time:114757ms step_avg:58.40ms
step:1966/2330 train_time:114818ms step_avg:58.40ms
step:1967/2330 train_time:114877ms step_avg:58.40ms
step:1968/2330 train_time:114937ms step_avg:58.40ms
step:1969/2330 train_time:114996ms step_avg:58.40ms
step:1970/2330 train_time:115057ms step_avg:58.40ms
step:1971/2330 train_time:115115ms step_avg:58.40ms
step:1972/2330 train_time:115176ms step_avg:58.41ms
step:1973/2330 train_time:115233ms step_avg:58.41ms
step:1974/2330 train_time:115294ms step_avg:58.41ms
step:1975/2330 train_time:115351ms step_avg:58.41ms
step:1976/2330 train_time:115413ms step_avg:58.41ms
step:1977/2330 train_time:115471ms step_avg:58.41ms
step:1978/2330 train_time:115531ms step_avg:58.41ms
step:1979/2330 train_time:115589ms step_avg:58.41ms
step:1980/2330 train_time:115649ms step_avg:58.41ms
step:1981/2330 train_time:115705ms step_avg:58.41ms
step:1982/2330 train_time:115768ms step_avg:58.41ms
step:1983/2330 train_time:115825ms step_avg:58.41ms
step:1984/2330 train_time:115886ms step_avg:58.41ms
step:1985/2330 train_time:115943ms step_avg:58.41ms
step:1986/2330 train_time:116005ms step_avg:58.41ms
step:1987/2330 train_time:116062ms step_avg:58.41ms
step:1988/2330 train_time:116124ms step_avg:58.41ms
step:1989/2330 train_time:116181ms step_avg:58.41ms
step:1990/2330 train_time:116242ms step_avg:58.41ms
step:1991/2330 train_time:116299ms step_avg:58.41ms
step:1992/2330 train_time:116359ms step_avg:58.41ms
step:1993/2330 train_time:116417ms step_avg:58.41ms
step:1994/2330 train_time:116478ms step_avg:58.41ms
step:1995/2330 train_time:116537ms step_avg:58.41ms
step:1996/2330 train_time:116597ms step_avg:58.42ms
step:1997/2330 train_time:116654ms step_avg:58.41ms
step:1998/2330 train_time:116716ms step_avg:58.42ms
step:1999/2330 train_time:116774ms step_avg:58.42ms
step:2000/2330 train_time:116835ms step_avg:58.42ms
step:2000/2330 val_loss:3.7581 train_time:116919ms step_avg:58.46ms
step:2001/2330 train_time:116936ms step_avg:58.44ms
step:2002/2330 train_time:116959ms step_avg:58.42ms
step:2003/2330 train_time:117022ms step_avg:58.42ms
step:2004/2330 train_time:117084ms step_avg:58.43ms
step:2005/2330 train_time:117142ms step_avg:58.42ms
step:2006/2330 train_time:117204ms step_avg:58.43ms
step:2007/2330 train_time:117261ms step_avg:58.43ms
step:2008/2330 train_time:117322ms step_avg:58.43ms
step:2009/2330 train_time:117378ms step_avg:58.43ms
step:2010/2330 train_time:117440ms step_avg:58.43ms
step:2011/2330 train_time:117496ms step_avg:58.43ms
step:2012/2330 train_time:117556ms step_avg:58.43ms
step:2013/2330 train_time:117613ms step_avg:58.43ms
step:2014/2330 train_time:117672ms step_avg:58.43ms
step:2015/2330 train_time:117729ms step_avg:58.43ms
step:2016/2330 train_time:117788ms step_avg:58.43ms
step:2017/2330 train_time:117845ms step_avg:58.43ms
step:2018/2330 train_time:117906ms step_avg:58.43ms
step:2019/2330 train_time:117965ms step_avg:58.43ms
step:2020/2330 train_time:118029ms step_avg:58.43ms
step:2021/2330 train_time:118088ms step_avg:58.43ms
step:2022/2330 train_time:118151ms step_avg:58.43ms
step:2023/2330 train_time:118209ms step_avg:58.43ms
step:2024/2330 train_time:118271ms step_avg:58.43ms
step:2025/2330 train_time:118328ms step_avg:58.43ms
step:2026/2330 train_time:118390ms step_avg:58.44ms
step:2027/2330 train_time:118447ms step_avg:58.43ms
step:2028/2330 train_time:118507ms step_avg:58.44ms
step:2029/2330 train_time:118563ms step_avg:58.43ms
step:2030/2330 train_time:118623ms step_avg:58.43ms
step:2031/2330 train_time:118679ms step_avg:58.43ms
step:2032/2330 train_time:118740ms step_avg:58.44ms
step:2033/2330 train_time:118797ms step_avg:58.43ms
step:2034/2330 train_time:118858ms step_avg:58.44ms
step:2035/2330 train_time:118916ms step_avg:58.44ms
step:2036/2330 train_time:118977ms step_avg:58.44ms
step:2037/2330 train_time:119037ms step_avg:58.44ms
step:2038/2330 train_time:119100ms step_avg:58.44ms
step:2039/2330 train_time:119158ms step_avg:58.44ms
step:2040/2330 train_time:119219ms step_avg:58.44ms
step:2041/2330 train_time:119277ms step_avg:58.44ms
step:2042/2330 train_time:119338ms step_avg:58.44ms
step:2043/2330 train_time:119396ms step_avg:58.44ms
step:2044/2330 train_time:119457ms step_avg:58.44ms
step:2045/2330 train_time:119514ms step_avg:58.44ms
step:2046/2330 train_time:119575ms step_avg:58.44ms
step:2047/2330 train_time:119632ms step_avg:58.44ms
step:2048/2330 train_time:119692ms step_avg:58.44ms
step:2049/2330 train_time:119750ms step_avg:58.44ms
step:2050/2330 train_time:119810ms step_avg:58.44ms
step:2051/2330 train_time:119868ms step_avg:58.44ms
step:2052/2330 train_time:119928ms step_avg:58.44ms
step:2053/2330 train_time:119986ms step_avg:58.44ms
step:2054/2330 train_time:120049ms step_avg:58.45ms
step:2055/2330 train_time:120107ms step_avg:58.45ms
step:2056/2330 train_time:120168ms step_avg:58.45ms
step:2057/2330 train_time:120226ms step_avg:58.45ms
step:2058/2330 train_time:120287ms step_avg:58.45ms
step:2059/2330 train_time:120345ms step_avg:58.45ms
step:2060/2330 train_time:120407ms step_avg:58.45ms
step:2061/2330 train_time:120463ms step_avg:58.45ms
step:2062/2330 train_time:120525ms step_avg:58.45ms
step:2063/2330 train_time:120581ms step_avg:58.45ms
step:2064/2330 train_time:120643ms step_avg:58.45ms
step:2065/2330 train_time:120700ms step_avg:58.45ms
step:2066/2330 train_time:120760ms step_avg:58.45ms
step:2067/2330 train_time:120817ms step_avg:58.45ms
step:2068/2330 train_time:120878ms step_avg:58.45ms
step:2069/2330 train_time:120935ms step_avg:58.45ms
step:2070/2330 train_time:120996ms step_avg:58.45ms
step:2071/2330 train_time:121054ms step_avg:58.45ms
step:2072/2330 train_time:121116ms step_avg:58.45ms
step:2073/2330 train_time:121175ms step_avg:58.45ms
step:2074/2330 train_time:121236ms step_avg:58.45ms
step:2075/2330 train_time:121295ms step_avg:58.46ms
step:2076/2330 train_time:121356ms step_avg:58.46ms
step:2077/2330 train_time:121414ms step_avg:58.46ms
step:2078/2330 train_time:121474ms step_avg:58.46ms
step:2079/2330 train_time:121531ms step_avg:58.46ms
step:2080/2330 train_time:121593ms step_avg:58.46ms
step:2081/2330 train_time:121650ms step_avg:58.46ms
step:2082/2330 train_time:121711ms step_avg:58.46ms
step:2083/2330 train_time:121768ms step_avg:58.46ms
step:2084/2330 train_time:121829ms step_avg:58.46ms
step:2085/2330 train_time:121886ms step_avg:58.46ms
step:2086/2330 train_time:121948ms step_avg:58.46ms
step:2087/2330 train_time:122005ms step_avg:58.46ms
step:2088/2330 train_time:122067ms step_avg:58.46ms
step:2089/2330 train_time:122124ms step_avg:58.46ms
step:2090/2330 train_time:122187ms step_avg:58.46ms
step:2091/2330 train_time:122243ms step_avg:58.46ms
step:2092/2330 train_time:122306ms step_avg:58.46ms
step:2093/2330 train_time:122363ms step_avg:58.46ms
step:2094/2330 train_time:122424ms step_avg:58.46ms
step:2095/2330 train_time:122481ms step_avg:58.46ms
step:2096/2330 train_time:122543ms step_avg:58.46ms
step:2097/2330 train_time:122600ms step_avg:58.46ms
step:2098/2330 train_time:122661ms step_avg:58.47ms
step:2099/2330 train_time:122717ms step_avg:58.46ms
step:2100/2330 train_time:122778ms step_avg:58.47ms
step:2101/2330 train_time:122836ms step_avg:58.47ms
step:2102/2330 train_time:122896ms step_avg:58.47ms
step:2103/2330 train_time:122955ms step_avg:58.47ms
step:2104/2330 train_time:123016ms step_avg:58.47ms
step:2105/2330 train_time:123075ms step_avg:58.47ms
step:2106/2330 train_time:123135ms step_avg:58.47ms
step:2107/2330 train_time:123193ms step_avg:58.47ms
step:2108/2330 train_time:123254ms step_avg:58.47ms
step:2109/2330 train_time:123313ms step_avg:58.47ms
step:2110/2330 train_time:123374ms step_avg:58.47ms
step:2111/2330 train_time:123431ms step_avg:58.47ms
step:2112/2330 train_time:123493ms step_avg:58.47ms
step:2113/2330 train_time:123551ms step_avg:58.47ms
step:2114/2330 train_time:123611ms step_avg:58.47ms
step:2115/2330 train_time:123669ms step_avg:58.47ms
step:2116/2330 train_time:123728ms step_avg:58.47ms
step:2117/2330 train_time:123785ms step_avg:58.47ms
step:2118/2330 train_time:123846ms step_avg:58.47ms
step:2119/2330 train_time:123903ms step_avg:58.47ms
step:2120/2330 train_time:123964ms step_avg:58.47ms
step:2121/2330 train_time:124021ms step_avg:58.47ms
step:2122/2330 train_time:124082ms step_avg:58.47ms
step:2123/2330 train_time:124140ms step_avg:58.47ms
step:2124/2330 train_time:124202ms step_avg:58.48ms
step:2125/2330 train_time:124259ms step_avg:58.47ms
step:2126/2330 train_time:124321ms step_avg:58.48ms
step:2127/2330 train_time:124378ms step_avg:58.48ms
step:2128/2330 train_time:124439ms step_avg:58.48ms
step:2129/2330 train_time:124496ms step_avg:58.48ms
step:2130/2330 train_time:124558ms step_avg:58.48ms
step:2131/2330 train_time:124616ms step_avg:58.48ms
step:2132/2330 train_time:124676ms step_avg:58.48ms
step:2133/2330 train_time:124734ms step_avg:58.48ms
step:2134/2330 train_time:124795ms step_avg:58.48ms
step:2135/2330 train_time:124853ms step_avg:58.48ms
step:2136/2330 train_time:124913ms step_avg:58.48ms
step:2137/2330 train_time:124971ms step_avg:58.48ms
step:2138/2330 train_time:125032ms step_avg:58.48ms
step:2139/2330 train_time:125090ms step_avg:58.48ms
step:2140/2330 train_time:125152ms step_avg:58.48ms
step:2141/2330 train_time:125209ms step_avg:58.48ms
step:2142/2330 train_time:125271ms step_avg:58.48ms
step:2143/2330 train_time:125329ms step_avg:58.48ms
step:2144/2330 train_time:125390ms step_avg:58.48ms
step:2145/2330 train_time:125447ms step_avg:58.48ms
step:2146/2330 train_time:125509ms step_avg:58.48ms
step:2147/2330 train_time:125566ms step_avg:58.48ms
step:2148/2330 train_time:125626ms step_avg:58.49ms
step:2149/2330 train_time:125683ms step_avg:58.48ms
step:2150/2330 train_time:125744ms step_avg:58.49ms
step:2151/2330 train_time:125801ms step_avg:58.49ms
step:2152/2330 train_time:125863ms step_avg:58.49ms
step:2153/2330 train_time:125920ms step_avg:58.49ms
step:2154/2330 train_time:125981ms step_avg:58.49ms
step:2155/2330 train_time:126038ms step_avg:58.49ms
step:2156/2330 train_time:126101ms step_avg:58.49ms
step:2157/2330 train_time:126159ms step_avg:58.49ms
step:2158/2330 train_time:126220ms step_avg:58.49ms
step:2159/2330 train_time:126278ms step_avg:58.49ms
step:2160/2330 train_time:126339ms step_avg:58.49ms
step:2161/2330 train_time:126398ms step_avg:58.49ms
step:2162/2330 train_time:126459ms step_avg:58.49ms
step:2163/2330 train_time:126518ms step_avg:58.49ms
step:2164/2330 train_time:126578ms step_avg:58.49ms
step:2165/2330 train_time:126635ms step_avg:58.49ms
step:2166/2330 train_time:126697ms step_avg:58.49ms
step:2167/2330 train_time:126756ms step_avg:58.49ms
step:2168/2330 train_time:126816ms step_avg:58.49ms
step:2169/2330 train_time:126873ms step_avg:58.49ms
step:2170/2330 train_time:126934ms step_avg:58.50ms
step:2171/2330 train_time:126992ms step_avg:58.49ms
step:2172/2330 train_time:127052ms step_avg:58.50ms
step:2173/2330 train_time:127110ms step_avg:58.50ms
step:2174/2330 train_time:127171ms step_avg:58.50ms
step:2175/2330 train_time:127229ms step_avg:58.50ms
step:2176/2330 train_time:127291ms step_avg:58.50ms
step:2177/2330 train_time:127348ms step_avg:58.50ms
step:2178/2330 train_time:127408ms step_avg:58.50ms
step:2179/2330 train_time:127465ms step_avg:58.50ms
step:2180/2330 train_time:127528ms step_avg:58.50ms
step:2181/2330 train_time:127586ms step_avg:58.50ms
step:2182/2330 train_time:127647ms step_avg:58.50ms
step:2183/2330 train_time:127703ms step_avg:58.50ms
step:2184/2330 train_time:127764ms step_avg:58.50ms
step:2185/2330 train_time:127822ms step_avg:58.50ms
step:2186/2330 train_time:127882ms step_avg:58.50ms
step:2187/2330 train_time:127939ms step_avg:58.50ms
step:2188/2330 train_time:128002ms step_avg:58.50ms
step:2189/2330 train_time:128059ms step_avg:58.50ms
step:2190/2330 train_time:128119ms step_avg:58.50ms
step:2191/2330 train_time:128176ms step_avg:58.50ms
step:2192/2330 train_time:128238ms step_avg:58.50ms
step:2193/2330 train_time:128296ms step_avg:58.50ms
step:2194/2330 train_time:128356ms step_avg:58.50ms
step:2195/2330 train_time:128415ms step_avg:58.50ms
step:2196/2330 train_time:128475ms step_avg:58.50ms
step:2197/2330 train_time:128533ms step_avg:58.50ms
step:2198/2330 train_time:128594ms step_avg:58.50ms
step:2199/2330 train_time:128653ms step_avg:58.51ms
step:2200/2330 train_time:128713ms step_avg:58.51ms
step:2201/2330 train_time:128771ms step_avg:58.51ms
step:2202/2330 train_time:128832ms step_avg:58.51ms
step:2203/2330 train_time:128889ms step_avg:58.51ms
step:2204/2330 train_time:128950ms step_avg:58.51ms
step:2205/2330 train_time:129008ms step_avg:58.51ms
step:2206/2330 train_time:129069ms step_avg:58.51ms
step:2207/2330 train_time:129126ms step_avg:58.51ms
step:2208/2330 train_time:129187ms step_avg:58.51ms
step:2209/2330 train_time:129244ms step_avg:58.51ms
step:2210/2330 train_time:129307ms step_avg:58.51ms
step:2211/2330 train_time:129364ms step_avg:58.51ms
step:2212/2330 train_time:129427ms step_avg:58.51ms
step:2213/2330 train_time:129484ms step_avg:58.51ms
step:2214/2330 train_time:129548ms step_avg:58.51ms
step:2215/2330 train_time:129604ms step_avg:58.51ms
step:2216/2330 train_time:129667ms step_avg:58.51ms
step:2217/2330 train_time:129724ms step_avg:58.51ms
step:2218/2330 train_time:129785ms step_avg:58.51ms
step:2219/2330 train_time:129842ms step_avg:58.51ms
step:2220/2330 train_time:129904ms step_avg:58.52ms
step:2221/2330 train_time:129961ms step_avg:58.51ms
step:2222/2330 train_time:130022ms step_avg:58.52ms
step:2223/2330 train_time:130079ms step_avg:58.51ms
step:2224/2330 train_time:130140ms step_avg:58.52ms
step:2225/2330 train_time:130198ms step_avg:58.52ms
step:2226/2330 train_time:130258ms step_avg:58.52ms
step:2227/2330 train_time:130317ms step_avg:58.52ms
step:2228/2330 train_time:130378ms step_avg:58.52ms
step:2229/2330 train_time:130436ms step_avg:58.52ms
step:2230/2330 train_time:130497ms step_avg:58.52ms
step:2231/2330 train_time:130556ms step_avg:58.52ms
step:2232/2330 train_time:130616ms step_avg:58.52ms
step:2233/2330 train_time:130673ms step_avg:58.52ms
step:2234/2330 train_time:130734ms step_avg:58.52ms
step:2235/2330 train_time:130792ms step_avg:58.52ms
step:2236/2330 train_time:130853ms step_avg:58.52ms
step:2237/2330 train_time:130911ms step_avg:58.52ms
step:2238/2330 train_time:130972ms step_avg:58.52ms
step:2239/2330 train_time:131030ms step_avg:58.52ms
step:2240/2330 train_time:131092ms step_avg:58.52ms
step:2241/2330 train_time:131150ms step_avg:58.52ms
step:2242/2330 train_time:131211ms step_avg:58.52ms
step:2243/2330 train_time:131268ms step_avg:58.52ms
step:2244/2330 train_time:131330ms step_avg:58.52ms
step:2245/2330 train_time:131388ms step_avg:58.52ms
step:2246/2330 train_time:131450ms step_avg:58.53ms
step:2247/2330 train_time:131507ms step_avg:58.53ms
step:2248/2330 train_time:131569ms step_avg:58.53ms
step:2249/2330 train_time:131626ms step_avg:58.53ms
step:2250/2330 train_time:131687ms step_avg:58.53ms
step:2250/2330 val_loss:3.7106 train_time:131770ms step_avg:58.56ms
step:2251/2330 train_time:131789ms step_avg:58.55ms
step:2252/2330 train_time:131810ms step_avg:58.53ms
step:2253/2330 train_time:131870ms step_avg:58.53ms
step:2254/2330 train_time:131937ms step_avg:58.53ms
step:2255/2330 train_time:131995ms step_avg:58.53ms
step:2256/2330 train_time:132057ms step_avg:58.54ms
step:2257/2330 train_time:132115ms step_avg:58.54ms
step:2258/2330 train_time:132175ms step_avg:58.54ms
step:2259/2330 train_time:132233ms step_avg:58.54ms
step:2260/2330 train_time:132293ms step_avg:58.54ms
step:2261/2330 train_time:132350ms step_avg:58.54ms
step:2262/2330 train_time:132410ms step_avg:58.54ms
step:2263/2330 train_time:132466ms step_avg:58.54ms
step:2264/2330 train_time:132527ms step_avg:58.54ms
step:2265/2330 train_time:132583ms step_avg:58.54ms
step:2266/2330 train_time:132643ms step_avg:58.54ms
step:2267/2330 train_time:132700ms step_avg:58.54ms
step:2268/2330 train_time:132763ms step_avg:58.54ms
step:2269/2330 train_time:132822ms step_avg:58.54ms
step:2270/2330 train_time:132886ms step_avg:58.54ms
step:2271/2330 train_time:132943ms step_avg:58.54ms
step:2272/2330 train_time:133006ms step_avg:58.54ms
step:2273/2330 train_time:133063ms step_avg:58.54ms
step:2274/2330 train_time:133126ms step_avg:58.54ms
step:2275/2330 train_time:133183ms step_avg:58.54ms
step:2276/2330 train_time:133244ms step_avg:58.54ms
step:2277/2330 train_time:133300ms step_avg:58.54ms
step:2278/2330 train_time:133361ms step_avg:58.54ms
step:2279/2330 train_time:133418ms step_avg:58.54ms
step:2280/2330 train_time:133478ms step_avg:58.54ms
step:2281/2330 train_time:133535ms step_avg:58.54ms
step:2282/2330 train_time:133595ms step_avg:58.54ms
step:2283/2330 train_time:133653ms step_avg:58.54ms
step:2284/2330 train_time:133714ms step_avg:58.54ms
step:2285/2330 train_time:133772ms step_avg:58.54ms
step:2286/2330 train_time:133835ms step_avg:58.55ms
step:2287/2330 train_time:133894ms step_avg:58.55ms
step:2288/2330 train_time:133956ms step_avg:58.55ms
step:2289/2330 train_time:134014ms step_avg:58.55ms
step:2290/2330 train_time:134077ms step_avg:58.55ms
step:2291/2330 train_time:134134ms step_avg:58.55ms
step:2292/2330 train_time:134195ms step_avg:58.55ms
step:2293/2330 train_time:134252ms step_avg:58.55ms
step:2294/2330 train_time:134313ms step_avg:58.55ms
step:2295/2330 train_time:134370ms step_avg:58.55ms
step:2296/2330 train_time:134430ms step_avg:58.55ms
step:2297/2330 train_time:134487ms step_avg:58.55ms
step:2298/2330 train_time:134548ms step_avg:58.55ms
step:2299/2330 train_time:134604ms step_avg:58.55ms
step:2300/2330 train_time:134664ms step_avg:58.55ms
step:2301/2330 train_time:134721ms step_avg:58.55ms
step:2302/2330 train_time:134783ms step_avg:58.55ms
step:2303/2330 train_time:134840ms step_avg:58.55ms
step:2304/2330 train_time:134904ms step_avg:58.55ms
step:2305/2330 train_time:134961ms step_avg:58.55ms
step:2306/2330 train_time:135025ms step_avg:58.55ms
step:2307/2330 train_time:135082ms step_avg:58.55ms
step:2308/2330 train_time:135143ms step_avg:58.55ms
step:2309/2330 train_time:135200ms step_avg:58.55ms
step:2310/2330 train_time:135261ms step_avg:58.55ms
step:2311/2330 train_time:135318ms step_avg:58.55ms
step:2312/2330 train_time:135378ms step_avg:58.55ms
step:2313/2330 train_time:135435ms step_avg:58.55ms
step:2314/2330 train_time:135496ms step_avg:58.55ms
step:2315/2330 train_time:135553ms step_avg:58.55ms
step:2316/2330 train_time:135613ms step_avg:58.55ms
step:2317/2330 train_time:135671ms step_avg:58.55ms
step:2318/2330 train_time:135731ms step_avg:58.56ms
step:2319/2330 train_time:135789ms step_avg:58.55ms
step:2320/2330 train_time:135851ms step_avg:58.56ms
step:2321/2330 train_time:135908ms step_avg:58.56ms
step:2322/2330 train_time:135972ms step_avg:58.56ms
step:2323/2330 train_time:136030ms step_avg:58.56ms
step:2324/2330 train_time:136091ms step_avg:58.56ms
step:2325/2330 train_time:136149ms step_avg:58.56ms
step:2326/2330 train_time:136211ms step_avg:58.56ms
step:2327/2330 train_time:136268ms step_avg:58.56ms
step:2328/2330 train_time:136330ms step_avg:58.56ms
step:2329/2330 train_time:136386ms step_avg:58.56ms
step:2330/2330 train_time:136447ms step_avg:58.56ms
step:2330/2330 val_loss:3.6955 train_time:136529ms step_avg:58.60ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
