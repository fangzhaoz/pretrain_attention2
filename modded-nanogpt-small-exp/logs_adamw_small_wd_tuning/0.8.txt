import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 08:33:43 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:89ms step_avg:89.38ms
step:2/2330 train_time:178ms step_avg:88.95ms
step:3/2330 train_time:198ms step_avg:65.87ms
step:4/2330 train_time:217ms step_avg:54.18ms
step:5/2330 train_time:269ms step_avg:53.89ms
step:6/2330 train_time:327ms step_avg:54.57ms
step:7/2330 train_time:383ms step_avg:54.64ms
step:8/2330 train_time:442ms step_avg:55.20ms
step:9/2330 train_time:497ms step_avg:55.23ms
step:10/2330 train_time:556ms step_avg:55.56ms
step:11/2330 train_time:611ms step_avg:55.54ms
step:12/2330 train_time:669ms step_avg:55.75ms
step:13/2330 train_time:724ms step_avg:55.72ms
step:14/2330 train_time:783ms step_avg:55.92ms
step:15/2330 train_time:839ms step_avg:55.92ms
step:16/2330 train_time:897ms step_avg:56.05ms
step:17/2330 train_time:953ms step_avg:56.04ms
step:18/2330 train_time:1012ms step_avg:56.21ms
step:19/2330 train_time:1071ms step_avg:56.37ms
step:20/2330 train_time:1131ms step_avg:56.57ms
step:21/2330 train_time:1188ms step_avg:56.58ms
step:22/2330 train_time:1249ms step_avg:56.76ms
step:23/2330 train_time:1304ms step_avg:56.71ms
step:24/2330 train_time:1364ms step_avg:56.85ms
step:25/2330 train_time:1420ms step_avg:56.80ms
step:26/2330 train_time:1478ms step_avg:56.85ms
step:27/2330 train_time:1533ms step_avg:56.79ms
step:28/2330 train_time:1593ms step_avg:56.89ms
step:29/2330 train_time:1648ms step_avg:56.83ms
step:30/2330 train_time:1706ms step_avg:56.87ms
step:31/2330 train_time:1762ms step_avg:56.84ms
step:32/2330 train_time:1820ms step_avg:56.89ms
step:33/2330 train_time:1876ms step_avg:56.85ms
step:34/2330 train_time:1934ms step_avg:56.89ms
step:35/2330 train_time:1990ms step_avg:56.87ms
step:36/2330 train_time:2051ms step_avg:56.96ms
step:37/2330 train_time:2108ms step_avg:56.97ms
step:38/2330 train_time:2168ms step_avg:57.05ms
step:39/2330 train_time:2225ms step_avg:57.04ms
step:40/2330 train_time:2284ms step_avg:57.09ms
step:41/2330 train_time:2340ms step_avg:57.08ms
step:42/2330 train_time:2399ms step_avg:57.12ms
step:43/2330 train_time:2455ms step_avg:57.09ms
step:44/2330 train_time:2514ms step_avg:57.13ms
step:45/2330 train_time:2570ms step_avg:57.11ms
step:46/2330 train_time:2628ms step_avg:57.14ms
step:47/2330 train_time:2684ms step_avg:57.10ms
step:48/2330 train_time:2742ms step_avg:57.12ms
step:49/2330 train_time:2798ms step_avg:57.10ms
step:50/2330 train_time:2856ms step_avg:57.12ms
step:51/2330 train_time:2912ms step_avg:57.09ms
step:52/2330 train_time:2970ms step_avg:57.12ms
step:53/2330 train_time:3027ms step_avg:57.11ms
step:54/2330 train_time:3088ms step_avg:57.18ms
step:55/2330 train_time:3144ms step_avg:57.17ms
step:56/2330 train_time:3203ms step_avg:57.20ms
step:57/2330 train_time:3260ms step_avg:57.19ms
step:58/2330 train_time:3319ms step_avg:57.22ms
step:59/2330 train_time:3375ms step_avg:57.20ms
step:60/2330 train_time:3434ms step_avg:57.23ms
step:61/2330 train_time:3490ms step_avg:57.21ms
step:62/2330 train_time:3548ms step_avg:57.23ms
step:63/2330 train_time:3604ms step_avg:57.20ms
step:64/2330 train_time:3663ms step_avg:57.23ms
step:65/2330 train_time:3718ms step_avg:57.21ms
step:66/2330 train_time:3777ms step_avg:57.23ms
step:67/2330 train_time:3832ms step_avg:57.20ms
step:68/2330 train_time:3892ms step_avg:57.23ms
step:69/2330 train_time:3948ms step_avg:57.21ms
step:70/2330 train_time:4007ms step_avg:57.24ms
step:71/2330 train_time:4064ms step_avg:57.23ms
step:72/2330 train_time:4122ms step_avg:57.26ms
step:73/2330 train_time:4178ms step_avg:57.24ms
step:74/2330 train_time:4238ms step_avg:57.27ms
step:75/2330 train_time:4294ms step_avg:57.25ms
step:76/2330 train_time:4353ms step_avg:57.28ms
step:77/2330 train_time:4409ms step_avg:57.27ms
step:78/2330 train_time:4468ms step_avg:57.28ms
step:79/2330 train_time:4523ms step_avg:57.26ms
step:80/2330 train_time:4583ms step_avg:57.29ms
step:81/2330 train_time:4639ms step_avg:57.27ms
step:82/2330 train_time:4698ms step_avg:57.30ms
step:83/2330 train_time:4754ms step_avg:57.27ms
step:84/2330 train_time:4813ms step_avg:57.29ms
step:85/2330 train_time:4868ms step_avg:57.27ms
step:86/2330 train_time:4927ms step_avg:57.29ms
step:87/2330 train_time:4983ms step_avg:57.28ms
step:88/2330 train_time:5043ms step_avg:57.31ms
step:89/2330 train_time:5099ms step_avg:57.29ms
step:90/2330 train_time:5159ms step_avg:57.32ms
step:91/2330 train_time:5215ms step_avg:57.31ms
step:92/2330 train_time:5275ms step_avg:57.33ms
step:93/2330 train_time:5331ms step_avg:57.33ms
step:94/2330 train_time:5390ms step_avg:57.34ms
step:95/2330 train_time:5447ms step_avg:57.33ms
step:96/2330 train_time:5505ms step_avg:57.35ms
step:97/2330 train_time:5561ms step_avg:57.33ms
step:98/2330 train_time:5620ms step_avg:57.35ms
step:99/2330 train_time:5675ms step_avg:57.33ms
step:100/2330 train_time:5734ms step_avg:57.34ms
step:101/2330 train_time:5790ms step_avg:57.32ms
step:102/2330 train_time:5848ms step_avg:57.34ms
step:103/2330 train_time:5904ms step_avg:57.32ms
step:104/2330 train_time:5963ms step_avg:57.34ms
step:105/2330 train_time:6019ms step_avg:57.32ms
step:106/2330 train_time:6078ms step_avg:57.34ms
step:107/2330 train_time:6134ms step_avg:57.32ms
step:108/2330 train_time:6195ms step_avg:57.36ms
step:109/2330 train_time:6251ms step_avg:57.35ms
step:110/2330 train_time:6310ms step_avg:57.37ms
step:111/2330 train_time:6366ms step_avg:57.35ms
step:112/2330 train_time:6427ms step_avg:57.38ms
step:113/2330 train_time:6482ms step_avg:57.37ms
step:114/2330 train_time:6542ms step_avg:57.38ms
step:115/2330 train_time:6598ms step_avg:57.37ms
step:116/2330 train_time:6656ms step_avg:57.38ms
step:117/2330 train_time:6713ms step_avg:57.37ms
step:118/2330 train_time:6771ms step_avg:57.38ms
step:119/2330 train_time:6827ms step_avg:57.37ms
step:120/2330 train_time:6886ms step_avg:57.39ms
step:121/2330 train_time:6942ms step_avg:57.37ms
step:122/2330 train_time:7001ms step_avg:57.38ms
step:123/2330 train_time:7056ms step_avg:57.37ms
step:124/2330 train_time:7115ms step_avg:57.38ms
step:125/2330 train_time:7172ms step_avg:57.37ms
step:126/2330 train_time:7231ms step_avg:57.39ms
step:127/2330 train_time:7287ms step_avg:57.37ms
step:128/2330 train_time:7346ms step_avg:57.39ms
step:129/2330 train_time:7402ms step_avg:57.38ms
step:130/2330 train_time:7462ms step_avg:57.40ms
step:131/2330 train_time:7518ms step_avg:57.39ms
step:132/2330 train_time:7577ms step_avg:57.40ms
step:133/2330 train_time:7633ms step_avg:57.39ms
step:134/2330 train_time:7692ms step_avg:57.40ms
step:135/2330 train_time:7748ms step_avg:57.39ms
step:136/2330 train_time:7806ms step_avg:57.40ms
step:137/2330 train_time:7862ms step_avg:57.39ms
step:138/2330 train_time:7921ms step_avg:57.40ms
step:139/2330 train_time:7977ms step_avg:57.39ms
step:140/2330 train_time:8036ms step_avg:57.40ms
step:141/2330 train_time:8092ms step_avg:57.39ms
step:142/2330 train_time:8150ms step_avg:57.40ms
step:143/2330 train_time:8206ms step_avg:57.39ms
step:144/2330 train_time:8266ms step_avg:57.40ms
step:145/2330 train_time:8322ms step_avg:57.39ms
step:146/2330 train_time:8381ms step_avg:57.40ms
step:147/2330 train_time:8437ms step_avg:57.39ms
step:148/2330 train_time:8496ms step_avg:57.40ms
step:149/2330 train_time:8552ms step_avg:57.39ms
step:150/2330 train_time:8611ms step_avg:57.41ms
step:151/2330 train_time:8667ms step_avg:57.40ms
step:152/2330 train_time:8726ms step_avg:57.41ms
step:153/2330 train_time:8782ms step_avg:57.40ms
step:154/2330 train_time:8841ms step_avg:57.41ms
step:155/2330 train_time:8897ms step_avg:57.40ms
step:156/2330 train_time:8956ms step_avg:57.41ms
step:157/2330 train_time:9012ms step_avg:57.40ms
step:158/2330 train_time:9071ms step_avg:57.41ms
step:159/2330 train_time:9127ms step_avg:57.40ms
step:160/2330 train_time:9186ms step_avg:57.41ms
step:161/2330 train_time:9242ms step_avg:57.40ms
step:162/2330 train_time:9301ms step_avg:57.41ms
step:163/2330 train_time:9356ms step_avg:57.40ms
step:164/2330 train_time:9415ms step_avg:57.41ms
step:165/2330 train_time:9471ms step_avg:57.40ms
step:166/2330 train_time:9530ms step_avg:57.41ms
step:167/2330 train_time:9586ms step_avg:57.40ms
step:168/2330 train_time:9645ms step_avg:57.41ms
step:169/2330 train_time:9701ms step_avg:57.40ms
step:170/2330 train_time:9760ms step_avg:57.41ms
step:171/2330 train_time:9815ms step_avg:57.40ms
step:172/2330 train_time:9874ms step_avg:57.41ms
step:173/2330 train_time:9930ms step_avg:57.40ms
step:174/2330 train_time:9989ms step_avg:57.41ms
step:175/2330 train_time:10045ms step_avg:57.40ms
step:176/2330 train_time:10104ms step_avg:57.41ms
step:177/2330 train_time:10160ms step_avg:57.40ms
step:178/2330 train_time:10219ms step_avg:57.41ms
step:179/2330 train_time:10274ms step_avg:57.40ms
step:180/2330 train_time:10334ms step_avg:57.41ms
step:181/2330 train_time:10390ms step_avg:57.40ms
step:182/2330 train_time:10449ms step_avg:57.41ms
step:183/2330 train_time:10505ms step_avg:57.40ms
step:184/2330 train_time:10564ms step_avg:57.41ms
step:185/2330 train_time:10620ms step_avg:57.41ms
step:186/2330 train_time:10679ms step_avg:57.41ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10795ms step_avg:57.42ms
step:189/2330 train_time:10851ms step_avg:57.41ms
step:190/2330 train_time:10910ms step_avg:57.42ms
step:191/2330 train_time:10965ms step_avg:57.41ms
step:192/2330 train_time:11025ms step_avg:57.42ms
step:193/2330 train_time:11080ms step_avg:57.41ms
step:194/2330 train_time:11140ms step_avg:57.42ms
step:195/2330 train_time:11196ms step_avg:57.42ms
step:196/2330 train_time:11256ms step_avg:57.43ms
step:197/2330 train_time:11311ms step_avg:57.42ms
step:198/2330 train_time:11371ms step_avg:57.43ms
step:199/2330 train_time:11427ms step_avg:57.42ms
step:200/2330 train_time:11487ms step_avg:57.43ms
step:201/2330 train_time:11543ms step_avg:57.43ms
step:202/2330 train_time:11602ms step_avg:57.44ms
step:203/2330 train_time:11658ms step_avg:57.43ms
step:204/2330 train_time:11717ms step_avg:57.43ms
step:205/2330 train_time:11773ms step_avg:57.43ms
step:206/2330 train_time:11832ms step_avg:57.44ms
step:207/2330 train_time:11888ms step_avg:57.43ms
step:208/2330 train_time:11947ms step_avg:57.44ms
step:209/2330 train_time:12002ms step_avg:57.43ms
step:210/2330 train_time:12062ms step_avg:57.44ms
step:211/2330 train_time:12118ms step_avg:57.43ms
step:212/2330 train_time:12177ms step_avg:57.44ms
step:213/2330 train_time:12232ms step_avg:57.43ms
step:214/2330 train_time:12291ms step_avg:57.43ms
step:215/2330 train_time:12347ms step_avg:57.43ms
step:216/2330 train_time:12406ms step_avg:57.44ms
step:217/2330 train_time:12462ms step_avg:57.43ms
step:218/2330 train_time:12521ms step_avg:57.43ms
step:219/2330 train_time:12576ms step_avg:57.43ms
step:220/2330 train_time:12636ms step_avg:57.44ms
step:221/2330 train_time:12691ms step_avg:57.43ms
step:222/2330 train_time:12751ms step_avg:57.44ms
step:223/2330 train_time:12807ms step_avg:57.43ms
step:224/2330 train_time:12867ms step_avg:57.44ms
step:225/2330 train_time:12923ms step_avg:57.44ms
step:226/2330 train_time:12982ms step_avg:57.44ms
step:227/2330 train_time:13038ms step_avg:57.44ms
step:228/2330 train_time:13097ms step_avg:57.44ms
step:229/2330 train_time:13153ms step_avg:57.44ms
step:230/2330 train_time:13212ms step_avg:57.45ms
step:231/2330 train_time:13269ms step_avg:57.44ms
step:232/2330 train_time:13328ms step_avg:57.45ms
step:233/2330 train_time:13384ms step_avg:57.44ms
step:234/2330 train_time:13443ms step_avg:57.45ms
step:235/2330 train_time:13500ms step_avg:57.45ms
step:236/2330 train_time:13559ms step_avg:57.45ms
step:237/2330 train_time:13615ms step_avg:57.45ms
step:238/2330 train_time:13674ms step_avg:57.45ms
step:239/2330 train_time:13730ms step_avg:57.45ms
step:240/2330 train_time:13789ms step_avg:57.45ms
step:241/2330 train_time:13845ms step_avg:57.45ms
step:242/2330 train_time:13904ms step_avg:57.46ms
step:243/2330 train_time:13960ms step_avg:57.45ms
step:244/2330 train_time:14019ms step_avg:57.45ms
step:245/2330 train_time:14075ms step_avg:57.45ms
step:246/2330 train_time:14134ms step_avg:57.45ms
step:247/2330 train_time:14190ms step_avg:57.45ms
step:248/2330 train_time:14249ms step_avg:57.46ms
step:249/2330 train_time:14304ms step_avg:57.45ms
step:250/2330 train_time:14365ms step_avg:57.46ms
step:250/2330 val_loss:4.9026 train_time:14443ms step_avg:57.77ms
step:251/2330 train_time:14462ms step_avg:57.62ms
step:252/2330 train_time:14483ms step_avg:57.47ms
step:253/2330 train_time:14535ms step_avg:57.45ms
step:254/2330 train_time:14601ms step_avg:57.48ms
step:255/2330 train_time:14656ms step_avg:57.47ms
step:256/2330 train_time:14720ms step_avg:57.50ms
step:257/2330 train_time:14775ms step_avg:57.49ms
step:258/2330 train_time:14834ms step_avg:57.50ms
step:259/2330 train_time:14890ms step_avg:57.49ms
step:260/2330 train_time:14950ms step_avg:57.50ms
step:261/2330 train_time:15005ms step_avg:57.49ms
step:262/2330 train_time:15064ms step_avg:57.49ms
step:263/2330 train_time:15119ms step_avg:57.49ms
step:264/2330 train_time:15177ms step_avg:57.49ms
step:265/2330 train_time:15232ms step_avg:57.48ms
step:266/2330 train_time:15291ms step_avg:57.48ms
step:267/2330 train_time:15346ms step_avg:57.48ms
step:268/2330 train_time:15407ms step_avg:57.49ms
step:269/2330 train_time:15465ms step_avg:57.49ms
step:270/2330 train_time:15526ms step_avg:57.50ms
step:271/2330 train_time:15583ms step_avg:57.50ms
step:272/2330 train_time:15642ms step_avg:57.51ms
step:273/2330 train_time:15698ms step_avg:57.50ms
step:274/2330 train_time:15758ms step_avg:57.51ms
step:275/2330 train_time:15813ms step_avg:57.50ms
step:276/2330 train_time:15873ms step_avg:57.51ms
step:277/2330 train_time:15928ms step_avg:57.50ms
step:278/2330 train_time:15987ms step_avg:57.51ms
step:279/2330 train_time:16043ms step_avg:57.50ms
step:280/2330 train_time:16102ms step_avg:57.51ms
step:281/2330 train_time:16157ms step_avg:57.50ms
step:282/2330 train_time:16215ms step_avg:57.50ms
step:283/2330 train_time:16271ms step_avg:57.49ms
step:284/2330 train_time:16332ms step_avg:57.51ms
step:285/2330 train_time:16388ms step_avg:57.50ms
step:286/2330 train_time:16447ms step_avg:57.51ms
step:287/2330 train_time:16503ms step_avg:57.50ms
step:288/2330 train_time:16564ms step_avg:57.51ms
step:289/2330 train_time:16621ms step_avg:57.51ms
step:290/2330 train_time:16681ms step_avg:57.52ms
step:291/2330 train_time:16738ms step_avg:57.52ms
step:292/2330 train_time:16797ms step_avg:57.52ms
step:293/2330 train_time:16852ms step_avg:57.52ms
step:294/2330 train_time:16911ms step_avg:57.52ms
step:295/2330 train_time:16966ms step_avg:57.51ms
step:296/2330 train_time:17026ms step_avg:57.52ms
step:297/2330 train_time:17082ms step_avg:57.51ms
step:298/2330 train_time:17141ms step_avg:57.52ms
step:299/2330 train_time:17197ms step_avg:57.51ms
step:300/2330 train_time:17256ms step_avg:57.52ms
step:301/2330 train_time:17311ms step_avg:57.51ms
step:302/2330 train_time:17371ms step_avg:57.52ms
step:303/2330 train_time:17427ms step_avg:57.51ms
step:304/2330 train_time:17487ms step_avg:57.52ms
step:305/2330 train_time:17543ms step_avg:57.52ms
step:306/2330 train_time:17602ms step_avg:57.52ms
step:307/2330 train_time:17659ms step_avg:57.52ms
step:308/2330 train_time:17718ms step_avg:57.53ms
step:309/2330 train_time:17775ms step_avg:57.52ms
step:310/2330 train_time:17835ms step_avg:57.53ms
step:311/2330 train_time:17891ms step_avg:57.53ms
step:312/2330 train_time:17950ms step_avg:57.53ms
step:313/2330 train_time:18006ms step_avg:57.53ms
step:314/2330 train_time:18065ms step_avg:57.53ms
step:315/2330 train_time:18121ms step_avg:57.53ms
step:316/2330 train_time:18179ms step_avg:57.53ms
step:317/2330 train_time:18235ms step_avg:57.52ms
step:318/2330 train_time:18294ms step_avg:57.53ms
step:319/2330 train_time:18349ms step_avg:57.52ms
step:320/2330 train_time:18409ms step_avg:57.53ms
step:321/2330 train_time:18465ms step_avg:57.52ms
step:322/2330 train_time:18525ms step_avg:57.53ms
step:323/2330 train_time:18581ms step_avg:57.53ms
step:324/2330 train_time:18640ms step_avg:57.53ms
step:325/2330 train_time:18697ms step_avg:57.53ms
step:326/2330 train_time:18755ms step_avg:57.53ms
step:327/2330 train_time:18811ms step_avg:57.53ms
step:328/2330 train_time:18871ms step_avg:57.53ms
step:329/2330 train_time:18927ms step_avg:57.53ms
step:330/2330 train_time:18986ms step_avg:57.53ms
step:331/2330 train_time:19042ms step_avg:57.53ms
step:332/2330 train_time:19101ms step_avg:57.53ms
step:333/2330 train_time:19156ms step_avg:57.53ms
step:334/2330 train_time:19216ms step_avg:57.53ms
step:335/2330 train_time:19271ms step_avg:57.53ms
step:336/2330 train_time:19331ms step_avg:57.53ms
step:337/2330 train_time:19387ms step_avg:57.53ms
step:338/2330 train_time:19447ms step_avg:57.54ms
step:339/2330 train_time:19503ms step_avg:57.53ms
step:340/2330 train_time:19562ms step_avg:57.54ms
step:341/2330 train_time:19618ms step_avg:57.53ms
step:342/2330 train_time:19677ms step_avg:57.54ms
step:343/2330 train_time:19733ms step_avg:57.53ms
step:344/2330 train_time:19794ms step_avg:57.54ms
step:345/2330 train_time:19850ms step_avg:57.54ms
step:346/2330 train_time:19910ms step_avg:57.54ms
step:347/2330 train_time:19966ms step_avg:57.54ms
step:348/2330 train_time:20026ms step_avg:57.54ms
step:349/2330 train_time:20082ms step_avg:57.54ms
step:350/2330 train_time:20140ms step_avg:57.54ms
step:351/2330 train_time:20197ms step_avg:57.54ms
step:352/2330 train_time:20256ms step_avg:57.55ms
step:353/2330 train_time:20311ms step_avg:57.54ms
step:354/2330 train_time:20371ms step_avg:57.55ms
step:355/2330 train_time:20427ms step_avg:57.54ms
step:356/2330 train_time:20487ms step_avg:57.55ms
step:357/2330 train_time:20544ms step_avg:57.55ms
step:358/2330 train_time:20603ms step_avg:57.55ms
step:359/2330 train_time:20660ms step_avg:57.55ms
step:360/2330 train_time:20720ms step_avg:57.55ms
step:361/2330 train_time:20776ms step_avg:57.55ms
step:362/2330 train_time:20835ms step_avg:57.55ms
step:363/2330 train_time:20890ms step_avg:57.55ms
step:364/2330 train_time:20951ms step_avg:57.56ms
step:365/2330 train_time:21007ms step_avg:57.55ms
step:366/2330 train_time:21067ms step_avg:57.56ms
step:367/2330 train_time:21124ms step_avg:57.56ms
step:368/2330 train_time:21183ms step_avg:57.56ms
step:369/2330 train_time:21239ms step_avg:57.56ms
step:370/2330 train_time:21298ms step_avg:57.56ms
step:371/2330 train_time:21354ms step_avg:57.56ms
step:372/2330 train_time:21414ms step_avg:57.56ms
step:373/2330 train_time:21469ms step_avg:57.56ms
step:374/2330 train_time:21530ms step_avg:57.57ms
step:375/2330 train_time:21586ms step_avg:57.56ms
step:376/2330 train_time:21645ms step_avg:57.57ms
step:377/2330 train_time:21701ms step_avg:57.56ms
step:378/2330 train_time:21761ms step_avg:57.57ms
step:379/2330 train_time:21817ms step_avg:57.56ms
step:380/2330 train_time:21875ms step_avg:57.57ms
step:381/2330 train_time:21931ms step_avg:57.56ms
step:382/2330 train_time:21991ms step_avg:57.57ms
step:383/2330 train_time:22047ms step_avg:57.56ms
step:384/2330 train_time:22108ms step_avg:57.57ms
step:385/2330 train_time:22165ms step_avg:57.57ms
step:386/2330 train_time:22224ms step_avg:57.57ms
step:387/2330 train_time:22280ms step_avg:57.57ms
step:388/2330 train_time:22338ms step_avg:57.57ms
step:389/2330 train_time:22394ms step_avg:57.57ms
step:390/2330 train_time:22453ms step_avg:57.57ms
step:391/2330 train_time:22509ms step_avg:57.57ms
step:392/2330 train_time:22570ms step_avg:57.58ms
step:393/2330 train_time:22626ms step_avg:57.57ms
step:394/2330 train_time:22686ms step_avg:57.58ms
step:395/2330 train_time:22742ms step_avg:57.58ms
step:396/2330 train_time:22801ms step_avg:57.58ms
step:397/2330 train_time:22857ms step_avg:57.57ms
step:398/2330 train_time:22917ms step_avg:57.58ms
step:399/2330 train_time:22972ms step_avg:57.57ms
step:400/2330 train_time:23032ms step_avg:57.58ms
step:401/2330 train_time:23087ms step_avg:57.57ms
step:402/2330 train_time:23147ms step_avg:57.58ms
step:403/2330 train_time:23203ms step_avg:57.58ms
step:404/2330 train_time:23263ms step_avg:57.58ms
step:405/2330 train_time:23319ms step_avg:57.58ms
step:406/2330 train_time:23378ms step_avg:57.58ms
step:407/2330 train_time:23434ms step_avg:57.58ms
step:408/2330 train_time:23494ms step_avg:57.58ms
step:409/2330 train_time:23549ms step_avg:57.58ms
step:410/2330 train_time:23610ms step_avg:57.58ms
step:411/2330 train_time:23666ms step_avg:57.58ms
step:412/2330 train_time:23726ms step_avg:57.59ms
step:413/2330 train_time:23782ms step_avg:57.58ms
step:414/2330 train_time:23841ms step_avg:57.59ms
step:415/2330 train_time:23897ms step_avg:57.58ms
step:416/2330 train_time:23957ms step_avg:57.59ms
step:417/2330 train_time:24012ms step_avg:57.58ms
step:418/2330 train_time:24072ms step_avg:57.59ms
step:419/2330 train_time:24127ms step_avg:57.58ms
step:420/2330 train_time:24188ms step_avg:57.59ms
step:421/2330 train_time:24244ms step_avg:57.59ms
step:422/2330 train_time:24303ms step_avg:57.59ms
step:423/2330 train_time:24359ms step_avg:57.59ms
step:424/2330 train_time:24420ms step_avg:57.59ms
step:425/2330 train_time:24475ms step_avg:57.59ms
step:426/2330 train_time:24536ms step_avg:57.60ms
step:427/2330 train_time:24592ms step_avg:57.59ms
step:428/2330 train_time:24651ms step_avg:57.60ms
step:429/2330 train_time:24706ms step_avg:57.59ms
step:430/2330 train_time:24767ms step_avg:57.60ms
step:431/2330 train_time:24823ms step_avg:57.59ms
step:432/2330 train_time:24883ms step_avg:57.60ms
step:433/2330 train_time:24939ms step_avg:57.60ms
step:434/2330 train_time:24998ms step_avg:57.60ms
step:435/2330 train_time:25054ms step_avg:57.60ms
step:436/2330 train_time:25114ms step_avg:57.60ms
step:437/2330 train_time:25170ms step_avg:57.60ms
step:438/2330 train_time:25230ms step_avg:57.60ms
step:439/2330 train_time:25286ms step_avg:57.60ms
step:440/2330 train_time:25345ms step_avg:57.60ms
step:441/2330 train_time:25401ms step_avg:57.60ms
step:442/2330 train_time:25460ms step_avg:57.60ms
step:443/2330 train_time:25517ms step_avg:57.60ms
step:444/2330 train_time:25576ms step_avg:57.60ms
step:445/2330 train_time:25631ms step_avg:57.60ms
step:446/2330 train_time:25692ms step_avg:57.60ms
step:447/2330 train_time:25747ms step_avg:57.60ms
step:448/2330 train_time:25809ms step_avg:57.61ms
step:449/2330 train_time:25865ms step_avg:57.60ms
step:450/2330 train_time:25925ms step_avg:57.61ms
step:451/2330 train_time:25981ms step_avg:57.61ms
step:452/2330 train_time:26040ms step_avg:57.61ms
step:453/2330 train_time:26096ms step_avg:57.61ms
step:454/2330 train_time:26155ms step_avg:57.61ms
step:455/2330 train_time:26212ms step_avg:57.61ms
step:456/2330 train_time:26272ms step_avg:57.61ms
step:457/2330 train_time:26328ms step_avg:57.61ms
step:458/2330 train_time:26388ms step_avg:57.62ms
step:459/2330 train_time:26444ms step_avg:57.61ms
step:460/2330 train_time:26503ms step_avg:57.62ms
step:461/2330 train_time:26559ms step_avg:57.61ms
step:462/2330 train_time:26619ms step_avg:57.62ms
step:463/2330 train_time:26675ms step_avg:57.61ms
step:464/2330 train_time:26734ms step_avg:57.62ms
step:465/2330 train_time:26790ms step_avg:57.61ms
step:466/2330 train_time:26850ms step_avg:57.62ms
step:467/2330 train_time:26905ms step_avg:57.61ms
step:468/2330 train_time:26966ms step_avg:57.62ms
step:469/2330 train_time:27022ms step_avg:57.62ms
step:470/2330 train_time:27081ms step_avg:57.62ms
step:471/2330 train_time:27138ms step_avg:57.62ms
step:472/2330 train_time:27196ms step_avg:57.62ms
step:473/2330 train_time:27252ms step_avg:57.62ms
step:474/2330 train_time:27311ms step_avg:57.62ms
step:475/2330 train_time:27367ms step_avg:57.62ms
step:476/2330 train_time:27427ms step_avg:57.62ms
step:477/2330 train_time:27484ms step_avg:57.62ms
step:478/2330 train_time:27543ms step_avg:57.62ms
step:479/2330 train_time:27599ms step_avg:57.62ms
step:480/2330 train_time:27658ms step_avg:57.62ms
step:481/2330 train_time:27714ms step_avg:57.62ms
step:482/2330 train_time:27774ms step_avg:57.62ms
step:483/2330 train_time:27830ms step_avg:57.62ms
step:484/2330 train_time:27890ms step_avg:57.62ms
step:485/2330 train_time:27946ms step_avg:57.62ms
step:486/2330 train_time:28005ms step_avg:57.62ms
step:487/2330 train_time:28061ms step_avg:57.62ms
step:488/2330 train_time:28121ms step_avg:57.62ms
step:489/2330 train_time:28177ms step_avg:57.62ms
step:490/2330 train_time:28235ms step_avg:57.62ms
step:491/2330 train_time:28291ms step_avg:57.62ms
step:492/2330 train_time:28351ms step_avg:57.62ms
step:493/2330 train_time:28406ms step_avg:57.62ms
step:494/2330 train_time:28466ms step_avg:57.62ms
step:495/2330 train_time:28522ms step_avg:57.62ms
step:496/2330 train_time:28581ms step_avg:57.62ms
step:497/2330 train_time:28637ms step_avg:57.62ms
step:498/2330 train_time:28697ms step_avg:57.63ms
step:499/2330 train_time:28753ms step_avg:57.62ms
step:500/2330 train_time:28813ms step_avg:57.63ms
step:500/2330 val_loss:4.4195 train_time:28894ms step_avg:57.79ms
step:501/2330 train_time:28913ms step_avg:57.71ms
step:502/2330 train_time:28933ms step_avg:57.63ms
step:503/2330 train_time:28990ms step_avg:57.63ms
step:504/2330 train_time:29052ms step_avg:57.64ms
step:505/2330 train_time:29109ms step_avg:57.64ms
step:506/2330 train_time:29171ms step_avg:57.65ms
step:507/2330 train_time:29227ms step_avg:57.65ms
step:508/2330 train_time:29285ms step_avg:57.65ms
step:509/2330 train_time:29341ms step_avg:57.64ms
step:510/2330 train_time:29400ms step_avg:57.65ms
step:511/2330 train_time:29455ms step_avg:57.64ms
step:512/2330 train_time:29513ms step_avg:57.64ms
step:513/2330 train_time:29568ms step_avg:57.64ms
step:514/2330 train_time:29627ms step_avg:57.64ms
step:515/2330 train_time:29682ms step_avg:57.64ms
step:516/2330 train_time:29741ms step_avg:57.64ms
step:517/2330 train_time:29796ms step_avg:57.63ms
step:518/2330 train_time:29856ms step_avg:57.64ms
step:519/2330 train_time:29913ms step_avg:57.64ms
step:520/2330 train_time:29973ms step_avg:57.64ms
step:521/2330 train_time:30030ms step_avg:57.64ms
step:522/2330 train_time:30091ms step_avg:57.65ms
step:523/2330 train_time:30147ms step_avg:57.64ms
step:524/2330 train_time:30208ms step_avg:57.65ms
step:525/2330 train_time:30264ms step_avg:57.65ms
step:526/2330 train_time:30323ms step_avg:57.65ms
step:527/2330 train_time:30379ms step_avg:57.65ms
step:528/2330 train_time:30438ms step_avg:57.65ms
step:529/2330 train_time:30494ms step_avg:57.64ms
step:530/2330 train_time:30552ms step_avg:57.65ms
step:531/2330 train_time:30607ms step_avg:57.64ms
step:532/2330 train_time:30666ms step_avg:57.64ms
step:533/2330 train_time:30721ms step_avg:57.64ms
step:534/2330 train_time:30780ms step_avg:57.64ms
step:535/2330 train_time:30836ms step_avg:57.64ms
step:536/2330 train_time:30895ms step_avg:57.64ms
step:537/2330 train_time:30952ms step_avg:57.64ms
step:538/2330 train_time:31012ms step_avg:57.64ms
step:539/2330 train_time:31068ms step_avg:57.64ms
step:540/2330 train_time:31129ms step_avg:57.65ms
step:541/2330 train_time:31185ms step_avg:57.64ms
step:542/2330 train_time:31245ms step_avg:57.65ms
step:543/2330 train_time:31301ms step_avg:57.64ms
step:544/2330 train_time:31359ms step_avg:57.65ms
step:545/2330 train_time:31415ms step_avg:57.64ms
step:546/2330 train_time:31474ms step_avg:57.64ms
step:547/2330 train_time:31529ms step_avg:57.64ms
step:548/2330 train_time:31588ms step_avg:57.64ms
step:549/2330 train_time:31645ms step_avg:57.64ms
step:550/2330 train_time:31703ms step_avg:57.64ms
step:551/2330 train_time:31759ms step_avg:57.64ms
step:552/2330 train_time:31817ms step_avg:57.64ms
step:553/2330 train_time:31873ms step_avg:57.64ms
step:554/2330 train_time:31933ms step_avg:57.64ms
step:555/2330 train_time:31990ms step_avg:57.64ms
step:556/2330 train_time:32049ms step_avg:57.64ms
step:557/2330 train_time:32105ms step_avg:57.64ms
step:558/2330 train_time:32165ms step_avg:57.64ms
step:559/2330 train_time:32222ms step_avg:57.64ms
step:560/2330 train_time:32282ms step_avg:57.65ms
step:561/2330 train_time:32338ms step_avg:57.64ms
step:562/2330 train_time:32397ms step_avg:57.65ms
step:563/2330 train_time:32453ms step_avg:57.64ms
step:564/2330 train_time:32511ms step_avg:57.64ms
step:565/2330 train_time:32567ms step_avg:57.64ms
step:566/2330 train_time:32626ms step_avg:57.64ms
step:567/2330 train_time:32682ms step_avg:57.64ms
step:568/2330 train_time:32741ms step_avg:57.64ms
step:569/2330 train_time:32797ms step_avg:57.64ms
step:570/2330 train_time:32856ms step_avg:57.64ms
step:571/2330 train_time:32912ms step_avg:57.64ms
step:572/2330 train_time:32972ms step_avg:57.64ms
step:573/2330 train_time:33028ms step_avg:57.64ms
step:574/2330 train_time:33088ms step_avg:57.65ms
step:575/2330 train_time:33145ms step_avg:57.64ms
step:576/2330 train_time:33203ms step_avg:57.64ms
step:577/2330 train_time:33260ms step_avg:57.64ms
step:578/2330 train_time:33318ms step_avg:57.64ms
step:579/2330 train_time:33375ms step_avg:57.64ms
step:580/2330 train_time:33434ms step_avg:57.65ms
step:581/2330 train_time:33490ms step_avg:57.64ms
step:582/2330 train_time:33549ms step_avg:57.64ms
step:583/2330 train_time:33605ms step_avg:57.64ms
step:584/2330 train_time:33664ms step_avg:57.64ms
step:585/2330 train_time:33721ms step_avg:57.64ms
step:586/2330 train_time:33779ms step_avg:57.64ms
step:587/2330 train_time:33836ms step_avg:57.64ms
step:588/2330 train_time:33895ms step_avg:57.64ms
step:589/2330 train_time:33950ms step_avg:57.64ms
step:590/2330 train_time:34011ms step_avg:57.65ms
step:591/2330 train_time:34067ms step_avg:57.64ms
step:592/2330 train_time:34127ms step_avg:57.65ms
step:593/2330 train_time:34184ms step_avg:57.65ms
step:594/2330 train_time:34243ms step_avg:57.65ms
step:595/2330 train_time:34300ms step_avg:57.65ms
step:596/2330 train_time:34358ms step_avg:57.65ms
step:597/2330 train_time:34415ms step_avg:57.65ms
step:598/2330 train_time:34474ms step_avg:57.65ms
step:599/2330 train_time:34530ms step_avg:57.65ms
step:600/2330 train_time:34589ms step_avg:57.65ms
step:601/2330 train_time:34646ms step_avg:57.65ms
step:602/2330 train_time:34704ms step_avg:57.65ms
step:603/2330 train_time:34761ms step_avg:57.65ms
step:604/2330 train_time:34820ms step_avg:57.65ms
step:605/2330 train_time:34876ms step_avg:57.65ms
step:606/2330 train_time:34935ms step_avg:57.65ms
step:607/2330 train_time:34991ms step_avg:57.65ms
step:608/2330 train_time:35051ms step_avg:57.65ms
step:609/2330 train_time:35107ms step_avg:57.65ms
step:610/2330 train_time:35167ms step_avg:57.65ms
step:611/2330 train_time:35224ms step_avg:57.65ms
step:612/2330 train_time:35283ms step_avg:57.65ms
step:613/2330 train_time:35340ms step_avg:57.65ms
step:614/2330 train_time:35399ms step_avg:57.65ms
step:615/2330 train_time:35454ms step_avg:57.65ms
step:616/2330 train_time:35513ms step_avg:57.65ms
step:617/2330 train_time:35569ms step_avg:57.65ms
step:618/2330 train_time:35628ms step_avg:57.65ms
step:619/2330 train_time:35684ms step_avg:57.65ms
step:620/2330 train_time:35742ms step_avg:57.65ms
step:621/2330 train_time:35798ms step_avg:57.65ms
step:622/2330 train_time:35857ms step_avg:57.65ms
step:623/2330 train_time:35913ms step_avg:57.65ms
step:624/2330 train_time:35973ms step_avg:57.65ms
step:625/2330 train_time:36029ms step_avg:57.65ms
step:626/2330 train_time:36089ms step_avg:57.65ms
step:627/2330 train_time:36145ms step_avg:57.65ms
step:628/2330 train_time:36205ms step_avg:57.65ms
step:629/2330 train_time:36262ms step_avg:57.65ms
step:630/2330 train_time:36321ms step_avg:57.65ms
step:631/2330 train_time:36377ms step_avg:57.65ms
step:632/2330 train_time:36436ms step_avg:57.65ms
step:633/2330 train_time:36492ms step_avg:57.65ms
step:634/2330 train_time:36553ms step_avg:57.65ms
step:635/2330 train_time:36608ms step_avg:57.65ms
step:636/2330 train_time:36668ms step_avg:57.65ms
step:637/2330 train_time:36724ms step_avg:57.65ms
step:638/2330 train_time:36783ms step_avg:57.65ms
step:639/2330 train_time:36840ms step_avg:57.65ms
step:640/2330 train_time:36899ms step_avg:57.65ms
step:641/2330 train_time:36955ms step_avg:57.65ms
step:642/2330 train_time:37015ms step_avg:57.66ms
step:643/2330 train_time:37071ms step_avg:57.65ms
step:644/2330 train_time:37131ms step_avg:57.66ms
step:645/2330 train_time:37186ms step_avg:57.65ms
step:646/2330 train_time:37246ms step_avg:57.66ms
step:647/2330 train_time:37303ms step_avg:57.65ms
step:648/2330 train_time:37362ms step_avg:57.66ms
step:649/2330 train_time:37419ms step_avg:57.66ms
step:650/2330 train_time:37478ms step_avg:57.66ms
step:651/2330 train_time:37534ms step_avg:57.66ms
step:652/2330 train_time:37593ms step_avg:57.66ms
step:653/2330 train_time:37648ms step_avg:57.65ms
step:654/2330 train_time:37709ms step_avg:57.66ms
step:655/2330 train_time:37765ms step_avg:57.66ms
step:656/2330 train_time:37824ms step_avg:57.66ms
step:657/2330 train_time:37880ms step_avg:57.66ms
step:658/2330 train_time:37939ms step_avg:57.66ms
step:659/2330 train_time:37996ms step_avg:57.66ms
step:660/2330 train_time:38055ms step_avg:57.66ms
step:661/2330 train_time:38111ms step_avg:57.66ms
step:662/2330 train_time:38171ms step_avg:57.66ms
step:663/2330 train_time:38227ms step_avg:57.66ms
step:664/2330 train_time:38286ms step_avg:57.66ms
step:665/2330 train_time:38342ms step_avg:57.66ms
step:666/2330 train_time:38402ms step_avg:57.66ms
step:667/2330 train_time:38458ms step_avg:57.66ms
step:668/2330 train_time:38518ms step_avg:57.66ms
step:669/2330 train_time:38574ms step_avg:57.66ms
step:670/2330 train_time:38633ms step_avg:57.66ms
step:671/2330 train_time:38689ms step_avg:57.66ms
step:672/2330 train_time:38749ms step_avg:57.66ms
step:673/2330 train_time:38805ms step_avg:57.66ms
step:674/2330 train_time:38864ms step_avg:57.66ms
step:675/2330 train_time:38921ms step_avg:57.66ms
step:676/2330 train_time:38980ms step_avg:57.66ms
step:677/2330 train_time:39035ms step_avg:57.66ms
step:678/2330 train_time:39095ms step_avg:57.66ms
step:679/2330 train_time:39151ms step_avg:57.66ms
step:680/2330 train_time:39211ms step_avg:57.66ms
step:681/2330 train_time:39266ms step_avg:57.66ms
step:682/2330 train_time:39326ms step_avg:57.66ms
step:683/2330 train_time:39382ms step_avg:57.66ms
step:684/2330 train_time:39441ms step_avg:57.66ms
step:685/2330 train_time:39497ms step_avg:57.66ms
step:686/2330 train_time:39556ms step_avg:57.66ms
step:687/2330 train_time:39612ms step_avg:57.66ms
step:688/2330 train_time:39671ms step_avg:57.66ms
step:689/2330 train_time:39727ms step_avg:57.66ms
step:690/2330 train_time:39786ms step_avg:57.66ms
step:691/2330 train_time:39842ms step_avg:57.66ms
step:692/2330 train_time:39902ms step_avg:57.66ms
step:693/2330 train_time:39958ms step_avg:57.66ms
step:694/2330 train_time:40018ms step_avg:57.66ms
step:695/2330 train_time:40074ms step_avg:57.66ms
step:696/2330 train_time:40134ms step_avg:57.66ms
step:697/2330 train_time:40189ms step_avg:57.66ms
step:698/2330 train_time:40249ms step_avg:57.66ms
step:699/2330 train_time:40305ms step_avg:57.66ms
step:700/2330 train_time:40365ms step_avg:57.66ms
step:701/2330 train_time:40420ms step_avg:57.66ms
step:702/2330 train_time:40480ms step_avg:57.66ms
step:703/2330 train_time:40536ms step_avg:57.66ms
step:704/2330 train_time:40594ms step_avg:57.66ms
step:705/2330 train_time:40650ms step_avg:57.66ms
step:706/2330 train_time:40711ms step_avg:57.66ms
step:707/2330 train_time:40767ms step_avg:57.66ms
step:708/2330 train_time:40826ms step_avg:57.66ms
step:709/2330 train_time:40882ms step_avg:57.66ms
step:710/2330 train_time:40942ms step_avg:57.66ms
step:711/2330 train_time:40998ms step_avg:57.66ms
step:712/2330 train_time:41057ms step_avg:57.66ms
step:713/2330 train_time:41114ms step_avg:57.66ms
step:714/2330 train_time:41172ms step_avg:57.66ms
step:715/2330 train_time:41228ms step_avg:57.66ms
step:716/2330 train_time:41288ms step_avg:57.66ms
step:717/2330 train_time:41344ms step_avg:57.66ms
step:718/2330 train_time:41403ms step_avg:57.66ms
step:719/2330 train_time:41459ms step_avg:57.66ms
step:720/2330 train_time:41519ms step_avg:57.67ms
step:721/2330 train_time:41575ms step_avg:57.66ms
step:722/2330 train_time:41634ms step_avg:57.67ms
step:723/2330 train_time:41690ms step_avg:57.66ms
step:724/2330 train_time:41750ms step_avg:57.67ms
step:725/2330 train_time:41806ms step_avg:57.66ms
step:726/2330 train_time:41865ms step_avg:57.67ms
step:727/2330 train_time:41921ms step_avg:57.66ms
step:728/2330 train_time:41980ms step_avg:57.67ms
step:729/2330 train_time:42037ms step_avg:57.66ms
step:730/2330 train_time:42096ms step_avg:57.67ms
step:731/2330 train_time:42152ms step_avg:57.66ms
step:732/2330 train_time:42211ms step_avg:57.67ms
step:733/2330 train_time:42267ms step_avg:57.66ms
step:734/2330 train_time:42327ms step_avg:57.67ms
step:735/2330 train_time:42383ms step_avg:57.66ms
step:736/2330 train_time:42442ms step_avg:57.67ms
step:737/2330 train_time:42498ms step_avg:57.66ms
step:738/2330 train_time:42558ms step_avg:57.67ms
step:739/2330 train_time:42613ms step_avg:57.66ms
step:740/2330 train_time:42673ms step_avg:57.67ms
step:741/2330 train_time:42728ms step_avg:57.66ms
step:742/2330 train_time:42788ms step_avg:57.67ms
step:743/2330 train_time:42844ms step_avg:57.66ms
step:744/2330 train_time:42903ms step_avg:57.66ms
step:745/2330 train_time:42958ms step_avg:57.66ms
step:746/2330 train_time:43018ms step_avg:57.67ms
step:747/2330 train_time:43074ms step_avg:57.66ms
step:748/2330 train_time:43134ms step_avg:57.67ms
step:749/2330 train_time:43190ms step_avg:57.66ms
step:750/2330 train_time:43250ms step_avg:57.67ms
step:750/2330 val_loss:4.2229 train_time:43329ms step_avg:57.77ms
step:751/2330 train_time:43348ms step_avg:57.72ms
step:752/2330 train_time:43367ms step_avg:57.67ms
step:753/2330 train_time:43424ms step_avg:57.67ms
step:754/2330 train_time:43488ms step_avg:57.68ms
step:755/2330 train_time:43545ms step_avg:57.68ms
step:756/2330 train_time:43604ms step_avg:57.68ms
step:757/2330 train_time:43661ms step_avg:57.68ms
step:758/2330 train_time:43720ms step_avg:57.68ms
step:759/2330 train_time:43775ms step_avg:57.68ms
step:760/2330 train_time:43834ms step_avg:57.68ms
step:761/2330 train_time:43889ms step_avg:57.67ms
step:762/2330 train_time:43948ms step_avg:57.67ms
step:763/2330 train_time:44003ms step_avg:57.67ms
step:764/2330 train_time:44063ms step_avg:57.67ms
step:765/2330 train_time:44119ms step_avg:57.67ms
step:766/2330 train_time:44177ms step_avg:57.67ms
step:767/2330 train_time:44234ms step_avg:57.67ms
step:768/2330 train_time:44293ms step_avg:57.67ms
step:769/2330 train_time:44351ms step_avg:57.67ms
step:770/2330 train_time:44413ms step_avg:57.68ms
step:771/2330 train_time:44470ms step_avg:57.68ms
step:772/2330 train_time:44533ms step_avg:57.68ms
step:773/2330 train_time:44590ms step_avg:57.68ms
step:774/2330 train_time:44652ms step_avg:57.69ms
step:775/2330 train_time:44709ms step_avg:57.69ms
step:776/2330 train_time:44768ms step_avg:57.69ms
step:777/2330 train_time:44824ms step_avg:57.69ms
step:778/2330 train_time:44885ms step_avg:57.69ms
step:779/2330 train_time:44941ms step_avg:57.69ms
step:780/2330 train_time:45000ms step_avg:57.69ms
step:781/2330 train_time:45056ms step_avg:57.69ms
step:782/2330 train_time:45116ms step_avg:57.69ms
step:783/2330 train_time:45172ms step_avg:57.69ms
step:784/2330 train_time:45231ms step_avg:57.69ms
step:785/2330 train_time:45288ms step_avg:57.69ms
step:786/2330 train_time:45350ms step_avg:57.70ms
step:787/2330 train_time:45407ms step_avg:57.70ms
step:788/2330 train_time:45468ms step_avg:57.70ms
step:789/2330 train_time:45526ms step_avg:57.70ms
step:790/2330 train_time:45586ms step_avg:57.70ms
step:791/2330 train_time:45644ms step_avg:57.70ms
step:792/2330 train_time:45704ms step_avg:57.71ms
step:793/2330 train_time:45761ms step_avg:57.71ms
step:794/2330 train_time:45821ms step_avg:57.71ms
step:795/2330 train_time:45878ms step_avg:57.71ms
step:796/2330 train_time:45937ms step_avg:57.71ms
step:797/2330 train_time:45993ms step_avg:57.71ms
step:798/2330 train_time:46054ms step_avg:57.71ms
step:799/2330 train_time:46110ms step_avg:57.71ms
step:800/2330 train_time:46170ms step_avg:57.71ms
step:801/2330 train_time:46226ms step_avg:57.71ms
step:802/2330 train_time:46286ms step_avg:57.71ms
step:803/2330 train_time:46343ms step_avg:57.71ms
step:804/2330 train_time:46403ms step_avg:57.72ms
step:805/2330 train_time:46462ms step_avg:57.72ms
step:806/2330 train_time:46523ms step_avg:57.72ms
step:807/2330 train_time:46581ms step_avg:57.72ms
step:808/2330 train_time:46641ms step_avg:57.72ms
step:809/2330 train_time:46699ms step_avg:57.72ms
step:810/2330 train_time:46758ms step_avg:57.73ms
step:811/2330 train_time:46815ms step_avg:57.72ms
step:812/2330 train_time:46876ms step_avg:57.73ms
step:813/2330 train_time:46932ms step_avg:57.73ms
step:814/2330 train_time:46992ms step_avg:57.73ms
step:815/2330 train_time:47048ms step_avg:57.73ms
step:816/2330 train_time:47108ms step_avg:57.73ms
step:817/2330 train_time:47164ms step_avg:57.73ms
step:818/2330 train_time:47225ms step_avg:57.73ms
step:819/2330 train_time:47282ms step_avg:57.73ms
step:820/2330 train_time:47342ms step_avg:57.73ms
step:821/2330 train_time:47399ms step_avg:57.73ms
step:822/2330 train_time:47460ms step_avg:57.74ms
step:823/2330 train_time:47516ms step_avg:57.74ms
step:824/2330 train_time:47577ms step_avg:57.74ms
step:825/2330 train_time:47635ms step_avg:57.74ms
step:826/2330 train_time:47695ms step_avg:57.74ms
step:827/2330 train_time:47752ms step_avg:57.74ms
step:828/2330 train_time:47812ms step_avg:57.74ms
step:829/2330 train_time:47869ms step_avg:57.74ms
step:830/2330 train_time:47929ms step_avg:57.75ms
step:831/2330 train_time:47985ms step_avg:57.74ms
step:832/2330 train_time:48045ms step_avg:57.75ms
step:833/2330 train_time:48101ms step_avg:57.74ms
step:834/2330 train_time:48161ms step_avg:57.75ms
step:835/2330 train_time:48218ms step_avg:57.75ms
step:836/2330 train_time:48278ms step_avg:57.75ms
step:837/2330 train_time:48335ms step_avg:57.75ms
step:838/2330 train_time:48395ms step_avg:57.75ms
step:839/2330 train_time:48452ms step_avg:57.75ms
step:840/2330 train_time:48513ms step_avg:57.75ms
step:841/2330 train_time:48570ms step_avg:57.75ms
step:842/2330 train_time:48630ms step_avg:57.76ms
step:843/2330 train_time:48686ms step_avg:57.75ms
step:844/2330 train_time:48747ms step_avg:57.76ms
step:845/2330 train_time:48805ms step_avg:57.76ms
step:846/2330 train_time:48864ms step_avg:57.76ms
step:847/2330 train_time:48922ms step_avg:57.76ms
step:848/2330 train_time:48982ms step_avg:57.76ms
step:849/2330 train_time:49039ms step_avg:57.76ms
step:850/2330 train_time:49098ms step_avg:57.76ms
step:851/2330 train_time:49155ms step_avg:57.76ms
step:852/2330 train_time:49215ms step_avg:57.76ms
step:853/2330 train_time:49271ms step_avg:57.76ms
step:854/2330 train_time:49332ms step_avg:57.77ms
step:855/2330 train_time:49388ms step_avg:57.76ms
step:856/2330 train_time:49449ms step_avg:57.77ms
step:857/2330 train_time:49506ms step_avg:57.77ms
step:858/2330 train_time:49566ms step_avg:57.77ms
step:859/2330 train_time:49624ms step_avg:57.77ms
step:860/2330 train_time:49684ms step_avg:57.77ms
step:861/2330 train_time:49741ms step_avg:57.77ms
step:862/2330 train_time:49802ms step_avg:57.77ms
step:863/2330 train_time:49859ms step_avg:57.77ms
step:864/2330 train_time:49920ms step_avg:57.78ms
step:865/2330 train_time:49976ms step_avg:57.78ms
step:866/2330 train_time:50035ms step_avg:57.78ms
step:867/2330 train_time:50092ms step_avg:57.78ms
step:868/2330 train_time:50152ms step_avg:57.78ms
step:869/2330 train_time:50209ms step_avg:57.78ms
step:870/2330 train_time:50269ms step_avg:57.78ms
step:871/2330 train_time:50326ms step_avg:57.78ms
step:872/2330 train_time:50386ms step_avg:57.78ms
step:873/2330 train_time:50444ms step_avg:57.78ms
step:874/2330 train_time:50504ms step_avg:57.78ms
step:875/2330 train_time:50562ms step_avg:57.78ms
step:876/2330 train_time:50621ms step_avg:57.79ms
step:877/2330 train_time:50678ms step_avg:57.79ms
step:878/2330 train_time:50737ms step_avg:57.79ms
step:879/2330 train_time:50794ms step_avg:57.79ms
step:880/2330 train_time:50855ms step_avg:57.79ms
step:881/2330 train_time:50912ms step_avg:57.79ms
step:882/2330 train_time:50973ms step_avg:57.79ms
step:883/2330 train_time:51030ms step_avg:57.79ms
step:884/2330 train_time:51089ms step_avg:57.79ms
step:885/2330 train_time:51146ms step_avg:57.79ms
step:886/2330 train_time:51206ms step_avg:57.79ms
step:887/2330 train_time:51263ms step_avg:57.79ms
step:888/2330 train_time:51323ms step_avg:57.80ms
step:889/2330 train_time:51381ms step_avg:57.80ms
step:890/2330 train_time:51440ms step_avg:57.80ms
step:891/2330 train_time:51497ms step_avg:57.80ms
step:892/2330 train_time:51558ms step_avg:57.80ms
step:893/2330 train_time:51615ms step_avg:57.80ms
step:894/2330 train_time:51675ms step_avg:57.80ms
step:895/2330 train_time:51732ms step_avg:57.80ms
step:896/2330 train_time:51794ms step_avg:57.81ms
step:897/2330 train_time:51850ms step_avg:57.80ms
step:898/2330 train_time:51910ms step_avg:57.81ms
step:899/2330 train_time:51966ms step_avg:57.80ms
step:900/2330 train_time:52027ms step_avg:57.81ms
step:901/2330 train_time:52084ms step_avg:57.81ms
step:902/2330 train_time:52144ms step_avg:57.81ms
step:903/2330 train_time:52201ms step_avg:57.81ms
step:904/2330 train_time:52261ms step_avg:57.81ms
step:905/2330 train_time:52318ms step_avg:57.81ms
step:906/2330 train_time:52378ms step_avg:57.81ms
step:907/2330 train_time:52435ms step_avg:57.81ms
step:908/2330 train_time:52495ms step_avg:57.81ms
step:909/2330 train_time:52552ms step_avg:57.81ms
step:910/2330 train_time:52613ms step_avg:57.82ms
step:911/2330 train_time:52670ms step_avg:57.82ms
step:912/2330 train_time:52730ms step_avg:57.82ms
step:913/2330 train_time:52788ms step_avg:57.82ms
step:914/2330 train_time:52848ms step_avg:57.82ms
step:915/2330 train_time:52905ms step_avg:57.82ms
step:916/2330 train_time:52965ms step_avg:57.82ms
step:917/2330 train_time:53023ms step_avg:57.82ms
step:918/2330 train_time:53082ms step_avg:57.82ms
step:919/2330 train_time:53139ms step_avg:57.82ms
step:920/2330 train_time:53200ms step_avg:57.83ms
step:921/2330 train_time:53256ms step_avg:57.82ms
step:922/2330 train_time:53316ms step_avg:57.83ms
step:923/2330 train_time:53374ms step_avg:57.83ms
step:924/2330 train_time:53433ms step_avg:57.83ms
step:925/2330 train_time:53489ms step_avg:57.83ms
step:926/2330 train_time:53550ms step_avg:57.83ms
step:927/2330 train_time:53607ms step_avg:57.83ms
step:928/2330 train_time:53668ms step_avg:57.83ms
step:929/2330 train_time:53726ms step_avg:57.83ms
step:930/2330 train_time:53785ms step_avg:57.83ms
step:931/2330 train_time:53842ms step_avg:57.83ms
step:932/2330 train_time:53902ms step_avg:57.83ms
step:933/2330 train_time:53958ms step_avg:57.83ms
step:934/2330 train_time:54018ms step_avg:57.84ms
step:935/2330 train_time:54075ms step_avg:57.83ms
step:936/2330 train_time:54135ms step_avg:57.84ms
step:937/2330 train_time:54192ms step_avg:57.84ms
step:938/2330 train_time:54251ms step_avg:57.84ms
step:939/2330 train_time:54308ms step_avg:57.84ms
step:940/2330 train_time:54369ms step_avg:57.84ms
step:941/2330 train_time:54426ms step_avg:57.84ms
step:942/2330 train_time:54486ms step_avg:57.84ms
step:943/2330 train_time:54542ms step_avg:57.84ms
step:944/2330 train_time:54603ms step_avg:57.84ms
step:945/2330 train_time:54661ms step_avg:57.84ms
step:946/2330 train_time:54721ms step_avg:57.84ms
step:947/2330 train_time:54778ms step_avg:57.84ms
step:948/2330 train_time:54838ms step_avg:57.85ms
step:949/2330 train_time:54895ms step_avg:57.85ms
step:950/2330 train_time:54955ms step_avg:57.85ms
step:951/2330 train_time:55012ms step_avg:57.85ms
step:952/2330 train_time:55072ms step_avg:57.85ms
step:953/2330 train_time:55130ms step_avg:57.85ms
step:954/2330 train_time:55190ms step_avg:57.85ms
step:955/2330 train_time:55246ms step_avg:57.85ms
step:956/2330 train_time:55306ms step_avg:57.85ms
step:957/2330 train_time:55363ms step_avg:57.85ms
step:958/2330 train_time:55423ms step_avg:57.85ms
step:959/2330 train_time:55481ms step_avg:57.85ms
step:960/2330 train_time:55541ms step_avg:57.86ms
step:961/2330 train_time:55598ms step_avg:57.85ms
step:962/2330 train_time:55659ms step_avg:57.86ms
step:963/2330 train_time:55717ms step_avg:57.86ms
step:964/2330 train_time:55777ms step_avg:57.86ms
step:965/2330 train_time:55834ms step_avg:57.86ms
step:966/2330 train_time:55894ms step_avg:57.86ms
step:967/2330 train_time:55951ms step_avg:57.86ms
step:968/2330 train_time:56012ms step_avg:57.86ms
step:969/2330 train_time:56069ms step_avg:57.86ms
step:970/2330 train_time:56129ms step_avg:57.87ms
step:971/2330 train_time:56186ms step_avg:57.86ms
step:972/2330 train_time:56246ms step_avg:57.87ms
step:973/2330 train_time:56303ms step_avg:57.87ms
step:974/2330 train_time:56363ms step_avg:57.87ms
step:975/2330 train_time:56420ms step_avg:57.87ms
step:976/2330 train_time:56481ms step_avg:57.87ms
step:977/2330 train_time:56538ms step_avg:57.87ms
step:978/2330 train_time:56597ms step_avg:57.87ms
step:979/2330 train_time:56656ms step_avg:57.87ms
step:980/2330 train_time:56714ms step_avg:57.87ms
step:981/2330 train_time:56772ms step_avg:57.87ms
step:982/2330 train_time:56832ms step_avg:57.87ms
step:983/2330 train_time:56890ms step_avg:57.87ms
step:984/2330 train_time:56950ms step_avg:57.88ms
step:985/2330 train_time:57007ms step_avg:57.88ms
step:986/2330 train_time:57067ms step_avg:57.88ms
step:987/2330 train_time:57124ms step_avg:57.88ms
step:988/2330 train_time:57184ms step_avg:57.88ms
step:989/2330 train_time:57241ms step_avg:57.88ms
step:990/2330 train_time:57301ms step_avg:57.88ms
step:991/2330 train_time:57358ms step_avg:57.88ms
step:992/2330 train_time:57417ms step_avg:57.88ms
step:993/2330 train_time:57475ms step_avg:57.88ms
step:994/2330 train_time:57534ms step_avg:57.88ms
step:995/2330 train_time:57591ms step_avg:57.88ms
step:996/2330 train_time:57651ms step_avg:57.88ms
step:997/2330 train_time:57708ms step_avg:57.88ms
step:998/2330 train_time:57768ms step_avg:57.88ms
step:999/2330 train_time:57825ms step_avg:57.88ms
step:1000/2330 train_time:57885ms step_avg:57.89ms
step:1000/2330 val_loss:4.0818 train_time:57966ms step_avg:57.97ms
step:1001/2330 train_time:57986ms step_avg:57.93ms
step:1002/2330 train_time:58007ms step_avg:57.89ms
step:1003/2330 train_time:58061ms step_avg:57.89ms
step:1004/2330 train_time:58127ms step_avg:57.90ms
step:1005/2330 train_time:58183ms step_avg:57.89ms
step:1006/2330 train_time:58252ms step_avg:57.90ms
step:1007/2330 train_time:58308ms step_avg:57.90ms
step:1008/2330 train_time:58367ms step_avg:57.90ms
step:1009/2330 train_time:58423ms step_avg:57.90ms
step:1010/2330 train_time:58483ms step_avg:57.90ms
step:1011/2330 train_time:58539ms step_avg:57.90ms
step:1012/2330 train_time:58598ms step_avg:57.90ms
step:1013/2330 train_time:58654ms step_avg:57.90ms
step:1014/2330 train_time:58713ms step_avg:57.90ms
step:1015/2330 train_time:58768ms step_avg:57.90ms
step:1016/2330 train_time:58828ms step_avg:57.90ms
step:1017/2330 train_time:58885ms step_avg:57.90ms
step:1018/2330 train_time:58948ms step_avg:57.91ms
step:1019/2330 train_time:59005ms step_avg:57.90ms
step:1020/2330 train_time:59067ms step_avg:57.91ms
step:1021/2330 train_time:59124ms step_avg:57.91ms
step:1022/2330 train_time:59187ms step_avg:57.91ms
step:1023/2330 train_time:59244ms step_avg:57.91ms
step:1024/2330 train_time:59306ms step_avg:57.92ms
step:1025/2330 train_time:59362ms step_avg:57.91ms
step:1026/2330 train_time:59422ms step_avg:57.92ms
step:1027/2330 train_time:59478ms step_avg:57.91ms
step:1028/2330 train_time:59538ms step_avg:57.92ms
step:1029/2330 train_time:59595ms step_avg:57.91ms
step:1030/2330 train_time:59654ms step_avg:57.92ms
step:1031/2330 train_time:59710ms step_avg:57.91ms
step:1032/2330 train_time:59770ms step_avg:57.92ms
step:1033/2330 train_time:59827ms step_avg:57.92ms
step:1034/2330 train_time:59887ms step_avg:57.92ms
step:1035/2330 train_time:59944ms step_avg:57.92ms
step:1036/2330 train_time:60005ms step_avg:57.92ms
step:1037/2330 train_time:60062ms step_avg:57.92ms
step:1038/2330 train_time:60124ms step_avg:57.92ms
step:1039/2330 train_time:60182ms step_avg:57.92ms
step:1040/2330 train_time:60241ms step_avg:57.92ms
step:1041/2330 train_time:60298ms step_avg:57.92ms
step:1042/2330 train_time:60359ms step_avg:57.93ms
step:1043/2330 train_time:60415ms step_avg:57.92ms
step:1044/2330 train_time:60475ms step_avg:57.93ms
step:1045/2330 train_time:60531ms step_avg:57.92ms
step:1046/2330 train_time:60591ms step_avg:57.93ms
step:1047/2330 train_time:60647ms step_avg:57.92ms
step:1048/2330 train_time:60707ms step_avg:57.93ms
step:1049/2330 train_time:60763ms step_avg:57.93ms
step:1050/2330 train_time:60824ms step_avg:57.93ms
step:1051/2330 train_time:60881ms step_avg:57.93ms
step:1052/2330 train_time:60942ms step_avg:57.93ms
step:1053/2330 train_time:60998ms step_avg:57.93ms
step:1054/2330 train_time:61060ms step_avg:57.93ms
step:1055/2330 train_time:61117ms step_avg:57.93ms
step:1056/2330 train_time:61178ms step_avg:57.93ms
step:1057/2330 train_time:61235ms step_avg:57.93ms
step:1058/2330 train_time:61296ms step_avg:57.94ms
step:1059/2330 train_time:61352ms step_avg:57.93ms
step:1060/2330 train_time:61413ms step_avg:57.94ms
step:1061/2330 train_time:61469ms step_avg:57.94ms
step:1062/2330 train_time:61530ms step_avg:57.94ms
step:1063/2330 train_time:61587ms step_avg:57.94ms
step:1064/2330 train_time:61645ms step_avg:57.94ms
step:1065/2330 train_time:61701ms step_avg:57.94ms
step:1066/2330 train_time:61762ms step_avg:57.94ms
step:1067/2330 train_time:61819ms step_avg:57.94ms
step:1068/2330 train_time:61879ms step_avg:57.94ms
step:1069/2330 train_time:61936ms step_avg:57.94ms
step:1070/2330 train_time:61995ms step_avg:57.94ms
step:1071/2330 train_time:62052ms step_avg:57.94ms
step:1072/2330 train_time:62113ms step_avg:57.94ms
step:1073/2330 train_time:62169ms step_avg:57.94ms
step:1074/2330 train_time:62231ms step_avg:57.94ms
step:1075/2330 train_time:62288ms step_avg:57.94ms
step:1076/2330 train_time:62349ms step_avg:57.94ms
step:1077/2330 train_time:62405ms step_avg:57.94ms
step:1078/2330 train_time:62466ms step_avg:57.95ms
step:1079/2330 train_time:62523ms step_avg:57.94ms
step:1080/2330 train_time:62582ms step_avg:57.95ms
step:1081/2330 train_time:62639ms step_avg:57.95ms
step:1082/2330 train_time:62699ms step_avg:57.95ms
step:1083/2330 train_time:62756ms step_avg:57.95ms
step:1084/2330 train_time:62816ms step_avg:57.95ms
step:1085/2330 train_time:62873ms step_avg:57.95ms
step:1086/2330 train_time:62932ms step_avg:57.95ms
step:1087/2330 train_time:62989ms step_avg:57.95ms
step:1088/2330 train_time:63050ms step_avg:57.95ms
step:1089/2330 train_time:63106ms step_avg:57.95ms
step:1090/2330 train_time:63168ms step_avg:57.95ms
step:1091/2330 train_time:63224ms step_avg:57.95ms
step:1092/2330 train_time:63286ms step_avg:57.95ms
step:1093/2330 train_time:63343ms step_avg:57.95ms
step:1094/2330 train_time:63403ms step_avg:57.96ms
step:1095/2330 train_time:63460ms step_avg:57.95ms
step:1096/2330 train_time:63519ms step_avg:57.96ms
step:1097/2330 train_time:63577ms step_avg:57.95ms
step:1098/2330 train_time:63636ms step_avg:57.96ms
step:1099/2330 train_time:63693ms step_avg:57.96ms
step:1100/2330 train_time:63753ms step_avg:57.96ms
step:1101/2330 train_time:63810ms step_avg:57.96ms
step:1102/2330 train_time:63870ms step_avg:57.96ms
step:1103/2330 train_time:63927ms step_avg:57.96ms
step:1104/2330 train_time:63987ms step_avg:57.96ms
step:1105/2330 train_time:64043ms step_avg:57.96ms
step:1106/2330 train_time:64106ms step_avg:57.96ms
step:1107/2330 train_time:64163ms step_avg:57.96ms
step:1108/2330 train_time:64223ms step_avg:57.96ms
step:1109/2330 train_time:64280ms step_avg:57.96ms
step:1110/2330 train_time:64340ms step_avg:57.96ms
step:1111/2330 train_time:64397ms step_avg:57.96ms
step:1112/2330 train_time:64457ms step_avg:57.96ms
step:1113/2330 train_time:64513ms step_avg:57.96ms
step:1114/2330 train_time:64573ms step_avg:57.97ms
step:1115/2330 train_time:64630ms step_avg:57.96ms
step:1116/2330 train_time:64690ms step_avg:57.97ms
step:1117/2330 train_time:64747ms step_avg:57.96ms
step:1118/2330 train_time:64808ms step_avg:57.97ms
step:1119/2330 train_time:64865ms step_avg:57.97ms
step:1120/2330 train_time:64925ms step_avg:57.97ms
step:1121/2330 train_time:64982ms step_avg:57.97ms
step:1122/2330 train_time:65043ms step_avg:57.97ms
step:1123/2330 train_time:65099ms step_avg:57.97ms
step:1124/2330 train_time:65160ms step_avg:57.97ms
step:1125/2330 train_time:65217ms step_avg:57.97ms
step:1126/2330 train_time:65277ms step_avg:57.97ms
step:1127/2330 train_time:65335ms step_avg:57.97ms
step:1128/2330 train_time:65395ms step_avg:57.97ms
step:1129/2330 train_time:65451ms step_avg:57.97ms
step:1130/2330 train_time:65512ms step_avg:57.97ms
step:1131/2330 train_time:65568ms step_avg:57.97ms
step:1132/2330 train_time:65629ms step_avg:57.98ms
step:1133/2330 train_time:65685ms step_avg:57.97ms
step:1134/2330 train_time:65745ms step_avg:57.98ms
step:1135/2330 train_time:65802ms step_avg:57.98ms
step:1136/2330 train_time:65863ms step_avg:57.98ms
step:1137/2330 train_time:65919ms step_avg:57.98ms
step:1138/2330 train_time:65980ms step_avg:57.98ms
step:1139/2330 train_time:66036ms step_avg:57.98ms
step:1140/2330 train_time:66097ms step_avg:57.98ms
step:1141/2330 train_time:66155ms step_avg:57.98ms
step:1142/2330 train_time:66214ms step_avg:57.98ms
step:1143/2330 train_time:66271ms step_avg:57.98ms
step:1144/2330 train_time:66332ms step_avg:57.98ms
step:1145/2330 train_time:66389ms step_avg:57.98ms
step:1146/2330 train_time:66450ms step_avg:57.98ms
step:1147/2330 train_time:66506ms step_avg:57.98ms
step:1148/2330 train_time:66567ms step_avg:57.98ms
step:1149/2330 train_time:66623ms step_avg:57.98ms
step:1150/2330 train_time:66684ms step_avg:57.99ms
step:1151/2330 train_time:66740ms step_avg:57.98ms
step:1152/2330 train_time:66801ms step_avg:57.99ms
step:1153/2330 train_time:66858ms step_avg:57.99ms
step:1154/2330 train_time:66918ms step_avg:57.99ms
step:1155/2330 train_time:66975ms step_avg:57.99ms
step:1156/2330 train_time:67035ms step_avg:57.99ms
step:1157/2330 train_time:67092ms step_avg:57.99ms
step:1158/2330 train_time:67151ms step_avg:57.99ms
step:1159/2330 train_time:67208ms step_avg:57.99ms
step:1160/2330 train_time:67269ms step_avg:57.99ms
step:1161/2330 train_time:67326ms step_avg:57.99ms
step:1162/2330 train_time:67387ms step_avg:57.99ms
step:1163/2330 train_time:67443ms step_avg:57.99ms
step:1164/2330 train_time:67504ms step_avg:57.99ms
step:1165/2330 train_time:67561ms step_avg:57.99ms
step:1166/2330 train_time:67621ms step_avg:57.99ms
step:1167/2330 train_time:67678ms step_avg:57.99ms
step:1168/2330 train_time:67738ms step_avg:57.99ms
step:1169/2330 train_time:67795ms step_avg:57.99ms
step:1170/2330 train_time:67854ms step_avg:58.00ms
step:1171/2330 train_time:67911ms step_avg:57.99ms
step:1172/2330 train_time:67973ms step_avg:58.00ms
step:1173/2330 train_time:68030ms step_avg:58.00ms
step:1174/2330 train_time:68090ms step_avg:58.00ms
step:1175/2330 train_time:68146ms step_avg:58.00ms
step:1176/2330 train_time:68207ms step_avg:58.00ms
step:1177/2330 train_time:68264ms step_avg:58.00ms
step:1178/2330 train_time:68325ms step_avg:58.00ms
step:1179/2330 train_time:68382ms step_avg:58.00ms
step:1180/2330 train_time:68442ms step_avg:58.00ms
step:1181/2330 train_time:68499ms step_avg:58.00ms
step:1182/2330 train_time:68559ms step_avg:58.00ms
step:1183/2330 train_time:68615ms step_avg:58.00ms
step:1184/2330 train_time:68675ms step_avg:58.00ms
step:1185/2330 train_time:68733ms step_avg:58.00ms
step:1186/2330 train_time:68792ms step_avg:58.00ms
step:1187/2330 train_time:68849ms step_avg:58.00ms
step:1188/2330 train_time:68910ms step_avg:58.00ms
step:1189/2330 train_time:68966ms step_avg:58.00ms
step:1190/2330 train_time:69027ms step_avg:58.01ms
step:1191/2330 train_time:69083ms step_avg:58.00ms
step:1192/2330 train_time:69144ms step_avg:58.01ms
step:1193/2330 train_time:69201ms step_avg:58.01ms
step:1194/2330 train_time:69261ms step_avg:58.01ms
step:1195/2330 train_time:69318ms step_avg:58.01ms
step:1196/2330 train_time:69378ms step_avg:58.01ms
step:1197/2330 train_time:69436ms step_avg:58.01ms
step:1198/2330 train_time:69495ms step_avg:58.01ms
step:1199/2330 train_time:69551ms step_avg:58.01ms
step:1200/2330 train_time:69612ms step_avg:58.01ms
step:1201/2330 train_time:69668ms step_avg:58.01ms
step:1202/2330 train_time:69730ms step_avg:58.01ms
step:1203/2330 train_time:69787ms step_avg:58.01ms
step:1204/2330 train_time:69847ms step_avg:58.01ms
step:1205/2330 train_time:69904ms step_avg:58.01ms
step:1206/2330 train_time:69964ms step_avg:58.01ms
step:1207/2330 train_time:70021ms step_avg:58.01ms
step:1208/2330 train_time:70080ms step_avg:58.01ms
step:1209/2330 train_time:70137ms step_avg:58.01ms
step:1210/2330 train_time:70198ms step_avg:58.01ms
step:1211/2330 train_time:70254ms step_avg:58.01ms
step:1212/2330 train_time:70315ms step_avg:58.02ms
step:1213/2330 train_time:70372ms step_avg:58.01ms
step:1214/2330 train_time:70433ms step_avg:58.02ms
step:1215/2330 train_time:70489ms step_avg:58.02ms
step:1216/2330 train_time:70551ms step_avg:58.02ms
step:1217/2330 train_time:70607ms step_avg:58.02ms
step:1218/2330 train_time:70668ms step_avg:58.02ms
step:1219/2330 train_time:70725ms step_avg:58.02ms
step:1220/2330 train_time:70785ms step_avg:58.02ms
step:1221/2330 train_time:70841ms step_avg:58.02ms
step:1222/2330 train_time:70902ms step_avg:58.02ms
step:1223/2330 train_time:70959ms step_avg:58.02ms
step:1224/2330 train_time:71019ms step_avg:58.02ms
step:1225/2330 train_time:71075ms step_avg:58.02ms
step:1226/2330 train_time:71135ms step_avg:58.02ms
step:1227/2330 train_time:71192ms step_avg:58.02ms
step:1228/2330 train_time:71253ms step_avg:58.02ms
step:1229/2330 train_time:71309ms step_avg:58.02ms
step:1230/2330 train_time:71370ms step_avg:58.02ms
step:1231/2330 train_time:71426ms step_avg:58.02ms
step:1232/2330 train_time:71487ms step_avg:58.03ms
step:1233/2330 train_time:71543ms step_avg:58.02ms
step:1234/2330 train_time:71603ms step_avg:58.03ms
step:1235/2330 train_time:71660ms step_avg:58.02ms
step:1236/2330 train_time:71721ms step_avg:58.03ms
step:1237/2330 train_time:71777ms step_avg:58.02ms
step:1238/2330 train_time:71837ms step_avg:58.03ms
step:1239/2330 train_time:71894ms step_avg:58.03ms
step:1240/2330 train_time:71953ms step_avg:58.03ms
step:1241/2330 train_time:72010ms step_avg:58.03ms
step:1242/2330 train_time:72071ms step_avg:58.03ms
step:1243/2330 train_time:72128ms step_avg:58.03ms
step:1244/2330 train_time:72188ms step_avg:58.03ms
step:1245/2330 train_time:72244ms step_avg:58.03ms
step:1246/2330 train_time:72305ms step_avg:58.03ms
step:1247/2330 train_time:72363ms step_avg:58.03ms
step:1248/2330 train_time:72423ms step_avg:58.03ms
step:1249/2330 train_time:72480ms step_avg:58.03ms
step:1250/2330 train_time:72539ms step_avg:58.03ms
step:1250/2330 val_loss:4.0091 train_time:72621ms step_avg:58.10ms
step:1251/2330 train_time:72641ms step_avg:58.07ms
step:1252/2330 train_time:72662ms step_avg:58.04ms
step:1253/2330 train_time:72718ms step_avg:58.04ms
step:1254/2330 train_time:72781ms step_avg:58.04ms
step:1255/2330 train_time:72838ms step_avg:58.04ms
step:1256/2330 train_time:72899ms step_avg:58.04ms
step:1257/2330 train_time:72956ms step_avg:58.04ms
step:1258/2330 train_time:73017ms step_avg:58.04ms
step:1259/2330 train_time:73072ms step_avg:58.04ms
step:1260/2330 train_time:73133ms step_avg:58.04ms
step:1261/2330 train_time:73189ms step_avg:58.04ms
step:1262/2330 train_time:73250ms step_avg:58.04ms
step:1263/2330 train_time:73306ms step_avg:58.04ms
step:1264/2330 train_time:73365ms step_avg:58.04ms
step:1265/2330 train_time:73421ms step_avg:58.04ms
step:1266/2330 train_time:73480ms step_avg:58.04ms
step:1267/2330 train_time:73536ms step_avg:58.04ms
step:1268/2330 train_time:73597ms step_avg:58.04ms
step:1269/2330 train_time:73658ms step_avg:58.04ms
step:1270/2330 train_time:73716ms step_avg:58.04ms
step:1271/2330 train_time:73773ms step_avg:58.04ms
step:1272/2330 train_time:73836ms step_avg:58.05ms
step:1273/2330 train_time:73893ms step_avg:58.05ms
step:1274/2330 train_time:73954ms step_avg:58.05ms
step:1275/2330 train_time:74010ms step_avg:58.05ms
step:1276/2330 train_time:74071ms step_avg:58.05ms
step:1277/2330 train_time:74127ms step_avg:58.05ms
step:1278/2330 train_time:74187ms step_avg:58.05ms
step:1279/2330 train_time:74243ms step_avg:58.05ms
step:1280/2330 train_time:74304ms step_avg:58.05ms
step:1281/2330 train_time:74360ms step_avg:58.05ms
step:1282/2330 train_time:74420ms step_avg:58.05ms
step:1283/2330 train_time:74476ms step_avg:58.05ms
step:1284/2330 train_time:74536ms step_avg:58.05ms
step:1285/2330 train_time:74593ms step_avg:58.05ms
step:1286/2330 train_time:74654ms step_avg:58.05ms
step:1287/2330 train_time:74710ms step_avg:58.05ms
step:1288/2330 train_time:74773ms step_avg:58.05ms
step:1289/2330 train_time:74830ms step_avg:58.05ms
step:1290/2330 train_time:74891ms step_avg:58.06ms
step:1291/2330 train_time:74948ms step_avg:58.05ms
step:1292/2330 train_time:75008ms step_avg:58.06ms
step:1293/2330 train_time:75064ms step_avg:58.05ms
step:1294/2330 train_time:75124ms step_avg:58.06ms
step:1295/2330 train_time:75181ms step_avg:58.06ms
step:1296/2330 train_time:75241ms step_avg:58.06ms
step:1297/2330 train_time:75297ms step_avg:58.05ms
step:1298/2330 train_time:75357ms step_avg:58.06ms
step:1299/2330 train_time:75414ms step_avg:58.06ms
step:1300/2330 train_time:75474ms step_avg:58.06ms
step:1301/2330 train_time:75530ms step_avg:58.06ms
step:1302/2330 train_time:75591ms step_avg:58.06ms
step:1303/2330 train_time:75648ms step_avg:58.06ms
step:1304/2330 train_time:75708ms step_avg:58.06ms
step:1305/2330 train_time:75766ms step_avg:58.06ms
step:1306/2330 train_time:75826ms step_avg:58.06ms
step:1307/2330 train_time:75883ms step_avg:58.06ms
step:1308/2330 train_time:75944ms step_avg:58.06ms
step:1309/2330 train_time:76001ms step_avg:58.06ms
step:1310/2330 train_time:76062ms step_avg:58.06ms
step:1311/2330 train_time:76119ms step_avg:58.06ms
step:1312/2330 train_time:76179ms step_avg:58.06ms
step:1313/2330 train_time:76235ms step_avg:58.06ms
step:1314/2330 train_time:76296ms step_avg:58.06ms
step:1315/2330 train_time:76353ms step_avg:58.06ms
step:1316/2330 train_time:76412ms step_avg:58.06ms
step:1317/2330 train_time:76468ms step_avg:58.06ms
step:1318/2330 train_time:76529ms step_avg:58.06ms
step:1319/2330 train_time:76586ms step_avg:58.06ms
step:1320/2330 train_time:76647ms step_avg:58.07ms
step:1321/2330 train_time:76704ms step_avg:58.07ms
step:1322/2330 train_time:76764ms step_avg:58.07ms
step:1323/2330 train_time:76821ms step_avg:58.07ms
step:1324/2330 train_time:76882ms step_avg:58.07ms
step:1325/2330 train_time:76939ms step_avg:58.07ms
step:1326/2330 train_time:77000ms step_avg:58.07ms
step:1327/2330 train_time:77057ms step_avg:58.07ms
step:1328/2330 train_time:77118ms step_avg:58.07ms
step:1329/2330 train_time:77174ms step_avg:58.07ms
step:1330/2330 train_time:77235ms step_avg:58.07ms
step:1331/2330 train_time:77291ms step_avg:58.07ms
step:1332/2330 train_time:77352ms step_avg:58.07ms
step:1333/2330 train_time:77407ms step_avg:58.07ms
step:1334/2330 train_time:77468ms step_avg:58.07ms
step:1335/2330 train_time:77525ms step_avg:58.07ms
step:1336/2330 train_time:77585ms step_avg:58.07ms
step:1337/2330 train_time:77643ms step_avg:58.07ms
step:1338/2330 train_time:77702ms step_avg:58.07ms
step:1339/2330 train_time:77759ms step_avg:58.07ms
step:1340/2330 train_time:77819ms step_avg:58.07ms
step:1341/2330 train_time:77875ms step_avg:58.07ms
step:1342/2330 train_time:77937ms step_avg:58.08ms
step:1343/2330 train_time:77994ms step_avg:58.07ms
step:1344/2330 train_time:78054ms step_avg:58.08ms
step:1345/2330 train_time:78110ms step_avg:58.07ms
step:1346/2330 train_time:78171ms step_avg:58.08ms
step:1347/2330 train_time:78228ms step_avg:58.08ms
step:1348/2330 train_time:78288ms step_avg:58.08ms
step:1349/2330 train_time:78344ms step_avg:58.08ms
step:1350/2330 train_time:78404ms step_avg:58.08ms
step:1351/2330 train_time:78461ms step_avg:58.08ms
step:1352/2330 train_time:78521ms step_avg:58.08ms
step:1353/2330 train_time:78578ms step_avg:58.08ms
step:1354/2330 train_time:78639ms step_avg:58.08ms
step:1355/2330 train_time:78695ms step_avg:58.08ms
step:1356/2330 train_time:78755ms step_avg:58.08ms
step:1357/2330 train_time:78813ms step_avg:58.08ms
step:1358/2330 train_time:78872ms step_avg:58.08ms
step:1359/2330 train_time:78929ms step_avg:58.08ms
step:1360/2330 train_time:78990ms step_avg:58.08ms
step:1361/2330 train_time:79047ms step_avg:58.08ms
step:1362/2330 train_time:79107ms step_avg:58.08ms
step:1363/2330 train_time:79164ms step_avg:58.08ms
step:1364/2330 train_time:79224ms step_avg:58.08ms
step:1365/2330 train_time:79282ms step_avg:58.08ms
step:1366/2330 train_time:79341ms step_avg:58.08ms
step:1367/2330 train_time:79397ms step_avg:58.08ms
step:1368/2330 train_time:79458ms step_avg:58.08ms
step:1369/2330 train_time:79515ms step_avg:58.08ms
step:1370/2330 train_time:79575ms step_avg:58.08ms
step:1371/2330 train_time:79632ms step_avg:58.08ms
step:1372/2330 train_time:79693ms step_avg:58.09ms
step:1373/2330 train_time:79749ms step_avg:58.08ms
step:1374/2330 train_time:79810ms step_avg:58.09ms
step:1375/2330 train_time:79867ms step_avg:58.08ms
step:1376/2330 train_time:79927ms step_avg:58.09ms
step:1377/2330 train_time:79985ms step_avg:58.09ms
step:1378/2330 train_time:80044ms step_avg:58.09ms
step:1379/2330 train_time:80102ms step_avg:58.09ms
step:1380/2330 train_time:80162ms step_avg:58.09ms
step:1381/2330 train_time:80218ms step_avg:58.09ms
step:1382/2330 train_time:80278ms step_avg:58.09ms
step:1383/2330 train_time:80336ms step_avg:58.09ms
step:1384/2330 train_time:80395ms step_avg:58.09ms
step:1385/2330 train_time:80451ms step_avg:58.09ms
step:1386/2330 train_time:80512ms step_avg:58.09ms
step:1387/2330 train_time:80569ms step_avg:58.09ms
step:1388/2330 train_time:80629ms step_avg:58.09ms
step:1389/2330 train_time:80686ms step_avg:58.09ms
step:1390/2330 train_time:80746ms step_avg:58.09ms
step:1391/2330 train_time:80803ms step_avg:58.09ms
step:1392/2330 train_time:80864ms step_avg:58.09ms
step:1393/2330 train_time:80921ms step_avg:58.09ms
step:1394/2330 train_time:80980ms step_avg:58.09ms
step:1395/2330 train_time:81037ms step_avg:58.09ms
step:1396/2330 train_time:81097ms step_avg:58.09ms
step:1397/2330 train_time:81154ms step_avg:58.09ms
step:1398/2330 train_time:81214ms step_avg:58.09ms
step:1399/2330 train_time:81271ms step_avg:58.09ms
step:1400/2330 train_time:81331ms step_avg:58.09ms
step:1401/2330 train_time:81388ms step_avg:58.09ms
step:1402/2330 train_time:81448ms step_avg:58.09ms
step:1403/2330 train_time:81505ms step_avg:58.09ms
step:1404/2330 train_time:81565ms step_avg:58.09ms
step:1405/2330 train_time:81622ms step_avg:58.09ms
step:1406/2330 train_time:81682ms step_avg:58.09ms
step:1407/2330 train_time:81738ms step_avg:58.09ms
step:1408/2330 train_time:81798ms step_avg:58.10ms
step:1409/2330 train_time:81855ms step_avg:58.09ms
step:1410/2330 train_time:81915ms step_avg:58.10ms
step:1411/2330 train_time:81972ms step_avg:58.10ms
step:1412/2330 train_time:82032ms step_avg:58.10ms
step:1413/2330 train_time:82089ms step_avg:58.10ms
step:1414/2330 train_time:82150ms step_avg:58.10ms
step:1415/2330 train_time:82206ms step_avg:58.10ms
step:1416/2330 train_time:82266ms step_avg:58.10ms
step:1417/2330 train_time:82323ms step_avg:58.10ms
step:1418/2330 train_time:82383ms step_avg:58.10ms
step:1419/2330 train_time:82440ms step_avg:58.10ms
step:1420/2330 train_time:82500ms step_avg:58.10ms
step:1421/2330 train_time:82557ms step_avg:58.10ms
step:1422/2330 train_time:82617ms step_avg:58.10ms
step:1423/2330 train_time:82673ms step_avg:58.10ms
step:1424/2330 train_time:82733ms step_avg:58.10ms
step:1425/2330 train_time:82790ms step_avg:58.10ms
step:1426/2330 train_time:82851ms step_avg:58.10ms
step:1427/2330 train_time:82908ms step_avg:58.10ms
step:1428/2330 train_time:82968ms step_avg:58.10ms
step:1429/2330 train_time:83025ms step_avg:58.10ms
step:1430/2330 train_time:83085ms step_avg:58.10ms
step:1431/2330 train_time:83143ms step_avg:58.10ms
step:1432/2330 train_time:83203ms step_avg:58.10ms
step:1433/2330 train_time:83260ms step_avg:58.10ms
step:1434/2330 train_time:83319ms step_avg:58.10ms
step:1435/2330 train_time:83375ms step_avg:58.10ms
step:1436/2330 train_time:83436ms step_avg:58.10ms
step:1437/2330 train_time:83493ms step_avg:58.10ms
step:1438/2330 train_time:83553ms step_avg:58.10ms
step:1439/2330 train_time:83610ms step_avg:58.10ms
step:1440/2330 train_time:83670ms step_avg:58.10ms
step:1441/2330 train_time:83728ms step_avg:58.10ms
step:1442/2330 train_time:83788ms step_avg:58.11ms
step:1443/2330 train_time:83845ms step_avg:58.10ms
step:1444/2330 train_time:83905ms step_avg:58.11ms
step:1445/2330 train_time:83962ms step_avg:58.11ms
step:1446/2330 train_time:84022ms step_avg:58.11ms
step:1447/2330 train_time:84078ms step_avg:58.11ms
step:1448/2330 train_time:84139ms step_avg:58.11ms
step:1449/2330 train_time:84195ms step_avg:58.11ms
step:1450/2330 train_time:84257ms step_avg:58.11ms
step:1451/2330 train_time:84314ms step_avg:58.11ms
step:1452/2330 train_time:84373ms step_avg:58.11ms
step:1453/2330 train_time:84430ms step_avg:58.11ms
step:1454/2330 train_time:84490ms step_avg:58.11ms
step:1455/2330 train_time:84548ms step_avg:58.11ms
step:1456/2330 train_time:84607ms step_avg:58.11ms
step:1457/2330 train_time:84664ms step_avg:58.11ms
step:1458/2330 train_time:84724ms step_avg:58.11ms
step:1459/2330 train_time:84781ms step_avg:58.11ms
step:1460/2330 train_time:84842ms step_avg:58.11ms
step:1461/2330 train_time:84900ms step_avg:58.11ms
step:1462/2330 train_time:84959ms step_avg:58.11ms
step:1463/2330 train_time:85015ms step_avg:58.11ms
step:1464/2330 train_time:85076ms step_avg:58.11ms
step:1465/2330 train_time:85133ms step_avg:58.11ms
step:1466/2330 train_time:85194ms step_avg:58.11ms
step:1467/2330 train_time:85250ms step_avg:58.11ms
step:1468/2330 train_time:85311ms step_avg:58.11ms
step:1469/2330 train_time:85368ms step_avg:58.11ms
step:1470/2330 train_time:85428ms step_avg:58.11ms
step:1471/2330 train_time:85485ms step_avg:58.11ms
step:1472/2330 train_time:85545ms step_avg:58.11ms
step:1473/2330 train_time:85602ms step_avg:58.11ms
step:1474/2330 train_time:85662ms step_avg:58.12ms
step:1475/2330 train_time:85719ms step_avg:58.11ms
step:1476/2330 train_time:85779ms step_avg:58.12ms
step:1477/2330 train_time:85836ms step_avg:58.12ms
step:1478/2330 train_time:85896ms step_avg:58.12ms
step:1479/2330 train_time:85953ms step_avg:58.12ms
step:1480/2330 train_time:86014ms step_avg:58.12ms
step:1481/2330 train_time:86070ms step_avg:58.12ms
step:1482/2330 train_time:86130ms step_avg:58.12ms
step:1483/2330 train_time:86187ms step_avg:58.12ms
step:1484/2330 train_time:86248ms step_avg:58.12ms
step:1485/2330 train_time:86304ms step_avg:58.12ms
step:1486/2330 train_time:86364ms step_avg:58.12ms
step:1487/2330 train_time:86421ms step_avg:58.12ms
step:1488/2330 train_time:86481ms step_avg:58.12ms
step:1489/2330 train_time:86538ms step_avg:58.12ms
step:1490/2330 train_time:86598ms step_avg:58.12ms
step:1491/2330 train_time:86654ms step_avg:58.12ms
step:1492/2330 train_time:86716ms step_avg:58.12ms
step:1493/2330 train_time:86772ms step_avg:58.12ms
step:1494/2330 train_time:86833ms step_avg:58.12ms
step:1495/2330 train_time:86890ms step_avg:58.12ms
step:1496/2330 train_time:86950ms step_avg:58.12ms
step:1497/2330 train_time:87006ms step_avg:58.12ms
step:1498/2330 train_time:87067ms step_avg:58.12ms
step:1499/2330 train_time:87124ms step_avg:58.12ms
step:1500/2330 train_time:87185ms step_avg:58.12ms
step:1500/2330 val_loss:3.9218 train_time:87265ms step_avg:58.18ms
step:1501/2330 train_time:87286ms step_avg:58.15ms
step:1502/2330 train_time:87307ms step_avg:58.13ms
step:1503/2330 train_time:87363ms step_avg:58.13ms
step:1504/2330 train_time:87428ms step_avg:58.13ms
step:1505/2330 train_time:87485ms step_avg:58.13ms
step:1506/2330 train_time:87550ms step_avg:58.13ms
step:1507/2330 train_time:87605ms step_avg:58.13ms
step:1508/2330 train_time:87667ms step_avg:58.13ms
step:1509/2330 train_time:87722ms step_avg:58.13ms
step:1510/2330 train_time:87783ms step_avg:58.13ms
step:1511/2330 train_time:87839ms step_avg:58.13ms
step:1512/2330 train_time:87900ms step_avg:58.13ms
step:1513/2330 train_time:87956ms step_avg:58.13ms
step:1514/2330 train_time:88015ms step_avg:58.13ms
step:1515/2330 train_time:88071ms step_avg:58.13ms
step:1516/2330 train_time:88130ms step_avg:58.13ms
step:1517/2330 train_time:88187ms step_avg:58.13ms
step:1518/2330 train_time:88247ms step_avg:58.13ms
step:1519/2330 train_time:88305ms step_avg:58.13ms
step:1520/2330 train_time:88366ms step_avg:58.14ms
step:1521/2330 train_time:88423ms step_avg:58.13ms
step:1522/2330 train_time:88485ms step_avg:58.14ms
step:1523/2330 train_time:88542ms step_avg:58.14ms
step:1524/2330 train_time:88603ms step_avg:58.14ms
step:1525/2330 train_time:88660ms step_avg:58.14ms
step:1526/2330 train_time:88719ms step_avg:58.14ms
step:1527/2330 train_time:88776ms step_avg:58.14ms
step:1528/2330 train_time:88836ms step_avg:58.14ms
step:1529/2330 train_time:88894ms step_avg:58.14ms
step:1530/2330 train_time:88952ms step_avg:58.14ms
step:1531/2330 train_time:89009ms step_avg:58.14ms
step:1532/2330 train_time:89069ms step_avg:58.14ms
step:1533/2330 train_time:89126ms step_avg:58.14ms
step:1534/2330 train_time:89186ms step_avg:58.14ms
step:1535/2330 train_time:89243ms step_avg:58.14ms
step:1536/2330 train_time:89305ms step_avg:58.14ms
step:1537/2330 train_time:89363ms step_avg:58.14ms
step:1538/2330 train_time:89424ms step_avg:58.14ms
step:1539/2330 train_time:89482ms step_avg:58.14ms
step:1540/2330 train_time:89544ms step_avg:58.15ms
step:1541/2330 train_time:89601ms step_avg:58.14ms
step:1542/2330 train_time:89662ms step_avg:58.15ms
step:1543/2330 train_time:89719ms step_avg:58.15ms
step:1544/2330 train_time:89781ms step_avg:58.15ms
step:1545/2330 train_time:89839ms step_avg:58.15ms
step:1546/2330 train_time:89898ms step_avg:58.15ms
step:1547/2330 train_time:89955ms step_avg:58.15ms
step:1548/2330 train_time:90015ms step_avg:58.15ms
step:1549/2330 train_time:90072ms step_avg:58.15ms
step:1550/2330 train_time:90132ms step_avg:58.15ms
step:1551/2330 train_time:90189ms step_avg:58.15ms
step:1552/2330 train_time:90251ms step_avg:58.15ms
step:1553/2330 train_time:90308ms step_avg:58.15ms
step:1554/2330 train_time:90370ms step_avg:58.15ms
step:1555/2330 train_time:90427ms step_avg:58.15ms
step:1556/2330 train_time:90490ms step_avg:58.16ms
step:1557/2330 train_time:90547ms step_avg:58.16ms
step:1558/2330 train_time:90609ms step_avg:58.16ms
step:1559/2330 train_time:90666ms step_avg:58.16ms
step:1560/2330 train_time:90728ms step_avg:58.16ms
step:1561/2330 train_time:90785ms step_avg:58.16ms
step:1562/2330 train_time:90847ms step_avg:58.16ms
step:1563/2330 train_time:90904ms step_avg:58.16ms
step:1564/2330 train_time:90965ms step_avg:58.16ms
step:1565/2330 train_time:91022ms step_avg:58.16ms
step:1566/2330 train_time:91084ms step_avg:58.16ms
step:1567/2330 train_time:91141ms step_avg:58.16ms
step:1568/2330 train_time:91202ms step_avg:58.16ms
step:1569/2330 train_time:91259ms step_avg:58.16ms
step:1570/2330 train_time:91320ms step_avg:58.17ms
step:1571/2330 train_time:91378ms step_avg:58.17ms
step:1572/2330 train_time:91439ms step_avg:58.17ms
step:1573/2330 train_time:91497ms step_avg:58.17ms
step:1574/2330 train_time:91558ms step_avg:58.17ms
step:1575/2330 train_time:91615ms step_avg:58.17ms
step:1576/2330 train_time:91676ms step_avg:58.17ms
step:1577/2330 train_time:91734ms step_avg:58.17ms
step:1578/2330 train_time:91794ms step_avg:58.17ms
step:1579/2330 train_time:91852ms step_avg:58.17ms
step:1580/2330 train_time:91913ms step_avg:58.17ms
step:1581/2330 train_time:91969ms step_avg:58.17ms
step:1582/2330 train_time:92031ms step_avg:58.17ms
step:1583/2330 train_time:92088ms step_avg:58.17ms
step:1584/2330 train_time:92149ms step_avg:58.18ms
step:1585/2330 train_time:92206ms step_avg:58.17ms
step:1586/2330 train_time:92267ms step_avg:58.18ms
step:1587/2330 train_time:92323ms step_avg:58.17ms
step:1588/2330 train_time:92385ms step_avg:58.18ms
step:1589/2330 train_time:92442ms step_avg:58.18ms
step:1590/2330 train_time:92506ms step_avg:58.18ms
step:1591/2330 train_time:92563ms step_avg:58.18ms
step:1592/2330 train_time:92625ms step_avg:58.18ms
step:1593/2330 train_time:92682ms step_avg:58.18ms
step:1594/2330 train_time:92745ms step_avg:58.18ms
step:1595/2330 train_time:92802ms step_avg:58.18ms
step:1596/2330 train_time:92863ms step_avg:58.19ms
step:1597/2330 train_time:92920ms step_avg:58.18ms
step:1598/2330 train_time:92981ms step_avg:58.19ms
step:1599/2330 train_time:93039ms step_avg:58.19ms
step:1600/2330 train_time:93099ms step_avg:58.19ms
step:1601/2330 train_time:93156ms step_avg:58.19ms
step:1602/2330 train_time:93216ms step_avg:58.19ms
step:1603/2330 train_time:93274ms step_avg:58.19ms
step:1604/2330 train_time:93335ms step_avg:58.19ms
step:1605/2330 train_time:93394ms step_avg:58.19ms
step:1606/2330 train_time:93454ms step_avg:58.19ms
step:1607/2330 train_time:93513ms step_avg:58.19ms
step:1608/2330 train_time:93574ms step_avg:58.19ms
step:1609/2330 train_time:93631ms step_avg:58.19ms
step:1610/2330 train_time:93693ms step_avg:58.19ms
step:1611/2330 train_time:93751ms step_avg:58.19ms
step:1612/2330 train_time:93811ms step_avg:58.20ms
step:1613/2330 train_time:93867ms step_avg:58.19ms
step:1614/2330 train_time:93930ms step_avg:58.20ms
step:1615/2330 train_time:93987ms step_avg:58.20ms
step:1616/2330 train_time:94048ms step_avg:58.20ms
step:1617/2330 train_time:94104ms step_avg:58.20ms
step:1618/2330 train_time:94167ms step_avg:58.20ms
step:1619/2330 train_time:94224ms step_avg:58.20ms
step:1620/2330 train_time:94285ms step_avg:58.20ms
step:1621/2330 train_time:94341ms step_avg:58.20ms
step:1622/2330 train_time:94403ms step_avg:58.20ms
step:1623/2330 train_time:94460ms step_avg:58.20ms
step:1624/2330 train_time:94522ms step_avg:58.20ms
step:1625/2330 train_time:94579ms step_avg:58.20ms
step:1626/2330 train_time:94641ms step_avg:58.20ms
step:1627/2330 train_time:94698ms step_avg:58.20ms
step:1628/2330 train_time:94759ms step_avg:58.21ms
step:1629/2330 train_time:94816ms step_avg:58.21ms
step:1630/2330 train_time:94876ms step_avg:58.21ms
step:1631/2330 train_time:94935ms step_avg:58.21ms
step:1632/2330 train_time:94996ms step_avg:58.21ms
step:1633/2330 train_time:95053ms step_avg:58.21ms
step:1634/2330 train_time:95113ms step_avg:58.21ms
step:1635/2330 train_time:95171ms step_avg:58.21ms
step:1636/2330 train_time:95231ms step_avg:58.21ms
step:1637/2330 train_time:95288ms step_avg:58.21ms
step:1638/2330 train_time:95349ms step_avg:58.21ms
step:1639/2330 train_time:95406ms step_avg:58.21ms
step:1640/2330 train_time:95468ms step_avg:58.21ms
step:1641/2330 train_time:95524ms step_avg:58.21ms
step:1642/2330 train_time:95586ms step_avg:58.21ms
step:1643/2330 train_time:95643ms step_avg:58.21ms
step:1644/2330 train_time:95706ms step_avg:58.22ms
step:1645/2330 train_time:95762ms step_avg:58.21ms
step:1646/2330 train_time:95824ms step_avg:58.22ms
step:1647/2330 train_time:95881ms step_avg:58.22ms
step:1648/2330 train_time:95942ms step_avg:58.22ms
step:1649/2330 train_time:96000ms step_avg:58.22ms
step:1650/2330 train_time:96061ms step_avg:58.22ms
step:1651/2330 train_time:96118ms step_avg:58.22ms
step:1652/2330 train_time:96178ms step_avg:58.22ms
step:1653/2330 train_time:96237ms step_avg:58.22ms
step:1654/2330 train_time:96297ms step_avg:58.22ms
step:1655/2330 train_time:96355ms step_avg:58.22ms
step:1656/2330 train_time:96416ms step_avg:58.22ms
step:1657/2330 train_time:96474ms step_avg:58.22ms
step:1658/2330 train_time:96534ms step_avg:58.22ms
step:1659/2330 train_time:96593ms step_avg:58.22ms
step:1660/2330 train_time:96653ms step_avg:58.22ms
step:1661/2330 train_time:96710ms step_avg:58.22ms
step:1662/2330 train_time:96772ms step_avg:58.23ms
step:1663/2330 train_time:96828ms step_avg:58.22ms
step:1664/2330 train_time:96891ms step_avg:58.23ms
step:1665/2330 train_time:96948ms step_avg:58.23ms
step:1666/2330 train_time:97009ms step_avg:58.23ms
step:1667/2330 train_time:97066ms step_avg:58.23ms
step:1668/2330 train_time:97126ms step_avg:58.23ms
step:1669/2330 train_time:97183ms step_avg:58.23ms
step:1670/2330 train_time:97245ms step_avg:58.23ms
step:1671/2330 train_time:97301ms step_avg:58.23ms
step:1672/2330 train_time:97363ms step_avg:58.23ms
step:1673/2330 train_time:97420ms step_avg:58.23ms
step:1674/2330 train_time:97482ms step_avg:58.23ms
step:1675/2330 train_time:97539ms step_avg:58.23ms
step:1676/2330 train_time:97600ms step_avg:58.23ms
step:1677/2330 train_time:97658ms step_avg:58.23ms
step:1678/2330 train_time:97718ms step_avg:58.23ms
step:1679/2330 train_time:97776ms step_avg:58.23ms
step:1680/2330 train_time:97837ms step_avg:58.24ms
step:1681/2330 train_time:97895ms step_avg:58.24ms
step:1682/2330 train_time:97955ms step_avg:58.24ms
step:1683/2330 train_time:98012ms step_avg:58.24ms
step:1684/2330 train_time:98073ms step_avg:58.24ms
step:1685/2330 train_time:98130ms step_avg:58.24ms
step:1686/2330 train_time:98191ms step_avg:58.24ms
step:1687/2330 train_time:98248ms step_avg:58.24ms
step:1688/2330 train_time:98309ms step_avg:58.24ms
step:1689/2330 train_time:98366ms step_avg:58.24ms
step:1690/2330 train_time:98428ms step_avg:58.24ms
step:1691/2330 train_time:98485ms step_avg:58.24ms
step:1692/2330 train_time:98547ms step_avg:58.24ms
step:1693/2330 train_time:98603ms step_avg:58.24ms
step:1694/2330 train_time:98666ms step_avg:58.24ms
step:1695/2330 train_time:98722ms step_avg:58.24ms
step:1696/2330 train_time:98785ms step_avg:58.25ms
step:1697/2330 train_time:98842ms step_avg:58.24ms
step:1698/2330 train_time:98904ms step_avg:58.25ms
step:1699/2330 train_time:98960ms step_avg:58.25ms
step:1700/2330 train_time:99022ms step_avg:58.25ms
step:1701/2330 train_time:99079ms step_avg:58.25ms
step:1702/2330 train_time:99141ms step_avg:58.25ms
step:1703/2330 train_time:99199ms step_avg:58.25ms
step:1704/2330 train_time:99259ms step_avg:58.25ms
step:1705/2330 train_time:99316ms step_avg:58.25ms
step:1706/2330 train_time:99377ms step_avg:58.25ms
step:1707/2330 train_time:99435ms step_avg:58.25ms
step:1708/2330 train_time:99496ms step_avg:58.25ms
step:1709/2330 train_time:99555ms step_avg:58.25ms
step:1710/2330 train_time:99615ms step_avg:58.25ms
step:1711/2330 train_time:99673ms step_avg:58.25ms
step:1712/2330 train_time:99734ms step_avg:58.26ms
step:1713/2330 train_time:99792ms step_avg:58.26ms
step:1714/2330 train_time:99852ms step_avg:58.26ms
step:1715/2330 train_time:99909ms step_avg:58.26ms
step:1716/2330 train_time:99971ms step_avg:58.26ms
step:1717/2330 train_time:100028ms step_avg:58.26ms
step:1718/2330 train_time:100088ms step_avg:58.26ms
step:1719/2330 train_time:100145ms step_avg:58.26ms
step:1720/2330 train_time:100206ms step_avg:58.26ms
step:1721/2330 train_time:100263ms step_avg:58.26ms
step:1722/2330 train_time:100326ms step_avg:58.26ms
step:1723/2330 train_time:100382ms step_avg:58.26ms
step:1724/2330 train_time:100443ms step_avg:58.26ms
step:1725/2330 train_time:100499ms step_avg:58.26ms
step:1726/2330 train_time:100562ms step_avg:58.26ms
step:1727/2330 train_time:100620ms step_avg:58.26ms
step:1728/2330 train_time:100681ms step_avg:58.26ms
step:1729/2330 train_time:100739ms step_avg:58.26ms
step:1730/2330 train_time:100800ms step_avg:58.27ms
step:1731/2330 train_time:100858ms step_avg:58.27ms
step:1732/2330 train_time:100918ms step_avg:58.27ms
step:1733/2330 train_time:100976ms step_avg:58.27ms
step:1734/2330 train_time:101036ms step_avg:58.27ms
step:1735/2330 train_time:101094ms step_avg:58.27ms
step:1736/2330 train_time:101155ms step_avg:58.27ms
step:1737/2330 train_time:101212ms step_avg:58.27ms
step:1738/2330 train_time:101273ms step_avg:58.27ms
step:1739/2330 train_time:101330ms step_avg:58.27ms
step:1740/2330 train_time:101391ms step_avg:58.27ms
step:1741/2330 train_time:101447ms step_avg:58.27ms
step:1742/2330 train_time:101510ms step_avg:58.27ms
step:1743/2330 train_time:101566ms step_avg:58.27ms
step:1744/2330 train_time:101628ms step_avg:58.27ms
step:1745/2330 train_time:101685ms step_avg:58.27ms
step:1746/2330 train_time:101746ms step_avg:58.27ms
step:1747/2330 train_time:101803ms step_avg:58.27ms
step:1748/2330 train_time:101865ms step_avg:58.28ms
step:1749/2330 train_time:101922ms step_avg:58.27ms
step:1750/2330 train_time:101983ms step_avg:58.28ms
step:1750/2330 val_loss:3.8339 train_time:102065ms step_avg:58.32ms
step:1751/2330 train_time:102085ms step_avg:58.30ms
step:1752/2330 train_time:102106ms step_avg:58.28ms
step:1753/2330 train_time:102158ms step_avg:58.28ms
step:1754/2330 train_time:102228ms step_avg:58.28ms
step:1755/2330 train_time:102285ms step_avg:58.28ms
step:1756/2330 train_time:102347ms step_avg:58.28ms
step:1757/2330 train_time:102403ms step_avg:58.28ms
step:1758/2330 train_time:102464ms step_avg:58.28ms
step:1759/2330 train_time:102520ms step_avg:58.28ms
step:1760/2330 train_time:102580ms step_avg:58.28ms
step:1761/2330 train_time:102636ms step_avg:58.28ms
step:1762/2330 train_time:102696ms step_avg:58.28ms
step:1763/2330 train_time:102752ms step_avg:58.28ms
step:1764/2330 train_time:102812ms step_avg:58.28ms
step:1765/2330 train_time:102869ms step_avg:58.28ms
step:1766/2330 train_time:102929ms step_avg:58.28ms
step:1767/2330 train_time:102987ms step_avg:58.28ms
step:1768/2330 train_time:103054ms step_avg:58.29ms
step:1769/2330 train_time:103113ms step_avg:58.29ms
step:1770/2330 train_time:103176ms step_avg:58.29ms
step:1771/2330 train_time:103235ms step_avg:58.29ms
step:1772/2330 train_time:103295ms step_avg:58.29ms
step:1773/2330 train_time:103353ms step_avg:58.29ms
step:1774/2330 train_time:103413ms step_avg:58.29ms
step:1775/2330 train_time:103470ms step_avg:58.29ms
step:1776/2330 train_time:103531ms step_avg:58.29ms
step:1777/2330 train_time:103588ms step_avg:58.29ms
step:1778/2330 train_time:103649ms step_avg:58.30ms
step:1779/2330 train_time:103706ms step_avg:58.29ms
step:1780/2330 train_time:103766ms step_avg:58.30ms
step:1781/2330 train_time:103823ms step_avg:58.29ms
step:1782/2330 train_time:103883ms step_avg:58.30ms
step:1783/2330 train_time:103940ms step_avg:58.30ms
step:1784/2330 train_time:104003ms step_avg:58.30ms
step:1785/2330 train_time:104060ms step_avg:58.30ms
step:1786/2330 train_time:104124ms step_avg:58.30ms
step:1787/2330 train_time:104181ms step_avg:58.30ms
step:1788/2330 train_time:104245ms step_avg:58.30ms
step:1789/2330 train_time:104302ms step_avg:58.30ms
step:1790/2330 train_time:104363ms step_avg:58.30ms
step:1791/2330 train_time:104419ms step_avg:58.30ms
step:1792/2330 train_time:104480ms step_avg:58.30ms
step:1793/2330 train_time:104537ms step_avg:58.30ms
step:1794/2330 train_time:104597ms step_avg:58.30ms
step:1795/2330 train_time:104654ms step_avg:58.30ms
step:1796/2330 train_time:104715ms step_avg:58.30ms
step:1797/2330 train_time:104771ms step_avg:58.30ms
step:1798/2330 train_time:104832ms step_avg:58.30ms
step:1799/2330 train_time:104890ms step_avg:58.30ms
step:1800/2330 train_time:104950ms step_avg:58.31ms
step:1801/2330 train_time:105008ms step_avg:58.31ms
step:1802/2330 train_time:105070ms step_avg:58.31ms
step:1803/2330 train_time:105128ms step_avg:58.31ms
step:1804/2330 train_time:105191ms step_avg:58.31ms
step:1805/2330 train_time:105249ms step_avg:58.31ms
step:1806/2330 train_time:105310ms step_avg:58.31ms
step:1807/2330 train_time:105368ms step_avg:58.31ms
step:1808/2330 train_time:105428ms step_avg:58.31ms
step:1809/2330 train_time:105485ms step_avg:58.31ms
step:1810/2330 train_time:105546ms step_avg:58.31ms
step:1811/2330 train_time:105603ms step_avg:58.31ms
step:1812/2330 train_time:105663ms step_avg:58.31ms
step:1813/2330 train_time:105720ms step_avg:58.31ms
step:1814/2330 train_time:105781ms step_avg:58.31ms
step:1815/2330 train_time:105838ms step_avg:58.31ms
step:1816/2330 train_time:105898ms step_avg:58.31ms
step:1817/2330 train_time:105956ms step_avg:58.31ms
step:1818/2330 train_time:106017ms step_avg:58.32ms
step:1819/2330 train_time:106076ms step_avg:58.32ms
step:1820/2330 train_time:106137ms step_avg:58.32ms
step:1821/2330 train_time:106195ms step_avg:58.32ms
step:1822/2330 train_time:106256ms step_avg:58.32ms
step:1823/2330 train_time:106314ms step_avg:58.32ms
step:1824/2330 train_time:106375ms step_avg:58.32ms
step:1825/2330 train_time:106433ms step_avg:58.32ms
step:1826/2330 train_time:106494ms step_avg:58.32ms
step:1827/2330 train_time:106551ms step_avg:58.32ms
step:1828/2330 train_time:106612ms step_avg:58.32ms
step:1829/2330 train_time:106669ms step_avg:58.32ms
step:1830/2330 train_time:106730ms step_avg:58.32ms
step:1831/2330 train_time:106787ms step_avg:58.32ms
step:1832/2330 train_time:106847ms step_avg:58.32ms
step:1833/2330 train_time:106904ms step_avg:58.32ms
step:1834/2330 train_time:106965ms step_avg:58.32ms
step:1835/2330 train_time:107022ms step_avg:58.32ms
step:1836/2330 train_time:107083ms step_avg:58.32ms
step:1837/2330 train_time:107140ms step_avg:58.32ms
step:1838/2330 train_time:107202ms step_avg:58.33ms
step:1839/2330 train_time:107259ms step_avg:58.32ms
step:1840/2330 train_time:107321ms step_avg:58.33ms
step:1841/2330 train_time:107379ms step_avg:58.33ms
step:1842/2330 train_time:107440ms step_avg:58.33ms
step:1843/2330 train_time:107498ms step_avg:58.33ms
step:1844/2330 train_time:107557ms step_avg:58.33ms
step:1845/2330 train_time:107615ms step_avg:58.33ms
step:1846/2330 train_time:107675ms step_avg:58.33ms
step:1847/2330 train_time:107734ms step_avg:58.33ms
step:1848/2330 train_time:107795ms step_avg:58.33ms
step:1849/2330 train_time:107852ms step_avg:58.33ms
step:1850/2330 train_time:107912ms step_avg:58.33ms
step:1851/2330 train_time:107969ms step_avg:58.33ms
step:1852/2330 train_time:108031ms step_avg:58.33ms
step:1853/2330 train_time:108089ms step_avg:58.33ms
step:1854/2330 train_time:108150ms step_avg:58.33ms
step:1855/2330 train_time:108206ms step_avg:58.33ms
step:1856/2330 train_time:108269ms step_avg:58.33ms
step:1857/2330 train_time:108326ms step_avg:58.33ms
step:1858/2330 train_time:108388ms step_avg:58.34ms
step:1859/2330 train_time:108444ms step_avg:58.33ms
step:1860/2330 train_time:108507ms step_avg:58.34ms
step:1861/2330 train_time:108563ms step_avg:58.34ms
step:1862/2330 train_time:108626ms step_avg:58.34ms
step:1863/2330 train_time:108682ms step_avg:58.34ms
step:1864/2330 train_time:108744ms step_avg:58.34ms
step:1865/2330 train_time:108800ms step_avg:58.34ms
step:1866/2330 train_time:108861ms step_avg:58.34ms
step:1867/2330 train_time:108918ms step_avg:58.34ms
step:1868/2330 train_time:108979ms step_avg:58.34ms
step:1869/2330 train_time:109037ms step_avg:58.34ms
step:1870/2330 train_time:109097ms step_avg:58.34ms
step:1871/2330 train_time:109156ms step_avg:58.34ms
step:1872/2330 train_time:109217ms step_avg:58.34ms
step:1873/2330 train_time:109274ms step_avg:58.34ms
step:1874/2330 train_time:109335ms step_avg:58.34ms
step:1875/2330 train_time:109394ms step_avg:58.34ms
step:1876/2330 train_time:109454ms step_avg:58.34ms
step:1877/2330 train_time:109512ms step_avg:58.34ms
step:1878/2330 train_time:109573ms step_avg:58.35ms
step:1879/2330 train_time:109630ms step_avg:58.35ms
step:1880/2330 train_time:109691ms step_avg:58.35ms
step:1881/2330 train_time:109747ms step_avg:58.35ms
step:1882/2330 train_time:109809ms step_avg:58.35ms
step:1883/2330 train_time:109865ms step_avg:58.35ms
step:1884/2330 train_time:109927ms step_avg:58.35ms
step:1885/2330 train_time:109984ms step_avg:58.35ms
step:1886/2330 train_time:110045ms step_avg:58.35ms
step:1887/2330 train_time:110102ms step_avg:58.35ms
step:1888/2330 train_time:110163ms step_avg:58.35ms
step:1889/2330 train_time:110220ms step_avg:58.35ms
step:1890/2330 train_time:110282ms step_avg:58.35ms
step:1891/2330 train_time:110339ms step_avg:58.35ms
step:1892/2330 train_time:110402ms step_avg:58.35ms
step:1893/2330 train_time:110458ms step_avg:58.35ms
step:1894/2330 train_time:110520ms step_avg:58.35ms
step:1895/2330 train_time:110578ms step_avg:58.35ms
step:1896/2330 train_time:110638ms step_avg:58.35ms
step:1897/2330 train_time:110695ms step_avg:58.35ms
step:1898/2330 train_time:110756ms step_avg:58.35ms
step:1899/2330 train_time:110814ms step_avg:58.35ms
step:1900/2330 train_time:110875ms step_avg:58.36ms
step:1901/2330 train_time:110933ms step_avg:58.36ms
step:1902/2330 train_time:110994ms step_avg:58.36ms
step:1903/2330 train_time:111052ms step_avg:58.36ms
step:1904/2330 train_time:111112ms step_avg:58.36ms
step:1905/2330 train_time:111171ms step_avg:58.36ms
step:1906/2330 train_time:111231ms step_avg:58.36ms
step:1907/2330 train_time:111289ms step_avg:58.36ms
step:1908/2330 train_time:111349ms step_avg:58.36ms
step:1909/2330 train_time:111405ms step_avg:58.36ms
step:1910/2330 train_time:111467ms step_avg:58.36ms
step:1911/2330 train_time:111524ms step_avg:58.36ms
step:1912/2330 train_time:111586ms step_avg:58.36ms
step:1913/2330 train_time:111643ms step_avg:58.36ms
step:1914/2330 train_time:111704ms step_avg:58.36ms
step:1915/2330 train_time:111761ms step_avg:58.36ms
step:1916/2330 train_time:111823ms step_avg:58.36ms
step:1917/2330 train_time:111880ms step_avg:58.36ms
step:1918/2330 train_time:111941ms step_avg:58.36ms
step:1919/2330 train_time:111999ms step_avg:58.36ms
step:1920/2330 train_time:112059ms step_avg:58.36ms
step:1921/2330 train_time:112118ms step_avg:58.36ms
step:1922/2330 train_time:112178ms step_avg:58.37ms
step:1923/2330 train_time:112236ms step_avg:58.36ms
step:1924/2330 train_time:112295ms step_avg:58.37ms
step:1925/2330 train_time:112354ms step_avg:58.37ms
step:1926/2330 train_time:112414ms step_avg:58.37ms
step:1927/2330 train_time:112473ms step_avg:58.37ms
step:1928/2330 train_time:112534ms step_avg:58.37ms
step:1929/2330 train_time:112593ms step_avg:58.37ms
step:1930/2330 train_time:112653ms step_avg:58.37ms
step:1931/2330 train_time:112710ms step_avg:58.37ms
step:1932/2330 train_time:112773ms step_avg:58.37ms
step:1933/2330 train_time:112829ms step_avg:58.37ms
step:1934/2330 train_time:112892ms step_avg:58.37ms
step:1935/2330 train_time:112948ms step_avg:58.37ms
step:1936/2330 train_time:113010ms step_avg:58.37ms
step:1937/2330 train_time:113067ms step_avg:58.37ms
step:1938/2330 train_time:113128ms step_avg:58.37ms
step:1939/2330 train_time:113185ms step_avg:58.37ms
step:1940/2330 train_time:113247ms step_avg:58.37ms
step:1941/2330 train_time:113304ms step_avg:58.37ms
step:1942/2330 train_time:113365ms step_avg:58.38ms
step:1943/2330 train_time:113422ms step_avg:58.37ms
step:1944/2330 train_time:113483ms step_avg:58.38ms
step:1945/2330 train_time:113540ms step_avg:58.38ms
step:1946/2330 train_time:113602ms step_avg:58.38ms
step:1947/2330 train_time:113660ms step_avg:58.38ms
step:1948/2330 train_time:113721ms step_avg:58.38ms
step:1949/2330 train_time:113779ms step_avg:58.38ms
step:1950/2330 train_time:113839ms step_avg:58.38ms
step:1951/2330 train_time:113896ms step_avg:58.38ms
step:1952/2330 train_time:113957ms step_avg:58.38ms
step:1953/2330 train_time:114015ms step_avg:58.38ms
step:1954/2330 train_time:114075ms step_avg:58.38ms
step:1955/2330 train_time:114134ms step_avg:58.38ms
step:1956/2330 train_time:114194ms step_avg:58.38ms
step:1957/2330 train_time:114253ms step_avg:58.38ms
step:1958/2330 train_time:114312ms step_avg:58.38ms
step:1959/2330 train_time:114369ms step_avg:58.38ms
step:1960/2330 train_time:114430ms step_avg:58.38ms
step:1961/2330 train_time:114488ms step_avg:58.38ms
step:1962/2330 train_time:114547ms step_avg:58.38ms
step:1963/2330 train_time:114604ms step_avg:58.38ms
step:1964/2330 train_time:114666ms step_avg:58.38ms
step:1965/2330 train_time:114723ms step_avg:58.38ms
step:1966/2330 train_time:114785ms step_avg:58.38ms
step:1967/2330 train_time:114842ms step_avg:58.38ms
step:1968/2330 train_time:114903ms step_avg:58.39ms
step:1969/2330 train_time:114960ms step_avg:58.38ms
step:1970/2330 train_time:115021ms step_avg:58.39ms
step:1971/2330 train_time:115079ms step_avg:58.39ms
step:1972/2330 train_time:115140ms step_avg:58.39ms
step:1973/2330 train_time:115198ms step_avg:58.39ms
step:1974/2330 train_time:115258ms step_avg:58.39ms
step:1975/2330 train_time:115315ms step_avg:58.39ms
step:1976/2330 train_time:115375ms step_avg:58.39ms
step:1977/2330 train_time:115434ms step_avg:58.39ms
step:1978/2330 train_time:115494ms step_avg:58.39ms
step:1979/2330 train_time:115551ms step_avg:58.39ms
step:1980/2330 train_time:115612ms step_avg:58.39ms
step:1981/2330 train_time:115670ms step_avg:58.39ms
step:1982/2330 train_time:115731ms step_avg:58.39ms
step:1983/2330 train_time:115788ms step_avg:58.39ms
step:1984/2330 train_time:115850ms step_avg:58.39ms
step:1985/2330 train_time:115907ms step_avg:58.39ms
step:1986/2330 train_time:115970ms step_avg:58.39ms
step:1987/2330 train_time:116027ms step_avg:58.39ms
step:1988/2330 train_time:116089ms step_avg:58.39ms
step:1989/2330 train_time:116146ms step_avg:58.39ms
step:1990/2330 train_time:116207ms step_avg:58.40ms
step:1991/2330 train_time:116264ms step_avg:58.39ms
step:1992/2330 train_time:116325ms step_avg:58.40ms
step:1993/2330 train_time:116382ms step_avg:58.40ms
step:1994/2330 train_time:116444ms step_avg:58.40ms
step:1995/2330 train_time:116501ms step_avg:58.40ms
step:1996/2330 train_time:116562ms step_avg:58.40ms
step:1997/2330 train_time:116620ms step_avg:58.40ms
step:1998/2330 train_time:116680ms step_avg:58.40ms
step:1999/2330 train_time:116738ms step_avg:58.40ms
step:2000/2330 train_time:116798ms step_avg:58.40ms
step:2000/2330 val_loss:3.7641 train_time:116880ms step_avg:58.44ms
step:2001/2330 train_time:116900ms step_avg:58.42ms
step:2002/2330 train_time:116920ms step_avg:58.40ms
step:2003/2330 train_time:116983ms step_avg:58.40ms
step:2004/2330 train_time:117047ms step_avg:58.41ms
step:2005/2330 train_time:117106ms step_avg:58.41ms
step:2006/2330 train_time:117166ms step_avg:58.41ms
step:2007/2330 train_time:117224ms step_avg:58.41ms
step:2008/2330 train_time:117283ms step_avg:58.41ms
step:2009/2330 train_time:117340ms step_avg:58.41ms
step:2010/2330 train_time:117400ms step_avg:58.41ms
step:2011/2330 train_time:117456ms step_avg:58.41ms
step:2012/2330 train_time:117516ms step_avg:58.41ms
step:2013/2330 train_time:117572ms step_avg:58.41ms
step:2014/2330 train_time:117633ms step_avg:58.41ms
step:2015/2330 train_time:117689ms step_avg:58.41ms
step:2016/2330 train_time:117749ms step_avg:58.41ms
step:2017/2330 train_time:117805ms step_avg:58.41ms
step:2018/2330 train_time:117867ms step_avg:58.41ms
step:2019/2330 train_time:117925ms step_avg:58.41ms
step:2020/2330 train_time:117990ms step_avg:58.41ms
step:2021/2330 train_time:118048ms step_avg:58.41ms
step:2022/2330 train_time:118111ms step_avg:58.41ms
step:2023/2330 train_time:118169ms step_avg:58.41ms
step:2024/2330 train_time:118231ms step_avg:58.41ms
step:2025/2330 train_time:118287ms step_avg:58.41ms
step:2026/2330 train_time:118349ms step_avg:58.42ms
step:2027/2330 train_time:118406ms step_avg:58.41ms
step:2028/2330 train_time:118466ms step_avg:58.42ms
step:2029/2330 train_time:118523ms step_avg:58.41ms
step:2030/2330 train_time:118583ms step_avg:58.42ms
step:2031/2330 train_time:118640ms step_avg:58.41ms
step:2032/2330 train_time:118701ms step_avg:58.42ms
step:2033/2330 train_time:118758ms step_avg:58.42ms
step:2034/2330 train_time:118819ms step_avg:58.42ms
step:2035/2330 train_time:118876ms step_avg:58.42ms
step:2036/2330 train_time:118937ms step_avg:58.42ms
step:2037/2330 train_time:118996ms step_avg:58.42ms
step:2038/2330 train_time:119059ms step_avg:58.42ms
step:2039/2330 train_time:119117ms step_avg:58.42ms
step:2040/2330 train_time:119179ms step_avg:58.42ms
step:2041/2330 train_time:119238ms step_avg:58.42ms
step:2042/2330 train_time:119299ms step_avg:58.42ms
step:2043/2330 train_time:119357ms step_avg:58.42ms
step:2044/2330 train_time:119418ms step_avg:58.42ms
step:2045/2330 train_time:119475ms step_avg:58.42ms
step:2046/2330 train_time:119535ms step_avg:58.42ms
step:2047/2330 train_time:119592ms step_avg:58.42ms
step:2048/2330 train_time:119653ms step_avg:58.42ms
step:2049/2330 train_time:119710ms step_avg:58.42ms
step:2050/2330 train_time:119770ms step_avg:58.42ms
step:2051/2330 train_time:119827ms step_avg:58.42ms
step:2052/2330 train_time:119889ms step_avg:58.43ms
step:2053/2330 train_time:119946ms step_avg:58.42ms
step:2054/2330 train_time:120008ms step_avg:58.43ms
step:2055/2330 train_time:120065ms step_avg:58.43ms
step:2056/2330 train_time:120127ms step_avg:58.43ms
step:2057/2330 train_time:120184ms step_avg:58.43ms
step:2058/2330 train_time:120246ms step_avg:58.43ms
step:2059/2330 train_time:120304ms step_avg:58.43ms
step:2060/2330 train_time:120364ms step_avg:58.43ms
step:2061/2330 train_time:120421ms step_avg:58.43ms
step:2062/2330 train_time:120481ms step_avg:58.43ms
step:2063/2330 train_time:120539ms step_avg:58.43ms
step:2064/2330 train_time:120599ms step_avg:58.43ms
step:2065/2330 train_time:120658ms step_avg:58.43ms
step:2066/2330 train_time:120719ms step_avg:58.43ms
step:2067/2330 train_time:120777ms step_avg:58.43ms
step:2068/2330 train_time:120837ms step_avg:58.43ms
step:2069/2330 train_time:120894ms step_avg:58.43ms
step:2070/2330 train_time:120956ms step_avg:58.43ms
step:2071/2330 train_time:121014ms step_avg:58.43ms
step:2072/2330 train_time:121076ms step_avg:58.43ms
step:2073/2330 train_time:121133ms step_avg:58.43ms
step:2074/2330 train_time:121195ms step_avg:58.44ms
step:2075/2330 train_time:121252ms step_avg:58.43ms
step:2076/2330 train_time:121313ms step_avg:58.44ms
step:2077/2330 train_time:121370ms step_avg:58.44ms
step:2078/2330 train_time:121432ms step_avg:58.44ms
step:2079/2330 train_time:121489ms step_avg:58.44ms
step:2080/2330 train_time:121550ms step_avg:58.44ms
step:2081/2330 train_time:121607ms step_avg:58.44ms
step:2082/2330 train_time:121668ms step_avg:58.44ms
step:2083/2330 train_time:121725ms step_avg:58.44ms
step:2084/2330 train_time:121786ms step_avg:58.44ms
step:2085/2330 train_time:121843ms step_avg:58.44ms
step:2086/2330 train_time:121904ms step_avg:58.44ms
step:2087/2330 train_time:121962ms step_avg:58.44ms
step:2088/2330 train_time:122022ms step_avg:58.44ms
step:2089/2330 train_time:122080ms step_avg:58.44ms
step:2090/2330 train_time:122141ms step_avg:58.44ms
step:2091/2330 train_time:122199ms step_avg:58.44ms
step:2092/2330 train_time:122260ms step_avg:58.44ms
step:2093/2330 train_time:122318ms step_avg:58.44ms
step:2094/2330 train_time:122379ms step_avg:58.44ms
step:2095/2330 train_time:122437ms step_avg:58.44ms
step:2096/2330 train_time:122497ms step_avg:58.44ms
step:2097/2330 train_time:122555ms step_avg:58.44ms
step:2098/2330 train_time:122617ms step_avg:58.44ms
step:2099/2330 train_time:122674ms step_avg:58.44ms
step:2100/2330 train_time:122734ms step_avg:58.44ms
step:2101/2330 train_time:122790ms step_avg:58.44ms
step:2102/2330 train_time:122852ms step_avg:58.45ms
step:2103/2330 train_time:122910ms step_avg:58.44ms
step:2104/2330 train_time:122971ms step_avg:58.45ms
step:2105/2330 train_time:123028ms step_avg:58.45ms
step:2106/2330 train_time:123089ms step_avg:58.45ms
step:2107/2330 train_time:123147ms step_avg:58.45ms
step:2108/2330 train_time:123208ms step_avg:58.45ms
step:2109/2330 train_time:123265ms step_avg:58.45ms
step:2110/2330 train_time:123327ms step_avg:58.45ms
step:2111/2330 train_time:123384ms step_avg:58.45ms
step:2112/2330 train_time:123446ms step_avg:58.45ms
step:2113/2330 train_time:123504ms step_avg:58.45ms
step:2114/2330 train_time:123565ms step_avg:58.45ms
step:2115/2330 train_time:123622ms step_avg:58.45ms
step:2116/2330 train_time:123682ms step_avg:58.45ms
step:2117/2330 train_time:123739ms step_avg:58.45ms
step:2118/2330 train_time:123800ms step_avg:58.45ms
step:2119/2330 train_time:123858ms step_avg:58.45ms
step:2120/2330 train_time:123919ms step_avg:58.45ms
step:2121/2330 train_time:123977ms step_avg:58.45ms
step:2122/2330 train_time:124038ms step_avg:58.45ms
step:2123/2330 train_time:124096ms step_avg:58.45ms
step:2124/2330 train_time:124157ms step_avg:58.45ms
step:2125/2330 train_time:124216ms step_avg:58.45ms
step:2126/2330 train_time:124275ms step_avg:58.46ms
step:2127/2330 train_time:124332ms step_avg:58.45ms
step:2128/2330 train_time:124395ms step_avg:58.46ms
step:2129/2330 train_time:124452ms step_avg:58.46ms
step:2130/2330 train_time:124515ms step_avg:58.46ms
step:2131/2330 train_time:124571ms step_avg:58.46ms
step:2132/2330 train_time:124633ms step_avg:58.46ms
step:2133/2330 train_time:124690ms step_avg:58.46ms
step:2134/2330 train_time:124751ms step_avg:58.46ms
step:2135/2330 train_time:124808ms step_avg:58.46ms
step:2136/2330 train_time:124869ms step_avg:58.46ms
step:2137/2330 train_time:124925ms step_avg:58.46ms
step:2138/2330 train_time:124988ms step_avg:58.46ms
step:2139/2330 train_time:125045ms step_avg:58.46ms
step:2140/2330 train_time:125107ms step_avg:58.46ms
step:2141/2330 train_time:125164ms step_avg:58.46ms
step:2142/2330 train_time:125226ms step_avg:58.46ms
step:2143/2330 train_time:125284ms step_avg:58.46ms
step:2144/2330 train_time:125345ms step_avg:58.46ms
step:2145/2330 train_time:125404ms step_avg:58.46ms
step:2146/2330 train_time:125464ms step_avg:58.46ms
step:2147/2330 train_time:125522ms step_avg:58.46ms
step:2148/2330 train_time:125582ms step_avg:58.46ms
step:2149/2330 train_time:125640ms step_avg:58.46ms
step:2150/2330 train_time:125701ms step_avg:58.47ms
step:2151/2330 train_time:125758ms step_avg:58.46ms
step:2152/2330 train_time:125819ms step_avg:58.47ms
step:2153/2330 train_time:125876ms step_avg:58.47ms
step:2154/2330 train_time:125937ms step_avg:58.47ms
step:2155/2330 train_time:125994ms step_avg:58.47ms
step:2156/2330 train_time:126056ms step_avg:58.47ms
step:2157/2330 train_time:126114ms step_avg:58.47ms
step:2158/2330 train_time:126174ms step_avg:58.47ms
step:2159/2330 train_time:126231ms step_avg:58.47ms
step:2160/2330 train_time:126294ms step_avg:58.47ms
step:2161/2330 train_time:126351ms step_avg:58.47ms
step:2162/2330 train_time:126412ms step_avg:58.47ms
step:2163/2330 train_time:126469ms step_avg:58.47ms
step:2164/2330 train_time:126532ms step_avg:58.47ms
step:2165/2330 train_time:126588ms step_avg:58.47ms
step:2166/2330 train_time:126650ms step_avg:58.47ms
step:2167/2330 train_time:126707ms step_avg:58.47ms
step:2168/2330 train_time:126769ms step_avg:58.47ms
step:2169/2330 train_time:126825ms step_avg:58.47ms
step:2170/2330 train_time:126886ms step_avg:58.47ms
step:2171/2330 train_time:126944ms step_avg:58.47ms
step:2172/2330 train_time:127004ms step_avg:58.47ms
step:2173/2330 train_time:127063ms step_avg:58.47ms
step:2174/2330 train_time:127123ms step_avg:58.47ms
step:2175/2330 train_time:127182ms step_avg:58.47ms
step:2176/2330 train_time:127242ms step_avg:58.48ms
step:2177/2330 train_time:127300ms step_avg:58.47ms
step:2178/2330 train_time:127360ms step_avg:58.48ms
step:2179/2330 train_time:127418ms step_avg:58.48ms
step:2180/2330 train_time:127479ms step_avg:58.48ms
step:2181/2330 train_time:127536ms step_avg:58.48ms
step:2182/2330 train_time:127598ms step_avg:58.48ms
step:2183/2330 train_time:127656ms step_avg:58.48ms
step:2184/2330 train_time:127717ms step_avg:58.48ms
step:2185/2330 train_time:127775ms step_avg:58.48ms
step:2186/2330 train_time:127836ms step_avg:58.48ms
step:2187/2330 train_time:127893ms step_avg:58.48ms
step:2188/2330 train_time:127955ms step_avg:58.48ms
step:2189/2330 train_time:128012ms step_avg:58.48ms
step:2190/2330 train_time:128073ms step_avg:58.48ms
step:2191/2330 train_time:128129ms step_avg:58.48ms
step:2192/2330 train_time:128191ms step_avg:58.48ms
step:2193/2330 train_time:128248ms step_avg:58.48ms
step:2194/2330 train_time:128310ms step_avg:58.48ms
step:2195/2330 train_time:128366ms step_avg:58.48ms
step:2196/2330 train_time:128429ms step_avg:58.48ms
step:2197/2330 train_time:128485ms step_avg:58.48ms
step:2198/2330 train_time:128547ms step_avg:58.48ms
step:2199/2330 train_time:128604ms step_avg:58.48ms
step:2200/2330 train_time:128664ms step_avg:58.48ms
step:2201/2330 train_time:128723ms step_avg:58.48ms
step:2202/2330 train_time:128783ms step_avg:58.48ms
step:2203/2330 train_time:128841ms step_avg:58.48ms
step:2204/2330 train_time:128902ms step_avg:58.49ms
step:2205/2330 train_time:128960ms step_avg:58.49ms
step:2206/2330 train_time:129020ms step_avg:58.49ms
step:2207/2330 train_time:129078ms step_avg:58.49ms
step:2208/2330 train_time:129139ms step_avg:58.49ms
step:2209/2330 train_time:129198ms step_avg:58.49ms
step:2210/2330 train_time:129258ms step_avg:58.49ms
step:2211/2330 train_time:129316ms step_avg:58.49ms
step:2212/2330 train_time:129377ms step_avg:58.49ms
step:2213/2330 train_time:129434ms step_avg:58.49ms
step:2214/2330 train_time:129496ms step_avg:58.49ms
step:2215/2330 train_time:129553ms step_avg:58.49ms
step:2216/2330 train_time:129615ms step_avg:58.49ms
step:2217/2330 train_time:129672ms step_avg:58.49ms
step:2218/2330 train_time:129734ms step_avg:58.49ms
step:2219/2330 train_time:129790ms step_avg:58.49ms
step:2220/2330 train_time:129852ms step_avg:58.49ms
step:2221/2330 train_time:129909ms step_avg:58.49ms
step:2222/2330 train_time:129971ms step_avg:58.49ms
step:2223/2330 train_time:130027ms step_avg:58.49ms
step:2224/2330 train_time:130090ms step_avg:58.49ms
step:2225/2330 train_time:130147ms step_avg:58.49ms
step:2226/2330 train_time:130208ms step_avg:58.49ms
step:2227/2330 train_time:130265ms step_avg:58.49ms
step:2228/2330 train_time:130327ms step_avg:58.49ms
step:2229/2330 train_time:130385ms step_avg:58.49ms
step:2230/2330 train_time:130444ms step_avg:58.50ms
step:2231/2330 train_time:130502ms step_avg:58.49ms
step:2232/2330 train_time:130563ms step_avg:58.50ms
step:2233/2330 train_time:130621ms step_avg:58.50ms
step:2234/2330 train_time:130681ms step_avg:58.50ms
step:2235/2330 train_time:130738ms step_avg:58.50ms
step:2236/2330 train_time:130799ms step_avg:58.50ms
step:2237/2330 train_time:130858ms step_avg:58.50ms
step:2238/2330 train_time:130918ms step_avg:58.50ms
step:2239/2330 train_time:130975ms step_avg:58.50ms
step:2240/2330 train_time:131037ms step_avg:58.50ms
step:2241/2330 train_time:131094ms step_avg:58.50ms
step:2242/2330 train_time:131156ms step_avg:58.50ms
step:2243/2330 train_time:131213ms step_avg:58.50ms
step:2244/2330 train_time:131275ms step_avg:58.50ms
step:2245/2330 train_time:131331ms step_avg:58.50ms
step:2246/2330 train_time:131393ms step_avg:58.50ms
step:2247/2330 train_time:131450ms step_avg:58.50ms
step:2248/2330 train_time:131513ms step_avg:58.50ms
step:2249/2330 train_time:131570ms step_avg:58.50ms
step:2250/2330 train_time:131631ms step_avg:58.50ms
step:2250/2330 val_loss:3.7059 train_time:131712ms step_avg:58.54ms
step:2251/2330 train_time:131732ms step_avg:58.52ms
step:2252/2330 train_time:131753ms step_avg:58.50ms
step:2253/2330 train_time:131811ms step_avg:58.50ms
step:2254/2330 train_time:131875ms step_avg:58.51ms
step:2255/2330 train_time:131932ms step_avg:58.51ms
step:2256/2330 train_time:131994ms step_avg:58.51ms
step:2257/2330 train_time:132050ms step_avg:58.51ms
step:2258/2330 train_time:132111ms step_avg:58.51ms
step:2259/2330 train_time:132168ms step_avg:58.51ms
step:2260/2330 train_time:132228ms step_avg:58.51ms
step:2261/2330 train_time:132285ms step_avg:58.51ms
step:2262/2330 train_time:132345ms step_avg:58.51ms
step:2263/2330 train_time:132402ms step_avg:58.51ms
step:2264/2330 train_time:132462ms step_avg:58.51ms
step:2265/2330 train_time:132519ms step_avg:58.51ms
step:2266/2330 train_time:132579ms step_avg:58.51ms
step:2267/2330 train_time:132637ms step_avg:58.51ms
step:2268/2330 train_time:132698ms step_avg:58.51ms
step:2269/2330 train_time:132756ms step_avg:58.51ms
step:2270/2330 train_time:132819ms step_avg:58.51ms
step:2271/2330 train_time:132878ms step_avg:58.51ms
step:2272/2330 train_time:132939ms step_avg:58.51ms
step:2273/2330 train_time:132996ms step_avg:58.51ms
step:2274/2330 train_time:133059ms step_avg:58.51ms
step:2275/2330 train_time:133115ms step_avg:58.51ms
step:2276/2330 train_time:133176ms step_avg:58.51ms
step:2277/2330 train_time:133233ms step_avg:58.51ms
step:2278/2330 train_time:133294ms step_avg:58.51ms
step:2279/2330 train_time:133350ms step_avg:58.51ms
step:2280/2330 train_time:133411ms step_avg:58.51ms
step:2281/2330 train_time:133467ms step_avg:58.51ms
step:2282/2330 train_time:133528ms step_avg:58.51ms
step:2283/2330 train_time:133585ms step_avg:58.51ms
step:2284/2330 train_time:133646ms step_avg:58.51ms
step:2285/2330 train_time:133704ms step_avg:58.51ms
step:2286/2330 train_time:133765ms step_avg:58.51ms
step:2287/2330 train_time:133824ms step_avg:58.52ms
step:2288/2330 train_time:133885ms step_avg:58.52ms
step:2289/2330 train_time:133944ms step_avg:58.52ms
step:2290/2330 train_time:134005ms step_avg:58.52ms
step:2291/2330 train_time:134064ms step_avg:58.52ms
step:2292/2330 train_time:134125ms step_avg:58.52ms
step:2293/2330 train_time:134183ms step_avg:58.52ms
step:2294/2330 train_time:134243ms step_avg:58.52ms
step:2295/2330 train_time:134302ms step_avg:58.52ms
step:2296/2330 train_time:134362ms step_avg:58.52ms
step:2297/2330 train_time:134419ms step_avg:58.52ms
step:2298/2330 train_time:134478ms step_avg:58.52ms
step:2299/2330 train_time:134536ms step_avg:58.52ms
step:2300/2330 train_time:134595ms step_avg:58.52ms
step:2301/2330 train_time:134653ms step_avg:58.52ms
step:2302/2330 train_time:134714ms step_avg:58.52ms
step:2303/2330 train_time:134772ms step_avg:58.52ms
step:2304/2330 train_time:134833ms step_avg:58.52ms
step:2305/2330 train_time:134891ms step_avg:58.52ms
step:2306/2330 train_time:134953ms step_avg:58.52ms
step:2307/2330 train_time:135010ms step_avg:58.52ms
step:2308/2330 train_time:135072ms step_avg:58.52ms
step:2309/2330 train_time:135129ms step_avg:58.52ms
step:2310/2330 train_time:135190ms step_avg:58.52ms
step:2311/2330 train_time:135247ms step_avg:58.52ms
step:2312/2330 train_time:135308ms step_avg:58.52ms
step:2313/2330 train_time:135366ms step_avg:58.52ms
step:2314/2330 train_time:135426ms step_avg:58.52ms
step:2315/2330 train_time:135484ms step_avg:58.52ms
step:2316/2330 train_time:135544ms step_avg:58.52ms
step:2317/2330 train_time:135602ms step_avg:58.52ms
step:2318/2330 train_time:135663ms step_avg:58.53ms
step:2319/2330 train_time:135721ms step_avg:58.53ms
step:2320/2330 train_time:135782ms step_avg:58.53ms
step:2321/2330 train_time:135841ms step_avg:58.53ms
step:2322/2330 train_time:135902ms step_avg:58.53ms
step:2323/2330 train_time:135961ms step_avg:58.53ms
step:2324/2330 train_time:136022ms step_avg:58.53ms
step:2325/2330 train_time:136080ms step_avg:58.53ms
step:2326/2330 train_time:136141ms step_avg:58.53ms
step:2327/2330 train_time:136197ms step_avg:58.53ms
step:2328/2330 train_time:136258ms step_avg:58.53ms
step:2329/2330 train_time:136315ms step_avg:58.53ms
step:2330/2330 train_time:136375ms step_avg:58.53ms
step:2330/2330 val_loss:3.6869 train_time:136457ms step_avg:58.57ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
