import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:58:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:84.99ms
step:2/2330 train_time:182ms step_avg:91.00ms
step:3/2330 train_time:201ms step_avg:66.86ms
step:4/2330 train_time:221ms step_avg:55.18ms
step:5/2330 train_time:273ms step_avg:54.60ms
step:6/2330 train_time:331ms step_avg:55.24ms
step:7/2330 train_time:387ms step_avg:55.24ms
step:8/2330 train_time:445ms step_avg:55.66ms
step:9/2330 train_time:501ms step_avg:55.65ms
step:10/2330 train_time:559ms step_avg:55.86ms
step:11/2330 train_time:614ms step_avg:55.80ms
step:12/2330 train_time:672ms step_avg:56.03ms
step:13/2330 train_time:728ms step_avg:56.02ms
step:14/2330 train_time:786ms step_avg:56.18ms
step:15/2330 train_time:842ms step_avg:56.14ms
step:16/2330 train_time:901ms step_avg:56.28ms
step:17/2330 train_time:956ms step_avg:56.22ms
step:18/2330 train_time:1014ms step_avg:56.35ms
step:19/2330 train_time:1072ms step_avg:56.42ms
step:20/2330 train_time:1133ms step_avg:56.65ms
step:21/2330 train_time:1191ms step_avg:56.70ms
step:22/2330 train_time:1252ms step_avg:56.89ms
step:23/2330 train_time:1308ms step_avg:56.86ms
step:24/2330 train_time:1367ms step_avg:56.95ms
step:25/2330 train_time:1423ms step_avg:56.91ms
step:26/2330 train_time:1482ms step_avg:56.99ms
step:27/2330 train_time:1537ms step_avg:56.91ms
step:28/2330 train_time:1596ms step_avg:56.99ms
step:29/2330 train_time:1651ms step_avg:56.93ms
step:30/2330 train_time:1710ms step_avg:56.99ms
step:31/2330 train_time:1765ms step_avg:56.94ms
step:32/2330 train_time:1824ms step_avg:57.00ms
step:33/2330 train_time:1880ms step_avg:56.95ms
step:34/2330 train_time:1938ms step_avg:57.00ms
step:35/2330 train_time:1993ms step_avg:56.95ms
step:36/2330 train_time:2053ms step_avg:57.02ms
step:37/2330 train_time:2109ms step_avg:57.01ms
step:38/2330 train_time:2171ms step_avg:57.14ms
step:39/2330 train_time:2228ms step_avg:57.13ms
step:40/2330 train_time:2288ms step_avg:57.20ms
step:41/2330 train_time:2344ms step_avg:57.16ms
step:42/2330 train_time:2403ms step_avg:57.21ms
step:43/2330 train_time:2459ms step_avg:57.18ms
step:44/2330 train_time:2517ms step_avg:57.20ms
step:45/2330 train_time:2573ms step_avg:57.17ms
step:46/2330 train_time:2631ms step_avg:57.20ms
step:47/2330 train_time:2686ms step_avg:57.15ms
step:48/2330 train_time:2745ms step_avg:57.19ms
step:49/2330 train_time:2800ms step_avg:57.15ms
step:50/2330 train_time:2860ms step_avg:57.20ms
step:51/2330 train_time:2915ms step_avg:57.16ms
step:52/2330 train_time:2975ms step_avg:57.21ms
step:53/2330 train_time:3031ms step_avg:57.18ms
step:54/2330 train_time:3090ms step_avg:57.23ms
step:55/2330 train_time:3146ms step_avg:57.20ms
step:56/2330 train_time:3206ms step_avg:57.26ms
step:57/2330 train_time:3262ms step_avg:57.23ms
step:58/2330 train_time:3323ms step_avg:57.29ms
step:59/2330 train_time:3379ms step_avg:57.27ms
step:60/2330 train_time:3438ms step_avg:57.30ms
step:61/2330 train_time:3494ms step_avg:57.28ms
step:62/2330 train_time:3552ms step_avg:57.29ms
step:63/2330 train_time:3607ms step_avg:57.26ms
step:64/2330 train_time:3667ms step_avg:57.29ms
step:65/2330 train_time:3722ms step_avg:57.27ms
step:66/2330 train_time:3781ms step_avg:57.29ms
step:67/2330 train_time:3836ms step_avg:57.26ms
step:68/2330 train_time:3895ms step_avg:57.28ms
step:69/2330 train_time:3951ms step_avg:57.25ms
step:70/2330 train_time:4010ms step_avg:57.29ms
step:71/2330 train_time:4066ms step_avg:57.27ms
step:72/2330 train_time:4126ms step_avg:57.30ms
step:73/2330 train_time:4182ms step_avg:57.29ms
step:74/2330 train_time:4242ms step_avg:57.32ms
step:75/2330 train_time:4298ms step_avg:57.30ms
step:76/2330 train_time:4356ms step_avg:57.32ms
step:77/2330 train_time:4413ms step_avg:57.31ms
step:78/2330 train_time:4471ms step_avg:57.32ms
step:79/2330 train_time:4527ms step_avg:57.30ms
step:80/2330 train_time:4586ms step_avg:57.33ms
step:81/2330 train_time:4642ms step_avg:57.31ms
step:82/2330 train_time:4700ms step_avg:57.32ms
step:83/2330 train_time:4756ms step_avg:57.30ms
step:84/2330 train_time:4815ms step_avg:57.33ms
step:85/2330 train_time:4871ms step_avg:57.30ms
step:86/2330 train_time:4930ms step_avg:57.32ms
step:87/2330 train_time:4986ms step_avg:57.31ms
step:88/2330 train_time:5044ms step_avg:57.32ms
step:89/2330 train_time:5100ms step_avg:57.30ms
step:90/2330 train_time:5159ms step_avg:57.32ms
step:91/2330 train_time:5215ms step_avg:57.30ms
step:92/2330 train_time:5275ms step_avg:57.34ms
step:93/2330 train_time:5331ms step_avg:57.32ms
step:94/2330 train_time:5390ms step_avg:57.34ms
step:95/2330 train_time:5446ms step_avg:57.33ms
step:96/2330 train_time:5505ms step_avg:57.34ms
step:97/2330 train_time:5561ms step_avg:57.33ms
step:98/2330 train_time:5620ms step_avg:57.34ms
step:99/2330 train_time:5676ms step_avg:57.33ms
step:100/2330 train_time:5736ms step_avg:57.36ms
step:101/2330 train_time:5791ms step_avg:57.34ms
step:102/2330 train_time:5850ms step_avg:57.35ms
step:103/2330 train_time:5905ms step_avg:57.33ms
step:104/2330 train_time:5964ms step_avg:57.35ms
step:105/2330 train_time:6019ms step_avg:57.32ms
step:106/2330 train_time:6079ms step_avg:57.35ms
step:107/2330 train_time:6134ms step_avg:57.33ms
step:108/2330 train_time:6194ms step_avg:57.35ms
step:109/2330 train_time:6249ms step_avg:57.33ms
step:110/2330 train_time:6310ms step_avg:57.36ms
step:111/2330 train_time:6365ms step_avg:57.34ms
step:112/2330 train_time:6425ms step_avg:57.37ms
step:113/2330 train_time:6481ms step_avg:57.35ms
step:114/2330 train_time:6540ms step_avg:57.37ms
step:115/2330 train_time:6596ms step_avg:57.36ms
step:116/2330 train_time:6655ms step_avg:57.37ms
step:117/2330 train_time:6711ms step_avg:57.36ms
step:118/2330 train_time:6769ms step_avg:57.37ms
step:119/2330 train_time:6825ms step_avg:57.36ms
step:120/2330 train_time:6884ms step_avg:57.37ms
step:121/2330 train_time:6940ms step_avg:57.35ms
step:122/2330 train_time:6998ms step_avg:57.36ms
step:123/2330 train_time:7055ms step_avg:57.36ms
step:124/2330 train_time:7114ms step_avg:57.37ms
step:125/2330 train_time:7170ms step_avg:57.36ms
step:126/2330 train_time:7229ms step_avg:57.37ms
step:127/2330 train_time:7284ms step_avg:57.36ms
step:128/2330 train_time:7344ms step_avg:57.38ms
step:129/2330 train_time:7401ms step_avg:57.37ms
step:130/2330 train_time:7459ms step_avg:57.38ms
step:131/2330 train_time:7515ms step_avg:57.37ms
step:132/2330 train_time:7574ms step_avg:57.38ms
step:133/2330 train_time:7631ms step_avg:57.37ms
step:134/2330 train_time:7689ms step_avg:57.38ms
step:135/2330 train_time:7745ms step_avg:57.37ms
step:136/2330 train_time:7804ms step_avg:57.38ms
step:137/2330 train_time:7860ms step_avg:57.37ms
step:138/2330 train_time:7918ms step_avg:57.38ms
step:139/2330 train_time:7975ms step_avg:57.37ms
step:140/2330 train_time:8034ms step_avg:57.38ms
step:141/2330 train_time:8090ms step_avg:57.37ms
step:142/2330 train_time:8148ms step_avg:57.38ms
step:143/2330 train_time:8204ms step_avg:57.37ms
step:144/2330 train_time:8263ms step_avg:57.38ms
step:145/2330 train_time:8319ms step_avg:57.37ms
step:146/2330 train_time:8379ms step_avg:57.39ms
step:147/2330 train_time:8434ms step_avg:57.38ms
step:148/2330 train_time:8493ms step_avg:57.39ms
step:149/2330 train_time:8549ms step_avg:57.37ms
step:150/2330 train_time:8608ms step_avg:57.39ms
step:151/2330 train_time:8664ms step_avg:57.38ms
step:152/2330 train_time:8723ms step_avg:57.39ms
step:153/2330 train_time:8780ms step_avg:57.38ms
step:154/2330 train_time:8839ms step_avg:57.40ms
step:155/2330 train_time:8895ms step_avg:57.39ms
step:156/2330 train_time:8954ms step_avg:57.40ms
step:157/2330 train_time:9010ms step_avg:57.39ms
step:158/2330 train_time:9068ms step_avg:57.40ms
step:159/2330 train_time:9124ms step_avg:57.38ms
step:160/2330 train_time:9184ms step_avg:57.40ms
step:161/2330 train_time:9239ms step_avg:57.39ms
step:162/2330 train_time:9298ms step_avg:57.40ms
step:163/2330 train_time:9354ms step_avg:57.39ms
step:164/2330 train_time:9413ms step_avg:57.40ms
step:165/2330 train_time:9469ms step_avg:57.39ms
step:166/2330 train_time:9527ms step_avg:57.39ms
step:167/2330 train_time:9583ms step_avg:57.38ms
step:168/2330 train_time:9642ms step_avg:57.39ms
step:169/2330 train_time:9698ms step_avg:57.38ms
step:170/2330 train_time:9757ms step_avg:57.39ms
step:171/2330 train_time:9813ms step_avg:57.39ms
step:172/2330 train_time:9872ms step_avg:57.40ms
step:173/2330 train_time:9928ms step_avg:57.39ms
step:174/2330 train_time:9987ms step_avg:57.40ms
step:175/2330 train_time:10044ms step_avg:57.39ms
step:176/2330 train_time:10102ms step_avg:57.40ms
step:177/2330 train_time:10157ms step_avg:57.38ms
step:178/2330 train_time:10217ms step_avg:57.40ms
step:179/2330 train_time:10273ms step_avg:57.39ms
step:180/2330 train_time:10332ms step_avg:57.40ms
step:181/2330 train_time:10387ms step_avg:57.39ms
step:182/2330 train_time:10447ms step_avg:57.40ms
step:183/2330 train_time:10503ms step_avg:57.39ms
step:184/2330 train_time:10563ms step_avg:57.41ms
step:185/2330 train_time:10619ms step_avg:57.40ms
step:186/2330 train_time:10677ms step_avg:57.40ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10792ms step_avg:57.41ms
step:189/2330 train_time:10848ms step_avg:57.40ms
step:190/2330 train_time:10908ms step_avg:57.41ms
step:191/2330 train_time:10964ms step_avg:57.40ms
step:192/2330 train_time:11023ms step_avg:57.41ms
step:193/2330 train_time:11079ms step_avg:57.40ms
step:194/2330 train_time:11138ms step_avg:57.41ms
step:195/2330 train_time:11194ms step_avg:57.40ms
step:196/2330 train_time:11252ms step_avg:57.41ms
step:197/2330 train_time:11308ms step_avg:57.40ms
step:198/2330 train_time:11367ms step_avg:57.41ms
step:199/2330 train_time:11423ms step_avg:57.40ms
step:200/2330 train_time:11482ms step_avg:57.41ms
step:201/2330 train_time:11538ms step_avg:57.40ms
step:202/2330 train_time:11597ms step_avg:57.41ms
step:203/2330 train_time:11652ms step_avg:57.40ms
step:204/2330 train_time:11711ms step_avg:57.41ms
step:205/2330 train_time:11766ms step_avg:57.40ms
step:206/2330 train_time:11826ms step_avg:57.41ms
step:207/2330 train_time:11883ms step_avg:57.41ms
step:208/2330 train_time:11942ms step_avg:57.42ms
step:209/2330 train_time:11998ms step_avg:57.41ms
step:210/2330 train_time:12056ms step_avg:57.41ms
step:211/2330 train_time:12113ms step_avg:57.41ms
step:212/2330 train_time:12172ms step_avg:57.42ms
step:213/2330 train_time:12228ms step_avg:57.41ms
step:214/2330 train_time:12287ms step_avg:57.42ms
step:215/2330 train_time:12343ms step_avg:57.41ms
step:216/2330 train_time:12402ms step_avg:57.42ms
step:217/2330 train_time:12458ms step_avg:57.41ms
step:218/2330 train_time:12517ms step_avg:57.42ms
step:219/2330 train_time:12573ms step_avg:57.41ms
step:220/2330 train_time:12633ms step_avg:57.42ms
step:221/2330 train_time:12688ms step_avg:57.41ms
step:222/2330 train_time:12748ms step_avg:57.42ms
step:223/2330 train_time:12804ms step_avg:57.42ms
step:224/2330 train_time:12863ms step_avg:57.42ms
step:225/2330 train_time:12920ms step_avg:57.42ms
step:226/2330 train_time:12980ms step_avg:57.43ms
step:227/2330 train_time:13036ms step_avg:57.43ms
step:228/2330 train_time:13095ms step_avg:57.43ms
step:229/2330 train_time:13151ms step_avg:57.43ms
step:230/2330 train_time:13209ms step_avg:57.43ms
step:231/2330 train_time:13265ms step_avg:57.42ms
step:232/2330 train_time:13324ms step_avg:57.43ms
step:233/2330 train_time:13380ms step_avg:57.42ms
step:234/2330 train_time:13439ms step_avg:57.43ms
step:235/2330 train_time:13495ms step_avg:57.42ms
step:236/2330 train_time:13554ms step_avg:57.43ms
step:237/2330 train_time:13609ms step_avg:57.42ms
step:238/2330 train_time:13669ms step_avg:57.43ms
step:239/2330 train_time:13725ms step_avg:57.43ms
step:240/2330 train_time:13784ms step_avg:57.43ms
step:241/2330 train_time:13841ms step_avg:57.43ms
step:242/2330 train_time:13899ms step_avg:57.43ms
step:243/2330 train_time:13956ms step_avg:57.43ms
step:244/2330 train_time:14014ms step_avg:57.44ms
step:245/2330 train_time:14070ms step_avg:57.43ms
step:246/2330 train_time:14130ms step_avg:57.44ms
step:247/2330 train_time:14185ms step_avg:57.43ms
step:248/2330 train_time:14244ms step_avg:57.44ms
step:249/2330 train_time:14301ms step_avg:57.43ms
step:250/2330 train_time:14360ms step_avg:57.44ms
step:250/2330 val_loss:4.9074 train_time:14438ms step_avg:57.75ms
step:251/2330 train_time:14457ms step_avg:57.60ms
step:252/2330 train_time:14476ms step_avg:57.45ms
step:253/2330 train_time:14532ms step_avg:57.44ms
step:254/2330 train_time:14595ms step_avg:57.46ms
step:255/2330 train_time:14651ms step_avg:57.45ms
step:256/2330 train_time:14715ms step_avg:57.48ms
step:257/2330 train_time:14770ms step_avg:57.47ms
step:258/2330 train_time:14829ms step_avg:57.48ms
step:259/2330 train_time:14885ms step_avg:57.47ms
step:260/2330 train_time:14945ms step_avg:57.48ms
step:261/2330 train_time:15000ms step_avg:57.47ms
step:262/2330 train_time:15058ms step_avg:57.47ms
step:263/2330 train_time:15113ms step_avg:57.46ms
step:264/2330 train_time:15171ms step_avg:57.47ms
step:265/2330 train_time:15227ms step_avg:57.46ms
step:266/2330 train_time:15285ms step_avg:57.46ms
step:267/2330 train_time:15343ms step_avg:57.46ms
step:268/2330 train_time:15403ms step_avg:57.48ms
step:269/2330 train_time:15461ms step_avg:57.48ms
step:270/2330 train_time:15521ms step_avg:57.49ms
step:271/2330 train_time:15578ms step_avg:57.48ms
step:272/2330 train_time:15637ms step_avg:57.49ms
step:273/2330 train_time:15693ms step_avg:57.48ms
step:274/2330 train_time:15753ms step_avg:57.49ms
step:275/2330 train_time:15808ms step_avg:57.48ms
step:276/2330 train_time:15869ms step_avg:57.49ms
step:277/2330 train_time:15924ms step_avg:57.49ms
step:278/2330 train_time:15983ms step_avg:57.49ms
step:279/2330 train_time:16038ms step_avg:57.49ms
step:280/2330 train_time:16097ms step_avg:57.49ms
step:281/2330 train_time:16153ms step_avg:57.48ms
step:282/2330 train_time:16211ms step_avg:57.49ms
step:283/2330 train_time:16267ms step_avg:57.48ms
step:284/2330 train_time:16325ms step_avg:57.48ms
step:285/2330 train_time:16382ms step_avg:57.48ms
step:286/2330 train_time:16441ms step_avg:57.49ms
step:287/2330 train_time:16498ms step_avg:57.48ms
step:288/2330 train_time:16557ms step_avg:57.49ms
step:289/2330 train_time:16614ms step_avg:57.49ms
step:290/2330 train_time:16674ms step_avg:57.50ms
step:291/2330 train_time:16731ms step_avg:57.49ms
step:292/2330 train_time:16791ms step_avg:57.50ms
step:293/2330 train_time:16846ms step_avg:57.50ms
step:294/2330 train_time:16905ms step_avg:57.50ms
step:295/2330 train_time:16960ms step_avg:57.49ms
step:296/2330 train_time:17019ms step_avg:57.50ms
step:297/2330 train_time:17075ms step_avg:57.49ms
step:298/2330 train_time:17133ms step_avg:57.49ms
step:299/2330 train_time:17189ms step_avg:57.49ms
step:300/2330 train_time:17248ms step_avg:57.49ms
step:301/2330 train_time:17303ms step_avg:57.49ms
step:302/2330 train_time:17363ms step_avg:57.49ms
step:303/2330 train_time:17419ms step_avg:57.49ms
step:304/2330 train_time:17479ms step_avg:57.50ms
step:305/2330 train_time:17535ms step_avg:57.49ms
step:306/2330 train_time:17595ms step_avg:57.50ms
step:307/2330 train_time:17652ms step_avg:57.50ms
step:308/2330 train_time:17711ms step_avg:57.50ms
step:309/2330 train_time:17767ms step_avg:57.50ms
step:310/2330 train_time:17826ms step_avg:57.50ms
step:311/2330 train_time:17882ms step_avg:57.50ms
step:312/2330 train_time:17941ms step_avg:57.50ms
step:313/2330 train_time:17997ms step_avg:57.50ms
step:314/2330 train_time:18055ms step_avg:57.50ms
step:315/2330 train_time:18111ms step_avg:57.49ms
step:316/2330 train_time:18170ms step_avg:57.50ms
step:317/2330 train_time:18226ms step_avg:57.50ms
step:318/2330 train_time:18285ms step_avg:57.50ms
step:319/2330 train_time:18341ms step_avg:57.50ms
step:320/2330 train_time:18400ms step_avg:57.50ms
step:321/2330 train_time:18456ms step_avg:57.50ms
step:322/2330 train_time:18514ms step_avg:57.50ms
step:323/2330 train_time:18571ms step_avg:57.50ms
step:324/2330 train_time:18631ms step_avg:57.50ms
step:325/2330 train_time:18687ms step_avg:57.50ms
step:326/2330 train_time:18747ms step_avg:57.50ms
step:327/2330 train_time:18803ms step_avg:57.50ms
step:328/2330 train_time:18862ms step_avg:57.50ms
step:329/2330 train_time:18917ms step_avg:57.50ms
step:330/2330 train_time:18976ms step_avg:57.50ms
step:331/2330 train_time:19032ms step_avg:57.50ms
step:332/2330 train_time:19091ms step_avg:57.50ms
step:333/2330 train_time:19147ms step_avg:57.50ms
step:334/2330 train_time:19206ms step_avg:57.50ms
step:335/2330 train_time:19261ms step_avg:57.50ms
step:336/2330 train_time:19321ms step_avg:57.50ms
step:337/2330 train_time:19378ms step_avg:57.50ms
step:338/2330 train_time:19437ms step_avg:57.51ms
step:339/2330 train_time:19494ms step_avg:57.50ms
step:340/2330 train_time:19552ms step_avg:57.51ms
step:341/2330 train_time:19607ms step_avg:57.50ms
step:342/2330 train_time:19667ms step_avg:57.51ms
step:343/2330 train_time:19723ms step_avg:57.50ms
step:344/2330 train_time:19783ms step_avg:57.51ms
step:345/2330 train_time:19839ms step_avg:57.50ms
step:346/2330 train_time:19899ms step_avg:57.51ms
step:347/2330 train_time:19955ms step_avg:57.51ms
step:348/2330 train_time:20013ms step_avg:57.51ms
step:349/2330 train_time:20069ms step_avg:57.50ms
step:350/2330 train_time:20129ms step_avg:57.51ms
step:351/2330 train_time:20185ms step_avg:57.51ms
step:352/2330 train_time:20245ms step_avg:57.51ms
step:353/2330 train_time:20301ms step_avg:57.51ms
step:354/2330 train_time:20360ms step_avg:57.51ms
step:355/2330 train_time:20416ms step_avg:57.51ms
step:356/2330 train_time:20475ms step_avg:57.51ms
step:357/2330 train_time:20531ms step_avg:57.51ms
step:358/2330 train_time:20590ms step_avg:57.51ms
step:359/2330 train_time:20646ms step_avg:57.51ms
step:360/2330 train_time:20705ms step_avg:57.51ms
step:361/2330 train_time:20762ms step_avg:57.51ms
step:362/2330 train_time:20821ms step_avg:57.52ms
step:363/2330 train_time:20877ms step_avg:57.51ms
step:364/2330 train_time:20936ms step_avg:57.52ms
step:365/2330 train_time:20992ms step_avg:57.51ms
step:366/2330 train_time:21050ms step_avg:57.51ms
step:367/2330 train_time:21105ms step_avg:57.51ms
step:368/2330 train_time:21166ms step_avg:57.52ms
step:369/2330 train_time:21221ms step_avg:57.51ms
step:370/2330 train_time:21281ms step_avg:57.52ms
step:371/2330 train_time:21337ms step_avg:57.51ms
step:372/2330 train_time:21396ms step_avg:57.52ms
step:373/2330 train_time:21452ms step_avg:57.51ms
step:374/2330 train_time:21511ms step_avg:57.52ms
step:375/2330 train_time:21568ms step_avg:57.51ms
step:376/2330 train_time:21627ms step_avg:57.52ms
step:377/2330 train_time:21683ms step_avg:57.52ms
step:378/2330 train_time:21742ms step_avg:57.52ms
step:379/2330 train_time:21798ms step_avg:57.51ms
step:380/2330 train_time:21857ms step_avg:57.52ms
step:381/2330 train_time:21913ms step_avg:57.51ms
step:382/2330 train_time:21972ms step_avg:57.52ms
step:383/2330 train_time:22029ms step_avg:57.52ms
step:384/2330 train_time:22088ms step_avg:57.52ms
step:385/2330 train_time:22144ms step_avg:57.52ms
step:386/2330 train_time:22202ms step_avg:57.52ms
step:387/2330 train_time:22258ms step_avg:57.51ms
step:388/2330 train_time:22318ms step_avg:57.52ms
step:389/2330 train_time:22374ms step_avg:57.52ms
step:390/2330 train_time:22432ms step_avg:57.52ms
step:391/2330 train_time:22488ms step_avg:57.51ms
step:392/2330 train_time:22548ms step_avg:57.52ms
step:393/2330 train_time:22604ms step_avg:57.52ms
step:394/2330 train_time:22663ms step_avg:57.52ms
step:395/2330 train_time:22719ms step_avg:57.52ms
step:396/2330 train_time:22777ms step_avg:57.52ms
step:397/2330 train_time:22833ms step_avg:57.51ms
step:398/2330 train_time:22892ms step_avg:57.52ms
step:399/2330 train_time:22948ms step_avg:57.51ms
step:400/2330 train_time:23007ms step_avg:57.52ms
step:401/2330 train_time:23063ms step_avg:57.51ms
step:402/2330 train_time:23122ms step_avg:57.52ms
step:403/2330 train_time:23178ms step_avg:57.51ms
step:404/2330 train_time:23237ms step_avg:57.52ms
step:405/2330 train_time:23292ms step_avg:57.51ms
step:406/2330 train_time:23352ms step_avg:57.52ms
step:407/2330 train_time:23407ms step_avg:57.51ms
step:408/2330 train_time:23468ms step_avg:57.52ms
step:409/2330 train_time:23524ms step_avg:57.52ms
step:410/2330 train_time:23584ms step_avg:57.52ms
step:411/2330 train_time:23640ms step_avg:57.52ms
step:412/2330 train_time:23700ms step_avg:57.52ms
step:413/2330 train_time:23756ms step_avg:57.52ms
step:414/2330 train_time:23815ms step_avg:57.52ms
step:415/2330 train_time:23871ms step_avg:57.52ms
step:416/2330 train_time:23930ms step_avg:57.52ms
step:417/2330 train_time:23986ms step_avg:57.52ms
step:418/2330 train_time:24046ms step_avg:57.53ms
step:419/2330 train_time:24102ms step_avg:57.52ms
step:420/2330 train_time:24161ms step_avg:57.53ms
step:421/2330 train_time:24217ms step_avg:57.52ms
step:422/2330 train_time:24276ms step_avg:57.52ms
step:423/2330 train_time:24332ms step_avg:57.52ms
step:424/2330 train_time:24391ms step_avg:57.53ms
step:425/2330 train_time:24447ms step_avg:57.52ms
step:426/2330 train_time:24506ms step_avg:57.53ms
step:427/2330 train_time:24562ms step_avg:57.52ms
step:428/2330 train_time:24622ms step_avg:57.53ms
step:429/2330 train_time:24678ms step_avg:57.52ms
step:430/2330 train_time:24737ms step_avg:57.53ms
step:431/2330 train_time:24794ms step_avg:57.53ms
step:432/2330 train_time:24852ms step_avg:57.53ms
step:433/2330 train_time:24907ms step_avg:57.52ms
step:434/2330 train_time:24968ms step_avg:57.53ms
step:435/2330 train_time:25024ms step_avg:57.53ms
step:436/2330 train_time:25084ms step_avg:57.53ms
step:437/2330 train_time:25140ms step_avg:57.53ms
step:438/2330 train_time:25201ms step_avg:57.54ms
step:439/2330 train_time:25257ms step_avg:57.53ms
step:440/2330 train_time:25316ms step_avg:57.54ms
step:441/2330 train_time:25371ms step_avg:57.53ms
step:442/2330 train_time:25432ms step_avg:57.54ms
step:443/2330 train_time:25488ms step_avg:57.54ms
step:444/2330 train_time:25547ms step_avg:57.54ms
step:445/2330 train_time:25603ms step_avg:57.53ms
step:446/2330 train_time:25663ms step_avg:57.54ms
step:447/2330 train_time:25720ms step_avg:57.54ms
step:448/2330 train_time:25778ms step_avg:57.54ms
step:449/2330 train_time:25835ms step_avg:57.54ms
step:450/2330 train_time:25894ms step_avg:57.54ms
step:451/2330 train_time:25949ms step_avg:57.54ms
step:452/2330 train_time:26008ms step_avg:57.54ms
step:453/2330 train_time:26065ms step_avg:57.54ms
step:454/2330 train_time:26125ms step_avg:57.54ms
step:455/2330 train_time:26182ms step_avg:57.54ms
step:456/2330 train_time:26241ms step_avg:57.55ms
step:457/2330 train_time:26297ms step_avg:57.54ms
step:458/2330 train_time:26356ms step_avg:57.55ms
step:459/2330 train_time:26412ms step_avg:57.54ms
step:460/2330 train_time:26473ms step_avg:57.55ms
step:461/2330 train_time:26528ms step_avg:57.55ms
step:462/2330 train_time:26587ms step_avg:57.55ms
step:463/2330 train_time:26644ms step_avg:57.55ms
step:464/2330 train_time:26703ms step_avg:57.55ms
step:465/2330 train_time:26759ms step_avg:57.55ms
step:466/2330 train_time:26818ms step_avg:57.55ms
step:467/2330 train_time:26873ms step_avg:57.54ms
step:468/2330 train_time:26933ms step_avg:57.55ms
step:469/2330 train_time:26989ms step_avg:57.55ms
step:470/2330 train_time:27048ms step_avg:57.55ms
step:471/2330 train_time:27104ms step_avg:57.55ms
step:472/2330 train_time:27163ms step_avg:57.55ms
step:473/2330 train_time:27219ms step_avg:57.55ms
step:474/2330 train_time:27278ms step_avg:57.55ms
step:475/2330 train_time:27334ms step_avg:57.55ms
step:476/2330 train_time:27393ms step_avg:57.55ms
step:477/2330 train_time:27448ms step_avg:57.54ms
step:478/2330 train_time:27508ms step_avg:57.55ms
step:479/2330 train_time:27564ms step_avg:57.55ms
step:480/2330 train_time:27623ms step_avg:57.55ms
step:481/2330 train_time:27679ms step_avg:57.55ms
step:482/2330 train_time:27738ms step_avg:57.55ms
step:483/2330 train_time:27794ms step_avg:57.54ms
step:484/2330 train_time:27852ms step_avg:57.55ms
step:485/2330 train_time:27908ms step_avg:57.54ms
step:486/2330 train_time:27967ms step_avg:57.55ms
step:487/2330 train_time:28023ms step_avg:57.54ms
step:488/2330 train_time:28082ms step_avg:57.55ms
step:489/2330 train_time:28139ms step_avg:57.54ms
step:490/2330 train_time:28198ms step_avg:57.55ms
step:491/2330 train_time:28254ms step_avg:57.54ms
step:492/2330 train_time:28313ms step_avg:57.55ms
step:493/2330 train_time:28369ms step_avg:57.54ms
step:494/2330 train_time:28429ms step_avg:57.55ms
step:495/2330 train_time:28485ms step_avg:57.55ms
step:496/2330 train_time:28544ms step_avg:57.55ms
step:497/2330 train_time:28600ms step_avg:57.55ms
step:498/2330 train_time:28659ms step_avg:57.55ms
step:499/2330 train_time:28715ms step_avg:57.55ms
step:500/2330 train_time:28775ms step_avg:57.55ms
step:500/2330 val_loss:4.4005 train_time:28854ms step_avg:57.71ms
step:501/2330 train_time:28871ms step_avg:57.63ms
step:502/2330 train_time:28891ms step_avg:57.55ms
step:503/2330 train_time:28946ms step_avg:57.55ms
step:504/2330 train_time:29012ms step_avg:57.56ms
step:505/2330 train_time:29068ms step_avg:57.56ms
step:506/2330 train_time:29130ms step_avg:57.57ms
step:507/2330 train_time:29187ms step_avg:57.57ms
step:508/2330 train_time:29247ms step_avg:57.57ms
step:509/2330 train_time:29303ms step_avg:57.57ms
step:510/2330 train_time:29361ms step_avg:57.57ms
step:511/2330 train_time:29416ms step_avg:57.57ms
step:512/2330 train_time:29475ms step_avg:57.57ms
step:513/2330 train_time:29531ms step_avg:57.57ms
step:514/2330 train_time:29589ms step_avg:57.57ms
step:515/2330 train_time:29645ms step_avg:57.56ms
step:516/2330 train_time:29703ms step_avg:57.56ms
step:517/2330 train_time:29759ms step_avg:57.56ms
step:518/2330 train_time:29818ms step_avg:57.56ms
step:519/2330 train_time:29874ms step_avg:57.56ms
step:520/2330 train_time:29937ms step_avg:57.57ms
step:521/2330 train_time:29993ms step_avg:57.57ms
step:522/2330 train_time:30055ms step_avg:57.58ms
step:523/2330 train_time:30112ms step_avg:57.58ms
step:524/2330 train_time:30171ms step_avg:57.58ms
step:525/2330 train_time:30228ms step_avg:57.58ms
step:526/2330 train_time:30288ms step_avg:57.58ms
step:527/2330 train_time:30344ms step_avg:57.58ms
step:528/2330 train_time:30403ms step_avg:57.58ms
step:529/2330 train_time:30458ms step_avg:57.58ms
step:530/2330 train_time:30517ms step_avg:57.58ms
step:531/2330 train_time:30572ms step_avg:57.57ms
step:532/2330 train_time:30631ms step_avg:57.58ms
step:533/2330 train_time:30687ms step_avg:57.57ms
step:534/2330 train_time:30746ms step_avg:57.58ms
step:535/2330 train_time:30802ms step_avg:57.57ms
step:536/2330 train_time:30861ms step_avg:57.58ms
step:537/2330 train_time:30917ms step_avg:57.57ms
step:538/2330 train_time:30977ms step_avg:57.58ms
step:539/2330 train_time:31034ms step_avg:57.58ms
step:540/2330 train_time:31095ms step_avg:57.58ms
step:541/2330 train_time:31151ms step_avg:57.58ms
step:542/2330 train_time:31212ms step_avg:57.59ms
step:543/2330 train_time:31268ms step_avg:57.58ms
step:544/2330 train_time:31327ms step_avg:57.59ms
step:545/2330 train_time:31383ms step_avg:57.58ms
step:546/2330 train_time:31442ms step_avg:57.59ms
step:547/2330 train_time:31498ms step_avg:57.58ms
step:548/2330 train_time:31557ms step_avg:57.59ms
step:549/2330 train_time:31613ms step_avg:57.58ms
step:550/2330 train_time:31671ms step_avg:57.58ms
step:551/2330 train_time:31727ms step_avg:57.58ms
step:552/2330 train_time:31786ms step_avg:57.58ms
step:553/2330 train_time:31843ms step_avg:57.58ms
step:554/2330 train_time:31902ms step_avg:57.58ms
step:555/2330 train_time:31957ms step_avg:57.58ms
step:556/2330 train_time:32018ms step_avg:57.59ms
step:557/2330 train_time:32074ms step_avg:57.58ms
step:558/2330 train_time:32135ms step_avg:57.59ms
step:559/2330 train_time:32192ms step_avg:57.59ms
step:560/2330 train_time:32251ms step_avg:57.59ms
step:561/2330 train_time:32308ms step_avg:57.59ms
step:562/2330 train_time:32368ms step_avg:57.59ms
step:563/2330 train_time:32424ms step_avg:57.59ms
step:564/2330 train_time:32483ms step_avg:57.59ms
step:565/2330 train_time:32539ms step_avg:57.59ms
step:566/2330 train_time:32597ms step_avg:57.59ms
step:567/2330 train_time:32653ms step_avg:57.59ms
step:568/2330 train_time:32713ms step_avg:57.59ms
step:569/2330 train_time:32768ms step_avg:57.59ms
step:570/2330 train_time:32828ms step_avg:57.59ms
step:571/2330 train_time:32884ms step_avg:57.59ms
step:572/2330 train_time:32943ms step_avg:57.59ms
step:573/2330 train_time:32999ms step_avg:57.59ms
step:574/2330 train_time:33059ms step_avg:57.59ms
step:575/2330 train_time:33115ms step_avg:57.59ms
step:576/2330 train_time:33176ms step_avg:57.60ms
step:577/2330 train_time:33232ms step_avg:57.59ms
step:578/2330 train_time:33291ms step_avg:57.60ms
step:579/2330 train_time:33347ms step_avg:57.59ms
step:580/2330 train_time:33406ms step_avg:57.60ms
step:581/2330 train_time:33462ms step_avg:57.59ms
step:582/2330 train_time:33520ms step_avg:57.60ms
step:583/2330 train_time:33576ms step_avg:57.59ms
step:584/2330 train_time:33635ms step_avg:57.59ms
step:585/2330 train_time:33691ms step_avg:57.59ms
step:586/2330 train_time:33750ms step_avg:57.59ms
step:587/2330 train_time:33806ms step_avg:57.59ms
step:588/2330 train_time:33866ms step_avg:57.60ms
step:589/2330 train_time:33922ms step_avg:57.59ms
step:590/2330 train_time:33982ms step_avg:57.60ms
step:591/2330 train_time:34038ms step_avg:57.59ms
step:592/2330 train_time:34097ms step_avg:57.60ms
step:593/2330 train_time:34153ms step_avg:57.59ms
step:594/2330 train_time:34213ms step_avg:57.60ms
step:595/2330 train_time:34269ms step_avg:57.60ms
step:596/2330 train_time:34329ms step_avg:57.60ms
step:597/2330 train_time:34385ms step_avg:57.60ms
step:598/2330 train_time:34444ms step_avg:57.60ms
step:599/2330 train_time:34500ms step_avg:57.60ms
step:600/2330 train_time:34559ms step_avg:57.60ms
step:601/2330 train_time:34616ms step_avg:57.60ms
step:602/2330 train_time:34675ms step_avg:57.60ms
step:603/2330 train_time:34732ms step_avg:57.60ms
step:604/2330 train_time:34791ms step_avg:57.60ms
step:605/2330 train_time:34848ms step_avg:57.60ms
step:606/2330 train_time:34906ms step_avg:57.60ms
step:607/2330 train_time:34962ms step_avg:57.60ms
step:608/2330 train_time:35021ms step_avg:57.60ms
step:609/2330 train_time:35077ms step_avg:57.60ms
step:610/2330 train_time:35137ms step_avg:57.60ms
step:611/2330 train_time:35194ms step_avg:57.60ms
step:612/2330 train_time:35253ms step_avg:57.60ms
step:613/2330 train_time:35309ms step_avg:57.60ms
step:614/2330 train_time:35368ms step_avg:57.60ms
step:615/2330 train_time:35424ms step_avg:57.60ms
step:616/2330 train_time:35484ms step_avg:57.60ms
step:617/2330 train_time:35540ms step_avg:57.60ms
step:618/2330 train_time:35600ms step_avg:57.61ms
step:619/2330 train_time:35655ms step_avg:57.60ms
step:620/2330 train_time:35715ms step_avg:57.60ms
step:621/2330 train_time:35771ms step_avg:57.60ms
step:622/2330 train_time:35830ms step_avg:57.61ms
step:623/2330 train_time:35887ms step_avg:57.60ms
step:624/2330 train_time:35946ms step_avg:57.61ms
step:625/2330 train_time:36002ms step_avg:57.60ms
step:626/2330 train_time:36061ms step_avg:57.61ms
step:627/2330 train_time:36117ms step_avg:57.60ms
step:628/2330 train_time:36177ms step_avg:57.61ms
step:629/2330 train_time:36234ms step_avg:57.61ms
step:630/2330 train_time:36293ms step_avg:57.61ms
step:631/2330 train_time:36349ms step_avg:57.61ms
step:632/2330 train_time:36408ms step_avg:57.61ms
step:633/2330 train_time:36464ms step_avg:57.61ms
step:634/2330 train_time:36524ms step_avg:57.61ms
step:635/2330 train_time:36580ms step_avg:57.61ms
step:636/2330 train_time:36638ms step_avg:57.61ms
step:637/2330 train_time:36694ms step_avg:57.61ms
step:638/2330 train_time:36753ms step_avg:57.61ms
step:639/2330 train_time:36810ms step_avg:57.61ms
step:640/2330 train_time:36869ms step_avg:57.61ms
step:641/2330 train_time:36925ms step_avg:57.61ms
step:642/2330 train_time:36984ms step_avg:57.61ms
step:643/2330 train_time:37041ms step_avg:57.61ms
step:644/2330 train_time:37100ms step_avg:57.61ms
step:645/2330 train_time:37155ms step_avg:57.60ms
step:646/2330 train_time:37215ms step_avg:57.61ms
step:647/2330 train_time:37272ms step_avg:57.61ms
step:648/2330 train_time:37331ms step_avg:57.61ms
step:649/2330 train_time:37388ms step_avg:57.61ms
step:650/2330 train_time:37447ms step_avg:57.61ms
step:651/2330 train_time:37503ms step_avg:57.61ms
step:652/2330 train_time:37563ms step_avg:57.61ms
step:653/2330 train_time:37619ms step_avg:57.61ms
step:654/2330 train_time:37677ms step_avg:57.61ms
step:655/2330 train_time:37733ms step_avg:57.61ms
step:656/2330 train_time:37793ms step_avg:57.61ms
step:657/2330 train_time:37849ms step_avg:57.61ms
step:658/2330 train_time:37908ms step_avg:57.61ms
step:659/2330 train_time:37965ms step_avg:57.61ms
step:660/2330 train_time:38024ms step_avg:57.61ms
step:661/2330 train_time:38079ms step_avg:57.61ms
step:662/2330 train_time:38139ms step_avg:57.61ms
step:663/2330 train_time:38195ms step_avg:57.61ms
step:664/2330 train_time:38255ms step_avg:57.61ms
step:665/2330 train_time:38312ms step_avg:57.61ms
step:666/2330 train_time:38371ms step_avg:57.61ms
step:667/2330 train_time:38427ms step_avg:57.61ms
step:668/2330 train_time:38487ms step_avg:57.61ms
step:669/2330 train_time:38543ms step_avg:57.61ms
step:670/2330 train_time:38603ms step_avg:57.62ms
step:671/2330 train_time:38658ms step_avg:57.61ms
step:672/2330 train_time:38718ms step_avg:57.62ms
step:673/2330 train_time:38773ms step_avg:57.61ms
step:674/2330 train_time:38833ms step_avg:57.62ms
step:675/2330 train_time:38889ms step_avg:57.61ms
step:676/2330 train_time:38949ms step_avg:57.62ms
step:677/2330 train_time:39005ms step_avg:57.61ms
step:678/2330 train_time:39064ms step_avg:57.62ms
step:679/2330 train_time:39120ms step_avg:57.61ms
step:680/2330 train_time:39179ms step_avg:57.62ms
step:681/2330 train_time:39235ms step_avg:57.61ms
step:682/2330 train_time:39295ms step_avg:57.62ms
step:683/2330 train_time:39350ms step_avg:57.61ms
step:684/2330 train_time:39411ms step_avg:57.62ms
step:685/2330 train_time:39467ms step_avg:57.62ms
step:686/2330 train_time:39526ms step_avg:57.62ms
step:687/2330 train_time:39582ms step_avg:57.62ms
step:688/2330 train_time:39642ms step_avg:57.62ms
step:689/2330 train_time:39698ms step_avg:57.62ms
step:690/2330 train_time:39757ms step_avg:57.62ms
step:691/2330 train_time:39813ms step_avg:57.62ms
step:692/2330 train_time:39872ms step_avg:57.62ms
step:693/2330 train_time:39928ms step_avg:57.62ms
step:694/2330 train_time:39987ms step_avg:57.62ms
step:695/2330 train_time:40043ms step_avg:57.62ms
step:696/2330 train_time:40102ms step_avg:57.62ms
step:697/2330 train_time:40158ms step_avg:57.62ms
step:698/2330 train_time:40218ms step_avg:57.62ms
step:699/2330 train_time:40273ms step_avg:57.62ms
step:700/2330 train_time:40334ms step_avg:57.62ms
step:701/2330 train_time:40390ms step_avg:57.62ms
step:702/2330 train_time:40450ms step_avg:57.62ms
step:703/2330 train_time:40506ms step_avg:57.62ms
step:704/2330 train_time:40565ms step_avg:57.62ms
step:705/2330 train_time:40622ms step_avg:57.62ms
step:706/2330 train_time:40681ms step_avg:57.62ms
step:707/2330 train_time:40738ms step_avg:57.62ms
step:708/2330 train_time:40797ms step_avg:57.62ms
step:709/2330 train_time:40852ms step_avg:57.62ms
step:710/2330 train_time:40912ms step_avg:57.62ms
step:711/2330 train_time:40968ms step_avg:57.62ms
step:712/2330 train_time:41027ms step_avg:57.62ms
step:713/2330 train_time:41083ms step_avg:57.62ms
step:714/2330 train_time:41144ms step_avg:57.62ms
step:715/2330 train_time:41200ms step_avg:57.62ms
step:716/2330 train_time:41259ms step_avg:57.62ms
step:717/2330 train_time:41315ms step_avg:57.62ms
step:718/2330 train_time:41375ms step_avg:57.63ms
step:719/2330 train_time:41431ms step_avg:57.62ms
step:720/2330 train_time:41490ms step_avg:57.63ms
step:721/2330 train_time:41546ms step_avg:57.62ms
step:722/2330 train_time:41605ms step_avg:57.63ms
step:723/2330 train_time:41661ms step_avg:57.62ms
step:724/2330 train_time:41720ms step_avg:57.62ms
step:725/2330 train_time:41776ms step_avg:57.62ms
step:726/2330 train_time:41835ms step_avg:57.62ms
step:727/2330 train_time:41891ms step_avg:57.62ms
step:728/2330 train_time:41950ms step_avg:57.62ms
step:729/2330 train_time:42007ms step_avg:57.62ms
step:730/2330 train_time:42066ms step_avg:57.62ms
step:731/2330 train_time:42122ms step_avg:57.62ms
step:732/2330 train_time:42181ms step_avg:57.62ms
step:733/2330 train_time:42237ms step_avg:57.62ms
step:734/2330 train_time:42298ms step_avg:57.63ms
step:735/2330 train_time:42354ms step_avg:57.62ms
step:736/2330 train_time:42414ms step_avg:57.63ms
step:737/2330 train_time:42470ms step_avg:57.63ms
step:738/2330 train_time:42529ms step_avg:57.63ms
step:739/2330 train_time:42586ms step_avg:57.63ms
step:740/2330 train_time:42645ms step_avg:57.63ms
step:741/2330 train_time:42701ms step_avg:57.63ms
step:742/2330 train_time:42761ms step_avg:57.63ms
step:743/2330 train_time:42817ms step_avg:57.63ms
step:744/2330 train_time:42876ms step_avg:57.63ms
step:745/2330 train_time:42932ms step_avg:57.63ms
step:746/2330 train_time:42993ms step_avg:57.63ms
step:747/2330 train_time:43049ms step_avg:57.63ms
step:748/2330 train_time:43109ms step_avg:57.63ms
step:749/2330 train_time:43164ms step_avg:57.63ms
step:750/2330 train_time:43224ms step_avg:57.63ms
step:750/2330 val_loss:4.2112 train_time:43304ms step_avg:57.74ms
step:751/2330 train_time:43322ms step_avg:57.69ms
step:752/2330 train_time:43342ms step_avg:57.64ms
step:753/2330 train_time:43400ms step_avg:57.64ms
step:754/2330 train_time:43466ms step_avg:57.65ms
step:755/2330 train_time:43522ms step_avg:57.65ms
step:756/2330 train_time:43583ms step_avg:57.65ms
step:757/2330 train_time:43639ms step_avg:57.65ms
step:758/2330 train_time:43700ms step_avg:57.65ms
step:759/2330 train_time:43755ms step_avg:57.65ms
step:760/2330 train_time:43814ms step_avg:57.65ms
step:761/2330 train_time:43869ms step_avg:57.65ms
step:762/2330 train_time:43928ms step_avg:57.65ms
step:763/2330 train_time:43983ms step_avg:57.64ms
step:764/2330 train_time:44042ms step_avg:57.65ms
step:765/2330 train_time:44098ms step_avg:57.64ms
step:766/2330 train_time:44156ms step_avg:57.64ms
step:767/2330 train_time:44212ms step_avg:57.64ms
step:768/2330 train_time:44272ms step_avg:57.65ms
step:769/2330 train_time:44330ms step_avg:57.65ms
step:770/2330 train_time:44395ms step_avg:57.66ms
step:771/2330 train_time:44454ms step_avg:57.66ms
step:772/2330 train_time:44515ms step_avg:57.66ms
step:773/2330 train_time:44573ms step_avg:57.66ms
step:774/2330 train_time:44633ms step_avg:57.67ms
step:775/2330 train_time:44690ms step_avg:57.66ms
step:776/2330 train_time:44751ms step_avg:57.67ms
step:777/2330 train_time:44807ms step_avg:57.67ms
step:778/2330 train_time:44867ms step_avg:57.67ms
step:779/2330 train_time:44924ms step_avg:57.67ms
step:780/2330 train_time:44982ms step_avg:57.67ms
step:781/2330 train_time:45038ms step_avg:57.67ms
step:782/2330 train_time:45098ms step_avg:57.67ms
step:783/2330 train_time:45154ms step_avg:57.67ms
step:784/2330 train_time:45213ms step_avg:57.67ms
step:785/2330 train_time:45269ms step_avg:57.67ms
step:786/2330 train_time:45330ms step_avg:57.67ms
step:787/2330 train_time:45388ms step_avg:57.67ms
step:788/2330 train_time:45449ms step_avg:57.68ms
step:789/2330 train_time:45505ms step_avg:57.67ms
step:790/2330 train_time:45568ms step_avg:57.68ms
step:791/2330 train_time:45625ms step_avg:57.68ms
step:792/2330 train_time:45685ms step_avg:57.68ms
step:793/2330 train_time:45741ms step_avg:57.68ms
step:794/2330 train_time:45802ms step_avg:57.68ms
step:795/2330 train_time:45858ms step_avg:57.68ms
step:796/2330 train_time:45918ms step_avg:57.69ms
step:797/2330 train_time:45975ms step_avg:57.68ms
step:798/2330 train_time:46035ms step_avg:57.69ms
step:799/2330 train_time:46091ms step_avg:57.69ms
step:800/2330 train_time:46150ms step_avg:57.69ms
step:801/2330 train_time:46206ms step_avg:57.69ms
step:802/2330 train_time:46267ms step_avg:57.69ms
step:803/2330 train_time:46323ms step_avg:57.69ms
step:804/2330 train_time:46385ms step_avg:57.69ms
step:805/2330 train_time:46442ms step_avg:57.69ms
step:806/2330 train_time:46503ms step_avg:57.70ms
step:807/2330 train_time:46560ms step_avg:57.69ms
step:808/2330 train_time:46622ms step_avg:57.70ms
step:809/2330 train_time:46679ms step_avg:57.70ms
step:810/2330 train_time:46740ms step_avg:57.70ms
step:811/2330 train_time:46796ms step_avg:57.70ms
step:812/2330 train_time:46858ms step_avg:57.71ms
step:813/2330 train_time:46914ms step_avg:57.70ms
step:814/2330 train_time:46974ms step_avg:57.71ms
step:815/2330 train_time:47031ms step_avg:57.71ms
step:816/2330 train_time:47090ms step_avg:57.71ms
step:817/2330 train_time:47146ms step_avg:57.71ms
step:818/2330 train_time:47206ms step_avg:57.71ms
step:819/2330 train_time:47262ms step_avg:57.71ms
step:820/2330 train_time:47322ms step_avg:57.71ms
step:821/2330 train_time:47380ms step_avg:57.71ms
step:822/2330 train_time:47441ms step_avg:57.71ms
step:823/2330 train_time:47497ms step_avg:57.71ms
step:824/2330 train_time:47559ms step_avg:57.72ms
step:825/2330 train_time:47616ms step_avg:57.72ms
step:826/2330 train_time:47677ms step_avg:57.72ms
step:827/2330 train_time:47734ms step_avg:57.72ms
step:828/2330 train_time:47794ms step_avg:57.72ms
step:829/2330 train_time:47852ms step_avg:57.72ms
step:830/2330 train_time:47912ms step_avg:57.72ms
step:831/2330 train_time:47969ms step_avg:57.72ms
step:832/2330 train_time:48029ms step_avg:57.73ms
step:833/2330 train_time:48086ms step_avg:57.73ms
step:834/2330 train_time:48146ms step_avg:57.73ms
step:835/2330 train_time:48202ms step_avg:57.73ms
step:836/2330 train_time:48262ms step_avg:57.73ms
step:837/2330 train_time:48319ms step_avg:57.73ms
step:838/2330 train_time:48379ms step_avg:57.73ms
step:839/2330 train_time:48436ms step_avg:57.73ms
step:840/2330 train_time:48496ms step_avg:57.73ms
step:841/2330 train_time:48553ms step_avg:57.73ms
step:842/2330 train_time:48614ms step_avg:57.74ms
step:843/2330 train_time:48670ms step_avg:57.73ms
step:844/2330 train_time:48732ms step_avg:57.74ms
step:845/2330 train_time:48790ms step_avg:57.74ms
step:846/2330 train_time:48849ms step_avg:57.74ms
step:847/2330 train_time:48905ms step_avg:57.74ms
step:848/2330 train_time:48966ms step_avg:57.74ms
step:849/2330 train_time:49022ms step_avg:57.74ms
step:850/2330 train_time:49082ms step_avg:57.74ms
step:851/2330 train_time:49138ms step_avg:57.74ms
step:852/2330 train_time:49199ms step_avg:57.75ms
step:853/2330 train_time:49255ms step_avg:57.74ms
step:854/2330 train_time:49316ms step_avg:57.75ms
step:855/2330 train_time:49372ms step_avg:57.74ms
step:856/2330 train_time:49433ms step_avg:57.75ms
step:857/2330 train_time:49490ms step_avg:57.75ms
step:858/2330 train_time:49551ms step_avg:57.75ms
step:859/2330 train_time:49607ms step_avg:57.75ms
step:860/2330 train_time:49667ms step_avg:57.75ms
step:861/2330 train_time:49725ms step_avg:57.75ms
step:862/2330 train_time:49785ms step_avg:57.76ms
step:863/2330 train_time:49842ms step_avg:57.75ms
step:864/2330 train_time:49902ms step_avg:57.76ms
step:865/2330 train_time:49959ms step_avg:57.76ms
step:866/2330 train_time:50020ms step_avg:57.76ms
step:867/2330 train_time:50077ms step_avg:57.76ms
step:868/2330 train_time:50136ms step_avg:57.76ms
step:869/2330 train_time:50193ms step_avg:57.76ms
step:870/2330 train_time:50254ms step_avg:57.76ms
step:871/2330 train_time:50310ms step_avg:57.76ms
step:872/2330 train_time:50371ms step_avg:57.77ms
step:873/2330 train_time:50428ms step_avg:57.76ms
step:874/2330 train_time:50489ms step_avg:57.77ms
step:875/2330 train_time:50545ms step_avg:57.77ms
step:876/2330 train_time:50605ms step_avg:57.77ms
step:877/2330 train_time:50663ms step_avg:57.77ms
step:878/2330 train_time:50724ms step_avg:57.77ms
step:879/2330 train_time:50780ms step_avg:57.77ms
step:880/2330 train_time:50842ms step_avg:57.77ms
step:881/2330 train_time:50898ms step_avg:57.77ms
step:882/2330 train_time:50959ms step_avg:57.78ms
step:883/2330 train_time:51016ms step_avg:57.78ms
step:884/2330 train_time:51076ms step_avg:57.78ms
step:885/2330 train_time:51133ms step_avg:57.78ms
step:886/2330 train_time:51193ms step_avg:57.78ms
step:887/2330 train_time:51250ms step_avg:57.78ms
step:888/2330 train_time:51310ms step_avg:57.78ms
step:889/2330 train_time:51367ms step_avg:57.78ms
step:890/2330 train_time:51428ms step_avg:57.78ms
step:891/2330 train_time:51484ms step_avg:57.78ms
step:892/2330 train_time:51544ms step_avg:57.79ms
step:893/2330 train_time:51601ms step_avg:57.78ms
step:894/2330 train_time:51661ms step_avg:57.79ms
step:895/2330 train_time:51718ms step_avg:57.79ms
step:896/2330 train_time:51779ms step_avg:57.79ms
step:897/2330 train_time:51835ms step_avg:57.79ms
step:898/2330 train_time:51896ms step_avg:57.79ms
step:899/2330 train_time:51953ms step_avg:57.79ms
step:900/2330 train_time:52013ms step_avg:57.79ms
step:901/2330 train_time:52069ms step_avg:57.79ms
step:902/2330 train_time:52129ms step_avg:57.79ms
step:903/2330 train_time:52186ms step_avg:57.79ms
step:904/2330 train_time:52246ms step_avg:57.79ms
step:905/2330 train_time:52302ms step_avg:57.79ms
step:906/2330 train_time:52363ms step_avg:57.80ms
step:907/2330 train_time:52419ms step_avg:57.79ms
step:908/2330 train_time:52480ms step_avg:57.80ms
step:909/2330 train_time:52537ms step_avg:57.80ms
step:910/2330 train_time:52597ms step_avg:57.80ms
step:911/2330 train_time:52654ms step_avg:57.80ms
step:912/2330 train_time:52714ms step_avg:57.80ms
step:913/2330 train_time:52770ms step_avg:57.80ms
step:914/2330 train_time:52831ms step_avg:57.80ms
step:915/2330 train_time:52888ms step_avg:57.80ms
step:916/2330 train_time:52949ms step_avg:57.80ms
step:917/2330 train_time:53005ms step_avg:57.80ms
step:918/2330 train_time:53066ms step_avg:57.81ms
step:919/2330 train_time:53122ms step_avg:57.80ms
step:920/2330 train_time:53183ms step_avg:57.81ms
step:921/2330 train_time:53240ms step_avg:57.81ms
step:922/2330 train_time:53300ms step_avg:57.81ms
step:923/2330 train_time:53357ms step_avg:57.81ms
step:924/2330 train_time:53418ms step_avg:57.81ms
step:925/2330 train_time:53474ms step_avg:57.81ms
step:926/2330 train_time:53535ms step_avg:57.81ms
step:927/2330 train_time:53592ms step_avg:57.81ms
step:928/2330 train_time:53652ms step_avg:57.81ms
step:929/2330 train_time:53708ms step_avg:57.81ms
step:930/2330 train_time:53768ms step_avg:57.81ms
step:931/2330 train_time:53824ms step_avg:57.81ms
step:932/2330 train_time:53885ms step_avg:57.82ms
step:933/2330 train_time:53942ms step_avg:57.82ms
step:934/2330 train_time:54003ms step_avg:57.82ms
step:935/2330 train_time:54060ms step_avg:57.82ms
step:936/2330 train_time:54121ms step_avg:57.82ms
step:937/2330 train_time:54178ms step_avg:57.82ms
step:938/2330 train_time:54237ms step_avg:57.82ms
step:939/2330 train_time:54294ms step_avg:57.82ms
step:940/2330 train_time:54354ms step_avg:57.82ms
step:941/2330 train_time:54411ms step_avg:57.82ms
step:942/2330 train_time:54472ms step_avg:57.83ms
step:943/2330 train_time:54529ms step_avg:57.82ms
step:944/2330 train_time:54588ms step_avg:57.83ms
step:945/2330 train_time:54646ms step_avg:57.83ms
step:946/2330 train_time:54705ms step_avg:57.83ms
step:947/2330 train_time:54762ms step_avg:57.83ms
step:948/2330 train_time:54823ms step_avg:57.83ms
step:949/2330 train_time:54880ms step_avg:57.83ms
step:950/2330 train_time:54940ms step_avg:57.83ms
step:951/2330 train_time:54996ms step_avg:57.83ms
step:952/2330 train_time:55057ms step_avg:57.83ms
step:953/2330 train_time:55114ms step_avg:57.83ms
step:954/2330 train_time:55174ms step_avg:57.83ms
step:955/2330 train_time:55231ms step_avg:57.83ms
step:956/2330 train_time:55290ms step_avg:57.84ms
step:957/2330 train_time:55347ms step_avg:57.83ms
step:958/2330 train_time:55409ms step_avg:57.84ms
step:959/2330 train_time:55465ms step_avg:57.84ms
step:960/2330 train_time:55526ms step_avg:57.84ms
step:961/2330 train_time:55582ms step_avg:57.84ms
step:962/2330 train_time:55644ms step_avg:57.84ms
step:963/2330 train_time:55700ms step_avg:57.84ms
step:964/2330 train_time:55761ms step_avg:57.84ms
step:965/2330 train_time:55817ms step_avg:57.84ms
step:966/2330 train_time:55878ms step_avg:57.84ms
step:967/2330 train_time:55934ms step_avg:57.84ms
step:968/2330 train_time:55995ms step_avg:57.85ms
step:969/2330 train_time:56051ms step_avg:57.84ms
step:970/2330 train_time:56112ms step_avg:57.85ms
step:971/2330 train_time:56169ms step_avg:57.85ms
step:972/2330 train_time:56230ms step_avg:57.85ms
step:973/2330 train_time:56287ms step_avg:57.85ms
step:974/2330 train_time:56348ms step_avg:57.85ms
step:975/2330 train_time:56404ms step_avg:57.85ms
step:976/2330 train_time:56465ms step_avg:57.85ms
step:977/2330 train_time:56521ms step_avg:57.85ms
step:978/2330 train_time:56582ms step_avg:57.85ms
step:979/2330 train_time:56638ms step_avg:57.85ms
step:980/2330 train_time:56698ms step_avg:57.86ms
step:981/2330 train_time:56754ms step_avg:57.85ms
step:982/2330 train_time:56816ms step_avg:57.86ms
step:983/2330 train_time:56873ms step_avg:57.86ms
step:984/2330 train_time:56933ms step_avg:57.86ms
step:985/2330 train_time:56990ms step_avg:57.86ms
step:986/2330 train_time:57049ms step_avg:57.86ms
step:987/2330 train_time:57106ms step_avg:57.86ms
step:988/2330 train_time:57167ms step_avg:57.86ms
step:989/2330 train_time:57224ms step_avg:57.86ms
step:990/2330 train_time:57283ms step_avg:57.86ms
step:991/2330 train_time:57341ms step_avg:57.86ms
step:992/2330 train_time:57400ms step_avg:57.86ms
step:993/2330 train_time:57457ms step_avg:57.86ms
step:994/2330 train_time:57517ms step_avg:57.86ms
step:995/2330 train_time:57574ms step_avg:57.86ms
step:996/2330 train_time:57634ms step_avg:57.87ms
step:997/2330 train_time:57691ms step_avg:57.86ms
step:998/2330 train_time:57751ms step_avg:57.87ms
step:999/2330 train_time:57808ms step_avg:57.87ms
step:1000/2330 train_time:57868ms step_avg:57.87ms
step:1000/2330 val_loss:4.0678 train_time:57949ms step_avg:57.95ms
step:1001/2330 train_time:57968ms step_avg:57.91ms
step:1002/2330 train_time:57987ms step_avg:57.87ms
step:1003/2330 train_time:58041ms step_avg:57.87ms
step:1004/2330 train_time:58109ms step_avg:57.88ms
step:1005/2330 train_time:58165ms step_avg:57.88ms
step:1006/2330 train_time:58228ms step_avg:57.88ms
step:1007/2330 train_time:58284ms step_avg:57.88ms
step:1008/2330 train_time:58343ms step_avg:57.88ms
step:1009/2330 train_time:58399ms step_avg:57.88ms
step:1010/2330 train_time:58458ms step_avg:57.88ms
step:1011/2330 train_time:58514ms step_avg:57.88ms
step:1012/2330 train_time:58573ms step_avg:57.88ms
step:1013/2330 train_time:58629ms step_avg:57.88ms
step:1014/2330 train_time:58689ms step_avg:57.88ms
step:1015/2330 train_time:58744ms step_avg:57.88ms
step:1016/2330 train_time:58803ms step_avg:57.88ms
step:1017/2330 train_time:58860ms step_avg:57.88ms
step:1018/2330 train_time:58928ms step_avg:57.89ms
step:1019/2330 train_time:58985ms step_avg:57.89ms
step:1020/2330 train_time:59047ms step_avg:57.89ms
step:1021/2330 train_time:59104ms step_avg:57.89ms
step:1022/2330 train_time:59165ms step_avg:57.89ms
step:1023/2330 train_time:59222ms step_avg:57.89ms
step:1024/2330 train_time:59282ms step_avg:57.89ms
step:1025/2330 train_time:59338ms step_avg:57.89ms
step:1026/2330 train_time:59397ms step_avg:57.89ms
step:1027/2330 train_time:59454ms step_avg:57.89ms
step:1028/2330 train_time:59513ms step_avg:57.89ms
step:1029/2330 train_time:59569ms step_avg:57.89ms
step:1030/2330 train_time:59629ms step_avg:57.89ms
step:1031/2330 train_time:59685ms step_avg:57.89ms
step:1032/2330 train_time:59744ms step_avg:57.89ms
step:1033/2330 train_time:59801ms step_avg:57.89ms
step:1034/2330 train_time:59862ms step_avg:57.89ms
step:1035/2330 train_time:59919ms step_avg:57.89ms
step:1036/2330 train_time:59983ms step_avg:57.90ms
step:1037/2330 train_time:60040ms step_avg:57.90ms
step:1038/2330 train_time:60101ms step_avg:57.90ms
step:1039/2330 train_time:60158ms step_avg:57.90ms
step:1040/2330 train_time:60218ms step_avg:57.90ms
step:1041/2330 train_time:60275ms step_avg:57.90ms
step:1042/2330 train_time:60336ms step_avg:57.90ms
step:1043/2330 train_time:60392ms step_avg:57.90ms
step:1044/2330 train_time:60452ms step_avg:57.90ms
step:1045/2330 train_time:60508ms step_avg:57.90ms
step:1046/2330 train_time:60568ms step_avg:57.90ms
step:1047/2330 train_time:60625ms step_avg:57.90ms
step:1048/2330 train_time:60685ms step_avg:57.91ms
step:1049/2330 train_time:60741ms step_avg:57.90ms
step:1050/2330 train_time:60801ms step_avg:57.91ms
step:1051/2330 train_time:60859ms step_avg:57.91ms
step:1052/2330 train_time:60920ms step_avg:57.91ms
step:1053/2330 train_time:60977ms step_avg:57.91ms
step:1054/2330 train_time:61038ms step_avg:57.91ms
step:1055/2330 train_time:61095ms step_avg:57.91ms
step:1056/2330 train_time:61155ms step_avg:57.91ms
step:1057/2330 train_time:61212ms step_avg:57.91ms
step:1058/2330 train_time:61272ms step_avg:57.91ms
step:1059/2330 train_time:61329ms step_avg:57.91ms
step:1060/2330 train_time:61389ms step_avg:57.91ms
step:1061/2330 train_time:61445ms step_avg:57.91ms
step:1062/2330 train_time:61505ms step_avg:57.91ms
step:1063/2330 train_time:61562ms step_avg:57.91ms
step:1064/2330 train_time:61622ms step_avg:57.92ms
step:1065/2330 train_time:61679ms step_avg:57.91ms
step:1066/2330 train_time:61739ms step_avg:57.92ms
step:1067/2330 train_time:61795ms step_avg:57.91ms
step:1068/2330 train_time:61856ms step_avg:57.92ms
step:1069/2330 train_time:61912ms step_avg:57.92ms
step:1070/2330 train_time:61973ms step_avg:57.92ms
step:1071/2330 train_time:62030ms step_avg:57.92ms
step:1072/2330 train_time:62091ms step_avg:57.92ms
step:1073/2330 train_time:62148ms step_avg:57.92ms
step:1074/2330 train_time:62208ms step_avg:57.92ms
step:1075/2330 train_time:62265ms step_avg:57.92ms
step:1076/2330 train_time:62326ms step_avg:57.92ms
step:1077/2330 train_time:62382ms step_avg:57.92ms
step:1078/2330 train_time:62443ms step_avg:57.92ms
step:1079/2330 train_time:62499ms step_avg:57.92ms
step:1080/2330 train_time:62559ms step_avg:57.92ms
step:1081/2330 train_time:62615ms step_avg:57.92ms
step:1082/2330 train_time:62676ms step_avg:57.93ms
step:1083/2330 train_time:62733ms step_avg:57.93ms
step:1084/2330 train_time:62793ms step_avg:57.93ms
step:1085/2330 train_time:62851ms step_avg:57.93ms
step:1086/2330 train_time:62910ms step_avg:57.93ms
step:1087/2330 train_time:62967ms step_avg:57.93ms
step:1088/2330 train_time:63028ms step_avg:57.93ms
step:1089/2330 train_time:63085ms step_avg:57.93ms
step:1090/2330 train_time:63145ms step_avg:57.93ms
step:1091/2330 train_time:63201ms step_avg:57.93ms
step:1092/2330 train_time:63263ms step_avg:57.93ms
step:1093/2330 train_time:63319ms step_avg:57.93ms
step:1094/2330 train_time:63380ms step_avg:57.93ms
step:1095/2330 train_time:63436ms step_avg:57.93ms
step:1096/2330 train_time:63497ms step_avg:57.94ms
step:1097/2330 train_time:63554ms step_avg:57.93ms
step:1098/2330 train_time:63615ms step_avg:57.94ms
step:1099/2330 train_time:63672ms step_avg:57.94ms
step:1100/2330 train_time:63731ms step_avg:57.94ms
step:1101/2330 train_time:63787ms step_avg:57.94ms
step:1102/2330 train_time:63848ms step_avg:57.94ms
step:1103/2330 train_time:63905ms step_avg:57.94ms
step:1104/2330 train_time:63965ms step_avg:57.94ms
step:1105/2330 train_time:64022ms step_avg:57.94ms
step:1106/2330 train_time:64083ms step_avg:57.94ms
step:1107/2330 train_time:64140ms step_avg:57.94ms
step:1108/2330 train_time:64200ms step_avg:57.94ms
step:1109/2330 train_time:64256ms step_avg:57.94ms
step:1110/2330 train_time:64317ms step_avg:57.94ms
step:1111/2330 train_time:64374ms step_avg:57.94ms
step:1112/2330 train_time:64434ms step_avg:57.94ms
step:1113/2330 train_time:64491ms step_avg:57.94ms
step:1114/2330 train_time:64552ms step_avg:57.95ms
step:1115/2330 train_time:64608ms step_avg:57.94ms
step:1116/2330 train_time:64669ms step_avg:57.95ms
step:1117/2330 train_time:64725ms step_avg:57.95ms
step:1118/2330 train_time:64786ms step_avg:57.95ms
step:1119/2330 train_time:64842ms step_avg:57.95ms
step:1120/2330 train_time:64903ms step_avg:57.95ms
step:1121/2330 train_time:64960ms step_avg:57.95ms
step:1122/2330 train_time:65020ms step_avg:57.95ms
step:1123/2330 train_time:65076ms step_avg:57.95ms
step:1124/2330 train_time:65137ms step_avg:57.95ms
step:1125/2330 train_time:65194ms step_avg:57.95ms
step:1126/2330 train_time:65254ms step_avg:57.95ms
step:1127/2330 train_time:65310ms step_avg:57.95ms
step:1128/2330 train_time:65371ms step_avg:57.95ms
step:1129/2330 train_time:65428ms step_avg:57.95ms
step:1130/2330 train_time:65488ms step_avg:57.95ms
step:1131/2330 train_time:65544ms step_avg:57.95ms
step:1132/2330 train_time:65605ms step_avg:57.95ms
step:1133/2330 train_time:65662ms step_avg:57.95ms
step:1134/2330 train_time:65723ms step_avg:57.96ms
step:1135/2330 train_time:65780ms step_avg:57.96ms
step:1136/2330 train_time:65841ms step_avg:57.96ms
step:1137/2330 train_time:65898ms step_avg:57.96ms
step:1138/2330 train_time:65958ms step_avg:57.96ms
step:1139/2330 train_time:66014ms step_avg:57.96ms
step:1140/2330 train_time:66075ms step_avg:57.96ms
step:1141/2330 train_time:66133ms step_avg:57.96ms
step:1142/2330 train_time:66193ms step_avg:57.96ms
step:1143/2330 train_time:66250ms step_avg:57.96ms
step:1144/2330 train_time:66310ms step_avg:57.96ms
step:1145/2330 train_time:66367ms step_avg:57.96ms
step:1146/2330 train_time:66427ms step_avg:57.96ms
step:1147/2330 train_time:66483ms step_avg:57.96ms
step:1148/2330 train_time:66544ms step_avg:57.97ms
step:1149/2330 train_time:66601ms step_avg:57.96ms
step:1150/2330 train_time:66661ms step_avg:57.97ms
step:1151/2330 train_time:66717ms step_avg:57.96ms
step:1152/2330 train_time:66779ms step_avg:57.97ms
step:1153/2330 train_time:66836ms step_avg:57.97ms
step:1154/2330 train_time:66897ms step_avg:57.97ms
step:1155/2330 train_time:66953ms step_avg:57.97ms
step:1156/2330 train_time:67013ms step_avg:57.97ms
step:1157/2330 train_time:67069ms step_avg:57.97ms
step:1158/2330 train_time:67131ms step_avg:57.97ms
step:1159/2330 train_time:67188ms step_avg:57.97ms
step:1160/2330 train_time:67248ms step_avg:57.97ms
step:1161/2330 train_time:67304ms step_avg:57.97ms
step:1162/2330 train_time:67366ms step_avg:57.97ms
step:1163/2330 train_time:67422ms step_avg:57.97ms
step:1164/2330 train_time:67483ms step_avg:57.98ms
step:1165/2330 train_time:67540ms step_avg:57.97ms
step:1166/2330 train_time:67600ms step_avg:57.98ms
step:1167/2330 train_time:67657ms step_avg:57.98ms
step:1168/2330 train_time:67717ms step_avg:57.98ms
step:1169/2330 train_time:67774ms step_avg:57.98ms
step:1170/2330 train_time:67834ms step_avg:57.98ms
step:1171/2330 train_time:67890ms step_avg:57.98ms
step:1172/2330 train_time:67951ms step_avg:57.98ms
step:1173/2330 train_time:68007ms step_avg:57.98ms
step:1174/2330 train_time:68068ms step_avg:57.98ms
step:1175/2330 train_time:68126ms step_avg:57.98ms
step:1176/2330 train_time:68186ms step_avg:57.98ms
step:1177/2330 train_time:68243ms step_avg:57.98ms
step:1178/2330 train_time:68303ms step_avg:57.98ms
step:1179/2330 train_time:68360ms step_avg:57.98ms
step:1180/2330 train_time:68421ms step_avg:57.98ms
step:1181/2330 train_time:68478ms step_avg:57.98ms
step:1182/2330 train_time:68538ms step_avg:57.98ms
step:1183/2330 train_time:68594ms step_avg:57.98ms
step:1184/2330 train_time:68655ms step_avg:57.99ms
step:1185/2330 train_time:68711ms step_avg:57.98ms
step:1186/2330 train_time:68771ms step_avg:57.99ms
step:1187/2330 train_time:68828ms step_avg:57.98ms
step:1188/2330 train_time:68889ms step_avg:57.99ms
step:1189/2330 train_time:68945ms step_avg:57.99ms
step:1190/2330 train_time:69006ms step_avg:57.99ms
step:1191/2330 train_time:69062ms step_avg:57.99ms
step:1192/2330 train_time:69124ms step_avg:57.99ms
step:1193/2330 train_time:69181ms step_avg:57.99ms
step:1194/2330 train_time:69242ms step_avg:57.99ms
step:1195/2330 train_time:69299ms step_avg:57.99ms
step:1196/2330 train_time:69360ms step_avg:57.99ms
step:1197/2330 train_time:69417ms step_avg:57.99ms
step:1198/2330 train_time:69477ms step_avg:57.99ms
step:1199/2330 train_time:69534ms step_avg:57.99ms
step:1200/2330 train_time:69595ms step_avg:58.00ms
step:1201/2330 train_time:69652ms step_avg:58.00ms
step:1202/2330 train_time:69712ms step_avg:58.00ms
step:1203/2330 train_time:69769ms step_avg:58.00ms
step:1204/2330 train_time:69830ms step_avg:58.00ms
step:1205/2330 train_time:69886ms step_avg:58.00ms
step:1206/2330 train_time:69947ms step_avg:58.00ms
step:1207/2330 train_time:70003ms step_avg:58.00ms
step:1208/2330 train_time:70065ms step_avg:58.00ms
step:1209/2330 train_time:70121ms step_avg:58.00ms
step:1210/2330 train_time:70183ms step_avg:58.00ms
step:1211/2330 train_time:70239ms step_avg:58.00ms
step:1212/2330 train_time:70300ms step_avg:58.00ms
step:1213/2330 train_time:70357ms step_avg:58.00ms
step:1214/2330 train_time:70417ms step_avg:58.00ms
step:1215/2330 train_time:70474ms step_avg:58.00ms
step:1216/2330 train_time:70534ms step_avg:58.00ms
step:1217/2330 train_time:70591ms step_avg:58.00ms
step:1218/2330 train_time:70652ms step_avg:58.01ms
step:1219/2330 train_time:70709ms step_avg:58.01ms
step:1220/2330 train_time:70769ms step_avg:58.01ms
step:1221/2330 train_time:70826ms step_avg:58.01ms
step:1222/2330 train_time:70886ms step_avg:58.01ms
step:1223/2330 train_time:70942ms step_avg:58.01ms
step:1224/2330 train_time:71003ms step_avg:58.01ms
step:1225/2330 train_time:71059ms step_avg:58.01ms
step:1226/2330 train_time:71120ms step_avg:58.01ms
step:1227/2330 train_time:71177ms step_avg:58.01ms
step:1228/2330 train_time:71237ms step_avg:58.01ms
step:1229/2330 train_time:71293ms step_avg:58.01ms
step:1230/2330 train_time:71354ms step_avg:58.01ms
step:1231/2330 train_time:71410ms step_avg:58.01ms
step:1232/2330 train_time:71471ms step_avg:58.01ms
step:1233/2330 train_time:71528ms step_avg:58.01ms
step:1234/2330 train_time:71589ms step_avg:58.01ms
step:1235/2330 train_time:71645ms step_avg:58.01ms
step:1236/2330 train_time:71706ms step_avg:58.01ms
step:1237/2330 train_time:71763ms step_avg:58.01ms
step:1238/2330 train_time:71824ms step_avg:58.02ms
step:1239/2330 train_time:71881ms step_avg:58.02ms
step:1240/2330 train_time:71941ms step_avg:58.02ms
step:1241/2330 train_time:71997ms step_avg:58.02ms
step:1242/2330 train_time:72058ms step_avg:58.02ms
step:1243/2330 train_time:72114ms step_avg:58.02ms
step:1244/2330 train_time:72174ms step_avg:58.02ms
step:1245/2330 train_time:72231ms step_avg:58.02ms
step:1246/2330 train_time:72291ms step_avg:58.02ms
step:1247/2330 train_time:72348ms step_avg:58.02ms
step:1248/2330 train_time:72408ms step_avg:58.02ms
step:1249/2330 train_time:72464ms step_avg:58.02ms
step:1250/2330 train_time:72525ms step_avg:58.02ms
step:1250/2330 val_loss:3.9891 train_time:72606ms step_avg:58.08ms
step:1251/2330 train_time:72623ms step_avg:58.05ms
step:1252/2330 train_time:72646ms step_avg:58.02ms
step:1253/2330 train_time:72706ms step_avg:58.03ms
step:1254/2330 train_time:72769ms step_avg:58.03ms
step:1255/2330 train_time:72827ms step_avg:58.03ms
step:1256/2330 train_time:72888ms step_avg:58.03ms
step:1257/2330 train_time:72944ms step_avg:58.03ms
step:1258/2330 train_time:73004ms step_avg:58.03ms
step:1259/2330 train_time:73060ms step_avg:58.03ms
step:1260/2330 train_time:73121ms step_avg:58.03ms
step:1261/2330 train_time:73177ms step_avg:58.03ms
step:1262/2330 train_time:73236ms step_avg:58.03ms
step:1263/2330 train_time:73292ms step_avg:58.03ms
step:1264/2330 train_time:73352ms step_avg:58.03ms
step:1265/2330 train_time:73407ms step_avg:58.03ms
step:1266/2330 train_time:73468ms step_avg:58.03ms
step:1267/2330 train_time:73525ms step_avg:58.03ms
step:1268/2330 train_time:73586ms step_avg:58.03ms
step:1269/2330 train_time:73646ms step_avg:58.03ms
step:1270/2330 train_time:73707ms step_avg:58.04ms
step:1271/2330 train_time:73766ms step_avg:58.04ms
step:1272/2330 train_time:73826ms step_avg:58.04ms
step:1273/2330 train_time:73883ms step_avg:58.04ms
step:1274/2330 train_time:73944ms step_avg:58.04ms
step:1275/2330 train_time:74000ms step_avg:58.04ms
step:1276/2330 train_time:74060ms step_avg:58.04ms
step:1277/2330 train_time:74116ms step_avg:58.04ms
step:1278/2330 train_time:74176ms step_avg:58.04ms
step:1279/2330 train_time:74232ms step_avg:58.04ms
step:1280/2330 train_time:74292ms step_avg:58.04ms
step:1281/2330 train_time:74348ms step_avg:58.04ms
step:1282/2330 train_time:74408ms step_avg:58.04ms
step:1283/2330 train_time:74465ms step_avg:58.04ms
step:1284/2330 train_time:74525ms step_avg:58.04ms
step:1285/2330 train_time:74584ms step_avg:58.04ms
step:1286/2330 train_time:74644ms step_avg:58.04ms
step:1287/2330 train_time:74701ms step_avg:58.04ms
step:1288/2330 train_time:74762ms step_avg:58.05ms
step:1289/2330 train_time:74820ms step_avg:58.05ms
step:1290/2330 train_time:74881ms step_avg:58.05ms
step:1291/2330 train_time:74937ms step_avg:58.05ms
step:1292/2330 train_time:74999ms step_avg:58.05ms
step:1293/2330 train_time:75055ms step_avg:58.05ms
step:1294/2330 train_time:75116ms step_avg:58.05ms
step:1295/2330 train_time:75173ms step_avg:58.05ms
step:1296/2330 train_time:75232ms step_avg:58.05ms
step:1297/2330 train_time:75289ms step_avg:58.05ms
step:1298/2330 train_time:75348ms step_avg:58.05ms
step:1299/2330 train_time:75405ms step_avg:58.05ms
step:1300/2330 train_time:75465ms step_avg:58.05ms
step:1301/2330 train_time:75522ms step_avg:58.05ms
step:1302/2330 train_time:75583ms step_avg:58.05ms
step:1303/2330 train_time:75640ms step_avg:58.05ms
step:1304/2330 train_time:75701ms step_avg:58.05ms
step:1305/2330 train_time:75758ms step_avg:58.05ms
step:1306/2330 train_time:75819ms step_avg:58.05ms
step:1307/2330 train_time:75875ms step_avg:58.05ms
step:1308/2330 train_time:75936ms step_avg:58.06ms
step:1309/2330 train_time:75993ms step_avg:58.05ms
step:1310/2330 train_time:76053ms step_avg:58.06ms
step:1311/2330 train_time:76110ms step_avg:58.06ms
step:1312/2330 train_time:76170ms step_avg:58.06ms
step:1313/2330 train_time:76226ms step_avg:58.06ms
step:1314/2330 train_time:76286ms step_avg:58.06ms
step:1315/2330 train_time:76342ms step_avg:58.05ms
step:1316/2330 train_time:76402ms step_avg:58.06ms
step:1317/2330 train_time:76460ms step_avg:58.06ms
step:1318/2330 train_time:76519ms step_avg:58.06ms
step:1319/2330 train_time:76576ms step_avg:58.06ms
step:1320/2330 train_time:76637ms step_avg:58.06ms
step:1321/2330 train_time:76694ms step_avg:58.06ms
step:1322/2330 train_time:76755ms step_avg:58.06ms
step:1323/2330 train_time:76813ms step_avg:58.06ms
step:1324/2330 train_time:76873ms step_avg:58.06ms
step:1325/2330 train_time:76930ms step_avg:58.06ms
step:1326/2330 train_time:76991ms step_avg:58.06ms
step:1327/2330 train_time:77048ms step_avg:58.06ms
step:1328/2330 train_time:77109ms step_avg:58.06ms
step:1329/2330 train_time:77167ms step_avg:58.06ms
step:1330/2330 train_time:77227ms step_avg:58.07ms
step:1331/2330 train_time:77284ms step_avg:58.06ms
step:1332/2330 train_time:77343ms step_avg:58.07ms
step:1333/2330 train_time:77400ms step_avg:58.06ms
step:1334/2330 train_time:77460ms step_avg:58.07ms
step:1335/2330 train_time:77517ms step_avg:58.06ms
step:1336/2330 train_time:77577ms step_avg:58.07ms
step:1337/2330 train_time:77634ms step_avg:58.07ms
step:1338/2330 train_time:77695ms step_avg:58.07ms
step:1339/2330 train_time:77752ms step_avg:58.07ms
step:1340/2330 train_time:77813ms step_avg:58.07ms
step:1341/2330 train_time:77870ms step_avg:58.07ms
step:1342/2330 train_time:77931ms step_avg:58.07ms
step:1343/2330 train_time:77988ms step_avg:58.07ms
step:1344/2330 train_time:78048ms step_avg:58.07ms
step:1345/2330 train_time:78105ms step_avg:58.07ms
step:1346/2330 train_time:78165ms step_avg:58.07ms
step:1347/2330 train_time:78222ms step_avg:58.07ms
step:1348/2330 train_time:78282ms step_avg:58.07ms
step:1349/2330 train_time:78339ms step_avg:58.07ms
step:1350/2330 train_time:78399ms step_avg:58.07ms
step:1351/2330 train_time:78455ms step_avg:58.07ms
step:1352/2330 train_time:78515ms step_avg:58.07ms
step:1353/2330 train_time:78572ms step_avg:58.07ms
step:1354/2330 train_time:78632ms step_avg:58.07ms
step:1355/2330 train_time:78689ms step_avg:58.07ms
step:1356/2330 train_time:78749ms step_avg:58.07ms
step:1357/2330 train_time:78806ms step_avg:58.07ms
step:1358/2330 train_time:78867ms step_avg:58.08ms
step:1359/2330 train_time:78924ms step_avg:58.08ms
step:1360/2330 train_time:78984ms step_avg:58.08ms
step:1361/2330 train_time:79041ms step_avg:58.08ms
step:1362/2330 train_time:79102ms step_avg:58.08ms
step:1363/2330 train_time:79159ms step_avg:58.08ms
step:1364/2330 train_time:79219ms step_avg:58.08ms
step:1365/2330 train_time:79276ms step_avg:58.08ms
step:1366/2330 train_time:79336ms step_avg:58.08ms
step:1367/2330 train_time:79393ms step_avg:58.08ms
step:1368/2330 train_time:79453ms step_avg:58.08ms
step:1369/2330 train_time:79510ms step_avg:58.08ms
step:1370/2330 train_time:79570ms step_avg:58.08ms
step:1371/2330 train_time:79627ms step_avg:58.08ms
step:1372/2330 train_time:79687ms step_avg:58.08ms
step:1373/2330 train_time:79744ms step_avg:58.08ms
step:1374/2330 train_time:79805ms step_avg:58.08ms
step:1375/2330 train_time:79862ms step_avg:58.08ms
step:1376/2330 train_time:79922ms step_avg:58.08ms
step:1377/2330 train_time:79979ms step_avg:58.08ms
step:1378/2330 train_time:80040ms step_avg:58.08ms
step:1379/2330 train_time:80097ms step_avg:58.08ms
step:1380/2330 train_time:80157ms step_avg:58.08ms
step:1381/2330 train_time:80213ms step_avg:58.08ms
step:1382/2330 train_time:80275ms step_avg:58.09ms
step:1383/2330 train_time:80332ms step_avg:58.09ms
step:1384/2330 train_time:80392ms step_avg:58.09ms
step:1385/2330 train_time:80448ms step_avg:58.09ms
step:1386/2330 train_time:80508ms step_avg:58.09ms
step:1387/2330 train_time:80566ms step_avg:58.09ms
step:1388/2330 train_time:80626ms step_avg:58.09ms
step:1389/2330 train_time:80683ms step_avg:58.09ms
step:1390/2330 train_time:80742ms step_avg:58.09ms
step:1391/2330 train_time:80799ms step_avg:58.09ms
step:1392/2330 train_time:80860ms step_avg:58.09ms
step:1393/2330 train_time:80917ms step_avg:58.09ms
step:1394/2330 train_time:80978ms step_avg:58.09ms
step:1395/2330 train_time:81035ms step_avg:58.09ms
step:1396/2330 train_time:81095ms step_avg:58.09ms
step:1397/2330 train_time:81152ms step_avg:58.09ms
step:1398/2330 train_time:81213ms step_avg:58.09ms
step:1399/2330 train_time:81270ms step_avg:58.09ms
step:1400/2330 train_time:81330ms step_avg:58.09ms
step:1401/2330 train_time:81387ms step_avg:58.09ms
step:1402/2330 train_time:81446ms step_avg:58.09ms
step:1403/2330 train_time:81503ms step_avg:58.09ms
step:1404/2330 train_time:81563ms step_avg:58.09ms
step:1405/2330 train_time:81620ms step_avg:58.09ms
step:1406/2330 train_time:81680ms step_avg:58.09ms
step:1407/2330 train_time:81738ms step_avg:58.09ms
step:1408/2330 train_time:81797ms step_avg:58.09ms
step:1409/2330 train_time:81854ms step_avg:58.09ms
step:1410/2330 train_time:81916ms step_avg:58.10ms
step:1411/2330 train_time:81973ms step_avg:58.10ms
step:1412/2330 train_time:82033ms step_avg:58.10ms
step:1413/2330 train_time:82090ms step_avg:58.10ms
step:1414/2330 train_time:82150ms step_avg:58.10ms
step:1415/2330 train_time:82207ms step_avg:58.10ms
step:1416/2330 train_time:82267ms step_avg:58.10ms
step:1417/2330 train_time:82324ms step_avg:58.10ms
step:1418/2330 train_time:82383ms step_avg:58.10ms
step:1419/2330 train_time:82440ms step_avg:58.10ms
step:1420/2330 train_time:82500ms step_avg:58.10ms
step:1421/2330 train_time:82557ms step_avg:58.10ms
step:1422/2330 train_time:82617ms step_avg:58.10ms
step:1423/2330 train_time:82673ms step_avg:58.10ms
step:1424/2330 train_time:82733ms step_avg:58.10ms
step:1425/2330 train_time:82791ms step_avg:58.10ms
step:1426/2330 train_time:82851ms step_avg:58.10ms
step:1427/2330 train_time:82909ms step_avg:58.10ms
step:1428/2330 train_time:82968ms step_avg:58.10ms
step:1429/2330 train_time:83026ms step_avg:58.10ms
step:1430/2330 train_time:83085ms step_avg:58.10ms
step:1431/2330 train_time:83143ms step_avg:58.10ms
step:1432/2330 train_time:83203ms step_avg:58.10ms
step:1433/2330 train_time:83260ms step_avg:58.10ms
step:1434/2330 train_time:83321ms step_avg:58.10ms
step:1435/2330 train_time:83377ms step_avg:58.10ms
step:1436/2330 train_time:83438ms step_avg:58.10ms
step:1437/2330 train_time:83495ms step_avg:58.10ms
step:1438/2330 train_time:83555ms step_avg:58.11ms
step:1439/2330 train_time:83612ms step_avg:58.10ms
step:1440/2330 train_time:83672ms step_avg:58.11ms
step:1441/2330 train_time:83729ms step_avg:58.10ms
step:1442/2330 train_time:83789ms step_avg:58.11ms
step:1443/2330 train_time:83846ms step_avg:58.11ms
step:1444/2330 train_time:83906ms step_avg:58.11ms
step:1445/2330 train_time:83963ms step_avg:58.11ms
step:1446/2330 train_time:84024ms step_avg:58.11ms
step:1447/2330 train_time:84081ms step_avg:58.11ms
step:1448/2330 train_time:84141ms step_avg:58.11ms
step:1449/2330 train_time:84197ms step_avg:58.11ms
step:1450/2330 train_time:84260ms step_avg:58.11ms
step:1451/2330 train_time:84317ms step_avg:58.11ms
step:1452/2330 train_time:84377ms step_avg:58.11ms
step:1453/2330 train_time:84434ms step_avg:58.11ms
step:1454/2330 train_time:84495ms step_avg:58.11ms
step:1455/2330 train_time:84551ms step_avg:58.11ms
step:1456/2330 train_time:84612ms step_avg:58.11ms
step:1457/2330 train_time:84668ms step_avg:58.11ms
step:1458/2330 train_time:84728ms step_avg:58.11ms
step:1459/2330 train_time:84785ms step_avg:58.11ms
step:1460/2330 train_time:84845ms step_avg:58.11ms
step:1461/2330 train_time:84903ms step_avg:58.11ms
step:1462/2330 train_time:84963ms step_avg:58.11ms
step:1463/2330 train_time:85021ms step_avg:58.11ms
step:1464/2330 train_time:85080ms step_avg:58.11ms
step:1465/2330 train_time:85137ms step_avg:58.11ms
step:1466/2330 train_time:85198ms step_avg:58.12ms
step:1467/2330 train_time:85255ms step_avg:58.12ms
step:1468/2330 train_time:85316ms step_avg:58.12ms
step:1469/2330 train_time:85372ms step_avg:58.12ms
step:1470/2330 train_time:85433ms step_avg:58.12ms
step:1471/2330 train_time:85490ms step_avg:58.12ms
step:1472/2330 train_time:85550ms step_avg:58.12ms
step:1473/2330 train_time:85607ms step_avg:58.12ms
step:1474/2330 train_time:85667ms step_avg:58.12ms
step:1475/2330 train_time:85724ms step_avg:58.12ms
step:1476/2330 train_time:85784ms step_avg:58.12ms
step:1477/2330 train_time:85841ms step_avg:58.12ms
step:1478/2330 train_time:85901ms step_avg:58.12ms
step:1479/2330 train_time:85958ms step_avg:58.12ms
step:1480/2330 train_time:86019ms step_avg:58.12ms
step:1481/2330 train_time:86077ms step_avg:58.12ms
step:1482/2330 train_time:86136ms step_avg:58.12ms
step:1483/2330 train_time:86193ms step_avg:58.12ms
step:1484/2330 train_time:86253ms step_avg:58.12ms
step:1485/2330 train_time:86310ms step_avg:58.12ms
step:1486/2330 train_time:86370ms step_avg:58.12ms
step:1487/2330 train_time:86427ms step_avg:58.12ms
step:1488/2330 train_time:86487ms step_avg:58.12ms
step:1489/2330 train_time:86544ms step_avg:58.12ms
step:1490/2330 train_time:86604ms step_avg:58.12ms
step:1491/2330 train_time:86661ms step_avg:58.12ms
step:1492/2330 train_time:86721ms step_avg:58.12ms
step:1493/2330 train_time:86778ms step_avg:58.12ms
step:1494/2330 train_time:86838ms step_avg:58.12ms
step:1495/2330 train_time:86895ms step_avg:58.12ms
step:1496/2330 train_time:86956ms step_avg:58.13ms
step:1497/2330 train_time:87014ms step_avg:58.13ms
step:1498/2330 train_time:87074ms step_avg:58.13ms
step:1499/2330 train_time:87131ms step_avg:58.13ms
step:1500/2330 train_time:87191ms step_avg:58.13ms
step:1500/2330 val_loss:3.9066 train_time:87271ms step_avg:58.18ms
step:1501/2330 train_time:87289ms step_avg:58.15ms
step:1502/2330 train_time:87310ms step_avg:58.13ms
step:1503/2330 train_time:87370ms step_avg:58.13ms
step:1504/2330 train_time:87437ms step_avg:58.14ms
step:1505/2330 train_time:87496ms step_avg:58.14ms
step:1506/2330 train_time:87557ms step_avg:58.14ms
step:1507/2330 train_time:87614ms step_avg:58.14ms
step:1508/2330 train_time:87674ms step_avg:58.14ms
step:1509/2330 train_time:87731ms step_avg:58.14ms
step:1510/2330 train_time:87791ms step_avg:58.14ms
step:1511/2330 train_time:87847ms step_avg:58.14ms
step:1512/2330 train_time:87907ms step_avg:58.14ms
step:1513/2330 train_time:87963ms step_avg:58.14ms
step:1514/2330 train_time:88023ms step_avg:58.14ms
step:1515/2330 train_time:88079ms step_avg:58.14ms
step:1516/2330 train_time:88139ms step_avg:58.14ms
step:1517/2330 train_time:88196ms step_avg:58.14ms
step:1518/2330 train_time:88256ms step_avg:58.14ms
step:1519/2330 train_time:88314ms step_avg:58.14ms
step:1520/2330 train_time:88376ms step_avg:58.14ms
step:1521/2330 train_time:88435ms step_avg:58.14ms
step:1522/2330 train_time:88496ms step_avg:58.14ms
step:1523/2330 train_time:88554ms step_avg:58.14ms
step:1524/2330 train_time:88614ms step_avg:58.15ms
step:1525/2330 train_time:88672ms step_avg:58.15ms
step:1526/2330 train_time:88733ms step_avg:58.15ms
step:1527/2330 train_time:88790ms step_avg:58.15ms
step:1528/2330 train_time:88850ms step_avg:58.15ms
step:1529/2330 train_time:88907ms step_avg:58.15ms
step:1530/2330 train_time:88966ms step_avg:58.15ms
step:1531/2330 train_time:89023ms step_avg:58.15ms
step:1532/2330 train_time:89084ms step_avg:58.15ms
step:1533/2330 train_time:89141ms step_avg:58.15ms
step:1534/2330 train_time:89203ms step_avg:58.15ms
step:1535/2330 train_time:89259ms step_avg:58.15ms
step:1536/2330 train_time:89322ms step_avg:58.15ms
step:1537/2330 train_time:89380ms step_avg:58.15ms
step:1538/2330 train_time:89442ms step_avg:58.15ms
step:1539/2330 train_time:89499ms step_avg:58.15ms
step:1540/2330 train_time:89560ms step_avg:58.16ms
step:1541/2330 train_time:89618ms step_avg:58.16ms
step:1542/2330 train_time:89680ms step_avg:58.16ms
step:1543/2330 train_time:89738ms step_avg:58.16ms
step:1544/2330 train_time:89798ms step_avg:58.16ms
step:1545/2330 train_time:89856ms step_avg:58.16ms
step:1546/2330 train_time:89917ms step_avg:58.16ms
step:1547/2330 train_time:89975ms step_avg:58.16ms
step:1548/2330 train_time:90035ms step_avg:58.16ms
step:1549/2330 train_time:90093ms step_avg:58.16ms
step:1550/2330 train_time:90153ms step_avg:58.16ms
step:1551/2330 train_time:90210ms step_avg:58.16ms
step:1552/2330 train_time:90271ms step_avg:58.16ms
step:1553/2330 train_time:90329ms step_avg:58.16ms
step:1554/2330 train_time:90391ms step_avg:58.17ms
step:1555/2330 train_time:90449ms step_avg:58.17ms
step:1556/2330 train_time:90510ms step_avg:58.17ms
step:1557/2330 train_time:90568ms step_avg:58.17ms
step:1558/2330 train_time:90629ms step_avg:58.17ms
step:1559/2330 train_time:90687ms step_avg:58.17ms
step:1560/2330 train_time:90748ms step_avg:58.17ms
step:1561/2330 train_time:90805ms step_avg:58.17ms
step:1562/2330 train_time:90866ms step_avg:58.17ms
step:1563/2330 train_time:90923ms step_avg:58.17ms
step:1564/2330 train_time:90984ms step_avg:58.17ms
step:1565/2330 train_time:91040ms step_avg:58.17ms
step:1566/2330 train_time:91102ms step_avg:58.17ms
step:1567/2330 train_time:91159ms step_avg:58.17ms
step:1568/2330 train_time:91220ms step_avg:58.18ms
step:1569/2330 train_time:91278ms step_avg:58.18ms
step:1570/2330 train_time:91339ms step_avg:58.18ms
step:1571/2330 train_time:91397ms step_avg:58.18ms
step:1572/2330 train_time:91458ms step_avg:58.18ms
step:1573/2330 train_time:91517ms step_avg:58.18ms
step:1574/2330 train_time:91578ms step_avg:58.18ms
step:1575/2330 train_time:91636ms step_avg:58.18ms
step:1576/2330 train_time:91698ms step_avg:58.18ms
step:1577/2330 train_time:91755ms step_avg:58.18ms
step:1578/2330 train_time:91816ms step_avg:58.19ms
step:1579/2330 train_time:91874ms step_avg:58.19ms
step:1580/2330 train_time:91935ms step_avg:58.19ms
step:1581/2330 train_time:91993ms step_avg:58.19ms
step:1582/2330 train_time:92053ms step_avg:58.19ms
step:1583/2330 train_time:92110ms step_avg:58.19ms
step:1584/2330 train_time:92171ms step_avg:58.19ms
step:1585/2330 train_time:92227ms step_avg:58.19ms
step:1586/2330 train_time:92288ms step_avg:58.19ms
step:1587/2330 train_time:92345ms step_avg:58.19ms
step:1588/2330 train_time:92407ms step_avg:58.19ms
step:1589/2330 train_time:92464ms step_avg:58.19ms
step:1590/2330 train_time:92528ms step_avg:58.19ms
step:1591/2330 train_time:92584ms step_avg:58.19ms
step:1592/2330 train_time:92647ms step_avg:58.20ms
step:1593/2330 train_time:92704ms step_avg:58.19ms
step:1594/2330 train_time:92767ms step_avg:58.20ms
step:1595/2330 train_time:92824ms step_avg:58.20ms
step:1596/2330 train_time:92885ms step_avg:58.20ms
step:1597/2330 train_time:92942ms step_avg:58.20ms
step:1598/2330 train_time:93003ms step_avg:58.20ms
step:1599/2330 train_time:93059ms step_avg:58.20ms
step:1600/2330 train_time:93120ms step_avg:58.20ms
step:1601/2330 train_time:93177ms step_avg:58.20ms
step:1602/2330 train_time:93239ms step_avg:58.20ms
step:1603/2330 train_time:93296ms step_avg:58.20ms
step:1604/2330 train_time:93357ms step_avg:58.20ms
step:1605/2330 train_time:93415ms step_avg:58.20ms
step:1606/2330 train_time:93476ms step_avg:58.20ms
step:1607/2330 train_time:93534ms step_avg:58.20ms
step:1608/2330 train_time:93595ms step_avg:58.21ms
step:1609/2330 train_time:93654ms step_avg:58.21ms
step:1610/2330 train_time:93714ms step_avg:58.21ms
step:1611/2330 train_time:93772ms step_avg:58.21ms
step:1612/2330 train_time:93833ms step_avg:58.21ms
step:1613/2330 train_time:93889ms step_avg:58.21ms
step:1614/2330 train_time:93950ms step_avg:58.21ms
step:1615/2330 train_time:94008ms step_avg:58.21ms
step:1616/2330 train_time:94069ms step_avg:58.21ms
step:1617/2330 train_time:94126ms step_avg:58.21ms
step:1618/2330 train_time:94189ms step_avg:58.21ms
step:1619/2330 train_time:94245ms step_avg:58.21ms
step:1620/2330 train_time:94307ms step_avg:58.21ms
step:1621/2330 train_time:94364ms step_avg:58.21ms
step:1622/2330 train_time:94426ms step_avg:58.22ms
step:1623/2330 train_time:94483ms step_avg:58.21ms
step:1624/2330 train_time:94545ms step_avg:58.22ms
step:1625/2330 train_time:94602ms step_avg:58.22ms
step:1626/2330 train_time:94665ms step_avg:58.22ms
step:1627/2330 train_time:94721ms step_avg:58.22ms
step:1628/2330 train_time:94783ms step_avg:58.22ms
step:1629/2330 train_time:94840ms step_avg:58.22ms
step:1630/2330 train_time:94902ms step_avg:58.22ms
step:1631/2330 train_time:94959ms step_avg:58.22ms
step:1632/2330 train_time:95020ms step_avg:58.22ms
step:1633/2330 train_time:95077ms step_avg:58.22ms
step:1634/2330 train_time:95139ms step_avg:58.22ms
step:1635/2330 train_time:95197ms step_avg:58.22ms
step:1636/2330 train_time:95256ms step_avg:58.23ms
step:1637/2330 train_time:95315ms step_avg:58.23ms
step:1638/2330 train_time:95376ms step_avg:58.23ms
step:1639/2330 train_time:95434ms step_avg:58.23ms
step:1640/2330 train_time:95494ms step_avg:58.23ms
step:1641/2330 train_time:95552ms step_avg:58.23ms
step:1642/2330 train_time:95614ms step_avg:58.23ms
step:1643/2330 train_time:95671ms step_avg:58.23ms
step:1644/2330 train_time:95732ms step_avg:58.23ms
step:1645/2330 train_time:95790ms step_avg:58.23ms
step:1646/2330 train_time:95851ms step_avg:58.23ms
step:1647/2330 train_time:95908ms step_avg:58.23ms
step:1648/2330 train_time:95970ms step_avg:58.23ms
step:1649/2330 train_time:96027ms step_avg:58.23ms
step:1650/2330 train_time:96089ms step_avg:58.24ms
step:1651/2330 train_time:96145ms step_avg:58.23ms
step:1652/2330 train_time:96207ms step_avg:58.24ms
step:1653/2330 train_time:96264ms step_avg:58.24ms
step:1654/2330 train_time:96326ms step_avg:58.24ms
step:1655/2330 train_time:96383ms step_avg:58.24ms
step:1656/2330 train_time:96445ms step_avg:58.24ms
step:1657/2330 train_time:96501ms step_avg:58.24ms
step:1658/2330 train_time:96562ms step_avg:58.24ms
step:1659/2330 train_time:96619ms step_avg:58.24ms
step:1660/2330 train_time:96681ms step_avg:58.24ms
step:1661/2330 train_time:96738ms step_avg:58.24ms
step:1662/2330 train_time:96799ms step_avg:58.24ms
step:1663/2330 train_time:96857ms step_avg:58.24ms
step:1664/2330 train_time:96918ms step_avg:58.24ms
step:1665/2330 train_time:96975ms step_avg:58.24ms
step:1666/2330 train_time:97037ms step_avg:58.25ms
step:1667/2330 train_time:97095ms step_avg:58.25ms
step:1668/2330 train_time:97156ms step_avg:58.25ms
step:1669/2330 train_time:97214ms step_avg:58.25ms
step:1670/2330 train_time:97274ms step_avg:58.25ms
step:1671/2330 train_time:97333ms step_avg:58.25ms
step:1672/2330 train_time:97394ms step_avg:58.25ms
step:1673/2330 train_time:97452ms step_avg:58.25ms
step:1674/2330 train_time:97512ms step_avg:58.25ms
step:1675/2330 train_time:97569ms step_avg:58.25ms
step:1676/2330 train_time:97631ms step_avg:58.25ms
step:1677/2330 train_time:97687ms step_avg:58.25ms
step:1678/2330 train_time:97750ms step_avg:58.25ms
step:1679/2330 train_time:97807ms step_avg:58.25ms
step:1680/2330 train_time:97868ms step_avg:58.26ms
step:1681/2330 train_time:97926ms step_avg:58.25ms
step:1682/2330 train_time:97986ms step_avg:58.26ms
step:1683/2330 train_time:98043ms step_avg:58.25ms
step:1684/2330 train_time:98107ms step_avg:58.26ms
step:1685/2330 train_time:98163ms step_avg:58.26ms
step:1686/2330 train_time:98225ms step_avg:58.26ms
step:1687/2330 train_time:98282ms step_avg:58.26ms
step:1688/2330 train_time:98343ms step_avg:58.26ms
step:1689/2330 train_time:98400ms step_avg:58.26ms
step:1690/2330 train_time:98461ms step_avg:58.26ms
step:1691/2330 train_time:98519ms step_avg:58.26ms
step:1692/2330 train_time:98581ms step_avg:58.26ms
step:1693/2330 train_time:98638ms step_avg:58.26ms
step:1694/2330 train_time:98699ms step_avg:58.26ms
step:1695/2330 train_time:98757ms step_avg:58.26ms
step:1696/2330 train_time:98817ms step_avg:58.26ms
step:1697/2330 train_time:98875ms step_avg:58.26ms
step:1698/2330 train_time:98937ms step_avg:58.27ms
step:1699/2330 train_time:98995ms step_avg:58.27ms
step:1700/2330 train_time:99056ms step_avg:58.27ms
step:1701/2330 train_time:99113ms step_avg:58.27ms
step:1702/2330 train_time:99174ms step_avg:58.27ms
step:1703/2330 train_time:99232ms step_avg:58.27ms
step:1704/2330 train_time:99292ms step_avg:58.27ms
step:1705/2330 train_time:99350ms step_avg:58.27ms
step:1706/2330 train_time:99410ms step_avg:58.27ms
step:1707/2330 train_time:99467ms step_avg:58.27ms
step:1708/2330 train_time:99529ms step_avg:58.27ms
step:1709/2330 train_time:99586ms step_avg:58.27ms
step:1710/2330 train_time:99647ms step_avg:58.27ms
step:1711/2330 train_time:99704ms step_avg:58.27ms
step:1712/2330 train_time:99766ms step_avg:58.27ms
step:1713/2330 train_time:99822ms step_avg:58.27ms
step:1714/2330 train_time:99884ms step_avg:58.28ms
step:1715/2330 train_time:99942ms step_avg:58.27ms
step:1716/2330 train_time:100003ms step_avg:58.28ms
step:1717/2330 train_time:100059ms step_avg:58.28ms
step:1718/2330 train_time:100121ms step_avg:58.28ms
step:1719/2330 train_time:100178ms step_avg:58.28ms
step:1720/2330 train_time:100240ms step_avg:58.28ms
step:1721/2330 train_time:100298ms step_avg:58.28ms
step:1722/2330 train_time:100358ms step_avg:58.28ms
step:1723/2330 train_time:100415ms step_avg:58.28ms
step:1724/2330 train_time:100477ms step_avg:58.28ms
step:1725/2330 train_time:100534ms step_avg:58.28ms
step:1726/2330 train_time:100595ms step_avg:58.28ms
step:1727/2330 train_time:100654ms step_avg:58.28ms
step:1728/2330 train_time:100715ms step_avg:58.28ms
step:1729/2330 train_time:100773ms step_avg:58.28ms
step:1730/2330 train_time:100833ms step_avg:58.29ms
step:1731/2330 train_time:100892ms step_avg:58.29ms
step:1732/2330 train_time:100953ms step_avg:58.29ms
step:1733/2330 train_time:101010ms step_avg:58.29ms
step:1734/2330 train_time:101071ms step_avg:58.29ms
step:1735/2330 train_time:101128ms step_avg:58.29ms
step:1736/2330 train_time:101190ms step_avg:58.29ms
step:1737/2330 train_time:101248ms step_avg:58.29ms
step:1738/2330 train_time:101308ms step_avg:58.29ms
step:1739/2330 train_time:101364ms step_avg:58.29ms
step:1740/2330 train_time:101427ms step_avg:58.29ms
step:1741/2330 train_time:101484ms step_avg:58.29ms
step:1742/2330 train_time:101546ms step_avg:58.29ms
step:1743/2330 train_time:101603ms step_avg:58.29ms
step:1744/2330 train_time:101665ms step_avg:58.29ms
step:1745/2330 train_time:101722ms step_avg:58.29ms
step:1746/2330 train_time:101783ms step_avg:58.30ms
step:1747/2330 train_time:101840ms step_avg:58.29ms
step:1748/2330 train_time:101902ms step_avg:58.30ms
step:1749/2330 train_time:101959ms step_avg:58.30ms
step:1750/2330 train_time:102020ms step_avg:58.30ms
step:1750/2330 val_loss:3.8206 train_time:102104ms step_avg:58.35ms
step:1751/2330 train_time:102122ms step_avg:58.32ms
step:1752/2330 train_time:102142ms step_avg:58.30ms
step:1753/2330 train_time:102197ms step_avg:58.30ms
step:1754/2330 train_time:102265ms step_avg:58.30ms
step:1755/2330 train_time:102321ms step_avg:58.30ms
step:1756/2330 train_time:102387ms step_avg:58.31ms
step:1757/2330 train_time:102443ms step_avg:58.31ms
step:1758/2330 train_time:102504ms step_avg:58.31ms
step:1759/2330 train_time:102560ms step_avg:58.31ms
step:1760/2330 train_time:102620ms step_avg:58.31ms
step:1761/2330 train_time:102677ms step_avg:58.31ms
step:1762/2330 train_time:102736ms step_avg:58.31ms
step:1763/2330 train_time:102793ms step_avg:58.31ms
step:1764/2330 train_time:102852ms step_avg:58.31ms
step:1765/2330 train_time:102909ms step_avg:58.31ms
step:1766/2330 train_time:102969ms step_avg:58.31ms
step:1767/2330 train_time:103031ms step_avg:58.31ms
step:1768/2330 train_time:103094ms step_avg:58.31ms
step:1769/2330 train_time:103154ms step_avg:58.31ms
step:1770/2330 train_time:103215ms step_avg:58.31ms
step:1771/2330 train_time:103274ms step_avg:58.31ms
step:1772/2330 train_time:103334ms step_avg:58.32ms
step:1773/2330 train_time:103392ms step_avg:58.31ms
step:1774/2330 train_time:103453ms step_avg:58.32ms
step:1775/2330 train_time:103510ms step_avg:58.32ms
step:1776/2330 train_time:103571ms step_avg:58.32ms
step:1777/2330 train_time:103629ms step_avg:58.32ms
step:1778/2330 train_time:103689ms step_avg:58.32ms
step:1779/2330 train_time:103745ms step_avg:58.32ms
step:1780/2330 train_time:103805ms step_avg:58.32ms
step:1781/2330 train_time:103861ms step_avg:58.32ms
step:1782/2330 train_time:103921ms step_avg:58.32ms
step:1783/2330 train_time:103979ms step_avg:58.32ms
step:1784/2330 train_time:104042ms step_avg:58.32ms
step:1785/2330 train_time:104099ms step_avg:58.32ms
step:1786/2330 train_time:104163ms step_avg:58.32ms
step:1787/2330 train_time:104220ms step_avg:58.32ms
step:1788/2330 train_time:104284ms step_avg:58.32ms
step:1789/2330 train_time:104341ms step_avg:58.32ms
step:1790/2330 train_time:104403ms step_avg:58.33ms
step:1791/2330 train_time:104459ms step_avg:58.32ms
step:1792/2330 train_time:104521ms step_avg:58.33ms
step:1793/2330 train_time:104578ms step_avg:58.33ms
step:1794/2330 train_time:104638ms step_avg:58.33ms
step:1795/2330 train_time:104694ms step_avg:58.33ms
step:1796/2330 train_time:104755ms step_avg:58.33ms
step:1797/2330 train_time:104811ms step_avg:58.33ms
step:1798/2330 train_time:104872ms step_avg:58.33ms
step:1799/2330 train_time:104930ms step_avg:58.33ms
step:1800/2330 train_time:104991ms step_avg:58.33ms
step:1801/2330 train_time:105050ms step_avg:58.33ms
step:1802/2330 train_time:105111ms step_avg:58.33ms
step:1803/2330 train_time:105170ms step_avg:58.33ms
step:1804/2330 train_time:105231ms step_avg:58.33ms
step:1805/2330 train_time:105290ms step_avg:58.33ms
step:1806/2330 train_time:105350ms step_avg:58.33ms
step:1807/2330 train_time:105408ms step_avg:58.33ms
step:1808/2330 train_time:105469ms step_avg:58.33ms
step:1809/2330 train_time:105527ms step_avg:58.33ms
step:1810/2330 train_time:105588ms step_avg:58.34ms
step:1811/2330 train_time:105645ms step_avg:58.34ms
step:1812/2330 train_time:105706ms step_avg:58.34ms
step:1813/2330 train_time:105762ms step_avg:58.34ms
step:1814/2330 train_time:105822ms step_avg:58.34ms
step:1815/2330 train_time:105879ms step_avg:58.34ms
step:1816/2330 train_time:105941ms step_avg:58.34ms
step:1817/2330 train_time:105997ms step_avg:58.34ms
step:1818/2330 train_time:106059ms step_avg:58.34ms
step:1819/2330 train_time:106117ms step_avg:58.34ms
step:1820/2330 train_time:106180ms step_avg:58.34ms
step:1821/2330 train_time:106237ms step_avg:58.34ms
step:1822/2330 train_time:106300ms step_avg:58.34ms
step:1823/2330 train_time:106357ms step_avg:58.34ms
step:1824/2330 train_time:106419ms step_avg:58.34ms
step:1825/2330 train_time:106475ms step_avg:58.34ms
step:1826/2330 train_time:106535ms step_avg:58.34ms
step:1827/2330 train_time:106592ms step_avg:58.34ms
step:1828/2330 train_time:106654ms step_avg:58.34ms
step:1829/2330 train_time:106711ms step_avg:58.34ms
step:1830/2330 train_time:106772ms step_avg:58.35ms
step:1831/2330 train_time:106830ms step_avg:58.35ms
step:1832/2330 train_time:106890ms step_avg:58.35ms
step:1833/2330 train_time:106948ms step_avg:58.35ms
step:1834/2330 train_time:107009ms step_avg:58.35ms
step:1835/2330 train_time:107067ms step_avg:58.35ms
step:1836/2330 train_time:107127ms step_avg:58.35ms
step:1837/2330 train_time:107185ms step_avg:58.35ms
step:1838/2330 train_time:107245ms step_avg:58.35ms
step:1839/2330 train_time:107302ms step_avg:58.35ms
step:1840/2330 train_time:107365ms step_avg:58.35ms
step:1841/2330 train_time:107422ms step_avg:58.35ms
step:1842/2330 train_time:107484ms step_avg:58.35ms
step:1843/2330 train_time:107541ms step_avg:58.35ms
step:1844/2330 train_time:107603ms step_avg:58.35ms
step:1845/2330 train_time:107660ms step_avg:58.35ms
step:1846/2330 train_time:107721ms step_avg:58.35ms
step:1847/2330 train_time:107777ms step_avg:58.35ms
step:1848/2330 train_time:107838ms step_avg:58.35ms
step:1849/2330 train_time:107894ms step_avg:58.35ms
step:1850/2330 train_time:107957ms step_avg:58.36ms
step:1851/2330 train_time:108013ms step_avg:58.35ms
step:1852/2330 train_time:108076ms step_avg:58.36ms
step:1853/2330 train_time:108132ms step_avg:58.36ms
step:1854/2330 train_time:108194ms step_avg:58.36ms
step:1855/2330 train_time:108252ms step_avg:58.36ms
step:1856/2330 train_time:108316ms step_avg:58.36ms
step:1857/2330 train_time:108374ms step_avg:58.36ms
step:1858/2330 train_time:108435ms step_avg:58.36ms
step:1859/2330 train_time:108493ms step_avg:58.36ms
step:1860/2330 train_time:108554ms step_avg:58.36ms
step:1861/2330 train_time:108612ms step_avg:58.36ms
step:1862/2330 train_time:108672ms step_avg:58.36ms
step:1863/2330 train_time:108729ms step_avg:58.36ms
step:1864/2330 train_time:108789ms step_avg:58.36ms
step:1865/2330 train_time:108846ms step_avg:58.36ms
step:1866/2330 train_time:108907ms step_avg:58.36ms
step:1867/2330 train_time:108964ms step_avg:58.36ms
step:1868/2330 train_time:109026ms step_avg:58.36ms
step:1869/2330 train_time:109082ms step_avg:58.36ms
step:1870/2330 train_time:109145ms step_avg:58.37ms
step:1871/2330 train_time:109201ms step_avg:58.37ms
step:1872/2330 train_time:109263ms step_avg:58.37ms
step:1873/2330 train_time:109320ms step_avg:58.37ms
step:1874/2330 train_time:109383ms step_avg:58.37ms
step:1875/2330 train_time:109440ms step_avg:58.37ms
step:1876/2330 train_time:109501ms step_avg:58.37ms
step:1877/2330 train_time:109557ms step_avg:58.37ms
step:1878/2330 train_time:109620ms step_avg:58.37ms
step:1879/2330 train_time:109677ms step_avg:58.37ms
step:1880/2330 train_time:109737ms step_avg:58.37ms
step:1881/2330 train_time:109794ms step_avg:58.37ms
step:1882/2330 train_time:109856ms step_avg:58.37ms
step:1883/2330 train_time:109912ms step_avg:58.37ms
step:1884/2330 train_time:109974ms step_avg:58.37ms
step:1885/2330 train_time:110032ms step_avg:58.37ms
step:1886/2330 train_time:110092ms step_avg:58.37ms
step:1887/2330 train_time:110150ms step_avg:58.37ms
step:1888/2330 train_time:110211ms step_avg:58.37ms
step:1889/2330 train_time:110268ms step_avg:58.37ms
step:1890/2330 train_time:110331ms step_avg:58.38ms
step:1891/2330 train_time:110390ms step_avg:58.38ms
step:1892/2330 train_time:110450ms step_avg:58.38ms
step:1893/2330 train_time:110508ms step_avg:58.38ms
step:1894/2330 train_time:110570ms step_avg:58.38ms
step:1895/2330 train_time:110627ms step_avg:58.38ms
step:1896/2330 train_time:110688ms step_avg:58.38ms
step:1897/2330 train_time:110746ms step_avg:58.38ms
step:1898/2330 train_time:110807ms step_avg:58.38ms
step:1899/2330 train_time:110863ms step_avg:58.38ms
step:1900/2330 train_time:110925ms step_avg:58.38ms
step:1901/2330 train_time:110981ms step_avg:58.38ms
step:1902/2330 train_time:111045ms step_avg:58.38ms
step:1903/2330 train_time:111101ms step_avg:58.38ms
step:1904/2330 train_time:111162ms step_avg:58.38ms
step:1905/2330 train_time:111219ms step_avg:58.38ms
step:1906/2330 train_time:111282ms step_avg:58.38ms
step:1907/2330 train_time:111338ms step_avg:58.38ms
step:1908/2330 train_time:111400ms step_avg:58.39ms
step:1909/2330 train_time:111456ms step_avg:58.38ms
step:1910/2330 train_time:111518ms step_avg:58.39ms
step:1911/2330 train_time:111575ms step_avg:58.39ms
step:1912/2330 train_time:111638ms step_avg:58.39ms
step:1913/2330 train_time:111694ms step_avg:58.39ms
step:1914/2330 train_time:111757ms step_avg:58.39ms
step:1915/2330 train_time:111814ms step_avg:58.39ms
step:1916/2330 train_time:111875ms step_avg:58.39ms
step:1917/2330 train_time:111933ms step_avg:58.39ms
step:1918/2330 train_time:111993ms step_avg:58.39ms
step:1919/2330 train_time:112051ms step_avg:58.39ms
step:1920/2330 train_time:112112ms step_avg:58.39ms
step:1921/2330 train_time:112170ms step_avg:58.39ms
step:1922/2330 train_time:112231ms step_avg:58.39ms
step:1923/2330 train_time:112289ms step_avg:58.39ms
step:1924/2330 train_time:112350ms step_avg:58.39ms
step:1925/2330 train_time:112407ms step_avg:58.39ms
step:1926/2330 train_time:112468ms step_avg:58.39ms
step:1927/2330 train_time:112525ms step_avg:58.39ms
step:1928/2330 train_time:112586ms step_avg:58.40ms
step:1929/2330 train_time:112643ms step_avg:58.39ms
step:1930/2330 train_time:112705ms step_avg:58.40ms
step:1931/2330 train_time:112761ms step_avg:58.40ms
step:1932/2330 train_time:112822ms step_avg:58.40ms
step:1933/2330 train_time:112879ms step_avg:58.40ms
step:1934/2330 train_time:112941ms step_avg:58.40ms
step:1935/2330 train_time:112998ms step_avg:58.40ms
step:1936/2330 train_time:113061ms step_avg:58.40ms
step:1937/2330 train_time:113118ms step_avg:58.40ms
step:1938/2330 train_time:113179ms step_avg:58.40ms
step:1939/2330 train_time:113236ms step_avg:58.40ms
step:1940/2330 train_time:113299ms step_avg:58.40ms
step:1941/2330 train_time:113356ms step_avg:58.40ms
step:1942/2330 train_time:113418ms step_avg:58.40ms
step:1943/2330 train_time:113475ms step_avg:58.40ms
step:1944/2330 train_time:113537ms step_avg:58.40ms
step:1945/2330 train_time:113593ms step_avg:58.40ms
step:1946/2330 train_time:113655ms step_avg:58.40ms
step:1947/2330 train_time:113712ms step_avg:58.40ms
step:1948/2330 train_time:113774ms step_avg:58.41ms
step:1949/2330 train_time:113832ms step_avg:58.41ms
step:1950/2330 train_time:113893ms step_avg:58.41ms
step:1951/2330 train_time:113950ms step_avg:58.41ms
step:1952/2330 train_time:114010ms step_avg:58.41ms
step:1953/2330 train_time:114067ms step_avg:58.41ms
step:1954/2330 train_time:114128ms step_avg:58.41ms
step:1955/2330 train_time:114185ms step_avg:58.41ms
step:1956/2330 train_time:114247ms step_avg:58.41ms
step:1957/2330 train_time:114304ms step_avg:58.41ms
step:1958/2330 train_time:114366ms step_avg:58.41ms
step:1959/2330 train_time:114423ms step_avg:58.41ms
step:1960/2330 train_time:114484ms step_avg:58.41ms
step:1961/2330 train_time:114541ms step_avg:58.41ms
step:1962/2330 train_time:114603ms step_avg:58.41ms
step:1963/2330 train_time:114659ms step_avg:58.41ms
step:1964/2330 train_time:114722ms step_avg:58.41ms
step:1965/2330 train_time:114778ms step_avg:58.41ms
step:1966/2330 train_time:114841ms step_avg:58.41ms
step:1967/2330 train_time:114898ms step_avg:58.41ms
step:1968/2330 train_time:114959ms step_avg:58.41ms
step:1969/2330 train_time:115015ms step_avg:58.41ms
step:1970/2330 train_time:115077ms step_avg:58.41ms
step:1971/2330 train_time:115134ms step_avg:58.41ms
step:1972/2330 train_time:115196ms step_avg:58.42ms
step:1973/2330 train_time:115253ms step_avg:58.42ms
step:1974/2330 train_time:115316ms step_avg:58.42ms
step:1975/2330 train_time:115374ms step_avg:58.42ms
step:1976/2330 train_time:115434ms step_avg:58.42ms
step:1977/2330 train_time:115492ms step_avg:58.42ms
step:1978/2330 train_time:115553ms step_avg:58.42ms
step:1979/2330 train_time:115610ms step_avg:58.42ms
step:1980/2330 train_time:115672ms step_avg:58.42ms
step:1981/2330 train_time:115730ms step_avg:58.42ms
step:1982/2330 train_time:115790ms step_avg:58.42ms
step:1983/2330 train_time:115848ms step_avg:58.42ms
step:1984/2330 train_time:115909ms step_avg:58.42ms
step:1985/2330 train_time:115966ms step_avg:58.42ms
step:1986/2330 train_time:116027ms step_avg:58.42ms
step:1987/2330 train_time:116085ms step_avg:58.42ms
step:1988/2330 train_time:116146ms step_avg:58.42ms
step:1989/2330 train_time:116203ms step_avg:58.42ms
step:1990/2330 train_time:116264ms step_avg:58.42ms
step:1991/2330 train_time:116320ms step_avg:58.42ms
step:1992/2330 train_time:116383ms step_avg:58.43ms
step:1993/2330 train_time:116440ms step_avg:58.42ms
step:1994/2330 train_time:116502ms step_avg:58.43ms
step:1995/2330 train_time:116559ms step_avg:58.43ms
step:1996/2330 train_time:116621ms step_avg:58.43ms
step:1997/2330 train_time:116677ms step_avg:58.43ms
step:1998/2330 train_time:116739ms step_avg:58.43ms
step:1999/2330 train_time:116795ms step_avg:58.43ms
step:2000/2330 train_time:116858ms step_avg:58.43ms
step:2000/2330 val_loss:3.7571 train_time:116940ms step_avg:58.47ms
step:2001/2330 train_time:116958ms step_avg:58.45ms
step:2002/2330 train_time:116979ms step_avg:58.43ms
step:2003/2330 train_time:117037ms step_avg:58.43ms
step:2004/2330 train_time:117104ms step_avg:58.43ms
step:2005/2330 train_time:117161ms step_avg:58.43ms
step:2006/2330 train_time:117222ms step_avg:58.44ms
step:2007/2330 train_time:117279ms step_avg:58.43ms
step:2008/2330 train_time:117341ms step_avg:58.44ms
step:2009/2330 train_time:117397ms step_avg:58.44ms
step:2010/2330 train_time:117458ms step_avg:58.44ms
step:2011/2330 train_time:117514ms step_avg:58.44ms
step:2012/2330 train_time:117574ms step_avg:58.44ms
step:2013/2330 train_time:117631ms step_avg:58.44ms
step:2014/2330 train_time:117691ms step_avg:58.44ms
step:2015/2330 train_time:117747ms step_avg:58.44ms
step:2016/2330 train_time:117807ms step_avg:58.44ms
step:2017/2330 train_time:117864ms step_avg:58.44ms
step:2018/2330 train_time:117925ms step_avg:58.44ms
step:2019/2330 train_time:117983ms step_avg:58.44ms
step:2020/2330 train_time:118046ms step_avg:58.44ms
step:2021/2330 train_time:118105ms step_avg:58.44ms
step:2022/2330 train_time:118165ms step_avg:58.44ms
step:2023/2330 train_time:118222ms step_avg:58.44ms
step:2024/2330 train_time:118284ms step_avg:58.44ms
step:2025/2330 train_time:118341ms step_avg:58.44ms
step:2026/2330 train_time:118402ms step_avg:58.44ms
step:2027/2330 train_time:118459ms step_avg:58.44ms
step:2028/2330 train_time:118520ms step_avg:58.44ms
step:2029/2330 train_time:118576ms step_avg:58.44ms
step:2030/2330 train_time:118636ms step_avg:58.44ms
step:2031/2330 train_time:118693ms step_avg:58.44ms
step:2032/2330 train_time:118753ms step_avg:58.44ms
step:2033/2330 train_time:118809ms step_avg:58.44ms
step:2034/2330 train_time:118872ms step_avg:58.44ms
step:2035/2330 train_time:118930ms step_avg:58.44ms
step:2036/2330 train_time:118993ms step_avg:58.44ms
step:2037/2330 train_time:119051ms step_avg:58.44ms
step:2038/2330 train_time:119113ms step_avg:58.45ms
step:2039/2330 train_time:119172ms step_avg:58.45ms
step:2040/2330 train_time:119234ms step_avg:58.45ms
step:2041/2330 train_time:119291ms step_avg:58.45ms
step:2042/2330 train_time:119352ms step_avg:58.45ms
step:2043/2330 train_time:119410ms step_avg:58.45ms
step:2044/2330 train_time:119471ms step_avg:58.45ms
step:2045/2330 train_time:119528ms step_avg:58.45ms
step:2046/2330 train_time:119590ms step_avg:58.45ms
step:2047/2330 train_time:119647ms step_avg:58.45ms
step:2048/2330 train_time:119707ms step_avg:58.45ms
step:2049/2330 train_time:119763ms step_avg:58.45ms
step:2050/2330 train_time:119824ms step_avg:58.45ms
step:2051/2330 train_time:119880ms step_avg:58.45ms
step:2052/2330 train_time:119942ms step_avg:58.45ms
step:2053/2330 train_time:119999ms step_avg:58.45ms
step:2054/2330 train_time:120061ms step_avg:58.45ms
step:2055/2330 train_time:120117ms step_avg:58.45ms
step:2056/2330 train_time:120182ms step_avg:58.45ms
step:2057/2330 train_time:120240ms step_avg:58.45ms
step:2058/2330 train_time:120301ms step_avg:58.46ms
step:2059/2330 train_time:120358ms step_avg:58.45ms
step:2060/2330 train_time:120419ms step_avg:58.46ms
step:2061/2330 train_time:120475ms step_avg:58.45ms
step:2062/2330 train_time:120536ms step_avg:58.46ms
step:2063/2330 train_time:120593ms step_avg:58.46ms
step:2064/2330 train_time:120654ms step_avg:58.46ms
step:2065/2330 train_time:120712ms step_avg:58.46ms
step:2066/2330 train_time:120772ms step_avg:58.46ms
step:2067/2330 train_time:120829ms step_avg:58.46ms
step:2068/2330 train_time:120890ms step_avg:58.46ms
step:2069/2330 train_time:120948ms step_avg:58.46ms
step:2070/2330 train_time:121009ms step_avg:58.46ms
step:2071/2330 train_time:121067ms step_avg:58.46ms
step:2072/2330 train_time:121128ms step_avg:58.46ms
step:2073/2330 train_time:121186ms step_avg:58.46ms
step:2074/2330 train_time:121247ms step_avg:58.46ms
step:2075/2330 train_time:121306ms step_avg:58.46ms
step:2076/2330 train_time:121365ms step_avg:58.46ms
step:2077/2330 train_time:121422ms step_avg:58.46ms
step:2078/2330 train_time:121482ms step_avg:58.46ms
step:2079/2330 train_time:121539ms step_avg:58.46ms
step:2080/2330 train_time:121601ms step_avg:58.46ms
step:2081/2330 train_time:121657ms step_avg:58.46ms
step:2082/2330 train_time:121719ms step_avg:58.46ms
step:2083/2330 train_time:121775ms step_avg:58.46ms
step:2084/2330 train_time:121836ms step_avg:58.46ms
step:2085/2330 train_time:121893ms step_avg:58.46ms
step:2086/2330 train_time:121955ms step_avg:58.46ms
step:2087/2330 train_time:122012ms step_avg:58.46ms
step:2088/2330 train_time:122074ms step_avg:58.46ms
step:2089/2330 train_time:122131ms step_avg:58.46ms
step:2090/2330 train_time:122194ms step_avg:58.47ms
step:2091/2330 train_time:122251ms step_avg:58.47ms
step:2092/2330 train_time:122313ms step_avg:58.47ms
step:2093/2330 train_time:122371ms step_avg:58.47ms
step:2094/2330 train_time:122432ms step_avg:58.47ms
step:2095/2330 train_time:122491ms step_avg:58.47ms
step:2096/2330 train_time:122551ms step_avg:58.47ms
step:2097/2330 train_time:122609ms step_avg:58.47ms
step:2098/2330 train_time:122669ms step_avg:58.47ms
step:2099/2330 train_time:122725ms step_avg:58.47ms
step:2100/2330 train_time:122786ms step_avg:58.47ms
step:2101/2330 train_time:122843ms step_avg:58.47ms
step:2102/2330 train_time:122903ms step_avg:58.47ms
step:2103/2330 train_time:122960ms step_avg:58.47ms
step:2104/2330 train_time:123022ms step_avg:58.47ms
step:2105/2330 train_time:123079ms step_avg:58.47ms
step:2106/2330 train_time:123140ms step_avg:58.47ms
step:2107/2330 train_time:123197ms step_avg:58.47ms
step:2108/2330 train_time:123258ms step_avg:58.47ms
step:2109/2330 train_time:123315ms step_avg:58.47ms
step:2110/2330 train_time:123378ms step_avg:58.47ms
step:2111/2330 train_time:123435ms step_avg:58.47ms
step:2112/2330 train_time:123496ms step_avg:58.47ms
step:2113/2330 train_time:123554ms step_avg:58.47ms
step:2114/2330 train_time:123615ms step_avg:58.47ms
step:2115/2330 train_time:123671ms step_avg:58.47ms
step:2116/2330 train_time:123734ms step_avg:58.48ms
step:2117/2330 train_time:123792ms step_avg:58.48ms
step:2118/2330 train_time:123852ms step_avg:58.48ms
step:2119/2330 train_time:123909ms step_avg:58.48ms
step:2120/2330 train_time:123970ms step_avg:58.48ms
step:2121/2330 train_time:124027ms step_avg:58.48ms
step:2122/2330 train_time:124088ms step_avg:58.48ms
step:2123/2330 train_time:124145ms step_avg:58.48ms
step:2124/2330 train_time:124207ms step_avg:58.48ms
step:2125/2330 train_time:124265ms step_avg:58.48ms
step:2126/2330 train_time:124325ms step_avg:58.48ms
step:2127/2330 train_time:124381ms step_avg:58.48ms
step:2128/2330 train_time:124444ms step_avg:58.48ms
step:2129/2330 train_time:124501ms step_avg:58.48ms
step:2130/2330 train_time:124564ms step_avg:58.48ms
step:2131/2330 train_time:124620ms step_avg:58.48ms
step:2132/2330 train_time:124682ms step_avg:58.48ms
step:2133/2330 train_time:124739ms step_avg:58.48ms
step:2134/2330 train_time:124800ms step_avg:58.48ms
step:2135/2330 train_time:124857ms step_avg:58.48ms
step:2136/2330 train_time:124918ms step_avg:58.48ms
step:2137/2330 train_time:124975ms step_avg:58.48ms
step:2138/2330 train_time:125038ms step_avg:58.48ms
step:2139/2330 train_time:125095ms step_avg:58.48ms
step:2140/2330 train_time:125155ms step_avg:58.48ms
step:2141/2330 train_time:125212ms step_avg:58.48ms
step:2142/2330 train_time:125274ms step_avg:58.48ms
step:2143/2330 train_time:125332ms step_avg:58.48ms
step:2144/2330 train_time:125393ms step_avg:58.49ms
step:2145/2330 train_time:125451ms step_avg:58.49ms
step:2146/2330 train_time:125512ms step_avg:58.49ms
step:2147/2330 train_time:125569ms step_avg:58.49ms
step:2148/2330 train_time:125631ms step_avg:58.49ms
step:2149/2330 train_time:125688ms step_avg:58.49ms
step:2150/2330 train_time:125750ms step_avg:58.49ms
step:2151/2330 train_time:125807ms step_avg:58.49ms
step:2152/2330 train_time:125868ms step_avg:58.49ms
step:2153/2330 train_time:125925ms step_avg:58.49ms
step:2154/2330 train_time:125985ms step_avg:58.49ms
step:2155/2330 train_time:126043ms step_avg:58.49ms
step:2156/2330 train_time:126103ms step_avg:58.49ms
step:2157/2330 train_time:126160ms step_avg:58.49ms
step:2158/2330 train_time:126222ms step_avg:58.49ms
step:2159/2330 train_time:126279ms step_avg:58.49ms
step:2160/2330 train_time:126341ms step_avg:58.49ms
step:2161/2330 train_time:126399ms step_avg:58.49ms
step:2162/2330 train_time:126459ms step_avg:58.49ms
step:2163/2330 train_time:126515ms step_avg:58.49ms
step:2164/2330 train_time:126577ms step_avg:58.49ms
step:2165/2330 train_time:126634ms step_avg:58.49ms
step:2166/2330 train_time:126695ms step_avg:58.49ms
step:2167/2330 train_time:126752ms step_avg:58.49ms
step:2168/2330 train_time:126813ms step_avg:58.49ms
step:2169/2330 train_time:126870ms step_avg:58.49ms
step:2170/2330 train_time:126932ms step_avg:58.49ms
step:2171/2330 train_time:126990ms step_avg:58.49ms
step:2172/2330 train_time:127052ms step_avg:58.50ms
step:2173/2330 train_time:127110ms step_avg:58.50ms
step:2174/2330 train_time:127170ms step_avg:58.50ms
step:2175/2330 train_time:127228ms step_avg:58.50ms
step:2176/2330 train_time:127290ms step_avg:58.50ms
step:2177/2330 train_time:127348ms step_avg:58.50ms
step:2178/2330 train_time:127410ms step_avg:58.50ms
step:2179/2330 train_time:127468ms step_avg:58.50ms
step:2180/2330 train_time:127529ms step_avg:58.50ms
step:2181/2330 train_time:127587ms step_avg:58.50ms
step:2182/2330 train_time:127647ms step_avg:58.50ms
step:2183/2330 train_time:127704ms step_avg:58.50ms
step:2184/2330 train_time:127764ms step_avg:58.50ms
step:2185/2330 train_time:127820ms step_avg:58.50ms
step:2186/2330 train_time:127883ms step_avg:58.50ms
step:2187/2330 train_time:127939ms step_avg:58.50ms
step:2188/2330 train_time:128001ms step_avg:58.50ms
step:2189/2330 train_time:128057ms step_avg:58.50ms
step:2190/2330 train_time:128118ms step_avg:58.50ms
step:2191/2330 train_time:128175ms step_avg:58.50ms
step:2192/2330 train_time:128237ms step_avg:58.50ms
step:2193/2330 train_time:128294ms step_avg:58.50ms
step:2194/2330 train_time:128356ms step_avg:58.50ms
step:2195/2330 train_time:128413ms step_avg:58.50ms
step:2196/2330 train_time:128474ms step_avg:58.50ms
step:2197/2330 train_time:128530ms step_avg:58.50ms
step:2198/2330 train_time:128593ms step_avg:58.50ms
step:2199/2330 train_time:128652ms step_avg:58.50ms
step:2200/2330 train_time:128712ms step_avg:58.51ms
step:2201/2330 train_time:128770ms step_avg:58.51ms
step:2202/2330 train_time:128831ms step_avg:58.51ms
step:2203/2330 train_time:128889ms step_avg:58.51ms
step:2204/2330 train_time:128950ms step_avg:58.51ms
step:2205/2330 train_time:129007ms step_avg:58.51ms
step:2206/2330 train_time:129068ms step_avg:58.51ms
step:2207/2330 train_time:129125ms step_avg:58.51ms
step:2208/2330 train_time:129185ms step_avg:58.51ms
step:2209/2330 train_time:129242ms step_avg:58.51ms
step:2210/2330 train_time:129303ms step_avg:58.51ms
step:2211/2330 train_time:129360ms step_avg:58.51ms
step:2212/2330 train_time:129422ms step_avg:58.51ms
step:2213/2330 train_time:129479ms step_avg:58.51ms
step:2214/2330 train_time:129541ms step_avg:58.51ms
step:2215/2330 train_time:129598ms step_avg:58.51ms
step:2216/2330 train_time:129660ms step_avg:58.51ms
step:2217/2330 train_time:129716ms step_avg:58.51ms
step:2218/2330 train_time:129777ms step_avg:58.51ms
step:2219/2330 train_time:129834ms step_avg:58.51ms
step:2220/2330 train_time:129895ms step_avg:58.51ms
step:2221/2330 train_time:129952ms step_avg:58.51ms
step:2222/2330 train_time:130014ms step_avg:58.51ms
step:2223/2330 train_time:130071ms step_avg:58.51ms
step:2224/2330 train_time:130132ms step_avg:58.51ms
step:2225/2330 train_time:130190ms step_avg:58.51ms
step:2226/2330 train_time:130252ms step_avg:58.51ms
step:2227/2330 train_time:130309ms step_avg:58.51ms
step:2228/2330 train_time:130371ms step_avg:58.51ms
step:2229/2330 train_time:130429ms step_avg:58.51ms
step:2230/2330 train_time:130490ms step_avg:58.52ms
step:2231/2330 train_time:130549ms step_avg:58.52ms
step:2232/2330 train_time:130609ms step_avg:58.52ms
step:2233/2330 train_time:130667ms step_avg:58.52ms
step:2234/2330 train_time:130728ms step_avg:58.52ms
step:2235/2330 train_time:130785ms step_avg:58.52ms
step:2236/2330 train_time:130846ms step_avg:58.52ms
step:2237/2330 train_time:130903ms step_avg:58.52ms
step:2238/2330 train_time:130965ms step_avg:58.52ms
step:2239/2330 train_time:131021ms step_avg:58.52ms
step:2240/2330 train_time:131083ms step_avg:58.52ms
step:2241/2330 train_time:131140ms step_avg:58.52ms
step:2242/2330 train_time:131200ms step_avg:58.52ms
step:2243/2330 train_time:131257ms step_avg:58.52ms
step:2244/2330 train_time:131319ms step_avg:58.52ms
step:2245/2330 train_time:131375ms step_avg:58.52ms
step:2246/2330 train_time:131438ms step_avg:58.52ms
step:2247/2330 train_time:131494ms step_avg:58.52ms
step:2248/2330 train_time:131557ms step_avg:58.52ms
step:2249/2330 train_time:131614ms step_avg:58.52ms
step:2250/2330 train_time:131676ms step_avg:58.52ms
step:2250/2330 val_loss:3.7071 train_time:131757ms step_avg:58.56ms
step:2251/2330 train_time:131775ms step_avg:58.54ms
step:2252/2330 train_time:131795ms step_avg:58.52ms
step:2253/2330 train_time:131854ms step_avg:58.52ms
step:2254/2330 train_time:131922ms step_avg:58.53ms
step:2255/2330 train_time:131979ms step_avg:58.53ms
step:2256/2330 train_time:132041ms step_avg:58.53ms
step:2257/2330 train_time:132098ms step_avg:58.53ms
step:2258/2330 train_time:132158ms step_avg:58.53ms
step:2259/2330 train_time:132215ms step_avg:58.53ms
step:2260/2330 train_time:132275ms step_avg:58.53ms
step:2261/2330 train_time:132332ms step_avg:58.53ms
step:2262/2330 train_time:132392ms step_avg:58.53ms
step:2263/2330 train_time:132448ms step_avg:58.53ms
step:2264/2330 train_time:132509ms step_avg:58.53ms
step:2265/2330 train_time:132566ms step_avg:58.53ms
step:2266/2330 train_time:132626ms step_avg:58.53ms
step:2267/2330 train_time:132684ms step_avg:58.53ms
step:2268/2330 train_time:132745ms step_avg:58.53ms
step:2269/2330 train_time:132805ms step_avg:58.53ms
step:2270/2330 train_time:132866ms step_avg:58.53ms
step:2271/2330 train_time:132923ms step_avg:58.53ms
step:2272/2330 train_time:132985ms step_avg:58.53ms
step:2273/2330 train_time:133042ms step_avg:58.53ms
step:2274/2330 train_time:133104ms step_avg:58.53ms
step:2275/2330 train_time:133161ms step_avg:58.53ms
step:2276/2330 train_time:133222ms step_avg:58.53ms
step:2277/2330 train_time:133279ms step_avg:58.53ms
step:2278/2330 train_time:133339ms step_avg:58.53ms
step:2279/2330 train_time:133396ms step_avg:58.53ms
step:2280/2330 train_time:133457ms step_avg:58.53ms
step:2281/2330 train_time:133513ms step_avg:58.53ms
step:2282/2330 train_time:133573ms step_avg:58.53ms
step:2283/2330 train_time:133630ms step_avg:58.53ms
step:2284/2330 train_time:133692ms step_avg:58.53ms
step:2285/2330 train_time:133749ms step_avg:58.53ms
step:2286/2330 train_time:133812ms step_avg:58.54ms
step:2287/2330 train_time:133871ms step_avg:58.54ms
step:2288/2330 train_time:133932ms step_avg:58.54ms
step:2289/2330 train_time:133990ms step_avg:58.54ms
step:2290/2330 train_time:134053ms step_avg:58.54ms
step:2291/2330 train_time:134112ms step_avg:58.54ms
step:2292/2330 train_time:134172ms step_avg:58.54ms
step:2293/2330 train_time:134229ms step_avg:58.54ms
step:2294/2330 train_time:134290ms step_avg:58.54ms
step:2295/2330 train_time:134347ms step_avg:58.54ms
step:2296/2330 train_time:134407ms step_avg:58.54ms
step:2297/2330 train_time:134464ms step_avg:58.54ms
step:2298/2330 train_time:134524ms step_avg:58.54ms
step:2299/2330 train_time:134581ms step_avg:58.54ms
step:2300/2330 train_time:134641ms step_avg:58.54ms
step:2301/2330 train_time:134698ms step_avg:58.54ms
step:2302/2330 train_time:134759ms step_avg:58.54ms
step:2303/2330 train_time:134816ms step_avg:58.54ms
step:2304/2330 train_time:134877ms step_avg:58.54ms
step:2305/2330 train_time:134934ms step_avg:58.54ms
step:2306/2330 train_time:134997ms step_avg:58.54ms
step:2307/2330 train_time:135054ms step_avg:58.54ms
step:2308/2330 train_time:135117ms step_avg:58.54ms
step:2309/2330 train_time:135174ms step_avg:58.54ms
step:2310/2330 train_time:135235ms step_avg:58.54ms
step:2311/2330 train_time:135293ms step_avg:58.54ms
step:2312/2330 train_time:135353ms step_avg:58.54ms
step:2313/2330 train_time:135410ms step_avg:58.54ms
step:2314/2330 train_time:135471ms step_avg:58.54ms
step:2315/2330 train_time:135528ms step_avg:58.54ms
step:2316/2330 train_time:135589ms step_avg:58.54ms
step:2317/2330 train_time:135646ms step_avg:58.54ms
step:2318/2330 train_time:135707ms step_avg:58.54ms
step:2319/2330 train_time:135765ms step_avg:58.54ms
step:2320/2330 train_time:135826ms step_avg:58.55ms
step:2321/2330 train_time:135883ms step_avg:58.55ms
step:2322/2330 train_time:135946ms step_avg:58.55ms
step:2323/2330 train_time:136003ms step_avg:58.55ms
step:2324/2330 train_time:136066ms step_avg:58.55ms
step:2325/2330 train_time:136123ms step_avg:58.55ms
step:2326/2330 train_time:136185ms step_avg:58.55ms
step:2327/2330 train_time:136241ms step_avg:58.55ms
step:2328/2330 train_time:136302ms step_avg:58.55ms
step:2329/2330 train_time:136359ms step_avg:58.55ms
step:2330/2330 train_time:136419ms step_avg:58.55ms
step:2330/2330 val_loss:3.6912 train_time:136501ms step_avg:58.58ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
