import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:38:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:104ms step_avg:103.98ms
step:2/2330 train_time:209ms step_avg:104.36ms
step:3/2330 train_time:227ms step_avg:75.67ms
step:4/2330 train_time:246ms step_avg:61.46ms
step:5/2330 train_time:300ms step_avg:59.97ms
step:6/2330 train_time:357ms step_avg:59.58ms
step:7/2330 train_time:413ms step_avg:58.96ms
step:8/2330 train_time:471ms step_avg:58.93ms
step:9/2330 train_time:527ms step_avg:58.51ms
step:10/2330 train_time:585ms step_avg:58.47ms
step:11/2330 train_time:640ms step_avg:58.21ms
step:12/2330 train_time:699ms step_avg:58.22ms
step:13/2330 train_time:754ms step_avg:58.01ms
step:14/2330 train_time:812ms step_avg:58.01ms
step:15/2330 train_time:868ms step_avg:57.84ms
step:16/2330 train_time:926ms step_avg:57.87ms
step:17/2330 train_time:981ms step_avg:57.70ms
step:18/2330 train_time:1039ms step_avg:57.74ms
step:19/2330 train_time:1096ms step_avg:57.68ms
step:20/2330 train_time:1161ms step_avg:58.04ms
step:21/2330 train_time:1219ms step_avg:58.03ms
step:22/2330 train_time:1280ms step_avg:58.18ms
step:23/2330 train_time:1336ms step_avg:58.09ms
step:24/2330 train_time:1395ms step_avg:58.12ms
step:25/2330 train_time:1451ms step_avg:58.02ms
step:26/2330 train_time:1510ms step_avg:58.09ms
step:27/2330 train_time:1566ms step_avg:57.99ms
step:28/2330 train_time:1624ms step_avg:58.00ms
step:29/2330 train_time:1679ms step_avg:57.91ms
step:30/2330 train_time:1738ms step_avg:57.92ms
step:31/2330 train_time:1793ms step_avg:57.83ms
step:32/2330 train_time:1852ms step_avg:57.86ms
step:33/2330 train_time:1907ms step_avg:57.78ms
step:34/2330 train_time:1966ms step_avg:57.81ms
step:35/2330 train_time:2021ms step_avg:57.75ms
step:36/2330 train_time:2080ms step_avg:57.78ms
step:37/2330 train_time:2137ms step_avg:57.74ms
step:38/2330 train_time:2198ms step_avg:57.85ms
step:39/2330 train_time:2255ms step_avg:57.83ms
step:40/2330 train_time:2314ms step_avg:57.85ms
step:41/2330 train_time:2370ms step_avg:57.81ms
step:42/2330 train_time:2430ms step_avg:57.85ms
step:43/2330 train_time:2486ms step_avg:57.81ms
step:44/2330 train_time:2544ms step_avg:57.82ms
step:45/2330 train_time:2600ms step_avg:57.77ms
step:46/2330 train_time:2658ms step_avg:57.78ms
step:47/2330 train_time:2714ms step_avg:57.74ms
step:48/2330 train_time:2772ms step_avg:57.75ms
step:49/2330 train_time:2828ms step_avg:57.71ms
step:50/2330 train_time:2886ms step_avg:57.72ms
step:51/2330 train_time:2941ms step_avg:57.68ms
step:52/2330 train_time:3000ms step_avg:57.70ms
step:53/2330 train_time:3057ms step_avg:57.67ms
step:54/2330 train_time:3116ms step_avg:57.70ms
step:55/2330 train_time:3172ms step_avg:57.66ms
step:56/2330 train_time:3232ms step_avg:57.72ms
step:57/2330 train_time:3288ms step_avg:57.68ms
step:58/2330 train_time:3348ms step_avg:57.73ms
step:59/2330 train_time:3404ms step_avg:57.70ms
step:60/2330 train_time:3463ms step_avg:57.72ms
step:61/2330 train_time:3519ms step_avg:57.70ms
step:62/2330 train_time:3578ms step_avg:57.71ms
step:63/2330 train_time:3634ms step_avg:57.68ms
step:64/2330 train_time:3692ms step_avg:57.69ms
step:65/2330 train_time:3748ms step_avg:57.66ms
step:66/2330 train_time:3806ms step_avg:57.67ms
step:67/2330 train_time:3862ms step_avg:57.65ms
step:68/2330 train_time:3921ms step_avg:57.66ms
step:69/2330 train_time:3977ms step_avg:57.64ms
step:70/2330 train_time:4036ms step_avg:57.66ms
step:71/2330 train_time:4093ms step_avg:57.64ms
step:72/2330 train_time:4152ms step_avg:57.66ms
step:73/2330 train_time:4207ms step_avg:57.63ms
step:74/2330 train_time:4267ms step_avg:57.66ms
step:75/2330 train_time:4323ms step_avg:57.64ms
step:76/2330 train_time:4382ms step_avg:57.66ms
step:77/2330 train_time:4438ms step_avg:57.63ms
step:78/2330 train_time:4497ms step_avg:57.66ms
step:79/2330 train_time:4552ms step_avg:57.62ms
step:80/2330 train_time:4613ms step_avg:57.66ms
step:81/2330 train_time:4668ms step_avg:57.63ms
step:82/2330 train_time:4728ms step_avg:57.66ms
step:83/2330 train_time:4784ms step_avg:57.63ms
step:84/2330 train_time:4842ms step_avg:57.64ms
step:85/2330 train_time:4897ms step_avg:57.61ms
step:86/2330 train_time:4955ms step_avg:57.62ms
step:87/2330 train_time:5011ms step_avg:57.60ms
step:88/2330 train_time:5070ms step_avg:57.62ms
step:89/2330 train_time:5126ms step_avg:57.59ms
step:90/2330 train_time:5186ms step_avg:57.62ms
step:91/2330 train_time:5243ms step_avg:57.61ms
step:92/2330 train_time:5302ms step_avg:57.63ms
step:93/2330 train_time:5359ms step_avg:57.62ms
step:94/2330 train_time:5417ms step_avg:57.63ms
step:95/2330 train_time:5474ms step_avg:57.62ms
step:96/2330 train_time:5533ms step_avg:57.63ms
step:97/2330 train_time:5588ms step_avg:57.61ms
step:98/2330 train_time:5647ms step_avg:57.63ms
step:99/2330 train_time:5703ms step_avg:57.61ms
step:100/2330 train_time:5763ms step_avg:57.63ms
step:101/2330 train_time:5819ms step_avg:57.61ms
step:102/2330 train_time:5877ms step_avg:57.61ms
step:103/2330 train_time:5932ms step_avg:57.60ms
step:104/2330 train_time:5991ms step_avg:57.61ms
step:105/2330 train_time:6046ms step_avg:57.58ms
step:106/2330 train_time:6106ms step_avg:57.60ms
step:107/2330 train_time:6163ms step_avg:57.60ms
step:108/2330 train_time:6222ms step_avg:57.61ms
step:109/2330 train_time:6278ms step_avg:57.59ms
step:110/2330 train_time:6336ms step_avg:57.60ms
step:111/2330 train_time:6393ms step_avg:57.59ms
step:112/2330 train_time:6452ms step_avg:57.61ms
step:113/2330 train_time:6508ms step_avg:57.59ms
step:114/2330 train_time:6567ms step_avg:57.60ms
step:115/2330 train_time:6623ms step_avg:57.59ms
step:116/2330 train_time:6682ms step_avg:57.60ms
step:117/2330 train_time:6737ms step_avg:57.58ms
step:118/2330 train_time:6796ms step_avg:57.59ms
step:119/2330 train_time:6851ms step_avg:57.57ms
step:120/2330 train_time:6911ms step_avg:57.59ms
step:121/2330 train_time:6966ms step_avg:57.57ms
step:122/2330 train_time:7026ms step_avg:57.59ms
step:123/2330 train_time:7082ms step_avg:57.58ms
step:124/2330 train_time:7140ms step_avg:57.58ms
step:125/2330 train_time:7196ms step_avg:57.57ms
step:126/2330 train_time:7256ms step_avg:57.59ms
step:127/2330 train_time:7312ms step_avg:57.57ms
step:128/2330 train_time:7371ms step_avg:57.59ms
step:129/2330 train_time:7427ms step_avg:57.57ms
step:130/2330 train_time:7485ms step_avg:57.58ms
step:131/2330 train_time:7541ms step_avg:57.57ms
step:132/2330 train_time:7600ms step_avg:57.57ms
step:133/2330 train_time:7656ms step_avg:57.56ms
step:134/2330 train_time:7714ms step_avg:57.57ms
step:135/2330 train_time:7771ms step_avg:57.56ms
step:136/2330 train_time:7829ms step_avg:57.57ms
step:137/2330 train_time:7885ms step_avg:57.55ms
step:138/2330 train_time:7944ms step_avg:57.56ms
step:139/2330 train_time:8000ms step_avg:57.55ms
step:140/2330 train_time:8058ms step_avg:57.56ms
step:141/2330 train_time:8114ms step_avg:57.54ms
step:142/2330 train_time:8174ms step_avg:57.57ms
step:143/2330 train_time:8230ms step_avg:57.56ms
step:144/2330 train_time:8289ms step_avg:57.56ms
step:145/2330 train_time:8345ms step_avg:57.55ms
step:146/2330 train_time:8404ms step_avg:57.56ms
step:147/2330 train_time:8459ms step_avg:57.55ms
step:148/2330 train_time:8519ms step_avg:57.56ms
step:149/2330 train_time:8574ms step_avg:57.54ms
step:150/2330 train_time:8634ms step_avg:57.56ms
step:151/2330 train_time:8690ms step_avg:57.55ms
step:152/2330 train_time:8748ms step_avg:57.55ms
step:153/2330 train_time:8804ms step_avg:57.54ms
step:154/2330 train_time:8863ms step_avg:57.55ms
step:155/2330 train_time:8919ms step_avg:57.54ms
step:156/2330 train_time:8978ms step_avg:57.55ms
step:157/2330 train_time:9034ms step_avg:57.54ms
step:158/2330 train_time:9093ms step_avg:57.55ms
step:159/2330 train_time:9149ms step_avg:57.54ms
step:160/2330 train_time:9209ms step_avg:57.56ms
step:161/2330 train_time:9265ms step_avg:57.55ms
step:162/2330 train_time:9324ms step_avg:57.55ms
step:163/2330 train_time:9380ms step_avg:57.54ms
step:164/2330 train_time:9439ms step_avg:57.56ms
step:165/2330 train_time:9495ms step_avg:57.55ms
step:166/2330 train_time:9554ms step_avg:57.55ms
step:167/2330 train_time:9610ms step_avg:57.54ms
step:168/2330 train_time:9668ms step_avg:57.55ms
step:169/2330 train_time:9724ms step_avg:57.54ms
step:170/2330 train_time:9783ms step_avg:57.55ms
step:171/2330 train_time:9838ms step_avg:57.53ms
step:172/2330 train_time:9897ms step_avg:57.54ms
step:173/2330 train_time:9953ms step_avg:57.53ms
step:174/2330 train_time:10012ms step_avg:57.54ms
step:175/2330 train_time:10068ms step_avg:57.53ms
step:176/2330 train_time:10127ms step_avg:57.54ms
step:177/2330 train_time:10183ms step_avg:57.53ms
step:178/2330 train_time:10243ms step_avg:57.55ms
step:179/2330 train_time:10299ms step_avg:57.54ms
step:180/2330 train_time:10358ms step_avg:57.54ms
step:181/2330 train_time:10414ms step_avg:57.54ms
step:182/2330 train_time:10473ms step_avg:57.54ms
step:183/2330 train_time:10529ms step_avg:57.53ms
step:184/2330 train_time:10588ms step_avg:57.54ms
step:185/2330 train_time:10644ms step_avg:57.54ms
step:186/2330 train_time:10702ms step_avg:57.54ms
step:187/2330 train_time:10758ms step_avg:57.53ms
step:188/2330 train_time:10817ms step_avg:57.54ms
step:189/2330 train_time:10873ms step_avg:57.53ms
step:190/2330 train_time:10934ms step_avg:57.55ms
step:191/2330 train_time:10989ms step_avg:57.54ms
step:192/2330 train_time:11049ms step_avg:57.55ms
step:193/2330 train_time:11105ms step_avg:57.54ms
step:194/2330 train_time:11165ms step_avg:57.55ms
step:195/2330 train_time:11221ms step_avg:57.54ms
step:196/2330 train_time:11279ms step_avg:57.55ms
step:197/2330 train_time:11335ms step_avg:57.54ms
step:198/2330 train_time:11394ms step_avg:57.55ms
step:199/2330 train_time:11450ms step_avg:57.54ms
step:200/2330 train_time:11509ms step_avg:57.55ms
step:201/2330 train_time:11565ms step_avg:57.54ms
step:202/2330 train_time:11624ms step_avg:57.54ms
step:203/2330 train_time:11680ms step_avg:57.54ms
step:204/2330 train_time:11739ms step_avg:57.54ms
step:205/2330 train_time:11795ms step_avg:57.54ms
step:206/2330 train_time:11854ms step_avg:57.54ms
step:207/2330 train_time:11910ms step_avg:57.54ms
step:208/2330 train_time:11969ms step_avg:57.54ms
step:209/2330 train_time:12025ms step_avg:57.54ms
step:210/2330 train_time:12083ms step_avg:57.54ms
step:211/2330 train_time:12139ms step_avg:57.53ms
step:212/2330 train_time:12199ms step_avg:57.54ms
step:213/2330 train_time:12255ms step_avg:57.53ms
step:214/2330 train_time:12315ms step_avg:57.55ms
step:215/2330 train_time:12371ms step_avg:57.54ms
step:216/2330 train_time:12429ms step_avg:57.54ms
step:217/2330 train_time:12485ms step_avg:57.53ms
step:218/2330 train_time:12544ms step_avg:57.54ms
step:219/2330 train_time:12600ms step_avg:57.54ms
step:220/2330 train_time:12659ms step_avg:57.54ms
step:221/2330 train_time:12715ms step_avg:57.54ms
step:222/2330 train_time:12775ms step_avg:57.54ms
step:223/2330 train_time:12831ms step_avg:57.54ms
step:224/2330 train_time:12890ms step_avg:57.54ms
step:225/2330 train_time:12946ms step_avg:57.54ms
step:226/2330 train_time:13005ms step_avg:57.54ms
step:227/2330 train_time:13061ms step_avg:57.54ms
step:228/2330 train_time:13120ms step_avg:57.54ms
step:229/2330 train_time:13176ms step_avg:57.54ms
step:230/2330 train_time:13235ms step_avg:57.54ms
step:231/2330 train_time:13291ms step_avg:57.53ms
step:232/2330 train_time:13349ms step_avg:57.54ms
step:233/2330 train_time:13405ms step_avg:57.53ms
step:234/2330 train_time:13465ms step_avg:57.54ms
step:235/2330 train_time:13520ms step_avg:57.53ms
step:236/2330 train_time:13579ms step_avg:57.54ms
step:237/2330 train_time:13635ms step_avg:57.53ms
step:238/2330 train_time:13695ms step_avg:57.54ms
step:239/2330 train_time:13751ms step_avg:57.54ms
step:240/2330 train_time:13810ms step_avg:57.54ms
step:241/2330 train_time:13866ms step_avg:57.54ms
step:242/2330 train_time:13925ms step_avg:57.54ms
step:243/2330 train_time:13981ms step_avg:57.53ms
step:244/2330 train_time:14040ms step_avg:57.54ms
step:245/2330 train_time:14096ms step_avg:57.53ms
step:246/2330 train_time:14155ms step_avg:57.54ms
step:247/2330 train_time:14211ms step_avg:57.53ms
step:248/2330 train_time:14270ms step_avg:57.54ms
step:249/2330 train_time:14325ms step_avg:57.53ms
step:250/2330 train_time:14386ms step_avg:57.55ms
step:250/2330 val_loss:4.8979 train_time:14465ms step_avg:57.86ms
step:251/2330 train_time:14484ms step_avg:57.70ms
step:252/2330 train_time:14503ms step_avg:57.55ms
step:253/2330 train_time:14558ms step_avg:57.54ms
step:254/2330 train_time:14625ms step_avg:57.58ms
step:255/2330 train_time:14681ms step_avg:57.57ms
step:256/2330 train_time:14745ms step_avg:57.60ms
step:257/2330 train_time:14800ms step_avg:57.59ms
step:258/2330 train_time:14861ms step_avg:57.60ms
step:259/2330 train_time:14916ms step_avg:57.59ms
step:260/2330 train_time:14975ms step_avg:57.60ms
step:261/2330 train_time:15032ms step_avg:57.59ms
step:262/2330 train_time:15090ms step_avg:57.59ms
step:263/2330 train_time:15145ms step_avg:57.59ms
step:264/2330 train_time:15204ms step_avg:57.59ms
step:265/2330 train_time:15259ms step_avg:57.58ms
step:266/2330 train_time:15317ms step_avg:57.58ms
step:267/2330 train_time:15375ms step_avg:57.58ms
step:268/2330 train_time:15435ms step_avg:57.59ms
step:269/2330 train_time:15493ms step_avg:57.59ms
step:270/2330 train_time:15553ms step_avg:57.60ms
step:271/2330 train_time:15609ms step_avg:57.60ms
step:272/2330 train_time:15669ms step_avg:57.61ms
step:273/2330 train_time:15725ms step_avg:57.60ms
step:274/2330 train_time:15786ms step_avg:57.61ms
step:275/2330 train_time:15842ms step_avg:57.61ms
step:276/2330 train_time:15901ms step_avg:57.61ms
step:277/2330 train_time:15956ms step_avg:57.60ms
step:278/2330 train_time:16017ms step_avg:57.61ms
step:279/2330 train_time:16072ms step_avg:57.61ms
step:280/2330 train_time:16133ms step_avg:57.62ms
step:281/2330 train_time:16188ms step_avg:57.61ms
step:282/2330 train_time:16246ms step_avg:57.61ms
step:283/2330 train_time:16302ms step_avg:57.60ms
step:284/2330 train_time:16361ms step_avg:57.61ms
step:285/2330 train_time:16417ms step_avg:57.60ms
step:286/2330 train_time:16476ms step_avg:57.61ms
step:287/2330 train_time:16533ms step_avg:57.61ms
step:288/2330 train_time:16592ms step_avg:57.61ms
step:289/2330 train_time:16649ms step_avg:57.61ms
step:290/2330 train_time:16708ms step_avg:57.61ms
step:291/2330 train_time:16764ms step_avg:57.61ms
step:292/2330 train_time:16824ms step_avg:57.62ms
step:293/2330 train_time:16880ms step_avg:57.61ms
step:294/2330 train_time:16940ms step_avg:57.62ms
step:295/2330 train_time:16995ms step_avg:57.61ms
step:296/2330 train_time:17054ms step_avg:57.61ms
step:297/2330 train_time:17110ms step_avg:57.61ms
step:298/2330 train_time:17169ms step_avg:57.61ms
step:299/2330 train_time:17224ms step_avg:57.60ms
step:300/2330 train_time:17283ms step_avg:57.61ms
step:301/2330 train_time:17338ms step_avg:57.60ms
step:302/2330 train_time:17397ms step_avg:57.61ms
step:303/2330 train_time:17453ms step_avg:57.60ms
step:304/2330 train_time:17512ms step_avg:57.61ms
step:305/2330 train_time:17568ms step_avg:57.60ms
step:306/2330 train_time:17628ms step_avg:57.61ms
step:307/2330 train_time:17685ms step_avg:57.60ms
step:308/2330 train_time:17745ms step_avg:57.61ms
step:309/2330 train_time:17801ms step_avg:57.61ms
step:310/2330 train_time:17860ms step_avg:57.61ms
step:311/2330 train_time:17917ms step_avg:57.61ms
step:312/2330 train_time:17976ms step_avg:57.62ms
step:313/2330 train_time:18032ms step_avg:57.61ms
step:314/2330 train_time:18090ms step_avg:57.61ms
step:315/2330 train_time:18146ms step_avg:57.61ms
step:316/2330 train_time:18205ms step_avg:57.61ms
step:317/2330 train_time:18260ms step_avg:57.60ms
step:318/2330 train_time:18320ms step_avg:57.61ms
step:319/2330 train_time:18375ms step_avg:57.60ms
step:320/2330 train_time:18435ms step_avg:57.61ms
step:321/2330 train_time:18491ms step_avg:57.60ms
step:322/2330 train_time:18550ms step_avg:57.61ms
step:323/2330 train_time:18606ms step_avg:57.61ms
step:324/2330 train_time:18666ms step_avg:57.61ms
step:325/2330 train_time:18722ms step_avg:57.61ms
step:326/2330 train_time:18781ms step_avg:57.61ms
step:327/2330 train_time:18837ms step_avg:57.61ms
step:328/2330 train_time:18897ms step_avg:57.61ms
step:329/2330 train_time:18953ms step_avg:57.61ms
step:330/2330 train_time:19013ms step_avg:57.61ms
step:331/2330 train_time:19068ms step_avg:57.61ms
step:332/2330 train_time:19128ms step_avg:57.61ms
step:333/2330 train_time:19184ms step_avg:57.61ms
step:334/2330 train_time:19242ms step_avg:57.61ms
step:335/2330 train_time:19298ms step_avg:57.61ms
step:336/2330 train_time:19356ms step_avg:57.61ms
step:337/2330 train_time:19412ms step_avg:57.60ms
step:338/2330 train_time:19471ms step_avg:57.61ms
step:339/2330 train_time:19527ms step_avg:57.60ms
step:340/2330 train_time:19586ms step_avg:57.61ms
step:341/2330 train_time:19643ms step_avg:57.60ms
step:342/2330 train_time:19701ms step_avg:57.61ms
step:343/2330 train_time:19757ms step_avg:57.60ms
step:344/2330 train_time:19816ms step_avg:57.61ms
step:345/2330 train_time:19873ms step_avg:57.60ms
step:346/2330 train_time:19932ms step_avg:57.61ms
step:347/2330 train_time:19988ms step_avg:57.60ms
step:348/2330 train_time:20048ms step_avg:57.61ms
step:349/2330 train_time:20103ms step_avg:57.60ms
step:350/2330 train_time:20163ms step_avg:57.61ms
step:351/2330 train_time:20219ms step_avg:57.60ms
step:352/2330 train_time:20279ms step_avg:57.61ms
step:353/2330 train_time:20335ms step_avg:57.61ms
step:354/2330 train_time:20394ms step_avg:57.61ms
step:355/2330 train_time:20450ms step_avg:57.61ms
step:356/2330 train_time:20508ms step_avg:57.61ms
step:357/2330 train_time:20564ms step_avg:57.60ms
step:358/2330 train_time:20624ms step_avg:57.61ms
step:359/2330 train_time:20680ms step_avg:57.60ms
step:360/2330 train_time:20740ms step_avg:57.61ms
step:361/2330 train_time:20795ms step_avg:57.60ms
step:362/2330 train_time:20855ms step_avg:57.61ms
step:363/2330 train_time:20911ms step_avg:57.61ms
step:364/2330 train_time:20970ms step_avg:57.61ms
step:365/2330 train_time:21026ms step_avg:57.60ms
step:366/2330 train_time:21086ms step_avg:57.61ms
step:367/2330 train_time:21142ms step_avg:57.61ms
step:368/2330 train_time:21200ms step_avg:57.61ms
step:369/2330 train_time:21256ms step_avg:57.60ms
step:370/2330 train_time:21315ms step_avg:57.61ms
step:371/2330 train_time:21371ms step_avg:57.60ms
step:372/2330 train_time:21431ms step_avg:57.61ms
step:373/2330 train_time:21487ms step_avg:57.61ms
step:374/2330 train_time:21546ms step_avg:57.61ms
step:375/2330 train_time:21602ms step_avg:57.61ms
step:376/2330 train_time:21661ms step_avg:57.61ms
step:377/2330 train_time:21717ms step_avg:57.61ms
step:378/2330 train_time:21777ms step_avg:57.61ms
step:379/2330 train_time:21833ms step_avg:57.61ms
step:380/2330 train_time:21892ms step_avg:57.61ms
step:381/2330 train_time:21949ms step_avg:57.61ms
step:382/2330 train_time:22008ms step_avg:57.61ms
step:383/2330 train_time:22064ms step_avg:57.61ms
step:384/2330 train_time:22123ms step_avg:57.61ms
step:385/2330 train_time:22179ms step_avg:57.61ms
step:386/2330 train_time:22238ms step_avg:57.61ms
step:387/2330 train_time:22295ms step_avg:57.61ms
step:388/2330 train_time:22353ms step_avg:57.61ms
step:389/2330 train_time:22409ms step_avg:57.61ms
step:390/2330 train_time:22468ms step_avg:57.61ms
step:391/2330 train_time:22524ms step_avg:57.61ms
step:392/2330 train_time:22583ms step_avg:57.61ms
step:393/2330 train_time:22639ms step_avg:57.61ms
step:394/2330 train_time:22699ms step_avg:57.61ms
step:395/2330 train_time:22755ms step_avg:57.61ms
step:396/2330 train_time:22814ms step_avg:57.61ms
step:397/2330 train_time:22870ms step_avg:57.61ms
step:398/2330 train_time:22929ms step_avg:57.61ms
step:399/2330 train_time:22986ms step_avg:57.61ms
step:400/2330 train_time:23044ms step_avg:57.61ms
step:401/2330 train_time:23100ms step_avg:57.61ms
step:402/2330 train_time:23159ms step_avg:57.61ms
step:403/2330 train_time:23215ms step_avg:57.61ms
step:404/2330 train_time:23274ms step_avg:57.61ms
step:405/2330 train_time:23330ms step_avg:57.60ms
step:406/2330 train_time:23389ms step_avg:57.61ms
step:407/2330 train_time:23445ms step_avg:57.60ms
step:408/2330 train_time:23505ms step_avg:57.61ms
step:409/2330 train_time:23561ms step_avg:57.61ms
step:410/2330 train_time:23621ms step_avg:57.61ms
step:411/2330 train_time:23676ms step_avg:57.61ms
step:412/2330 train_time:23736ms step_avg:57.61ms
step:413/2330 train_time:23792ms step_avg:57.61ms
step:414/2330 train_time:23851ms step_avg:57.61ms
step:415/2330 train_time:23908ms step_avg:57.61ms
step:416/2330 train_time:23967ms step_avg:57.61ms
step:417/2330 train_time:24023ms step_avg:57.61ms
step:418/2330 train_time:24082ms step_avg:57.61ms
step:419/2330 train_time:24138ms step_avg:57.61ms
step:420/2330 train_time:24197ms step_avg:57.61ms
step:421/2330 train_time:24253ms step_avg:57.61ms
step:422/2330 train_time:24312ms step_avg:57.61ms
step:423/2330 train_time:24368ms step_avg:57.61ms
step:424/2330 train_time:24426ms step_avg:57.61ms
step:425/2330 train_time:24482ms step_avg:57.61ms
step:426/2330 train_time:24542ms step_avg:57.61ms
step:427/2330 train_time:24597ms step_avg:57.60ms
step:428/2330 train_time:24657ms step_avg:57.61ms
step:429/2330 train_time:24713ms step_avg:57.61ms
step:430/2330 train_time:24772ms step_avg:57.61ms
step:431/2330 train_time:24829ms step_avg:57.61ms
step:432/2330 train_time:24888ms step_avg:57.61ms
step:433/2330 train_time:24944ms step_avg:57.61ms
step:434/2330 train_time:25002ms step_avg:57.61ms
step:435/2330 train_time:25058ms step_avg:57.60ms
step:436/2330 train_time:25118ms step_avg:57.61ms
step:437/2330 train_time:25174ms step_avg:57.61ms
step:438/2330 train_time:25233ms step_avg:57.61ms
step:439/2330 train_time:25290ms step_avg:57.61ms
step:440/2330 train_time:25349ms step_avg:57.61ms
step:441/2330 train_time:25405ms step_avg:57.61ms
step:442/2330 train_time:25464ms step_avg:57.61ms
step:443/2330 train_time:25520ms step_avg:57.61ms
step:444/2330 train_time:25579ms step_avg:57.61ms
step:445/2330 train_time:25635ms step_avg:57.61ms
step:446/2330 train_time:25694ms step_avg:57.61ms
step:447/2330 train_time:25750ms step_avg:57.61ms
step:448/2330 train_time:25810ms step_avg:57.61ms
step:449/2330 train_time:25866ms step_avg:57.61ms
step:450/2330 train_time:25925ms step_avg:57.61ms
step:451/2330 train_time:25981ms step_avg:57.61ms
step:452/2330 train_time:26041ms step_avg:57.61ms
step:453/2330 train_time:26096ms step_avg:57.61ms
step:454/2330 train_time:26155ms step_avg:57.61ms
step:455/2330 train_time:26211ms step_avg:57.61ms
step:456/2330 train_time:26271ms step_avg:57.61ms
step:457/2330 train_time:26327ms step_avg:57.61ms
step:458/2330 train_time:26386ms step_avg:57.61ms
step:459/2330 train_time:26442ms step_avg:57.61ms
step:460/2330 train_time:26501ms step_avg:57.61ms
step:461/2330 train_time:26556ms step_avg:57.61ms
step:462/2330 train_time:26616ms step_avg:57.61ms
step:463/2330 train_time:26672ms step_avg:57.61ms
step:464/2330 train_time:26732ms step_avg:57.61ms
step:465/2330 train_time:26788ms step_avg:57.61ms
step:466/2330 train_time:26847ms step_avg:57.61ms
step:467/2330 train_time:26903ms step_avg:57.61ms
step:468/2330 train_time:26961ms step_avg:57.61ms
step:469/2330 train_time:27017ms step_avg:57.61ms
step:470/2330 train_time:27077ms step_avg:57.61ms
step:471/2330 train_time:27134ms step_avg:57.61ms
step:472/2330 train_time:27193ms step_avg:57.61ms
step:473/2330 train_time:27249ms step_avg:57.61ms
step:474/2330 train_time:27308ms step_avg:57.61ms
step:475/2330 train_time:27364ms step_avg:57.61ms
step:476/2330 train_time:27423ms step_avg:57.61ms
step:477/2330 train_time:27479ms step_avg:57.61ms
step:478/2330 train_time:27539ms step_avg:57.61ms
step:479/2330 train_time:27595ms step_avg:57.61ms
step:480/2330 train_time:27654ms step_avg:57.61ms
step:481/2330 train_time:27711ms step_avg:57.61ms
step:482/2330 train_time:27770ms step_avg:57.61ms
step:483/2330 train_time:27826ms step_avg:57.61ms
step:484/2330 train_time:27885ms step_avg:57.61ms
step:485/2330 train_time:27941ms step_avg:57.61ms
step:486/2330 train_time:28000ms step_avg:57.61ms
step:487/2330 train_time:28056ms step_avg:57.61ms
step:488/2330 train_time:28116ms step_avg:57.61ms
step:489/2330 train_time:28172ms step_avg:57.61ms
step:490/2330 train_time:28231ms step_avg:57.61ms
step:491/2330 train_time:28287ms step_avg:57.61ms
step:492/2330 train_time:28346ms step_avg:57.61ms
step:493/2330 train_time:28402ms step_avg:57.61ms
step:494/2330 train_time:28461ms step_avg:57.61ms
step:495/2330 train_time:28517ms step_avg:57.61ms
step:496/2330 train_time:28577ms step_avg:57.61ms
step:497/2330 train_time:28633ms step_avg:57.61ms
step:498/2330 train_time:28692ms step_avg:57.61ms
step:499/2330 train_time:28748ms step_avg:57.61ms
step:500/2330 train_time:28807ms step_avg:57.61ms
step:500/2330 val_loss:4.4161 train_time:28887ms step_avg:57.77ms
step:501/2330 train_time:28905ms step_avg:57.69ms
step:502/2330 train_time:28925ms step_avg:57.62ms
step:503/2330 train_time:28984ms step_avg:57.62ms
step:504/2330 train_time:29047ms step_avg:57.63ms
step:505/2330 train_time:29105ms step_avg:57.63ms
step:506/2330 train_time:29165ms step_avg:57.64ms
step:507/2330 train_time:29221ms step_avg:57.64ms
step:508/2330 train_time:29281ms step_avg:57.64ms
step:509/2330 train_time:29336ms step_avg:57.64ms
step:510/2330 train_time:29395ms step_avg:57.64ms
step:511/2330 train_time:29451ms step_avg:57.63ms
step:512/2330 train_time:29509ms step_avg:57.64ms
step:513/2330 train_time:29565ms step_avg:57.63ms
step:514/2330 train_time:29623ms step_avg:57.63ms
step:515/2330 train_time:29678ms step_avg:57.63ms
step:516/2330 train_time:29737ms step_avg:57.63ms
step:517/2330 train_time:29793ms step_avg:57.63ms
step:518/2330 train_time:29851ms step_avg:57.63ms
step:519/2330 train_time:29908ms step_avg:57.63ms
step:520/2330 train_time:29968ms step_avg:57.63ms
step:521/2330 train_time:30025ms step_avg:57.63ms
step:522/2330 train_time:30086ms step_avg:57.64ms
step:523/2330 train_time:30143ms step_avg:57.63ms
step:524/2330 train_time:30203ms step_avg:57.64ms
step:525/2330 train_time:30260ms step_avg:57.64ms
step:526/2330 train_time:30321ms step_avg:57.64ms
step:527/2330 train_time:30376ms step_avg:57.64ms
step:528/2330 train_time:30435ms step_avg:57.64ms
step:529/2330 train_time:30491ms step_avg:57.64ms
step:530/2330 train_time:30550ms step_avg:57.64ms
step:531/2330 train_time:30605ms step_avg:57.64ms
step:532/2330 train_time:30664ms step_avg:57.64ms
step:533/2330 train_time:30719ms step_avg:57.63ms
step:534/2330 train_time:30778ms step_avg:57.64ms
step:535/2330 train_time:30833ms step_avg:57.63ms
step:536/2330 train_time:30894ms step_avg:57.64ms
step:537/2330 train_time:30950ms step_avg:57.63ms
step:538/2330 train_time:31010ms step_avg:57.64ms
step:539/2330 train_time:31066ms step_avg:57.64ms
step:540/2330 train_time:31127ms step_avg:57.64ms
step:541/2330 train_time:31183ms step_avg:57.64ms
step:542/2330 train_time:31243ms step_avg:57.64ms
step:543/2330 train_time:31300ms step_avg:57.64ms
step:544/2330 train_time:31359ms step_avg:57.65ms
step:545/2330 train_time:31415ms step_avg:57.64ms
step:546/2330 train_time:31474ms step_avg:57.64ms
step:547/2330 train_time:31530ms step_avg:57.64ms
step:548/2330 train_time:31588ms step_avg:57.64ms
step:549/2330 train_time:31643ms step_avg:57.64ms
step:550/2330 train_time:31702ms step_avg:57.64ms
step:551/2330 train_time:31758ms step_avg:57.64ms
step:552/2330 train_time:31818ms step_avg:57.64ms
step:553/2330 train_time:31874ms step_avg:57.64ms
step:554/2330 train_time:31933ms step_avg:57.64ms
step:555/2330 train_time:31989ms step_avg:57.64ms
step:556/2330 train_time:32048ms step_avg:57.64ms
step:557/2330 train_time:32104ms step_avg:57.64ms
step:558/2330 train_time:32165ms step_avg:57.64ms
step:559/2330 train_time:32221ms step_avg:57.64ms
step:560/2330 train_time:32281ms step_avg:57.64ms
step:561/2330 train_time:32337ms step_avg:57.64ms
step:562/2330 train_time:32397ms step_avg:57.65ms
step:563/2330 train_time:32453ms step_avg:57.64ms
step:564/2330 train_time:32511ms step_avg:57.64ms
step:565/2330 train_time:32567ms step_avg:57.64ms
step:566/2330 train_time:32626ms step_avg:57.64ms
step:567/2330 train_time:32682ms step_avg:57.64ms
step:568/2330 train_time:32742ms step_avg:57.64ms
step:569/2330 train_time:32798ms step_avg:57.64ms
step:570/2330 train_time:32857ms step_avg:57.64ms
step:571/2330 train_time:32913ms step_avg:57.64ms
step:572/2330 train_time:32973ms step_avg:57.64ms
step:573/2330 train_time:33029ms step_avg:57.64ms
step:574/2330 train_time:33088ms step_avg:57.65ms
step:575/2330 train_time:33144ms step_avg:57.64ms
step:576/2330 train_time:33204ms step_avg:57.65ms
step:577/2330 train_time:33261ms step_avg:57.64ms
step:578/2330 train_time:33321ms step_avg:57.65ms
step:579/2330 train_time:33377ms step_avg:57.65ms
step:580/2330 train_time:33436ms step_avg:57.65ms
step:581/2330 train_time:33492ms step_avg:57.65ms
step:582/2330 train_time:33551ms step_avg:57.65ms
step:583/2330 train_time:33607ms step_avg:57.64ms
step:584/2330 train_time:33667ms step_avg:57.65ms
step:585/2330 train_time:33722ms step_avg:57.64ms
step:586/2330 train_time:33783ms step_avg:57.65ms
step:587/2330 train_time:33840ms step_avg:57.65ms
step:588/2330 train_time:33900ms step_avg:57.65ms
step:589/2330 train_time:33956ms step_avg:57.65ms
step:590/2330 train_time:34015ms step_avg:57.65ms
step:591/2330 train_time:34070ms step_avg:57.65ms
step:592/2330 train_time:34132ms step_avg:57.66ms
step:593/2330 train_time:34188ms step_avg:57.65ms
step:594/2330 train_time:34247ms step_avg:57.66ms
step:595/2330 train_time:34303ms step_avg:57.65ms
step:596/2330 train_time:34362ms step_avg:57.65ms
step:597/2330 train_time:34418ms step_avg:57.65ms
step:598/2330 train_time:34478ms step_avg:57.66ms
step:599/2330 train_time:34534ms step_avg:57.65ms
step:600/2330 train_time:34594ms step_avg:57.66ms
step:601/2330 train_time:34650ms step_avg:57.65ms
step:602/2330 train_time:34709ms step_avg:57.66ms
step:603/2330 train_time:34765ms step_avg:57.65ms
step:604/2330 train_time:34825ms step_avg:57.66ms
step:605/2330 train_time:34882ms step_avg:57.66ms
step:606/2330 train_time:34941ms step_avg:57.66ms
step:607/2330 train_time:34998ms step_avg:57.66ms
step:608/2330 train_time:35057ms step_avg:57.66ms
step:609/2330 train_time:35114ms step_avg:57.66ms
step:610/2330 train_time:35173ms step_avg:57.66ms
step:611/2330 train_time:35229ms step_avg:57.66ms
step:612/2330 train_time:35288ms step_avg:57.66ms
step:613/2330 train_time:35344ms step_avg:57.66ms
step:614/2330 train_time:35405ms step_avg:57.66ms
step:615/2330 train_time:35461ms step_avg:57.66ms
step:616/2330 train_time:35520ms step_avg:57.66ms
step:617/2330 train_time:35576ms step_avg:57.66ms
step:618/2330 train_time:35635ms step_avg:57.66ms
step:619/2330 train_time:35691ms step_avg:57.66ms
step:620/2330 train_time:35751ms step_avg:57.66ms
step:621/2330 train_time:35806ms step_avg:57.66ms
step:622/2330 train_time:35866ms step_avg:57.66ms
step:623/2330 train_time:35921ms step_avg:57.66ms
step:624/2330 train_time:35984ms step_avg:57.67ms
step:625/2330 train_time:36040ms step_avg:57.66ms
step:626/2330 train_time:36100ms step_avg:57.67ms
step:627/2330 train_time:36156ms step_avg:57.67ms
step:628/2330 train_time:36215ms step_avg:57.67ms
step:629/2330 train_time:36271ms step_avg:57.66ms
step:630/2330 train_time:36330ms step_avg:57.67ms
step:631/2330 train_time:36386ms step_avg:57.66ms
step:632/2330 train_time:36446ms step_avg:57.67ms
step:633/2330 train_time:36502ms step_avg:57.67ms
step:634/2330 train_time:36561ms step_avg:57.67ms
step:635/2330 train_time:36618ms step_avg:57.67ms
step:636/2330 train_time:36678ms step_avg:57.67ms
step:637/2330 train_time:36733ms step_avg:57.67ms
step:638/2330 train_time:36793ms step_avg:57.67ms
step:639/2330 train_time:36848ms step_avg:57.67ms
step:640/2330 train_time:36909ms step_avg:57.67ms
step:641/2330 train_time:36964ms step_avg:57.67ms
step:642/2330 train_time:37024ms step_avg:57.67ms
step:643/2330 train_time:37080ms step_avg:57.67ms
step:644/2330 train_time:37141ms step_avg:57.67ms
step:645/2330 train_time:37197ms step_avg:57.67ms
step:646/2330 train_time:37256ms step_avg:57.67ms
step:647/2330 train_time:37312ms step_avg:57.67ms
step:648/2330 train_time:37372ms step_avg:57.67ms
step:649/2330 train_time:37427ms step_avg:57.67ms
step:650/2330 train_time:37488ms step_avg:57.67ms
step:651/2330 train_time:37543ms step_avg:57.67ms
step:652/2330 train_time:37602ms step_avg:57.67ms
step:653/2330 train_time:37659ms step_avg:57.67ms
step:654/2330 train_time:37718ms step_avg:57.67ms
step:655/2330 train_time:37774ms step_avg:57.67ms
step:656/2330 train_time:37834ms step_avg:57.67ms
step:657/2330 train_time:37889ms step_avg:57.67ms
step:658/2330 train_time:37949ms step_avg:57.67ms
step:659/2330 train_time:38005ms step_avg:57.67ms
step:660/2330 train_time:38064ms step_avg:57.67ms
step:661/2330 train_time:38120ms step_avg:57.67ms
step:662/2330 train_time:38180ms step_avg:57.67ms
step:663/2330 train_time:38237ms step_avg:57.67ms
step:664/2330 train_time:38296ms step_avg:57.67ms
step:665/2330 train_time:38352ms step_avg:57.67ms
step:666/2330 train_time:38411ms step_avg:57.67ms
step:667/2330 train_time:38467ms step_avg:57.67ms
step:668/2330 train_time:38527ms step_avg:57.68ms
step:669/2330 train_time:38582ms step_avg:57.67ms
step:670/2330 train_time:38642ms step_avg:57.67ms
step:671/2330 train_time:38698ms step_avg:57.67ms
step:672/2330 train_time:38758ms step_avg:57.68ms
step:673/2330 train_time:38814ms step_avg:57.67ms
step:674/2330 train_time:38873ms step_avg:57.67ms
step:675/2330 train_time:38929ms step_avg:57.67ms
step:676/2330 train_time:38989ms step_avg:57.68ms
step:677/2330 train_time:39044ms step_avg:57.67ms
step:678/2330 train_time:39104ms step_avg:57.68ms
step:679/2330 train_time:39160ms step_avg:57.67ms
step:680/2330 train_time:39220ms step_avg:57.68ms
step:681/2330 train_time:39276ms step_avg:57.67ms
step:682/2330 train_time:39335ms step_avg:57.68ms
step:683/2330 train_time:39392ms step_avg:57.68ms
step:684/2330 train_time:39452ms step_avg:57.68ms
step:685/2330 train_time:39508ms step_avg:57.68ms
step:686/2330 train_time:39568ms step_avg:57.68ms
step:687/2330 train_time:39624ms step_avg:57.68ms
step:688/2330 train_time:39684ms step_avg:57.68ms
step:689/2330 train_time:39740ms step_avg:57.68ms
step:690/2330 train_time:39798ms step_avg:57.68ms
step:691/2330 train_time:39854ms step_avg:57.68ms
step:692/2330 train_time:39914ms step_avg:57.68ms
step:693/2330 train_time:39969ms step_avg:57.68ms
step:694/2330 train_time:40030ms step_avg:57.68ms
step:695/2330 train_time:40085ms step_avg:57.68ms
step:696/2330 train_time:40145ms step_avg:57.68ms
step:697/2330 train_time:40201ms step_avg:57.68ms
step:698/2330 train_time:40261ms step_avg:57.68ms
step:699/2330 train_time:40317ms step_avg:57.68ms
step:700/2330 train_time:40376ms step_avg:57.68ms
step:701/2330 train_time:40433ms step_avg:57.68ms
step:702/2330 train_time:40493ms step_avg:57.68ms
step:703/2330 train_time:40548ms step_avg:57.68ms
step:704/2330 train_time:40608ms step_avg:57.68ms
step:705/2330 train_time:40664ms step_avg:57.68ms
step:706/2330 train_time:40723ms step_avg:57.68ms
step:707/2330 train_time:40779ms step_avg:57.68ms
step:708/2330 train_time:40839ms step_avg:57.68ms
step:709/2330 train_time:40895ms step_avg:57.68ms
step:710/2330 train_time:40954ms step_avg:57.68ms
step:711/2330 train_time:41010ms step_avg:57.68ms
step:712/2330 train_time:41071ms step_avg:57.68ms
step:713/2330 train_time:41127ms step_avg:57.68ms
step:714/2330 train_time:41186ms step_avg:57.68ms
step:715/2330 train_time:41242ms step_avg:57.68ms
step:716/2330 train_time:41302ms step_avg:57.68ms
step:717/2330 train_time:41358ms step_avg:57.68ms
step:718/2330 train_time:41418ms step_avg:57.69ms
step:719/2330 train_time:41474ms step_avg:57.68ms
step:720/2330 train_time:41534ms step_avg:57.69ms
step:721/2330 train_time:41590ms step_avg:57.68ms
step:722/2330 train_time:41651ms step_avg:57.69ms
step:723/2330 train_time:41707ms step_avg:57.69ms
step:724/2330 train_time:41766ms step_avg:57.69ms
step:725/2330 train_time:41822ms step_avg:57.68ms
step:726/2330 train_time:41883ms step_avg:57.69ms
step:727/2330 train_time:41940ms step_avg:57.69ms
step:728/2330 train_time:41999ms step_avg:57.69ms
step:729/2330 train_time:42055ms step_avg:57.69ms
step:730/2330 train_time:42113ms step_avg:57.69ms
step:731/2330 train_time:42170ms step_avg:57.69ms
step:732/2330 train_time:42228ms step_avg:57.69ms
step:733/2330 train_time:42284ms step_avg:57.69ms
step:734/2330 train_time:42344ms step_avg:57.69ms
step:735/2330 train_time:42400ms step_avg:57.69ms
step:736/2330 train_time:42460ms step_avg:57.69ms
step:737/2330 train_time:42516ms step_avg:57.69ms
step:738/2330 train_time:42577ms step_avg:57.69ms
step:739/2330 train_time:42633ms step_avg:57.69ms
step:740/2330 train_time:42693ms step_avg:57.69ms
step:741/2330 train_time:42748ms step_avg:57.69ms
step:742/2330 train_time:42809ms step_avg:57.69ms
step:743/2330 train_time:42864ms step_avg:57.69ms
step:744/2330 train_time:42924ms step_avg:57.69ms
step:745/2330 train_time:42980ms step_avg:57.69ms
step:746/2330 train_time:43040ms step_avg:57.69ms
step:747/2330 train_time:43097ms step_avg:57.69ms
step:748/2330 train_time:43156ms step_avg:57.69ms
step:749/2330 train_time:43212ms step_avg:57.69ms
step:750/2330 train_time:43272ms step_avg:57.70ms
step:750/2330 val_loss:4.2150 train_time:43351ms step_avg:57.80ms
step:751/2330 train_time:43369ms step_avg:57.75ms
step:752/2330 train_time:43390ms step_avg:57.70ms
step:753/2330 train_time:43447ms step_avg:57.70ms
step:754/2330 train_time:43510ms step_avg:57.71ms
step:755/2330 train_time:43567ms step_avg:57.71ms
step:756/2330 train_time:43628ms step_avg:57.71ms
step:757/2330 train_time:43685ms step_avg:57.71ms
step:758/2330 train_time:43744ms step_avg:57.71ms
step:759/2330 train_time:43800ms step_avg:57.71ms
step:760/2330 train_time:43859ms step_avg:57.71ms
step:761/2330 train_time:43914ms step_avg:57.71ms
step:762/2330 train_time:43973ms step_avg:57.71ms
step:763/2330 train_time:44029ms step_avg:57.70ms
step:764/2330 train_time:44087ms step_avg:57.71ms
step:765/2330 train_time:44144ms step_avg:57.70ms
step:766/2330 train_time:44201ms step_avg:57.70ms
step:767/2330 train_time:44257ms step_avg:57.70ms
step:768/2330 train_time:44319ms step_avg:57.71ms
step:769/2330 train_time:44376ms step_avg:57.71ms
step:770/2330 train_time:44439ms step_avg:57.71ms
step:771/2330 train_time:44498ms step_avg:57.71ms
step:772/2330 train_time:44559ms step_avg:57.72ms
step:773/2330 train_time:44617ms step_avg:57.72ms
step:774/2330 train_time:44678ms step_avg:57.72ms
step:775/2330 train_time:44734ms step_avg:57.72ms
step:776/2330 train_time:44794ms step_avg:57.72ms
step:777/2330 train_time:44850ms step_avg:57.72ms
step:778/2330 train_time:44910ms step_avg:57.73ms
step:779/2330 train_time:44967ms step_avg:57.72ms
step:780/2330 train_time:45026ms step_avg:57.73ms
step:781/2330 train_time:45082ms step_avg:57.72ms
step:782/2330 train_time:45141ms step_avg:57.73ms
step:783/2330 train_time:45198ms step_avg:57.72ms
step:784/2330 train_time:45257ms step_avg:57.73ms
step:785/2330 train_time:45314ms step_avg:57.72ms
step:786/2330 train_time:45375ms step_avg:57.73ms
step:787/2330 train_time:45431ms step_avg:57.73ms
step:788/2330 train_time:45494ms step_avg:57.73ms
step:789/2330 train_time:45550ms step_avg:57.73ms
step:790/2330 train_time:45612ms step_avg:57.74ms
step:791/2330 train_time:45670ms step_avg:57.74ms
step:792/2330 train_time:45731ms step_avg:57.74ms
step:793/2330 train_time:45787ms step_avg:57.74ms
step:794/2330 train_time:45847ms step_avg:57.74ms
step:795/2330 train_time:45903ms step_avg:57.74ms
step:796/2330 train_time:45963ms step_avg:57.74ms
step:797/2330 train_time:46020ms step_avg:57.74ms
step:798/2330 train_time:46080ms step_avg:57.74ms
step:799/2330 train_time:46136ms step_avg:57.74ms
step:800/2330 train_time:46195ms step_avg:57.74ms
step:801/2330 train_time:46252ms step_avg:57.74ms
step:802/2330 train_time:46312ms step_avg:57.75ms
step:803/2330 train_time:46369ms step_avg:57.74ms
step:804/2330 train_time:46429ms step_avg:57.75ms
step:805/2330 train_time:46486ms step_avg:57.75ms
step:806/2330 train_time:46547ms step_avg:57.75ms
step:807/2330 train_time:46604ms step_avg:57.75ms
step:808/2330 train_time:46666ms step_avg:57.76ms
step:809/2330 train_time:46723ms step_avg:57.75ms
step:810/2330 train_time:46783ms step_avg:57.76ms
step:811/2330 train_time:46841ms step_avg:57.76ms
step:812/2330 train_time:46900ms step_avg:57.76ms
step:813/2330 train_time:46957ms step_avg:57.76ms
step:814/2330 train_time:47016ms step_avg:57.76ms
step:815/2330 train_time:47073ms step_avg:57.76ms
step:816/2330 train_time:47132ms step_avg:57.76ms
step:817/2330 train_time:47189ms step_avg:57.76ms
step:818/2330 train_time:47248ms step_avg:57.76ms
step:819/2330 train_time:47305ms step_avg:57.76ms
step:820/2330 train_time:47366ms step_avg:57.76ms
step:821/2330 train_time:47423ms step_avg:57.76ms
step:822/2330 train_time:47484ms step_avg:57.77ms
step:823/2330 train_time:47542ms step_avg:57.77ms
step:824/2330 train_time:47601ms step_avg:57.77ms
step:825/2330 train_time:47659ms step_avg:57.77ms
step:826/2330 train_time:47720ms step_avg:57.77ms
step:827/2330 train_time:47778ms step_avg:57.77ms
step:828/2330 train_time:47838ms step_avg:57.78ms
step:829/2330 train_time:47897ms step_avg:57.78ms
step:830/2330 train_time:47956ms step_avg:57.78ms
step:831/2330 train_time:48014ms step_avg:57.78ms
step:832/2330 train_time:48072ms step_avg:57.78ms
step:833/2330 train_time:48129ms step_avg:57.78ms
step:834/2330 train_time:48189ms step_avg:57.78ms
step:835/2330 train_time:48245ms step_avg:57.78ms
step:836/2330 train_time:48305ms step_avg:57.78ms
step:837/2330 train_time:48362ms step_avg:57.78ms
step:838/2330 train_time:48424ms step_avg:57.79ms
step:839/2330 train_time:48481ms step_avg:57.78ms
step:840/2330 train_time:48541ms step_avg:57.79ms
step:841/2330 train_time:48598ms step_avg:57.79ms
step:842/2330 train_time:48659ms step_avg:57.79ms
step:843/2330 train_time:48717ms step_avg:57.79ms
step:844/2330 train_time:48777ms step_avg:57.79ms
step:845/2330 train_time:48833ms step_avg:57.79ms
step:846/2330 train_time:48894ms step_avg:57.79ms
step:847/2330 train_time:48951ms step_avg:57.79ms
step:848/2330 train_time:49010ms step_avg:57.79ms
step:849/2330 train_time:49067ms step_avg:57.79ms
step:850/2330 train_time:49127ms step_avg:57.80ms
step:851/2330 train_time:49184ms step_avg:57.80ms
step:852/2330 train_time:49243ms step_avg:57.80ms
step:853/2330 train_time:49300ms step_avg:57.80ms
step:854/2330 train_time:49361ms step_avg:57.80ms
step:855/2330 train_time:49418ms step_avg:57.80ms
step:856/2330 train_time:49478ms step_avg:57.80ms
step:857/2330 train_time:49535ms step_avg:57.80ms
step:858/2330 train_time:49595ms step_avg:57.80ms
step:859/2330 train_time:49651ms step_avg:57.80ms
step:860/2330 train_time:49713ms step_avg:57.81ms
step:861/2330 train_time:49770ms step_avg:57.81ms
step:862/2330 train_time:49830ms step_avg:57.81ms
step:863/2330 train_time:49887ms step_avg:57.81ms
step:864/2330 train_time:49947ms step_avg:57.81ms
step:865/2330 train_time:50004ms step_avg:57.81ms
step:866/2330 train_time:50064ms step_avg:57.81ms
step:867/2330 train_time:50122ms step_avg:57.81ms
step:868/2330 train_time:50181ms step_avg:57.81ms
step:869/2330 train_time:50238ms step_avg:57.81ms
step:870/2330 train_time:50298ms step_avg:57.81ms
step:871/2330 train_time:50355ms step_avg:57.81ms
step:872/2330 train_time:50414ms step_avg:57.81ms
step:873/2330 train_time:50471ms step_avg:57.81ms
step:874/2330 train_time:50532ms step_avg:57.82ms
step:875/2330 train_time:50589ms step_avg:57.82ms
step:876/2330 train_time:50649ms step_avg:57.82ms
step:877/2330 train_time:50706ms step_avg:57.82ms
step:878/2330 train_time:50766ms step_avg:57.82ms
step:879/2330 train_time:50823ms step_avg:57.82ms
step:880/2330 train_time:50884ms step_avg:57.82ms
step:881/2330 train_time:50941ms step_avg:57.82ms
step:882/2330 train_time:51001ms step_avg:57.82ms
step:883/2330 train_time:51058ms step_avg:57.82ms
step:884/2330 train_time:51119ms step_avg:57.83ms
step:885/2330 train_time:51176ms step_avg:57.83ms
step:886/2330 train_time:51236ms step_avg:57.83ms
step:887/2330 train_time:51293ms step_avg:57.83ms
step:888/2330 train_time:51353ms step_avg:57.83ms
step:889/2330 train_time:51410ms step_avg:57.83ms
step:890/2330 train_time:51471ms step_avg:57.83ms
step:891/2330 train_time:51528ms step_avg:57.83ms
step:892/2330 train_time:51587ms step_avg:57.83ms
step:893/2330 train_time:51644ms step_avg:57.83ms
step:894/2330 train_time:51705ms step_avg:57.84ms
step:895/2330 train_time:51762ms step_avg:57.83ms
step:896/2330 train_time:51823ms step_avg:57.84ms
step:897/2330 train_time:51880ms step_avg:57.84ms
step:898/2330 train_time:51940ms step_avg:57.84ms
step:899/2330 train_time:51996ms step_avg:57.84ms
step:900/2330 train_time:52056ms step_avg:57.84ms
step:901/2330 train_time:52113ms step_avg:57.84ms
step:902/2330 train_time:52174ms step_avg:57.84ms
step:903/2330 train_time:52230ms step_avg:57.84ms
step:904/2330 train_time:52290ms step_avg:57.84ms
step:905/2330 train_time:52347ms step_avg:57.84ms
step:906/2330 train_time:52408ms step_avg:57.84ms
step:907/2330 train_time:52464ms step_avg:57.84ms
step:908/2330 train_time:52524ms step_avg:57.85ms
step:909/2330 train_time:52581ms step_avg:57.84ms
step:910/2330 train_time:52641ms step_avg:57.85ms
step:911/2330 train_time:52698ms step_avg:57.85ms
step:912/2330 train_time:52758ms step_avg:57.85ms
step:913/2330 train_time:52816ms step_avg:57.85ms
step:914/2330 train_time:52876ms step_avg:57.85ms
step:915/2330 train_time:52933ms step_avg:57.85ms
step:916/2330 train_time:52993ms step_avg:57.85ms
step:917/2330 train_time:53049ms step_avg:57.85ms
step:918/2330 train_time:53110ms step_avg:57.85ms
step:919/2330 train_time:53167ms step_avg:57.85ms
step:920/2330 train_time:53227ms step_avg:57.86ms
step:921/2330 train_time:53283ms step_avg:57.85ms
step:922/2330 train_time:53344ms step_avg:57.86ms
step:923/2330 train_time:53401ms step_avg:57.86ms
step:924/2330 train_time:53461ms step_avg:57.86ms
step:925/2330 train_time:53518ms step_avg:57.86ms
step:926/2330 train_time:53578ms step_avg:57.86ms
step:927/2330 train_time:53636ms step_avg:57.86ms
step:928/2330 train_time:53696ms step_avg:57.86ms
step:929/2330 train_time:53752ms step_avg:57.86ms
step:930/2330 train_time:53814ms step_avg:57.86ms
step:931/2330 train_time:53870ms step_avg:57.86ms
step:932/2330 train_time:53931ms step_avg:57.87ms
step:933/2330 train_time:53988ms step_avg:57.86ms
step:934/2330 train_time:54049ms step_avg:57.87ms
step:935/2330 train_time:54106ms step_avg:57.87ms
step:936/2330 train_time:54166ms step_avg:57.87ms
step:937/2330 train_time:54223ms step_avg:57.87ms
step:938/2330 train_time:54283ms step_avg:57.87ms
step:939/2330 train_time:54340ms step_avg:57.87ms
step:940/2330 train_time:54400ms step_avg:57.87ms
step:941/2330 train_time:54457ms step_avg:57.87ms
step:942/2330 train_time:54517ms step_avg:57.87ms
step:943/2330 train_time:54573ms step_avg:57.87ms
step:944/2330 train_time:54634ms step_avg:57.88ms
step:945/2330 train_time:54691ms step_avg:57.87ms
step:946/2330 train_time:54751ms step_avg:57.88ms
step:947/2330 train_time:54809ms step_avg:57.88ms
step:948/2330 train_time:54868ms step_avg:57.88ms
step:949/2330 train_time:54924ms step_avg:57.88ms
step:950/2330 train_time:54985ms step_avg:57.88ms
step:951/2330 train_time:55041ms step_avg:57.88ms
step:952/2330 train_time:55101ms step_avg:57.88ms
step:953/2330 train_time:55158ms step_avg:57.88ms
step:954/2330 train_time:55220ms step_avg:57.88ms
step:955/2330 train_time:55277ms step_avg:57.88ms
step:956/2330 train_time:55337ms step_avg:57.88ms
step:957/2330 train_time:55394ms step_avg:57.88ms
step:958/2330 train_time:55454ms step_avg:57.88ms
step:959/2330 train_time:55510ms step_avg:57.88ms
step:960/2330 train_time:55572ms step_avg:57.89ms
step:961/2330 train_time:55629ms step_avg:57.89ms
step:962/2330 train_time:55689ms step_avg:57.89ms
step:963/2330 train_time:55746ms step_avg:57.89ms
step:964/2330 train_time:55807ms step_avg:57.89ms
step:965/2330 train_time:55864ms step_avg:57.89ms
step:966/2330 train_time:55925ms step_avg:57.89ms
step:967/2330 train_time:55982ms step_avg:57.89ms
step:968/2330 train_time:56042ms step_avg:57.89ms
step:969/2330 train_time:56098ms step_avg:57.89ms
step:970/2330 train_time:56158ms step_avg:57.90ms
step:971/2330 train_time:56215ms step_avg:57.89ms
step:972/2330 train_time:56277ms step_avg:57.90ms
step:973/2330 train_time:56334ms step_avg:57.90ms
step:974/2330 train_time:56394ms step_avg:57.90ms
step:975/2330 train_time:56450ms step_avg:57.90ms
step:976/2330 train_time:56511ms step_avg:57.90ms
step:977/2330 train_time:56568ms step_avg:57.90ms
step:978/2330 train_time:56628ms step_avg:57.90ms
step:979/2330 train_time:56685ms step_avg:57.90ms
step:980/2330 train_time:56746ms step_avg:57.90ms
step:981/2330 train_time:56803ms step_avg:57.90ms
step:982/2330 train_time:56862ms step_avg:57.90ms
step:983/2330 train_time:56920ms step_avg:57.90ms
step:984/2330 train_time:56979ms step_avg:57.91ms
step:985/2330 train_time:57036ms step_avg:57.90ms
step:986/2330 train_time:57095ms step_avg:57.91ms
step:987/2330 train_time:57151ms step_avg:57.90ms
step:988/2330 train_time:57213ms step_avg:57.91ms
step:989/2330 train_time:57271ms step_avg:57.91ms
step:990/2330 train_time:57331ms step_avg:57.91ms
step:991/2330 train_time:57388ms step_avg:57.91ms
step:992/2330 train_time:57448ms step_avg:57.91ms
step:993/2330 train_time:57505ms step_avg:57.91ms
step:994/2330 train_time:57565ms step_avg:57.91ms
step:995/2330 train_time:57622ms step_avg:57.91ms
step:996/2330 train_time:57683ms step_avg:57.91ms
step:997/2330 train_time:57740ms step_avg:57.91ms
step:998/2330 train_time:57799ms step_avg:57.92ms
step:999/2330 train_time:57856ms step_avg:57.91ms
step:1000/2330 train_time:57917ms step_avg:57.92ms
step:1000/2330 val_loss:4.0699 train_time:57998ms step_avg:58.00ms
step:1001/2330 train_time:58017ms step_avg:57.96ms
step:1002/2330 train_time:58038ms step_avg:57.92ms
step:1003/2330 train_time:58096ms step_avg:57.92ms
step:1004/2330 train_time:58161ms step_avg:57.93ms
step:1005/2330 train_time:58219ms step_avg:57.93ms
step:1006/2330 train_time:58281ms step_avg:57.93ms
step:1007/2330 train_time:58337ms step_avg:57.93ms
step:1008/2330 train_time:58397ms step_avg:57.93ms
step:1009/2330 train_time:58453ms step_avg:57.93ms
step:1010/2330 train_time:58512ms step_avg:57.93ms
step:1011/2330 train_time:58569ms step_avg:57.93ms
step:1012/2330 train_time:58628ms step_avg:57.93ms
step:1013/2330 train_time:58684ms step_avg:57.93ms
step:1014/2330 train_time:58743ms step_avg:57.93ms
step:1015/2330 train_time:58798ms step_avg:57.93ms
step:1016/2330 train_time:58858ms step_avg:57.93ms
step:1017/2330 train_time:58915ms step_avg:57.93ms
step:1018/2330 train_time:58977ms step_avg:57.93ms
step:1019/2330 train_time:59034ms step_avg:57.93ms
step:1020/2330 train_time:59098ms step_avg:57.94ms
step:1021/2330 train_time:59157ms step_avg:57.94ms
step:1022/2330 train_time:59217ms step_avg:57.94ms
step:1023/2330 train_time:59275ms step_avg:57.94ms
step:1024/2330 train_time:59335ms step_avg:57.94ms
step:1025/2330 train_time:59393ms step_avg:57.94ms
step:1026/2330 train_time:59453ms step_avg:57.95ms
step:1027/2330 train_time:59510ms step_avg:57.94ms
step:1028/2330 train_time:59569ms step_avg:57.95ms
step:1029/2330 train_time:59625ms step_avg:57.94ms
step:1030/2330 train_time:59684ms step_avg:57.95ms
step:1031/2330 train_time:59740ms step_avg:57.94ms
step:1032/2330 train_time:59800ms step_avg:57.95ms
step:1033/2330 train_time:59857ms step_avg:57.94ms
step:1034/2330 train_time:59917ms step_avg:57.95ms
step:1035/2330 train_time:59974ms step_avg:57.95ms
step:1036/2330 train_time:60036ms step_avg:57.95ms
step:1037/2330 train_time:60093ms step_avg:57.95ms
step:1038/2330 train_time:60154ms step_avg:57.95ms
step:1039/2330 train_time:60212ms step_avg:57.95ms
step:1040/2330 train_time:60273ms step_avg:57.95ms
step:1041/2330 train_time:60331ms step_avg:57.95ms
step:1042/2330 train_time:60390ms step_avg:57.96ms
step:1043/2330 train_time:60448ms step_avg:57.96ms
step:1044/2330 train_time:60507ms step_avg:57.96ms
step:1045/2330 train_time:60564ms step_avg:57.96ms
step:1046/2330 train_time:60623ms step_avg:57.96ms
step:1047/2330 train_time:60679ms step_avg:57.96ms
step:1048/2330 train_time:60739ms step_avg:57.96ms
step:1049/2330 train_time:60795ms step_avg:57.96ms
step:1050/2330 train_time:60856ms step_avg:57.96ms
step:1051/2330 train_time:60913ms step_avg:57.96ms
step:1052/2330 train_time:60973ms step_avg:57.96ms
step:1053/2330 train_time:61030ms step_avg:57.96ms
step:1054/2330 train_time:61091ms step_avg:57.96ms
step:1055/2330 train_time:61148ms step_avg:57.96ms
step:1056/2330 train_time:61209ms step_avg:57.96ms
step:1057/2330 train_time:61265ms step_avg:57.96ms
step:1058/2330 train_time:61327ms step_avg:57.96ms
step:1059/2330 train_time:61383ms step_avg:57.96ms
step:1060/2330 train_time:61444ms step_avg:57.97ms
step:1061/2330 train_time:61501ms step_avg:57.97ms
step:1062/2330 train_time:61561ms step_avg:57.97ms
step:1063/2330 train_time:61618ms step_avg:57.97ms
step:1064/2330 train_time:61677ms step_avg:57.97ms
step:1065/2330 train_time:61734ms step_avg:57.97ms
step:1066/2330 train_time:61794ms step_avg:57.97ms
step:1067/2330 train_time:61850ms step_avg:57.97ms
step:1068/2330 train_time:61911ms step_avg:57.97ms
step:1069/2330 train_time:61967ms step_avg:57.97ms
step:1070/2330 train_time:62029ms step_avg:57.97ms
step:1071/2330 train_time:62086ms step_avg:57.97ms
step:1072/2330 train_time:62146ms step_avg:57.97ms
step:1073/2330 train_time:62204ms step_avg:57.97ms
step:1074/2330 train_time:62265ms step_avg:57.97ms
step:1075/2330 train_time:62321ms step_avg:57.97ms
step:1076/2330 train_time:62381ms step_avg:57.98ms
step:1077/2330 train_time:62438ms step_avg:57.97ms
step:1078/2330 train_time:62498ms step_avg:57.98ms
step:1079/2330 train_time:62555ms step_avg:57.97ms
step:1080/2330 train_time:62615ms step_avg:57.98ms
step:1081/2330 train_time:62672ms step_avg:57.98ms
step:1082/2330 train_time:62732ms step_avg:57.98ms
step:1083/2330 train_time:62789ms step_avg:57.98ms
step:1084/2330 train_time:62849ms step_avg:57.98ms
step:1085/2330 train_time:62906ms step_avg:57.98ms
step:1086/2330 train_time:62967ms step_avg:57.98ms
step:1087/2330 train_time:63024ms step_avg:57.98ms
step:1088/2330 train_time:63083ms step_avg:57.98ms
step:1089/2330 train_time:63141ms step_avg:57.98ms
step:1090/2330 train_time:63201ms step_avg:57.98ms
step:1091/2330 train_time:63258ms step_avg:57.98ms
step:1092/2330 train_time:63319ms step_avg:57.98ms
step:1093/2330 train_time:63375ms step_avg:57.98ms
step:1094/2330 train_time:63436ms step_avg:57.99ms
step:1095/2330 train_time:63493ms step_avg:57.98ms
step:1096/2330 train_time:63552ms step_avg:57.99ms
step:1097/2330 train_time:63609ms step_avg:57.98ms
step:1098/2330 train_time:63669ms step_avg:57.99ms
step:1099/2330 train_time:63726ms step_avg:57.99ms
step:1100/2330 train_time:63785ms step_avg:57.99ms
step:1101/2330 train_time:63841ms step_avg:57.98ms
step:1102/2330 train_time:63902ms step_avg:57.99ms
step:1103/2330 train_time:63959ms step_avg:57.99ms
step:1104/2330 train_time:64019ms step_avg:57.99ms
step:1105/2330 train_time:64077ms step_avg:57.99ms
step:1106/2330 train_time:64137ms step_avg:57.99ms
step:1107/2330 train_time:64194ms step_avg:57.99ms
step:1108/2330 train_time:64254ms step_avg:57.99ms
step:1109/2330 train_time:64313ms step_avg:57.99ms
step:1110/2330 train_time:64373ms step_avg:57.99ms
step:1111/2330 train_time:64430ms step_avg:57.99ms
step:1112/2330 train_time:64489ms step_avg:57.99ms
step:1113/2330 train_time:64546ms step_avg:57.99ms
step:1114/2330 train_time:64607ms step_avg:58.00ms
step:1115/2330 train_time:64664ms step_avg:57.99ms
step:1116/2330 train_time:64723ms step_avg:58.00ms
step:1117/2330 train_time:64780ms step_avg:57.99ms
step:1118/2330 train_time:64841ms step_avg:58.00ms
step:1119/2330 train_time:64897ms step_avg:58.00ms
step:1120/2330 train_time:64959ms step_avg:58.00ms
step:1121/2330 train_time:65017ms step_avg:58.00ms
step:1122/2330 train_time:65076ms step_avg:58.00ms
step:1123/2330 train_time:65134ms step_avg:58.00ms
step:1124/2330 train_time:65194ms step_avg:58.00ms
step:1125/2330 train_time:65251ms step_avg:58.00ms
step:1126/2330 train_time:65311ms step_avg:58.00ms
step:1127/2330 train_time:65367ms step_avg:58.00ms
step:1128/2330 train_time:65427ms step_avg:58.00ms
step:1129/2330 train_time:65484ms step_avg:58.00ms
step:1130/2330 train_time:65543ms step_avg:58.00ms
step:1131/2330 train_time:65600ms step_avg:58.00ms
step:1132/2330 train_time:65662ms step_avg:58.01ms
step:1133/2330 train_time:65719ms step_avg:58.00ms
step:1134/2330 train_time:65778ms step_avg:58.01ms
step:1135/2330 train_time:65836ms step_avg:58.00ms
step:1136/2330 train_time:65896ms step_avg:58.01ms
step:1137/2330 train_time:65952ms step_avg:58.01ms
step:1138/2330 train_time:66012ms step_avg:58.01ms
step:1139/2330 train_time:66069ms step_avg:58.01ms
step:1140/2330 train_time:66128ms step_avg:58.01ms
step:1141/2330 train_time:66185ms step_avg:58.01ms
step:1142/2330 train_time:66247ms step_avg:58.01ms
step:1143/2330 train_time:66303ms step_avg:58.01ms
step:1144/2330 train_time:66364ms step_avg:58.01ms
step:1145/2330 train_time:66420ms step_avg:58.01ms
step:1146/2330 train_time:66480ms step_avg:58.01ms
step:1147/2330 train_time:66536ms step_avg:58.01ms
step:1148/2330 train_time:66597ms step_avg:58.01ms
step:1149/2330 train_time:66656ms step_avg:58.01ms
step:1150/2330 train_time:66716ms step_avg:58.01ms
step:1151/2330 train_time:66772ms step_avg:58.01ms
step:1152/2330 train_time:66833ms step_avg:58.01ms
step:1153/2330 train_time:66889ms step_avg:58.01ms
step:1154/2330 train_time:66950ms step_avg:58.02ms
step:1155/2330 train_time:67007ms step_avg:58.02ms
step:1156/2330 train_time:67067ms step_avg:58.02ms
step:1157/2330 train_time:67125ms step_avg:58.02ms
step:1158/2330 train_time:67185ms step_avg:58.02ms
step:1159/2330 train_time:67242ms step_avg:58.02ms
step:1160/2330 train_time:67302ms step_avg:58.02ms
step:1161/2330 train_time:67358ms step_avg:58.02ms
step:1162/2330 train_time:67419ms step_avg:58.02ms
step:1163/2330 train_time:67476ms step_avg:58.02ms
step:1164/2330 train_time:67536ms step_avg:58.02ms
step:1165/2330 train_time:67593ms step_avg:58.02ms
step:1166/2330 train_time:67653ms step_avg:58.02ms
step:1167/2330 train_time:67710ms step_avg:58.02ms
step:1168/2330 train_time:67771ms step_avg:58.02ms
step:1169/2330 train_time:67828ms step_avg:58.02ms
step:1170/2330 train_time:67887ms step_avg:58.02ms
step:1171/2330 train_time:67944ms step_avg:58.02ms
step:1172/2330 train_time:68005ms step_avg:58.02ms
step:1173/2330 train_time:68062ms step_avg:58.02ms
step:1174/2330 train_time:68122ms step_avg:58.03ms
step:1175/2330 train_time:68179ms step_avg:58.02ms
step:1176/2330 train_time:68239ms step_avg:58.03ms
step:1177/2330 train_time:68296ms step_avg:58.03ms
step:1178/2330 train_time:68357ms step_avg:58.03ms
step:1179/2330 train_time:68414ms step_avg:58.03ms
step:1180/2330 train_time:68474ms step_avg:58.03ms
step:1181/2330 train_time:68531ms step_avg:58.03ms
step:1182/2330 train_time:68590ms step_avg:58.03ms
step:1183/2330 train_time:68647ms step_avg:58.03ms
step:1184/2330 train_time:68709ms step_avg:58.03ms
step:1185/2330 train_time:68766ms step_avg:58.03ms
step:1186/2330 train_time:68826ms step_avg:58.03ms
step:1187/2330 train_time:68882ms step_avg:58.03ms
step:1188/2330 train_time:68943ms step_avg:58.03ms
step:1189/2330 train_time:69000ms step_avg:58.03ms
step:1190/2330 train_time:69060ms step_avg:58.03ms
step:1191/2330 train_time:69117ms step_avg:58.03ms
step:1192/2330 train_time:69177ms step_avg:58.03ms
step:1193/2330 train_time:69234ms step_avg:58.03ms
step:1194/2330 train_time:69294ms step_avg:58.04ms
step:1195/2330 train_time:69352ms step_avg:58.03ms
step:1196/2330 train_time:69412ms step_avg:58.04ms
step:1197/2330 train_time:69469ms step_avg:58.04ms
step:1198/2330 train_time:69530ms step_avg:58.04ms
step:1199/2330 train_time:69586ms step_avg:58.04ms
step:1200/2330 train_time:69647ms step_avg:58.04ms
step:1201/2330 train_time:69703ms step_avg:58.04ms
step:1202/2330 train_time:69764ms step_avg:58.04ms
step:1203/2330 train_time:69820ms step_avg:58.04ms
step:1204/2330 train_time:69880ms step_avg:58.04ms
step:1205/2330 train_time:69937ms step_avg:58.04ms
step:1206/2330 train_time:69997ms step_avg:58.04ms
step:1207/2330 train_time:70054ms step_avg:58.04ms
step:1208/2330 train_time:70114ms step_avg:58.04ms
step:1209/2330 train_time:70171ms step_avg:58.04ms
step:1210/2330 train_time:70232ms step_avg:58.04ms
step:1211/2330 train_time:70288ms step_avg:58.04ms
step:1212/2330 train_time:70349ms step_avg:58.04ms
step:1213/2330 train_time:70405ms step_avg:58.04ms
step:1214/2330 train_time:70467ms step_avg:58.05ms
step:1215/2330 train_time:70524ms step_avg:58.04ms
step:1216/2330 train_time:70583ms step_avg:58.05ms
step:1217/2330 train_time:70640ms step_avg:58.04ms
step:1218/2330 train_time:70700ms step_avg:58.05ms
step:1219/2330 train_time:70757ms step_avg:58.04ms
step:1220/2330 train_time:70818ms step_avg:58.05ms
step:1221/2330 train_time:70874ms step_avg:58.05ms
step:1222/2330 train_time:70935ms step_avg:58.05ms
step:1223/2330 train_time:70991ms step_avg:58.05ms
step:1224/2330 train_time:71051ms step_avg:58.05ms
step:1225/2330 train_time:71108ms step_avg:58.05ms
step:1226/2330 train_time:71169ms step_avg:58.05ms
step:1227/2330 train_time:71225ms step_avg:58.05ms
step:1228/2330 train_time:71286ms step_avg:58.05ms
step:1229/2330 train_time:71343ms step_avg:58.05ms
step:1230/2330 train_time:71403ms step_avg:58.05ms
step:1231/2330 train_time:71460ms step_avg:58.05ms
step:1232/2330 train_time:71519ms step_avg:58.05ms
step:1233/2330 train_time:71577ms step_avg:58.05ms
step:1234/2330 train_time:71636ms step_avg:58.05ms
step:1235/2330 train_time:71693ms step_avg:58.05ms
step:1236/2330 train_time:71753ms step_avg:58.05ms
step:1237/2330 train_time:71810ms step_avg:58.05ms
step:1238/2330 train_time:71871ms step_avg:58.05ms
step:1239/2330 train_time:71927ms step_avg:58.05ms
step:1240/2330 train_time:71989ms step_avg:58.06ms
step:1241/2330 train_time:72045ms step_avg:58.05ms
step:1242/2330 train_time:72106ms step_avg:58.06ms
step:1243/2330 train_time:72163ms step_avg:58.06ms
step:1244/2330 train_time:72222ms step_avg:58.06ms
step:1245/2330 train_time:72279ms step_avg:58.06ms
step:1246/2330 train_time:72340ms step_avg:58.06ms
step:1247/2330 train_time:72398ms step_avg:58.06ms
step:1248/2330 train_time:72458ms step_avg:58.06ms
step:1249/2330 train_time:72516ms step_avg:58.06ms
step:1250/2330 train_time:72576ms step_avg:58.06ms
step:1250/2330 val_loss:3.9898 train_time:72657ms step_avg:58.13ms
step:1251/2330 train_time:72676ms step_avg:58.09ms
step:1252/2330 train_time:72697ms step_avg:58.06ms
step:1253/2330 train_time:72753ms step_avg:58.06ms
step:1254/2330 train_time:72816ms step_avg:58.07ms
step:1255/2330 train_time:72871ms step_avg:58.06ms
step:1256/2330 train_time:72935ms step_avg:58.07ms
step:1257/2330 train_time:72991ms step_avg:58.07ms
step:1258/2330 train_time:73052ms step_avg:58.07ms
step:1259/2330 train_time:73108ms step_avg:58.07ms
step:1260/2330 train_time:73168ms step_avg:58.07ms
step:1261/2330 train_time:73224ms step_avg:58.07ms
step:1262/2330 train_time:73283ms step_avg:58.07ms
step:1263/2330 train_time:73340ms step_avg:58.07ms
step:1264/2330 train_time:73400ms step_avg:58.07ms
step:1265/2330 train_time:73456ms step_avg:58.07ms
step:1266/2330 train_time:73515ms step_avg:58.07ms
step:1267/2330 train_time:73572ms step_avg:58.07ms
step:1268/2330 train_time:73633ms step_avg:58.07ms
step:1269/2330 train_time:73690ms step_avg:58.07ms
step:1270/2330 train_time:73752ms step_avg:58.07ms
step:1271/2330 train_time:73809ms step_avg:58.07ms
step:1272/2330 train_time:73870ms step_avg:58.07ms
step:1273/2330 train_time:73927ms step_avg:58.07ms
step:1274/2330 train_time:73989ms step_avg:58.08ms
step:1275/2330 train_time:74045ms step_avg:58.07ms
step:1276/2330 train_time:74105ms step_avg:58.08ms
step:1277/2330 train_time:74161ms step_avg:58.07ms
step:1278/2330 train_time:74222ms step_avg:58.08ms
step:1279/2330 train_time:74279ms step_avg:58.08ms
step:1280/2330 train_time:74338ms step_avg:58.08ms
step:1281/2330 train_time:74396ms step_avg:58.08ms
step:1282/2330 train_time:74455ms step_avg:58.08ms
step:1283/2330 train_time:74512ms step_avg:58.08ms
step:1284/2330 train_time:74572ms step_avg:58.08ms
step:1285/2330 train_time:74630ms step_avg:58.08ms
step:1286/2330 train_time:74690ms step_avg:58.08ms
step:1287/2330 train_time:74747ms step_avg:58.08ms
step:1288/2330 train_time:74807ms step_avg:58.08ms
step:1289/2330 train_time:74865ms step_avg:58.08ms
step:1290/2330 train_time:74925ms step_avg:58.08ms
step:1291/2330 train_time:74983ms step_avg:58.08ms
step:1292/2330 train_time:75043ms step_avg:58.08ms
step:1293/2330 train_time:75100ms step_avg:58.08ms
step:1294/2330 train_time:75159ms step_avg:58.08ms
step:1295/2330 train_time:75216ms step_avg:58.08ms
step:1296/2330 train_time:75276ms step_avg:58.08ms
step:1297/2330 train_time:75333ms step_avg:58.08ms
step:1298/2330 train_time:75393ms step_avg:58.08ms
step:1299/2330 train_time:75449ms step_avg:58.08ms
step:1300/2330 train_time:75508ms step_avg:58.08ms
step:1301/2330 train_time:75565ms step_avg:58.08ms
step:1302/2330 train_time:75625ms step_avg:58.08ms
step:1303/2330 train_time:75684ms step_avg:58.08ms
step:1304/2330 train_time:75744ms step_avg:58.09ms
step:1305/2330 train_time:75801ms step_avg:58.09ms
step:1306/2330 train_time:75861ms step_avg:58.09ms
step:1307/2330 train_time:75919ms step_avg:58.09ms
step:1308/2330 train_time:75979ms step_avg:58.09ms
step:1309/2330 train_time:76036ms step_avg:58.09ms
step:1310/2330 train_time:76096ms step_avg:58.09ms
step:1311/2330 train_time:76152ms step_avg:58.09ms
step:1312/2330 train_time:76213ms step_avg:58.09ms
step:1313/2330 train_time:76269ms step_avg:58.09ms
step:1314/2330 train_time:76329ms step_avg:58.09ms
step:1315/2330 train_time:76385ms step_avg:58.09ms
step:1316/2330 train_time:76446ms step_avg:58.09ms
step:1317/2330 train_time:76502ms step_avg:58.09ms
step:1318/2330 train_time:76563ms step_avg:58.09ms
step:1319/2330 train_time:76619ms step_avg:58.09ms
step:1320/2330 train_time:76680ms step_avg:58.09ms
step:1321/2330 train_time:76737ms step_avg:58.09ms
step:1322/2330 train_time:76798ms step_avg:58.09ms
step:1323/2330 train_time:76855ms step_avg:58.09ms
step:1324/2330 train_time:76915ms step_avg:58.09ms
step:1325/2330 train_time:76972ms step_avg:58.09ms
step:1326/2330 train_time:77033ms step_avg:58.09ms
step:1327/2330 train_time:77090ms step_avg:58.09ms
step:1328/2330 train_time:77150ms step_avg:58.09ms
step:1329/2330 train_time:77206ms step_avg:58.09ms
step:1330/2330 train_time:77266ms step_avg:58.09ms
step:1331/2330 train_time:77322ms step_avg:58.09ms
step:1332/2330 train_time:77383ms step_avg:58.10ms
step:1333/2330 train_time:77440ms step_avg:58.09ms
step:1334/2330 train_time:77500ms step_avg:58.10ms
step:1335/2330 train_time:77556ms step_avg:58.09ms
step:1336/2330 train_time:77617ms step_avg:58.10ms
step:1337/2330 train_time:77674ms step_avg:58.10ms
step:1338/2330 train_time:77734ms step_avg:58.10ms
step:1339/2330 train_time:77791ms step_avg:58.10ms
step:1340/2330 train_time:77851ms step_avg:58.10ms
step:1341/2330 train_time:77908ms step_avg:58.10ms
step:1342/2330 train_time:77969ms step_avg:58.10ms
step:1343/2330 train_time:78026ms step_avg:58.10ms
step:1344/2330 train_time:78086ms step_avg:58.10ms
step:1345/2330 train_time:78143ms step_avg:58.10ms
step:1346/2330 train_time:78203ms step_avg:58.10ms
step:1347/2330 train_time:78260ms step_avg:58.10ms
step:1348/2330 train_time:78320ms step_avg:58.10ms
step:1349/2330 train_time:78377ms step_avg:58.10ms
step:1350/2330 train_time:78437ms step_avg:58.10ms
step:1351/2330 train_time:78494ms step_avg:58.10ms
step:1352/2330 train_time:78555ms step_avg:58.10ms
step:1353/2330 train_time:78611ms step_avg:58.10ms
step:1354/2330 train_time:78673ms step_avg:58.10ms
step:1355/2330 train_time:78730ms step_avg:58.10ms
step:1356/2330 train_time:78790ms step_avg:58.10ms
step:1357/2330 train_time:78846ms step_avg:58.10ms
step:1358/2330 train_time:78907ms step_avg:58.11ms
step:1359/2330 train_time:78964ms step_avg:58.10ms
step:1360/2330 train_time:79024ms step_avg:58.11ms
step:1361/2330 train_time:79081ms step_avg:58.10ms
step:1362/2330 train_time:79141ms step_avg:58.11ms
step:1363/2330 train_time:79198ms step_avg:58.11ms
step:1364/2330 train_time:79258ms step_avg:58.11ms
step:1365/2330 train_time:79314ms step_avg:58.11ms
step:1366/2330 train_time:79375ms step_avg:58.11ms
step:1367/2330 train_time:79432ms step_avg:58.11ms
step:1368/2330 train_time:79492ms step_avg:58.11ms
step:1369/2330 train_time:79549ms step_avg:58.11ms
step:1370/2330 train_time:79608ms step_avg:58.11ms
step:1371/2330 train_time:79665ms step_avg:58.11ms
step:1372/2330 train_time:79726ms step_avg:58.11ms
step:1373/2330 train_time:79783ms step_avg:58.11ms
step:1374/2330 train_time:79843ms step_avg:58.11ms
step:1375/2330 train_time:79900ms step_avg:58.11ms
step:1376/2330 train_time:79960ms step_avg:58.11ms
step:1377/2330 train_time:80017ms step_avg:58.11ms
step:1378/2330 train_time:80076ms step_avg:58.11ms
step:1379/2330 train_time:80132ms step_avg:58.11ms
step:1380/2330 train_time:80193ms step_avg:58.11ms
step:1381/2330 train_time:80250ms step_avg:58.11ms
step:1382/2330 train_time:80310ms step_avg:58.11ms
step:1383/2330 train_time:80366ms step_avg:58.11ms
step:1384/2330 train_time:80428ms step_avg:58.11ms
step:1385/2330 train_time:80485ms step_avg:58.11ms
step:1386/2330 train_time:80545ms step_avg:58.11ms
step:1387/2330 train_time:80601ms step_avg:58.11ms
step:1388/2330 train_time:80662ms step_avg:58.11ms
step:1389/2330 train_time:80720ms step_avg:58.11ms
step:1390/2330 train_time:80780ms step_avg:58.11ms
step:1391/2330 train_time:80837ms step_avg:58.11ms
step:1392/2330 train_time:80897ms step_avg:58.12ms
step:1393/2330 train_time:80954ms step_avg:58.12ms
step:1394/2330 train_time:81014ms step_avg:58.12ms
step:1395/2330 train_time:81071ms step_avg:58.12ms
step:1396/2330 train_time:81130ms step_avg:58.12ms
step:1397/2330 train_time:81187ms step_avg:58.12ms
step:1398/2330 train_time:81248ms step_avg:58.12ms
step:1399/2330 train_time:81304ms step_avg:58.12ms
step:1400/2330 train_time:81365ms step_avg:58.12ms
step:1401/2330 train_time:81422ms step_avg:58.12ms
step:1402/2330 train_time:81483ms step_avg:58.12ms
step:1403/2330 train_time:81541ms step_avg:58.12ms
step:1404/2330 train_time:81601ms step_avg:58.12ms
step:1405/2330 train_time:81658ms step_avg:58.12ms
step:1406/2330 train_time:81718ms step_avg:58.12ms
step:1407/2330 train_time:81775ms step_avg:58.12ms
step:1408/2330 train_time:81835ms step_avg:58.12ms
step:1409/2330 train_time:81892ms step_avg:58.12ms
step:1410/2330 train_time:81952ms step_avg:58.12ms
step:1411/2330 train_time:82009ms step_avg:58.12ms
step:1412/2330 train_time:82069ms step_avg:58.12ms
step:1413/2330 train_time:82126ms step_avg:58.12ms
step:1414/2330 train_time:82187ms step_avg:58.12ms
step:1415/2330 train_time:82244ms step_avg:58.12ms
step:1416/2330 train_time:82303ms step_avg:58.12ms
step:1417/2330 train_time:82360ms step_avg:58.12ms
step:1418/2330 train_time:82420ms step_avg:58.12ms
step:1419/2330 train_time:82477ms step_avg:58.12ms
step:1420/2330 train_time:82538ms step_avg:58.13ms
step:1421/2330 train_time:82594ms step_avg:58.12ms
step:1422/2330 train_time:82655ms step_avg:58.13ms
step:1423/2330 train_time:82711ms step_avg:58.12ms
step:1424/2330 train_time:82771ms step_avg:58.13ms
step:1425/2330 train_time:82828ms step_avg:58.12ms
step:1426/2330 train_time:82889ms step_avg:58.13ms
step:1427/2330 train_time:82945ms step_avg:58.13ms
step:1428/2330 train_time:83005ms step_avg:58.13ms
step:1429/2330 train_time:83063ms step_avg:58.13ms
step:1430/2330 train_time:83123ms step_avg:58.13ms
step:1431/2330 train_time:83180ms step_avg:58.13ms
step:1432/2330 train_time:83240ms step_avg:58.13ms
step:1433/2330 train_time:83297ms step_avg:58.13ms
step:1434/2330 train_time:83357ms step_avg:58.13ms
step:1435/2330 train_time:83414ms step_avg:58.13ms
step:1436/2330 train_time:83473ms step_avg:58.13ms
step:1437/2330 train_time:83530ms step_avg:58.13ms
step:1438/2330 train_time:83591ms step_avg:58.13ms
step:1439/2330 train_time:83647ms step_avg:58.13ms
step:1440/2330 train_time:83708ms step_avg:58.13ms
step:1441/2330 train_time:83765ms step_avg:58.13ms
step:1442/2330 train_time:83826ms step_avg:58.13ms
step:1443/2330 train_time:83883ms step_avg:58.13ms
step:1444/2330 train_time:83943ms step_avg:58.13ms
step:1445/2330 train_time:84000ms step_avg:58.13ms
step:1446/2330 train_time:84060ms step_avg:58.13ms
step:1447/2330 train_time:84117ms step_avg:58.13ms
step:1448/2330 train_time:84177ms step_avg:58.13ms
step:1449/2330 train_time:84234ms step_avg:58.13ms
step:1450/2330 train_time:84295ms step_avg:58.13ms
step:1451/2330 train_time:84351ms step_avg:58.13ms
step:1452/2330 train_time:84411ms step_avg:58.13ms
step:1453/2330 train_time:84469ms step_avg:58.13ms
step:1454/2330 train_time:84529ms step_avg:58.14ms
step:1455/2330 train_time:84586ms step_avg:58.13ms
step:1456/2330 train_time:84646ms step_avg:58.14ms
step:1457/2330 train_time:84703ms step_avg:58.14ms
step:1458/2330 train_time:84764ms step_avg:58.14ms
step:1459/2330 train_time:84821ms step_avg:58.14ms
step:1460/2330 train_time:84883ms step_avg:58.14ms
step:1461/2330 train_time:84940ms step_avg:58.14ms
step:1462/2330 train_time:85000ms step_avg:58.14ms
step:1463/2330 train_time:85058ms step_avg:58.14ms
step:1464/2330 train_time:85117ms step_avg:58.14ms
step:1465/2330 train_time:85175ms step_avg:58.14ms
step:1466/2330 train_time:85234ms step_avg:58.14ms
step:1467/2330 train_time:85291ms step_avg:58.14ms
step:1468/2330 train_time:85352ms step_avg:58.14ms
step:1469/2330 train_time:85408ms step_avg:58.14ms
step:1470/2330 train_time:85470ms step_avg:58.14ms
step:1471/2330 train_time:85526ms step_avg:58.14ms
step:1472/2330 train_time:85586ms step_avg:58.14ms
step:1473/2330 train_time:85643ms step_avg:58.14ms
step:1474/2330 train_time:85703ms step_avg:58.14ms
step:1475/2330 train_time:85759ms step_avg:58.14ms
step:1476/2330 train_time:85819ms step_avg:58.14ms
step:1477/2330 train_time:85877ms step_avg:58.14ms
step:1478/2330 train_time:85937ms step_avg:58.14ms
step:1479/2330 train_time:85994ms step_avg:58.14ms
step:1480/2330 train_time:86054ms step_avg:58.14ms
step:1481/2330 train_time:86110ms step_avg:58.14ms
step:1482/2330 train_time:86170ms step_avg:58.14ms
step:1483/2330 train_time:86228ms step_avg:58.14ms
step:1484/2330 train_time:86288ms step_avg:58.15ms
step:1485/2330 train_time:86345ms step_avg:58.14ms
step:1486/2330 train_time:86406ms step_avg:58.15ms
step:1487/2330 train_time:86462ms step_avg:58.15ms
step:1488/2330 train_time:86523ms step_avg:58.15ms
step:1489/2330 train_time:86580ms step_avg:58.15ms
step:1490/2330 train_time:86640ms step_avg:58.15ms
step:1491/2330 train_time:86698ms step_avg:58.15ms
step:1492/2330 train_time:86757ms step_avg:58.15ms
step:1493/2330 train_time:86814ms step_avg:58.15ms
step:1494/2330 train_time:86875ms step_avg:58.15ms
step:1495/2330 train_time:86932ms step_avg:58.15ms
step:1496/2330 train_time:86992ms step_avg:58.15ms
step:1497/2330 train_time:87049ms step_avg:58.15ms
step:1498/2330 train_time:87109ms step_avg:58.15ms
step:1499/2330 train_time:87166ms step_avg:58.15ms
step:1500/2330 train_time:87227ms step_avg:58.15ms
step:1500/2330 val_loss:3.9046 train_time:87308ms step_avg:58.21ms
step:1501/2330 train_time:87328ms step_avg:58.18ms
step:1502/2330 train_time:87348ms step_avg:58.15ms
step:1503/2330 train_time:87405ms step_avg:58.15ms
step:1504/2330 train_time:87472ms step_avg:58.16ms
step:1505/2330 train_time:87532ms step_avg:58.16ms
step:1506/2330 train_time:87592ms step_avg:58.16ms
step:1507/2330 train_time:87649ms step_avg:58.16ms
step:1508/2330 train_time:87709ms step_avg:58.16ms
step:1509/2330 train_time:87765ms step_avg:58.16ms
step:1510/2330 train_time:87825ms step_avg:58.16ms
step:1511/2330 train_time:87881ms step_avg:58.16ms
step:1512/2330 train_time:87941ms step_avg:58.16ms
step:1513/2330 train_time:87997ms step_avg:58.16ms
step:1514/2330 train_time:88056ms step_avg:58.16ms
step:1515/2330 train_time:88112ms step_avg:58.16ms
step:1516/2330 train_time:88172ms step_avg:58.16ms
step:1517/2330 train_time:88228ms step_avg:58.16ms
step:1518/2330 train_time:88289ms step_avg:58.16ms
step:1519/2330 train_time:88346ms step_avg:58.16ms
step:1520/2330 train_time:88410ms step_avg:58.16ms
step:1521/2330 train_time:88469ms step_avg:58.16ms
step:1522/2330 train_time:88530ms step_avg:58.17ms
step:1523/2330 train_time:88588ms step_avg:58.17ms
step:1524/2330 train_time:88648ms step_avg:58.17ms
step:1525/2330 train_time:88705ms step_avg:58.17ms
step:1526/2330 train_time:88764ms step_avg:58.17ms
step:1527/2330 train_time:88821ms step_avg:58.17ms
step:1528/2330 train_time:88880ms step_avg:58.17ms
step:1529/2330 train_time:88938ms step_avg:58.17ms
step:1530/2330 train_time:88996ms step_avg:58.17ms
step:1531/2330 train_time:89053ms step_avg:58.17ms
step:1532/2330 train_time:89113ms step_avg:58.17ms
step:1533/2330 train_time:89169ms step_avg:58.17ms
step:1534/2330 train_time:89230ms step_avg:58.17ms
step:1535/2330 train_time:89287ms step_avg:58.17ms
step:1536/2330 train_time:89348ms step_avg:58.17ms
step:1537/2330 train_time:89405ms step_avg:58.17ms
step:1538/2330 train_time:89469ms step_avg:58.17ms
step:1539/2330 train_time:89526ms step_avg:58.17ms
step:1540/2330 train_time:89588ms step_avg:58.17ms
step:1541/2330 train_time:89647ms step_avg:58.17ms
step:1542/2330 train_time:89707ms step_avg:58.18ms
step:1543/2330 train_time:89764ms step_avg:58.18ms
step:1544/2330 train_time:89824ms step_avg:58.18ms
step:1545/2330 train_time:89881ms step_avg:58.18ms
step:1546/2330 train_time:89942ms step_avg:58.18ms
step:1547/2330 train_time:89999ms step_avg:58.18ms
step:1548/2330 train_time:90060ms step_avg:58.18ms
step:1549/2330 train_time:90116ms step_avg:58.18ms
step:1550/2330 train_time:90177ms step_avg:58.18ms
step:1551/2330 train_time:90234ms step_avg:58.18ms
step:1552/2330 train_time:90294ms step_avg:58.18ms
step:1553/2330 train_time:90352ms step_avg:58.18ms
step:1554/2330 train_time:90414ms step_avg:58.18ms
step:1555/2330 train_time:90472ms step_avg:58.18ms
step:1556/2330 train_time:90535ms step_avg:58.18ms
step:1557/2330 train_time:90594ms step_avg:58.19ms
step:1558/2330 train_time:90655ms step_avg:58.19ms
step:1559/2330 train_time:90714ms step_avg:58.19ms
step:1560/2330 train_time:90775ms step_avg:58.19ms
step:1561/2330 train_time:90834ms step_avg:58.19ms
step:1562/2330 train_time:90894ms step_avg:58.19ms
step:1563/2330 train_time:90952ms step_avg:58.19ms
step:1564/2330 train_time:91012ms step_avg:58.19ms
step:1565/2330 train_time:91070ms step_avg:58.19ms
step:1566/2330 train_time:91129ms step_avg:58.19ms
step:1567/2330 train_time:91187ms step_avg:58.19ms
step:1568/2330 train_time:91247ms step_avg:58.19ms
step:1569/2330 train_time:91304ms step_avg:58.19ms
step:1570/2330 train_time:91366ms step_avg:58.19ms
step:1571/2330 train_time:91423ms step_avg:58.19ms
step:1572/2330 train_time:91485ms step_avg:58.20ms
step:1573/2330 train_time:91542ms step_avg:58.20ms
step:1574/2330 train_time:91604ms step_avg:58.20ms
step:1575/2330 train_time:91661ms step_avg:58.20ms
step:1576/2330 train_time:91723ms step_avg:58.20ms
step:1577/2330 train_time:91781ms step_avg:58.20ms
step:1578/2330 train_time:91841ms step_avg:58.20ms
step:1579/2330 train_time:91899ms step_avg:58.20ms
step:1580/2330 train_time:91958ms step_avg:58.20ms
step:1581/2330 train_time:92015ms step_avg:58.20ms
step:1582/2330 train_time:92076ms step_avg:58.20ms
step:1583/2330 train_time:92133ms step_avg:58.20ms
step:1584/2330 train_time:92194ms step_avg:58.20ms
step:1585/2330 train_time:92251ms step_avg:58.20ms
step:1586/2330 train_time:92313ms step_avg:58.20ms
step:1587/2330 train_time:92370ms step_avg:58.20ms
step:1588/2330 train_time:92432ms step_avg:58.21ms
step:1589/2330 train_time:92489ms step_avg:58.21ms
step:1590/2330 train_time:92552ms step_avg:58.21ms
step:1591/2330 train_time:92610ms step_avg:58.21ms
step:1592/2330 train_time:92670ms step_avg:58.21ms
step:1593/2330 train_time:92728ms step_avg:58.21ms
step:1594/2330 train_time:92789ms step_avg:58.21ms
step:1595/2330 train_time:92848ms step_avg:58.21ms
step:1596/2330 train_time:92908ms step_avg:58.21ms
step:1597/2330 train_time:92966ms step_avg:58.21ms
step:1598/2330 train_time:93025ms step_avg:58.21ms
step:1599/2330 train_time:93082ms step_avg:58.21ms
step:1600/2330 train_time:93144ms step_avg:58.21ms
step:1601/2330 train_time:93201ms step_avg:58.21ms
step:1602/2330 train_time:93262ms step_avg:58.22ms
step:1603/2330 train_time:93320ms step_avg:58.22ms
step:1604/2330 train_time:93380ms step_avg:58.22ms
step:1605/2330 train_time:93436ms step_avg:58.22ms
step:1606/2330 train_time:93498ms step_avg:58.22ms
step:1607/2330 train_time:93555ms step_avg:58.22ms
step:1608/2330 train_time:93616ms step_avg:58.22ms
step:1609/2330 train_time:93673ms step_avg:58.22ms
step:1610/2330 train_time:93735ms step_avg:58.22ms
step:1611/2330 train_time:93793ms step_avg:58.22ms
step:1612/2330 train_time:93855ms step_avg:58.22ms
step:1613/2330 train_time:93912ms step_avg:58.22ms
step:1614/2330 train_time:93974ms step_avg:58.22ms
step:1615/2330 train_time:94032ms step_avg:58.22ms
step:1616/2330 train_time:94092ms step_avg:58.23ms
step:1617/2330 train_time:94150ms step_avg:58.22ms
step:1618/2330 train_time:94210ms step_avg:58.23ms
step:1619/2330 train_time:94268ms step_avg:58.23ms
step:1620/2330 train_time:94327ms step_avg:58.23ms
step:1621/2330 train_time:94384ms step_avg:58.23ms
step:1622/2330 train_time:94445ms step_avg:58.23ms
step:1623/2330 train_time:94502ms step_avg:58.23ms
step:1624/2330 train_time:94563ms step_avg:58.23ms
step:1625/2330 train_time:94620ms step_avg:58.23ms
step:1626/2330 train_time:94682ms step_avg:58.23ms
step:1627/2330 train_time:94738ms step_avg:58.23ms
step:1628/2330 train_time:94802ms step_avg:58.23ms
step:1629/2330 train_time:94859ms step_avg:58.23ms
step:1630/2330 train_time:94919ms step_avg:58.23ms
step:1631/2330 train_time:94975ms step_avg:58.23ms
step:1632/2330 train_time:95036ms step_avg:58.23ms
step:1633/2330 train_time:95094ms step_avg:58.23ms
step:1634/2330 train_time:95154ms step_avg:58.23ms
step:1635/2330 train_time:95211ms step_avg:58.23ms
step:1636/2330 train_time:95272ms step_avg:58.23ms
step:1637/2330 train_time:95329ms step_avg:58.23ms
step:1638/2330 train_time:95391ms step_avg:58.24ms
step:1639/2330 train_time:95448ms step_avg:58.24ms
step:1640/2330 train_time:95510ms step_avg:58.24ms
step:1641/2330 train_time:95569ms step_avg:58.24ms
step:1642/2330 train_time:95629ms step_avg:58.24ms
step:1643/2330 train_time:95687ms step_avg:58.24ms
step:1644/2330 train_time:95748ms step_avg:58.24ms
step:1645/2330 train_time:95807ms step_avg:58.24ms
step:1646/2330 train_time:95866ms step_avg:58.24ms
step:1647/2330 train_time:95924ms step_avg:58.24ms
step:1648/2330 train_time:95985ms step_avg:58.24ms
step:1649/2330 train_time:96042ms step_avg:58.24ms
step:1650/2330 train_time:96104ms step_avg:58.24ms
step:1651/2330 train_time:96160ms step_avg:58.24ms
step:1652/2330 train_time:96221ms step_avg:58.25ms
step:1653/2330 train_time:96278ms step_avg:58.24ms
step:1654/2330 train_time:96339ms step_avg:58.25ms
step:1655/2330 train_time:96397ms step_avg:58.25ms
step:1656/2330 train_time:96457ms step_avg:58.25ms
step:1657/2330 train_time:96515ms step_avg:58.25ms
step:1658/2330 train_time:96576ms step_avg:58.25ms
step:1659/2330 train_time:96634ms step_avg:58.25ms
step:1660/2330 train_time:96694ms step_avg:58.25ms
step:1661/2330 train_time:96752ms step_avg:58.25ms
step:1662/2330 train_time:96814ms step_avg:58.25ms
step:1663/2330 train_time:96873ms step_avg:58.25ms
step:1664/2330 train_time:96933ms step_avg:58.25ms
step:1665/2330 train_time:96990ms step_avg:58.25ms
step:1666/2330 train_time:97051ms step_avg:58.25ms
step:1667/2330 train_time:97109ms step_avg:58.25ms
step:1668/2330 train_time:97169ms step_avg:58.25ms
step:1669/2330 train_time:97227ms step_avg:58.25ms
step:1670/2330 train_time:97287ms step_avg:58.26ms
step:1671/2330 train_time:97344ms step_avg:58.26ms
step:1672/2330 train_time:97405ms step_avg:58.26ms
step:1673/2330 train_time:97463ms step_avg:58.26ms
step:1674/2330 train_time:97523ms step_avg:58.26ms
step:1675/2330 train_time:97580ms step_avg:58.26ms
step:1676/2330 train_time:97642ms step_avg:58.26ms
step:1677/2330 train_time:97699ms step_avg:58.26ms
step:1678/2330 train_time:97762ms step_avg:58.26ms
step:1679/2330 train_time:97819ms step_avg:58.26ms
step:1680/2330 train_time:97881ms step_avg:58.26ms
step:1681/2330 train_time:97938ms step_avg:58.26ms
step:1682/2330 train_time:97999ms step_avg:58.26ms
step:1683/2330 train_time:98056ms step_avg:58.26ms
step:1684/2330 train_time:98117ms step_avg:58.26ms
step:1685/2330 train_time:98174ms step_avg:58.26ms
step:1686/2330 train_time:98234ms step_avg:58.26ms
step:1687/2330 train_time:98290ms step_avg:58.26ms
step:1688/2330 train_time:98352ms step_avg:58.27ms
step:1689/2330 train_time:98411ms step_avg:58.27ms
step:1690/2330 train_time:98471ms step_avg:58.27ms
step:1691/2330 train_time:98530ms step_avg:58.27ms
step:1692/2330 train_time:98590ms step_avg:58.27ms
step:1693/2330 train_time:98648ms step_avg:58.27ms
step:1694/2330 train_time:98709ms step_avg:58.27ms
step:1695/2330 train_time:98768ms step_avg:58.27ms
step:1696/2330 train_time:98829ms step_avg:58.27ms
step:1697/2330 train_time:98886ms step_avg:58.27ms
step:1698/2330 train_time:98946ms step_avg:58.27ms
step:1699/2330 train_time:99004ms step_avg:58.27ms
step:1700/2330 train_time:99064ms step_avg:58.27ms
step:1701/2330 train_time:99121ms step_avg:58.27ms
step:1702/2330 train_time:99183ms step_avg:58.27ms
step:1703/2330 train_time:99239ms step_avg:58.27ms
step:1704/2330 train_time:99301ms step_avg:58.28ms
step:1705/2330 train_time:99358ms step_avg:58.27ms
step:1706/2330 train_time:99419ms step_avg:58.28ms
step:1707/2330 train_time:99475ms step_avg:58.27ms
step:1708/2330 train_time:99538ms step_avg:58.28ms
step:1709/2330 train_time:99595ms step_avg:58.28ms
step:1710/2330 train_time:99657ms step_avg:58.28ms
step:1711/2330 train_time:99714ms step_avg:58.28ms
step:1712/2330 train_time:99776ms step_avg:58.28ms
step:1713/2330 train_time:99833ms step_avg:58.28ms
step:1714/2330 train_time:99895ms step_avg:58.28ms
step:1715/2330 train_time:99952ms step_avg:58.28ms
step:1716/2330 train_time:100014ms step_avg:58.28ms
step:1717/2330 train_time:100072ms step_avg:58.28ms
step:1718/2330 train_time:100132ms step_avg:58.28ms
step:1719/2330 train_time:100190ms step_avg:58.28ms
step:1720/2330 train_time:100250ms step_avg:58.28ms
step:1721/2330 train_time:100308ms step_avg:58.28ms
step:1722/2330 train_time:100368ms step_avg:58.29ms
step:1723/2330 train_time:100425ms step_avg:58.28ms
step:1724/2330 train_time:100486ms step_avg:58.29ms
step:1725/2330 train_time:100543ms step_avg:58.29ms
step:1726/2330 train_time:100606ms step_avg:58.29ms
step:1727/2330 train_time:100664ms step_avg:58.29ms
step:1728/2330 train_time:100725ms step_avg:58.29ms
step:1729/2330 train_time:100782ms step_avg:58.29ms
step:1730/2330 train_time:100843ms step_avg:58.29ms
step:1731/2330 train_time:100900ms step_avg:58.29ms
step:1732/2330 train_time:100962ms step_avg:58.29ms
step:1733/2330 train_time:101018ms step_avg:58.29ms
step:1734/2330 train_time:101080ms step_avg:58.29ms
step:1735/2330 train_time:101137ms step_avg:58.29ms
step:1736/2330 train_time:101198ms step_avg:58.29ms
step:1737/2330 train_time:101255ms step_avg:58.29ms
step:1738/2330 train_time:101316ms step_avg:58.29ms
step:1739/2330 train_time:101373ms step_avg:58.29ms
step:1740/2330 train_time:101435ms step_avg:58.30ms
step:1741/2330 train_time:101493ms step_avg:58.30ms
step:1742/2330 train_time:101555ms step_avg:58.30ms
step:1743/2330 train_time:101613ms step_avg:58.30ms
step:1744/2330 train_time:101675ms step_avg:58.30ms
step:1745/2330 train_time:101733ms step_avg:58.30ms
step:1746/2330 train_time:101794ms step_avg:58.30ms
step:1747/2330 train_time:101852ms step_avg:58.30ms
step:1748/2330 train_time:101912ms step_avg:58.30ms
step:1749/2330 train_time:101970ms step_avg:58.30ms
step:1750/2330 train_time:102031ms step_avg:58.30ms
step:1750/2330 val_loss:3.8195 train_time:102112ms step_avg:58.35ms
step:1751/2330 train_time:102131ms step_avg:58.33ms
step:1752/2330 train_time:102150ms step_avg:58.30ms
step:1753/2330 train_time:102205ms step_avg:58.30ms
step:1754/2330 train_time:102277ms step_avg:58.31ms
step:1755/2330 train_time:102333ms step_avg:58.31ms
step:1756/2330 train_time:102397ms step_avg:58.31ms
step:1757/2330 train_time:102454ms step_avg:58.31ms
step:1758/2330 train_time:102514ms step_avg:58.31ms
step:1759/2330 train_time:102570ms step_avg:58.31ms
step:1760/2330 train_time:102630ms step_avg:58.31ms
step:1761/2330 train_time:102687ms step_avg:58.31ms
step:1762/2330 train_time:102748ms step_avg:58.31ms
step:1763/2330 train_time:102804ms step_avg:58.31ms
step:1764/2330 train_time:102865ms step_avg:58.31ms
step:1765/2330 train_time:102921ms step_avg:58.31ms
step:1766/2330 train_time:102981ms step_avg:58.31ms
step:1767/2330 train_time:103040ms step_avg:58.31ms
step:1768/2330 train_time:103104ms step_avg:58.32ms
step:1769/2330 train_time:103162ms step_avg:58.32ms
step:1770/2330 train_time:103224ms step_avg:58.32ms
step:1771/2330 train_time:103282ms step_avg:58.32ms
step:1772/2330 train_time:103344ms step_avg:58.32ms
step:1773/2330 train_time:103400ms step_avg:58.32ms
step:1774/2330 train_time:103463ms step_avg:58.32ms
step:1775/2330 train_time:103519ms step_avg:58.32ms
step:1776/2330 train_time:103580ms step_avg:58.32ms
step:1777/2330 train_time:103636ms step_avg:58.32ms
step:1778/2330 train_time:103698ms step_avg:58.32ms
step:1779/2330 train_time:103754ms step_avg:58.32ms
step:1780/2330 train_time:103815ms step_avg:58.32ms
step:1781/2330 train_time:103871ms step_avg:58.32ms
step:1782/2330 train_time:103932ms step_avg:58.32ms
step:1783/2330 train_time:103991ms step_avg:58.32ms
step:1784/2330 train_time:104051ms step_avg:58.32ms
step:1785/2330 train_time:104110ms step_avg:58.32ms
step:1786/2330 train_time:104172ms step_avg:58.33ms
step:1787/2330 train_time:104231ms step_avg:58.33ms
step:1788/2330 train_time:104292ms step_avg:58.33ms
step:1789/2330 train_time:104350ms step_avg:58.33ms
step:1790/2330 train_time:104411ms step_avg:58.33ms
step:1791/2330 train_time:104469ms step_avg:58.33ms
step:1792/2330 train_time:104530ms step_avg:58.33ms
step:1793/2330 train_time:104587ms step_avg:58.33ms
step:1794/2330 train_time:104647ms step_avg:58.33ms
step:1795/2330 train_time:104703ms step_avg:58.33ms
step:1796/2330 train_time:104764ms step_avg:58.33ms
step:1797/2330 train_time:104821ms step_avg:58.33ms
step:1798/2330 train_time:104881ms step_avg:58.33ms
step:1799/2330 train_time:104937ms step_avg:58.33ms
step:1800/2330 train_time:104998ms step_avg:58.33ms
step:1801/2330 train_time:105056ms step_avg:58.33ms
step:1802/2330 train_time:105117ms step_avg:58.33ms
step:1803/2330 train_time:105177ms step_avg:58.33ms
step:1804/2330 train_time:105237ms step_avg:58.34ms
step:1805/2330 train_time:105295ms step_avg:58.33ms
step:1806/2330 train_time:105357ms step_avg:58.34ms
step:1807/2330 train_time:105414ms step_avg:58.34ms
step:1808/2330 train_time:105476ms step_avg:58.34ms
step:1809/2330 train_time:105533ms step_avg:58.34ms
step:1810/2330 train_time:105595ms step_avg:58.34ms
step:1811/2330 train_time:105653ms step_avg:58.34ms
step:1812/2330 train_time:105714ms step_avg:58.34ms
step:1813/2330 train_time:105771ms step_avg:58.34ms
step:1814/2330 train_time:105832ms step_avg:58.34ms
step:1815/2330 train_time:105889ms step_avg:58.34ms
step:1816/2330 train_time:105949ms step_avg:58.34ms
step:1817/2330 train_time:106006ms step_avg:58.34ms
step:1818/2330 train_time:106067ms step_avg:58.34ms
step:1819/2330 train_time:106124ms step_avg:58.34ms
step:1820/2330 train_time:106186ms step_avg:58.34ms
step:1821/2330 train_time:106244ms step_avg:58.34ms
step:1822/2330 train_time:106306ms step_avg:58.35ms
step:1823/2330 train_time:106363ms step_avg:58.35ms
step:1824/2330 train_time:106425ms step_avg:58.35ms
step:1825/2330 train_time:106482ms step_avg:58.35ms
step:1826/2330 train_time:106544ms step_avg:58.35ms
step:1827/2330 train_time:106600ms step_avg:58.35ms
step:1828/2330 train_time:106663ms step_avg:58.35ms
step:1829/2330 train_time:106719ms step_avg:58.35ms
step:1830/2330 train_time:106780ms step_avg:58.35ms
step:1831/2330 train_time:106837ms step_avg:58.35ms
step:1832/2330 train_time:106898ms step_avg:58.35ms
step:1833/2330 train_time:106955ms step_avg:58.35ms
step:1834/2330 train_time:107017ms step_avg:58.35ms
step:1835/2330 train_time:107075ms step_avg:58.35ms
step:1836/2330 train_time:107136ms step_avg:58.35ms
step:1837/2330 train_time:107195ms step_avg:58.35ms
step:1838/2330 train_time:107255ms step_avg:58.35ms
step:1839/2330 train_time:107314ms step_avg:58.35ms
step:1840/2330 train_time:107376ms step_avg:58.36ms
step:1841/2330 train_time:107433ms step_avg:58.36ms
step:1842/2330 train_time:107495ms step_avg:58.36ms
step:1843/2330 train_time:107553ms step_avg:58.36ms
step:1844/2330 train_time:107613ms step_avg:58.36ms
step:1845/2330 train_time:107671ms step_avg:58.36ms
step:1846/2330 train_time:107731ms step_avg:58.36ms
step:1847/2330 train_time:107788ms step_avg:58.36ms
step:1848/2330 train_time:107849ms step_avg:58.36ms
step:1849/2330 train_time:107905ms step_avg:58.36ms
step:1850/2330 train_time:107967ms step_avg:58.36ms
step:1851/2330 train_time:108024ms step_avg:58.36ms
step:1852/2330 train_time:108086ms step_avg:58.36ms
step:1853/2330 train_time:108142ms step_avg:58.36ms
step:1854/2330 train_time:108205ms step_avg:58.36ms
step:1855/2330 train_time:108262ms step_avg:58.36ms
step:1856/2330 train_time:108324ms step_avg:58.36ms
step:1857/2330 train_time:108381ms step_avg:58.36ms
step:1858/2330 train_time:108443ms step_avg:58.37ms
step:1859/2330 train_time:108500ms step_avg:58.36ms
step:1860/2330 train_time:108563ms step_avg:58.37ms
step:1861/2330 train_time:108620ms step_avg:58.37ms
step:1862/2330 train_time:108681ms step_avg:58.37ms
step:1863/2330 train_time:108737ms step_avg:58.37ms
step:1864/2330 train_time:108797ms step_avg:58.37ms
step:1865/2330 train_time:108854ms step_avg:58.37ms
step:1866/2330 train_time:108916ms step_avg:58.37ms
step:1867/2330 train_time:108974ms step_avg:58.37ms
step:1868/2330 train_time:109035ms step_avg:58.37ms
step:1869/2330 train_time:109093ms step_avg:58.37ms
step:1870/2330 train_time:109154ms step_avg:58.37ms
step:1871/2330 train_time:109211ms step_avg:58.37ms
step:1872/2330 train_time:109273ms step_avg:58.37ms
step:1873/2330 train_time:109331ms step_avg:58.37ms
step:1874/2330 train_time:109392ms step_avg:58.37ms
step:1875/2330 train_time:109450ms step_avg:58.37ms
step:1876/2330 train_time:109510ms step_avg:58.37ms
step:1877/2330 train_time:109566ms step_avg:58.37ms
step:1878/2330 train_time:109628ms step_avg:58.37ms
step:1879/2330 train_time:109685ms step_avg:58.37ms
step:1880/2330 train_time:109746ms step_avg:58.38ms
step:1881/2330 train_time:109802ms step_avg:58.37ms
step:1882/2330 train_time:109865ms step_avg:58.38ms
step:1883/2330 train_time:109921ms step_avg:58.38ms
step:1884/2330 train_time:109983ms step_avg:58.38ms
step:1885/2330 train_time:110039ms step_avg:58.38ms
step:1886/2330 train_time:110100ms step_avg:58.38ms
step:1887/2330 train_time:110157ms step_avg:58.38ms
step:1888/2330 train_time:110219ms step_avg:58.38ms
step:1889/2330 train_time:110275ms step_avg:58.38ms
step:1890/2330 train_time:110339ms step_avg:58.38ms
step:1891/2330 train_time:110397ms step_avg:58.38ms
step:1892/2330 train_time:110458ms step_avg:58.38ms
step:1893/2330 train_time:110515ms step_avg:58.38ms
step:1894/2330 train_time:110579ms step_avg:58.38ms
step:1895/2330 train_time:110637ms step_avg:58.38ms
step:1896/2330 train_time:110696ms step_avg:58.38ms
step:1897/2330 train_time:110754ms step_avg:58.38ms
step:1898/2330 train_time:110815ms step_avg:58.39ms
step:1899/2330 train_time:110872ms step_avg:58.38ms
step:1900/2330 train_time:110933ms step_avg:58.39ms
step:1901/2330 train_time:110991ms step_avg:58.39ms
step:1902/2330 train_time:111052ms step_avg:58.39ms
step:1903/2330 train_time:111109ms step_avg:58.39ms
step:1904/2330 train_time:111171ms step_avg:58.39ms
step:1905/2330 train_time:111228ms step_avg:58.39ms
step:1906/2330 train_time:111290ms step_avg:58.39ms
step:1907/2330 train_time:111347ms step_avg:58.39ms
step:1908/2330 train_time:111409ms step_avg:58.39ms
step:1909/2330 train_time:111466ms step_avg:58.39ms
step:1910/2330 train_time:111528ms step_avg:58.39ms
step:1911/2330 train_time:111586ms step_avg:58.39ms
step:1912/2330 train_time:111647ms step_avg:58.39ms
step:1913/2330 train_time:111704ms step_avg:58.39ms
step:1914/2330 train_time:111765ms step_avg:58.39ms
step:1915/2330 train_time:111822ms step_avg:58.39ms
step:1916/2330 train_time:111883ms step_avg:58.39ms
step:1917/2330 train_time:111940ms step_avg:58.39ms
step:1918/2330 train_time:112000ms step_avg:58.39ms
step:1919/2330 train_time:112057ms step_avg:58.39ms
step:1920/2330 train_time:112118ms step_avg:58.39ms
step:1921/2330 train_time:112176ms step_avg:58.39ms
step:1922/2330 train_time:112238ms step_avg:58.40ms
step:1923/2330 train_time:112295ms step_avg:58.40ms
step:1924/2330 train_time:112358ms step_avg:58.40ms
step:1925/2330 train_time:112415ms step_avg:58.40ms
step:1926/2330 train_time:112478ms step_avg:58.40ms
step:1927/2330 train_time:112536ms step_avg:58.40ms
step:1928/2330 train_time:112596ms step_avg:58.40ms
step:1929/2330 train_time:112653ms step_avg:58.40ms
step:1930/2330 train_time:112714ms step_avg:58.40ms
step:1931/2330 train_time:112772ms step_avg:58.40ms
step:1932/2330 train_time:112832ms step_avg:58.40ms
step:1933/2330 train_time:112890ms step_avg:58.40ms
step:1934/2330 train_time:112950ms step_avg:58.40ms
step:1935/2330 train_time:113007ms step_avg:58.40ms
step:1936/2330 train_time:113067ms step_avg:58.40ms
step:1937/2330 train_time:113125ms step_avg:58.40ms
step:1938/2330 train_time:113186ms step_avg:58.40ms
step:1939/2330 train_time:113243ms step_avg:58.40ms
step:1940/2330 train_time:113306ms step_avg:58.40ms
step:1941/2330 train_time:113363ms step_avg:58.40ms
step:1942/2330 train_time:113426ms step_avg:58.41ms
step:1943/2330 train_time:113482ms step_avg:58.41ms
step:1944/2330 train_time:113545ms step_avg:58.41ms
step:1945/2330 train_time:113601ms step_avg:58.41ms
step:1946/2330 train_time:113663ms step_avg:58.41ms
step:1947/2330 train_time:113719ms step_avg:58.41ms
step:1948/2330 train_time:113780ms step_avg:58.41ms
step:1949/2330 train_time:113836ms step_avg:58.41ms
step:1950/2330 train_time:113898ms step_avg:58.41ms
step:1951/2330 train_time:113954ms step_avg:58.41ms
step:1952/2330 train_time:114017ms step_avg:58.41ms
step:1953/2330 train_time:114073ms step_avg:58.41ms
step:1954/2330 train_time:114134ms step_avg:58.41ms
step:1955/2330 train_time:114192ms step_avg:58.41ms
step:1956/2330 train_time:114253ms step_avg:58.41ms
step:1957/2330 train_time:114310ms step_avg:58.41ms
step:1958/2330 train_time:114372ms step_avg:58.41ms
step:1959/2330 train_time:114430ms step_avg:58.41ms
step:1960/2330 train_time:114491ms step_avg:58.41ms
step:1961/2330 train_time:114549ms step_avg:58.41ms
step:1962/2330 train_time:114609ms step_avg:58.41ms
step:1963/2330 train_time:114667ms step_avg:58.41ms
step:1964/2330 train_time:114729ms step_avg:58.42ms
step:1965/2330 train_time:114786ms step_avg:58.42ms
step:1966/2330 train_time:114846ms step_avg:58.42ms
step:1967/2330 train_time:114903ms step_avg:58.42ms
step:1968/2330 train_time:114966ms step_avg:58.42ms
step:1969/2330 train_time:115022ms step_avg:58.42ms
step:1970/2330 train_time:115084ms step_avg:58.42ms
step:1971/2330 train_time:115141ms step_avg:58.42ms
step:1972/2330 train_time:115203ms step_avg:58.42ms
step:1973/2330 train_time:115259ms step_avg:58.42ms
step:1974/2330 train_time:115321ms step_avg:58.42ms
step:1975/2330 train_time:115378ms step_avg:58.42ms
step:1976/2330 train_time:115439ms step_avg:58.42ms
step:1977/2330 train_time:115496ms step_avg:58.42ms
step:1978/2330 train_time:115558ms step_avg:58.42ms
step:1979/2330 train_time:115615ms step_avg:58.42ms
step:1980/2330 train_time:115677ms step_avg:58.42ms
step:1981/2330 train_time:115735ms step_avg:58.42ms
step:1982/2330 train_time:115795ms step_avg:58.42ms
step:1983/2330 train_time:115854ms step_avg:58.42ms
step:1984/2330 train_time:115914ms step_avg:58.42ms
step:1985/2330 train_time:115972ms step_avg:58.42ms
step:1986/2330 train_time:116032ms step_avg:58.43ms
step:1987/2330 train_time:116090ms step_avg:58.42ms
step:1988/2330 train_time:116150ms step_avg:58.43ms
step:1989/2330 train_time:116206ms step_avg:58.42ms
step:1990/2330 train_time:116268ms step_avg:58.43ms
step:1991/2330 train_time:116325ms step_avg:58.43ms
step:1992/2330 train_time:116388ms step_avg:58.43ms
step:1993/2330 train_time:116444ms step_avg:58.43ms
step:1994/2330 train_time:116507ms step_avg:58.43ms
step:1995/2330 train_time:116563ms step_avg:58.43ms
step:1996/2330 train_time:116626ms step_avg:58.43ms
step:1997/2330 train_time:116682ms step_avg:58.43ms
step:1998/2330 train_time:116745ms step_avg:58.43ms
step:1999/2330 train_time:116801ms step_avg:58.43ms
step:2000/2330 train_time:116864ms step_avg:58.43ms
step:2000/2330 val_loss:3.7562 train_time:116945ms step_avg:58.47ms
step:2001/2330 train_time:116963ms step_avg:58.45ms
step:2002/2330 train_time:116984ms step_avg:58.43ms
step:2003/2330 train_time:117042ms step_avg:58.43ms
step:2004/2330 train_time:117111ms step_avg:58.44ms
step:2005/2330 train_time:117168ms step_avg:58.44ms
step:2006/2330 train_time:117229ms step_avg:58.44ms
step:2007/2330 train_time:117286ms step_avg:58.44ms
step:2008/2330 train_time:117345ms step_avg:58.44ms
step:2009/2330 train_time:117402ms step_avg:58.44ms
step:2010/2330 train_time:117463ms step_avg:58.44ms
step:2011/2330 train_time:117519ms step_avg:58.44ms
step:2012/2330 train_time:117579ms step_avg:58.44ms
step:2013/2330 train_time:117635ms step_avg:58.44ms
step:2014/2330 train_time:117695ms step_avg:58.44ms
step:2015/2330 train_time:117751ms step_avg:58.44ms
step:2016/2330 train_time:117811ms step_avg:58.44ms
step:2017/2330 train_time:117869ms step_avg:58.44ms
step:2018/2330 train_time:117932ms step_avg:58.44ms
step:2019/2330 train_time:117990ms step_avg:58.44ms
step:2020/2330 train_time:118054ms step_avg:58.44ms
step:2021/2330 train_time:118111ms step_avg:58.44ms
step:2022/2330 train_time:118174ms step_avg:58.44ms
step:2023/2330 train_time:118231ms step_avg:58.44ms
step:2024/2330 train_time:118292ms step_avg:58.44ms
step:2025/2330 train_time:118349ms step_avg:58.44ms
step:2026/2330 train_time:118409ms step_avg:58.44ms
step:2027/2330 train_time:118466ms step_avg:58.44ms
step:2028/2330 train_time:118526ms step_avg:58.44ms
step:2029/2330 train_time:118582ms step_avg:58.44ms
step:2030/2330 train_time:118643ms step_avg:58.44ms
step:2031/2330 train_time:118700ms step_avg:58.44ms
step:2032/2330 train_time:118760ms step_avg:58.44ms
step:2033/2330 train_time:118818ms step_avg:58.44ms
step:2034/2330 train_time:118878ms step_avg:58.45ms
step:2035/2330 train_time:118936ms step_avg:58.45ms
step:2036/2330 train_time:118997ms step_avg:58.45ms
step:2037/2330 train_time:119056ms step_avg:58.45ms
step:2038/2330 train_time:119117ms step_avg:58.45ms
step:2039/2330 train_time:119175ms step_avg:58.45ms
step:2040/2330 train_time:119237ms step_avg:58.45ms
step:2041/2330 train_time:119293ms step_avg:58.45ms
step:2042/2330 train_time:119356ms step_avg:58.45ms
step:2043/2330 train_time:119412ms step_avg:58.45ms
step:2044/2330 train_time:119474ms step_avg:58.45ms
step:2045/2330 train_time:119530ms step_avg:58.45ms
step:2046/2330 train_time:119591ms step_avg:58.45ms
step:2047/2330 train_time:119648ms step_avg:58.45ms
step:2048/2330 train_time:119708ms step_avg:58.45ms
step:2049/2330 train_time:119765ms step_avg:58.45ms
step:2050/2330 train_time:119825ms step_avg:58.45ms
step:2051/2330 train_time:119883ms step_avg:58.45ms
step:2052/2330 train_time:119945ms step_avg:58.45ms
step:2053/2330 train_time:120003ms step_avg:58.45ms
step:2054/2330 train_time:120065ms step_avg:58.45ms
step:2055/2330 train_time:120124ms step_avg:58.45ms
step:2056/2330 train_time:120185ms step_avg:58.46ms
step:2057/2330 train_time:120243ms step_avg:58.46ms
step:2058/2330 train_time:120304ms step_avg:58.46ms
step:2059/2330 train_time:120362ms step_avg:58.46ms
step:2060/2330 train_time:120423ms step_avg:58.46ms
step:2061/2330 train_time:120481ms step_avg:58.46ms
step:2062/2330 train_time:120540ms step_avg:58.46ms
step:2063/2330 train_time:120598ms step_avg:58.46ms
step:2064/2330 train_time:120659ms step_avg:58.46ms
step:2065/2330 train_time:120716ms step_avg:58.46ms
step:2066/2330 train_time:120776ms step_avg:58.46ms
step:2067/2330 train_time:120833ms step_avg:58.46ms
step:2068/2330 train_time:120894ms step_avg:58.46ms
step:2069/2330 train_time:120951ms step_avg:58.46ms
step:2070/2330 train_time:121012ms step_avg:58.46ms
step:2071/2330 train_time:121069ms step_avg:58.46ms
step:2072/2330 train_time:121132ms step_avg:58.46ms
step:2073/2330 train_time:121189ms step_avg:58.46ms
step:2074/2330 train_time:121250ms step_avg:58.46ms
step:2075/2330 train_time:121307ms step_avg:58.46ms
step:2076/2330 train_time:121368ms step_avg:58.46ms
step:2077/2330 train_time:121426ms step_avg:58.46ms
step:2078/2330 train_time:121487ms step_avg:58.46ms
step:2079/2330 train_time:121544ms step_avg:58.46ms
step:2080/2330 train_time:121605ms step_avg:58.46ms
step:2081/2330 train_time:121663ms step_avg:58.46ms
step:2082/2330 train_time:121723ms step_avg:58.46ms
step:2083/2330 train_time:121781ms step_avg:58.46ms
step:2084/2330 train_time:121842ms step_avg:58.47ms
step:2085/2330 train_time:121899ms step_avg:58.46ms
step:2086/2330 train_time:121959ms step_avg:58.47ms
step:2087/2330 train_time:122016ms step_avg:58.47ms
step:2088/2330 train_time:122077ms step_avg:58.47ms
step:2089/2330 train_time:122134ms step_avg:58.47ms
step:2090/2330 train_time:122197ms step_avg:58.47ms
step:2091/2330 train_time:122253ms step_avg:58.47ms
step:2092/2330 train_time:122316ms step_avg:58.47ms
step:2093/2330 train_time:122372ms step_avg:58.47ms
step:2094/2330 train_time:122434ms step_avg:58.47ms
step:2095/2330 train_time:122490ms step_avg:58.47ms
step:2096/2330 train_time:122552ms step_avg:58.47ms
step:2097/2330 train_time:122608ms step_avg:58.47ms
step:2098/2330 train_time:122670ms step_avg:58.47ms
step:2099/2330 train_time:122726ms step_avg:58.47ms
step:2100/2330 train_time:122787ms step_avg:58.47ms
step:2101/2330 train_time:122844ms step_avg:58.47ms
step:2102/2330 train_time:122905ms step_avg:58.47ms
step:2103/2330 train_time:122964ms step_avg:58.47ms
step:2104/2330 train_time:123025ms step_avg:58.47ms
step:2105/2330 train_time:123084ms step_avg:58.47ms
step:2106/2330 train_time:123145ms step_avg:58.47ms
step:2107/2330 train_time:123202ms step_avg:58.47ms
step:2108/2330 train_time:123262ms step_avg:58.47ms
step:2109/2330 train_time:123321ms step_avg:58.47ms
step:2110/2330 train_time:123381ms step_avg:58.47ms
step:2111/2330 train_time:123439ms step_avg:58.47ms
step:2112/2330 train_time:123499ms step_avg:58.48ms
step:2113/2330 train_time:123557ms step_avg:58.47ms
step:2114/2330 train_time:123617ms step_avg:58.48ms
step:2115/2330 train_time:123674ms step_avg:58.47ms
step:2116/2330 train_time:123735ms step_avg:58.48ms
step:2117/2330 train_time:123791ms step_avg:58.47ms
step:2118/2330 train_time:123853ms step_avg:58.48ms
step:2119/2330 train_time:123910ms step_avg:58.48ms
step:2120/2330 train_time:123972ms step_avg:58.48ms
step:2121/2330 train_time:124029ms step_avg:58.48ms
step:2122/2330 train_time:124090ms step_avg:58.48ms
step:2123/2330 train_time:124147ms step_avg:58.48ms
step:2124/2330 train_time:124208ms step_avg:58.48ms
step:2125/2330 train_time:124266ms step_avg:58.48ms
step:2126/2330 train_time:124327ms step_avg:58.48ms
step:2127/2330 train_time:124384ms step_avg:58.48ms
step:2128/2330 train_time:124445ms step_avg:58.48ms
step:2129/2330 train_time:124503ms step_avg:58.48ms
step:2130/2330 train_time:124564ms step_avg:58.48ms
step:2131/2330 train_time:124621ms step_avg:58.48ms
step:2132/2330 train_time:124683ms step_avg:58.48ms
step:2133/2330 train_time:124741ms step_avg:58.48ms
step:2134/2330 train_time:124801ms step_avg:58.48ms
step:2135/2330 train_time:124858ms step_avg:58.48ms
step:2136/2330 train_time:124919ms step_avg:58.48ms
step:2137/2330 train_time:124976ms step_avg:58.48ms
step:2138/2330 train_time:125038ms step_avg:58.48ms
step:2139/2330 train_time:125095ms step_avg:58.48ms
step:2140/2330 train_time:125156ms step_avg:58.48ms
step:2141/2330 train_time:125212ms step_avg:58.48ms
step:2142/2330 train_time:125274ms step_avg:58.48ms
step:2143/2330 train_time:125331ms step_avg:58.48ms
step:2144/2330 train_time:125392ms step_avg:58.49ms
step:2145/2330 train_time:125449ms step_avg:58.48ms
step:2146/2330 train_time:125509ms step_avg:58.49ms
step:2147/2330 train_time:125566ms step_avg:58.48ms
step:2148/2330 train_time:125628ms step_avg:58.49ms
step:2149/2330 train_time:125686ms step_avg:58.49ms
step:2150/2330 train_time:125747ms step_avg:58.49ms
step:2151/2330 train_time:125804ms step_avg:58.49ms
step:2152/2330 train_time:125866ms step_avg:58.49ms
step:2153/2330 train_time:125923ms step_avg:58.49ms
step:2154/2330 train_time:125984ms step_avg:58.49ms
step:2155/2330 train_time:126043ms step_avg:58.49ms
step:2156/2330 train_time:126103ms step_avg:58.49ms
step:2157/2330 train_time:126161ms step_avg:58.49ms
step:2158/2330 train_time:126221ms step_avg:58.49ms
step:2159/2330 train_time:126278ms step_avg:58.49ms
step:2160/2330 train_time:126339ms step_avg:58.49ms
step:2161/2330 train_time:126396ms step_avg:58.49ms
step:2162/2330 train_time:126458ms step_avg:58.49ms
step:2163/2330 train_time:126515ms step_avg:58.49ms
step:2164/2330 train_time:126577ms step_avg:58.49ms
step:2165/2330 train_time:126634ms step_avg:58.49ms
step:2166/2330 train_time:126696ms step_avg:58.49ms
step:2167/2330 train_time:126753ms step_avg:58.49ms
step:2168/2330 train_time:126814ms step_avg:58.49ms
step:2169/2330 train_time:126871ms step_avg:58.49ms
step:2170/2330 train_time:126932ms step_avg:58.49ms
step:2171/2330 train_time:126989ms step_avg:58.49ms
step:2172/2330 train_time:127050ms step_avg:58.49ms
step:2173/2330 train_time:127107ms step_avg:58.49ms
step:2174/2330 train_time:127168ms step_avg:58.49ms
step:2175/2330 train_time:127224ms step_avg:58.49ms
step:2176/2330 train_time:127287ms step_avg:58.50ms
step:2177/2330 train_time:127345ms step_avg:58.50ms
step:2178/2330 train_time:127406ms step_avg:58.50ms
step:2179/2330 train_time:127463ms step_avg:58.50ms
step:2180/2330 train_time:127526ms step_avg:58.50ms
step:2181/2330 train_time:127583ms step_avg:58.50ms
step:2182/2330 train_time:127645ms step_avg:58.50ms
step:2183/2330 train_time:127703ms step_avg:58.50ms
step:2184/2330 train_time:127764ms step_avg:58.50ms
step:2185/2330 train_time:127821ms step_avg:58.50ms
step:2186/2330 train_time:127883ms step_avg:58.50ms
step:2187/2330 train_time:127940ms step_avg:58.50ms
step:2188/2330 train_time:128002ms step_avg:58.50ms
step:2189/2330 train_time:128060ms step_avg:58.50ms
step:2190/2330 train_time:128120ms step_avg:58.50ms
step:2191/2330 train_time:128177ms step_avg:58.50ms
step:2192/2330 train_time:128238ms step_avg:58.50ms
step:2193/2330 train_time:128295ms step_avg:58.50ms
step:2194/2330 train_time:128356ms step_avg:58.50ms
step:2195/2330 train_time:128412ms step_avg:58.50ms
step:2196/2330 train_time:128474ms step_avg:58.50ms
step:2197/2330 train_time:128531ms step_avg:58.50ms
step:2198/2330 train_time:128593ms step_avg:58.50ms
step:2199/2330 train_time:128650ms step_avg:58.50ms
step:2200/2330 train_time:128711ms step_avg:58.50ms
step:2201/2330 train_time:128767ms step_avg:58.50ms
step:2202/2330 train_time:128828ms step_avg:58.51ms
step:2203/2330 train_time:128885ms step_avg:58.50ms
step:2204/2330 train_time:128947ms step_avg:58.51ms
step:2205/2330 train_time:129004ms step_avg:58.51ms
step:2206/2330 train_time:129064ms step_avg:58.51ms
step:2207/2330 train_time:129121ms step_avg:58.51ms
step:2208/2330 train_time:129184ms step_avg:58.51ms
step:2209/2330 train_time:129242ms step_avg:58.51ms
step:2210/2330 train_time:129303ms step_avg:58.51ms
step:2211/2330 train_time:129361ms step_avg:58.51ms
step:2212/2330 train_time:129422ms step_avg:58.51ms
step:2213/2330 train_time:129480ms step_avg:58.51ms
step:2214/2330 train_time:129541ms step_avg:58.51ms
step:2215/2330 train_time:129599ms step_avg:58.51ms
step:2216/2330 train_time:129661ms step_avg:58.51ms
step:2217/2330 train_time:129718ms step_avg:58.51ms
step:2218/2330 train_time:129779ms step_avg:58.51ms
step:2219/2330 train_time:129836ms step_avg:58.51ms
step:2220/2330 train_time:129897ms step_avg:58.51ms
step:2221/2330 train_time:129954ms step_avg:58.51ms
step:2222/2330 train_time:130015ms step_avg:58.51ms
step:2223/2330 train_time:130071ms step_avg:58.51ms
step:2224/2330 train_time:130133ms step_avg:58.51ms
step:2225/2330 train_time:130189ms step_avg:58.51ms
step:2226/2330 train_time:130251ms step_avg:58.51ms
step:2227/2330 train_time:130307ms step_avg:58.51ms
step:2228/2330 train_time:130368ms step_avg:58.51ms
step:2229/2330 train_time:130426ms step_avg:58.51ms
step:2230/2330 train_time:130488ms step_avg:58.51ms
step:2231/2330 train_time:130545ms step_avg:58.51ms
step:2232/2330 train_time:130606ms step_avg:58.52ms
step:2233/2330 train_time:130664ms step_avg:58.51ms
step:2234/2330 train_time:130725ms step_avg:58.52ms
step:2235/2330 train_time:130783ms step_avg:58.52ms
step:2236/2330 train_time:130844ms step_avg:58.52ms
step:2237/2330 train_time:130901ms step_avg:58.52ms
step:2238/2330 train_time:130962ms step_avg:58.52ms
step:2239/2330 train_time:131020ms step_avg:58.52ms
step:2240/2330 train_time:131080ms step_avg:58.52ms
step:2241/2330 train_time:131138ms step_avg:58.52ms
step:2242/2330 train_time:131198ms step_avg:58.52ms
step:2243/2330 train_time:131255ms step_avg:58.52ms
step:2244/2330 train_time:131316ms step_avg:58.52ms
step:2245/2330 train_time:131373ms step_avg:58.52ms
step:2246/2330 train_time:131435ms step_avg:58.52ms
step:2247/2330 train_time:131491ms step_avg:58.52ms
step:2248/2330 train_time:131554ms step_avg:58.52ms
step:2249/2330 train_time:131610ms step_avg:58.52ms
step:2250/2330 train_time:131672ms step_avg:58.52ms
step:2250/2330 val_loss:3.7070 train_time:131753ms step_avg:58.56ms
step:2251/2330 train_time:131771ms step_avg:58.54ms
step:2252/2330 train_time:131792ms step_avg:58.52ms
step:2253/2330 train_time:131852ms step_avg:58.52ms
step:2254/2330 train_time:131917ms step_avg:58.53ms
step:2255/2330 train_time:131973ms step_avg:58.52ms
step:2256/2330 train_time:132035ms step_avg:58.53ms
step:2257/2330 train_time:132092ms step_avg:58.53ms
step:2258/2330 train_time:132152ms step_avg:58.53ms
step:2259/2330 train_time:132209ms step_avg:58.53ms
step:2260/2330 train_time:132269ms step_avg:58.53ms
step:2261/2330 train_time:132326ms step_avg:58.53ms
step:2262/2330 train_time:132385ms step_avg:58.53ms
step:2263/2330 train_time:132441ms step_avg:58.52ms
step:2264/2330 train_time:132502ms step_avg:58.53ms
step:2265/2330 train_time:132559ms step_avg:58.52ms
step:2266/2330 train_time:132620ms step_avg:58.53ms
step:2267/2330 train_time:132677ms step_avg:58.53ms
step:2268/2330 train_time:132740ms step_avg:58.53ms
step:2269/2330 train_time:132798ms step_avg:58.53ms
step:2270/2330 train_time:132862ms step_avg:58.53ms
step:2271/2330 train_time:132921ms step_avg:58.53ms
step:2272/2330 train_time:132984ms step_avg:58.53ms
step:2273/2330 train_time:133041ms step_avg:58.53ms
step:2274/2330 train_time:133102ms step_avg:58.53ms
step:2275/2330 train_time:133159ms step_avg:58.53ms
step:2276/2330 train_time:133220ms step_avg:58.53ms
step:2277/2330 train_time:133277ms step_avg:58.53ms
step:2278/2330 train_time:133337ms step_avg:58.53ms
step:2279/2330 train_time:133393ms step_avg:58.53ms
step:2280/2330 train_time:133454ms step_avg:58.53ms
step:2281/2330 train_time:133510ms step_avg:58.53ms
step:2282/2330 train_time:133571ms step_avg:58.53ms
step:2283/2330 train_time:133627ms step_avg:58.53ms
step:2284/2330 train_time:133689ms step_avg:58.53ms
step:2285/2330 train_time:133746ms step_avg:58.53ms
step:2286/2330 train_time:133809ms step_avg:58.53ms
step:2287/2330 train_time:133866ms step_avg:58.53ms
step:2288/2330 train_time:133930ms step_avg:58.54ms
step:2289/2330 train_time:133987ms step_avg:58.53ms
step:2290/2330 train_time:134049ms step_avg:58.54ms
step:2291/2330 train_time:134107ms step_avg:58.54ms
step:2292/2330 train_time:134168ms step_avg:58.54ms
step:2293/2330 train_time:134225ms step_avg:58.54ms
step:2294/2330 train_time:134286ms step_avg:58.54ms
step:2295/2330 train_time:134345ms step_avg:58.54ms
step:2296/2330 train_time:134405ms step_avg:58.54ms
step:2297/2330 train_time:134463ms step_avg:58.54ms
step:2298/2330 train_time:134523ms step_avg:58.54ms
step:2299/2330 train_time:134580ms step_avg:58.54ms
step:2300/2330 train_time:134640ms step_avg:58.54ms
step:2301/2330 train_time:134699ms step_avg:58.54ms
step:2302/2330 train_time:134759ms step_avg:58.54ms
step:2303/2330 train_time:134817ms step_avg:58.54ms
step:2304/2330 train_time:134879ms step_avg:58.54ms
step:2305/2330 train_time:134936ms step_avg:58.54ms
step:2306/2330 train_time:134998ms step_avg:58.54ms
step:2307/2330 train_time:135055ms step_avg:58.54ms
step:2308/2330 train_time:135118ms step_avg:58.54ms
step:2309/2330 train_time:135175ms step_avg:58.54ms
step:2310/2330 train_time:135236ms step_avg:58.54ms
step:2311/2330 train_time:135294ms step_avg:58.54ms
step:2312/2330 train_time:135355ms step_avg:58.54ms
step:2313/2330 train_time:135412ms step_avg:58.54ms
step:2314/2330 train_time:135472ms step_avg:58.54ms
step:2315/2330 train_time:135528ms step_avg:58.54ms
step:2316/2330 train_time:135589ms step_avg:58.54ms
step:2317/2330 train_time:135646ms step_avg:58.54ms
step:2318/2330 train_time:135708ms step_avg:58.55ms
step:2319/2330 train_time:135765ms step_avg:58.54ms
step:2320/2330 train_time:135827ms step_avg:58.55ms
step:2321/2330 train_time:135886ms step_avg:58.55ms
step:2322/2330 train_time:135947ms step_avg:58.55ms
step:2323/2330 train_time:136006ms step_avg:58.55ms
step:2324/2330 train_time:136066ms step_avg:58.55ms
step:2325/2330 train_time:136123ms step_avg:58.55ms
step:2326/2330 train_time:136184ms step_avg:58.55ms
step:2327/2330 train_time:136242ms step_avg:58.55ms
step:2328/2330 train_time:136302ms step_avg:58.55ms
step:2329/2330 train_time:136360ms step_avg:58.55ms
step:2330/2330 train_time:136421ms step_avg:58.55ms
step:2330/2330 val_loss:3.6916 train_time:136504ms step_avg:58.59ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
