import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:23:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:95ms step_avg:94.81ms
step:2/2330 train_time:187ms step_avg:93.51ms
step:3/2330 train_time:205ms step_avg:68.47ms
step:4/2330 train_time:224ms step_avg:56.02ms
step:5/2330 train_time:278ms step_avg:55.62ms
step:6/2330 train_time:336ms step_avg:56.05ms
step:7/2330 train_time:392ms step_avg:55.96ms
step:8/2330 train_time:450ms step_avg:56.26ms
step:9/2330 train_time:506ms step_avg:56.18ms
step:10/2330 train_time:564ms step_avg:56.38ms
step:11/2330 train_time:619ms step_avg:56.28ms
step:12/2330 train_time:677ms step_avg:56.45ms
step:13/2330 train_time:733ms step_avg:56.38ms
step:14/2330 train_time:792ms step_avg:56.55ms
step:15/2330 train_time:847ms step_avg:56.47ms
step:16/2330 train_time:905ms step_avg:56.58ms
step:17/2330 train_time:960ms step_avg:56.50ms
step:18/2330 train_time:1020ms step_avg:56.65ms
step:19/2330 train_time:1077ms step_avg:56.67ms
step:20/2330 train_time:1139ms step_avg:56.95ms
step:21/2330 train_time:1195ms step_avg:56.93ms
step:22/2330 train_time:1257ms step_avg:57.14ms
step:23/2330 train_time:1313ms step_avg:57.08ms
step:24/2330 train_time:1373ms step_avg:57.21ms
step:25/2330 train_time:1431ms step_avg:57.22ms
step:26/2330 train_time:1489ms step_avg:57.27ms
step:27/2330 train_time:1545ms step_avg:57.21ms
step:28/2330 train_time:1603ms step_avg:57.24ms
step:29/2330 train_time:1658ms step_avg:57.17ms
step:30/2330 train_time:1716ms step_avg:57.20ms
step:31/2330 train_time:1771ms step_avg:57.13ms
step:32/2330 train_time:1830ms step_avg:57.18ms
step:33/2330 train_time:1885ms step_avg:57.13ms
step:34/2330 train_time:1944ms step_avg:57.18ms
step:35/2330 train_time:2000ms step_avg:57.14ms
step:36/2330 train_time:2059ms step_avg:57.19ms
step:37/2330 train_time:2115ms step_avg:57.17ms
step:38/2330 train_time:2177ms step_avg:57.28ms
step:39/2330 train_time:2233ms step_avg:57.25ms
step:40/2330 train_time:2293ms step_avg:57.32ms
step:41/2330 train_time:2349ms step_avg:57.29ms
step:42/2330 train_time:2409ms step_avg:57.35ms
step:43/2330 train_time:2465ms step_avg:57.32ms
step:44/2330 train_time:2523ms step_avg:57.35ms
step:45/2330 train_time:2579ms step_avg:57.32ms
step:46/2330 train_time:2637ms step_avg:57.34ms
step:47/2330 train_time:2693ms step_avg:57.30ms
step:48/2330 train_time:2751ms step_avg:57.32ms
step:49/2330 train_time:2807ms step_avg:57.29ms
step:50/2330 train_time:2866ms step_avg:57.32ms
step:51/2330 train_time:2922ms step_avg:57.29ms
step:52/2330 train_time:2980ms step_avg:57.31ms
step:53/2330 train_time:3037ms step_avg:57.30ms
step:54/2330 train_time:3096ms step_avg:57.34ms
step:55/2330 train_time:3152ms step_avg:57.31ms
step:56/2330 train_time:3213ms step_avg:57.37ms
step:57/2330 train_time:3269ms step_avg:57.35ms
step:58/2330 train_time:3329ms step_avg:57.39ms
step:59/2330 train_time:3385ms step_avg:57.37ms
step:60/2330 train_time:3444ms step_avg:57.40ms
step:61/2330 train_time:3500ms step_avg:57.38ms
step:62/2330 train_time:3558ms step_avg:57.39ms
step:63/2330 train_time:3614ms step_avg:57.37ms
step:64/2330 train_time:3673ms step_avg:57.39ms
step:65/2330 train_time:3729ms step_avg:57.37ms
step:66/2330 train_time:3788ms step_avg:57.39ms
step:67/2330 train_time:3843ms step_avg:57.36ms
step:68/2330 train_time:3902ms step_avg:57.38ms
step:69/2330 train_time:3957ms step_avg:57.35ms
step:70/2330 train_time:4016ms step_avg:57.37ms
step:71/2330 train_time:4072ms step_avg:57.35ms
step:72/2330 train_time:4132ms step_avg:57.38ms
step:73/2330 train_time:4187ms step_avg:57.36ms
step:74/2330 train_time:4248ms step_avg:57.40ms
step:75/2330 train_time:4304ms step_avg:57.38ms
step:76/2330 train_time:4363ms step_avg:57.40ms
step:77/2330 train_time:4419ms step_avg:57.39ms
step:78/2330 train_time:4478ms step_avg:57.41ms
step:79/2330 train_time:4533ms step_avg:57.38ms
step:80/2330 train_time:4592ms step_avg:57.41ms
step:81/2330 train_time:4649ms step_avg:57.39ms
step:82/2330 train_time:4708ms step_avg:57.41ms
step:83/2330 train_time:4763ms step_avg:57.38ms
step:84/2330 train_time:4822ms step_avg:57.41ms
step:85/2330 train_time:4877ms step_avg:57.38ms
step:86/2330 train_time:4936ms step_avg:57.40ms
step:87/2330 train_time:4992ms step_avg:57.38ms
step:88/2330 train_time:5051ms step_avg:57.40ms
step:89/2330 train_time:5108ms step_avg:57.40ms
step:90/2330 train_time:5167ms step_avg:57.42ms
step:91/2330 train_time:5223ms step_avg:57.40ms
step:92/2330 train_time:5283ms step_avg:57.42ms
step:93/2330 train_time:5340ms step_avg:57.42ms
step:94/2330 train_time:5398ms step_avg:57.43ms
step:95/2330 train_time:5454ms step_avg:57.41ms
step:96/2330 train_time:5512ms step_avg:57.42ms
step:97/2330 train_time:5568ms step_avg:57.40ms
step:98/2330 train_time:5627ms step_avg:57.42ms
step:99/2330 train_time:5683ms step_avg:57.40ms
step:100/2330 train_time:5742ms step_avg:57.42ms
step:101/2330 train_time:5797ms step_avg:57.40ms
step:102/2330 train_time:5856ms step_avg:57.41ms
step:103/2330 train_time:5912ms step_avg:57.40ms
step:104/2330 train_time:5970ms step_avg:57.41ms
step:105/2330 train_time:6026ms step_avg:57.39ms
step:106/2330 train_time:6086ms step_avg:57.41ms
step:107/2330 train_time:6142ms step_avg:57.41ms
step:108/2330 train_time:6201ms step_avg:57.42ms
step:109/2330 train_time:6257ms step_avg:57.40ms
step:110/2330 train_time:6316ms step_avg:57.42ms
step:111/2330 train_time:6372ms step_avg:57.40ms
step:112/2330 train_time:6432ms step_avg:57.43ms
step:113/2330 train_time:6488ms step_avg:57.42ms
step:114/2330 train_time:6548ms step_avg:57.44ms
step:115/2330 train_time:6603ms step_avg:57.42ms
step:116/2330 train_time:6662ms step_avg:57.43ms
step:117/2330 train_time:6718ms step_avg:57.42ms
step:118/2330 train_time:6776ms step_avg:57.43ms
step:119/2330 train_time:6832ms step_avg:57.41ms
step:120/2330 train_time:6892ms step_avg:57.43ms
step:121/2330 train_time:6947ms step_avg:57.42ms
step:122/2330 train_time:7008ms step_avg:57.44ms
step:123/2330 train_time:7063ms step_avg:57.42ms
step:124/2330 train_time:7122ms step_avg:57.44ms
step:125/2330 train_time:7178ms step_avg:57.42ms
step:126/2330 train_time:7238ms step_avg:57.44ms
step:127/2330 train_time:7293ms step_avg:57.43ms
step:128/2330 train_time:7352ms step_avg:57.44ms
step:129/2330 train_time:7408ms step_avg:57.43ms
step:130/2330 train_time:7467ms step_avg:57.44ms
step:131/2330 train_time:7523ms step_avg:57.43ms
step:132/2330 train_time:7582ms step_avg:57.44ms
step:133/2330 train_time:7638ms step_avg:57.43ms
step:134/2330 train_time:7696ms step_avg:57.44ms
step:135/2330 train_time:7752ms step_avg:57.42ms
step:136/2330 train_time:7811ms step_avg:57.44ms
step:137/2330 train_time:7867ms step_avg:57.42ms
step:138/2330 train_time:7928ms step_avg:57.45ms
step:139/2330 train_time:7983ms step_avg:57.43ms
step:140/2330 train_time:8043ms step_avg:57.45ms
step:141/2330 train_time:8098ms step_avg:57.43ms
step:142/2330 train_time:8157ms step_avg:57.44ms
step:143/2330 train_time:8213ms step_avg:57.43ms
step:144/2330 train_time:8272ms step_avg:57.45ms
step:145/2330 train_time:8329ms step_avg:57.44ms
step:146/2330 train_time:8388ms step_avg:57.45ms
step:147/2330 train_time:8444ms step_avg:57.44ms
step:148/2330 train_time:8503ms step_avg:57.45ms
step:149/2330 train_time:8558ms step_avg:57.44ms
step:150/2330 train_time:8617ms step_avg:57.45ms
step:151/2330 train_time:8673ms step_avg:57.44ms
step:152/2330 train_time:8733ms step_avg:57.45ms
step:153/2330 train_time:8788ms step_avg:57.44ms
step:154/2330 train_time:8848ms step_avg:57.46ms
step:155/2330 train_time:8904ms step_avg:57.44ms
step:156/2330 train_time:8963ms step_avg:57.46ms
step:157/2330 train_time:9020ms step_avg:57.45ms
step:158/2330 train_time:9078ms step_avg:57.45ms
step:159/2330 train_time:9134ms step_avg:57.45ms
step:160/2330 train_time:9193ms step_avg:57.45ms
step:161/2330 train_time:9248ms step_avg:57.44ms
step:162/2330 train_time:9308ms step_avg:57.45ms
step:163/2330 train_time:9363ms step_avg:57.44ms
step:164/2330 train_time:9423ms step_avg:57.46ms
step:165/2330 train_time:9479ms step_avg:57.45ms
step:166/2330 train_time:9538ms step_avg:57.46ms
step:167/2330 train_time:9593ms step_avg:57.45ms
step:168/2330 train_time:9653ms step_avg:57.46ms
step:169/2330 train_time:9708ms step_avg:57.45ms
step:170/2330 train_time:9767ms step_avg:57.45ms
step:171/2330 train_time:9823ms step_avg:57.44ms
step:172/2330 train_time:9882ms step_avg:57.45ms
step:173/2330 train_time:9938ms step_avg:57.44ms
step:174/2330 train_time:9998ms step_avg:57.46ms
step:175/2330 train_time:10054ms step_avg:57.45ms
step:176/2330 train_time:10113ms step_avg:57.46ms
step:177/2330 train_time:10168ms step_avg:57.45ms
step:178/2330 train_time:10228ms step_avg:57.46ms
step:179/2330 train_time:10283ms step_avg:57.45ms
step:180/2330 train_time:10343ms step_avg:57.46ms
step:181/2330 train_time:10399ms step_avg:57.45ms
step:182/2330 train_time:10457ms step_avg:57.46ms
step:183/2330 train_time:10513ms step_avg:57.45ms
step:184/2330 train_time:10572ms step_avg:57.46ms
step:185/2330 train_time:10628ms step_avg:57.45ms
step:186/2330 train_time:10687ms step_avg:57.46ms
step:187/2330 train_time:10743ms step_avg:57.45ms
step:188/2330 train_time:10802ms step_avg:57.46ms
step:189/2330 train_time:10857ms step_avg:57.45ms
step:190/2330 train_time:10917ms step_avg:57.46ms
step:191/2330 train_time:10972ms step_avg:57.45ms
step:192/2330 train_time:11032ms step_avg:57.46ms
step:193/2330 train_time:11089ms step_avg:57.45ms
step:194/2330 train_time:11147ms step_avg:57.46ms
step:195/2330 train_time:11203ms step_avg:57.45ms
step:196/2330 train_time:11262ms step_avg:57.46ms
step:197/2330 train_time:11317ms step_avg:57.45ms
step:198/2330 train_time:11376ms step_avg:57.46ms
step:199/2330 train_time:11432ms step_avg:57.45ms
step:200/2330 train_time:11492ms step_avg:57.46ms
step:201/2330 train_time:11547ms step_avg:57.45ms
step:202/2330 train_time:11606ms step_avg:57.46ms
step:203/2330 train_time:11662ms step_avg:57.45ms
step:204/2330 train_time:11722ms step_avg:57.46ms
step:205/2330 train_time:11777ms step_avg:57.45ms
step:206/2330 train_time:11837ms step_avg:57.46ms
step:207/2330 train_time:11893ms step_avg:57.45ms
step:208/2330 train_time:11952ms step_avg:57.46ms
step:209/2330 train_time:12008ms step_avg:57.45ms
step:210/2330 train_time:12067ms step_avg:57.46ms
step:211/2330 train_time:12123ms step_avg:57.46ms
step:212/2330 train_time:12182ms step_avg:57.46ms
step:213/2330 train_time:12238ms step_avg:57.46ms
step:214/2330 train_time:12297ms step_avg:57.46ms
step:215/2330 train_time:12353ms step_avg:57.45ms
step:216/2330 train_time:12412ms step_avg:57.46ms
step:217/2330 train_time:12468ms step_avg:57.45ms
step:218/2330 train_time:12527ms step_avg:57.46ms
step:219/2330 train_time:12583ms step_avg:57.46ms
step:220/2330 train_time:12641ms step_avg:57.46ms
step:221/2330 train_time:12697ms step_avg:57.45ms
step:222/2330 train_time:12756ms step_avg:57.46ms
step:223/2330 train_time:12812ms step_avg:57.45ms
step:224/2330 train_time:12872ms step_avg:57.46ms
step:225/2330 train_time:12928ms step_avg:57.46ms
step:226/2330 train_time:12986ms step_avg:57.46ms
step:227/2330 train_time:13042ms step_avg:57.45ms
step:228/2330 train_time:13102ms step_avg:57.46ms
step:229/2330 train_time:13157ms step_avg:57.45ms
step:230/2330 train_time:13217ms step_avg:57.46ms
step:231/2330 train_time:13272ms step_avg:57.46ms
step:232/2330 train_time:13332ms step_avg:57.47ms
step:233/2330 train_time:13388ms step_avg:57.46ms
step:234/2330 train_time:13447ms step_avg:57.46ms
step:235/2330 train_time:13502ms step_avg:57.46ms
step:236/2330 train_time:13562ms step_avg:57.47ms
step:237/2330 train_time:13619ms step_avg:57.46ms
step:238/2330 train_time:13677ms step_avg:57.47ms
step:239/2330 train_time:13733ms step_avg:57.46ms
step:240/2330 train_time:13792ms step_avg:57.47ms
step:241/2330 train_time:13848ms step_avg:57.46ms
step:242/2330 train_time:13907ms step_avg:57.47ms
step:243/2330 train_time:13963ms step_avg:57.46ms
step:244/2330 train_time:14022ms step_avg:57.47ms
step:245/2330 train_time:14078ms step_avg:57.46ms
step:246/2330 train_time:14137ms step_avg:57.47ms
step:247/2330 train_time:14192ms step_avg:57.46ms
step:248/2330 train_time:14251ms step_avg:57.46ms
step:249/2330 train_time:14307ms step_avg:57.46ms
step:250/2330 train_time:14366ms step_avg:57.46ms
step:250/2330 val_loss:4.8893 train_time:14445ms step_avg:57.78ms
step:251/2330 train_time:14464ms step_avg:57.63ms
step:252/2330 train_time:14484ms step_avg:57.48ms
step:253/2330 train_time:14541ms step_avg:57.47ms
step:254/2330 train_time:14603ms step_avg:57.49ms
step:255/2330 train_time:14661ms step_avg:57.49ms
step:256/2330 train_time:14722ms step_avg:57.51ms
step:257/2330 train_time:14778ms step_avg:57.50ms
step:258/2330 train_time:14837ms step_avg:57.51ms
step:259/2330 train_time:14892ms step_avg:57.50ms
step:260/2330 train_time:14951ms step_avg:57.50ms
step:261/2330 train_time:15007ms step_avg:57.50ms
step:262/2330 train_time:15065ms step_avg:57.50ms
step:263/2330 train_time:15120ms step_avg:57.49ms
step:264/2330 train_time:15178ms step_avg:57.49ms
step:265/2330 train_time:15233ms step_avg:57.48ms
step:266/2330 train_time:15292ms step_avg:57.49ms
step:267/2330 train_time:15348ms step_avg:57.48ms
step:268/2330 train_time:15406ms step_avg:57.49ms
step:269/2330 train_time:15463ms step_avg:57.48ms
step:270/2330 train_time:15522ms step_avg:57.49ms
step:271/2330 train_time:15580ms step_avg:57.49ms
step:272/2330 train_time:15640ms step_avg:57.50ms
step:273/2330 train_time:15697ms step_avg:57.50ms
step:274/2330 train_time:15756ms step_avg:57.50ms
step:275/2330 train_time:15813ms step_avg:57.50ms
step:276/2330 train_time:15871ms step_avg:57.50ms
step:277/2330 train_time:15926ms step_avg:57.50ms
step:278/2330 train_time:15985ms step_avg:57.50ms
step:279/2330 train_time:16041ms step_avg:57.50ms
step:280/2330 train_time:16100ms step_avg:57.50ms
step:281/2330 train_time:16155ms step_avg:57.49ms
step:282/2330 train_time:16213ms step_avg:57.49ms
step:283/2330 train_time:16269ms step_avg:57.49ms
step:284/2330 train_time:16327ms step_avg:57.49ms
step:285/2330 train_time:16383ms step_avg:57.48ms
step:286/2330 train_time:16442ms step_avg:57.49ms
step:287/2330 train_time:16499ms step_avg:57.49ms
step:288/2330 train_time:16558ms step_avg:57.49ms
step:289/2330 train_time:16614ms step_avg:57.49ms
step:290/2330 train_time:16675ms step_avg:57.50ms
step:291/2330 train_time:16731ms step_avg:57.49ms
step:292/2330 train_time:16790ms step_avg:57.50ms
step:293/2330 train_time:16846ms step_avg:57.50ms
step:294/2330 train_time:16906ms step_avg:57.50ms
step:295/2330 train_time:16963ms step_avg:57.50ms
step:296/2330 train_time:17022ms step_avg:57.51ms
step:297/2330 train_time:17077ms step_avg:57.50ms
step:298/2330 train_time:17136ms step_avg:57.50ms
step:299/2330 train_time:17191ms step_avg:57.50ms
step:300/2330 train_time:17249ms step_avg:57.50ms
step:301/2330 train_time:17305ms step_avg:57.49ms
step:302/2330 train_time:17363ms step_avg:57.49ms
step:303/2330 train_time:17419ms step_avg:57.49ms
step:304/2330 train_time:17478ms step_avg:57.49ms
step:305/2330 train_time:17535ms step_avg:57.49ms
step:306/2330 train_time:17593ms step_avg:57.50ms
step:307/2330 train_time:17650ms step_avg:57.49ms
step:308/2330 train_time:17710ms step_avg:57.50ms
step:309/2330 train_time:17766ms step_avg:57.50ms
step:310/2330 train_time:17826ms step_avg:57.50ms
step:311/2330 train_time:17882ms step_avg:57.50ms
step:312/2330 train_time:17941ms step_avg:57.50ms
step:313/2330 train_time:17997ms step_avg:57.50ms
step:314/2330 train_time:18056ms step_avg:57.50ms
step:315/2330 train_time:18112ms step_avg:57.50ms
step:316/2330 train_time:18170ms step_avg:57.50ms
step:317/2330 train_time:18226ms step_avg:57.49ms
step:318/2330 train_time:18284ms step_avg:57.50ms
step:319/2330 train_time:18339ms step_avg:57.49ms
step:320/2330 train_time:18398ms step_avg:57.49ms
step:321/2330 train_time:18454ms step_avg:57.49ms
step:322/2330 train_time:18514ms step_avg:57.50ms
step:323/2330 train_time:18570ms step_avg:57.49ms
step:324/2330 train_time:18629ms step_avg:57.50ms
step:325/2330 train_time:18685ms step_avg:57.49ms
step:326/2330 train_time:18744ms step_avg:57.50ms
step:327/2330 train_time:18800ms step_avg:57.49ms
step:328/2330 train_time:18860ms step_avg:57.50ms
step:329/2330 train_time:18916ms step_avg:57.49ms
step:330/2330 train_time:18975ms step_avg:57.50ms
step:331/2330 train_time:19030ms step_avg:57.49ms
step:332/2330 train_time:19090ms step_avg:57.50ms
step:333/2330 train_time:19145ms step_avg:57.49ms
step:334/2330 train_time:19204ms step_avg:57.50ms
step:335/2330 train_time:19259ms step_avg:57.49ms
step:336/2330 train_time:19318ms step_avg:57.49ms
step:337/2330 train_time:19374ms step_avg:57.49ms
step:338/2330 train_time:19432ms step_avg:57.49ms
step:339/2330 train_time:19489ms step_avg:57.49ms
step:340/2330 train_time:19547ms step_avg:57.49ms
step:341/2330 train_time:19604ms step_avg:57.49ms
step:342/2330 train_time:19663ms step_avg:57.49ms
step:343/2330 train_time:19720ms step_avg:57.49ms
step:344/2330 train_time:19779ms step_avg:57.50ms
step:345/2330 train_time:19835ms step_avg:57.49ms
step:346/2330 train_time:19894ms step_avg:57.50ms
step:347/2330 train_time:19950ms step_avg:57.49ms
step:348/2330 train_time:20009ms step_avg:57.50ms
step:349/2330 train_time:20065ms step_avg:57.49ms
step:350/2330 train_time:20124ms step_avg:57.50ms
step:351/2330 train_time:20180ms step_avg:57.49ms
step:352/2330 train_time:20238ms step_avg:57.49ms
step:353/2330 train_time:20293ms step_avg:57.49ms
step:354/2330 train_time:20352ms step_avg:57.49ms
step:355/2330 train_time:20408ms step_avg:57.49ms
step:356/2330 train_time:20468ms step_avg:57.49ms
step:357/2330 train_time:20523ms step_avg:57.49ms
step:358/2330 train_time:20583ms step_avg:57.49ms
step:359/2330 train_time:20639ms step_avg:57.49ms
step:360/2330 train_time:20698ms step_avg:57.49ms
step:361/2330 train_time:20754ms step_avg:57.49ms
step:362/2330 train_time:20813ms step_avg:57.50ms
step:363/2330 train_time:20870ms step_avg:57.49ms
step:364/2330 train_time:20929ms step_avg:57.50ms
step:365/2330 train_time:20985ms step_avg:57.49ms
step:366/2330 train_time:21044ms step_avg:57.50ms
step:367/2330 train_time:21100ms step_avg:57.49ms
step:368/2330 train_time:21159ms step_avg:57.50ms
step:369/2330 train_time:21214ms step_avg:57.49ms
step:370/2330 train_time:21273ms step_avg:57.50ms
step:371/2330 train_time:21329ms step_avg:57.49ms
step:372/2330 train_time:21388ms step_avg:57.50ms
step:373/2330 train_time:21444ms step_avg:57.49ms
step:374/2330 train_time:21504ms step_avg:57.50ms
step:375/2330 train_time:21560ms step_avg:57.49ms
step:376/2330 train_time:21619ms step_avg:57.50ms
step:377/2330 train_time:21675ms step_avg:57.49ms
step:378/2330 train_time:21735ms step_avg:57.50ms
step:379/2330 train_time:21790ms step_avg:57.49ms
step:380/2330 train_time:21850ms step_avg:57.50ms
step:381/2330 train_time:21905ms step_avg:57.49ms
step:382/2330 train_time:21965ms step_avg:57.50ms
step:383/2330 train_time:22021ms step_avg:57.50ms
step:384/2330 train_time:22080ms step_avg:57.50ms
step:385/2330 train_time:22136ms step_avg:57.50ms
step:386/2330 train_time:22195ms step_avg:57.50ms
step:387/2330 train_time:22250ms step_avg:57.49ms
step:388/2330 train_time:22309ms step_avg:57.50ms
step:389/2330 train_time:22364ms step_avg:57.49ms
step:390/2330 train_time:22425ms step_avg:57.50ms
step:391/2330 train_time:22480ms step_avg:57.49ms
step:392/2330 train_time:22540ms step_avg:57.50ms
step:393/2330 train_time:22596ms step_avg:57.50ms
step:394/2330 train_time:22654ms step_avg:57.50ms
step:395/2330 train_time:22710ms step_avg:57.49ms
step:396/2330 train_time:22770ms step_avg:57.50ms
step:397/2330 train_time:22826ms step_avg:57.50ms
step:398/2330 train_time:22885ms step_avg:57.50ms
step:399/2330 train_time:22941ms step_avg:57.50ms
step:400/2330 train_time:23001ms step_avg:57.50ms
step:401/2330 train_time:23058ms step_avg:57.50ms
step:402/2330 train_time:23116ms step_avg:57.50ms
step:403/2330 train_time:23173ms step_avg:57.50ms
step:404/2330 train_time:23231ms step_avg:57.50ms
step:405/2330 train_time:23287ms step_avg:57.50ms
step:406/2330 train_time:23346ms step_avg:57.50ms
step:407/2330 train_time:23401ms step_avg:57.50ms
step:408/2330 train_time:23460ms step_avg:57.50ms
step:409/2330 train_time:23515ms step_avg:57.49ms
step:410/2330 train_time:23574ms step_avg:57.50ms
step:411/2330 train_time:23630ms step_avg:57.49ms
step:412/2330 train_time:23689ms step_avg:57.50ms
step:413/2330 train_time:23746ms step_avg:57.50ms
step:414/2330 train_time:23805ms step_avg:57.50ms
step:415/2330 train_time:23861ms step_avg:57.50ms
step:416/2330 train_time:23921ms step_avg:57.50ms
step:417/2330 train_time:23977ms step_avg:57.50ms
step:418/2330 train_time:24036ms step_avg:57.50ms
step:419/2330 train_time:24092ms step_avg:57.50ms
step:420/2330 train_time:24150ms step_avg:57.50ms
step:421/2330 train_time:24206ms step_avg:57.50ms
step:422/2330 train_time:24265ms step_avg:57.50ms
step:423/2330 train_time:24321ms step_avg:57.50ms
step:424/2330 train_time:24380ms step_avg:57.50ms
step:425/2330 train_time:24437ms step_avg:57.50ms
step:426/2330 train_time:24495ms step_avg:57.50ms
step:427/2330 train_time:24551ms step_avg:57.50ms
step:428/2330 train_time:24611ms step_avg:57.50ms
step:429/2330 train_time:24666ms step_avg:57.50ms
step:430/2330 train_time:24726ms step_avg:57.50ms
step:431/2330 train_time:24782ms step_avg:57.50ms
step:432/2330 train_time:24842ms step_avg:57.50ms
step:433/2330 train_time:24897ms step_avg:57.50ms
step:434/2330 train_time:24958ms step_avg:57.51ms
step:435/2330 train_time:25014ms step_avg:57.50ms
step:436/2330 train_time:25073ms step_avg:57.51ms
step:437/2330 train_time:25129ms step_avg:57.50ms
step:438/2330 train_time:25188ms step_avg:57.51ms
step:439/2330 train_time:25243ms step_avg:57.50ms
step:440/2330 train_time:25303ms step_avg:57.51ms
step:441/2330 train_time:25360ms step_avg:57.50ms
step:442/2330 train_time:25419ms step_avg:57.51ms
step:443/2330 train_time:25475ms step_avg:57.51ms
step:444/2330 train_time:25535ms step_avg:57.51ms
step:445/2330 train_time:25590ms step_avg:57.51ms
step:446/2330 train_time:25649ms step_avg:57.51ms
step:447/2330 train_time:25705ms step_avg:57.51ms
step:448/2330 train_time:25764ms step_avg:57.51ms
step:449/2330 train_time:25820ms step_avg:57.50ms
step:450/2330 train_time:25880ms step_avg:57.51ms
step:451/2330 train_time:25936ms step_avg:57.51ms
step:452/2330 train_time:25995ms step_avg:57.51ms
step:453/2330 train_time:26050ms step_avg:57.51ms
step:454/2330 train_time:26111ms step_avg:57.51ms
step:455/2330 train_time:26166ms step_avg:57.51ms
step:456/2330 train_time:26225ms step_avg:57.51ms
step:457/2330 train_time:26281ms step_avg:57.51ms
step:458/2330 train_time:26341ms step_avg:57.51ms
step:459/2330 train_time:26397ms step_avg:57.51ms
step:460/2330 train_time:26457ms step_avg:57.51ms
step:461/2330 train_time:26514ms step_avg:57.51ms
step:462/2330 train_time:26572ms step_avg:57.51ms
step:463/2330 train_time:26628ms step_avg:57.51ms
step:464/2330 train_time:26687ms step_avg:57.51ms
step:465/2330 train_time:26743ms step_avg:57.51ms
step:466/2330 train_time:26802ms step_avg:57.51ms
step:467/2330 train_time:26859ms step_avg:57.51ms
step:468/2330 train_time:26918ms step_avg:57.52ms
step:469/2330 train_time:26974ms step_avg:57.51ms
step:470/2330 train_time:27034ms step_avg:57.52ms
step:471/2330 train_time:27090ms step_avg:57.52ms
step:472/2330 train_time:27149ms step_avg:57.52ms
step:473/2330 train_time:27204ms step_avg:57.51ms
step:474/2330 train_time:27264ms step_avg:57.52ms
step:475/2330 train_time:27319ms step_avg:57.51ms
step:476/2330 train_time:27379ms step_avg:57.52ms
step:477/2330 train_time:27434ms step_avg:57.51ms
step:478/2330 train_time:27494ms step_avg:57.52ms
step:479/2330 train_time:27550ms step_avg:57.52ms
step:480/2330 train_time:27608ms step_avg:57.52ms
step:481/2330 train_time:27664ms step_avg:57.51ms
step:482/2330 train_time:27723ms step_avg:57.52ms
step:483/2330 train_time:27779ms step_avg:57.51ms
step:484/2330 train_time:27839ms step_avg:57.52ms
step:485/2330 train_time:27895ms step_avg:57.51ms
step:486/2330 train_time:27954ms step_avg:57.52ms
step:487/2330 train_time:28011ms step_avg:57.52ms
step:488/2330 train_time:28069ms step_avg:57.52ms
step:489/2330 train_time:28125ms step_avg:57.51ms
step:490/2330 train_time:28184ms step_avg:57.52ms
step:491/2330 train_time:28240ms step_avg:57.51ms
step:492/2330 train_time:28300ms step_avg:57.52ms
step:493/2330 train_time:28355ms step_avg:57.52ms
step:494/2330 train_time:28415ms step_avg:57.52ms
step:495/2330 train_time:28471ms step_avg:57.52ms
step:496/2330 train_time:28530ms step_avg:57.52ms
step:497/2330 train_time:28585ms step_avg:57.51ms
step:498/2330 train_time:28646ms step_avg:57.52ms
step:499/2330 train_time:28702ms step_avg:57.52ms
step:500/2330 train_time:28761ms step_avg:57.52ms
step:500/2330 val_loss:4.4070 train_time:28840ms step_avg:57.68ms
step:501/2330 train_time:28858ms step_avg:57.60ms
step:502/2330 train_time:28879ms step_avg:57.53ms
step:503/2330 train_time:28935ms step_avg:57.53ms
step:504/2330 train_time:29000ms step_avg:57.54ms
step:505/2330 train_time:29056ms step_avg:57.54ms
step:506/2330 train_time:29119ms step_avg:57.55ms
step:507/2330 train_time:29174ms step_avg:57.54ms
step:508/2330 train_time:29234ms step_avg:57.55ms
step:509/2330 train_time:29290ms step_avg:57.54ms
step:510/2330 train_time:29349ms step_avg:57.55ms
step:511/2330 train_time:29404ms step_avg:57.54ms
step:512/2330 train_time:29463ms step_avg:57.54ms
step:513/2330 train_time:29518ms step_avg:57.54ms
step:514/2330 train_time:29577ms step_avg:57.54ms
step:515/2330 train_time:29633ms step_avg:57.54ms
step:516/2330 train_time:29691ms step_avg:57.54ms
step:517/2330 train_time:29746ms step_avg:57.54ms
step:518/2330 train_time:29805ms step_avg:57.54ms
step:519/2330 train_time:29862ms step_avg:57.54ms
step:520/2330 train_time:29922ms step_avg:57.54ms
step:521/2330 train_time:29980ms step_avg:57.54ms
step:522/2330 train_time:30040ms step_avg:57.55ms
step:523/2330 train_time:30097ms step_avg:57.55ms
step:524/2330 train_time:30156ms step_avg:57.55ms
step:525/2330 train_time:30212ms step_avg:57.55ms
step:526/2330 train_time:30271ms step_avg:57.55ms
step:527/2330 train_time:30327ms step_avg:57.55ms
step:528/2330 train_time:30385ms step_avg:57.55ms
step:529/2330 train_time:30441ms step_avg:57.54ms
step:530/2330 train_time:30501ms step_avg:57.55ms
step:531/2330 train_time:30556ms step_avg:57.55ms
step:532/2330 train_time:30615ms step_avg:57.55ms
step:533/2330 train_time:30671ms step_avg:57.54ms
step:534/2330 train_time:30729ms step_avg:57.55ms
step:535/2330 train_time:30785ms step_avg:57.54ms
step:536/2330 train_time:30845ms step_avg:57.55ms
step:537/2330 train_time:30901ms step_avg:57.54ms
step:538/2330 train_time:30962ms step_avg:57.55ms
step:539/2330 train_time:31020ms step_avg:57.55ms
step:540/2330 train_time:31080ms step_avg:57.56ms
step:541/2330 train_time:31138ms step_avg:57.56ms
step:542/2330 train_time:31196ms step_avg:57.56ms
step:543/2330 train_time:31253ms step_avg:57.56ms
step:544/2330 train_time:31313ms step_avg:57.56ms
step:545/2330 train_time:31368ms step_avg:57.56ms
step:546/2330 train_time:31427ms step_avg:57.56ms
step:547/2330 train_time:31483ms step_avg:57.56ms
step:548/2330 train_time:31542ms step_avg:57.56ms
step:549/2330 train_time:31598ms step_avg:57.56ms
step:550/2330 train_time:31657ms step_avg:57.56ms
step:551/2330 train_time:31712ms step_avg:57.55ms
step:552/2330 train_time:31772ms step_avg:57.56ms
step:553/2330 train_time:31828ms step_avg:57.55ms
step:554/2330 train_time:31887ms step_avg:57.56ms
step:555/2330 train_time:31943ms step_avg:57.55ms
step:556/2330 train_time:32004ms step_avg:57.56ms
step:557/2330 train_time:32060ms step_avg:57.56ms
step:558/2330 train_time:32121ms step_avg:57.56ms
step:559/2330 train_time:32178ms step_avg:57.56ms
step:560/2330 train_time:32237ms step_avg:57.57ms
step:561/2330 train_time:32293ms step_avg:57.56ms
step:562/2330 train_time:32351ms step_avg:57.56ms
step:563/2330 train_time:32407ms step_avg:57.56ms
step:564/2330 train_time:32466ms step_avg:57.56ms
step:565/2330 train_time:32522ms step_avg:57.56ms
step:566/2330 train_time:32581ms step_avg:57.56ms
step:567/2330 train_time:32637ms step_avg:57.56ms
step:568/2330 train_time:32696ms step_avg:57.56ms
step:569/2330 train_time:32751ms step_avg:57.56ms
step:570/2330 train_time:32811ms step_avg:57.56ms
step:571/2330 train_time:32866ms step_avg:57.56ms
step:572/2330 train_time:32926ms step_avg:57.56ms
step:573/2330 train_time:32981ms step_avg:57.56ms
step:574/2330 train_time:33041ms step_avg:57.56ms
step:575/2330 train_time:33097ms step_avg:57.56ms
step:576/2330 train_time:33157ms step_avg:57.56ms
step:577/2330 train_time:33213ms step_avg:57.56ms
step:578/2330 train_time:33272ms step_avg:57.56ms
step:579/2330 train_time:33328ms step_avg:57.56ms
step:580/2330 train_time:33388ms step_avg:57.57ms
step:581/2330 train_time:33445ms step_avg:57.56ms
step:582/2330 train_time:33503ms step_avg:57.57ms
step:583/2330 train_time:33559ms step_avg:57.56ms
step:584/2330 train_time:33618ms step_avg:57.56ms
step:585/2330 train_time:33674ms step_avg:57.56ms
step:586/2330 train_time:33733ms step_avg:57.56ms
step:587/2330 train_time:33789ms step_avg:57.56ms
step:588/2330 train_time:33848ms step_avg:57.57ms
step:589/2330 train_time:33904ms step_avg:57.56ms
step:590/2330 train_time:33963ms step_avg:57.56ms
step:591/2330 train_time:34019ms step_avg:57.56ms
step:592/2330 train_time:34079ms step_avg:57.57ms
step:593/2330 train_time:34135ms step_avg:57.56ms
step:594/2330 train_time:34196ms step_avg:57.57ms
step:595/2330 train_time:34252ms step_avg:57.57ms
step:596/2330 train_time:34312ms step_avg:57.57ms
step:597/2330 train_time:34368ms step_avg:57.57ms
step:598/2330 train_time:34427ms step_avg:57.57ms
step:599/2330 train_time:34483ms step_avg:57.57ms
step:600/2330 train_time:34544ms step_avg:57.57ms
step:601/2330 train_time:34599ms step_avg:57.57ms
step:602/2330 train_time:34658ms step_avg:57.57ms
step:603/2330 train_time:34713ms step_avg:57.57ms
step:604/2330 train_time:34773ms step_avg:57.57ms
step:605/2330 train_time:34829ms step_avg:57.57ms
step:606/2330 train_time:34889ms step_avg:57.57ms
step:607/2330 train_time:34945ms step_avg:57.57ms
step:608/2330 train_time:35005ms step_avg:57.57ms
step:609/2330 train_time:35062ms step_avg:57.57ms
step:610/2330 train_time:35121ms step_avg:57.58ms
step:611/2330 train_time:35178ms step_avg:57.57ms
step:612/2330 train_time:35236ms step_avg:57.58ms
step:613/2330 train_time:35293ms step_avg:57.57ms
step:614/2330 train_time:35353ms step_avg:57.58ms
step:615/2330 train_time:35409ms step_avg:57.58ms
step:616/2330 train_time:35468ms step_avg:57.58ms
step:617/2330 train_time:35524ms step_avg:57.58ms
step:618/2330 train_time:35584ms step_avg:57.58ms
step:619/2330 train_time:35640ms step_avg:57.58ms
step:620/2330 train_time:35699ms step_avg:57.58ms
step:621/2330 train_time:35755ms step_avg:57.58ms
step:622/2330 train_time:35815ms step_avg:57.58ms
step:623/2330 train_time:35870ms step_avg:57.58ms
step:624/2330 train_time:35930ms step_avg:57.58ms
step:625/2330 train_time:35985ms step_avg:57.58ms
step:626/2330 train_time:36047ms step_avg:57.58ms
step:627/2330 train_time:36103ms step_avg:57.58ms
step:628/2330 train_time:36162ms step_avg:57.58ms
step:629/2330 train_time:36219ms step_avg:57.58ms
step:630/2330 train_time:36278ms step_avg:57.58ms
step:631/2330 train_time:36335ms step_avg:57.58ms
step:632/2330 train_time:36394ms step_avg:57.59ms
step:633/2330 train_time:36450ms step_avg:57.58ms
step:634/2330 train_time:36510ms step_avg:57.59ms
step:635/2330 train_time:36566ms step_avg:57.58ms
step:636/2330 train_time:36625ms step_avg:57.59ms
step:637/2330 train_time:36681ms step_avg:57.58ms
step:638/2330 train_time:36741ms step_avg:57.59ms
step:639/2330 train_time:36797ms step_avg:57.58ms
step:640/2330 train_time:36856ms step_avg:57.59ms
step:641/2330 train_time:36912ms step_avg:57.58ms
step:642/2330 train_time:36971ms step_avg:57.59ms
step:643/2330 train_time:37027ms step_avg:57.58ms
step:644/2330 train_time:37087ms step_avg:57.59ms
step:645/2330 train_time:37144ms step_avg:57.59ms
step:646/2330 train_time:37204ms step_avg:57.59ms
step:647/2330 train_time:37260ms step_avg:57.59ms
step:648/2330 train_time:37319ms step_avg:57.59ms
step:649/2330 train_time:37375ms step_avg:57.59ms
step:650/2330 train_time:37434ms step_avg:57.59ms
step:651/2330 train_time:37490ms step_avg:57.59ms
step:652/2330 train_time:37550ms step_avg:57.59ms
step:653/2330 train_time:37605ms step_avg:57.59ms
step:654/2330 train_time:37664ms step_avg:57.59ms
step:655/2330 train_time:37720ms step_avg:57.59ms
step:656/2330 train_time:37780ms step_avg:57.59ms
step:657/2330 train_time:37835ms step_avg:57.59ms
step:658/2330 train_time:37895ms step_avg:57.59ms
step:659/2330 train_time:37951ms step_avg:57.59ms
step:660/2330 train_time:38011ms step_avg:57.59ms
step:661/2330 train_time:38066ms step_avg:57.59ms
step:662/2330 train_time:38126ms step_avg:57.59ms
step:663/2330 train_time:38182ms step_avg:57.59ms
step:664/2330 train_time:38242ms step_avg:57.59ms
step:665/2330 train_time:38298ms step_avg:57.59ms
step:666/2330 train_time:38357ms step_avg:57.59ms
step:667/2330 train_time:38413ms step_avg:57.59ms
step:668/2330 train_time:38474ms step_avg:57.60ms
step:669/2330 train_time:38529ms step_avg:57.59ms
step:670/2330 train_time:38588ms step_avg:57.59ms
step:671/2330 train_time:38645ms step_avg:57.59ms
step:672/2330 train_time:38704ms step_avg:57.60ms
step:673/2330 train_time:38761ms step_avg:57.59ms
step:674/2330 train_time:38820ms step_avg:57.60ms
step:675/2330 train_time:38876ms step_avg:57.59ms
step:676/2330 train_time:38935ms step_avg:57.60ms
step:677/2330 train_time:38991ms step_avg:57.59ms
step:678/2330 train_time:39051ms step_avg:57.60ms
step:679/2330 train_time:39106ms step_avg:57.59ms
step:680/2330 train_time:39167ms step_avg:57.60ms
step:681/2330 train_time:39223ms step_avg:57.60ms
step:682/2330 train_time:39283ms step_avg:57.60ms
step:683/2330 train_time:39340ms step_avg:57.60ms
step:684/2330 train_time:39399ms step_avg:57.60ms
step:685/2330 train_time:39455ms step_avg:57.60ms
step:686/2330 train_time:39516ms step_avg:57.60ms
step:687/2330 train_time:39571ms step_avg:57.60ms
step:688/2330 train_time:39631ms step_avg:57.60ms
step:689/2330 train_time:39687ms step_avg:57.60ms
step:690/2330 train_time:39746ms step_avg:57.60ms
step:691/2330 train_time:39802ms step_avg:57.60ms
step:692/2330 train_time:39861ms step_avg:57.60ms
step:693/2330 train_time:39918ms step_avg:57.60ms
step:694/2330 train_time:39977ms step_avg:57.60ms
step:695/2330 train_time:40033ms step_avg:57.60ms
step:696/2330 train_time:40093ms step_avg:57.60ms
step:697/2330 train_time:40149ms step_avg:57.60ms
step:698/2330 train_time:40207ms step_avg:57.60ms
step:699/2330 train_time:40264ms step_avg:57.60ms
step:700/2330 train_time:40323ms step_avg:57.60ms
step:701/2330 train_time:40380ms step_avg:57.60ms
step:702/2330 train_time:40439ms step_avg:57.61ms
step:703/2330 train_time:40496ms step_avg:57.60ms
step:704/2330 train_time:40555ms step_avg:57.61ms
step:705/2330 train_time:40611ms step_avg:57.60ms
step:706/2330 train_time:40669ms step_avg:57.61ms
step:707/2330 train_time:40725ms step_avg:57.60ms
step:708/2330 train_time:40785ms step_avg:57.61ms
step:709/2330 train_time:40842ms step_avg:57.60ms
step:710/2330 train_time:40901ms step_avg:57.61ms
step:711/2330 train_time:40956ms step_avg:57.60ms
step:712/2330 train_time:41015ms step_avg:57.61ms
step:713/2330 train_time:41071ms step_avg:57.60ms
step:714/2330 train_time:41130ms step_avg:57.61ms
step:715/2330 train_time:41186ms step_avg:57.60ms
step:716/2330 train_time:41247ms step_avg:57.61ms
step:717/2330 train_time:41302ms step_avg:57.60ms
step:718/2330 train_time:41362ms step_avg:57.61ms
step:719/2330 train_time:41419ms step_avg:57.61ms
step:720/2330 train_time:41478ms step_avg:57.61ms
step:721/2330 train_time:41534ms step_avg:57.61ms
step:722/2330 train_time:41594ms step_avg:57.61ms
step:723/2330 train_time:41650ms step_avg:57.61ms
step:724/2330 train_time:41709ms step_avg:57.61ms
step:725/2330 train_time:41764ms step_avg:57.61ms
step:726/2330 train_time:41823ms step_avg:57.61ms
step:727/2330 train_time:41880ms step_avg:57.61ms
step:728/2330 train_time:41939ms step_avg:57.61ms
step:729/2330 train_time:41995ms step_avg:57.61ms
step:730/2330 train_time:42055ms step_avg:57.61ms
step:731/2330 train_time:42111ms step_avg:57.61ms
step:732/2330 train_time:42170ms step_avg:57.61ms
step:733/2330 train_time:42226ms step_avg:57.61ms
step:734/2330 train_time:42286ms step_avg:57.61ms
step:735/2330 train_time:42343ms step_avg:57.61ms
step:736/2330 train_time:42402ms step_avg:57.61ms
step:737/2330 train_time:42459ms step_avg:57.61ms
step:738/2330 train_time:42517ms step_avg:57.61ms
step:739/2330 train_time:42573ms step_avg:57.61ms
step:740/2330 train_time:42633ms step_avg:57.61ms
step:741/2330 train_time:42689ms step_avg:57.61ms
step:742/2330 train_time:42748ms step_avg:57.61ms
step:743/2330 train_time:42804ms step_avg:57.61ms
step:744/2330 train_time:42864ms step_avg:57.61ms
step:745/2330 train_time:42920ms step_avg:57.61ms
step:746/2330 train_time:42979ms step_avg:57.61ms
step:747/2330 train_time:43036ms step_avg:57.61ms
step:748/2330 train_time:43095ms step_avg:57.61ms
step:749/2330 train_time:43151ms step_avg:57.61ms
step:750/2330 train_time:43212ms step_avg:57.62ms
step:750/2330 val_loss:4.2041 train_time:43291ms step_avg:57.72ms
step:751/2330 train_time:43308ms step_avg:57.67ms
step:752/2330 train_time:43329ms step_avg:57.62ms
step:753/2330 train_time:43384ms step_avg:57.61ms
step:754/2330 train_time:43449ms step_avg:57.62ms
step:755/2330 train_time:43506ms step_avg:57.62ms
step:756/2330 train_time:43570ms step_avg:57.63ms
step:757/2330 train_time:43625ms step_avg:57.63ms
step:758/2330 train_time:43685ms step_avg:57.63ms
step:759/2330 train_time:43740ms step_avg:57.63ms
step:760/2330 train_time:43800ms step_avg:57.63ms
step:761/2330 train_time:43856ms step_avg:57.63ms
step:762/2330 train_time:43914ms step_avg:57.63ms
step:763/2330 train_time:43970ms step_avg:57.63ms
step:764/2330 train_time:44029ms step_avg:57.63ms
step:765/2330 train_time:44086ms step_avg:57.63ms
step:766/2330 train_time:44143ms step_avg:57.63ms
step:767/2330 train_time:44200ms step_avg:57.63ms
step:768/2330 train_time:44260ms step_avg:57.63ms
step:769/2330 train_time:44317ms step_avg:57.63ms
step:770/2330 train_time:44379ms step_avg:57.63ms
step:771/2330 train_time:44437ms step_avg:57.64ms
step:772/2330 train_time:44498ms step_avg:57.64ms
step:773/2330 train_time:44556ms step_avg:57.64ms
step:774/2330 train_time:44617ms step_avg:57.64ms
step:775/2330 train_time:44674ms step_avg:57.64ms
step:776/2330 train_time:44734ms step_avg:57.65ms
step:777/2330 train_time:44790ms step_avg:57.65ms
step:778/2330 train_time:44850ms step_avg:57.65ms
step:779/2330 train_time:44906ms step_avg:57.65ms
step:780/2330 train_time:44965ms step_avg:57.65ms
step:781/2330 train_time:45021ms step_avg:57.65ms
step:782/2330 train_time:45082ms step_avg:57.65ms
step:783/2330 train_time:45137ms step_avg:57.65ms
step:784/2330 train_time:45197ms step_avg:57.65ms
step:785/2330 train_time:45254ms step_avg:57.65ms
step:786/2330 train_time:45314ms step_avg:57.65ms
step:787/2330 train_time:45371ms step_avg:57.65ms
step:788/2330 train_time:45432ms step_avg:57.66ms
step:789/2330 train_time:45489ms step_avg:57.65ms
step:790/2330 train_time:45552ms step_avg:57.66ms
step:791/2330 train_time:45608ms step_avg:57.66ms
step:792/2330 train_time:45670ms step_avg:57.66ms
step:793/2330 train_time:45726ms step_avg:57.66ms
step:794/2330 train_time:45787ms step_avg:57.67ms
step:795/2330 train_time:45843ms step_avg:57.66ms
step:796/2330 train_time:45903ms step_avg:57.67ms
step:797/2330 train_time:45959ms step_avg:57.67ms
step:798/2330 train_time:46019ms step_avg:57.67ms
step:799/2330 train_time:46075ms step_avg:57.67ms
step:800/2330 train_time:46135ms step_avg:57.67ms
step:801/2330 train_time:46192ms step_avg:57.67ms
step:802/2330 train_time:46251ms step_avg:57.67ms
step:803/2330 train_time:46308ms step_avg:57.67ms
step:804/2330 train_time:46369ms step_avg:57.67ms
step:805/2330 train_time:46426ms step_avg:57.67ms
step:806/2330 train_time:46487ms step_avg:57.68ms
step:807/2330 train_time:46545ms step_avg:57.68ms
step:808/2330 train_time:46606ms step_avg:57.68ms
step:809/2330 train_time:46663ms step_avg:57.68ms
step:810/2330 train_time:46725ms step_avg:57.68ms
step:811/2330 train_time:46781ms step_avg:57.68ms
step:812/2330 train_time:46842ms step_avg:57.69ms
step:813/2330 train_time:46898ms step_avg:57.69ms
step:814/2330 train_time:46958ms step_avg:57.69ms
step:815/2330 train_time:47014ms step_avg:57.69ms
step:816/2330 train_time:47073ms step_avg:57.69ms
step:817/2330 train_time:47130ms step_avg:57.69ms
step:818/2330 train_time:47190ms step_avg:57.69ms
step:819/2330 train_time:47247ms step_avg:57.69ms
step:820/2330 train_time:47306ms step_avg:57.69ms
step:821/2330 train_time:47363ms step_avg:57.69ms
step:822/2330 train_time:47423ms step_avg:57.69ms
step:823/2330 train_time:47482ms step_avg:57.69ms
step:824/2330 train_time:47542ms step_avg:57.70ms
step:825/2330 train_time:47600ms step_avg:57.70ms
step:826/2330 train_time:47660ms step_avg:57.70ms
step:827/2330 train_time:47717ms step_avg:57.70ms
step:828/2330 train_time:47777ms step_avg:57.70ms
step:829/2330 train_time:47834ms step_avg:57.70ms
step:830/2330 train_time:47895ms step_avg:57.70ms
step:831/2330 train_time:47952ms step_avg:57.70ms
step:832/2330 train_time:48012ms step_avg:57.71ms
step:833/2330 train_time:48068ms step_avg:57.70ms
step:834/2330 train_time:48128ms step_avg:57.71ms
step:835/2330 train_time:48184ms step_avg:57.71ms
step:836/2330 train_time:48245ms step_avg:57.71ms
step:837/2330 train_time:48301ms step_avg:57.71ms
step:838/2330 train_time:48361ms step_avg:57.71ms
step:839/2330 train_time:48419ms step_avg:57.71ms
step:840/2330 train_time:48480ms step_avg:57.71ms
step:841/2330 train_time:48537ms step_avg:57.71ms
step:842/2330 train_time:48597ms step_avg:57.72ms
step:843/2330 train_time:48654ms step_avg:57.71ms
step:844/2330 train_time:48715ms step_avg:57.72ms
step:845/2330 train_time:48772ms step_avg:57.72ms
step:846/2330 train_time:48832ms step_avg:57.72ms
step:847/2330 train_time:48889ms step_avg:57.72ms
step:848/2330 train_time:48950ms step_avg:57.72ms
step:849/2330 train_time:49007ms step_avg:57.72ms
step:850/2330 train_time:49067ms step_avg:57.73ms
step:851/2330 train_time:49123ms step_avg:57.72ms
step:852/2330 train_time:49184ms step_avg:57.73ms
step:853/2330 train_time:49240ms step_avg:57.73ms
step:854/2330 train_time:49300ms step_avg:57.73ms
step:855/2330 train_time:49357ms step_avg:57.73ms
step:856/2330 train_time:49418ms step_avg:57.73ms
step:857/2330 train_time:49475ms step_avg:57.73ms
step:858/2330 train_time:49536ms step_avg:57.73ms
step:859/2330 train_time:49594ms step_avg:57.73ms
step:860/2330 train_time:49653ms step_avg:57.74ms
step:861/2330 train_time:49711ms step_avg:57.74ms
step:862/2330 train_time:49770ms step_avg:57.74ms
step:863/2330 train_time:49827ms step_avg:57.74ms
step:864/2330 train_time:49887ms step_avg:57.74ms
step:865/2330 train_time:49943ms step_avg:57.74ms
step:866/2330 train_time:50004ms step_avg:57.74ms
step:867/2330 train_time:50061ms step_avg:57.74ms
step:868/2330 train_time:50121ms step_avg:57.74ms
step:869/2330 train_time:50177ms step_avg:57.74ms
step:870/2330 train_time:50237ms step_avg:57.74ms
step:871/2330 train_time:50294ms step_avg:57.74ms
step:872/2330 train_time:50355ms step_avg:57.75ms
step:873/2330 train_time:50411ms step_avg:57.75ms
step:874/2330 train_time:50473ms step_avg:57.75ms
step:875/2330 train_time:50529ms step_avg:57.75ms
step:876/2330 train_time:50590ms step_avg:57.75ms
step:877/2330 train_time:50646ms step_avg:57.75ms
step:878/2330 train_time:50708ms step_avg:57.75ms
step:879/2330 train_time:50765ms step_avg:57.75ms
step:880/2330 train_time:50825ms step_avg:57.76ms
step:881/2330 train_time:50882ms step_avg:57.75ms
step:882/2330 train_time:50943ms step_avg:57.76ms
step:883/2330 train_time:51000ms step_avg:57.76ms
step:884/2330 train_time:51060ms step_avg:57.76ms
step:885/2330 train_time:51116ms step_avg:57.76ms
step:886/2330 train_time:51177ms step_avg:57.76ms
step:887/2330 train_time:51234ms step_avg:57.76ms
step:888/2330 train_time:51295ms step_avg:57.76ms
step:889/2330 train_time:51351ms step_avg:57.76ms
step:890/2330 train_time:51411ms step_avg:57.77ms
step:891/2330 train_time:51468ms step_avg:57.76ms
step:892/2330 train_time:51528ms step_avg:57.77ms
step:893/2330 train_time:51585ms step_avg:57.77ms
step:894/2330 train_time:51646ms step_avg:57.77ms
step:895/2330 train_time:51702ms step_avg:57.77ms
step:896/2330 train_time:51763ms step_avg:57.77ms
step:897/2330 train_time:51820ms step_avg:57.77ms
step:898/2330 train_time:51881ms step_avg:57.77ms
step:899/2330 train_time:51938ms step_avg:57.77ms
step:900/2330 train_time:51998ms step_avg:57.78ms
step:901/2330 train_time:52054ms step_avg:57.77ms
step:902/2330 train_time:52114ms step_avg:57.78ms
step:903/2330 train_time:52171ms step_avg:57.78ms
step:904/2330 train_time:52231ms step_avg:57.78ms
step:905/2330 train_time:52288ms step_avg:57.78ms
step:906/2330 train_time:52348ms step_avg:57.78ms
step:907/2330 train_time:52404ms step_avg:57.78ms
step:908/2330 train_time:52464ms step_avg:57.78ms
step:909/2330 train_time:52521ms step_avg:57.78ms
step:910/2330 train_time:52582ms step_avg:57.78ms
step:911/2330 train_time:52639ms step_avg:57.78ms
step:912/2330 train_time:52698ms step_avg:57.78ms
step:913/2330 train_time:52755ms step_avg:57.78ms
step:914/2330 train_time:52816ms step_avg:57.79ms
step:915/2330 train_time:52873ms step_avg:57.78ms
step:916/2330 train_time:52933ms step_avg:57.79ms
step:917/2330 train_time:52990ms step_avg:57.79ms
step:918/2330 train_time:53051ms step_avg:57.79ms
step:919/2330 train_time:53107ms step_avg:57.79ms
step:920/2330 train_time:53167ms step_avg:57.79ms
step:921/2330 train_time:53224ms step_avg:57.79ms
step:922/2330 train_time:53286ms step_avg:57.79ms
step:923/2330 train_time:53342ms step_avg:57.79ms
step:924/2330 train_time:53403ms step_avg:57.80ms
step:925/2330 train_time:53460ms step_avg:57.79ms
step:926/2330 train_time:53521ms step_avg:57.80ms
step:927/2330 train_time:53577ms step_avg:57.80ms
step:928/2330 train_time:53637ms step_avg:57.80ms
step:929/2330 train_time:53694ms step_avg:57.80ms
step:930/2330 train_time:53754ms step_avg:57.80ms
step:931/2330 train_time:53810ms step_avg:57.80ms
step:932/2330 train_time:53871ms step_avg:57.80ms
step:933/2330 train_time:53927ms step_avg:57.80ms
step:934/2330 train_time:53988ms step_avg:57.80ms
step:935/2330 train_time:54045ms step_avg:57.80ms
step:936/2330 train_time:54106ms step_avg:57.81ms
step:937/2330 train_time:54162ms step_avg:57.80ms
step:938/2330 train_time:54223ms step_avg:57.81ms
step:939/2330 train_time:54279ms step_avg:57.81ms
step:940/2330 train_time:54341ms step_avg:57.81ms
step:941/2330 train_time:54398ms step_avg:57.81ms
step:942/2330 train_time:54458ms step_avg:57.81ms
step:943/2330 train_time:54515ms step_avg:57.81ms
step:944/2330 train_time:54574ms step_avg:57.81ms
step:945/2330 train_time:54631ms step_avg:57.81ms
step:946/2330 train_time:54692ms step_avg:57.81ms
step:947/2330 train_time:54749ms step_avg:57.81ms
step:948/2330 train_time:54809ms step_avg:57.82ms
step:949/2330 train_time:54865ms step_avg:57.81ms
step:950/2330 train_time:54926ms step_avg:57.82ms
step:951/2330 train_time:54982ms step_avg:57.82ms
step:952/2330 train_time:55042ms step_avg:57.82ms
step:953/2330 train_time:55100ms step_avg:57.82ms
step:954/2330 train_time:55160ms step_avg:57.82ms
step:955/2330 train_time:55216ms step_avg:57.82ms
step:956/2330 train_time:55277ms step_avg:57.82ms
step:957/2330 train_time:55335ms step_avg:57.82ms
step:958/2330 train_time:55394ms step_avg:57.82ms
step:959/2330 train_time:55451ms step_avg:57.82ms
step:960/2330 train_time:55511ms step_avg:57.82ms
step:961/2330 train_time:55568ms step_avg:57.82ms
step:962/2330 train_time:55628ms step_avg:57.83ms
step:963/2330 train_time:55684ms step_avg:57.82ms
step:964/2330 train_time:55746ms step_avg:57.83ms
step:965/2330 train_time:55802ms step_avg:57.83ms
step:966/2330 train_time:55863ms step_avg:57.83ms
step:967/2330 train_time:55920ms step_avg:57.83ms
step:968/2330 train_time:55979ms step_avg:57.83ms
step:969/2330 train_time:56036ms step_avg:57.83ms
step:970/2330 train_time:56096ms step_avg:57.83ms
step:971/2330 train_time:56153ms step_avg:57.83ms
step:972/2330 train_time:56214ms step_avg:57.83ms
step:973/2330 train_time:56270ms step_avg:57.83ms
step:974/2330 train_time:56331ms step_avg:57.83ms
step:975/2330 train_time:56388ms step_avg:57.83ms
step:976/2330 train_time:56449ms step_avg:57.84ms
step:977/2330 train_time:56505ms step_avg:57.84ms
step:978/2330 train_time:56565ms step_avg:57.84ms
step:979/2330 train_time:56622ms step_avg:57.84ms
step:980/2330 train_time:56683ms step_avg:57.84ms
step:981/2330 train_time:56739ms step_avg:57.84ms
step:982/2330 train_time:56801ms step_avg:57.84ms
step:983/2330 train_time:56858ms step_avg:57.84ms
step:984/2330 train_time:56919ms step_avg:57.84ms
step:985/2330 train_time:56976ms step_avg:57.84ms
step:986/2330 train_time:57036ms step_avg:57.85ms
step:987/2330 train_time:57093ms step_avg:57.84ms
step:988/2330 train_time:57153ms step_avg:57.85ms
step:989/2330 train_time:57210ms step_avg:57.85ms
step:990/2330 train_time:57270ms step_avg:57.85ms
step:991/2330 train_time:57328ms step_avg:57.85ms
step:992/2330 train_time:57387ms step_avg:57.85ms
step:993/2330 train_time:57443ms step_avg:57.85ms
step:994/2330 train_time:57504ms step_avg:57.85ms
step:995/2330 train_time:57561ms step_avg:57.85ms
step:996/2330 train_time:57621ms step_avg:57.85ms
step:997/2330 train_time:57678ms step_avg:57.85ms
step:998/2330 train_time:57739ms step_avg:57.85ms
step:999/2330 train_time:57796ms step_avg:57.85ms
step:1000/2330 train_time:57857ms step_avg:57.86ms
step:1000/2330 val_loss:4.0639 train_time:57938ms step_avg:57.94ms
step:1001/2330 train_time:57957ms step_avg:57.90ms
step:1002/2330 train_time:57977ms step_avg:57.86ms
step:1003/2330 train_time:58035ms step_avg:57.86ms
step:1004/2330 train_time:58097ms step_avg:57.87ms
step:1005/2330 train_time:58155ms step_avg:57.87ms
step:1006/2330 train_time:58215ms step_avg:57.87ms
step:1007/2330 train_time:58271ms step_avg:57.87ms
step:1008/2330 train_time:58331ms step_avg:57.87ms
step:1009/2330 train_time:58388ms step_avg:57.87ms
step:1010/2330 train_time:58447ms step_avg:57.87ms
step:1011/2330 train_time:58504ms step_avg:57.87ms
step:1012/2330 train_time:58563ms step_avg:57.87ms
step:1013/2330 train_time:58620ms step_avg:57.87ms
step:1014/2330 train_time:58679ms step_avg:57.87ms
step:1015/2330 train_time:58735ms step_avg:57.87ms
step:1016/2330 train_time:58793ms step_avg:57.87ms
step:1017/2330 train_time:58850ms step_avg:57.87ms
step:1018/2330 train_time:58916ms step_avg:57.87ms
step:1019/2330 train_time:58973ms step_avg:57.87ms
step:1020/2330 train_time:59035ms step_avg:57.88ms
step:1021/2330 train_time:59092ms step_avg:57.88ms
step:1022/2330 train_time:59152ms step_avg:57.88ms
step:1023/2330 train_time:59210ms step_avg:57.88ms
step:1024/2330 train_time:59270ms step_avg:57.88ms
step:1025/2330 train_time:59327ms step_avg:57.88ms
step:1026/2330 train_time:59386ms step_avg:57.88ms
step:1027/2330 train_time:59443ms step_avg:57.88ms
step:1028/2330 train_time:59503ms step_avg:57.88ms
step:1029/2330 train_time:59560ms step_avg:57.88ms
step:1030/2330 train_time:59619ms step_avg:57.88ms
step:1031/2330 train_time:59675ms step_avg:57.88ms
step:1032/2330 train_time:59734ms step_avg:57.88ms
step:1033/2330 train_time:59791ms step_avg:57.88ms
step:1034/2330 train_time:59852ms step_avg:57.88ms
step:1035/2330 train_time:59909ms step_avg:57.88ms
step:1036/2330 train_time:59971ms step_avg:57.89ms
step:1037/2330 train_time:60028ms step_avg:57.89ms
step:1038/2330 train_time:60089ms step_avg:57.89ms
step:1039/2330 train_time:60146ms step_avg:57.89ms
step:1040/2330 train_time:60207ms step_avg:57.89ms
step:1041/2330 train_time:60265ms step_avg:57.89ms
step:1042/2330 train_time:60325ms step_avg:57.89ms
step:1043/2330 train_time:60382ms step_avg:57.89ms
step:1044/2330 train_time:60442ms step_avg:57.89ms
step:1045/2330 train_time:60499ms step_avg:57.89ms
step:1046/2330 train_time:60559ms step_avg:57.90ms
step:1047/2330 train_time:60615ms step_avg:57.89ms
step:1048/2330 train_time:60674ms step_avg:57.89ms
step:1049/2330 train_time:60730ms step_avg:57.89ms
step:1050/2330 train_time:60791ms step_avg:57.90ms
step:1051/2330 train_time:60848ms step_avg:57.90ms
step:1052/2330 train_time:60909ms step_avg:57.90ms
step:1053/2330 train_time:60967ms step_avg:57.90ms
step:1054/2330 train_time:61028ms step_avg:57.90ms
step:1055/2330 train_time:61085ms step_avg:57.90ms
step:1056/2330 train_time:61146ms step_avg:57.90ms
step:1057/2330 train_time:61203ms step_avg:57.90ms
step:1058/2330 train_time:61264ms step_avg:57.91ms
step:1059/2330 train_time:61321ms step_avg:57.90ms
step:1060/2330 train_time:61381ms step_avg:57.91ms
step:1061/2330 train_time:61438ms step_avg:57.91ms
step:1062/2330 train_time:61498ms step_avg:57.91ms
step:1063/2330 train_time:61555ms step_avg:57.91ms
step:1064/2330 train_time:61613ms step_avg:57.91ms
step:1065/2330 train_time:61670ms step_avg:57.91ms
step:1066/2330 train_time:61729ms step_avg:57.91ms
step:1067/2330 train_time:61786ms step_avg:57.91ms
step:1068/2330 train_time:61846ms step_avg:57.91ms
step:1069/2330 train_time:61903ms step_avg:57.91ms
step:1070/2330 train_time:61964ms step_avg:57.91ms
step:1071/2330 train_time:62021ms step_avg:57.91ms
step:1072/2330 train_time:62081ms step_avg:57.91ms
step:1073/2330 train_time:62138ms step_avg:57.91ms
step:1074/2330 train_time:62199ms step_avg:57.91ms
step:1075/2330 train_time:62255ms step_avg:57.91ms
step:1076/2330 train_time:62316ms step_avg:57.91ms
step:1077/2330 train_time:62373ms step_avg:57.91ms
step:1078/2330 train_time:62433ms step_avg:57.92ms
step:1079/2330 train_time:62489ms step_avg:57.91ms
step:1080/2330 train_time:62549ms step_avg:57.92ms
step:1081/2330 train_time:62606ms step_avg:57.91ms
step:1082/2330 train_time:62666ms step_avg:57.92ms
step:1083/2330 train_time:62723ms step_avg:57.92ms
step:1084/2330 train_time:62783ms step_avg:57.92ms
step:1085/2330 train_time:62840ms step_avg:57.92ms
step:1086/2330 train_time:62900ms step_avg:57.92ms
step:1087/2330 train_time:62957ms step_avg:57.92ms
step:1088/2330 train_time:63018ms step_avg:57.92ms
step:1089/2330 train_time:63075ms step_avg:57.92ms
step:1090/2330 train_time:63134ms step_avg:57.92ms
step:1091/2330 train_time:63192ms step_avg:57.92ms
step:1092/2330 train_time:63252ms step_avg:57.92ms
step:1093/2330 train_time:63310ms step_avg:57.92ms
step:1094/2330 train_time:63369ms step_avg:57.92ms
step:1095/2330 train_time:63426ms step_avg:57.92ms
step:1096/2330 train_time:63486ms step_avg:57.93ms
step:1097/2330 train_time:63543ms step_avg:57.92ms
step:1098/2330 train_time:63603ms step_avg:57.93ms
step:1099/2330 train_time:63660ms step_avg:57.93ms
step:1100/2330 train_time:63720ms step_avg:57.93ms
step:1101/2330 train_time:63776ms step_avg:57.93ms
step:1102/2330 train_time:63837ms step_avg:57.93ms
step:1103/2330 train_time:63894ms step_avg:57.93ms
step:1104/2330 train_time:63954ms step_avg:57.93ms
step:1105/2330 train_time:64010ms step_avg:57.93ms
step:1106/2330 train_time:64071ms step_avg:57.93ms
step:1107/2330 train_time:64127ms step_avg:57.93ms
step:1108/2330 train_time:64189ms step_avg:57.93ms
step:1109/2330 train_time:64245ms step_avg:57.93ms
step:1110/2330 train_time:64306ms step_avg:57.93ms
step:1111/2330 train_time:64364ms step_avg:57.93ms
step:1112/2330 train_time:64423ms step_avg:57.93ms
step:1113/2330 train_time:64480ms step_avg:57.93ms
step:1114/2330 train_time:64539ms step_avg:57.93ms
step:1115/2330 train_time:64596ms step_avg:57.93ms
step:1116/2330 train_time:64656ms step_avg:57.94ms
step:1117/2330 train_time:64712ms step_avg:57.93ms
step:1118/2330 train_time:64772ms step_avg:57.94ms
step:1119/2330 train_time:64829ms step_avg:57.93ms
step:1120/2330 train_time:64890ms step_avg:57.94ms
step:1121/2330 train_time:64947ms step_avg:57.94ms
step:1122/2330 train_time:65008ms step_avg:57.94ms
step:1123/2330 train_time:65065ms step_avg:57.94ms
step:1124/2330 train_time:65125ms step_avg:57.94ms
step:1125/2330 train_time:65183ms step_avg:57.94ms
step:1126/2330 train_time:65242ms step_avg:57.94ms
step:1127/2330 train_time:65300ms step_avg:57.94ms
step:1128/2330 train_time:65360ms step_avg:57.94ms
step:1129/2330 train_time:65417ms step_avg:57.94ms
step:1130/2330 train_time:65476ms step_avg:57.94ms
step:1131/2330 train_time:65532ms step_avg:57.94ms
step:1132/2330 train_time:65593ms step_avg:57.94ms
step:1133/2330 train_time:65650ms step_avg:57.94ms
step:1134/2330 train_time:65710ms step_avg:57.95ms
step:1135/2330 train_time:65767ms step_avg:57.94ms
step:1136/2330 train_time:65827ms step_avg:57.95ms
step:1137/2330 train_time:65884ms step_avg:57.95ms
step:1138/2330 train_time:65944ms step_avg:57.95ms
step:1139/2330 train_time:66002ms step_avg:57.95ms
step:1140/2330 train_time:66062ms step_avg:57.95ms
step:1141/2330 train_time:66120ms step_avg:57.95ms
step:1142/2330 train_time:66179ms step_avg:57.95ms
step:1143/2330 train_time:66236ms step_avg:57.95ms
step:1144/2330 train_time:66297ms step_avg:57.95ms
step:1145/2330 train_time:66354ms step_avg:57.95ms
step:1146/2330 train_time:66413ms step_avg:57.95ms
step:1147/2330 train_time:66470ms step_avg:57.95ms
step:1148/2330 train_time:66531ms step_avg:57.95ms
step:1149/2330 train_time:66587ms step_avg:57.95ms
step:1150/2330 train_time:66648ms step_avg:57.96ms
step:1151/2330 train_time:66706ms step_avg:57.95ms
step:1152/2330 train_time:66765ms step_avg:57.96ms
step:1153/2330 train_time:66822ms step_avg:57.95ms
step:1154/2330 train_time:66882ms step_avg:57.96ms
step:1155/2330 train_time:66938ms step_avg:57.96ms
step:1156/2330 train_time:67000ms step_avg:57.96ms
step:1157/2330 train_time:67056ms step_avg:57.96ms
step:1158/2330 train_time:67116ms step_avg:57.96ms
step:1159/2330 train_time:67173ms step_avg:57.96ms
step:1160/2330 train_time:67234ms step_avg:57.96ms
step:1161/2330 train_time:67291ms step_avg:57.96ms
step:1162/2330 train_time:67352ms step_avg:57.96ms
step:1163/2330 train_time:67408ms step_avg:57.96ms
step:1164/2330 train_time:67468ms step_avg:57.96ms
step:1165/2330 train_time:67525ms step_avg:57.96ms
step:1166/2330 train_time:67586ms step_avg:57.96ms
step:1167/2330 train_time:67643ms step_avg:57.96ms
step:1168/2330 train_time:67703ms step_avg:57.97ms
step:1169/2330 train_time:67759ms step_avg:57.96ms
step:1170/2330 train_time:67819ms step_avg:57.97ms
step:1171/2330 train_time:67876ms step_avg:57.96ms
step:1172/2330 train_time:67937ms step_avg:57.97ms
step:1173/2330 train_time:67994ms step_avg:57.97ms
step:1174/2330 train_time:68054ms step_avg:57.97ms
step:1175/2330 train_time:68111ms step_avg:57.97ms
step:1176/2330 train_time:68171ms step_avg:57.97ms
step:1177/2330 train_time:68228ms step_avg:57.97ms
step:1178/2330 train_time:68289ms step_avg:57.97ms
step:1179/2330 train_time:68346ms step_avg:57.97ms
step:1180/2330 train_time:68407ms step_avg:57.97ms
step:1181/2330 train_time:68465ms step_avg:57.97ms
step:1182/2330 train_time:68524ms step_avg:57.97ms
step:1183/2330 train_time:68582ms step_avg:57.97ms
step:1184/2330 train_time:68641ms step_avg:57.97ms
step:1185/2330 train_time:68698ms step_avg:57.97ms
step:1186/2330 train_time:68758ms step_avg:57.97ms
step:1187/2330 train_time:68815ms step_avg:57.97ms
step:1188/2330 train_time:68874ms step_avg:57.98ms
step:1189/2330 train_time:68931ms step_avg:57.97ms
step:1190/2330 train_time:68992ms step_avg:57.98ms
step:1191/2330 train_time:69049ms step_avg:57.98ms
step:1192/2330 train_time:69110ms step_avg:57.98ms
step:1193/2330 train_time:69166ms step_avg:57.98ms
step:1194/2330 train_time:69227ms step_avg:57.98ms
step:1195/2330 train_time:69284ms step_avg:57.98ms
step:1196/2330 train_time:69344ms step_avg:57.98ms
step:1197/2330 train_time:69401ms step_avg:57.98ms
step:1198/2330 train_time:69461ms step_avg:57.98ms
step:1199/2330 train_time:69517ms step_avg:57.98ms
step:1200/2330 train_time:69578ms step_avg:57.98ms
step:1201/2330 train_time:69634ms step_avg:57.98ms
step:1202/2330 train_time:69696ms step_avg:57.98ms
step:1203/2330 train_time:69752ms step_avg:57.98ms
step:1204/2330 train_time:69813ms step_avg:57.98ms
step:1205/2330 train_time:69869ms step_avg:57.98ms
step:1206/2330 train_time:69930ms step_avg:57.98ms
step:1207/2330 train_time:69987ms step_avg:57.98ms
step:1208/2330 train_time:70048ms step_avg:57.99ms
step:1209/2330 train_time:70105ms step_avg:57.99ms
step:1210/2330 train_time:70165ms step_avg:57.99ms
step:1211/2330 train_time:70223ms step_avg:57.99ms
step:1212/2330 train_time:70282ms step_avg:57.99ms
step:1213/2330 train_time:70339ms step_avg:57.99ms
step:1214/2330 train_time:70399ms step_avg:57.99ms
step:1215/2330 train_time:70455ms step_avg:57.99ms
step:1216/2330 train_time:70516ms step_avg:57.99ms
step:1217/2330 train_time:70573ms step_avg:57.99ms
step:1218/2330 train_time:70633ms step_avg:57.99ms
step:1219/2330 train_time:70689ms step_avg:57.99ms
step:1220/2330 train_time:70750ms step_avg:57.99ms
step:1221/2330 train_time:70807ms step_avg:57.99ms
step:1222/2330 train_time:70867ms step_avg:57.99ms
step:1223/2330 train_time:70924ms step_avg:57.99ms
step:1224/2330 train_time:70984ms step_avg:57.99ms
step:1225/2330 train_time:71041ms step_avg:57.99ms
step:1226/2330 train_time:71102ms step_avg:57.99ms
step:1227/2330 train_time:71158ms step_avg:57.99ms
step:1228/2330 train_time:71219ms step_avg:58.00ms
step:1229/2330 train_time:71277ms step_avg:58.00ms
step:1230/2330 train_time:71336ms step_avg:58.00ms
step:1231/2330 train_time:71393ms step_avg:58.00ms
step:1232/2330 train_time:71453ms step_avg:58.00ms
step:1233/2330 train_time:71510ms step_avg:58.00ms
step:1234/2330 train_time:71571ms step_avg:58.00ms
step:1235/2330 train_time:71628ms step_avg:58.00ms
step:1236/2330 train_time:71688ms step_avg:58.00ms
step:1237/2330 train_time:71744ms step_avg:58.00ms
step:1238/2330 train_time:71805ms step_avg:58.00ms
step:1239/2330 train_time:71862ms step_avg:58.00ms
step:1240/2330 train_time:71921ms step_avg:58.00ms
step:1241/2330 train_time:71979ms step_avg:58.00ms
step:1242/2330 train_time:72039ms step_avg:58.00ms
step:1243/2330 train_time:72095ms step_avg:58.00ms
step:1244/2330 train_time:72155ms step_avg:58.00ms
step:1245/2330 train_time:72213ms step_avg:58.00ms
step:1246/2330 train_time:72272ms step_avg:58.00ms
step:1247/2330 train_time:72329ms step_avg:58.00ms
step:1248/2330 train_time:72389ms step_avg:58.00ms
step:1249/2330 train_time:72446ms step_avg:58.00ms
step:1250/2330 train_time:72506ms step_avg:58.01ms
step:1250/2330 val_loss:3.9856 train_time:72587ms step_avg:58.07ms
step:1251/2330 train_time:72607ms step_avg:58.04ms
step:1252/2330 train_time:72626ms step_avg:58.01ms
step:1253/2330 train_time:72685ms step_avg:58.01ms
step:1254/2330 train_time:72749ms step_avg:58.01ms
step:1255/2330 train_time:72807ms step_avg:58.01ms
step:1256/2330 train_time:72867ms step_avg:58.02ms
step:1257/2330 train_time:72923ms step_avg:58.01ms
step:1258/2330 train_time:72983ms step_avg:58.02ms
step:1259/2330 train_time:73039ms step_avg:58.01ms
step:1260/2330 train_time:73100ms step_avg:58.02ms
step:1261/2330 train_time:73156ms step_avg:58.01ms
step:1262/2330 train_time:73216ms step_avg:58.02ms
step:1263/2330 train_time:73272ms step_avg:58.01ms
step:1264/2330 train_time:73332ms step_avg:58.02ms
step:1265/2330 train_time:73389ms step_avg:58.01ms
step:1266/2330 train_time:73448ms step_avg:58.02ms
step:1267/2330 train_time:73504ms step_avg:58.01ms
step:1268/2330 train_time:73565ms step_avg:58.02ms
step:1269/2330 train_time:73623ms step_avg:58.02ms
step:1270/2330 train_time:73684ms step_avg:58.02ms
step:1271/2330 train_time:73742ms step_avg:58.02ms
step:1272/2330 train_time:73805ms step_avg:58.02ms
step:1273/2330 train_time:73861ms step_avg:58.02ms
step:1274/2330 train_time:73922ms step_avg:58.02ms
step:1275/2330 train_time:73978ms step_avg:58.02ms
step:1276/2330 train_time:74040ms step_avg:58.03ms
step:1277/2330 train_time:74096ms step_avg:58.02ms
step:1278/2330 train_time:74156ms step_avg:58.03ms
step:1279/2330 train_time:74213ms step_avg:58.02ms
step:1280/2330 train_time:74272ms step_avg:58.03ms
step:1281/2330 train_time:74329ms step_avg:58.02ms
step:1282/2330 train_time:74388ms step_avg:58.03ms
step:1283/2330 train_time:74445ms step_avg:58.02ms
step:1284/2330 train_time:74505ms step_avg:58.03ms
step:1285/2330 train_time:74563ms step_avg:58.03ms
step:1286/2330 train_time:74622ms step_avg:58.03ms
step:1287/2330 train_time:74681ms step_avg:58.03ms
step:1288/2330 train_time:74742ms step_avg:58.03ms
step:1289/2330 train_time:74799ms step_avg:58.03ms
step:1290/2330 train_time:74859ms step_avg:58.03ms
step:1291/2330 train_time:74917ms step_avg:58.03ms
step:1292/2330 train_time:74977ms step_avg:58.03ms
step:1293/2330 train_time:75034ms step_avg:58.03ms
step:1294/2330 train_time:75094ms step_avg:58.03ms
step:1295/2330 train_time:75150ms step_avg:58.03ms
step:1296/2330 train_time:75211ms step_avg:58.03ms
step:1297/2330 train_time:75268ms step_avg:58.03ms
step:1298/2330 train_time:75327ms step_avg:58.03ms
step:1299/2330 train_time:75384ms step_avg:58.03ms
step:1300/2330 train_time:75443ms step_avg:58.03ms
step:1301/2330 train_time:75501ms step_avg:58.03ms
step:1302/2330 train_time:75560ms step_avg:58.03ms
step:1303/2330 train_time:75618ms step_avg:58.03ms
step:1304/2330 train_time:75678ms step_avg:58.04ms
step:1305/2330 train_time:75735ms step_avg:58.03ms
step:1306/2330 train_time:75798ms step_avg:58.04ms
step:1307/2330 train_time:75855ms step_avg:58.04ms
step:1308/2330 train_time:75916ms step_avg:58.04ms
step:1309/2330 train_time:75973ms step_avg:58.04ms
step:1310/2330 train_time:76033ms step_avg:58.04ms
step:1311/2330 train_time:76091ms step_avg:58.04ms
step:1312/2330 train_time:76150ms step_avg:58.04ms
step:1313/2330 train_time:76207ms step_avg:58.04ms
step:1314/2330 train_time:76266ms step_avg:58.04ms
step:1315/2330 train_time:76323ms step_avg:58.04ms
step:1316/2330 train_time:76382ms step_avg:58.04ms
step:1317/2330 train_time:76440ms step_avg:58.04ms
step:1318/2330 train_time:76499ms step_avg:58.04ms
step:1319/2330 train_time:76556ms step_avg:58.04ms
step:1320/2330 train_time:76616ms step_avg:58.04ms
step:1321/2330 train_time:76674ms step_avg:58.04ms
step:1322/2330 train_time:76734ms step_avg:58.04ms
step:1323/2330 train_time:76792ms step_avg:58.04ms
step:1324/2330 train_time:76852ms step_avg:58.05ms
step:1325/2330 train_time:76909ms step_avg:58.04ms
step:1326/2330 train_time:76969ms step_avg:58.05ms
step:1327/2330 train_time:77026ms step_avg:58.05ms
step:1328/2330 train_time:77086ms step_avg:58.05ms
step:1329/2330 train_time:77143ms step_avg:58.05ms
step:1330/2330 train_time:77202ms step_avg:58.05ms
step:1331/2330 train_time:77259ms step_avg:58.05ms
step:1332/2330 train_time:77319ms step_avg:58.05ms
step:1333/2330 train_time:77377ms step_avg:58.05ms
step:1334/2330 train_time:77437ms step_avg:58.05ms
step:1335/2330 train_time:77494ms step_avg:58.05ms
step:1336/2330 train_time:77553ms step_avg:58.05ms
step:1337/2330 train_time:77611ms step_avg:58.05ms
step:1338/2330 train_time:77671ms step_avg:58.05ms
step:1339/2330 train_time:77728ms step_avg:58.05ms
step:1340/2330 train_time:77788ms step_avg:58.05ms
step:1341/2330 train_time:77846ms step_avg:58.05ms
step:1342/2330 train_time:77906ms step_avg:58.05ms
step:1343/2330 train_time:77963ms step_avg:58.05ms
step:1344/2330 train_time:78023ms step_avg:58.05ms
step:1345/2330 train_time:78080ms step_avg:58.05ms
step:1346/2330 train_time:78140ms step_avg:58.05ms
step:1347/2330 train_time:78197ms step_avg:58.05ms
step:1348/2330 train_time:78256ms step_avg:58.05ms
step:1349/2330 train_time:78313ms step_avg:58.05ms
step:1350/2330 train_time:78374ms step_avg:58.05ms
step:1351/2330 train_time:78431ms step_avg:58.05ms
step:1352/2330 train_time:78492ms step_avg:58.06ms
step:1353/2330 train_time:78548ms step_avg:58.05ms
step:1354/2330 train_time:78608ms step_avg:58.06ms
step:1355/2330 train_time:78665ms step_avg:58.06ms
step:1356/2330 train_time:78724ms step_avg:58.06ms
step:1357/2330 train_time:78782ms step_avg:58.06ms
step:1358/2330 train_time:78842ms step_avg:58.06ms
step:1359/2330 train_time:78899ms step_avg:58.06ms
step:1360/2330 train_time:78959ms step_avg:58.06ms
step:1361/2330 train_time:79016ms step_avg:58.06ms
step:1362/2330 train_time:79077ms step_avg:58.06ms
step:1363/2330 train_time:79134ms step_avg:58.06ms
step:1364/2330 train_time:79193ms step_avg:58.06ms
step:1365/2330 train_time:79250ms step_avg:58.06ms
step:1366/2330 train_time:79311ms step_avg:58.06ms
step:1367/2330 train_time:79367ms step_avg:58.06ms
step:1368/2330 train_time:79427ms step_avg:58.06ms
step:1369/2330 train_time:79484ms step_avg:58.06ms
step:1370/2330 train_time:79544ms step_avg:58.06ms
step:1371/2330 train_time:79601ms step_avg:58.06ms
step:1372/2330 train_time:79661ms step_avg:58.06ms
step:1373/2330 train_time:79718ms step_avg:58.06ms
step:1374/2330 train_time:79778ms step_avg:58.06ms
step:1375/2330 train_time:79835ms step_avg:58.06ms
step:1376/2330 train_time:79896ms step_avg:58.06ms
step:1377/2330 train_time:79953ms step_avg:58.06ms
step:1378/2330 train_time:80014ms step_avg:58.07ms
step:1379/2330 train_time:80071ms step_avg:58.06ms
step:1380/2330 train_time:80130ms step_avg:58.07ms
step:1381/2330 train_time:80187ms step_avg:58.06ms
step:1382/2330 train_time:80247ms step_avg:58.07ms
step:1383/2330 train_time:80305ms step_avg:58.07ms
step:1384/2330 train_time:80364ms step_avg:58.07ms
step:1385/2330 train_time:80422ms step_avg:58.07ms
step:1386/2330 train_time:80481ms step_avg:58.07ms
step:1387/2330 train_time:80539ms step_avg:58.07ms
step:1388/2330 train_time:80598ms step_avg:58.07ms
step:1389/2330 train_time:80655ms step_avg:58.07ms
step:1390/2330 train_time:80716ms step_avg:58.07ms
step:1391/2330 train_time:80773ms step_avg:58.07ms
step:1392/2330 train_time:80833ms step_avg:58.07ms
step:1393/2330 train_time:80890ms step_avg:58.07ms
step:1394/2330 train_time:80949ms step_avg:58.07ms
step:1395/2330 train_time:81007ms step_avg:58.07ms
step:1396/2330 train_time:81066ms step_avg:58.07ms
step:1397/2330 train_time:81123ms step_avg:58.07ms
step:1398/2330 train_time:81183ms step_avg:58.07ms
step:1399/2330 train_time:81241ms step_avg:58.07ms
step:1400/2330 train_time:81300ms step_avg:58.07ms
step:1401/2330 train_time:81357ms step_avg:58.07ms
step:1402/2330 train_time:81417ms step_avg:58.07ms
step:1403/2330 train_time:81475ms step_avg:58.07ms
step:1404/2330 train_time:81535ms step_avg:58.07ms
step:1405/2330 train_time:81593ms step_avg:58.07ms
step:1406/2330 train_time:81653ms step_avg:58.07ms
step:1407/2330 train_time:81710ms step_avg:58.07ms
step:1408/2330 train_time:81770ms step_avg:58.08ms
step:1409/2330 train_time:81828ms step_avg:58.07ms
step:1410/2330 train_time:81887ms step_avg:58.08ms
step:1411/2330 train_time:81945ms step_avg:58.08ms
step:1412/2330 train_time:82004ms step_avg:58.08ms
step:1413/2330 train_time:82060ms step_avg:58.08ms
step:1414/2330 train_time:82121ms step_avg:58.08ms
step:1415/2330 train_time:82178ms step_avg:58.08ms
step:1416/2330 train_time:82240ms step_avg:58.08ms
step:1417/2330 train_time:82296ms step_avg:58.08ms
step:1418/2330 train_time:82357ms step_avg:58.08ms
step:1419/2330 train_time:82414ms step_avg:58.08ms
step:1420/2330 train_time:82475ms step_avg:58.08ms
step:1421/2330 train_time:82533ms step_avg:58.08ms
step:1422/2330 train_time:82594ms step_avg:58.08ms
step:1423/2330 train_time:82651ms step_avg:58.08ms
step:1424/2330 train_time:82711ms step_avg:58.08ms
step:1425/2330 train_time:82768ms step_avg:58.08ms
step:1426/2330 train_time:82828ms step_avg:58.08ms
step:1427/2330 train_time:82886ms step_avg:58.08ms
step:1428/2330 train_time:82946ms step_avg:58.09ms
step:1429/2330 train_time:83003ms step_avg:58.08ms
step:1430/2330 train_time:83063ms step_avg:58.09ms
step:1431/2330 train_time:83119ms step_avg:58.08ms
step:1432/2330 train_time:83180ms step_avg:58.09ms
step:1433/2330 train_time:83236ms step_avg:58.09ms
step:1434/2330 train_time:83297ms step_avg:58.09ms
step:1435/2330 train_time:83354ms step_avg:58.09ms
step:1436/2330 train_time:83414ms step_avg:58.09ms
step:1437/2330 train_time:83471ms step_avg:58.09ms
step:1438/2330 train_time:83531ms step_avg:58.09ms
step:1439/2330 train_time:83588ms step_avg:58.09ms
step:1440/2330 train_time:83648ms step_avg:58.09ms
step:1441/2330 train_time:83705ms step_avg:58.09ms
step:1442/2330 train_time:83765ms step_avg:58.09ms
step:1443/2330 train_time:83822ms step_avg:58.09ms
step:1444/2330 train_time:83882ms step_avg:58.09ms
step:1445/2330 train_time:83939ms step_avg:58.09ms
step:1446/2330 train_time:83999ms step_avg:58.09ms
step:1447/2330 train_time:84057ms step_avg:58.09ms
step:1448/2330 train_time:84116ms step_avg:58.09ms
step:1449/2330 train_time:84174ms step_avg:58.09ms
step:1450/2330 train_time:84235ms step_avg:58.09ms
step:1451/2330 train_time:84292ms step_avg:58.09ms
step:1452/2330 train_time:84352ms step_avg:58.09ms
step:1453/2330 train_time:84409ms step_avg:58.09ms
step:1454/2330 train_time:84469ms step_avg:58.09ms
step:1455/2330 train_time:84526ms step_avg:58.09ms
step:1456/2330 train_time:84585ms step_avg:58.09ms
step:1457/2330 train_time:84641ms step_avg:58.09ms
step:1458/2330 train_time:84702ms step_avg:58.09ms
step:1459/2330 train_time:84759ms step_avg:58.09ms
step:1460/2330 train_time:84820ms step_avg:58.10ms
step:1461/2330 train_time:84877ms step_avg:58.10ms
step:1462/2330 train_time:84937ms step_avg:58.10ms
step:1463/2330 train_time:84995ms step_avg:58.10ms
step:1464/2330 train_time:85055ms step_avg:58.10ms
step:1465/2330 train_time:85113ms step_avg:58.10ms
step:1466/2330 train_time:85173ms step_avg:58.10ms
step:1467/2330 train_time:85230ms step_avg:58.10ms
step:1468/2330 train_time:85290ms step_avg:58.10ms
step:1469/2330 train_time:85348ms step_avg:58.10ms
step:1470/2330 train_time:85407ms step_avg:58.10ms
step:1471/2330 train_time:85465ms step_avg:58.10ms
step:1472/2330 train_time:85524ms step_avg:58.10ms
step:1473/2330 train_time:85581ms step_avg:58.10ms
step:1474/2330 train_time:85641ms step_avg:58.10ms
step:1475/2330 train_time:85698ms step_avg:58.10ms
step:1476/2330 train_time:85758ms step_avg:58.10ms
step:1477/2330 train_time:85816ms step_avg:58.10ms
step:1478/2330 train_time:85875ms step_avg:58.10ms
step:1479/2330 train_time:85933ms step_avg:58.10ms
step:1480/2330 train_time:85993ms step_avg:58.10ms
step:1481/2330 train_time:86050ms step_avg:58.10ms
step:1482/2330 train_time:86110ms step_avg:58.10ms
step:1483/2330 train_time:86167ms step_avg:58.10ms
step:1484/2330 train_time:86227ms step_avg:58.10ms
step:1485/2330 train_time:86284ms step_avg:58.10ms
step:1486/2330 train_time:86344ms step_avg:58.10ms
step:1487/2330 train_time:86401ms step_avg:58.10ms
step:1488/2330 train_time:86460ms step_avg:58.10ms
step:1489/2330 train_time:86517ms step_avg:58.10ms
step:1490/2330 train_time:86577ms step_avg:58.11ms
step:1491/2330 train_time:86635ms step_avg:58.10ms
step:1492/2330 train_time:86694ms step_avg:58.11ms
step:1493/2330 train_time:86752ms step_avg:58.11ms
step:1494/2330 train_time:86812ms step_avg:58.11ms
step:1495/2330 train_time:86869ms step_avg:58.11ms
step:1496/2330 train_time:86929ms step_avg:58.11ms
step:1497/2330 train_time:86986ms step_avg:58.11ms
step:1498/2330 train_time:87045ms step_avg:58.11ms
step:1499/2330 train_time:87103ms step_avg:58.11ms
step:1500/2330 train_time:87162ms step_avg:58.11ms
step:1500/2330 val_loss:3.9015 train_time:87242ms step_avg:58.16ms
step:1501/2330 train_time:87263ms step_avg:58.14ms
step:1502/2330 train_time:87283ms step_avg:58.11ms
step:1503/2330 train_time:87340ms step_avg:58.11ms
step:1504/2330 train_time:87405ms step_avg:58.12ms
step:1505/2330 train_time:87463ms step_avg:58.12ms
step:1506/2330 train_time:87525ms step_avg:58.12ms
step:1507/2330 train_time:87582ms step_avg:58.12ms
step:1508/2330 train_time:87643ms step_avg:58.12ms
step:1509/2330 train_time:87700ms step_avg:58.12ms
step:1510/2330 train_time:87759ms step_avg:58.12ms
step:1511/2330 train_time:87816ms step_avg:58.12ms
step:1512/2330 train_time:87875ms step_avg:58.12ms
step:1513/2330 train_time:87932ms step_avg:58.12ms
step:1514/2330 train_time:87991ms step_avg:58.12ms
step:1515/2330 train_time:88047ms step_avg:58.12ms
step:1516/2330 train_time:88106ms step_avg:58.12ms
step:1517/2330 train_time:88162ms step_avg:58.12ms
step:1518/2330 train_time:88223ms step_avg:58.12ms
step:1519/2330 train_time:88282ms step_avg:58.12ms
step:1520/2330 train_time:88343ms step_avg:58.12ms
step:1521/2330 train_time:88401ms step_avg:58.12ms
step:1522/2330 train_time:88464ms step_avg:58.12ms
step:1523/2330 train_time:88521ms step_avg:58.12ms
step:1524/2330 train_time:88581ms step_avg:58.12ms
step:1525/2330 train_time:88639ms step_avg:58.12ms
step:1526/2330 train_time:88699ms step_avg:58.12ms
step:1527/2330 train_time:88755ms step_avg:58.12ms
step:1528/2330 train_time:88816ms step_avg:58.13ms
step:1529/2330 train_time:88874ms step_avg:58.13ms
step:1530/2330 train_time:88933ms step_avg:58.13ms
step:1531/2330 train_time:88989ms step_avg:58.12ms
step:1532/2330 train_time:89049ms step_avg:58.13ms
step:1533/2330 train_time:89106ms step_avg:58.13ms
step:1534/2330 train_time:89166ms step_avg:58.13ms
step:1535/2330 train_time:89224ms step_avg:58.13ms
step:1536/2330 train_time:89285ms step_avg:58.13ms
step:1537/2330 train_time:89341ms step_avg:58.13ms
step:1538/2330 train_time:89405ms step_avg:58.13ms
step:1539/2330 train_time:89463ms step_avg:58.13ms
step:1540/2330 train_time:89524ms step_avg:58.13ms
step:1541/2330 train_time:89582ms step_avg:58.13ms
step:1542/2330 train_time:89644ms step_avg:58.13ms
step:1543/2330 train_time:89701ms step_avg:58.13ms
step:1544/2330 train_time:89764ms step_avg:58.14ms
step:1545/2330 train_time:89821ms step_avg:58.14ms
step:1546/2330 train_time:89882ms step_avg:58.14ms
step:1547/2330 train_time:89939ms step_avg:58.14ms
step:1548/2330 train_time:90000ms step_avg:58.14ms
step:1549/2330 train_time:90058ms step_avg:58.14ms
step:1550/2330 train_time:90118ms step_avg:58.14ms
step:1551/2330 train_time:90176ms step_avg:58.14ms
step:1552/2330 train_time:90237ms step_avg:58.14ms
step:1553/2330 train_time:90295ms step_avg:58.14ms
step:1554/2330 train_time:90356ms step_avg:58.14ms
step:1555/2330 train_time:90414ms step_avg:58.14ms
step:1556/2330 train_time:90474ms step_avg:58.15ms
step:1557/2330 train_time:90532ms step_avg:58.15ms
step:1558/2330 train_time:90593ms step_avg:58.15ms
step:1559/2330 train_time:90650ms step_avg:58.15ms
step:1560/2330 train_time:90711ms step_avg:58.15ms
step:1561/2330 train_time:90769ms step_avg:58.15ms
step:1562/2330 train_time:90829ms step_avg:58.15ms
step:1563/2330 train_time:90886ms step_avg:58.15ms
step:1564/2330 train_time:90946ms step_avg:58.15ms
step:1565/2330 train_time:91003ms step_avg:58.15ms
step:1566/2330 train_time:91065ms step_avg:58.15ms
step:1567/2330 train_time:91123ms step_avg:58.15ms
step:1568/2330 train_time:91184ms step_avg:58.15ms
step:1569/2330 train_time:91242ms step_avg:58.15ms
step:1570/2330 train_time:91302ms step_avg:58.15ms
step:1571/2330 train_time:91360ms step_avg:58.15ms
step:1572/2330 train_time:91421ms step_avg:58.16ms
step:1573/2330 train_time:91480ms step_avg:58.16ms
step:1574/2330 train_time:91540ms step_avg:58.16ms
step:1575/2330 train_time:91598ms step_avg:58.16ms
step:1576/2330 train_time:91660ms step_avg:58.16ms
step:1577/2330 train_time:91718ms step_avg:58.16ms
step:1578/2330 train_time:91779ms step_avg:58.16ms
step:1579/2330 train_time:91836ms step_avg:58.16ms
step:1580/2330 train_time:91897ms step_avg:58.16ms
step:1581/2330 train_time:91954ms step_avg:58.16ms
step:1582/2330 train_time:92015ms step_avg:58.16ms
step:1583/2330 train_time:92072ms step_avg:58.16ms
step:1584/2330 train_time:92134ms step_avg:58.17ms
step:1585/2330 train_time:92190ms step_avg:58.16ms
step:1586/2330 train_time:92250ms step_avg:58.17ms
step:1587/2330 train_time:92307ms step_avg:58.16ms
step:1588/2330 train_time:92368ms step_avg:58.17ms
step:1589/2330 train_time:92425ms step_avg:58.17ms
step:1590/2330 train_time:92488ms step_avg:58.17ms
step:1591/2330 train_time:92544ms step_avg:58.17ms
step:1592/2330 train_time:92606ms step_avg:58.17ms
step:1593/2330 train_time:92663ms step_avg:58.17ms
step:1594/2330 train_time:92726ms step_avg:58.17ms
step:1595/2330 train_time:92783ms step_avg:58.17ms
step:1596/2330 train_time:92844ms step_avg:58.17ms
step:1597/2330 train_time:92901ms step_avg:58.17ms
step:1598/2330 train_time:92961ms step_avg:58.17ms
step:1599/2330 train_time:93019ms step_avg:58.17ms
step:1600/2330 train_time:93080ms step_avg:58.17ms
step:1601/2330 train_time:93139ms step_avg:58.18ms
step:1602/2330 train_time:93199ms step_avg:58.18ms
step:1603/2330 train_time:93257ms step_avg:58.18ms
step:1604/2330 train_time:93318ms step_avg:58.18ms
step:1605/2330 train_time:93377ms step_avg:58.18ms
step:1606/2330 train_time:93437ms step_avg:58.18ms
step:1607/2330 train_time:93495ms step_avg:58.18ms
step:1608/2330 train_time:93556ms step_avg:58.18ms
step:1609/2330 train_time:93614ms step_avg:58.18ms
step:1610/2330 train_time:93675ms step_avg:58.18ms
step:1611/2330 train_time:93732ms step_avg:58.18ms
step:1612/2330 train_time:93792ms step_avg:58.18ms
step:1613/2330 train_time:93849ms step_avg:58.18ms
step:1614/2330 train_time:93910ms step_avg:58.18ms
step:1615/2330 train_time:93967ms step_avg:58.18ms
step:1616/2330 train_time:94027ms step_avg:58.19ms
step:1617/2330 train_time:94084ms step_avg:58.18ms
step:1618/2330 train_time:94146ms step_avg:58.19ms
step:1619/2330 train_time:94203ms step_avg:58.19ms
step:1620/2330 train_time:94265ms step_avg:58.19ms
step:1621/2330 train_time:94322ms step_avg:58.19ms
step:1622/2330 train_time:94384ms step_avg:58.19ms
step:1623/2330 train_time:94442ms step_avg:58.19ms
step:1624/2330 train_time:94502ms step_avg:58.19ms
step:1625/2330 train_time:94560ms step_avg:58.19ms
step:1626/2330 train_time:94622ms step_avg:58.19ms
step:1627/2330 train_time:94680ms step_avg:58.19ms
step:1628/2330 train_time:94740ms step_avg:58.19ms
step:1629/2330 train_time:94799ms step_avg:58.19ms
step:1630/2330 train_time:94859ms step_avg:58.20ms
step:1631/2330 train_time:94917ms step_avg:58.20ms
step:1632/2330 train_time:94977ms step_avg:58.20ms
step:1633/2330 train_time:95035ms step_avg:58.20ms
step:1634/2330 train_time:95095ms step_avg:58.20ms
step:1635/2330 train_time:95152ms step_avg:58.20ms
step:1636/2330 train_time:95213ms step_avg:58.20ms
step:1637/2330 train_time:95271ms step_avg:58.20ms
step:1638/2330 train_time:95331ms step_avg:58.20ms
step:1639/2330 train_time:95388ms step_avg:58.20ms
step:1640/2330 train_time:95449ms step_avg:58.20ms
step:1641/2330 train_time:95506ms step_avg:58.20ms
step:1642/2330 train_time:95567ms step_avg:58.20ms
step:1643/2330 train_time:95624ms step_avg:58.20ms
step:1644/2330 train_time:95686ms step_avg:58.20ms
step:1645/2330 train_time:95743ms step_avg:58.20ms
step:1646/2330 train_time:95805ms step_avg:58.21ms
step:1647/2330 train_time:95864ms step_avg:58.21ms
step:1648/2330 train_time:95925ms step_avg:58.21ms
step:1649/2330 train_time:95983ms step_avg:58.21ms
step:1650/2330 train_time:96044ms step_avg:58.21ms
step:1651/2330 train_time:96102ms step_avg:58.21ms
step:1652/2330 train_time:96163ms step_avg:58.21ms
step:1653/2330 train_time:96221ms step_avg:58.21ms
step:1654/2330 train_time:96281ms step_avg:58.21ms
step:1655/2330 train_time:96339ms step_avg:58.21ms
step:1656/2330 train_time:96399ms step_avg:58.21ms
step:1657/2330 train_time:96457ms step_avg:58.21ms
step:1658/2330 train_time:96518ms step_avg:58.21ms
step:1659/2330 train_time:96576ms step_avg:58.21ms
step:1660/2330 train_time:96637ms step_avg:58.21ms
step:1661/2330 train_time:96694ms step_avg:58.21ms
step:1662/2330 train_time:96755ms step_avg:58.22ms
step:1663/2330 train_time:96812ms step_avg:58.22ms
step:1664/2330 train_time:96872ms step_avg:58.22ms
step:1665/2330 train_time:96929ms step_avg:58.22ms
step:1666/2330 train_time:96990ms step_avg:58.22ms
step:1667/2330 train_time:97047ms step_avg:58.22ms
step:1668/2330 train_time:97108ms step_avg:58.22ms
step:1669/2330 train_time:97166ms step_avg:58.22ms
step:1670/2330 train_time:97226ms step_avg:58.22ms
step:1671/2330 train_time:97283ms step_avg:58.22ms
step:1672/2330 train_time:97344ms step_avg:58.22ms
step:1673/2330 train_time:97402ms step_avg:58.22ms
step:1674/2330 train_time:97463ms step_avg:58.22ms
step:1675/2330 train_time:97521ms step_avg:58.22ms
step:1676/2330 train_time:97582ms step_avg:58.22ms
step:1677/2330 train_time:97640ms step_avg:58.22ms
step:1678/2330 train_time:97701ms step_avg:58.22ms
step:1679/2330 train_time:97759ms step_avg:58.22ms
step:1680/2330 train_time:97819ms step_avg:58.23ms
step:1681/2330 train_time:97876ms step_avg:58.23ms
step:1682/2330 train_time:97938ms step_avg:58.23ms
step:1683/2330 train_time:97995ms step_avg:58.23ms
step:1684/2330 train_time:98056ms step_avg:58.23ms
step:1685/2330 train_time:98114ms step_avg:58.23ms
step:1686/2330 train_time:98174ms step_avg:58.23ms
step:1687/2330 train_time:98232ms step_avg:58.23ms
step:1688/2330 train_time:98292ms step_avg:58.23ms
step:1689/2330 train_time:98349ms step_avg:58.23ms
step:1690/2330 train_time:98410ms step_avg:58.23ms
step:1691/2330 train_time:98467ms step_avg:58.23ms
step:1692/2330 train_time:98528ms step_avg:58.23ms
step:1693/2330 train_time:98584ms step_avg:58.23ms
step:1694/2330 train_time:98646ms step_avg:58.23ms
step:1695/2330 train_time:98703ms step_avg:58.23ms
step:1696/2330 train_time:98765ms step_avg:58.23ms
step:1697/2330 train_time:98822ms step_avg:58.23ms
step:1698/2330 train_time:98884ms step_avg:58.24ms
step:1699/2330 train_time:98942ms step_avg:58.24ms
step:1700/2330 train_time:99003ms step_avg:58.24ms
step:1701/2330 train_time:99060ms step_avg:58.24ms
step:1702/2330 train_time:99122ms step_avg:58.24ms
step:1703/2330 train_time:99180ms step_avg:58.24ms
step:1704/2330 train_time:99241ms step_avg:58.24ms
step:1705/2330 train_time:99298ms step_avg:58.24ms
step:1706/2330 train_time:99358ms step_avg:58.24ms
step:1707/2330 train_time:99415ms step_avg:58.24ms
step:1708/2330 train_time:99477ms step_avg:58.24ms
step:1709/2330 train_time:99535ms step_avg:58.24ms
step:1710/2330 train_time:99595ms step_avg:58.24ms
step:1711/2330 train_time:99653ms step_avg:58.24ms
step:1712/2330 train_time:99713ms step_avg:58.24ms
step:1713/2330 train_time:99772ms step_avg:58.24ms
step:1714/2330 train_time:99831ms step_avg:58.24ms
step:1715/2330 train_time:99888ms step_avg:58.24ms
step:1716/2330 train_time:99949ms step_avg:58.25ms
step:1717/2330 train_time:100006ms step_avg:58.24ms
step:1718/2330 train_time:100068ms step_avg:58.25ms
step:1719/2330 train_time:100125ms step_avg:58.25ms
step:1720/2330 train_time:100186ms step_avg:58.25ms
step:1721/2330 train_time:100243ms step_avg:58.25ms
step:1722/2330 train_time:100305ms step_avg:58.25ms
step:1723/2330 train_time:100363ms step_avg:58.25ms
step:1724/2330 train_time:100424ms step_avg:58.25ms
step:1725/2330 train_time:100481ms step_avg:58.25ms
step:1726/2330 train_time:100543ms step_avg:58.25ms
step:1727/2330 train_time:100601ms step_avg:58.25ms
step:1728/2330 train_time:100662ms step_avg:58.25ms
step:1729/2330 train_time:100720ms step_avg:58.25ms
step:1730/2330 train_time:100781ms step_avg:58.25ms
step:1731/2330 train_time:100839ms step_avg:58.25ms
step:1732/2330 train_time:100899ms step_avg:58.26ms
step:1733/2330 train_time:100958ms step_avg:58.26ms
step:1734/2330 train_time:101018ms step_avg:58.26ms
step:1735/2330 train_time:101076ms step_avg:58.26ms
step:1736/2330 train_time:101137ms step_avg:58.26ms
step:1737/2330 train_time:101195ms step_avg:58.26ms
step:1738/2330 train_time:101255ms step_avg:58.26ms
step:1739/2330 train_time:101312ms step_avg:58.26ms
step:1740/2330 train_time:101374ms step_avg:58.26ms
step:1741/2330 train_time:101431ms step_avg:58.26ms
step:1742/2330 train_time:101492ms step_avg:58.26ms
step:1743/2330 train_time:101549ms step_avg:58.26ms
step:1744/2330 train_time:101610ms step_avg:58.26ms
step:1745/2330 train_time:101667ms step_avg:58.26ms
step:1746/2330 train_time:101727ms step_avg:58.26ms
step:1747/2330 train_time:101784ms step_avg:58.26ms
step:1748/2330 train_time:101844ms step_avg:58.26ms
step:1749/2330 train_time:101902ms step_avg:58.26ms
step:1750/2330 train_time:101963ms step_avg:58.26ms
step:1750/2330 val_loss:3.8183 train_time:102047ms step_avg:58.31ms
step:1751/2330 train_time:102067ms step_avg:58.29ms
step:1752/2330 train_time:102087ms step_avg:58.27ms
step:1753/2330 train_time:102140ms step_avg:58.27ms
step:1754/2330 train_time:102208ms step_avg:58.27ms
step:1755/2330 train_time:102264ms step_avg:58.27ms
step:1756/2330 train_time:102331ms step_avg:58.28ms
step:1757/2330 train_time:102388ms step_avg:58.27ms
step:1758/2330 train_time:102450ms step_avg:58.28ms
step:1759/2330 train_time:102506ms step_avg:58.28ms
step:1760/2330 train_time:102566ms step_avg:58.28ms
step:1761/2330 train_time:102624ms step_avg:58.28ms
step:1762/2330 train_time:102683ms step_avg:58.28ms
step:1763/2330 train_time:102740ms step_avg:58.28ms
step:1764/2330 train_time:102799ms step_avg:58.28ms
step:1765/2330 train_time:102855ms step_avg:58.27ms
step:1766/2330 train_time:102915ms step_avg:58.28ms
step:1767/2330 train_time:102975ms step_avg:58.28ms
step:1768/2330 train_time:103038ms step_avg:58.28ms
step:1769/2330 train_time:103097ms step_avg:58.28ms
step:1770/2330 train_time:103157ms step_avg:58.28ms
step:1771/2330 train_time:103216ms step_avg:58.28ms
step:1772/2330 train_time:103277ms step_avg:58.28ms
step:1773/2330 train_time:103334ms step_avg:58.28ms
step:1774/2330 train_time:103395ms step_avg:58.28ms
step:1775/2330 train_time:103451ms step_avg:58.28ms
step:1776/2330 train_time:103513ms step_avg:58.28ms
step:1777/2330 train_time:103569ms step_avg:58.28ms
step:1778/2330 train_time:103632ms step_avg:58.29ms
step:1779/2330 train_time:103689ms step_avg:58.29ms
step:1780/2330 train_time:103749ms step_avg:58.29ms
step:1781/2330 train_time:103806ms step_avg:58.29ms
step:1782/2330 train_time:103866ms step_avg:58.29ms
step:1783/2330 train_time:103926ms step_avg:58.29ms
step:1784/2330 train_time:103987ms step_avg:58.29ms
step:1785/2330 train_time:104046ms step_avg:58.29ms
step:1786/2330 train_time:104106ms step_avg:58.29ms
step:1787/2330 train_time:104165ms step_avg:58.29ms
step:1788/2330 train_time:104226ms step_avg:58.29ms
step:1789/2330 train_time:104285ms step_avg:58.29ms
step:1790/2330 train_time:104346ms step_avg:58.29ms
step:1791/2330 train_time:104403ms step_avg:58.29ms
step:1792/2330 train_time:104464ms step_avg:58.29ms
step:1793/2330 train_time:104521ms step_avg:58.29ms
step:1794/2330 train_time:104582ms step_avg:58.30ms
step:1795/2330 train_time:104638ms step_avg:58.29ms
step:1796/2330 train_time:104699ms step_avg:58.30ms
step:1797/2330 train_time:104756ms step_avg:58.29ms
step:1798/2330 train_time:104817ms step_avg:58.30ms
step:1799/2330 train_time:104874ms step_avg:58.30ms
step:1800/2330 train_time:104936ms step_avg:58.30ms
step:1801/2330 train_time:104994ms step_avg:58.30ms
step:1802/2330 train_time:105055ms step_avg:58.30ms
step:1803/2330 train_time:105112ms step_avg:58.30ms
step:1804/2330 train_time:105173ms step_avg:58.30ms
step:1805/2330 train_time:105231ms step_avg:58.30ms
step:1806/2330 train_time:105293ms step_avg:58.30ms
step:1807/2330 train_time:105350ms step_avg:58.30ms
step:1808/2330 train_time:105411ms step_avg:58.30ms
step:1809/2330 train_time:105469ms step_avg:58.30ms
step:1810/2330 train_time:105529ms step_avg:58.30ms
step:1811/2330 train_time:105588ms step_avg:58.30ms
step:1812/2330 train_time:105648ms step_avg:58.30ms
step:1813/2330 train_time:105705ms step_avg:58.30ms
step:1814/2330 train_time:105767ms step_avg:58.31ms
step:1815/2330 train_time:105825ms step_avg:58.31ms
step:1816/2330 train_time:105885ms step_avg:58.31ms
step:1817/2330 train_time:105942ms step_avg:58.31ms
step:1818/2330 train_time:106003ms step_avg:58.31ms
step:1819/2330 train_time:106061ms step_avg:58.31ms
step:1820/2330 train_time:106122ms step_avg:58.31ms
step:1821/2330 train_time:106180ms step_avg:58.31ms
step:1822/2330 train_time:106241ms step_avg:58.31ms
step:1823/2330 train_time:106298ms step_avg:58.31ms
step:1824/2330 train_time:106358ms step_avg:58.31ms
step:1825/2330 train_time:106416ms step_avg:58.31ms
step:1826/2330 train_time:106476ms step_avg:58.31ms
step:1827/2330 train_time:106533ms step_avg:58.31ms
step:1828/2330 train_time:106595ms step_avg:58.31ms
step:1829/2330 train_time:106653ms step_avg:58.31ms
step:1830/2330 train_time:106714ms step_avg:58.31ms
step:1831/2330 train_time:106771ms step_avg:58.31ms
step:1832/2330 train_time:106834ms step_avg:58.32ms
step:1833/2330 train_time:106892ms step_avg:58.32ms
step:1834/2330 train_time:106952ms step_avg:58.32ms
step:1835/2330 train_time:107009ms step_avg:58.32ms
step:1836/2330 train_time:107071ms step_avg:58.32ms
step:1837/2330 train_time:107128ms step_avg:58.32ms
step:1838/2330 train_time:107189ms step_avg:58.32ms
step:1839/2330 train_time:107248ms step_avg:58.32ms
step:1840/2330 train_time:107308ms step_avg:58.32ms
step:1841/2330 train_time:107367ms step_avg:58.32ms
step:1842/2330 train_time:107428ms step_avg:58.32ms
step:1843/2330 train_time:107486ms step_avg:58.32ms
step:1844/2330 train_time:107547ms step_avg:58.32ms
step:1845/2330 train_time:107604ms step_avg:58.32ms
step:1846/2330 train_time:107666ms step_avg:58.32ms
step:1847/2330 train_time:107723ms step_avg:58.32ms
step:1848/2330 train_time:107784ms step_avg:58.32ms
step:1849/2330 train_time:107842ms step_avg:58.32ms
step:1850/2330 train_time:107902ms step_avg:58.33ms
step:1851/2330 train_time:107959ms step_avg:58.32ms
step:1852/2330 train_time:108021ms step_avg:58.33ms
step:1853/2330 train_time:108078ms step_avg:58.33ms
step:1854/2330 train_time:108139ms step_avg:58.33ms
step:1855/2330 train_time:108197ms step_avg:58.33ms
step:1856/2330 train_time:108259ms step_avg:58.33ms
step:1857/2330 train_time:108316ms step_avg:58.33ms
step:1858/2330 train_time:108379ms step_avg:58.33ms
step:1859/2330 train_time:108436ms step_avg:58.33ms
step:1860/2330 train_time:108498ms step_avg:58.33ms
step:1861/2330 train_time:108554ms step_avg:58.33ms
step:1862/2330 train_time:108616ms step_avg:58.33ms
step:1863/2330 train_time:108674ms step_avg:58.33ms
step:1864/2330 train_time:108735ms step_avg:58.33ms
step:1865/2330 train_time:108793ms step_avg:58.33ms
step:1866/2330 train_time:108853ms step_avg:58.33ms
step:1867/2330 train_time:108911ms step_avg:58.33ms
step:1868/2330 train_time:108971ms step_avg:58.34ms
step:1869/2330 train_time:109030ms step_avg:58.34ms
step:1870/2330 train_time:109090ms step_avg:58.34ms
step:1871/2330 train_time:109147ms step_avg:58.34ms
step:1872/2330 train_time:109209ms step_avg:58.34ms
step:1873/2330 train_time:109267ms step_avg:58.34ms
step:1874/2330 train_time:109328ms step_avg:58.34ms
step:1875/2330 train_time:109386ms step_avg:58.34ms
step:1876/2330 train_time:109447ms step_avg:58.34ms
step:1877/2330 train_time:109505ms step_avg:58.34ms
step:1878/2330 train_time:109566ms step_avg:58.34ms
step:1879/2330 train_time:109623ms step_avg:58.34ms
step:1880/2330 train_time:109683ms step_avg:58.34ms
step:1881/2330 train_time:109741ms step_avg:58.34ms
step:1882/2330 train_time:109801ms step_avg:58.34ms
step:1883/2330 train_time:109857ms step_avg:58.34ms
step:1884/2330 train_time:109918ms step_avg:58.34ms
step:1885/2330 train_time:109976ms step_avg:58.34ms
step:1886/2330 train_time:110037ms step_avg:58.34ms
step:1887/2330 train_time:110095ms step_avg:58.34ms
step:1888/2330 train_time:110155ms step_avg:58.34ms
step:1889/2330 train_time:110213ms step_avg:58.34ms
step:1890/2330 train_time:110274ms step_avg:58.35ms
step:1891/2330 train_time:110332ms step_avg:58.35ms
step:1892/2330 train_time:110394ms step_avg:58.35ms
step:1893/2330 train_time:110452ms step_avg:58.35ms
step:1894/2330 train_time:110513ms step_avg:58.35ms
step:1895/2330 train_time:110571ms step_avg:58.35ms
step:1896/2330 train_time:110632ms step_avg:58.35ms
step:1897/2330 train_time:110690ms step_avg:58.35ms
step:1898/2330 train_time:110750ms step_avg:58.35ms
step:1899/2330 train_time:110808ms step_avg:58.35ms
step:1900/2330 train_time:110868ms step_avg:58.35ms
step:1901/2330 train_time:110927ms step_avg:58.35ms
step:1902/2330 train_time:110987ms step_avg:58.35ms
step:1903/2330 train_time:111046ms step_avg:58.35ms
step:1904/2330 train_time:111106ms step_avg:58.35ms
step:1905/2330 train_time:111164ms step_avg:58.35ms
step:1906/2330 train_time:111224ms step_avg:58.35ms
step:1907/2330 train_time:111282ms step_avg:58.35ms
step:1908/2330 train_time:111343ms step_avg:58.36ms
step:1909/2330 train_time:111400ms step_avg:58.36ms
step:1910/2330 train_time:111463ms step_avg:58.36ms
step:1911/2330 train_time:111520ms step_avg:58.36ms
step:1912/2330 train_time:111583ms step_avg:58.36ms
step:1913/2330 train_time:111640ms step_avg:58.36ms
step:1914/2330 train_time:111701ms step_avg:58.36ms
step:1915/2330 train_time:111758ms step_avg:58.36ms
step:1916/2330 train_time:111819ms step_avg:58.36ms
step:1917/2330 train_time:111876ms step_avg:58.36ms
step:1918/2330 train_time:111937ms step_avg:58.36ms
step:1919/2330 train_time:111994ms step_avg:58.36ms
step:1920/2330 train_time:112055ms step_avg:58.36ms
step:1921/2330 train_time:112113ms step_avg:58.36ms
step:1922/2330 train_time:112173ms step_avg:58.36ms
step:1923/2330 train_time:112231ms step_avg:58.36ms
step:1924/2330 train_time:112293ms step_avg:58.36ms
step:1925/2330 train_time:112351ms step_avg:58.36ms
step:1926/2330 train_time:112412ms step_avg:58.37ms
step:1927/2330 train_time:112469ms step_avg:58.37ms
step:1928/2330 train_time:112531ms step_avg:58.37ms
step:1929/2330 train_time:112589ms step_avg:58.37ms
step:1930/2330 train_time:112649ms step_avg:58.37ms
step:1931/2330 train_time:112707ms step_avg:58.37ms
step:1932/2330 train_time:112767ms step_avg:58.37ms
step:1933/2330 train_time:112825ms step_avg:58.37ms
step:1934/2330 train_time:112885ms step_avg:58.37ms
step:1935/2330 train_time:112942ms step_avg:58.37ms
step:1936/2330 train_time:113003ms step_avg:58.37ms
step:1937/2330 train_time:113060ms step_avg:58.37ms
step:1938/2330 train_time:113122ms step_avg:58.37ms
step:1939/2330 train_time:113178ms step_avg:58.37ms
step:1940/2330 train_time:113241ms step_avg:58.37ms
step:1941/2330 train_time:113298ms step_avg:58.37ms
step:1942/2330 train_time:113358ms step_avg:58.37ms
step:1943/2330 train_time:113415ms step_avg:58.37ms
step:1944/2330 train_time:113476ms step_avg:58.37ms
step:1945/2330 train_time:113534ms step_avg:58.37ms
step:1946/2330 train_time:113594ms step_avg:58.37ms
step:1947/2330 train_time:113652ms step_avg:58.37ms
step:1948/2330 train_time:113714ms step_avg:58.37ms
step:1949/2330 train_time:113772ms step_avg:58.37ms
step:1950/2330 train_time:113833ms step_avg:58.38ms
step:1951/2330 train_time:113891ms step_avg:58.38ms
step:1952/2330 train_time:113951ms step_avg:58.38ms
step:1953/2330 train_time:114009ms step_avg:58.38ms
step:1954/2330 train_time:114070ms step_avg:58.38ms
step:1955/2330 train_time:114127ms step_avg:58.38ms
step:1956/2330 train_time:114188ms step_avg:58.38ms
step:1957/2330 train_time:114246ms step_avg:58.38ms
step:1958/2330 train_time:114306ms step_avg:58.38ms
step:1959/2330 train_time:114363ms step_avg:58.38ms
step:1960/2330 train_time:114425ms step_avg:58.38ms
step:1961/2330 train_time:114483ms step_avg:58.38ms
step:1962/2330 train_time:114543ms step_avg:58.38ms
step:1963/2330 train_time:114600ms step_avg:58.38ms
step:1964/2330 train_time:114662ms step_avg:58.38ms
step:1965/2330 train_time:114719ms step_avg:58.38ms
step:1966/2330 train_time:114781ms step_avg:58.38ms
step:1967/2330 train_time:114838ms step_avg:58.38ms
step:1968/2330 train_time:114899ms step_avg:58.38ms
step:1969/2330 train_time:114956ms step_avg:58.38ms
step:1970/2330 train_time:115017ms step_avg:58.38ms
step:1971/2330 train_time:115074ms step_avg:58.38ms
step:1972/2330 train_time:115136ms step_avg:58.39ms
step:1973/2330 train_time:115194ms step_avg:58.38ms
step:1974/2330 train_time:115255ms step_avg:58.39ms
step:1975/2330 train_time:115313ms step_avg:58.39ms
step:1976/2330 train_time:115374ms step_avg:58.39ms
step:1977/2330 train_time:115431ms step_avg:58.39ms
step:1978/2330 train_time:115493ms step_avg:58.39ms
step:1979/2330 train_time:115551ms step_avg:58.39ms
step:1980/2330 train_time:115612ms step_avg:58.39ms
step:1981/2330 train_time:115671ms step_avg:58.39ms
step:1982/2330 train_time:115731ms step_avg:58.39ms
step:1983/2330 train_time:115789ms step_avg:58.39ms
step:1984/2330 train_time:115850ms step_avg:58.39ms
step:1985/2330 train_time:115907ms step_avg:58.39ms
step:1986/2330 train_time:115967ms step_avg:58.39ms
step:1987/2330 train_time:116025ms step_avg:58.39ms
step:1988/2330 train_time:116086ms step_avg:58.39ms
step:1989/2330 train_time:116144ms step_avg:58.39ms
step:1990/2330 train_time:116204ms step_avg:58.39ms
step:1991/2330 train_time:116262ms step_avg:58.39ms
step:1992/2330 train_time:116324ms step_avg:58.40ms
step:1993/2330 train_time:116381ms step_avg:58.39ms
step:1994/2330 train_time:116442ms step_avg:58.40ms
step:1995/2330 train_time:116499ms step_avg:58.40ms
step:1996/2330 train_time:116561ms step_avg:58.40ms
step:1997/2330 train_time:116619ms step_avg:58.40ms
step:1998/2330 train_time:116679ms step_avg:58.40ms
step:1999/2330 train_time:116735ms step_avg:58.40ms
step:2000/2330 train_time:116797ms step_avg:58.40ms
step:2000/2330 val_loss:3.7552 train_time:116879ms step_avg:58.44ms
step:2001/2330 train_time:116898ms step_avg:58.42ms
step:2002/2330 train_time:116918ms step_avg:58.40ms
step:2003/2330 train_time:116975ms step_avg:58.40ms
step:2004/2330 train_time:117041ms step_avg:58.40ms
step:2005/2330 train_time:117097ms step_avg:58.40ms
step:2006/2330 train_time:117160ms step_avg:58.40ms
step:2007/2330 train_time:117216ms step_avg:58.40ms
step:2008/2330 train_time:117276ms step_avg:58.40ms
step:2009/2330 train_time:117332ms step_avg:58.40ms
step:2010/2330 train_time:117393ms step_avg:58.40ms
step:2011/2330 train_time:117451ms step_avg:58.40ms
step:2012/2330 train_time:117510ms step_avg:58.40ms
step:2013/2330 train_time:117567ms step_avg:58.40ms
step:2014/2330 train_time:117627ms step_avg:58.40ms
step:2015/2330 train_time:117684ms step_avg:58.40ms
step:2016/2330 train_time:117744ms step_avg:58.40ms
step:2017/2330 train_time:117803ms step_avg:58.40ms
step:2018/2330 train_time:117864ms step_avg:58.41ms
step:2019/2330 train_time:117922ms step_avg:58.41ms
step:2020/2330 train_time:117986ms step_avg:58.41ms
step:2021/2330 train_time:118045ms step_avg:58.41ms
step:2022/2330 train_time:118107ms step_avg:58.41ms
step:2023/2330 train_time:118164ms step_avg:58.41ms
step:2024/2330 train_time:118226ms step_avg:58.41ms
step:2025/2330 train_time:118282ms step_avg:58.41ms
step:2026/2330 train_time:118344ms step_avg:58.41ms
step:2027/2330 train_time:118400ms step_avg:58.41ms
step:2028/2330 train_time:118461ms step_avg:58.41ms
step:2029/2330 train_time:118518ms step_avg:58.41ms
step:2030/2330 train_time:118578ms step_avg:58.41ms
step:2031/2330 train_time:118635ms step_avg:58.41ms
step:2032/2330 train_time:118696ms step_avg:58.41ms
step:2033/2330 train_time:118753ms step_avg:58.41ms
step:2034/2330 train_time:118813ms step_avg:58.41ms
step:2035/2330 train_time:118871ms step_avg:58.41ms
step:2036/2330 train_time:118934ms step_avg:58.42ms
step:2037/2330 train_time:118992ms step_avg:58.42ms
step:2038/2330 train_time:119055ms step_avg:58.42ms
step:2039/2330 train_time:119113ms step_avg:58.42ms
step:2040/2330 train_time:119173ms step_avg:58.42ms
step:2041/2330 train_time:119231ms step_avg:58.42ms
step:2042/2330 train_time:119292ms step_avg:58.42ms
step:2043/2330 train_time:119350ms step_avg:58.42ms
step:2044/2330 train_time:119411ms step_avg:58.42ms
step:2045/2330 train_time:119468ms step_avg:58.42ms
step:2046/2330 train_time:119529ms step_avg:58.42ms
step:2047/2330 train_time:119586ms step_avg:58.42ms
step:2048/2330 train_time:119647ms step_avg:58.42ms
step:2049/2330 train_time:119705ms step_avg:58.42ms
step:2050/2330 train_time:119765ms step_avg:58.42ms
step:2051/2330 train_time:119822ms step_avg:58.42ms
step:2052/2330 train_time:119884ms step_avg:58.42ms
step:2053/2330 train_time:119942ms step_avg:58.42ms
step:2054/2330 train_time:120005ms step_avg:58.42ms
step:2055/2330 train_time:120062ms step_avg:58.42ms
step:2056/2330 train_time:120124ms step_avg:58.43ms
step:2057/2330 train_time:120181ms step_avg:58.43ms
step:2058/2330 train_time:120244ms step_avg:58.43ms
step:2059/2330 train_time:120301ms step_avg:58.43ms
step:2060/2330 train_time:120362ms step_avg:58.43ms
step:2061/2330 train_time:120418ms step_avg:58.43ms
step:2062/2330 train_time:120480ms step_avg:58.43ms
step:2063/2330 train_time:120537ms step_avg:58.43ms
step:2064/2330 train_time:120597ms step_avg:58.43ms
step:2065/2330 train_time:120654ms step_avg:58.43ms
step:2066/2330 train_time:120714ms step_avg:58.43ms
step:2067/2330 train_time:120772ms step_avg:58.43ms
step:2068/2330 train_time:120832ms step_avg:58.43ms
step:2069/2330 train_time:120890ms step_avg:58.43ms
step:2070/2330 train_time:120953ms step_avg:58.43ms
step:2071/2330 train_time:121011ms step_avg:58.43ms
step:2072/2330 train_time:121071ms step_avg:58.43ms
step:2073/2330 train_time:121130ms step_avg:58.43ms
step:2074/2330 train_time:121191ms step_avg:58.43ms
step:2075/2330 train_time:121250ms step_avg:58.43ms
step:2076/2330 train_time:121310ms step_avg:58.43ms
step:2077/2330 train_time:121368ms step_avg:58.43ms
step:2078/2330 train_time:121429ms step_avg:58.44ms
step:2079/2330 train_time:121487ms step_avg:58.44ms
step:2080/2330 train_time:121546ms step_avg:58.44ms
step:2081/2330 train_time:121603ms step_avg:58.43ms
step:2082/2330 train_time:121666ms step_avg:58.44ms
step:2083/2330 train_time:121724ms step_avg:58.44ms
step:2084/2330 train_time:121785ms step_avg:58.44ms
step:2085/2330 train_time:121843ms step_avg:58.44ms
step:2086/2330 train_time:121905ms step_avg:58.44ms
step:2087/2330 train_time:121963ms step_avg:58.44ms
step:2088/2330 train_time:122024ms step_avg:58.44ms
step:2089/2330 train_time:122080ms step_avg:58.44ms
step:2090/2330 train_time:122143ms step_avg:58.44ms
step:2091/2330 train_time:122200ms step_avg:58.44ms
step:2092/2330 train_time:122262ms step_avg:58.44ms
step:2093/2330 train_time:122319ms step_avg:58.44ms
step:2094/2330 train_time:122382ms step_avg:58.44ms
step:2095/2330 train_time:122439ms step_avg:58.44ms
step:2096/2330 train_time:122500ms step_avg:58.44ms
step:2097/2330 train_time:122557ms step_avg:58.44ms
step:2098/2330 train_time:122617ms step_avg:58.44ms
step:2099/2330 train_time:122674ms step_avg:58.44ms
step:2100/2330 train_time:122736ms step_avg:58.45ms
step:2101/2330 train_time:122793ms step_avg:58.44ms
step:2102/2330 train_time:122855ms step_avg:58.45ms
step:2103/2330 train_time:122912ms step_avg:58.45ms
step:2104/2330 train_time:122973ms step_avg:58.45ms
step:2105/2330 train_time:123032ms step_avg:58.45ms
step:2106/2330 train_time:123093ms step_avg:58.45ms
step:2107/2330 train_time:123151ms step_avg:58.45ms
step:2108/2330 train_time:123213ms step_avg:58.45ms
step:2109/2330 train_time:123270ms step_avg:58.45ms
step:2110/2330 train_time:123331ms step_avg:58.45ms
step:2111/2330 train_time:123388ms step_avg:58.45ms
step:2112/2330 train_time:123450ms step_avg:58.45ms
step:2113/2330 train_time:123508ms step_avg:58.45ms
step:2114/2330 train_time:123568ms step_avg:58.45ms
step:2115/2330 train_time:123627ms step_avg:58.45ms
step:2116/2330 train_time:123687ms step_avg:58.45ms
step:2117/2330 train_time:123745ms step_avg:58.45ms
step:2118/2330 train_time:123807ms step_avg:58.45ms
step:2119/2330 train_time:123865ms step_avg:58.45ms
step:2120/2330 train_time:123926ms step_avg:58.46ms
step:2121/2330 train_time:123984ms step_avg:58.46ms
step:2122/2330 train_time:124044ms step_avg:58.46ms
step:2123/2330 train_time:124102ms step_avg:58.46ms
step:2124/2330 train_time:124163ms step_avg:58.46ms
step:2125/2330 train_time:124220ms step_avg:58.46ms
step:2126/2330 train_time:124282ms step_avg:58.46ms
step:2127/2330 train_time:124339ms step_avg:58.46ms
step:2128/2330 train_time:124400ms step_avg:58.46ms
step:2129/2330 train_time:124457ms step_avg:58.46ms
step:2130/2330 train_time:124518ms step_avg:58.46ms
step:2131/2330 train_time:124575ms step_avg:58.46ms
step:2132/2330 train_time:124637ms step_avg:58.46ms
step:2133/2330 train_time:124694ms step_avg:58.46ms
step:2134/2330 train_time:124757ms step_avg:58.46ms
step:2135/2330 train_time:124814ms step_avg:58.46ms
step:2136/2330 train_time:124875ms step_avg:58.46ms
step:2137/2330 train_time:124931ms step_avg:58.46ms
step:2138/2330 train_time:124994ms step_avg:58.46ms
step:2139/2330 train_time:125052ms step_avg:58.46ms
step:2140/2330 train_time:125113ms step_avg:58.46ms
step:2141/2330 train_time:125171ms step_avg:58.46ms
step:2142/2330 train_time:125233ms step_avg:58.47ms
step:2143/2330 train_time:125291ms step_avg:58.47ms
step:2144/2330 train_time:125352ms step_avg:58.47ms
step:2145/2330 train_time:125411ms step_avg:58.47ms
step:2146/2330 train_time:125470ms step_avg:58.47ms
step:2147/2330 train_time:125528ms step_avg:58.47ms
step:2148/2330 train_time:125588ms step_avg:58.47ms
step:2149/2330 train_time:125647ms step_avg:58.47ms
step:2150/2330 train_time:125707ms step_avg:58.47ms
step:2151/2330 train_time:125765ms step_avg:58.47ms
step:2152/2330 train_time:125825ms step_avg:58.47ms
step:2153/2330 train_time:125882ms step_avg:58.47ms
step:2154/2330 train_time:125943ms step_avg:58.47ms
step:2155/2330 train_time:126001ms step_avg:58.47ms
step:2156/2330 train_time:126062ms step_avg:58.47ms
step:2157/2330 train_time:126119ms step_avg:58.47ms
step:2158/2330 train_time:126182ms step_avg:58.47ms
step:2159/2330 train_time:126239ms step_avg:58.47ms
step:2160/2330 train_time:126301ms step_avg:58.47ms
step:2161/2330 train_time:126358ms step_avg:58.47ms
step:2162/2330 train_time:126418ms step_avg:58.47ms
step:2163/2330 train_time:126475ms step_avg:58.47ms
step:2164/2330 train_time:126536ms step_avg:58.47ms
step:2165/2330 train_time:126593ms step_avg:58.47ms
step:2166/2330 train_time:126654ms step_avg:58.47ms
step:2167/2330 train_time:126711ms step_avg:58.47ms
step:2168/2330 train_time:126772ms step_avg:58.47ms
step:2169/2330 train_time:126830ms step_avg:58.47ms
step:2170/2330 train_time:126890ms step_avg:58.47ms
step:2171/2330 train_time:126950ms step_avg:58.48ms
step:2172/2330 train_time:127011ms step_avg:58.48ms
step:2173/2330 train_time:127069ms step_avg:58.48ms
step:2174/2330 train_time:127129ms step_avg:58.48ms
step:2175/2330 train_time:127187ms step_avg:58.48ms
step:2176/2330 train_time:127248ms step_avg:58.48ms
step:2177/2330 train_time:127306ms step_avg:58.48ms
step:2178/2330 train_time:127366ms step_avg:58.48ms
step:2179/2330 train_time:127424ms step_avg:58.48ms
step:2180/2330 train_time:127484ms step_avg:58.48ms
step:2181/2330 train_time:127542ms step_avg:58.48ms
step:2182/2330 train_time:127603ms step_avg:58.48ms
step:2183/2330 train_time:127661ms step_avg:58.48ms
step:2184/2330 train_time:127721ms step_avg:58.48ms
step:2185/2330 train_time:127778ms step_avg:58.48ms
step:2186/2330 train_time:127840ms step_avg:58.48ms
step:2187/2330 train_time:127898ms step_avg:58.48ms
step:2188/2330 train_time:127959ms step_avg:58.48ms
step:2189/2330 train_time:128016ms step_avg:58.48ms
step:2190/2330 train_time:128076ms step_avg:58.48ms
step:2191/2330 train_time:128134ms step_avg:58.48ms
step:2192/2330 train_time:128196ms step_avg:58.48ms
step:2193/2330 train_time:128253ms step_avg:58.48ms
step:2194/2330 train_time:128314ms step_avg:58.48ms
step:2195/2330 train_time:128371ms step_avg:58.48ms
step:2196/2330 train_time:128432ms step_avg:58.48ms
step:2197/2330 train_time:128488ms step_avg:58.48ms
step:2198/2330 train_time:128551ms step_avg:58.49ms
step:2199/2330 train_time:128609ms step_avg:58.49ms
step:2200/2330 train_time:128670ms step_avg:58.49ms
step:2201/2330 train_time:128729ms step_avg:58.49ms
step:2202/2330 train_time:128789ms step_avg:58.49ms
step:2203/2330 train_time:128847ms step_avg:58.49ms
step:2204/2330 train_time:128908ms step_avg:58.49ms
step:2205/2330 train_time:128967ms step_avg:58.49ms
step:2206/2330 train_time:129027ms step_avg:58.49ms
step:2207/2330 train_time:129086ms step_avg:58.49ms
step:2208/2330 train_time:129146ms step_avg:58.49ms
step:2209/2330 train_time:129203ms step_avg:58.49ms
step:2210/2330 train_time:129265ms step_avg:58.49ms
step:2211/2330 train_time:129323ms step_avg:58.49ms
step:2212/2330 train_time:129383ms step_avg:58.49ms
step:2213/2330 train_time:129440ms step_avg:58.49ms
step:2214/2330 train_time:129502ms step_avg:58.49ms
step:2215/2330 train_time:129560ms step_avg:58.49ms
step:2216/2330 train_time:129621ms step_avg:58.49ms
step:2217/2330 train_time:129678ms step_avg:58.49ms
step:2218/2330 train_time:129739ms step_avg:58.49ms
step:2219/2330 train_time:129795ms step_avg:58.49ms
step:2220/2330 train_time:129857ms step_avg:58.49ms
step:2221/2330 train_time:129914ms step_avg:58.49ms
step:2222/2330 train_time:129976ms step_avg:58.49ms
step:2223/2330 train_time:130033ms step_avg:58.49ms
step:2224/2330 train_time:130095ms step_avg:58.50ms
step:2225/2330 train_time:130152ms step_avg:58.50ms
step:2226/2330 train_time:130213ms step_avg:58.50ms
step:2227/2330 train_time:130271ms step_avg:58.50ms
step:2228/2330 train_time:130332ms step_avg:58.50ms
step:2229/2330 train_time:130391ms step_avg:58.50ms
step:2230/2330 train_time:130451ms step_avg:58.50ms
step:2231/2330 train_time:130509ms step_avg:58.50ms
step:2232/2330 train_time:130570ms step_avg:58.50ms
step:2233/2330 train_time:130627ms step_avg:58.50ms
step:2234/2330 train_time:130688ms step_avg:58.50ms
step:2235/2330 train_time:130745ms step_avg:58.50ms
step:2236/2330 train_time:130807ms step_avg:58.50ms
step:2237/2330 train_time:130865ms step_avg:58.50ms
step:2238/2330 train_time:130927ms step_avg:58.50ms
step:2239/2330 train_time:130984ms step_avg:58.50ms
step:2240/2330 train_time:131046ms step_avg:58.50ms
step:2241/2330 train_time:131103ms step_avg:58.50ms
step:2242/2330 train_time:131164ms step_avg:58.50ms
step:2243/2330 train_time:131222ms step_avg:58.50ms
step:2244/2330 train_time:131283ms step_avg:58.50ms
step:2245/2330 train_time:131340ms step_avg:58.50ms
step:2246/2330 train_time:131401ms step_avg:58.50ms
step:2247/2330 train_time:131458ms step_avg:58.50ms
step:2248/2330 train_time:131519ms step_avg:58.50ms
step:2249/2330 train_time:131576ms step_avg:58.50ms
step:2250/2330 train_time:131637ms step_avg:58.51ms
step:2250/2330 val_loss:3.7075 train_time:131719ms step_avg:58.54ms
step:2251/2330 train_time:131738ms step_avg:58.52ms
step:2252/2330 train_time:131759ms step_avg:58.51ms
step:2253/2330 train_time:131821ms step_avg:58.51ms
step:2254/2330 train_time:131885ms step_avg:58.51ms
step:2255/2330 train_time:131944ms step_avg:58.51ms
step:2256/2330 train_time:132005ms step_avg:58.51ms
step:2257/2330 train_time:132063ms step_avg:58.51ms
step:2258/2330 train_time:132124ms step_avg:58.51ms
step:2259/2330 train_time:132181ms step_avg:58.51ms
step:2260/2330 train_time:132241ms step_avg:58.51ms
step:2261/2330 train_time:132298ms step_avg:58.51ms
step:2262/2330 train_time:132358ms step_avg:58.51ms
step:2263/2330 train_time:132415ms step_avg:58.51ms
step:2264/2330 train_time:132475ms step_avg:58.51ms
step:2265/2330 train_time:132532ms step_avg:58.51ms
step:2266/2330 train_time:132592ms step_avg:58.51ms
step:2267/2330 train_time:132648ms step_avg:58.51ms
step:2268/2330 train_time:132710ms step_avg:58.51ms
step:2269/2330 train_time:132770ms step_avg:58.51ms
step:2270/2330 train_time:132832ms step_avg:58.52ms
step:2271/2330 train_time:132891ms step_avg:58.52ms
step:2272/2330 train_time:132953ms step_avg:58.52ms
step:2273/2330 train_time:133011ms step_avg:58.52ms
step:2274/2330 train_time:133073ms step_avg:58.52ms
step:2275/2330 train_time:133130ms step_avg:58.52ms
step:2276/2330 train_time:133191ms step_avg:58.52ms
step:2277/2330 train_time:133248ms step_avg:58.52ms
step:2278/2330 train_time:133310ms step_avg:58.52ms
step:2279/2330 train_time:133368ms step_avg:58.52ms
step:2280/2330 train_time:133427ms step_avg:58.52ms
step:2281/2330 train_time:133485ms step_avg:58.52ms
step:2282/2330 train_time:133544ms step_avg:58.52ms
step:2283/2330 train_time:133602ms step_avg:58.52ms
step:2284/2330 train_time:133662ms step_avg:58.52ms
step:2285/2330 train_time:133719ms step_avg:58.52ms
step:2286/2330 train_time:133781ms step_avg:58.52ms
step:2287/2330 train_time:133838ms step_avg:58.52ms
step:2288/2330 train_time:133900ms step_avg:58.52ms
step:2289/2330 train_time:133958ms step_avg:58.52ms
step:2290/2330 train_time:134021ms step_avg:58.52ms
step:2291/2330 train_time:134078ms step_avg:58.52ms
step:2292/2330 train_time:134141ms step_avg:58.53ms
step:2293/2330 train_time:134197ms step_avg:58.52ms
step:2294/2330 train_time:134258ms step_avg:58.53ms
step:2295/2330 train_time:134315ms step_avg:58.53ms
step:2296/2330 train_time:134375ms step_avg:58.53ms
step:2297/2330 train_time:134433ms step_avg:58.53ms
step:2298/2330 train_time:134492ms step_avg:58.53ms
step:2299/2330 train_time:134549ms step_avg:58.53ms
step:2300/2330 train_time:134609ms step_avg:58.53ms
step:2301/2330 train_time:134668ms step_avg:58.53ms
step:2302/2330 train_time:134729ms step_avg:58.53ms
step:2303/2330 train_time:134788ms step_avg:58.53ms
step:2304/2330 train_time:134849ms step_avg:58.53ms
step:2305/2330 train_time:134907ms step_avg:58.53ms
step:2306/2330 train_time:134968ms step_avg:58.53ms
step:2307/2330 train_time:135027ms step_avg:58.53ms
step:2308/2330 train_time:135088ms step_avg:58.53ms
step:2309/2330 train_time:135146ms step_avg:58.53ms
step:2310/2330 train_time:135207ms step_avg:58.53ms
step:2311/2330 train_time:135266ms step_avg:58.53ms
step:2312/2330 train_time:135325ms step_avg:58.53ms
step:2313/2330 train_time:135382ms step_avg:58.53ms
step:2314/2330 train_time:135442ms step_avg:58.53ms
step:2315/2330 train_time:135499ms step_avg:58.53ms
step:2316/2330 train_time:135560ms step_avg:58.53ms
step:2317/2330 train_time:135617ms step_avg:58.53ms
step:2318/2330 train_time:135678ms step_avg:58.53ms
step:2319/2330 train_time:135735ms step_avg:58.53ms
step:2320/2330 train_time:135796ms step_avg:58.53ms
step:2321/2330 train_time:135853ms step_avg:58.53ms
step:2322/2330 train_time:135915ms step_avg:58.53ms
step:2323/2330 train_time:135973ms step_avg:58.53ms
step:2324/2330 train_time:136035ms step_avg:58.53ms
step:2325/2330 train_time:136092ms step_avg:58.53ms
step:2326/2330 train_time:136153ms step_avg:58.54ms
step:2327/2330 train_time:136211ms step_avg:58.53ms
step:2328/2330 train_time:136273ms step_avg:58.54ms
step:2329/2330 train_time:136331ms step_avg:58.54ms
step:2330/2330 train_time:136391ms step_avg:58.54ms
step:2330/2330 val_loss:3.6920 train_time:136472ms step_avg:58.57ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
