import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:59:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:100ms step_avg:100.23ms
step:2/2330 train_time:192ms step_avg:95.83ms
step:3/2330 train_time:210ms step_avg:70.05ms
step:4/2330 train_time:230ms step_avg:57.42ms
step:5/2330 train_time:283ms step_avg:56.51ms
step:6/2330 train_time:340ms step_avg:56.72ms
step:7/2330 train_time:395ms step_avg:56.48ms
step:8/2330 train_time:454ms step_avg:56.75ms
step:9/2330 train_time:510ms step_avg:56.61ms
step:10/2330 train_time:568ms step_avg:56.76ms
step:11/2330 train_time:623ms step_avg:56.66ms
step:12/2330 train_time:681ms step_avg:56.76ms
step:13/2330 train_time:737ms step_avg:56.68ms
step:14/2330 train_time:795ms step_avg:56.76ms
step:15/2330 train_time:850ms step_avg:56.66ms
step:16/2330 train_time:908ms step_avg:56.76ms
step:17/2330 train_time:963ms step_avg:56.67ms
step:18/2330 train_time:1023ms step_avg:56.85ms
step:19/2330 train_time:1082ms step_avg:56.95ms
step:20/2330 train_time:1143ms step_avg:57.14ms
step:21/2330 train_time:1200ms step_avg:57.15ms
step:22/2330 train_time:1260ms step_avg:57.27ms
step:23/2330 train_time:1315ms step_avg:57.19ms
step:24/2330 train_time:1377ms step_avg:57.36ms
step:25/2330 train_time:1433ms step_avg:57.30ms
step:26/2330 train_time:1490ms step_avg:57.31ms
step:27/2330 train_time:1545ms step_avg:57.24ms
step:28/2330 train_time:1604ms step_avg:57.28ms
step:29/2330 train_time:1659ms step_avg:57.21ms
step:30/2330 train_time:1717ms step_avg:57.24ms
step:31/2330 train_time:1773ms step_avg:57.19ms
step:32/2330 train_time:1831ms step_avg:57.21ms
step:33/2330 train_time:1886ms step_avg:57.15ms
step:34/2330 train_time:1944ms step_avg:57.19ms
step:35/2330 train_time:2000ms step_avg:57.16ms
step:36/2330 train_time:2060ms step_avg:57.23ms
step:37/2330 train_time:2118ms step_avg:57.24ms
step:38/2330 train_time:2177ms step_avg:57.29ms
step:39/2330 train_time:2234ms step_avg:57.28ms
step:40/2330 train_time:2294ms step_avg:57.34ms
step:41/2330 train_time:2349ms step_avg:57.30ms
step:42/2330 train_time:2408ms step_avg:57.34ms
step:43/2330 train_time:2464ms step_avg:57.30ms
step:44/2330 train_time:2522ms step_avg:57.33ms
step:45/2330 train_time:2578ms step_avg:57.29ms
step:46/2330 train_time:2636ms step_avg:57.31ms
step:47/2330 train_time:2692ms step_avg:57.27ms
step:48/2330 train_time:2750ms step_avg:57.29ms
step:49/2330 train_time:2805ms step_avg:57.25ms
step:50/2330 train_time:2864ms step_avg:57.28ms
step:51/2330 train_time:2920ms step_avg:57.25ms
step:52/2330 train_time:2979ms step_avg:57.28ms
step:53/2330 train_time:3034ms step_avg:57.25ms
step:54/2330 train_time:3093ms step_avg:57.28ms
step:55/2330 train_time:3150ms step_avg:57.26ms
step:56/2330 train_time:3210ms step_avg:57.32ms
step:57/2330 train_time:3266ms step_avg:57.29ms
step:58/2330 train_time:3325ms step_avg:57.33ms
step:59/2330 train_time:3381ms step_avg:57.31ms
step:60/2330 train_time:3440ms step_avg:57.34ms
step:61/2330 train_time:3496ms step_avg:57.31ms
step:62/2330 train_time:3555ms step_avg:57.34ms
step:63/2330 train_time:3611ms step_avg:57.31ms
step:64/2330 train_time:3670ms step_avg:57.34ms
step:65/2330 train_time:3726ms step_avg:57.32ms
step:66/2330 train_time:3784ms step_avg:57.34ms
step:67/2330 train_time:3840ms step_avg:57.31ms
step:68/2330 train_time:3898ms step_avg:57.33ms
step:69/2330 train_time:3955ms step_avg:57.31ms
step:70/2330 train_time:4013ms step_avg:57.33ms
step:71/2330 train_time:4069ms step_avg:57.32ms
step:72/2330 train_time:4128ms step_avg:57.33ms
step:73/2330 train_time:4185ms step_avg:57.33ms
step:74/2330 train_time:4244ms step_avg:57.35ms
step:75/2330 train_time:4300ms step_avg:57.34ms
step:76/2330 train_time:4359ms step_avg:57.35ms
step:77/2330 train_time:4415ms step_avg:57.34ms
step:78/2330 train_time:4474ms step_avg:57.35ms
step:79/2330 train_time:4530ms step_avg:57.34ms
step:80/2330 train_time:4589ms step_avg:57.36ms
step:81/2330 train_time:4644ms step_avg:57.33ms
step:82/2330 train_time:4704ms step_avg:57.36ms
step:83/2330 train_time:4760ms step_avg:57.35ms
step:84/2330 train_time:4818ms step_avg:57.36ms
step:85/2330 train_time:4874ms step_avg:57.34ms
step:86/2330 train_time:4932ms step_avg:57.35ms
step:87/2330 train_time:4987ms step_avg:57.33ms
step:88/2330 train_time:5047ms step_avg:57.35ms
step:89/2330 train_time:5103ms step_avg:57.34ms
step:90/2330 train_time:5162ms step_avg:57.36ms
step:91/2330 train_time:5218ms step_avg:57.35ms
step:92/2330 train_time:5277ms step_avg:57.36ms
step:93/2330 train_time:5333ms step_avg:57.34ms
step:94/2330 train_time:5391ms step_avg:57.36ms
step:95/2330 train_time:5448ms step_avg:57.34ms
step:96/2330 train_time:5507ms step_avg:57.37ms
step:97/2330 train_time:5564ms step_avg:57.36ms
step:98/2330 train_time:5623ms step_avg:57.37ms
step:99/2330 train_time:5678ms step_avg:57.36ms
step:100/2330 train_time:5737ms step_avg:57.37ms
step:101/2330 train_time:5793ms step_avg:57.36ms
step:102/2330 train_time:5851ms step_avg:57.37ms
step:103/2330 train_time:5907ms step_avg:57.35ms
step:104/2330 train_time:5966ms step_avg:57.36ms
step:105/2330 train_time:6022ms step_avg:57.35ms
step:106/2330 train_time:6080ms step_avg:57.36ms
step:107/2330 train_time:6136ms step_avg:57.35ms
step:108/2330 train_time:6196ms step_avg:57.37ms
step:109/2330 train_time:6253ms step_avg:57.36ms
step:110/2330 train_time:6311ms step_avg:57.37ms
step:111/2330 train_time:6367ms step_avg:57.36ms
step:112/2330 train_time:6427ms step_avg:57.38ms
step:113/2330 train_time:6483ms step_avg:57.37ms
step:114/2330 train_time:6542ms step_avg:57.39ms
step:115/2330 train_time:6598ms step_avg:57.37ms
step:116/2330 train_time:6657ms step_avg:57.39ms
step:117/2330 train_time:6713ms step_avg:57.38ms
step:118/2330 train_time:6771ms step_avg:57.38ms
step:119/2330 train_time:6827ms step_avg:57.37ms
step:120/2330 train_time:6886ms step_avg:57.39ms
step:121/2330 train_time:6943ms step_avg:57.38ms
step:122/2330 train_time:7002ms step_avg:57.39ms
step:123/2330 train_time:7058ms step_avg:57.38ms
step:124/2330 train_time:7117ms step_avg:57.39ms
step:125/2330 train_time:7173ms step_avg:57.39ms
step:126/2330 train_time:7231ms step_avg:57.39ms
step:127/2330 train_time:7287ms step_avg:57.38ms
step:128/2330 train_time:7346ms step_avg:57.39ms
step:129/2330 train_time:7402ms step_avg:57.38ms
step:130/2330 train_time:7462ms step_avg:57.40ms
step:131/2330 train_time:7518ms step_avg:57.39ms
step:132/2330 train_time:7578ms step_avg:57.41ms
step:133/2330 train_time:7633ms step_avg:57.39ms
step:134/2330 train_time:7693ms step_avg:57.41ms
step:135/2330 train_time:7748ms step_avg:57.39ms
step:136/2330 train_time:7806ms step_avg:57.40ms
step:137/2330 train_time:7862ms step_avg:57.39ms
step:138/2330 train_time:7921ms step_avg:57.40ms
step:139/2330 train_time:7977ms step_avg:57.39ms
step:140/2330 train_time:8036ms step_avg:57.40ms
step:141/2330 train_time:8092ms step_avg:57.39ms
step:142/2330 train_time:8151ms step_avg:57.40ms
step:143/2330 train_time:8207ms step_avg:57.39ms
step:144/2330 train_time:8265ms step_avg:57.40ms
step:145/2330 train_time:8321ms step_avg:57.39ms
step:146/2330 train_time:8381ms step_avg:57.40ms
step:147/2330 train_time:8437ms step_avg:57.39ms
step:148/2330 train_time:8497ms step_avg:57.41ms
step:149/2330 train_time:8553ms step_avg:57.40ms
step:150/2330 train_time:8612ms step_avg:57.41ms
step:151/2330 train_time:8668ms step_avg:57.41ms
step:152/2330 train_time:8727ms step_avg:57.41ms
step:153/2330 train_time:8783ms step_avg:57.40ms
step:154/2330 train_time:8842ms step_avg:57.41ms
step:155/2330 train_time:8897ms step_avg:57.40ms
step:156/2330 train_time:8956ms step_avg:57.41ms
step:157/2330 train_time:9012ms step_avg:57.40ms
step:158/2330 train_time:9072ms step_avg:57.42ms
step:159/2330 train_time:9127ms step_avg:57.40ms
step:160/2330 train_time:9186ms step_avg:57.42ms
step:161/2330 train_time:9243ms step_avg:57.41ms
step:162/2330 train_time:9301ms step_avg:57.42ms
step:163/2330 train_time:9357ms step_avg:57.41ms
step:164/2330 train_time:9417ms step_avg:57.42ms
step:165/2330 train_time:9473ms step_avg:57.41ms
step:166/2330 train_time:9531ms step_avg:57.42ms
step:167/2330 train_time:9587ms step_avg:57.41ms
step:168/2330 train_time:9646ms step_avg:57.41ms
step:169/2330 train_time:9701ms step_avg:57.40ms
step:170/2330 train_time:9760ms step_avg:57.41ms
step:171/2330 train_time:9816ms step_avg:57.40ms
step:172/2330 train_time:9875ms step_avg:57.41ms
step:173/2330 train_time:9932ms step_avg:57.41ms
step:174/2330 train_time:9990ms step_avg:57.41ms
step:175/2330 train_time:10046ms step_avg:57.41ms
step:176/2330 train_time:10105ms step_avg:57.41ms
step:177/2330 train_time:10161ms step_avg:57.41ms
step:178/2330 train_time:10220ms step_avg:57.42ms
step:179/2330 train_time:10276ms step_avg:57.41ms
step:180/2330 train_time:10334ms step_avg:57.41ms
step:181/2330 train_time:10390ms step_avg:57.40ms
step:182/2330 train_time:10449ms step_avg:57.41ms
step:183/2330 train_time:10506ms step_avg:57.41ms
step:184/2330 train_time:10565ms step_avg:57.42ms
step:185/2330 train_time:10621ms step_avg:57.41ms
step:186/2330 train_time:10680ms step_avg:57.42ms
step:187/2330 train_time:10735ms step_avg:57.41ms
step:188/2330 train_time:10794ms step_avg:57.42ms
step:189/2330 train_time:10850ms step_avg:57.41ms
step:190/2330 train_time:10909ms step_avg:57.41ms
step:191/2330 train_time:10965ms step_avg:57.41ms
step:192/2330 train_time:11024ms step_avg:57.42ms
step:193/2330 train_time:11080ms step_avg:57.41ms
step:194/2330 train_time:11139ms step_avg:57.42ms
step:195/2330 train_time:11195ms step_avg:57.41ms
step:196/2330 train_time:11254ms step_avg:57.42ms
step:197/2330 train_time:11310ms step_avg:57.41ms
step:198/2330 train_time:11368ms step_avg:57.42ms
step:199/2330 train_time:11424ms step_avg:57.41ms
step:200/2330 train_time:11483ms step_avg:57.42ms
step:201/2330 train_time:11540ms step_avg:57.41ms
step:202/2330 train_time:11598ms step_avg:57.42ms
step:203/2330 train_time:11653ms step_avg:57.41ms
step:204/2330 train_time:11713ms step_avg:57.41ms
step:205/2330 train_time:11768ms step_avg:57.41ms
step:206/2330 train_time:11828ms step_avg:57.42ms
step:207/2330 train_time:11884ms step_avg:57.41ms
step:208/2330 train_time:11943ms step_avg:57.42ms
step:209/2330 train_time:11999ms step_avg:57.41ms
step:210/2330 train_time:12057ms step_avg:57.42ms
step:211/2330 train_time:12114ms step_avg:57.41ms
step:212/2330 train_time:12172ms step_avg:57.42ms
step:213/2330 train_time:12229ms step_avg:57.41ms
step:214/2330 train_time:12287ms step_avg:57.42ms
step:215/2330 train_time:12343ms step_avg:57.41ms
step:216/2330 train_time:12402ms step_avg:57.42ms
step:217/2330 train_time:12458ms step_avg:57.41ms
step:218/2330 train_time:12517ms step_avg:57.42ms
step:219/2330 train_time:12573ms step_avg:57.41ms
step:220/2330 train_time:12631ms step_avg:57.41ms
step:221/2330 train_time:12686ms step_avg:57.40ms
step:222/2330 train_time:12746ms step_avg:57.41ms
step:223/2330 train_time:12802ms step_avg:57.41ms
step:224/2330 train_time:12861ms step_avg:57.41ms
step:225/2330 train_time:12916ms step_avg:57.41ms
step:226/2330 train_time:12975ms step_avg:57.41ms
step:227/2330 train_time:13031ms step_avg:57.41ms
step:228/2330 train_time:13090ms step_avg:57.41ms
step:229/2330 train_time:13146ms step_avg:57.41ms
step:230/2330 train_time:13205ms step_avg:57.41ms
step:231/2330 train_time:13262ms step_avg:57.41ms
step:232/2330 train_time:13320ms step_avg:57.41ms
step:233/2330 train_time:13377ms step_avg:57.41ms
step:234/2330 train_time:13435ms step_avg:57.41ms
step:235/2330 train_time:13491ms step_avg:57.41ms
step:236/2330 train_time:13550ms step_avg:57.41ms
step:237/2330 train_time:13606ms step_avg:57.41ms
step:238/2330 train_time:13665ms step_avg:57.42ms
step:239/2330 train_time:13721ms step_avg:57.41ms
step:240/2330 train_time:13780ms step_avg:57.41ms
step:241/2330 train_time:13835ms step_avg:57.41ms
step:242/2330 train_time:13895ms step_avg:57.42ms
step:243/2330 train_time:13950ms step_avg:57.41ms
step:244/2330 train_time:14009ms step_avg:57.42ms
step:245/2330 train_time:14065ms step_avg:57.41ms
step:246/2330 train_time:14124ms step_avg:57.41ms
step:247/2330 train_time:14180ms step_avg:57.41ms
step:248/2330 train_time:14239ms step_avg:57.42ms
step:249/2330 train_time:14296ms step_avg:57.41ms
step:250/2330 train_time:14354ms step_avg:57.42ms
step:250/2330 val_loss:4.8782 train_time:14433ms step_avg:57.73ms
step:251/2330 train_time:14453ms step_avg:57.58ms
step:252/2330 train_time:14472ms step_avg:57.43ms
step:253/2330 train_time:14525ms step_avg:57.41ms
step:254/2330 train_time:14589ms step_avg:57.44ms
step:255/2330 train_time:14645ms step_avg:57.43ms
step:256/2330 train_time:14708ms step_avg:57.45ms
step:257/2330 train_time:14764ms step_avg:57.45ms
step:258/2330 train_time:14824ms step_avg:57.46ms
step:259/2330 train_time:14880ms step_avg:57.45ms
step:260/2330 train_time:14938ms step_avg:57.45ms
step:261/2330 train_time:14994ms step_avg:57.45ms
step:262/2330 train_time:15051ms step_avg:57.45ms
step:263/2330 train_time:15107ms step_avg:57.44ms
step:264/2330 train_time:15165ms step_avg:57.44ms
step:265/2330 train_time:15220ms step_avg:57.44ms
step:266/2330 train_time:15278ms step_avg:57.44ms
step:267/2330 train_time:15336ms step_avg:57.44ms
step:268/2330 train_time:15396ms step_avg:57.45ms
step:269/2330 train_time:15454ms step_avg:57.45ms
step:270/2330 train_time:15513ms step_avg:57.46ms
step:271/2330 train_time:15569ms step_avg:57.45ms
step:272/2330 train_time:15628ms step_avg:57.46ms
step:273/2330 train_time:15685ms step_avg:57.45ms
step:274/2330 train_time:15745ms step_avg:57.46ms
step:275/2330 train_time:15800ms step_avg:57.45ms
step:276/2330 train_time:15861ms step_avg:57.47ms
step:277/2330 train_time:15916ms step_avg:57.46ms
step:278/2330 train_time:15975ms step_avg:57.46ms
step:279/2330 train_time:16030ms step_avg:57.46ms
step:280/2330 train_time:16089ms step_avg:57.46ms
step:281/2330 train_time:16144ms step_avg:57.45ms
step:282/2330 train_time:16202ms step_avg:57.46ms
step:283/2330 train_time:16258ms step_avg:57.45ms
step:284/2330 train_time:16317ms step_avg:57.46ms
step:285/2330 train_time:16374ms step_avg:57.45ms
step:286/2330 train_time:16433ms step_avg:57.46ms
step:287/2330 train_time:16489ms step_avg:57.45ms
step:288/2330 train_time:16549ms step_avg:57.46ms
step:289/2330 train_time:16606ms step_avg:57.46ms
step:290/2330 train_time:16665ms step_avg:57.46ms
step:291/2330 train_time:16721ms step_avg:57.46ms
step:292/2330 train_time:16780ms step_avg:57.47ms
step:293/2330 train_time:16836ms step_avg:57.46ms
step:294/2330 train_time:16896ms step_avg:57.47ms
step:295/2330 train_time:16952ms step_avg:57.46ms
step:296/2330 train_time:17011ms step_avg:57.47ms
step:297/2330 train_time:17066ms step_avg:57.46ms
step:298/2330 train_time:17125ms step_avg:57.47ms
step:299/2330 train_time:17181ms step_avg:57.46ms
step:300/2330 train_time:17239ms step_avg:57.46ms
step:301/2330 train_time:17295ms step_avg:57.46ms
step:302/2330 train_time:17355ms step_avg:57.47ms
step:303/2330 train_time:17411ms step_avg:57.46ms
step:304/2330 train_time:17470ms step_avg:57.47ms
step:305/2330 train_time:17527ms step_avg:57.47ms
step:306/2330 train_time:17586ms step_avg:57.47ms
step:307/2330 train_time:17643ms step_avg:57.47ms
step:308/2330 train_time:17703ms step_avg:57.48ms
step:309/2330 train_time:17758ms step_avg:57.47ms
step:310/2330 train_time:17818ms step_avg:57.48ms
step:311/2330 train_time:17874ms step_avg:57.47ms
step:312/2330 train_time:17933ms step_avg:57.48ms
step:313/2330 train_time:17988ms step_avg:57.47ms
step:314/2330 train_time:18048ms step_avg:57.48ms
step:315/2330 train_time:18104ms step_avg:57.47ms
step:316/2330 train_time:18162ms step_avg:57.48ms
step:317/2330 train_time:18219ms step_avg:57.47ms
step:318/2330 train_time:18277ms step_avg:57.48ms
step:319/2330 train_time:18333ms step_avg:57.47ms
step:320/2330 train_time:18392ms step_avg:57.47ms
step:321/2330 train_time:18448ms step_avg:57.47ms
step:322/2330 train_time:18507ms step_avg:57.48ms
step:323/2330 train_time:18564ms step_avg:57.47ms
step:324/2330 train_time:18623ms step_avg:57.48ms
step:325/2330 train_time:18680ms step_avg:57.48ms
step:326/2330 train_time:18739ms step_avg:57.48ms
step:327/2330 train_time:18795ms step_avg:57.48ms
step:328/2330 train_time:18855ms step_avg:57.48ms
step:329/2330 train_time:18911ms step_avg:57.48ms
step:330/2330 train_time:18969ms step_avg:57.48ms
step:331/2330 train_time:19025ms step_avg:57.48ms
step:332/2330 train_time:19084ms step_avg:57.48ms
step:333/2330 train_time:19141ms step_avg:57.48ms
step:334/2330 train_time:19199ms step_avg:57.48ms
step:335/2330 train_time:19255ms step_avg:57.48ms
step:336/2330 train_time:19314ms step_avg:57.48ms
step:337/2330 train_time:19369ms step_avg:57.48ms
step:338/2330 train_time:19429ms step_avg:57.48ms
step:339/2330 train_time:19485ms step_avg:57.48ms
step:340/2330 train_time:19543ms step_avg:57.48ms
step:341/2330 train_time:19600ms step_avg:57.48ms
step:342/2330 train_time:19658ms step_avg:57.48ms
step:343/2330 train_time:19714ms step_avg:57.48ms
step:344/2330 train_time:19774ms step_avg:57.48ms
step:345/2330 train_time:19830ms step_avg:57.48ms
step:346/2330 train_time:19888ms step_avg:57.48ms
step:347/2330 train_time:19944ms step_avg:57.47ms
step:348/2330 train_time:20003ms step_avg:57.48ms
step:349/2330 train_time:20059ms step_avg:57.48ms
step:350/2330 train_time:20118ms step_avg:57.48ms
step:351/2330 train_time:20174ms step_avg:57.48ms
step:352/2330 train_time:20233ms step_avg:57.48ms
step:353/2330 train_time:20289ms step_avg:57.48ms
step:354/2330 train_time:20348ms step_avg:57.48ms
step:355/2330 train_time:20404ms step_avg:57.48ms
step:356/2330 train_time:20464ms step_avg:57.48ms
step:357/2330 train_time:20521ms step_avg:57.48ms
step:358/2330 train_time:20579ms step_avg:57.48ms
step:359/2330 train_time:20636ms step_avg:57.48ms
step:360/2330 train_time:20695ms step_avg:57.49ms
step:361/2330 train_time:20751ms step_avg:57.48ms
step:362/2330 train_time:20810ms step_avg:57.49ms
step:363/2330 train_time:20867ms step_avg:57.49ms
step:364/2330 train_time:20926ms step_avg:57.49ms
step:365/2330 train_time:20981ms step_avg:57.48ms
step:366/2330 train_time:21041ms step_avg:57.49ms
step:367/2330 train_time:21097ms step_avg:57.49ms
step:368/2330 train_time:21156ms step_avg:57.49ms
step:369/2330 train_time:21212ms step_avg:57.49ms
step:370/2330 train_time:21271ms step_avg:57.49ms
step:371/2330 train_time:21327ms step_avg:57.48ms
step:372/2330 train_time:21386ms step_avg:57.49ms
step:373/2330 train_time:21442ms step_avg:57.49ms
step:374/2330 train_time:21502ms step_avg:57.49ms
step:375/2330 train_time:21558ms step_avg:57.49ms
step:376/2330 train_time:21617ms step_avg:57.49ms
step:377/2330 train_time:21672ms step_avg:57.49ms
step:378/2330 train_time:21733ms step_avg:57.49ms
step:379/2330 train_time:21788ms step_avg:57.49ms
step:380/2330 train_time:21848ms step_avg:57.49ms
step:381/2330 train_time:21904ms step_avg:57.49ms
step:382/2330 train_time:21963ms step_avg:57.50ms
step:383/2330 train_time:22020ms step_avg:57.49ms
step:384/2330 train_time:22079ms step_avg:57.50ms
step:385/2330 train_time:22135ms step_avg:57.49ms
step:386/2330 train_time:22193ms step_avg:57.49ms
step:387/2330 train_time:22249ms step_avg:57.49ms
step:388/2330 train_time:22307ms step_avg:57.49ms
step:389/2330 train_time:22363ms step_avg:57.49ms
step:390/2330 train_time:22422ms step_avg:57.49ms
step:391/2330 train_time:22478ms step_avg:57.49ms
step:392/2330 train_time:22538ms step_avg:57.49ms
step:393/2330 train_time:22594ms step_avg:57.49ms
step:394/2330 train_time:22653ms step_avg:57.49ms
step:395/2330 train_time:22709ms step_avg:57.49ms
step:396/2330 train_time:22767ms step_avg:57.49ms
step:397/2330 train_time:22824ms step_avg:57.49ms
step:398/2330 train_time:22883ms step_avg:57.50ms
step:399/2330 train_time:22940ms step_avg:57.49ms
step:400/2330 train_time:22999ms step_avg:57.50ms
step:401/2330 train_time:23056ms step_avg:57.50ms
step:402/2330 train_time:23115ms step_avg:57.50ms
step:403/2330 train_time:23171ms step_avg:57.50ms
step:404/2330 train_time:23230ms step_avg:57.50ms
step:405/2330 train_time:23285ms step_avg:57.49ms
step:406/2330 train_time:23344ms step_avg:57.50ms
step:407/2330 train_time:23400ms step_avg:57.49ms
step:408/2330 train_time:23460ms step_avg:57.50ms
step:409/2330 train_time:23516ms step_avg:57.50ms
step:410/2330 train_time:23575ms step_avg:57.50ms
step:411/2330 train_time:23631ms step_avg:57.50ms
step:412/2330 train_time:23690ms step_avg:57.50ms
step:413/2330 train_time:23746ms step_avg:57.50ms
step:414/2330 train_time:23805ms step_avg:57.50ms
step:415/2330 train_time:23861ms step_avg:57.50ms
step:416/2330 train_time:23921ms step_avg:57.50ms
step:417/2330 train_time:23977ms step_avg:57.50ms
step:418/2330 train_time:24037ms step_avg:57.50ms
step:419/2330 train_time:24093ms step_avg:57.50ms
step:420/2330 train_time:24152ms step_avg:57.50ms
step:421/2330 train_time:24208ms step_avg:57.50ms
step:422/2330 train_time:24267ms step_avg:57.50ms
step:423/2330 train_time:24323ms step_avg:57.50ms
step:424/2330 train_time:24382ms step_avg:57.50ms
step:425/2330 train_time:24438ms step_avg:57.50ms
step:426/2330 train_time:24497ms step_avg:57.50ms
step:427/2330 train_time:24553ms step_avg:57.50ms
step:428/2330 train_time:24612ms step_avg:57.50ms
step:429/2330 train_time:24668ms step_avg:57.50ms
step:430/2330 train_time:24727ms step_avg:57.50ms
step:431/2330 train_time:24783ms step_avg:57.50ms
step:432/2330 train_time:24842ms step_avg:57.50ms
step:433/2330 train_time:24898ms step_avg:57.50ms
step:434/2330 train_time:24958ms step_avg:57.51ms
step:435/2330 train_time:25015ms step_avg:57.50ms
step:436/2330 train_time:25073ms step_avg:57.51ms
step:437/2330 train_time:25129ms step_avg:57.50ms
step:438/2330 train_time:25188ms step_avg:57.51ms
step:439/2330 train_time:25244ms step_avg:57.50ms
step:440/2330 train_time:25303ms step_avg:57.51ms
step:441/2330 train_time:25360ms step_avg:57.50ms
step:442/2330 train_time:25418ms step_avg:57.51ms
step:443/2330 train_time:25474ms step_avg:57.50ms
step:444/2330 train_time:25533ms step_avg:57.51ms
step:445/2330 train_time:25588ms step_avg:57.50ms
step:446/2330 train_time:25649ms step_avg:57.51ms
step:447/2330 train_time:25705ms step_avg:57.51ms
step:448/2330 train_time:25763ms step_avg:57.51ms
step:449/2330 train_time:25819ms step_avg:57.50ms
step:450/2330 train_time:25878ms step_avg:57.51ms
step:451/2330 train_time:25934ms step_avg:57.50ms
step:452/2330 train_time:25993ms step_avg:57.51ms
step:453/2330 train_time:26049ms step_avg:57.50ms
step:454/2330 train_time:26109ms step_avg:57.51ms
step:455/2330 train_time:26165ms step_avg:57.51ms
step:456/2330 train_time:26224ms step_avg:57.51ms
step:457/2330 train_time:26280ms step_avg:57.51ms
step:458/2330 train_time:26339ms step_avg:57.51ms
step:459/2330 train_time:26396ms step_avg:57.51ms
step:460/2330 train_time:26456ms step_avg:57.51ms
step:461/2330 train_time:26512ms step_avg:57.51ms
step:462/2330 train_time:26571ms step_avg:57.51ms
step:463/2330 train_time:26627ms step_avg:57.51ms
step:464/2330 train_time:26687ms step_avg:57.52ms
step:465/2330 train_time:26743ms step_avg:57.51ms
step:466/2330 train_time:26802ms step_avg:57.51ms
step:467/2330 train_time:26858ms step_avg:57.51ms
step:468/2330 train_time:26917ms step_avg:57.52ms
step:469/2330 train_time:26973ms step_avg:57.51ms
step:470/2330 train_time:27032ms step_avg:57.52ms
step:471/2330 train_time:27088ms step_avg:57.51ms
step:472/2330 train_time:27147ms step_avg:57.52ms
step:473/2330 train_time:27203ms step_avg:57.51ms
step:474/2330 train_time:27263ms step_avg:57.52ms
step:475/2330 train_time:27319ms step_avg:57.51ms
step:476/2330 train_time:27379ms step_avg:57.52ms
step:477/2330 train_time:27436ms step_avg:57.52ms
step:478/2330 train_time:27495ms step_avg:57.52ms
step:479/2330 train_time:27551ms step_avg:57.52ms
step:480/2330 train_time:27610ms step_avg:57.52ms
step:481/2330 train_time:27666ms step_avg:57.52ms
step:482/2330 train_time:27726ms step_avg:57.52ms
step:483/2330 train_time:27782ms step_avg:57.52ms
step:484/2330 train_time:27841ms step_avg:57.52ms
step:485/2330 train_time:27897ms step_avg:57.52ms
step:486/2330 train_time:27956ms step_avg:57.52ms
step:487/2330 train_time:28012ms step_avg:57.52ms
step:488/2330 train_time:28071ms step_avg:57.52ms
step:489/2330 train_time:28127ms step_avg:57.52ms
step:490/2330 train_time:28187ms step_avg:57.52ms
step:491/2330 train_time:28243ms step_avg:57.52ms
step:492/2330 train_time:28302ms step_avg:57.52ms
step:493/2330 train_time:28358ms step_avg:57.52ms
step:494/2330 train_time:28418ms step_avg:57.53ms
step:495/2330 train_time:28474ms step_avg:57.52ms
step:496/2330 train_time:28533ms step_avg:57.53ms
step:497/2330 train_time:28589ms step_avg:57.52ms
step:498/2330 train_time:28648ms step_avg:57.53ms
step:499/2330 train_time:28704ms step_avg:57.52ms
step:500/2330 train_time:28763ms step_avg:57.53ms
step:500/2330 val_loss:4.3942 train_time:28843ms step_avg:57.69ms
step:501/2330 train_time:28862ms step_avg:57.61ms
step:502/2330 train_time:28881ms step_avg:57.53ms
step:503/2330 train_time:28935ms step_avg:57.53ms
step:504/2330 train_time:29003ms step_avg:57.55ms
step:505/2330 train_time:29060ms step_avg:57.54ms
step:506/2330 train_time:29124ms step_avg:57.56ms
step:507/2330 train_time:29180ms step_avg:57.55ms
step:508/2330 train_time:29240ms step_avg:57.56ms
step:509/2330 train_time:29297ms step_avg:57.56ms
step:510/2330 train_time:29355ms step_avg:57.56ms
step:511/2330 train_time:29411ms step_avg:57.55ms
step:512/2330 train_time:29469ms step_avg:57.56ms
step:513/2330 train_time:29524ms step_avg:57.55ms
step:514/2330 train_time:29583ms step_avg:57.55ms
step:515/2330 train_time:29638ms step_avg:57.55ms
step:516/2330 train_time:29697ms step_avg:57.55ms
step:517/2330 train_time:29753ms step_avg:57.55ms
step:518/2330 train_time:29812ms step_avg:57.55ms
step:519/2330 train_time:29868ms step_avg:57.55ms
step:520/2330 train_time:29929ms step_avg:57.55ms
step:521/2330 train_time:29985ms step_avg:57.55ms
step:522/2330 train_time:30046ms step_avg:57.56ms
step:523/2330 train_time:30103ms step_avg:57.56ms
step:524/2330 train_time:30163ms step_avg:57.56ms
step:525/2330 train_time:30219ms step_avg:57.56ms
step:526/2330 train_time:30279ms step_avg:57.57ms
step:527/2330 train_time:30336ms step_avg:57.56ms
step:528/2330 train_time:30395ms step_avg:57.57ms
step:529/2330 train_time:30451ms step_avg:57.56ms
step:530/2330 train_time:30509ms step_avg:57.56ms
step:531/2330 train_time:30564ms step_avg:57.56ms
step:532/2330 train_time:30623ms step_avg:57.56ms
step:533/2330 train_time:30679ms step_avg:57.56ms
step:534/2330 train_time:30737ms step_avg:57.56ms
step:535/2330 train_time:30794ms step_avg:57.56ms
step:536/2330 train_time:30853ms step_avg:57.56ms
step:537/2330 train_time:30910ms step_avg:57.56ms
step:538/2330 train_time:30970ms step_avg:57.57ms
step:539/2330 train_time:31027ms step_avg:57.56ms
step:540/2330 train_time:31087ms step_avg:57.57ms
step:541/2330 train_time:31144ms step_avg:57.57ms
step:542/2330 train_time:31203ms step_avg:57.57ms
step:543/2330 train_time:31259ms step_avg:57.57ms
step:544/2330 train_time:31319ms step_avg:57.57ms
step:545/2330 train_time:31375ms step_avg:57.57ms
step:546/2330 train_time:31435ms step_avg:57.57ms
step:547/2330 train_time:31490ms step_avg:57.57ms
step:548/2330 train_time:31549ms step_avg:57.57ms
step:549/2330 train_time:31605ms step_avg:57.57ms
step:550/2330 train_time:31663ms step_avg:57.57ms
step:551/2330 train_time:31720ms step_avg:57.57ms
step:552/2330 train_time:31779ms step_avg:57.57ms
step:553/2330 train_time:31836ms step_avg:57.57ms
step:554/2330 train_time:31895ms step_avg:57.57ms
step:555/2330 train_time:31951ms step_avg:57.57ms
step:556/2330 train_time:32012ms step_avg:57.57ms
step:557/2330 train_time:32067ms step_avg:57.57ms
step:558/2330 train_time:32128ms step_avg:57.58ms
step:559/2330 train_time:32184ms step_avg:57.57ms
step:560/2330 train_time:32244ms step_avg:57.58ms
step:561/2330 train_time:32300ms step_avg:57.58ms
step:562/2330 train_time:32359ms step_avg:57.58ms
step:563/2330 train_time:32415ms step_avg:57.58ms
step:564/2330 train_time:32474ms step_avg:57.58ms
step:565/2330 train_time:32530ms step_avg:57.57ms
step:566/2330 train_time:32589ms step_avg:57.58ms
step:567/2330 train_time:32645ms step_avg:57.57ms
step:568/2330 train_time:32703ms step_avg:57.58ms
step:569/2330 train_time:32759ms step_avg:57.57ms
step:570/2330 train_time:32819ms step_avg:57.58ms
step:571/2330 train_time:32876ms step_avg:57.58ms
step:572/2330 train_time:32937ms step_avg:57.58ms
step:573/2330 train_time:32994ms step_avg:57.58ms
step:574/2330 train_time:33053ms step_avg:57.58ms
step:575/2330 train_time:33109ms step_avg:57.58ms
step:576/2330 train_time:33168ms step_avg:57.58ms
step:577/2330 train_time:33224ms step_avg:57.58ms
step:578/2330 train_time:33283ms step_avg:57.58ms
step:579/2330 train_time:33339ms step_avg:57.58ms
step:580/2330 train_time:33398ms step_avg:57.58ms
step:581/2330 train_time:33454ms step_avg:57.58ms
step:582/2330 train_time:33512ms step_avg:57.58ms
step:583/2330 train_time:33567ms step_avg:57.58ms
step:584/2330 train_time:33627ms step_avg:57.58ms
step:585/2330 train_time:33682ms step_avg:57.58ms
step:586/2330 train_time:33743ms step_avg:57.58ms
step:587/2330 train_time:33799ms step_avg:57.58ms
step:588/2330 train_time:33859ms step_avg:57.58ms
step:589/2330 train_time:33916ms step_avg:57.58ms
step:590/2330 train_time:33975ms step_avg:57.59ms
step:591/2330 train_time:34032ms step_avg:57.58ms
step:592/2330 train_time:34091ms step_avg:57.59ms
step:593/2330 train_time:34147ms step_avg:57.58ms
step:594/2330 train_time:34207ms step_avg:57.59ms
step:595/2330 train_time:34263ms step_avg:57.58ms
step:596/2330 train_time:34322ms step_avg:57.59ms
step:597/2330 train_time:34379ms step_avg:57.59ms
step:598/2330 train_time:34437ms step_avg:57.59ms
step:599/2330 train_time:34494ms step_avg:57.59ms
step:600/2330 train_time:34552ms step_avg:57.59ms
step:601/2330 train_time:34609ms step_avg:57.58ms
step:602/2330 train_time:34667ms step_avg:57.59ms
step:603/2330 train_time:34723ms step_avg:57.58ms
step:604/2330 train_time:34782ms step_avg:57.59ms
step:605/2330 train_time:34839ms step_avg:57.58ms
step:606/2330 train_time:34898ms step_avg:57.59ms
step:607/2330 train_time:34954ms step_avg:57.59ms
step:608/2330 train_time:35014ms step_avg:57.59ms
step:609/2330 train_time:35071ms step_avg:57.59ms
step:610/2330 train_time:35130ms step_avg:57.59ms
step:611/2330 train_time:35186ms step_avg:57.59ms
step:612/2330 train_time:35245ms step_avg:57.59ms
step:613/2330 train_time:35301ms step_avg:57.59ms
step:614/2330 train_time:35361ms step_avg:57.59ms
step:615/2330 train_time:35417ms step_avg:57.59ms
step:616/2330 train_time:35477ms step_avg:57.59ms
step:617/2330 train_time:35533ms step_avg:57.59ms
step:618/2330 train_time:35592ms step_avg:57.59ms
step:619/2330 train_time:35648ms step_avg:57.59ms
step:620/2330 train_time:35706ms step_avg:57.59ms
step:621/2330 train_time:35762ms step_avg:57.59ms
step:622/2330 train_time:35822ms step_avg:57.59ms
step:623/2330 train_time:35879ms step_avg:57.59ms
step:624/2330 train_time:35938ms step_avg:57.59ms
step:625/2330 train_time:35995ms step_avg:57.59ms
step:626/2330 train_time:36055ms step_avg:57.60ms
step:627/2330 train_time:36112ms step_avg:57.60ms
step:628/2330 train_time:36171ms step_avg:57.60ms
step:629/2330 train_time:36227ms step_avg:57.60ms
step:630/2330 train_time:36286ms step_avg:57.60ms
step:631/2330 train_time:36342ms step_avg:57.59ms
step:632/2330 train_time:36400ms step_avg:57.60ms
step:633/2330 train_time:36457ms step_avg:57.59ms
step:634/2330 train_time:36517ms step_avg:57.60ms
step:635/2330 train_time:36573ms step_avg:57.60ms
step:636/2330 train_time:36632ms step_avg:57.60ms
step:637/2330 train_time:36688ms step_avg:57.60ms
step:638/2330 train_time:36747ms step_avg:57.60ms
step:639/2330 train_time:36803ms step_avg:57.59ms
step:640/2330 train_time:36862ms step_avg:57.60ms
step:641/2330 train_time:36918ms step_avg:57.59ms
step:642/2330 train_time:36980ms step_avg:57.60ms
step:643/2330 train_time:37036ms step_avg:57.60ms
step:644/2330 train_time:37096ms step_avg:57.60ms
step:645/2330 train_time:37152ms step_avg:57.60ms
step:646/2330 train_time:37212ms step_avg:57.60ms
step:647/2330 train_time:37268ms step_avg:57.60ms
step:648/2330 train_time:37327ms step_avg:57.60ms
step:649/2330 train_time:37383ms step_avg:57.60ms
step:650/2330 train_time:37442ms step_avg:57.60ms
step:651/2330 train_time:37500ms step_avg:57.60ms
step:652/2330 train_time:37559ms step_avg:57.61ms
step:653/2330 train_time:37615ms step_avg:57.60ms
step:654/2330 train_time:37674ms step_avg:57.61ms
step:655/2330 train_time:37729ms step_avg:57.60ms
step:656/2330 train_time:37789ms step_avg:57.61ms
step:657/2330 train_time:37845ms step_avg:57.60ms
step:658/2330 train_time:37905ms step_avg:57.61ms
step:659/2330 train_time:37961ms step_avg:57.60ms
step:660/2330 train_time:38021ms step_avg:57.61ms
step:661/2330 train_time:38077ms step_avg:57.61ms
step:662/2330 train_time:38138ms step_avg:57.61ms
step:663/2330 train_time:38195ms step_avg:57.61ms
step:664/2330 train_time:38255ms step_avg:57.61ms
step:665/2330 train_time:38311ms step_avg:57.61ms
step:666/2330 train_time:38370ms step_avg:57.61ms
step:667/2330 train_time:38426ms step_avg:57.61ms
step:668/2330 train_time:38485ms step_avg:57.61ms
step:669/2330 train_time:38541ms step_avg:57.61ms
step:670/2330 train_time:38600ms step_avg:57.61ms
step:671/2330 train_time:38657ms step_avg:57.61ms
step:672/2330 train_time:38717ms step_avg:57.61ms
step:673/2330 train_time:38773ms step_avg:57.61ms
step:674/2330 train_time:38832ms step_avg:57.61ms
step:675/2330 train_time:38888ms step_avg:57.61ms
step:676/2330 train_time:38947ms step_avg:57.61ms
step:677/2330 train_time:39003ms step_avg:57.61ms
step:678/2330 train_time:39062ms step_avg:57.61ms
step:679/2330 train_time:39118ms step_avg:57.61ms
step:680/2330 train_time:39180ms step_avg:57.62ms
step:681/2330 train_time:39236ms step_avg:57.62ms
step:682/2330 train_time:39296ms step_avg:57.62ms
step:683/2330 train_time:39353ms step_avg:57.62ms
step:684/2330 train_time:39411ms step_avg:57.62ms
step:685/2330 train_time:39467ms step_avg:57.62ms
step:686/2330 train_time:39527ms step_avg:57.62ms
step:687/2330 train_time:39583ms step_avg:57.62ms
step:688/2330 train_time:39642ms step_avg:57.62ms
step:689/2330 train_time:39699ms step_avg:57.62ms
step:690/2330 train_time:39757ms step_avg:57.62ms
step:691/2330 train_time:39813ms step_avg:57.62ms
step:692/2330 train_time:39873ms step_avg:57.62ms
step:693/2330 train_time:39929ms step_avg:57.62ms
step:694/2330 train_time:39989ms step_avg:57.62ms
step:695/2330 train_time:40045ms step_avg:57.62ms
step:696/2330 train_time:40104ms step_avg:57.62ms
step:697/2330 train_time:40160ms step_avg:57.62ms
step:698/2330 train_time:40220ms step_avg:57.62ms
step:699/2330 train_time:40276ms step_avg:57.62ms
step:700/2330 train_time:40337ms step_avg:57.62ms
step:701/2330 train_time:40393ms step_avg:57.62ms
step:702/2330 train_time:40453ms step_avg:57.63ms
step:703/2330 train_time:40509ms step_avg:57.62ms
step:704/2330 train_time:40569ms step_avg:57.63ms
step:705/2330 train_time:40625ms step_avg:57.62ms
step:706/2330 train_time:40684ms step_avg:57.63ms
step:707/2330 train_time:40741ms step_avg:57.62ms
step:708/2330 train_time:40800ms step_avg:57.63ms
step:709/2330 train_time:40857ms step_avg:57.63ms
step:710/2330 train_time:40917ms step_avg:57.63ms
step:711/2330 train_time:40974ms step_avg:57.63ms
step:712/2330 train_time:41032ms step_avg:57.63ms
step:713/2330 train_time:41088ms step_avg:57.63ms
step:714/2330 train_time:41147ms step_avg:57.63ms
step:715/2330 train_time:41204ms step_avg:57.63ms
step:716/2330 train_time:41262ms step_avg:57.63ms
step:717/2330 train_time:41319ms step_avg:57.63ms
step:718/2330 train_time:41380ms step_avg:57.63ms
step:719/2330 train_time:41437ms step_avg:57.63ms
step:720/2330 train_time:41496ms step_avg:57.63ms
step:721/2330 train_time:41553ms step_avg:57.63ms
step:722/2330 train_time:41612ms step_avg:57.63ms
step:723/2330 train_time:41668ms step_avg:57.63ms
step:724/2330 train_time:41726ms step_avg:57.63ms
step:725/2330 train_time:41782ms step_avg:57.63ms
step:726/2330 train_time:41842ms step_avg:57.63ms
step:727/2330 train_time:41898ms step_avg:57.63ms
step:728/2330 train_time:41958ms step_avg:57.63ms
step:729/2330 train_time:42014ms step_avg:57.63ms
step:730/2330 train_time:42073ms step_avg:57.63ms
step:731/2330 train_time:42129ms step_avg:57.63ms
step:732/2330 train_time:42188ms step_avg:57.63ms
step:733/2330 train_time:42244ms step_avg:57.63ms
step:734/2330 train_time:42305ms step_avg:57.64ms
step:735/2330 train_time:42361ms step_avg:57.63ms
step:736/2330 train_time:42421ms step_avg:57.64ms
step:737/2330 train_time:42477ms step_avg:57.64ms
step:738/2330 train_time:42537ms step_avg:57.64ms
step:739/2330 train_time:42594ms step_avg:57.64ms
step:740/2330 train_time:42654ms step_avg:57.64ms
step:741/2330 train_time:42710ms step_avg:57.64ms
step:742/2330 train_time:42769ms step_avg:57.64ms
step:743/2330 train_time:42825ms step_avg:57.64ms
step:744/2330 train_time:42885ms step_avg:57.64ms
step:745/2330 train_time:42942ms step_avg:57.64ms
step:746/2330 train_time:43001ms step_avg:57.64ms
step:747/2330 train_time:43057ms step_avg:57.64ms
step:748/2330 train_time:43117ms step_avg:57.64ms
step:749/2330 train_time:43173ms step_avg:57.64ms
step:750/2330 train_time:43232ms step_avg:57.64ms
step:750/2330 val_loss:4.2080 train_time:43311ms step_avg:57.75ms
step:751/2330 train_time:43331ms step_avg:57.70ms
step:752/2330 train_time:43350ms step_avg:57.65ms
step:753/2330 train_time:43406ms step_avg:57.64ms
step:754/2330 train_time:43471ms step_avg:57.65ms
step:755/2330 train_time:43528ms step_avg:57.65ms
step:756/2330 train_time:43588ms step_avg:57.66ms
step:757/2330 train_time:43645ms step_avg:57.65ms
step:758/2330 train_time:43703ms step_avg:57.66ms
step:759/2330 train_time:43759ms step_avg:57.65ms
step:760/2330 train_time:43818ms step_avg:57.66ms
step:761/2330 train_time:43874ms step_avg:57.65ms
step:762/2330 train_time:43932ms step_avg:57.65ms
step:763/2330 train_time:43988ms step_avg:57.65ms
step:764/2330 train_time:44046ms step_avg:57.65ms
step:765/2330 train_time:44103ms step_avg:57.65ms
step:766/2330 train_time:44161ms step_avg:57.65ms
step:767/2330 train_time:44218ms step_avg:57.65ms
step:768/2330 train_time:44277ms step_avg:57.65ms
step:769/2330 train_time:44335ms step_avg:57.65ms
step:770/2330 train_time:44398ms step_avg:57.66ms
step:771/2330 train_time:44455ms step_avg:57.66ms
step:772/2330 train_time:44517ms step_avg:57.66ms
step:773/2330 train_time:44574ms step_avg:57.66ms
step:774/2330 train_time:44635ms step_avg:57.67ms
step:775/2330 train_time:44692ms step_avg:57.67ms
step:776/2330 train_time:44753ms step_avg:57.67ms
step:777/2330 train_time:44810ms step_avg:57.67ms
step:778/2330 train_time:44869ms step_avg:57.67ms
step:779/2330 train_time:44925ms step_avg:57.67ms
step:780/2330 train_time:44985ms step_avg:57.67ms
step:781/2330 train_time:45042ms step_avg:57.67ms
step:782/2330 train_time:45100ms step_avg:57.67ms
step:783/2330 train_time:45157ms step_avg:57.67ms
step:784/2330 train_time:45216ms step_avg:57.67ms
step:785/2330 train_time:45273ms step_avg:57.67ms
step:786/2330 train_time:45333ms step_avg:57.68ms
step:787/2330 train_time:45392ms step_avg:57.68ms
step:788/2330 train_time:45451ms step_avg:57.68ms
step:789/2330 train_time:45508ms step_avg:57.68ms
step:790/2330 train_time:45569ms step_avg:57.68ms
step:791/2330 train_time:45626ms step_avg:57.68ms
step:792/2330 train_time:45687ms step_avg:57.69ms
step:793/2330 train_time:45744ms step_avg:57.68ms
step:794/2330 train_time:45804ms step_avg:57.69ms
step:795/2330 train_time:45861ms step_avg:57.69ms
step:796/2330 train_time:45920ms step_avg:57.69ms
step:797/2330 train_time:45977ms step_avg:57.69ms
step:798/2330 train_time:46038ms step_avg:57.69ms
step:799/2330 train_time:46094ms step_avg:57.69ms
step:800/2330 train_time:46154ms step_avg:57.69ms
step:801/2330 train_time:46210ms step_avg:57.69ms
step:802/2330 train_time:46269ms step_avg:57.69ms
step:803/2330 train_time:46327ms step_avg:57.69ms
step:804/2330 train_time:46387ms step_avg:57.70ms
step:805/2330 train_time:46444ms step_avg:57.69ms
step:806/2330 train_time:46505ms step_avg:57.70ms
step:807/2330 train_time:46563ms step_avg:57.70ms
step:808/2330 train_time:46624ms step_avg:57.70ms
step:809/2330 train_time:46682ms step_avg:57.70ms
step:810/2330 train_time:46742ms step_avg:57.71ms
step:811/2330 train_time:46799ms step_avg:57.71ms
step:812/2330 train_time:46859ms step_avg:57.71ms
step:813/2330 train_time:46916ms step_avg:57.71ms
step:814/2330 train_time:46976ms step_avg:57.71ms
step:815/2330 train_time:47033ms step_avg:57.71ms
step:816/2330 train_time:47092ms step_avg:57.71ms
step:817/2330 train_time:47149ms step_avg:57.71ms
step:818/2330 train_time:47208ms step_avg:57.71ms
step:819/2330 train_time:47264ms step_avg:57.71ms
step:820/2330 train_time:47324ms step_avg:57.71ms
step:821/2330 train_time:47382ms step_avg:57.71ms
step:822/2330 train_time:47442ms step_avg:57.72ms
step:823/2330 train_time:47499ms step_avg:57.71ms
step:824/2330 train_time:47560ms step_avg:57.72ms
step:825/2330 train_time:47618ms step_avg:57.72ms
step:826/2330 train_time:47677ms step_avg:57.72ms
step:827/2330 train_time:47735ms step_avg:57.72ms
step:828/2330 train_time:47796ms step_avg:57.72ms
step:829/2330 train_time:47853ms step_avg:57.72ms
step:830/2330 train_time:47913ms step_avg:57.73ms
step:831/2330 train_time:47970ms step_avg:57.73ms
step:832/2330 train_time:48029ms step_avg:57.73ms
step:833/2330 train_time:48086ms step_avg:57.73ms
step:834/2330 train_time:48145ms step_avg:57.73ms
step:835/2330 train_time:48201ms step_avg:57.73ms
step:836/2330 train_time:48261ms step_avg:57.73ms
step:837/2330 train_time:48318ms step_avg:57.73ms
step:838/2330 train_time:48379ms step_avg:57.73ms
step:839/2330 train_time:48436ms step_avg:57.73ms
step:840/2330 train_time:48496ms step_avg:57.73ms
step:841/2330 train_time:48553ms step_avg:57.73ms
step:842/2330 train_time:48613ms step_avg:57.74ms
step:843/2330 train_time:48670ms step_avg:57.73ms
step:844/2330 train_time:48730ms step_avg:57.74ms
step:845/2330 train_time:48787ms step_avg:57.74ms
step:846/2330 train_time:48847ms step_avg:57.74ms
step:847/2330 train_time:48905ms step_avg:57.74ms
step:848/2330 train_time:48965ms step_avg:57.74ms
step:849/2330 train_time:49022ms step_avg:57.74ms
step:850/2330 train_time:49082ms step_avg:57.74ms
step:851/2330 train_time:49138ms step_avg:57.74ms
step:852/2330 train_time:49197ms step_avg:57.74ms
step:853/2330 train_time:49254ms step_avg:57.74ms
step:854/2330 train_time:49314ms step_avg:57.75ms
step:855/2330 train_time:49371ms step_avg:57.74ms
step:856/2330 train_time:49431ms step_avg:57.75ms
step:857/2330 train_time:49488ms step_avg:57.75ms
step:858/2330 train_time:49548ms step_avg:57.75ms
step:859/2330 train_time:49605ms step_avg:57.75ms
step:860/2330 train_time:49665ms step_avg:57.75ms
step:861/2330 train_time:49722ms step_avg:57.75ms
step:862/2330 train_time:49783ms step_avg:57.75ms
step:863/2330 train_time:49840ms step_avg:57.75ms
step:864/2330 train_time:49900ms step_avg:57.75ms
step:865/2330 train_time:49957ms step_avg:57.75ms
step:866/2330 train_time:50018ms step_avg:57.76ms
step:867/2330 train_time:50075ms step_avg:57.76ms
step:868/2330 train_time:50135ms step_avg:57.76ms
step:869/2330 train_time:50192ms step_avg:57.76ms
step:870/2330 train_time:50251ms step_avg:57.76ms
step:871/2330 train_time:50307ms step_avg:57.76ms
step:872/2330 train_time:50368ms step_avg:57.76ms
step:873/2330 train_time:50425ms step_avg:57.76ms
step:874/2330 train_time:50485ms step_avg:57.76ms
step:875/2330 train_time:50543ms step_avg:57.76ms
step:876/2330 train_time:50602ms step_avg:57.76ms
step:877/2330 train_time:50659ms step_avg:57.76ms
step:878/2330 train_time:50719ms step_avg:57.77ms
step:879/2330 train_time:50776ms step_avg:57.77ms
step:880/2330 train_time:50836ms step_avg:57.77ms
step:881/2330 train_time:50894ms step_avg:57.77ms
step:882/2330 train_time:50954ms step_avg:57.77ms
step:883/2330 train_time:51010ms step_avg:57.77ms
step:884/2330 train_time:51070ms step_avg:57.77ms
step:885/2330 train_time:51127ms step_avg:57.77ms
step:886/2330 train_time:51187ms step_avg:57.77ms
step:887/2330 train_time:51244ms step_avg:57.77ms
step:888/2330 train_time:51303ms step_avg:57.77ms
step:889/2330 train_time:51361ms step_avg:57.77ms
step:890/2330 train_time:51420ms step_avg:57.78ms
step:891/2330 train_time:51478ms step_avg:57.78ms
step:892/2330 train_time:51537ms step_avg:57.78ms
step:893/2330 train_time:51595ms step_avg:57.78ms
step:894/2330 train_time:51654ms step_avg:57.78ms
step:895/2330 train_time:51711ms step_avg:57.78ms
step:896/2330 train_time:51771ms step_avg:57.78ms
step:897/2330 train_time:51829ms step_avg:57.78ms
step:898/2330 train_time:51888ms step_avg:57.78ms
step:899/2330 train_time:51946ms step_avg:57.78ms
step:900/2330 train_time:52005ms step_avg:57.78ms
step:901/2330 train_time:52063ms step_avg:57.78ms
step:902/2330 train_time:52123ms step_avg:57.79ms
step:903/2330 train_time:52179ms step_avg:57.78ms
step:904/2330 train_time:52239ms step_avg:57.79ms
step:905/2330 train_time:52296ms step_avg:57.79ms
step:906/2330 train_time:52356ms step_avg:57.79ms
step:907/2330 train_time:52413ms step_avg:57.79ms
step:908/2330 train_time:52473ms step_avg:57.79ms
step:909/2330 train_time:52530ms step_avg:57.79ms
step:910/2330 train_time:52589ms step_avg:57.79ms
step:911/2330 train_time:52645ms step_avg:57.79ms
step:912/2330 train_time:52707ms step_avg:57.79ms
step:913/2330 train_time:52764ms step_avg:57.79ms
step:914/2330 train_time:52825ms step_avg:57.79ms
step:915/2330 train_time:52882ms step_avg:57.79ms
step:916/2330 train_time:52942ms step_avg:57.80ms
step:917/2330 train_time:52999ms step_avg:57.80ms
step:918/2330 train_time:53059ms step_avg:57.80ms
step:919/2330 train_time:53116ms step_avg:57.80ms
step:920/2330 train_time:53175ms step_avg:57.80ms
step:921/2330 train_time:53232ms step_avg:57.80ms
step:922/2330 train_time:53293ms step_avg:57.80ms
step:923/2330 train_time:53350ms step_avg:57.80ms
step:924/2330 train_time:53410ms step_avg:57.80ms
step:925/2330 train_time:53466ms step_avg:57.80ms
step:926/2330 train_time:53527ms step_avg:57.80ms
step:927/2330 train_time:53584ms step_avg:57.80ms
step:928/2330 train_time:53644ms step_avg:57.81ms
step:929/2330 train_time:53702ms step_avg:57.81ms
step:930/2330 train_time:53761ms step_avg:57.81ms
step:931/2330 train_time:53819ms step_avg:57.81ms
step:932/2330 train_time:53879ms step_avg:57.81ms
step:933/2330 train_time:53935ms step_avg:57.81ms
step:934/2330 train_time:53996ms step_avg:57.81ms
step:935/2330 train_time:54054ms step_avg:57.81ms
step:936/2330 train_time:54113ms step_avg:57.81ms
step:937/2330 train_time:54169ms step_avg:57.81ms
step:938/2330 train_time:54229ms step_avg:57.81ms
step:939/2330 train_time:54286ms step_avg:57.81ms
step:940/2330 train_time:54346ms step_avg:57.81ms
step:941/2330 train_time:54403ms step_avg:57.81ms
step:942/2330 train_time:54462ms step_avg:57.82ms
step:943/2330 train_time:54519ms step_avg:57.81ms
step:944/2330 train_time:54580ms step_avg:57.82ms
step:945/2330 train_time:54637ms step_avg:57.82ms
step:946/2330 train_time:54697ms step_avg:57.82ms
step:947/2330 train_time:54755ms step_avg:57.82ms
step:948/2330 train_time:54814ms step_avg:57.82ms
step:949/2330 train_time:54871ms step_avg:57.82ms
step:950/2330 train_time:54931ms step_avg:57.82ms
step:951/2330 train_time:54989ms step_avg:57.82ms
step:952/2330 train_time:55048ms step_avg:57.82ms
step:953/2330 train_time:55106ms step_avg:57.82ms
step:954/2330 train_time:55166ms step_avg:57.83ms
step:955/2330 train_time:55223ms step_avg:57.82ms
step:956/2330 train_time:55282ms step_avg:57.83ms
step:957/2330 train_time:55340ms step_avg:57.83ms
step:958/2330 train_time:55400ms step_avg:57.83ms
step:959/2330 train_time:55457ms step_avg:57.83ms
step:960/2330 train_time:55516ms step_avg:57.83ms
step:961/2330 train_time:55573ms step_avg:57.83ms
step:962/2330 train_time:55634ms step_avg:57.83ms
step:963/2330 train_time:55691ms step_avg:57.83ms
step:964/2330 train_time:55751ms step_avg:57.83ms
step:965/2330 train_time:55807ms step_avg:57.83ms
step:966/2330 train_time:55867ms step_avg:57.83ms
step:967/2330 train_time:55924ms step_avg:57.83ms
step:968/2330 train_time:55985ms step_avg:57.84ms
step:969/2330 train_time:56042ms step_avg:57.83ms
step:970/2330 train_time:56102ms step_avg:57.84ms
step:971/2330 train_time:56160ms step_avg:57.84ms
step:972/2330 train_time:56220ms step_avg:57.84ms
step:973/2330 train_time:56277ms step_avg:57.84ms
step:974/2330 train_time:56336ms step_avg:57.84ms
step:975/2330 train_time:56393ms step_avg:57.84ms
step:976/2330 train_time:56453ms step_avg:57.84ms
step:977/2330 train_time:56510ms step_avg:57.84ms
step:978/2330 train_time:56570ms step_avg:57.84ms
step:979/2330 train_time:56627ms step_avg:57.84ms
step:980/2330 train_time:56687ms step_avg:57.84ms
step:981/2330 train_time:56745ms step_avg:57.84ms
step:982/2330 train_time:56804ms step_avg:57.85ms
step:983/2330 train_time:56862ms step_avg:57.85ms
step:984/2330 train_time:56922ms step_avg:57.85ms
step:985/2330 train_time:56978ms step_avg:57.85ms
step:986/2330 train_time:57040ms step_avg:57.85ms
step:987/2330 train_time:57097ms step_avg:57.85ms
step:988/2330 train_time:57157ms step_avg:57.85ms
step:989/2330 train_time:57214ms step_avg:57.85ms
step:990/2330 train_time:57273ms step_avg:57.85ms
step:991/2330 train_time:57331ms step_avg:57.85ms
step:992/2330 train_time:57390ms step_avg:57.85ms
step:993/2330 train_time:57447ms step_avg:57.85ms
step:994/2330 train_time:57507ms step_avg:57.85ms
step:995/2330 train_time:57564ms step_avg:57.85ms
step:996/2330 train_time:57624ms step_avg:57.86ms
step:997/2330 train_time:57682ms step_avg:57.86ms
step:998/2330 train_time:57742ms step_avg:57.86ms
step:999/2330 train_time:57800ms step_avg:57.86ms
step:1000/2330 train_time:57860ms step_avg:57.86ms
step:1000/2330 val_loss:4.0600 train_time:57940ms step_avg:57.94ms
step:1001/2330 train_time:57961ms step_avg:57.90ms
step:1002/2330 train_time:57982ms step_avg:57.87ms
step:1003/2330 train_time:58033ms step_avg:57.86ms
step:1004/2330 train_time:58104ms step_avg:57.87ms
step:1005/2330 train_time:58160ms step_avg:57.87ms
step:1006/2330 train_time:58223ms step_avg:57.88ms
step:1007/2330 train_time:58279ms step_avg:57.87ms
step:1008/2330 train_time:58338ms step_avg:57.87ms
step:1009/2330 train_time:58394ms step_avg:57.87ms
step:1010/2330 train_time:58453ms step_avg:57.87ms
step:1011/2330 train_time:58509ms step_avg:57.87ms
step:1012/2330 train_time:58568ms step_avg:57.87ms
step:1013/2330 train_time:58624ms step_avg:57.87ms
step:1014/2330 train_time:58683ms step_avg:57.87ms
step:1015/2330 train_time:58738ms step_avg:57.87ms
step:1016/2330 train_time:58798ms step_avg:57.87ms
step:1017/2330 train_time:58857ms step_avg:57.87ms
step:1018/2330 train_time:58919ms step_avg:57.88ms
step:1019/2330 train_time:58977ms step_avg:57.88ms
step:1020/2330 train_time:59040ms step_avg:57.88ms
step:1021/2330 train_time:59097ms step_avg:57.88ms
step:1022/2330 train_time:59157ms step_avg:57.88ms
step:1023/2330 train_time:59214ms step_avg:57.88ms
step:1024/2330 train_time:59274ms step_avg:57.88ms
step:1025/2330 train_time:59330ms step_avg:57.88ms
step:1026/2330 train_time:59390ms step_avg:57.89ms
step:1027/2330 train_time:59447ms step_avg:57.88ms
step:1028/2330 train_time:59506ms step_avg:57.89ms
step:1029/2330 train_time:59562ms step_avg:57.88ms
step:1030/2330 train_time:59622ms step_avg:57.89ms
step:1031/2330 train_time:59678ms step_avg:57.88ms
step:1032/2330 train_time:59738ms step_avg:57.89ms
step:1033/2330 train_time:59794ms step_avg:57.88ms
step:1034/2330 train_time:59856ms step_avg:57.89ms
step:1035/2330 train_time:59913ms step_avg:57.89ms
step:1036/2330 train_time:59976ms step_avg:57.89ms
step:1037/2330 train_time:60034ms step_avg:57.89ms
step:1038/2330 train_time:60094ms step_avg:57.89ms
step:1039/2330 train_time:60151ms step_avg:57.89ms
step:1040/2330 train_time:60212ms step_avg:57.90ms
step:1041/2330 train_time:60269ms step_avg:57.90ms
step:1042/2330 train_time:60329ms step_avg:57.90ms
step:1043/2330 train_time:60385ms step_avg:57.90ms
step:1044/2330 train_time:60445ms step_avg:57.90ms
step:1045/2330 train_time:60502ms step_avg:57.90ms
step:1046/2330 train_time:60561ms step_avg:57.90ms
step:1047/2330 train_time:60617ms step_avg:57.90ms
step:1048/2330 train_time:60677ms step_avg:57.90ms
step:1049/2330 train_time:60733ms step_avg:57.90ms
step:1050/2330 train_time:60795ms step_avg:57.90ms
step:1051/2330 train_time:60852ms step_avg:57.90ms
step:1052/2330 train_time:60912ms step_avg:57.90ms
step:1053/2330 train_time:60969ms step_avg:57.90ms
step:1054/2330 train_time:61031ms step_avg:57.90ms
step:1055/2330 train_time:61088ms step_avg:57.90ms
step:1056/2330 train_time:61150ms step_avg:57.91ms
step:1057/2330 train_time:61207ms step_avg:57.91ms
step:1058/2330 train_time:61268ms step_avg:57.91ms
step:1059/2330 train_time:61324ms step_avg:57.91ms
step:1060/2330 train_time:61386ms step_avg:57.91ms
step:1061/2330 train_time:61443ms step_avg:57.91ms
step:1062/2330 train_time:61503ms step_avg:57.91ms
step:1063/2330 train_time:61559ms step_avg:57.91ms
step:1064/2330 train_time:61618ms step_avg:57.91ms
step:1065/2330 train_time:61674ms step_avg:57.91ms
step:1066/2330 train_time:61735ms step_avg:57.91ms
step:1067/2330 train_time:61791ms step_avg:57.91ms
step:1068/2330 train_time:61852ms step_avg:57.91ms
step:1069/2330 train_time:61909ms step_avg:57.91ms
step:1070/2330 train_time:61970ms step_avg:57.92ms
step:1071/2330 train_time:62027ms step_avg:57.91ms
step:1072/2330 train_time:62087ms step_avg:57.92ms
step:1073/2330 train_time:62144ms step_avg:57.92ms
step:1074/2330 train_time:62205ms step_avg:57.92ms
step:1075/2330 train_time:62261ms step_avg:57.92ms
step:1076/2330 train_time:62322ms step_avg:57.92ms
step:1077/2330 train_time:62379ms step_avg:57.92ms
step:1078/2330 train_time:62438ms step_avg:57.92ms
step:1079/2330 train_time:62495ms step_avg:57.92ms
step:1080/2330 train_time:62555ms step_avg:57.92ms
step:1081/2330 train_time:62611ms step_avg:57.92ms
step:1082/2330 train_time:62671ms step_avg:57.92ms
step:1083/2330 train_time:62728ms step_avg:57.92ms
step:1084/2330 train_time:62788ms step_avg:57.92ms
step:1085/2330 train_time:62844ms step_avg:57.92ms
step:1086/2330 train_time:62905ms step_avg:57.92ms
step:1087/2330 train_time:62962ms step_avg:57.92ms
step:1088/2330 train_time:63022ms step_avg:57.92ms
step:1089/2330 train_time:63079ms step_avg:57.92ms
step:1090/2330 train_time:63139ms step_avg:57.93ms
step:1091/2330 train_time:63197ms step_avg:57.93ms
step:1092/2330 train_time:63257ms step_avg:57.93ms
step:1093/2330 train_time:63313ms step_avg:57.93ms
step:1094/2330 train_time:63375ms step_avg:57.93ms
step:1095/2330 train_time:63431ms step_avg:57.93ms
step:1096/2330 train_time:63492ms step_avg:57.93ms
step:1097/2330 train_time:63549ms step_avg:57.93ms
step:1098/2330 train_time:63609ms step_avg:57.93ms
step:1099/2330 train_time:63666ms step_avg:57.93ms
step:1100/2330 train_time:63726ms step_avg:57.93ms
step:1101/2330 train_time:63783ms step_avg:57.93ms
step:1102/2330 train_time:63842ms step_avg:57.93ms
step:1103/2330 train_time:63899ms step_avg:57.93ms
step:1104/2330 train_time:63959ms step_avg:57.93ms
step:1105/2330 train_time:64016ms step_avg:57.93ms
step:1106/2330 train_time:64076ms step_avg:57.94ms
step:1107/2330 train_time:64134ms step_avg:57.93ms
step:1108/2330 train_time:64194ms step_avg:57.94ms
step:1109/2330 train_time:64250ms step_avg:57.94ms
step:1110/2330 train_time:64311ms step_avg:57.94ms
step:1111/2330 train_time:64369ms step_avg:57.94ms
step:1112/2330 train_time:64429ms step_avg:57.94ms
step:1113/2330 train_time:64486ms step_avg:57.94ms
step:1114/2330 train_time:64547ms step_avg:57.94ms
step:1115/2330 train_time:64604ms step_avg:57.94ms
step:1116/2330 train_time:64663ms step_avg:57.94ms
step:1117/2330 train_time:64719ms step_avg:57.94ms
step:1118/2330 train_time:64780ms step_avg:57.94ms
step:1119/2330 train_time:64837ms step_avg:57.94ms
step:1120/2330 train_time:64897ms step_avg:57.94ms
step:1121/2330 train_time:64954ms step_avg:57.94ms
step:1122/2330 train_time:65014ms step_avg:57.94ms
step:1123/2330 train_time:65071ms step_avg:57.94ms
step:1124/2330 train_time:65132ms step_avg:57.95ms
step:1125/2330 train_time:65189ms step_avg:57.95ms
step:1126/2330 train_time:65249ms step_avg:57.95ms
step:1127/2330 train_time:65307ms step_avg:57.95ms
step:1128/2330 train_time:65367ms step_avg:57.95ms
step:1129/2330 train_time:65424ms step_avg:57.95ms
step:1130/2330 train_time:65484ms step_avg:57.95ms
step:1131/2330 train_time:65541ms step_avg:57.95ms
step:1132/2330 train_time:65601ms step_avg:57.95ms
step:1133/2330 train_time:65658ms step_avg:57.95ms
step:1134/2330 train_time:65717ms step_avg:57.95ms
step:1135/2330 train_time:65775ms step_avg:57.95ms
step:1136/2330 train_time:65835ms step_avg:57.95ms
step:1137/2330 train_time:65892ms step_avg:57.95ms
step:1138/2330 train_time:65952ms step_avg:57.95ms
step:1139/2330 train_time:66010ms step_avg:57.95ms
step:1140/2330 train_time:66070ms step_avg:57.96ms
step:1141/2330 train_time:66127ms step_avg:57.96ms
step:1142/2330 train_time:66187ms step_avg:57.96ms
step:1143/2330 train_time:66244ms step_avg:57.96ms
step:1144/2330 train_time:66304ms step_avg:57.96ms
step:1145/2330 train_time:66361ms step_avg:57.96ms
step:1146/2330 train_time:66420ms step_avg:57.96ms
step:1147/2330 train_time:66478ms step_avg:57.96ms
step:1148/2330 train_time:66538ms step_avg:57.96ms
step:1149/2330 train_time:66595ms step_avg:57.96ms
step:1150/2330 train_time:66655ms step_avg:57.96ms
step:1151/2330 train_time:66712ms step_avg:57.96ms
step:1152/2330 train_time:66773ms step_avg:57.96ms
step:1153/2330 train_time:66830ms step_avg:57.96ms
step:1154/2330 train_time:66890ms step_avg:57.96ms
step:1155/2330 train_time:66949ms step_avg:57.96ms
step:1156/2330 train_time:67009ms step_avg:57.97ms
step:1157/2330 train_time:67066ms step_avg:57.97ms
step:1158/2330 train_time:67126ms step_avg:57.97ms
step:1159/2330 train_time:67183ms step_avg:57.97ms
step:1160/2330 train_time:67243ms step_avg:57.97ms
step:1161/2330 train_time:67300ms step_avg:57.97ms
step:1162/2330 train_time:67360ms step_avg:57.97ms
step:1163/2330 train_time:67417ms step_avg:57.97ms
step:1164/2330 train_time:67477ms step_avg:57.97ms
step:1165/2330 train_time:67534ms step_avg:57.97ms
step:1166/2330 train_time:67594ms step_avg:57.97ms
step:1167/2330 train_time:67651ms step_avg:57.97ms
step:1168/2330 train_time:67711ms step_avg:57.97ms
step:1169/2330 train_time:67769ms step_avg:57.97ms
step:1170/2330 train_time:67829ms step_avg:57.97ms
step:1171/2330 train_time:67886ms step_avg:57.97ms
step:1172/2330 train_time:67946ms step_avg:57.97ms
step:1173/2330 train_time:68004ms step_avg:57.97ms
step:1174/2330 train_time:68063ms step_avg:57.98ms
step:1175/2330 train_time:68120ms step_avg:57.97ms
step:1176/2330 train_time:68180ms step_avg:57.98ms
step:1177/2330 train_time:68237ms step_avg:57.98ms
step:1178/2330 train_time:68297ms step_avg:57.98ms
step:1179/2330 train_time:68353ms step_avg:57.98ms
step:1180/2330 train_time:68414ms step_avg:57.98ms
step:1181/2330 train_time:68471ms step_avg:57.98ms
step:1182/2330 train_time:68532ms step_avg:57.98ms
step:1183/2330 train_time:68589ms step_avg:57.98ms
step:1184/2330 train_time:68650ms step_avg:57.98ms
step:1185/2330 train_time:68707ms step_avg:57.98ms
step:1186/2330 train_time:68767ms step_avg:57.98ms
step:1187/2330 train_time:68824ms step_avg:57.98ms
step:1188/2330 train_time:68883ms step_avg:57.98ms
step:1189/2330 train_time:68941ms step_avg:57.98ms
step:1190/2330 train_time:69000ms step_avg:57.98ms
step:1191/2330 train_time:69058ms step_avg:57.98ms
step:1192/2330 train_time:69118ms step_avg:57.98ms
step:1193/2330 train_time:69175ms step_avg:57.98ms
step:1194/2330 train_time:69234ms step_avg:57.98ms
step:1195/2330 train_time:69291ms step_avg:57.98ms
step:1196/2330 train_time:69351ms step_avg:57.99ms
step:1197/2330 train_time:69409ms step_avg:57.99ms
step:1198/2330 train_time:69468ms step_avg:57.99ms
step:1199/2330 train_time:69525ms step_avg:57.99ms
step:1200/2330 train_time:69585ms step_avg:57.99ms
step:1201/2330 train_time:69642ms step_avg:57.99ms
step:1202/2330 train_time:69703ms step_avg:57.99ms
step:1203/2330 train_time:69760ms step_avg:57.99ms
step:1204/2330 train_time:69820ms step_avg:57.99ms
step:1205/2330 train_time:69877ms step_avg:57.99ms
step:1206/2330 train_time:69937ms step_avg:57.99ms
step:1207/2330 train_time:69994ms step_avg:57.99ms
step:1208/2330 train_time:70055ms step_avg:57.99ms
step:1209/2330 train_time:70111ms step_avg:57.99ms
step:1210/2330 train_time:70171ms step_avg:57.99ms
step:1211/2330 train_time:70228ms step_avg:57.99ms
step:1212/2330 train_time:70289ms step_avg:57.99ms
step:1213/2330 train_time:70347ms step_avg:57.99ms
step:1214/2330 train_time:70407ms step_avg:58.00ms
step:1215/2330 train_time:70464ms step_avg:57.99ms
step:1216/2330 train_time:70523ms step_avg:58.00ms
step:1217/2330 train_time:70580ms step_avg:57.99ms
step:1218/2330 train_time:70640ms step_avg:58.00ms
step:1219/2330 train_time:70697ms step_avg:58.00ms
step:1220/2330 train_time:70757ms step_avg:58.00ms
step:1221/2330 train_time:70813ms step_avg:58.00ms
step:1222/2330 train_time:70873ms step_avg:58.00ms
step:1223/2330 train_time:70930ms step_avg:58.00ms
step:1224/2330 train_time:70991ms step_avg:58.00ms
step:1225/2330 train_time:71049ms step_avg:58.00ms
step:1226/2330 train_time:71109ms step_avg:58.00ms
step:1227/2330 train_time:71166ms step_avg:58.00ms
step:1228/2330 train_time:71226ms step_avg:58.00ms
step:1229/2330 train_time:71283ms step_avg:58.00ms
step:1230/2330 train_time:71343ms step_avg:58.00ms
step:1231/2330 train_time:71400ms step_avg:58.00ms
step:1232/2330 train_time:71459ms step_avg:58.00ms
step:1233/2330 train_time:71516ms step_avg:58.00ms
step:1234/2330 train_time:71576ms step_avg:58.00ms
step:1235/2330 train_time:71633ms step_avg:58.00ms
step:1236/2330 train_time:71693ms step_avg:58.00ms
step:1237/2330 train_time:71751ms step_avg:58.00ms
step:1238/2330 train_time:71810ms step_avg:58.01ms
step:1239/2330 train_time:71868ms step_avg:58.00ms
step:1240/2330 train_time:71928ms step_avg:58.01ms
step:1241/2330 train_time:71986ms step_avg:58.01ms
step:1242/2330 train_time:72045ms step_avg:58.01ms
step:1243/2330 train_time:72102ms step_avg:58.01ms
step:1244/2330 train_time:72162ms step_avg:58.01ms
step:1245/2330 train_time:72219ms step_avg:58.01ms
step:1246/2330 train_time:72279ms step_avg:58.01ms
step:1247/2330 train_time:72337ms step_avg:58.01ms
step:1248/2330 train_time:72397ms step_avg:58.01ms
step:1249/2330 train_time:72454ms step_avg:58.01ms
step:1250/2330 train_time:72514ms step_avg:58.01ms
step:1250/2330 val_loss:3.9836 train_time:72594ms step_avg:58.08ms
step:1251/2330 train_time:72614ms step_avg:58.05ms
step:1252/2330 train_time:72634ms step_avg:58.01ms
step:1253/2330 train_time:72694ms step_avg:58.02ms
step:1254/2330 train_time:72758ms step_avg:58.02ms
step:1255/2330 train_time:72815ms step_avg:58.02ms
step:1256/2330 train_time:72877ms step_avg:58.02ms
step:1257/2330 train_time:72933ms step_avg:58.02ms
step:1258/2330 train_time:72995ms step_avg:58.02ms
step:1259/2330 train_time:73050ms step_avg:58.02ms
step:1260/2330 train_time:73110ms step_avg:58.02ms
step:1261/2330 train_time:73166ms step_avg:58.02ms
step:1262/2330 train_time:73226ms step_avg:58.02ms
step:1263/2330 train_time:73283ms step_avg:58.02ms
step:1264/2330 train_time:73342ms step_avg:58.02ms
step:1265/2330 train_time:73398ms step_avg:58.02ms
step:1266/2330 train_time:73457ms step_avg:58.02ms
step:1267/2330 train_time:73514ms step_avg:58.02ms
step:1268/2330 train_time:73574ms step_avg:58.02ms
step:1269/2330 train_time:73632ms step_avg:58.02ms
step:1270/2330 train_time:73694ms step_avg:58.03ms
step:1271/2330 train_time:73751ms step_avg:58.03ms
step:1272/2330 train_time:73813ms step_avg:58.03ms
step:1273/2330 train_time:73871ms step_avg:58.03ms
step:1274/2330 train_time:73931ms step_avg:58.03ms
step:1275/2330 train_time:73987ms step_avg:58.03ms
step:1276/2330 train_time:74048ms step_avg:58.03ms
step:1277/2330 train_time:74104ms step_avg:58.03ms
step:1278/2330 train_time:74165ms step_avg:58.03ms
step:1279/2330 train_time:74221ms step_avg:58.03ms
step:1280/2330 train_time:74281ms step_avg:58.03ms
step:1281/2330 train_time:74338ms step_avg:58.03ms
step:1282/2330 train_time:74397ms step_avg:58.03ms
step:1283/2330 train_time:74454ms step_avg:58.03ms
step:1284/2330 train_time:74513ms step_avg:58.03ms
step:1285/2330 train_time:74570ms step_avg:58.03ms
step:1286/2330 train_time:74632ms step_avg:58.03ms
step:1287/2330 train_time:74690ms step_avg:58.03ms
step:1288/2330 train_time:74752ms step_avg:58.04ms
step:1289/2330 train_time:74809ms step_avg:58.04ms
step:1290/2330 train_time:74870ms step_avg:58.04ms
step:1291/2330 train_time:74928ms step_avg:58.04ms
step:1292/2330 train_time:75417ms step_avg:58.37ms
step:1293/2330 train_time:75473ms step_avg:58.37ms
step:1294/2330 train_time:75531ms step_avg:58.37ms
step:1295/2330 train_time:75588ms step_avg:58.37ms
step:1296/2330 train_time:75647ms step_avg:58.37ms
step:1297/2330 train_time:75703ms step_avg:58.37ms
step:1298/2330 train_time:75762ms step_avg:58.37ms
step:1299/2330 train_time:75818ms step_avg:58.37ms
step:1300/2330 train_time:75878ms step_avg:58.37ms
step:1301/2330 train_time:75934ms step_avg:58.37ms
step:1302/2330 train_time:75993ms step_avg:58.37ms
step:1303/2330 train_time:76049ms step_avg:58.36ms
step:1304/2330 train_time:76108ms step_avg:58.37ms
step:1305/2330 train_time:76164ms step_avg:58.36ms
step:1306/2330 train_time:76224ms step_avg:58.36ms
step:1307/2330 train_time:76284ms step_avg:58.37ms
step:1308/2330 train_time:76351ms step_avg:58.37ms
step:1309/2330 train_time:76410ms step_avg:58.37ms
step:1310/2330 train_time:76472ms step_avg:58.38ms
step:1311/2330 train_time:76529ms step_avg:58.37ms
step:1312/2330 train_time:76589ms step_avg:58.38ms
step:1313/2330 train_time:76645ms step_avg:58.37ms
step:1314/2330 train_time:76705ms step_avg:58.38ms
step:1315/2330 train_time:76762ms step_avg:58.37ms
step:1316/2330 train_time:76822ms step_avg:58.38ms
step:1317/2330 train_time:76879ms step_avg:58.37ms
step:1318/2330 train_time:76938ms step_avg:58.37ms
step:1319/2330 train_time:76994ms step_avg:58.37ms
step:1320/2330 train_time:77053ms step_avg:58.37ms
step:1321/2330 train_time:77109ms step_avg:58.37ms
step:1322/2330 train_time:77169ms step_avg:58.37ms
step:1323/2330 train_time:77227ms step_avg:58.37ms
step:1324/2330 train_time:77289ms step_avg:58.38ms
step:1325/2330 train_time:77347ms step_avg:58.37ms
step:1326/2330 train_time:77410ms step_avg:58.38ms
step:1327/2330 train_time:77467ms step_avg:58.38ms
step:1328/2330 train_time:77529ms step_avg:58.38ms
step:1329/2330 train_time:77586ms step_avg:58.38ms
step:1330/2330 train_time:77645ms step_avg:58.38ms
step:1331/2330 train_time:77703ms step_avg:58.38ms
step:1332/2330 train_time:77763ms step_avg:58.38ms
step:1333/2330 train_time:77821ms step_avg:58.38ms
step:1334/2330 train_time:77880ms step_avg:58.38ms
step:1335/2330 train_time:77937ms step_avg:58.38ms
step:1336/2330 train_time:77996ms step_avg:58.38ms
step:1337/2330 train_time:78052ms step_avg:58.38ms
step:1338/2330 train_time:78112ms step_avg:58.38ms
step:1339/2330 train_time:78168ms step_avg:58.38ms
step:1340/2330 train_time:78229ms step_avg:58.38ms
step:1341/2330 train_time:78286ms step_avg:58.38ms
step:1342/2330 train_time:78348ms step_avg:58.38ms
step:1343/2330 train_time:78406ms step_avg:58.38ms
step:1344/2330 train_time:78468ms step_avg:58.38ms
step:1345/2330 train_time:78525ms step_avg:58.38ms
step:1346/2330 train_time:78585ms step_avg:58.38ms
step:1347/2330 train_time:78643ms step_avg:58.38ms
step:1348/2330 train_time:78703ms step_avg:58.38ms
step:1349/2330 train_time:78760ms step_avg:58.38ms
step:1350/2330 train_time:78819ms step_avg:58.38ms
step:1351/2330 train_time:78877ms step_avg:58.38ms
step:1352/2330 train_time:78936ms step_avg:58.38ms
step:1353/2330 train_time:78993ms step_avg:58.38ms
step:1354/2330 train_time:79051ms step_avg:58.38ms
step:1355/2330 train_time:79108ms step_avg:58.38ms
step:1356/2330 train_time:79168ms step_avg:58.38ms
step:1357/2330 train_time:79226ms step_avg:58.38ms
step:1358/2330 train_time:79286ms step_avg:58.38ms
step:1359/2330 train_time:79342ms step_avg:58.38ms
step:1360/2330 train_time:79404ms step_avg:58.39ms
step:1361/2330 train_time:79461ms step_avg:58.38ms
step:1362/2330 train_time:79522ms step_avg:58.39ms
step:1363/2330 train_time:79579ms step_avg:58.39ms
step:1364/2330 train_time:79640ms step_avg:58.39ms
step:1365/2330 train_time:79697ms step_avg:58.39ms
step:1366/2330 train_time:79756ms step_avg:58.39ms
step:1367/2330 train_time:79813ms step_avg:58.39ms
step:1368/2330 train_time:79873ms step_avg:58.39ms
step:1369/2330 train_time:79930ms step_avg:58.39ms
step:1370/2330 train_time:79990ms step_avg:58.39ms
step:1371/2330 train_time:80046ms step_avg:58.39ms
step:1372/2330 train_time:80107ms step_avg:58.39ms
step:1373/2330 train_time:80163ms step_avg:58.39ms
step:1374/2330 train_time:80224ms step_avg:58.39ms
step:1375/2330 train_time:80281ms step_avg:58.39ms
step:1376/2330 train_time:80342ms step_avg:58.39ms
step:1377/2330 train_time:80398ms step_avg:58.39ms
step:1378/2330 train_time:80459ms step_avg:58.39ms
step:1379/2330 train_time:80515ms step_avg:58.39ms
step:1380/2330 train_time:80577ms step_avg:58.39ms
step:1381/2330 train_time:80634ms step_avg:58.39ms
step:1382/2330 train_time:80694ms step_avg:58.39ms
step:1383/2330 train_time:80751ms step_avg:58.39ms
step:1384/2330 train_time:80811ms step_avg:58.39ms
step:1385/2330 train_time:80867ms step_avg:58.39ms
step:1386/2330 train_time:80928ms step_avg:58.39ms
step:1387/2330 train_time:80985ms step_avg:58.39ms
step:1388/2330 train_time:81045ms step_avg:58.39ms
step:1389/2330 train_time:81102ms step_avg:58.39ms
step:1390/2330 train_time:81162ms step_avg:58.39ms
step:1391/2330 train_time:81219ms step_avg:58.39ms
step:1392/2330 train_time:81280ms step_avg:58.39ms
step:1393/2330 train_time:81337ms step_avg:58.39ms
step:1394/2330 train_time:81397ms step_avg:58.39ms
step:1395/2330 train_time:81454ms step_avg:58.39ms
step:1396/2330 train_time:81515ms step_avg:58.39ms
step:1397/2330 train_time:81573ms step_avg:58.39ms
step:1398/2330 train_time:81632ms step_avg:58.39ms
step:1399/2330 train_time:81689ms step_avg:58.39ms
step:1400/2330 train_time:81749ms step_avg:58.39ms
step:1401/2330 train_time:81806ms step_avg:58.39ms
step:1402/2330 train_time:81866ms step_avg:58.39ms
step:1403/2330 train_time:81923ms step_avg:58.39ms
step:1404/2330 train_time:81983ms step_avg:58.39ms
step:1405/2330 train_time:82040ms step_avg:58.39ms
step:1406/2330 train_time:82100ms step_avg:58.39ms
step:1407/2330 train_time:82157ms step_avg:58.39ms
step:1408/2330 train_time:82217ms step_avg:58.39ms
step:1409/2330 train_time:82275ms step_avg:58.39ms
step:1410/2330 train_time:82335ms step_avg:58.39ms
step:1411/2330 train_time:82393ms step_avg:58.39ms
step:1412/2330 train_time:82452ms step_avg:58.39ms
step:1413/2330 train_time:82509ms step_avg:58.39ms
step:1414/2330 train_time:82570ms step_avg:58.39ms
step:1415/2330 train_time:82627ms step_avg:58.39ms
step:1416/2330 train_time:82688ms step_avg:58.40ms
step:1417/2330 train_time:82746ms step_avg:58.40ms
step:1418/2330 train_time:82805ms step_avg:58.40ms
step:1419/2330 train_time:82862ms step_avg:58.39ms
step:1420/2330 train_time:82923ms step_avg:58.40ms
step:1421/2330 train_time:82979ms step_avg:58.39ms
step:1422/2330 train_time:83041ms step_avg:58.40ms
step:1423/2330 train_time:83097ms step_avg:58.40ms
step:1424/2330 train_time:83158ms step_avg:58.40ms
step:1425/2330 train_time:83215ms step_avg:58.40ms
step:1426/2330 train_time:83274ms step_avg:58.40ms
step:1427/2330 train_time:83332ms step_avg:58.40ms
step:1428/2330 train_time:83391ms step_avg:58.40ms
step:1429/2330 train_time:83448ms step_avg:58.40ms
step:1430/2330 train_time:83510ms step_avg:58.40ms
step:1431/2330 train_time:83566ms step_avg:58.40ms
step:1432/2330 train_time:83626ms step_avg:58.40ms
step:1433/2330 train_time:83683ms step_avg:58.40ms
step:1434/2330 train_time:83744ms step_avg:58.40ms
step:1435/2330 train_time:83801ms step_avg:58.40ms
step:1436/2330 train_time:83861ms step_avg:58.40ms
step:1437/2330 train_time:83919ms step_avg:58.40ms
step:1438/2330 train_time:83978ms step_avg:58.40ms
step:1439/2330 train_time:84036ms step_avg:58.40ms
step:1440/2330 train_time:84095ms step_avg:58.40ms
step:1441/2330 train_time:84153ms step_avg:58.40ms
step:1442/2330 train_time:84213ms step_avg:58.40ms
step:1443/2330 train_time:84269ms step_avg:58.40ms
step:1444/2330 train_time:84330ms step_avg:58.40ms
step:1445/2330 train_time:84387ms step_avg:58.40ms
step:1446/2330 train_time:84448ms step_avg:58.40ms
step:1447/2330 train_time:84506ms step_avg:58.40ms
step:1448/2330 train_time:84567ms step_avg:58.40ms
step:1449/2330 train_time:84624ms step_avg:58.40ms
step:1450/2330 train_time:84684ms step_avg:58.40ms
step:1451/2330 train_time:84740ms step_avg:58.40ms
step:1452/2330 train_time:84800ms step_avg:58.40ms
step:1453/2330 train_time:84857ms step_avg:58.40ms
step:1454/2330 train_time:84918ms step_avg:58.40ms
step:1455/2330 train_time:84974ms step_avg:58.40ms
step:1456/2330 train_time:85035ms step_avg:58.40ms
step:1457/2330 train_time:85091ms step_avg:58.40ms
step:1458/2330 train_time:85151ms step_avg:58.40ms
step:1459/2330 train_time:85208ms step_avg:58.40ms
step:1460/2330 train_time:85269ms step_avg:58.40ms
step:1461/2330 train_time:85326ms step_avg:58.40ms
step:1462/2330 train_time:85387ms step_avg:58.40ms
step:1463/2330 train_time:85443ms step_avg:58.40ms
step:1464/2330 train_time:85504ms step_avg:58.40ms
step:1465/2330 train_time:85560ms step_avg:58.40ms
step:1466/2330 train_time:85622ms step_avg:58.41ms
step:1467/2330 train_time:85679ms step_avg:58.40ms
step:1468/2330 train_time:85739ms step_avg:58.41ms
step:1469/2330 train_time:85796ms step_avg:58.40ms
step:1470/2330 train_time:85856ms step_avg:58.41ms
step:1471/2330 train_time:85913ms step_avg:58.40ms
step:1472/2330 train_time:85973ms step_avg:58.41ms
step:1473/2330 train_time:86030ms step_avg:58.40ms
step:1474/2330 train_time:86090ms step_avg:58.41ms
step:1475/2330 train_time:86148ms step_avg:58.41ms
step:1476/2330 train_time:86208ms step_avg:58.41ms
step:1477/2330 train_time:86265ms step_avg:58.41ms
step:1478/2330 train_time:86326ms step_avg:58.41ms
step:1479/2330 train_time:86383ms step_avg:58.41ms
step:1480/2330 train_time:86443ms step_avg:58.41ms
step:1481/2330 train_time:86500ms step_avg:58.41ms
step:1482/2330 train_time:86560ms step_avg:58.41ms
step:1483/2330 train_time:86617ms step_avg:58.41ms
step:1484/2330 train_time:86678ms step_avg:58.41ms
step:1485/2330 train_time:86735ms step_avg:58.41ms
step:1486/2330 train_time:86795ms step_avg:58.41ms
step:1487/2330 train_time:86852ms step_avg:58.41ms
step:1488/2330 train_time:86912ms step_avg:58.41ms
step:1489/2330 train_time:86969ms step_avg:58.41ms
step:1490/2330 train_time:87029ms step_avg:58.41ms
step:1491/2330 train_time:87085ms step_avg:58.41ms
step:1492/2330 train_time:87145ms step_avg:58.41ms
step:1493/2330 train_time:87202ms step_avg:58.41ms
step:1494/2330 train_time:87264ms step_avg:58.41ms
step:1495/2330 train_time:87321ms step_avg:58.41ms
step:1496/2330 train_time:87381ms step_avg:58.41ms
step:1497/2330 train_time:87439ms step_avg:58.41ms
step:1498/2330 train_time:87498ms step_avg:58.41ms
step:1499/2330 train_time:87555ms step_avg:58.41ms
step:1500/2330 train_time:87615ms step_avg:58.41ms
step:1500/2330 val_loss:3.9005 train_time:87696ms step_avg:58.46ms
step:1501/2330 train_time:87716ms step_avg:58.44ms
step:1502/2330 train_time:87737ms step_avg:58.41ms
step:1503/2330 train_time:87792ms step_avg:58.41ms
step:1504/2330 train_time:87858ms step_avg:58.42ms
step:1505/2330 train_time:87915ms step_avg:58.42ms
step:1506/2330 train_time:87980ms step_avg:58.42ms
step:1507/2330 train_time:88037ms step_avg:58.42ms
step:1508/2330 train_time:88096ms step_avg:58.42ms
step:1509/2330 train_time:88153ms step_avg:58.42ms
step:1510/2330 train_time:88212ms step_avg:58.42ms
step:1511/2330 train_time:88268ms step_avg:58.42ms
step:1512/2330 train_time:88328ms step_avg:58.42ms
step:1513/2330 train_time:88384ms step_avg:58.42ms
step:1514/2330 train_time:88443ms step_avg:58.42ms
step:1515/2330 train_time:88499ms step_avg:58.42ms
step:1516/2330 train_time:88559ms step_avg:58.42ms
step:1517/2330 train_time:88616ms step_avg:58.42ms
step:1518/2330 train_time:88676ms step_avg:58.42ms
step:1519/2330 train_time:88734ms step_avg:58.42ms
step:1520/2330 train_time:88795ms step_avg:58.42ms
step:1521/2330 train_time:88854ms step_avg:58.42ms
step:1522/2330 train_time:88916ms step_avg:58.42ms
step:1523/2330 train_time:88973ms step_avg:58.42ms
step:1524/2330 train_time:89034ms step_avg:58.42ms
step:1525/2330 train_time:89091ms step_avg:58.42ms
step:1526/2330 train_time:89151ms step_avg:58.42ms
step:1527/2330 train_time:89208ms step_avg:58.42ms
step:1528/2330 train_time:89268ms step_avg:58.42ms
step:1529/2330 train_time:89327ms step_avg:58.42ms
step:1530/2330 train_time:89386ms step_avg:58.42ms
step:1531/2330 train_time:89442ms step_avg:58.42ms
step:1532/2330 train_time:89504ms step_avg:58.42ms
step:1533/2330 train_time:89561ms step_avg:58.42ms
step:1534/2330 train_time:89621ms step_avg:58.42ms
step:1535/2330 train_time:89678ms step_avg:58.42ms
step:1536/2330 train_time:89739ms step_avg:58.42ms
step:1537/2330 train_time:89796ms step_avg:58.42ms
step:1538/2330 train_time:89859ms step_avg:58.43ms
step:1539/2330 train_time:89916ms step_avg:58.43ms
step:1540/2330 train_time:89979ms step_avg:58.43ms
step:1541/2330 train_time:90036ms step_avg:58.43ms
step:1542/2330 train_time:90097ms step_avg:58.43ms
step:1543/2330 train_time:90153ms step_avg:58.43ms
step:1544/2330 train_time:90214ms step_avg:58.43ms
step:1545/2330 train_time:90272ms step_avg:58.43ms
step:1546/2330 train_time:90332ms step_avg:58.43ms
step:1547/2330 train_time:90390ms step_avg:58.43ms
step:1548/2330 train_time:90451ms step_avg:58.43ms
step:1549/2330 train_time:90509ms step_avg:58.43ms
step:1550/2330 train_time:90570ms step_avg:58.43ms
step:1551/2330 train_time:90627ms step_avg:58.43ms
step:1552/2330 train_time:90688ms step_avg:58.43ms
step:1553/2330 train_time:90746ms step_avg:58.43ms
step:1554/2330 train_time:90807ms step_avg:58.43ms
step:1555/2330 train_time:90864ms step_avg:58.43ms
step:1556/2330 train_time:90928ms step_avg:58.44ms
step:1557/2330 train_time:90985ms step_avg:58.44ms
step:1558/2330 train_time:91047ms step_avg:58.44ms
step:1559/2330 train_time:91104ms step_avg:58.44ms
step:1560/2330 train_time:91166ms step_avg:58.44ms
step:1561/2330 train_time:91222ms step_avg:58.44ms
step:1562/2330 train_time:91284ms step_avg:58.44ms
step:1563/2330 train_time:91341ms step_avg:58.44ms
step:1564/2330 train_time:91401ms step_avg:58.44ms
step:1565/2330 train_time:91458ms step_avg:58.44ms
step:1566/2330 train_time:91518ms step_avg:58.44ms
step:1567/2330 train_time:91575ms step_avg:58.44ms
step:1568/2330 train_time:91636ms step_avg:58.44ms
step:1569/2330 train_time:91694ms step_avg:58.44ms
step:1570/2330 train_time:91755ms step_avg:58.44ms
step:1571/2330 train_time:91813ms step_avg:58.44ms
step:1572/2330 train_time:91874ms step_avg:58.44ms
step:1573/2330 train_time:91932ms step_avg:58.44ms
step:1574/2330 train_time:91994ms step_avg:58.45ms
step:1575/2330 train_time:92052ms step_avg:58.45ms
step:1576/2330 train_time:92114ms step_avg:58.45ms
step:1577/2330 train_time:92172ms step_avg:58.45ms
step:1578/2330 train_time:92232ms step_avg:58.45ms
step:1579/2330 train_time:92289ms step_avg:58.45ms
step:1580/2330 train_time:92350ms step_avg:58.45ms
step:1581/2330 train_time:92409ms step_avg:58.45ms
step:1582/2330 train_time:92469ms step_avg:58.45ms
step:1583/2330 train_time:92526ms step_avg:58.45ms
step:1584/2330 train_time:92587ms step_avg:58.45ms
step:1585/2330 train_time:92644ms step_avg:58.45ms
step:1586/2330 train_time:92705ms step_avg:58.45ms
step:1587/2330 train_time:92762ms step_avg:58.45ms
step:1588/2330 train_time:92823ms step_avg:58.45ms
step:1589/2330 train_time:92880ms step_avg:58.45ms
step:1590/2330 train_time:92943ms step_avg:58.45ms
step:1591/2330 train_time:93001ms step_avg:58.45ms
step:1592/2330 train_time:93062ms step_avg:58.46ms
step:1593/2330 train_time:93120ms step_avg:58.46ms
step:1594/2330 train_time:93181ms step_avg:58.46ms
step:1595/2330 train_time:93238ms step_avg:58.46ms
step:1596/2330 train_time:93299ms step_avg:58.46ms
step:1597/2330 train_time:93356ms step_avg:58.46ms
step:1598/2330 train_time:93417ms step_avg:58.46ms
step:1599/2330 train_time:93473ms step_avg:58.46ms
step:1600/2330 train_time:93535ms step_avg:58.46ms
step:1601/2330 train_time:93592ms step_avg:58.46ms
step:1602/2330 train_time:93653ms step_avg:58.46ms
step:1603/2330 train_time:93711ms step_avg:58.46ms
step:1604/2330 train_time:93771ms step_avg:58.46ms
step:1605/2330 train_time:93829ms step_avg:58.46ms
step:1606/2330 train_time:93892ms step_avg:58.46ms
step:1607/2330 train_time:93951ms step_avg:58.46ms
step:1608/2330 train_time:94012ms step_avg:58.47ms
step:1609/2330 train_time:94070ms step_avg:58.46ms
step:1610/2330 train_time:94130ms step_avg:58.47ms
step:1611/2330 train_time:94187ms step_avg:58.47ms
step:1612/2330 train_time:94249ms step_avg:58.47ms
step:1613/2330 train_time:94306ms step_avg:58.47ms
step:1614/2330 train_time:94367ms step_avg:58.47ms
step:1615/2330 train_time:94424ms step_avg:58.47ms
step:1616/2330 train_time:94485ms step_avg:58.47ms
step:1617/2330 train_time:94542ms step_avg:58.47ms
step:1618/2330 train_time:94604ms step_avg:58.47ms
step:1619/2330 train_time:94660ms step_avg:58.47ms
step:1620/2330 train_time:94721ms step_avg:58.47ms
step:1621/2330 train_time:94777ms step_avg:58.47ms
step:1622/2330 train_time:94839ms step_avg:58.47ms
step:1623/2330 train_time:94896ms step_avg:58.47ms
step:1624/2330 train_time:94958ms step_avg:58.47ms
step:1625/2330 train_time:95015ms step_avg:58.47ms
step:1626/2330 train_time:95075ms step_avg:58.47ms
step:1627/2330 train_time:95133ms step_avg:58.47ms
step:1628/2330 train_time:95194ms step_avg:58.47ms
step:1629/2330 train_time:95251ms step_avg:58.47ms
step:1630/2330 train_time:95312ms step_avg:58.47ms
step:1631/2330 train_time:95369ms step_avg:58.47ms
step:1632/2330 train_time:95431ms step_avg:58.47ms
step:1633/2330 train_time:95488ms step_avg:58.47ms
step:1634/2330 train_time:95549ms step_avg:58.48ms
step:1635/2330 train_time:95608ms step_avg:58.48ms
step:1636/2330 train_time:95669ms step_avg:58.48ms
step:1637/2330 train_time:95727ms step_avg:58.48ms
step:1638/2330 train_time:95787ms step_avg:58.48ms
step:1639/2330 train_time:95844ms step_avg:58.48ms
step:1640/2330 train_time:95906ms step_avg:58.48ms
step:1641/2330 train_time:95963ms step_avg:58.48ms
step:1642/2330 train_time:96025ms step_avg:58.48ms
step:1643/2330 train_time:96081ms step_avg:58.48ms
step:1644/2330 train_time:96143ms step_avg:58.48ms
step:1645/2330 train_time:96200ms step_avg:58.48ms
step:1646/2330 train_time:96262ms step_avg:58.48ms
step:1647/2330 train_time:96318ms step_avg:58.48ms
step:1648/2330 train_time:96379ms step_avg:58.48ms
step:1649/2330 train_time:96436ms step_avg:58.48ms
step:1650/2330 train_time:96497ms step_avg:58.48ms
step:1651/2330 train_time:96554ms step_avg:58.48ms
step:1652/2330 train_time:96615ms step_avg:58.48ms
step:1653/2330 train_time:96672ms step_avg:58.48ms
step:1654/2330 train_time:96733ms step_avg:58.48ms
step:1655/2330 train_time:96791ms step_avg:58.48ms
step:1656/2330 train_time:96853ms step_avg:58.49ms
step:1657/2330 train_time:96911ms step_avg:58.49ms
step:1658/2330 train_time:96972ms step_avg:58.49ms
step:1659/2330 train_time:97031ms step_avg:58.49ms
step:1660/2330 train_time:97092ms step_avg:58.49ms
step:1661/2330 train_time:97151ms step_avg:58.49ms
step:1662/2330 train_time:97211ms step_avg:58.49ms
step:1663/2330 train_time:97269ms step_avg:58.49ms
step:1664/2330 train_time:97330ms step_avg:58.49ms
step:1665/2330 train_time:97387ms step_avg:58.49ms
step:1666/2330 train_time:97448ms step_avg:58.49ms
step:1667/2330 train_time:97506ms step_avg:58.49ms
step:1668/2330 train_time:97567ms step_avg:58.49ms
step:1669/2330 train_time:97625ms step_avg:58.49ms
step:1670/2330 train_time:97685ms step_avg:58.49ms
step:1671/2330 train_time:97742ms step_avg:58.49ms
step:1672/2330 train_time:97805ms step_avg:58.50ms
step:1673/2330 train_time:97862ms step_avg:58.49ms
step:1674/2330 train_time:97923ms step_avg:58.50ms
step:1675/2330 train_time:97980ms step_avg:58.50ms
step:1676/2330 train_time:98040ms step_avg:58.50ms
step:1677/2330 train_time:98097ms step_avg:58.50ms
step:1678/2330 train_time:98158ms step_avg:58.50ms
step:1679/2330 train_time:98214ms step_avg:58.50ms
step:1680/2330 train_time:98275ms step_avg:58.50ms
step:1681/2330 train_time:98332ms step_avg:58.50ms
step:1682/2330 train_time:98394ms step_avg:58.50ms
step:1683/2330 train_time:98452ms step_avg:58.50ms
step:1684/2330 train_time:98513ms step_avg:58.50ms
step:1685/2330 train_time:98571ms step_avg:58.50ms
step:1686/2330 train_time:98634ms step_avg:58.50ms
step:1687/2330 train_time:98692ms step_avg:58.50ms
step:1688/2330 train_time:98753ms step_avg:58.50ms
step:1689/2330 train_time:98812ms step_avg:58.50ms
step:1690/2330 train_time:98872ms step_avg:58.50ms
step:1691/2330 train_time:98930ms step_avg:58.50ms
step:1692/2330 train_time:98992ms step_avg:58.51ms
step:1693/2330 train_time:99050ms step_avg:58.51ms
step:1694/2330 train_time:99111ms step_avg:58.51ms
step:1695/2330 train_time:99169ms step_avg:58.51ms
step:1696/2330 train_time:99230ms step_avg:58.51ms
step:1697/2330 train_time:99287ms step_avg:58.51ms
step:1698/2330 train_time:99348ms step_avg:58.51ms
step:1699/2330 train_time:99405ms step_avg:58.51ms
step:1700/2330 train_time:99467ms step_avg:58.51ms
step:1701/2330 train_time:99524ms step_avg:58.51ms
step:1702/2330 train_time:99586ms step_avg:58.51ms
step:1703/2330 train_time:99643ms step_avg:58.51ms
step:1704/2330 train_time:99705ms step_avg:58.51ms
step:1705/2330 train_time:99762ms step_avg:58.51ms
step:1706/2330 train_time:99824ms step_avg:58.51ms
step:1707/2330 train_time:99881ms step_avg:58.51ms
step:1708/2330 train_time:99943ms step_avg:58.51ms
step:1709/2330 train_time:100001ms step_avg:58.51ms
step:1710/2330 train_time:100062ms step_avg:58.52ms
step:1711/2330 train_time:100118ms step_avg:58.51ms
step:1712/2330 train_time:100179ms step_avg:58.52ms
step:1713/2330 train_time:100236ms step_avg:58.51ms
step:1714/2330 train_time:100297ms step_avg:58.52ms
step:1715/2330 train_time:100354ms step_avg:58.52ms
step:1716/2330 train_time:100417ms step_avg:58.52ms
step:1717/2330 train_time:100474ms step_avg:58.52ms
step:1718/2330 train_time:100536ms step_avg:58.52ms
step:1719/2330 train_time:100593ms step_avg:58.52ms
step:1720/2330 train_time:100655ms step_avg:58.52ms
step:1721/2330 train_time:100712ms step_avg:58.52ms
step:1722/2330 train_time:100773ms step_avg:58.52ms
step:1723/2330 train_time:100831ms step_avg:58.52ms
step:1724/2330 train_time:100892ms step_avg:58.52ms
step:1725/2330 train_time:100951ms step_avg:58.52ms
step:1726/2330 train_time:101011ms step_avg:58.52ms
step:1727/2330 train_time:101068ms step_avg:58.52ms
step:1728/2330 train_time:101130ms step_avg:58.52ms
step:1729/2330 train_time:101188ms step_avg:58.52ms
step:1730/2330 train_time:101250ms step_avg:58.53ms
step:1731/2330 train_time:101307ms step_avg:58.53ms
step:1732/2330 train_time:101369ms step_avg:58.53ms
step:1733/2330 train_time:101427ms step_avg:58.53ms
step:1734/2330 train_time:101488ms step_avg:58.53ms
step:1735/2330 train_time:101545ms step_avg:58.53ms
step:1736/2330 train_time:101607ms step_avg:58.53ms
step:1737/2330 train_time:101665ms step_avg:58.53ms
step:1738/2330 train_time:101725ms step_avg:58.53ms
step:1739/2330 train_time:101782ms step_avg:58.53ms
step:1740/2330 train_time:101842ms step_avg:58.53ms
step:1741/2330 train_time:101899ms step_avg:58.53ms
step:1742/2330 train_time:101961ms step_avg:58.53ms
step:1743/2330 train_time:102018ms step_avg:58.53ms
step:1744/2330 train_time:102078ms step_avg:58.53ms
step:1745/2330 train_time:102135ms step_avg:58.53ms
step:1746/2330 train_time:102196ms step_avg:58.53ms
step:1747/2330 train_time:102253ms step_avg:58.53ms
step:1748/2330 train_time:102314ms step_avg:58.53ms
step:1749/2330 train_time:102372ms step_avg:58.53ms
step:1750/2330 train_time:102433ms step_avg:58.53ms
step:1750/2330 val_loss:3.8157 train_time:102515ms step_avg:58.58ms
step:1751/2330 train_time:102535ms step_avg:58.56ms
step:1752/2330 train_time:102556ms step_avg:58.54ms
step:1753/2330 train_time:102609ms step_avg:58.53ms
step:1754/2330 train_time:102672ms step_avg:58.54ms
step:1755/2330 train_time:102728ms step_avg:58.53ms
step:1756/2330 train_time:102794ms step_avg:58.54ms
step:1757/2330 train_time:102850ms step_avg:58.54ms
step:1758/2330 train_time:102910ms step_avg:58.54ms
step:1759/2330 train_time:102967ms step_avg:58.54ms
step:1760/2330 train_time:103027ms step_avg:58.54ms
step:1761/2330 train_time:103084ms step_avg:58.54ms
step:1762/2330 train_time:103143ms step_avg:58.54ms
step:1763/2330 train_time:103200ms step_avg:58.54ms
step:1764/2330 train_time:103259ms step_avg:58.54ms
step:1765/2330 train_time:103316ms step_avg:58.54ms
step:1766/2330 train_time:103375ms step_avg:58.54ms
step:1767/2330 train_time:103436ms step_avg:58.54ms
step:1768/2330 train_time:103500ms step_avg:58.54ms
step:1769/2330 train_time:103557ms step_avg:58.54ms
step:1770/2330 train_time:103620ms step_avg:58.54ms
step:1771/2330 train_time:103677ms step_avg:58.54ms
step:1772/2330 train_time:103738ms step_avg:58.54ms
step:1773/2330 train_time:103795ms step_avg:58.54ms
step:1774/2330 train_time:103856ms step_avg:58.54ms
step:1775/2330 train_time:103913ms step_avg:58.54ms
step:1776/2330 train_time:103973ms step_avg:58.54ms
step:1777/2330 train_time:104030ms step_avg:58.54ms
step:1778/2330 train_time:104091ms step_avg:58.54ms
step:1779/2330 train_time:104148ms step_avg:58.54ms
step:1780/2330 train_time:104208ms step_avg:58.54ms
step:1781/2330 train_time:104264ms step_avg:58.54ms
step:1782/2330 train_time:104325ms step_avg:58.54ms
step:1783/2330 train_time:104384ms step_avg:58.54ms
step:1784/2330 train_time:104445ms step_avg:58.55ms
step:1785/2330 train_time:104506ms step_avg:58.55ms
step:1786/2330 train_time:104567ms step_avg:58.55ms
step:1787/2330 train_time:104627ms step_avg:58.55ms
step:1788/2330 train_time:104687ms step_avg:58.55ms
step:1789/2330 train_time:104746ms step_avg:58.55ms
step:1790/2330 train_time:104806ms step_avg:58.55ms
step:1791/2330 train_time:104863ms step_avg:58.55ms
step:1792/2330 train_time:104924ms step_avg:58.55ms
step:1793/2330 train_time:104982ms step_avg:58.55ms
step:1794/2330 train_time:105042ms step_avg:58.55ms
step:1795/2330 train_time:105099ms step_avg:58.55ms
step:1796/2330 train_time:105160ms step_avg:58.55ms
step:1797/2330 train_time:105216ms step_avg:58.55ms
step:1798/2330 train_time:105277ms step_avg:58.55ms
step:1799/2330 train_time:105334ms step_avg:58.55ms
step:1800/2330 train_time:105395ms step_avg:58.55ms
step:1801/2330 train_time:105453ms step_avg:58.55ms
step:1802/2330 train_time:105514ms step_avg:58.55ms
step:1803/2330 train_time:105572ms step_avg:58.55ms
step:1804/2330 train_time:105633ms step_avg:58.56ms
step:1805/2330 train_time:105691ms step_avg:58.55ms
step:1806/2330 train_time:105751ms step_avg:58.56ms
step:1807/2330 train_time:105808ms step_avg:58.55ms
step:1808/2330 train_time:105870ms step_avg:58.56ms
step:1809/2330 train_time:105928ms step_avg:58.56ms
step:1810/2330 train_time:105987ms step_avg:58.56ms
step:1811/2330 train_time:106045ms step_avg:58.56ms
step:1812/2330 train_time:106105ms step_avg:58.56ms
step:1813/2330 train_time:106163ms step_avg:58.56ms
step:1814/2330 train_time:106224ms step_avg:58.56ms
step:1815/2330 train_time:106282ms step_avg:58.56ms
step:1816/2330 train_time:106342ms step_avg:58.56ms
step:1817/2330 train_time:106400ms step_avg:58.56ms
step:1818/2330 train_time:106461ms step_avg:58.56ms
step:1819/2330 train_time:106519ms step_avg:58.56ms
step:1820/2330 train_time:106581ms step_avg:58.56ms
step:1821/2330 train_time:106639ms step_avg:58.56ms
step:1822/2330 train_time:106699ms step_avg:58.56ms
step:1823/2330 train_time:106757ms step_avg:58.56ms
step:1824/2330 train_time:106817ms step_avg:58.56ms
step:1825/2330 train_time:106874ms step_avg:58.56ms
step:1826/2330 train_time:106934ms step_avg:58.56ms
step:1827/2330 train_time:106991ms step_avg:58.56ms
step:1828/2330 train_time:107051ms step_avg:58.56ms
step:1829/2330 train_time:107108ms step_avg:58.56ms
step:1830/2330 train_time:107169ms step_avg:58.56ms
step:1831/2330 train_time:107227ms step_avg:58.56ms
step:1832/2330 train_time:107288ms step_avg:58.56ms
step:1833/2330 train_time:107347ms step_avg:58.56ms
step:1834/2330 train_time:107406ms step_avg:58.56ms
step:1835/2330 train_time:107465ms step_avg:58.56ms
step:1836/2330 train_time:107527ms step_avg:58.57ms
step:1837/2330 train_time:107586ms step_avg:58.57ms
step:1838/2330 train_time:107646ms step_avg:58.57ms
step:1839/2330 train_time:107703ms step_avg:58.57ms
step:1840/2330 train_time:107765ms step_avg:58.57ms
step:1841/2330 train_time:107823ms step_avg:58.57ms
step:1842/2330 train_time:107884ms step_avg:58.57ms
step:1843/2330 train_time:107942ms step_avg:58.57ms
step:1844/2330 train_time:108002ms step_avg:58.57ms
step:1845/2330 train_time:108059ms step_avg:58.57ms
step:1846/2330 train_time:108119ms step_avg:58.57ms
step:1847/2330 train_time:108177ms step_avg:58.57ms
step:1848/2330 train_time:108237ms step_avg:58.57ms
step:1849/2330 train_time:108294ms step_avg:58.57ms
step:1850/2330 train_time:108354ms step_avg:58.57ms
step:1851/2330 train_time:108411ms step_avg:58.57ms
step:1852/2330 train_time:108473ms step_avg:58.57ms
step:1853/2330 train_time:108530ms step_avg:58.57ms
step:1854/2330 train_time:108591ms step_avg:58.57ms
step:1855/2330 train_time:108648ms step_avg:58.57ms
step:1856/2330 train_time:108710ms step_avg:58.57ms
step:1857/2330 train_time:108768ms step_avg:58.57ms
step:1858/2330 train_time:108828ms step_avg:58.57ms
step:1859/2330 train_time:108886ms step_avg:58.57ms
step:1860/2330 train_time:108947ms step_avg:58.57ms
step:1861/2330 train_time:109006ms step_avg:58.57ms
step:1862/2330 train_time:109067ms step_avg:58.58ms
step:1863/2330 train_time:109125ms step_avg:58.57ms
step:1864/2330 train_time:109186ms step_avg:58.58ms
step:1865/2330 train_time:109244ms step_avg:58.58ms
step:1866/2330 train_time:109305ms step_avg:58.58ms
step:1867/2330 train_time:109363ms step_avg:58.58ms
step:1868/2330 train_time:109425ms step_avg:58.58ms
step:1869/2330 train_time:109483ms step_avg:58.58ms
step:1870/2330 train_time:109544ms step_avg:58.58ms
step:1871/2330 train_time:109601ms step_avg:58.58ms
step:1872/2330 train_time:109661ms step_avg:58.58ms
step:1873/2330 train_time:109719ms step_avg:58.58ms
step:1874/2330 train_time:109779ms step_avg:58.58ms
step:1875/2330 train_time:109837ms step_avg:58.58ms
step:1876/2330 train_time:109898ms step_avg:58.58ms
step:1877/2330 train_time:109955ms step_avg:58.58ms
step:1878/2330 train_time:110015ms step_avg:58.58ms
step:1879/2330 train_time:110073ms step_avg:58.58ms
step:1880/2330 train_time:110134ms step_avg:58.58ms
step:1881/2330 train_time:110190ms step_avg:58.58ms
step:1882/2330 train_time:110251ms step_avg:58.58ms
step:1883/2330 train_time:110307ms step_avg:58.58ms
step:1884/2330 train_time:110370ms step_avg:58.58ms
step:1885/2330 train_time:110428ms step_avg:58.58ms
step:1886/2330 train_time:110489ms step_avg:58.58ms
step:1887/2330 train_time:110547ms step_avg:58.58ms
step:1888/2330 train_time:110607ms step_avg:58.58ms
step:1889/2330 train_time:110665ms step_avg:58.58ms
step:1890/2330 train_time:110727ms step_avg:58.59ms
step:1891/2330 train_time:110785ms step_avg:58.59ms
step:1892/2330 train_time:110845ms step_avg:58.59ms
step:1893/2330 train_time:110904ms step_avg:58.59ms
step:1894/2330 train_time:110964ms step_avg:58.59ms
step:1895/2330 train_time:111023ms step_avg:58.59ms
step:1896/2330 train_time:111083ms step_avg:58.59ms
step:1897/2330 train_time:111142ms step_avg:58.59ms
step:1898/2330 train_time:111202ms step_avg:58.59ms
step:1899/2330 train_time:111259ms step_avg:58.59ms
step:1900/2330 train_time:111319ms step_avg:58.59ms
step:1901/2330 train_time:111377ms step_avg:58.59ms
step:1902/2330 train_time:111437ms step_avg:58.59ms
step:1903/2330 train_time:111494ms step_avg:58.59ms
step:1904/2330 train_time:111555ms step_avg:58.59ms
step:1905/2330 train_time:111612ms step_avg:58.59ms
step:1906/2330 train_time:111674ms step_avg:58.59ms
step:1907/2330 train_time:111732ms step_avg:58.59ms
step:1908/2330 train_time:111792ms step_avg:58.59ms
step:1909/2330 train_time:111850ms step_avg:58.59ms
step:1910/2330 train_time:111911ms step_avg:58.59ms
step:1911/2330 train_time:111969ms step_avg:58.59ms
step:1912/2330 train_time:112029ms step_avg:58.59ms
step:1913/2330 train_time:112087ms step_avg:58.59ms
step:1914/2330 train_time:112147ms step_avg:58.59ms
step:1915/2330 train_time:112206ms step_avg:58.59ms
step:1916/2330 train_time:112265ms step_avg:58.59ms
step:1917/2330 train_time:112324ms step_avg:58.59ms
step:1918/2330 train_time:112384ms step_avg:58.59ms
step:1919/2330 train_time:112442ms step_avg:58.59ms
step:1920/2330 train_time:112503ms step_avg:58.60ms
step:1921/2330 train_time:112560ms step_avg:58.59ms
step:1922/2330 train_time:112622ms step_avg:58.60ms
step:1923/2330 train_time:112680ms step_avg:58.60ms
step:1924/2330 train_time:112742ms step_avg:58.60ms
step:1925/2330 train_time:112799ms step_avg:58.60ms
step:1926/2330 train_time:112859ms step_avg:58.60ms
step:1927/2330 train_time:112916ms step_avg:58.60ms
step:1928/2330 train_time:112977ms step_avg:58.60ms
step:1929/2330 train_time:113034ms step_avg:58.60ms
step:1930/2330 train_time:113094ms step_avg:58.60ms
step:1931/2330 train_time:113151ms step_avg:58.60ms
step:1932/2330 train_time:113212ms step_avg:58.60ms
step:1933/2330 train_time:113269ms step_avg:58.60ms
step:1934/2330 train_time:113331ms step_avg:58.60ms
step:1935/2330 train_time:113389ms step_avg:58.60ms
step:1936/2330 train_time:113448ms step_avg:58.60ms
step:1937/2330 train_time:113506ms step_avg:58.60ms
step:1938/2330 train_time:113567ms step_avg:58.60ms
step:1939/2330 train_time:113625ms step_avg:58.60ms
step:1940/2330 train_time:113686ms step_avg:58.60ms
step:1941/2330 train_time:113745ms step_avg:58.60ms
step:1942/2330 train_time:113806ms step_avg:58.60ms
step:1943/2330 train_time:113864ms step_avg:58.60ms
step:1944/2330 train_time:113925ms step_avg:58.60ms
step:1945/2330 train_time:113983ms step_avg:58.60ms
step:1946/2330 train_time:114045ms step_avg:58.60ms
step:1947/2330 train_time:114102ms step_avg:58.60ms
step:1948/2330 train_time:114163ms step_avg:58.61ms
step:1949/2330 train_time:114221ms step_avg:58.61ms
step:1950/2330 train_time:114282ms step_avg:58.61ms
step:1951/2330 train_time:114339ms step_avg:58.61ms
step:1952/2330 train_time:114399ms step_avg:58.61ms
step:1953/2330 train_time:114457ms step_avg:58.61ms
step:1954/2330 train_time:114516ms step_avg:58.61ms
step:1955/2330 train_time:114574ms step_avg:58.61ms
step:1956/2330 train_time:114636ms step_avg:58.61ms
step:1957/2330 train_time:114694ms step_avg:58.61ms
step:1958/2330 train_time:114754ms step_avg:58.61ms
step:1959/2330 train_time:114812ms step_avg:58.61ms
step:1960/2330 train_time:114873ms step_avg:58.61ms
step:1961/2330 train_time:114931ms step_avg:58.61ms
step:1962/2330 train_time:114991ms step_avg:58.61ms
step:1963/2330 train_time:115048ms step_avg:58.61ms
step:1964/2330 train_time:115110ms step_avg:58.61ms
step:1965/2330 train_time:115167ms step_avg:58.61ms
step:1966/2330 train_time:115228ms step_avg:58.61ms
step:1967/2330 train_time:115285ms step_avg:58.61ms
step:1968/2330 train_time:115346ms step_avg:58.61ms
step:1969/2330 train_time:115404ms step_avg:58.61ms
step:1970/2330 train_time:115466ms step_avg:58.61ms
step:1971/2330 train_time:115523ms step_avg:58.61ms
step:1972/2330 train_time:115585ms step_avg:58.61ms
step:1973/2330 train_time:115642ms step_avg:58.61ms
step:1974/2330 train_time:115704ms step_avg:58.61ms
step:1975/2330 train_time:115761ms step_avg:58.61ms
step:1976/2330 train_time:115822ms step_avg:58.61ms
step:1977/2330 train_time:115880ms step_avg:58.61ms
step:1978/2330 train_time:115941ms step_avg:58.62ms
step:1979/2330 train_time:115999ms step_avg:58.61ms
step:1980/2330 train_time:116060ms step_avg:58.62ms
step:1981/2330 train_time:116116ms step_avg:58.61ms
step:1982/2330 train_time:116178ms step_avg:58.62ms
step:1983/2330 train_time:116234ms step_avg:58.62ms
step:1984/2330 train_time:116295ms step_avg:58.62ms
step:1985/2330 train_time:116351ms step_avg:58.62ms
step:1986/2330 train_time:116413ms step_avg:58.62ms
step:1987/2330 train_time:116470ms step_avg:58.62ms
step:1988/2330 train_time:116532ms step_avg:58.62ms
step:1989/2330 train_time:116589ms step_avg:58.62ms
step:1990/2330 train_time:116650ms step_avg:58.62ms
step:1991/2330 train_time:116708ms step_avg:58.62ms
step:1992/2330 train_time:116771ms step_avg:58.62ms
step:1993/2330 train_time:116829ms step_avg:58.62ms
step:1994/2330 train_time:116890ms step_avg:58.62ms
step:1995/2330 train_time:116948ms step_avg:58.62ms
step:1996/2330 train_time:117011ms step_avg:58.62ms
step:1997/2330 train_time:117069ms step_avg:58.62ms
step:1998/2330 train_time:117129ms step_avg:58.62ms
step:1999/2330 train_time:117187ms step_avg:58.62ms
step:2000/2330 train_time:117247ms step_avg:58.62ms
step:2000/2330 val_loss:3.7536 train_time:117329ms step_avg:58.66ms
step:2001/2330 train_time:117348ms step_avg:58.64ms
step:2002/2330 train_time:117370ms step_avg:58.63ms
step:2003/2330 train_time:117431ms step_avg:58.63ms
step:2004/2330 train_time:117493ms step_avg:58.63ms
step:2005/2330 train_time:117551ms step_avg:58.63ms
step:2006/2330 train_time:117612ms step_avg:58.63ms
step:2007/2330 train_time:117669ms step_avg:58.63ms
step:2008/2330 train_time:117728ms step_avg:58.63ms
step:2009/2330 train_time:117785ms step_avg:58.63ms
step:2010/2330 train_time:117846ms step_avg:58.63ms
step:2011/2330 train_time:117903ms step_avg:58.63ms
step:2012/2330 train_time:117963ms step_avg:58.63ms
step:2013/2330 train_time:118019ms step_avg:58.63ms
step:2014/2330 train_time:118080ms step_avg:58.63ms
step:2015/2330 train_time:118136ms step_avg:58.63ms
step:2016/2330 train_time:118196ms step_avg:58.63ms
step:2017/2330 train_time:118254ms step_avg:58.63ms
step:2018/2330 train_time:118315ms step_avg:58.63ms
step:2019/2330 train_time:118374ms step_avg:58.63ms
step:2020/2330 train_time:118435ms step_avg:58.63ms
step:2021/2330 train_time:118493ms step_avg:58.63ms
step:2022/2330 train_time:118555ms step_avg:58.63ms
step:2023/2330 train_time:118612ms step_avg:58.63ms
step:2024/2330 train_time:118673ms step_avg:58.63ms
step:2025/2330 train_time:118730ms step_avg:58.63ms
step:2026/2330 train_time:118790ms step_avg:58.63ms
step:2027/2330 train_time:118848ms step_avg:58.63ms
step:2028/2330 train_time:118908ms step_avg:58.63ms
step:2029/2330 train_time:118966ms step_avg:58.63ms
step:2030/2330 train_time:119025ms step_avg:58.63ms
step:2031/2330 train_time:119083ms step_avg:58.63ms
step:2032/2330 train_time:119142ms step_avg:58.63ms
step:2033/2330 train_time:119200ms step_avg:58.63ms
step:2034/2330 train_time:119260ms step_avg:58.63ms
step:2035/2330 train_time:119319ms step_avg:58.63ms
step:2036/2330 train_time:119381ms step_avg:58.63ms
step:2037/2330 train_time:119439ms step_avg:58.63ms
step:2038/2330 train_time:119503ms step_avg:58.64ms
step:2039/2330 train_time:119561ms step_avg:58.64ms
step:2040/2330 train_time:119624ms step_avg:58.64ms
step:2041/2330 train_time:119681ms step_avg:58.64ms
step:2042/2330 train_time:119741ms step_avg:58.64ms
step:2043/2330 train_time:119798ms step_avg:58.64ms
step:2044/2330 train_time:119859ms step_avg:58.64ms
step:2045/2330 train_time:119916ms step_avg:58.64ms
step:2046/2330 train_time:119976ms step_avg:58.64ms
step:2047/2330 train_time:120033ms step_avg:58.64ms
step:2048/2330 train_time:120093ms step_avg:58.64ms
step:2049/2330 train_time:120150ms step_avg:58.64ms
step:2050/2330 train_time:120211ms step_avg:58.64ms
step:2051/2330 train_time:120269ms step_avg:58.64ms
step:2052/2330 train_time:120329ms step_avg:58.64ms
step:2053/2330 train_time:120388ms step_avg:58.64ms
step:2054/2330 train_time:120449ms step_avg:58.64ms
step:2055/2330 train_time:120507ms step_avg:58.64ms
step:2056/2330 train_time:120569ms step_avg:58.64ms
step:2057/2330 train_time:120627ms step_avg:58.64ms
step:2058/2330 train_time:120688ms step_avg:58.64ms
step:2059/2330 train_time:120745ms step_avg:58.64ms
step:2060/2330 train_time:120806ms step_avg:58.64ms
step:2061/2330 train_time:120863ms step_avg:58.64ms
step:2062/2330 train_time:120924ms step_avg:58.64ms
step:2063/2330 train_time:120981ms step_avg:58.64ms
step:2064/2330 train_time:121041ms step_avg:58.64ms
step:2065/2330 train_time:121098ms step_avg:58.64ms
step:2066/2330 train_time:121158ms step_avg:58.64ms
step:2067/2330 train_time:121215ms step_avg:58.64ms
step:2068/2330 train_time:121275ms step_avg:58.64ms
step:2069/2330 train_time:121333ms step_avg:58.64ms
step:2070/2330 train_time:121394ms step_avg:58.64ms
step:2071/2330 train_time:121452ms step_avg:58.64ms
step:2072/2330 train_time:121513ms step_avg:58.65ms
step:2073/2330 train_time:121571ms step_avg:58.64ms
step:2074/2330 train_time:121633ms step_avg:58.65ms
step:2075/2330 train_time:121690ms step_avg:58.65ms
step:2076/2330 train_time:121752ms step_avg:58.65ms
step:2077/2330 train_time:121809ms step_avg:58.65ms
step:2078/2330 train_time:121869ms step_avg:58.65ms
step:2079/2330 train_time:121927ms step_avg:58.65ms
step:2080/2330 train_time:121987ms step_avg:58.65ms
step:2081/2330 train_time:122045ms step_avg:58.65ms
step:2082/2330 train_time:122105ms step_avg:58.65ms
step:2083/2330 train_time:122163ms step_avg:58.65ms
step:2084/2330 train_time:122223ms step_avg:58.65ms
step:2085/2330 train_time:122280ms step_avg:58.65ms
step:2086/2330 train_time:122341ms step_avg:58.65ms
step:2087/2330 train_time:122399ms step_avg:58.65ms
step:2088/2330 train_time:122461ms step_avg:58.65ms
step:2089/2330 train_time:122518ms step_avg:58.65ms
step:2090/2330 train_time:122580ms step_avg:58.65ms
step:2091/2330 train_time:122637ms step_avg:58.65ms
step:2092/2330 train_time:122698ms step_avg:58.65ms
step:2093/2330 train_time:122755ms step_avg:58.65ms
step:2094/2330 train_time:122817ms step_avg:58.65ms
step:2095/2330 train_time:122874ms step_avg:58.65ms
step:2096/2330 train_time:122935ms step_avg:58.65ms
step:2097/2330 train_time:122992ms step_avg:58.65ms
step:2098/2330 train_time:123053ms step_avg:58.65ms
step:2099/2330 train_time:123111ms step_avg:58.65ms
step:2100/2330 train_time:123170ms step_avg:58.65ms
step:2101/2330 train_time:123228ms step_avg:58.65ms
step:2102/2330 train_time:123289ms step_avg:58.65ms
step:2103/2330 train_time:123348ms step_avg:58.65ms
step:2104/2330 train_time:123409ms step_avg:58.65ms
step:2105/2330 train_time:123468ms step_avg:58.65ms
step:2106/2330 train_time:123528ms step_avg:58.66ms
step:2107/2330 train_time:123586ms step_avg:58.66ms
step:2108/2330 train_time:123647ms step_avg:58.66ms
step:2109/2330 train_time:123706ms step_avg:58.66ms
step:2110/2330 train_time:123766ms step_avg:58.66ms
step:2111/2330 train_time:123824ms step_avg:58.66ms
step:2112/2330 train_time:123884ms step_avg:58.66ms
step:2113/2330 train_time:123941ms step_avg:58.66ms
step:2114/2330 train_time:124002ms step_avg:58.66ms
step:2115/2330 train_time:124060ms step_avg:58.66ms
step:2116/2330 train_time:124120ms step_avg:58.66ms
step:2117/2330 train_time:124177ms step_avg:58.66ms
step:2118/2330 train_time:124237ms step_avg:58.66ms
step:2119/2330 train_time:124295ms step_avg:58.66ms
step:2120/2330 train_time:124355ms step_avg:58.66ms
step:2121/2330 train_time:124412ms step_avg:58.66ms
step:2122/2330 train_time:124473ms step_avg:58.66ms
step:2123/2330 train_time:124531ms step_avg:58.66ms
step:2124/2330 train_time:124591ms step_avg:58.66ms
step:2125/2330 train_time:124650ms step_avg:58.66ms
step:2126/2330 train_time:124711ms step_avg:58.66ms
step:2127/2330 train_time:124768ms step_avg:58.66ms
step:2128/2330 train_time:124829ms step_avg:58.66ms
step:2129/2330 train_time:124887ms step_avg:58.66ms
step:2130/2330 train_time:124948ms step_avg:58.66ms
step:2131/2330 train_time:125007ms step_avg:58.66ms
step:2132/2330 train_time:125066ms step_avg:58.66ms
step:2133/2330 train_time:125125ms step_avg:58.66ms
step:2134/2330 train_time:125185ms step_avg:58.66ms
step:2135/2330 train_time:125244ms step_avg:58.66ms
step:2136/2330 train_time:125304ms step_avg:58.66ms
step:2137/2330 train_time:125363ms step_avg:58.66ms
step:2138/2330 train_time:125422ms step_avg:58.66ms
step:2139/2330 train_time:125479ms step_avg:58.66ms
step:2140/2330 train_time:125541ms step_avg:58.66ms
step:2141/2330 train_time:125598ms step_avg:58.66ms
step:2142/2330 train_time:125658ms step_avg:58.66ms
step:2143/2330 train_time:125716ms step_avg:58.66ms
step:2144/2330 train_time:125776ms step_avg:58.66ms
step:2145/2330 train_time:125833ms step_avg:58.66ms
step:2146/2330 train_time:125895ms step_avg:58.67ms
step:2147/2330 train_time:125953ms step_avg:58.66ms
step:2148/2330 train_time:126014ms step_avg:58.67ms
step:2149/2330 train_time:126071ms step_avg:58.66ms
step:2150/2330 train_time:126133ms step_avg:58.67ms
step:2151/2330 train_time:126190ms step_avg:58.67ms
step:2152/2330 train_time:126251ms step_avg:58.67ms
step:2153/2330 train_time:126310ms step_avg:58.67ms
step:2154/2330 train_time:126370ms step_avg:58.67ms
step:2155/2330 train_time:126428ms step_avg:58.67ms
step:2156/2330 train_time:126488ms step_avg:58.67ms
step:2157/2330 train_time:126547ms step_avg:58.67ms
step:2158/2330 train_time:126607ms step_avg:58.67ms
step:2159/2330 train_time:126666ms step_avg:58.67ms
step:2160/2330 train_time:126726ms step_avg:58.67ms
step:2161/2330 train_time:126784ms step_avg:58.67ms
step:2162/2330 train_time:126845ms step_avg:58.67ms
step:2163/2330 train_time:126902ms step_avg:58.67ms
step:2164/2330 train_time:126963ms step_avg:58.67ms
step:2165/2330 train_time:127020ms step_avg:58.67ms
step:2166/2330 train_time:127081ms step_avg:58.67ms
step:2167/2330 train_time:127139ms step_avg:58.67ms
step:2168/2330 train_time:127198ms step_avg:58.67ms
step:2169/2330 train_time:127255ms step_avg:58.67ms
step:2170/2330 train_time:127316ms step_avg:58.67ms
step:2171/2330 train_time:127373ms step_avg:58.67ms
step:2172/2330 train_time:127434ms step_avg:58.67ms
step:2173/2330 train_time:127492ms step_avg:58.67ms
step:2174/2330 train_time:127553ms step_avg:58.67ms
step:2175/2330 train_time:127611ms step_avg:58.67ms
step:2176/2330 train_time:127671ms step_avg:58.67ms
step:2177/2330 train_time:127729ms step_avg:58.67ms
step:2178/2330 train_time:127789ms step_avg:58.67ms
step:2179/2330 train_time:127848ms step_avg:58.67ms
step:2180/2330 train_time:127909ms step_avg:58.67ms
step:2181/2330 train_time:127967ms step_avg:58.67ms
step:2182/2330 train_time:128028ms step_avg:58.67ms
step:2183/2330 train_time:128086ms step_avg:58.67ms
step:2184/2330 train_time:128146ms step_avg:58.67ms
step:2185/2330 train_time:128204ms step_avg:58.67ms
step:2186/2330 train_time:128265ms step_avg:58.68ms
step:2187/2330 train_time:128323ms step_avg:58.68ms
step:2188/2330 train_time:128384ms step_avg:58.68ms
step:2189/2330 train_time:128443ms step_avg:58.68ms
step:2190/2330 train_time:128503ms step_avg:58.68ms
step:2191/2330 train_time:128562ms step_avg:58.68ms
step:2192/2330 train_time:128622ms step_avg:58.68ms
step:2193/2330 train_time:128679ms step_avg:58.68ms
step:2194/2330 train_time:128739ms step_avg:58.68ms
step:2195/2330 train_time:128797ms step_avg:58.68ms
step:2196/2330 train_time:128856ms step_avg:58.68ms
step:2197/2330 train_time:128913ms step_avg:58.68ms
step:2198/2330 train_time:128974ms step_avg:58.68ms
step:2199/2330 train_time:129031ms step_avg:58.68ms
step:2200/2330 train_time:129093ms step_avg:58.68ms
step:2201/2330 train_time:129151ms step_avg:58.68ms
step:2202/2330 train_time:129211ms step_avg:58.68ms
step:2203/2330 train_time:129269ms step_avg:58.68ms
step:2204/2330 train_time:129330ms step_avg:58.68ms
step:2205/2330 train_time:129389ms step_avg:58.68ms
step:2206/2330 train_time:129449ms step_avg:58.68ms
step:2207/2330 train_time:129506ms step_avg:58.68ms
step:2208/2330 train_time:129567ms step_avg:58.68ms
step:2209/2330 train_time:129625ms step_avg:58.68ms
step:2210/2330 train_time:129686ms step_avg:58.68ms
step:2211/2330 train_time:129743ms step_avg:58.68ms
step:2212/2330 train_time:129804ms step_avg:58.68ms
step:2213/2330 train_time:129862ms step_avg:58.68ms
step:2214/2330 train_time:129923ms step_avg:58.68ms
step:2215/2330 train_time:129980ms step_avg:58.68ms
step:2216/2330 train_time:130041ms step_avg:58.68ms
step:2217/2330 train_time:130099ms step_avg:58.68ms
step:2218/2330 train_time:130161ms step_avg:58.68ms
step:2219/2330 train_time:130217ms step_avg:58.68ms
step:2220/2330 train_time:130278ms step_avg:58.68ms
step:2221/2330 train_time:130335ms step_avg:58.68ms
step:2222/2330 train_time:130397ms step_avg:58.68ms
step:2223/2330 train_time:130454ms step_avg:58.68ms
step:2224/2330 train_time:130514ms step_avg:58.68ms
step:2225/2330 train_time:130570ms step_avg:58.68ms
step:2226/2330 train_time:130632ms step_avg:58.68ms
step:2227/2330 train_time:130690ms step_avg:58.68ms
step:2228/2330 train_time:130750ms step_avg:58.69ms
step:2229/2330 train_time:130808ms step_avg:58.68ms
step:2230/2330 train_time:130869ms step_avg:58.69ms
step:2231/2330 train_time:130926ms step_avg:58.68ms
step:2232/2330 train_time:130988ms step_avg:58.69ms
step:2233/2330 train_time:131047ms step_avg:58.69ms
step:2234/2330 train_time:131107ms step_avg:58.69ms
step:2235/2330 train_time:131166ms step_avg:58.69ms
step:2236/2330 train_time:131226ms step_avg:58.69ms
step:2237/2330 train_time:131284ms step_avg:58.69ms
step:2238/2330 train_time:131345ms step_avg:58.69ms
step:2239/2330 train_time:131403ms step_avg:58.69ms
step:2240/2330 train_time:131463ms step_avg:58.69ms
step:2241/2330 train_time:131520ms step_avg:58.69ms
step:2242/2330 train_time:131581ms step_avg:58.69ms
step:2243/2330 train_time:131638ms step_avg:58.69ms
step:2244/2330 train_time:131699ms step_avg:58.69ms
step:2245/2330 train_time:131756ms step_avg:58.69ms
step:2246/2330 train_time:131817ms step_avg:58.69ms
step:2247/2330 train_time:131873ms step_avg:58.69ms
step:2248/2330 train_time:131935ms step_avg:58.69ms
step:2249/2330 train_time:131993ms step_avg:58.69ms
step:2250/2330 train_time:132054ms step_avg:58.69ms
step:2250/2330 val_loss:3.7059 train_time:132136ms step_avg:58.73ms
step:2251/2330 train_time:132156ms step_avg:58.71ms
step:2252/2330 train_time:132177ms step_avg:58.69ms
step:2253/2330 train_time:132237ms step_avg:58.69ms
step:2254/2330 train_time:132299ms step_avg:58.70ms
step:2255/2330 train_time:132357ms step_avg:58.69ms
step:2256/2330 train_time:132419ms step_avg:58.70ms
step:2257/2330 train_time:132476ms step_avg:58.70ms
step:2258/2330 train_time:132536ms step_avg:58.70ms
step:2259/2330 train_time:132592ms step_avg:58.70ms
step:2260/2330 train_time:132652ms step_avg:58.70ms
step:2261/2330 train_time:132710ms step_avg:58.70ms
step:2262/2330 train_time:132770ms step_avg:58.70ms
step:2263/2330 train_time:132827ms step_avg:58.70ms
step:2264/2330 train_time:132886ms step_avg:58.70ms
step:2265/2330 train_time:132943ms step_avg:58.69ms
step:2266/2330 train_time:133003ms step_avg:58.69ms
step:2267/2330 train_time:133059ms step_avg:58.69ms
step:2268/2330 train_time:133122ms step_avg:58.70ms
step:2269/2330 train_time:133182ms step_avg:58.70ms
step:2270/2330 train_time:133243ms step_avg:58.70ms
step:2271/2330 train_time:133301ms step_avg:58.70ms
step:2272/2330 train_time:133364ms step_avg:58.70ms
step:2273/2330 train_time:133423ms step_avg:58.70ms
step:2274/2330 train_time:133485ms step_avg:58.70ms
step:2275/2330 train_time:133543ms step_avg:58.70ms
step:2276/2330 train_time:133602ms step_avg:58.70ms
step:2277/2330 train_time:133659ms step_avg:58.70ms
step:2278/2330 train_time:133721ms step_avg:58.70ms
step:2279/2330 train_time:133778ms step_avg:58.70ms
step:2280/2330 train_time:133838ms step_avg:58.70ms
step:2281/2330 train_time:133894ms step_avg:58.70ms
step:2282/2330 train_time:133954ms step_avg:58.70ms
step:2283/2330 train_time:134011ms step_avg:58.70ms
step:2284/2330 train_time:134071ms step_avg:58.70ms
step:2285/2330 train_time:134129ms step_avg:58.70ms
step:2286/2330 train_time:134190ms step_avg:58.70ms
step:2287/2330 train_time:134248ms step_avg:58.70ms
step:2288/2330 train_time:134309ms step_avg:58.70ms
step:2289/2330 train_time:134368ms step_avg:58.70ms
step:2290/2330 train_time:134429ms step_avg:58.70ms
step:2291/2330 train_time:134487ms step_avg:58.70ms
step:2292/2330 train_time:134547ms step_avg:58.70ms
step:2293/2330 train_time:134604ms step_avg:58.70ms
step:2294/2330 train_time:134666ms step_avg:58.70ms
step:2295/2330 train_time:134723ms step_avg:58.70ms
step:2296/2330 train_time:134785ms step_avg:58.70ms
step:2297/2330 train_time:134842ms step_avg:58.70ms
step:2298/2330 train_time:134903ms step_avg:58.70ms
step:2299/2330 train_time:134960ms step_avg:58.70ms
step:2300/2330 train_time:135020ms step_avg:58.70ms
step:2301/2330 train_time:135077ms step_avg:58.70ms
step:2302/2330 train_time:135137ms step_avg:58.70ms
step:2303/2330 train_time:135195ms step_avg:58.70ms
step:2304/2330 train_time:135256ms step_avg:58.70ms
step:2305/2330 train_time:135313ms step_avg:58.70ms
step:2306/2330 train_time:135375ms step_avg:58.71ms
step:2307/2330 train_time:135432ms step_avg:58.70ms
step:2308/2330 train_time:135494ms step_avg:58.71ms
step:2309/2330 train_time:135551ms step_avg:58.71ms
step:2310/2330 train_time:135613ms step_avg:58.71ms
step:2311/2330 train_time:135670ms step_avg:58.71ms
step:2312/2330 train_time:135731ms step_avg:58.71ms
step:2313/2330 train_time:135788ms step_avg:58.71ms
step:2314/2330 train_time:135849ms step_avg:58.71ms
step:2315/2330 train_time:135906ms step_avg:58.71ms
step:2316/2330 train_time:135966ms step_avg:58.71ms
step:2317/2330 train_time:136024ms step_avg:58.71ms
step:2318/2330 train_time:136084ms step_avg:58.71ms
step:2319/2330 train_time:136142ms step_avg:58.71ms
step:2320/2330 train_time:136203ms step_avg:58.71ms
step:2321/2330 train_time:136260ms step_avg:58.71ms
step:2322/2330 train_time:136322ms step_avg:58.71ms
step:2323/2330 train_time:136380ms step_avg:58.71ms
step:2324/2330 train_time:136440ms step_avg:58.71ms
step:2325/2330 train_time:136498ms step_avg:58.71ms
step:2326/2330 train_time:136560ms step_avg:58.71ms
step:2327/2330 train_time:136616ms step_avg:58.71ms
step:2328/2330 train_time:136677ms step_avg:58.71ms
step:2329/2330 train_time:136734ms step_avg:58.71ms
step:2330/2330 train_time:136795ms step_avg:58.71ms
step:2330/2330 val_loss:3.6907 train_time:136878ms step_avg:58.75ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
