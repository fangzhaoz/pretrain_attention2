import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:08:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:80ms step_avg:79.72ms
step:2/2330 train_time:189ms step_avg:94.74ms
step:3/2330 train_time:207ms step_avg:68.97ms
step:4/2330 train_time:226ms step_avg:56.57ms
step:5/2330 train_time:280ms step_avg:56.10ms
step:6/2330 train_time:338ms step_avg:56.36ms
step:7/2330 train_time:393ms step_avg:56.14ms
step:8/2330 train_time:452ms step_avg:56.53ms
step:9/2330 train_time:508ms step_avg:56.40ms
step:10/2330 train_time:566ms step_avg:56.59ms
step:11/2330 train_time:621ms step_avg:56.46ms
step:12/2330 train_time:679ms step_avg:56.62ms
step:13/2330 train_time:734ms step_avg:56.49ms
step:14/2330 train_time:794ms step_avg:56.70ms
step:15/2330 train_time:849ms step_avg:56.58ms
step:16/2330 train_time:907ms step_avg:56.69ms
step:17/2330 train_time:962ms step_avg:56.61ms
step:18/2330 train_time:1021ms step_avg:56.72ms
step:19/2330 train_time:1077ms step_avg:56.69ms
step:20/2330 train_time:1141ms step_avg:57.04ms
step:21/2330 train_time:1199ms step_avg:57.09ms
step:22/2330 train_time:1261ms step_avg:57.34ms
step:23/2330 train_time:1317ms step_avg:57.27ms
step:24/2330 train_time:1376ms step_avg:57.35ms
step:25/2330 train_time:1433ms step_avg:57.30ms
step:26/2330 train_time:1491ms step_avg:57.34ms
step:27/2330 train_time:1546ms step_avg:57.27ms
step:28/2330 train_time:1605ms step_avg:57.31ms
step:29/2330 train_time:1660ms step_avg:57.26ms
step:30/2330 train_time:1718ms step_avg:57.28ms
step:31/2330 train_time:1774ms step_avg:57.23ms
step:32/2330 train_time:1832ms step_avg:57.26ms
step:33/2330 train_time:1888ms step_avg:57.21ms
step:34/2330 train_time:1946ms step_avg:57.24ms
step:35/2330 train_time:2002ms step_avg:57.19ms
step:36/2330 train_time:2061ms step_avg:57.26ms
step:37/2330 train_time:2119ms step_avg:57.27ms
step:38/2330 train_time:2179ms step_avg:57.35ms
step:39/2330 train_time:2236ms step_avg:57.33ms
step:40/2330 train_time:2295ms step_avg:57.39ms
step:41/2330 train_time:2352ms step_avg:57.36ms
step:42/2330 train_time:2411ms step_avg:57.41ms
step:43/2330 train_time:2467ms step_avg:57.37ms
step:44/2330 train_time:2526ms step_avg:57.41ms
step:45/2330 train_time:2582ms step_avg:57.38ms
step:46/2330 train_time:2640ms step_avg:57.40ms
step:47/2330 train_time:2696ms step_avg:57.35ms
step:48/2330 train_time:2755ms step_avg:57.39ms
step:49/2330 train_time:2810ms step_avg:57.35ms
step:50/2330 train_time:2868ms step_avg:57.37ms
step:51/2330 train_time:2923ms step_avg:57.32ms
step:52/2330 train_time:2983ms step_avg:57.36ms
step:53/2330 train_time:3039ms step_avg:57.33ms
step:54/2330 train_time:3098ms step_avg:57.37ms
step:55/2330 train_time:3155ms step_avg:57.36ms
step:56/2330 train_time:3215ms step_avg:57.42ms
step:57/2330 train_time:3271ms step_avg:57.39ms
step:58/2330 train_time:3332ms step_avg:57.45ms
step:59/2330 train_time:3388ms step_avg:57.42ms
step:60/2330 train_time:3447ms step_avg:57.45ms
step:61/2330 train_time:3503ms step_avg:57.43ms
step:62/2330 train_time:3562ms step_avg:57.44ms
step:63/2330 train_time:3617ms step_avg:57.42ms
step:64/2330 train_time:3676ms step_avg:57.43ms
step:65/2330 train_time:3731ms step_avg:57.40ms
step:66/2330 train_time:3790ms step_avg:57.42ms
step:67/2330 train_time:3845ms step_avg:57.39ms
step:68/2330 train_time:3905ms step_avg:57.42ms
step:69/2330 train_time:3961ms step_avg:57.40ms
step:70/2330 train_time:4020ms step_avg:57.43ms
step:71/2330 train_time:4076ms step_avg:57.40ms
step:72/2330 train_time:4135ms step_avg:57.43ms
step:73/2330 train_time:4191ms step_avg:57.41ms
step:74/2330 train_time:4251ms step_avg:57.45ms
step:75/2330 train_time:4308ms step_avg:57.43ms
step:76/2330 train_time:4367ms step_avg:57.46ms
step:77/2330 train_time:4423ms step_avg:57.44ms
step:78/2330 train_time:4482ms step_avg:57.47ms
step:79/2330 train_time:4538ms step_avg:57.45ms
step:80/2330 train_time:4597ms step_avg:57.46ms
step:81/2330 train_time:4653ms step_avg:57.45ms
step:82/2330 train_time:4712ms step_avg:57.47ms
step:83/2330 train_time:4768ms step_avg:57.45ms
step:84/2330 train_time:4827ms step_avg:57.46ms
step:85/2330 train_time:4883ms step_avg:57.45ms
step:86/2330 train_time:4941ms step_avg:57.46ms
step:87/2330 train_time:4998ms step_avg:57.45ms
step:88/2330 train_time:5057ms step_avg:57.46ms
step:89/2330 train_time:5113ms step_avg:57.45ms
step:90/2330 train_time:5172ms step_avg:57.47ms
step:91/2330 train_time:5228ms step_avg:57.45ms
step:92/2330 train_time:5288ms step_avg:57.47ms
step:93/2330 train_time:5343ms step_avg:57.46ms
step:94/2330 train_time:5403ms step_avg:57.48ms
step:95/2330 train_time:5459ms step_avg:57.47ms
step:96/2330 train_time:5519ms step_avg:57.49ms
step:97/2330 train_time:5575ms step_avg:57.47ms
step:98/2330 train_time:5633ms step_avg:57.48ms
step:99/2330 train_time:5689ms step_avg:57.46ms
step:100/2330 train_time:5748ms step_avg:57.48ms
step:101/2330 train_time:5803ms step_avg:57.46ms
step:102/2330 train_time:5863ms step_avg:57.48ms
step:103/2330 train_time:5919ms step_avg:57.47ms
step:104/2330 train_time:5978ms step_avg:57.48ms
step:105/2330 train_time:6033ms step_avg:57.46ms
step:106/2330 train_time:6093ms step_avg:57.48ms
step:107/2330 train_time:6150ms step_avg:57.47ms
step:108/2330 train_time:6208ms step_avg:57.49ms
step:109/2330 train_time:6264ms step_avg:57.47ms
step:110/2330 train_time:6324ms step_avg:57.49ms
step:111/2330 train_time:6381ms step_avg:57.49ms
step:112/2330 train_time:6440ms step_avg:57.50ms
step:113/2330 train_time:6496ms step_avg:57.48ms
step:114/2330 train_time:6555ms step_avg:57.50ms
step:115/2330 train_time:6611ms step_avg:57.48ms
step:116/2330 train_time:6669ms step_avg:57.49ms
step:117/2330 train_time:6724ms step_avg:57.47ms
step:118/2330 train_time:6784ms step_avg:57.49ms
step:119/2330 train_time:6840ms step_avg:57.47ms
step:120/2330 train_time:6898ms step_avg:57.48ms
step:121/2330 train_time:6953ms step_avg:57.47ms
step:122/2330 train_time:7014ms step_avg:57.49ms
step:123/2330 train_time:7071ms step_avg:57.48ms
step:124/2330 train_time:7129ms step_avg:57.49ms
step:125/2330 train_time:7184ms step_avg:57.48ms
step:126/2330 train_time:7243ms step_avg:57.48ms
step:127/2330 train_time:7299ms step_avg:57.47ms
step:128/2330 train_time:7359ms step_avg:57.49ms
step:129/2330 train_time:7415ms step_avg:57.48ms
step:130/2330 train_time:7475ms step_avg:57.50ms
step:131/2330 train_time:7530ms step_avg:57.48ms
step:132/2330 train_time:7590ms step_avg:57.50ms
step:133/2330 train_time:7645ms step_avg:57.48ms
step:134/2330 train_time:7705ms step_avg:57.50ms
step:135/2330 train_time:7762ms step_avg:57.49ms
step:136/2330 train_time:7820ms step_avg:57.50ms
step:137/2330 train_time:7875ms step_avg:57.48ms
step:138/2330 train_time:7935ms step_avg:57.50ms
step:139/2330 train_time:7990ms step_avg:57.48ms
step:140/2330 train_time:8051ms step_avg:57.50ms
step:141/2330 train_time:8106ms step_avg:57.49ms
step:142/2330 train_time:8166ms step_avg:57.50ms
step:143/2330 train_time:8221ms step_avg:57.49ms
step:144/2330 train_time:8280ms step_avg:57.50ms
step:145/2330 train_time:8336ms step_avg:57.49ms
step:146/2330 train_time:8395ms step_avg:57.50ms
step:147/2330 train_time:8451ms step_avg:57.49ms
step:148/2330 train_time:8511ms step_avg:57.50ms
step:149/2330 train_time:8567ms step_avg:57.49ms
step:150/2330 train_time:8625ms step_avg:57.50ms
step:151/2330 train_time:8681ms step_avg:57.49ms
step:152/2330 train_time:8740ms step_avg:57.50ms
step:153/2330 train_time:8795ms step_avg:57.49ms
step:154/2330 train_time:8855ms step_avg:57.50ms
step:155/2330 train_time:8911ms step_avg:57.49ms
step:156/2330 train_time:8969ms step_avg:57.49ms
step:157/2330 train_time:9025ms step_avg:57.48ms
step:158/2330 train_time:9084ms step_avg:57.49ms
step:159/2330 train_time:9140ms step_avg:57.48ms
step:160/2330 train_time:9199ms step_avg:57.49ms
step:161/2330 train_time:9255ms step_avg:57.48ms
step:162/2330 train_time:9314ms step_avg:57.49ms
step:163/2330 train_time:9370ms step_avg:57.48ms
step:164/2330 train_time:9429ms step_avg:57.49ms
step:165/2330 train_time:9484ms step_avg:57.48ms
step:166/2330 train_time:9544ms step_avg:57.50ms
step:167/2330 train_time:9601ms step_avg:57.49ms
step:168/2330 train_time:9660ms step_avg:57.50ms
step:169/2330 train_time:9716ms step_avg:57.49ms
step:170/2330 train_time:9775ms step_avg:57.50ms
step:171/2330 train_time:9831ms step_avg:57.49ms
step:172/2330 train_time:9889ms step_avg:57.50ms
step:173/2330 train_time:9945ms step_avg:57.48ms
step:174/2330 train_time:10004ms step_avg:57.50ms
step:175/2330 train_time:10061ms step_avg:57.49ms
step:176/2330 train_time:10119ms step_avg:57.50ms
step:177/2330 train_time:10176ms step_avg:57.49ms
step:178/2330 train_time:10234ms step_avg:57.50ms
step:179/2330 train_time:10291ms step_avg:57.49ms
step:180/2330 train_time:10349ms step_avg:57.50ms
step:181/2330 train_time:10405ms step_avg:57.49ms
step:182/2330 train_time:10464ms step_avg:57.49ms
step:183/2330 train_time:10520ms step_avg:57.48ms
step:184/2330 train_time:10578ms step_avg:57.49ms
step:185/2330 train_time:10634ms step_avg:57.48ms
step:186/2330 train_time:10694ms step_avg:57.49ms
step:187/2330 train_time:10750ms step_avg:57.48ms
step:188/2330 train_time:10809ms step_avg:57.49ms
step:189/2330 train_time:10864ms step_avg:57.48ms
step:190/2330 train_time:10923ms step_avg:57.49ms
step:191/2330 train_time:10978ms step_avg:57.48ms
step:192/2330 train_time:11038ms step_avg:57.49ms
step:193/2330 train_time:11094ms step_avg:57.48ms
step:194/2330 train_time:11153ms step_avg:57.49ms
step:195/2330 train_time:11209ms step_avg:57.48ms
step:196/2330 train_time:11269ms step_avg:57.49ms
step:197/2330 train_time:11324ms step_avg:57.48ms
step:198/2330 train_time:11384ms step_avg:57.49ms
step:199/2330 train_time:11439ms step_avg:57.48ms
step:200/2330 train_time:11499ms step_avg:57.49ms
step:201/2330 train_time:11555ms step_avg:57.49ms
step:202/2330 train_time:11614ms step_avg:57.50ms
step:203/2330 train_time:11670ms step_avg:57.49ms
step:204/2330 train_time:11730ms step_avg:57.50ms
step:205/2330 train_time:11785ms step_avg:57.49ms
step:206/2330 train_time:11844ms step_avg:57.50ms
step:207/2330 train_time:11901ms step_avg:57.49ms
step:208/2330 train_time:11959ms step_avg:57.50ms
step:209/2330 train_time:12015ms step_avg:57.49ms
step:210/2330 train_time:12074ms step_avg:57.50ms
step:211/2330 train_time:12130ms step_avg:57.49ms
step:212/2330 train_time:12189ms step_avg:57.49ms
step:213/2330 train_time:12244ms step_avg:57.49ms
step:214/2330 train_time:12304ms step_avg:57.50ms
step:215/2330 train_time:12361ms step_avg:57.49ms
step:216/2330 train_time:12419ms step_avg:57.50ms
step:217/2330 train_time:12475ms step_avg:57.49ms
step:218/2330 train_time:12534ms step_avg:57.50ms
step:219/2330 train_time:12590ms step_avg:57.49ms
step:220/2330 train_time:12650ms step_avg:57.50ms
step:221/2330 train_time:12705ms step_avg:57.49ms
step:222/2330 train_time:12765ms step_avg:57.50ms
step:223/2330 train_time:12821ms step_avg:57.49ms
step:224/2330 train_time:12881ms step_avg:57.50ms
step:225/2330 train_time:12937ms step_avg:57.50ms
step:226/2330 train_time:12996ms step_avg:57.51ms
step:227/2330 train_time:13052ms step_avg:57.50ms
step:228/2330 train_time:13112ms step_avg:57.51ms
step:229/2330 train_time:13167ms step_avg:57.50ms
step:230/2330 train_time:13226ms step_avg:57.50ms
step:231/2330 train_time:13283ms step_avg:57.50ms
step:232/2330 train_time:13342ms step_avg:57.51ms
step:233/2330 train_time:13398ms step_avg:57.50ms
step:234/2330 train_time:13456ms step_avg:57.51ms
step:235/2330 train_time:13513ms step_avg:57.50ms
step:236/2330 train_time:13572ms step_avg:57.51ms
step:237/2330 train_time:13628ms step_avg:57.50ms
step:238/2330 train_time:13688ms step_avg:57.51ms
step:239/2330 train_time:13743ms step_avg:57.50ms
step:240/2330 train_time:13803ms step_avg:57.51ms
step:241/2330 train_time:13859ms step_avg:57.51ms
step:242/2330 train_time:13918ms step_avg:57.51ms
step:243/2330 train_time:13975ms step_avg:57.51ms
step:244/2330 train_time:14033ms step_avg:57.51ms
step:245/2330 train_time:14088ms step_avg:57.50ms
step:246/2330 train_time:14148ms step_avg:57.51ms
step:247/2330 train_time:14204ms step_avg:57.50ms
step:248/2330 train_time:14263ms step_avg:57.51ms
step:249/2330 train_time:14319ms step_avg:57.50ms
step:250/2330 train_time:14378ms step_avg:57.51ms
step:250/2330 val_loss:4.8958 train_time:14458ms step_avg:57.83ms
step:251/2330 train_time:14476ms step_avg:57.67ms
step:252/2330 train_time:14495ms step_avg:57.52ms
step:253/2330 train_time:14550ms step_avg:57.51ms
step:254/2330 train_time:14612ms step_avg:57.53ms
step:255/2330 train_time:14667ms step_avg:57.52ms
step:256/2330 train_time:14734ms step_avg:57.55ms
step:257/2330 train_time:14789ms step_avg:57.54ms
step:258/2330 train_time:14849ms step_avg:57.56ms
step:259/2330 train_time:14904ms step_avg:57.55ms
step:260/2330 train_time:14964ms step_avg:57.55ms
step:261/2330 train_time:15019ms step_avg:57.54ms
step:262/2330 train_time:15078ms step_avg:57.55ms
step:263/2330 train_time:15133ms step_avg:57.54ms
step:264/2330 train_time:15191ms step_avg:57.54ms
step:265/2330 train_time:15247ms step_avg:57.54ms
step:266/2330 train_time:15305ms step_avg:57.54ms
step:267/2330 train_time:15360ms step_avg:57.53ms
step:268/2330 train_time:15422ms step_avg:57.55ms
step:269/2330 train_time:15479ms step_avg:57.54ms
step:270/2330 train_time:15538ms step_avg:57.55ms
step:271/2330 train_time:15595ms step_avg:57.55ms
step:272/2330 train_time:15656ms step_avg:57.56ms
step:273/2330 train_time:15712ms step_avg:57.55ms
step:274/2330 train_time:15773ms step_avg:57.56ms
step:275/2330 train_time:15829ms step_avg:57.56ms
step:276/2330 train_time:15888ms step_avg:57.57ms
step:277/2330 train_time:15944ms step_avg:57.56ms
step:278/2330 train_time:16003ms step_avg:57.56ms
step:279/2330 train_time:16058ms step_avg:57.56ms
step:280/2330 train_time:16117ms step_avg:57.56ms
step:281/2330 train_time:16172ms step_avg:57.55ms
step:282/2330 train_time:16231ms step_avg:57.56ms
step:283/2330 train_time:16286ms step_avg:57.55ms
step:284/2330 train_time:16345ms step_avg:57.55ms
step:285/2330 train_time:16401ms step_avg:57.55ms
step:286/2330 train_time:16461ms step_avg:57.56ms
step:287/2330 train_time:16518ms step_avg:57.55ms
step:288/2330 train_time:16578ms step_avg:57.56ms
step:289/2330 train_time:16634ms step_avg:57.56ms
step:290/2330 train_time:16695ms step_avg:57.57ms
step:291/2330 train_time:16752ms step_avg:57.57ms
step:292/2330 train_time:16811ms step_avg:57.57ms
step:293/2330 train_time:16867ms step_avg:57.57ms
step:294/2330 train_time:16926ms step_avg:57.57ms
step:295/2330 train_time:16981ms step_avg:57.56ms
step:296/2330 train_time:17040ms step_avg:57.57ms
step:297/2330 train_time:17096ms step_avg:57.56ms
step:298/2330 train_time:17155ms step_avg:57.57ms
step:299/2330 train_time:17210ms step_avg:57.56ms
step:300/2330 train_time:17269ms step_avg:57.56ms
step:301/2330 train_time:17324ms step_avg:57.56ms
step:302/2330 train_time:17384ms step_avg:57.56ms
step:303/2330 train_time:17439ms step_avg:57.56ms
step:304/2330 train_time:17500ms step_avg:57.57ms
step:305/2330 train_time:17557ms step_avg:57.56ms
step:306/2330 train_time:17616ms step_avg:57.57ms
step:307/2330 train_time:17672ms step_avg:57.56ms
step:308/2330 train_time:17732ms step_avg:57.57ms
step:309/2330 train_time:17788ms step_avg:57.57ms
step:310/2330 train_time:17848ms step_avg:57.57ms
step:311/2330 train_time:17904ms step_avg:57.57ms
step:312/2330 train_time:17963ms step_avg:57.57ms
step:313/2330 train_time:18019ms step_avg:57.57ms
step:314/2330 train_time:18078ms step_avg:57.57ms
step:315/2330 train_time:18134ms step_avg:57.57ms
step:316/2330 train_time:18193ms step_avg:57.57ms
step:317/2330 train_time:18249ms step_avg:57.57ms
step:318/2330 train_time:18309ms step_avg:57.58ms
step:319/2330 train_time:18365ms step_avg:57.57ms
step:320/2330 train_time:18424ms step_avg:57.58ms
step:321/2330 train_time:18479ms step_avg:57.57ms
step:322/2330 train_time:18540ms step_avg:57.58ms
step:323/2330 train_time:18597ms step_avg:57.58ms
step:324/2330 train_time:18656ms step_avg:57.58ms
step:325/2330 train_time:18713ms step_avg:57.58ms
step:326/2330 train_time:18772ms step_avg:57.58ms
step:327/2330 train_time:18828ms step_avg:57.58ms
step:328/2330 train_time:18887ms step_avg:57.58ms
step:329/2330 train_time:18943ms step_avg:57.58ms
step:330/2330 train_time:19002ms step_avg:57.58ms
step:331/2330 train_time:19057ms step_avg:57.58ms
step:332/2330 train_time:19116ms step_avg:57.58ms
step:333/2330 train_time:19172ms step_avg:57.57ms
step:334/2330 train_time:19232ms step_avg:57.58ms
step:335/2330 train_time:19288ms step_avg:57.58ms
step:336/2330 train_time:19347ms step_avg:57.58ms
step:337/2330 train_time:19403ms step_avg:57.57ms
step:338/2330 train_time:19462ms step_avg:57.58ms
step:339/2330 train_time:19518ms step_avg:57.58ms
step:340/2330 train_time:19577ms step_avg:57.58ms
step:341/2330 train_time:19634ms step_avg:57.58ms
step:342/2330 train_time:19694ms step_avg:57.59ms
step:343/2330 train_time:19750ms step_avg:57.58ms
step:344/2330 train_time:19811ms step_avg:57.59ms
step:345/2330 train_time:19867ms step_avg:57.58ms
step:346/2330 train_time:19926ms step_avg:57.59ms
step:347/2330 train_time:19981ms step_avg:57.58ms
step:348/2330 train_time:20041ms step_avg:57.59ms
step:349/2330 train_time:20097ms step_avg:57.58ms
step:350/2330 train_time:20156ms step_avg:57.59ms
step:351/2330 train_time:20212ms step_avg:57.58ms
step:352/2330 train_time:20271ms step_avg:57.59ms
step:353/2330 train_time:20326ms step_avg:57.58ms
step:354/2330 train_time:20386ms step_avg:57.59ms
step:355/2330 train_time:20442ms step_avg:57.58ms
step:356/2330 train_time:20501ms step_avg:57.59ms
step:357/2330 train_time:20557ms step_avg:57.58ms
step:358/2330 train_time:20616ms step_avg:57.59ms
step:359/2330 train_time:20672ms step_avg:57.58ms
step:360/2330 train_time:20733ms step_avg:57.59ms
step:361/2330 train_time:20789ms step_avg:57.59ms
step:362/2330 train_time:20849ms step_avg:57.60ms
step:363/2330 train_time:20905ms step_avg:57.59ms
step:364/2330 train_time:20964ms step_avg:57.59ms
step:365/2330 train_time:21020ms step_avg:57.59ms
step:366/2330 train_time:21080ms step_avg:57.59ms
step:367/2330 train_time:21136ms step_avg:57.59ms
step:368/2330 train_time:21195ms step_avg:57.60ms
step:369/2330 train_time:21252ms step_avg:57.59ms
step:370/2330 train_time:21311ms step_avg:57.60ms
step:371/2330 train_time:21366ms step_avg:57.59ms
step:372/2330 train_time:21426ms step_avg:57.60ms
step:373/2330 train_time:21481ms step_avg:57.59ms
step:374/2330 train_time:21541ms step_avg:57.60ms
step:375/2330 train_time:21597ms step_avg:57.59ms
step:376/2330 train_time:21657ms step_avg:57.60ms
step:377/2330 train_time:21712ms step_avg:57.59ms
step:378/2330 train_time:21772ms step_avg:57.60ms
step:379/2330 train_time:21828ms step_avg:57.59ms
step:380/2330 train_time:21887ms step_avg:57.60ms
step:381/2330 train_time:21943ms step_avg:57.59ms
step:382/2330 train_time:22001ms step_avg:57.59ms
step:383/2330 train_time:22057ms step_avg:57.59ms
step:384/2330 train_time:22116ms step_avg:57.59ms
step:385/2330 train_time:22172ms step_avg:57.59ms
step:386/2330 train_time:22231ms step_avg:57.59ms
step:387/2330 train_time:22287ms step_avg:57.59ms
step:388/2330 train_time:22347ms step_avg:57.59ms
step:389/2330 train_time:22403ms step_avg:57.59ms
step:390/2330 train_time:22462ms step_avg:57.59ms
step:391/2330 train_time:22518ms step_avg:57.59ms
step:392/2330 train_time:22577ms step_avg:57.59ms
step:393/2330 train_time:22633ms step_avg:57.59ms
step:394/2330 train_time:22693ms step_avg:57.60ms
step:395/2330 train_time:22748ms step_avg:57.59ms
step:396/2330 train_time:22808ms step_avg:57.60ms
step:397/2330 train_time:22864ms step_avg:57.59ms
step:398/2330 train_time:22923ms step_avg:57.60ms
step:399/2330 train_time:22979ms step_avg:57.59ms
step:400/2330 train_time:23038ms step_avg:57.59ms
step:401/2330 train_time:23095ms step_avg:57.59ms
step:402/2330 train_time:23154ms step_avg:57.60ms
step:403/2330 train_time:23210ms step_avg:57.59ms
step:404/2330 train_time:23269ms step_avg:57.60ms
step:405/2330 train_time:23326ms step_avg:57.60ms
step:406/2330 train_time:23385ms step_avg:57.60ms
step:407/2330 train_time:23440ms step_avg:57.59ms
step:408/2330 train_time:23500ms step_avg:57.60ms
step:409/2330 train_time:23557ms step_avg:57.60ms
step:410/2330 train_time:23615ms step_avg:57.60ms
step:411/2330 train_time:23671ms step_avg:57.59ms
step:412/2330 train_time:23730ms step_avg:57.60ms
step:413/2330 train_time:23786ms step_avg:57.59ms
step:414/2330 train_time:23846ms step_avg:57.60ms
step:415/2330 train_time:23901ms step_avg:57.59ms
step:416/2330 train_time:23960ms step_avg:57.60ms
step:417/2330 train_time:24016ms step_avg:57.59ms
step:418/2330 train_time:24075ms step_avg:57.60ms
step:419/2330 train_time:24131ms step_avg:57.59ms
step:420/2330 train_time:24191ms step_avg:57.60ms
step:421/2330 train_time:24246ms step_avg:57.59ms
step:422/2330 train_time:24306ms step_avg:57.60ms
step:423/2330 train_time:24361ms step_avg:57.59ms
step:424/2330 train_time:24421ms step_avg:57.60ms
step:425/2330 train_time:24477ms step_avg:57.59ms
step:426/2330 train_time:24535ms step_avg:57.59ms
step:427/2330 train_time:24591ms step_avg:57.59ms
step:428/2330 train_time:24651ms step_avg:57.60ms
step:429/2330 train_time:24706ms step_avg:57.59ms
step:430/2330 train_time:24765ms step_avg:57.59ms
step:431/2330 train_time:24821ms step_avg:57.59ms
step:432/2330 train_time:24881ms step_avg:57.59ms
step:433/2330 train_time:24937ms step_avg:57.59ms
step:434/2330 train_time:24996ms step_avg:57.59ms
step:435/2330 train_time:25052ms step_avg:57.59ms
step:436/2330 train_time:25112ms step_avg:57.60ms
step:437/2330 train_time:25168ms step_avg:57.59ms
step:438/2330 train_time:25228ms step_avg:57.60ms
step:439/2330 train_time:25284ms step_avg:57.59ms
step:440/2330 train_time:25343ms step_avg:57.60ms
step:441/2330 train_time:25399ms step_avg:57.59ms
step:442/2330 train_time:25457ms step_avg:57.60ms
step:443/2330 train_time:25513ms step_avg:57.59ms
step:444/2330 train_time:25573ms step_avg:57.60ms
step:445/2330 train_time:25629ms step_avg:57.59ms
step:446/2330 train_time:25689ms step_avg:57.60ms
step:447/2330 train_time:25745ms step_avg:57.60ms
step:448/2330 train_time:25804ms step_avg:57.60ms
step:449/2330 train_time:25859ms step_avg:57.59ms
step:450/2330 train_time:25919ms step_avg:57.60ms
step:451/2330 train_time:25975ms step_avg:57.59ms
step:452/2330 train_time:26035ms step_avg:57.60ms
step:453/2330 train_time:26091ms step_avg:57.60ms
step:454/2330 train_time:26151ms step_avg:57.60ms
step:455/2330 train_time:26207ms step_avg:57.60ms
step:456/2330 train_time:26266ms step_avg:57.60ms
step:457/2330 train_time:26322ms step_avg:57.60ms
step:458/2330 train_time:26381ms step_avg:57.60ms
step:459/2330 train_time:26437ms step_avg:57.60ms
step:460/2330 train_time:26497ms step_avg:57.60ms
step:461/2330 train_time:26554ms step_avg:57.60ms
step:462/2330 train_time:26613ms step_avg:57.60ms
step:463/2330 train_time:26669ms step_avg:57.60ms
step:464/2330 train_time:26727ms step_avg:57.60ms
step:465/2330 train_time:26783ms step_avg:57.60ms
step:466/2330 train_time:26842ms step_avg:57.60ms
step:467/2330 train_time:26898ms step_avg:57.60ms
step:468/2330 train_time:26958ms step_avg:57.60ms
step:469/2330 train_time:27014ms step_avg:57.60ms
step:470/2330 train_time:27073ms step_avg:57.60ms
step:471/2330 train_time:27130ms step_avg:57.60ms
step:472/2330 train_time:27189ms step_avg:57.60ms
step:473/2330 train_time:27245ms step_avg:57.60ms
step:474/2330 train_time:27303ms step_avg:57.60ms
step:475/2330 train_time:27359ms step_avg:57.60ms
step:476/2330 train_time:27419ms step_avg:57.60ms
step:477/2330 train_time:27475ms step_avg:57.60ms
step:478/2330 train_time:27535ms step_avg:57.61ms
step:479/2330 train_time:27591ms step_avg:57.60ms
step:480/2330 train_time:27652ms step_avg:57.61ms
step:481/2330 train_time:27708ms step_avg:57.60ms
step:482/2330 train_time:27768ms step_avg:57.61ms
step:483/2330 train_time:27824ms step_avg:57.61ms
step:484/2330 train_time:27883ms step_avg:57.61ms
step:485/2330 train_time:27939ms step_avg:57.61ms
step:486/2330 train_time:27998ms step_avg:57.61ms
step:487/2330 train_time:28054ms step_avg:57.61ms
step:488/2330 train_time:28115ms step_avg:57.61ms
step:489/2330 train_time:28171ms step_avg:57.61ms
step:490/2330 train_time:28230ms step_avg:57.61ms
step:491/2330 train_time:28286ms step_avg:57.61ms
step:492/2330 train_time:28345ms step_avg:57.61ms
step:493/2330 train_time:28400ms step_avg:57.61ms
step:494/2330 train_time:28460ms step_avg:57.61ms
step:495/2330 train_time:28516ms step_avg:57.61ms
step:496/2330 train_time:28576ms step_avg:57.61ms
step:497/2330 train_time:28632ms step_avg:57.61ms
step:498/2330 train_time:28692ms step_avg:57.61ms
step:499/2330 train_time:28748ms step_avg:57.61ms
step:500/2330 train_time:28807ms step_avg:57.61ms
step:500/2330 val_loss:4.3888 train_time:28885ms step_avg:57.77ms
step:501/2330 train_time:28904ms step_avg:57.69ms
step:502/2330 train_time:28924ms step_avg:57.62ms
step:503/2330 train_time:28983ms step_avg:57.62ms
step:504/2330 train_time:29046ms step_avg:57.63ms
step:505/2330 train_time:29103ms step_avg:57.63ms
step:506/2330 train_time:29163ms step_avg:57.63ms
step:507/2330 train_time:29219ms step_avg:57.63ms
step:508/2330 train_time:29278ms step_avg:57.63ms
step:509/2330 train_time:29333ms step_avg:57.63ms
step:510/2330 train_time:29392ms step_avg:57.63ms
step:511/2330 train_time:29448ms step_avg:57.63ms
step:512/2330 train_time:29506ms step_avg:57.63ms
step:513/2330 train_time:29561ms step_avg:57.62ms
step:514/2330 train_time:29620ms step_avg:57.63ms
step:515/2330 train_time:29675ms step_avg:57.62ms
step:516/2330 train_time:29734ms step_avg:57.62ms
step:517/2330 train_time:29789ms step_avg:57.62ms
step:518/2330 train_time:29849ms step_avg:57.62ms
step:519/2330 train_time:29905ms step_avg:57.62ms
step:520/2330 train_time:29968ms step_avg:57.63ms
step:521/2330 train_time:30024ms step_avg:57.63ms
step:522/2330 train_time:30086ms step_avg:57.64ms
step:523/2330 train_time:30143ms step_avg:57.63ms
step:524/2330 train_time:30202ms step_avg:57.64ms
step:525/2330 train_time:30259ms step_avg:57.64ms
step:526/2330 train_time:30319ms step_avg:57.64ms
step:527/2330 train_time:30375ms step_avg:57.64ms
step:528/2330 train_time:30434ms step_avg:57.64ms
step:529/2330 train_time:30489ms step_avg:57.64ms
step:530/2330 train_time:30547ms step_avg:57.64ms
step:531/2330 train_time:30603ms step_avg:57.63ms
step:532/2330 train_time:30661ms step_avg:57.63ms
step:533/2330 train_time:30717ms step_avg:57.63ms
step:534/2330 train_time:30776ms step_avg:57.63ms
step:535/2330 train_time:30831ms step_avg:57.63ms
step:536/2330 train_time:30892ms step_avg:57.64ms
step:537/2330 train_time:30948ms step_avg:57.63ms
step:538/2330 train_time:31009ms step_avg:57.64ms
step:539/2330 train_time:31065ms step_avg:57.64ms
step:540/2330 train_time:31127ms step_avg:57.64ms
step:541/2330 train_time:31183ms step_avg:57.64ms
step:542/2330 train_time:31243ms step_avg:57.64ms
step:543/2330 train_time:31300ms step_avg:57.64ms
step:544/2330 train_time:31360ms step_avg:57.65ms
step:545/2330 train_time:31416ms step_avg:57.64ms
step:546/2330 train_time:31475ms step_avg:57.65ms
step:547/2330 train_time:31531ms step_avg:57.64ms
step:548/2330 train_time:31590ms step_avg:57.65ms
step:549/2330 train_time:31646ms step_avg:57.64ms
step:550/2330 train_time:31704ms step_avg:57.64ms
step:551/2330 train_time:31761ms step_avg:57.64ms
step:552/2330 train_time:31820ms step_avg:57.64ms
step:553/2330 train_time:31877ms step_avg:57.64ms
step:554/2330 train_time:31936ms step_avg:57.65ms
step:555/2330 train_time:31993ms step_avg:57.64ms
step:556/2330 train_time:32053ms step_avg:57.65ms
step:557/2330 train_time:32108ms step_avg:57.65ms
step:558/2330 train_time:32169ms step_avg:57.65ms
step:559/2330 train_time:32225ms step_avg:57.65ms
step:560/2330 train_time:32286ms step_avg:57.65ms
step:561/2330 train_time:32342ms step_avg:57.65ms
step:562/2330 train_time:32401ms step_avg:57.65ms
step:563/2330 train_time:32458ms step_avg:57.65ms
step:564/2330 train_time:32517ms step_avg:57.65ms
step:565/2330 train_time:32573ms step_avg:57.65ms
step:566/2330 train_time:32632ms step_avg:57.65ms
step:567/2330 train_time:32688ms step_avg:57.65ms
step:568/2330 train_time:32747ms step_avg:57.65ms
step:569/2330 train_time:32802ms step_avg:57.65ms
step:570/2330 train_time:32862ms step_avg:57.65ms
step:571/2330 train_time:32918ms step_avg:57.65ms
step:572/2330 train_time:32978ms step_avg:57.65ms
step:573/2330 train_time:33035ms step_avg:57.65ms
step:574/2330 train_time:33095ms step_avg:57.66ms
step:575/2330 train_time:33151ms step_avg:57.65ms
step:576/2330 train_time:33211ms step_avg:57.66ms
step:577/2330 train_time:33268ms step_avg:57.66ms
step:578/2330 train_time:33326ms step_avg:57.66ms
step:579/2330 train_time:33383ms step_avg:57.66ms
step:580/2330 train_time:33442ms step_avg:57.66ms
step:581/2330 train_time:33498ms step_avg:57.66ms
step:582/2330 train_time:33557ms step_avg:57.66ms
step:583/2330 train_time:33613ms step_avg:57.66ms
step:584/2330 train_time:33672ms step_avg:57.66ms
step:585/2330 train_time:33728ms step_avg:57.66ms
step:586/2330 train_time:33787ms step_avg:57.66ms
step:587/2330 train_time:33843ms step_avg:57.65ms
step:588/2330 train_time:33902ms step_avg:57.66ms
step:589/2330 train_time:33958ms step_avg:57.65ms
step:590/2330 train_time:34020ms step_avg:57.66ms
step:591/2330 train_time:34076ms step_avg:57.66ms
step:592/2330 train_time:34137ms step_avg:57.66ms
step:593/2330 train_time:34193ms step_avg:57.66ms
step:594/2330 train_time:34254ms step_avg:57.67ms
step:595/2330 train_time:34310ms step_avg:57.66ms
step:596/2330 train_time:34370ms step_avg:57.67ms
step:597/2330 train_time:34426ms step_avg:57.66ms
step:598/2330 train_time:34486ms step_avg:57.67ms
step:599/2330 train_time:34541ms step_avg:57.67ms
step:600/2330 train_time:34600ms step_avg:57.67ms
step:601/2330 train_time:34656ms step_avg:57.66ms
step:602/2330 train_time:34716ms step_avg:57.67ms
step:603/2330 train_time:34771ms step_avg:57.66ms
step:604/2330 train_time:34830ms step_avg:57.67ms
step:605/2330 train_time:34886ms step_avg:57.66ms
step:606/2330 train_time:34945ms step_avg:57.67ms
step:607/2330 train_time:35002ms step_avg:57.66ms
step:608/2330 train_time:35062ms step_avg:57.67ms
step:609/2330 train_time:35118ms step_avg:57.66ms
step:610/2330 train_time:35178ms step_avg:57.67ms
step:611/2330 train_time:35235ms step_avg:57.67ms
step:612/2330 train_time:35295ms step_avg:57.67ms
step:613/2330 train_time:35351ms step_avg:57.67ms
step:614/2330 train_time:35410ms step_avg:57.67ms
step:615/2330 train_time:35466ms step_avg:57.67ms
step:616/2330 train_time:35526ms step_avg:57.67ms
step:617/2330 train_time:35581ms step_avg:57.67ms
step:618/2330 train_time:35640ms step_avg:57.67ms
step:619/2330 train_time:35696ms step_avg:57.67ms
step:620/2330 train_time:35756ms step_avg:57.67ms
step:621/2330 train_time:35812ms step_avg:57.67ms
step:622/2330 train_time:35871ms step_avg:57.67ms
step:623/2330 train_time:35927ms step_avg:57.67ms
step:624/2330 train_time:35986ms step_avg:57.67ms
step:625/2330 train_time:36042ms step_avg:57.67ms
step:626/2330 train_time:36101ms step_avg:57.67ms
step:627/2330 train_time:36159ms step_avg:57.67ms
step:628/2330 train_time:36218ms step_avg:57.67ms
step:629/2330 train_time:36274ms step_avg:57.67ms
step:630/2330 train_time:36334ms step_avg:57.67ms
step:631/2330 train_time:36390ms step_avg:57.67ms
step:632/2330 train_time:36450ms step_avg:57.67ms
step:633/2330 train_time:36505ms step_avg:57.67ms
step:634/2330 train_time:36565ms step_avg:57.67ms
step:635/2330 train_time:36620ms step_avg:57.67ms
step:636/2330 train_time:36679ms step_avg:57.67ms
step:637/2330 train_time:36734ms step_avg:57.67ms
step:638/2330 train_time:36794ms step_avg:57.67ms
step:639/2330 train_time:36850ms step_avg:57.67ms
step:640/2330 train_time:36909ms step_avg:57.67ms
step:641/2330 train_time:36965ms step_avg:57.67ms
step:642/2330 train_time:37024ms step_avg:57.67ms
step:643/2330 train_time:37081ms step_avg:57.67ms
step:644/2330 train_time:37140ms step_avg:57.67ms
step:645/2330 train_time:37197ms step_avg:57.67ms
step:646/2330 train_time:37257ms step_avg:57.67ms
step:647/2330 train_time:37313ms step_avg:57.67ms
step:648/2330 train_time:37373ms step_avg:57.67ms
step:649/2330 train_time:37429ms step_avg:57.67ms
step:650/2330 train_time:37489ms step_avg:57.68ms
step:651/2330 train_time:37545ms step_avg:57.67ms
step:652/2330 train_time:37604ms step_avg:57.67ms
step:653/2330 train_time:37660ms step_avg:57.67ms
step:654/2330 train_time:37720ms step_avg:57.68ms
step:655/2330 train_time:37777ms step_avg:57.67ms
step:656/2330 train_time:37836ms step_avg:57.68ms
step:657/2330 train_time:37892ms step_avg:57.67ms
step:658/2330 train_time:37950ms step_avg:57.67ms
step:659/2330 train_time:38005ms step_avg:57.67ms
step:660/2330 train_time:38065ms step_avg:57.67ms
step:661/2330 train_time:38121ms step_avg:57.67ms
step:662/2330 train_time:38182ms step_avg:57.68ms
step:663/2330 train_time:38238ms step_avg:57.67ms
step:664/2330 train_time:38298ms step_avg:57.68ms
step:665/2330 train_time:38354ms step_avg:57.67ms
step:666/2330 train_time:38413ms step_avg:57.68ms
step:667/2330 train_time:38469ms step_avg:57.67ms
step:668/2330 train_time:38528ms step_avg:57.68ms
step:669/2330 train_time:38584ms step_avg:57.67ms
step:670/2330 train_time:38642ms step_avg:57.68ms
step:671/2330 train_time:38698ms step_avg:57.67ms
step:672/2330 train_time:38758ms step_avg:57.68ms
step:673/2330 train_time:38814ms step_avg:57.67ms
step:674/2330 train_time:38873ms step_avg:57.68ms
step:675/2330 train_time:38928ms step_avg:57.67ms
step:676/2330 train_time:38989ms step_avg:57.68ms
step:677/2330 train_time:39044ms step_avg:57.67ms
step:678/2330 train_time:39104ms step_avg:57.68ms
step:679/2330 train_time:39161ms step_avg:57.67ms
step:680/2330 train_time:39220ms step_avg:57.68ms
step:681/2330 train_time:39276ms step_avg:57.67ms
step:682/2330 train_time:39335ms step_avg:57.68ms
step:683/2330 train_time:39391ms step_avg:57.67ms
step:684/2330 train_time:39451ms step_avg:57.68ms
step:685/2330 train_time:39507ms step_avg:57.67ms
step:686/2330 train_time:39566ms step_avg:57.68ms
step:687/2330 train_time:39621ms step_avg:57.67ms
step:688/2330 train_time:39682ms step_avg:57.68ms
step:689/2330 train_time:39738ms step_avg:57.67ms
step:690/2330 train_time:39798ms step_avg:57.68ms
step:691/2330 train_time:39854ms step_avg:57.68ms
step:692/2330 train_time:39914ms step_avg:57.68ms
step:693/2330 train_time:39969ms step_avg:57.68ms
step:694/2330 train_time:40030ms step_avg:57.68ms
step:695/2330 train_time:40085ms step_avg:57.68ms
step:696/2330 train_time:40145ms step_avg:57.68ms
step:697/2330 train_time:40200ms step_avg:57.68ms
step:698/2330 train_time:40260ms step_avg:57.68ms
step:699/2330 train_time:40317ms step_avg:57.68ms
step:700/2330 train_time:40376ms step_avg:57.68ms
step:701/2330 train_time:40433ms step_avg:57.68ms
step:702/2330 train_time:40492ms step_avg:57.68ms
step:703/2330 train_time:40548ms step_avg:57.68ms
step:704/2330 train_time:40607ms step_avg:57.68ms
step:705/2330 train_time:40663ms step_avg:57.68ms
step:706/2330 train_time:40722ms step_avg:57.68ms
step:707/2330 train_time:40779ms step_avg:57.68ms
step:708/2330 train_time:40838ms step_avg:57.68ms
step:709/2330 train_time:40894ms step_avg:57.68ms
step:710/2330 train_time:40954ms step_avg:57.68ms
step:711/2330 train_time:41010ms step_avg:57.68ms
step:712/2330 train_time:41070ms step_avg:57.68ms
step:713/2330 train_time:41126ms step_avg:57.68ms
step:714/2330 train_time:41185ms step_avg:57.68ms
step:715/2330 train_time:41241ms step_avg:57.68ms
step:716/2330 train_time:41300ms step_avg:57.68ms
step:717/2330 train_time:41356ms step_avg:57.68ms
step:718/2330 train_time:41416ms step_avg:57.68ms
step:719/2330 train_time:41472ms step_avg:57.68ms
step:720/2330 train_time:41532ms step_avg:57.68ms
step:721/2330 train_time:41588ms step_avg:57.68ms
step:722/2330 train_time:41647ms step_avg:57.68ms
step:723/2330 train_time:41703ms step_avg:57.68ms
step:724/2330 train_time:41764ms step_avg:57.68ms
step:725/2330 train_time:41820ms step_avg:57.68ms
step:726/2330 train_time:41879ms step_avg:57.68ms
step:727/2330 train_time:41935ms step_avg:57.68ms
step:728/2330 train_time:41995ms step_avg:57.68ms
step:729/2330 train_time:42050ms step_avg:57.68ms
step:730/2330 train_time:42110ms step_avg:57.68ms
step:731/2330 train_time:42165ms step_avg:57.68ms
step:732/2330 train_time:42225ms step_avg:57.68ms
step:733/2330 train_time:42281ms step_avg:57.68ms
step:734/2330 train_time:42340ms step_avg:57.68ms
step:735/2330 train_time:42397ms step_avg:57.68ms
step:736/2330 train_time:42457ms step_avg:57.69ms
step:737/2330 train_time:42513ms step_avg:57.68ms
step:738/2330 train_time:42572ms step_avg:57.69ms
step:739/2330 train_time:42627ms step_avg:57.68ms
step:740/2330 train_time:42688ms step_avg:57.69ms
step:741/2330 train_time:42743ms step_avg:57.68ms
step:742/2330 train_time:42803ms step_avg:57.69ms
step:743/2330 train_time:42860ms step_avg:57.69ms
step:744/2330 train_time:42920ms step_avg:57.69ms
step:745/2330 train_time:42976ms step_avg:57.69ms
step:746/2330 train_time:43035ms step_avg:57.69ms
step:747/2330 train_time:43092ms step_avg:57.69ms
step:748/2330 train_time:43150ms step_avg:57.69ms
step:749/2330 train_time:43206ms step_avg:57.69ms
step:750/2330 train_time:43265ms step_avg:57.69ms
step:750/2330 val_loss:4.2045 train_time:43345ms step_avg:57.79ms
step:751/2330 train_time:43363ms step_avg:57.74ms
step:752/2330 train_time:43384ms step_avg:57.69ms
step:753/2330 train_time:43440ms step_avg:57.69ms
step:754/2330 train_time:43503ms step_avg:57.70ms
step:755/2330 train_time:43560ms step_avg:57.70ms
step:756/2330 train_time:43622ms step_avg:57.70ms
step:757/2330 train_time:43678ms step_avg:57.70ms
step:758/2330 train_time:43738ms step_avg:57.70ms
step:759/2330 train_time:43794ms step_avg:57.70ms
step:760/2330 train_time:43853ms step_avg:57.70ms
step:761/2330 train_time:43908ms step_avg:57.70ms
step:762/2330 train_time:43967ms step_avg:57.70ms
step:763/2330 train_time:44023ms step_avg:57.70ms
step:764/2330 train_time:44081ms step_avg:57.70ms
step:765/2330 train_time:44138ms step_avg:57.70ms
step:766/2330 train_time:44196ms step_avg:57.70ms
step:767/2330 train_time:44251ms step_avg:57.69ms
step:768/2330 train_time:44312ms step_avg:57.70ms
step:769/2330 train_time:44369ms step_avg:57.70ms
step:770/2330 train_time:44431ms step_avg:57.70ms
step:771/2330 train_time:44488ms step_avg:57.70ms
step:772/2330 train_time:44549ms step_avg:57.71ms
step:773/2330 train_time:44607ms step_avg:57.71ms
step:774/2330 train_time:44669ms step_avg:57.71ms
step:775/2330 train_time:44726ms step_avg:57.71ms
step:776/2330 train_time:44786ms step_avg:57.71ms
step:777/2330 train_time:44843ms step_avg:57.71ms
step:778/2330 train_time:44902ms step_avg:57.71ms
step:779/2330 train_time:44959ms step_avg:57.71ms
step:780/2330 train_time:45019ms step_avg:57.72ms
step:781/2330 train_time:45076ms step_avg:57.72ms
step:782/2330 train_time:45135ms step_avg:57.72ms
step:783/2330 train_time:45192ms step_avg:57.72ms
step:784/2330 train_time:45251ms step_avg:57.72ms
step:785/2330 train_time:45308ms step_avg:57.72ms
step:786/2330 train_time:45369ms step_avg:57.72ms
step:787/2330 train_time:45427ms step_avg:57.72ms
step:788/2330 train_time:45487ms step_avg:57.72ms
step:789/2330 train_time:45545ms step_avg:57.72ms
step:790/2330 train_time:45606ms step_avg:57.73ms
step:791/2330 train_time:45664ms step_avg:57.73ms
step:792/2330 train_time:45724ms step_avg:57.73ms
step:793/2330 train_time:45782ms step_avg:57.73ms
step:794/2330 train_time:45842ms step_avg:57.74ms
step:795/2330 train_time:45900ms step_avg:57.74ms
step:796/2330 train_time:45959ms step_avg:57.74ms
step:797/2330 train_time:46015ms step_avg:57.74ms
step:798/2330 train_time:46076ms step_avg:57.74ms
step:799/2330 train_time:46132ms step_avg:57.74ms
step:800/2330 train_time:46192ms step_avg:57.74ms
step:801/2330 train_time:46248ms step_avg:57.74ms
step:802/2330 train_time:46308ms step_avg:57.74ms
step:803/2330 train_time:46365ms step_avg:57.74ms
step:804/2330 train_time:46426ms step_avg:57.74ms
step:805/2330 train_time:46484ms step_avg:57.74ms
step:806/2330 train_time:46544ms step_avg:57.75ms
step:807/2330 train_time:46601ms step_avg:57.75ms
step:808/2330 train_time:46661ms step_avg:57.75ms
step:809/2330 train_time:46719ms step_avg:57.75ms
step:810/2330 train_time:46779ms step_avg:57.75ms
step:811/2330 train_time:46836ms step_avg:57.75ms
step:812/2330 train_time:46896ms step_avg:57.75ms
step:813/2330 train_time:46953ms step_avg:57.75ms
step:814/2330 train_time:47014ms step_avg:57.76ms
step:815/2330 train_time:47070ms step_avg:57.76ms
step:816/2330 train_time:47130ms step_avg:57.76ms
step:817/2330 train_time:47187ms step_avg:57.76ms
step:818/2330 train_time:47246ms step_avg:57.76ms
step:819/2330 train_time:47303ms step_avg:57.76ms
step:820/2330 train_time:47363ms step_avg:57.76ms
step:821/2330 train_time:47420ms step_avg:57.76ms
step:822/2330 train_time:47480ms step_avg:57.76ms
step:823/2330 train_time:47537ms step_avg:57.76ms
step:824/2330 train_time:47598ms step_avg:57.76ms
step:825/2330 train_time:47655ms step_avg:57.76ms
step:826/2330 train_time:47714ms step_avg:57.76ms
step:827/2330 train_time:47771ms step_avg:57.76ms
step:828/2330 train_time:47832ms step_avg:57.77ms
step:829/2330 train_time:47889ms step_avg:57.77ms
step:830/2330 train_time:47950ms step_avg:57.77ms
step:831/2330 train_time:48008ms step_avg:57.77ms
step:832/2330 train_time:48067ms step_avg:57.77ms
step:833/2330 train_time:48124ms step_avg:57.77ms
step:834/2330 train_time:48184ms step_avg:57.77ms
step:835/2330 train_time:48240ms step_avg:57.77ms
step:836/2330 train_time:48300ms step_avg:57.78ms
step:837/2330 train_time:48357ms step_avg:57.77ms
step:838/2330 train_time:48418ms step_avg:57.78ms
step:839/2330 train_time:48474ms step_avg:57.78ms
step:840/2330 train_time:48535ms step_avg:57.78ms
step:841/2330 train_time:48592ms step_avg:57.78ms
step:842/2330 train_time:48651ms step_avg:57.78ms
step:843/2330 train_time:48709ms step_avg:57.78ms
step:844/2330 train_time:48769ms step_avg:57.78ms
step:845/2330 train_time:48826ms step_avg:57.78ms
step:846/2330 train_time:48886ms step_avg:57.79ms
step:847/2330 train_time:48944ms step_avg:57.78ms
step:848/2330 train_time:49004ms step_avg:57.79ms
step:849/2330 train_time:49061ms step_avg:57.79ms
step:850/2330 train_time:49121ms step_avg:57.79ms
step:851/2330 train_time:49178ms step_avg:57.79ms
step:852/2330 train_time:49238ms step_avg:57.79ms
step:853/2330 train_time:49294ms step_avg:57.79ms
step:854/2330 train_time:49354ms step_avg:57.79ms
step:855/2330 train_time:49411ms step_avg:57.79ms
step:856/2330 train_time:49472ms step_avg:57.79ms
step:857/2330 train_time:49528ms step_avg:57.79ms
step:858/2330 train_time:49589ms step_avg:57.80ms
step:859/2330 train_time:49646ms step_avg:57.80ms
step:860/2330 train_time:49707ms step_avg:57.80ms
step:861/2330 train_time:49764ms step_avg:57.80ms
step:862/2330 train_time:49825ms step_avg:57.80ms
step:863/2330 train_time:49882ms step_avg:57.80ms
step:864/2330 train_time:49943ms step_avg:57.80ms
step:865/2330 train_time:50000ms step_avg:57.80ms
step:866/2330 train_time:50059ms step_avg:57.80ms
step:867/2330 train_time:50116ms step_avg:57.80ms
step:868/2330 train_time:50177ms step_avg:57.81ms
step:869/2330 train_time:50234ms step_avg:57.81ms
step:870/2330 train_time:50293ms step_avg:57.81ms
step:871/2330 train_time:50349ms step_avg:57.81ms
step:872/2330 train_time:50410ms step_avg:57.81ms
step:873/2330 train_time:50467ms step_avg:57.81ms
step:874/2330 train_time:50527ms step_avg:57.81ms
step:875/2330 train_time:50584ms step_avg:57.81ms
step:876/2330 train_time:50644ms step_avg:57.81ms
step:877/2330 train_time:50701ms step_avg:57.81ms
step:878/2330 train_time:50760ms step_avg:57.81ms
step:879/2330 train_time:50818ms step_avg:57.81ms
step:880/2330 train_time:50878ms step_avg:57.82ms
step:881/2330 train_time:50935ms step_avg:57.81ms
step:882/2330 train_time:50994ms step_avg:57.82ms
step:883/2330 train_time:51050ms step_avg:57.81ms
step:884/2330 train_time:51111ms step_avg:57.82ms
step:885/2330 train_time:51168ms step_avg:57.82ms
step:886/2330 train_time:51229ms step_avg:57.82ms
step:887/2330 train_time:51286ms step_avg:57.82ms
step:888/2330 train_time:51346ms step_avg:57.82ms
step:889/2330 train_time:51403ms step_avg:57.82ms
step:890/2330 train_time:51462ms step_avg:57.82ms
step:891/2330 train_time:51519ms step_avg:57.82ms
step:892/2330 train_time:51580ms step_avg:57.83ms
step:893/2330 train_time:51636ms step_avg:57.82ms
step:894/2330 train_time:51698ms step_avg:57.83ms
step:895/2330 train_time:51754ms step_avg:57.83ms
step:896/2330 train_time:51816ms step_avg:57.83ms
step:897/2330 train_time:51873ms step_avg:57.83ms
step:898/2330 train_time:51933ms step_avg:57.83ms
step:899/2330 train_time:51990ms step_avg:57.83ms
step:900/2330 train_time:52050ms step_avg:57.83ms
step:901/2330 train_time:52107ms step_avg:57.83ms
step:902/2330 train_time:52166ms step_avg:57.83ms
step:903/2330 train_time:52224ms step_avg:57.83ms
step:904/2330 train_time:52285ms step_avg:57.84ms
step:905/2330 train_time:52342ms step_avg:57.84ms
step:906/2330 train_time:52401ms step_avg:57.84ms
step:907/2330 train_time:52458ms step_avg:57.84ms
step:908/2330 train_time:52518ms step_avg:57.84ms
step:909/2330 train_time:52575ms step_avg:57.84ms
step:910/2330 train_time:52635ms step_avg:57.84ms
step:911/2330 train_time:52692ms step_avg:57.84ms
step:912/2330 train_time:52753ms step_avg:57.84ms
step:913/2330 train_time:52810ms step_avg:57.84ms
step:914/2330 train_time:52870ms step_avg:57.84ms
step:915/2330 train_time:52927ms step_avg:57.84ms
step:916/2330 train_time:52987ms step_avg:57.85ms
step:917/2330 train_time:53044ms step_avg:57.85ms
step:918/2330 train_time:53105ms step_avg:57.85ms
step:919/2330 train_time:53162ms step_avg:57.85ms
step:920/2330 train_time:53222ms step_avg:57.85ms
step:921/2330 train_time:53279ms step_avg:57.85ms
step:922/2330 train_time:53339ms step_avg:57.85ms
step:923/2330 train_time:53396ms step_avg:57.85ms
step:924/2330 train_time:53455ms step_avg:57.85ms
step:925/2330 train_time:53512ms step_avg:57.85ms
step:926/2330 train_time:53572ms step_avg:57.85ms
step:927/2330 train_time:53629ms step_avg:57.85ms
step:928/2330 train_time:53689ms step_avg:57.85ms
step:929/2330 train_time:53747ms step_avg:57.85ms
step:930/2330 train_time:53807ms step_avg:57.86ms
step:931/2330 train_time:53864ms step_avg:57.86ms
step:932/2330 train_time:53924ms step_avg:57.86ms
step:933/2330 train_time:53981ms step_avg:57.86ms
step:934/2330 train_time:54042ms step_avg:57.86ms
step:935/2330 train_time:54098ms step_avg:57.86ms
step:936/2330 train_time:54158ms step_avg:57.86ms
step:937/2330 train_time:54215ms step_avg:57.86ms
step:938/2330 train_time:54275ms step_avg:57.86ms
step:939/2330 train_time:54332ms step_avg:57.86ms
step:940/2330 train_time:54392ms step_avg:57.86ms
step:941/2330 train_time:54449ms step_avg:57.86ms
step:942/2330 train_time:54509ms step_avg:57.86ms
step:943/2330 train_time:54566ms step_avg:57.86ms
step:944/2330 train_time:54626ms step_avg:57.87ms
step:945/2330 train_time:54682ms step_avg:57.86ms
step:946/2330 train_time:54743ms step_avg:57.87ms
step:947/2330 train_time:54800ms step_avg:57.87ms
step:948/2330 train_time:54860ms step_avg:57.87ms
step:949/2330 train_time:54917ms step_avg:57.87ms
step:950/2330 train_time:54977ms step_avg:57.87ms
step:951/2330 train_time:55035ms step_avg:57.87ms
step:952/2330 train_time:55095ms step_avg:57.87ms
step:953/2330 train_time:55151ms step_avg:57.87ms
step:954/2330 train_time:55212ms step_avg:57.87ms
step:955/2330 train_time:55268ms step_avg:57.87ms
step:956/2330 train_time:55330ms step_avg:57.88ms
step:957/2330 train_time:55387ms step_avg:57.88ms
step:958/2330 train_time:55446ms step_avg:57.88ms
step:959/2330 train_time:55504ms step_avg:57.88ms
step:960/2330 train_time:55563ms step_avg:57.88ms
step:961/2330 train_time:55621ms step_avg:57.88ms
step:962/2330 train_time:55680ms step_avg:57.88ms
step:963/2330 train_time:55737ms step_avg:57.88ms
step:964/2330 train_time:55797ms step_avg:57.88ms
step:965/2330 train_time:55854ms step_avg:57.88ms
step:966/2330 train_time:55914ms step_avg:57.88ms
step:967/2330 train_time:55971ms step_avg:57.88ms
step:968/2330 train_time:56031ms step_avg:57.88ms
step:969/2330 train_time:56088ms step_avg:57.88ms
step:970/2330 train_time:56148ms step_avg:57.88ms
step:971/2330 train_time:56205ms step_avg:57.88ms
step:972/2330 train_time:56266ms step_avg:57.89ms
step:973/2330 train_time:56323ms step_avg:57.89ms
step:974/2330 train_time:56383ms step_avg:57.89ms
step:975/2330 train_time:56440ms step_avg:57.89ms
step:976/2330 train_time:56499ms step_avg:57.89ms
step:977/2330 train_time:56556ms step_avg:57.89ms
step:978/2330 train_time:56617ms step_avg:57.89ms
step:979/2330 train_time:56674ms step_avg:57.89ms
step:980/2330 train_time:56733ms step_avg:57.89ms
step:981/2330 train_time:56790ms step_avg:57.89ms
step:982/2330 train_time:56851ms step_avg:57.89ms
step:983/2330 train_time:56909ms step_avg:57.89ms
step:984/2330 train_time:56969ms step_avg:57.90ms
step:985/2330 train_time:57027ms step_avg:57.90ms
step:986/2330 train_time:57086ms step_avg:57.90ms
step:987/2330 train_time:57143ms step_avg:57.90ms
step:988/2330 train_time:57204ms step_avg:57.90ms
step:989/2330 train_time:57260ms step_avg:57.90ms
step:990/2330 train_time:57320ms step_avg:57.90ms
step:991/2330 train_time:57378ms step_avg:57.90ms
step:992/2330 train_time:57438ms step_avg:57.90ms
step:993/2330 train_time:57495ms step_avg:57.90ms
step:994/2330 train_time:57554ms step_avg:57.90ms
step:995/2330 train_time:57611ms step_avg:57.90ms
step:996/2330 train_time:57671ms step_avg:57.90ms
step:997/2330 train_time:57727ms step_avg:57.90ms
step:998/2330 train_time:57789ms step_avg:57.90ms
step:999/2330 train_time:57846ms step_avg:57.90ms
step:1000/2330 train_time:57907ms step_avg:57.91ms
step:1000/2330 val_loss:4.0616 train_time:57987ms step_avg:57.99ms
step:1001/2330 train_time:58006ms step_avg:57.95ms
step:1002/2330 train_time:58025ms step_avg:57.91ms
step:1003/2330 train_time:58079ms step_avg:57.91ms
step:1004/2330 train_time:58146ms step_avg:57.91ms
step:1005/2330 train_time:58201ms step_avg:57.91ms
step:1006/2330 train_time:58267ms step_avg:57.92ms
step:1007/2330 train_time:58323ms step_avg:57.92ms
step:1008/2330 train_time:58385ms step_avg:57.92ms
step:1009/2330 train_time:58440ms step_avg:57.92ms
step:1010/2330 train_time:58501ms step_avg:57.92ms
step:1011/2330 train_time:58557ms step_avg:57.92ms
step:1012/2330 train_time:58618ms step_avg:57.92ms
step:1013/2330 train_time:58674ms step_avg:57.92ms
step:1014/2330 train_time:58733ms step_avg:57.92ms
step:1015/2330 train_time:58789ms step_avg:57.92ms
step:1016/2330 train_time:58848ms step_avg:57.92ms
step:1017/2330 train_time:58907ms step_avg:57.92ms
step:1018/2330 train_time:58969ms step_avg:57.93ms
step:1019/2330 train_time:59027ms step_avg:57.93ms
step:1020/2330 train_time:59087ms step_avg:57.93ms
step:1021/2330 train_time:59145ms step_avg:57.93ms
step:1022/2330 train_time:59206ms step_avg:57.93ms
step:1023/2330 train_time:59263ms step_avg:57.93ms
step:1024/2330 train_time:59324ms step_avg:57.93ms
step:1025/2330 train_time:59381ms step_avg:57.93ms
step:1026/2330 train_time:59441ms step_avg:57.93ms
step:1027/2330 train_time:59498ms step_avg:57.93ms
step:1028/2330 train_time:59557ms step_avg:57.94ms
step:1029/2330 train_time:59613ms step_avg:57.93ms
step:1030/2330 train_time:59673ms step_avg:57.93ms
step:1031/2330 train_time:59729ms step_avg:57.93ms
step:1032/2330 train_time:59789ms step_avg:57.94ms
step:1033/2330 train_time:59846ms step_avg:57.93ms
step:1034/2330 train_time:59907ms step_avg:57.94ms
step:1035/2330 train_time:59965ms step_avg:57.94ms
step:1036/2330 train_time:60026ms step_avg:57.94ms
step:1037/2330 train_time:60083ms step_avg:57.94ms
step:1038/2330 train_time:60144ms step_avg:57.94ms
step:1039/2330 train_time:60200ms step_avg:57.94ms
step:1040/2330 train_time:60262ms step_avg:57.94ms
step:1041/2330 train_time:60319ms step_avg:57.94ms
step:1042/2330 train_time:60380ms step_avg:57.95ms
step:1043/2330 train_time:60436ms step_avg:57.94ms
step:1044/2330 train_time:60496ms step_avg:57.95ms
step:1045/2330 train_time:60552ms step_avg:57.94ms
step:1046/2330 train_time:60612ms step_avg:57.95ms
step:1047/2330 train_time:60668ms step_avg:57.94ms
step:1048/2330 train_time:60728ms step_avg:57.95ms
step:1049/2330 train_time:60784ms step_avg:57.94ms
step:1050/2330 train_time:60844ms step_avg:57.95ms
step:1051/2330 train_time:60900ms step_avg:57.95ms
step:1052/2330 train_time:60961ms step_avg:57.95ms
step:1053/2330 train_time:61018ms step_avg:57.95ms
step:1054/2330 train_time:61079ms step_avg:57.95ms
step:1055/2330 train_time:61135ms step_avg:57.95ms
step:1056/2330 train_time:61197ms step_avg:57.95ms
step:1057/2330 train_time:61254ms step_avg:57.95ms
step:1058/2330 train_time:61314ms step_avg:57.95ms
step:1059/2330 train_time:61371ms step_avg:57.95ms
step:1060/2330 train_time:61433ms step_avg:57.96ms
step:1061/2330 train_time:61490ms step_avg:57.95ms
step:1062/2330 train_time:61549ms step_avg:57.96ms
step:1063/2330 train_time:61607ms step_avg:57.96ms
step:1064/2330 train_time:61666ms step_avg:57.96ms
step:1065/2330 train_time:61722ms step_avg:57.96ms
step:1066/2330 train_time:61782ms step_avg:57.96ms
step:1067/2330 train_time:61839ms step_avg:57.96ms
step:1068/2330 train_time:61898ms step_avg:57.96ms
step:1069/2330 train_time:61955ms step_avg:57.96ms
step:1070/2330 train_time:62015ms step_avg:57.96ms
step:1071/2330 train_time:62072ms step_avg:57.96ms
step:1072/2330 train_time:62134ms step_avg:57.96ms
step:1073/2330 train_time:62191ms step_avg:57.96ms
step:1074/2330 train_time:62253ms step_avg:57.96ms
step:1075/2330 train_time:62309ms step_avg:57.96ms
step:1076/2330 train_time:62370ms step_avg:57.96ms
step:1077/2330 train_time:62427ms step_avg:57.96ms
step:1078/2330 train_time:62487ms step_avg:57.97ms
step:1079/2330 train_time:62544ms step_avg:57.96ms
step:1080/2330 train_time:62603ms step_avg:57.97ms
step:1081/2330 train_time:62659ms step_avg:57.96ms
step:1082/2330 train_time:62719ms step_avg:57.97ms
step:1083/2330 train_time:62776ms step_avg:57.96ms
step:1084/2330 train_time:62835ms step_avg:57.97ms
step:1085/2330 train_time:62892ms step_avg:57.96ms
step:1086/2330 train_time:62952ms step_avg:57.97ms
step:1087/2330 train_time:63010ms step_avg:57.97ms
step:1088/2330 train_time:63069ms step_avg:57.97ms
step:1089/2330 train_time:63126ms step_avg:57.97ms
step:1090/2330 train_time:63187ms step_avg:57.97ms
step:1091/2330 train_time:63244ms step_avg:57.97ms
step:1092/2330 train_time:63306ms step_avg:57.97ms
step:1093/2330 train_time:63363ms step_avg:57.97ms
step:1094/2330 train_time:63423ms step_avg:57.97ms
step:1095/2330 train_time:63480ms step_avg:57.97ms
step:1096/2330 train_time:63541ms step_avg:57.98ms
step:1097/2330 train_time:63598ms step_avg:57.97ms
step:1098/2330 train_time:63657ms step_avg:57.98ms
step:1099/2330 train_time:63714ms step_avg:57.97ms
step:1100/2330 train_time:63774ms step_avg:57.98ms
step:1101/2330 train_time:63831ms step_avg:57.98ms
step:1102/2330 train_time:63891ms step_avg:57.98ms
step:1103/2330 train_time:63948ms step_avg:57.98ms
step:1104/2330 train_time:64007ms step_avg:57.98ms
step:1105/2330 train_time:64065ms step_avg:57.98ms
step:1106/2330 train_time:64125ms step_avg:57.98ms
step:1107/2330 train_time:64182ms step_avg:57.98ms
step:1108/2330 train_time:64243ms step_avg:57.98ms
step:1109/2330 train_time:64300ms step_avg:57.98ms
step:1110/2330 train_time:64361ms step_avg:57.98ms
step:1111/2330 train_time:64418ms step_avg:57.98ms
step:1112/2330 train_time:64478ms step_avg:57.98ms
step:1113/2330 train_time:64535ms step_avg:57.98ms
step:1114/2330 train_time:64595ms step_avg:57.98ms
step:1115/2330 train_time:64651ms step_avg:57.98ms
step:1116/2330 train_time:64712ms step_avg:57.99ms
step:1117/2330 train_time:64768ms step_avg:57.98ms
step:1118/2330 train_time:64828ms step_avg:57.99ms
step:1119/2330 train_time:64886ms step_avg:57.99ms
step:1120/2330 train_time:64946ms step_avg:57.99ms
step:1121/2330 train_time:65003ms step_avg:57.99ms
step:1122/2330 train_time:65064ms step_avg:57.99ms
step:1123/2330 train_time:65121ms step_avg:57.99ms
step:1124/2330 train_time:65181ms step_avg:57.99ms
step:1125/2330 train_time:65239ms step_avg:57.99ms
step:1126/2330 train_time:65299ms step_avg:57.99ms
step:1127/2330 train_time:65356ms step_avg:57.99ms
step:1128/2330 train_time:65416ms step_avg:57.99ms
step:1129/2330 train_time:65473ms step_avg:57.99ms
step:1130/2330 train_time:65534ms step_avg:57.99ms
step:1131/2330 train_time:65590ms step_avg:57.99ms
step:1132/2330 train_time:65651ms step_avg:58.00ms
step:1133/2330 train_time:65708ms step_avg:57.99ms
step:1134/2330 train_time:65767ms step_avg:58.00ms
step:1135/2330 train_time:65824ms step_avg:58.00ms
step:1136/2330 train_time:65884ms step_avg:58.00ms
step:1137/2330 train_time:65940ms step_avg:58.00ms
step:1138/2330 train_time:66000ms step_avg:58.00ms
step:1139/2330 train_time:66056ms step_avg:58.00ms
step:1140/2330 train_time:66117ms step_avg:58.00ms
step:1141/2330 train_time:66174ms step_avg:58.00ms
step:1142/2330 train_time:66234ms step_avg:58.00ms
step:1143/2330 train_time:66291ms step_avg:58.00ms
step:1144/2330 train_time:66352ms step_avg:58.00ms
step:1145/2330 train_time:66409ms step_avg:58.00ms
step:1146/2330 train_time:66468ms step_avg:58.00ms
step:1147/2330 train_time:66525ms step_avg:58.00ms
step:1148/2330 train_time:66586ms step_avg:58.00ms
step:1149/2330 train_time:66642ms step_avg:58.00ms
step:1150/2330 train_time:66703ms step_avg:58.00ms
step:1151/2330 train_time:66760ms step_avg:58.00ms
step:1152/2330 train_time:66820ms step_avg:58.00ms
step:1153/2330 train_time:66876ms step_avg:58.00ms
step:1154/2330 train_time:66937ms step_avg:58.00ms
step:1155/2330 train_time:66993ms step_avg:58.00ms
step:1156/2330 train_time:67053ms step_avg:58.00ms
step:1157/2330 train_time:67109ms step_avg:58.00ms
step:1158/2330 train_time:67171ms step_avg:58.01ms
step:1159/2330 train_time:67227ms step_avg:58.00ms
step:1160/2330 train_time:67289ms step_avg:58.01ms
step:1161/2330 train_time:67345ms step_avg:58.01ms
step:1162/2330 train_time:67407ms step_avg:58.01ms
step:1163/2330 train_time:67464ms step_avg:58.01ms
step:1164/2330 train_time:67524ms step_avg:58.01ms
step:1165/2330 train_time:67580ms step_avg:58.01ms
step:1166/2330 train_time:67641ms step_avg:58.01ms
step:1167/2330 train_time:67697ms step_avg:58.01ms
step:1168/2330 train_time:67759ms step_avg:58.01ms
step:1169/2330 train_time:67815ms step_avg:58.01ms
step:1170/2330 train_time:67876ms step_avg:58.01ms
step:1171/2330 train_time:67932ms step_avg:58.01ms
step:1172/2330 train_time:67993ms step_avg:58.01ms
step:1173/2330 train_time:68049ms step_avg:58.01ms
step:1174/2330 train_time:68110ms step_avg:58.02ms
step:1175/2330 train_time:68167ms step_avg:58.01ms
step:1176/2330 train_time:68228ms step_avg:58.02ms
step:1177/2330 train_time:68285ms step_avg:58.02ms
step:1178/2330 train_time:68345ms step_avg:58.02ms
step:1179/2330 train_time:68402ms step_avg:58.02ms
step:1180/2330 train_time:68462ms step_avg:58.02ms
step:1181/2330 train_time:68520ms step_avg:58.02ms
step:1182/2330 train_time:68580ms step_avg:58.02ms
step:1183/2330 train_time:68637ms step_avg:58.02ms
step:1184/2330 train_time:68697ms step_avg:58.02ms
step:1185/2330 train_time:68755ms step_avg:58.02ms
step:1186/2330 train_time:68814ms step_avg:58.02ms
step:1187/2330 train_time:68870ms step_avg:58.02ms
step:1188/2330 train_time:68931ms step_avg:58.02ms
step:1189/2330 train_time:68987ms step_avg:58.02ms
step:1190/2330 train_time:69048ms step_avg:58.02ms
step:1191/2330 train_time:69104ms step_avg:58.02ms
step:1192/2330 train_time:69165ms step_avg:58.02ms
step:1193/2330 train_time:69222ms step_avg:58.02ms
step:1194/2330 train_time:69282ms step_avg:58.03ms
step:1195/2330 train_time:69339ms step_avg:58.02ms
step:1196/2330 train_time:69400ms step_avg:58.03ms
step:1197/2330 train_time:69456ms step_avg:58.03ms
step:1198/2330 train_time:69517ms step_avg:58.03ms
step:1199/2330 train_time:69573ms step_avg:58.03ms
step:1200/2330 train_time:69634ms step_avg:58.03ms
step:1201/2330 train_time:69691ms step_avg:58.03ms
step:1202/2330 train_time:69750ms step_avg:58.03ms
step:1203/2330 train_time:69808ms step_avg:58.03ms
step:1204/2330 train_time:69868ms step_avg:58.03ms
step:1205/2330 train_time:69925ms step_avg:58.03ms
step:1206/2330 train_time:69985ms step_avg:58.03ms
step:1207/2330 train_time:70042ms step_avg:58.03ms
step:1208/2330 train_time:70103ms step_avg:58.03ms
step:1209/2330 train_time:70160ms step_avg:58.03ms
step:1210/2330 train_time:70220ms step_avg:58.03ms
step:1211/2330 train_time:70277ms step_avg:58.03ms
step:1212/2330 train_time:70337ms step_avg:58.03ms
step:1213/2330 train_time:70393ms step_avg:58.03ms
step:1214/2330 train_time:70454ms step_avg:58.03ms
step:1215/2330 train_time:70511ms step_avg:58.03ms
step:1216/2330 train_time:70571ms step_avg:58.04ms
step:1217/2330 train_time:70629ms step_avg:58.04ms
step:1218/2330 train_time:70689ms step_avg:58.04ms
step:1219/2330 train_time:70746ms step_avg:58.04ms
step:1220/2330 train_time:70806ms step_avg:58.04ms
step:1221/2330 train_time:70863ms step_avg:58.04ms
step:1222/2330 train_time:70924ms step_avg:58.04ms
step:1223/2330 train_time:70980ms step_avg:58.04ms
step:1224/2330 train_time:71041ms step_avg:58.04ms
step:1225/2330 train_time:71097ms step_avg:58.04ms
step:1226/2330 train_time:71157ms step_avg:58.04ms
step:1227/2330 train_time:71214ms step_avg:58.04ms
step:1228/2330 train_time:71274ms step_avg:58.04ms
step:1229/2330 train_time:71331ms step_avg:58.04ms
step:1230/2330 train_time:71391ms step_avg:58.04ms
step:1231/2330 train_time:71448ms step_avg:58.04ms
step:1232/2330 train_time:71508ms step_avg:58.04ms
step:1233/2330 train_time:71565ms step_avg:58.04ms
step:1234/2330 train_time:71626ms step_avg:58.04ms
step:1235/2330 train_time:71682ms step_avg:58.04ms
step:1236/2330 train_time:71742ms step_avg:58.04ms
step:1237/2330 train_time:71798ms step_avg:58.04ms
step:1238/2330 train_time:71859ms step_avg:58.04ms
step:1239/2330 train_time:71916ms step_avg:58.04ms
step:1240/2330 train_time:71976ms step_avg:58.05ms
step:1241/2330 train_time:72032ms step_avg:58.04ms
step:1242/2330 train_time:72094ms step_avg:58.05ms
step:1243/2330 train_time:72150ms step_avg:58.04ms
step:1244/2330 train_time:72210ms step_avg:58.05ms
step:1245/2330 train_time:72267ms step_avg:58.05ms
step:1246/2330 train_time:72330ms step_avg:58.05ms
step:1247/2330 train_time:72387ms step_avg:58.05ms
step:1248/2330 train_time:72447ms step_avg:58.05ms
step:1249/2330 train_time:72505ms step_avg:58.05ms
step:1250/2330 train_time:72565ms step_avg:58.05ms
step:1250/2330 val_loss:3.9843 train_time:72645ms step_avg:58.12ms
step:1251/2330 train_time:72663ms step_avg:58.08ms
step:1252/2330 train_time:72683ms step_avg:58.05ms
step:1253/2330 train_time:72744ms step_avg:58.06ms
step:1254/2330 train_time:72810ms step_avg:58.06ms
step:1255/2330 train_time:72867ms step_avg:58.06ms
step:1256/2330 train_time:72928ms step_avg:58.06ms
step:1257/2330 train_time:72984ms step_avg:58.06ms
step:1258/2330 train_time:73043ms step_avg:58.06ms
step:1259/2330 train_time:73100ms step_avg:58.06ms
step:1260/2330 train_time:73160ms step_avg:58.06ms
step:1261/2330 train_time:73216ms step_avg:58.06ms
step:1262/2330 train_time:73275ms step_avg:58.06ms
step:1263/2330 train_time:73331ms step_avg:58.06ms
step:1264/2330 train_time:73391ms step_avg:58.06ms
step:1265/2330 train_time:73446ms step_avg:58.06ms
step:1266/2330 train_time:73505ms step_avg:58.06ms
step:1267/2330 train_time:73561ms step_avg:58.06ms
step:1268/2330 train_time:73623ms step_avg:58.06ms
step:1269/2330 train_time:73682ms step_avg:58.06ms
step:1270/2330 train_time:73744ms step_avg:58.07ms
step:1271/2330 train_time:73802ms step_avg:58.07ms
step:1272/2330 train_time:73864ms step_avg:58.07ms
step:1273/2330 train_time:73921ms step_avg:58.07ms
step:1274/2330 train_time:73982ms step_avg:58.07ms
step:1275/2330 train_time:74040ms step_avg:58.07ms
step:1276/2330 train_time:74100ms step_avg:58.07ms
step:1277/2330 train_time:74156ms step_avg:58.07ms
step:1278/2330 train_time:74216ms step_avg:58.07ms
step:1279/2330 train_time:74273ms step_avg:58.07ms
step:1280/2330 train_time:74333ms step_avg:58.07ms
step:1281/2330 train_time:74389ms step_avg:58.07ms
step:1282/2330 train_time:74449ms step_avg:58.07ms
step:1283/2330 train_time:74505ms step_avg:58.07ms
step:1284/2330 train_time:74564ms step_avg:58.07ms
step:1285/2330 train_time:74621ms step_avg:58.07ms
step:1286/2330 train_time:74682ms step_avg:58.07ms
step:1287/2330 train_time:74740ms step_avg:58.07ms
step:1288/2330 train_time:74801ms step_avg:58.08ms
step:1289/2330 train_time:74860ms step_avg:58.08ms
step:1290/2330 train_time:74920ms step_avg:58.08ms
step:1291/2330 train_time:74977ms step_avg:58.08ms
step:1292/2330 train_time:75037ms step_avg:58.08ms
step:1293/2330 train_time:75094ms step_avg:58.08ms
step:1294/2330 train_time:75154ms step_avg:58.08ms
step:1295/2330 train_time:75210ms step_avg:58.08ms
step:1296/2330 train_time:75270ms step_avg:58.08ms
step:1297/2330 train_time:75326ms step_avg:58.08ms
step:1298/2330 train_time:75386ms step_avg:58.08ms
step:1299/2330 train_time:75442ms step_avg:58.08ms
step:1300/2330 train_time:75502ms step_avg:58.08ms
step:1301/2330 train_time:75559ms step_avg:58.08ms
step:1302/2330 train_time:75618ms step_avg:58.08ms
step:1303/2330 train_time:75675ms step_avg:58.08ms
step:1304/2330 train_time:75736ms step_avg:58.08ms
step:1305/2330 train_time:75792ms step_avg:58.08ms
step:1306/2330 train_time:75855ms step_avg:58.08ms
step:1307/2330 train_time:75911ms step_avg:58.08ms
step:1308/2330 train_time:75972ms step_avg:58.08ms
step:1309/2330 train_time:76029ms step_avg:58.08ms
step:1310/2330 train_time:76089ms step_avg:58.08ms
step:1311/2330 train_time:76146ms step_avg:58.08ms
step:1312/2330 train_time:76206ms step_avg:58.08ms
step:1313/2330 train_time:76263ms step_avg:58.08ms
step:1314/2330 train_time:76322ms step_avg:58.08ms
step:1315/2330 train_time:76379ms step_avg:58.08ms
step:1316/2330 train_time:76439ms step_avg:58.08ms
step:1317/2330 train_time:76495ms step_avg:58.08ms
step:1318/2330 train_time:76555ms step_avg:58.08ms
step:1319/2330 train_time:76612ms step_avg:58.08ms
step:1320/2330 train_time:76672ms step_avg:58.08ms
step:1321/2330 train_time:76730ms step_avg:58.08ms
step:1322/2330 train_time:76790ms step_avg:58.09ms
step:1323/2330 train_time:76848ms step_avg:58.09ms
step:1324/2330 train_time:76908ms step_avg:58.09ms
step:1325/2330 train_time:76965ms step_avg:58.09ms
step:1326/2330 train_time:77025ms step_avg:58.09ms
step:1327/2330 train_time:77083ms step_avg:58.09ms
step:1328/2330 train_time:77144ms step_avg:58.09ms
step:1329/2330 train_time:77201ms step_avg:58.09ms
step:1330/2330 train_time:77260ms step_avg:58.09ms
step:1331/2330 train_time:77317ms step_avg:58.09ms
step:1332/2330 train_time:77377ms step_avg:58.09ms
step:1333/2330 train_time:77433ms step_avg:58.09ms
step:1334/2330 train_time:77493ms step_avg:58.09ms
step:1335/2330 train_time:77550ms step_avg:58.09ms
step:1336/2330 train_time:77610ms step_avg:58.09ms
step:1337/2330 train_time:77667ms step_avg:58.09ms
step:1338/2330 train_time:77727ms step_avg:58.09ms
step:1339/2330 train_time:77784ms step_avg:58.09ms
step:1340/2330 train_time:77844ms step_avg:58.09ms
step:1341/2330 train_time:77900ms step_avg:58.09ms
step:1342/2330 train_time:77961ms step_avg:58.09ms
step:1343/2330 train_time:78019ms step_avg:58.09ms
step:1344/2330 train_time:78079ms step_avg:58.09ms
step:1345/2330 train_time:78135ms step_avg:58.09ms
step:1346/2330 train_time:78197ms step_avg:58.10ms
step:1347/2330 train_time:78253ms step_avg:58.09ms
step:1348/2330 train_time:78313ms step_avg:58.10ms
step:1349/2330 train_time:78370ms step_avg:58.10ms
step:1350/2330 train_time:78430ms step_avg:58.10ms
step:1351/2330 train_time:78487ms step_avg:58.10ms
step:1352/2330 train_time:78546ms step_avg:58.10ms
step:1353/2330 train_time:78603ms step_avg:58.10ms
step:1354/2330 train_time:78663ms step_avg:58.10ms
step:1355/2330 train_time:78720ms step_avg:58.10ms
step:1356/2330 train_time:78781ms step_avg:58.10ms
step:1357/2330 train_time:78838ms step_avg:58.10ms
step:1358/2330 train_time:78898ms step_avg:58.10ms
step:1359/2330 train_time:78955ms step_avg:58.10ms
step:1360/2330 train_time:79015ms step_avg:58.10ms
step:1361/2330 train_time:79072ms step_avg:58.10ms
step:1362/2330 train_time:79132ms step_avg:58.10ms
step:1363/2330 train_time:79188ms step_avg:58.10ms
step:1364/2330 train_time:79250ms step_avg:58.10ms
step:1365/2330 train_time:79306ms step_avg:58.10ms
step:1366/2330 train_time:79367ms step_avg:58.10ms
step:1367/2330 train_time:79424ms step_avg:58.10ms
step:1368/2330 train_time:79484ms step_avg:58.10ms
step:1369/2330 train_time:79541ms step_avg:58.10ms
step:1370/2330 train_time:79601ms step_avg:58.10ms
step:1371/2330 train_time:79658ms step_avg:58.10ms
step:1372/2330 train_time:79718ms step_avg:58.10ms
step:1373/2330 train_time:79775ms step_avg:58.10ms
step:1374/2330 train_time:79835ms step_avg:58.10ms
step:1375/2330 train_time:79892ms step_avg:58.10ms
step:1376/2330 train_time:79953ms step_avg:58.11ms
step:1377/2330 train_time:80009ms step_avg:58.10ms
step:1378/2330 train_time:80069ms step_avg:58.11ms
step:1379/2330 train_time:80125ms step_avg:58.10ms
step:1380/2330 train_time:80186ms step_avg:58.11ms
step:1381/2330 train_time:80243ms step_avg:58.10ms
step:1382/2330 train_time:80302ms step_avg:58.11ms
step:1383/2330 train_time:80359ms step_avg:58.11ms
step:1384/2330 train_time:80419ms step_avg:58.11ms
step:1385/2330 train_time:80477ms step_avg:58.11ms
step:1386/2330 train_time:80536ms step_avg:58.11ms
step:1387/2330 train_time:80593ms step_avg:58.11ms
step:1388/2330 train_time:80654ms step_avg:58.11ms
step:1389/2330 train_time:80711ms step_avg:58.11ms
step:1390/2330 train_time:80771ms step_avg:58.11ms
step:1391/2330 train_time:80828ms step_avg:58.11ms
step:1392/2330 train_time:80888ms step_avg:58.11ms
step:1393/2330 train_time:80945ms step_avg:58.11ms
step:1394/2330 train_time:81005ms step_avg:58.11ms
step:1395/2330 train_time:81062ms step_avg:58.11ms
step:1396/2330 train_time:81122ms step_avg:58.11ms
step:1397/2330 train_time:81178ms step_avg:58.11ms
step:1398/2330 train_time:81238ms step_avg:58.11ms
step:1399/2330 train_time:81295ms step_avg:58.11ms
step:1400/2330 train_time:81356ms step_avg:58.11ms
step:1401/2330 train_time:81411ms step_avg:58.11ms
step:1402/2330 train_time:81473ms step_avg:58.11ms
step:1403/2330 train_time:81530ms step_avg:58.11ms
step:1404/2330 train_time:81590ms step_avg:58.11ms
step:1405/2330 train_time:81647ms step_avg:58.11ms
step:1406/2330 train_time:81707ms step_avg:58.11ms
step:1407/2330 train_time:81763ms step_avg:58.11ms
step:1408/2330 train_time:81824ms step_avg:58.11ms
step:1409/2330 train_time:81882ms step_avg:58.11ms
step:1410/2330 train_time:81941ms step_avg:58.11ms
step:1411/2330 train_time:81999ms step_avg:58.11ms
step:1412/2330 train_time:82059ms step_avg:58.12ms
step:1413/2330 train_time:82116ms step_avg:58.11ms
step:1414/2330 train_time:82176ms step_avg:58.12ms
step:1415/2330 train_time:82234ms step_avg:58.12ms
step:1416/2330 train_time:82295ms step_avg:58.12ms
step:1417/2330 train_time:82351ms step_avg:58.12ms
step:1418/2330 train_time:82410ms step_avg:58.12ms
step:1419/2330 train_time:82468ms step_avg:58.12ms
step:1420/2330 train_time:82528ms step_avg:58.12ms
step:1421/2330 train_time:82585ms step_avg:58.12ms
step:1422/2330 train_time:82645ms step_avg:58.12ms
step:1423/2330 train_time:82703ms step_avg:58.12ms
step:1424/2330 train_time:82762ms step_avg:58.12ms
step:1425/2330 train_time:82819ms step_avg:58.12ms
step:1426/2330 train_time:82879ms step_avg:58.12ms
step:1427/2330 train_time:82937ms step_avg:58.12ms
step:1428/2330 train_time:82997ms step_avg:58.12ms
step:1429/2330 train_time:83054ms step_avg:58.12ms
step:1430/2330 train_time:83114ms step_avg:58.12ms
step:1431/2330 train_time:83171ms step_avg:58.12ms
step:1432/2330 train_time:83232ms step_avg:58.12ms
step:1433/2330 train_time:83289ms step_avg:58.12ms
step:1434/2330 train_time:83348ms step_avg:58.12ms
step:1435/2330 train_time:83404ms step_avg:58.12ms
step:1436/2330 train_time:83466ms step_avg:58.12ms
step:1437/2330 train_time:83523ms step_avg:58.12ms
step:1438/2330 train_time:83583ms step_avg:58.12ms
step:1439/2330 train_time:83640ms step_avg:58.12ms
step:1440/2330 train_time:83700ms step_avg:58.13ms
step:1441/2330 train_time:83757ms step_avg:58.12ms
step:1442/2330 train_time:83817ms step_avg:58.13ms
step:1443/2330 train_time:83875ms step_avg:58.13ms
step:1444/2330 train_time:83934ms step_avg:58.13ms
step:1445/2330 train_time:83991ms step_avg:58.13ms
step:1446/2330 train_time:84050ms step_avg:58.13ms
step:1447/2330 train_time:84107ms step_avg:58.13ms
step:1448/2330 train_time:84168ms step_avg:58.13ms
step:1449/2330 train_time:84225ms step_avg:58.13ms
step:1450/2330 train_time:84285ms step_avg:58.13ms
step:1451/2330 train_time:84342ms step_avg:58.13ms
step:1452/2330 train_time:84402ms step_avg:58.13ms
step:1453/2330 train_time:84459ms step_avg:58.13ms
step:1454/2330 train_time:84519ms step_avg:58.13ms
step:1455/2330 train_time:84576ms step_avg:58.13ms
step:1456/2330 train_time:84636ms step_avg:58.13ms
step:1457/2330 train_time:84693ms step_avg:58.13ms
step:1458/2330 train_time:84753ms step_avg:58.13ms
step:1459/2330 train_time:84810ms step_avg:58.13ms
step:1460/2330 train_time:84870ms step_avg:58.13ms
step:1461/2330 train_time:84927ms step_avg:58.13ms
step:1462/2330 train_time:84987ms step_avg:58.13ms
step:1463/2330 train_time:85044ms step_avg:58.13ms
step:1464/2330 train_time:85104ms step_avg:58.13ms
step:1465/2330 train_time:85161ms step_avg:58.13ms
step:1466/2330 train_time:85221ms step_avg:58.13ms
step:1467/2330 train_time:85278ms step_avg:58.13ms
step:1468/2330 train_time:85337ms step_avg:58.13ms
step:1469/2330 train_time:85394ms step_avg:58.13ms
step:1470/2330 train_time:85455ms step_avg:58.13ms
step:1471/2330 train_time:85512ms step_avg:58.13ms
step:1472/2330 train_time:85573ms step_avg:58.13ms
step:1473/2330 train_time:85629ms step_avg:58.13ms
step:1474/2330 train_time:85690ms step_avg:58.13ms
step:1475/2330 train_time:85747ms step_avg:58.13ms
step:1476/2330 train_time:85807ms step_avg:58.13ms
step:1477/2330 train_time:85864ms step_avg:58.13ms
step:1478/2330 train_time:85924ms step_avg:58.14ms
step:1479/2330 train_time:85982ms step_avg:58.14ms
step:1480/2330 train_time:86041ms step_avg:58.14ms
step:1481/2330 train_time:86099ms step_avg:58.14ms
step:1482/2330 train_time:86158ms step_avg:58.14ms
step:1483/2330 train_time:86215ms step_avg:58.14ms
step:1484/2330 train_time:86275ms step_avg:58.14ms
step:1485/2330 train_time:86332ms step_avg:58.14ms
step:1486/2330 train_time:86393ms step_avg:58.14ms
step:1487/2330 train_time:86449ms step_avg:58.14ms
step:1488/2330 train_time:86509ms step_avg:58.14ms
step:1489/2330 train_time:86566ms step_avg:58.14ms
step:1490/2330 train_time:86627ms step_avg:58.14ms
step:1491/2330 train_time:86684ms step_avg:58.14ms
step:1492/2330 train_time:86744ms step_avg:58.14ms
step:1493/2330 train_time:86800ms step_avg:58.14ms
step:1494/2330 train_time:86860ms step_avg:58.14ms
step:1495/2330 train_time:86917ms step_avg:58.14ms
step:1496/2330 train_time:86977ms step_avg:58.14ms
step:1497/2330 train_time:87034ms step_avg:58.14ms
step:1498/2330 train_time:87096ms step_avg:58.14ms
step:1499/2330 train_time:87152ms step_avg:58.14ms
step:1500/2330 train_time:87212ms step_avg:58.14ms
step:1500/2330 val_loss:3.9012 train_time:87292ms step_avg:58.19ms
step:1501/2330 train_time:87310ms step_avg:58.17ms
step:1502/2330 train_time:87331ms step_avg:58.14ms
step:1503/2330 train_time:87393ms step_avg:58.15ms
step:1504/2330 train_time:87457ms step_avg:58.15ms
step:1505/2330 train_time:87514ms step_avg:58.15ms
step:1506/2330 train_time:87574ms step_avg:58.15ms
step:1507/2330 train_time:87630ms step_avg:58.15ms
step:1508/2330 train_time:87690ms step_avg:58.15ms
step:1509/2330 train_time:87745ms step_avg:58.15ms
step:1510/2330 train_time:87806ms step_avg:58.15ms
step:1511/2330 train_time:87861ms step_avg:58.15ms
step:1512/2330 train_time:87921ms step_avg:58.15ms
step:1513/2330 train_time:87977ms step_avg:58.15ms
step:1514/2330 train_time:88036ms step_avg:58.15ms
step:1515/2330 train_time:88092ms step_avg:58.15ms
step:1516/2330 train_time:88152ms step_avg:58.15ms
step:1517/2330 train_time:88207ms step_avg:58.15ms
step:1518/2330 train_time:88269ms step_avg:58.15ms
step:1519/2330 train_time:88326ms step_avg:58.15ms
step:1520/2330 train_time:88389ms step_avg:58.15ms
step:1521/2330 train_time:88446ms step_avg:58.15ms
step:1522/2330 train_time:88509ms step_avg:58.15ms
step:1523/2330 train_time:88565ms step_avg:58.15ms
step:1524/2330 train_time:88626ms step_avg:58.15ms
step:1525/2330 train_time:88683ms step_avg:58.15ms
step:1526/2330 train_time:88742ms step_avg:58.15ms
step:1527/2330 train_time:88799ms step_avg:58.15ms
step:1528/2330 train_time:88859ms step_avg:58.15ms
step:1529/2330 train_time:88917ms step_avg:58.15ms
step:1530/2330 train_time:88975ms step_avg:58.15ms
step:1531/2330 train_time:89032ms step_avg:58.15ms
step:1532/2330 train_time:89092ms step_avg:58.15ms
step:1533/2330 train_time:89148ms step_avg:58.15ms
step:1534/2330 train_time:89209ms step_avg:58.15ms
step:1535/2330 train_time:89266ms step_avg:58.15ms
step:1536/2330 train_time:89327ms step_avg:58.16ms
step:1537/2330 train_time:89385ms step_avg:58.16ms
step:1538/2330 train_time:89448ms step_avg:58.16ms
step:1539/2330 train_time:89506ms step_avg:58.16ms
step:1540/2330 train_time:89566ms step_avg:58.16ms
step:1541/2330 train_time:89623ms step_avg:58.16ms
step:1542/2330 train_time:89684ms step_avg:58.16ms
step:1543/2330 train_time:89741ms step_avg:58.16ms
step:1544/2330 train_time:89802ms step_avg:58.16ms
step:1545/2330 train_time:89859ms step_avg:58.16ms
step:1546/2330 train_time:89919ms step_avg:58.16ms
step:1547/2330 train_time:89977ms step_avg:58.16ms
step:1548/2330 train_time:90036ms step_avg:58.16ms
step:1549/2330 train_time:90094ms step_avg:58.16ms
step:1550/2330 train_time:90153ms step_avg:58.16ms
step:1551/2330 train_time:90211ms step_avg:58.16ms
step:1552/2330 train_time:90272ms step_avg:58.16ms
step:1553/2330 train_time:90329ms step_avg:58.16ms
step:1554/2330 train_time:90391ms step_avg:58.17ms
step:1555/2330 train_time:90448ms step_avg:58.17ms
step:1556/2330 train_time:90512ms step_avg:58.17ms
step:1557/2330 train_time:90568ms step_avg:58.17ms
step:1558/2330 train_time:90630ms step_avg:58.17ms
step:1559/2330 train_time:90687ms step_avg:58.17ms
step:1560/2330 train_time:90748ms step_avg:58.17ms
step:1561/2330 train_time:90805ms step_avg:58.17ms
step:1562/2330 train_time:90867ms step_avg:58.17ms
step:1563/2330 train_time:90923ms step_avg:58.17ms
step:1564/2330 train_time:90985ms step_avg:58.17ms
step:1565/2330 train_time:91041ms step_avg:58.17ms
step:1566/2330 train_time:91102ms step_avg:58.17ms
step:1567/2330 train_time:91159ms step_avg:58.17ms
step:1568/2330 train_time:91220ms step_avg:58.18ms
step:1569/2330 train_time:91278ms step_avg:58.18ms
step:1570/2330 train_time:91339ms step_avg:58.18ms
step:1571/2330 train_time:91397ms step_avg:58.18ms
step:1572/2330 train_time:91458ms step_avg:58.18ms
step:1573/2330 train_time:91517ms step_avg:58.18ms
step:1574/2330 train_time:91578ms step_avg:58.18ms
step:1575/2330 train_time:91635ms step_avg:58.18ms
step:1576/2330 train_time:91697ms step_avg:58.18ms
step:1577/2330 train_time:91754ms step_avg:58.18ms
step:1578/2330 train_time:91817ms step_avg:58.19ms
step:1579/2330 train_time:91874ms step_avg:58.18ms
step:1580/2330 train_time:91935ms step_avg:58.19ms
step:1581/2330 train_time:91991ms step_avg:58.19ms
step:1582/2330 train_time:92052ms step_avg:58.19ms
step:1583/2330 train_time:92108ms step_avg:58.19ms
step:1584/2330 train_time:92170ms step_avg:58.19ms
step:1585/2330 train_time:92226ms step_avg:58.19ms
step:1586/2330 train_time:92287ms step_avg:58.19ms
step:1587/2330 train_time:92344ms step_avg:58.19ms
step:1588/2330 train_time:92405ms step_avg:58.19ms
step:1589/2330 train_time:92463ms step_avg:58.19ms
step:1590/2330 train_time:92524ms step_avg:58.19ms
step:1591/2330 train_time:92582ms step_avg:58.19ms
step:1592/2330 train_time:92644ms step_avg:58.19ms
step:1593/2330 train_time:92701ms step_avg:58.19ms
step:1594/2330 train_time:92763ms step_avg:58.20ms
step:1595/2330 train_time:92821ms step_avg:58.19ms
step:1596/2330 train_time:92881ms step_avg:58.20ms
step:1597/2330 train_time:92938ms step_avg:58.20ms
step:1598/2330 train_time:92999ms step_avg:58.20ms
step:1599/2330 train_time:93056ms step_avg:58.20ms
step:1600/2330 train_time:93117ms step_avg:58.20ms
step:1601/2330 train_time:93174ms step_avg:58.20ms
step:1602/2330 train_time:93234ms step_avg:58.20ms
step:1603/2330 train_time:93291ms step_avg:58.20ms
step:1604/2330 train_time:93353ms step_avg:58.20ms
step:1605/2330 train_time:93410ms step_avg:58.20ms
step:1606/2330 train_time:93473ms step_avg:58.20ms
step:1607/2330 train_time:93529ms step_avg:58.20ms
step:1608/2330 train_time:93593ms step_avg:58.20ms
step:1609/2330 train_time:93649ms step_avg:58.20ms
step:1610/2330 train_time:93712ms step_avg:58.21ms
step:1611/2330 train_time:93768ms step_avg:58.21ms
step:1612/2330 train_time:93830ms step_avg:58.21ms
step:1613/2330 train_time:93886ms step_avg:58.21ms
step:1614/2330 train_time:93948ms step_avg:58.21ms
step:1615/2330 train_time:94004ms step_avg:58.21ms
step:1616/2330 train_time:94066ms step_avg:58.21ms
step:1617/2330 train_time:94122ms step_avg:58.21ms
step:1618/2330 train_time:94183ms step_avg:58.21ms
step:1619/2330 train_time:94240ms step_avg:58.21ms
step:1620/2330 train_time:94301ms step_avg:58.21ms
step:1621/2330 train_time:94359ms step_avg:58.21ms
step:1622/2330 train_time:94420ms step_avg:58.21ms
step:1623/2330 train_time:94478ms step_avg:58.21ms
step:1624/2330 train_time:94540ms step_avg:58.21ms
step:1625/2330 train_time:94597ms step_avg:58.21ms
step:1626/2330 train_time:94658ms step_avg:58.22ms
step:1627/2330 train_time:94716ms step_avg:58.22ms
step:1628/2330 train_time:94777ms step_avg:58.22ms
step:1629/2330 train_time:94835ms step_avg:58.22ms
step:1630/2330 train_time:94895ms step_avg:58.22ms
step:1631/2330 train_time:94952ms step_avg:58.22ms
step:1632/2330 train_time:95014ms step_avg:58.22ms
step:1633/2330 train_time:95071ms step_avg:58.22ms
step:1634/2330 train_time:95133ms step_avg:58.22ms
step:1635/2330 train_time:95189ms step_avg:58.22ms
step:1636/2330 train_time:95251ms step_avg:58.22ms
step:1637/2330 train_time:95307ms step_avg:58.22ms
step:1638/2330 train_time:95370ms step_avg:58.22ms
step:1639/2330 train_time:95426ms step_avg:58.22ms
step:1640/2330 train_time:95489ms step_avg:58.22ms
step:1641/2330 train_time:95545ms step_avg:58.22ms
step:1642/2330 train_time:95606ms step_avg:58.23ms
step:1643/2330 train_time:95663ms step_avg:58.22ms
step:1644/2330 train_time:95724ms step_avg:58.23ms
step:1645/2330 train_time:95782ms step_avg:58.23ms
step:1646/2330 train_time:95842ms step_avg:58.23ms
step:1647/2330 train_time:95901ms step_avg:58.23ms
step:1648/2330 train_time:95961ms step_avg:58.23ms
step:1649/2330 train_time:96019ms step_avg:58.23ms
step:1650/2330 train_time:96080ms step_avg:58.23ms
step:1651/2330 train_time:96137ms step_avg:58.23ms
step:1652/2330 train_time:96198ms step_avg:58.23ms
step:1653/2330 train_time:96256ms step_avg:58.23ms
step:1654/2330 train_time:96317ms step_avg:58.23ms
step:1655/2330 train_time:96375ms step_avg:58.23ms
step:1656/2330 train_time:96436ms step_avg:58.23ms
step:1657/2330 train_time:96493ms step_avg:58.23ms
step:1658/2330 train_time:96555ms step_avg:58.24ms
step:1659/2330 train_time:96612ms step_avg:58.24ms
step:1660/2330 train_time:96673ms step_avg:58.24ms
step:1661/2330 train_time:96729ms step_avg:58.24ms
step:1662/2330 train_time:96793ms step_avg:58.24ms
step:1663/2330 train_time:96850ms step_avg:58.24ms
step:1664/2330 train_time:96912ms step_avg:58.24ms
step:1665/2330 train_time:96969ms step_avg:58.24ms
step:1666/2330 train_time:97030ms step_avg:58.24ms
step:1667/2330 train_time:97087ms step_avg:58.24ms
step:1668/2330 train_time:97148ms step_avg:58.24ms
step:1669/2330 train_time:97205ms step_avg:58.24ms
step:1670/2330 train_time:97265ms step_avg:58.24ms
step:1671/2330 train_time:97323ms step_avg:58.24ms
step:1672/2330 train_time:97384ms step_avg:58.24ms
step:1673/2330 train_time:97441ms step_avg:58.24ms
step:1674/2330 train_time:97502ms step_avg:58.24ms
step:1675/2330 train_time:97559ms step_avg:58.24ms
step:1676/2330 train_time:97619ms step_avg:58.25ms
step:1677/2330 train_time:97677ms step_avg:58.24ms
step:1678/2330 train_time:97739ms step_avg:58.25ms
step:1679/2330 train_time:97796ms step_avg:58.25ms
step:1680/2330 train_time:97858ms step_avg:58.25ms
step:1681/2330 train_time:97917ms step_avg:58.25ms
step:1682/2330 train_time:97977ms step_avg:58.25ms
step:1683/2330 train_time:98034ms step_avg:58.25ms
step:1684/2330 train_time:98095ms step_avg:58.25ms
step:1685/2330 train_time:98153ms step_avg:58.25ms
step:1686/2330 train_time:98214ms step_avg:58.25ms
step:1687/2330 train_time:98270ms step_avg:58.25ms
step:1688/2330 train_time:98332ms step_avg:58.25ms
step:1689/2330 train_time:98388ms step_avg:58.25ms
step:1690/2330 train_time:98450ms step_avg:58.25ms
step:1691/2330 train_time:98506ms step_avg:58.25ms
step:1692/2330 train_time:98567ms step_avg:58.25ms
step:1693/2330 train_time:98624ms step_avg:58.25ms
step:1694/2330 train_time:98686ms step_avg:58.26ms
step:1695/2330 train_time:98743ms step_avg:58.26ms
step:1696/2330 train_time:98806ms step_avg:58.26ms
step:1697/2330 train_time:98862ms step_avg:58.26ms
step:1698/2330 train_time:98923ms step_avg:58.26ms
step:1699/2330 train_time:98981ms step_avg:58.26ms
step:1700/2330 train_time:99041ms step_avg:58.26ms
step:1701/2330 train_time:99098ms step_avg:58.26ms
step:1702/2330 train_time:99160ms step_avg:58.26ms
step:1703/2330 train_time:99218ms step_avg:58.26ms
step:1704/2330 train_time:99279ms step_avg:58.26ms
step:1705/2330 train_time:99336ms step_avg:58.26ms
step:1706/2330 train_time:99396ms step_avg:58.26ms
step:1707/2330 train_time:99453ms step_avg:58.26ms
step:1708/2330 train_time:99515ms step_avg:58.26ms
step:1709/2330 train_time:99571ms step_avg:58.26ms
step:1710/2330 train_time:99633ms step_avg:58.27ms
step:1711/2330 train_time:99690ms step_avg:58.26ms
step:1712/2330 train_time:99752ms step_avg:58.27ms
step:1713/2330 train_time:99808ms step_avg:58.27ms
step:1714/2330 train_time:99872ms step_avg:58.27ms
step:1715/2330 train_time:99928ms step_avg:58.27ms
step:1716/2330 train_time:99990ms step_avg:58.27ms
step:1717/2330 train_time:100046ms step_avg:58.27ms
step:1718/2330 train_time:100108ms step_avg:58.27ms
step:1719/2330 train_time:100165ms step_avg:58.27ms
step:1720/2330 train_time:100226ms step_avg:58.27ms
step:1721/2330 train_time:100282ms step_avg:58.27ms
step:1722/2330 train_time:100344ms step_avg:58.27ms
step:1723/2330 train_time:100401ms step_avg:58.27ms
step:1724/2330 train_time:100462ms step_avg:58.27ms
step:1725/2330 train_time:100520ms step_avg:58.27ms
step:1726/2330 train_time:100581ms step_avg:58.27ms
step:1727/2330 train_time:100640ms step_avg:58.27ms
step:1728/2330 train_time:100701ms step_avg:58.28ms
step:1729/2330 train_time:100760ms step_avg:58.28ms
step:1730/2330 train_time:100820ms step_avg:58.28ms
step:1731/2330 train_time:100878ms step_avg:58.28ms
step:1732/2330 train_time:100938ms step_avg:58.28ms
step:1733/2330 train_time:100995ms step_avg:58.28ms
step:1734/2330 train_time:101058ms step_avg:58.28ms
step:1735/2330 train_time:101115ms step_avg:58.28ms
step:1736/2330 train_time:101176ms step_avg:58.28ms
step:1737/2330 train_time:101233ms step_avg:58.28ms
step:1738/2330 train_time:101294ms step_avg:58.28ms
step:1739/2330 train_time:101350ms step_avg:58.28ms
step:1740/2330 train_time:101412ms step_avg:58.28ms
step:1741/2330 train_time:101469ms step_avg:58.28ms
step:1742/2330 train_time:101530ms step_avg:58.28ms
step:1743/2330 train_time:101586ms step_avg:58.28ms
step:1744/2330 train_time:101648ms step_avg:58.28ms
step:1745/2330 train_time:101705ms step_avg:58.28ms
step:1746/2330 train_time:101767ms step_avg:58.29ms
step:1747/2330 train_time:101823ms step_avg:58.28ms
step:1748/2330 train_time:101885ms step_avg:58.29ms
step:1749/2330 train_time:101942ms step_avg:58.29ms
step:1750/2330 train_time:102003ms step_avg:58.29ms
step:1750/2330 val_loss:3.8162 train_time:102085ms step_avg:58.33ms
step:1751/2330 train_time:102102ms step_avg:58.31ms
step:1752/2330 train_time:102122ms step_avg:58.29ms
step:1753/2330 train_time:102178ms step_avg:58.29ms
step:1754/2330 train_time:102250ms step_avg:58.30ms
step:1755/2330 train_time:102305ms step_avg:58.29ms
step:1756/2330 train_time:102374ms step_avg:58.30ms
step:1757/2330 train_time:102430ms step_avg:58.30ms
step:1758/2330 train_time:102490ms step_avg:58.30ms
step:1759/2330 train_time:102547ms step_avg:58.30ms
step:1760/2330 train_time:102609ms step_avg:58.30ms
step:1761/2330 train_time:102665ms step_avg:58.30ms
step:1762/2330 train_time:102725ms step_avg:58.30ms
step:1763/2330 train_time:102782ms step_avg:58.30ms
step:1764/2330 train_time:102841ms step_avg:58.30ms
step:1765/2330 train_time:102898ms step_avg:58.30ms
step:1766/2330 train_time:102957ms step_avg:58.30ms
step:1767/2330 train_time:103017ms step_avg:58.30ms
step:1768/2330 train_time:103079ms step_avg:58.30ms
step:1769/2330 train_time:103136ms step_avg:58.30ms
step:1770/2330 train_time:103198ms step_avg:58.30ms
step:1771/2330 train_time:103257ms step_avg:58.30ms
step:1772/2330 train_time:103318ms step_avg:58.31ms
step:1773/2330 train_time:103375ms step_avg:58.31ms
step:1774/2330 train_time:103436ms step_avg:58.31ms
step:1775/2330 train_time:103492ms step_avg:58.31ms
step:1776/2330 train_time:103554ms step_avg:58.31ms
step:1777/2330 train_time:103611ms step_avg:58.31ms
step:1778/2330 train_time:103672ms step_avg:58.31ms
step:1779/2330 train_time:103729ms step_avg:58.31ms
step:1780/2330 train_time:103789ms step_avg:58.31ms
step:1781/2330 train_time:103846ms step_avg:58.31ms
step:1782/2330 train_time:103905ms step_avg:58.31ms
step:1783/2330 train_time:103963ms step_avg:58.31ms
step:1784/2330 train_time:104024ms step_avg:58.31ms
step:1785/2330 train_time:104083ms step_avg:58.31ms
step:1786/2330 train_time:104146ms step_avg:58.31ms
step:1787/2330 train_time:104205ms step_avg:58.31ms
step:1788/2330 train_time:104267ms step_avg:58.31ms
step:1789/2330 train_time:104326ms step_avg:58.32ms
step:1790/2330 train_time:104386ms step_avg:58.32ms
step:1791/2330 train_time:104444ms step_avg:58.32ms
step:1792/2330 train_time:104505ms step_avg:58.32ms
step:1793/2330 train_time:104563ms step_avg:58.32ms
step:1794/2330 train_time:104623ms step_avg:58.32ms
step:1795/2330 train_time:104680ms step_avg:58.32ms
step:1796/2330 train_time:104740ms step_avg:58.32ms
step:1797/2330 train_time:104797ms step_avg:58.32ms
step:1798/2330 train_time:104858ms step_avg:58.32ms
step:1799/2330 train_time:104915ms step_avg:58.32ms
step:1800/2330 train_time:104976ms step_avg:58.32ms
step:1801/2330 train_time:105034ms step_avg:58.32ms
step:1802/2330 train_time:105094ms step_avg:58.32ms
step:1803/2330 train_time:105152ms step_avg:58.32ms
step:1804/2330 train_time:105213ms step_avg:58.32ms
step:1805/2330 train_time:105271ms step_avg:58.32ms
step:1806/2330 train_time:105332ms step_avg:58.32ms
step:1807/2330 train_time:105389ms step_avg:58.32ms
step:1808/2330 train_time:105451ms step_avg:58.32ms
step:1809/2330 train_time:105508ms step_avg:58.32ms
step:1810/2330 train_time:105569ms step_avg:58.33ms
step:1811/2330 train_time:105626ms step_avg:58.32ms
step:1812/2330 train_time:105687ms step_avg:58.33ms
step:1813/2330 train_time:105744ms step_avg:58.33ms
step:1814/2330 train_time:105805ms step_avg:58.33ms
step:1815/2330 train_time:105862ms step_avg:58.33ms
step:1816/2330 train_time:105924ms step_avg:58.33ms
step:1817/2330 train_time:105982ms step_avg:58.33ms
step:1818/2330 train_time:106042ms step_avg:58.33ms
step:1819/2330 train_time:106101ms step_avg:58.33ms
step:1820/2330 train_time:106163ms step_avg:58.33ms
step:1821/2330 train_time:106222ms step_avg:58.33ms
step:1822/2330 train_time:106283ms step_avg:58.33ms
step:1823/2330 train_time:106341ms step_avg:58.33ms
step:1824/2330 train_time:106402ms step_avg:58.33ms
step:1825/2330 train_time:106460ms step_avg:58.33ms
step:1826/2330 train_time:106520ms step_avg:58.34ms
step:1827/2330 train_time:106577ms step_avg:58.33ms
step:1828/2330 train_time:106639ms step_avg:58.34ms
step:1829/2330 train_time:106695ms step_avg:58.34ms
step:1830/2330 train_time:106757ms step_avg:58.34ms
step:1831/2330 train_time:106813ms step_avg:58.34ms
step:1832/2330 train_time:106876ms step_avg:58.34ms
step:1833/2330 train_time:106932ms step_avg:58.34ms
step:1834/2330 train_time:106993ms step_avg:58.34ms
step:1835/2330 train_time:107050ms step_avg:58.34ms
step:1836/2330 train_time:107112ms step_avg:58.34ms
step:1837/2330 train_time:107169ms step_avg:58.34ms
step:1838/2330 train_time:107229ms step_avg:58.34ms
step:1839/2330 train_time:107286ms step_avg:58.34ms
step:1840/2330 train_time:107349ms step_avg:58.34ms
step:1841/2330 train_time:107407ms step_avg:58.34ms
step:1842/2330 train_time:107470ms step_avg:58.34ms
step:1843/2330 train_time:107528ms step_avg:58.34ms
step:1844/2330 train_time:107589ms step_avg:58.35ms
step:1845/2330 train_time:107648ms step_avg:58.35ms
step:1846/2330 train_time:107708ms step_avg:58.35ms
step:1847/2330 train_time:107766ms step_avg:58.35ms
step:1848/2330 train_time:107825ms step_avg:58.35ms
step:1849/2330 train_time:107883ms step_avg:58.35ms
step:1850/2330 train_time:107943ms step_avg:58.35ms
step:1851/2330 train_time:108000ms step_avg:58.35ms
step:1852/2330 train_time:108061ms step_avg:58.35ms
step:1853/2330 train_time:108119ms step_avg:58.35ms
step:1854/2330 train_time:108181ms step_avg:58.35ms
step:1855/2330 train_time:108238ms step_avg:58.35ms
step:1856/2330 train_time:108300ms step_avg:58.35ms
step:1857/2330 train_time:108357ms step_avg:58.35ms
step:1858/2330 train_time:108419ms step_avg:58.35ms
step:1859/2330 train_time:108476ms step_avg:58.35ms
step:1860/2330 train_time:108538ms step_avg:58.35ms
step:1861/2330 train_time:108595ms step_avg:58.35ms
step:1862/2330 train_time:108656ms step_avg:58.35ms
step:1863/2330 train_time:108713ms step_avg:58.35ms
step:1864/2330 train_time:108774ms step_avg:58.35ms
step:1865/2330 train_time:108830ms step_avg:58.35ms
step:1866/2330 train_time:108891ms step_avg:58.36ms
step:1867/2330 train_time:108948ms step_avg:58.35ms
step:1868/2330 train_time:109009ms step_avg:58.36ms
step:1869/2330 train_time:109067ms step_avg:58.36ms
step:1870/2330 train_time:109128ms step_avg:58.36ms
step:1871/2330 train_time:109186ms step_avg:58.36ms
step:1872/2330 train_time:109247ms step_avg:58.36ms
step:1873/2330 train_time:109305ms step_avg:58.36ms
step:1874/2330 train_time:109366ms step_avg:58.36ms
step:1875/2330 train_time:109424ms step_avg:58.36ms
step:1876/2330 train_time:109485ms step_avg:58.36ms
step:1877/2330 train_time:109543ms step_avg:58.36ms
step:1878/2330 train_time:109605ms step_avg:58.36ms
step:1879/2330 train_time:109664ms step_avg:58.36ms
step:1880/2330 train_time:109723ms step_avg:58.36ms
step:1881/2330 train_time:109780ms step_avg:58.36ms
step:1882/2330 train_time:109842ms step_avg:58.36ms
step:1883/2330 train_time:109899ms step_avg:58.36ms
step:1884/2330 train_time:109960ms step_avg:58.37ms
step:1885/2330 train_time:110017ms step_avg:58.36ms
step:1886/2330 train_time:110078ms step_avg:58.37ms
step:1887/2330 train_time:110136ms step_avg:58.37ms
step:1888/2330 train_time:110196ms step_avg:58.37ms
step:1889/2330 train_time:110253ms step_avg:58.37ms
step:1890/2330 train_time:110315ms step_avg:58.37ms
step:1891/2330 train_time:110372ms step_avg:58.37ms
step:1892/2330 train_time:110433ms step_avg:58.37ms
step:1893/2330 train_time:110490ms step_avg:58.37ms
step:1894/2330 train_time:110553ms step_avg:58.37ms
step:1895/2330 train_time:110611ms step_avg:58.37ms
step:1896/2330 train_time:110671ms step_avg:58.37ms
step:1897/2330 train_time:110728ms step_avg:58.37ms
step:1898/2330 train_time:110789ms step_avg:58.37ms
step:1899/2330 train_time:110845ms step_avg:58.37ms
step:1900/2330 train_time:110906ms step_avg:58.37ms
step:1901/2330 train_time:110964ms step_avg:58.37ms
step:1902/2330 train_time:111025ms step_avg:58.37ms
step:1903/2330 train_time:111082ms step_avg:58.37ms
step:1904/2330 train_time:111143ms step_avg:58.37ms
step:1905/2330 train_time:111201ms step_avg:58.37ms
step:1906/2330 train_time:111263ms step_avg:58.38ms
step:1907/2330 train_time:111321ms step_avg:58.38ms
step:1908/2330 train_time:111382ms step_avg:58.38ms
step:1909/2330 train_time:111440ms step_avg:58.38ms
step:1910/2330 train_time:111501ms step_avg:58.38ms
step:1911/2330 train_time:111559ms step_avg:58.38ms
step:1912/2330 train_time:111619ms step_avg:58.38ms
step:1913/2330 train_time:111676ms step_avg:58.38ms
step:1914/2330 train_time:111738ms step_avg:58.38ms
step:1915/2330 train_time:111794ms step_avg:58.38ms
step:1916/2330 train_time:111856ms step_avg:58.38ms
step:1917/2330 train_time:111913ms step_avg:58.38ms
step:1918/2330 train_time:111974ms step_avg:58.38ms
step:1919/2330 train_time:112031ms step_avg:58.38ms
step:1920/2330 train_time:112092ms step_avg:58.38ms
step:1921/2330 train_time:112149ms step_avg:58.38ms
step:1922/2330 train_time:112210ms step_avg:58.38ms
step:1923/2330 train_time:112267ms step_avg:58.38ms
step:1924/2330 train_time:112329ms step_avg:58.38ms
step:1925/2330 train_time:112387ms step_avg:58.38ms
step:1926/2330 train_time:112449ms step_avg:58.38ms
step:1927/2330 train_time:112507ms step_avg:58.38ms
step:1928/2330 train_time:112568ms step_avg:58.39ms
step:1929/2330 train_time:112626ms step_avg:58.39ms
step:1930/2330 train_time:112687ms step_avg:58.39ms
step:1931/2330 train_time:112745ms step_avg:58.39ms
step:1932/2330 train_time:112805ms step_avg:58.39ms
step:1933/2330 train_time:112862ms step_avg:58.39ms
step:1934/2330 train_time:112924ms step_avg:58.39ms
step:1935/2330 train_time:112981ms step_avg:58.39ms
step:1936/2330 train_time:113042ms step_avg:58.39ms
step:1937/2330 train_time:113101ms step_avg:58.39ms
step:1938/2330 train_time:113162ms step_avg:58.39ms
step:1939/2330 train_time:113219ms step_avg:58.39ms
step:1940/2330 train_time:113280ms step_avg:58.39ms
step:1941/2330 train_time:113338ms step_avg:58.39ms
step:1942/2330 train_time:113399ms step_avg:58.39ms
step:1943/2330 train_time:113456ms step_avg:58.39ms
step:1944/2330 train_time:113516ms step_avg:58.39ms
step:1945/2330 train_time:113573ms step_avg:58.39ms
step:1946/2330 train_time:113635ms step_avg:58.39ms
step:1947/2330 train_time:113692ms step_avg:58.39ms
step:1948/2330 train_time:113754ms step_avg:58.40ms
step:1949/2330 train_time:113812ms step_avg:58.39ms
step:1950/2330 train_time:113872ms step_avg:58.40ms
step:1951/2330 train_time:113928ms step_avg:58.39ms
step:1952/2330 train_time:113989ms step_avg:58.40ms
step:1953/2330 train_time:114046ms step_avg:58.40ms
step:1954/2330 train_time:114108ms step_avg:58.40ms
step:1955/2330 train_time:114166ms step_avg:58.40ms
step:1956/2330 train_time:114227ms step_avg:58.40ms
step:1957/2330 train_time:114285ms step_avg:58.40ms
step:1958/2330 train_time:114346ms step_avg:58.40ms
step:1959/2330 train_time:114404ms step_avg:58.40ms
step:1960/2330 train_time:114466ms step_avg:58.40ms
step:1961/2330 train_time:114524ms step_avg:58.40ms
step:1962/2330 train_time:114584ms step_avg:58.40ms
step:1963/2330 train_time:114642ms step_avg:58.40ms
step:1964/2330 train_time:114703ms step_avg:58.40ms
step:1965/2330 train_time:114761ms step_avg:58.40ms
step:1966/2330 train_time:114821ms step_avg:58.40ms
step:1967/2330 train_time:114878ms step_avg:58.40ms
step:1968/2330 train_time:114939ms step_avg:58.40ms
step:1969/2330 train_time:114996ms step_avg:58.40ms
step:1970/2330 train_time:115058ms step_avg:58.41ms
step:1971/2330 train_time:115115ms step_avg:58.40ms
step:1972/2330 train_time:115178ms step_avg:58.41ms
step:1973/2330 train_time:115234ms step_avg:58.41ms
step:1974/2330 train_time:115295ms step_avg:58.41ms
step:1975/2330 train_time:115353ms step_avg:58.41ms
step:1976/2330 train_time:115413ms step_avg:58.41ms
step:1977/2330 train_time:115470ms step_avg:58.41ms
step:1978/2330 train_time:115531ms step_avg:58.41ms
step:1979/2330 train_time:115588ms step_avg:58.41ms
step:1980/2330 train_time:115649ms step_avg:58.41ms
step:1981/2330 train_time:115706ms step_avg:58.41ms
step:1982/2330 train_time:115767ms step_avg:58.41ms
step:1983/2330 train_time:115825ms step_avg:58.41ms
step:1984/2330 train_time:115885ms step_avg:58.41ms
step:1985/2330 train_time:115943ms step_avg:58.41ms
step:1986/2330 train_time:116004ms step_avg:58.41ms
step:1987/2330 train_time:116062ms step_avg:58.41ms
step:1988/2330 train_time:116123ms step_avg:58.41ms
step:1989/2330 train_time:116180ms step_avg:58.41ms
step:1990/2330 train_time:116241ms step_avg:58.41ms
step:1991/2330 train_time:116299ms step_avg:58.41ms
step:1992/2330 train_time:116359ms step_avg:58.41ms
step:1993/2330 train_time:116417ms step_avg:58.41ms
step:1994/2330 train_time:116479ms step_avg:58.41ms
step:1995/2330 train_time:116535ms step_avg:58.41ms
step:1996/2330 train_time:116598ms step_avg:58.42ms
step:1997/2330 train_time:116655ms step_avg:58.42ms
step:1998/2330 train_time:116717ms step_avg:58.42ms
step:1999/2330 train_time:116773ms step_avg:58.42ms
step:2000/2330 train_time:116836ms step_avg:58.42ms
step:2000/2330 val_loss:3.7535 train_time:116916ms step_avg:58.46ms
step:2001/2330 train_time:116934ms step_avg:58.44ms
step:2002/2330 train_time:116956ms step_avg:58.42ms
step:2003/2330 train_time:117016ms step_avg:58.42ms
step:2004/2330 train_time:117079ms step_avg:58.42ms
step:2005/2330 train_time:117136ms step_avg:58.42ms
step:2006/2330 train_time:117197ms step_avg:58.42ms
step:2007/2330 train_time:117254ms step_avg:58.42ms
step:2008/2330 train_time:117314ms step_avg:58.42ms
step:2009/2330 train_time:117370ms step_avg:58.42ms
step:2010/2330 train_time:117431ms step_avg:58.42ms
step:2011/2330 train_time:117488ms step_avg:58.42ms
step:2012/2330 train_time:117548ms step_avg:58.42ms
step:2013/2330 train_time:117604ms step_avg:58.42ms
step:2014/2330 train_time:117665ms step_avg:58.42ms
step:2015/2330 train_time:117722ms step_avg:58.42ms
step:2016/2330 train_time:117782ms step_avg:58.42ms
step:2017/2330 train_time:117839ms step_avg:58.42ms
step:2018/2330 train_time:117901ms step_avg:58.42ms
step:2019/2330 train_time:117959ms step_avg:58.42ms
step:2020/2330 train_time:118021ms step_avg:58.43ms
step:2021/2330 train_time:118078ms step_avg:58.43ms
step:2022/2330 train_time:118141ms step_avg:58.43ms
step:2023/2330 train_time:118197ms step_avg:58.43ms
step:2024/2330 train_time:118258ms step_avg:58.43ms
step:2025/2330 train_time:118315ms step_avg:58.43ms
step:2026/2330 train_time:118376ms step_avg:58.43ms
step:2027/2330 train_time:118433ms step_avg:58.43ms
step:2028/2330 train_time:118493ms step_avg:58.43ms
step:2029/2330 train_time:118551ms step_avg:58.43ms
step:2030/2330 train_time:118611ms step_avg:58.43ms
step:2031/2330 train_time:118668ms step_avg:58.43ms
step:2032/2330 train_time:118728ms step_avg:58.43ms
step:2033/2330 train_time:118785ms step_avg:58.43ms
step:2034/2330 train_time:118846ms step_avg:58.43ms
step:2035/2330 train_time:118905ms step_avg:58.43ms
step:2036/2330 train_time:118966ms step_avg:58.43ms
step:2037/2330 train_time:119023ms step_avg:58.43ms
step:2038/2330 train_time:119087ms step_avg:58.43ms
step:2039/2330 train_time:119143ms step_avg:58.43ms
step:2040/2330 train_time:119208ms step_avg:58.44ms
step:2041/2330 train_time:119265ms step_avg:58.43ms
step:2042/2330 train_time:119328ms step_avg:58.44ms
step:2043/2330 train_time:119384ms step_avg:58.44ms
step:2044/2330 train_time:119445ms step_avg:58.44ms
step:2045/2330 train_time:119501ms step_avg:58.44ms
step:2046/2330 train_time:119562ms step_avg:58.44ms
step:2047/2330 train_time:119619ms step_avg:58.44ms
step:2048/2330 train_time:119680ms step_avg:58.44ms
step:2049/2330 train_time:119736ms step_avg:58.44ms
step:2050/2330 train_time:119797ms step_avg:58.44ms
step:2051/2330 train_time:119856ms step_avg:58.44ms
step:2052/2330 train_time:119916ms step_avg:58.44ms
step:2053/2330 train_time:119975ms step_avg:58.44ms
step:2054/2330 train_time:120036ms step_avg:58.44ms
step:2055/2330 train_time:120094ms step_avg:58.44ms
step:2056/2330 train_time:120156ms step_avg:58.44ms
step:2057/2330 train_time:120214ms step_avg:58.44ms
step:2058/2330 train_time:120276ms step_avg:58.44ms
step:2059/2330 train_time:120333ms step_avg:58.44ms
step:2060/2330 train_time:120394ms step_avg:58.44ms
step:2061/2330 train_time:120451ms step_avg:58.44ms
step:2062/2330 train_time:120512ms step_avg:58.44ms
step:2063/2330 train_time:120569ms step_avg:58.44ms
step:2064/2330 train_time:120630ms step_avg:58.44ms
step:2065/2330 train_time:120686ms step_avg:58.44ms
step:2066/2330 train_time:120747ms step_avg:58.44ms
step:2067/2330 train_time:120804ms step_avg:58.44ms
step:2068/2330 train_time:120866ms step_avg:58.45ms
step:2069/2330 train_time:120923ms step_avg:58.45ms
step:2070/2330 train_time:120986ms step_avg:58.45ms
step:2071/2330 train_time:121043ms step_avg:58.45ms
step:2072/2330 train_time:121106ms step_avg:58.45ms
step:2073/2330 train_time:121162ms step_avg:58.45ms
step:2074/2330 train_time:121224ms step_avg:58.45ms
step:2075/2330 train_time:121281ms step_avg:58.45ms
step:2076/2330 train_time:121342ms step_avg:58.45ms
step:2077/2330 train_time:121399ms step_avg:58.45ms
step:2078/2330 train_time:121459ms step_avg:58.45ms
step:2079/2330 train_time:121515ms step_avg:58.45ms
step:2080/2330 train_time:121577ms step_avg:58.45ms
step:2081/2330 train_time:121634ms step_avg:58.45ms
step:2082/2330 train_time:121695ms step_avg:58.45ms
step:2083/2330 train_time:121753ms step_avg:58.45ms
step:2084/2330 train_time:121813ms step_avg:58.45ms
step:2085/2330 train_time:121872ms step_avg:58.45ms
step:2086/2330 train_time:121934ms step_avg:58.45ms
step:2087/2330 train_time:121992ms step_avg:58.45ms
step:2088/2330 train_time:122052ms step_avg:58.45ms
step:2089/2330 train_time:122110ms step_avg:58.45ms
step:2090/2330 train_time:122171ms step_avg:58.46ms
step:2091/2330 train_time:122229ms step_avg:58.45ms
step:2092/2330 train_time:122290ms step_avg:58.46ms
step:2093/2330 train_time:122347ms step_avg:58.46ms
step:2094/2330 train_time:122410ms step_avg:58.46ms
step:2095/2330 train_time:122467ms step_avg:58.46ms
step:2096/2330 train_time:122527ms step_avg:58.46ms
step:2097/2330 train_time:122584ms step_avg:58.46ms
step:2098/2330 train_time:122645ms step_avg:58.46ms
step:2099/2330 train_time:122702ms step_avg:58.46ms
step:2100/2330 train_time:122763ms step_avg:58.46ms
step:2101/2330 train_time:122820ms step_avg:58.46ms
step:2102/2330 train_time:122881ms step_avg:58.46ms
step:2103/2330 train_time:122939ms step_avg:58.46ms
step:2104/2330 train_time:123000ms step_avg:58.46ms
step:2105/2330 train_time:123057ms step_avg:58.46ms
step:2106/2330 train_time:123118ms step_avg:58.46ms
step:2107/2330 train_time:123176ms step_avg:58.46ms
step:2108/2330 train_time:123237ms step_avg:58.46ms
step:2109/2330 train_time:123295ms step_avg:58.46ms
step:2110/2330 train_time:123355ms step_avg:58.46ms
step:2111/2330 train_time:123413ms step_avg:58.46ms
step:2112/2330 train_time:123473ms step_avg:58.46ms
step:2113/2330 train_time:123531ms step_avg:58.46ms
step:2114/2330 train_time:123592ms step_avg:58.46ms
step:2115/2330 train_time:123649ms step_avg:58.46ms
step:2116/2330 train_time:123709ms step_avg:58.46ms
step:2117/2330 train_time:123766ms step_avg:58.46ms
step:2118/2330 train_time:123827ms step_avg:58.46ms
step:2119/2330 train_time:123884ms step_avg:58.46ms
step:2120/2330 train_time:123946ms step_avg:58.46ms
step:2121/2330 train_time:124002ms step_avg:58.46ms
step:2122/2330 train_time:124065ms step_avg:58.47ms
step:2123/2330 train_time:124121ms step_avg:58.47ms
step:2124/2330 train_time:124183ms step_avg:58.47ms
step:2125/2330 train_time:124240ms step_avg:58.47ms
step:2126/2330 train_time:124301ms step_avg:58.47ms
step:2127/2330 train_time:124357ms step_avg:58.47ms
step:2128/2330 train_time:124419ms step_avg:58.47ms
step:2129/2330 train_time:124475ms step_avg:58.47ms
step:2130/2330 train_time:124537ms step_avg:58.47ms
step:2131/2330 train_time:124595ms step_avg:58.47ms
step:2132/2330 train_time:124656ms step_avg:58.47ms
step:2133/2330 train_time:124714ms step_avg:58.47ms
step:2134/2330 train_time:124774ms step_avg:58.47ms
step:2135/2330 train_time:124831ms step_avg:58.47ms
step:2136/2330 train_time:124893ms step_avg:58.47ms
step:2137/2330 train_time:124951ms step_avg:58.47ms
step:2138/2330 train_time:125012ms step_avg:58.47ms
step:2139/2330 train_time:125070ms step_avg:58.47ms
step:2140/2330 train_time:125130ms step_avg:58.47ms
step:2141/2330 train_time:125189ms step_avg:58.47ms
step:2142/2330 train_time:125249ms step_avg:58.47ms
step:2143/2330 train_time:125306ms step_avg:58.47ms
step:2144/2330 train_time:125367ms step_avg:58.47ms
step:2145/2330 train_time:125424ms step_avg:58.47ms
step:2146/2330 train_time:125486ms step_avg:58.47ms
step:2147/2330 train_time:125542ms step_avg:58.47ms
step:2148/2330 train_time:125606ms step_avg:58.48ms
step:2149/2330 train_time:125662ms step_avg:58.47ms
step:2150/2330 train_time:125724ms step_avg:58.48ms
step:2151/2330 train_time:125781ms step_avg:58.48ms
step:2152/2330 train_time:125843ms step_avg:58.48ms
step:2153/2330 train_time:125900ms step_avg:58.48ms
step:2154/2330 train_time:125960ms step_avg:58.48ms
step:2155/2330 train_time:126017ms step_avg:58.48ms
step:2156/2330 train_time:126078ms step_avg:58.48ms
step:2157/2330 train_time:126136ms step_avg:58.48ms
step:2158/2330 train_time:126196ms step_avg:58.48ms
step:2159/2330 train_time:126254ms step_avg:58.48ms
step:2160/2330 train_time:126315ms step_avg:58.48ms
step:2161/2330 train_time:126373ms step_avg:58.48ms
step:2162/2330 train_time:126434ms step_avg:58.48ms
step:2163/2330 train_time:126492ms step_avg:58.48ms
step:2164/2330 train_time:126552ms step_avg:58.48ms
step:2165/2330 train_time:126610ms step_avg:58.48ms
step:2166/2330 train_time:126670ms step_avg:58.48ms
step:2167/2330 train_time:126728ms step_avg:58.48ms
step:2168/2330 train_time:126788ms step_avg:58.48ms
step:2169/2330 train_time:126845ms step_avg:58.48ms
step:2170/2330 train_time:126907ms step_avg:58.48ms
step:2171/2330 train_time:126963ms step_avg:58.48ms
step:2172/2330 train_time:127026ms step_avg:58.48ms
step:2173/2330 train_time:127083ms step_avg:58.48ms
step:2174/2330 train_time:127145ms step_avg:58.48ms
step:2175/2330 train_time:127202ms step_avg:58.48ms
step:2176/2330 train_time:127263ms step_avg:58.48ms
step:2177/2330 train_time:127319ms step_avg:58.48ms
step:2178/2330 train_time:127381ms step_avg:58.49ms
step:2179/2330 train_time:127437ms step_avg:58.48ms
step:2180/2330 train_time:127500ms step_avg:58.49ms
step:2181/2330 train_time:127557ms step_avg:58.49ms
step:2182/2330 train_time:127617ms step_avg:58.49ms
step:2183/2330 train_time:127675ms step_avg:58.49ms
step:2184/2330 train_time:127736ms step_avg:58.49ms
step:2185/2330 train_time:127794ms step_avg:58.49ms
step:2186/2330 train_time:127855ms step_avg:58.49ms
step:2187/2330 train_time:127913ms step_avg:58.49ms
step:2188/2330 train_time:127974ms step_avg:58.49ms
step:2189/2330 train_time:128032ms step_avg:58.49ms
step:2190/2330 train_time:128093ms step_avg:58.49ms
step:2191/2330 train_time:128151ms step_avg:58.49ms
step:2192/2330 train_time:128212ms step_avg:58.49ms
step:2193/2330 train_time:128269ms step_avg:58.49ms
step:2194/2330 train_time:128330ms step_avg:58.49ms
step:2195/2330 train_time:128387ms step_avg:58.49ms
step:2196/2330 train_time:128448ms step_avg:58.49ms
step:2197/2330 train_time:128504ms step_avg:58.49ms
step:2198/2330 train_time:128567ms step_avg:58.49ms
step:2199/2330 train_time:128624ms step_avg:58.49ms
step:2200/2330 train_time:128685ms step_avg:58.49ms
step:2201/2330 train_time:128741ms step_avg:58.49ms
step:2202/2330 train_time:128804ms step_avg:58.49ms
step:2203/2330 train_time:128860ms step_avg:58.49ms
step:2204/2330 train_time:128923ms step_avg:58.50ms
step:2205/2330 train_time:128980ms step_avg:58.49ms
step:2206/2330 train_time:129041ms step_avg:58.50ms
step:2207/2330 train_time:129098ms step_avg:58.49ms
step:2208/2330 train_time:129160ms step_avg:58.50ms
step:2209/2330 train_time:129217ms step_avg:58.50ms
step:2210/2330 train_time:129278ms step_avg:58.50ms
step:2211/2330 train_time:129335ms step_avg:58.50ms
step:2212/2330 train_time:129396ms step_avg:58.50ms
step:2213/2330 train_time:129454ms step_avg:58.50ms
step:2214/2330 train_time:129515ms step_avg:58.50ms
step:2215/2330 train_time:129572ms step_avg:58.50ms
step:2216/2330 train_time:129634ms step_avg:58.50ms
step:2217/2330 train_time:129691ms step_avg:58.50ms
step:2218/2330 train_time:129752ms step_avg:58.50ms
step:2219/2330 train_time:129811ms step_avg:58.50ms
step:2220/2330 train_time:129871ms step_avg:58.50ms
step:2221/2330 train_time:129928ms step_avg:58.50ms
step:2222/2330 train_time:129989ms step_avg:58.50ms
step:2223/2330 train_time:130046ms step_avg:58.50ms
step:2224/2330 train_time:130109ms step_avg:58.50ms
step:2225/2330 train_time:130166ms step_avg:58.50ms
step:2226/2330 train_time:130227ms step_avg:58.50ms
step:2227/2330 train_time:130284ms step_avg:58.50ms
step:2228/2330 train_time:130346ms step_avg:58.50ms
step:2229/2330 train_time:130402ms step_avg:58.50ms
step:2230/2330 train_time:130464ms step_avg:58.50ms
step:2231/2330 train_time:130520ms step_avg:58.50ms
step:2232/2330 train_time:130581ms step_avg:58.50ms
step:2233/2330 train_time:130638ms step_avg:58.50ms
step:2234/2330 train_time:130699ms step_avg:58.50ms
step:2235/2330 train_time:130756ms step_avg:58.50ms
step:2236/2330 train_time:130817ms step_avg:58.50ms
step:2237/2330 train_time:130875ms step_avg:58.50ms
step:2238/2330 train_time:130935ms step_avg:58.51ms
step:2239/2330 train_time:130994ms step_avg:58.51ms
step:2240/2330 train_time:131054ms step_avg:58.51ms
step:2241/2330 train_time:131111ms step_avg:58.51ms
step:2242/2330 train_time:131173ms step_avg:58.51ms
step:2243/2330 train_time:131232ms step_avg:58.51ms
step:2244/2330 train_time:131292ms step_avg:58.51ms
step:2245/2330 train_time:131351ms step_avg:58.51ms
step:2246/2330 train_time:131411ms step_avg:58.51ms
step:2247/2330 train_time:131468ms step_avg:58.51ms
step:2248/2330 train_time:131529ms step_avg:58.51ms
step:2249/2330 train_time:131586ms step_avg:58.51ms
step:2250/2330 train_time:131647ms step_avg:58.51ms
step:2250/2330 val_loss:3.7055 train_time:131729ms step_avg:58.55ms
step:2251/2330 train_time:131748ms step_avg:58.53ms
step:2252/2330 train_time:131769ms step_avg:58.51ms
step:2253/2330 train_time:131830ms step_avg:58.51ms
step:2254/2330 train_time:131893ms step_avg:58.52ms
step:2255/2330 train_time:131950ms step_avg:58.51ms
step:2256/2330 train_time:132014ms step_avg:58.52ms
step:2257/2330 train_time:132071ms step_avg:58.52ms
step:2258/2330 train_time:132132ms step_avg:58.52ms
step:2259/2330 train_time:132188ms step_avg:58.52ms
step:2260/2330 train_time:132249ms step_avg:58.52ms
step:2261/2330 train_time:132306ms step_avg:58.52ms
step:2262/2330 train_time:132365ms step_avg:58.52ms
step:2263/2330 train_time:132421ms step_avg:58.52ms
step:2264/2330 train_time:132482ms step_avg:58.52ms
step:2265/2330 train_time:132538ms step_avg:58.52ms
step:2266/2330 train_time:132598ms step_avg:58.52ms
step:2267/2330 train_time:132656ms step_avg:58.52ms
step:2268/2330 train_time:132719ms step_avg:58.52ms
step:2269/2330 train_time:132779ms step_avg:58.52ms
step:2270/2330 train_time:132840ms step_avg:58.52ms
step:2271/2330 train_time:132899ms step_avg:58.52ms
step:2272/2330 train_time:132960ms step_avg:58.52ms
step:2273/2330 train_time:133017ms step_avg:58.52ms
step:2274/2330 train_time:133078ms step_avg:58.52ms
step:2275/2330 train_time:133135ms step_avg:58.52ms
step:2276/2330 train_time:133196ms step_avg:58.52ms
step:2277/2330 train_time:133253ms step_avg:58.52ms
step:2278/2330 train_time:133314ms step_avg:58.52ms
step:2279/2330 train_time:133372ms step_avg:58.52ms
step:2280/2330 train_time:133431ms step_avg:58.52ms
step:2281/2330 train_time:133488ms step_avg:58.52ms
step:2282/2330 train_time:133549ms step_avg:58.52ms
step:2283/2330 train_time:133605ms step_avg:58.52ms
step:2284/2330 train_time:133667ms step_avg:58.52ms
step:2285/2330 train_time:133724ms step_avg:58.52ms
step:2286/2330 train_time:133787ms step_avg:58.52ms
step:2287/2330 train_time:133844ms step_avg:58.52ms
step:2288/2330 train_time:133906ms step_avg:58.53ms
step:2289/2330 train_time:133963ms step_avg:58.52ms
step:2290/2330 train_time:134025ms step_avg:58.53ms
step:2291/2330 train_time:134083ms step_avg:58.53ms
step:2292/2330 train_time:134143ms step_avg:58.53ms
step:2293/2330 train_time:134199ms step_avg:58.53ms
step:2294/2330 train_time:134261ms step_avg:58.53ms
step:2295/2330 train_time:134318ms step_avg:58.53ms
step:2296/2330 train_time:134379ms step_avg:58.53ms
step:2297/2330 train_time:134436ms step_avg:58.53ms
step:2298/2330 train_time:134497ms step_avg:58.53ms
step:2299/2330 train_time:134555ms step_avg:58.53ms
step:2300/2330 train_time:134615ms step_avg:58.53ms
step:2301/2330 train_time:134673ms step_avg:58.53ms
step:2302/2330 train_time:134733ms step_avg:58.53ms
step:2303/2330 train_time:134792ms step_avg:58.53ms
step:2304/2330 train_time:134853ms step_avg:58.53ms
step:2305/2330 train_time:134910ms step_avg:58.53ms
step:2306/2330 train_time:134973ms step_avg:58.53ms
step:2307/2330 train_time:135030ms step_avg:58.53ms
step:2308/2330 train_time:135093ms step_avg:58.53ms
step:2309/2330 train_time:135149ms step_avg:58.53ms
step:2310/2330 train_time:135211ms step_avg:58.53ms
step:2311/2330 train_time:135268ms step_avg:58.53ms
step:2312/2330 train_time:135330ms step_avg:58.53ms
step:2313/2330 train_time:135387ms step_avg:58.53ms
step:2314/2330 train_time:135448ms step_avg:58.53ms
step:2315/2330 train_time:135505ms step_avg:58.53ms
step:2316/2330 train_time:135565ms step_avg:58.53ms
step:2317/2330 train_time:135622ms step_avg:58.53ms
step:2318/2330 train_time:135683ms step_avg:58.53ms
step:2319/2330 train_time:135740ms step_avg:58.53ms
step:2320/2330 train_time:135801ms step_avg:58.54ms
step:2321/2330 train_time:135859ms step_avg:58.53ms
step:2322/2330 train_time:135922ms step_avg:58.54ms
step:2323/2330 train_time:135979ms step_avg:58.54ms
step:2324/2330 train_time:136039ms step_avg:58.54ms
step:2325/2330 train_time:136098ms step_avg:58.54ms
step:2326/2330 train_time:136158ms step_avg:58.54ms
step:2327/2330 train_time:136216ms step_avg:58.54ms
step:2328/2330 train_time:136276ms step_avg:58.54ms
step:2329/2330 train_time:136334ms step_avg:58.54ms
step:2330/2330 train_time:136395ms step_avg:58.54ms
step:2330/2330 val_loss:3.6900 train_time:136476ms step_avg:58.57ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
