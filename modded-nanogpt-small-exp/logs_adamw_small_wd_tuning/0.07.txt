import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:48:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:85.14ms
step:2/2330 train_time:189ms step_avg:94.74ms
step:3/2330 train_time:207ms step_avg:68.98ms
step:4/2330 train_time:226ms step_avg:56.57ms
step:5/2330 train_time:280ms step_avg:56.02ms
step:6/2330 train_time:339ms step_avg:56.46ms
step:7/2330 train_time:394ms step_avg:56.28ms
step:8/2330 train_time:452ms step_avg:56.52ms
step:9/2330 train_time:508ms step_avg:56.49ms
step:10/2330 train_time:567ms step_avg:56.67ms
step:11/2330 train_time:622ms step_avg:56.52ms
step:12/2330 train_time:680ms step_avg:56.69ms
step:13/2330 train_time:736ms step_avg:56.62ms
step:14/2330 train_time:794ms step_avg:56.71ms
step:15/2330 train_time:849ms step_avg:56.60ms
step:16/2330 train_time:908ms step_avg:56.74ms
step:17/2330 train_time:963ms step_avg:56.65ms
step:18/2330 train_time:1021ms step_avg:56.73ms
step:19/2330 train_time:1078ms step_avg:56.76ms
step:20/2330 train_time:1143ms step_avg:57.16ms
step:21/2330 train_time:1202ms step_avg:57.22ms
step:22/2330 train_time:1261ms step_avg:57.33ms
step:23/2330 train_time:1318ms step_avg:57.32ms
step:24/2330 train_time:1377ms step_avg:57.38ms
step:25/2330 train_time:1433ms step_avg:57.31ms
step:26/2330 train_time:1492ms step_avg:57.37ms
step:27/2330 train_time:1547ms step_avg:57.29ms
step:28/2330 train_time:1606ms step_avg:57.36ms
step:29/2330 train_time:1661ms step_avg:57.29ms
step:30/2330 train_time:1719ms step_avg:57.31ms
step:31/2330 train_time:1775ms step_avg:57.26ms
step:32/2330 train_time:1833ms step_avg:57.29ms
step:33/2330 train_time:1889ms step_avg:57.23ms
step:34/2330 train_time:1947ms step_avg:57.28ms
step:35/2330 train_time:2003ms step_avg:57.23ms
step:36/2330 train_time:2063ms step_avg:57.31ms
step:37/2330 train_time:2120ms step_avg:57.29ms
step:38/2330 train_time:2180ms step_avg:57.38ms
step:39/2330 train_time:2237ms step_avg:57.36ms
step:40/2330 train_time:2297ms step_avg:57.42ms
step:41/2330 train_time:2353ms step_avg:57.40ms
step:42/2330 train_time:2413ms step_avg:57.46ms
step:43/2330 train_time:2469ms step_avg:57.41ms
step:44/2330 train_time:2528ms step_avg:57.45ms
step:45/2330 train_time:2583ms step_avg:57.40ms
step:46/2330 train_time:2642ms step_avg:57.43ms
step:47/2330 train_time:2698ms step_avg:57.40ms
step:48/2330 train_time:2757ms step_avg:57.43ms
step:49/2330 train_time:2813ms step_avg:57.40ms
step:50/2330 train_time:2871ms step_avg:57.42ms
step:51/2330 train_time:2927ms step_avg:57.38ms
step:52/2330 train_time:2986ms step_avg:57.42ms
step:53/2330 train_time:3042ms step_avg:57.40ms
step:54/2330 train_time:3102ms step_avg:57.45ms
step:55/2330 train_time:3159ms step_avg:57.44ms
step:56/2330 train_time:3219ms step_avg:57.48ms
step:57/2330 train_time:3275ms step_avg:57.46ms
step:58/2330 train_time:3335ms step_avg:57.49ms
step:59/2330 train_time:3392ms step_avg:57.48ms
step:60/2330 train_time:3450ms step_avg:57.50ms
step:61/2330 train_time:3505ms step_avg:57.47ms
step:62/2330 train_time:3565ms step_avg:57.50ms
step:63/2330 train_time:3621ms step_avg:57.47ms
step:64/2330 train_time:3679ms step_avg:57.48ms
step:65/2330 train_time:3735ms step_avg:57.47ms
step:66/2330 train_time:3794ms step_avg:57.49ms
step:67/2330 train_time:3850ms step_avg:57.47ms
step:68/2330 train_time:3908ms step_avg:57.48ms
step:69/2330 train_time:3964ms step_avg:57.45ms
step:70/2330 train_time:4023ms step_avg:57.47ms
step:71/2330 train_time:4080ms step_avg:57.46ms
step:72/2330 train_time:4138ms step_avg:57.48ms
step:73/2330 train_time:4194ms step_avg:57.46ms
step:74/2330 train_time:4254ms step_avg:57.48ms
step:75/2330 train_time:4310ms step_avg:57.46ms
step:76/2330 train_time:4369ms step_avg:57.49ms
step:77/2330 train_time:4425ms step_avg:57.47ms
step:78/2330 train_time:4484ms step_avg:57.48ms
step:79/2330 train_time:4540ms step_avg:57.47ms
step:80/2330 train_time:4599ms step_avg:57.49ms
step:81/2330 train_time:4655ms step_avg:57.47ms
step:82/2330 train_time:4714ms step_avg:57.48ms
step:83/2330 train_time:4770ms step_avg:57.47ms
step:84/2330 train_time:4828ms step_avg:57.47ms
step:85/2330 train_time:4883ms step_avg:57.45ms
step:86/2330 train_time:4943ms step_avg:57.48ms
step:87/2330 train_time:5001ms step_avg:57.48ms
step:88/2330 train_time:5059ms step_avg:57.49ms
step:89/2330 train_time:5115ms step_avg:57.48ms
step:90/2330 train_time:5174ms step_avg:57.49ms
step:91/2330 train_time:5230ms step_avg:57.48ms
step:92/2330 train_time:5289ms step_avg:57.49ms
step:93/2330 train_time:5345ms step_avg:57.47ms
step:94/2330 train_time:5405ms step_avg:57.50ms
step:95/2330 train_time:5461ms step_avg:57.48ms
step:96/2330 train_time:5520ms step_avg:57.50ms
step:97/2330 train_time:5576ms step_avg:57.48ms
step:98/2330 train_time:5635ms step_avg:57.50ms
step:99/2330 train_time:5690ms step_avg:57.48ms
step:100/2330 train_time:5749ms step_avg:57.49ms
step:101/2330 train_time:5804ms step_avg:57.47ms
step:102/2330 train_time:5863ms step_avg:57.48ms
step:103/2330 train_time:5920ms step_avg:57.47ms
step:104/2330 train_time:5978ms step_avg:57.48ms
step:105/2330 train_time:6034ms step_avg:57.47ms
step:106/2330 train_time:6093ms step_avg:57.48ms
step:107/2330 train_time:6149ms step_avg:57.47ms
step:108/2330 train_time:6208ms step_avg:57.48ms
step:109/2330 train_time:6264ms step_avg:57.47ms
step:110/2330 train_time:6324ms step_avg:57.49ms
step:111/2330 train_time:6380ms step_avg:57.47ms
step:112/2330 train_time:6439ms step_avg:57.49ms
step:113/2330 train_time:6495ms step_avg:57.48ms
step:114/2330 train_time:6554ms step_avg:57.49ms
step:115/2330 train_time:6609ms step_avg:57.47ms
step:116/2330 train_time:6669ms step_avg:57.49ms
step:117/2330 train_time:6724ms step_avg:57.47ms
step:118/2330 train_time:6783ms step_avg:57.48ms
step:119/2330 train_time:6839ms step_avg:57.47ms
step:120/2330 train_time:6899ms step_avg:57.49ms
step:121/2330 train_time:6955ms step_avg:57.48ms
step:122/2330 train_time:7014ms step_avg:57.49ms
step:123/2330 train_time:7070ms step_avg:57.48ms
step:124/2330 train_time:7128ms step_avg:57.49ms
step:125/2330 train_time:7184ms step_avg:57.47ms
step:126/2330 train_time:7244ms step_avg:57.49ms
step:127/2330 train_time:7300ms step_avg:57.48ms
step:128/2330 train_time:7358ms step_avg:57.49ms
step:129/2330 train_time:7415ms step_avg:57.48ms
step:130/2330 train_time:7473ms step_avg:57.49ms
step:131/2330 train_time:7529ms step_avg:57.47ms
step:132/2330 train_time:7588ms step_avg:57.48ms
step:133/2330 train_time:7643ms step_avg:57.47ms
step:134/2330 train_time:7703ms step_avg:57.49ms
step:135/2330 train_time:7759ms step_avg:57.48ms
step:136/2330 train_time:7818ms step_avg:57.48ms
step:137/2330 train_time:7873ms step_avg:57.47ms
step:138/2330 train_time:7932ms step_avg:57.48ms
step:139/2330 train_time:7988ms step_avg:57.47ms
step:140/2330 train_time:8047ms step_avg:57.48ms
step:141/2330 train_time:8103ms step_avg:57.47ms
step:142/2330 train_time:8163ms step_avg:57.48ms
step:143/2330 train_time:8218ms step_avg:57.47ms
step:144/2330 train_time:8277ms step_avg:57.48ms
step:145/2330 train_time:8333ms step_avg:57.47ms
step:146/2330 train_time:8392ms step_avg:57.48ms
step:147/2330 train_time:8449ms step_avg:57.47ms
step:148/2330 train_time:8507ms step_avg:57.48ms
step:149/2330 train_time:8563ms step_avg:57.47ms
step:150/2330 train_time:8622ms step_avg:57.48ms
step:151/2330 train_time:8678ms step_avg:57.47ms
step:152/2330 train_time:8737ms step_avg:57.48ms
step:153/2330 train_time:8793ms step_avg:57.47ms
step:154/2330 train_time:8852ms step_avg:57.48ms
step:155/2330 train_time:8908ms step_avg:57.47ms
step:156/2330 train_time:8966ms step_avg:57.48ms
step:157/2330 train_time:9021ms step_avg:57.46ms
step:158/2330 train_time:9082ms step_avg:57.48ms
step:159/2330 train_time:9138ms step_avg:57.47ms
step:160/2330 train_time:9196ms step_avg:57.48ms
step:161/2330 train_time:9252ms step_avg:57.47ms
step:162/2330 train_time:9312ms step_avg:57.48ms
step:163/2330 train_time:9368ms step_avg:57.47ms
step:164/2330 train_time:9427ms step_avg:57.48ms
step:165/2330 train_time:9483ms step_avg:57.47ms
step:166/2330 train_time:9542ms step_avg:57.48ms
step:167/2330 train_time:9599ms step_avg:57.48ms
step:168/2330 train_time:9658ms step_avg:57.49ms
step:169/2330 train_time:9713ms step_avg:57.48ms
step:170/2330 train_time:9773ms step_avg:57.49ms
step:171/2330 train_time:9829ms step_avg:57.48ms
step:172/2330 train_time:9887ms step_avg:57.48ms
step:173/2330 train_time:9942ms step_avg:57.47ms
step:174/2330 train_time:10002ms step_avg:57.48ms
step:175/2330 train_time:10059ms step_avg:57.48ms
step:176/2330 train_time:10117ms step_avg:57.48ms
step:177/2330 train_time:10173ms step_avg:57.47ms
step:178/2330 train_time:10231ms step_avg:57.48ms
step:179/2330 train_time:10287ms step_avg:57.47ms
step:180/2330 train_time:10346ms step_avg:57.48ms
step:181/2330 train_time:10401ms step_avg:57.47ms
step:182/2330 train_time:10461ms step_avg:57.48ms
step:183/2330 train_time:10517ms step_avg:57.47ms
step:184/2330 train_time:10576ms step_avg:57.48ms
step:185/2330 train_time:10632ms step_avg:57.47ms
step:186/2330 train_time:10690ms step_avg:57.47ms
step:187/2330 train_time:10746ms step_avg:57.47ms
step:188/2330 train_time:10805ms step_avg:57.47ms
step:189/2330 train_time:10861ms step_avg:57.47ms
step:190/2330 train_time:10919ms step_avg:57.47ms
step:191/2330 train_time:10976ms step_avg:57.47ms
step:192/2330 train_time:11035ms step_avg:57.47ms
step:193/2330 train_time:11090ms step_avg:57.46ms
step:194/2330 train_time:11149ms step_avg:57.47ms
step:195/2330 train_time:11205ms step_avg:57.46ms
step:196/2330 train_time:11264ms step_avg:57.47ms
step:197/2330 train_time:11320ms step_avg:57.46ms
step:198/2330 train_time:11379ms step_avg:57.47ms
step:199/2330 train_time:11435ms step_avg:57.46ms
step:200/2330 train_time:11493ms step_avg:57.47ms
step:201/2330 train_time:11550ms step_avg:57.46ms
step:202/2330 train_time:11608ms step_avg:57.47ms
step:203/2330 train_time:11664ms step_avg:57.46ms
step:204/2330 train_time:11723ms step_avg:57.47ms
step:205/2330 train_time:11780ms step_avg:57.46ms
step:206/2330 train_time:11839ms step_avg:57.47ms
step:207/2330 train_time:11894ms step_avg:57.46ms
step:208/2330 train_time:11953ms step_avg:57.47ms
step:209/2330 train_time:12008ms step_avg:57.46ms
step:210/2330 train_time:12068ms step_avg:57.47ms
step:211/2330 train_time:12123ms step_avg:57.46ms
step:212/2330 train_time:12183ms step_avg:57.47ms
step:213/2330 train_time:12239ms step_avg:57.46ms
step:214/2330 train_time:12298ms step_avg:57.47ms
step:215/2330 train_time:12354ms step_avg:57.46ms
step:216/2330 train_time:12412ms step_avg:57.46ms
step:217/2330 train_time:12468ms step_avg:57.46ms
step:218/2330 train_time:12527ms step_avg:57.46ms
step:219/2330 train_time:12583ms step_avg:57.45ms
step:220/2330 train_time:12641ms step_avg:57.46ms
step:221/2330 train_time:12697ms step_avg:57.45ms
step:222/2330 train_time:12757ms step_avg:57.46ms
step:223/2330 train_time:12812ms step_avg:57.45ms
step:224/2330 train_time:12871ms step_avg:57.46ms
step:225/2330 train_time:12926ms step_avg:57.45ms
step:226/2330 train_time:12985ms step_avg:57.46ms
step:227/2330 train_time:13041ms step_avg:57.45ms
step:228/2330 train_time:13101ms step_avg:57.46ms
step:229/2330 train_time:13157ms step_avg:57.45ms
step:230/2330 train_time:13216ms step_avg:57.46ms
step:231/2330 train_time:13271ms step_avg:57.45ms
step:232/2330 train_time:13330ms step_avg:57.46ms
step:233/2330 train_time:13386ms step_avg:57.45ms
step:234/2330 train_time:13446ms step_avg:57.46ms
step:235/2330 train_time:13502ms step_avg:57.46ms
step:236/2330 train_time:13561ms step_avg:57.46ms
step:237/2330 train_time:13617ms step_avg:57.46ms
step:238/2330 train_time:13677ms step_avg:57.47ms
step:239/2330 train_time:13733ms step_avg:57.46ms
step:240/2330 train_time:13791ms step_avg:57.46ms
step:241/2330 train_time:13847ms step_avg:57.46ms
step:242/2330 train_time:13906ms step_avg:57.46ms
step:243/2330 train_time:13962ms step_avg:57.46ms
step:244/2330 train_time:14021ms step_avg:57.46ms
step:245/2330 train_time:14077ms step_avg:57.46ms
step:246/2330 train_time:14135ms step_avg:57.46ms
step:247/2330 train_time:14191ms step_avg:57.45ms
step:248/2330 train_time:14251ms step_avg:57.46ms
step:249/2330 train_time:14308ms step_avg:57.46ms
step:250/2330 train_time:14366ms step_avg:57.46ms
step:250/2330 val_loss:4.8958 train_time:14445ms step_avg:57.78ms
step:251/2330 train_time:14463ms step_avg:57.62ms
step:252/2330 train_time:14482ms step_avg:57.47ms
step:253/2330 train_time:14537ms step_avg:57.46ms
step:254/2330 train_time:14601ms step_avg:57.49ms
step:255/2330 train_time:14657ms step_avg:57.48ms
step:256/2330 train_time:14722ms step_avg:57.51ms
step:257/2330 train_time:14778ms step_avg:57.50ms
step:258/2330 train_time:14838ms step_avg:57.51ms
step:259/2330 train_time:14894ms step_avg:57.50ms
step:260/2330 train_time:14954ms step_avg:57.51ms
step:261/2330 train_time:15009ms step_avg:57.51ms
step:262/2330 train_time:15067ms step_avg:57.51ms
step:263/2330 train_time:15123ms step_avg:57.50ms
step:264/2330 train_time:15181ms step_avg:57.50ms
step:265/2330 train_time:15236ms step_avg:57.50ms
step:266/2330 train_time:15295ms step_avg:57.50ms
step:267/2330 train_time:15353ms step_avg:57.50ms
step:268/2330 train_time:15413ms step_avg:57.51ms
step:269/2330 train_time:15470ms step_avg:57.51ms
step:270/2330 train_time:15529ms step_avg:57.51ms
step:271/2330 train_time:15585ms step_avg:57.51ms
step:272/2330 train_time:15645ms step_avg:57.52ms
step:273/2330 train_time:15701ms step_avg:57.51ms
step:274/2330 train_time:15762ms step_avg:57.52ms
step:275/2330 train_time:15817ms step_avg:57.52ms
step:276/2330 train_time:15877ms step_avg:57.53ms
step:277/2330 train_time:15933ms step_avg:57.52ms
step:278/2330 train_time:15992ms step_avg:57.52ms
step:279/2330 train_time:16047ms step_avg:57.52ms
step:280/2330 train_time:16106ms step_avg:57.52ms
step:281/2330 train_time:16161ms step_avg:57.51ms
step:282/2330 train_time:16220ms step_avg:57.52ms
step:283/2330 train_time:16276ms step_avg:57.51ms
step:284/2330 train_time:16335ms step_avg:57.52ms
step:285/2330 train_time:16393ms step_avg:57.52ms
step:286/2330 train_time:16453ms step_avg:57.53ms
step:287/2330 train_time:16509ms step_avg:57.52ms
step:288/2330 train_time:16569ms step_avg:57.53ms
step:289/2330 train_time:16626ms step_avg:57.53ms
step:290/2330 train_time:16685ms step_avg:57.54ms
step:291/2330 train_time:16741ms step_avg:57.53ms
step:292/2330 train_time:16802ms step_avg:57.54ms
step:293/2330 train_time:16857ms step_avg:57.53ms
step:294/2330 train_time:16918ms step_avg:57.54ms
step:295/2330 train_time:16973ms step_avg:57.54ms
step:296/2330 train_time:17032ms step_avg:57.54ms
step:297/2330 train_time:17088ms step_avg:57.53ms
step:298/2330 train_time:17147ms step_avg:57.54ms
step:299/2330 train_time:17202ms step_avg:57.53ms
step:300/2330 train_time:17261ms step_avg:57.54ms
step:301/2330 train_time:17317ms step_avg:57.53ms
step:302/2330 train_time:17377ms step_avg:57.54ms
step:303/2330 train_time:17434ms step_avg:57.54ms
step:304/2330 train_time:17493ms step_avg:57.54ms
step:305/2330 train_time:17550ms step_avg:57.54ms
step:306/2330 train_time:17608ms step_avg:57.54ms
step:307/2330 train_time:17665ms step_avg:57.54ms
step:308/2330 train_time:17724ms step_avg:57.55ms
step:309/2330 train_time:17780ms step_avg:57.54ms
step:310/2330 train_time:17841ms step_avg:57.55ms
step:311/2330 train_time:17897ms step_avg:57.55ms
step:312/2330 train_time:17956ms step_avg:57.55ms
step:313/2330 train_time:18011ms step_avg:57.54ms
step:314/2330 train_time:18071ms step_avg:57.55ms
step:315/2330 train_time:18126ms step_avg:57.54ms
step:316/2330 train_time:18186ms step_avg:57.55ms
step:317/2330 train_time:18242ms step_avg:57.55ms
step:318/2330 train_time:18300ms step_avg:57.55ms
step:319/2330 train_time:18356ms step_avg:57.54ms
step:320/2330 train_time:18416ms step_avg:57.55ms
step:321/2330 train_time:18472ms step_avg:57.54ms
step:322/2330 train_time:18531ms step_avg:57.55ms
step:323/2330 train_time:18587ms step_avg:57.54ms
step:324/2330 train_time:18646ms step_avg:57.55ms
step:325/2330 train_time:18702ms step_avg:57.54ms
step:326/2330 train_time:18761ms step_avg:57.55ms
step:327/2330 train_time:18817ms step_avg:57.54ms
step:328/2330 train_time:18877ms step_avg:57.55ms
step:329/2330 train_time:18933ms step_avg:57.55ms
step:330/2330 train_time:18992ms step_avg:57.55ms
step:331/2330 train_time:19048ms step_avg:57.55ms
step:332/2330 train_time:19107ms step_avg:57.55ms
step:333/2330 train_time:19163ms step_avg:57.55ms
step:334/2330 train_time:19221ms step_avg:57.55ms
step:335/2330 train_time:19276ms step_avg:57.54ms
step:336/2330 train_time:19337ms step_avg:57.55ms
step:337/2330 train_time:19393ms step_avg:57.55ms
step:338/2330 train_time:19452ms step_avg:57.55ms
step:339/2330 train_time:19509ms step_avg:57.55ms
step:340/2330 train_time:19567ms step_avg:57.55ms
step:341/2330 train_time:19623ms step_avg:57.55ms
step:342/2330 train_time:19682ms step_avg:57.55ms
step:343/2330 train_time:19738ms step_avg:57.55ms
step:344/2330 train_time:19797ms step_avg:57.55ms
step:345/2330 train_time:19853ms step_avg:57.54ms
step:346/2330 train_time:19913ms step_avg:57.55ms
step:347/2330 train_time:19969ms step_avg:57.55ms
step:348/2330 train_time:20028ms step_avg:57.55ms
step:349/2330 train_time:20084ms step_avg:57.55ms
step:350/2330 train_time:20142ms step_avg:57.55ms
step:351/2330 train_time:20197ms step_avg:57.54ms
step:352/2330 train_time:20258ms step_avg:57.55ms
step:353/2330 train_time:20314ms step_avg:57.55ms
step:354/2330 train_time:20373ms step_avg:57.55ms
step:355/2330 train_time:20429ms step_avg:57.55ms
step:356/2330 train_time:20489ms step_avg:57.55ms
step:357/2330 train_time:20545ms step_avg:57.55ms
step:358/2330 train_time:20605ms step_avg:57.56ms
step:359/2330 train_time:20661ms step_avg:57.55ms
step:360/2330 train_time:20721ms step_avg:57.56ms
step:361/2330 train_time:20777ms step_avg:57.55ms
step:362/2330 train_time:20836ms step_avg:57.56ms
step:363/2330 train_time:20892ms step_avg:57.55ms
step:364/2330 train_time:20951ms step_avg:57.56ms
step:365/2330 train_time:21008ms step_avg:57.56ms
step:366/2330 train_time:21066ms step_avg:57.56ms
step:367/2330 train_time:21123ms step_avg:57.55ms
step:368/2330 train_time:21182ms step_avg:57.56ms
step:369/2330 train_time:21237ms step_avg:57.55ms
step:370/2330 train_time:21297ms step_avg:57.56ms
step:371/2330 train_time:21353ms step_avg:57.55ms
step:372/2330 train_time:21412ms step_avg:57.56ms
step:373/2330 train_time:21468ms step_avg:57.56ms
step:374/2330 train_time:21527ms step_avg:57.56ms
step:375/2330 train_time:21582ms step_avg:57.55ms
step:376/2330 train_time:21642ms step_avg:57.56ms
step:377/2330 train_time:21698ms step_avg:57.55ms
step:378/2330 train_time:21758ms step_avg:57.56ms
step:379/2330 train_time:21815ms step_avg:57.56ms
step:380/2330 train_time:21874ms step_avg:57.56ms
step:381/2330 train_time:21930ms step_avg:57.56ms
step:382/2330 train_time:21989ms step_avg:57.56ms
step:383/2330 train_time:22045ms step_avg:57.56ms
step:384/2330 train_time:22105ms step_avg:57.56ms
step:385/2330 train_time:22160ms step_avg:57.56ms
step:386/2330 train_time:22220ms step_avg:57.56ms
step:387/2330 train_time:22275ms step_avg:57.56ms
step:388/2330 train_time:22335ms step_avg:57.57ms
step:389/2330 train_time:22393ms step_avg:57.56ms
step:390/2330 train_time:22452ms step_avg:57.57ms
step:391/2330 train_time:22508ms step_avg:57.57ms
step:392/2330 train_time:22567ms step_avg:57.57ms
step:393/2330 train_time:22623ms step_avg:57.57ms
step:394/2330 train_time:22683ms step_avg:57.57ms
step:395/2330 train_time:22738ms step_avg:57.57ms
step:396/2330 train_time:22799ms step_avg:57.57ms
step:397/2330 train_time:22855ms step_avg:57.57ms
step:398/2330 train_time:22914ms step_avg:57.57ms
step:399/2330 train_time:22971ms step_avg:57.57ms
step:400/2330 train_time:23030ms step_avg:57.58ms
step:401/2330 train_time:23086ms step_avg:57.57ms
step:402/2330 train_time:23145ms step_avg:57.57ms
step:403/2330 train_time:23200ms step_avg:57.57ms
step:404/2330 train_time:23261ms step_avg:57.58ms
step:405/2330 train_time:23316ms step_avg:57.57ms
step:406/2330 train_time:23375ms step_avg:57.57ms
step:407/2330 train_time:23432ms step_avg:57.57ms
step:408/2330 train_time:23491ms step_avg:57.58ms
step:409/2330 train_time:23548ms step_avg:57.57ms
step:410/2330 train_time:23607ms step_avg:57.58ms
step:411/2330 train_time:23663ms step_avg:57.57ms
step:412/2330 train_time:23723ms step_avg:57.58ms
step:413/2330 train_time:23778ms step_avg:57.57ms
step:414/2330 train_time:23837ms step_avg:57.58ms
step:415/2330 train_time:23893ms step_avg:57.57ms
step:416/2330 train_time:23953ms step_avg:57.58ms
step:417/2330 train_time:24009ms step_avg:57.58ms
step:418/2330 train_time:24069ms step_avg:57.58ms
step:419/2330 train_time:24124ms step_avg:57.58ms
step:420/2330 train_time:24184ms step_avg:57.58ms
step:421/2330 train_time:24239ms step_avg:57.58ms
step:422/2330 train_time:24299ms step_avg:57.58ms
step:423/2330 train_time:24355ms step_avg:57.58ms
step:424/2330 train_time:24414ms step_avg:57.58ms
step:425/2330 train_time:24470ms step_avg:57.58ms
step:426/2330 train_time:24529ms step_avg:57.58ms
step:427/2330 train_time:24585ms step_avg:57.58ms
step:428/2330 train_time:24645ms step_avg:57.58ms
step:429/2330 train_time:24700ms step_avg:57.58ms
step:430/2330 train_time:24760ms step_avg:57.58ms
step:431/2330 train_time:24816ms step_avg:57.58ms
step:432/2330 train_time:24875ms step_avg:57.58ms
step:433/2330 train_time:24932ms step_avg:57.58ms
step:434/2330 train_time:24991ms step_avg:57.58ms
step:435/2330 train_time:25047ms step_avg:57.58ms
step:436/2330 train_time:25106ms step_avg:57.58ms
step:437/2330 train_time:25161ms step_avg:57.58ms
step:438/2330 train_time:25222ms step_avg:57.58ms
step:439/2330 train_time:25277ms step_avg:57.58ms
step:440/2330 train_time:25337ms step_avg:57.58ms
step:441/2330 train_time:25394ms step_avg:57.58ms
step:442/2330 train_time:25453ms step_avg:57.59ms
step:443/2330 train_time:25509ms step_avg:57.58ms
step:444/2330 train_time:25569ms step_avg:57.59ms
step:445/2330 train_time:25625ms step_avg:57.58ms
step:446/2330 train_time:25684ms step_avg:57.59ms
step:447/2330 train_time:25740ms step_avg:57.58ms
step:448/2330 train_time:25799ms step_avg:57.59ms
step:449/2330 train_time:25855ms step_avg:57.58ms
step:450/2330 train_time:25915ms step_avg:57.59ms
step:451/2330 train_time:25971ms step_avg:57.59ms
step:452/2330 train_time:26030ms step_avg:57.59ms
step:453/2330 train_time:26085ms step_avg:57.58ms
step:454/2330 train_time:26145ms step_avg:57.59ms
step:455/2330 train_time:26202ms step_avg:57.59ms
step:456/2330 train_time:26261ms step_avg:57.59ms
step:457/2330 train_time:26316ms step_avg:57.58ms
step:458/2330 train_time:26375ms step_avg:57.59ms
step:459/2330 train_time:26431ms step_avg:57.58ms
step:460/2330 train_time:26490ms step_avg:57.59ms
step:461/2330 train_time:26546ms step_avg:57.58ms
step:462/2330 train_time:26605ms step_avg:57.59ms
step:463/2330 train_time:26662ms step_avg:57.58ms
step:464/2330 train_time:26721ms step_avg:57.59ms
step:465/2330 train_time:26777ms step_avg:57.58ms
step:466/2330 train_time:26836ms step_avg:57.59ms
step:467/2330 train_time:26893ms step_avg:57.59ms
step:468/2330 train_time:26952ms step_avg:57.59ms
step:469/2330 train_time:27008ms step_avg:57.59ms
step:470/2330 train_time:27066ms step_avg:57.59ms
step:471/2330 train_time:27122ms step_avg:57.58ms
step:472/2330 train_time:27183ms step_avg:57.59ms
step:473/2330 train_time:27238ms step_avg:57.59ms
step:474/2330 train_time:27297ms step_avg:57.59ms
step:475/2330 train_time:27354ms step_avg:57.59ms
step:476/2330 train_time:27413ms step_avg:57.59ms
step:477/2330 train_time:27469ms step_avg:57.59ms
step:478/2330 train_time:27528ms step_avg:57.59ms
step:479/2330 train_time:27585ms step_avg:57.59ms
step:480/2330 train_time:27644ms step_avg:57.59ms
step:481/2330 train_time:27699ms step_avg:57.59ms
step:482/2330 train_time:27758ms step_avg:57.59ms
step:483/2330 train_time:27814ms step_avg:57.59ms
step:484/2330 train_time:27873ms step_avg:57.59ms
step:485/2330 train_time:27929ms step_avg:57.59ms
step:486/2330 train_time:27989ms step_avg:57.59ms
step:487/2330 train_time:28045ms step_avg:57.59ms
step:488/2330 train_time:28104ms step_avg:57.59ms
step:489/2330 train_time:28160ms step_avg:57.59ms
step:490/2330 train_time:28220ms step_avg:57.59ms
step:491/2330 train_time:28276ms step_avg:57.59ms
step:492/2330 train_time:28336ms step_avg:57.59ms
step:493/2330 train_time:28392ms step_avg:57.59ms
step:494/2330 train_time:28451ms step_avg:57.59ms
step:495/2330 train_time:28507ms step_avg:57.59ms
step:496/2330 train_time:28566ms step_avg:57.59ms
step:497/2330 train_time:28622ms step_avg:57.59ms
step:498/2330 train_time:28682ms step_avg:57.59ms
step:499/2330 train_time:28737ms step_avg:57.59ms
step:500/2330 train_time:28796ms step_avg:57.59ms
step:500/2330 val_loss:4.3988 train_time:28876ms step_avg:57.75ms
step:501/2330 train_time:28895ms step_avg:57.67ms
step:502/2330 train_time:28915ms step_avg:57.60ms
step:503/2330 train_time:28972ms step_avg:57.60ms
step:504/2330 train_time:29033ms step_avg:57.60ms
step:505/2330 train_time:29090ms step_avg:57.60ms
step:506/2330 train_time:29152ms step_avg:57.61ms
step:507/2330 train_time:29208ms step_avg:57.61ms
step:508/2330 train_time:29268ms step_avg:57.61ms
step:509/2330 train_time:29323ms step_avg:57.61ms
step:510/2330 train_time:29383ms step_avg:57.61ms
step:511/2330 train_time:29438ms step_avg:57.61ms
step:512/2330 train_time:29496ms step_avg:57.61ms
step:513/2330 train_time:29551ms step_avg:57.60ms
step:514/2330 train_time:29610ms step_avg:57.61ms
step:515/2330 train_time:29665ms step_avg:57.60ms
step:516/2330 train_time:29724ms step_avg:57.60ms
step:517/2330 train_time:29779ms step_avg:57.60ms
step:518/2330 train_time:29839ms step_avg:57.60ms
step:519/2330 train_time:29895ms step_avg:57.60ms
step:520/2330 train_time:29957ms step_avg:57.61ms
step:521/2330 train_time:30013ms step_avg:57.61ms
step:522/2330 train_time:30075ms step_avg:57.61ms
step:523/2330 train_time:30131ms step_avg:57.61ms
step:524/2330 train_time:30190ms step_avg:57.62ms
step:525/2330 train_time:30246ms step_avg:57.61ms
step:526/2330 train_time:30307ms step_avg:57.62ms
step:527/2330 train_time:30363ms step_avg:57.61ms
step:528/2330 train_time:30423ms step_avg:57.62ms
step:529/2330 train_time:30479ms step_avg:57.62ms
step:530/2330 train_time:30537ms step_avg:57.62ms
step:531/2330 train_time:30592ms step_avg:57.61ms
step:532/2330 train_time:30651ms step_avg:57.61ms
step:533/2330 train_time:30707ms step_avg:57.61ms
step:534/2330 train_time:30766ms step_avg:57.61ms
step:535/2330 train_time:30822ms step_avg:57.61ms
step:536/2330 train_time:30883ms step_avg:57.62ms
step:537/2330 train_time:30938ms step_avg:57.61ms
step:538/2330 train_time:31000ms step_avg:57.62ms
step:539/2330 train_time:31056ms step_avg:57.62ms
step:540/2330 train_time:31117ms step_avg:57.62ms
step:541/2330 train_time:31172ms step_avg:57.62ms
step:542/2330 train_time:31233ms step_avg:57.63ms
step:543/2330 train_time:31289ms step_avg:57.62ms
step:544/2330 train_time:31349ms step_avg:57.63ms
step:545/2330 train_time:31405ms step_avg:57.62ms
step:546/2330 train_time:31464ms step_avg:57.63ms
step:547/2330 train_time:31519ms step_avg:57.62ms
step:548/2330 train_time:31578ms step_avg:57.62ms
step:549/2330 train_time:31634ms step_avg:57.62ms
step:550/2330 train_time:31693ms step_avg:57.62ms
step:551/2330 train_time:31749ms step_avg:57.62ms
step:552/2330 train_time:31808ms step_avg:57.62ms
step:553/2330 train_time:31865ms step_avg:57.62ms
step:554/2330 train_time:31924ms step_avg:57.63ms
step:555/2330 train_time:31981ms step_avg:57.62ms
step:556/2330 train_time:32041ms step_avg:57.63ms
step:557/2330 train_time:32097ms step_avg:57.62ms
step:558/2330 train_time:32157ms step_avg:57.63ms
step:559/2330 train_time:32212ms step_avg:57.63ms
step:560/2330 train_time:32272ms step_avg:57.63ms
step:561/2330 train_time:32328ms step_avg:57.63ms
step:562/2330 train_time:32387ms step_avg:57.63ms
step:563/2330 train_time:32444ms step_avg:57.63ms
step:564/2330 train_time:32502ms step_avg:57.63ms
step:565/2330 train_time:32558ms step_avg:57.63ms
step:566/2330 train_time:32617ms step_avg:57.63ms
step:567/2330 train_time:32673ms step_avg:57.62ms
step:568/2330 train_time:32732ms step_avg:57.63ms
step:569/2330 train_time:32787ms step_avg:57.62ms
step:570/2330 train_time:32848ms step_avg:57.63ms
step:571/2330 train_time:32904ms step_avg:57.63ms
step:572/2330 train_time:32964ms step_avg:57.63ms
step:573/2330 train_time:33020ms step_avg:57.63ms
step:574/2330 train_time:33081ms step_avg:57.63ms
step:575/2330 train_time:33137ms step_avg:57.63ms
step:576/2330 train_time:33197ms step_avg:57.63ms
step:577/2330 train_time:33252ms step_avg:57.63ms
step:578/2330 train_time:33312ms step_avg:57.63ms
step:579/2330 train_time:33368ms step_avg:57.63ms
step:580/2330 train_time:33427ms step_avg:57.63ms
step:581/2330 train_time:33483ms step_avg:57.63ms
step:582/2330 train_time:33542ms step_avg:57.63ms
step:583/2330 train_time:33598ms step_avg:57.63ms
step:584/2330 train_time:33657ms step_avg:57.63ms
step:585/2330 train_time:33713ms step_avg:57.63ms
step:586/2330 train_time:33772ms step_avg:57.63ms
step:587/2330 train_time:33829ms step_avg:57.63ms
step:588/2330 train_time:33889ms step_avg:57.63ms
step:589/2330 train_time:33945ms step_avg:57.63ms
step:590/2330 train_time:34004ms step_avg:57.63ms
step:591/2330 train_time:34060ms step_avg:57.63ms
step:592/2330 train_time:34121ms step_avg:57.64ms
step:593/2330 train_time:34178ms step_avg:57.64ms
step:594/2330 train_time:34237ms step_avg:57.64ms
step:595/2330 train_time:34293ms step_avg:57.64ms
step:596/2330 train_time:34353ms step_avg:57.64ms
step:597/2330 train_time:34409ms step_avg:57.64ms
step:598/2330 train_time:34468ms step_avg:57.64ms
step:599/2330 train_time:34524ms step_avg:57.64ms
step:600/2330 train_time:34583ms step_avg:57.64ms
step:601/2330 train_time:34639ms step_avg:57.64ms
step:602/2330 train_time:34698ms step_avg:57.64ms
step:603/2330 train_time:34753ms step_avg:57.63ms
step:604/2330 train_time:34813ms step_avg:57.64ms
step:605/2330 train_time:34869ms step_avg:57.64ms
step:606/2330 train_time:34929ms step_avg:57.64ms
step:607/2330 train_time:34985ms step_avg:57.64ms
step:608/2330 train_time:35044ms step_avg:57.64ms
step:609/2330 train_time:35101ms step_avg:57.64ms
step:610/2330 train_time:35160ms step_avg:57.64ms
step:611/2330 train_time:35216ms step_avg:57.64ms
step:612/2330 train_time:35276ms step_avg:57.64ms
step:613/2330 train_time:35332ms step_avg:57.64ms
step:614/2330 train_time:35391ms step_avg:57.64ms
step:615/2330 train_time:35447ms step_avg:57.64ms
step:616/2330 train_time:35507ms step_avg:57.64ms
step:617/2330 train_time:35563ms step_avg:57.64ms
step:618/2330 train_time:35622ms step_avg:57.64ms
step:619/2330 train_time:35677ms step_avg:57.64ms
step:620/2330 train_time:35737ms step_avg:57.64ms
step:621/2330 train_time:35793ms step_avg:57.64ms
step:622/2330 train_time:35853ms step_avg:57.64ms
step:623/2330 train_time:35909ms step_avg:57.64ms
step:624/2330 train_time:35969ms step_avg:57.64ms
step:625/2330 train_time:36026ms step_avg:57.64ms
step:626/2330 train_time:36086ms step_avg:57.65ms
step:627/2330 train_time:36142ms step_avg:57.64ms
step:628/2330 train_time:36203ms step_avg:57.65ms
step:629/2330 train_time:36258ms step_avg:57.64ms
step:630/2330 train_time:36319ms step_avg:57.65ms
step:631/2330 train_time:36374ms step_avg:57.65ms
step:632/2330 train_time:36435ms step_avg:57.65ms
step:633/2330 train_time:36491ms step_avg:57.65ms
step:634/2330 train_time:36550ms step_avg:57.65ms
step:635/2330 train_time:36606ms step_avg:57.65ms
step:636/2330 train_time:36666ms step_avg:57.65ms
step:637/2330 train_time:36721ms step_avg:57.65ms
step:638/2330 train_time:36781ms step_avg:57.65ms
step:639/2330 train_time:36836ms step_avg:57.65ms
step:640/2330 train_time:36896ms step_avg:57.65ms
step:641/2330 train_time:36952ms step_avg:57.65ms
step:642/2330 train_time:37012ms step_avg:57.65ms
step:643/2330 train_time:37069ms step_avg:57.65ms
step:644/2330 train_time:37129ms step_avg:57.65ms
step:645/2330 train_time:37186ms step_avg:57.65ms
step:646/2330 train_time:37245ms step_avg:57.65ms
step:647/2330 train_time:37301ms step_avg:57.65ms
step:648/2330 train_time:37360ms step_avg:57.65ms
step:649/2330 train_time:37417ms step_avg:57.65ms
step:650/2330 train_time:37476ms step_avg:57.65ms
step:651/2330 train_time:37532ms step_avg:57.65ms
step:652/2330 train_time:37591ms step_avg:57.65ms
step:653/2330 train_time:37648ms step_avg:57.65ms
step:654/2330 train_time:37707ms step_avg:57.66ms
step:655/2330 train_time:37762ms step_avg:57.65ms
step:656/2330 train_time:37822ms step_avg:57.66ms
step:657/2330 train_time:37878ms step_avg:57.65ms
step:658/2330 train_time:37938ms step_avg:57.66ms
step:659/2330 train_time:37994ms step_avg:57.65ms
step:660/2330 train_time:38054ms step_avg:57.66ms
step:661/2330 train_time:38111ms step_avg:57.66ms
step:662/2330 train_time:38172ms step_avg:57.66ms
step:663/2330 train_time:38229ms step_avg:57.66ms
step:664/2330 train_time:38288ms step_avg:57.66ms
step:665/2330 train_time:38345ms step_avg:57.66ms
step:666/2330 train_time:38404ms step_avg:57.66ms
step:667/2330 train_time:38460ms step_avg:57.66ms
step:668/2330 train_time:38519ms step_avg:57.66ms
step:669/2330 train_time:38575ms step_avg:57.66ms
step:670/2330 train_time:38634ms step_avg:57.66ms
step:671/2330 train_time:38690ms step_avg:57.66ms
step:672/2330 train_time:38749ms step_avg:57.66ms
step:673/2330 train_time:38806ms step_avg:57.66ms
step:674/2330 train_time:38865ms step_avg:57.66ms
step:675/2330 train_time:38921ms step_avg:57.66ms
step:676/2330 train_time:38981ms step_avg:57.66ms
step:677/2330 train_time:39036ms step_avg:57.66ms
step:678/2330 train_time:39095ms step_avg:57.66ms
step:679/2330 train_time:39151ms step_avg:57.66ms
step:680/2330 train_time:39212ms step_avg:57.66ms
step:681/2330 train_time:39269ms step_avg:57.66ms
step:682/2330 train_time:39328ms step_avg:57.67ms
step:683/2330 train_time:39384ms step_avg:57.66ms
step:684/2330 train_time:39443ms step_avg:57.67ms
step:685/2330 train_time:39499ms step_avg:57.66ms
step:686/2330 train_time:39558ms step_avg:57.67ms
step:687/2330 train_time:39614ms step_avg:57.66ms
step:688/2330 train_time:39674ms step_avg:57.67ms
step:689/2330 train_time:39731ms step_avg:57.66ms
step:690/2330 train_time:39790ms step_avg:57.67ms
step:691/2330 train_time:39846ms step_avg:57.66ms
step:692/2330 train_time:39906ms step_avg:57.67ms
step:693/2330 train_time:39961ms step_avg:57.66ms
step:694/2330 train_time:40021ms step_avg:57.67ms
step:695/2330 train_time:40077ms step_avg:57.66ms
step:696/2330 train_time:40137ms step_avg:57.67ms
step:697/2330 train_time:40192ms step_avg:57.66ms
step:698/2330 train_time:40254ms step_avg:57.67ms
step:699/2330 train_time:40310ms step_avg:57.67ms
step:700/2330 train_time:40369ms step_avg:57.67ms
step:701/2330 train_time:40426ms step_avg:57.67ms
step:702/2330 train_time:40485ms step_avg:57.67ms
step:703/2330 train_time:40542ms step_avg:57.67ms
step:704/2330 train_time:40601ms step_avg:57.67ms
step:705/2330 train_time:40657ms step_avg:57.67ms
step:706/2330 train_time:40717ms step_avg:57.67ms
step:707/2330 train_time:40773ms step_avg:57.67ms
step:708/2330 train_time:40832ms step_avg:57.67ms
step:709/2330 train_time:40888ms step_avg:57.67ms
step:710/2330 train_time:40948ms step_avg:57.67ms
step:711/2330 train_time:41005ms step_avg:57.67ms
step:712/2330 train_time:41064ms step_avg:57.67ms
step:713/2330 train_time:41120ms step_avg:57.67ms
step:714/2330 train_time:41180ms step_avg:57.67ms
step:715/2330 train_time:41236ms step_avg:57.67ms
step:716/2330 train_time:41296ms step_avg:57.68ms
step:717/2330 train_time:41351ms step_avg:57.67ms
step:718/2330 train_time:41412ms step_avg:57.68ms
step:719/2330 train_time:41469ms step_avg:57.68ms
step:720/2330 train_time:41528ms step_avg:57.68ms
step:721/2330 train_time:41585ms step_avg:57.68ms
step:722/2330 train_time:41644ms step_avg:57.68ms
step:723/2330 train_time:41701ms step_avg:57.68ms
step:724/2330 train_time:41760ms step_avg:57.68ms
step:725/2330 train_time:41815ms step_avg:57.68ms
step:726/2330 train_time:41875ms step_avg:57.68ms
step:727/2330 train_time:41931ms step_avg:57.68ms
step:728/2330 train_time:41991ms step_avg:57.68ms
step:729/2330 train_time:42047ms step_avg:57.68ms
step:730/2330 train_time:42107ms step_avg:57.68ms
step:731/2330 train_time:42164ms step_avg:57.68ms
step:732/2330 train_time:42222ms step_avg:57.68ms
step:733/2330 train_time:42278ms step_avg:57.68ms
step:734/2330 train_time:42338ms step_avg:57.68ms
step:735/2330 train_time:42393ms step_avg:57.68ms
step:736/2330 train_time:42454ms step_avg:57.68ms
step:737/2330 train_time:42510ms step_avg:57.68ms
step:738/2330 train_time:42570ms step_avg:57.68ms
step:739/2330 train_time:42626ms step_avg:57.68ms
step:740/2330 train_time:42686ms step_avg:57.68ms
step:741/2330 train_time:42742ms step_avg:57.68ms
step:742/2330 train_time:42802ms step_avg:57.68ms
step:743/2330 train_time:42857ms step_avg:57.68ms
step:744/2330 train_time:42918ms step_avg:57.69ms
step:745/2330 train_time:42974ms step_avg:57.68ms
step:746/2330 train_time:43034ms step_avg:57.69ms
step:747/2330 train_time:43091ms step_avg:57.68ms
step:748/2330 train_time:43150ms step_avg:57.69ms
step:749/2330 train_time:43207ms step_avg:57.69ms
step:750/2330 train_time:43267ms step_avg:57.69ms
step:750/2330 val_loss:4.2086 train_time:43346ms step_avg:57.79ms
step:751/2330 train_time:43364ms step_avg:57.74ms
step:752/2330 train_time:43384ms step_avg:57.69ms
step:753/2330 train_time:43441ms step_avg:57.69ms
step:754/2330 train_time:43504ms step_avg:57.70ms
step:755/2330 train_time:43560ms step_avg:57.70ms
step:756/2330 train_time:43621ms step_avg:57.70ms
step:757/2330 train_time:43677ms step_avg:57.70ms
step:758/2330 train_time:43736ms step_avg:57.70ms
step:759/2330 train_time:43792ms step_avg:57.70ms
step:760/2330 train_time:43850ms step_avg:57.70ms
step:761/2330 train_time:43905ms step_avg:57.69ms
step:762/2330 train_time:43964ms step_avg:57.70ms
step:763/2330 train_time:44019ms step_avg:57.69ms
step:764/2330 train_time:44078ms step_avg:57.69ms
step:765/2330 train_time:44134ms step_avg:57.69ms
step:766/2330 train_time:44192ms step_avg:57.69ms
step:767/2330 train_time:44249ms step_avg:57.69ms
step:768/2330 train_time:44309ms step_avg:57.69ms
step:769/2330 train_time:44368ms step_avg:57.70ms
step:770/2330 train_time:44429ms step_avg:57.70ms
step:771/2330 train_time:44487ms step_avg:57.70ms
step:772/2330 train_time:44548ms step_avg:57.70ms
step:773/2330 train_time:44606ms step_avg:57.70ms
step:774/2330 train_time:44665ms step_avg:57.71ms
step:775/2330 train_time:44723ms step_avg:57.71ms
step:776/2330 train_time:44782ms step_avg:57.71ms
step:777/2330 train_time:44838ms step_avg:57.71ms
step:778/2330 train_time:44899ms step_avg:57.71ms
step:779/2330 train_time:44955ms step_avg:57.71ms
step:780/2330 train_time:45015ms step_avg:57.71ms
step:781/2330 train_time:45072ms step_avg:57.71ms
step:782/2330 train_time:45132ms step_avg:57.71ms
step:783/2330 train_time:45188ms step_avg:57.71ms
step:784/2330 train_time:45247ms step_avg:57.71ms
step:785/2330 train_time:45305ms step_avg:57.71ms
step:786/2330 train_time:45364ms step_avg:57.72ms
step:787/2330 train_time:45422ms step_avg:57.72ms
step:788/2330 train_time:45483ms step_avg:57.72ms
step:789/2330 train_time:45539ms step_avg:57.72ms
step:790/2330 train_time:45601ms step_avg:57.72ms
step:791/2330 train_time:45658ms step_avg:57.72ms
step:792/2330 train_time:45718ms step_avg:57.73ms
step:793/2330 train_time:45775ms step_avg:57.72ms
step:794/2330 train_time:45835ms step_avg:57.73ms
step:795/2330 train_time:45891ms step_avg:57.72ms
step:796/2330 train_time:45951ms step_avg:57.73ms
step:797/2330 train_time:46007ms step_avg:57.73ms
step:798/2330 train_time:46066ms step_avg:57.73ms
step:799/2330 train_time:46123ms step_avg:57.73ms
step:800/2330 train_time:46182ms step_avg:57.73ms
step:801/2330 train_time:46239ms step_avg:57.73ms
step:802/2330 train_time:46299ms step_avg:57.73ms
step:803/2330 train_time:46355ms step_avg:57.73ms
step:804/2330 train_time:46417ms step_avg:57.73ms
step:805/2330 train_time:46474ms step_avg:57.73ms
step:806/2330 train_time:46534ms step_avg:57.73ms
step:807/2330 train_time:46592ms step_avg:57.73ms
step:808/2330 train_time:46652ms step_avg:57.74ms
step:809/2330 train_time:46709ms step_avg:57.74ms
step:810/2330 train_time:46770ms step_avg:57.74ms
step:811/2330 train_time:46826ms step_avg:57.74ms
step:812/2330 train_time:46886ms step_avg:57.74ms
step:813/2330 train_time:46942ms step_avg:57.74ms
step:814/2330 train_time:47002ms step_avg:57.74ms
step:815/2330 train_time:47059ms step_avg:57.74ms
step:816/2330 train_time:47118ms step_avg:57.74ms
step:817/2330 train_time:47174ms step_avg:57.74ms
step:818/2330 train_time:47234ms step_avg:57.74ms
step:819/2330 train_time:47291ms step_avg:57.74ms
step:820/2330 train_time:47352ms step_avg:57.75ms
step:821/2330 train_time:47409ms step_avg:57.75ms
step:822/2330 train_time:47470ms step_avg:57.75ms
step:823/2330 train_time:47527ms step_avg:57.75ms
step:824/2330 train_time:47587ms step_avg:57.75ms
step:825/2330 train_time:47644ms step_avg:57.75ms
step:826/2330 train_time:47704ms step_avg:57.75ms
step:827/2330 train_time:47760ms step_avg:57.75ms
step:828/2330 train_time:47820ms step_avg:57.75ms
step:829/2330 train_time:47877ms step_avg:57.75ms
step:830/2330 train_time:47937ms step_avg:57.76ms
step:831/2330 train_time:47993ms step_avg:57.75ms
step:832/2330 train_time:48055ms step_avg:57.76ms
step:833/2330 train_time:48111ms step_avg:57.76ms
step:834/2330 train_time:48171ms step_avg:57.76ms
step:835/2330 train_time:48228ms step_avg:57.76ms
step:836/2330 train_time:48288ms step_avg:57.76ms
step:837/2330 train_time:48345ms step_avg:57.76ms
step:838/2330 train_time:48405ms step_avg:57.76ms
step:839/2330 train_time:48461ms step_avg:57.76ms
step:840/2330 train_time:48522ms step_avg:57.76ms
step:841/2330 train_time:48579ms step_avg:57.76ms
step:842/2330 train_time:48639ms step_avg:57.77ms
step:843/2330 train_time:48696ms step_avg:57.77ms
step:844/2330 train_time:48757ms step_avg:57.77ms
step:845/2330 train_time:48814ms step_avg:57.77ms
step:846/2330 train_time:48873ms step_avg:57.77ms
step:847/2330 train_time:48930ms step_avg:57.77ms
step:848/2330 train_time:48990ms step_avg:57.77ms
step:849/2330 train_time:49047ms step_avg:57.77ms
step:850/2330 train_time:49107ms step_avg:57.77ms
step:851/2330 train_time:49163ms step_avg:57.77ms
step:852/2330 train_time:49223ms step_avg:57.77ms
step:853/2330 train_time:49280ms step_avg:57.77ms
step:854/2330 train_time:49341ms step_avg:57.78ms
step:855/2330 train_time:49397ms step_avg:57.77ms
step:856/2330 train_time:49458ms step_avg:57.78ms
step:857/2330 train_time:49515ms step_avg:57.78ms
step:858/2330 train_time:49576ms step_avg:57.78ms
step:859/2330 train_time:49633ms step_avg:57.78ms
step:860/2330 train_time:49693ms step_avg:57.78ms
step:861/2330 train_time:49750ms step_avg:57.78ms
step:862/2330 train_time:49811ms step_avg:57.79ms
step:863/2330 train_time:49868ms step_avg:57.78ms
step:864/2330 train_time:49928ms step_avg:57.79ms
step:865/2330 train_time:49985ms step_avg:57.79ms
step:866/2330 train_time:50044ms step_avg:57.79ms
step:867/2330 train_time:50101ms step_avg:57.79ms
step:868/2330 train_time:50161ms step_avg:57.79ms
step:869/2330 train_time:50218ms step_avg:57.79ms
step:870/2330 train_time:50277ms step_avg:57.79ms
step:871/2330 train_time:50334ms step_avg:57.79ms
step:872/2330 train_time:50394ms step_avg:57.79ms
step:873/2330 train_time:50450ms step_avg:57.79ms
step:874/2330 train_time:50511ms step_avg:57.79ms
step:875/2330 train_time:50568ms step_avg:57.79ms
step:876/2330 train_time:50628ms step_avg:57.79ms
step:877/2330 train_time:50685ms step_avg:57.79ms
step:878/2330 train_time:50745ms step_avg:57.80ms
step:879/2330 train_time:50803ms step_avg:57.80ms
step:880/2330 train_time:50863ms step_avg:57.80ms
step:881/2330 train_time:50920ms step_avg:57.80ms
step:882/2330 train_time:50979ms step_avg:57.80ms
step:883/2330 train_time:51036ms step_avg:57.80ms
step:884/2330 train_time:51097ms step_avg:57.80ms
step:885/2330 train_time:51154ms step_avg:57.80ms
step:886/2330 train_time:51214ms step_avg:57.80ms
step:887/2330 train_time:51271ms step_avg:57.80ms
step:888/2330 train_time:51331ms step_avg:57.81ms
step:889/2330 train_time:51388ms step_avg:57.80ms
step:890/2330 train_time:51447ms step_avg:57.81ms
step:891/2330 train_time:51504ms step_avg:57.80ms
step:892/2330 train_time:51564ms step_avg:57.81ms
step:893/2330 train_time:51621ms step_avg:57.81ms
step:894/2330 train_time:51681ms step_avg:57.81ms
step:895/2330 train_time:51737ms step_avg:57.81ms
step:896/2330 train_time:51799ms step_avg:57.81ms
step:897/2330 train_time:51856ms step_avg:57.81ms
step:898/2330 train_time:51916ms step_avg:57.81ms
step:899/2330 train_time:51973ms step_avg:57.81ms
step:900/2330 train_time:52033ms step_avg:57.81ms
step:901/2330 train_time:52089ms step_avg:57.81ms
step:902/2330 train_time:52150ms step_avg:57.82ms
step:903/2330 train_time:52207ms step_avg:57.82ms
step:904/2330 train_time:52267ms step_avg:57.82ms
step:905/2330 train_time:52323ms step_avg:57.82ms
step:906/2330 train_time:52384ms step_avg:57.82ms
step:907/2330 train_time:52440ms step_avg:57.82ms
step:908/2330 train_time:52501ms step_avg:57.82ms
step:909/2330 train_time:52558ms step_avg:57.82ms
step:910/2330 train_time:52618ms step_avg:57.82ms
step:911/2330 train_time:52675ms step_avg:57.82ms
step:912/2330 train_time:52735ms step_avg:57.82ms
step:913/2330 train_time:52792ms step_avg:57.82ms
step:914/2330 train_time:52852ms step_avg:57.83ms
step:915/2330 train_time:52909ms step_avg:57.82ms
step:916/2330 train_time:52970ms step_avg:57.83ms
step:917/2330 train_time:53027ms step_avg:57.83ms
step:918/2330 train_time:53088ms step_avg:57.83ms
step:919/2330 train_time:53144ms step_avg:57.83ms
step:920/2330 train_time:53204ms step_avg:57.83ms
step:921/2330 train_time:53261ms step_avg:57.83ms
step:922/2330 train_time:53322ms step_avg:57.83ms
step:923/2330 train_time:53379ms step_avg:57.83ms
step:924/2330 train_time:53439ms step_avg:57.83ms
step:925/2330 train_time:53495ms step_avg:57.83ms
step:926/2330 train_time:53556ms step_avg:57.84ms
step:927/2330 train_time:53613ms step_avg:57.84ms
step:928/2330 train_time:53674ms step_avg:57.84ms
step:929/2330 train_time:53730ms step_avg:57.84ms
step:930/2330 train_time:53790ms step_avg:57.84ms
step:931/2330 train_time:53847ms step_avg:57.84ms
step:932/2330 train_time:53907ms step_avg:57.84ms
step:933/2330 train_time:53964ms step_avg:57.84ms
step:934/2330 train_time:54023ms step_avg:57.84ms
step:935/2330 train_time:54080ms step_avg:57.84ms
step:936/2330 train_time:54140ms step_avg:57.84ms
step:937/2330 train_time:54197ms step_avg:57.84ms
step:938/2330 train_time:54258ms step_avg:57.84ms
step:939/2330 train_time:54314ms step_avg:57.84ms
step:940/2330 train_time:54375ms step_avg:57.85ms
step:941/2330 train_time:54432ms step_avg:57.85ms
step:942/2330 train_time:54492ms step_avg:57.85ms
step:943/2330 train_time:54549ms step_avg:57.85ms
step:944/2330 train_time:54611ms step_avg:57.85ms
step:945/2330 train_time:54667ms step_avg:57.85ms
step:946/2330 train_time:54729ms step_avg:57.85ms
step:947/2330 train_time:54785ms step_avg:57.85ms
step:948/2330 train_time:54846ms step_avg:57.85ms
step:949/2330 train_time:54902ms step_avg:57.85ms
step:950/2330 train_time:54962ms step_avg:57.85ms
step:951/2330 train_time:55018ms step_avg:57.85ms
step:952/2330 train_time:55079ms step_avg:57.86ms
step:953/2330 train_time:55137ms step_avg:57.86ms
step:954/2330 train_time:55195ms step_avg:57.86ms
step:955/2330 train_time:55253ms step_avg:57.86ms
step:956/2330 train_time:55313ms step_avg:57.86ms
step:957/2330 train_time:55370ms step_avg:57.86ms
step:958/2330 train_time:55431ms step_avg:57.86ms
step:959/2330 train_time:55489ms step_avg:57.86ms
step:960/2330 train_time:55549ms step_avg:57.86ms
step:961/2330 train_time:55605ms step_avg:57.86ms
step:962/2330 train_time:55665ms step_avg:57.86ms
step:963/2330 train_time:55722ms step_avg:57.86ms
step:964/2330 train_time:55783ms step_avg:57.87ms
step:965/2330 train_time:55838ms step_avg:57.86ms
step:966/2330 train_time:55899ms step_avg:57.87ms
step:967/2330 train_time:55955ms step_avg:57.86ms
step:968/2330 train_time:56016ms step_avg:57.87ms
step:969/2330 train_time:56073ms step_avg:57.87ms
step:970/2330 train_time:56133ms step_avg:57.87ms
step:971/2330 train_time:56189ms step_avg:57.87ms
step:972/2330 train_time:56251ms step_avg:57.87ms
step:973/2330 train_time:56308ms step_avg:57.87ms
step:974/2330 train_time:56367ms step_avg:57.87ms
step:975/2330 train_time:56424ms step_avg:57.87ms
step:976/2330 train_time:56484ms step_avg:57.87ms
step:977/2330 train_time:56540ms step_avg:57.87ms
step:978/2330 train_time:56601ms step_avg:57.87ms
step:979/2330 train_time:56658ms step_avg:57.87ms
step:980/2330 train_time:56718ms step_avg:57.88ms
step:981/2330 train_time:56775ms step_avg:57.87ms
step:982/2330 train_time:56836ms step_avg:57.88ms
step:983/2330 train_time:56893ms step_avg:57.88ms
step:984/2330 train_time:56953ms step_avg:57.88ms
step:985/2330 train_time:57010ms step_avg:57.88ms
step:986/2330 train_time:57070ms step_avg:57.88ms
step:987/2330 train_time:57127ms step_avg:57.88ms
step:988/2330 train_time:57188ms step_avg:57.88ms
step:989/2330 train_time:57245ms step_avg:57.88ms
step:990/2330 train_time:57305ms step_avg:57.88ms
step:991/2330 train_time:57362ms step_avg:57.88ms
step:992/2330 train_time:57422ms step_avg:57.89ms
step:993/2330 train_time:57478ms step_avg:57.88ms
step:994/2330 train_time:57539ms step_avg:57.89ms
step:995/2330 train_time:57595ms step_avg:57.88ms
step:996/2330 train_time:57656ms step_avg:57.89ms
step:997/2330 train_time:57713ms step_avg:57.89ms
step:998/2330 train_time:57773ms step_avg:57.89ms
step:999/2330 train_time:57831ms step_avg:57.89ms
step:1000/2330 train_time:57891ms step_avg:57.89ms
step:1000/2330 val_loss:4.0692 train_time:57971ms step_avg:57.97ms
step:1001/2330 train_time:57989ms step_avg:57.93ms
step:1002/2330 train_time:58008ms step_avg:57.89ms
step:1003/2330 train_time:58063ms step_avg:57.89ms
step:1004/2330 train_time:58132ms step_avg:57.90ms
step:1005/2330 train_time:58187ms step_avg:57.90ms
step:1006/2330 train_time:58251ms step_avg:57.90ms
step:1007/2330 train_time:58307ms step_avg:57.90ms
step:1008/2330 train_time:58366ms step_avg:57.90ms
step:1009/2330 train_time:58422ms step_avg:57.90ms
step:1010/2330 train_time:58482ms step_avg:57.90ms
step:1011/2330 train_time:58538ms step_avg:57.90ms
step:1012/2330 train_time:58598ms step_avg:57.90ms
step:1013/2330 train_time:58654ms step_avg:57.90ms
step:1014/2330 train_time:58713ms step_avg:57.90ms
step:1015/2330 train_time:58769ms step_avg:57.90ms
step:1016/2330 train_time:58828ms step_avg:57.90ms
step:1017/2330 train_time:58886ms step_avg:57.90ms
step:1018/2330 train_time:58951ms step_avg:57.91ms
step:1019/2330 train_time:59008ms step_avg:57.91ms
step:1020/2330 train_time:59070ms step_avg:57.91ms
step:1021/2330 train_time:59127ms step_avg:57.91ms
step:1022/2330 train_time:59188ms step_avg:57.91ms
step:1023/2330 train_time:59245ms step_avg:57.91ms
step:1024/2330 train_time:59305ms step_avg:57.91ms
step:1025/2330 train_time:59361ms step_avg:57.91ms
step:1026/2330 train_time:59421ms step_avg:57.92ms
step:1027/2330 train_time:59477ms step_avg:57.91ms
step:1028/2330 train_time:59537ms step_avg:57.91ms
step:1029/2330 train_time:59593ms step_avg:57.91ms
step:1030/2330 train_time:59652ms step_avg:57.91ms
step:1031/2330 train_time:59709ms step_avg:57.91ms
step:1032/2330 train_time:59768ms step_avg:57.92ms
step:1033/2330 train_time:59826ms step_avg:57.91ms
step:1034/2330 train_time:59887ms step_avg:57.92ms
step:1035/2330 train_time:59945ms step_avg:57.92ms
step:1036/2330 train_time:60006ms step_avg:57.92ms
step:1037/2330 train_time:60064ms step_avg:57.92ms
step:1038/2330 train_time:60125ms step_avg:57.92ms
step:1039/2330 train_time:60182ms step_avg:57.92ms
step:1040/2330 train_time:60242ms step_avg:57.93ms
step:1041/2330 train_time:60300ms step_avg:57.93ms
step:1042/2330 train_time:60360ms step_avg:57.93ms
step:1043/2330 train_time:60416ms step_avg:57.93ms
step:1044/2330 train_time:60475ms step_avg:57.93ms
step:1045/2330 train_time:60532ms step_avg:57.93ms
step:1046/2330 train_time:60592ms step_avg:57.93ms
step:1047/2330 train_time:60648ms step_avg:57.93ms
step:1048/2330 train_time:60708ms step_avg:57.93ms
step:1049/2330 train_time:60764ms step_avg:57.93ms
step:1050/2330 train_time:60824ms step_avg:57.93ms
step:1051/2330 train_time:60882ms step_avg:57.93ms
step:1052/2330 train_time:60943ms step_avg:57.93ms
step:1053/2330 train_time:61001ms step_avg:57.93ms
step:1054/2330 train_time:61061ms step_avg:57.93ms
step:1055/2330 train_time:61119ms step_avg:57.93ms
step:1056/2330 train_time:61178ms step_avg:57.93ms
step:1057/2330 train_time:61235ms step_avg:57.93ms
step:1058/2330 train_time:61296ms step_avg:57.94ms
step:1059/2330 train_time:61352ms step_avg:57.93ms
step:1060/2330 train_time:61413ms step_avg:57.94ms
step:1061/2330 train_time:61470ms step_avg:57.94ms
step:1062/2330 train_time:61529ms step_avg:57.94ms
step:1063/2330 train_time:61586ms step_avg:57.94ms
step:1064/2330 train_time:61646ms step_avg:57.94ms
step:1065/2330 train_time:61702ms step_avg:57.94ms
step:1066/2330 train_time:61761ms step_avg:57.94ms
step:1067/2330 train_time:61818ms step_avg:57.94ms
step:1068/2330 train_time:61879ms step_avg:57.94ms
step:1069/2330 train_time:61937ms step_avg:57.94ms
step:1070/2330 train_time:61997ms step_avg:57.94ms
step:1071/2330 train_time:62055ms step_avg:57.94ms
step:1072/2330 train_time:62115ms step_avg:57.94ms
step:1073/2330 train_time:62172ms step_avg:57.94ms
step:1074/2330 train_time:62233ms step_avg:57.94ms
step:1075/2330 train_time:62289ms step_avg:57.94ms
step:1076/2330 train_time:62350ms step_avg:57.95ms
step:1077/2330 train_time:62407ms step_avg:57.95ms
step:1078/2330 train_time:62467ms step_avg:57.95ms
step:1079/2330 train_time:62523ms step_avg:57.95ms
step:1080/2330 train_time:62583ms step_avg:57.95ms
step:1081/2330 train_time:62639ms step_avg:57.95ms
step:1082/2330 train_time:62700ms step_avg:57.95ms
step:1083/2330 train_time:62756ms step_avg:57.95ms
step:1084/2330 train_time:62816ms step_avg:57.95ms
step:1085/2330 train_time:62873ms step_avg:57.95ms
step:1086/2330 train_time:62934ms step_avg:57.95ms
step:1087/2330 train_time:62991ms step_avg:57.95ms
step:1088/2330 train_time:63052ms step_avg:57.95ms
step:1089/2330 train_time:63109ms step_avg:57.95ms
step:1090/2330 train_time:63170ms step_avg:57.95ms
step:1091/2330 train_time:63228ms step_avg:57.95ms
step:1092/2330 train_time:63289ms step_avg:57.96ms
step:1093/2330 train_time:63345ms step_avg:57.96ms
step:1094/2330 train_time:63406ms step_avg:57.96ms
step:1095/2330 train_time:63463ms step_avg:57.96ms
step:1096/2330 train_time:63523ms step_avg:57.96ms
step:1097/2330 train_time:63580ms step_avg:57.96ms
step:1098/2330 train_time:63640ms step_avg:57.96ms
step:1099/2330 train_time:63697ms step_avg:57.96ms
step:1100/2330 train_time:63757ms step_avg:57.96ms
step:1101/2330 train_time:63814ms step_avg:57.96ms
step:1102/2330 train_time:63874ms step_avg:57.96ms
step:1103/2330 train_time:63931ms step_avg:57.96ms
step:1104/2330 train_time:63991ms step_avg:57.96ms
step:1105/2330 train_time:64049ms step_avg:57.96ms
step:1106/2330 train_time:64110ms step_avg:57.97ms
step:1107/2330 train_time:64167ms step_avg:57.96ms
step:1108/2330 train_time:64227ms step_avg:57.97ms
step:1109/2330 train_time:64284ms step_avg:57.97ms
step:1110/2330 train_time:64346ms step_avg:57.97ms
step:1111/2330 train_time:64403ms step_avg:57.97ms
step:1112/2330 train_time:64463ms step_avg:57.97ms
step:1113/2330 train_time:64520ms step_avg:57.97ms
step:1114/2330 train_time:64579ms step_avg:57.97ms
step:1115/2330 train_time:64635ms step_avg:57.97ms
step:1116/2330 train_time:64695ms step_avg:57.97ms
step:1117/2330 train_time:64752ms step_avg:57.97ms
step:1118/2330 train_time:64811ms step_avg:57.97ms
step:1119/2330 train_time:64868ms step_avg:57.97ms
step:1120/2330 train_time:64929ms step_avg:57.97ms
step:1121/2330 train_time:64986ms step_avg:57.97ms
step:1122/2330 train_time:65046ms step_avg:57.97ms
step:1123/2330 train_time:65103ms step_avg:57.97ms
step:1124/2330 train_time:65165ms step_avg:57.98ms
step:1125/2330 train_time:65223ms step_avg:57.98ms
step:1126/2330 train_time:65283ms step_avg:57.98ms
step:1127/2330 train_time:65340ms step_avg:57.98ms
step:1128/2330 train_time:65400ms step_avg:57.98ms
step:1129/2330 train_time:65457ms step_avg:57.98ms
step:1130/2330 train_time:65516ms step_avg:57.98ms
step:1131/2330 train_time:65573ms step_avg:57.98ms
step:1132/2330 train_time:65633ms step_avg:57.98ms
step:1133/2330 train_time:65690ms step_avg:57.98ms
step:1134/2330 train_time:65750ms step_avg:57.98ms
step:1135/2330 train_time:65807ms step_avg:57.98ms
step:1136/2330 train_time:65867ms step_avg:57.98ms
step:1137/2330 train_time:65923ms step_avg:57.98ms
step:1138/2330 train_time:65983ms step_avg:57.98ms
step:1139/2330 train_time:66040ms step_avg:57.98ms
step:1140/2330 train_time:66101ms step_avg:57.98ms
step:1141/2330 train_time:66157ms step_avg:57.98ms
step:1142/2330 train_time:66219ms step_avg:57.99ms
step:1143/2330 train_time:66277ms step_avg:57.98ms
step:1144/2330 train_time:66337ms step_avg:57.99ms
step:1145/2330 train_time:66394ms step_avg:57.99ms
step:1146/2330 train_time:66453ms step_avg:57.99ms
step:1147/2330 train_time:66510ms step_avg:57.99ms
step:1148/2330 train_time:66570ms step_avg:57.99ms
step:1149/2330 train_time:66627ms step_avg:57.99ms
step:1150/2330 train_time:66687ms step_avg:57.99ms
step:1151/2330 train_time:66744ms step_avg:57.99ms
step:1152/2330 train_time:66804ms step_avg:57.99ms
step:1153/2330 train_time:66861ms step_avg:57.99ms
step:1154/2330 train_time:66921ms step_avg:57.99ms
step:1155/2330 train_time:66978ms step_avg:57.99ms
step:1156/2330 train_time:67038ms step_avg:57.99ms
step:1157/2330 train_time:67096ms step_avg:57.99ms
step:1158/2330 train_time:67156ms step_avg:57.99ms
step:1159/2330 train_time:67213ms step_avg:57.99ms
step:1160/2330 train_time:67274ms step_avg:57.99ms
step:1161/2330 train_time:67330ms step_avg:57.99ms
step:1162/2330 train_time:67391ms step_avg:58.00ms
step:1163/2330 train_time:67448ms step_avg:57.99ms
step:1164/2330 train_time:67508ms step_avg:58.00ms
step:1165/2330 train_time:67565ms step_avg:58.00ms
step:1166/2330 train_time:67626ms step_avg:58.00ms
step:1167/2330 train_time:67682ms step_avg:58.00ms
step:1168/2330 train_time:67742ms step_avg:58.00ms
step:1169/2330 train_time:67799ms step_avg:58.00ms
step:1170/2330 train_time:67859ms step_avg:58.00ms
step:1171/2330 train_time:67917ms step_avg:58.00ms
step:1172/2330 train_time:67976ms step_avg:58.00ms
step:1173/2330 train_time:68033ms step_avg:58.00ms
step:1174/2330 train_time:68092ms step_avg:58.00ms
step:1175/2330 train_time:68150ms step_avg:58.00ms
step:1176/2330 train_time:68210ms step_avg:58.00ms
step:1177/2330 train_time:68266ms step_avg:58.00ms
step:1178/2330 train_time:68327ms step_avg:58.00ms
step:1179/2330 train_time:68384ms step_avg:58.00ms
step:1180/2330 train_time:68445ms step_avg:58.00ms
step:1181/2330 train_time:68502ms step_avg:58.00ms
step:1182/2330 train_time:68562ms step_avg:58.01ms
step:1183/2330 train_time:68619ms step_avg:58.00ms
step:1184/2330 train_time:68679ms step_avg:58.01ms
step:1185/2330 train_time:68735ms step_avg:58.00ms
step:1186/2330 train_time:68796ms step_avg:58.01ms
step:1187/2330 train_time:68853ms step_avg:58.01ms
step:1188/2330 train_time:68914ms step_avg:58.01ms
step:1189/2330 train_time:68970ms step_avg:58.01ms
step:1190/2330 train_time:69031ms step_avg:58.01ms
step:1191/2330 train_time:69087ms step_avg:58.01ms
step:1192/2330 train_time:69149ms step_avg:58.01ms
step:1193/2330 train_time:69206ms step_avg:58.01ms
step:1194/2330 train_time:69266ms step_avg:58.01ms
step:1195/2330 train_time:69322ms step_avg:58.01ms
step:1196/2330 train_time:69383ms step_avg:58.01ms
step:1197/2330 train_time:69440ms step_avg:58.01ms
step:1198/2330 train_time:69500ms step_avg:58.01ms
step:1199/2330 train_time:69558ms step_avg:58.01ms
step:1200/2330 train_time:69618ms step_avg:58.01ms
step:1201/2330 train_time:69675ms step_avg:58.01ms
step:1202/2330 train_time:69734ms step_avg:58.01ms
step:1203/2330 train_time:69792ms step_avg:58.01ms
step:1204/2330 train_time:69851ms step_avg:58.02ms
step:1205/2330 train_time:69908ms step_avg:58.01ms
step:1206/2330 train_time:69969ms step_avg:58.02ms
step:1207/2330 train_time:70026ms step_avg:58.02ms
step:1208/2330 train_time:70086ms step_avg:58.02ms
step:1209/2330 train_time:70143ms step_avg:58.02ms
step:1210/2330 train_time:70204ms step_avg:58.02ms
step:1211/2330 train_time:70261ms step_avg:58.02ms
step:1212/2330 train_time:70321ms step_avg:58.02ms
step:1213/2330 train_time:70377ms step_avg:58.02ms
step:1214/2330 train_time:70438ms step_avg:58.02ms
step:1215/2330 train_time:70495ms step_avg:58.02ms
step:1216/2330 train_time:70555ms step_avg:58.02ms
step:1217/2330 train_time:70612ms step_avg:58.02ms
step:1218/2330 train_time:70673ms step_avg:58.02ms
step:1219/2330 train_time:70731ms step_avg:58.02ms
step:1220/2330 train_time:70790ms step_avg:58.02ms
step:1221/2330 train_time:70847ms step_avg:58.02ms
step:1222/2330 train_time:70907ms step_avg:58.03ms
step:1223/2330 train_time:70964ms step_avg:58.02ms
step:1224/2330 train_time:71024ms step_avg:58.03ms
step:1225/2330 train_time:71081ms step_avg:58.03ms
step:1226/2330 train_time:71141ms step_avg:58.03ms
step:1227/2330 train_time:71198ms step_avg:58.03ms
step:1228/2330 train_time:71259ms step_avg:58.03ms
step:1229/2330 train_time:71316ms step_avg:58.03ms
step:1230/2330 train_time:71376ms step_avg:58.03ms
step:1231/2330 train_time:71433ms step_avg:58.03ms
step:1232/2330 train_time:71493ms step_avg:58.03ms
step:1233/2330 train_time:71550ms step_avg:58.03ms
step:1234/2330 train_time:71611ms step_avg:58.03ms
step:1235/2330 train_time:71667ms step_avg:58.03ms
step:1236/2330 train_time:71728ms step_avg:58.03ms
step:1237/2330 train_time:71784ms step_avg:58.03ms
step:1238/2330 train_time:71846ms step_avg:58.03ms
step:1239/2330 train_time:71902ms step_avg:58.03ms
step:1240/2330 train_time:71962ms step_avg:58.03ms
step:1241/2330 train_time:72020ms step_avg:58.03ms
step:1242/2330 train_time:72080ms step_avg:58.04ms
step:1243/2330 train_time:72137ms step_avg:58.03ms
step:1244/2330 train_time:72197ms step_avg:58.04ms
step:1245/2330 train_time:72254ms step_avg:58.04ms
step:1246/2330 train_time:72315ms step_avg:58.04ms
step:1247/2330 train_time:72372ms step_avg:58.04ms
step:1248/2330 train_time:72432ms step_avg:58.04ms
step:1249/2330 train_time:72489ms step_avg:58.04ms
step:1250/2330 train_time:72549ms step_avg:58.04ms
step:1250/2330 val_loss:3.9945 train_time:72630ms step_avg:58.10ms
step:1251/2330 train_time:72648ms step_avg:58.07ms
step:1252/2330 train_time:72668ms step_avg:58.04ms
step:1253/2330 train_time:72729ms step_avg:58.04ms
step:1254/2330 train_time:72794ms step_avg:58.05ms
step:1255/2330 train_time:72853ms step_avg:58.05ms
step:1256/2330 train_time:72913ms step_avg:58.05ms
step:1257/2330 train_time:72970ms step_avg:58.05ms
step:1258/2330 train_time:73029ms step_avg:58.05ms
step:1259/2330 train_time:73086ms step_avg:58.05ms
step:1260/2330 train_time:73146ms step_avg:58.05ms
step:1261/2330 train_time:73202ms step_avg:58.05ms
step:1262/2330 train_time:73261ms step_avg:58.05ms
step:1263/2330 train_time:73318ms step_avg:58.05ms
step:1264/2330 train_time:73377ms step_avg:58.05ms
step:1265/2330 train_time:73434ms step_avg:58.05ms
step:1266/2330 train_time:73493ms step_avg:58.05ms
step:1267/2330 train_time:73549ms step_avg:58.05ms
step:1268/2330 train_time:73611ms step_avg:58.05ms
step:1269/2330 train_time:73671ms step_avg:58.05ms
step:1270/2330 train_time:73732ms step_avg:58.06ms
step:1271/2330 train_time:73791ms step_avg:58.06ms
step:1272/2330 train_time:73852ms step_avg:58.06ms
step:1273/2330 train_time:73909ms step_avg:58.06ms
step:1274/2330 train_time:73969ms step_avg:58.06ms
step:1275/2330 train_time:74026ms step_avg:58.06ms
step:1276/2330 train_time:74086ms step_avg:58.06ms
step:1277/2330 train_time:74143ms step_avg:58.06ms
step:1278/2330 train_time:74202ms step_avg:58.06ms
step:1279/2330 train_time:74259ms step_avg:58.06ms
step:1280/2330 train_time:74318ms step_avg:58.06ms
step:1281/2330 train_time:74374ms step_avg:58.06ms
step:1282/2330 train_time:74435ms step_avg:58.06ms
step:1283/2330 train_time:74491ms step_avg:58.06ms
step:1284/2330 train_time:74551ms step_avg:58.06ms
step:1285/2330 train_time:74608ms step_avg:58.06ms
step:1286/2330 train_time:74669ms step_avg:58.06ms
step:1287/2330 train_time:74727ms step_avg:58.06ms
step:1288/2330 train_time:74790ms step_avg:58.07ms
step:1289/2330 train_time:74848ms step_avg:58.07ms
step:1290/2330 train_time:74908ms step_avg:58.07ms
step:1291/2330 train_time:74964ms step_avg:58.07ms
step:1292/2330 train_time:75025ms step_avg:58.07ms
step:1293/2330 train_time:75082ms step_avg:58.07ms
step:1294/2330 train_time:75142ms step_avg:58.07ms
step:1295/2330 train_time:75199ms step_avg:58.07ms
step:1296/2330 train_time:75258ms step_avg:58.07ms
step:1297/2330 train_time:75315ms step_avg:58.07ms
step:1298/2330 train_time:75375ms step_avg:58.07ms
step:1299/2330 train_time:75431ms step_avg:58.07ms
step:1300/2330 train_time:75491ms step_avg:58.07ms
step:1301/2330 train_time:75548ms step_avg:58.07ms
step:1302/2330 train_time:75608ms step_avg:58.07ms
step:1303/2330 train_time:75664ms step_avg:58.07ms
step:1304/2330 train_time:75726ms step_avg:58.07ms
step:1305/2330 train_time:75783ms step_avg:58.07ms
step:1306/2330 train_time:75846ms step_avg:58.07ms
step:1307/2330 train_time:75903ms step_avg:58.07ms
step:1308/2330 train_time:75963ms step_avg:58.08ms
step:1309/2330 train_time:76020ms step_avg:58.08ms
step:1310/2330 train_time:76080ms step_avg:58.08ms
step:1311/2330 train_time:76138ms step_avg:58.08ms
step:1312/2330 train_time:76197ms step_avg:58.08ms
step:1313/2330 train_time:76254ms step_avg:58.08ms
step:1314/2330 train_time:76314ms step_avg:58.08ms
step:1315/2330 train_time:76371ms step_avg:58.08ms
step:1316/2330 train_time:76430ms step_avg:58.08ms
step:1317/2330 train_time:76487ms step_avg:58.08ms
step:1318/2330 train_time:76546ms step_avg:58.08ms
step:1319/2330 train_time:76603ms step_avg:58.08ms
step:1320/2330 train_time:76663ms step_avg:58.08ms
step:1321/2330 train_time:76720ms step_avg:58.08ms
step:1322/2330 train_time:76782ms step_avg:58.08ms
step:1323/2330 train_time:76839ms step_avg:58.08ms
step:1324/2330 train_time:76901ms step_avg:58.08ms
step:1325/2330 train_time:76957ms step_avg:58.08ms
step:1326/2330 train_time:77019ms step_avg:58.08ms
step:1327/2330 train_time:77075ms step_avg:58.08ms
step:1328/2330 train_time:77137ms step_avg:58.08ms
step:1329/2330 train_time:77193ms step_avg:58.08ms
step:1330/2330 train_time:77253ms step_avg:58.09ms
step:1331/2330 train_time:77310ms step_avg:58.08ms
step:1332/2330 train_time:77370ms step_avg:58.09ms
step:1333/2330 train_time:77427ms step_avg:58.08ms
step:1334/2330 train_time:77487ms step_avg:58.09ms
step:1335/2330 train_time:77545ms step_avg:58.09ms
step:1336/2330 train_time:77605ms step_avg:58.09ms
step:1337/2330 train_time:77662ms step_avg:58.09ms
step:1338/2330 train_time:77722ms step_avg:58.09ms
step:1339/2330 train_time:77780ms step_avg:58.09ms
step:1340/2330 train_time:77841ms step_avg:58.09ms
step:1341/2330 train_time:77897ms step_avg:58.09ms
step:1342/2330 train_time:77958ms step_avg:58.09ms
step:1343/2330 train_time:78015ms step_avg:58.09ms
step:1344/2330 train_time:78075ms step_avg:58.09ms
step:1345/2330 train_time:78132ms step_avg:58.09ms
step:1346/2330 train_time:78192ms step_avg:58.09ms
step:1347/2330 train_time:78248ms step_avg:58.09ms
step:1348/2330 train_time:78309ms step_avg:58.09ms
step:1349/2330 train_time:78365ms step_avg:58.09ms
step:1350/2330 train_time:78426ms step_avg:58.09ms
step:1351/2330 train_time:78483ms step_avg:58.09ms
step:1352/2330 train_time:78542ms step_avg:58.09ms
step:1353/2330 train_time:78599ms step_avg:58.09ms
step:1354/2330 train_time:78660ms step_avg:58.09ms
step:1355/2330 train_time:78717ms step_avg:58.09ms
step:1356/2330 train_time:78778ms step_avg:58.10ms
step:1357/2330 train_time:78835ms step_avg:58.10ms
step:1358/2330 train_time:78895ms step_avg:58.10ms
step:1359/2330 train_time:78952ms step_avg:58.10ms
step:1360/2330 train_time:79013ms step_avg:58.10ms
step:1361/2330 train_time:79069ms step_avg:58.10ms
step:1362/2330 train_time:79129ms step_avg:58.10ms
step:1363/2330 train_time:79187ms step_avg:58.10ms
step:1364/2330 train_time:79247ms step_avg:58.10ms
step:1365/2330 train_time:79304ms step_avg:58.10ms
step:1366/2330 train_time:79364ms step_avg:58.10ms
step:1367/2330 train_time:79422ms step_avg:58.10ms
step:1368/2330 train_time:79481ms step_avg:58.10ms
step:1369/2330 train_time:79539ms step_avg:58.10ms
step:1370/2330 train_time:79599ms step_avg:58.10ms
step:1371/2330 train_time:79656ms step_avg:58.10ms
step:1372/2330 train_time:79716ms step_avg:58.10ms
step:1373/2330 train_time:79773ms step_avg:58.10ms
step:1374/2330 train_time:79834ms step_avg:58.10ms
step:1375/2330 train_time:79891ms step_avg:58.10ms
step:1376/2330 train_time:79951ms step_avg:58.10ms
step:1377/2330 train_time:80008ms step_avg:58.10ms
step:1378/2330 train_time:80069ms step_avg:58.10ms
step:1379/2330 train_time:80126ms step_avg:58.10ms
step:1380/2330 train_time:80186ms step_avg:58.11ms
step:1381/2330 train_time:80242ms step_avg:58.10ms
step:1382/2330 train_time:80302ms step_avg:58.11ms
step:1383/2330 train_time:80360ms step_avg:58.11ms
step:1384/2330 train_time:80419ms step_avg:58.11ms
step:1385/2330 train_time:80476ms step_avg:58.11ms
step:1386/2330 train_time:80537ms step_avg:58.11ms
step:1387/2330 train_time:80593ms step_avg:58.11ms
step:1388/2330 train_time:80655ms step_avg:58.11ms
step:1389/2330 train_time:80711ms step_avg:58.11ms
step:1390/2330 train_time:80772ms step_avg:58.11ms
step:1391/2330 train_time:80829ms step_avg:58.11ms
step:1392/2330 train_time:80889ms step_avg:58.11ms
step:1393/2330 train_time:80946ms step_avg:58.11ms
step:1394/2330 train_time:81006ms step_avg:58.11ms
step:1395/2330 train_time:81064ms step_avg:58.11ms
step:1396/2330 train_time:81124ms step_avg:58.11ms
step:1397/2330 train_time:81181ms step_avg:58.11ms
step:1398/2330 train_time:81241ms step_avg:58.11ms
step:1399/2330 train_time:81298ms step_avg:58.11ms
step:1400/2330 train_time:81360ms step_avg:58.11ms
step:1401/2330 train_time:81417ms step_avg:58.11ms
step:1402/2330 train_time:81477ms step_avg:58.11ms
step:1403/2330 train_time:81534ms step_avg:58.11ms
step:1404/2330 train_time:81594ms step_avg:58.12ms
step:1405/2330 train_time:81651ms step_avg:58.11ms
step:1406/2330 train_time:81712ms step_avg:58.12ms
step:1407/2330 train_time:81769ms step_avg:58.12ms
step:1408/2330 train_time:81829ms step_avg:58.12ms
step:1409/2330 train_time:81886ms step_avg:58.12ms
step:1410/2330 train_time:81946ms step_avg:58.12ms
step:1411/2330 train_time:82003ms step_avg:58.12ms
step:1412/2330 train_time:82063ms step_avg:58.12ms
step:1413/2330 train_time:82120ms step_avg:58.12ms
step:1414/2330 train_time:82181ms step_avg:58.12ms
step:1415/2330 train_time:82238ms step_avg:58.12ms
step:1416/2330 train_time:82298ms step_avg:58.12ms
step:1417/2330 train_time:82356ms step_avg:58.12ms
step:1418/2330 train_time:82416ms step_avg:58.12ms
step:1419/2330 train_time:82472ms step_avg:58.12ms
step:1420/2330 train_time:82532ms step_avg:58.12ms
step:1421/2330 train_time:82589ms step_avg:58.12ms
step:1422/2330 train_time:82650ms step_avg:58.12ms
step:1423/2330 train_time:82707ms step_avg:58.12ms
step:1424/2330 train_time:82768ms step_avg:58.12ms
step:1425/2330 train_time:82825ms step_avg:58.12ms
step:1426/2330 train_time:82885ms step_avg:58.12ms
step:1427/2330 train_time:82942ms step_avg:58.12ms
step:1428/2330 train_time:83002ms step_avg:58.12ms
step:1429/2330 train_time:83059ms step_avg:58.12ms
step:1430/2330 train_time:83119ms step_avg:58.13ms
step:1431/2330 train_time:83176ms step_avg:58.12ms
step:1432/2330 train_time:83236ms step_avg:58.13ms
step:1433/2330 train_time:83292ms step_avg:58.12ms
step:1434/2330 train_time:83353ms step_avg:58.13ms
step:1435/2330 train_time:83409ms step_avg:58.12ms
step:1436/2330 train_time:83470ms step_avg:58.13ms
step:1437/2330 train_time:83528ms step_avg:58.13ms
step:1438/2330 train_time:83588ms step_avg:58.13ms
step:1439/2330 train_time:83645ms step_avg:58.13ms
step:1440/2330 train_time:83705ms step_avg:58.13ms
step:1441/2330 train_time:83761ms step_avg:58.13ms
step:1442/2330 train_time:83823ms step_avg:58.13ms
step:1443/2330 train_time:83879ms step_avg:58.13ms
step:1444/2330 train_time:83940ms step_avg:58.13ms
step:1445/2330 train_time:83997ms step_avg:58.13ms
step:1446/2330 train_time:84057ms step_avg:58.13ms
step:1447/2330 train_time:84114ms step_avg:58.13ms
step:1448/2330 train_time:84175ms step_avg:58.13ms
step:1449/2330 train_time:84232ms step_avg:58.13ms
step:1450/2330 train_time:84293ms step_avg:58.13ms
step:1451/2330 train_time:84350ms step_avg:58.13ms
step:1452/2330 train_time:84411ms step_avg:58.13ms
step:1453/2330 train_time:84467ms step_avg:58.13ms
step:1454/2330 train_time:84528ms step_avg:58.13ms
step:1455/2330 train_time:84586ms step_avg:58.13ms
step:1456/2330 train_time:84645ms step_avg:58.14ms
step:1457/2330 train_time:84702ms step_avg:58.13ms
step:1458/2330 train_time:84763ms step_avg:58.14ms
step:1459/2330 train_time:84820ms step_avg:58.14ms
step:1460/2330 train_time:84880ms step_avg:58.14ms
step:1461/2330 train_time:84937ms step_avg:58.14ms
step:1462/2330 train_time:84997ms step_avg:58.14ms
step:1463/2330 train_time:85054ms step_avg:58.14ms
step:1464/2330 train_time:85115ms step_avg:58.14ms
step:1465/2330 train_time:85171ms step_avg:58.14ms
step:1466/2330 train_time:85233ms step_avg:58.14ms
step:1467/2330 train_time:85289ms step_avg:58.14ms
step:1468/2330 train_time:85350ms step_avg:58.14ms
step:1469/2330 train_time:85407ms step_avg:58.14ms
step:1470/2330 train_time:85468ms step_avg:58.14ms
step:1471/2330 train_time:85525ms step_avg:58.14ms
step:1472/2330 train_time:85585ms step_avg:58.14ms
step:1473/2330 train_time:85642ms step_avg:58.14ms
step:1474/2330 train_time:85703ms step_avg:58.14ms
step:1475/2330 train_time:85760ms step_avg:58.14ms
step:1476/2330 train_time:85820ms step_avg:58.14ms
step:1477/2330 train_time:85877ms step_avg:58.14ms
step:1478/2330 train_time:85937ms step_avg:58.14ms
step:1479/2330 train_time:85994ms step_avg:58.14ms
step:1480/2330 train_time:86055ms step_avg:58.15ms
step:1481/2330 train_time:86112ms step_avg:58.14ms
step:1482/2330 train_time:86171ms step_avg:58.15ms
step:1483/2330 train_time:86228ms step_avg:58.14ms
step:1484/2330 train_time:86288ms step_avg:58.15ms
step:1485/2330 train_time:86345ms step_avg:58.14ms
step:1486/2330 train_time:86406ms step_avg:58.15ms
step:1487/2330 train_time:86463ms step_avg:58.15ms
step:1488/2330 train_time:86524ms step_avg:58.15ms
step:1489/2330 train_time:86580ms step_avg:58.15ms
step:1490/2330 train_time:86641ms step_avg:58.15ms
step:1491/2330 train_time:86698ms step_avg:58.15ms
step:1492/2330 train_time:86758ms step_avg:58.15ms
step:1493/2330 train_time:86815ms step_avg:58.15ms
step:1494/2330 train_time:86876ms step_avg:58.15ms
step:1495/2330 train_time:86933ms step_avg:58.15ms
step:1496/2330 train_time:86993ms step_avg:58.15ms
step:1497/2330 train_time:87051ms step_avg:58.15ms
step:1498/2330 train_time:87111ms step_avg:58.15ms
step:1499/2330 train_time:87168ms step_avg:58.15ms
step:1500/2330 train_time:87228ms step_avg:58.15ms
step:1500/2330 val_loss:3.9032 train_time:87309ms step_avg:58.21ms
step:1501/2330 train_time:87326ms step_avg:58.18ms
step:1502/2330 train_time:87347ms step_avg:58.15ms
step:1503/2330 train_time:87407ms step_avg:58.16ms
step:1504/2330 train_time:87473ms step_avg:58.16ms
step:1505/2330 train_time:87529ms step_avg:58.16ms
step:1506/2330 train_time:87593ms step_avg:58.16ms
step:1507/2330 train_time:87650ms step_avg:58.16ms
step:1508/2330 train_time:87709ms step_avg:58.16ms
step:1509/2330 train_time:87766ms step_avg:58.16ms
step:1510/2330 train_time:87826ms step_avg:58.16ms
step:1511/2330 train_time:87883ms step_avg:58.16ms
step:1512/2330 train_time:87942ms step_avg:58.16ms
step:1513/2330 train_time:87999ms step_avg:58.16ms
step:1514/2330 train_time:88058ms step_avg:58.16ms
step:1515/2330 train_time:88114ms step_avg:58.16ms
step:1516/2330 train_time:88173ms step_avg:58.16ms
step:1517/2330 train_time:88230ms step_avg:58.16ms
step:1518/2330 train_time:88291ms step_avg:58.16ms
step:1519/2330 train_time:88348ms step_avg:58.16ms
step:1520/2330 train_time:88411ms step_avg:58.16ms
step:1521/2330 train_time:88469ms step_avg:58.17ms
step:1522/2330 train_time:88531ms step_avg:58.17ms
step:1523/2330 train_time:88588ms step_avg:58.17ms
step:1524/2330 train_time:88649ms step_avg:58.17ms
step:1525/2330 train_time:88706ms step_avg:58.17ms
step:1526/2330 train_time:88766ms step_avg:58.17ms
step:1527/2330 train_time:88823ms step_avg:58.17ms
step:1528/2330 train_time:88883ms step_avg:58.17ms
step:1529/2330 train_time:88941ms step_avg:58.17ms
step:1530/2330 train_time:89000ms step_avg:58.17ms
step:1531/2330 train_time:89057ms step_avg:58.17ms
step:1532/2330 train_time:89117ms step_avg:58.17ms
step:1533/2330 train_time:89174ms step_avg:58.17ms
step:1534/2330 train_time:89234ms step_avg:58.17ms
step:1535/2330 train_time:89292ms step_avg:58.17ms
step:1536/2330 train_time:89353ms step_avg:58.17ms
step:1537/2330 train_time:89411ms step_avg:58.17ms
step:1538/2330 train_time:89474ms step_avg:58.18ms
step:1539/2330 train_time:89532ms step_avg:58.18ms
step:1540/2330 train_time:89594ms step_avg:58.18ms
step:1541/2330 train_time:89652ms step_avg:58.18ms
step:1542/2330 train_time:89713ms step_avg:58.18ms
step:1543/2330 train_time:89770ms step_avg:58.18ms
step:1544/2330 train_time:89832ms step_avg:58.18ms
step:1545/2330 train_time:89890ms step_avg:58.18ms
step:1546/2330 train_time:89950ms step_avg:58.18ms
step:1547/2330 train_time:90007ms step_avg:58.18ms
step:1548/2330 train_time:90068ms step_avg:58.18ms
step:1549/2330 train_time:90125ms step_avg:58.18ms
step:1550/2330 train_time:90185ms step_avg:58.18ms
step:1551/2330 train_time:90242ms step_avg:58.18ms
step:1552/2330 train_time:90304ms step_avg:58.19ms
step:1553/2330 train_time:90361ms step_avg:58.18ms
step:1554/2330 train_time:90423ms step_avg:58.19ms
step:1555/2330 train_time:90479ms step_avg:58.19ms
step:1556/2330 train_time:90542ms step_avg:58.19ms
step:1557/2330 train_time:90600ms step_avg:58.19ms
step:1558/2330 train_time:90662ms step_avg:58.19ms
step:1559/2330 train_time:90719ms step_avg:58.19ms
step:1560/2330 train_time:90781ms step_avg:58.19ms
step:1561/2330 train_time:90838ms step_avg:58.19ms
step:1562/2330 train_time:90900ms step_avg:58.19ms
step:1563/2330 train_time:90957ms step_avg:58.19ms
step:1564/2330 train_time:91017ms step_avg:58.19ms
step:1565/2330 train_time:91074ms step_avg:58.19ms
step:1566/2330 train_time:91136ms step_avg:58.20ms
step:1567/2330 train_time:91193ms step_avg:58.20ms
step:1568/2330 train_time:91254ms step_avg:58.20ms
step:1569/2330 train_time:91311ms step_avg:58.20ms
step:1570/2330 train_time:91372ms step_avg:58.20ms
step:1571/2330 train_time:91430ms step_avg:58.20ms
step:1572/2330 train_time:91491ms step_avg:58.20ms
step:1573/2330 train_time:91551ms step_avg:58.20ms
step:1574/2330 train_time:91611ms step_avg:58.20ms
step:1575/2330 train_time:91669ms step_avg:58.20ms
step:1576/2330 train_time:91730ms step_avg:58.20ms
step:1577/2330 train_time:91789ms step_avg:58.20ms
step:1578/2330 train_time:91849ms step_avg:58.21ms
step:1579/2330 train_time:91907ms step_avg:58.21ms
step:1580/2330 train_time:91967ms step_avg:58.21ms
step:1581/2330 train_time:92024ms step_avg:58.21ms
step:1582/2330 train_time:92085ms step_avg:58.21ms
step:1583/2330 train_time:92141ms step_avg:58.21ms
step:1584/2330 train_time:92204ms step_avg:58.21ms
step:1585/2330 train_time:92261ms step_avg:58.21ms
step:1586/2330 train_time:92322ms step_avg:58.21ms
step:1587/2330 train_time:92379ms step_avg:58.21ms
step:1588/2330 train_time:92443ms step_avg:58.21ms
step:1589/2330 train_time:92499ms step_avg:58.21ms
step:1590/2330 train_time:92562ms step_avg:58.22ms
step:1591/2330 train_time:92619ms step_avg:58.21ms
step:1592/2330 train_time:92682ms step_avg:58.22ms
step:1593/2330 train_time:92739ms step_avg:58.22ms
step:1594/2330 train_time:92801ms step_avg:58.22ms
step:1595/2330 train_time:92858ms step_avg:58.22ms
step:1596/2330 train_time:92920ms step_avg:58.22ms
step:1597/2330 train_time:92976ms step_avg:58.22ms
step:1598/2330 train_time:93037ms step_avg:58.22ms
step:1599/2330 train_time:93094ms step_avg:58.22ms
step:1600/2330 train_time:93156ms step_avg:58.22ms
step:1601/2330 train_time:93213ms step_avg:58.22ms
step:1602/2330 train_time:93273ms step_avg:58.22ms
step:1603/2330 train_time:93331ms step_avg:58.22ms
step:1604/2330 train_time:93393ms step_avg:58.22ms
step:1605/2330 train_time:93451ms step_avg:58.22ms
step:1606/2330 train_time:93511ms step_avg:58.23ms
step:1607/2330 train_time:93570ms step_avg:58.23ms
step:1608/2330 train_time:93630ms step_avg:58.23ms
step:1609/2330 train_time:93688ms step_avg:58.23ms
step:1610/2330 train_time:93749ms step_avg:58.23ms
step:1611/2330 train_time:93807ms step_avg:58.23ms
step:1612/2330 train_time:93867ms step_avg:58.23ms
step:1613/2330 train_time:93924ms step_avg:58.23ms
step:1614/2330 train_time:93985ms step_avg:58.23ms
step:1615/2330 train_time:94042ms step_avg:58.23ms
step:1616/2330 train_time:94104ms step_avg:58.23ms
step:1617/2330 train_time:94161ms step_avg:58.23ms
step:1618/2330 train_time:94224ms step_avg:58.23ms
step:1619/2330 train_time:94280ms step_avg:58.23ms
step:1620/2330 train_time:94342ms step_avg:58.24ms
step:1621/2330 train_time:94399ms step_avg:58.24ms
step:1622/2330 train_time:94462ms step_avg:58.24ms
step:1623/2330 train_time:94519ms step_avg:58.24ms
step:1624/2330 train_time:94581ms step_avg:58.24ms
step:1625/2330 train_time:94638ms step_avg:58.24ms
step:1626/2330 train_time:94700ms step_avg:58.24ms
step:1627/2330 train_time:94757ms step_avg:58.24ms
step:1628/2330 train_time:94817ms step_avg:58.24ms
step:1629/2330 train_time:94875ms step_avg:58.24ms
step:1630/2330 train_time:94936ms step_avg:58.24ms
step:1631/2330 train_time:94993ms step_avg:58.24ms
step:1632/2330 train_time:95054ms step_avg:58.24ms
step:1633/2330 train_time:95112ms step_avg:58.24ms
step:1634/2330 train_time:95173ms step_avg:58.25ms
step:1635/2330 train_time:95231ms step_avg:58.25ms
step:1636/2330 train_time:95292ms step_avg:58.25ms
step:1637/2330 train_time:95350ms step_avg:58.25ms
step:1638/2330 train_time:95411ms step_avg:58.25ms
step:1639/2330 train_time:95469ms step_avg:58.25ms
step:1640/2330 train_time:95530ms step_avg:58.25ms
step:1641/2330 train_time:95587ms step_avg:58.25ms
step:1642/2330 train_time:95648ms step_avg:58.25ms
step:1643/2330 train_time:95707ms step_avg:58.25ms
step:1644/2330 train_time:95768ms step_avg:58.25ms
step:1645/2330 train_time:95824ms step_avg:58.25ms
step:1646/2330 train_time:95885ms step_avg:58.25ms
step:1647/2330 train_time:95942ms step_avg:58.25ms
step:1648/2330 train_time:96003ms step_avg:58.25ms
step:1649/2330 train_time:96060ms step_avg:58.25ms
step:1650/2330 train_time:96122ms step_avg:58.26ms
step:1651/2330 train_time:96179ms step_avg:58.26ms
step:1652/2330 train_time:96241ms step_avg:58.26ms
step:1653/2330 train_time:96298ms step_avg:58.26ms
step:1654/2330 train_time:96360ms step_avg:58.26ms
step:1655/2330 train_time:96417ms step_avg:58.26ms
step:1656/2330 train_time:96478ms step_avg:58.26ms
step:1657/2330 train_time:96535ms step_avg:58.26ms
step:1658/2330 train_time:96597ms step_avg:58.26ms
step:1659/2330 train_time:96654ms step_avg:58.26ms
step:1660/2330 train_time:96715ms step_avg:58.26ms
step:1661/2330 train_time:96773ms step_avg:58.26ms
step:1662/2330 train_time:96834ms step_avg:58.26ms
step:1663/2330 train_time:96891ms step_avg:58.26ms
step:1664/2330 train_time:96952ms step_avg:58.26ms
step:1665/2330 train_time:97010ms step_avg:58.26ms
step:1666/2330 train_time:97071ms step_avg:58.27ms
step:1667/2330 train_time:97128ms step_avg:58.27ms
step:1668/2330 train_time:97191ms step_avg:58.27ms
step:1669/2330 train_time:97249ms step_avg:58.27ms
step:1670/2330 train_time:97310ms step_avg:58.27ms
step:1671/2330 train_time:97367ms step_avg:58.27ms
step:1672/2330 train_time:97429ms step_avg:58.27ms
step:1673/2330 train_time:97488ms step_avg:58.27ms
step:1674/2330 train_time:97549ms step_avg:58.27ms
step:1675/2330 train_time:97607ms step_avg:58.27ms
step:1676/2330 train_time:97667ms step_avg:58.27ms
step:1677/2330 train_time:97723ms step_avg:58.27ms
step:1678/2330 train_time:97785ms step_avg:58.27ms
step:1679/2330 train_time:97842ms step_avg:58.27ms
step:1680/2330 train_time:97903ms step_avg:58.28ms
step:1681/2330 train_time:97960ms step_avg:58.28ms
step:1682/2330 train_time:98021ms step_avg:58.28ms
step:1683/2330 train_time:98078ms step_avg:58.28ms
step:1684/2330 train_time:98141ms step_avg:58.28ms
step:1685/2330 train_time:98198ms step_avg:58.28ms
step:1686/2330 train_time:98261ms step_avg:58.28ms
step:1687/2330 train_time:98317ms step_avg:58.28ms
step:1688/2330 train_time:98380ms step_avg:58.28ms
step:1689/2330 train_time:98437ms step_avg:58.28ms
step:1690/2330 train_time:98499ms step_avg:58.28ms
step:1691/2330 train_time:98556ms step_avg:58.28ms
step:1692/2330 train_time:98617ms step_avg:58.28ms
step:1693/2330 train_time:98674ms step_avg:58.28ms
step:1694/2330 train_time:98736ms step_avg:58.29ms
step:1695/2330 train_time:98794ms step_avg:58.29ms
step:1696/2330 train_time:98854ms step_avg:58.29ms
step:1697/2330 train_time:98912ms step_avg:58.29ms
step:1698/2330 train_time:98973ms step_avg:58.29ms
step:1699/2330 train_time:99032ms step_avg:58.29ms
step:1700/2330 train_time:99092ms step_avg:58.29ms
step:1701/2330 train_time:99151ms step_avg:58.29ms
step:1702/2330 train_time:99211ms step_avg:58.29ms
step:1703/2330 train_time:99268ms step_avg:58.29ms
step:1704/2330 train_time:99329ms step_avg:58.29ms
step:1705/2330 train_time:99388ms step_avg:58.29ms
step:1706/2330 train_time:99449ms step_avg:58.29ms
step:1707/2330 train_time:99506ms step_avg:58.29ms
step:1708/2330 train_time:99568ms step_avg:58.30ms
step:1709/2330 train_time:99626ms step_avg:58.29ms
step:1710/2330 train_time:99687ms step_avg:58.30ms
step:1711/2330 train_time:99745ms step_avg:58.30ms
step:1712/2330 train_time:99805ms step_avg:58.30ms
step:1713/2330 train_time:99863ms step_avg:58.30ms
step:1714/2330 train_time:99924ms step_avg:58.30ms
step:1715/2330 train_time:99981ms step_avg:58.30ms
step:1716/2330 train_time:100043ms step_avg:58.30ms
step:1717/2330 train_time:100101ms step_avg:58.30ms
step:1718/2330 train_time:100162ms step_avg:58.30ms
step:1719/2330 train_time:100218ms step_avg:58.30ms
step:1720/2330 train_time:100281ms step_avg:58.30ms
step:1721/2330 train_time:100337ms step_avg:58.30ms
step:1722/2330 train_time:100399ms step_avg:58.30ms
step:1723/2330 train_time:100456ms step_avg:58.30ms
step:1724/2330 train_time:100517ms step_avg:58.30ms
step:1725/2330 train_time:100574ms step_avg:58.30ms
step:1726/2330 train_time:100636ms step_avg:58.31ms
step:1727/2330 train_time:100693ms step_avg:58.31ms
step:1728/2330 train_time:100754ms step_avg:58.31ms
step:1729/2330 train_time:100811ms step_avg:58.31ms
step:1730/2330 train_time:100873ms step_avg:58.31ms
step:1731/2330 train_time:100930ms step_avg:58.31ms
step:1732/2330 train_time:100993ms step_avg:58.31ms
step:1733/2330 train_time:101052ms step_avg:58.31ms
step:1734/2330 train_time:101113ms step_avg:58.31ms
step:1735/2330 train_time:101170ms step_avg:58.31ms
step:1736/2330 train_time:101231ms step_avg:58.31ms
step:1737/2330 train_time:101289ms step_avg:58.31ms
step:1738/2330 train_time:101349ms step_avg:58.31ms
step:1739/2330 train_time:101407ms step_avg:58.31ms
step:1740/2330 train_time:101468ms step_avg:58.31ms
step:1741/2330 train_time:101526ms step_avg:58.31ms
step:1742/2330 train_time:101586ms step_avg:58.32ms
step:1743/2330 train_time:101644ms step_avg:58.32ms
step:1744/2330 train_time:101705ms step_avg:58.32ms
step:1745/2330 train_time:101763ms step_avg:58.32ms
step:1746/2330 train_time:101823ms step_avg:58.32ms
step:1747/2330 train_time:101880ms step_avg:58.32ms
step:1748/2330 train_time:101942ms step_avg:58.32ms
step:1749/2330 train_time:102000ms step_avg:58.32ms
step:1750/2330 train_time:102061ms step_avg:58.32ms
step:1750/2330 val_loss:3.8185 train_time:102144ms step_avg:58.37ms
step:1751/2330 train_time:102161ms step_avg:58.34ms
step:1752/2330 train_time:102181ms step_avg:58.32ms
step:1753/2330 train_time:102238ms step_avg:58.32ms
step:1754/2330 train_time:102306ms step_avg:58.33ms
step:1755/2330 train_time:102362ms step_avg:58.33ms
step:1756/2330 train_time:102428ms step_avg:58.33ms
step:1757/2330 train_time:102484ms step_avg:58.33ms
step:1758/2330 train_time:102547ms step_avg:58.33ms
step:1759/2330 train_time:102603ms step_avg:58.33ms
step:1760/2330 train_time:102665ms step_avg:58.33ms
step:1761/2330 train_time:102722ms step_avg:58.33ms
step:1762/2330 train_time:102782ms step_avg:58.33ms
step:1763/2330 train_time:102839ms step_avg:58.33ms
step:1764/2330 train_time:102899ms step_avg:58.33ms
step:1765/2330 train_time:102955ms step_avg:58.33ms
step:1766/2330 train_time:103015ms step_avg:58.33ms
step:1767/2330 train_time:103073ms step_avg:58.33ms
step:1768/2330 train_time:103137ms step_avg:58.34ms
step:1769/2330 train_time:103195ms step_avg:58.34ms
step:1770/2330 train_time:103258ms step_avg:58.34ms
step:1771/2330 train_time:103315ms step_avg:58.34ms
step:1772/2330 train_time:103377ms step_avg:58.34ms
step:1773/2330 train_time:103434ms step_avg:58.34ms
step:1774/2330 train_time:103495ms step_avg:58.34ms
step:1775/2330 train_time:103551ms step_avg:58.34ms
step:1776/2330 train_time:103613ms step_avg:58.34ms
step:1777/2330 train_time:103671ms step_avg:58.34ms
step:1778/2330 train_time:103733ms step_avg:58.34ms
step:1779/2330 train_time:103790ms step_avg:58.34ms
step:1780/2330 train_time:103850ms step_avg:58.34ms
step:1781/2330 train_time:103908ms step_avg:58.34ms
step:1782/2330 train_time:103968ms step_avg:58.34ms
step:1783/2330 train_time:104026ms step_avg:58.34ms
step:1784/2330 train_time:104087ms step_avg:58.34ms
step:1785/2330 train_time:104146ms step_avg:58.34ms
step:1786/2330 train_time:104205ms step_avg:58.35ms
step:1787/2330 train_time:104263ms step_avg:58.35ms
step:1788/2330 train_time:104324ms step_avg:58.35ms
step:1789/2330 train_time:104381ms step_avg:58.35ms
step:1790/2330 train_time:104444ms step_avg:58.35ms
step:1791/2330 train_time:104500ms step_avg:58.35ms
step:1792/2330 train_time:104562ms step_avg:58.35ms
step:1793/2330 train_time:104619ms step_avg:58.35ms
step:1794/2330 train_time:104681ms step_avg:58.35ms
step:1795/2330 train_time:104737ms step_avg:58.35ms
step:1796/2330 train_time:104798ms step_avg:58.35ms
step:1797/2330 train_time:104854ms step_avg:58.35ms
step:1798/2330 train_time:104915ms step_avg:58.35ms
step:1799/2330 train_time:104972ms step_avg:58.35ms
step:1800/2330 train_time:105033ms step_avg:58.35ms
step:1801/2330 train_time:105091ms step_avg:58.35ms
step:1802/2330 train_time:105152ms step_avg:58.35ms
step:1803/2330 train_time:105210ms step_avg:58.35ms
step:1804/2330 train_time:105271ms step_avg:58.35ms
step:1805/2330 train_time:105330ms step_avg:58.35ms
step:1806/2330 train_time:105390ms step_avg:58.36ms
step:1807/2330 train_time:105449ms step_avg:58.36ms
step:1808/2330 train_time:105509ms step_avg:58.36ms
step:1809/2330 train_time:105567ms step_avg:58.36ms
step:1810/2330 train_time:105628ms step_avg:58.36ms
step:1811/2330 train_time:105686ms step_avg:58.36ms
step:1812/2330 train_time:105745ms step_avg:58.36ms
step:1813/2330 train_time:105802ms step_avg:58.36ms
step:1814/2330 train_time:105864ms step_avg:58.36ms
step:1815/2330 train_time:105920ms step_avg:58.36ms
step:1816/2330 train_time:105982ms step_avg:58.36ms
step:1817/2330 train_time:106039ms step_avg:58.36ms
step:1818/2330 train_time:106101ms step_avg:58.36ms
step:1819/2330 train_time:106158ms step_avg:58.36ms
step:1820/2330 train_time:106220ms step_avg:58.36ms
step:1821/2330 train_time:106277ms step_avg:58.36ms
step:1822/2330 train_time:106338ms step_avg:58.36ms
step:1823/2330 train_time:106396ms step_avg:58.36ms
step:1824/2330 train_time:106457ms step_avg:58.36ms
step:1825/2330 train_time:106514ms step_avg:58.36ms
step:1826/2330 train_time:106576ms step_avg:58.37ms
step:1827/2330 train_time:106633ms step_avg:58.36ms
step:1828/2330 train_time:106694ms step_avg:58.37ms
step:1829/2330 train_time:106753ms step_avg:58.37ms
step:1830/2330 train_time:106813ms step_avg:58.37ms
step:1831/2330 train_time:106870ms step_avg:58.37ms
step:1832/2330 train_time:106931ms step_avg:58.37ms
step:1833/2330 train_time:106989ms step_avg:58.37ms
step:1834/2330 train_time:107049ms step_avg:58.37ms
step:1835/2330 train_time:107106ms step_avg:58.37ms
step:1836/2330 train_time:107167ms step_avg:58.37ms
step:1837/2330 train_time:107226ms step_avg:58.37ms
step:1838/2330 train_time:107286ms step_avg:58.37ms
step:1839/2330 train_time:107343ms step_avg:58.37ms
step:1840/2330 train_time:107406ms step_avg:58.37ms
step:1841/2330 train_time:107462ms step_avg:58.37ms
step:1842/2330 train_time:107525ms step_avg:58.37ms
step:1843/2330 train_time:107581ms step_avg:58.37ms
step:1844/2330 train_time:107644ms step_avg:58.38ms
step:1845/2330 train_time:107700ms step_avg:58.37ms
step:1846/2330 train_time:107763ms step_avg:58.38ms
step:1847/2330 train_time:107819ms step_avg:58.38ms
step:1848/2330 train_time:107881ms step_avg:58.38ms
step:1849/2330 train_time:107938ms step_avg:58.38ms
step:1850/2330 train_time:107998ms step_avg:58.38ms
step:1851/2330 train_time:108055ms step_avg:58.38ms
step:1852/2330 train_time:108117ms step_avg:58.38ms
step:1853/2330 train_time:108174ms step_avg:58.38ms
step:1854/2330 train_time:108235ms step_avg:58.38ms
step:1855/2330 train_time:108293ms step_avg:58.38ms
step:1856/2330 train_time:108353ms step_avg:58.38ms
step:1857/2330 train_time:108410ms step_avg:58.38ms
step:1858/2330 train_time:108473ms step_avg:58.38ms
step:1859/2330 train_time:108530ms step_avg:58.38ms
step:1860/2330 train_time:108591ms step_avg:58.38ms
step:1861/2330 train_time:108648ms step_avg:58.38ms
step:1862/2330 train_time:108709ms step_avg:58.38ms
step:1863/2330 train_time:108766ms step_avg:58.38ms
step:1864/2330 train_time:108826ms step_avg:58.38ms
step:1865/2330 train_time:108883ms step_avg:58.38ms
step:1866/2330 train_time:108945ms step_avg:58.38ms
step:1867/2330 train_time:109001ms step_avg:58.38ms
step:1868/2330 train_time:109063ms step_avg:58.39ms
step:1869/2330 train_time:109120ms step_avg:58.38ms
step:1870/2330 train_time:109182ms step_avg:58.39ms
step:1871/2330 train_time:109239ms step_avg:58.39ms
step:1872/2330 train_time:109301ms step_avg:58.39ms
step:1873/2330 train_time:109357ms step_avg:58.39ms
step:1874/2330 train_time:109420ms step_avg:58.39ms
step:1875/2330 train_time:109477ms step_avg:58.39ms
step:1876/2330 train_time:109539ms step_avg:58.39ms
step:1877/2330 train_time:109595ms step_avg:58.39ms
step:1878/2330 train_time:109657ms step_avg:58.39ms
step:1879/2330 train_time:109714ms step_avg:58.39ms
step:1880/2330 train_time:109775ms step_avg:58.39ms
step:1881/2330 train_time:109832ms step_avg:58.39ms
step:1882/2330 train_time:109894ms step_avg:58.39ms
step:1883/2330 train_time:109951ms step_avg:58.39ms
step:1884/2330 train_time:110013ms step_avg:58.39ms
step:1885/2330 train_time:110071ms step_avg:58.39ms
step:1886/2330 train_time:110132ms step_avg:58.39ms
step:1887/2330 train_time:110191ms step_avg:58.39ms
step:1888/2330 train_time:110251ms step_avg:58.40ms
step:1889/2330 train_time:110309ms step_avg:58.40ms
step:1890/2330 train_time:110370ms step_avg:58.40ms
step:1891/2330 train_time:110428ms step_avg:58.40ms
step:1892/2330 train_time:110489ms step_avg:58.40ms
step:1893/2330 train_time:110546ms step_avg:58.40ms
step:1894/2330 train_time:110608ms step_avg:58.40ms
step:1895/2330 train_time:110664ms step_avg:58.40ms
step:1896/2330 train_time:110726ms step_avg:58.40ms
step:1897/2330 train_time:110783ms step_avg:58.40ms
step:1898/2330 train_time:110844ms step_avg:58.40ms
step:1899/2330 train_time:110901ms step_avg:58.40ms
step:1900/2330 train_time:110963ms step_avg:58.40ms
step:1901/2330 train_time:111020ms step_avg:58.40ms
step:1902/2330 train_time:111082ms step_avg:58.40ms
step:1903/2330 train_time:111138ms step_avg:58.40ms
step:1904/2330 train_time:111200ms step_avg:58.40ms
step:1905/2330 train_time:111256ms step_avg:58.40ms
step:1906/2330 train_time:111320ms step_avg:58.40ms
step:1907/2330 train_time:111376ms step_avg:58.40ms
step:1908/2330 train_time:111438ms step_avg:58.41ms
step:1909/2330 train_time:111494ms step_avg:58.40ms
step:1910/2330 train_time:111556ms step_avg:58.41ms
step:1911/2330 train_time:111613ms step_avg:58.41ms
step:1912/2330 train_time:111674ms step_avg:58.41ms
step:1913/2330 train_time:111732ms step_avg:58.41ms
step:1914/2330 train_time:111792ms step_avg:58.41ms
step:1915/2330 train_time:111850ms step_avg:58.41ms
step:1916/2330 train_time:111911ms step_avg:58.41ms
step:1917/2330 train_time:111969ms step_avg:58.41ms
step:1918/2330 train_time:112032ms step_avg:58.41ms
step:1919/2330 train_time:112090ms step_avg:58.41ms
step:1920/2330 train_time:112151ms step_avg:58.41ms
step:1921/2330 train_time:112208ms step_avg:58.41ms
step:1922/2330 train_time:112268ms step_avg:58.41ms
step:1923/2330 train_time:112325ms step_avg:58.41ms
step:1924/2330 train_time:112387ms step_avg:58.41ms
step:1925/2330 train_time:112445ms step_avg:58.41ms
step:1926/2330 train_time:112506ms step_avg:58.41ms
step:1927/2330 train_time:112563ms step_avg:58.41ms
step:1928/2330 train_time:112625ms step_avg:58.42ms
step:1929/2330 train_time:112681ms step_avg:58.41ms
step:1930/2330 train_time:112744ms step_avg:58.42ms
step:1931/2330 train_time:112801ms step_avg:58.42ms
step:1932/2330 train_time:112863ms step_avg:58.42ms
step:1933/2330 train_time:112920ms step_avg:58.42ms
step:1934/2330 train_time:112982ms step_avg:58.42ms
step:1935/2330 train_time:113038ms step_avg:58.42ms
step:1936/2330 train_time:113100ms step_avg:58.42ms
step:1937/2330 train_time:113157ms step_avg:58.42ms
step:1938/2330 train_time:113219ms step_avg:58.42ms
step:1939/2330 train_time:113275ms step_avg:58.42ms
step:1940/2330 train_time:113338ms step_avg:58.42ms
step:1941/2330 train_time:113395ms step_avg:58.42ms
step:1942/2330 train_time:113455ms step_avg:58.42ms
step:1943/2330 train_time:113513ms step_avg:58.42ms
step:1944/2330 train_time:113574ms step_avg:58.42ms
step:1945/2330 train_time:113632ms step_avg:58.42ms
step:1946/2330 train_time:113693ms step_avg:58.42ms
step:1947/2330 train_time:113751ms step_avg:58.42ms
step:1948/2330 train_time:113811ms step_avg:58.42ms
step:1949/2330 train_time:113870ms step_avg:58.42ms
step:1950/2330 train_time:113930ms step_avg:58.43ms
step:1951/2330 train_time:113989ms step_avg:58.43ms
step:1952/2330 train_time:114049ms step_avg:58.43ms
step:1953/2330 train_time:114108ms step_avg:58.43ms
step:1954/2330 train_time:114168ms step_avg:58.43ms
step:1955/2330 train_time:114226ms step_avg:58.43ms
step:1956/2330 train_time:114285ms step_avg:58.43ms
step:1957/2330 train_time:114343ms step_avg:58.43ms
step:1958/2330 train_time:114404ms step_avg:58.43ms
step:1959/2330 train_time:114461ms step_avg:58.43ms
step:1960/2330 train_time:114523ms step_avg:58.43ms
step:1961/2330 train_time:114580ms step_avg:58.43ms
step:1962/2330 train_time:114643ms step_avg:58.43ms
step:1963/2330 train_time:114699ms step_avg:58.43ms
step:1964/2330 train_time:114762ms step_avg:58.43ms
step:1965/2330 train_time:114819ms step_avg:58.43ms
step:1966/2330 train_time:114881ms step_avg:58.43ms
step:1967/2330 train_time:114938ms step_avg:58.43ms
step:1968/2330 train_time:115000ms step_avg:58.43ms
step:1969/2330 train_time:115056ms step_avg:58.43ms
step:1970/2330 train_time:115118ms step_avg:58.44ms
step:1971/2330 train_time:115175ms step_avg:58.43ms
step:1972/2330 train_time:115236ms step_avg:58.44ms
step:1973/2330 train_time:115293ms step_avg:58.44ms
step:1974/2330 train_time:115354ms step_avg:58.44ms
step:1975/2330 train_time:115412ms step_avg:58.44ms
step:1976/2330 train_time:115472ms step_avg:58.44ms
step:1977/2330 train_time:115530ms step_avg:58.44ms
step:1978/2330 train_time:115592ms step_avg:58.44ms
step:1979/2330 train_time:115650ms step_avg:58.44ms
step:1980/2330 train_time:115710ms step_avg:58.44ms
step:1981/2330 train_time:115768ms step_avg:58.44ms
step:1982/2330 train_time:115828ms step_avg:58.44ms
step:1983/2330 train_time:115887ms step_avg:58.44ms
step:1984/2330 train_time:115947ms step_avg:58.44ms
step:1985/2330 train_time:116004ms step_avg:58.44ms
step:1986/2330 train_time:116065ms step_avg:58.44ms
step:1987/2330 train_time:116122ms step_avg:58.44ms
step:1988/2330 train_time:116183ms step_avg:58.44ms
step:1989/2330 train_time:116239ms step_avg:58.44ms
step:1990/2330 train_time:116301ms step_avg:58.44ms
step:1991/2330 train_time:116358ms step_avg:58.44ms
step:1992/2330 train_time:116420ms step_avg:58.44ms
step:1993/2330 train_time:116477ms step_avg:58.44ms
step:1994/2330 train_time:116538ms step_avg:58.44ms
step:1995/2330 train_time:116595ms step_avg:58.44ms
step:1996/2330 train_time:116657ms step_avg:58.45ms
step:1997/2330 train_time:116714ms step_avg:58.44ms
step:1998/2330 train_time:116775ms step_avg:58.45ms
step:1999/2330 train_time:116831ms step_avg:58.44ms
step:2000/2330 train_time:116894ms step_avg:58.45ms
step:2000/2330 val_loss:3.7566 train_time:116976ms step_avg:58.49ms
step:2001/2330 train_time:116995ms step_avg:58.47ms
step:2002/2330 train_time:117015ms step_avg:58.45ms
step:2003/2330 train_time:117075ms step_avg:58.45ms
step:2004/2330 train_time:117143ms step_avg:58.45ms
step:2005/2330 train_time:117201ms step_avg:58.45ms
step:2006/2330 train_time:117262ms step_avg:58.46ms
step:2007/2330 train_time:117319ms step_avg:58.45ms
step:2008/2330 train_time:117379ms step_avg:58.46ms
step:2009/2330 train_time:117435ms step_avg:58.45ms
step:2010/2330 train_time:117496ms step_avg:58.46ms
step:2011/2330 train_time:117553ms step_avg:58.45ms
step:2012/2330 train_time:117613ms step_avg:58.46ms
step:2013/2330 train_time:117669ms step_avg:58.45ms
step:2014/2330 train_time:117730ms step_avg:58.46ms
step:2015/2330 train_time:117786ms step_avg:58.45ms
step:2016/2330 train_time:117847ms step_avg:58.46ms
step:2017/2330 train_time:117904ms step_avg:58.46ms
step:2018/2330 train_time:117965ms step_avg:58.46ms
step:2019/2330 train_time:118024ms step_avg:58.46ms
step:2020/2330 train_time:118088ms step_avg:58.46ms
step:2021/2330 train_time:118146ms step_avg:58.46ms
step:2022/2330 train_time:118210ms step_avg:58.46ms
step:2023/2330 train_time:118268ms step_avg:58.46ms
step:2024/2330 train_time:118329ms step_avg:58.46ms
step:2025/2330 train_time:118386ms step_avg:58.46ms
step:2026/2330 train_time:118447ms step_avg:58.46ms
step:2027/2330 train_time:118503ms step_avg:58.46ms
step:2028/2330 train_time:118564ms step_avg:58.46ms
step:2029/2330 train_time:118620ms step_avg:58.46ms
step:2030/2330 train_time:118681ms step_avg:58.46ms
step:2031/2330 train_time:118738ms step_avg:58.46ms
step:2032/2330 train_time:118799ms step_avg:58.46ms
step:2033/2330 train_time:118855ms step_avg:58.46ms
step:2034/2330 train_time:118916ms step_avg:58.46ms
step:2035/2330 train_time:118974ms step_avg:58.46ms
step:2036/2330 train_time:119036ms step_avg:58.47ms
step:2037/2330 train_time:119094ms step_avg:58.47ms
step:2038/2330 train_time:119158ms step_avg:58.47ms
step:2039/2330 train_time:119216ms step_avg:58.47ms
step:2040/2330 train_time:119277ms step_avg:58.47ms
step:2041/2330 train_time:119335ms step_avg:58.47ms
step:2042/2330 train_time:119396ms step_avg:58.47ms
step:2043/2330 train_time:119455ms step_avg:58.47ms
step:2044/2330 train_time:119514ms step_avg:58.47ms
step:2045/2330 train_time:119571ms step_avg:58.47ms
step:2046/2330 train_time:119632ms step_avg:58.47ms
step:2047/2330 train_time:119689ms step_avg:58.47ms
step:2048/2330 train_time:119750ms step_avg:58.47ms
step:2049/2330 train_time:119807ms step_avg:58.47ms
step:2050/2330 train_time:119868ms step_avg:58.47ms
step:2051/2330 train_time:119925ms step_avg:58.47ms
step:2052/2330 train_time:119987ms step_avg:58.47ms
step:2053/2330 train_time:120044ms step_avg:58.47ms
step:2054/2330 train_time:120108ms step_avg:58.47ms
step:2055/2330 train_time:120165ms step_avg:58.47ms
step:2056/2330 train_time:120228ms step_avg:58.48ms
step:2057/2330 train_time:120285ms step_avg:58.48ms
step:2058/2330 train_time:120346ms step_avg:58.48ms
step:2059/2330 train_time:120403ms step_avg:58.48ms
step:2060/2330 train_time:120466ms step_avg:58.48ms
step:2061/2330 train_time:120522ms step_avg:58.48ms
step:2062/2330 train_time:120583ms step_avg:58.48ms
step:2063/2330 train_time:120640ms step_avg:58.48ms
step:2064/2330 train_time:120702ms step_avg:58.48ms
step:2065/2330 train_time:120759ms step_avg:58.48ms
step:2066/2330 train_time:120818ms step_avg:58.48ms
step:2067/2330 train_time:120875ms step_avg:58.48ms
step:2068/2330 train_time:120938ms step_avg:58.48ms
step:2069/2330 train_time:120995ms step_avg:58.48ms
step:2070/2330 train_time:121057ms step_avg:58.48ms
step:2071/2330 train_time:121114ms step_avg:58.48ms
step:2072/2330 train_time:121177ms step_avg:58.48ms
step:2073/2330 train_time:121235ms step_avg:58.48ms
step:2074/2330 train_time:121296ms step_avg:58.48ms
step:2075/2330 train_time:121355ms step_avg:58.48ms
step:2076/2330 train_time:121416ms step_avg:58.49ms
step:2077/2330 train_time:121474ms step_avg:58.49ms
step:2078/2330 train_time:121535ms step_avg:58.49ms
step:2079/2330 train_time:121592ms step_avg:58.49ms
step:2080/2330 train_time:121653ms step_avg:58.49ms
step:2081/2330 train_time:121710ms step_avg:58.49ms
step:2082/2330 train_time:121770ms step_avg:58.49ms
step:2083/2330 train_time:121827ms step_avg:58.49ms
step:2084/2330 train_time:121888ms step_avg:58.49ms
step:2085/2330 train_time:121945ms step_avg:58.49ms
step:2086/2330 train_time:122007ms step_avg:58.49ms
step:2087/2330 train_time:122064ms step_avg:58.49ms
step:2088/2330 train_time:122126ms step_avg:58.49ms
step:2089/2330 train_time:122182ms step_avg:58.49ms
step:2090/2330 train_time:122246ms step_avg:58.49ms
step:2091/2330 train_time:122303ms step_avg:58.49ms
step:2092/2330 train_time:122365ms step_avg:58.49ms
step:2093/2330 train_time:122422ms step_avg:58.49ms
step:2094/2330 train_time:122484ms step_avg:58.49ms
step:2095/2330 train_time:122541ms step_avg:58.49ms
step:2096/2330 train_time:122602ms step_avg:58.49ms
step:2097/2330 train_time:122658ms step_avg:58.49ms
step:2098/2330 train_time:122720ms step_avg:58.49ms
step:2099/2330 train_time:122776ms step_avg:58.49ms
step:2100/2330 train_time:122837ms step_avg:58.49ms
step:2101/2330 train_time:122894ms step_avg:58.49ms
step:2102/2330 train_time:122957ms step_avg:58.50ms
step:2103/2330 train_time:123015ms step_avg:58.49ms
step:2104/2330 train_time:123075ms step_avg:58.50ms
step:2105/2330 train_time:123133ms step_avg:58.50ms
step:2106/2330 train_time:123194ms step_avg:58.50ms
step:2107/2330 train_time:123252ms step_avg:58.50ms
step:2108/2330 train_time:123313ms step_avg:58.50ms
step:2109/2330 train_time:123371ms step_avg:58.50ms
step:2110/2330 train_time:123432ms step_avg:58.50ms
step:2111/2330 train_time:123490ms step_avg:58.50ms
step:2112/2330 train_time:123551ms step_avg:58.50ms
step:2113/2330 train_time:123609ms step_avg:58.50ms
step:2114/2330 train_time:123670ms step_avg:58.50ms
step:2115/2330 train_time:123727ms step_avg:58.50ms
step:2116/2330 train_time:123787ms step_avg:58.50ms
step:2117/2330 train_time:123844ms step_avg:58.50ms
step:2118/2330 train_time:123905ms step_avg:58.50ms
step:2119/2330 train_time:123961ms step_avg:58.50ms
step:2120/2330 train_time:124024ms step_avg:58.50ms
step:2121/2330 train_time:124080ms step_avg:58.50ms
step:2122/2330 train_time:124142ms step_avg:58.50ms
step:2123/2330 train_time:124199ms step_avg:58.50ms
step:2124/2330 train_time:124262ms step_avg:58.50ms
step:2125/2330 train_time:124318ms step_avg:58.50ms
step:2126/2330 train_time:124381ms step_avg:58.50ms
step:2127/2330 train_time:124438ms step_avg:58.50ms
step:2128/2330 train_time:124500ms step_avg:58.51ms
step:2129/2330 train_time:124557ms step_avg:58.51ms
step:2130/2330 train_time:124619ms step_avg:58.51ms
step:2131/2330 train_time:124675ms step_avg:58.51ms
step:2132/2330 train_time:124737ms step_avg:58.51ms
step:2133/2330 train_time:124794ms step_avg:58.51ms
step:2134/2330 train_time:124856ms step_avg:58.51ms
step:2135/2330 train_time:124913ms step_avg:58.51ms
step:2136/2330 train_time:124973ms step_avg:58.51ms
step:2137/2330 train_time:125030ms step_avg:58.51ms
step:2138/2330 train_time:125092ms step_avg:58.51ms
step:2139/2330 train_time:125150ms step_avg:58.51ms
step:2140/2330 train_time:125211ms step_avg:58.51ms
step:2141/2330 train_time:125269ms step_avg:58.51ms
step:2142/2330 train_time:125329ms step_avg:58.51ms
step:2143/2330 train_time:125387ms step_avg:58.51ms
step:2144/2330 train_time:125448ms step_avg:58.51ms
step:2145/2330 train_time:125506ms step_avg:58.51ms
step:2146/2330 train_time:125566ms step_avg:58.51ms
step:2147/2330 train_time:125624ms step_avg:58.51ms
step:2148/2330 train_time:125684ms step_avg:58.51ms
step:2149/2330 train_time:125741ms step_avg:58.51ms
step:2150/2330 train_time:125802ms step_avg:58.51ms
step:2151/2330 train_time:125859ms step_avg:58.51ms
step:2152/2330 train_time:125920ms step_avg:58.51ms
step:2153/2330 train_time:125976ms step_avg:58.51ms
step:2154/2330 train_time:126039ms step_avg:58.51ms
step:2155/2330 train_time:126096ms step_avg:58.51ms
step:2156/2330 train_time:126158ms step_avg:58.52ms
step:2157/2330 train_time:126216ms step_avg:58.51ms
step:2158/2330 train_time:126276ms step_avg:58.52ms
step:2159/2330 train_time:126334ms step_avg:58.51ms
step:2160/2330 train_time:126396ms step_avg:58.52ms
step:2161/2330 train_time:126454ms step_avg:58.52ms
step:2162/2330 train_time:126514ms step_avg:58.52ms
step:2163/2330 train_time:126573ms step_avg:58.52ms
step:2164/2330 train_time:126633ms step_avg:58.52ms
step:2165/2330 train_time:126691ms step_avg:58.52ms
step:2166/2330 train_time:126752ms step_avg:58.52ms
step:2167/2330 train_time:126810ms step_avg:58.52ms
step:2168/2330 train_time:126870ms step_avg:58.52ms
step:2169/2330 train_time:126927ms step_avg:58.52ms
step:2170/2330 train_time:126988ms step_avg:58.52ms
step:2171/2330 train_time:127045ms step_avg:58.52ms
step:2172/2330 train_time:127107ms step_avg:58.52ms
step:2173/2330 train_time:127165ms step_avg:58.52ms
step:2174/2330 train_time:127226ms step_avg:58.52ms
step:2175/2330 train_time:127282ms step_avg:58.52ms
step:2176/2330 train_time:127345ms step_avg:58.52ms
step:2177/2330 train_time:127401ms step_avg:58.52ms
step:2178/2330 train_time:127464ms step_avg:58.52ms
step:2179/2330 train_time:127520ms step_avg:58.52ms
step:2180/2330 train_time:127582ms step_avg:58.52ms
step:2181/2330 train_time:127639ms step_avg:58.52ms
step:2182/2330 train_time:127702ms step_avg:58.53ms
step:2183/2330 train_time:127759ms step_avg:58.52ms
step:2184/2330 train_time:127821ms step_avg:58.53ms
step:2185/2330 train_time:127877ms step_avg:58.53ms
step:2186/2330 train_time:127940ms step_avg:58.53ms
step:2187/2330 train_time:127996ms step_avg:58.53ms
step:2188/2330 train_time:128058ms step_avg:58.53ms
step:2189/2330 train_time:128116ms step_avg:58.53ms
step:2190/2330 train_time:128177ms step_avg:58.53ms
step:2191/2330 train_time:128234ms step_avg:58.53ms
step:2192/2330 train_time:128296ms step_avg:58.53ms
step:2193/2330 train_time:128354ms step_avg:58.53ms
step:2194/2330 train_time:128415ms step_avg:58.53ms
step:2195/2330 train_time:128473ms step_avg:58.53ms
step:2196/2330 train_time:128533ms step_avg:58.53ms
step:2197/2330 train_time:128591ms step_avg:58.53ms
step:2198/2330 train_time:128653ms step_avg:58.53ms
step:2199/2330 train_time:128711ms step_avg:58.53ms
step:2200/2330 train_time:128772ms step_avg:58.53ms
step:2201/2330 train_time:128829ms step_avg:58.53ms
step:2202/2330 train_time:128890ms step_avg:58.53ms
step:2203/2330 train_time:128948ms step_avg:58.53ms
step:2204/2330 train_time:129008ms step_avg:58.53ms
step:2205/2330 train_time:129067ms step_avg:58.53ms
step:2206/2330 train_time:129128ms step_avg:58.53ms
step:2207/2330 train_time:129185ms step_avg:58.53ms
step:2208/2330 train_time:129247ms step_avg:58.54ms
step:2209/2330 train_time:129304ms step_avg:58.54ms
step:2210/2330 train_time:129366ms step_avg:58.54ms
step:2211/2330 train_time:129423ms step_avg:58.54ms
step:2212/2330 train_time:129484ms step_avg:58.54ms
step:2213/2330 train_time:129541ms step_avg:58.54ms
step:2214/2330 train_time:129603ms step_avg:58.54ms
step:2215/2330 train_time:129660ms step_avg:58.54ms
step:2216/2330 train_time:129722ms step_avg:58.54ms
step:2217/2330 train_time:129778ms step_avg:58.54ms
step:2218/2330 train_time:129841ms step_avg:58.54ms
step:2219/2330 train_time:129897ms step_avg:58.54ms
step:2220/2330 train_time:129959ms step_avg:58.54ms
step:2221/2330 train_time:130016ms step_avg:58.54ms
step:2222/2330 train_time:130077ms step_avg:58.54ms
step:2223/2330 train_time:130134ms step_avg:58.54ms
step:2224/2330 train_time:130195ms step_avg:58.54ms
step:2225/2330 train_time:130253ms step_avg:58.54ms
step:2226/2330 train_time:130315ms step_avg:58.54ms
step:2227/2330 train_time:130373ms step_avg:58.54ms
step:2228/2330 train_time:130433ms step_avg:58.54ms
step:2229/2330 train_time:130491ms step_avg:58.54ms
step:2230/2330 train_time:130552ms step_avg:58.54ms
step:2231/2330 train_time:130611ms step_avg:58.54ms
step:2232/2330 train_time:130670ms step_avg:58.54ms
step:2233/2330 train_time:130728ms step_avg:58.54ms
step:2234/2330 train_time:130788ms step_avg:58.54ms
step:2235/2330 train_time:130846ms step_avg:58.54ms
step:2236/2330 train_time:130907ms step_avg:58.55ms
step:2237/2330 train_time:130964ms step_avg:58.54ms
step:2238/2330 train_time:131025ms step_avg:58.55ms
step:2239/2330 train_time:131081ms step_avg:58.54ms
step:2240/2330 train_time:131144ms step_avg:58.55ms
step:2241/2330 train_time:131201ms step_avg:58.55ms
step:2242/2330 train_time:131262ms step_avg:58.55ms
step:2243/2330 train_time:131319ms step_avg:58.55ms
step:2244/2330 train_time:131381ms step_avg:58.55ms
step:2245/2330 train_time:131437ms step_avg:58.55ms
step:2246/2330 train_time:131500ms step_avg:58.55ms
step:2247/2330 train_time:131557ms step_avg:58.55ms
step:2248/2330 train_time:131618ms step_avg:58.55ms
step:2249/2330 train_time:131675ms step_avg:58.55ms
step:2250/2330 train_time:131736ms step_avg:58.55ms
step:2250/2330 val_loss:3.7076 train_time:131818ms step_avg:58.59ms
step:2251/2330 train_time:131836ms step_avg:58.57ms
step:2252/2330 train_time:131857ms step_avg:58.55ms
step:2253/2330 train_time:131917ms step_avg:58.55ms
step:2254/2330 train_time:131982ms step_avg:58.55ms
step:2255/2330 train_time:132040ms step_avg:58.55ms
step:2256/2330 train_time:132102ms step_avg:58.56ms
step:2257/2330 train_time:132159ms step_avg:58.56ms
step:2258/2330 train_time:132220ms step_avg:58.56ms
step:2259/2330 train_time:132276ms step_avg:58.56ms
step:2260/2330 train_time:132337ms step_avg:58.56ms
step:2261/2330 train_time:132393ms step_avg:58.56ms
step:2262/2330 train_time:132453ms step_avg:58.56ms
step:2263/2330 train_time:132510ms step_avg:58.55ms
step:2264/2330 train_time:132571ms step_avg:58.56ms
step:2265/2330 train_time:132627ms step_avg:58.56ms
step:2266/2330 train_time:132688ms step_avg:58.56ms
step:2267/2330 train_time:132745ms step_avg:58.56ms
step:2268/2330 train_time:132807ms step_avg:58.56ms
step:2269/2330 train_time:132867ms step_avg:58.56ms
step:2270/2330 train_time:132929ms step_avg:58.56ms
step:2271/2330 train_time:132988ms step_avg:58.56ms
step:2272/2330 train_time:133051ms step_avg:58.56ms
step:2273/2330 train_time:133108ms step_avg:58.56ms
step:2274/2330 train_time:133171ms step_avg:58.56ms
step:2275/2330 train_time:133229ms step_avg:58.56ms
step:2276/2330 train_time:133288ms step_avg:58.56ms
step:2277/2330 train_time:133346ms step_avg:58.56ms
step:2278/2330 train_time:133407ms step_avg:58.56ms
step:2279/2330 train_time:133464ms step_avg:58.56ms
step:2280/2330 train_time:133523ms step_avg:58.56ms
step:2281/2330 train_time:133579ms step_avg:58.56ms
step:2282/2330 train_time:133641ms step_avg:58.56ms
step:2283/2330 train_time:133697ms step_avg:58.56ms
step:2284/2330 train_time:133759ms step_avg:58.56ms
step:2285/2330 train_time:133816ms step_avg:58.56ms
step:2286/2330 train_time:133878ms step_avg:58.56ms
step:2287/2330 train_time:133935ms step_avg:58.56ms
step:2288/2330 train_time:133999ms step_avg:58.57ms
step:2289/2330 train_time:134057ms step_avg:58.57ms
step:2290/2330 train_time:134120ms step_avg:58.57ms
step:2291/2330 train_time:134177ms step_avg:58.57ms
step:2292/2330 train_time:134238ms step_avg:58.57ms
step:2293/2330 train_time:134294ms step_avg:58.57ms
step:2294/2330 train_time:134356ms step_avg:58.57ms
step:2295/2330 train_time:134413ms step_avg:58.57ms
step:2296/2330 train_time:134474ms step_avg:58.57ms
step:2297/2330 train_time:134531ms step_avg:58.57ms
step:2298/2330 train_time:134591ms step_avg:58.57ms
step:2299/2330 train_time:134649ms step_avg:58.57ms
step:2300/2330 train_time:134709ms step_avg:58.57ms
step:2301/2330 train_time:134766ms step_avg:58.57ms
step:2302/2330 train_time:134829ms step_avg:58.57ms
step:2303/2330 train_time:134887ms step_avg:58.57ms
step:2304/2330 train_time:134949ms step_avg:58.57ms
step:2305/2330 train_time:135007ms step_avg:58.57ms
step:2306/2330 train_time:135071ms step_avg:58.57ms
step:2307/2330 train_time:135128ms step_avg:58.57ms
step:2308/2330 train_time:135189ms step_avg:58.57ms
step:2309/2330 train_time:135247ms step_avg:58.57ms
step:2310/2330 train_time:135307ms step_avg:58.57ms
step:2311/2330 train_time:135365ms step_avg:58.57ms
step:2312/2330 train_time:135426ms step_avg:58.58ms
step:2313/2330 train_time:135483ms step_avg:58.57ms
step:2314/2330 train_time:135543ms step_avg:58.58ms
step:2315/2330 train_time:135600ms step_avg:58.57ms
step:2316/2330 train_time:135662ms step_avg:58.58ms
step:2317/2330 train_time:135719ms step_avg:58.58ms
step:2318/2330 train_time:135780ms step_avg:58.58ms
step:2319/2330 train_time:135836ms step_avg:58.58ms
step:2320/2330 train_time:135899ms step_avg:58.58ms
step:2321/2330 train_time:135956ms step_avg:58.58ms
step:2322/2330 train_time:136019ms step_avg:58.58ms
step:2323/2330 train_time:136076ms step_avg:58.58ms
step:2324/2330 train_time:136138ms step_avg:58.58ms
step:2325/2330 train_time:136195ms step_avg:58.58ms
step:2326/2330 train_time:136256ms step_avg:58.58ms
step:2327/2330 train_time:136313ms step_avg:58.58ms
step:2328/2330 train_time:136375ms step_avg:58.58ms
step:2329/2330 train_time:136431ms step_avg:58.58ms
step:2330/2330 train_time:136492ms step_avg:58.58ms
step:2330/2330 val_loss:3.6922 train_time:136574ms step_avg:58.62ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
