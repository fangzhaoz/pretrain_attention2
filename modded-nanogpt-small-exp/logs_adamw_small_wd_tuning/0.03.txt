import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:28:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:83ms step_avg:83.32ms
step:2/2330 train_time:173ms step_avg:86.58ms
step:3/2330 train_time:192ms step_avg:64.12ms
step:4/2330 train_time:212ms step_avg:53.02ms
step:5/2330 train_time:264ms step_avg:52.86ms
step:6/2330 train_time:322ms step_avg:53.71ms
step:7/2330 train_time:378ms step_avg:54.02ms
step:8/2330 train_time:437ms step_avg:54.59ms
step:9/2330 train_time:492ms step_avg:54.71ms
step:10/2330 train_time:550ms step_avg:55.04ms
step:11/2330 train_time:606ms step_avg:55.07ms
step:12/2330 train_time:664ms step_avg:55.33ms
step:13/2330 train_time:720ms step_avg:55.35ms
step:14/2330 train_time:778ms step_avg:55.57ms
step:15/2330 train_time:834ms step_avg:55.59ms
step:16/2330 train_time:892ms step_avg:55.74ms
step:17/2330 train_time:947ms step_avg:55.71ms
step:18/2330 train_time:1007ms step_avg:55.92ms
step:19/2330 train_time:1066ms step_avg:56.09ms
step:20/2330 train_time:1127ms step_avg:56.33ms
step:21/2330 train_time:1184ms step_avg:56.38ms
step:22/2330 train_time:1244ms step_avg:56.53ms
step:23/2330 train_time:1300ms step_avg:56.52ms
step:24/2330 train_time:1360ms step_avg:56.65ms
step:25/2330 train_time:1416ms step_avg:56.64ms
step:26/2330 train_time:1474ms step_avg:56.69ms
step:27/2330 train_time:1529ms step_avg:56.64ms
step:28/2330 train_time:1588ms step_avg:56.70ms
step:29/2330 train_time:1643ms step_avg:56.65ms
step:30/2330 train_time:1701ms step_avg:56.70ms
step:31/2330 train_time:1756ms step_avg:56.66ms
step:32/2330 train_time:1816ms step_avg:56.76ms
step:33/2330 train_time:1872ms step_avg:56.73ms
step:34/2330 train_time:1930ms step_avg:56.77ms
step:35/2330 train_time:1986ms step_avg:56.75ms
step:36/2330 train_time:2046ms step_avg:56.84ms
step:37/2330 train_time:2103ms step_avg:56.83ms
step:38/2330 train_time:2163ms step_avg:56.92ms
step:39/2330 train_time:2219ms step_avg:56.91ms
step:40/2330 train_time:2280ms step_avg:57.00ms
step:41/2330 train_time:2336ms step_avg:56.99ms
step:42/2330 train_time:2396ms step_avg:57.04ms
step:43/2330 train_time:2452ms step_avg:57.01ms
step:44/2330 train_time:2510ms step_avg:57.04ms
step:45/2330 train_time:2565ms step_avg:57.01ms
step:46/2330 train_time:2624ms step_avg:57.03ms
step:47/2330 train_time:2679ms step_avg:57.01ms
step:48/2330 train_time:2738ms step_avg:57.03ms
step:49/2330 train_time:2793ms step_avg:57.01ms
step:50/2330 train_time:2851ms step_avg:57.03ms
step:51/2330 train_time:2908ms step_avg:57.01ms
step:52/2330 train_time:2966ms step_avg:57.04ms
step:53/2330 train_time:3023ms step_avg:57.05ms
step:54/2330 train_time:3082ms step_avg:57.08ms
step:55/2330 train_time:3138ms step_avg:57.06ms
step:56/2330 train_time:3198ms step_avg:57.11ms
step:57/2330 train_time:3255ms step_avg:57.10ms
step:58/2330 train_time:3315ms step_avg:57.15ms
step:59/2330 train_time:3371ms step_avg:57.14ms
step:60/2330 train_time:3430ms step_avg:57.17ms
step:61/2330 train_time:3486ms step_avg:57.15ms
step:62/2330 train_time:3544ms step_avg:57.16ms
step:63/2330 train_time:3600ms step_avg:57.15ms
step:64/2330 train_time:3658ms step_avg:57.16ms
step:65/2330 train_time:3714ms step_avg:57.13ms
step:66/2330 train_time:3772ms step_avg:57.16ms
step:67/2330 train_time:3828ms step_avg:57.14ms
step:68/2330 train_time:3886ms step_avg:57.15ms
step:69/2330 train_time:3942ms step_avg:57.14ms
step:70/2330 train_time:4002ms step_avg:57.17ms
step:71/2330 train_time:4058ms step_avg:57.16ms
step:72/2330 train_time:4117ms step_avg:57.18ms
step:73/2330 train_time:4174ms step_avg:57.17ms
step:74/2330 train_time:4233ms step_avg:57.20ms
step:75/2330 train_time:4289ms step_avg:57.19ms
step:76/2330 train_time:4348ms step_avg:57.21ms
step:77/2330 train_time:4404ms step_avg:57.19ms
step:78/2330 train_time:4463ms step_avg:57.22ms
step:79/2330 train_time:4519ms step_avg:57.20ms
step:80/2330 train_time:4578ms step_avg:57.23ms
step:81/2330 train_time:4634ms step_avg:57.21ms
step:82/2330 train_time:4692ms step_avg:57.23ms
step:83/2330 train_time:4748ms step_avg:57.20ms
step:84/2330 train_time:4806ms step_avg:57.22ms
step:85/2330 train_time:4862ms step_avg:57.20ms
step:86/2330 train_time:4921ms step_avg:57.22ms
step:87/2330 train_time:4978ms step_avg:57.22ms
step:88/2330 train_time:5036ms step_avg:57.23ms
step:89/2330 train_time:5093ms step_avg:57.22ms
step:90/2330 train_time:5152ms step_avg:57.24ms
step:91/2330 train_time:5208ms step_avg:57.23ms
step:92/2330 train_time:5267ms step_avg:57.25ms
step:93/2330 train_time:5324ms step_avg:57.25ms
step:94/2330 train_time:5382ms step_avg:57.26ms
step:95/2330 train_time:5438ms step_avg:57.24ms
step:96/2330 train_time:5497ms step_avg:57.26ms
step:97/2330 train_time:5554ms step_avg:57.25ms
step:98/2330 train_time:5612ms step_avg:57.26ms
step:99/2330 train_time:5668ms step_avg:57.25ms
step:100/2330 train_time:5726ms step_avg:57.26ms
step:101/2330 train_time:5782ms step_avg:57.25ms
step:102/2330 train_time:5841ms step_avg:57.26ms
step:103/2330 train_time:5897ms step_avg:57.26ms
step:104/2330 train_time:5956ms step_avg:57.27ms
step:105/2330 train_time:6012ms step_avg:57.26ms
step:106/2330 train_time:6071ms step_avg:57.27ms
step:107/2330 train_time:6127ms step_avg:57.26ms
step:108/2330 train_time:6186ms step_avg:57.28ms
step:109/2330 train_time:6242ms step_avg:57.27ms
step:110/2330 train_time:6301ms step_avg:57.28ms
step:111/2330 train_time:6358ms step_avg:57.28ms
step:112/2330 train_time:6416ms step_avg:57.29ms
step:113/2330 train_time:6473ms step_avg:57.28ms
step:114/2330 train_time:6532ms step_avg:57.30ms
step:115/2330 train_time:6588ms step_avg:57.29ms
step:116/2330 train_time:6646ms step_avg:57.29ms
step:117/2330 train_time:6702ms step_avg:57.29ms
step:118/2330 train_time:6761ms step_avg:57.29ms
step:119/2330 train_time:6817ms step_avg:57.28ms
step:120/2330 train_time:6875ms step_avg:57.29ms
step:121/2330 train_time:6931ms step_avg:57.28ms
step:122/2330 train_time:6990ms step_avg:57.29ms
step:123/2330 train_time:7046ms step_avg:57.28ms
step:124/2330 train_time:7105ms step_avg:57.30ms
step:125/2330 train_time:7161ms step_avg:57.29ms
step:126/2330 train_time:7221ms step_avg:57.31ms
step:127/2330 train_time:7278ms step_avg:57.30ms
step:128/2330 train_time:7338ms step_avg:57.33ms
step:129/2330 train_time:7394ms step_avg:57.31ms
step:130/2330 train_time:7453ms step_avg:57.33ms
step:131/2330 train_time:7509ms step_avg:57.32ms
step:132/2330 train_time:7567ms step_avg:57.33ms
step:133/2330 train_time:7624ms step_avg:57.32ms
step:134/2330 train_time:7682ms step_avg:57.33ms
step:135/2330 train_time:7738ms step_avg:57.32ms
step:136/2330 train_time:7796ms step_avg:57.33ms
step:137/2330 train_time:7852ms step_avg:57.32ms
step:138/2330 train_time:7910ms step_avg:57.32ms
step:139/2330 train_time:7967ms step_avg:57.31ms
step:140/2330 train_time:8026ms step_avg:57.33ms
step:141/2330 train_time:8081ms step_avg:57.31ms
step:142/2330 train_time:8140ms step_avg:57.33ms
step:143/2330 train_time:8196ms step_avg:57.32ms
step:144/2330 train_time:8256ms step_avg:57.33ms
step:145/2330 train_time:8311ms step_avg:57.32ms
step:146/2330 train_time:8371ms step_avg:57.33ms
step:147/2330 train_time:8427ms step_avg:57.32ms
step:148/2330 train_time:8485ms step_avg:57.33ms
step:149/2330 train_time:8541ms step_avg:57.32ms
step:150/2330 train_time:8600ms step_avg:57.34ms
step:151/2330 train_time:8656ms step_avg:57.33ms
step:152/2330 train_time:8716ms step_avg:57.34ms
step:153/2330 train_time:8772ms step_avg:57.33ms
step:154/2330 train_time:8830ms step_avg:57.34ms
step:155/2330 train_time:8887ms step_avg:57.33ms
step:156/2330 train_time:8945ms step_avg:57.34ms
step:157/2330 train_time:9002ms step_avg:57.33ms
step:158/2330 train_time:9060ms step_avg:57.34ms
step:159/2330 train_time:9116ms step_avg:57.34ms
step:160/2330 train_time:9175ms step_avg:57.35ms
step:161/2330 train_time:9232ms step_avg:57.34ms
step:162/2330 train_time:9290ms step_avg:57.35ms
step:163/2330 train_time:9346ms step_avg:57.34ms
step:164/2330 train_time:9405ms step_avg:57.35ms
step:165/2330 train_time:9462ms step_avg:57.34ms
step:166/2330 train_time:9521ms step_avg:57.35ms
step:167/2330 train_time:9576ms step_avg:57.34ms
step:168/2330 train_time:9636ms step_avg:57.35ms
step:169/2330 train_time:9692ms step_avg:57.35ms
step:170/2330 train_time:9750ms step_avg:57.36ms
step:171/2330 train_time:9806ms step_avg:57.35ms
step:172/2330 train_time:9865ms step_avg:57.35ms
step:173/2330 train_time:9921ms step_avg:57.35ms
step:174/2330 train_time:9980ms step_avg:57.36ms
step:175/2330 train_time:10036ms step_avg:57.35ms
step:176/2330 train_time:10095ms step_avg:57.36ms
step:177/2330 train_time:10150ms step_avg:57.35ms
step:178/2330 train_time:10210ms step_avg:57.36ms
step:179/2330 train_time:10266ms step_avg:57.35ms
step:180/2330 train_time:10324ms step_avg:57.36ms
step:181/2330 train_time:10380ms step_avg:57.35ms
step:182/2330 train_time:10439ms step_avg:57.36ms
step:183/2330 train_time:10495ms step_avg:57.35ms
step:184/2330 train_time:10554ms step_avg:57.36ms
step:185/2330 train_time:10609ms step_avg:57.35ms
step:186/2330 train_time:10669ms step_avg:57.36ms
step:187/2330 train_time:10725ms step_avg:57.35ms
step:188/2330 train_time:10783ms step_avg:57.36ms
step:189/2330 train_time:10838ms step_avg:57.35ms
step:190/2330 train_time:10898ms step_avg:57.36ms
step:191/2330 train_time:10955ms step_avg:57.35ms
step:192/2330 train_time:11013ms step_avg:57.36ms
step:193/2330 train_time:11069ms step_avg:57.35ms
step:194/2330 train_time:11127ms step_avg:57.36ms
step:195/2330 train_time:11183ms step_avg:57.35ms
step:196/2330 train_time:11242ms step_avg:57.36ms
step:197/2330 train_time:11298ms step_avg:57.35ms
step:198/2330 train_time:11356ms step_avg:57.36ms
step:199/2330 train_time:11413ms step_avg:57.35ms
step:200/2330 train_time:11472ms step_avg:57.36ms
step:201/2330 train_time:11528ms step_avg:57.35ms
step:202/2330 train_time:11587ms step_avg:57.36ms
step:203/2330 train_time:11643ms step_avg:57.35ms
step:204/2330 train_time:11701ms step_avg:57.36ms
step:205/2330 train_time:11757ms step_avg:57.35ms
step:206/2330 train_time:11816ms step_avg:57.36ms
step:207/2330 train_time:11873ms step_avg:57.36ms
step:208/2330 train_time:11932ms step_avg:57.37ms
step:209/2330 train_time:11988ms step_avg:57.36ms
step:210/2330 train_time:12046ms step_avg:57.36ms
step:211/2330 train_time:12102ms step_avg:57.36ms
step:212/2330 train_time:12161ms step_avg:57.36ms
step:213/2330 train_time:12217ms step_avg:57.36ms
step:214/2330 train_time:12276ms step_avg:57.37ms
step:215/2330 train_time:12333ms step_avg:57.36ms
step:216/2330 train_time:12391ms step_avg:57.37ms
step:217/2330 train_time:12447ms step_avg:57.36ms
step:218/2330 train_time:12506ms step_avg:57.37ms
step:219/2330 train_time:12563ms step_avg:57.36ms
step:220/2330 train_time:12621ms step_avg:57.37ms
step:221/2330 train_time:12677ms step_avg:57.36ms
step:222/2330 train_time:12736ms step_avg:57.37ms
step:223/2330 train_time:12792ms step_avg:57.36ms
step:224/2330 train_time:12850ms step_avg:57.37ms
step:225/2330 train_time:12907ms step_avg:57.37ms
step:226/2330 train_time:12966ms step_avg:57.37ms
step:227/2330 train_time:13022ms step_avg:57.37ms
step:228/2330 train_time:13081ms step_avg:57.37ms
step:229/2330 train_time:13137ms step_avg:57.37ms
step:230/2330 train_time:13196ms step_avg:57.38ms
step:231/2330 train_time:13253ms step_avg:57.37ms
step:232/2330 train_time:13311ms step_avg:57.38ms
step:233/2330 train_time:13367ms step_avg:57.37ms
step:234/2330 train_time:13427ms step_avg:57.38ms
step:235/2330 train_time:13483ms step_avg:57.37ms
step:236/2330 train_time:13541ms step_avg:57.38ms
step:237/2330 train_time:13597ms step_avg:57.37ms
step:238/2330 train_time:13656ms step_avg:57.38ms
step:239/2330 train_time:13712ms step_avg:57.37ms
step:240/2330 train_time:13770ms step_avg:57.38ms
step:241/2330 train_time:13827ms step_avg:57.37ms
step:242/2330 train_time:13886ms step_avg:57.38ms
step:243/2330 train_time:13941ms step_avg:57.37ms
step:244/2330 train_time:14001ms step_avg:57.38ms
step:245/2330 train_time:14057ms step_avg:57.37ms
step:246/2330 train_time:14116ms step_avg:57.38ms
step:247/2330 train_time:14173ms step_avg:57.38ms
step:248/2330 train_time:14231ms step_avg:57.38ms
step:249/2330 train_time:14287ms step_avg:57.38ms
step:250/2330 train_time:14347ms step_avg:57.39ms
step:250/2330 val_loss:4.8909 train_time:14426ms step_avg:57.71ms
step:251/2330 train_time:14446ms step_avg:57.55ms
step:252/2330 train_time:14466ms step_avg:57.40ms
step:253/2330 train_time:14519ms step_avg:57.39ms
step:254/2330 train_time:14584ms step_avg:57.42ms
step:255/2330 train_time:14639ms step_avg:57.41ms
step:256/2330 train_time:14703ms step_avg:57.43ms
step:257/2330 train_time:14758ms step_avg:57.43ms
step:258/2330 train_time:14818ms step_avg:57.43ms
step:259/2330 train_time:14873ms step_avg:57.43ms
step:260/2330 train_time:14932ms step_avg:57.43ms
step:261/2330 train_time:14988ms step_avg:57.43ms
step:262/2330 train_time:15046ms step_avg:57.43ms
step:263/2330 train_time:15101ms step_avg:57.42ms
step:264/2330 train_time:15159ms step_avg:57.42ms
step:265/2330 train_time:15215ms step_avg:57.41ms
step:266/2330 train_time:15274ms step_avg:57.42ms
step:267/2330 train_time:15330ms step_avg:57.41ms
step:268/2330 train_time:15390ms step_avg:57.42ms
step:269/2330 train_time:15447ms step_avg:57.43ms
step:270/2330 train_time:15506ms step_avg:57.43ms
step:271/2330 train_time:15564ms step_avg:57.43ms
step:272/2330 train_time:15623ms step_avg:57.44ms
step:273/2330 train_time:15680ms step_avg:57.43ms
step:274/2330 train_time:15739ms step_avg:57.44ms
step:275/2330 train_time:15795ms step_avg:57.44ms
step:276/2330 train_time:15854ms step_avg:57.44ms
step:277/2330 train_time:15910ms step_avg:57.44ms
step:278/2330 train_time:15969ms step_avg:57.44ms
step:279/2330 train_time:16025ms step_avg:57.44ms
step:280/2330 train_time:16083ms step_avg:57.44ms
step:281/2330 train_time:16139ms step_avg:57.43ms
step:282/2330 train_time:16197ms step_avg:57.44ms
step:283/2330 train_time:16253ms step_avg:57.43ms
step:284/2330 train_time:16312ms step_avg:57.44ms
step:285/2330 train_time:16369ms step_avg:57.44ms
step:286/2330 train_time:16428ms step_avg:57.44ms
step:287/2330 train_time:16485ms step_avg:57.44ms
step:288/2330 train_time:16544ms step_avg:57.44ms
step:289/2330 train_time:16600ms step_avg:57.44ms
step:290/2330 train_time:16660ms step_avg:57.45ms
step:291/2330 train_time:16716ms step_avg:57.44ms
step:292/2330 train_time:16777ms step_avg:57.45ms
step:293/2330 train_time:16833ms step_avg:57.45ms
step:294/2330 train_time:16893ms step_avg:57.46ms
step:295/2330 train_time:16949ms step_avg:57.45ms
step:296/2330 train_time:17008ms step_avg:57.46ms
step:297/2330 train_time:17064ms step_avg:57.45ms
step:298/2330 train_time:17123ms step_avg:57.46ms
step:299/2330 train_time:17178ms step_avg:57.45ms
step:300/2330 train_time:17237ms step_avg:57.46ms
step:301/2330 train_time:17293ms step_avg:57.45ms
step:302/2330 train_time:17352ms step_avg:57.46ms
step:303/2330 train_time:17409ms step_avg:57.45ms
step:304/2330 train_time:17468ms step_avg:57.46ms
step:305/2330 train_time:17524ms step_avg:57.46ms
step:306/2330 train_time:17584ms step_avg:57.47ms
step:307/2330 train_time:17641ms step_avg:57.46ms
step:308/2330 train_time:17700ms step_avg:57.47ms
step:309/2330 train_time:17756ms step_avg:57.46ms
step:310/2330 train_time:17815ms step_avg:57.47ms
step:311/2330 train_time:17873ms step_avg:57.47ms
step:312/2330 train_time:17932ms step_avg:57.47ms
step:313/2330 train_time:17988ms step_avg:57.47ms
step:314/2330 train_time:18046ms step_avg:57.47ms
step:315/2330 train_time:18102ms step_avg:57.47ms
step:316/2330 train_time:18160ms step_avg:57.47ms
step:317/2330 train_time:18216ms step_avg:57.46ms
step:318/2330 train_time:18275ms step_avg:57.47ms
step:319/2330 train_time:18332ms step_avg:57.47ms
step:320/2330 train_time:18390ms step_avg:57.47ms
step:321/2330 train_time:18446ms step_avg:57.46ms
step:322/2330 train_time:18505ms step_avg:57.47ms
step:323/2330 train_time:18561ms step_avg:57.46ms
step:324/2330 train_time:18621ms step_avg:57.47ms
step:325/2330 train_time:18677ms step_avg:57.47ms
step:326/2330 train_time:18737ms step_avg:57.47ms
step:327/2330 train_time:18794ms step_avg:57.47ms
step:328/2330 train_time:18852ms step_avg:57.48ms
step:329/2330 train_time:18909ms step_avg:57.47ms
step:330/2330 train_time:18968ms step_avg:57.48ms
step:331/2330 train_time:19024ms step_avg:57.48ms
step:332/2330 train_time:19083ms step_avg:57.48ms
step:333/2330 train_time:19139ms step_avg:57.47ms
step:334/2330 train_time:19197ms step_avg:57.48ms
step:335/2330 train_time:19254ms step_avg:57.47ms
step:336/2330 train_time:19312ms step_avg:57.48ms
step:337/2330 train_time:19368ms step_avg:57.47ms
step:338/2330 train_time:19428ms step_avg:57.48ms
step:339/2330 train_time:19484ms step_avg:57.47ms
step:340/2330 train_time:19543ms step_avg:57.48ms
step:341/2330 train_time:19599ms step_avg:57.47ms
step:342/2330 train_time:19658ms step_avg:57.48ms
step:343/2330 train_time:19714ms step_avg:57.48ms
step:344/2330 train_time:19774ms step_avg:57.48ms
step:345/2330 train_time:19830ms step_avg:57.48ms
step:346/2330 train_time:19889ms step_avg:57.48ms
step:347/2330 train_time:19945ms step_avg:57.48ms
step:348/2330 train_time:20004ms step_avg:57.48ms
step:349/2330 train_time:20060ms step_avg:57.48ms
step:350/2330 train_time:20118ms step_avg:57.48ms
step:351/2330 train_time:20174ms step_avg:57.48ms
step:352/2330 train_time:20234ms step_avg:57.48ms
step:353/2330 train_time:20291ms step_avg:57.48ms
step:354/2330 train_time:20350ms step_avg:57.48ms
step:355/2330 train_time:20406ms step_avg:57.48ms
step:356/2330 train_time:20464ms step_avg:57.48ms
step:357/2330 train_time:20520ms step_avg:57.48ms
step:358/2330 train_time:20580ms step_avg:57.49ms
step:359/2330 train_time:20636ms step_avg:57.48ms
step:360/2330 train_time:20695ms step_avg:57.49ms
step:361/2330 train_time:20751ms step_avg:57.48ms
step:362/2330 train_time:20810ms step_avg:57.49ms
step:363/2330 train_time:20866ms step_avg:57.48ms
step:364/2330 train_time:20926ms step_avg:57.49ms
step:365/2330 train_time:20982ms step_avg:57.49ms
step:366/2330 train_time:21040ms step_avg:57.49ms
step:367/2330 train_time:21096ms step_avg:57.48ms
step:368/2330 train_time:21156ms step_avg:57.49ms
step:369/2330 train_time:21212ms step_avg:57.48ms
step:370/2330 train_time:21270ms step_avg:57.49ms
step:371/2330 train_time:21327ms step_avg:57.48ms
step:372/2330 train_time:21385ms step_avg:57.49ms
step:373/2330 train_time:21442ms step_avg:57.48ms
step:374/2330 train_time:21500ms step_avg:57.49ms
step:375/2330 train_time:21556ms step_avg:57.48ms
step:376/2330 train_time:21616ms step_avg:57.49ms
step:377/2330 train_time:21673ms step_avg:57.49ms
step:378/2330 train_time:21732ms step_avg:57.49ms
step:379/2330 train_time:21788ms step_avg:57.49ms
step:380/2330 train_time:21847ms step_avg:57.49ms
step:381/2330 train_time:21903ms step_avg:57.49ms
step:382/2330 train_time:21963ms step_avg:57.49ms
step:383/2330 train_time:22019ms step_avg:57.49ms
step:384/2330 train_time:22077ms step_avg:57.49ms
step:385/2330 train_time:22134ms step_avg:57.49ms
step:386/2330 train_time:22192ms step_avg:57.49ms
step:387/2330 train_time:22249ms step_avg:57.49ms
step:388/2330 train_time:22308ms step_avg:57.49ms
step:389/2330 train_time:22364ms step_avg:57.49ms
step:390/2330 train_time:22423ms step_avg:57.49ms
step:391/2330 train_time:22479ms step_avg:57.49ms
step:392/2330 train_time:22538ms step_avg:57.49ms
step:393/2330 train_time:22594ms step_avg:57.49ms
step:394/2330 train_time:22654ms step_avg:57.50ms
step:395/2330 train_time:22710ms step_avg:57.49ms
step:396/2330 train_time:22769ms step_avg:57.50ms
step:397/2330 train_time:22826ms step_avg:57.50ms
step:398/2330 train_time:22884ms step_avg:57.50ms
step:399/2330 train_time:22941ms step_avg:57.50ms
step:400/2330 train_time:22999ms step_avg:57.50ms
step:401/2330 train_time:23054ms step_avg:57.49ms
step:402/2330 train_time:23114ms step_avg:57.50ms
step:403/2330 train_time:23171ms step_avg:57.50ms
step:404/2330 train_time:23230ms step_avg:57.50ms
step:405/2330 train_time:23287ms step_avg:57.50ms
step:406/2330 train_time:23345ms step_avg:57.50ms
step:407/2330 train_time:23401ms step_avg:57.50ms
step:408/2330 train_time:23459ms step_avg:57.50ms
step:409/2330 train_time:23515ms step_avg:57.49ms
step:410/2330 train_time:23576ms step_avg:57.50ms
step:411/2330 train_time:23632ms step_avg:57.50ms
step:412/2330 train_time:23691ms step_avg:57.50ms
step:413/2330 train_time:23748ms step_avg:57.50ms
step:414/2330 train_time:23806ms step_avg:57.50ms
step:415/2330 train_time:23862ms step_avg:57.50ms
step:416/2330 train_time:23921ms step_avg:57.50ms
step:417/2330 train_time:23976ms step_avg:57.50ms
step:418/2330 train_time:24036ms step_avg:57.50ms
step:419/2330 train_time:24091ms step_avg:57.50ms
step:420/2330 train_time:24150ms step_avg:57.50ms
step:421/2330 train_time:24206ms step_avg:57.50ms
step:422/2330 train_time:24266ms step_avg:57.50ms
step:423/2330 train_time:24322ms step_avg:57.50ms
step:424/2330 train_time:24381ms step_avg:57.50ms
step:425/2330 train_time:24437ms step_avg:57.50ms
step:426/2330 train_time:24496ms step_avg:57.50ms
step:427/2330 train_time:24552ms step_avg:57.50ms
step:428/2330 train_time:24613ms step_avg:57.51ms
step:429/2330 train_time:24669ms step_avg:57.50ms
step:430/2330 train_time:24728ms step_avg:57.51ms
step:431/2330 train_time:24785ms step_avg:57.51ms
step:432/2330 train_time:24844ms step_avg:57.51ms
step:433/2330 train_time:24899ms step_avg:57.50ms
step:434/2330 train_time:24958ms step_avg:57.51ms
step:435/2330 train_time:25014ms step_avg:57.50ms
step:436/2330 train_time:25073ms step_avg:57.51ms
step:437/2330 train_time:25129ms step_avg:57.50ms
step:438/2330 train_time:25188ms step_avg:57.51ms
step:439/2330 train_time:25245ms step_avg:57.50ms
step:440/2330 train_time:25304ms step_avg:57.51ms
step:441/2330 train_time:25359ms step_avg:57.50ms
step:442/2330 train_time:25419ms step_avg:57.51ms
step:443/2330 train_time:25476ms step_avg:57.51ms
step:444/2330 train_time:25535ms step_avg:57.51ms
step:445/2330 train_time:25591ms step_avg:57.51ms
step:446/2330 train_time:25650ms step_avg:57.51ms
step:447/2330 train_time:25706ms step_avg:57.51ms
step:448/2330 train_time:25766ms step_avg:57.51ms
step:449/2330 train_time:25822ms step_avg:57.51ms
step:450/2330 train_time:25881ms step_avg:57.51ms
step:451/2330 train_time:25936ms step_avg:57.51ms
step:452/2330 train_time:25995ms step_avg:57.51ms
step:453/2330 train_time:26051ms step_avg:57.51ms
step:454/2330 train_time:26110ms step_avg:57.51ms
step:455/2330 train_time:26167ms step_avg:57.51ms
step:456/2330 train_time:26225ms step_avg:57.51ms
step:457/2330 train_time:26282ms step_avg:57.51ms
step:458/2330 train_time:26341ms step_avg:57.51ms
step:459/2330 train_time:26397ms step_avg:57.51ms
step:460/2330 train_time:26456ms step_avg:57.51ms
step:461/2330 train_time:26512ms step_avg:57.51ms
step:462/2330 train_time:26570ms step_avg:57.51ms
step:463/2330 train_time:26626ms step_avg:57.51ms
step:464/2330 train_time:26687ms step_avg:57.51ms
step:465/2330 train_time:26743ms step_avg:57.51ms
step:466/2330 train_time:26802ms step_avg:57.52ms
step:467/2330 train_time:26858ms step_avg:57.51ms
step:468/2330 train_time:26918ms step_avg:57.52ms
step:469/2330 train_time:26973ms step_avg:57.51ms
step:470/2330 train_time:27033ms step_avg:57.52ms
step:471/2330 train_time:27089ms step_avg:57.51ms
step:472/2330 train_time:27148ms step_avg:57.52ms
step:473/2330 train_time:27204ms step_avg:57.51ms
step:474/2330 train_time:27263ms step_avg:57.52ms
step:475/2330 train_time:27319ms step_avg:57.51ms
step:476/2330 train_time:27378ms step_avg:57.52ms
step:477/2330 train_time:27434ms step_avg:57.51ms
step:478/2330 train_time:27494ms step_avg:57.52ms
step:479/2330 train_time:27551ms step_avg:57.52ms
step:480/2330 train_time:27610ms step_avg:57.52ms
step:481/2330 train_time:27667ms step_avg:57.52ms
step:482/2330 train_time:27725ms step_avg:57.52ms
step:483/2330 train_time:27782ms step_avg:57.52ms
step:484/2330 train_time:27840ms step_avg:57.52ms
step:485/2330 train_time:27896ms step_avg:57.52ms
step:486/2330 train_time:27956ms step_avg:57.52ms
step:487/2330 train_time:28012ms step_avg:57.52ms
step:488/2330 train_time:28071ms step_avg:57.52ms
step:489/2330 train_time:28127ms step_avg:57.52ms
step:490/2330 train_time:28186ms step_avg:57.52ms
step:491/2330 train_time:28242ms step_avg:57.52ms
step:492/2330 train_time:28301ms step_avg:57.52ms
step:493/2330 train_time:28358ms step_avg:57.52ms
step:494/2330 train_time:28416ms step_avg:57.52ms
step:495/2330 train_time:28473ms step_avg:57.52ms
step:496/2330 train_time:28532ms step_avg:57.52ms
step:497/2330 train_time:28589ms step_avg:57.52ms
step:498/2330 train_time:28648ms step_avg:57.53ms
step:499/2330 train_time:28705ms step_avg:57.52ms
step:500/2330 train_time:28764ms step_avg:57.53ms
step:500/2330 val_loss:4.4110 train_time:28844ms step_avg:57.69ms
step:501/2330 train_time:28863ms step_avg:57.61ms
step:502/2330 train_time:28884ms step_avg:57.54ms
step:503/2330 train_time:28940ms step_avg:57.54ms
step:504/2330 train_time:29003ms step_avg:57.54ms
step:505/2330 train_time:29060ms step_avg:57.54ms
step:506/2330 train_time:29119ms step_avg:57.55ms
step:507/2330 train_time:29176ms step_avg:57.55ms
step:508/2330 train_time:29235ms step_avg:57.55ms
step:509/2330 train_time:29290ms step_avg:57.54ms
step:510/2330 train_time:29349ms step_avg:57.55ms
step:511/2330 train_time:29404ms step_avg:57.54ms
step:512/2330 train_time:29462ms step_avg:57.54ms
step:513/2330 train_time:29518ms step_avg:57.54ms
step:514/2330 train_time:29576ms step_avg:57.54ms
step:515/2330 train_time:29632ms step_avg:57.54ms
step:516/2330 train_time:29690ms step_avg:57.54ms
step:517/2330 train_time:29745ms step_avg:57.53ms
step:518/2330 train_time:29805ms step_avg:57.54ms
step:519/2330 train_time:29861ms step_avg:57.54ms
step:520/2330 train_time:29922ms step_avg:57.54ms
step:521/2330 train_time:29979ms step_avg:57.54ms
step:522/2330 train_time:30039ms step_avg:57.55ms
step:523/2330 train_time:30096ms step_avg:57.55ms
step:524/2330 train_time:30155ms step_avg:57.55ms
step:525/2330 train_time:30212ms step_avg:57.55ms
step:526/2330 train_time:30271ms step_avg:57.55ms
step:527/2330 train_time:30327ms step_avg:57.55ms
step:528/2330 train_time:30386ms step_avg:57.55ms
step:529/2330 train_time:30441ms step_avg:57.55ms
step:530/2330 train_time:30500ms step_avg:57.55ms
step:531/2330 train_time:30555ms step_avg:57.54ms
step:532/2330 train_time:30614ms step_avg:57.55ms
step:533/2330 train_time:30671ms step_avg:57.54ms
step:534/2330 train_time:30729ms step_avg:57.55ms
step:535/2330 train_time:30786ms step_avg:57.54ms
step:536/2330 train_time:30845ms step_avg:57.55ms
step:537/2330 train_time:30901ms step_avg:57.54ms
step:538/2330 train_time:30962ms step_avg:57.55ms
step:539/2330 train_time:31019ms step_avg:57.55ms
step:540/2330 train_time:31078ms step_avg:57.55ms
step:541/2330 train_time:31135ms step_avg:57.55ms
step:542/2330 train_time:31194ms step_avg:57.55ms
step:543/2330 train_time:31251ms step_avg:57.55ms
step:544/2330 train_time:31310ms step_avg:57.55ms
step:545/2330 train_time:31366ms step_avg:57.55ms
step:546/2330 train_time:31425ms step_avg:57.55ms
step:547/2330 train_time:31480ms step_avg:57.55ms
step:548/2330 train_time:31539ms step_avg:57.55ms
step:549/2330 train_time:31595ms step_avg:57.55ms
step:550/2330 train_time:31654ms step_avg:57.55ms
step:551/2330 train_time:31711ms step_avg:57.55ms
step:552/2330 train_time:31769ms step_avg:57.55ms
step:553/2330 train_time:31826ms step_avg:57.55ms
step:554/2330 train_time:31885ms step_avg:57.55ms
step:555/2330 train_time:31941ms step_avg:57.55ms
step:556/2330 train_time:32001ms step_avg:57.56ms
step:557/2330 train_time:32057ms step_avg:57.55ms
step:558/2330 train_time:32118ms step_avg:57.56ms
step:559/2330 train_time:32175ms step_avg:57.56ms
step:560/2330 train_time:32234ms step_avg:57.56ms
step:561/2330 train_time:32291ms step_avg:57.56ms
step:562/2330 train_time:32350ms step_avg:57.56ms
step:563/2330 train_time:32406ms step_avg:57.56ms
step:564/2330 train_time:32465ms step_avg:57.56ms
step:565/2330 train_time:32521ms step_avg:57.56ms
step:566/2330 train_time:32580ms step_avg:57.56ms
step:567/2330 train_time:32636ms step_avg:57.56ms
step:568/2330 train_time:32695ms step_avg:57.56ms
step:569/2330 train_time:32751ms step_avg:57.56ms
step:570/2330 train_time:32810ms step_avg:57.56ms
step:571/2330 train_time:32866ms step_avg:57.56ms
step:572/2330 train_time:32926ms step_avg:57.56ms
step:573/2330 train_time:32982ms step_avg:57.56ms
step:574/2330 train_time:33043ms step_avg:57.57ms
step:575/2330 train_time:33098ms step_avg:57.56ms
step:576/2330 train_time:33158ms step_avg:57.57ms
step:577/2330 train_time:33216ms step_avg:57.57ms
step:578/2330 train_time:33274ms step_avg:57.57ms
step:579/2330 train_time:33331ms step_avg:57.57ms
step:580/2330 train_time:33389ms step_avg:57.57ms
step:581/2330 train_time:33445ms step_avg:57.57ms
step:582/2330 train_time:33504ms step_avg:57.57ms
step:583/2330 train_time:33560ms step_avg:57.56ms
step:584/2330 train_time:33620ms step_avg:57.57ms
step:585/2330 train_time:33676ms step_avg:57.57ms
step:586/2330 train_time:33735ms step_avg:57.57ms
step:587/2330 train_time:33792ms step_avg:57.57ms
step:588/2330 train_time:33851ms step_avg:57.57ms
step:589/2330 train_time:33907ms step_avg:57.57ms
step:590/2330 train_time:33967ms step_avg:57.57ms
step:591/2330 train_time:34023ms step_avg:57.57ms
step:592/2330 train_time:34083ms step_avg:57.57ms
step:593/2330 train_time:34139ms step_avg:57.57ms
step:594/2330 train_time:34199ms step_avg:57.57ms
step:595/2330 train_time:34255ms step_avg:57.57ms
step:596/2330 train_time:34316ms step_avg:57.58ms
step:597/2330 train_time:34372ms step_avg:57.57ms
step:598/2330 train_time:34431ms step_avg:57.58ms
step:599/2330 train_time:34488ms step_avg:57.58ms
step:600/2330 train_time:34547ms step_avg:57.58ms
step:601/2330 train_time:34603ms step_avg:57.58ms
step:602/2330 train_time:34662ms step_avg:57.58ms
step:603/2330 train_time:34718ms step_avg:57.58ms
step:604/2330 train_time:34777ms step_avg:57.58ms
step:605/2330 train_time:34833ms step_avg:57.58ms
step:606/2330 train_time:34893ms step_avg:57.58ms
step:607/2330 train_time:34949ms step_avg:57.58ms
step:608/2330 train_time:35008ms step_avg:57.58ms
step:609/2330 train_time:35064ms step_avg:57.58ms
step:610/2330 train_time:35124ms step_avg:57.58ms
step:611/2330 train_time:35179ms step_avg:57.58ms
step:612/2330 train_time:35240ms step_avg:57.58ms
step:613/2330 train_time:35295ms step_avg:57.58ms
step:614/2330 train_time:35356ms step_avg:57.58ms
step:615/2330 train_time:35413ms step_avg:57.58ms
step:616/2330 train_time:35472ms step_avg:57.58ms
step:617/2330 train_time:35528ms step_avg:57.58ms
step:618/2330 train_time:35588ms step_avg:57.59ms
step:619/2330 train_time:35644ms step_avg:57.58ms
step:620/2330 train_time:35703ms step_avg:57.59ms
step:621/2330 train_time:35759ms step_avg:57.58ms
step:622/2330 train_time:35820ms step_avg:57.59ms
step:623/2330 train_time:35876ms step_avg:57.59ms
step:624/2330 train_time:35935ms step_avg:57.59ms
step:625/2330 train_time:35991ms step_avg:57.59ms
step:626/2330 train_time:36050ms step_avg:57.59ms
step:627/2330 train_time:36107ms step_avg:57.59ms
step:628/2330 train_time:36167ms step_avg:57.59ms
step:629/2330 train_time:36223ms step_avg:57.59ms
step:630/2330 train_time:36283ms step_avg:57.59ms
step:631/2330 train_time:36338ms step_avg:57.59ms
step:632/2330 train_time:36398ms step_avg:57.59ms
step:633/2330 train_time:36455ms step_avg:57.59ms
step:634/2330 train_time:36515ms step_avg:57.59ms
step:635/2330 train_time:36571ms step_avg:57.59ms
step:636/2330 train_time:36630ms step_avg:57.59ms
step:637/2330 train_time:36685ms step_avg:57.59ms
step:638/2330 train_time:36746ms step_avg:57.60ms
step:639/2330 train_time:36802ms step_avg:57.59ms
step:640/2330 train_time:36861ms step_avg:57.60ms
step:641/2330 train_time:36917ms step_avg:57.59ms
step:642/2330 train_time:36977ms step_avg:57.60ms
step:643/2330 train_time:37033ms step_avg:57.59ms
step:644/2330 train_time:37093ms step_avg:57.60ms
step:645/2330 train_time:37149ms step_avg:57.59ms
step:646/2330 train_time:37209ms step_avg:57.60ms
step:647/2330 train_time:37265ms step_avg:57.60ms
step:648/2330 train_time:37324ms step_avg:57.60ms
step:649/2330 train_time:37380ms step_avg:57.60ms
step:650/2330 train_time:37439ms step_avg:57.60ms
step:651/2330 train_time:37496ms step_avg:57.60ms
step:652/2330 train_time:37555ms step_avg:57.60ms
step:653/2330 train_time:37611ms step_avg:57.60ms
step:654/2330 train_time:37671ms step_avg:57.60ms
step:655/2330 train_time:37727ms step_avg:57.60ms
step:656/2330 train_time:37787ms step_avg:57.60ms
step:657/2330 train_time:37843ms step_avg:57.60ms
step:658/2330 train_time:37903ms step_avg:57.60ms
step:659/2330 train_time:37958ms step_avg:57.60ms
step:660/2330 train_time:38019ms step_avg:57.60ms
step:661/2330 train_time:38075ms step_avg:57.60ms
step:662/2330 train_time:38135ms step_avg:57.61ms
step:663/2330 train_time:38191ms step_avg:57.60ms
step:664/2330 train_time:38251ms step_avg:57.61ms
step:665/2330 train_time:38307ms step_avg:57.60ms
step:666/2330 train_time:38366ms step_avg:57.61ms
step:667/2330 train_time:38422ms step_avg:57.60ms
step:668/2330 train_time:38481ms step_avg:57.61ms
step:669/2330 train_time:38537ms step_avg:57.60ms
step:670/2330 train_time:38596ms step_avg:57.61ms
step:671/2330 train_time:38653ms step_avg:57.60ms
step:672/2330 train_time:38712ms step_avg:57.61ms
step:673/2330 train_time:38768ms step_avg:57.61ms
step:674/2330 train_time:38829ms step_avg:57.61ms
step:675/2330 train_time:38885ms step_avg:57.61ms
step:676/2330 train_time:38945ms step_avg:57.61ms
step:677/2330 train_time:39000ms step_avg:57.61ms
step:678/2330 train_time:39059ms step_avg:57.61ms
step:679/2330 train_time:39116ms step_avg:57.61ms
step:680/2330 train_time:39175ms step_avg:57.61ms
step:681/2330 train_time:39231ms step_avg:57.61ms
step:682/2330 train_time:39290ms step_avg:57.61ms
step:683/2330 train_time:39346ms step_avg:57.61ms
step:684/2330 train_time:39404ms step_avg:57.61ms
step:685/2330 train_time:39460ms step_avg:57.61ms
step:686/2330 train_time:39520ms step_avg:57.61ms
step:687/2330 train_time:39575ms step_avg:57.61ms
step:688/2330 train_time:39636ms step_avg:57.61ms
step:689/2330 train_time:39692ms step_avg:57.61ms
step:690/2330 train_time:39751ms step_avg:57.61ms
step:691/2330 train_time:39808ms step_avg:57.61ms
step:692/2330 train_time:39867ms step_avg:57.61ms
step:693/2330 train_time:39924ms step_avg:57.61ms
step:694/2330 train_time:39983ms step_avg:57.61ms
step:695/2330 train_time:40038ms step_avg:57.61ms
step:696/2330 train_time:40098ms step_avg:57.61ms
step:697/2330 train_time:40153ms step_avg:57.61ms
step:698/2330 train_time:40214ms step_avg:57.61ms
step:699/2330 train_time:40271ms step_avg:57.61ms
step:700/2330 train_time:40330ms step_avg:57.61ms
step:701/2330 train_time:40387ms step_avg:57.61ms
step:702/2330 train_time:40446ms step_avg:57.62ms
step:703/2330 train_time:40502ms step_avg:57.61ms
step:704/2330 train_time:40561ms step_avg:57.61ms
step:705/2330 train_time:40617ms step_avg:57.61ms
step:706/2330 train_time:40677ms step_avg:57.62ms
step:707/2330 train_time:40734ms step_avg:57.62ms
step:708/2330 train_time:40794ms step_avg:57.62ms
step:709/2330 train_time:40850ms step_avg:57.62ms
step:710/2330 train_time:40909ms step_avg:57.62ms
step:711/2330 train_time:40965ms step_avg:57.62ms
step:712/2330 train_time:41025ms step_avg:57.62ms
step:713/2330 train_time:41080ms step_avg:57.62ms
step:714/2330 train_time:41140ms step_avg:57.62ms
step:715/2330 train_time:41196ms step_avg:57.62ms
step:716/2330 train_time:41256ms step_avg:57.62ms
step:717/2330 train_time:41312ms step_avg:57.62ms
step:718/2330 train_time:41372ms step_avg:57.62ms
step:719/2330 train_time:41428ms step_avg:57.62ms
step:720/2330 train_time:41489ms step_avg:57.62ms
step:721/2330 train_time:41544ms step_avg:57.62ms
step:722/2330 train_time:41605ms step_avg:57.62ms
step:723/2330 train_time:41661ms step_avg:57.62ms
step:724/2330 train_time:41720ms step_avg:57.62ms
step:725/2330 train_time:41775ms step_avg:57.62ms
step:726/2330 train_time:41835ms step_avg:57.62ms
step:727/2330 train_time:41892ms step_avg:57.62ms
step:728/2330 train_time:41952ms step_avg:57.63ms
step:729/2330 train_time:42008ms step_avg:57.62ms
step:730/2330 train_time:42066ms step_avg:57.63ms
step:731/2330 train_time:42123ms step_avg:57.62ms
step:732/2330 train_time:42182ms step_avg:57.63ms
step:733/2330 train_time:42237ms step_avg:57.62ms
step:734/2330 train_time:42297ms step_avg:57.63ms
step:735/2330 train_time:42354ms step_avg:57.63ms
step:736/2330 train_time:42414ms step_avg:57.63ms
step:737/2330 train_time:42471ms step_avg:57.63ms
step:738/2330 train_time:42530ms step_avg:57.63ms
step:739/2330 train_time:42586ms step_avg:57.63ms
step:740/2330 train_time:42646ms step_avg:57.63ms
step:741/2330 train_time:42702ms step_avg:57.63ms
step:742/2330 train_time:42762ms step_avg:57.63ms
step:743/2330 train_time:42818ms step_avg:57.63ms
step:744/2330 train_time:42878ms step_avg:57.63ms
step:745/2330 train_time:42933ms step_avg:57.63ms
step:746/2330 train_time:42993ms step_avg:57.63ms
step:747/2330 train_time:43050ms step_avg:57.63ms
step:748/2330 train_time:43109ms step_avg:57.63ms
step:749/2330 train_time:43164ms step_avg:57.63ms
step:750/2330 train_time:43224ms step_avg:57.63ms
step:750/2330 val_loss:4.2133 train_time:43304ms step_avg:57.74ms
step:751/2330 train_time:43323ms step_avg:57.69ms
step:752/2330 train_time:43343ms step_avg:57.64ms
step:753/2330 train_time:43401ms step_avg:57.64ms
step:754/2330 train_time:43463ms step_avg:57.64ms
step:755/2330 train_time:43520ms step_avg:57.64ms
step:756/2330 train_time:43583ms step_avg:57.65ms
step:757/2330 train_time:43639ms step_avg:57.65ms
step:758/2330 train_time:43698ms step_avg:57.65ms
step:759/2330 train_time:43754ms step_avg:57.65ms
step:760/2330 train_time:43813ms step_avg:57.65ms
step:761/2330 train_time:43868ms step_avg:57.65ms
step:762/2330 train_time:43927ms step_avg:57.65ms
step:763/2330 train_time:43982ms step_avg:57.64ms
step:764/2330 train_time:44040ms step_avg:57.64ms
step:765/2330 train_time:44097ms step_avg:57.64ms
step:766/2330 train_time:44155ms step_avg:57.64ms
step:767/2330 train_time:44212ms step_avg:57.64ms
step:768/2330 train_time:44272ms step_avg:57.65ms
step:769/2330 train_time:44330ms step_avg:57.65ms
step:770/2330 train_time:44392ms step_avg:57.65ms
step:771/2330 train_time:44450ms step_avg:57.65ms
step:772/2330 train_time:44511ms step_avg:57.66ms
step:773/2330 train_time:44568ms step_avg:57.66ms
step:774/2330 train_time:44630ms step_avg:57.66ms
step:775/2330 train_time:44687ms step_avg:57.66ms
step:776/2330 train_time:44746ms step_avg:57.66ms
step:777/2330 train_time:44802ms step_avg:57.66ms
step:778/2330 train_time:44862ms step_avg:57.66ms
step:779/2330 train_time:44918ms step_avg:57.66ms
step:780/2330 train_time:44978ms step_avg:57.66ms
step:781/2330 train_time:45035ms step_avg:57.66ms
step:782/2330 train_time:45094ms step_avg:57.66ms
step:783/2330 train_time:45150ms step_avg:57.66ms
step:784/2330 train_time:45210ms step_avg:57.67ms
step:785/2330 train_time:45266ms step_avg:57.66ms
step:786/2330 train_time:45327ms step_avg:57.67ms
step:787/2330 train_time:45384ms step_avg:57.67ms
step:788/2330 train_time:45445ms step_avg:57.67ms
step:789/2330 train_time:45502ms step_avg:57.67ms
step:790/2330 train_time:45563ms step_avg:57.67ms
step:791/2330 train_time:45620ms step_avg:57.67ms
step:792/2330 train_time:45680ms step_avg:57.68ms
step:793/2330 train_time:45737ms step_avg:57.68ms
step:794/2330 train_time:45797ms step_avg:57.68ms
step:795/2330 train_time:45854ms step_avg:57.68ms
step:796/2330 train_time:45915ms step_avg:57.68ms
step:797/2330 train_time:45972ms step_avg:57.68ms
step:798/2330 train_time:46031ms step_avg:57.68ms
step:799/2330 train_time:46088ms step_avg:57.68ms
step:800/2330 train_time:46147ms step_avg:57.68ms
step:801/2330 train_time:46204ms step_avg:57.68ms
step:802/2330 train_time:46263ms step_avg:57.68ms
step:803/2330 train_time:46320ms step_avg:57.68ms
step:804/2330 train_time:46381ms step_avg:57.69ms
step:805/2330 train_time:46438ms step_avg:57.69ms
step:806/2330 train_time:46498ms step_avg:57.69ms
step:807/2330 train_time:46555ms step_avg:57.69ms
step:808/2330 train_time:46616ms step_avg:57.69ms
step:809/2330 train_time:46674ms step_avg:57.69ms
step:810/2330 train_time:46733ms step_avg:57.70ms
step:811/2330 train_time:46790ms step_avg:57.69ms
step:812/2330 train_time:46849ms step_avg:57.70ms
step:813/2330 train_time:46906ms step_avg:57.70ms
step:814/2330 train_time:46966ms step_avg:57.70ms
step:815/2330 train_time:47022ms step_avg:57.70ms
step:816/2330 train_time:47082ms step_avg:57.70ms
step:817/2330 train_time:47138ms step_avg:57.70ms
step:818/2330 train_time:47198ms step_avg:57.70ms
step:819/2330 train_time:47255ms step_avg:57.70ms
step:820/2330 train_time:47315ms step_avg:57.70ms
step:821/2330 train_time:47373ms step_avg:57.70ms
step:822/2330 train_time:47433ms step_avg:57.70ms
step:823/2330 train_time:47490ms step_avg:57.70ms
step:824/2330 train_time:47551ms step_avg:57.71ms
step:825/2330 train_time:47609ms step_avg:57.71ms
step:826/2330 train_time:47669ms step_avg:57.71ms
step:827/2330 train_time:47726ms step_avg:57.71ms
step:828/2330 train_time:47786ms step_avg:57.71ms
step:829/2330 train_time:47843ms step_avg:57.71ms
step:830/2330 train_time:47903ms step_avg:57.71ms
step:831/2330 train_time:47959ms step_avg:57.71ms
step:832/2330 train_time:48018ms step_avg:57.71ms
step:833/2330 train_time:48075ms step_avg:57.71ms
step:834/2330 train_time:48135ms step_avg:57.72ms
step:835/2330 train_time:48192ms step_avg:57.71ms
step:836/2330 train_time:48252ms step_avg:57.72ms
step:837/2330 train_time:48310ms step_avg:57.72ms
step:838/2330 train_time:48369ms step_avg:57.72ms
step:839/2330 train_time:48426ms step_avg:57.72ms
step:840/2330 train_time:48486ms step_avg:57.72ms
step:841/2330 train_time:48543ms step_avg:57.72ms
step:842/2330 train_time:48603ms step_avg:57.72ms
step:843/2330 train_time:48659ms step_avg:57.72ms
step:844/2330 train_time:48721ms step_avg:57.73ms
step:845/2330 train_time:48777ms step_avg:57.72ms
step:846/2330 train_time:48839ms step_avg:57.73ms
step:847/2330 train_time:48896ms step_avg:57.73ms
step:848/2330 train_time:48955ms step_avg:57.73ms
step:849/2330 train_time:49013ms step_avg:57.73ms
step:850/2330 train_time:49072ms step_avg:57.73ms
step:851/2330 train_time:49129ms step_avg:57.73ms
step:852/2330 train_time:49189ms step_avg:57.73ms
step:853/2330 train_time:49246ms step_avg:57.73ms
step:854/2330 train_time:49306ms step_avg:57.74ms
step:855/2330 train_time:49364ms step_avg:57.74ms
step:856/2330 train_time:49423ms step_avg:57.74ms
step:857/2330 train_time:49480ms step_avg:57.74ms
step:858/2330 train_time:49541ms step_avg:57.74ms
step:859/2330 train_time:49598ms step_avg:57.74ms
step:860/2330 train_time:49659ms step_avg:57.74ms
step:861/2330 train_time:49716ms step_avg:57.74ms
step:862/2330 train_time:49776ms step_avg:57.74ms
step:863/2330 train_time:49833ms step_avg:57.74ms
step:864/2330 train_time:49893ms step_avg:57.75ms
step:865/2330 train_time:49950ms step_avg:57.75ms
step:866/2330 train_time:50010ms step_avg:57.75ms
step:867/2330 train_time:50067ms step_avg:57.75ms
step:868/2330 train_time:50126ms step_avg:57.75ms
step:869/2330 train_time:50182ms step_avg:57.75ms
step:870/2330 train_time:50243ms step_avg:57.75ms
step:871/2330 train_time:50300ms step_avg:57.75ms
step:872/2330 train_time:50360ms step_avg:57.75ms
step:873/2330 train_time:50417ms step_avg:57.75ms
step:874/2330 train_time:50477ms step_avg:57.75ms
step:875/2330 train_time:50534ms step_avg:57.75ms
step:876/2330 train_time:50595ms step_avg:57.76ms
step:877/2330 train_time:50652ms step_avg:57.76ms
step:878/2330 train_time:50712ms step_avg:57.76ms
step:879/2330 train_time:50769ms step_avg:57.76ms
step:880/2330 train_time:50829ms step_avg:57.76ms
step:881/2330 train_time:50885ms step_avg:57.76ms
step:882/2330 train_time:50945ms step_avg:57.76ms
step:883/2330 train_time:51002ms step_avg:57.76ms
step:884/2330 train_time:51061ms step_avg:57.76ms
step:885/2330 train_time:51118ms step_avg:57.76ms
step:886/2330 train_time:51178ms step_avg:57.76ms
step:887/2330 train_time:51236ms step_avg:57.76ms
step:888/2330 train_time:51296ms step_avg:57.77ms
step:889/2330 train_time:51354ms step_avg:57.77ms
step:890/2330 train_time:51413ms step_avg:57.77ms
step:891/2330 train_time:51471ms step_avg:57.77ms
step:892/2330 train_time:51531ms step_avg:57.77ms
step:893/2330 train_time:51588ms step_avg:57.77ms
step:894/2330 train_time:51648ms step_avg:57.77ms
step:895/2330 train_time:51704ms step_avg:57.77ms
step:896/2330 train_time:51763ms step_avg:57.77ms
step:897/2330 train_time:51821ms step_avg:57.77ms
step:898/2330 train_time:51880ms step_avg:57.77ms
step:899/2330 train_time:51938ms step_avg:57.77ms
step:900/2330 train_time:51998ms step_avg:57.78ms
step:901/2330 train_time:52055ms step_avg:57.77ms
step:902/2330 train_time:52115ms step_avg:57.78ms
step:903/2330 train_time:52172ms step_avg:57.78ms
step:904/2330 train_time:52232ms step_avg:57.78ms
step:905/2330 train_time:52290ms step_avg:57.78ms
step:906/2330 train_time:52350ms step_avg:57.78ms
step:907/2330 train_time:52407ms step_avg:57.78ms
step:908/2330 train_time:52467ms step_avg:57.78ms
step:909/2330 train_time:52523ms step_avg:57.78ms
step:910/2330 train_time:52584ms step_avg:57.78ms
step:911/2330 train_time:52641ms step_avg:57.78ms
step:912/2330 train_time:52701ms step_avg:57.79ms
step:913/2330 train_time:52758ms step_avg:57.79ms
step:914/2330 train_time:52818ms step_avg:57.79ms
step:915/2330 train_time:52876ms step_avg:57.79ms
step:916/2330 train_time:52935ms step_avg:57.79ms
step:917/2330 train_time:52992ms step_avg:57.79ms
step:918/2330 train_time:53052ms step_avg:57.79ms
step:919/2330 train_time:53109ms step_avg:57.79ms
step:920/2330 train_time:53168ms step_avg:57.79ms
step:921/2330 train_time:53226ms step_avg:57.79ms
step:922/2330 train_time:53286ms step_avg:57.79ms
step:923/2330 train_time:53343ms step_avg:57.79ms
step:924/2330 train_time:53403ms step_avg:57.80ms
step:925/2330 train_time:53460ms step_avg:57.79ms
step:926/2330 train_time:53520ms step_avg:57.80ms
step:927/2330 train_time:53577ms step_avg:57.80ms
step:928/2330 train_time:53637ms step_avg:57.80ms
step:929/2330 train_time:53694ms step_avg:57.80ms
step:930/2330 train_time:53755ms step_avg:57.80ms
step:931/2330 train_time:53812ms step_avg:57.80ms
step:932/2330 train_time:53871ms step_avg:57.80ms
step:933/2330 train_time:53928ms step_avg:57.80ms
step:934/2330 train_time:53988ms step_avg:57.80ms
step:935/2330 train_time:54045ms step_avg:57.80ms
step:936/2330 train_time:54104ms step_avg:57.80ms
step:937/2330 train_time:54162ms step_avg:57.80ms
step:938/2330 train_time:54221ms step_avg:57.81ms
step:939/2330 train_time:54278ms step_avg:57.80ms
step:940/2330 train_time:54338ms step_avg:57.81ms
step:941/2330 train_time:54395ms step_avg:57.81ms
step:942/2330 train_time:54455ms step_avg:57.81ms
step:943/2330 train_time:54512ms step_avg:57.81ms
step:944/2330 train_time:54572ms step_avg:57.81ms
step:945/2330 train_time:54629ms step_avg:57.81ms
step:946/2330 train_time:54689ms step_avg:57.81ms
step:947/2330 train_time:54747ms step_avg:57.81ms
step:948/2330 train_time:54806ms step_avg:57.81ms
step:949/2330 train_time:54862ms step_avg:57.81ms
step:950/2330 train_time:54922ms step_avg:57.81ms
step:951/2330 train_time:54979ms step_avg:57.81ms
step:952/2330 train_time:55039ms step_avg:57.81ms
step:953/2330 train_time:55096ms step_avg:57.81ms
step:954/2330 train_time:55156ms step_avg:57.82ms
step:955/2330 train_time:55213ms step_avg:57.81ms
step:956/2330 train_time:55273ms step_avg:57.82ms
step:957/2330 train_time:55330ms step_avg:57.82ms
step:958/2330 train_time:55390ms step_avg:57.82ms
step:959/2330 train_time:55447ms step_avg:57.82ms
step:960/2330 train_time:55507ms step_avg:57.82ms
step:961/2330 train_time:55563ms step_avg:57.82ms
step:962/2330 train_time:55623ms step_avg:57.82ms
step:963/2330 train_time:55680ms step_avg:57.82ms
step:964/2330 train_time:55740ms step_avg:57.82ms
step:965/2330 train_time:55797ms step_avg:57.82ms
step:966/2330 train_time:55857ms step_avg:57.82ms
step:967/2330 train_time:55914ms step_avg:57.82ms
step:968/2330 train_time:55974ms step_avg:57.82ms
step:969/2330 train_time:56031ms step_avg:57.82ms
step:970/2330 train_time:56091ms step_avg:57.83ms
step:971/2330 train_time:56148ms step_avg:57.82ms
step:972/2330 train_time:56209ms step_avg:57.83ms
step:973/2330 train_time:56266ms step_avg:57.83ms
step:974/2330 train_time:56327ms step_avg:57.83ms
step:975/2330 train_time:56384ms step_avg:57.83ms
step:976/2330 train_time:56443ms step_avg:57.83ms
step:977/2330 train_time:56500ms step_avg:57.83ms
step:978/2330 train_time:56560ms step_avg:57.83ms
step:979/2330 train_time:56617ms step_avg:57.83ms
step:980/2330 train_time:56677ms step_avg:57.83ms
step:981/2330 train_time:56735ms step_avg:57.83ms
step:982/2330 train_time:56795ms step_avg:57.84ms
step:983/2330 train_time:56853ms step_avg:57.84ms
step:984/2330 train_time:56913ms step_avg:57.84ms
step:985/2330 train_time:56970ms step_avg:57.84ms
step:986/2330 train_time:57029ms step_avg:57.84ms
step:987/2330 train_time:57086ms step_avg:57.84ms
step:988/2330 train_time:57146ms step_avg:57.84ms
step:989/2330 train_time:57203ms step_avg:57.84ms
step:990/2330 train_time:57263ms step_avg:57.84ms
step:991/2330 train_time:57321ms step_avg:57.84ms
step:992/2330 train_time:57381ms step_avg:57.84ms
step:993/2330 train_time:57438ms step_avg:57.84ms
step:994/2330 train_time:57498ms step_avg:57.85ms
step:995/2330 train_time:57556ms step_avg:57.84ms
step:996/2330 train_time:57616ms step_avg:57.85ms
step:997/2330 train_time:57673ms step_avg:57.85ms
step:998/2330 train_time:57735ms step_avg:57.85ms
step:999/2330 train_time:57792ms step_avg:57.85ms
step:1000/2330 train_time:57852ms step_avg:57.85ms
step:1000/2330 val_loss:4.0689 train_time:57933ms step_avg:57.93ms
step:1001/2330 train_time:57952ms step_avg:57.89ms
step:1002/2330 train_time:57971ms step_avg:57.86ms
step:1003/2330 train_time:58025ms step_avg:57.85ms
step:1004/2330 train_time:58090ms step_avg:57.86ms
step:1005/2330 train_time:58146ms step_avg:57.86ms
step:1006/2330 train_time:58214ms step_avg:57.87ms
step:1007/2330 train_time:58270ms step_avg:57.87ms
step:1008/2330 train_time:58330ms step_avg:57.87ms
step:1009/2330 train_time:58387ms step_avg:57.87ms
step:1010/2330 train_time:58446ms step_avg:57.87ms
step:1011/2330 train_time:58502ms step_avg:57.87ms
step:1012/2330 train_time:58562ms step_avg:57.87ms
step:1013/2330 train_time:58618ms step_avg:57.87ms
step:1014/2330 train_time:58677ms step_avg:57.87ms
step:1015/2330 train_time:58733ms step_avg:57.87ms
step:1016/2330 train_time:58792ms step_avg:57.87ms
step:1017/2330 train_time:58850ms step_avg:57.87ms
step:1018/2330 train_time:58914ms step_avg:57.87ms
step:1019/2330 train_time:58972ms step_avg:57.87ms
step:1020/2330 train_time:59035ms step_avg:57.88ms
step:1021/2330 train_time:59094ms step_avg:57.88ms
step:1022/2330 train_time:59154ms step_avg:57.88ms
step:1023/2330 train_time:59212ms step_avg:57.88ms
step:1024/2330 train_time:59272ms step_avg:57.88ms
step:1025/2330 train_time:59329ms step_avg:57.88ms
step:1026/2330 train_time:59388ms step_avg:57.88ms
step:1027/2330 train_time:59445ms step_avg:57.88ms
step:1028/2330 train_time:59504ms step_avg:57.88ms
step:1029/2330 train_time:59560ms step_avg:57.88ms
step:1030/2330 train_time:59619ms step_avg:57.88ms
step:1031/2330 train_time:59676ms step_avg:57.88ms
step:1032/2330 train_time:59735ms step_avg:57.88ms
step:1033/2330 train_time:59792ms step_avg:57.88ms
step:1034/2330 train_time:59853ms step_avg:57.88ms
step:1035/2330 train_time:59911ms step_avg:57.88ms
step:1036/2330 train_time:59973ms step_avg:57.89ms
step:1037/2330 train_time:60030ms step_avg:57.89ms
step:1038/2330 train_time:60091ms step_avg:57.89ms
step:1039/2330 train_time:60148ms step_avg:57.89ms
step:1040/2330 train_time:60209ms step_avg:57.89ms
step:1041/2330 train_time:60266ms step_avg:57.89ms
step:1042/2330 train_time:60325ms step_avg:57.89ms
step:1043/2330 train_time:60382ms step_avg:57.89ms
step:1044/2330 train_time:60441ms step_avg:57.89ms
step:1045/2330 train_time:60498ms step_avg:57.89ms
step:1046/2330 train_time:60557ms step_avg:57.89ms
step:1047/2330 train_time:60614ms step_avg:57.89ms
step:1048/2330 train_time:60673ms step_avg:57.89ms
step:1049/2330 train_time:60730ms step_avg:57.89ms
step:1050/2330 train_time:60790ms step_avg:57.90ms
step:1051/2330 train_time:60847ms step_avg:57.89ms
step:1052/2330 train_time:60908ms step_avg:57.90ms
step:1053/2330 train_time:60966ms step_avg:57.90ms
step:1054/2330 train_time:61026ms step_avg:57.90ms
step:1055/2330 train_time:61082ms step_avg:57.90ms
step:1056/2330 train_time:61143ms step_avg:57.90ms
step:1057/2330 train_time:61200ms step_avg:57.90ms
step:1058/2330 train_time:61261ms step_avg:57.90ms
step:1059/2330 train_time:61318ms step_avg:57.90ms
step:1060/2330 train_time:61378ms step_avg:57.90ms
step:1061/2330 train_time:61434ms step_avg:57.90ms
step:1062/2330 train_time:61495ms step_avg:57.90ms
step:1063/2330 train_time:61552ms step_avg:57.90ms
step:1064/2330 train_time:61611ms step_avg:57.91ms
step:1065/2330 train_time:61667ms step_avg:57.90ms
step:1066/2330 train_time:61727ms step_avg:57.91ms
step:1067/2330 train_time:61784ms step_avg:57.90ms
step:1068/2330 train_time:61846ms step_avg:57.91ms
step:1069/2330 train_time:61903ms step_avg:57.91ms
step:1070/2330 train_time:61963ms step_avg:57.91ms
step:1071/2330 train_time:62020ms step_avg:57.91ms
step:1072/2330 train_time:62081ms step_avg:57.91ms
step:1073/2330 train_time:62137ms step_avg:57.91ms
step:1074/2330 train_time:62199ms step_avg:57.91ms
step:1075/2330 train_time:62257ms step_avg:57.91ms
step:1076/2330 train_time:62317ms step_avg:57.92ms
step:1077/2330 train_time:62374ms step_avg:57.91ms
step:1078/2330 train_time:62433ms step_avg:57.92ms
step:1079/2330 train_time:62490ms step_avg:57.91ms
step:1080/2330 train_time:62550ms step_avg:57.92ms
step:1081/2330 train_time:62606ms step_avg:57.92ms
step:1082/2330 train_time:62666ms step_avg:57.92ms
step:1083/2330 train_time:62723ms step_avg:57.92ms
step:1084/2330 train_time:62784ms step_avg:57.92ms
step:1085/2330 train_time:62840ms step_avg:57.92ms
step:1086/2330 train_time:62901ms step_avg:57.92ms
step:1087/2330 train_time:62958ms step_avg:57.92ms
step:1088/2330 train_time:63018ms step_avg:57.92ms
step:1089/2330 train_time:63075ms step_avg:57.92ms
step:1090/2330 train_time:63135ms step_avg:57.92ms
step:1091/2330 train_time:63192ms step_avg:57.92ms
step:1092/2330 train_time:63253ms step_avg:57.92ms
step:1093/2330 train_time:63309ms step_avg:57.92ms
step:1094/2330 train_time:63371ms step_avg:57.93ms
step:1095/2330 train_time:63428ms step_avg:57.92ms
step:1096/2330 train_time:63488ms step_avg:57.93ms
step:1097/2330 train_time:63544ms step_avg:57.93ms
step:1098/2330 train_time:63605ms step_avg:57.93ms
step:1099/2330 train_time:63661ms step_avg:57.93ms
step:1100/2330 train_time:63721ms step_avg:57.93ms
step:1101/2330 train_time:63778ms step_avg:57.93ms
step:1102/2330 train_time:63838ms step_avg:57.93ms
step:1103/2330 train_time:63894ms step_avg:57.93ms
step:1104/2330 train_time:63956ms step_avg:57.93ms
step:1105/2330 train_time:64013ms step_avg:57.93ms
step:1106/2330 train_time:64074ms step_avg:57.93ms
step:1107/2330 train_time:64130ms step_avg:57.93ms
step:1108/2330 train_time:64190ms step_avg:57.93ms
step:1109/2330 train_time:64246ms step_avg:57.93ms
step:1110/2330 train_time:64308ms step_avg:57.94ms
step:1111/2330 train_time:64365ms step_avg:57.93ms
step:1112/2330 train_time:64424ms step_avg:57.94ms
step:1113/2330 train_time:64481ms step_avg:57.93ms
step:1114/2330 train_time:64541ms step_avg:57.94ms
step:1115/2330 train_time:64598ms step_avg:57.94ms
step:1116/2330 train_time:64658ms step_avg:57.94ms
step:1117/2330 train_time:64715ms step_avg:57.94ms
step:1118/2330 train_time:64775ms step_avg:57.94ms
step:1119/2330 train_time:64831ms step_avg:57.94ms
step:1120/2330 train_time:64892ms step_avg:57.94ms
step:1121/2330 train_time:64948ms step_avg:57.94ms
step:1122/2330 train_time:65009ms step_avg:57.94ms
step:1123/2330 train_time:65066ms step_avg:57.94ms
step:1124/2330 train_time:65127ms step_avg:57.94ms
step:1125/2330 train_time:65184ms step_avg:57.94ms
step:1126/2330 train_time:65245ms step_avg:57.94ms
step:1127/2330 train_time:65301ms step_avg:57.94ms
step:1128/2330 train_time:65361ms step_avg:57.94ms
step:1129/2330 train_time:65418ms step_avg:57.94ms
step:1130/2330 train_time:65478ms step_avg:57.95ms
step:1131/2330 train_time:65536ms step_avg:57.94ms
step:1132/2330 train_time:65595ms step_avg:57.95ms
step:1133/2330 train_time:65652ms step_avg:57.95ms
step:1134/2330 train_time:65712ms step_avg:57.95ms
step:1135/2330 train_time:65770ms step_avg:57.95ms
step:1136/2330 train_time:65829ms step_avg:57.95ms
step:1137/2330 train_time:65886ms step_avg:57.95ms
step:1138/2330 train_time:65946ms step_avg:57.95ms
step:1139/2330 train_time:66004ms step_avg:57.95ms
step:1140/2330 train_time:66064ms step_avg:57.95ms
step:1141/2330 train_time:66120ms step_avg:57.95ms
step:1142/2330 train_time:66180ms step_avg:57.95ms
step:1143/2330 train_time:66238ms step_avg:57.95ms
step:1144/2330 train_time:66298ms step_avg:57.95ms
step:1145/2330 train_time:66356ms step_avg:57.95ms
step:1146/2330 train_time:66824ms step_avg:58.31ms
step:1147/2330 train_time:66879ms step_avg:58.31ms
step:1148/2330 train_time:66939ms step_avg:58.31ms
step:1149/2330 train_time:66995ms step_avg:58.31ms
step:1150/2330 train_time:67055ms step_avg:58.31ms
step:1151/2330 train_time:67112ms step_avg:58.31ms
step:1152/2330 train_time:67171ms step_avg:58.31ms
step:1153/2330 train_time:67226ms step_avg:58.31ms
step:1154/2330 train_time:67286ms step_avg:58.31ms
step:1155/2330 train_time:67342ms step_avg:58.30ms
step:1156/2330 train_time:67402ms step_avg:58.31ms
step:1157/2330 train_time:67458ms step_avg:58.30ms
step:1158/2330 train_time:67517ms step_avg:58.30ms
step:1159/2330 train_time:67573ms step_avg:58.30ms
step:1160/2330 train_time:67632ms step_avg:58.30ms
step:1161/2330 train_time:67693ms step_avg:58.31ms
step:1162/2330 train_time:67756ms step_avg:58.31ms
step:1163/2330 train_time:67815ms step_avg:58.31ms
step:1164/2330 train_time:67877ms step_avg:58.31ms
step:1165/2330 train_time:67934ms step_avg:58.31ms
step:1166/2330 train_time:67995ms step_avg:58.31ms
step:1167/2330 train_time:68053ms step_avg:58.31ms
step:1168/2330 train_time:68112ms step_avg:58.32ms
step:1169/2330 train_time:68170ms step_avg:58.31ms
step:1170/2330 train_time:68229ms step_avg:58.32ms
step:1171/2330 train_time:68285ms step_avg:58.31ms
step:1172/2330 train_time:68344ms step_avg:58.31ms
step:1173/2330 train_time:68401ms step_avg:58.31ms
step:1174/2330 train_time:68460ms step_avg:58.31ms
step:1175/2330 train_time:68516ms step_avg:58.31ms
step:1176/2330 train_time:68576ms step_avg:58.31ms
step:1177/2330 train_time:68634ms step_avg:58.31ms
step:1178/2330 train_time:68695ms step_avg:58.31ms
step:1179/2330 train_time:68754ms step_avg:58.32ms
step:1180/2330 train_time:68814ms step_avg:58.32ms
step:1181/2330 train_time:68872ms step_avg:58.32ms
step:1182/2330 train_time:68934ms step_avg:58.32ms
step:1183/2330 train_time:68990ms step_avg:58.32ms
step:1184/2330 train_time:69052ms step_avg:58.32ms
step:1185/2330 train_time:69108ms step_avg:58.32ms
step:1186/2330 train_time:69168ms step_avg:58.32ms
step:1187/2330 train_time:69224ms step_avg:58.32ms
step:1188/2330 train_time:69285ms step_avg:58.32ms
step:1189/2330 train_time:69341ms step_avg:58.32ms
step:1190/2330 train_time:69400ms step_avg:58.32ms
step:1191/2330 train_time:69456ms step_avg:58.32ms
step:1192/2330 train_time:69516ms step_avg:58.32ms
step:1193/2330 train_time:69573ms step_avg:58.32ms
step:1194/2330 train_time:69633ms step_avg:58.32ms
step:1195/2330 train_time:69691ms step_avg:58.32ms
step:1196/2330 train_time:69752ms step_avg:58.32ms
step:1197/2330 train_time:69809ms step_avg:58.32ms
step:1198/2330 train_time:69870ms step_avg:58.32ms
step:1199/2330 train_time:69927ms step_avg:58.32ms
step:1200/2330 train_time:69987ms step_avg:58.32ms
step:1201/2330 train_time:70044ms step_avg:58.32ms
step:1202/2330 train_time:70105ms step_avg:58.32ms
step:1203/2330 train_time:70162ms step_avg:58.32ms
step:1204/2330 train_time:70222ms step_avg:58.32ms
step:1205/2330 train_time:70278ms step_avg:58.32ms
step:1206/2330 train_time:70338ms step_avg:58.32ms
step:1207/2330 train_time:70395ms step_avg:58.32ms
step:1208/2330 train_time:70454ms step_avg:58.32ms
step:1209/2330 train_time:70511ms step_avg:58.32ms
step:1210/2330 train_time:70571ms step_avg:58.32ms
step:1211/2330 train_time:70628ms step_avg:58.32ms
step:1212/2330 train_time:70687ms step_avg:58.32ms
step:1213/2330 train_time:70744ms step_avg:58.32ms
step:1214/2330 train_time:70805ms step_avg:58.32ms
step:1215/2330 train_time:70862ms step_avg:58.32ms
step:1216/2330 train_time:70923ms step_avg:58.32ms
step:1217/2330 train_time:70980ms step_avg:58.32ms
step:1218/2330 train_time:71041ms step_avg:58.33ms
step:1219/2330 train_time:71098ms step_avg:58.33ms
step:1220/2330 train_time:71158ms step_avg:58.33ms
step:1221/2330 train_time:71216ms step_avg:58.33ms
step:1222/2330 train_time:71275ms step_avg:58.33ms
step:1223/2330 train_time:71331ms step_avg:58.32ms
step:1224/2330 train_time:71392ms step_avg:58.33ms
step:1225/2330 train_time:71448ms step_avg:58.33ms
step:1226/2330 train_time:71509ms step_avg:58.33ms
step:1227/2330 train_time:71566ms step_avg:58.33ms
step:1228/2330 train_time:71625ms step_avg:58.33ms
step:1229/2330 train_time:71682ms step_avg:58.33ms
step:1230/2330 train_time:71743ms step_avg:58.33ms
step:1231/2330 train_time:71800ms step_avg:58.33ms
step:1232/2330 train_time:71862ms step_avg:58.33ms
step:1233/2330 train_time:71919ms step_avg:58.33ms
step:1234/2330 train_time:71979ms step_avg:58.33ms
step:1235/2330 train_time:72036ms step_avg:58.33ms
step:1236/2330 train_time:72096ms step_avg:58.33ms
step:1237/2330 train_time:72154ms step_avg:58.33ms
step:1238/2330 train_time:72214ms step_avg:58.33ms
step:1239/2330 train_time:72271ms step_avg:58.33ms
step:1240/2330 train_time:72330ms step_avg:58.33ms
step:1241/2330 train_time:72387ms step_avg:58.33ms
step:1242/2330 train_time:72447ms step_avg:58.33ms
step:1243/2330 train_time:72505ms step_avg:58.33ms
step:1244/2330 train_time:72564ms step_avg:58.33ms
step:1245/2330 train_time:72621ms step_avg:58.33ms
step:1246/2330 train_time:72682ms step_avg:58.33ms
step:1247/2330 train_time:72739ms step_avg:58.33ms
step:1248/2330 train_time:72799ms step_avg:58.33ms
step:1249/2330 train_time:72856ms step_avg:58.33ms
step:1250/2330 train_time:72916ms step_avg:58.33ms
step:1250/2330 val_loss:3.9864 train_time:72997ms step_avg:58.40ms
step:1251/2330 train_time:73016ms step_avg:58.37ms
step:1252/2330 train_time:73036ms step_avg:58.34ms
step:1253/2330 train_time:73094ms step_avg:58.34ms
step:1254/2330 train_time:73157ms step_avg:58.34ms
step:1255/2330 train_time:73214ms step_avg:58.34ms
step:1256/2330 train_time:73276ms step_avg:58.34ms
step:1257/2330 train_time:73332ms step_avg:58.34ms
step:1258/2330 train_time:73392ms step_avg:58.34ms
step:1259/2330 train_time:73449ms step_avg:58.34ms
step:1260/2330 train_time:73508ms step_avg:58.34ms
step:1261/2330 train_time:73565ms step_avg:58.34ms
step:1262/2330 train_time:73624ms step_avg:58.34ms
step:1263/2330 train_time:73681ms step_avg:58.34ms
step:1264/2330 train_time:73741ms step_avg:58.34ms
step:1265/2330 train_time:73797ms step_avg:58.34ms
step:1266/2330 train_time:73856ms step_avg:58.34ms
step:1267/2330 train_time:73914ms step_avg:58.34ms
step:1268/2330 train_time:73975ms step_avg:58.34ms
step:1269/2330 train_time:74033ms step_avg:58.34ms
step:1270/2330 train_time:74095ms step_avg:58.34ms
step:1271/2330 train_time:74152ms step_avg:58.34ms
step:1272/2330 train_time:74215ms step_avg:58.34ms
step:1273/2330 train_time:74271ms step_avg:58.34ms
step:1274/2330 train_time:74333ms step_avg:58.35ms
step:1275/2330 train_time:74390ms step_avg:58.34ms
step:1276/2330 train_time:74449ms step_avg:58.35ms
step:1277/2330 train_time:74506ms step_avg:58.34ms
step:1278/2330 train_time:74565ms step_avg:58.34ms
step:1279/2330 train_time:74621ms step_avg:58.34ms
step:1280/2330 train_time:74681ms step_avg:58.34ms
step:1281/2330 train_time:74739ms step_avg:58.34ms
step:1282/2330 train_time:74798ms step_avg:58.34ms
step:1283/2330 train_time:74855ms step_avg:58.34ms
step:1284/2330 train_time:74914ms step_avg:58.34ms
step:1285/2330 train_time:74971ms step_avg:58.34ms
step:1286/2330 train_time:75034ms step_avg:58.35ms
step:1287/2330 train_time:75092ms step_avg:58.35ms
step:1288/2330 train_time:75152ms step_avg:58.35ms
step:1289/2330 train_time:75210ms step_avg:58.35ms
step:1290/2330 train_time:75739ms step_avg:58.71ms
step:1291/2330 train_time:75757ms step_avg:58.68ms
step:1292/2330 train_time:75802ms step_avg:58.67ms
step:1293/2330 train_time:75858ms step_avg:58.67ms
step:1294/2330 train_time:75916ms step_avg:58.67ms
step:1295/2330 train_time:75972ms step_avg:58.67ms
step:1296/2330 train_time:76032ms step_avg:58.67ms
step:1297/2330 train_time:76089ms step_avg:58.67ms
step:1298/2330 train_time:76148ms step_avg:58.67ms
step:1299/2330 train_time:76204ms step_avg:58.66ms
step:1300/2330 train_time:76263ms step_avg:58.66ms
step:1301/2330 train_time:76319ms step_avg:58.66ms
step:1302/2330 train_time:76379ms step_avg:58.66ms
step:1303/2330 train_time:76435ms step_avg:58.66ms
step:1304/2330 train_time:76494ms step_avg:58.66ms
step:1305/2330 train_time:76550ms step_avg:58.66ms
step:1306/2330 train_time:76612ms step_avg:58.66ms
step:1307/2330 train_time:76674ms step_avg:58.66ms
step:1308/2330 train_time:76736ms step_avg:58.67ms
step:1309/2330 train_time:76793ms step_avg:58.67ms
step:1310/2330 train_time:76855ms step_avg:58.67ms
step:1311/2330 train_time:76911ms step_avg:58.67ms
step:1312/2330 train_time:76972ms step_avg:58.67ms
step:1313/2330 train_time:77029ms step_avg:58.67ms
step:1314/2330 train_time:77088ms step_avg:58.67ms
step:1315/2330 train_time:77145ms step_avg:58.67ms
step:1316/2330 train_time:77204ms step_avg:58.67ms
step:1317/2330 train_time:77261ms step_avg:58.66ms
step:1318/2330 train_time:77320ms step_avg:58.67ms
step:1319/2330 train_time:77376ms step_avg:58.66ms
step:1320/2330 train_time:77436ms step_avg:58.66ms
step:1321/2330 train_time:77492ms step_avg:58.66ms
step:1322/2330 train_time:77551ms step_avg:58.66ms
step:1323/2330 train_time:77609ms step_avg:58.66ms
step:1324/2330 train_time:77670ms step_avg:58.66ms
step:1325/2330 train_time:77729ms step_avg:58.66ms
step:1326/2330 train_time:77790ms step_avg:58.67ms
step:1327/2330 train_time:77848ms step_avg:58.66ms
step:1328/2330 train_time:77910ms step_avg:58.67ms
step:1329/2330 train_time:77967ms step_avg:58.67ms
step:1330/2330 train_time:78027ms step_avg:58.67ms
step:1331/2330 train_time:78084ms step_avg:58.67ms
step:1332/2330 train_time:78143ms step_avg:58.67ms
step:1333/2330 train_time:78200ms step_avg:58.66ms
step:1334/2330 train_time:78260ms step_avg:58.67ms
step:1335/2330 train_time:78316ms step_avg:58.66ms
step:1336/2330 train_time:78376ms step_avg:58.66ms
step:1337/2330 train_time:78432ms step_avg:58.66ms
step:1338/2330 train_time:78492ms step_avg:58.66ms
step:1339/2330 train_time:78549ms step_avg:58.66ms
step:1340/2330 train_time:78609ms step_avg:58.66ms
step:1341/2330 train_time:78666ms step_avg:58.66ms
step:1342/2330 train_time:78727ms step_avg:58.66ms
step:1343/2330 train_time:78784ms step_avg:58.66ms
step:1344/2330 train_time:78846ms step_avg:58.66ms
step:1345/2330 train_time:78902ms step_avg:58.66ms
step:1346/2330 train_time:78963ms step_avg:58.67ms
step:1347/2330 train_time:79020ms step_avg:58.66ms
step:1348/2330 train_time:79080ms step_avg:58.66ms
step:1349/2330 train_time:79137ms step_avg:58.66ms
step:1350/2330 train_time:79196ms step_avg:58.66ms
step:1351/2330 train_time:79252ms step_avg:58.66ms
step:1352/2330 train_time:79312ms step_avg:58.66ms
step:1353/2330 train_time:79369ms step_avg:58.66ms
step:1354/2330 train_time:79428ms step_avg:58.66ms
step:1355/2330 train_time:79486ms step_avg:58.66ms
step:1356/2330 train_time:79546ms step_avg:58.66ms
step:1357/2330 train_time:79603ms step_avg:58.66ms
step:1358/2330 train_time:79663ms step_avg:58.66ms
step:1359/2330 train_time:79721ms step_avg:58.66ms
step:1360/2330 train_time:79781ms step_avg:58.66ms
step:1361/2330 train_time:79838ms step_avg:58.66ms
step:1362/2330 train_time:79898ms step_avg:58.66ms
step:1363/2330 train_time:79955ms step_avg:58.66ms
step:1364/2330 train_time:80016ms step_avg:58.66ms
step:1365/2330 train_time:80073ms step_avg:58.66ms
step:1366/2330 train_time:80134ms step_avg:58.66ms
step:1367/2330 train_time:80191ms step_avg:58.66ms
step:1368/2330 train_time:80251ms step_avg:58.66ms
step:1369/2330 train_time:80308ms step_avg:58.66ms
step:1370/2330 train_time:80367ms step_avg:58.66ms
step:1371/2330 train_time:80423ms step_avg:58.66ms
step:1372/2330 train_time:80483ms step_avg:58.66ms
step:1373/2330 train_time:80540ms step_avg:58.66ms
step:1374/2330 train_time:80599ms step_avg:58.66ms
step:1375/2330 train_time:80656ms step_avg:58.66ms
step:1376/2330 train_time:80717ms step_avg:58.66ms
step:1377/2330 train_time:80774ms step_avg:58.66ms
step:1378/2330 train_time:80835ms step_avg:58.66ms
step:1379/2330 train_time:80892ms step_avg:58.66ms
step:1380/2330 train_time:80952ms step_avg:58.66ms
step:1381/2330 train_time:81010ms step_avg:58.66ms
step:1382/2330 train_time:81070ms step_avg:58.66ms
step:1383/2330 train_time:81129ms step_avg:58.66ms
step:1384/2330 train_time:81189ms step_avg:58.66ms
step:1385/2330 train_time:81246ms step_avg:58.66ms
step:1386/2330 train_time:81305ms step_avg:58.66ms
step:1387/2330 train_time:81362ms step_avg:58.66ms
step:1388/2330 train_time:81421ms step_avg:58.66ms
step:1389/2330 train_time:81478ms step_avg:58.66ms
step:1390/2330 train_time:81537ms step_avg:58.66ms
step:1391/2330 train_time:81595ms step_avg:58.66ms
step:1392/2330 train_time:81655ms step_avg:58.66ms
step:1393/2330 train_time:81712ms step_avg:58.66ms
step:1394/2330 train_time:81773ms step_avg:58.66ms
step:1395/2330 train_time:81830ms step_avg:58.66ms
step:1396/2330 train_time:81890ms step_avg:58.66ms
step:1397/2330 train_time:81947ms step_avg:58.66ms
step:1398/2330 train_time:82007ms step_avg:58.66ms
step:1399/2330 train_time:82065ms step_avg:58.66ms
step:1400/2330 train_time:82125ms step_avg:58.66ms
step:1401/2330 train_time:82183ms step_avg:58.66ms
step:1402/2330 train_time:82243ms step_avg:58.66ms
step:1403/2330 train_time:82299ms step_avg:58.66ms
step:1404/2330 train_time:82359ms step_avg:58.66ms
step:1405/2330 train_time:82416ms step_avg:58.66ms
step:1406/2330 train_time:82476ms step_avg:58.66ms
step:1407/2330 train_time:82532ms step_avg:58.66ms
step:1408/2330 train_time:82593ms step_avg:58.66ms
step:1409/2330 train_time:82650ms step_avg:58.66ms
step:1410/2330 train_time:82710ms step_avg:58.66ms
step:1411/2330 train_time:82768ms step_avg:58.66ms
step:1412/2330 train_time:82827ms step_avg:58.66ms
step:1413/2330 train_time:82885ms step_avg:58.66ms
step:1414/2330 train_time:82945ms step_avg:58.66ms
step:1415/2330 train_time:83002ms step_avg:58.66ms
step:1416/2330 train_time:83061ms step_avg:58.66ms
step:1417/2330 train_time:83119ms step_avg:58.66ms
step:1418/2330 train_time:83178ms step_avg:58.66ms
step:1419/2330 train_time:83236ms step_avg:58.66ms
step:1420/2330 train_time:83295ms step_avg:58.66ms
step:1421/2330 train_time:83352ms step_avg:58.66ms
step:1422/2330 train_time:83412ms step_avg:58.66ms
step:1423/2330 train_time:83469ms step_avg:58.66ms
step:1424/2330 train_time:83529ms step_avg:58.66ms
step:1425/2330 train_time:83586ms step_avg:58.66ms
step:1426/2330 train_time:83646ms step_avg:58.66ms
step:1427/2330 train_time:83703ms step_avg:58.66ms
step:1428/2330 train_time:83763ms step_avg:58.66ms
step:1429/2330 train_time:83820ms step_avg:58.66ms
step:1430/2330 train_time:83880ms step_avg:58.66ms
step:1431/2330 train_time:83937ms step_avg:58.66ms
step:1432/2330 train_time:83997ms step_avg:58.66ms
step:1433/2330 train_time:84054ms step_avg:58.66ms
step:1434/2330 train_time:84115ms step_avg:58.66ms
step:1435/2330 train_time:84172ms step_avg:58.66ms
step:1436/2330 train_time:84233ms step_avg:58.66ms
step:1437/2330 train_time:84290ms step_avg:58.66ms
step:1438/2330 train_time:84350ms step_avg:58.66ms
step:1439/2330 train_time:84407ms step_avg:58.66ms
step:1440/2330 train_time:84467ms step_avg:58.66ms
step:1441/2330 train_time:84525ms step_avg:58.66ms
step:1442/2330 train_time:84584ms step_avg:58.66ms
step:1443/2330 train_time:84641ms step_avg:58.66ms
step:1444/2330 train_time:84702ms step_avg:58.66ms
step:1445/2330 train_time:84759ms step_avg:58.66ms
step:1446/2330 train_time:84818ms step_avg:58.66ms
step:1447/2330 train_time:84876ms step_avg:58.66ms
step:1448/2330 train_time:84935ms step_avg:58.66ms
step:1449/2330 train_time:84992ms step_avg:58.66ms
step:1450/2330 train_time:85053ms step_avg:58.66ms
step:1451/2330 train_time:85111ms step_avg:58.66ms
step:1452/2330 train_time:85171ms step_avg:58.66ms
step:1453/2330 train_time:85228ms step_avg:58.66ms
step:1454/2330 train_time:85289ms step_avg:58.66ms
step:1455/2330 train_time:85346ms step_avg:58.66ms
step:1456/2330 train_time:85405ms step_avg:58.66ms
step:1457/2330 train_time:85461ms step_avg:58.66ms
step:1458/2330 train_time:85522ms step_avg:58.66ms
step:1459/2330 train_time:85579ms step_avg:58.66ms
step:1460/2330 train_time:85638ms step_avg:58.66ms
step:1461/2330 train_time:85695ms step_avg:58.66ms
step:1462/2330 train_time:85755ms step_avg:58.66ms
step:1463/2330 train_time:85812ms step_avg:58.65ms
step:1464/2330 train_time:85874ms step_avg:58.66ms
step:1465/2330 train_time:85931ms step_avg:58.66ms
step:1466/2330 train_time:85991ms step_avg:58.66ms
step:1467/2330 train_time:86048ms step_avg:58.66ms
step:1468/2330 train_time:86109ms step_avg:58.66ms
step:1469/2330 train_time:86166ms step_avg:58.66ms
step:1470/2330 train_time:86226ms step_avg:58.66ms
step:1471/2330 train_time:86283ms step_avg:58.66ms
step:1472/2330 train_time:86343ms step_avg:58.66ms
step:1473/2330 train_time:86399ms step_avg:58.66ms
step:1474/2330 train_time:86460ms step_avg:58.66ms
step:1475/2330 train_time:86517ms step_avg:58.66ms
step:1476/2330 train_time:86577ms step_avg:58.66ms
step:1477/2330 train_time:86633ms step_avg:58.65ms
step:1478/2330 train_time:86694ms step_avg:58.66ms
step:1479/2330 train_time:86750ms step_avg:58.65ms
step:1480/2330 train_time:86810ms step_avg:58.66ms
step:1481/2330 train_time:86867ms step_avg:58.65ms
step:1482/2330 train_time:86928ms step_avg:58.66ms
step:1483/2330 train_time:86985ms step_avg:58.65ms
step:1484/2330 train_time:87046ms step_avg:58.66ms
step:1485/2330 train_time:87103ms step_avg:58.66ms
step:1486/2330 train_time:87164ms step_avg:58.66ms
step:1487/2330 train_time:87220ms step_avg:58.66ms
step:1488/2330 train_time:87280ms step_avg:58.66ms
step:1489/2330 train_time:87337ms step_avg:58.65ms
step:1490/2330 train_time:87397ms step_avg:58.66ms
step:1491/2330 train_time:87454ms step_avg:58.65ms
step:1492/2330 train_time:87515ms step_avg:58.66ms
step:1493/2330 train_time:87571ms step_avg:58.65ms
step:1494/2330 train_time:87631ms step_avg:58.66ms
step:1495/2330 train_time:87689ms step_avg:58.65ms
step:1496/2330 train_time:87749ms step_avg:58.66ms
step:1497/2330 train_time:87806ms step_avg:58.65ms
step:1498/2330 train_time:87866ms step_avg:58.66ms
step:1499/2330 train_time:87923ms step_avg:58.65ms
step:1500/2330 train_time:87983ms step_avg:58.66ms
step:1500/2330 val_loss:3.9054 train_time:88063ms step_avg:58.71ms
step:1501/2330 train_time:88083ms step_avg:58.68ms
step:1502/2330 train_time:88103ms step_avg:58.66ms
step:1503/2330 train_time:88163ms step_avg:58.66ms
step:1504/2330 train_time:88224ms step_avg:58.66ms
step:1505/2330 train_time:88281ms step_avg:58.66ms
step:1506/2330 train_time:88343ms step_avg:58.66ms
step:1507/2330 train_time:88399ms step_avg:58.66ms
step:1508/2330 train_time:88459ms step_avg:58.66ms
step:1509/2330 train_time:88515ms step_avg:58.66ms
step:1510/2330 train_time:88575ms step_avg:58.66ms
step:1511/2330 train_time:88631ms step_avg:58.66ms
step:1512/2330 train_time:88690ms step_avg:58.66ms
step:1513/2330 train_time:88747ms step_avg:58.66ms
step:1514/2330 train_time:88806ms step_avg:58.66ms
step:1515/2330 train_time:88862ms step_avg:58.65ms
step:1516/2330 train_time:88921ms step_avg:58.66ms
step:1517/2330 train_time:88977ms step_avg:58.65ms
step:1518/2330 train_time:89040ms step_avg:58.66ms
step:1519/2330 train_time:89098ms step_avg:58.66ms
step:1520/2330 train_time:89158ms step_avg:58.66ms
step:1521/2330 train_time:89217ms step_avg:58.66ms
step:1522/2330 train_time:89277ms step_avg:58.66ms
step:1523/2330 train_time:89334ms step_avg:58.66ms
step:1524/2330 train_time:89397ms step_avg:58.66ms
step:1525/2330 train_time:89453ms step_avg:58.66ms
step:1526/2330 train_time:89514ms step_avg:58.66ms
step:1527/2330 train_time:89570ms step_avg:58.66ms
step:1528/2330 train_time:89630ms step_avg:58.66ms
step:1529/2330 train_time:89688ms step_avg:58.66ms
step:1530/2330 train_time:89746ms step_avg:58.66ms
step:1531/2330 train_time:89803ms step_avg:58.66ms
step:1532/2330 train_time:89863ms step_avg:58.66ms
step:1533/2330 train_time:89920ms step_avg:58.66ms
step:1534/2330 train_time:89980ms step_avg:58.66ms
step:1535/2330 train_time:90038ms step_avg:58.66ms
step:1536/2330 train_time:90098ms step_avg:58.66ms
step:1537/2330 train_time:90157ms step_avg:58.66ms
step:1538/2330 train_time:90218ms step_avg:58.66ms
step:1539/2330 train_time:90276ms step_avg:58.66ms
step:1540/2330 train_time:90338ms step_avg:58.66ms
step:1541/2330 train_time:90395ms step_avg:58.66ms
step:1542/2330 train_time:90457ms step_avg:58.66ms
step:1543/2330 train_time:90513ms step_avg:58.66ms
step:1544/2330 train_time:90575ms step_avg:58.66ms
step:1545/2330 train_time:90631ms step_avg:58.66ms
step:1546/2330 train_time:90693ms step_avg:58.66ms
step:1547/2330 train_time:90750ms step_avg:58.66ms
step:1548/2330 train_time:90811ms step_avg:58.66ms
step:1549/2330 train_time:90868ms step_avg:58.66ms
step:1550/2330 train_time:90930ms step_avg:58.66ms
step:1551/2330 train_time:90987ms step_avg:58.66ms
step:1552/2330 train_time:91049ms step_avg:58.67ms
step:1553/2330 train_time:91108ms step_avg:58.67ms
step:1554/2330 train_time:91169ms step_avg:58.67ms
step:1555/2330 train_time:91229ms step_avg:58.67ms
step:1556/2330 train_time:91290ms step_avg:58.67ms
step:1557/2330 train_time:91349ms step_avg:58.67ms
step:1558/2330 train_time:91409ms step_avg:58.67ms
step:1559/2330 train_time:91467ms step_avg:58.67ms
step:1560/2330 train_time:91527ms step_avg:58.67ms
step:1561/2330 train_time:91585ms step_avg:58.67ms
step:1562/2330 train_time:91645ms step_avg:58.67ms
step:1563/2330 train_time:91704ms step_avg:58.67ms
step:1564/2330 train_time:91764ms step_avg:58.67ms
step:1565/2330 train_time:91821ms step_avg:58.67ms
step:1566/2330 train_time:91880ms step_avg:58.67ms
step:1567/2330 train_time:91937ms step_avg:58.67ms
step:1568/2330 train_time:91997ms step_avg:58.67ms
step:1569/2330 train_time:92055ms step_avg:58.67ms
step:1570/2330 train_time:92116ms step_avg:58.67ms
step:1571/2330 train_time:92174ms step_avg:58.67ms
step:1572/2330 train_time:92236ms step_avg:58.67ms
step:1573/2330 train_time:92294ms step_avg:58.67ms
step:1574/2330 train_time:92356ms step_avg:58.68ms
step:1575/2330 train_time:92413ms step_avg:58.67ms
step:1576/2330 train_time:92474ms step_avg:58.68ms
step:1577/2330 train_time:92532ms step_avg:58.68ms
step:1578/2330 train_time:92592ms step_avg:58.68ms
step:1579/2330 train_time:92650ms step_avg:58.68ms
step:1580/2330 train_time:92711ms step_avg:58.68ms
step:1581/2330 train_time:92769ms step_avg:58.68ms
step:1582/2330 train_time:92829ms step_avg:58.68ms
step:1583/2330 train_time:92887ms step_avg:58.68ms
step:1584/2330 train_time:92948ms step_avg:58.68ms
step:1585/2330 train_time:93005ms step_avg:58.68ms
step:1586/2330 train_time:93065ms step_avg:58.68ms
step:1587/2330 train_time:93123ms step_avg:58.68ms
step:1588/2330 train_time:93185ms step_avg:58.68ms
step:1589/2330 train_time:93242ms step_avg:58.68ms
step:1590/2330 train_time:93304ms step_avg:58.68ms
step:1591/2330 train_time:93362ms step_avg:58.68ms
step:1592/2330 train_time:93423ms step_avg:58.68ms
step:1593/2330 train_time:93480ms step_avg:58.68ms
step:1594/2330 train_time:93542ms step_avg:58.68ms
step:1595/2330 train_time:93599ms step_avg:58.68ms
step:1596/2330 train_time:93659ms step_avg:58.68ms
step:1597/2330 train_time:93716ms step_avg:58.68ms
step:1598/2330 train_time:93776ms step_avg:58.68ms
step:1599/2330 train_time:93832ms step_avg:58.68ms
step:1600/2330 train_time:93895ms step_avg:58.68ms
step:1601/2330 train_time:93951ms step_avg:58.68ms
step:1602/2330 train_time:94014ms step_avg:58.69ms
step:1603/2330 train_time:94072ms step_avg:58.68ms
step:1604/2330 train_time:94132ms step_avg:58.69ms
step:1605/2330 train_time:94191ms step_avg:58.69ms
step:1606/2330 train_time:94252ms step_avg:58.69ms
step:1607/2330 train_time:94310ms step_avg:58.69ms
step:1608/2330 train_time:94370ms step_avg:58.69ms
step:1609/2330 train_time:94429ms step_avg:58.69ms
step:1610/2330 train_time:94489ms step_avg:58.69ms
step:1611/2330 train_time:94548ms step_avg:58.69ms
step:1612/2330 train_time:94608ms step_avg:58.69ms
step:1613/2330 train_time:94666ms step_avg:58.69ms
step:1614/2330 train_time:94726ms step_avg:58.69ms
step:1615/2330 train_time:94783ms step_avg:58.69ms
step:1616/2330 train_time:94846ms step_avg:58.69ms
step:1617/2330 train_time:94903ms step_avg:58.69ms
step:1618/2330 train_time:94963ms step_avg:58.69ms
step:1619/2330 train_time:95020ms step_avg:58.69ms
step:1620/2330 train_time:95082ms step_avg:58.69ms
step:1621/2330 train_time:95138ms step_avg:58.69ms
step:1622/2330 train_time:95200ms step_avg:58.69ms
step:1623/2330 train_time:95256ms step_avg:58.69ms
step:1624/2330 train_time:95317ms step_avg:58.69ms
step:1625/2330 train_time:95373ms step_avg:58.69ms
step:1626/2330 train_time:95435ms step_avg:58.69ms
step:1627/2330 train_time:95492ms step_avg:58.69ms
step:1628/2330 train_time:95555ms step_avg:58.69ms
step:1629/2330 train_time:95612ms step_avg:58.69ms
step:1630/2330 train_time:95673ms step_avg:58.70ms
step:1631/2330 train_time:95730ms step_avg:58.69ms
step:1632/2330 train_time:95791ms step_avg:58.70ms
step:1633/2330 train_time:95850ms step_avg:58.70ms
step:1634/2330 train_time:95910ms step_avg:58.70ms
step:1635/2330 train_time:95967ms step_avg:58.70ms
step:1636/2330 train_time:96028ms step_avg:58.70ms
step:1637/2330 train_time:96086ms step_avg:58.70ms
step:1638/2330 train_time:96146ms step_avg:58.70ms
step:1639/2330 train_time:96204ms step_avg:58.70ms
step:1640/2330 train_time:96264ms step_avg:58.70ms
step:1641/2330 train_time:96321ms step_avg:58.70ms
step:1642/2330 train_time:96383ms step_avg:58.70ms
step:1643/2330 train_time:96440ms step_avg:58.70ms
step:1644/2330 train_time:96501ms step_avg:58.70ms
step:1645/2330 train_time:96558ms step_avg:58.70ms
step:1646/2330 train_time:96619ms step_avg:58.70ms
step:1647/2330 train_time:96676ms step_avg:58.70ms
step:1648/2330 train_time:96738ms step_avg:58.70ms
step:1649/2330 train_time:96795ms step_avg:58.70ms
step:1650/2330 train_time:96856ms step_avg:58.70ms
step:1651/2330 train_time:96912ms step_avg:58.70ms
step:1652/2330 train_time:96974ms step_avg:58.70ms
step:1653/2330 train_time:97031ms step_avg:58.70ms
step:1654/2330 train_time:97093ms step_avg:58.70ms
step:1655/2330 train_time:97150ms step_avg:58.70ms
step:1656/2330 train_time:97212ms step_avg:58.70ms
step:1657/2330 train_time:97268ms step_avg:58.70ms
step:1658/2330 train_time:97332ms step_avg:58.70ms
step:1659/2330 train_time:97390ms step_avg:58.70ms
step:1660/2330 train_time:97451ms step_avg:58.71ms
step:1661/2330 train_time:97508ms step_avg:58.70ms
step:1662/2330 train_time:97569ms step_avg:58.71ms
step:1663/2330 train_time:97627ms step_avg:58.71ms
step:1664/2330 train_time:97687ms step_avg:58.71ms
step:1665/2330 train_time:97744ms step_avg:58.71ms
step:1666/2330 train_time:97806ms step_avg:58.71ms
step:1667/2330 train_time:97863ms step_avg:58.71ms
step:1668/2330 train_time:97925ms step_avg:58.71ms
step:1669/2330 train_time:97981ms step_avg:58.71ms
step:1670/2330 train_time:98042ms step_avg:58.71ms
step:1671/2330 train_time:98100ms step_avg:58.71ms
step:1672/2330 train_time:98160ms step_avg:58.71ms
step:1673/2330 train_time:98217ms step_avg:58.71ms
step:1674/2330 train_time:98278ms step_avg:58.71ms
step:1675/2330 train_time:98334ms step_avg:58.71ms
step:1676/2330 train_time:98396ms step_avg:58.71ms
step:1677/2330 train_time:98453ms step_avg:58.71ms
step:1678/2330 train_time:98515ms step_avg:58.71ms
step:1679/2330 train_time:98572ms step_avg:58.71ms
step:1680/2330 train_time:98633ms step_avg:58.71ms
step:1681/2330 train_time:98690ms step_avg:58.71ms
step:1682/2330 train_time:98752ms step_avg:58.71ms
step:1683/2330 train_time:98810ms step_avg:58.71ms
step:1684/2330 train_time:98871ms step_avg:58.71ms
step:1685/2330 train_time:98928ms step_avg:58.71ms
step:1686/2330 train_time:98989ms step_avg:58.71ms
step:1687/2330 train_time:99047ms step_avg:58.71ms
step:1688/2330 train_time:99107ms step_avg:58.71ms
step:1689/2330 train_time:99166ms step_avg:58.71ms
step:1690/2330 train_time:99226ms step_avg:58.71ms
step:1691/2330 train_time:99283ms step_avg:58.71ms
step:1692/2330 train_time:99344ms step_avg:58.71ms
step:1693/2330 train_time:99401ms step_avg:58.71ms
step:1694/2330 train_time:99463ms step_avg:58.71ms
step:1695/2330 train_time:99520ms step_avg:58.71ms
step:1696/2330 train_time:99580ms step_avg:58.71ms
step:1697/2330 train_time:99636ms step_avg:58.71ms
step:1698/2330 train_time:99698ms step_avg:58.72ms
step:1699/2330 train_time:99755ms step_avg:58.71ms
step:1700/2330 train_time:99816ms step_avg:58.72ms
step:1701/2330 train_time:99872ms step_avg:58.71ms
step:1702/2330 train_time:99934ms step_avg:58.72ms
step:1703/2330 train_time:99991ms step_avg:58.71ms
step:1704/2330 train_time:100053ms step_avg:58.72ms
step:1705/2330 train_time:100110ms step_avg:58.72ms
step:1706/2330 train_time:100172ms step_avg:58.72ms
step:1707/2330 train_time:100230ms step_avg:58.72ms
step:1708/2330 train_time:100290ms step_avg:58.72ms
step:1709/2330 train_time:100348ms step_avg:58.72ms
step:1710/2330 train_time:100409ms step_avg:58.72ms
step:1711/2330 train_time:100468ms step_avg:58.72ms
step:1712/2330 train_time:100528ms step_avg:58.72ms
step:1713/2330 train_time:100585ms step_avg:58.72ms
step:1714/2330 train_time:100646ms step_avg:58.72ms
step:1715/2330 train_time:100703ms step_avg:58.72ms
step:1716/2330 train_time:100765ms step_avg:58.72ms
step:1717/2330 train_time:100821ms step_avg:58.72ms
step:1718/2330 train_time:100882ms step_avg:58.72ms
step:1719/2330 train_time:100938ms step_avg:58.72ms
step:1720/2330 train_time:101000ms step_avg:58.72ms
step:1721/2330 train_time:101056ms step_avg:58.72ms
step:1722/2330 train_time:101118ms step_avg:58.72ms
step:1723/2330 train_time:101175ms step_avg:58.72ms
step:1724/2330 train_time:101236ms step_avg:58.72ms
step:1725/2330 train_time:101293ms step_avg:58.72ms
step:1726/2330 train_time:101356ms step_avg:58.72ms
step:1727/2330 train_time:101414ms step_avg:58.72ms
step:1728/2330 train_time:101474ms step_avg:58.72ms
step:1729/2330 train_time:101532ms step_avg:58.72ms
step:1730/2330 train_time:101592ms step_avg:58.72ms
step:1731/2330 train_time:101650ms step_avg:58.72ms
step:1732/2330 train_time:101712ms step_avg:58.72ms
step:1733/2330 train_time:101769ms step_avg:58.72ms
step:1734/2330 train_time:101831ms step_avg:58.73ms
step:1735/2330 train_time:101888ms step_avg:58.73ms
step:1736/2330 train_time:101950ms step_avg:58.73ms
step:1737/2330 train_time:102008ms step_avg:58.73ms
step:1738/2330 train_time:102068ms step_avg:58.73ms
step:1739/2330 train_time:102126ms step_avg:58.73ms
step:1740/2330 train_time:102186ms step_avg:58.73ms
step:1741/2330 train_time:102244ms step_avg:58.73ms
step:1742/2330 train_time:102305ms step_avg:58.73ms
step:1743/2330 train_time:102362ms step_avg:58.73ms
step:1744/2330 train_time:102424ms step_avg:58.73ms
step:1745/2330 train_time:102481ms step_avg:58.73ms
step:1746/2330 train_time:102542ms step_avg:58.73ms
step:1747/2330 train_time:102599ms step_avg:58.73ms
step:1748/2330 train_time:102660ms step_avg:58.73ms
step:1749/2330 train_time:102717ms step_avg:58.73ms
step:1750/2330 train_time:102778ms step_avg:58.73ms
step:1750/2330 val_loss:3.8190 train_time:102861ms step_avg:58.78ms
step:1751/2330 train_time:102880ms step_avg:58.76ms
step:1752/2330 train_time:102902ms step_avg:58.73ms
step:1753/2330 train_time:102958ms step_avg:58.73ms
step:1754/2330 train_time:103022ms step_avg:58.74ms
step:1755/2330 train_time:103078ms step_avg:58.73ms
step:1756/2330 train_time:103145ms step_avg:58.74ms
step:1757/2330 train_time:103201ms step_avg:58.74ms
step:1758/2330 train_time:103262ms step_avg:58.74ms
step:1759/2330 train_time:103320ms step_avg:58.74ms
step:1760/2330 train_time:103379ms step_avg:58.74ms
step:1761/2330 train_time:103436ms step_avg:58.74ms
step:1762/2330 train_time:103495ms step_avg:58.74ms
step:1763/2330 train_time:103551ms step_avg:58.74ms
step:1764/2330 train_time:103612ms step_avg:58.74ms
step:1765/2330 train_time:103669ms step_avg:58.74ms
step:1766/2330 train_time:103728ms step_avg:58.74ms
step:1767/2330 train_time:103786ms step_avg:58.74ms
step:1768/2330 train_time:103852ms step_avg:58.74ms
step:1769/2330 train_time:103910ms step_avg:58.74ms
step:1770/2330 train_time:103971ms step_avg:58.74ms
step:1771/2330 train_time:104029ms step_avg:58.74ms
step:1772/2330 train_time:104091ms step_avg:58.74ms
step:1773/2330 train_time:104148ms step_avg:58.74ms
step:1774/2330 train_time:104212ms step_avg:58.74ms
step:1775/2330 train_time:104269ms step_avg:58.74ms
step:1776/2330 train_time:104330ms step_avg:58.74ms
step:1777/2330 train_time:104387ms step_avg:58.74ms
step:1778/2330 train_time:104448ms step_avg:58.74ms
step:1779/2330 train_time:104505ms step_avg:58.74ms
step:1780/2330 train_time:104566ms step_avg:58.75ms
step:1781/2330 train_time:104624ms step_avg:58.74ms
step:1782/2330 train_time:104683ms step_avg:58.74ms
step:1783/2330 train_time:104741ms step_avg:58.74ms
step:1784/2330 train_time:104802ms step_avg:58.75ms
step:1785/2330 train_time:104861ms step_avg:58.75ms
step:1786/2330 train_time:104923ms step_avg:58.75ms
step:1787/2330 train_time:104982ms step_avg:58.75ms
step:1788/2330 train_time:105042ms step_avg:58.75ms
step:1789/2330 train_time:105100ms step_avg:58.75ms
step:1790/2330 train_time:105160ms step_avg:58.75ms
step:1791/2330 train_time:105219ms step_avg:58.75ms
step:1792/2330 train_time:105279ms step_avg:58.75ms
step:1793/2330 train_time:105336ms step_avg:58.75ms
step:1794/2330 train_time:105396ms step_avg:58.75ms
step:1795/2330 train_time:105453ms step_avg:58.75ms
step:1796/2330 train_time:105513ms step_avg:58.75ms
step:1797/2330 train_time:105570ms step_avg:58.75ms
step:1798/2330 train_time:105631ms step_avg:58.75ms
step:1799/2330 train_time:105687ms step_avg:58.75ms
step:1800/2330 train_time:105748ms step_avg:58.75ms
step:1801/2330 train_time:105805ms step_avg:58.75ms
step:1802/2330 train_time:105868ms step_avg:58.75ms
step:1803/2330 train_time:105926ms step_avg:58.75ms
step:1804/2330 train_time:105988ms step_avg:58.75ms
step:1805/2330 train_time:106046ms step_avg:58.75ms
step:1806/2330 train_time:106107ms step_avg:58.75ms
step:1807/2330 train_time:106164ms step_avg:58.75ms
step:1808/2330 train_time:106226ms step_avg:58.75ms
step:1809/2330 train_time:106284ms step_avg:58.75ms
step:1810/2330 train_time:106345ms step_avg:58.75ms
step:1811/2330 train_time:106403ms step_avg:58.75ms
step:1812/2330 train_time:106464ms step_avg:58.75ms
step:1813/2330 train_time:106521ms step_avg:58.75ms
step:1814/2330 train_time:106581ms step_avg:58.75ms
step:1815/2330 train_time:106638ms step_avg:58.75ms
step:1816/2330 train_time:106698ms step_avg:58.75ms
step:1817/2330 train_time:106756ms step_avg:58.75ms
step:1818/2330 train_time:106817ms step_avg:58.76ms
step:1819/2330 train_time:106874ms step_avg:58.75ms
step:1820/2330 train_time:106935ms step_avg:58.76ms
step:1821/2330 train_time:106992ms step_avg:58.75ms
step:1822/2330 train_time:107054ms step_avg:58.76ms
step:1823/2330 train_time:107111ms step_avg:58.76ms
step:1824/2330 train_time:107172ms step_avg:58.76ms
step:1825/2330 train_time:107229ms step_avg:58.76ms
step:1826/2330 train_time:107289ms step_avg:58.76ms
step:1827/2330 train_time:107346ms step_avg:58.76ms
step:1828/2330 train_time:107408ms step_avg:58.76ms
step:1829/2330 train_time:107465ms step_avg:58.76ms
step:1830/2330 train_time:107527ms step_avg:58.76ms
step:1831/2330 train_time:107584ms step_avg:58.76ms
step:1832/2330 train_time:107645ms step_avg:58.76ms
step:1833/2330 train_time:107703ms step_avg:58.76ms
step:1834/2330 train_time:107764ms step_avg:58.76ms
step:1835/2330 train_time:107822ms step_avg:58.76ms
step:1836/2330 train_time:107883ms step_avg:58.76ms
step:1837/2330 train_time:107942ms step_avg:58.76ms
step:1838/2330 train_time:108002ms step_avg:58.76ms
step:1839/2330 train_time:108060ms step_avg:58.76ms
step:1840/2330 train_time:108120ms step_avg:58.76ms
step:1841/2330 train_time:108178ms step_avg:58.76ms
step:1842/2330 train_time:108239ms step_avg:58.76ms
step:1843/2330 train_time:108296ms step_avg:58.76ms
step:1844/2330 train_time:108357ms step_avg:58.76ms
step:1845/2330 train_time:108414ms step_avg:58.76ms
step:1846/2330 train_time:108474ms step_avg:58.76ms
step:1847/2330 train_time:108531ms step_avg:58.76ms
step:1848/2330 train_time:108591ms step_avg:58.76ms
step:1849/2330 train_time:108648ms step_avg:58.76ms
step:1850/2330 train_time:108710ms step_avg:58.76ms
step:1851/2330 train_time:108767ms step_avg:58.76ms
step:1852/2330 train_time:108828ms step_avg:58.76ms
step:1853/2330 train_time:108886ms step_avg:58.76ms
step:1854/2330 train_time:108947ms step_avg:58.76ms
step:1855/2330 train_time:109005ms step_avg:58.76ms
step:1856/2330 train_time:109066ms step_avg:58.76ms
step:1857/2330 train_time:109125ms step_avg:58.76ms
step:1858/2330 train_time:109186ms step_avg:58.77ms
step:1859/2330 train_time:109243ms step_avg:58.76ms
step:1860/2330 train_time:109305ms step_avg:58.77ms
step:1861/2330 train_time:109362ms step_avg:58.77ms
step:1862/2330 train_time:109424ms step_avg:58.77ms
step:1863/2330 train_time:109481ms step_avg:58.77ms
step:1864/2330 train_time:109541ms step_avg:58.77ms
step:1865/2330 train_time:109598ms step_avg:58.77ms
step:1866/2330 train_time:109659ms step_avg:58.77ms
step:1867/2330 train_time:109717ms step_avg:58.77ms
step:1868/2330 train_time:109777ms step_avg:58.77ms
step:1869/2330 train_time:109834ms step_avg:58.77ms
step:1870/2330 train_time:109895ms step_avg:58.77ms
step:1871/2330 train_time:109952ms step_avg:58.77ms
step:1872/2330 train_time:110013ms step_avg:58.77ms
step:1873/2330 train_time:110071ms step_avg:58.77ms
step:1874/2330 train_time:110131ms step_avg:58.77ms
step:1875/2330 train_time:110188ms step_avg:58.77ms
step:1876/2330 train_time:110249ms step_avg:58.77ms
step:1877/2330 train_time:110306ms step_avg:58.77ms
step:1878/2330 train_time:110368ms step_avg:58.77ms
step:1879/2330 train_time:110425ms step_avg:58.77ms
step:1880/2330 train_time:110486ms step_avg:58.77ms
step:1881/2330 train_time:110544ms step_avg:58.77ms
step:1882/2330 train_time:110606ms step_avg:58.77ms
step:1883/2330 train_time:110663ms step_avg:58.77ms
step:1884/2330 train_time:110725ms step_avg:58.77ms
step:1885/2330 train_time:110783ms step_avg:58.77ms
step:1886/2330 train_time:110844ms step_avg:58.77ms
step:1887/2330 train_time:110902ms step_avg:58.77ms
step:1888/2330 train_time:110962ms step_avg:58.77ms
step:1889/2330 train_time:111020ms step_avg:58.77ms
step:1890/2330 train_time:111080ms step_avg:58.77ms
step:1891/2330 train_time:111138ms step_avg:58.77ms
step:1892/2330 train_time:111198ms step_avg:58.77ms
step:1893/2330 train_time:111256ms step_avg:58.77ms
step:1894/2330 train_time:111316ms step_avg:58.77ms
step:1895/2330 train_time:111373ms step_avg:58.77ms
step:1896/2330 train_time:111435ms step_avg:58.77ms
step:1897/2330 train_time:111491ms step_avg:58.77ms
step:1898/2330 train_time:111553ms step_avg:58.77ms
step:1899/2330 train_time:111609ms step_avg:58.77ms
step:1900/2330 train_time:111671ms step_avg:58.77ms
step:1901/2330 train_time:111729ms step_avg:58.77ms
step:1902/2330 train_time:111790ms step_avg:58.78ms
step:1903/2330 train_time:111847ms step_avg:58.77ms
step:1904/2330 train_time:111908ms step_avg:58.78ms
step:1905/2330 train_time:111966ms step_avg:58.77ms
step:1906/2330 train_time:112027ms step_avg:58.78ms
step:1907/2330 train_time:112086ms step_avg:58.78ms
step:1908/2330 train_time:112146ms step_avg:58.78ms
step:1909/2330 train_time:112204ms step_avg:58.78ms
step:1910/2330 train_time:112265ms step_avg:58.78ms
step:1911/2330 train_time:112324ms step_avg:58.78ms
step:1912/2330 train_time:112384ms step_avg:58.78ms
step:1913/2330 train_time:112443ms step_avg:58.78ms
step:1914/2330 train_time:112503ms step_avg:58.78ms
step:1915/2330 train_time:112560ms step_avg:58.78ms
step:1916/2330 train_time:112620ms step_avg:58.78ms
step:1917/2330 train_time:112677ms step_avg:58.78ms
step:1918/2330 train_time:112738ms step_avg:58.78ms
step:1919/2330 train_time:112795ms step_avg:58.78ms
step:1920/2330 train_time:112856ms step_avg:58.78ms
step:1921/2330 train_time:112912ms step_avg:58.78ms
step:1922/2330 train_time:112974ms step_avg:58.78ms
step:1923/2330 train_time:113031ms step_avg:58.78ms
step:1924/2330 train_time:113092ms step_avg:58.78ms
step:1925/2330 train_time:113149ms step_avg:58.78ms
step:1926/2330 train_time:113210ms step_avg:58.78ms
step:1927/2330 train_time:113267ms step_avg:58.78ms
step:1928/2330 train_time:113328ms step_avg:58.78ms
step:1929/2330 train_time:113386ms step_avg:58.78ms
step:1930/2330 train_time:113447ms step_avg:58.78ms
step:1931/2330 train_time:113504ms step_avg:58.78ms
step:1932/2330 train_time:113566ms step_avg:58.78ms
step:1933/2330 train_time:113624ms step_avg:58.78ms
step:1934/2330 train_time:113686ms step_avg:58.78ms
step:1935/2330 train_time:113744ms step_avg:58.78ms
step:1936/2330 train_time:113805ms step_avg:58.78ms
step:1937/2330 train_time:113863ms step_avg:58.78ms
step:1938/2330 train_time:113923ms step_avg:58.78ms
step:1939/2330 train_time:113980ms step_avg:58.78ms
step:1940/2330 train_time:114041ms step_avg:58.78ms
step:1941/2330 train_time:114099ms step_avg:58.78ms
step:1942/2330 train_time:114160ms step_avg:58.78ms
step:1943/2330 train_time:114217ms step_avg:58.78ms
step:1944/2330 train_time:114278ms step_avg:58.79ms
step:1945/2330 train_time:114336ms step_avg:58.78ms
step:1946/2330 train_time:114397ms step_avg:58.79ms
step:1947/2330 train_time:114454ms step_avg:58.78ms
step:1948/2330 train_time:114516ms step_avg:58.79ms
step:1949/2330 train_time:114573ms step_avg:58.79ms
step:1950/2330 train_time:114634ms step_avg:58.79ms
step:1951/2330 train_time:114690ms step_avg:58.79ms
step:1952/2330 train_time:114751ms step_avg:58.79ms
step:1953/2330 train_time:114808ms step_avg:58.79ms
step:1954/2330 train_time:114870ms step_avg:58.79ms
step:1955/2330 train_time:114927ms step_avg:58.79ms
step:1956/2330 train_time:114987ms step_avg:58.79ms
step:1957/2330 train_time:115045ms step_avg:58.79ms
step:1958/2330 train_time:115106ms step_avg:58.79ms
step:1959/2330 train_time:115164ms step_avg:58.79ms
step:1960/2330 train_time:115225ms step_avg:58.79ms
step:1961/2330 train_time:115283ms step_avg:58.79ms
step:1962/2330 train_time:115343ms step_avg:58.79ms
step:1963/2330 train_time:115402ms step_avg:58.79ms
step:1964/2330 train_time:115462ms step_avg:58.79ms
step:1965/2330 train_time:115521ms step_avg:58.79ms
step:1966/2330 train_time:115581ms step_avg:58.79ms
step:1967/2330 train_time:115638ms step_avg:58.79ms
step:1968/2330 train_time:115699ms step_avg:58.79ms
step:1969/2330 train_time:115756ms step_avg:58.79ms
step:1970/2330 train_time:115818ms step_avg:58.79ms
step:1971/2330 train_time:115875ms step_avg:58.79ms
step:1972/2330 train_time:115935ms step_avg:58.79ms
step:1973/2330 train_time:115991ms step_avg:58.79ms
step:1974/2330 train_time:116054ms step_avg:58.79ms
step:1975/2330 train_time:116111ms step_avg:58.79ms
step:1976/2330 train_time:116171ms step_avg:58.79ms
step:1977/2330 train_time:116228ms step_avg:58.79ms
step:1978/2330 train_time:116289ms step_avg:58.79ms
step:1979/2330 train_time:116346ms step_avg:58.79ms
step:1980/2330 train_time:116407ms step_avg:58.79ms
step:1981/2330 train_time:116465ms step_avg:58.79ms
step:1982/2330 train_time:116526ms step_avg:58.79ms
step:1983/2330 train_time:116585ms step_avg:58.79ms
step:1984/2330 train_time:116644ms step_avg:58.79ms
step:1985/2330 train_time:116702ms step_avg:58.79ms
step:1986/2330 train_time:116764ms step_avg:58.79ms
step:1987/2330 train_time:116821ms step_avg:58.79ms
step:1988/2330 train_time:116881ms step_avg:58.79ms
step:1989/2330 train_time:116939ms step_avg:58.79ms
step:1990/2330 train_time:117000ms step_avg:58.79ms
step:1991/2330 train_time:117058ms step_avg:58.79ms
step:1992/2330 train_time:117118ms step_avg:58.79ms
step:1993/2330 train_time:117176ms step_avg:58.79ms
step:1994/2330 train_time:117236ms step_avg:58.79ms
step:1995/2330 train_time:117293ms step_avg:58.79ms
step:1996/2330 train_time:117355ms step_avg:58.80ms
step:1997/2330 train_time:117411ms step_avg:58.79ms
step:1998/2330 train_time:117473ms step_avg:58.80ms
step:1999/2330 train_time:117530ms step_avg:58.79ms
step:2000/2330 train_time:117591ms step_avg:58.80ms
step:2000/2330 val_loss:3.7562 train_time:117673ms step_avg:58.84ms
step:2001/2330 train_time:117693ms step_avg:58.82ms
step:2002/2330 train_time:117713ms step_avg:58.80ms
step:2003/2330 train_time:117773ms step_avg:58.80ms
step:2004/2330 train_time:117837ms step_avg:58.80ms
step:2005/2330 train_time:117894ms step_avg:58.80ms
step:2006/2330 train_time:117956ms step_avg:58.80ms
step:2007/2330 train_time:118013ms step_avg:58.80ms
step:2008/2330 train_time:118074ms step_avg:58.80ms
step:2009/2330 train_time:118130ms step_avg:58.80ms
step:2010/2330 train_time:118192ms step_avg:58.80ms
step:2011/2330 train_time:118248ms step_avg:58.80ms
step:2012/2330 train_time:118307ms step_avg:58.80ms
step:2013/2330 train_time:118364ms step_avg:58.80ms
step:2014/2330 train_time:118424ms step_avg:58.80ms
step:2015/2330 train_time:118481ms step_avg:58.80ms
step:2016/2330 train_time:118542ms step_avg:58.80ms
step:2017/2330 train_time:118599ms step_avg:58.80ms
step:2018/2330 train_time:118660ms step_avg:58.80ms
step:2019/2330 train_time:118719ms step_avg:58.80ms
step:2020/2330 train_time:118783ms step_avg:58.80ms
step:2021/2330 train_time:118842ms step_avg:58.80ms
step:2022/2330 train_time:118904ms step_avg:58.80ms
step:2023/2330 train_time:118962ms step_avg:58.80ms
step:2024/2330 train_time:119022ms step_avg:58.81ms
step:2025/2330 train_time:119080ms step_avg:58.80ms
step:2026/2330 train_time:119140ms step_avg:58.81ms
step:2027/2330 train_time:119197ms step_avg:58.80ms
step:2028/2330 train_time:119257ms step_avg:58.81ms
step:2029/2330 train_time:119314ms step_avg:58.80ms
step:2030/2330 train_time:119374ms step_avg:58.80ms
step:2031/2330 train_time:119431ms step_avg:58.80ms
step:2032/2330 train_time:119491ms step_avg:58.80ms
step:2033/2330 train_time:119548ms step_avg:58.80ms
step:2034/2330 train_time:119608ms step_avg:58.80ms
step:2035/2330 train_time:119666ms step_avg:58.80ms
step:2036/2330 train_time:119728ms step_avg:58.81ms
step:2037/2330 train_time:119786ms step_avg:58.80ms
step:2038/2330 train_time:119849ms step_avg:58.81ms
step:2039/2330 train_time:119906ms step_avg:58.81ms
step:2040/2330 train_time:119968ms step_avg:58.81ms
step:2041/2330 train_time:120025ms step_avg:58.81ms
step:2042/2330 train_time:120087ms step_avg:58.81ms
step:2043/2330 train_time:120144ms step_avg:58.81ms
step:2044/2330 train_time:120205ms step_avg:58.81ms
step:2045/2330 train_time:120262ms step_avg:58.81ms
step:2046/2330 train_time:120325ms step_avg:58.81ms
step:2047/2330 train_time:120383ms step_avg:58.81ms
step:2048/2330 train_time:120443ms step_avg:58.81ms
step:2049/2330 train_time:120501ms step_avg:58.81ms
step:2050/2330 train_time:120561ms step_avg:58.81ms
step:2051/2330 train_time:120619ms step_avg:58.81ms
step:2052/2330 train_time:120679ms step_avg:58.81ms
step:2053/2330 train_time:120738ms step_avg:58.81ms
step:2054/2330 train_time:120798ms step_avg:58.81ms
step:2055/2330 train_time:120856ms step_avg:58.81ms
step:2056/2330 train_time:120917ms step_avg:58.81ms
step:2057/2330 train_time:120975ms step_avg:58.81ms
step:2058/2330 train_time:121036ms step_avg:58.81ms
step:2059/2330 train_time:121093ms step_avg:58.81ms
step:2060/2330 train_time:121154ms step_avg:58.81ms
step:2061/2330 train_time:121211ms step_avg:58.81ms
step:2062/2330 train_time:121272ms step_avg:58.81ms
step:2063/2330 train_time:121329ms step_avg:58.81ms
step:2064/2330 train_time:121389ms step_avg:58.81ms
step:2065/2330 train_time:121446ms step_avg:58.81ms
step:2066/2330 train_time:121506ms step_avg:58.81ms
step:2067/2330 train_time:121563ms step_avg:58.81ms
step:2068/2330 train_time:121625ms step_avg:58.81ms
step:2069/2330 train_time:121684ms step_avg:58.81ms
step:2070/2330 train_time:121744ms step_avg:58.81ms
step:2071/2330 train_time:121802ms step_avg:58.81ms
step:2072/2330 train_time:121863ms step_avg:58.81ms
step:2073/2330 train_time:121921ms step_avg:58.81ms
step:2074/2330 train_time:121982ms step_avg:58.81ms
step:2075/2330 train_time:122040ms step_avg:58.81ms
step:2076/2330 train_time:122102ms step_avg:58.82ms
step:2077/2330 train_time:122160ms step_avg:58.82ms
step:2078/2330 train_time:122220ms step_avg:58.82ms
step:2079/2330 train_time:122278ms step_avg:58.82ms
step:2080/2330 train_time:122338ms step_avg:58.82ms
step:2081/2330 train_time:122396ms step_avg:58.82ms
step:2082/2330 train_time:122455ms step_avg:58.82ms
step:2083/2330 train_time:122513ms step_avg:58.82ms
step:2084/2330 train_time:122573ms step_avg:58.82ms
step:2085/2330 train_time:122630ms step_avg:58.82ms
step:2086/2330 train_time:122692ms step_avg:58.82ms
step:2087/2330 train_time:122749ms step_avg:58.82ms
step:2088/2330 train_time:122809ms step_avg:58.82ms
step:2089/2330 train_time:122866ms step_avg:58.82ms
step:2090/2330 train_time:122927ms step_avg:58.82ms
step:2091/2330 train_time:122985ms step_avg:58.82ms
step:2092/2330 train_time:123046ms step_avg:58.82ms
step:2093/2330 train_time:123104ms step_avg:58.82ms
step:2094/2330 train_time:123164ms step_avg:58.82ms
step:2095/2330 train_time:123221ms step_avg:58.82ms
step:2096/2330 train_time:123282ms step_avg:58.82ms
step:2097/2330 train_time:123340ms step_avg:58.82ms
step:2098/2330 train_time:123401ms step_avg:58.82ms
step:2099/2330 train_time:123458ms step_avg:58.82ms
step:2100/2330 train_time:123519ms step_avg:58.82ms
step:2101/2330 train_time:123576ms step_avg:58.82ms
step:2102/2330 train_time:123637ms step_avg:58.82ms
step:2103/2330 train_time:123694ms step_avg:58.82ms
step:2104/2330 train_time:123756ms step_avg:58.82ms
step:2105/2330 train_time:123813ms step_avg:58.82ms
step:2106/2330 train_time:123874ms step_avg:58.82ms
step:2107/2330 train_time:123931ms step_avg:58.82ms
step:2108/2330 train_time:123992ms step_avg:58.82ms
step:2109/2330 train_time:124049ms step_avg:58.82ms
step:2110/2330 train_time:124110ms step_avg:58.82ms
step:2111/2330 train_time:124167ms step_avg:58.82ms
step:2112/2330 train_time:124228ms step_avg:58.82ms
step:2113/2330 train_time:124285ms step_avg:58.82ms
step:2114/2330 train_time:124347ms step_avg:58.82ms
step:2115/2330 train_time:124404ms step_avg:58.82ms
step:2116/2330 train_time:124464ms step_avg:58.82ms
step:2117/2330 train_time:124522ms step_avg:58.82ms
step:2118/2330 train_time:124583ms step_avg:58.82ms
step:2119/2330 train_time:124641ms step_avg:58.82ms
step:2120/2330 train_time:124702ms step_avg:58.82ms
step:2121/2330 train_time:124760ms step_avg:58.82ms
step:2122/2330 train_time:124821ms step_avg:58.82ms
step:2123/2330 train_time:124878ms step_avg:58.82ms
step:2124/2330 train_time:124939ms step_avg:58.82ms
step:2125/2330 train_time:124997ms step_avg:58.82ms
step:2126/2330 train_time:125057ms step_avg:58.82ms
step:2127/2330 train_time:125115ms step_avg:58.82ms
step:2128/2330 train_time:125176ms step_avg:58.82ms
step:2129/2330 train_time:125233ms step_avg:58.82ms
step:2130/2330 train_time:125295ms step_avg:58.82ms
step:2131/2330 train_time:125352ms step_avg:58.82ms
step:2132/2330 train_time:125413ms step_avg:58.82ms
step:2133/2330 train_time:125470ms step_avg:58.82ms
step:2134/2330 train_time:125531ms step_avg:58.82ms
step:2135/2330 train_time:125588ms step_avg:58.82ms
step:2136/2330 train_time:125649ms step_avg:58.82ms
step:2137/2330 train_time:125706ms step_avg:58.82ms
step:2138/2330 train_time:125766ms step_avg:58.82ms
step:2139/2330 train_time:125824ms step_avg:58.82ms
step:2140/2330 train_time:125886ms step_avg:58.83ms
step:2141/2330 train_time:125943ms step_avg:58.82ms
step:2142/2330 train_time:126004ms step_avg:58.83ms
step:2143/2330 train_time:126062ms step_avg:58.83ms
step:2144/2330 train_time:126123ms step_avg:58.83ms
step:2145/2330 train_time:126182ms step_avg:58.83ms
step:2146/2330 train_time:126242ms step_avg:58.83ms
step:2147/2330 train_time:126300ms step_avg:58.83ms
step:2148/2330 train_time:126361ms step_avg:58.83ms
step:2149/2330 train_time:126419ms step_avg:58.83ms
step:2150/2330 train_time:126479ms step_avg:58.83ms
step:2151/2330 train_time:126537ms step_avg:58.83ms
step:2152/2330 train_time:126597ms step_avg:58.83ms
step:2153/2330 train_time:126655ms step_avg:58.83ms
step:2154/2330 train_time:126715ms step_avg:58.83ms
step:2155/2330 train_time:126772ms step_avg:58.83ms
step:2156/2330 train_time:126832ms step_avg:58.83ms
step:2157/2330 train_time:126890ms step_avg:58.83ms
step:2158/2330 train_time:126950ms step_avg:58.83ms
step:2159/2330 train_time:127006ms step_avg:58.83ms
step:2160/2330 train_time:127068ms step_avg:58.83ms
step:2161/2330 train_time:127125ms step_avg:58.83ms
step:2162/2330 train_time:127188ms step_avg:58.83ms
step:2163/2330 train_time:127245ms step_avg:58.83ms
step:2164/2330 train_time:127306ms step_avg:58.83ms
step:2165/2330 train_time:127364ms step_avg:58.83ms
step:2166/2330 train_time:127425ms step_avg:58.83ms
step:2167/2330 train_time:127482ms step_avg:58.83ms
step:2168/2330 train_time:127543ms step_avg:58.83ms
step:2169/2330 train_time:127600ms step_avg:58.83ms
step:2170/2330 train_time:127661ms step_avg:58.83ms
step:2171/2330 train_time:127719ms step_avg:58.83ms
step:2172/2330 train_time:127779ms step_avg:58.83ms
step:2173/2330 train_time:127838ms step_avg:58.83ms
step:2174/2330 train_time:127897ms step_avg:58.83ms
step:2175/2330 train_time:127954ms step_avg:58.83ms
step:2176/2330 train_time:128017ms step_avg:58.83ms
step:2177/2330 train_time:128074ms step_avg:58.83ms
step:2178/2330 train_time:128135ms step_avg:58.83ms
step:2179/2330 train_time:128191ms step_avg:58.83ms
step:2180/2330 train_time:128253ms step_avg:58.83ms
step:2181/2330 train_time:128310ms step_avg:58.83ms
step:2182/2330 train_time:128371ms step_avg:58.83ms
step:2183/2330 train_time:128428ms step_avg:58.83ms
step:2184/2330 train_time:128489ms step_avg:58.83ms
step:2185/2330 train_time:128545ms step_avg:58.83ms
step:2186/2330 train_time:128607ms step_avg:58.83ms
step:2187/2330 train_time:128664ms step_avg:58.83ms
step:2188/2330 train_time:128728ms step_avg:58.83ms
step:2189/2330 train_time:128785ms step_avg:58.83ms
step:2190/2330 train_time:128847ms step_avg:58.83ms
step:2191/2330 train_time:128904ms step_avg:58.83ms
step:2192/2330 train_time:128965ms step_avg:58.83ms
step:2193/2330 train_time:129023ms step_avg:58.83ms
step:2194/2330 train_time:129084ms step_avg:58.84ms
step:2195/2330 train_time:129142ms step_avg:58.83ms
step:2196/2330 train_time:129203ms step_avg:58.84ms
step:2197/2330 train_time:129260ms step_avg:58.83ms
step:2198/2330 train_time:129320ms step_avg:58.84ms
step:2199/2330 train_time:129379ms step_avg:58.84ms
step:2200/2330 train_time:129440ms step_avg:58.84ms
step:2201/2330 train_time:129497ms step_avg:58.84ms
step:2202/2330 train_time:129557ms step_avg:58.84ms
step:2203/2330 train_time:129614ms step_avg:58.84ms
step:2204/2330 train_time:129675ms step_avg:58.84ms
step:2205/2330 train_time:129732ms step_avg:58.84ms
step:2206/2330 train_time:129793ms step_avg:58.84ms
step:2207/2330 train_time:129849ms step_avg:58.84ms
step:2208/2330 train_time:129912ms step_avg:58.84ms
step:2209/2330 train_time:129969ms step_avg:58.84ms
step:2210/2330 train_time:130030ms step_avg:58.84ms
step:2211/2330 train_time:130087ms step_avg:58.84ms
step:2212/2330 train_time:130148ms step_avg:58.84ms
step:2213/2330 train_time:130205ms step_avg:58.84ms
step:2214/2330 train_time:130266ms step_avg:58.84ms
step:2215/2330 train_time:130323ms step_avg:58.84ms
step:2216/2330 train_time:130385ms step_avg:58.84ms
step:2217/2330 train_time:130444ms step_avg:58.84ms
step:2218/2330 train_time:130503ms step_avg:58.84ms
step:2219/2330 train_time:130561ms step_avg:58.84ms
step:2220/2330 train_time:130622ms step_avg:58.84ms
step:2221/2330 train_time:130680ms step_avg:58.84ms
step:2222/2330 train_time:130740ms step_avg:58.84ms
step:2223/2330 train_time:130799ms step_avg:58.84ms
step:2224/2330 train_time:130859ms step_avg:58.84ms
step:2225/2330 train_time:130917ms step_avg:58.84ms
step:2226/2330 train_time:130978ms step_avg:58.84ms
step:2227/2330 train_time:131035ms step_avg:58.84ms
step:2228/2330 train_time:131096ms step_avg:58.84ms
step:2229/2330 train_time:131153ms step_avg:58.84ms
step:2230/2330 train_time:131213ms step_avg:58.84ms
step:2231/2330 train_time:131270ms step_avg:58.84ms
step:2232/2330 train_time:131331ms step_avg:58.84ms
step:2233/2330 train_time:131389ms step_avg:58.84ms
step:2234/2330 train_time:131448ms step_avg:58.84ms
step:2235/2330 train_time:131505ms step_avg:58.84ms
step:2236/2330 train_time:131567ms step_avg:58.84ms
step:2237/2330 train_time:131624ms step_avg:58.84ms
step:2238/2330 train_time:131685ms step_avg:58.84ms
step:2239/2330 train_time:131743ms step_avg:58.84ms
step:2240/2330 train_time:131803ms step_avg:58.84ms
step:2241/2330 train_time:131861ms step_avg:58.84ms
step:2242/2330 train_time:131922ms step_avg:58.84ms
step:2243/2330 train_time:131981ms step_avg:58.84ms
step:2244/2330 train_time:132042ms step_avg:58.84ms
step:2245/2330 train_time:132099ms step_avg:58.84ms
step:2246/2330 train_time:132160ms step_avg:58.84ms
step:2247/2330 train_time:132218ms step_avg:58.84ms
step:2248/2330 train_time:132279ms step_avg:58.84ms
step:2249/2330 train_time:132337ms step_avg:58.84ms
step:2250/2330 train_time:132397ms step_avg:58.84ms
step:2250/2330 val_loss:3.7080 train_time:132478ms step_avg:58.88ms
step:2251/2330 train_time:132497ms step_avg:58.86ms
step:2252/2330 train_time:132518ms step_avg:58.84ms
step:2253/2330 train_time:132577ms step_avg:58.84ms
step:2254/2330 train_time:132639ms step_avg:58.85ms
step:2255/2330 train_time:132696ms step_avg:58.85ms
step:2256/2330 train_time:132756ms step_avg:58.85ms
step:2257/2330 train_time:132813ms step_avg:58.84ms
step:2258/2330 train_time:132874ms step_avg:58.85ms
step:2259/2330 train_time:132930ms step_avg:58.84ms
step:2260/2330 train_time:132990ms step_avg:58.85ms
step:2261/2330 train_time:133047ms step_avg:58.84ms
step:2262/2330 train_time:133107ms step_avg:58.84ms
step:2263/2330 train_time:133164ms step_avg:58.84ms
step:2264/2330 train_time:133224ms step_avg:58.84ms
step:2265/2330 train_time:133280ms step_avg:58.84ms
step:2266/2330 train_time:133340ms step_avg:58.84ms
step:2267/2330 train_time:133398ms step_avg:58.84ms
step:2268/2330 train_time:133461ms step_avg:58.85ms
step:2269/2330 train_time:133521ms step_avg:58.85ms
step:2270/2330 train_time:133583ms step_avg:58.85ms
step:2271/2330 train_time:133641ms step_avg:58.85ms
step:2272/2330 train_time:133702ms step_avg:58.85ms
step:2273/2330 train_time:133761ms step_avg:58.85ms
step:2274/2330 train_time:133821ms step_avg:58.85ms
step:2275/2330 train_time:133879ms step_avg:58.85ms
step:2276/2330 train_time:133938ms step_avg:58.85ms
step:2277/2330 train_time:133995ms step_avg:58.85ms
step:2278/2330 train_time:134056ms step_avg:58.85ms
step:2279/2330 train_time:134112ms step_avg:58.85ms
step:2280/2330 train_time:134173ms step_avg:58.85ms
step:2281/2330 train_time:134229ms step_avg:58.85ms
step:2282/2330 train_time:134289ms step_avg:58.85ms
step:2283/2330 train_time:134346ms step_avg:58.85ms
step:2284/2330 train_time:134408ms step_avg:58.85ms
step:2285/2330 train_time:134466ms step_avg:58.85ms
step:2286/2330 train_time:134527ms step_avg:58.85ms
step:2287/2330 train_time:134584ms step_avg:58.85ms
step:2288/2330 train_time:134647ms step_avg:58.85ms
step:2289/2330 train_time:134705ms step_avg:58.85ms
step:2290/2330 train_time:134768ms step_avg:58.85ms
step:2291/2330 train_time:134826ms step_avg:58.85ms
step:2292/2330 train_time:134886ms step_avg:58.85ms
step:2293/2330 train_time:134943ms step_avg:58.85ms
step:2294/2330 train_time:135005ms step_avg:58.85ms
step:2295/2330 train_time:135064ms step_avg:58.85ms
step:2296/2330 train_time:135124ms step_avg:58.85ms
step:2297/2330 train_time:135181ms step_avg:58.85ms
step:2298/2330 train_time:135241ms step_avg:58.85ms
step:2299/2330 train_time:135299ms step_avg:58.85ms
step:2300/2330 train_time:135359ms step_avg:58.85ms
step:2301/2330 train_time:135416ms step_avg:58.85ms
step:2302/2330 train_time:135477ms step_avg:58.85ms
step:2303/2330 train_time:135535ms step_avg:58.85ms
step:2304/2330 train_time:135595ms step_avg:58.85ms
step:2305/2330 train_time:135652ms step_avg:58.85ms
step:2306/2330 train_time:135715ms step_avg:58.85ms
step:2307/2330 train_time:135772ms step_avg:58.85ms
step:2308/2330 train_time:135833ms step_avg:58.85ms
step:2309/2330 train_time:135890ms step_avg:58.85ms
step:2310/2330 train_time:135952ms step_avg:58.85ms
step:2311/2330 train_time:136009ms step_avg:58.85ms
step:2312/2330 train_time:136071ms step_avg:58.85ms
step:2313/2330 train_time:136127ms step_avg:58.85ms
step:2314/2330 train_time:136189ms step_avg:58.85ms
step:2315/2330 train_time:136246ms step_avg:58.85ms
step:2316/2330 train_time:136307ms step_avg:58.85ms
step:2317/2330 train_time:136364ms step_avg:58.85ms
step:2318/2330 train_time:136425ms step_avg:58.85ms
step:2319/2330 train_time:136482ms step_avg:58.85ms
step:2320/2330 train_time:136543ms step_avg:58.85ms
step:2321/2330 train_time:136602ms step_avg:58.85ms
step:2322/2330 train_time:136663ms step_avg:58.86ms
step:2323/2330 train_time:136721ms step_avg:58.86ms
step:2324/2330 train_time:136782ms step_avg:58.86ms
step:2325/2330 train_time:136839ms step_avg:58.86ms
step:2326/2330 train_time:136900ms step_avg:58.86ms
step:2327/2330 train_time:136957ms step_avg:58.86ms
step:2328/2330 train_time:137019ms step_avg:58.86ms
step:2329/2330 train_time:137076ms step_avg:58.86ms
step:2330/2330 train_time:137138ms step_avg:58.86ms
step:2330/2330 val_loss:3.6930 train_time:137220ms step_avg:58.89ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
