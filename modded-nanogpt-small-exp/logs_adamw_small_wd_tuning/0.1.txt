import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 08:01:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:118ms step_avg:117.79ms
step:2/2330 train_time:208ms step_avg:103.99ms
step:3/2330 train_time:227ms step_avg:75.56ms
step:4/2330 train_time:245ms step_avg:61.32ms
step:5/2330 train_time:299ms step_avg:59.84ms
step:6/2330 train_time:357ms step_avg:59.57ms
step:7/2330 train_time:413ms step_avg:58.98ms
step:8/2330 train_time:471ms step_avg:58.89ms
step:9/2330 train_time:527ms step_avg:58.50ms
step:10/2330 train_time:585ms step_avg:58.49ms
step:11/2330 train_time:641ms step_avg:58.28ms
step:12/2330 train_time:700ms step_avg:58.32ms
step:13/2330 train_time:755ms step_avg:58.11ms
step:14/2330 train_time:813ms step_avg:58.09ms
step:15/2330 train_time:869ms step_avg:57.93ms
step:16/2330 train_time:927ms step_avg:57.96ms
step:17/2330 train_time:983ms step_avg:57.84ms
step:18/2330 train_time:1043ms step_avg:57.95ms
step:19/2330 train_time:1102ms step_avg:57.99ms
step:20/2330 train_time:1164ms step_avg:58.20ms
step:21/2330 train_time:1222ms step_avg:58.21ms
step:22/2330 train_time:1282ms step_avg:58.27ms
step:23/2330 train_time:1338ms step_avg:58.19ms
step:24/2330 train_time:1397ms step_avg:58.20ms
step:25/2330 train_time:1453ms step_avg:58.11ms
step:26/2330 train_time:1512ms step_avg:58.14ms
step:27/2330 train_time:1567ms step_avg:58.04ms
step:28/2330 train_time:1625ms step_avg:58.05ms
step:29/2330 train_time:1681ms step_avg:57.96ms
step:30/2330 train_time:1740ms step_avg:57.99ms
step:31/2330 train_time:1795ms step_avg:57.90ms
step:32/2330 train_time:1853ms step_avg:57.92ms
step:33/2330 train_time:1909ms step_avg:57.86ms
step:34/2330 train_time:1968ms step_avg:57.88ms
step:35/2330 train_time:2025ms step_avg:57.87ms
step:36/2330 train_time:2085ms step_avg:57.92ms
step:37/2330 train_time:2143ms step_avg:57.93ms
step:38/2330 train_time:2203ms step_avg:57.97ms
step:39/2330 train_time:2260ms step_avg:57.95ms
step:40/2330 train_time:2319ms step_avg:57.98ms
step:41/2330 train_time:2375ms step_avg:57.93ms
step:42/2330 train_time:2435ms step_avg:57.98ms
step:43/2330 train_time:2491ms step_avg:57.92ms
step:44/2330 train_time:2550ms step_avg:57.95ms
step:45/2330 train_time:2605ms step_avg:57.90ms
step:46/2330 train_time:2665ms step_avg:57.94ms
step:47/2330 train_time:2721ms step_avg:57.90ms
step:48/2330 train_time:2779ms step_avg:57.90ms
step:49/2330 train_time:2835ms step_avg:57.86ms
step:50/2330 train_time:2893ms step_avg:57.86ms
step:51/2330 train_time:2949ms step_avg:57.82ms
step:52/2330 train_time:3007ms step_avg:57.83ms
step:53/2330 train_time:3063ms step_avg:57.80ms
step:54/2330 train_time:3124ms step_avg:57.85ms
step:55/2330 train_time:3181ms step_avg:57.83ms
step:56/2330 train_time:3240ms step_avg:57.85ms
step:57/2330 train_time:3296ms step_avg:57.83ms
step:58/2330 train_time:3355ms step_avg:57.84ms
step:59/2330 train_time:3411ms step_avg:57.82ms
step:60/2330 train_time:3470ms step_avg:57.84ms
step:61/2330 train_time:3526ms step_avg:57.81ms
step:62/2330 train_time:3585ms step_avg:57.82ms
step:63/2330 train_time:3641ms step_avg:57.80ms
step:64/2330 train_time:3699ms step_avg:57.80ms
step:65/2330 train_time:3755ms step_avg:57.77ms
step:66/2330 train_time:3814ms step_avg:57.79ms
step:67/2330 train_time:3870ms step_avg:57.76ms
step:68/2330 train_time:3929ms step_avg:57.77ms
step:69/2330 train_time:3985ms step_avg:57.75ms
step:70/2330 train_time:4043ms step_avg:57.76ms
step:71/2330 train_time:4099ms step_avg:57.74ms
step:72/2330 train_time:4158ms step_avg:57.75ms
step:73/2330 train_time:4214ms step_avg:57.73ms
step:74/2330 train_time:4275ms step_avg:57.77ms
step:75/2330 train_time:4331ms step_avg:57.75ms
step:76/2330 train_time:4390ms step_avg:57.77ms
step:77/2330 train_time:4447ms step_avg:57.75ms
step:78/2330 train_time:4506ms step_avg:57.77ms
step:79/2330 train_time:4562ms step_avg:57.74ms
step:80/2330 train_time:4620ms step_avg:57.75ms
step:81/2330 train_time:4676ms step_avg:57.72ms
step:82/2330 train_time:4734ms step_avg:57.74ms
step:83/2330 train_time:4790ms step_avg:57.71ms
step:84/2330 train_time:4849ms step_avg:57.72ms
step:85/2330 train_time:4904ms step_avg:57.70ms
step:86/2330 train_time:4964ms step_avg:57.72ms
step:87/2330 train_time:5020ms step_avg:57.70ms
step:88/2330 train_time:5080ms step_avg:57.73ms
step:89/2330 train_time:5136ms step_avg:57.71ms
step:90/2330 train_time:5195ms step_avg:57.72ms
step:91/2330 train_time:5251ms step_avg:57.71ms
step:92/2330 train_time:5310ms step_avg:57.72ms
step:93/2330 train_time:5366ms step_avg:57.70ms
step:94/2330 train_time:5426ms step_avg:57.72ms
step:95/2330 train_time:5482ms step_avg:57.71ms
step:96/2330 train_time:5541ms step_avg:57.72ms
step:97/2330 train_time:5597ms step_avg:57.70ms
step:98/2330 train_time:5656ms step_avg:57.71ms
step:99/2330 train_time:5711ms step_avg:57.69ms
step:100/2330 train_time:5770ms step_avg:57.70ms
step:101/2330 train_time:5826ms step_avg:57.68ms
step:102/2330 train_time:5885ms step_avg:57.69ms
step:103/2330 train_time:5941ms step_avg:57.68ms
step:104/2330 train_time:6001ms step_avg:57.70ms
step:105/2330 train_time:6057ms step_avg:57.68ms
step:106/2330 train_time:6116ms step_avg:57.70ms
step:107/2330 train_time:6172ms step_avg:57.68ms
step:108/2330 train_time:6231ms step_avg:57.69ms
step:109/2330 train_time:6287ms step_avg:57.68ms
step:110/2330 train_time:6346ms step_avg:57.69ms
step:111/2330 train_time:6403ms step_avg:57.68ms
step:112/2330 train_time:6462ms step_avg:57.69ms
step:113/2330 train_time:6518ms step_avg:57.68ms
step:114/2330 train_time:6577ms step_avg:57.70ms
step:115/2330 train_time:6634ms step_avg:57.68ms
step:116/2330 train_time:6692ms step_avg:57.69ms
step:117/2330 train_time:6747ms step_avg:57.67ms
step:118/2330 train_time:6806ms step_avg:57.68ms
step:119/2330 train_time:6861ms step_avg:57.66ms
step:120/2330 train_time:6921ms step_avg:57.67ms
step:121/2330 train_time:6977ms step_avg:57.66ms
step:122/2330 train_time:7036ms step_avg:57.67ms
step:123/2330 train_time:7093ms step_avg:57.66ms
step:124/2330 train_time:7151ms step_avg:57.67ms
step:125/2330 train_time:7207ms step_avg:57.65ms
step:126/2330 train_time:7266ms step_avg:57.67ms
step:127/2330 train_time:7322ms step_avg:57.66ms
step:128/2330 train_time:7381ms step_avg:57.67ms
step:129/2330 train_time:7437ms step_avg:57.65ms
step:130/2330 train_time:7497ms step_avg:57.67ms
step:131/2330 train_time:7553ms step_avg:57.65ms
step:132/2330 train_time:7613ms step_avg:57.68ms
step:133/2330 train_time:7669ms step_avg:57.66ms
step:134/2330 train_time:7728ms step_avg:57.67ms
step:135/2330 train_time:7784ms step_avg:57.66ms
step:136/2330 train_time:7843ms step_avg:57.67ms
step:137/2330 train_time:7899ms step_avg:57.66ms
step:138/2330 train_time:7957ms step_avg:57.66ms
step:139/2330 train_time:8014ms step_avg:57.65ms
step:140/2330 train_time:8074ms step_avg:57.67ms
step:141/2330 train_time:8129ms step_avg:57.65ms
step:142/2330 train_time:8189ms step_avg:57.67ms
step:143/2330 train_time:8245ms step_avg:57.66ms
step:144/2330 train_time:8304ms step_avg:57.67ms
step:145/2330 train_time:8360ms step_avg:57.66ms
step:146/2330 train_time:8419ms step_avg:57.66ms
step:147/2330 train_time:8475ms step_avg:57.65ms
step:148/2330 train_time:8534ms step_avg:57.66ms
step:149/2330 train_time:8590ms step_avg:57.65ms
step:150/2330 train_time:8649ms step_avg:57.66ms
step:151/2330 train_time:8705ms step_avg:57.65ms
step:152/2330 train_time:8764ms step_avg:57.66ms
step:153/2330 train_time:8820ms step_avg:57.64ms
step:154/2330 train_time:8880ms step_avg:57.66ms
step:155/2330 train_time:8935ms step_avg:57.65ms
step:156/2330 train_time:8994ms step_avg:57.66ms
step:157/2330 train_time:9050ms step_avg:57.64ms
step:158/2330 train_time:9110ms step_avg:57.66ms
step:159/2330 train_time:9165ms step_avg:57.64ms
step:160/2330 train_time:9225ms step_avg:57.66ms
step:161/2330 train_time:9281ms step_avg:57.65ms
step:162/2330 train_time:9340ms step_avg:57.65ms
step:163/2330 train_time:9396ms step_avg:57.65ms
step:164/2330 train_time:9455ms step_avg:57.65ms
step:165/2330 train_time:9512ms step_avg:57.65ms
step:166/2330 train_time:9571ms step_avg:57.66ms
step:167/2330 train_time:9627ms step_avg:57.64ms
step:168/2330 train_time:9685ms step_avg:57.65ms
step:169/2330 train_time:9741ms step_avg:57.64ms
step:170/2330 train_time:9800ms step_avg:57.65ms
step:171/2330 train_time:9856ms step_avg:57.64ms
step:172/2330 train_time:9915ms step_avg:57.65ms
step:173/2330 train_time:9971ms step_avg:57.64ms
step:174/2330 train_time:10030ms step_avg:57.64ms
step:175/2330 train_time:10086ms step_avg:57.63ms
step:176/2330 train_time:10145ms step_avg:57.64ms
step:177/2330 train_time:10201ms step_avg:57.63ms
step:178/2330 train_time:10260ms step_avg:57.64ms
step:179/2330 train_time:10316ms step_avg:57.63ms
step:180/2330 train_time:10375ms step_avg:57.64ms
step:181/2330 train_time:10431ms step_avg:57.63ms
step:182/2330 train_time:10490ms step_avg:57.64ms
step:183/2330 train_time:10546ms step_avg:57.63ms
step:184/2330 train_time:10606ms step_avg:57.64ms
step:185/2330 train_time:10661ms step_avg:57.63ms
step:186/2330 train_time:10720ms step_avg:57.64ms
step:187/2330 train_time:10776ms step_avg:57.63ms
step:188/2330 train_time:10836ms step_avg:57.64ms
step:189/2330 train_time:10891ms step_avg:57.63ms
step:190/2330 train_time:10950ms step_avg:57.63ms
step:191/2330 train_time:11005ms step_avg:57.62ms
step:192/2330 train_time:11064ms step_avg:57.63ms
step:193/2330 train_time:11121ms step_avg:57.62ms
step:194/2330 train_time:11179ms step_avg:57.62ms
step:195/2330 train_time:11235ms step_avg:57.61ms
step:196/2330 train_time:11294ms step_avg:57.62ms
step:197/2330 train_time:11350ms step_avg:57.61ms
step:198/2330 train_time:11409ms step_avg:57.62ms
step:199/2330 train_time:11465ms step_avg:57.61ms
step:200/2330 train_time:11524ms step_avg:57.62ms
step:201/2330 train_time:11580ms step_avg:57.61ms
step:202/2330 train_time:11639ms step_avg:57.62ms
step:203/2330 train_time:11695ms step_avg:57.61ms
step:204/2330 train_time:11754ms step_avg:57.62ms
step:205/2330 train_time:11810ms step_avg:57.61ms
step:206/2330 train_time:11869ms step_avg:57.62ms
step:207/2330 train_time:11926ms step_avg:57.61ms
step:208/2330 train_time:11984ms step_avg:57.62ms
step:209/2330 train_time:12041ms step_avg:57.61ms
step:210/2330 train_time:12100ms step_avg:57.62ms
step:211/2330 train_time:12155ms step_avg:57.61ms
step:212/2330 train_time:12214ms step_avg:57.61ms
step:213/2330 train_time:12270ms step_avg:57.61ms
step:214/2330 train_time:12330ms step_avg:57.62ms
step:215/2330 train_time:12386ms step_avg:57.61ms
step:216/2330 train_time:12445ms step_avg:57.61ms
step:217/2330 train_time:12502ms step_avg:57.61ms
step:218/2330 train_time:12561ms step_avg:57.62ms
step:219/2330 train_time:12617ms step_avg:57.61ms
step:220/2330 train_time:12676ms step_avg:57.62ms
step:221/2330 train_time:12731ms step_avg:57.61ms
step:222/2330 train_time:12791ms step_avg:57.62ms
step:223/2330 train_time:12847ms step_avg:57.61ms
step:224/2330 train_time:12906ms step_avg:57.61ms
step:225/2330 train_time:12962ms step_avg:57.61ms
step:226/2330 train_time:13021ms step_avg:57.62ms
step:227/2330 train_time:13078ms step_avg:57.61ms
step:228/2330 train_time:13136ms step_avg:57.61ms
step:229/2330 train_time:13192ms step_avg:57.61ms
step:230/2330 train_time:13250ms step_avg:57.61ms
step:231/2330 train_time:13306ms step_avg:57.60ms
step:232/2330 train_time:13366ms step_avg:57.61ms
step:233/2330 train_time:13422ms step_avg:57.60ms
step:234/2330 train_time:13481ms step_avg:57.61ms
step:235/2330 train_time:13537ms step_avg:57.60ms
step:236/2330 train_time:13596ms step_avg:57.61ms
step:237/2330 train_time:13652ms step_avg:57.60ms
step:238/2330 train_time:13711ms step_avg:57.61ms
step:239/2330 train_time:13767ms step_avg:57.60ms
step:240/2330 train_time:13826ms step_avg:57.61ms
step:241/2330 train_time:13882ms step_avg:57.60ms
step:242/2330 train_time:13940ms step_avg:57.60ms
step:243/2330 train_time:13997ms step_avg:57.60ms
step:244/2330 train_time:14055ms step_avg:57.60ms
step:245/2330 train_time:14112ms step_avg:57.60ms
step:246/2330 train_time:14170ms step_avg:57.60ms
step:247/2330 train_time:14226ms step_avg:57.60ms
step:248/2330 train_time:14285ms step_avg:57.60ms
step:249/2330 train_time:14342ms step_avg:57.60ms
step:250/2330 train_time:14400ms step_avg:57.60ms
step:250/2330 val_loss:4.9038 train_time:14479ms step_avg:57.92ms
step:251/2330 train_time:14498ms step_avg:57.76ms
step:252/2330 train_time:14519ms step_avg:57.62ms
step:253/2330 train_time:14575ms step_avg:57.61ms
step:254/2330 train_time:14638ms step_avg:57.63ms
step:255/2330 train_time:14695ms step_avg:57.63ms
step:256/2330 train_time:14757ms step_avg:57.65ms
step:257/2330 train_time:14813ms step_avg:57.64ms
step:258/2330 train_time:14872ms step_avg:57.64ms
step:259/2330 train_time:14927ms step_avg:57.63ms
step:260/2330 train_time:14986ms step_avg:57.64ms
step:261/2330 train_time:15042ms step_avg:57.63ms
step:262/2330 train_time:15100ms step_avg:57.63ms
step:263/2330 train_time:15156ms step_avg:57.63ms
step:264/2330 train_time:15214ms step_avg:57.63ms
step:265/2330 train_time:15270ms step_avg:57.62ms
step:266/2330 train_time:15328ms step_avg:57.62ms
step:267/2330 train_time:15384ms step_avg:57.62ms
step:268/2330 train_time:15442ms step_avg:57.62ms
step:269/2330 train_time:15500ms step_avg:57.62ms
step:270/2330 train_time:15560ms step_avg:57.63ms
step:271/2330 train_time:15617ms step_avg:57.63ms
step:272/2330 train_time:15677ms step_avg:57.64ms
step:273/2330 train_time:15734ms step_avg:57.63ms
step:274/2330 train_time:15794ms step_avg:57.64ms
step:275/2330 train_time:15850ms step_avg:57.64ms
step:276/2330 train_time:15909ms step_avg:57.64ms
step:277/2330 train_time:15964ms step_avg:57.63ms
step:278/2330 train_time:16024ms step_avg:57.64ms
step:279/2330 train_time:16079ms step_avg:57.63ms
step:280/2330 train_time:16138ms step_avg:57.63ms
step:281/2330 train_time:16193ms step_avg:57.63ms
step:282/2330 train_time:16252ms step_avg:57.63ms
step:283/2330 train_time:16308ms step_avg:57.63ms
step:284/2330 train_time:16366ms step_avg:57.63ms
step:285/2330 train_time:16422ms step_avg:57.62ms
step:286/2330 train_time:16482ms step_avg:57.63ms
step:287/2330 train_time:16539ms step_avg:57.63ms
step:288/2330 train_time:16598ms step_avg:57.63ms
step:289/2330 train_time:16656ms step_avg:57.63ms
step:290/2330 train_time:16715ms step_avg:57.64ms
step:291/2330 train_time:16771ms step_avg:57.63ms
step:292/2330 train_time:16830ms step_avg:57.64ms
step:293/2330 train_time:16886ms step_avg:57.63ms
step:294/2330 train_time:16946ms step_avg:57.64ms
step:295/2330 train_time:17002ms step_avg:57.63ms
step:296/2330 train_time:17060ms step_avg:57.64ms
step:297/2330 train_time:17116ms step_avg:57.63ms
step:298/2330 train_time:17175ms step_avg:57.63ms
step:299/2330 train_time:17231ms step_avg:57.63ms
step:300/2330 train_time:17290ms step_avg:57.63ms
step:301/2330 train_time:17345ms step_avg:57.62ms
step:302/2330 train_time:17404ms step_avg:57.63ms
step:303/2330 train_time:17460ms step_avg:57.62ms
step:304/2330 train_time:17519ms step_avg:57.63ms
step:305/2330 train_time:17576ms step_avg:57.63ms
step:306/2330 train_time:17635ms step_avg:57.63ms
step:307/2330 train_time:17691ms step_avg:57.63ms
step:308/2330 train_time:17752ms step_avg:57.63ms
step:309/2330 train_time:17808ms step_avg:57.63ms
step:310/2330 train_time:17866ms step_avg:57.63ms
step:311/2330 train_time:17922ms step_avg:57.63ms
step:312/2330 train_time:17982ms step_avg:57.63ms
step:313/2330 train_time:18037ms step_avg:57.63ms
step:314/2330 train_time:18096ms step_avg:57.63ms
step:315/2330 train_time:18152ms step_avg:57.63ms
step:316/2330 train_time:18211ms step_avg:57.63ms
step:317/2330 train_time:18268ms step_avg:57.63ms
step:318/2330 train_time:18326ms step_avg:57.63ms
step:319/2330 train_time:18382ms step_avg:57.62ms
step:320/2330 train_time:18440ms step_avg:57.63ms
step:321/2330 train_time:18497ms step_avg:57.62ms
step:322/2330 train_time:18556ms step_avg:57.63ms
step:323/2330 train_time:18612ms step_avg:57.62ms
step:324/2330 train_time:18671ms step_avg:57.63ms
step:325/2330 train_time:18728ms step_avg:57.62ms
step:326/2330 train_time:18787ms step_avg:57.63ms
step:327/2330 train_time:18843ms step_avg:57.62ms
step:328/2330 train_time:18902ms step_avg:57.63ms
step:329/2330 train_time:18959ms step_avg:57.63ms
step:330/2330 train_time:19018ms step_avg:57.63ms
step:331/2330 train_time:19075ms step_avg:57.63ms
step:332/2330 train_time:19133ms step_avg:57.63ms
step:333/2330 train_time:19190ms step_avg:57.63ms
step:334/2330 train_time:19248ms step_avg:57.63ms
step:335/2330 train_time:19304ms step_avg:57.62ms
step:336/2330 train_time:19364ms step_avg:57.63ms
step:337/2330 train_time:19421ms step_avg:57.63ms
step:338/2330 train_time:19480ms step_avg:57.63ms
step:339/2330 train_time:19536ms step_avg:57.63ms
step:340/2330 train_time:19595ms step_avg:57.63ms
step:341/2330 train_time:19651ms step_avg:57.63ms
step:342/2330 train_time:19710ms step_avg:57.63ms
step:343/2330 train_time:19766ms step_avg:57.63ms
step:344/2330 train_time:19825ms step_avg:57.63ms
step:345/2330 train_time:19881ms step_avg:57.63ms
step:346/2330 train_time:19940ms step_avg:57.63ms
step:347/2330 train_time:19998ms step_avg:57.63ms
step:348/2330 train_time:20057ms step_avg:57.63ms
step:349/2330 train_time:20113ms step_avg:57.63ms
step:350/2330 train_time:20172ms step_avg:57.63ms
step:351/2330 train_time:20228ms step_avg:57.63ms
step:352/2330 train_time:20286ms step_avg:57.63ms
step:353/2330 train_time:20342ms step_avg:57.63ms
step:354/2330 train_time:20401ms step_avg:57.63ms
step:355/2330 train_time:20458ms step_avg:57.63ms
step:356/2330 train_time:20518ms step_avg:57.63ms
step:357/2330 train_time:20574ms step_avg:57.63ms
step:358/2330 train_time:20633ms step_avg:57.64ms
step:359/2330 train_time:20690ms step_avg:57.63ms
step:360/2330 train_time:20748ms step_avg:57.63ms
step:361/2330 train_time:20804ms step_avg:57.63ms
step:362/2330 train_time:20864ms step_avg:57.64ms
step:363/2330 train_time:20921ms step_avg:57.63ms
step:364/2330 train_time:20980ms step_avg:57.64ms
step:365/2330 train_time:21036ms step_avg:57.63ms
step:366/2330 train_time:21094ms step_avg:57.63ms
step:367/2330 train_time:21151ms step_avg:57.63ms
step:368/2330 train_time:21209ms step_avg:57.63ms
step:369/2330 train_time:21265ms step_avg:57.63ms
step:370/2330 train_time:21324ms step_avg:57.63ms
step:371/2330 train_time:21380ms step_avg:57.63ms
step:372/2330 train_time:21439ms step_avg:57.63ms
step:373/2330 train_time:21496ms step_avg:57.63ms
step:374/2330 train_time:21556ms step_avg:57.64ms
step:375/2330 train_time:21612ms step_avg:57.63ms
step:376/2330 train_time:21670ms step_avg:57.63ms
step:377/2330 train_time:21727ms step_avg:57.63ms
step:378/2330 train_time:21785ms step_avg:57.63ms
step:379/2330 train_time:21842ms step_avg:57.63ms
step:380/2330 train_time:21901ms step_avg:57.63ms
step:381/2330 train_time:21957ms step_avg:57.63ms
step:382/2330 train_time:22016ms step_avg:57.63ms
step:383/2330 train_time:22072ms step_avg:57.63ms
step:384/2330 train_time:22131ms step_avg:57.63ms
step:385/2330 train_time:22188ms step_avg:57.63ms
step:386/2330 train_time:22247ms step_avg:57.63ms
step:387/2330 train_time:22303ms step_avg:57.63ms
step:388/2330 train_time:22362ms step_avg:57.63ms
step:389/2330 train_time:22419ms step_avg:57.63ms
step:390/2330 train_time:22478ms step_avg:57.63ms
step:391/2330 train_time:22534ms step_avg:57.63ms
step:392/2330 train_time:22593ms step_avg:57.64ms
step:393/2330 train_time:22650ms step_avg:57.63ms
step:394/2330 train_time:22708ms step_avg:57.63ms
step:395/2330 train_time:22764ms step_avg:57.63ms
step:396/2330 train_time:22824ms step_avg:57.64ms
step:397/2330 train_time:22879ms step_avg:57.63ms
step:398/2330 train_time:22939ms step_avg:57.64ms
step:399/2330 train_time:22996ms step_avg:57.63ms
step:400/2330 train_time:23056ms step_avg:57.64ms
step:401/2330 train_time:23112ms step_avg:57.64ms
step:402/2330 train_time:23170ms step_avg:57.64ms
step:403/2330 train_time:23226ms step_avg:57.63ms
step:404/2330 train_time:23286ms step_avg:57.64ms
step:405/2330 train_time:23342ms step_avg:57.63ms
step:406/2330 train_time:23401ms step_avg:57.64ms
step:407/2330 train_time:23458ms step_avg:57.64ms
step:408/2330 train_time:23517ms step_avg:57.64ms
step:409/2330 train_time:23573ms step_avg:57.64ms
step:410/2330 train_time:23632ms step_avg:57.64ms
step:411/2330 train_time:23688ms step_avg:57.64ms
step:412/2330 train_time:23747ms step_avg:57.64ms
step:413/2330 train_time:23803ms step_avg:57.63ms
step:414/2330 train_time:23863ms step_avg:57.64ms
step:415/2330 train_time:23919ms step_avg:57.64ms
step:416/2330 train_time:23979ms step_avg:57.64ms
step:417/2330 train_time:24035ms step_avg:57.64ms
step:418/2330 train_time:24094ms step_avg:57.64ms
step:419/2330 train_time:24151ms step_avg:57.64ms
step:420/2330 train_time:24210ms step_avg:57.64ms
step:421/2330 train_time:24266ms step_avg:57.64ms
step:422/2330 train_time:24325ms step_avg:57.64ms
step:423/2330 train_time:24381ms step_avg:57.64ms
step:424/2330 train_time:24440ms step_avg:57.64ms
step:425/2330 train_time:24497ms step_avg:57.64ms
step:426/2330 train_time:24556ms step_avg:57.64ms
step:427/2330 train_time:24613ms step_avg:57.64ms
step:428/2330 train_time:24672ms step_avg:57.64ms
step:429/2330 train_time:24728ms step_avg:57.64ms
step:430/2330 train_time:24787ms step_avg:57.64ms
step:431/2330 train_time:24843ms step_avg:57.64ms
step:432/2330 train_time:24902ms step_avg:57.64ms
step:433/2330 train_time:24959ms step_avg:57.64ms
step:434/2330 train_time:25017ms step_avg:57.64ms
step:435/2330 train_time:25073ms step_avg:57.64ms
step:436/2330 train_time:25133ms step_avg:57.64ms
step:437/2330 train_time:25189ms step_avg:57.64ms
step:438/2330 train_time:25248ms step_avg:57.64ms
step:439/2330 train_time:25304ms step_avg:57.64ms
step:440/2330 train_time:25363ms step_avg:57.64ms
step:441/2330 train_time:25419ms step_avg:57.64ms
step:442/2330 train_time:25479ms step_avg:57.64ms
step:443/2330 train_time:25536ms step_avg:57.64ms
step:444/2330 train_time:25595ms step_avg:57.65ms
step:445/2330 train_time:25651ms step_avg:57.64ms
step:446/2330 train_time:25710ms step_avg:57.64ms
step:447/2330 train_time:25766ms step_avg:57.64ms
step:448/2330 train_time:25826ms step_avg:57.65ms
step:449/2330 train_time:25882ms step_avg:57.64ms
step:450/2330 train_time:25940ms step_avg:57.65ms
step:451/2330 train_time:25997ms step_avg:57.64ms
step:452/2330 train_time:26056ms step_avg:57.65ms
step:453/2330 train_time:26112ms step_avg:57.64ms
step:454/2330 train_time:26171ms step_avg:57.65ms
step:455/2330 train_time:26227ms step_avg:57.64ms
step:456/2330 train_time:26286ms step_avg:57.64ms
step:457/2330 train_time:26342ms step_avg:57.64ms
step:458/2330 train_time:26402ms step_avg:57.65ms
step:459/2330 train_time:26458ms step_avg:57.64ms
step:460/2330 train_time:26517ms step_avg:57.65ms
step:461/2330 train_time:26573ms step_avg:57.64ms
step:462/2330 train_time:26633ms step_avg:57.65ms
step:463/2330 train_time:26689ms step_avg:57.64ms
step:464/2330 train_time:26748ms step_avg:57.65ms
step:465/2330 train_time:26804ms step_avg:57.64ms
step:466/2330 train_time:26863ms step_avg:57.65ms
step:467/2330 train_time:26920ms step_avg:57.64ms
step:468/2330 train_time:26979ms step_avg:57.65ms
step:469/2330 train_time:27036ms step_avg:57.65ms
step:470/2330 train_time:27095ms step_avg:57.65ms
step:471/2330 train_time:27151ms step_avg:57.65ms
step:472/2330 train_time:27210ms step_avg:57.65ms
step:473/2330 train_time:27266ms step_avg:57.64ms
step:474/2330 train_time:27324ms step_avg:57.65ms
step:475/2330 train_time:27380ms step_avg:57.64ms
step:476/2330 train_time:27441ms step_avg:57.65ms
step:477/2330 train_time:27497ms step_avg:57.65ms
step:478/2330 train_time:27557ms step_avg:57.65ms
step:479/2330 train_time:27613ms step_avg:57.65ms
step:480/2330 train_time:27672ms step_avg:57.65ms
step:481/2330 train_time:27728ms step_avg:57.65ms
step:482/2330 train_time:27787ms step_avg:57.65ms
step:483/2330 train_time:27843ms step_avg:57.65ms
step:484/2330 train_time:27903ms step_avg:57.65ms
step:485/2330 train_time:27959ms step_avg:57.65ms
step:486/2330 train_time:28019ms step_avg:57.65ms
step:487/2330 train_time:28076ms step_avg:57.65ms
step:488/2330 train_time:28134ms step_avg:57.65ms
step:489/2330 train_time:28191ms step_avg:57.65ms
step:490/2330 train_time:28249ms step_avg:57.65ms
step:491/2330 train_time:28305ms step_avg:57.65ms
step:492/2330 train_time:28365ms step_avg:57.65ms
step:493/2330 train_time:28421ms step_avg:57.65ms
step:494/2330 train_time:28480ms step_avg:57.65ms
step:495/2330 train_time:28537ms step_avg:57.65ms
step:496/2330 train_time:28596ms step_avg:57.65ms
step:497/2330 train_time:28652ms step_avg:57.65ms
step:498/2330 train_time:28711ms step_avg:57.65ms
step:499/2330 train_time:28767ms step_avg:57.65ms
step:500/2330 train_time:28827ms step_avg:57.65ms
step:500/2330 val_loss:4.4174 train_time:28907ms step_avg:57.81ms
step:501/2330 train_time:28927ms step_avg:57.74ms
step:502/2330 train_time:28946ms step_avg:57.66ms
step:503/2330 train_time:29002ms step_avg:57.66ms
step:504/2330 train_time:29067ms step_avg:57.67ms
step:505/2330 train_time:29124ms step_avg:57.67ms
step:506/2330 train_time:29186ms step_avg:57.68ms
step:507/2330 train_time:29242ms step_avg:57.68ms
step:508/2330 train_time:29302ms step_avg:57.68ms
step:509/2330 train_time:29358ms step_avg:57.68ms
step:510/2330 train_time:29417ms step_avg:57.68ms
step:511/2330 train_time:29472ms step_avg:57.68ms
step:512/2330 train_time:29530ms step_avg:57.68ms
step:513/2330 train_time:29587ms step_avg:57.67ms
step:514/2330 train_time:29645ms step_avg:57.67ms
step:515/2330 train_time:29700ms step_avg:57.67ms
step:516/2330 train_time:29759ms step_avg:57.67ms
step:517/2330 train_time:29814ms step_avg:57.67ms
step:518/2330 train_time:29874ms step_avg:57.67ms
step:519/2330 train_time:29930ms step_avg:57.67ms
step:520/2330 train_time:29991ms step_avg:57.67ms
step:521/2330 train_time:30048ms step_avg:57.67ms
step:522/2330 train_time:30108ms step_avg:57.68ms
step:523/2330 train_time:30165ms step_avg:57.68ms
step:524/2330 train_time:30225ms step_avg:57.68ms
step:525/2330 train_time:30281ms step_avg:57.68ms
step:526/2330 train_time:30341ms step_avg:57.68ms
step:527/2330 train_time:30397ms step_avg:57.68ms
step:528/2330 train_time:30456ms step_avg:57.68ms
step:529/2330 train_time:30512ms step_avg:57.68ms
step:530/2330 train_time:30570ms step_avg:57.68ms
step:531/2330 train_time:30626ms step_avg:57.68ms
step:532/2330 train_time:30684ms step_avg:57.68ms
step:533/2330 train_time:30740ms step_avg:57.67ms
step:534/2330 train_time:30799ms step_avg:57.68ms
step:535/2330 train_time:30856ms step_avg:57.68ms
step:536/2330 train_time:30916ms step_avg:57.68ms
step:537/2330 train_time:30973ms step_avg:57.68ms
step:538/2330 train_time:31033ms step_avg:57.68ms
step:539/2330 train_time:31089ms step_avg:57.68ms
step:540/2330 train_time:31150ms step_avg:57.68ms
step:541/2330 train_time:31206ms step_avg:57.68ms
step:542/2330 train_time:31266ms step_avg:57.69ms
step:543/2330 train_time:31321ms step_avg:57.68ms
step:544/2330 train_time:31382ms step_avg:57.69ms
step:545/2330 train_time:31438ms step_avg:57.68ms
step:546/2330 train_time:31497ms step_avg:57.69ms
step:547/2330 train_time:31553ms step_avg:57.68ms
step:548/2330 train_time:31611ms step_avg:57.69ms
step:549/2330 train_time:31667ms step_avg:57.68ms
step:550/2330 train_time:31726ms step_avg:57.68ms
step:551/2330 train_time:31781ms step_avg:57.68ms
step:552/2330 train_time:31840ms step_avg:57.68ms
step:553/2330 train_time:31897ms step_avg:57.68ms
step:554/2330 train_time:31957ms step_avg:57.68ms
step:555/2330 train_time:32014ms step_avg:57.68ms
step:556/2330 train_time:32073ms step_avg:57.69ms
step:557/2330 train_time:32130ms step_avg:57.68ms
step:558/2330 train_time:32190ms step_avg:57.69ms
step:559/2330 train_time:32246ms step_avg:57.69ms
step:560/2330 train_time:32306ms step_avg:57.69ms
step:561/2330 train_time:32362ms step_avg:57.69ms
step:562/2330 train_time:32422ms step_avg:57.69ms
step:563/2330 train_time:32479ms step_avg:57.69ms
step:564/2330 train_time:32537ms step_avg:57.69ms
step:565/2330 train_time:32593ms step_avg:57.69ms
step:566/2330 train_time:32652ms step_avg:57.69ms
step:567/2330 train_time:32708ms step_avg:57.69ms
step:568/2330 train_time:32766ms step_avg:57.69ms
step:569/2330 train_time:32823ms step_avg:57.68ms
step:570/2330 train_time:32882ms step_avg:57.69ms
step:571/2330 train_time:32938ms step_avg:57.69ms
step:572/2330 train_time:32997ms step_avg:57.69ms
step:573/2330 train_time:33054ms step_avg:57.69ms
step:574/2330 train_time:33113ms step_avg:57.69ms
step:575/2330 train_time:33170ms step_avg:57.69ms
step:576/2330 train_time:33229ms step_avg:57.69ms
step:577/2330 train_time:33285ms step_avg:57.69ms
step:578/2330 train_time:33347ms step_avg:57.69ms
step:579/2330 train_time:33402ms step_avg:57.69ms
step:580/2330 train_time:33463ms step_avg:57.70ms
step:581/2330 train_time:33519ms step_avg:57.69ms
step:582/2330 train_time:33580ms step_avg:57.70ms
step:583/2330 train_time:33637ms step_avg:57.70ms
step:584/2330 train_time:33695ms step_avg:57.70ms
step:585/2330 train_time:33751ms step_avg:57.69ms
step:586/2330 train_time:33810ms step_avg:57.70ms
step:587/2330 train_time:33867ms step_avg:57.69ms
step:588/2330 train_time:33926ms step_avg:57.70ms
step:589/2330 train_time:33982ms step_avg:57.70ms
step:590/2330 train_time:34042ms step_avg:57.70ms
step:591/2330 train_time:34099ms step_avg:57.70ms
step:592/2330 train_time:34157ms step_avg:57.70ms
step:593/2330 train_time:34215ms step_avg:57.70ms
step:594/2330 train_time:34274ms step_avg:57.70ms
step:595/2330 train_time:34331ms step_avg:57.70ms
step:596/2330 train_time:34390ms step_avg:57.70ms
step:597/2330 train_time:34446ms step_avg:57.70ms
step:598/2330 train_time:34507ms step_avg:57.70ms
step:599/2330 train_time:34563ms step_avg:57.70ms
step:600/2330 train_time:34623ms step_avg:57.70ms
step:601/2330 train_time:34679ms step_avg:57.70ms
step:602/2330 train_time:34738ms step_avg:57.70ms
step:603/2330 train_time:34794ms step_avg:57.70ms
step:604/2330 train_time:34853ms step_avg:57.70ms
step:605/2330 train_time:34909ms step_avg:57.70ms
step:606/2330 train_time:34967ms step_avg:57.70ms
step:607/2330 train_time:35023ms step_avg:57.70ms
step:608/2330 train_time:35083ms step_avg:57.70ms
step:609/2330 train_time:35140ms step_avg:57.70ms
step:610/2330 train_time:35199ms step_avg:57.70ms
step:611/2330 train_time:35255ms step_avg:57.70ms
step:612/2330 train_time:35315ms step_avg:57.70ms
step:613/2330 train_time:35372ms step_avg:57.70ms
step:614/2330 train_time:35431ms step_avg:57.71ms
step:615/2330 train_time:35488ms step_avg:57.70ms
step:616/2330 train_time:35548ms step_avg:57.71ms
step:617/2330 train_time:35604ms step_avg:57.70ms
step:618/2330 train_time:35664ms step_avg:57.71ms
step:619/2330 train_time:35720ms step_avg:57.71ms
step:620/2330 train_time:35779ms step_avg:57.71ms
step:621/2330 train_time:35835ms step_avg:57.71ms
step:622/2330 train_time:35894ms step_avg:57.71ms
step:623/2330 train_time:35950ms step_avg:57.70ms
step:624/2330 train_time:36009ms step_avg:57.71ms
step:625/2330 train_time:36065ms step_avg:57.70ms
step:626/2330 train_time:36126ms step_avg:57.71ms
step:627/2330 train_time:36182ms step_avg:57.71ms
step:628/2330 train_time:36241ms step_avg:57.71ms
step:629/2330 train_time:36297ms step_avg:57.71ms
step:630/2330 train_time:36357ms step_avg:57.71ms
step:631/2330 train_time:36414ms step_avg:57.71ms
step:632/2330 train_time:36473ms step_avg:57.71ms
step:633/2330 train_time:36529ms step_avg:57.71ms
step:634/2330 train_time:36589ms step_avg:57.71ms
step:635/2330 train_time:36644ms step_avg:57.71ms
step:636/2330 train_time:36706ms step_avg:57.71ms
step:637/2330 train_time:36762ms step_avg:57.71ms
step:638/2330 train_time:36821ms step_avg:57.71ms
step:639/2330 train_time:36878ms step_avg:57.71ms
step:640/2330 train_time:36937ms step_avg:57.71ms
step:641/2330 train_time:36993ms step_avg:57.71ms
step:642/2330 train_time:37052ms step_avg:57.71ms
step:643/2330 train_time:37109ms step_avg:57.71ms
step:644/2330 train_time:37168ms step_avg:57.71ms
step:645/2330 train_time:37224ms step_avg:57.71ms
step:646/2330 train_time:37282ms step_avg:57.71ms
step:647/2330 train_time:37339ms step_avg:57.71ms
step:648/2330 train_time:37399ms step_avg:57.71ms
step:649/2330 train_time:37455ms step_avg:57.71ms
step:650/2330 train_time:37515ms step_avg:57.72ms
step:651/2330 train_time:37572ms step_avg:57.71ms
step:652/2330 train_time:37631ms step_avg:57.72ms
step:653/2330 train_time:37688ms step_avg:57.72ms
step:654/2330 train_time:37749ms step_avg:57.72ms
step:655/2330 train_time:37805ms step_avg:57.72ms
step:656/2330 train_time:37865ms step_avg:57.72ms
step:657/2330 train_time:37921ms step_avg:57.72ms
step:658/2330 train_time:37980ms step_avg:57.72ms
step:659/2330 train_time:38036ms step_avg:57.72ms
step:660/2330 train_time:38095ms step_avg:57.72ms
step:661/2330 train_time:38151ms step_avg:57.72ms
step:662/2330 train_time:38211ms step_avg:57.72ms
step:663/2330 train_time:38267ms step_avg:57.72ms
step:664/2330 train_time:38329ms step_avg:57.72ms
step:665/2330 train_time:38385ms step_avg:57.72ms
step:666/2330 train_time:38444ms step_avg:57.72ms
step:667/2330 train_time:38500ms step_avg:57.72ms
step:668/2330 train_time:38560ms step_avg:57.72ms
step:669/2330 train_time:38617ms step_avg:57.72ms
step:670/2330 train_time:38676ms step_avg:57.72ms
step:671/2330 train_time:38732ms step_avg:57.72ms
step:672/2330 train_time:38792ms step_avg:57.73ms
step:673/2330 train_time:38848ms step_avg:57.72ms
step:674/2330 train_time:38907ms step_avg:57.73ms
step:675/2330 train_time:38963ms step_avg:57.72ms
step:676/2330 train_time:39022ms step_avg:57.73ms
step:677/2330 train_time:39078ms step_avg:57.72ms
step:678/2330 train_time:39138ms step_avg:57.73ms
step:679/2330 train_time:39194ms step_avg:57.72ms
step:680/2330 train_time:39254ms step_avg:57.73ms
step:681/2330 train_time:39311ms step_avg:57.72ms
step:682/2330 train_time:39369ms step_avg:57.73ms
step:683/2330 train_time:39425ms step_avg:57.72ms
step:684/2330 train_time:39485ms step_avg:57.73ms
step:685/2330 train_time:39541ms step_avg:57.72ms
step:686/2330 train_time:39599ms step_avg:57.72ms
step:687/2330 train_time:39656ms step_avg:57.72ms
step:688/2330 train_time:39716ms step_avg:57.73ms
step:689/2330 train_time:39772ms step_avg:57.72ms
step:690/2330 train_time:39831ms step_avg:57.73ms
step:691/2330 train_time:39888ms step_avg:57.72ms
step:692/2330 train_time:39948ms step_avg:57.73ms
step:693/2330 train_time:40004ms step_avg:57.73ms
step:694/2330 train_time:40064ms step_avg:57.73ms
step:695/2330 train_time:40120ms step_avg:57.73ms
step:696/2330 train_time:40179ms step_avg:57.73ms
step:697/2330 train_time:40236ms step_avg:57.73ms
step:698/2330 train_time:40295ms step_avg:57.73ms
step:699/2330 train_time:40352ms step_avg:57.73ms
step:700/2330 train_time:40411ms step_avg:57.73ms
step:701/2330 train_time:40467ms step_avg:57.73ms
step:702/2330 train_time:40525ms step_avg:57.73ms
step:703/2330 train_time:40581ms step_avg:57.73ms
step:704/2330 train_time:40640ms step_avg:57.73ms
step:705/2330 train_time:40697ms step_avg:57.73ms
step:706/2330 train_time:40757ms step_avg:57.73ms
step:707/2330 train_time:40813ms step_avg:57.73ms
step:708/2330 train_time:40872ms step_avg:57.73ms
step:709/2330 train_time:40929ms step_avg:57.73ms
step:710/2330 train_time:40988ms step_avg:57.73ms
step:711/2330 train_time:41045ms step_avg:57.73ms
step:712/2330 train_time:41103ms step_avg:57.73ms
step:713/2330 train_time:41159ms step_avg:57.73ms
step:714/2330 train_time:41219ms step_avg:57.73ms
step:715/2330 train_time:41275ms step_avg:57.73ms
step:716/2330 train_time:41335ms step_avg:57.73ms
step:717/2330 train_time:41392ms step_avg:57.73ms
step:718/2330 train_time:41451ms step_avg:57.73ms
step:719/2330 train_time:41508ms step_avg:57.73ms
step:720/2330 train_time:41566ms step_avg:57.73ms
step:721/2330 train_time:41622ms step_avg:57.73ms
step:722/2330 train_time:41682ms step_avg:57.73ms
step:723/2330 train_time:41738ms step_avg:57.73ms
step:724/2330 train_time:41798ms step_avg:57.73ms
step:725/2330 train_time:41854ms step_avg:57.73ms
step:726/2330 train_time:41914ms step_avg:57.73ms
step:727/2330 train_time:41970ms step_avg:57.73ms
step:728/2330 train_time:42029ms step_avg:57.73ms
step:729/2330 train_time:42085ms step_avg:57.73ms
step:730/2330 train_time:42145ms step_avg:57.73ms
step:731/2330 train_time:42201ms step_avg:57.73ms
step:732/2330 train_time:42261ms step_avg:57.73ms
step:733/2330 train_time:42317ms step_avg:57.73ms
step:734/2330 train_time:42376ms step_avg:57.73ms
step:735/2330 train_time:42433ms step_avg:57.73ms
step:736/2330 train_time:42492ms step_avg:57.73ms
step:737/2330 train_time:42548ms step_avg:57.73ms
step:738/2330 train_time:42608ms step_avg:57.73ms
step:739/2330 train_time:42664ms step_avg:57.73ms
step:740/2330 train_time:42725ms step_avg:57.74ms
step:741/2330 train_time:42780ms step_avg:57.73ms
step:742/2330 train_time:42840ms step_avg:57.74ms
step:743/2330 train_time:42897ms step_avg:57.73ms
step:744/2330 train_time:42956ms step_avg:57.74ms
step:745/2330 train_time:43012ms step_avg:57.73ms
step:746/2330 train_time:43071ms step_avg:57.74ms
step:747/2330 train_time:43126ms step_avg:57.73ms
step:748/2330 train_time:43186ms step_avg:57.74ms
step:749/2330 train_time:43242ms step_avg:57.73ms
step:750/2330 train_time:43301ms step_avg:57.73ms
step:750/2330 val_loss:4.2148 train_time:43381ms step_avg:57.84ms
step:751/2330 train_time:43400ms step_avg:57.79ms
step:752/2330 train_time:43420ms step_avg:57.74ms
step:753/2330 train_time:43476ms step_avg:57.74ms
step:754/2330 train_time:43540ms step_avg:57.75ms
step:755/2330 train_time:43597ms step_avg:57.74ms
step:756/2330 train_time:43659ms step_avg:57.75ms
step:757/2330 train_time:43716ms step_avg:57.75ms
step:758/2330 train_time:43774ms step_avg:57.75ms
step:759/2330 train_time:43830ms step_avg:57.75ms
step:760/2330 train_time:43889ms step_avg:57.75ms
step:761/2330 train_time:43944ms step_avg:57.75ms
step:762/2330 train_time:44003ms step_avg:57.75ms
step:763/2330 train_time:44058ms step_avg:57.74ms
step:764/2330 train_time:44117ms step_avg:57.74ms
step:765/2330 train_time:44175ms step_avg:57.74ms
step:766/2330 train_time:44232ms step_avg:57.74ms
step:767/2330 train_time:44289ms step_avg:57.74ms
step:768/2330 train_time:44350ms step_avg:57.75ms
step:769/2330 train_time:44407ms step_avg:57.75ms
step:770/2330 train_time:44470ms step_avg:57.75ms
step:771/2330 train_time:44528ms step_avg:57.75ms
step:772/2330 train_time:44591ms step_avg:57.76ms
step:773/2330 train_time:44648ms step_avg:57.76ms
step:774/2330 train_time:44709ms step_avg:57.76ms
step:775/2330 train_time:44766ms step_avg:57.76ms
step:776/2330 train_time:44826ms step_avg:57.77ms
step:777/2330 train_time:44882ms step_avg:57.76ms
step:778/2330 train_time:44942ms step_avg:57.77ms
step:779/2330 train_time:44998ms step_avg:57.76ms
step:780/2330 train_time:45057ms step_avg:57.77ms
step:781/2330 train_time:45114ms step_avg:57.76ms
step:782/2330 train_time:45173ms step_avg:57.77ms
step:783/2330 train_time:45230ms step_avg:57.76ms
step:784/2330 train_time:45290ms step_avg:57.77ms
step:785/2330 train_time:45347ms step_avg:57.77ms
step:786/2330 train_time:45407ms step_avg:57.77ms
step:787/2330 train_time:45464ms step_avg:57.77ms
step:788/2330 train_time:45526ms step_avg:57.77ms
step:789/2330 train_time:45583ms step_avg:57.77ms
step:790/2330 train_time:45645ms step_avg:57.78ms
step:791/2330 train_time:45702ms step_avg:57.78ms
step:792/2330 train_time:45763ms step_avg:57.78ms
step:793/2330 train_time:45820ms step_avg:57.78ms
step:794/2330 train_time:45879ms step_avg:57.78ms
step:795/2330 train_time:45936ms step_avg:57.78ms
step:796/2330 train_time:45995ms step_avg:57.78ms
step:797/2330 train_time:46052ms step_avg:57.78ms
step:798/2330 train_time:46111ms step_avg:57.78ms
step:799/2330 train_time:46167ms step_avg:57.78ms
step:800/2330 train_time:46227ms step_avg:57.78ms
step:801/2330 train_time:46284ms step_avg:57.78ms
step:802/2330 train_time:46344ms step_avg:57.78ms
step:803/2330 train_time:46401ms step_avg:57.78ms
step:804/2330 train_time:46463ms step_avg:57.79ms
step:805/2330 train_time:46520ms step_avg:57.79ms
step:806/2330 train_time:46581ms step_avg:57.79ms
step:807/2330 train_time:46638ms step_avg:57.79ms
step:808/2330 train_time:46699ms step_avg:57.80ms
step:809/2330 train_time:46756ms step_avg:57.80ms
step:810/2330 train_time:46816ms step_avg:57.80ms
step:811/2330 train_time:46873ms step_avg:57.80ms
step:812/2330 train_time:46933ms step_avg:57.80ms
step:813/2330 train_time:46990ms step_avg:57.80ms
step:814/2330 train_time:47049ms step_avg:57.80ms
step:815/2330 train_time:47106ms step_avg:57.80ms
step:816/2330 train_time:47166ms step_avg:57.80ms
step:817/2330 train_time:47223ms step_avg:57.80ms
step:818/2330 train_time:47282ms step_avg:57.80ms
step:819/2330 train_time:47339ms step_avg:57.80ms
step:820/2330 train_time:47399ms step_avg:57.80ms
step:821/2330 train_time:47457ms step_avg:57.80ms
step:822/2330 train_time:47516ms step_avg:57.81ms
step:823/2330 train_time:47574ms step_avg:57.81ms
step:824/2330 train_time:47634ms step_avg:57.81ms
step:825/2330 train_time:47692ms step_avg:57.81ms
step:826/2330 train_time:47752ms step_avg:57.81ms
step:827/2330 train_time:47810ms step_avg:57.81ms
step:828/2330 train_time:47869ms step_avg:57.81ms
step:829/2330 train_time:47926ms step_avg:57.81ms
step:830/2330 train_time:47987ms step_avg:57.82ms
step:831/2330 train_time:48043ms step_avg:57.81ms
step:832/2330 train_time:48104ms step_avg:57.82ms
step:833/2330 train_time:48160ms step_avg:57.82ms
step:834/2330 train_time:48221ms step_avg:57.82ms
step:835/2330 train_time:48278ms step_avg:57.82ms
step:836/2330 train_time:48338ms step_avg:57.82ms
step:837/2330 train_time:48396ms step_avg:57.82ms
step:838/2330 train_time:48455ms step_avg:57.82ms
step:839/2330 train_time:48513ms step_avg:57.82ms
step:840/2330 train_time:48572ms step_avg:57.82ms
step:841/2330 train_time:48629ms step_avg:57.82ms
step:842/2330 train_time:48690ms step_avg:57.83ms
step:843/2330 train_time:48747ms step_avg:57.83ms
step:844/2330 train_time:48809ms step_avg:57.83ms
step:845/2330 train_time:48865ms step_avg:57.83ms
step:846/2330 train_time:48927ms step_avg:57.83ms
step:847/2330 train_time:48984ms step_avg:57.83ms
step:848/2330 train_time:49044ms step_avg:57.83ms
step:849/2330 train_time:49100ms step_avg:57.83ms
step:850/2330 train_time:49160ms step_avg:57.84ms
step:851/2330 train_time:49217ms step_avg:57.83ms
step:852/2330 train_time:49277ms step_avg:57.84ms
step:853/2330 train_time:49334ms step_avg:57.84ms
step:854/2330 train_time:49394ms step_avg:57.84ms
step:855/2330 train_time:49450ms step_avg:57.84ms
step:856/2330 train_time:49511ms step_avg:57.84ms
step:857/2330 train_time:49568ms step_avg:57.84ms
step:858/2330 train_time:49629ms step_avg:57.84ms
step:859/2330 train_time:49685ms step_avg:57.84ms
step:860/2330 train_time:49747ms step_avg:57.85ms
step:861/2330 train_time:49804ms step_avg:57.84ms
step:862/2330 train_time:49864ms step_avg:57.85ms
step:863/2330 train_time:49921ms step_avg:57.85ms
step:864/2330 train_time:49981ms step_avg:57.85ms
step:865/2330 train_time:50038ms step_avg:57.85ms
step:866/2330 train_time:50098ms step_avg:57.85ms
step:867/2330 train_time:50155ms step_avg:57.85ms
step:868/2330 train_time:50215ms step_avg:57.85ms
step:869/2330 train_time:50272ms step_avg:57.85ms
step:870/2330 train_time:50332ms step_avg:57.85ms
step:871/2330 train_time:50389ms step_avg:57.85ms
step:872/2330 train_time:50449ms step_avg:57.85ms
step:873/2330 train_time:50507ms step_avg:57.85ms
step:874/2330 train_time:50568ms step_avg:57.86ms
step:875/2330 train_time:50625ms step_avg:57.86ms
step:876/2330 train_time:50685ms step_avg:57.86ms
step:877/2330 train_time:50742ms step_avg:57.86ms
step:878/2330 train_time:50803ms step_avg:57.86ms
step:879/2330 train_time:50861ms step_avg:57.86ms
step:880/2330 train_time:50921ms step_avg:57.86ms
step:881/2330 train_time:50978ms step_avg:57.86ms
step:882/2330 train_time:51038ms step_avg:57.87ms
step:883/2330 train_time:51095ms step_avg:57.86ms
step:884/2330 train_time:51154ms step_avg:57.87ms
step:885/2330 train_time:51211ms step_avg:57.87ms
step:886/2330 train_time:51271ms step_avg:57.87ms
step:887/2330 train_time:51328ms step_avg:57.87ms
step:888/2330 train_time:51388ms step_avg:57.87ms
step:889/2330 train_time:51445ms step_avg:57.87ms
step:890/2330 train_time:51507ms step_avg:57.87ms
step:891/2330 train_time:51564ms step_avg:57.87ms
step:892/2330 train_time:51624ms step_avg:57.87ms
step:893/2330 train_time:51681ms step_avg:57.87ms
step:894/2330 train_time:51741ms step_avg:57.88ms
step:895/2330 train_time:51798ms step_avg:57.87ms
step:896/2330 train_time:51858ms step_avg:57.88ms
step:897/2330 train_time:51915ms step_avg:57.88ms
step:898/2330 train_time:51976ms step_avg:57.88ms
step:899/2330 train_time:52032ms step_avg:57.88ms
step:900/2330 train_time:52092ms step_avg:57.88ms
step:901/2330 train_time:52149ms step_avg:57.88ms
step:902/2330 train_time:52209ms step_avg:57.88ms
step:903/2330 train_time:52266ms step_avg:57.88ms
step:904/2330 train_time:52326ms step_avg:57.88ms
step:905/2330 train_time:52383ms step_avg:57.88ms
step:906/2330 train_time:52443ms step_avg:57.88ms
step:907/2330 train_time:52500ms step_avg:57.88ms
step:908/2330 train_time:52561ms step_avg:57.89ms
step:909/2330 train_time:52618ms step_avg:57.89ms
step:910/2330 train_time:52678ms step_avg:57.89ms
step:911/2330 train_time:52735ms step_avg:57.89ms
step:912/2330 train_time:52796ms step_avg:57.89ms
step:913/2330 train_time:52852ms step_avg:57.89ms
step:914/2330 train_time:52912ms step_avg:57.89ms
step:915/2330 train_time:52970ms step_avg:57.89ms
step:916/2330 train_time:53030ms step_avg:57.89ms
step:917/2330 train_time:53087ms step_avg:57.89ms
step:918/2330 train_time:53147ms step_avg:57.89ms
step:919/2330 train_time:53204ms step_avg:57.89ms
step:920/2330 train_time:53263ms step_avg:57.89ms
step:921/2330 train_time:53321ms step_avg:57.89ms
step:922/2330 train_time:53382ms step_avg:57.90ms
step:923/2330 train_time:53438ms step_avg:57.90ms
step:924/2330 train_time:53499ms step_avg:57.90ms
step:925/2330 train_time:53556ms step_avg:57.90ms
step:926/2330 train_time:53616ms step_avg:57.90ms
step:927/2330 train_time:53673ms step_avg:57.90ms
step:928/2330 train_time:53733ms step_avg:57.90ms
step:929/2330 train_time:53790ms step_avg:57.90ms
step:930/2330 train_time:53850ms step_avg:57.90ms
step:931/2330 train_time:53907ms step_avg:57.90ms
step:932/2330 train_time:53967ms step_avg:57.90ms
step:933/2330 train_time:54023ms step_avg:57.90ms
step:934/2330 train_time:54086ms step_avg:57.91ms
step:935/2330 train_time:54142ms step_avg:57.91ms
step:936/2330 train_time:54202ms step_avg:57.91ms
step:937/2330 train_time:54259ms step_avg:57.91ms
step:938/2330 train_time:54319ms step_avg:57.91ms
step:939/2330 train_time:54376ms step_avg:57.91ms
step:940/2330 train_time:54437ms step_avg:57.91ms
step:941/2330 train_time:54495ms step_avg:57.91ms
step:942/2330 train_time:54553ms step_avg:57.91ms
step:943/2330 train_time:54611ms step_avg:57.91ms
step:944/2330 train_time:54671ms step_avg:57.91ms
step:945/2330 train_time:54728ms step_avg:57.91ms
step:946/2330 train_time:54788ms step_avg:57.92ms
step:947/2330 train_time:54845ms step_avg:57.91ms
step:948/2330 train_time:54905ms step_avg:57.92ms
step:949/2330 train_time:54963ms step_avg:57.92ms
step:950/2330 train_time:55023ms step_avg:57.92ms
step:951/2330 train_time:55079ms step_avg:57.92ms
step:952/2330 train_time:55140ms step_avg:57.92ms
step:953/2330 train_time:55197ms step_avg:57.92ms
step:954/2330 train_time:55257ms step_avg:57.92ms
step:955/2330 train_time:55314ms step_avg:57.92ms
step:956/2330 train_time:55375ms step_avg:57.92ms
step:957/2330 train_time:55433ms step_avg:57.92ms
step:958/2330 train_time:55492ms step_avg:57.92ms
step:959/2330 train_time:55549ms step_avg:57.92ms
step:960/2330 train_time:55610ms step_avg:57.93ms
step:961/2330 train_time:55667ms step_avg:57.93ms
step:962/2330 train_time:55726ms step_avg:57.93ms
step:963/2330 train_time:55783ms step_avg:57.93ms
step:964/2330 train_time:55843ms step_avg:57.93ms
step:965/2330 train_time:55900ms step_avg:57.93ms
step:966/2330 train_time:55960ms step_avg:57.93ms
step:967/2330 train_time:56017ms step_avg:57.93ms
step:968/2330 train_time:56077ms step_avg:57.93ms
step:969/2330 train_time:56135ms step_avg:57.93ms
step:970/2330 train_time:56194ms step_avg:57.93ms
step:971/2330 train_time:56251ms step_avg:57.93ms
step:972/2330 train_time:56311ms step_avg:57.93ms
step:973/2330 train_time:56368ms step_avg:57.93ms
step:974/2330 train_time:56428ms step_avg:57.93ms
step:975/2330 train_time:56485ms step_avg:57.93ms
step:976/2330 train_time:56547ms step_avg:57.94ms
step:977/2330 train_time:56603ms step_avg:57.94ms
step:978/2330 train_time:56664ms step_avg:57.94ms
step:979/2330 train_time:56720ms step_avg:57.94ms
step:980/2330 train_time:56781ms step_avg:57.94ms
step:981/2330 train_time:56838ms step_avg:57.94ms
step:982/2330 train_time:56898ms step_avg:57.94ms
step:983/2330 train_time:56954ms step_avg:57.94ms
step:984/2330 train_time:57015ms step_avg:57.94ms
step:985/2330 train_time:57072ms step_avg:57.94ms
step:986/2330 train_time:57131ms step_avg:57.94ms
step:987/2330 train_time:57188ms step_avg:57.94ms
step:988/2330 train_time:57248ms step_avg:57.94ms
step:989/2330 train_time:57305ms step_avg:57.94ms
step:990/2330 train_time:57366ms step_avg:57.95ms
step:991/2330 train_time:57423ms step_avg:57.94ms
step:992/2330 train_time:57484ms step_avg:57.95ms
step:993/2330 train_time:57541ms step_avg:57.95ms
step:994/2330 train_time:57602ms step_avg:57.95ms
step:995/2330 train_time:57659ms step_avg:57.95ms
step:996/2330 train_time:57719ms step_avg:57.95ms
step:997/2330 train_time:57777ms step_avg:57.95ms
step:998/2330 train_time:57836ms step_avg:57.95ms
step:999/2330 train_time:57893ms step_avg:57.95ms
step:1000/2330 train_time:57953ms step_avg:57.95ms
step:1000/2330 val_loss:4.0648 train_time:58033ms step_avg:58.03ms
step:1001/2330 train_time:58053ms step_avg:57.99ms
step:1002/2330 train_time:58072ms step_avg:57.96ms
step:1003/2330 train_time:58125ms step_avg:57.95ms
step:1004/2330 train_time:58193ms step_avg:57.96ms
step:1005/2330 train_time:58249ms step_avg:57.96ms
step:1006/2330 train_time:58314ms step_avg:57.97ms
step:1007/2330 train_time:58370ms step_avg:57.96ms
step:1008/2330 train_time:58432ms step_avg:57.97ms
step:1009/2330 train_time:58488ms step_avg:57.97ms
step:1010/2330 train_time:58547ms step_avg:57.97ms
step:1011/2330 train_time:58603ms step_avg:57.97ms
step:1012/2330 train_time:58662ms step_avg:57.97ms
step:1013/2330 train_time:58718ms step_avg:57.96ms
step:1014/2330 train_time:58778ms step_avg:57.97ms
step:1015/2330 train_time:58835ms step_avg:57.97ms
step:1016/2330 train_time:58893ms step_avg:57.97ms
step:1017/2330 train_time:58955ms step_avg:57.97ms
step:1018/2330 train_time:59016ms step_avg:57.97ms
step:1019/2330 train_time:59075ms step_avg:57.97ms
step:1020/2330 train_time:59136ms step_avg:57.98ms
step:1021/2330 train_time:59193ms step_avg:57.98ms
step:1022/2330 train_time:59254ms step_avg:57.98ms
step:1023/2330 train_time:59312ms step_avg:57.98ms
step:1024/2330 train_time:59372ms step_avg:57.98ms
step:1025/2330 train_time:59429ms step_avg:57.98ms
step:1026/2330 train_time:59490ms step_avg:57.98ms
step:1027/2330 train_time:59547ms step_avg:57.98ms
step:1028/2330 train_time:59607ms step_avg:57.98ms
step:1029/2330 train_time:59664ms step_avg:57.98ms
step:1030/2330 train_time:59722ms step_avg:57.98ms
step:1031/2330 train_time:59779ms step_avg:57.98ms
step:1032/2330 train_time:59838ms step_avg:57.98ms
step:1033/2330 train_time:59896ms step_avg:57.98ms
step:1034/2330 train_time:59956ms step_avg:57.98ms
step:1035/2330 train_time:60015ms step_avg:57.99ms
step:1036/2330 train_time:60075ms step_avg:57.99ms
step:1037/2330 train_time:60133ms step_avg:57.99ms
step:1038/2330 train_time:60193ms step_avg:57.99ms
step:1039/2330 train_time:60251ms step_avg:57.99ms
step:1040/2330 train_time:60310ms step_avg:57.99ms
step:1041/2330 train_time:60368ms step_avg:57.99ms
step:1042/2330 train_time:60429ms step_avg:57.99ms
step:1043/2330 train_time:60486ms step_avg:57.99ms
step:1044/2330 train_time:60546ms step_avg:57.99ms
step:1045/2330 train_time:60604ms step_avg:57.99ms
step:1046/2330 train_time:60663ms step_avg:58.00ms
step:1047/2330 train_time:60719ms step_avg:57.99ms
step:1048/2330 train_time:60779ms step_avg:58.00ms
step:1049/2330 train_time:60836ms step_avg:57.99ms
step:1050/2330 train_time:60896ms step_avg:58.00ms
step:1051/2330 train_time:60954ms step_avg:58.00ms
step:1052/2330 train_time:61014ms step_avg:58.00ms
step:1053/2330 train_time:61072ms step_avg:58.00ms
step:1054/2330 train_time:61131ms step_avg:58.00ms
step:1055/2330 train_time:61189ms step_avg:58.00ms
step:1056/2330 train_time:61249ms step_avg:58.00ms
step:1057/2330 train_time:61306ms step_avg:58.00ms
step:1058/2330 train_time:61367ms step_avg:58.00ms
step:1059/2330 train_time:61423ms step_avg:58.00ms
step:1060/2330 train_time:61483ms step_avg:58.00ms
step:1061/2330 train_time:61540ms step_avg:58.00ms
step:1062/2330 train_time:61599ms step_avg:58.00ms
step:1063/2330 train_time:61657ms step_avg:58.00ms
step:1064/2330 train_time:61716ms step_avg:58.00ms
step:1065/2330 train_time:61773ms step_avg:58.00ms
step:1066/2330 train_time:61833ms step_avg:58.00ms
step:1067/2330 train_time:61890ms step_avg:58.00ms
step:1068/2330 train_time:61950ms step_avg:58.01ms
step:1069/2330 train_time:62008ms step_avg:58.01ms
step:1070/2330 train_time:62068ms step_avg:58.01ms
step:1071/2330 train_time:62125ms step_avg:58.01ms
step:1072/2330 train_time:62184ms step_avg:58.01ms
step:1073/2330 train_time:62242ms step_avg:58.01ms
step:1074/2330 train_time:62301ms step_avg:58.01ms
step:1075/2330 train_time:62359ms step_avg:58.01ms
step:1076/2330 train_time:62418ms step_avg:58.01ms
step:1077/2330 train_time:62475ms step_avg:58.01ms
step:1078/2330 train_time:62535ms step_avg:58.01ms
step:1079/2330 train_time:62591ms step_avg:58.01ms
step:1080/2330 train_time:62652ms step_avg:58.01ms
step:1081/2330 train_time:62709ms step_avg:58.01ms
step:1082/2330 train_time:62768ms step_avg:58.01ms
step:1083/2330 train_time:62825ms step_avg:58.01ms
step:1084/2330 train_time:62885ms step_avg:58.01ms
step:1085/2330 train_time:62943ms step_avg:58.01ms
step:1086/2330 train_time:63002ms step_avg:58.01ms
step:1087/2330 train_time:63060ms step_avg:58.01ms
step:1088/2330 train_time:63119ms step_avg:58.01ms
step:1089/2330 train_time:63176ms step_avg:58.01ms
step:1090/2330 train_time:63236ms step_avg:58.02ms
step:1091/2330 train_time:63294ms step_avg:58.01ms
step:1092/2330 train_time:63355ms step_avg:58.02ms
step:1093/2330 train_time:63412ms step_avg:58.02ms
step:1094/2330 train_time:63471ms step_avg:58.02ms
step:1095/2330 train_time:63529ms step_avg:58.02ms
step:1096/2330 train_time:63589ms step_avg:58.02ms
step:1097/2330 train_time:63647ms step_avg:58.02ms
step:1098/2330 train_time:63706ms step_avg:58.02ms
step:1099/2330 train_time:63764ms step_avg:58.02ms
step:1100/2330 train_time:63823ms step_avg:58.02ms
step:1101/2330 train_time:63881ms step_avg:58.02ms
step:1102/2330 train_time:63940ms step_avg:58.02ms
step:1103/2330 train_time:63997ms step_avg:58.02ms
step:1104/2330 train_time:64056ms step_avg:58.02ms
step:1105/2330 train_time:64113ms step_avg:58.02ms
step:1106/2330 train_time:64175ms step_avg:58.02ms
step:1107/2330 train_time:64232ms step_avg:58.02ms
step:1108/2330 train_time:64292ms step_avg:58.03ms
step:1109/2330 train_time:64350ms step_avg:58.02ms
step:1110/2330 train_time:64410ms step_avg:58.03ms
step:1111/2330 train_time:64467ms step_avg:58.03ms
step:1112/2330 train_time:64527ms step_avg:58.03ms
step:1113/2330 train_time:64584ms step_avg:58.03ms
step:1114/2330 train_time:64644ms step_avg:58.03ms
step:1115/2330 train_time:64701ms step_avg:58.03ms
step:1116/2330 train_time:64761ms step_avg:58.03ms
step:1117/2330 train_time:64818ms step_avg:58.03ms
step:1118/2330 train_time:64878ms step_avg:58.03ms
step:1119/2330 train_time:64935ms step_avg:58.03ms
step:1120/2330 train_time:64995ms step_avg:58.03ms
step:1121/2330 train_time:65053ms step_avg:58.03ms
step:1122/2330 train_time:65112ms step_avg:58.03ms
step:1123/2330 train_time:65170ms step_avg:58.03ms
step:1124/2330 train_time:65230ms step_avg:58.03ms
step:1125/2330 train_time:65288ms step_avg:58.03ms
step:1126/2330 train_time:65347ms step_avg:58.03ms
step:1127/2330 train_time:65404ms step_avg:58.03ms
step:1128/2330 train_time:65464ms step_avg:58.04ms
step:1129/2330 train_time:65521ms step_avg:58.03ms
step:1130/2330 train_time:65580ms step_avg:58.04ms
step:1131/2330 train_time:65637ms step_avg:58.03ms
step:1132/2330 train_time:65697ms step_avg:58.04ms
step:1133/2330 train_time:65754ms step_avg:58.04ms
step:1134/2330 train_time:65814ms step_avg:58.04ms
step:1135/2330 train_time:65872ms step_avg:58.04ms
step:1136/2330 train_time:65932ms step_avg:58.04ms
step:1137/2330 train_time:65990ms step_avg:58.04ms
step:1138/2330 train_time:66049ms step_avg:58.04ms
step:1139/2330 train_time:66107ms step_avg:58.04ms
step:1140/2330 train_time:66167ms step_avg:58.04ms
step:1141/2330 train_time:66224ms step_avg:58.04ms
step:1142/2330 train_time:66285ms step_avg:58.04ms
step:1143/2330 train_time:66342ms step_avg:58.04ms
step:1144/2330 train_time:66402ms step_avg:58.04ms
step:1145/2330 train_time:66459ms step_avg:58.04ms
step:1146/2330 train_time:66519ms step_avg:58.04ms
step:1147/2330 train_time:66575ms step_avg:58.04ms
step:1148/2330 train_time:66635ms step_avg:58.04ms
step:1149/2330 train_time:66693ms step_avg:58.04ms
step:1150/2330 train_time:66753ms step_avg:58.05ms
step:1151/2330 train_time:66811ms step_avg:58.05ms
step:1152/2330 train_time:66870ms step_avg:58.05ms
step:1153/2330 train_time:66928ms step_avg:58.05ms
step:1154/2330 train_time:66988ms step_avg:58.05ms
step:1155/2330 train_time:67045ms step_avg:58.05ms
step:1156/2330 train_time:67104ms step_avg:58.05ms
step:1157/2330 train_time:67162ms step_avg:58.05ms
step:1158/2330 train_time:67222ms step_avg:58.05ms
step:1159/2330 train_time:67279ms step_avg:58.05ms
step:1160/2330 train_time:67340ms step_avg:58.05ms
step:1161/2330 train_time:67397ms step_avg:58.05ms
step:1162/2330 train_time:67456ms step_avg:58.05ms
step:1163/2330 train_time:67513ms step_avg:58.05ms
step:1164/2330 train_time:67574ms step_avg:58.05ms
step:1165/2330 train_time:67631ms step_avg:58.05ms
step:1166/2330 train_time:67691ms step_avg:58.05ms
step:1167/2330 train_time:67748ms step_avg:58.05ms
step:1168/2330 train_time:67808ms step_avg:58.06ms
step:1169/2330 train_time:67865ms step_avg:58.05ms
step:1170/2330 train_time:67925ms step_avg:58.06ms
step:1171/2330 train_time:67982ms step_avg:58.05ms
step:1172/2330 train_time:68043ms step_avg:58.06ms
step:1173/2330 train_time:68100ms step_avg:58.06ms
step:1174/2330 train_time:68159ms step_avg:58.06ms
step:1175/2330 train_time:68216ms step_avg:58.06ms
step:1176/2330 train_time:68276ms step_avg:58.06ms
step:1177/2330 train_time:68334ms step_avg:58.06ms
step:1178/2330 train_time:68394ms step_avg:58.06ms
step:1179/2330 train_time:68451ms step_avg:58.06ms
step:1180/2330 train_time:68512ms step_avg:58.06ms
step:1181/2330 train_time:68569ms step_avg:58.06ms
step:1182/2330 train_time:68628ms step_avg:58.06ms
step:1183/2330 train_time:68685ms step_avg:58.06ms
step:1184/2330 train_time:68745ms step_avg:58.06ms
step:1185/2330 train_time:68802ms step_avg:58.06ms
step:1186/2330 train_time:68862ms step_avg:58.06ms
step:1187/2330 train_time:68919ms step_avg:58.06ms
step:1188/2330 train_time:68979ms step_avg:58.06ms
step:1189/2330 train_time:69036ms step_avg:58.06ms
step:1190/2330 train_time:69096ms step_avg:58.06ms
step:1191/2330 train_time:69153ms step_avg:58.06ms
step:1192/2330 train_time:69214ms step_avg:58.07ms
step:1193/2330 train_time:69272ms step_avg:58.07ms
step:1194/2330 train_time:69332ms step_avg:58.07ms
step:1195/2330 train_time:69390ms step_avg:58.07ms
step:1196/2330 train_time:69449ms step_avg:58.07ms
step:1197/2330 train_time:69507ms step_avg:58.07ms
step:1198/2330 train_time:69567ms step_avg:58.07ms
step:1199/2330 train_time:69623ms step_avg:58.07ms
step:1200/2330 train_time:69684ms step_avg:58.07ms
step:1201/2330 train_time:69741ms step_avg:58.07ms
step:1202/2330 train_time:69801ms step_avg:58.07ms
step:1203/2330 train_time:69858ms step_avg:58.07ms
step:1204/2330 train_time:69917ms step_avg:58.07ms
step:1205/2330 train_time:69975ms step_avg:58.07ms
step:1206/2330 train_time:70034ms step_avg:58.07ms
step:1207/2330 train_time:70092ms step_avg:58.07ms
step:1208/2330 train_time:70151ms step_avg:58.07ms
step:1209/2330 train_time:70209ms step_avg:58.07ms
step:1210/2330 train_time:70269ms step_avg:58.07ms
step:1211/2330 train_time:70326ms step_avg:58.07ms
step:1212/2330 train_time:70386ms step_avg:58.07ms
step:1213/2330 train_time:70442ms step_avg:58.07ms
step:1214/2330 train_time:70503ms step_avg:58.07ms
step:1215/2330 train_time:70560ms step_avg:58.07ms
step:1216/2330 train_time:70620ms step_avg:58.08ms
step:1217/2330 train_time:70676ms step_avg:58.07ms
step:1218/2330 train_time:70737ms step_avg:58.08ms
step:1219/2330 train_time:70794ms step_avg:58.08ms
step:1220/2330 train_time:70854ms step_avg:58.08ms
step:1221/2330 train_time:70912ms step_avg:58.08ms
step:1222/2330 train_time:70971ms step_avg:58.08ms
step:1223/2330 train_time:71028ms step_avg:58.08ms
step:1224/2330 train_time:71088ms step_avg:58.08ms
step:1225/2330 train_time:71146ms step_avg:58.08ms
step:1226/2330 train_time:71205ms step_avg:58.08ms
step:1227/2330 train_time:71263ms step_avg:58.08ms
step:1228/2330 train_time:71323ms step_avg:58.08ms
step:1229/2330 train_time:71380ms step_avg:58.08ms
step:1230/2330 train_time:71439ms step_avg:58.08ms
step:1231/2330 train_time:71496ms step_avg:58.08ms
step:1232/2330 train_time:71555ms step_avg:58.08ms
step:1233/2330 train_time:71613ms step_avg:58.08ms
step:1234/2330 train_time:71673ms step_avg:58.08ms
step:1235/2330 train_time:71730ms step_avg:58.08ms
step:1236/2330 train_time:71790ms step_avg:58.08ms
step:1237/2330 train_time:71848ms step_avg:58.08ms
step:1238/2330 train_time:71908ms step_avg:58.08ms
step:1239/2330 train_time:71965ms step_avg:58.08ms
step:1240/2330 train_time:72025ms step_avg:58.08ms
step:1241/2330 train_time:72082ms step_avg:58.08ms
step:1242/2330 train_time:72141ms step_avg:58.08ms
step:1243/2330 train_time:72199ms step_avg:58.08ms
step:1244/2330 train_time:72259ms step_avg:58.09ms
step:1245/2330 train_time:72315ms step_avg:58.08ms
step:1246/2330 train_time:72376ms step_avg:58.09ms
step:1247/2330 train_time:72433ms step_avg:58.09ms
step:1248/2330 train_time:72493ms step_avg:58.09ms
step:1249/2330 train_time:72550ms step_avg:58.09ms
step:1250/2330 train_time:72611ms step_avg:58.09ms
step:1250/2330 val_loss:3.9871 train_time:72691ms step_avg:58.15ms
step:1251/2330 train_time:72711ms step_avg:58.12ms
step:1252/2330 train_time:72731ms step_avg:58.09ms
step:1253/2330 train_time:72791ms step_avg:58.09ms
step:1254/2330 train_time:72854ms step_avg:58.10ms
step:1255/2330 train_time:72911ms step_avg:58.10ms
step:1256/2330 train_time:72971ms step_avg:58.10ms
step:1257/2330 train_time:73028ms step_avg:58.10ms
step:1258/2330 train_time:73088ms step_avg:58.10ms
step:1259/2330 train_time:73145ms step_avg:58.10ms
step:1260/2330 train_time:73204ms step_avg:58.10ms
step:1261/2330 train_time:73260ms step_avg:58.10ms
step:1262/2330 train_time:73319ms step_avg:58.10ms
step:1263/2330 train_time:73375ms step_avg:58.10ms
step:1264/2330 train_time:73434ms step_avg:58.10ms
step:1265/2330 train_time:73491ms step_avg:58.10ms
step:1266/2330 train_time:73550ms step_avg:58.10ms
step:1267/2330 train_time:73608ms step_avg:58.10ms
step:1268/2330 train_time:73670ms step_avg:58.10ms
step:1269/2330 train_time:73729ms step_avg:58.10ms
step:1270/2330 train_time:73790ms step_avg:58.10ms
step:1271/2330 train_time:73848ms step_avg:58.10ms
step:1272/2330 train_time:73907ms step_avg:58.10ms
step:1273/2330 train_time:73965ms step_avg:58.10ms
step:1274/2330 train_time:74025ms step_avg:58.10ms
step:1275/2330 train_time:74082ms step_avg:58.10ms
step:1276/2330 train_time:74142ms step_avg:58.10ms
step:1277/2330 train_time:74199ms step_avg:58.10ms
step:1278/2330 train_time:74258ms step_avg:58.11ms
step:1279/2330 train_time:74315ms step_avg:58.10ms
step:1280/2330 train_time:74374ms step_avg:58.10ms
step:1281/2330 train_time:74431ms step_avg:58.10ms
step:1282/2330 train_time:74490ms step_avg:58.10ms
step:1283/2330 train_time:74547ms step_avg:58.10ms
step:1284/2330 train_time:74607ms step_avg:58.11ms
step:1285/2330 train_time:74664ms step_avg:58.10ms
step:1286/2330 train_time:74725ms step_avg:58.11ms
step:1287/2330 train_time:74782ms step_avg:58.11ms
step:1288/2330 train_time:74843ms step_avg:58.11ms
step:1289/2330 train_time:74901ms step_avg:58.11ms
step:1290/2330 train_time:75583ms step_avg:58.59ms
step:1291/2330 train_time:75600ms step_avg:58.56ms
step:1292/2330 train_time:75645ms step_avg:58.55ms
step:1293/2330 train_time:75701ms step_avg:58.55ms
step:1294/2330 train_time:75760ms step_avg:58.55ms
step:1295/2330 train_time:75816ms step_avg:58.55ms
step:1296/2330 train_time:75875ms step_avg:58.55ms
step:1297/2330 train_time:75932ms step_avg:58.54ms
step:1298/2330 train_time:75991ms step_avg:58.54ms
step:1299/2330 train_time:76047ms step_avg:58.54ms
step:1300/2330 train_time:76106ms step_avg:58.54ms
step:1301/2330 train_time:76162ms step_avg:58.54ms
step:1302/2330 train_time:76221ms step_avg:58.54ms
step:1303/2330 train_time:76277ms step_avg:58.54ms
step:1304/2330 train_time:76337ms step_avg:58.54ms
step:1305/2330 train_time:76393ms step_avg:58.54ms
step:1306/2330 train_time:76454ms step_avg:58.54ms
step:1307/2330 train_time:76517ms step_avg:58.54ms
step:1308/2330 train_time:76579ms step_avg:58.55ms
step:1309/2330 train_time:76637ms step_avg:58.55ms
step:1310/2330 train_time:76700ms step_avg:58.55ms
step:1311/2330 train_time:76756ms step_avg:58.55ms
step:1312/2330 train_time:76817ms step_avg:58.55ms
step:1313/2330 train_time:76873ms step_avg:58.55ms
step:1314/2330 train_time:76933ms step_avg:58.55ms
step:1315/2330 train_time:76989ms step_avg:58.55ms
step:1316/2330 train_time:77049ms step_avg:58.55ms
step:1317/2330 train_time:77105ms step_avg:58.55ms
step:1318/2330 train_time:77165ms step_avg:58.55ms
step:1319/2330 train_time:77221ms step_avg:58.55ms
step:1320/2330 train_time:77280ms step_avg:58.55ms
step:1321/2330 train_time:77336ms step_avg:58.54ms
step:1322/2330 train_time:77396ms step_avg:58.54ms
step:1323/2330 train_time:77454ms step_avg:58.54ms
step:1324/2330 train_time:77515ms step_avg:58.55ms
step:1325/2330 train_time:77575ms step_avg:58.55ms
step:1326/2330 train_time:77636ms step_avg:58.55ms
step:1327/2330 train_time:77693ms step_avg:58.55ms
step:1328/2330 train_time:77755ms step_avg:58.55ms
step:1329/2330 train_time:77812ms step_avg:58.55ms
step:1330/2330 train_time:77872ms step_avg:58.55ms
step:1331/2330 train_time:77929ms step_avg:58.55ms
step:1332/2330 train_time:77989ms step_avg:58.55ms
step:1333/2330 train_time:78046ms step_avg:58.55ms
step:1334/2330 train_time:78105ms step_avg:58.55ms
step:1335/2330 train_time:78161ms step_avg:58.55ms
step:1336/2330 train_time:78221ms step_avg:58.55ms
step:1337/2330 train_time:78277ms step_avg:58.55ms
step:1338/2330 train_time:78337ms step_avg:58.55ms
step:1339/2330 train_time:78394ms step_avg:58.55ms
step:1340/2330 train_time:78454ms step_avg:58.55ms
step:1341/2330 train_time:78512ms step_avg:58.55ms
step:1342/2330 train_time:78574ms step_avg:58.55ms
step:1343/2330 train_time:78632ms step_avg:58.55ms
step:1344/2330 train_time:78693ms step_avg:58.55ms
step:1345/2330 train_time:78750ms step_avg:58.55ms
step:1346/2330 train_time:78810ms step_avg:58.55ms
step:1347/2330 train_time:78867ms step_avg:58.55ms
step:1348/2330 train_time:78928ms step_avg:58.55ms
step:1349/2330 train_time:78984ms step_avg:58.55ms
step:1350/2330 train_time:79044ms step_avg:58.55ms
step:1351/2330 train_time:79101ms step_avg:58.55ms
step:1352/2330 train_time:79161ms step_avg:58.55ms
step:1353/2330 train_time:79217ms step_avg:58.55ms
step:1354/2330 train_time:79277ms step_avg:58.55ms
step:1355/2330 train_time:79334ms step_avg:58.55ms
step:1356/2330 train_time:79393ms step_avg:58.55ms
step:1357/2330 train_time:79451ms step_avg:58.55ms
step:1358/2330 train_time:79511ms step_avg:58.55ms
step:1359/2330 train_time:79569ms step_avg:58.55ms
step:1360/2330 train_time:79629ms step_avg:58.55ms
step:1361/2330 train_time:79686ms step_avg:58.55ms
step:1362/2330 train_time:79749ms step_avg:58.55ms
step:1363/2330 train_time:79806ms step_avg:58.55ms
step:1364/2330 train_time:79867ms step_avg:58.55ms
step:1365/2330 train_time:79923ms step_avg:58.55ms
step:1366/2330 train_time:79983ms step_avg:58.55ms
step:1367/2330 train_time:80039ms step_avg:58.55ms
step:1368/2330 train_time:80100ms step_avg:58.55ms
step:1369/2330 train_time:80156ms step_avg:58.55ms
step:1370/2330 train_time:80216ms step_avg:58.55ms
step:1371/2330 train_time:80272ms step_avg:58.55ms
step:1372/2330 train_time:80333ms step_avg:58.55ms
step:1373/2330 train_time:80390ms step_avg:58.55ms
step:1374/2330 train_time:80450ms step_avg:58.55ms
step:1375/2330 train_time:80507ms step_avg:58.55ms
step:1376/2330 train_time:80568ms step_avg:58.55ms
step:1377/2330 train_time:80626ms step_avg:58.55ms
step:1378/2330 train_time:80686ms step_avg:58.55ms
step:1379/2330 train_time:80742ms step_avg:58.55ms
step:1380/2330 train_time:80804ms step_avg:58.55ms
step:1381/2330 train_time:80862ms step_avg:58.55ms
step:1382/2330 train_time:80923ms step_avg:58.55ms
step:1383/2330 train_time:80979ms step_avg:58.55ms
step:1384/2330 train_time:81040ms step_avg:58.55ms
step:1385/2330 train_time:81097ms step_avg:58.55ms
step:1386/2330 train_time:81156ms step_avg:58.55ms
step:1387/2330 train_time:81213ms step_avg:58.55ms
step:1388/2330 train_time:81273ms step_avg:58.55ms
step:1389/2330 train_time:81329ms step_avg:58.55ms
step:1390/2330 train_time:81390ms step_avg:58.55ms
step:1391/2330 train_time:81447ms step_avg:58.55ms
step:1392/2330 train_time:81507ms step_avg:58.55ms
step:1393/2330 train_time:81564ms step_avg:58.55ms
step:1394/2330 train_time:81624ms step_avg:58.55ms
step:1395/2330 train_time:81682ms step_avg:58.55ms
step:1396/2330 train_time:81742ms step_avg:58.55ms
step:1397/2330 train_time:81799ms step_avg:58.55ms
step:1398/2330 train_time:81860ms step_avg:58.55ms
step:1399/2330 train_time:81916ms step_avg:58.55ms
step:1400/2330 train_time:81977ms step_avg:58.56ms
step:1401/2330 train_time:82034ms step_avg:58.55ms
step:1402/2330 train_time:82094ms step_avg:58.55ms
step:1403/2330 train_time:82151ms step_avg:58.55ms
step:1404/2330 train_time:82211ms step_avg:58.55ms
step:1405/2330 train_time:82267ms step_avg:58.55ms
step:1406/2330 train_time:82328ms step_avg:58.55ms
step:1407/2330 train_time:82385ms step_avg:58.55ms
step:1408/2330 train_time:82445ms step_avg:58.56ms
step:1409/2330 train_time:82503ms step_avg:58.55ms
step:1410/2330 train_time:82562ms step_avg:58.55ms
step:1411/2330 train_time:82620ms step_avg:58.55ms
step:1412/2330 train_time:82680ms step_avg:58.56ms
step:1413/2330 train_time:82736ms step_avg:58.55ms
step:1414/2330 train_time:82797ms step_avg:58.56ms
step:1415/2330 train_time:82854ms step_avg:58.55ms
step:1416/2330 train_time:82916ms step_avg:58.56ms
step:1417/2330 train_time:82973ms step_avg:58.56ms
step:1418/2330 train_time:83033ms step_avg:58.56ms
step:1419/2330 train_time:83090ms step_avg:58.56ms
step:1420/2330 train_time:83149ms step_avg:58.56ms
step:1421/2330 train_time:83207ms step_avg:58.55ms
step:1422/2330 train_time:83267ms step_avg:58.56ms
step:1423/2330 train_time:83324ms step_avg:58.55ms
step:1424/2330 train_time:83383ms step_avg:58.56ms
step:1425/2330 train_time:83440ms step_avg:58.55ms
step:1426/2330 train_time:83501ms step_avg:58.56ms
step:1427/2330 train_time:83558ms step_avg:58.56ms
step:1428/2330 train_time:83618ms step_avg:58.56ms
step:1429/2330 train_time:83675ms step_avg:58.55ms
step:1430/2330 train_time:83736ms step_avg:58.56ms
step:1431/2330 train_time:83793ms step_avg:58.56ms
step:1432/2330 train_time:83854ms step_avg:58.56ms
step:1433/2330 train_time:83911ms step_avg:58.56ms
step:1434/2330 train_time:83970ms step_avg:58.56ms
step:1435/2330 train_time:84027ms step_avg:58.56ms
step:1436/2330 train_time:84088ms step_avg:58.56ms
step:1437/2330 train_time:84145ms step_avg:58.56ms
step:1438/2330 train_time:84205ms step_avg:58.56ms
step:1439/2330 train_time:84262ms step_avg:58.56ms
step:1440/2330 train_time:84323ms step_avg:58.56ms
step:1441/2330 train_time:84379ms step_avg:58.56ms
step:1442/2330 train_time:84440ms step_avg:58.56ms
step:1443/2330 train_time:84496ms step_avg:58.56ms
step:1444/2330 train_time:84557ms step_avg:58.56ms
step:1445/2330 train_time:84614ms step_avg:58.56ms
step:1446/2330 train_time:84674ms step_avg:58.56ms
step:1447/2330 train_time:84731ms step_avg:58.56ms
step:1448/2330 train_time:84791ms step_avg:58.56ms
step:1449/2330 train_time:84848ms step_avg:58.56ms
step:1450/2330 train_time:84908ms step_avg:58.56ms
step:1451/2330 train_time:84965ms step_avg:58.56ms
step:1452/2330 train_time:85027ms step_avg:58.56ms
step:1453/2330 train_time:85083ms step_avg:58.56ms
step:1454/2330 train_time:85144ms step_avg:58.56ms
step:1455/2330 train_time:85201ms step_avg:58.56ms
step:1456/2330 train_time:85261ms step_avg:58.56ms
step:1457/2330 train_time:85319ms step_avg:58.56ms
step:1458/2330 train_time:85378ms step_avg:58.56ms
step:1459/2330 train_time:85436ms step_avg:58.56ms
step:1460/2330 train_time:85496ms step_avg:58.56ms
step:1461/2330 train_time:85553ms step_avg:58.56ms
step:1462/2330 train_time:85613ms step_avg:58.56ms
step:1463/2330 train_time:85670ms step_avg:58.56ms
step:1464/2330 train_time:85730ms step_avg:58.56ms
step:1465/2330 train_time:85788ms step_avg:58.56ms
step:1466/2330 train_time:85848ms step_avg:58.56ms
step:1467/2330 train_time:85905ms step_avg:58.56ms
step:1468/2330 train_time:85964ms step_avg:58.56ms
step:1469/2330 train_time:86021ms step_avg:58.56ms
step:1470/2330 train_time:86081ms step_avg:58.56ms
step:1471/2330 train_time:86138ms step_avg:58.56ms
step:1472/2330 train_time:86199ms step_avg:58.56ms
step:1473/2330 train_time:86256ms step_avg:58.56ms
step:1474/2330 train_time:86316ms step_avg:58.56ms
step:1475/2330 train_time:86374ms step_avg:58.56ms
step:1476/2330 train_time:86434ms step_avg:58.56ms
step:1477/2330 train_time:86492ms step_avg:58.56ms
step:1478/2330 train_time:86551ms step_avg:58.56ms
step:1479/2330 train_time:86608ms step_avg:58.56ms
step:1480/2330 train_time:86668ms step_avg:58.56ms
step:1481/2330 train_time:86725ms step_avg:58.56ms
step:1482/2330 train_time:86785ms step_avg:58.56ms
step:1483/2330 train_time:86841ms step_avg:58.56ms
step:1484/2330 train_time:86902ms step_avg:58.56ms
step:1485/2330 train_time:86959ms step_avg:58.56ms
step:1486/2330 train_time:87020ms step_avg:58.56ms
step:1487/2330 train_time:87077ms step_avg:58.56ms
step:1488/2330 train_time:87137ms step_avg:58.56ms
step:1489/2330 train_time:87195ms step_avg:58.56ms
step:1490/2330 train_time:87254ms step_avg:58.56ms
step:1491/2330 train_time:87312ms step_avg:58.56ms
step:1492/2330 train_time:87372ms step_avg:58.56ms
step:1493/2330 train_time:87429ms step_avg:58.56ms
step:1494/2330 train_time:87489ms step_avg:58.56ms
step:1495/2330 train_time:87546ms step_avg:58.56ms
step:1496/2330 train_time:87606ms step_avg:58.56ms
step:1497/2330 train_time:87663ms step_avg:58.56ms
step:1498/2330 train_time:87723ms step_avg:58.56ms
step:1499/2330 train_time:87780ms step_avg:58.56ms
step:1500/2330 train_time:87841ms step_avg:58.56ms
step:1500/2330 val_loss:3.9017 train_time:87921ms step_avg:58.61ms
step:1501/2330 train_time:87942ms step_avg:58.59ms
step:1502/2330 train_time:87962ms step_avg:58.56ms
step:1503/2330 train_time:88020ms step_avg:58.56ms
step:1504/2330 train_time:88083ms step_avg:58.57ms
step:1505/2330 train_time:88139ms step_avg:58.56ms
step:1506/2330 train_time:88201ms step_avg:58.57ms
step:1507/2330 train_time:88257ms step_avg:58.56ms
step:1508/2330 train_time:88316ms step_avg:58.57ms
step:1509/2330 train_time:88372ms step_avg:58.56ms
step:1510/2330 train_time:88432ms step_avg:58.56ms
step:1511/2330 train_time:88488ms step_avg:58.56ms
step:1512/2330 train_time:88548ms step_avg:58.56ms
step:1513/2330 train_time:88604ms step_avg:58.56ms
step:1514/2330 train_time:88663ms step_avg:58.56ms
step:1515/2330 train_time:88719ms step_avg:58.56ms
step:1516/2330 train_time:88779ms step_avg:58.56ms
step:1517/2330 train_time:88837ms step_avg:58.56ms
step:1518/2330 train_time:88897ms step_avg:58.56ms
step:1519/2330 train_time:88955ms step_avg:58.56ms
step:1520/2330 train_time:89018ms step_avg:58.56ms
step:1521/2330 train_time:89075ms step_avg:58.56ms
step:1522/2330 train_time:89138ms step_avg:58.57ms
step:1523/2330 train_time:89194ms step_avg:58.56ms
step:1524/2330 train_time:89255ms step_avg:58.57ms
step:1525/2330 train_time:89311ms step_avg:58.56ms
step:1526/2330 train_time:89371ms step_avg:58.57ms
step:1527/2330 train_time:89427ms step_avg:58.56ms
step:1528/2330 train_time:89488ms step_avg:58.57ms
step:1529/2330 train_time:89546ms step_avg:58.56ms
step:1530/2330 train_time:89604ms step_avg:58.56ms
step:1531/2330 train_time:89661ms step_avg:58.56ms
step:1532/2330 train_time:89721ms step_avg:58.56ms
step:1533/2330 train_time:89778ms step_avg:58.56ms
step:1534/2330 train_time:89838ms step_avg:58.56ms
step:1535/2330 train_time:89895ms step_avg:58.56ms
step:1536/2330 train_time:89957ms step_avg:58.57ms
step:1537/2330 train_time:90016ms step_avg:58.57ms
step:1538/2330 train_time:90077ms step_avg:58.57ms
step:1539/2330 train_time:90135ms step_avg:58.57ms
step:1540/2330 train_time:90196ms step_avg:58.57ms
step:1541/2330 train_time:90253ms step_avg:58.57ms
step:1542/2330 train_time:90314ms step_avg:58.57ms
step:1543/2330 train_time:90371ms step_avg:58.57ms
step:1544/2330 train_time:90431ms step_avg:58.57ms
step:1545/2330 train_time:90487ms step_avg:58.57ms
step:1546/2330 train_time:90549ms step_avg:58.57ms
step:1547/2330 train_time:90607ms step_avg:58.57ms
step:1548/2330 train_time:90667ms step_avg:58.57ms
step:1549/2330 train_time:90725ms step_avg:58.57ms
step:1550/2330 train_time:90785ms step_avg:58.57ms
step:1551/2330 train_time:90843ms step_avg:58.57ms
step:1552/2330 train_time:90904ms step_avg:58.57ms
step:1553/2330 train_time:90962ms step_avg:58.57ms
step:1554/2330 train_time:91023ms step_avg:58.57ms
step:1555/2330 train_time:91081ms step_avg:58.57ms
step:1556/2330 train_time:91142ms step_avg:58.57ms
step:1557/2330 train_time:91200ms step_avg:58.57ms
step:1558/2330 train_time:91261ms step_avg:58.58ms
step:1559/2330 train_time:91318ms step_avg:58.57ms
step:1560/2330 train_time:91379ms step_avg:58.58ms
step:1561/2330 train_time:91436ms step_avg:58.58ms
step:1562/2330 train_time:91497ms step_avg:58.58ms
step:1563/2330 train_time:91554ms step_avg:58.58ms
step:1564/2330 train_time:91614ms step_avg:58.58ms
step:1565/2330 train_time:91670ms step_avg:58.58ms
step:1566/2330 train_time:91731ms step_avg:58.58ms
step:1567/2330 train_time:91789ms step_avg:58.58ms
step:1568/2330 train_time:91850ms step_avg:58.58ms
step:1569/2330 train_time:91908ms step_avg:58.58ms
step:1570/2330 train_time:91969ms step_avg:58.58ms
step:1571/2330 train_time:92027ms step_avg:58.58ms
step:1572/2330 train_time:92089ms step_avg:58.58ms
step:1573/2330 train_time:92147ms step_avg:58.58ms
step:1574/2330 train_time:92209ms step_avg:58.58ms
step:1575/2330 train_time:92267ms step_avg:58.58ms
step:1576/2330 train_time:92328ms step_avg:58.58ms
step:1577/2330 train_time:92386ms step_avg:58.58ms
step:1578/2330 train_time:92446ms step_avg:58.58ms
step:1579/2330 train_time:92504ms step_avg:58.58ms
step:1580/2330 train_time:92564ms step_avg:58.58ms
step:1581/2330 train_time:92621ms step_avg:58.58ms
step:1582/2330 train_time:92681ms step_avg:58.58ms
step:1583/2330 train_time:92738ms step_avg:58.58ms
step:1584/2330 train_time:92800ms step_avg:58.59ms
step:1585/2330 train_time:92856ms step_avg:58.58ms
step:1586/2330 train_time:92916ms step_avg:58.59ms
step:1587/2330 train_time:92972ms step_avg:58.58ms
step:1588/2330 train_time:93035ms step_avg:58.59ms
step:1589/2330 train_time:93092ms step_avg:58.59ms
step:1590/2330 train_time:93154ms step_avg:58.59ms
step:1591/2330 train_time:93211ms step_avg:58.59ms
step:1592/2330 train_time:93272ms step_avg:58.59ms
step:1593/2330 train_time:93330ms step_avg:58.59ms
step:1594/2330 train_time:93391ms step_avg:58.59ms
step:1595/2330 train_time:93450ms step_avg:58.59ms
step:1596/2330 train_time:93510ms step_avg:58.59ms
step:1597/2330 train_time:93568ms step_avg:58.59ms
step:1598/2330 train_time:93628ms step_avg:58.59ms
step:1599/2330 train_time:93685ms step_avg:58.59ms
step:1600/2330 train_time:93747ms step_avg:58.59ms
step:1601/2330 train_time:93806ms step_avg:58.59ms
step:1602/2330 train_time:93866ms step_avg:58.59ms
step:1603/2330 train_time:93924ms step_avg:58.59ms
step:1604/2330 train_time:93984ms step_avg:58.59ms
step:1605/2330 train_time:94043ms step_avg:58.59ms
step:1606/2330 train_time:94103ms step_avg:58.59ms
step:1607/2330 train_time:94160ms step_avg:58.59ms
step:1608/2330 train_time:94221ms step_avg:58.60ms
step:1609/2330 train_time:94279ms step_avg:58.59ms
step:1610/2330 train_time:94339ms step_avg:58.60ms
step:1611/2330 train_time:94396ms step_avg:58.59ms
step:1612/2330 train_time:94457ms step_avg:58.60ms
step:1613/2330 train_time:94514ms step_avg:58.60ms
step:1614/2330 train_time:94575ms step_avg:58.60ms
step:1615/2330 train_time:94631ms step_avg:58.60ms
step:1616/2330 train_time:94693ms step_avg:58.60ms
step:1617/2330 train_time:94750ms step_avg:58.60ms
step:1618/2330 train_time:94812ms step_avg:58.60ms
step:1619/2330 train_time:94869ms step_avg:58.60ms
step:1620/2330 train_time:94930ms step_avg:58.60ms
step:1621/2330 train_time:94987ms step_avg:58.60ms
step:1622/2330 train_time:95050ms step_avg:58.60ms
step:1623/2330 train_time:95108ms step_avg:58.60ms
step:1624/2330 train_time:95169ms step_avg:58.60ms
step:1625/2330 train_time:95228ms step_avg:58.60ms
step:1626/2330 train_time:95289ms step_avg:58.60ms
step:1627/2330 train_time:95347ms step_avg:58.60ms
step:1628/2330 train_time:95407ms step_avg:58.60ms
step:1629/2330 train_time:95466ms step_avg:58.60ms
step:1630/2330 train_time:95526ms step_avg:58.61ms
step:1631/2330 train_time:95584ms step_avg:58.60ms
step:1632/2330 train_time:95644ms step_avg:58.61ms
step:1633/2330 train_time:95702ms step_avg:58.60ms
step:1634/2330 train_time:95762ms step_avg:58.61ms
step:1635/2330 train_time:95818ms step_avg:58.60ms
step:1636/2330 train_time:95880ms step_avg:58.61ms
step:1637/2330 train_time:95937ms step_avg:58.61ms
step:1638/2330 train_time:95998ms step_avg:58.61ms
step:1639/2330 train_time:96055ms step_avg:58.61ms
step:1640/2330 train_time:96117ms step_avg:58.61ms
step:1641/2330 train_time:96174ms step_avg:58.61ms
step:1642/2330 train_time:96235ms step_avg:58.61ms
step:1643/2330 train_time:96292ms step_avg:58.61ms
step:1644/2330 train_time:96353ms step_avg:58.61ms
step:1645/2330 train_time:96411ms step_avg:58.61ms
step:1646/2330 train_time:96471ms step_avg:58.61ms
step:1647/2330 train_time:96529ms step_avg:58.61ms
step:1648/2330 train_time:96589ms step_avg:58.61ms
step:1649/2330 train_time:96647ms step_avg:58.61ms
step:1650/2330 train_time:96708ms step_avg:58.61ms
step:1651/2330 train_time:96765ms step_avg:58.61ms
step:1652/2330 train_time:96827ms step_avg:58.61ms
step:1653/2330 train_time:96886ms step_avg:58.61ms
step:1654/2330 train_time:96947ms step_avg:58.61ms
step:1655/2330 train_time:97004ms step_avg:58.61ms
step:1656/2330 train_time:97066ms step_avg:58.61ms
step:1657/2330 train_time:97123ms step_avg:58.61ms
step:1658/2330 train_time:97184ms step_avg:58.62ms
step:1659/2330 train_time:97242ms step_avg:58.61ms
step:1660/2330 train_time:97302ms step_avg:58.62ms
step:1661/2330 train_time:97359ms step_avg:58.61ms
step:1662/2330 train_time:97420ms step_avg:58.62ms
step:1663/2330 train_time:97477ms step_avg:58.62ms
step:1664/2330 train_time:97538ms step_avg:58.62ms
step:1665/2330 train_time:97595ms step_avg:58.62ms
step:1666/2330 train_time:97656ms step_avg:58.62ms
step:1667/2330 train_time:97713ms step_avg:58.62ms
step:1668/2330 train_time:97773ms step_avg:58.62ms
step:1669/2330 train_time:97830ms step_avg:58.62ms
step:1670/2330 train_time:97892ms step_avg:58.62ms
step:1671/2330 train_time:97950ms step_avg:58.62ms
step:1672/2330 train_time:98011ms step_avg:58.62ms
step:1673/2330 train_time:98068ms step_avg:58.62ms
step:1674/2330 train_time:98129ms step_avg:58.62ms
step:1675/2330 train_time:98187ms step_avg:58.62ms
step:1676/2330 train_time:98248ms step_avg:58.62ms
step:1677/2330 train_time:98306ms step_avg:58.62ms
step:1678/2330 train_time:98367ms step_avg:58.62ms
step:1679/2330 train_time:98426ms step_avg:58.62ms
step:1680/2330 train_time:98486ms step_avg:58.62ms
step:1681/2330 train_time:98544ms step_avg:58.62ms
step:1682/2330 train_time:98604ms step_avg:58.62ms
step:1683/2330 train_time:98662ms step_avg:58.62ms
step:1684/2330 train_time:98722ms step_avg:58.62ms
step:1685/2330 train_time:98779ms step_avg:58.62ms
step:1686/2330 train_time:98840ms step_avg:58.62ms
step:1687/2330 train_time:98898ms step_avg:58.62ms
step:1688/2330 train_time:98958ms step_avg:58.62ms
step:1689/2330 train_time:99016ms step_avg:58.62ms
step:1690/2330 train_time:99076ms step_avg:58.62ms
step:1691/2330 train_time:99133ms step_avg:58.62ms
step:1692/2330 train_time:99194ms step_avg:58.63ms
step:1693/2330 train_time:99252ms step_avg:58.62ms
step:1694/2330 train_time:99312ms step_avg:58.63ms
step:1695/2330 train_time:99370ms step_avg:58.63ms
step:1696/2330 train_time:99431ms step_avg:58.63ms
step:1697/2330 train_time:99488ms step_avg:58.63ms
step:1698/2330 train_time:99549ms step_avg:58.63ms
step:1699/2330 train_time:99607ms step_avg:58.63ms
step:1700/2330 train_time:99668ms step_avg:58.63ms
step:1701/2330 train_time:99725ms step_avg:58.63ms
step:1702/2330 train_time:99787ms step_avg:58.63ms
step:1703/2330 train_time:99846ms step_avg:58.63ms
step:1704/2330 train_time:99906ms step_avg:58.63ms
step:1705/2330 train_time:99964ms step_avg:58.63ms
step:1706/2330 train_time:100023ms step_avg:58.63ms
step:1707/2330 train_time:100081ms step_avg:58.63ms
step:1708/2330 train_time:100142ms step_avg:58.63ms
step:1709/2330 train_time:100200ms step_avg:58.63ms
step:1710/2330 train_time:100259ms step_avg:58.63ms
step:1711/2330 train_time:100317ms step_avg:58.63ms
step:1712/2330 train_time:100377ms step_avg:58.63ms
step:1713/2330 train_time:100434ms step_avg:58.63ms
step:1714/2330 train_time:100497ms step_avg:58.63ms
step:1715/2330 train_time:100554ms step_avg:58.63ms
step:1716/2330 train_time:100616ms step_avg:58.63ms
step:1717/2330 train_time:100672ms step_avg:58.63ms
step:1718/2330 train_time:100734ms step_avg:58.63ms
step:1719/2330 train_time:100791ms step_avg:58.63ms
step:1720/2330 train_time:100853ms step_avg:58.64ms
step:1721/2330 train_time:100910ms step_avg:58.63ms
step:1722/2330 train_time:100971ms step_avg:58.64ms
step:1723/2330 train_time:101029ms step_avg:58.64ms
step:1724/2330 train_time:101089ms step_avg:58.64ms
step:1725/2330 train_time:101147ms step_avg:58.64ms
step:1726/2330 train_time:101208ms step_avg:58.64ms
step:1727/2330 train_time:101267ms step_avg:58.64ms
step:1728/2330 train_time:101327ms step_avg:58.64ms
step:1729/2330 train_time:101385ms step_avg:58.64ms
step:1730/2330 train_time:101446ms step_avg:58.64ms
step:1731/2330 train_time:101504ms step_avg:58.64ms
step:1732/2330 train_time:101565ms step_avg:58.64ms
step:1733/2330 train_time:101623ms step_avg:58.64ms
step:1734/2330 train_time:101683ms step_avg:58.64ms
step:1735/2330 train_time:101741ms step_avg:58.64ms
step:1736/2330 train_time:101802ms step_avg:58.64ms
step:1737/2330 train_time:101859ms step_avg:58.64ms
step:1738/2330 train_time:101920ms step_avg:58.64ms
step:1739/2330 train_time:101976ms step_avg:58.64ms
step:1740/2330 train_time:102037ms step_avg:58.64ms
step:1741/2330 train_time:102094ms step_avg:58.64ms
step:1742/2330 train_time:102156ms step_avg:58.64ms
step:1743/2330 train_time:102213ms step_avg:58.64ms
step:1744/2330 train_time:102275ms step_avg:58.64ms
step:1745/2330 train_time:102332ms step_avg:58.64ms
step:1746/2330 train_time:102393ms step_avg:58.64ms
step:1747/2330 train_time:102451ms step_avg:58.64ms
step:1748/2330 train_time:102512ms step_avg:58.65ms
step:1749/2330 train_time:102569ms step_avg:58.64ms
step:1750/2330 train_time:102630ms step_avg:58.65ms
step:1750/2330 val_loss:3.8187 train_time:102712ms step_avg:58.69ms
step:1751/2330 train_time:102731ms step_avg:58.67ms
step:1752/2330 train_time:102752ms step_avg:58.65ms
step:1753/2330 train_time:102813ms step_avg:58.65ms
step:1754/2330 train_time:102877ms step_avg:58.65ms
step:1755/2330 train_time:102936ms step_avg:58.65ms
step:1756/2330 train_time:102996ms step_avg:58.65ms
step:1757/2330 train_time:103053ms step_avg:58.65ms
step:1758/2330 train_time:103112ms step_avg:58.65ms
step:1759/2330 train_time:103168ms step_avg:58.65ms
step:1760/2330 train_time:103229ms step_avg:58.65ms
step:1761/2330 train_time:103287ms step_avg:58.65ms
step:1762/2330 train_time:103346ms step_avg:58.65ms
step:1763/2330 train_time:103403ms step_avg:58.65ms
step:1764/2330 train_time:103462ms step_avg:58.65ms
step:1765/2330 train_time:103519ms step_avg:58.65ms
step:1766/2330 train_time:103579ms step_avg:58.65ms
step:1767/2330 train_time:103636ms step_avg:58.65ms
step:1768/2330 train_time:103699ms step_avg:58.65ms
step:1769/2330 train_time:103757ms step_avg:58.65ms
step:1770/2330 train_time:103820ms step_avg:58.66ms
step:1771/2330 train_time:103878ms step_avg:58.65ms
step:1772/2330 train_time:103941ms step_avg:58.66ms
step:1773/2330 train_time:103997ms step_avg:58.66ms
step:1774/2330 train_time:104060ms step_avg:58.66ms
step:1775/2330 train_time:104116ms step_avg:58.66ms
step:1776/2330 train_time:104178ms step_avg:58.66ms
step:1777/2330 train_time:104235ms step_avg:58.66ms
step:1778/2330 train_time:104296ms step_avg:58.66ms
step:1779/2330 train_time:104354ms step_avg:58.66ms
step:1780/2330 train_time:104413ms step_avg:58.66ms
step:1781/2330 train_time:104470ms step_avg:58.66ms
step:1782/2330 train_time:104530ms step_avg:58.66ms
step:1783/2330 train_time:104587ms step_avg:58.66ms
step:1784/2330 train_time:104647ms step_avg:58.66ms
step:1785/2330 train_time:104705ms step_avg:58.66ms
step:1786/2330 train_time:104767ms step_avg:58.66ms
step:1787/2330 train_time:104826ms step_avg:58.66ms
step:1788/2330 train_time:104888ms step_avg:58.66ms
step:1789/2330 train_time:104946ms step_avg:58.66ms
step:1790/2330 train_time:105007ms step_avg:58.66ms
step:1791/2330 train_time:105067ms step_avg:58.66ms
step:1792/2330 train_time:105127ms step_avg:58.66ms
step:1793/2330 train_time:105185ms step_avg:58.66ms
step:1794/2330 train_time:105245ms step_avg:58.67ms
step:1795/2330 train_time:105302ms step_avg:58.66ms
step:1796/2330 train_time:105363ms step_avg:58.67ms
step:1797/2330 train_time:105420ms step_avg:58.66ms
step:1798/2330 train_time:105481ms step_avg:58.67ms
step:1799/2330 train_time:105537ms step_avg:58.66ms
step:1800/2330 train_time:105598ms step_avg:58.67ms
step:1801/2330 train_time:105655ms step_avg:58.66ms
step:1802/2330 train_time:105716ms step_avg:58.67ms
step:1803/2330 train_time:105774ms step_avg:58.67ms
step:1804/2330 train_time:105837ms step_avg:58.67ms
step:1805/2330 train_time:105894ms step_avg:58.67ms
step:1806/2330 train_time:105957ms step_avg:58.67ms
step:1807/2330 train_time:106015ms step_avg:58.67ms
step:1808/2330 train_time:106077ms step_avg:58.67ms
step:1809/2330 train_time:106134ms step_avg:58.67ms
step:1810/2330 train_time:106196ms step_avg:58.67ms
step:1811/2330 train_time:106253ms step_avg:58.67ms
step:1812/2330 train_time:106315ms step_avg:58.67ms
step:1813/2330 train_time:106372ms step_avg:58.67ms
step:1814/2330 train_time:106433ms step_avg:58.67ms
step:1815/2330 train_time:106490ms step_avg:58.67ms
step:1816/2330 train_time:106550ms step_avg:58.67ms
step:1817/2330 train_time:106608ms step_avg:58.67ms
step:1818/2330 train_time:106669ms step_avg:58.67ms
step:1819/2330 train_time:106727ms step_avg:58.67ms
step:1820/2330 train_time:106788ms step_avg:58.67ms
step:1821/2330 train_time:106846ms step_avg:58.67ms
step:1822/2330 train_time:106907ms step_avg:58.68ms
step:1823/2330 train_time:106967ms step_avg:58.68ms
step:1824/2330 train_time:107027ms step_avg:58.68ms
step:1825/2330 train_time:107086ms step_avg:58.68ms
step:1826/2330 train_time:107146ms step_avg:58.68ms
step:1827/2330 train_time:107204ms step_avg:58.68ms
step:1828/2330 train_time:107264ms step_avg:58.68ms
step:1829/2330 train_time:107321ms step_avg:58.68ms
step:1830/2330 train_time:107382ms step_avg:58.68ms
step:1831/2330 train_time:107438ms step_avg:58.68ms
step:1832/2330 train_time:107501ms step_avg:58.68ms
step:1833/2330 train_time:107558ms step_avg:58.68ms
step:1834/2330 train_time:107618ms step_avg:58.68ms
step:1835/2330 train_time:107675ms step_avg:58.68ms
step:1836/2330 train_time:107736ms step_avg:58.68ms
step:1837/2330 train_time:107793ms step_avg:58.68ms
step:1838/2330 train_time:107855ms step_avg:58.68ms
step:1839/2330 train_time:107912ms step_avg:58.68ms
step:1840/2330 train_time:107975ms step_avg:58.68ms
step:1841/2330 train_time:108032ms step_avg:58.68ms
step:1842/2330 train_time:108093ms step_avg:58.68ms
step:1843/2330 train_time:108150ms step_avg:58.68ms
step:1844/2330 train_time:108212ms step_avg:58.68ms
step:1845/2330 train_time:108270ms step_avg:58.68ms
step:1846/2330 train_time:108330ms step_avg:58.68ms
step:1847/2330 train_time:108387ms step_avg:58.68ms
step:1848/2330 train_time:108449ms step_avg:58.68ms
step:1849/2330 train_time:108506ms step_avg:58.68ms
step:1850/2330 train_time:108567ms step_avg:58.68ms
step:1851/2330 train_time:108626ms step_avg:58.69ms
step:1852/2330 train_time:108686ms step_avg:58.69ms
step:1853/2330 train_time:108744ms step_avg:58.69ms
step:1854/2330 train_time:108804ms step_avg:58.69ms
step:1855/2330 train_time:108862ms step_avg:58.69ms
step:1856/2330 train_time:108924ms step_avg:58.69ms
step:1857/2330 train_time:108982ms step_avg:58.69ms
step:1858/2330 train_time:109043ms step_avg:58.69ms
step:1859/2330 train_time:109100ms step_avg:58.69ms
step:1860/2330 train_time:109162ms step_avg:58.69ms
step:1861/2330 train_time:109220ms step_avg:58.69ms
step:1862/2330 train_time:109282ms step_avg:58.69ms
step:1863/2330 train_time:109339ms step_avg:58.69ms
step:1864/2330 train_time:109400ms step_avg:58.69ms
step:1865/2330 train_time:109457ms step_avg:58.69ms
step:1866/2330 train_time:109518ms step_avg:58.69ms
step:1867/2330 train_time:109575ms step_avg:58.69ms
step:1868/2330 train_time:109638ms step_avg:58.69ms
step:1869/2330 train_time:109694ms step_avg:58.69ms
step:1870/2330 train_time:109756ms step_avg:58.69ms
step:1871/2330 train_time:109813ms step_avg:58.69ms
step:1872/2330 train_time:109874ms step_avg:58.69ms
step:1873/2330 train_time:109932ms step_avg:58.69ms
step:1874/2330 train_time:109993ms step_avg:58.69ms
step:1875/2330 train_time:110051ms step_avg:58.69ms
step:1876/2330 train_time:110112ms step_avg:58.70ms
step:1877/2330 train_time:110170ms step_avg:58.69ms
step:1878/2330 train_time:110230ms step_avg:58.70ms
step:1879/2330 train_time:110289ms step_avg:58.70ms
step:1880/2330 train_time:110350ms step_avg:58.70ms
step:1881/2330 train_time:110407ms step_avg:58.70ms
step:1882/2330 train_time:110467ms step_avg:58.70ms
step:1883/2330 train_time:110526ms step_avg:58.70ms
step:1884/2330 train_time:110586ms step_avg:58.70ms
step:1885/2330 train_time:110645ms step_avg:58.70ms
step:1886/2330 train_time:110705ms step_avg:58.70ms
step:1887/2330 train_time:110764ms step_avg:58.70ms
step:1888/2330 train_time:110824ms step_avg:58.70ms
step:1889/2330 train_time:110881ms step_avg:58.70ms
step:1890/2330 train_time:110943ms step_avg:58.70ms
step:1891/2330 train_time:111000ms step_avg:58.70ms
step:1892/2330 train_time:111062ms step_avg:58.70ms
step:1893/2330 train_time:111119ms step_avg:58.70ms
step:1894/2330 train_time:111180ms step_avg:58.70ms
step:1895/2330 train_time:111237ms step_avg:58.70ms
step:1896/2330 train_time:111299ms step_avg:58.70ms
step:1897/2330 train_time:111356ms step_avg:58.70ms
step:1898/2330 train_time:111417ms step_avg:58.70ms
step:1899/2330 train_time:111474ms step_avg:58.70ms
step:1900/2330 train_time:111536ms step_avg:58.70ms
step:1901/2330 train_time:111593ms step_avg:58.70ms
step:1902/2330 train_time:111655ms step_avg:58.70ms
step:1903/2330 train_time:111712ms step_avg:58.70ms
step:1904/2330 train_time:111772ms step_avg:58.70ms
step:1905/2330 train_time:111830ms step_avg:58.70ms
step:1906/2330 train_time:111891ms step_avg:58.70ms
step:1907/2330 train_time:111949ms step_avg:58.70ms
step:1908/2330 train_time:112009ms step_avg:58.71ms
step:1909/2330 train_time:112068ms step_avg:58.71ms
step:1910/2330 train_time:112129ms step_avg:58.71ms
step:1911/2330 train_time:112186ms step_avg:58.71ms
step:1912/2330 train_time:112247ms step_avg:58.71ms
step:1913/2330 train_time:112305ms step_avg:58.71ms
step:1914/2330 train_time:112366ms step_avg:58.71ms
step:1915/2330 train_time:112424ms step_avg:58.71ms
step:1916/2330 train_time:112485ms step_avg:58.71ms
step:1917/2330 train_time:112543ms step_avg:58.71ms
step:1918/2330 train_time:112604ms step_avg:58.71ms
step:1919/2330 train_time:112662ms step_avg:58.71ms
step:1920/2330 train_time:112722ms step_avg:58.71ms
step:1921/2330 train_time:112780ms step_avg:58.71ms
step:1922/2330 train_time:112840ms step_avg:58.71ms
step:1923/2330 train_time:112897ms step_avg:58.71ms
step:1924/2330 train_time:112960ms step_avg:58.71ms
step:1925/2330 train_time:113017ms step_avg:58.71ms
step:1926/2330 train_time:113079ms step_avg:58.71ms
step:1927/2330 train_time:113136ms step_avg:58.71ms
step:1928/2330 train_time:113198ms step_avg:58.71ms
step:1929/2330 train_time:113255ms step_avg:58.71ms
step:1930/2330 train_time:113316ms step_avg:58.71ms
step:1931/2330 train_time:113373ms step_avg:58.71ms
step:1932/2330 train_time:113434ms step_avg:58.71ms
step:1933/2330 train_time:113490ms step_avg:58.71ms
step:1934/2330 train_time:113553ms step_avg:58.71ms
step:1935/2330 train_time:113609ms step_avg:58.71ms
step:1936/2330 train_time:113671ms step_avg:58.71ms
step:1937/2330 train_time:113729ms step_avg:58.71ms
step:1938/2330 train_time:113789ms step_avg:58.71ms
step:1939/2330 train_time:113847ms step_avg:58.71ms
step:1940/2330 train_time:113909ms step_avg:58.72ms
step:1941/2330 train_time:113967ms step_avg:58.72ms
step:1942/2330 train_time:114029ms step_avg:58.72ms
step:1943/2330 train_time:114087ms step_avg:58.72ms
step:1944/2330 train_time:114147ms step_avg:58.72ms
step:1945/2330 train_time:114205ms step_avg:58.72ms
step:1946/2330 train_time:114265ms step_avg:58.72ms
step:1947/2330 train_time:114323ms step_avg:58.72ms
step:1948/2330 train_time:114384ms step_avg:58.72ms
step:1949/2330 train_time:114442ms step_avg:58.72ms
step:1950/2330 train_time:114502ms step_avg:58.72ms
step:1951/2330 train_time:114560ms step_avg:58.72ms
step:1952/2330 train_time:114620ms step_avg:58.72ms
step:1953/2330 train_time:114677ms step_avg:58.72ms
step:1954/2330 train_time:114739ms step_avg:58.72ms
step:1955/2330 train_time:114795ms step_avg:58.72ms
step:1956/2330 train_time:114859ms step_avg:58.72ms
step:1957/2330 train_time:114916ms step_avg:58.72ms
step:1958/2330 train_time:114978ms step_avg:58.72ms
step:1959/2330 train_time:115036ms step_avg:58.72ms
step:1960/2330 train_time:115096ms step_avg:58.72ms
step:1961/2330 train_time:115153ms step_avg:58.72ms
step:1962/2330 train_time:115214ms step_avg:58.72ms
step:1963/2330 train_time:115272ms step_avg:58.72ms
step:1964/2330 train_time:115332ms step_avg:58.72ms
step:1965/2330 train_time:115389ms step_avg:58.72ms
step:1966/2330 train_time:115450ms step_avg:58.72ms
step:1967/2330 train_time:115508ms step_avg:58.72ms
step:1968/2330 train_time:115569ms step_avg:58.72ms
step:1969/2330 train_time:115628ms step_avg:58.72ms
step:1970/2330 train_time:115688ms step_avg:58.73ms
step:1971/2330 train_time:115747ms step_avg:58.72ms
step:1972/2330 train_time:115808ms step_avg:58.73ms
step:1973/2330 train_time:115866ms step_avg:58.73ms
step:1974/2330 train_time:115926ms step_avg:58.73ms
step:1975/2330 train_time:115985ms step_avg:58.73ms
step:1976/2330 train_time:116045ms step_avg:58.73ms
step:1977/2330 train_time:116103ms step_avg:58.73ms
step:1978/2330 train_time:116163ms step_avg:58.73ms
step:1979/2330 train_time:116221ms step_avg:58.73ms
step:1980/2330 train_time:116282ms step_avg:58.73ms
step:1981/2330 train_time:116339ms step_avg:58.73ms
step:1982/2330 train_time:116400ms step_avg:58.73ms
step:1983/2330 train_time:116458ms step_avg:58.73ms
step:1984/2330 train_time:116519ms step_avg:58.73ms
step:1985/2330 train_time:116576ms step_avg:58.73ms
step:1986/2330 train_time:116637ms step_avg:58.73ms
step:1987/2330 train_time:116694ms step_avg:58.73ms
step:1988/2330 train_time:116756ms step_avg:58.73ms
step:1989/2330 train_time:116814ms step_avg:58.73ms
step:1990/2330 train_time:116875ms step_avg:58.73ms
step:1991/2330 train_time:116932ms step_avg:58.73ms
step:1992/2330 train_time:116994ms step_avg:58.73ms
step:1993/2330 train_time:117051ms step_avg:58.73ms
step:1994/2330 train_time:117112ms step_avg:58.73ms
step:1995/2330 train_time:117170ms step_avg:58.73ms
step:1996/2330 train_time:117232ms step_avg:58.73ms
step:1997/2330 train_time:117290ms step_avg:58.73ms
step:1998/2330 train_time:117350ms step_avg:58.73ms
step:1999/2330 train_time:117408ms step_avg:58.73ms
step:2000/2330 train_time:117469ms step_avg:58.73ms
step:2000/2330 val_loss:3.7541 train_time:117551ms step_avg:58.78ms
step:2001/2330 train_time:117571ms step_avg:58.76ms
step:2002/2330 train_time:117593ms step_avg:58.74ms
step:2003/2330 train_time:117653ms step_avg:58.74ms
step:2004/2330 train_time:117714ms step_avg:58.74ms
step:2005/2330 train_time:117772ms step_avg:58.74ms
step:2006/2330 train_time:117832ms step_avg:58.74ms
step:2007/2330 train_time:117889ms step_avg:58.74ms
step:2008/2330 train_time:117949ms step_avg:58.74ms
step:2009/2330 train_time:118006ms step_avg:58.74ms
step:2010/2330 train_time:118067ms step_avg:58.74ms
step:2011/2330 train_time:118124ms step_avg:58.74ms
step:2012/2330 train_time:118184ms step_avg:58.74ms
step:2013/2330 train_time:118240ms step_avg:58.74ms
step:2014/2330 train_time:118301ms step_avg:58.74ms
step:2015/2330 train_time:118357ms step_avg:58.74ms
step:2016/2330 train_time:118418ms step_avg:58.74ms
step:2017/2330 train_time:118475ms step_avg:58.74ms
step:2018/2330 train_time:118538ms step_avg:58.74ms
step:2019/2330 train_time:118597ms step_avg:58.74ms
step:2020/2330 train_time:118661ms step_avg:58.74ms
step:2021/2330 train_time:118718ms step_avg:58.74ms
step:2022/2330 train_time:118782ms step_avg:58.74ms
step:2023/2330 train_time:118839ms step_avg:58.74ms
step:2024/2330 train_time:118900ms step_avg:58.75ms
step:2025/2330 train_time:118956ms step_avg:58.74ms
step:2026/2330 train_time:119019ms step_avg:58.75ms
step:2027/2330 train_time:119076ms step_avg:58.74ms
step:2028/2330 train_time:119135ms step_avg:58.75ms
step:2029/2330 train_time:119192ms step_avg:58.74ms
step:2030/2330 train_time:119252ms step_avg:58.74ms
step:2031/2330 train_time:119308ms step_avg:58.74ms
step:2032/2330 train_time:119369ms step_avg:58.74ms
step:2033/2330 train_time:119426ms step_avg:58.74ms
step:2034/2330 train_time:119487ms step_avg:58.74ms
step:2035/2330 train_time:119545ms step_avg:58.74ms
step:2036/2330 train_time:119607ms step_avg:58.75ms
step:2037/2330 train_time:119666ms step_avg:58.75ms
step:2038/2330 train_time:119727ms step_avg:58.75ms
step:2039/2330 train_time:119785ms step_avg:58.75ms
step:2040/2330 train_time:119846ms step_avg:58.75ms
step:2041/2330 train_time:119904ms step_avg:58.75ms
step:2042/2330 train_time:119964ms step_avg:58.75ms
step:2043/2330 train_time:120022ms step_avg:58.75ms
step:2044/2330 train_time:120083ms step_avg:58.75ms
step:2045/2330 train_time:120140ms step_avg:58.75ms
step:2046/2330 train_time:120200ms step_avg:58.75ms
step:2047/2330 train_time:120257ms step_avg:58.75ms
step:2048/2330 train_time:120317ms step_avg:58.75ms
step:2049/2330 train_time:120374ms step_avg:58.75ms
step:2050/2330 train_time:120436ms step_avg:58.75ms
step:2051/2330 train_time:120493ms step_avg:58.75ms
step:2052/2330 train_time:120556ms step_avg:58.75ms
step:2053/2330 train_time:120613ms step_avg:58.75ms
step:2054/2330 train_time:120676ms step_avg:58.75ms
step:2055/2330 train_time:120733ms step_avg:58.75ms
step:2056/2330 train_time:120795ms step_avg:58.75ms
step:2057/2330 train_time:120853ms step_avg:58.75ms
step:2058/2330 train_time:120915ms step_avg:58.75ms
step:2059/2330 train_time:120972ms step_avg:58.75ms
step:2060/2330 train_time:121034ms step_avg:58.75ms
step:2061/2330 train_time:121090ms step_avg:58.75ms
step:2062/2330 train_time:121151ms step_avg:58.75ms
step:2063/2330 train_time:121209ms step_avg:58.75ms
step:2064/2330 train_time:121269ms step_avg:58.75ms
step:2065/2330 train_time:121327ms step_avg:58.75ms
step:2066/2330 train_time:121387ms step_avg:58.75ms
step:2067/2330 train_time:121445ms step_avg:58.75ms
step:2068/2330 train_time:121505ms step_avg:58.75ms
step:2069/2330 train_time:121563ms step_avg:58.75ms
step:2070/2330 train_time:121623ms step_avg:58.76ms
step:2071/2330 train_time:121680ms step_avg:58.75ms
step:2072/2330 train_time:121743ms step_avg:58.76ms
step:2073/2330 train_time:121801ms step_avg:58.76ms
step:2074/2330 train_time:121861ms step_avg:58.76ms
step:2075/2330 train_time:121920ms step_avg:58.76ms
step:2076/2330 train_time:121980ms step_avg:58.76ms
step:2077/2330 train_time:122037ms step_avg:58.76ms
step:2078/2330 train_time:122097ms step_avg:58.76ms
step:2079/2330 train_time:122154ms step_avg:58.76ms
step:2080/2330 train_time:122216ms step_avg:58.76ms
step:2081/2330 train_time:122272ms step_avg:58.76ms
step:2082/2330 train_time:122334ms step_avg:58.76ms
step:2083/2330 train_time:122391ms step_avg:58.76ms
step:2084/2330 train_time:122453ms step_avg:58.76ms
step:2085/2330 train_time:122509ms step_avg:58.76ms
step:2086/2330 train_time:122572ms step_avg:58.76ms
step:2087/2330 train_time:122630ms step_avg:58.76ms
step:2088/2330 train_time:122692ms step_avg:58.76ms
step:2089/2330 train_time:122749ms step_avg:58.76ms
step:2090/2330 train_time:122810ms step_avg:58.76ms
step:2091/2330 train_time:122867ms step_avg:58.76ms
step:2092/2330 train_time:122929ms step_avg:58.76ms
step:2093/2330 train_time:122988ms step_avg:58.76ms
step:2094/2330 train_time:123049ms step_avg:58.76ms
step:2095/2330 train_time:123108ms step_avg:58.76ms
step:2096/2330 train_time:123167ms step_avg:58.76ms
step:2097/2330 train_time:123225ms step_avg:58.76ms
step:2098/2330 train_time:123286ms step_avg:58.76ms
step:2099/2330 train_time:123344ms step_avg:58.76ms
step:2100/2330 train_time:123404ms step_avg:58.76ms
step:2101/2330 train_time:123462ms step_avg:58.76ms
step:2102/2330 train_time:123522ms step_avg:58.76ms
step:2103/2330 train_time:123580ms step_avg:58.76ms
step:2104/2330 train_time:123640ms step_avg:58.76ms
step:2105/2330 train_time:123698ms step_avg:58.76ms
step:2106/2330 train_time:123760ms step_avg:58.77ms
step:2107/2330 train_time:123816ms step_avg:58.76ms
step:2108/2330 train_time:123878ms step_avg:58.77ms
step:2109/2330 train_time:123936ms step_avg:58.77ms
step:2110/2330 train_time:123997ms step_avg:58.77ms
step:2111/2330 train_time:124055ms step_avg:58.77ms
step:2112/2330 train_time:124116ms step_avg:58.77ms
step:2113/2330 train_time:124173ms step_avg:58.77ms
step:2114/2330 train_time:124236ms step_avg:58.77ms
step:2115/2330 train_time:124293ms step_avg:58.77ms
step:2116/2330 train_time:124354ms step_avg:58.77ms
step:2117/2330 train_time:124411ms step_avg:58.77ms
step:2118/2330 train_time:124472ms step_avg:58.77ms
step:2119/2330 train_time:124529ms step_avg:58.77ms
step:2120/2330 train_time:124590ms step_avg:58.77ms
step:2121/2330 train_time:124647ms step_avg:58.77ms
step:2122/2330 train_time:124708ms step_avg:58.77ms
step:2123/2330 train_time:124765ms step_avg:58.77ms
step:2124/2330 train_time:124826ms step_avg:58.77ms
step:2125/2330 train_time:124884ms step_avg:58.77ms
step:2126/2330 train_time:124945ms step_avg:58.77ms
step:2127/2330 train_time:125003ms step_avg:58.77ms
step:2128/2330 train_time:125064ms step_avg:58.77ms
step:2129/2330 train_time:125121ms step_avg:58.77ms
step:2130/2330 train_time:125182ms step_avg:58.77ms
step:2131/2330 train_time:125239ms step_avg:58.77ms
step:2132/2330 train_time:125302ms step_avg:58.77ms
step:2133/2330 train_time:125359ms step_avg:58.77ms
step:2134/2330 train_time:125420ms step_avg:58.77ms
step:2135/2330 train_time:125477ms step_avg:58.77ms
step:2136/2330 train_time:125539ms step_avg:58.77ms
step:2137/2330 train_time:125596ms step_avg:58.77ms
step:2138/2330 train_time:125657ms step_avg:58.77ms
step:2139/2330 train_time:125713ms step_avg:58.77ms
step:2140/2330 train_time:125776ms step_avg:58.77ms
step:2141/2330 train_time:125833ms step_avg:58.77ms
step:2142/2330 train_time:125894ms step_avg:58.77ms
step:2143/2330 train_time:125951ms step_avg:58.77ms
step:2144/2330 train_time:126012ms step_avg:58.77ms
step:2145/2330 train_time:126069ms step_avg:58.77ms
step:2146/2330 train_time:126131ms step_avg:58.77ms
step:2147/2330 train_time:126189ms step_avg:58.77ms
step:2148/2330 train_time:126250ms step_avg:58.78ms
step:2149/2330 train_time:126308ms step_avg:58.78ms
step:2150/2330 train_time:126368ms step_avg:58.78ms
step:2151/2330 train_time:126426ms step_avg:58.78ms
step:2152/2330 train_time:126487ms step_avg:58.78ms
step:2153/2330 train_time:126546ms step_avg:58.78ms
step:2154/2330 train_time:126606ms step_avg:58.78ms
step:2155/2330 train_time:126664ms step_avg:58.78ms
step:2156/2330 train_time:126724ms step_avg:58.78ms
step:2157/2330 train_time:126782ms step_avg:58.78ms
step:2158/2330 train_time:126843ms step_avg:58.78ms
step:2159/2330 train_time:126900ms step_avg:58.78ms
step:2160/2330 train_time:126961ms step_avg:58.78ms
step:2161/2330 train_time:127019ms step_avg:58.78ms
step:2162/2330 train_time:127080ms step_avg:58.78ms
step:2163/2330 train_time:127137ms step_avg:58.78ms
step:2164/2330 train_time:127199ms step_avg:58.78ms
step:2165/2330 train_time:127255ms step_avg:58.78ms
step:2166/2330 train_time:127318ms step_avg:58.78ms
step:2167/2330 train_time:127375ms step_avg:58.78ms
step:2168/2330 train_time:127437ms step_avg:58.78ms
step:2169/2330 train_time:127494ms step_avg:58.78ms
step:2170/2330 train_time:127555ms step_avg:58.78ms
step:2171/2330 train_time:127612ms step_avg:58.78ms
step:2172/2330 train_time:127674ms step_avg:58.78ms
step:2173/2330 train_time:127731ms step_avg:58.78ms
step:2174/2330 train_time:127792ms step_avg:58.78ms
step:2175/2330 train_time:127849ms step_avg:58.78ms
step:2176/2330 train_time:127910ms step_avg:58.78ms
step:2177/2330 train_time:127967ms step_avg:58.78ms
step:2178/2330 train_time:128028ms step_avg:58.78ms
step:2179/2330 train_time:128087ms step_avg:58.78ms
step:2180/2330 train_time:128148ms step_avg:58.78ms
step:2181/2330 train_time:128207ms step_avg:58.78ms
step:2182/2330 train_time:128267ms step_avg:58.78ms
step:2183/2330 train_time:128324ms step_avg:58.78ms
step:2184/2330 train_time:128386ms step_avg:58.78ms
step:2185/2330 train_time:128443ms step_avg:58.78ms
step:2186/2330 train_time:128505ms step_avg:58.79ms
step:2187/2330 train_time:128563ms step_avg:58.78ms
step:2188/2330 train_time:128624ms step_avg:58.79ms
step:2189/2330 train_time:128681ms step_avg:58.79ms
step:2190/2330 train_time:128741ms step_avg:58.79ms
step:2191/2330 train_time:128799ms step_avg:58.79ms
step:2192/2330 train_time:128861ms step_avg:58.79ms
step:2193/2330 train_time:128917ms step_avg:58.79ms
step:2194/2330 train_time:128979ms step_avg:58.79ms
step:2195/2330 train_time:129036ms step_avg:58.79ms
step:2196/2330 train_time:129099ms step_avg:58.79ms
step:2197/2330 train_time:129155ms step_avg:58.79ms
step:2198/2330 train_time:129218ms step_avg:58.79ms
step:2199/2330 train_time:129275ms step_avg:58.79ms
step:2200/2330 train_time:129337ms step_avg:58.79ms
step:2201/2330 train_time:129394ms step_avg:58.79ms
step:2202/2330 train_time:129456ms step_avg:58.79ms
step:2203/2330 train_time:129512ms step_avg:58.79ms
step:2204/2330 train_time:129573ms step_avg:58.79ms
step:2205/2330 train_time:129630ms step_avg:58.79ms
step:2206/2330 train_time:129691ms step_avg:58.79ms
step:2207/2330 train_time:129747ms step_avg:58.79ms
step:2208/2330 train_time:129809ms step_avg:58.79ms
step:2209/2330 train_time:129867ms step_avg:58.79ms
step:2210/2330 train_time:129928ms step_avg:58.79ms
step:2211/2330 train_time:129986ms step_avg:58.79ms
step:2212/2330 train_time:130048ms step_avg:58.79ms
step:2213/2330 train_time:130106ms step_avg:58.79ms
step:2214/2330 train_time:130167ms step_avg:58.79ms
step:2215/2330 train_time:130225ms step_avg:58.79ms
step:2216/2330 train_time:130286ms step_avg:58.79ms
step:2217/2330 train_time:130344ms step_avg:58.79ms
step:2218/2330 train_time:130405ms step_avg:58.79ms
step:2219/2330 train_time:130462ms step_avg:58.79ms
step:2220/2330 train_time:130523ms step_avg:58.79ms
step:2221/2330 train_time:130580ms step_avg:58.79ms
step:2222/2330 train_time:130640ms step_avg:58.79ms
step:2223/2330 train_time:130698ms step_avg:58.79ms
step:2224/2330 train_time:130759ms step_avg:58.79ms
step:2225/2330 train_time:130816ms step_avg:58.79ms
step:2226/2330 train_time:130878ms step_avg:58.80ms
step:2227/2330 train_time:130935ms step_avg:58.79ms
step:2228/2330 train_time:130997ms step_avg:58.80ms
step:2229/2330 train_time:131054ms step_avg:58.79ms
step:2230/2330 train_time:131117ms step_avg:58.80ms
step:2231/2330 train_time:131173ms step_avg:58.80ms
step:2232/2330 train_time:131235ms step_avg:58.80ms
step:2233/2330 train_time:131292ms step_avg:58.80ms
step:2234/2330 train_time:131354ms step_avg:58.80ms
step:2235/2330 train_time:131410ms step_avg:58.80ms
step:2236/2330 train_time:131473ms step_avg:58.80ms
step:2237/2330 train_time:131529ms step_avg:58.80ms
step:2238/2330 train_time:131591ms step_avg:58.80ms
step:2239/2330 train_time:131647ms step_avg:58.80ms
step:2240/2330 train_time:131708ms step_avg:58.80ms
step:2241/2330 train_time:131766ms step_avg:58.80ms
step:2242/2330 train_time:131827ms step_avg:58.80ms
step:2243/2330 train_time:131885ms step_avg:58.80ms
step:2244/2330 train_time:131946ms step_avg:58.80ms
step:2245/2330 train_time:132004ms step_avg:58.80ms
step:2246/2330 train_time:132065ms step_avg:58.80ms
step:2247/2330 train_time:132122ms step_avg:58.80ms
step:2248/2330 train_time:132185ms step_avg:58.80ms
step:2249/2330 train_time:132243ms step_avg:58.80ms
step:2250/2330 train_time:132303ms step_avg:58.80ms
step:2250/2330 val_loss:3.7037 train_time:132386ms step_avg:58.84ms
step:2251/2330 train_time:132406ms step_avg:58.82ms
step:2252/2330 train_time:132426ms step_avg:58.80ms
step:2253/2330 train_time:132484ms step_avg:58.80ms
step:2254/2330 train_time:132551ms step_avg:58.81ms
step:2255/2330 train_time:132609ms step_avg:58.81ms
step:2256/2330 train_time:132671ms step_avg:58.81ms
step:2257/2330 train_time:132727ms step_avg:58.81ms
step:2258/2330 train_time:132788ms step_avg:58.81ms
step:2259/2330 train_time:132845ms step_avg:58.81ms
step:2260/2330 train_time:132905ms step_avg:58.81ms
step:2261/2330 train_time:132963ms step_avg:58.81ms
step:2262/2330 train_time:133022ms step_avg:58.81ms
step:2263/2330 train_time:133079ms step_avg:58.81ms
step:2264/2330 train_time:133140ms step_avg:58.81ms
step:2265/2330 train_time:133196ms step_avg:58.81ms
step:2266/2330 train_time:133256ms step_avg:58.81ms
step:2267/2330 train_time:133314ms step_avg:58.81ms
step:2268/2330 train_time:133376ms step_avg:58.81ms
step:2269/2330 train_time:133434ms step_avg:58.81ms
step:2270/2330 train_time:133496ms step_avg:58.81ms
step:2271/2330 train_time:133554ms step_avg:58.81ms
step:2272/2330 train_time:133616ms step_avg:58.81ms
step:2273/2330 train_time:133673ms step_avg:58.81ms
step:2274/2330 train_time:133735ms step_avg:58.81ms
step:2275/2330 train_time:133792ms step_avg:58.81ms
step:2276/2330 train_time:133853ms step_avg:58.81ms
step:2277/2330 train_time:133910ms step_avg:58.81ms
step:2278/2330 train_time:133971ms step_avg:58.81ms
step:2279/2330 train_time:134027ms step_avg:58.81ms
step:2280/2330 train_time:134087ms step_avg:58.81ms
step:2281/2330 train_time:134144ms step_avg:58.81ms
step:2282/2330 train_time:134205ms step_avg:58.81ms
step:2283/2330 train_time:134264ms step_avg:58.81ms
step:2284/2330 train_time:134324ms step_avg:58.81ms
step:2285/2330 train_time:134382ms step_avg:58.81ms
step:2286/2330 train_time:134443ms step_avg:58.81ms
step:2287/2330 train_time:134503ms step_avg:58.81ms
step:2288/2330 train_time:134563ms step_avg:58.81ms
step:2289/2330 train_time:134621ms step_avg:58.81ms
step:2290/2330 train_time:134683ms step_avg:58.81ms
step:2291/2330 train_time:134740ms step_avg:58.81ms
step:2292/2330 train_time:134801ms step_avg:58.81ms
step:2293/2330 train_time:134859ms step_avg:58.81ms
step:2294/2330 train_time:134920ms step_avg:58.81ms
step:2295/2330 train_time:134978ms step_avg:58.81ms
step:2296/2330 train_time:135038ms step_avg:58.81ms
step:2297/2330 train_time:135095ms step_avg:58.81ms
step:2298/2330 train_time:135156ms step_avg:58.81ms
step:2299/2330 train_time:135213ms step_avg:58.81ms
step:2300/2330 train_time:135273ms step_avg:58.81ms
step:2301/2330 train_time:135331ms step_avg:58.81ms
step:2302/2330 train_time:135391ms step_avg:58.81ms
step:2303/2330 train_time:135448ms step_avg:58.81ms
step:2304/2330 train_time:135509ms step_avg:58.81ms
step:2305/2330 train_time:135568ms step_avg:58.81ms
step:2306/2330 train_time:135628ms step_avg:58.82ms
step:2307/2330 train_time:135686ms step_avg:58.81ms
step:2308/2330 train_time:135748ms step_avg:58.82ms
step:2309/2330 train_time:135805ms step_avg:58.82ms
step:2310/2330 train_time:135866ms step_avg:58.82ms
step:2311/2330 train_time:135924ms step_avg:58.82ms
step:2312/2330 train_time:135985ms step_avg:58.82ms
step:2313/2330 train_time:136042ms step_avg:58.82ms
step:2314/2330 train_time:136104ms step_avg:58.82ms
step:2315/2330 train_time:136161ms step_avg:58.82ms
step:2316/2330 train_time:136222ms step_avg:58.82ms
step:2317/2330 train_time:136280ms step_avg:58.82ms
step:2318/2330 train_time:136340ms step_avg:58.82ms
step:2319/2330 train_time:136397ms step_avg:58.82ms
step:2320/2330 train_time:136458ms step_avg:58.82ms
step:2321/2330 train_time:136515ms step_avg:58.82ms
step:2322/2330 train_time:136579ms step_avg:58.82ms
step:2323/2330 train_time:136636ms step_avg:58.82ms
step:2324/2330 train_time:136697ms step_avg:58.82ms
step:2325/2330 train_time:136754ms step_avg:58.82ms
step:2326/2330 train_time:136814ms step_avg:58.82ms
step:2327/2330 train_time:136871ms step_avg:58.82ms
step:2328/2330 train_time:136932ms step_avg:58.82ms
step:2329/2330 train_time:136988ms step_avg:58.82ms
step:2330/2330 train_time:137050ms step_avg:58.82ms
step:2330/2330 val_loss:3.6877 train_time:137131ms step_avg:58.85ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
