import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 08:10:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:87.05ms
step:2/2330 train_time:184ms step_avg:91.99ms
step:3/2330 train_time:202ms step_avg:67.45ms
step:4/2330 train_time:222ms step_avg:55.46ms
step:5/2330 train_time:275ms step_avg:55.07ms
step:6/2330 train_time:333ms step_avg:55.54ms
step:7/2330 train_time:388ms step_avg:55.45ms
step:8/2330 train_time:447ms step_avg:55.92ms
step:9/2330 train_time:503ms step_avg:55.84ms
step:10/2330 train_time:561ms step_avg:56.13ms
step:11/2330 train_time:616ms step_avg:56.01ms
step:12/2330 train_time:675ms step_avg:56.23ms
step:13/2330 train_time:731ms step_avg:56.20ms
step:14/2330 train_time:789ms step_avg:56.34ms
step:15/2330 train_time:844ms step_avg:56.27ms
step:16/2330 train_time:902ms step_avg:56.39ms
step:17/2330 train_time:958ms step_avg:56.36ms
step:18/2330 train_time:1017ms step_avg:56.49ms
step:19/2330 train_time:1074ms step_avg:56.52ms
step:20/2330 train_time:1138ms step_avg:56.88ms
step:21/2330 train_time:1196ms step_avg:56.93ms
step:22/2330 train_time:1257ms step_avg:57.13ms
step:23/2330 train_time:1313ms step_avg:57.10ms
step:24/2330 train_time:1372ms step_avg:57.17ms
step:25/2330 train_time:1428ms step_avg:57.12ms
step:26/2330 train_time:1487ms step_avg:57.18ms
step:27/2330 train_time:1542ms step_avg:57.12ms
step:28/2330 train_time:1600ms step_avg:57.16ms
step:29/2330 train_time:1656ms step_avg:57.11ms
step:30/2330 train_time:1714ms step_avg:57.14ms
step:31/2330 train_time:1770ms step_avg:57.08ms
step:32/2330 train_time:1829ms step_avg:57.14ms
step:33/2330 train_time:1884ms step_avg:57.09ms
step:34/2330 train_time:1943ms step_avg:57.14ms
step:35/2330 train_time:1998ms step_avg:57.10ms
step:36/2330 train_time:2059ms step_avg:57.18ms
step:37/2330 train_time:2116ms step_avg:57.19ms
step:38/2330 train_time:2176ms step_avg:57.26ms
step:39/2330 train_time:2233ms step_avg:57.25ms
step:40/2330 train_time:2293ms step_avg:57.32ms
step:41/2330 train_time:2350ms step_avg:57.31ms
step:42/2330 train_time:2408ms step_avg:57.34ms
step:43/2330 train_time:2464ms step_avg:57.31ms
step:44/2330 train_time:2523ms step_avg:57.34ms
step:45/2330 train_time:2579ms step_avg:57.30ms
step:46/2330 train_time:2637ms step_avg:57.32ms
step:47/2330 train_time:2693ms step_avg:57.29ms
step:48/2330 train_time:2751ms step_avg:57.31ms
step:49/2330 train_time:2807ms step_avg:57.28ms
step:50/2330 train_time:2865ms step_avg:57.30ms
step:51/2330 train_time:2921ms step_avg:57.27ms
step:52/2330 train_time:2979ms step_avg:57.28ms
step:53/2330 train_time:3035ms step_avg:57.27ms
step:54/2330 train_time:3095ms step_avg:57.31ms
step:55/2330 train_time:3151ms step_avg:57.29ms
step:56/2330 train_time:3211ms step_avg:57.33ms
step:57/2330 train_time:3267ms step_avg:57.32ms
step:58/2330 train_time:3327ms step_avg:57.36ms
step:59/2330 train_time:3383ms step_avg:57.33ms
step:60/2330 train_time:3443ms step_avg:57.38ms
step:61/2330 train_time:3498ms step_avg:57.35ms
step:62/2330 train_time:3557ms step_avg:57.37ms
step:63/2330 train_time:3612ms step_avg:57.34ms
step:64/2330 train_time:3671ms step_avg:57.36ms
step:65/2330 train_time:3727ms step_avg:57.33ms
step:66/2330 train_time:3786ms step_avg:57.36ms
step:67/2330 train_time:3841ms step_avg:57.33ms
step:68/2330 train_time:3900ms step_avg:57.35ms
step:69/2330 train_time:3956ms step_avg:57.33ms
step:70/2330 train_time:4015ms step_avg:57.36ms
step:71/2330 train_time:4072ms step_avg:57.35ms
step:72/2330 train_time:4131ms step_avg:57.37ms
step:73/2330 train_time:4187ms step_avg:57.35ms
step:74/2330 train_time:4246ms step_avg:57.38ms
step:75/2330 train_time:4302ms step_avg:57.36ms
step:76/2330 train_time:4362ms step_avg:57.40ms
step:77/2330 train_time:4418ms step_avg:57.38ms
step:78/2330 train_time:4477ms step_avg:57.40ms
step:79/2330 train_time:4534ms step_avg:57.39ms
step:80/2330 train_time:4593ms step_avg:57.41ms
step:81/2330 train_time:4649ms step_avg:57.39ms
step:82/2330 train_time:4707ms step_avg:57.40ms
step:83/2330 train_time:4763ms step_avg:57.38ms
step:84/2330 train_time:4821ms step_avg:57.39ms
step:85/2330 train_time:4876ms step_avg:57.37ms
step:86/2330 train_time:4935ms step_avg:57.38ms
step:87/2330 train_time:4991ms step_avg:57.37ms
step:88/2330 train_time:5051ms step_avg:57.40ms
step:89/2330 train_time:5107ms step_avg:57.38ms
step:90/2330 train_time:5166ms step_avg:57.40ms
step:91/2330 train_time:5221ms step_avg:57.38ms
step:92/2330 train_time:5281ms step_avg:57.40ms
step:93/2330 train_time:5337ms step_avg:57.38ms
step:94/2330 train_time:5396ms step_avg:57.40ms
step:95/2330 train_time:5453ms step_avg:57.40ms
step:96/2330 train_time:5512ms step_avg:57.42ms
step:97/2330 train_time:5569ms step_avg:57.41ms
step:98/2330 train_time:5627ms step_avg:57.42ms
step:99/2330 train_time:5683ms step_avg:57.40ms
step:100/2330 train_time:5742ms step_avg:57.42ms
step:101/2330 train_time:5798ms step_avg:57.40ms
step:102/2330 train_time:5856ms step_avg:57.41ms
step:103/2330 train_time:5912ms step_avg:57.40ms
step:104/2330 train_time:5971ms step_avg:57.42ms
step:105/2330 train_time:6027ms step_avg:57.40ms
step:106/2330 train_time:6086ms step_avg:57.42ms
step:107/2330 train_time:6142ms step_avg:57.41ms
step:108/2330 train_time:6201ms step_avg:57.42ms
step:109/2330 train_time:6257ms step_avg:57.40ms
step:110/2330 train_time:6316ms step_avg:57.42ms
step:111/2330 train_time:6372ms step_avg:57.40ms
step:112/2330 train_time:6432ms step_avg:57.43ms
step:113/2330 train_time:6488ms step_avg:57.42ms
step:114/2330 train_time:6548ms step_avg:57.44ms
step:115/2330 train_time:6604ms step_avg:57.43ms
step:116/2330 train_time:6664ms step_avg:57.45ms
step:117/2330 train_time:6719ms step_avg:57.43ms
step:118/2330 train_time:6779ms step_avg:57.45ms
step:119/2330 train_time:6835ms step_avg:57.44ms
step:120/2330 train_time:6894ms step_avg:57.45ms
step:121/2330 train_time:6949ms step_avg:57.43ms
step:122/2330 train_time:7009ms step_avg:57.45ms
step:123/2330 train_time:7066ms step_avg:57.44ms
step:124/2330 train_time:7124ms step_avg:57.45ms
step:125/2330 train_time:7179ms step_avg:57.43ms
step:126/2330 train_time:7239ms step_avg:57.45ms
step:127/2330 train_time:7294ms step_avg:57.44ms
step:128/2330 train_time:7354ms step_avg:57.45ms
step:129/2330 train_time:7410ms step_avg:57.44ms
step:130/2330 train_time:7471ms step_avg:57.47ms
step:131/2330 train_time:7527ms step_avg:57.46ms
step:132/2330 train_time:7586ms step_avg:57.47ms
step:133/2330 train_time:7642ms step_avg:57.46ms
step:134/2330 train_time:7700ms step_avg:57.46ms
step:135/2330 train_time:7756ms step_avg:57.45ms
step:136/2330 train_time:7815ms step_avg:57.46ms
step:137/2330 train_time:7872ms step_avg:57.46ms
step:138/2330 train_time:7931ms step_avg:57.47ms
step:139/2330 train_time:7987ms step_avg:57.46ms
step:140/2330 train_time:8045ms step_avg:57.47ms
step:141/2330 train_time:8101ms step_avg:57.46ms
step:142/2330 train_time:8160ms step_avg:57.47ms
step:143/2330 train_time:8216ms step_avg:57.45ms
step:144/2330 train_time:8275ms step_avg:57.47ms
step:145/2330 train_time:8331ms step_avg:57.46ms
step:146/2330 train_time:8389ms step_avg:57.46ms
step:147/2330 train_time:8445ms step_avg:57.45ms
step:148/2330 train_time:8504ms step_avg:57.46ms
step:149/2330 train_time:8560ms step_avg:57.45ms
step:150/2330 train_time:8619ms step_avg:57.46ms
step:151/2330 train_time:8675ms step_avg:57.45ms
step:152/2330 train_time:8734ms step_avg:57.46ms
step:153/2330 train_time:8789ms step_avg:57.45ms
step:154/2330 train_time:8849ms step_avg:57.46ms
step:155/2330 train_time:8905ms step_avg:57.45ms
step:156/2330 train_time:8964ms step_avg:57.46ms
step:157/2330 train_time:9020ms step_avg:57.45ms
step:158/2330 train_time:9078ms step_avg:57.46ms
step:159/2330 train_time:9133ms step_avg:57.44ms
step:160/2330 train_time:9193ms step_avg:57.46ms
step:161/2330 train_time:9249ms step_avg:57.45ms
step:162/2330 train_time:9308ms step_avg:57.45ms
step:163/2330 train_time:9364ms step_avg:57.45ms
step:164/2330 train_time:9422ms step_avg:57.45ms
step:165/2330 train_time:9478ms step_avg:57.44ms
step:166/2330 train_time:9537ms step_avg:57.45ms
step:167/2330 train_time:9593ms step_avg:57.45ms
step:168/2330 train_time:9653ms step_avg:57.46ms
step:169/2330 train_time:9709ms step_avg:57.45ms
step:170/2330 train_time:9769ms step_avg:57.46ms
step:171/2330 train_time:9824ms step_avg:57.45ms
step:172/2330 train_time:9883ms step_avg:57.46ms
step:173/2330 train_time:9939ms step_avg:57.45ms
step:174/2330 train_time:9998ms step_avg:57.46ms
step:175/2330 train_time:10053ms step_avg:57.45ms
step:176/2330 train_time:10113ms step_avg:57.46ms
step:177/2330 train_time:10169ms step_avg:57.45ms
step:178/2330 train_time:10229ms step_avg:57.47ms
step:179/2330 train_time:10285ms step_avg:57.46ms
step:180/2330 train_time:10344ms step_avg:57.47ms
step:181/2330 train_time:10400ms step_avg:57.46ms
step:182/2330 train_time:10459ms step_avg:57.47ms
step:183/2330 train_time:10515ms step_avg:57.46ms
step:184/2330 train_time:10575ms step_avg:57.47ms
step:185/2330 train_time:10631ms step_avg:57.46ms
step:186/2330 train_time:10690ms step_avg:57.47ms
step:187/2330 train_time:10746ms step_avg:57.46ms
step:188/2330 train_time:10805ms step_avg:57.47ms
step:189/2330 train_time:10860ms step_avg:57.46ms
step:190/2330 train_time:10919ms step_avg:57.47ms
step:191/2330 train_time:10975ms step_avg:57.46ms
step:192/2330 train_time:11034ms step_avg:57.47ms
step:193/2330 train_time:11090ms step_avg:57.46ms
step:194/2330 train_time:11149ms step_avg:57.47ms
step:195/2330 train_time:11204ms step_avg:57.46ms
step:196/2330 train_time:11264ms step_avg:57.47ms
step:197/2330 train_time:11320ms step_avg:57.46ms
step:198/2330 train_time:11379ms step_avg:57.47ms
step:199/2330 train_time:11435ms step_avg:57.46ms
step:200/2330 train_time:11494ms step_avg:57.47ms
step:201/2330 train_time:11550ms step_avg:57.46ms
step:202/2330 train_time:11609ms step_avg:57.47ms
step:203/2330 train_time:11665ms step_avg:57.46ms
step:204/2330 train_time:11724ms step_avg:57.47ms
step:205/2330 train_time:11780ms step_avg:57.46ms
step:206/2330 train_time:11839ms step_avg:57.47ms
step:207/2330 train_time:11894ms step_avg:57.46ms
step:208/2330 train_time:11955ms step_avg:57.47ms
step:209/2330 train_time:12011ms step_avg:57.47ms
step:210/2330 train_time:12070ms step_avg:57.47ms
step:211/2330 train_time:12126ms step_avg:57.47ms
step:212/2330 train_time:12184ms step_avg:57.47ms
step:213/2330 train_time:12240ms step_avg:57.47ms
step:214/2330 train_time:12299ms step_avg:57.47ms
step:215/2330 train_time:12356ms step_avg:57.47ms
step:216/2330 train_time:12415ms step_avg:57.48ms
step:217/2330 train_time:12471ms step_avg:57.47ms
step:218/2330 train_time:12530ms step_avg:57.48ms
step:219/2330 train_time:12586ms step_avg:57.47ms
step:220/2330 train_time:12645ms step_avg:57.48ms
step:221/2330 train_time:12700ms step_avg:57.47ms
step:222/2330 train_time:12760ms step_avg:57.48ms
step:223/2330 train_time:12815ms step_avg:57.47ms
step:224/2330 train_time:12874ms step_avg:57.47ms
step:225/2330 train_time:12931ms step_avg:57.47ms
step:226/2330 train_time:12990ms step_avg:57.48ms
step:227/2330 train_time:13046ms step_avg:57.47ms
step:228/2330 train_time:13105ms step_avg:57.48ms
step:229/2330 train_time:13162ms step_avg:57.47ms
step:230/2330 train_time:13220ms step_avg:57.48ms
step:231/2330 train_time:13276ms step_avg:57.47ms
step:232/2330 train_time:13335ms step_avg:57.48ms
step:233/2330 train_time:13391ms step_avg:57.47ms
step:234/2330 train_time:13450ms step_avg:57.48ms
step:235/2330 train_time:13506ms step_avg:57.47ms
step:236/2330 train_time:13565ms step_avg:57.48ms
step:237/2330 train_time:13621ms step_avg:57.47ms
step:238/2330 train_time:13680ms step_avg:57.48ms
step:239/2330 train_time:13736ms step_avg:57.47ms
step:240/2330 train_time:13795ms step_avg:57.48ms
step:241/2330 train_time:13851ms step_avg:57.47ms
step:242/2330 train_time:13912ms step_avg:57.49ms
step:243/2330 train_time:13967ms step_avg:57.48ms
step:244/2330 train_time:14028ms step_avg:57.49ms
step:245/2330 train_time:14084ms step_avg:57.49ms
step:246/2330 train_time:14143ms step_avg:57.49ms
step:247/2330 train_time:14199ms step_avg:57.49ms
step:248/2330 train_time:14258ms step_avg:57.49ms
step:249/2330 train_time:14314ms step_avg:57.49ms
step:250/2330 train_time:14373ms step_avg:57.49ms
step:250/2330 val_loss:4.8980 train_time:14452ms step_avg:57.81ms
step:251/2330 train_time:14470ms step_avg:57.65ms
step:252/2330 train_time:14490ms step_avg:57.50ms
step:253/2330 train_time:14545ms step_avg:57.49ms
step:254/2330 train_time:14610ms step_avg:57.52ms
step:255/2330 train_time:14667ms step_avg:57.52ms
step:256/2330 train_time:14730ms step_avg:57.54ms
step:257/2330 train_time:14785ms step_avg:57.53ms
step:258/2330 train_time:14845ms step_avg:57.54ms
step:259/2330 train_time:14900ms step_avg:57.53ms
step:260/2330 train_time:14960ms step_avg:57.54ms
step:261/2330 train_time:15015ms step_avg:57.53ms
step:262/2330 train_time:15074ms step_avg:57.53ms
step:263/2330 train_time:15130ms step_avg:57.53ms
step:264/2330 train_time:15188ms step_avg:57.53ms
step:265/2330 train_time:15243ms step_avg:57.52ms
step:266/2330 train_time:15301ms step_avg:57.52ms
step:267/2330 train_time:15357ms step_avg:57.52ms
step:268/2330 train_time:15416ms step_avg:57.52ms
step:269/2330 train_time:15472ms step_avg:57.52ms
step:270/2330 train_time:15532ms step_avg:57.53ms
step:271/2330 train_time:15589ms step_avg:57.52ms
step:272/2330 train_time:15650ms step_avg:57.54ms
step:273/2330 train_time:15708ms step_avg:57.54ms
step:274/2330 train_time:15767ms step_avg:57.54ms
step:275/2330 train_time:15823ms step_avg:57.54ms
step:276/2330 train_time:15883ms step_avg:57.55ms
step:277/2330 train_time:15939ms step_avg:57.54ms
step:278/2330 train_time:15998ms step_avg:57.55ms
step:279/2330 train_time:16053ms step_avg:57.54ms
step:280/2330 train_time:16112ms step_avg:57.54ms
step:281/2330 train_time:16168ms step_avg:57.54ms
step:282/2330 train_time:16226ms step_avg:57.54ms
step:283/2330 train_time:16281ms step_avg:57.53ms
step:284/2330 train_time:16339ms step_avg:57.53ms
step:285/2330 train_time:16395ms step_avg:57.53ms
step:286/2330 train_time:16454ms step_avg:57.53ms
step:287/2330 train_time:16510ms step_avg:57.53ms
step:288/2330 train_time:16571ms step_avg:57.54ms
step:289/2330 train_time:16627ms step_avg:57.53ms
step:290/2330 train_time:16687ms step_avg:57.54ms
step:291/2330 train_time:16744ms step_avg:57.54ms
step:292/2330 train_time:16803ms step_avg:57.54ms
step:293/2330 train_time:16859ms step_avg:57.54ms
step:294/2330 train_time:16919ms step_avg:57.55ms
step:295/2330 train_time:16974ms step_avg:57.54ms
step:296/2330 train_time:17033ms step_avg:57.54ms
step:297/2330 train_time:17089ms step_avg:57.54ms
step:298/2330 train_time:17147ms step_avg:57.54ms
step:299/2330 train_time:17203ms step_avg:57.53ms
step:300/2330 train_time:17262ms step_avg:57.54ms
step:301/2330 train_time:17317ms step_avg:57.53ms
step:302/2330 train_time:17376ms step_avg:57.54ms
step:303/2330 train_time:17432ms step_avg:57.53ms
step:304/2330 train_time:17492ms step_avg:57.54ms
step:305/2330 train_time:17548ms step_avg:57.54ms
step:306/2330 train_time:17607ms step_avg:57.54ms
step:307/2330 train_time:17664ms step_avg:57.54ms
step:308/2330 train_time:17723ms step_avg:57.54ms
step:309/2330 train_time:17779ms step_avg:57.54ms
step:310/2330 train_time:17838ms step_avg:57.54ms
step:311/2330 train_time:17894ms step_avg:57.54ms
step:312/2330 train_time:17954ms step_avg:57.54ms
step:313/2330 train_time:18010ms step_avg:57.54ms
step:314/2330 train_time:18068ms step_avg:57.54ms
step:315/2330 train_time:18124ms step_avg:57.54ms
step:316/2330 train_time:18183ms step_avg:57.54ms
step:317/2330 train_time:18239ms step_avg:57.54ms
step:318/2330 train_time:18298ms step_avg:57.54ms
step:319/2330 train_time:18353ms step_avg:57.53ms
step:320/2330 train_time:18412ms step_avg:57.54ms
step:321/2330 train_time:18468ms step_avg:57.53ms
step:322/2330 train_time:18527ms step_avg:57.54ms
step:323/2330 train_time:18583ms step_avg:57.53ms
step:324/2330 train_time:18642ms step_avg:57.54ms
step:325/2330 train_time:18698ms step_avg:57.53ms
step:326/2330 train_time:18758ms step_avg:57.54ms
step:327/2330 train_time:18814ms step_avg:57.53ms
step:328/2330 train_time:18873ms step_avg:57.54ms
step:329/2330 train_time:18930ms step_avg:57.54ms
step:330/2330 train_time:18989ms step_avg:57.54ms
step:331/2330 train_time:19045ms step_avg:57.54ms
step:332/2330 train_time:19103ms step_avg:57.54ms
step:333/2330 train_time:19159ms step_avg:57.53ms
step:334/2330 train_time:19217ms step_avg:57.54ms
step:335/2330 train_time:19273ms step_avg:57.53ms
step:336/2330 train_time:19332ms step_avg:57.54ms
step:337/2330 train_time:19388ms step_avg:57.53ms
step:338/2330 train_time:19447ms step_avg:57.54ms
step:339/2330 train_time:19503ms step_avg:57.53ms
step:340/2330 train_time:19562ms step_avg:57.54ms
step:341/2330 train_time:19619ms step_avg:57.53ms
step:342/2330 train_time:19677ms step_avg:57.54ms
step:343/2330 train_time:19733ms step_avg:57.53ms
step:344/2330 train_time:19793ms step_avg:57.54ms
step:345/2330 train_time:19849ms step_avg:57.53ms
step:346/2330 train_time:19908ms step_avg:57.54ms
step:347/2330 train_time:19964ms step_avg:57.53ms
step:348/2330 train_time:20023ms step_avg:57.54ms
step:349/2330 train_time:20079ms step_avg:57.53ms
step:350/2330 train_time:20138ms step_avg:57.54ms
step:351/2330 train_time:20194ms step_avg:57.53ms
step:352/2330 train_time:20253ms step_avg:57.54ms
step:353/2330 train_time:20309ms step_avg:57.53ms
step:354/2330 train_time:20367ms step_avg:57.53ms
step:355/2330 train_time:20423ms step_avg:57.53ms
step:356/2330 train_time:20482ms step_avg:57.53ms
step:357/2330 train_time:20538ms step_avg:57.53ms
step:358/2330 train_time:20597ms step_avg:57.53ms
step:359/2330 train_time:20654ms step_avg:57.53ms
step:360/2330 train_time:20713ms step_avg:57.53ms
step:361/2330 train_time:20768ms step_avg:57.53ms
step:362/2330 train_time:20828ms step_avg:57.54ms
step:363/2330 train_time:20884ms step_avg:57.53ms
step:364/2330 train_time:20943ms step_avg:57.54ms
step:365/2330 train_time:21000ms step_avg:57.53ms
step:366/2330 train_time:21058ms step_avg:57.54ms
step:367/2330 train_time:21114ms step_avg:57.53ms
step:368/2330 train_time:21173ms step_avg:57.54ms
step:369/2330 train_time:21229ms step_avg:57.53ms
step:370/2330 train_time:21287ms step_avg:57.53ms
step:371/2330 train_time:21343ms step_avg:57.53ms
step:372/2330 train_time:21402ms step_avg:57.53ms
step:373/2330 train_time:21458ms step_avg:57.53ms
step:374/2330 train_time:21517ms step_avg:57.53ms
step:375/2330 train_time:21572ms step_avg:57.53ms
step:376/2330 train_time:21631ms step_avg:57.53ms
step:377/2330 train_time:21687ms step_avg:57.53ms
step:378/2330 train_time:21747ms step_avg:57.53ms
step:379/2330 train_time:21803ms step_avg:57.53ms
step:380/2330 train_time:21863ms step_avg:57.53ms
step:381/2330 train_time:21919ms step_avg:57.53ms
step:382/2330 train_time:21977ms step_avg:57.53ms
step:383/2330 train_time:22033ms step_avg:57.53ms
step:384/2330 train_time:22092ms step_avg:57.53ms
step:385/2330 train_time:22149ms step_avg:57.53ms
step:386/2330 train_time:22208ms step_avg:57.53ms
step:387/2330 train_time:22264ms step_avg:57.53ms
step:388/2330 train_time:22324ms step_avg:57.53ms
step:389/2330 train_time:22380ms step_avg:57.53ms
step:390/2330 train_time:22438ms step_avg:57.53ms
step:391/2330 train_time:22494ms step_avg:57.53ms
step:392/2330 train_time:22553ms step_avg:57.53ms
step:393/2330 train_time:22610ms step_avg:57.53ms
step:394/2330 train_time:22669ms step_avg:57.54ms
step:395/2330 train_time:22726ms step_avg:57.53ms
step:396/2330 train_time:22785ms step_avg:57.54ms
step:397/2330 train_time:22841ms step_avg:57.53ms
step:398/2330 train_time:22900ms step_avg:57.54ms
step:399/2330 train_time:22957ms step_avg:57.54ms
step:400/2330 train_time:23015ms step_avg:57.54ms
step:401/2330 train_time:23071ms step_avg:57.53ms
step:402/2330 train_time:23130ms step_avg:57.54ms
step:403/2330 train_time:23186ms step_avg:57.53ms
step:404/2330 train_time:23246ms step_avg:57.54ms
step:405/2330 train_time:23302ms step_avg:57.53ms
step:406/2330 train_time:23361ms step_avg:57.54ms
step:407/2330 train_time:23417ms step_avg:57.53ms
step:408/2330 train_time:23476ms step_avg:57.54ms
step:409/2330 train_time:23532ms step_avg:57.54ms
step:410/2330 train_time:23591ms step_avg:57.54ms
step:411/2330 train_time:23647ms step_avg:57.54ms
step:412/2330 train_time:23707ms step_avg:57.54ms
step:413/2330 train_time:23763ms step_avg:57.54ms
step:414/2330 train_time:23823ms step_avg:57.54ms
step:415/2330 train_time:23879ms step_avg:57.54ms
step:416/2330 train_time:23938ms step_avg:57.54ms
step:417/2330 train_time:23994ms step_avg:57.54ms
step:418/2330 train_time:24054ms step_avg:57.54ms
step:419/2330 train_time:24111ms step_avg:57.54ms
step:420/2330 train_time:24169ms step_avg:57.55ms
step:421/2330 train_time:24226ms step_avg:57.54ms
step:422/2330 train_time:24285ms step_avg:57.55ms
step:423/2330 train_time:24341ms step_avg:57.54ms
step:424/2330 train_time:24400ms step_avg:57.55ms
step:425/2330 train_time:24455ms step_avg:57.54ms
step:426/2330 train_time:24515ms step_avg:57.55ms
step:427/2330 train_time:24571ms step_avg:57.54ms
step:428/2330 train_time:24630ms step_avg:57.55ms
step:429/2330 train_time:24687ms step_avg:57.55ms
step:430/2330 train_time:24747ms step_avg:57.55ms
step:431/2330 train_time:24803ms step_avg:57.55ms
step:432/2330 train_time:24862ms step_avg:57.55ms
step:433/2330 train_time:24918ms step_avg:57.55ms
step:434/2330 train_time:24977ms step_avg:57.55ms
step:435/2330 train_time:25034ms step_avg:57.55ms
step:436/2330 train_time:25092ms step_avg:57.55ms
step:437/2330 train_time:25149ms step_avg:57.55ms
step:438/2330 train_time:25208ms step_avg:57.55ms
step:439/2330 train_time:25264ms step_avg:57.55ms
step:440/2330 train_time:25322ms step_avg:57.55ms
step:441/2330 train_time:25378ms step_avg:57.55ms
step:442/2330 train_time:25437ms step_avg:57.55ms
step:443/2330 train_time:25493ms step_avg:57.55ms
step:444/2330 train_time:25552ms step_avg:57.55ms
step:445/2330 train_time:25608ms step_avg:57.55ms
step:446/2330 train_time:25667ms step_avg:57.55ms
step:447/2330 train_time:25723ms step_avg:57.55ms
step:448/2330 train_time:25782ms step_avg:57.55ms
step:449/2330 train_time:25838ms step_avg:57.55ms
step:450/2330 train_time:25898ms step_avg:57.55ms
step:451/2330 train_time:25954ms step_avg:57.55ms
step:452/2330 train_time:26013ms step_avg:57.55ms
step:453/2330 train_time:26069ms step_avg:57.55ms
step:454/2330 train_time:26128ms step_avg:57.55ms
step:455/2330 train_time:26184ms step_avg:57.55ms
step:456/2330 train_time:26244ms step_avg:57.55ms
step:457/2330 train_time:26300ms step_avg:57.55ms
step:458/2330 train_time:26359ms step_avg:57.55ms
step:459/2330 train_time:26414ms step_avg:57.55ms
step:460/2330 train_time:26474ms step_avg:57.55ms
step:461/2330 train_time:26529ms step_avg:57.55ms
step:462/2330 train_time:26588ms step_avg:57.55ms
step:463/2330 train_time:26644ms step_avg:57.55ms
step:464/2330 train_time:26703ms step_avg:57.55ms
step:465/2330 train_time:26759ms step_avg:57.55ms
step:466/2330 train_time:26818ms step_avg:57.55ms
step:467/2330 train_time:26874ms step_avg:57.55ms
step:468/2330 train_time:26933ms step_avg:57.55ms
step:469/2330 train_time:26989ms step_avg:57.54ms
step:470/2330 train_time:27048ms step_avg:57.55ms
step:471/2330 train_time:27104ms step_avg:57.55ms
step:472/2330 train_time:27163ms step_avg:57.55ms
step:473/2330 train_time:27220ms step_avg:57.55ms
step:474/2330 train_time:27278ms step_avg:57.55ms
step:475/2330 train_time:27334ms step_avg:57.54ms
step:476/2330 train_time:27393ms step_avg:57.55ms
step:477/2330 train_time:27449ms step_avg:57.55ms
step:478/2330 train_time:27508ms step_avg:57.55ms
step:479/2330 train_time:27564ms step_avg:57.55ms
step:480/2330 train_time:27623ms step_avg:57.55ms
step:481/2330 train_time:27680ms step_avg:57.55ms
step:482/2330 train_time:27738ms step_avg:57.55ms
step:483/2330 train_time:27794ms step_avg:57.54ms
step:484/2330 train_time:27853ms step_avg:57.55ms
step:485/2330 train_time:27909ms step_avg:57.54ms
step:486/2330 train_time:27968ms step_avg:57.55ms
step:487/2330 train_time:28024ms step_avg:57.54ms
step:488/2330 train_time:28083ms step_avg:57.55ms
step:489/2330 train_time:28139ms step_avg:57.54ms
step:490/2330 train_time:28199ms step_avg:57.55ms
step:491/2330 train_time:28254ms step_avg:57.54ms
step:492/2330 train_time:28314ms step_avg:57.55ms
step:493/2330 train_time:28371ms step_avg:57.55ms
step:494/2330 train_time:28430ms step_avg:57.55ms
step:495/2330 train_time:28486ms step_avg:57.55ms
step:496/2330 train_time:28545ms step_avg:57.55ms
step:497/2330 train_time:28601ms step_avg:57.55ms
step:498/2330 train_time:28660ms step_avg:57.55ms
step:499/2330 train_time:28716ms step_avg:57.55ms
step:500/2330 train_time:28776ms step_avg:57.55ms
step:500/2330 val_loss:4.4098 train_time:28855ms step_avg:57.71ms
step:501/2330 train_time:28873ms step_avg:57.63ms
step:502/2330 train_time:28894ms step_avg:57.56ms
step:503/2330 train_time:28950ms step_avg:57.56ms
step:504/2330 train_time:29015ms step_avg:57.57ms
step:505/2330 train_time:29073ms step_avg:57.57ms
step:506/2330 train_time:29136ms step_avg:57.58ms
step:507/2330 train_time:29192ms step_avg:57.58ms
step:508/2330 train_time:29251ms step_avg:57.58ms
step:509/2330 train_time:29307ms step_avg:57.58ms
step:510/2330 train_time:29366ms step_avg:57.58ms
step:511/2330 train_time:29421ms step_avg:57.58ms
step:512/2330 train_time:29479ms step_avg:57.58ms
step:513/2330 train_time:29535ms step_avg:57.57ms
step:514/2330 train_time:29593ms step_avg:57.57ms
step:515/2330 train_time:29649ms step_avg:57.57ms
step:516/2330 train_time:29707ms step_avg:57.57ms
step:517/2330 train_time:29762ms step_avg:57.57ms
step:518/2330 train_time:29822ms step_avg:57.57ms
step:519/2330 train_time:29878ms step_avg:57.57ms
step:520/2330 train_time:29938ms step_avg:57.57ms
step:521/2330 train_time:29995ms step_avg:57.57ms
step:522/2330 train_time:30057ms step_avg:57.58ms
step:523/2330 train_time:30114ms step_avg:57.58ms
step:524/2330 train_time:30173ms step_avg:57.58ms
step:525/2330 train_time:30229ms step_avg:57.58ms
step:526/2330 train_time:30289ms step_avg:57.58ms
step:527/2330 train_time:30345ms step_avg:57.58ms
step:528/2330 train_time:30404ms step_avg:57.58ms
step:529/2330 train_time:30460ms step_avg:57.58ms
step:530/2330 train_time:30518ms step_avg:57.58ms
step:531/2330 train_time:30574ms step_avg:57.58ms
step:532/2330 train_time:30632ms step_avg:57.58ms
step:533/2330 train_time:30688ms step_avg:57.58ms
step:534/2330 train_time:30747ms step_avg:57.58ms
step:535/2330 train_time:30803ms step_avg:57.58ms
step:536/2330 train_time:30862ms step_avg:57.58ms
step:537/2330 train_time:30918ms step_avg:57.58ms
step:538/2330 train_time:30978ms step_avg:57.58ms
step:539/2330 train_time:31034ms step_avg:57.58ms
step:540/2330 train_time:31094ms step_avg:57.58ms
step:541/2330 train_time:31151ms step_avg:57.58ms
step:542/2330 train_time:31211ms step_avg:57.58ms
step:543/2330 train_time:31267ms step_avg:57.58ms
step:544/2330 train_time:31326ms step_avg:57.58ms
step:545/2330 train_time:31382ms step_avg:57.58ms
step:546/2330 train_time:31440ms step_avg:57.58ms
step:547/2330 train_time:31495ms step_avg:57.58ms
step:548/2330 train_time:31555ms step_avg:57.58ms
step:549/2330 train_time:31611ms step_avg:57.58ms
step:550/2330 train_time:31670ms step_avg:57.58ms
step:551/2330 train_time:31726ms step_avg:57.58ms
step:552/2330 train_time:31785ms step_avg:57.58ms
step:553/2330 train_time:31840ms step_avg:57.58ms
step:554/2330 train_time:31900ms step_avg:57.58ms
step:555/2330 train_time:31956ms step_avg:57.58ms
step:556/2330 train_time:32017ms step_avg:57.58ms
step:557/2330 train_time:32072ms step_avg:57.58ms
step:558/2330 train_time:32132ms step_avg:57.58ms
step:559/2330 train_time:32189ms step_avg:57.58ms
step:560/2330 train_time:32249ms step_avg:57.59ms
step:561/2330 train_time:32305ms step_avg:57.58ms
step:562/2330 train_time:32364ms step_avg:57.59ms
step:563/2330 train_time:32420ms step_avg:57.58ms
step:564/2330 train_time:32478ms step_avg:57.59ms
step:565/2330 train_time:32534ms step_avg:57.58ms
step:566/2330 train_time:32593ms step_avg:57.58ms
step:567/2330 train_time:32649ms step_avg:57.58ms
step:568/2330 train_time:32708ms step_avg:57.58ms
step:569/2330 train_time:32764ms step_avg:57.58ms
step:570/2330 train_time:32824ms step_avg:57.59ms
step:571/2330 train_time:32879ms step_avg:57.58ms
step:572/2330 train_time:32940ms step_avg:57.59ms
step:573/2330 train_time:32996ms step_avg:57.58ms
step:574/2330 train_time:33056ms step_avg:57.59ms
step:575/2330 train_time:33113ms step_avg:57.59ms
step:576/2330 train_time:33172ms step_avg:57.59ms
step:577/2330 train_time:33229ms step_avg:57.59ms
step:578/2330 train_time:33288ms step_avg:57.59ms
step:579/2330 train_time:33345ms step_avg:57.59ms
step:580/2330 train_time:33403ms step_avg:57.59ms
step:581/2330 train_time:33459ms step_avg:57.59ms
step:582/2330 train_time:33517ms step_avg:57.59ms
step:583/2330 train_time:33573ms step_avg:57.59ms
step:584/2330 train_time:33632ms step_avg:57.59ms
step:585/2330 train_time:33688ms step_avg:57.59ms
step:586/2330 train_time:33748ms step_avg:57.59ms
step:587/2330 train_time:33804ms step_avg:57.59ms
step:588/2330 train_time:33863ms step_avg:57.59ms
step:589/2330 train_time:33919ms step_avg:57.59ms
step:590/2330 train_time:33979ms step_avg:57.59ms
step:591/2330 train_time:34035ms step_avg:57.59ms
step:592/2330 train_time:34095ms step_avg:57.59ms
step:593/2330 train_time:34152ms step_avg:57.59ms
step:594/2330 train_time:34211ms step_avg:57.59ms
step:595/2330 train_time:34267ms step_avg:57.59ms
step:596/2330 train_time:34328ms step_avg:57.60ms
step:597/2330 train_time:34384ms step_avg:57.59ms
step:598/2330 train_time:34443ms step_avg:57.60ms
step:599/2330 train_time:34499ms step_avg:57.59ms
step:600/2330 train_time:34558ms step_avg:57.60ms
step:601/2330 train_time:34614ms step_avg:57.59ms
step:602/2330 train_time:34673ms step_avg:57.60ms
step:603/2330 train_time:34729ms step_avg:57.59ms
step:604/2330 train_time:34788ms step_avg:57.60ms
step:605/2330 train_time:34844ms step_avg:57.59ms
step:606/2330 train_time:34903ms step_avg:57.60ms
step:607/2330 train_time:34959ms step_avg:57.59ms
step:608/2330 train_time:35020ms step_avg:57.60ms
step:609/2330 train_time:35075ms step_avg:57.59ms
step:610/2330 train_time:35135ms step_avg:57.60ms
step:611/2330 train_time:35191ms step_avg:57.60ms
step:612/2330 train_time:35251ms step_avg:57.60ms
step:613/2330 train_time:35308ms step_avg:57.60ms
step:614/2330 train_time:35367ms step_avg:57.60ms
step:615/2330 train_time:35423ms step_avg:57.60ms
step:616/2330 train_time:35482ms step_avg:57.60ms
step:617/2330 train_time:35538ms step_avg:57.60ms
step:618/2330 train_time:35597ms step_avg:57.60ms
step:619/2330 train_time:35653ms step_avg:57.60ms
step:620/2330 train_time:35712ms step_avg:57.60ms
step:621/2330 train_time:35767ms step_avg:57.60ms
step:622/2330 train_time:35827ms step_avg:57.60ms
step:623/2330 train_time:35883ms step_avg:57.60ms
step:624/2330 train_time:35942ms step_avg:57.60ms
step:625/2330 train_time:35998ms step_avg:57.60ms
step:626/2330 train_time:36058ms step_avg:57.60ms
step:627/2330 train_time:36113ms step_avg:57.60ms
step:628/2330 train_time:36173ms step_avg:57.60ms
step:629/2330 train_time:36229ms step_avg:57.60ms
step:630/2330 train_time:36290ms step_avg:57.60ms
step:631/2330 train_time:36346ms step_avg:57.60ms
step:632/2330 train_time:36406ms step_avg:57.60ms
step:633/2330 train_time:36462ms step_avg:57.60ms
step:634/2330 train_time:36521ms step_avg:57.60ms
step:635/2330 train_time:36576ms step_avg:57.60ms
step:636/2330 train_time:36636ms step_avg:57.60ms
step:637/2330 train_time:36692ms step_avg:57.60ms
step:638/2330 train_time:36751ms step_avg:57.60ms
step:639/2330 train_time:36808ms step_avg:57.60ms
step:640/2330 train_time:36867ms step_avg:57.61ms
step:641/2330 train_time:36923ms step_avg:57.60ms
step:642/2330 train_time:36982ms step_avg:57.60ms
step:643/2330 train_time:37037ms step_avg:57.60ms
step:644/2330 train_time:37097ms step_avg:57.60ms
step:645/2330 train_time:37153ms step_avg:57.60ms
step:646/2330 train_time:37212ms step_avg:57.60ms
step:647/2330 train_time:37269ms step_avg:57.60ms
step:648/2330 train_time:37329ms step_avg:57.61ms
step:649/2330 train_time:37385ms step_avg:57.60ms
step:650/2330 train_time:37444ms step_avg:57.61ms
step:651/2330 train_time:37499ms step_avg:57.60ms
step:652/2330 train_time:37559ms step_avg:57.61ms
step:653/2330 train_time:37615ms step_avg:57.60ms
step:654/2330 train_time:37674ms step_avg:57.60ms
step:655/2330 train_time:37730ms step_avg:57.60ms
step:656/2330 train_time:37789ms step_avg:57.61ms
step:657/2330 train_time:37846ms step_avg:57.60ms
step:658/2330 train_time:37905ms step_avg:57.61ms
step:659/2330 train_time:37961ms step_avg:57.60ms
step:660/2330 train_time:38020ms step_avg:57.61ms
step:661/2330 train_time:38075ms step_avg:57.60ms
step:662/2330 train_time:38136ms step_avg:57.61ms
step:663/2330 train_time:38192ms step_avg:57.60ms
step:664/2330 train_time:38251ms step_avg:57.61ms
step:665/2330 train_time:38307ms step_avg:57.60ms
step:666/2330 train_time:38366ms step_avg:57.61ms
step:667/2330 train_time:38422ms step_avg:57.60ms
step:668/2330 train_time:38482ms step_avg:57.61ms
step:669/2330 train_time:38538ms step_avg:57.61ms
step:670/2330 train_time:38598ms step_avg:57.61ms
step:671/2330 train_time:38654ms step_avg:57.61ms
step:672/2330 train_time:38712ms step_avg:57.61ms
step:673/2330 train_time:38768ms step_avg:57.61ms
step:674/2330 train_time:38829ms step_avg:57.61ms
step:675/2330 train_time:38885ms step_avg:57.61ms
step:676/2330 train_time:38943ms step_avg:57.61ms
step:677/2330 train_time:39000ms step_avg:57.61ms
step:678/2330 train_time:39059ms step_avg:57.61ms
step:679/2330 train_time:39115ms step_avg:57.61ms
step:680/2330 train_time:39174ms step_avg:57.61ms
step:681/2330 train_time:39230ms step_avg:57.61ms
step:682/2330 train_time:39290ms step_avg:57.61ms
step:683/2330 train_time:39346ms step_avg:57.61ms
step:684/2330 train_time:39405ms step_avg:57.61ms
step:685/2330 train_time:39461ms step_avg:57.61ms
step:686/2330 train_time:39520ms step_avg:57.61ms
step:687/2330 train_time:39576ms step_avg:57.61ms
step:688/2330 train_time:39636ms step_avg:57.61ms
step:689/2330 train_time:39692ms step_avg:57.61ms
step:690/2330 train_time:39751ms step_avg:57.61ms
step:691/2330 train_time:39808ms step_avg:57.61ms
step:692/2330 train_time:39867ms step_avg:57.61ms
step:693/2330 train_time:39923ms step_avg:57.61ms
step:694/2330 train_time:39982ms step_avg:57.61ms
step:695/2330 train_time:40038ms step_avg:57.61ms
step:696/2330 train_time:40097ms step_avg:57.61ms
step:697/2330 train_time:40152ms step_avg:57.61ms
step:698/2330 train_time:40212ms step_avg:57.61ms
step:699/2330 train_time:40269ms step_avg:57.61ms
step:700/2330 train_time:40328ms step_avg:57.61ms
step:701/2330 train_time:40385ms step_avg:57.61ms
step:702/2330 train_time:40443ms step_avg:57.61ms
step:703/2330 train_time:40499ms step_avg:57.61ms
step:704/2330 train_time:40558ms step_avg:57.61ms
step:705/2330 train_time:40615ms step_avg:57.61ms
step:706/2330 train_time:40674ms step_avg:57.61ms
step:707/2330 train_time:40730ms step_avg:57.61ms
step:708/2330 train_time:40790ms step_avg:57.61ms
step:709/2330 train_time:40846ms step_avg:57.61ms
step:710/2330 train_time:40905ms step_avg:57.61ms
step:711/2330 train_time:40961ms step_avg:57.61ms
step:712/2330 train_time:41020ms step_avg:57.61ms
step:713/2330 train_time:41076ms step_avg:57.61ms
step:714/2330 train_time:41136ms step_avg:57.61ms
step:715/2330 train_time:41192ms step_avg:57.61ms
step:716/2330 train_time:41251ms step_avg:57.61ms
step:717/2330 train_time:41307ms step_avg:57.61ms
step:718/2330 train_time:41367ms step_avg:57.61ms
step:719/2330 train_time:41423ms step_avg:57.61ms
step:720/2330 train_time:41482ms step_avg:57.61ms
step:721/2330 train_time:41538ms step_avg:57.61ms
step:722/2330 train_time:41598ms step_avg:57.62ms
step:723/2330 train_time:41654ms step_avg:57.61ms
step:724/2330 train_time:41713ms step_avg:57.61ms
step:725/2330 train_time:41770ms step_avg:57.61ms
step:726/2330 train_time:41829ms step_avg:57.62ms
step:727/2330 train_time:41885ms step_avg:57.61ms
step:728/2330 train_time:41944ms step_avg:57.62ms
step:729/2330 train_time:42000ms step_avg:57.61ms
step:730/2330 train_time:42059ms step_avg:57.61ms
step:731/2330 train_time:42115ms step_avg:57.61ms
step:732/2330 train_time:42173ms step_avg:57.61ms
step:733/2330 train_time:42231ms step_avg:57.61ms
step:734/2330 train_time:42291ms step_avg:57.62ms
step:735/2330 train_time:42348ms step_avg:57.62ms
step:736/2330 train_time:42407ms step_avg:57.62ms
step:737/2330 train_time:42463ms step_avg:57.62ms
step:738/2330 train_time:42522ms step_avg:57.62ms
step:739/2330 train_time:42577ms step_avg:57.61ms
step:740/2330 train_time:42638ms step_avg:57.62ms
step:741/2330 train_time:42694ms step_avg:57.62ms
step:742/2330 train_time:42752ms step_avg:57.62ms
step:743/2330 train_time:42808ms step_avg:57.62ms
step:744/2330 train_time:42868ms step_avg:57.62ms
step:745/2330 train_time:42925ms step_avg:57.62ms
step:746/2330 train_time:42983ms step_avg:57.62ms
step:747/2330 train_time:43039ms step_avg:57.62ms
step:748/2330 train_time:43099ms step_avg:57.62ms
step:749/2330 train_time:43155ms step_avg:57.62ms
step:750/2330 train_time:43214ms step_avg:57.62ms
step:750/2330 val_loss:4.2109 train_time:43294ms step_avg:57.73ms
step:751/2330 train_time:43313ms step_avg:57.67ms
step:752/2330 train_time:43332ms step_avg:57.62ms
step:753/2330 train_time:43387ms step_avg:57.62ms
step:754/2330 train_time:43454ms step_avg:57.63ms
step:755/2330 train_time:43512ms step_avg:57.63ms
step:756/2330 train_time:43572ms step_avg:57.63ms
step:757/2330 train_time:43628ms step_avg:57.63ms
step:758/2330 train_time:43687ms step_avg:57.64ms
step:759/2330 train_time:43743ms step_avg:57.63ms
step:760/2330 train_time:43803ms step_avg:57.64ms
step:761/2330 train_time:43858ms step_avg:57.63ms
step:762/2330 train_time:43917ms step_avg:57.63ms
step:763/2330 train_time:43973ms step_avg:57.63ms
step:764/2330 train_time:44031ms step_avg:57.63ms
step:765/2330 train_time:44088ms step_avg:57.63ms
step:766/2330 train_time:44146ms step_avg:57.63ms
step:767/2330 train_time:44204ms step_avg:57.63ms
step:768/2330 train_time:44264ms step_avg:57.64ms
step:769/2330 train_time:44321ms step_avg:57.63ms
step:770/2330 train_time:44384ms step_avg:57.64ms
step:771/2330 train_time:44441ms step_avg:57.64ms
step:772/2330 train_time:44505ms step_avg:57.65ms
step:773/2330 train_time:44562ms step_avg:57.65ms
step:774/2330 train_time:44623ms step_avg:57.65ms
step:775/2330 train_time:44680ms step_avg:57.65ms
step:776/2330 train_time:44740ms step_avg:57.65ms
step:777/2330 train_time:44796ms step_avg:57.65ms
step:778/2330 train_time:44855ms step_avg:57.65ms
step:779/2330 train_time:44911ms step_avg:57.65ms
step:780/2330 train_time:44970ms step_avg:57.65ms
step:781/2330 train_time:45027ms step_avg:57.65ms
step:782/2330 train_time:45087ms step_avg:57.66ms
step:783/2330 train_time:45145ms step_avg:57.66ms
step:784/2330 train_time:45204ms step_avg:57.66ms
step:785/2330 train_time:45263ms step_avg:57.66ms
step:786/2330 train_time:45323ms step_avg:57.66ms
step:787/2330 train_time:45381ms step_avg:57.66ms
step:788/2330 train_time:45441ms step_avg:57.67ms
step:789/2330 train_time:45499ms step_avg:57.67ms
step:790/2330 train_time:45559ms step_avg:57.67ms
step:791/2330 train_time:45617ms step_avg:57.67ms
step:792/2330 train_time:45677ms step_avg:57.67ms
step:793/2330 train_time:45734ms step_avg:57.67ms
step:794/2330 train_time:45795ms step_avg:57.68ms
step:795/2330 train_time:45851ms step_avg:57.67ms
step:796/2330 train_time:45911ms step_avg:57.68ms
step:797/2330 train_time:45968ms step_avg:57.68ms
step:798/2330 train_time:46027ms step_avg:57.68ms
step:799/2330 train_time:46083ms step_avg:57.68ms
step:800/2330 train_time:46144ms step_avg:57.68ms
step:801/2330 train_time:46201ms step_avg:57.68ms
step:802/2330 train_time:46262ms step_avg:57.68ms
step:803/2330 train_time:46318ms step_avg:57.68ms
step:804/2330 train_time:46380ms step_avg:57.69ms
step:805/2330 train_time:46436ms step_avg:57.68ms
step:806/2330 train_time:46499ms step_avg:57.69ms
step:807/2330 train_time:46556ms step_avg:57.69ms
step:808/2330 train_time:46617ms step_avg:57.69ms
step:809/2330 train_time:46674ms step_avg:57.69ms
step:810/2330 train_time:46736ms step_avg:57.70ms
step:811/2330 train_time:46792ms step_avg:57.70ms
step:812/2330 train_time:46853ms step_avg:57.70ms
step:813/2330 train_time:46909ms step_avg:57.70ms
step:814/2330 train_time:46968ms step_avg:57.70ms
step:815/2330 train_time:47025ms step_avg:57.70ms
step:816/2330 train_time:47084ms step_avg:57.70ms
step:817/2330 train_time:47141ms step_avg:57.70ms
step:818/2330 train_time:47201ms step_avg:57.70ms
step:819/2330 train_time:47258ms step_avg:57.70ms
step:820/2330 train_time:47319ms step_avg:57.71ms
step:821/2330 train_time:47377ms step_avg:57.71ms
step:822/2330 train_time:47437ms step_avg:57.71ms
step:823/2330 train_time:47494ms step_avg:57.71ms
step:824/2330 train_time:47555ms step_avg:57.71ms
step:825/2330 train_time:47611ms step_avg:57.71ms
step:826/2330 train_time:47671ms step_avg:57.71ms
step:827/2330 train_time:47729ms step_avg:57.71ms
step:828/2330 train_time:47789ms step_avg:57.72ms
step:829/2330 train_time:47846ms step_avg:57.72ms
step:830/2330 train_time:47906ms step_avg:57.72ms
step:831/2330 train_time:47963ms step_avg:57.72ms
step:832/2330 train_time:48022ms step_avg:57.72ms
step:833/2330 train_time:48079ms step_avg:57.72ms
step:834/2330 train_time:48139ms step_avg:57.72ms
step:835/2330 train_time:48196ms step_avg:57.72ms
step:836/2330 train_time:48258ms step_avg:57.72ms
step:837/2330 train_time:48314ms step_avg:57.72ms
step:838/2330 train_time:48375ms step_avg:57.73ms
step:839/2330 train_time:48432ms step_avg:57.73ms
step:840/2330 train_time:48492ms step_avg:57.73ms
step:841/2330 train_time:48549ms step_avg:57.73ms
step:842/2330 train_time:48609ms step_avg:57.73ms
step:843/2330 train_time:48666ms step_avg:57.73ms
step:844/2330 train_time:48728ms step_avg:57.73ms
step:845/2330 train_time:48784ms step_avg:57.73ms
step:846/2330 train_time:48844ms step_avg:57.74ms
step:847/2330 train_time:48901ms step_avg:57.73ms
step:848/2330 train_time:48961ms step_avg:57.74ms
step:849/2330 train_time:49018ms step_avg:57.74ms
step:850/2330 train_time:49078ms step_avg:57.74ms
step:851/2330 train_time:49134ms step_avg:57.74ms
step:852/2330 train_time:49195ms step_avg:57.74ms
step:853/2330 train_time:49251ms step_avg:57.74ms
step:854/2330 train_time:49312ms step_avg:57.74ms
step:855/2330 train_time:49369ms step_avg:57.74ms
step:856/2330 train_time:49429ms step_avg:57.74ms
step:857/2330 train_time:49487ms step_avg:57.74ms
step:858/2330 train_time:49547ms step_avg:57.75ms
step:859/2330 train_time:49604ms step_avg:57.75ms
step:860/2330 train_time:49665ms step_avg:57.75ms
step:861/2330 train_time:49722ms step_avg:57.75ms
step:862/2330 train_time:49783ms step_avg:57.75ms
step:863/2330 train_time:49840ms step_avg:57.75ms
step:864/2330 train_time:49900ms step_avg:57.76ms
step:865/2330 train_time:49957ms step_avg:57.75ms
step:866/2330 train_time:50017ms step_avg:57.76ms
step:867/2330 train_time:50075ms step_avg:57.76ms
step:868/2330 train_time:50134ms step_avg:57.76ms
step:869/2330 train_time:50191ms step_avg:57.76ms
step:870/2330 train_time:50250ms step_avg:57.76ms
step:871/2330 train_time:50307ms step_avg:57.76ms
step:872/2330 train_time:50368ms step_avg:57.76ms
step:873/2330 train_time:50424ms step_avg:57.76ms
step:874/2330 train_time:50485ms step_avg:57.76ms
step:875/2330 train_time:50542ms step_avg:57.76ms
step:876/2330 train_time:50603ms step_avg:57.77ms
step:877/2330 train_time:50659ms step_avg:57.76ms
step:878/2330 train_time:50719ms step_avg:57.77ms
step:879/2330 train_time:50776ms step_avg:57.77ms
step:880/2330 train_time:50836ms step_avg:57.77ms
step:881/2330 train_time:50894ms step_avg:57.77ms
step:882/2330 train_time:50953ms step_avg:57.77ms
step:883/2330 train_time:51011ms step_avg:57.77ms
step:884/2330 train_time:51070ms step_avg:57.77ms
step:885/2330 train_time:51128ms step_avg:57.77ms
step:886/2330 train_time:51188ms step_avg:57.77ms
step:887/2330 train_time:51245ms step_avg:57.77ms
step:888/2330 train_time:51305ms step_avg:57.78ms
step:889/2330 train_time:51362ms step_avg:57.77ms
step:890/2330 train_time:51422ms step_avg:57.78ms
step:891/2330 train_time:51479ms step_avg:57.78ms
step:892/2330 train_time:51540ms step_avg:57.78ms
step:893/2330 train_time:51597ms step_avg:57.78ms
step:894/2330 train_time:51658ms step_avg:57.78ms
step:895/2330 train_time:51715ms step_avg:57.78ms
step:896/2330 train_time:51775ms step_avg:57.79ms
step:897/2330 train_time:51832ms step_avg:57.78ms
step:898/2330 train_time:51892ms step_avg:57.79ms
step:899/2330 train_time:51949ms step_avg:57.79ms
step:900/2330 train_time:52009ms step_avg:57.79ms
step:901/2330 train_time:52066ms step_avg:57.79ms
step:902/2330 train_time:52127ms step_avg:57.79ms
step:903/2330 train_time:52184ms step_avg:57.79ms
step:904/2330 train_time:52245ms step_avg:57.79ms
step:905/2330 train_time:52302ms step_avg:57.79ms
step:906/2330 train_time:52362ms step_avg:57.79ms
step:907/2330 train_time:52419ms step_avg:57.79ms
step:908/2330 train_time:52480ms step_avg:57.80ms
step:909/2330 train_time:52536ms step_avg:57.80ms
step:910/2330 train_time:52596ms step_avg:57.80ms
step:911/2330 train_time:52653ms step_avg:57.80ms
step:912/2330 train_time:52715ms step_avg:57.80ms
step:913/2330 train_time:52771ms step_avg:57.80ms
step:914/2330 train_time:52832ms step_avg:57.80ms
step:915/2330 train_time:52889ms step_avg:57.80ms
step:916/2330 train_time:52949ms step_avg:57.80ms
step:917/2330 train_time:53006ms step_avg:57.80ms
step:918/2330 train_time:53066ms step_avg:57.81ms
step:919/2330 train_time:53123ms step_avg:57.80ms
step:920/2330 train_time:53183ms step_avg:57.81ms
step:921/2330 train_time:53241ms step_avg:57.81ms
step:922/2330 train_time:53300ms step_avg:57.81ms
step:923/2330 train_time:53358ms step_avg:57.81ms
step:924/2330 train_time:53417ms step_avg:57.81ms
step:925/2330 train_time:53475ms step_avg:57.81ms
step:926/2330 train_time:53535ms step_avg:57.81ms
step:927/2330 train_time:53592ms step_avg:57.81ms
step:928/2330 train_time:53652ms step_avg:57.81ms
step:929/2330 train_time:53709ms step_avg:57.81ms
step:930/2330 train_time:53769ms step_avg:57.82ms
step:931/2330 train_time:53825ms step_avg:57.81ms
step:932/2330 train_time:53886ms step_avg:57.82ms
step:933/2330 train_time:53943ms step_avg:57.82ms
step:934/2330 train_time:54004ms step_avg:57.82ms
step:935/2330 train_time:54060ms step_avg:57.82ms
step:936/2330 train_time:54121ms step_avg:57.82ms
step:937/2330 train_time:54178ms step_avg:57.82ms
step:938/2330 train_time:54238ms step_avg:57.82ms
step:939/2330 train_time:54295ms step_avg:57.82ms
step:940/2330 train_time:54355ms step_avg:57.82ms
step:941/2330 train_time:54412ms step_avg:57.82ms
step:942/2330 train_time:54472ms step_avg:57.83ms
step:943/2330 train_time:54528ms step_avg:57.82ms
step:944/2330 train_time:54590ms step_avg:57.83ms
step:945/2330 train_time:54648ms step_avg:57.83ms
step:946/2330 train_time:54707ms step_avg:57.83ms
step:947/2330 train_time:54764ms step_avg:57.83ms
step:948/2330 train_time:54824ms step_avg:57.83ms
step:949/2330 train_time:54881ms step_avg:57.83ms
step:950/2330 train_time:54942ms step_avg:57.83ms
step:951/2330 train_time:54998ms step_avg:57.83ms
step:952/2330 train_time:55059ms step_avg:57.83ms
step:953/2330 train_time:55115ms step_avg:57.83ms
step:954/2330 train_time:55176ms step_avg:57.84ms
step:955/2330 train_time:55233ms step_avg:57.84ms
step:956/2330 train_time:55293ms step_avg:57.84ms
step:957/2330 train_time:55351ms step_avg:57.84ms
step:958/2330 train_time:55411ms step_avg:57.84ms
step:959/2330 train_time:55468ms step_avg:57.84ms
step:960/2330 train_time:55529ms step_avg:57.84ms
step:961/2330 train_time:55587ms step_avg:57.84ms
step:962/2330 train_time:55646ms step_avg:57.84ms
step:963/2330 train_time:55703ms step_avg:57.84ms
step:964/2330 train_time:55763ms step_avg:57.85ms
step:965/2330 train_time:55820ms step_avg:57.84ms
step:966/2330 train_time:55880ms step_avg:57.85ms
step:967/2330 train_time:55937ms step_avg:57.85ms
step:968/2330 train_time:55998ms step_avg:57.85ms
step:969/2330 train_time:56055ms step_avg:57.85ms
step:970/2330 train_time:56115ms step_avg:57.85ms
step:971/2330 train_time:56172ms step_avg:57.85ms
step:972/2330 train_time:56232ms step_avg:57.85ms
step:973/2330 train_time:56289ms step_avg:57.85ms
step:974/2330 train_time:56349ms step_avg:57.85ms
step:975/2330 train_time:56406ms step_avg:57.85ms
step:976/2330 train_time:56466ms step_avg:57.85ms
step:977/2330 train_time:56523ms step_avg:57.85ms
step:978/2330 train_time:56583ms step_avg:57.86ms
step:979/2330 train_time:56640ms step_avg:57.85ms
step:980/2330 train_time:56700ms step_avg:57.86ms
step:981/2330 train_time:56757ms step_avg:57.86ms
step:982/2330 train_time:56816ms step_avg:57.86ms
step:983/2330 train_time:56874ms step_avg:57.86ms
step:984/2330 train_time:56934ms step_avg:57.86ms
step:985/2330 train_time:56992ms step_avg:57.86ms
step:986/2330 train_time:57052ms step_avg:57.86ms
step:987/2330 train_time:57109ms step_avg:57.86ms
step:988/2330 train_time:57169ms step_avg:57.86ms
step:989/2330 train_time:57226ms step_avg:57.86ms
step:990/2330 train_time:57286ms step_avg:57.86ms
step:991/2330 train_time:57343ms step_avg:57.86ms
step:992/2330 train_time:57403ms step_avg:57.87ms
step:993/2330 train_time:57460ms step_avg:57.87ms
step:994/2330 train_time:57522ms step_avg:57.87ms
step:995/2330 train_time:57578ms step_avg:57.87ms
step:996/2330 train_time:57639ms step_avg:57.87ms
step:997/2330 train_time:57695ms step_avg:57.87ms
step:998/2330 train_time:57756ms step_avg:57.87ms
step:999/2330 train_time:57812ms step_avg:57.87ms
step:1000/2330 train_time:57873ms step_avg:57.87ms
step:1000/2330 val_loss:4.0713 train_time:57954ms step_avg:57.95ms
step:1001/2330 train_time:57974ms step_avg:57.92ms
step:1002/2330 train_time:57995ms step_avg:57.88ms
step:1003/2330 train_time:58047ms step_avg:57.87ms
step:1004/2330 train_time:58113ms step_avg:57.88ms
step:1005/2330 train_time:58169ms step_avg:57.88ms
step:1006/2330 train_time:58233ms step_avg:57.89ms
step:1007/2330 train_time:58289ms step_avg:57.88ms
step:1008/2330 train_time:58349ms step_avg:57.89ms
step:1009/2330 train_time:58405ms step_avg:57.88ms
step:1010/2330 train_time:58464ms step_avg:57.89ms
step:1011/2330 train_time:58521ms step_avg:57.88ms
step:1012/2330 train_time:58580ms step_avg:57.89ms
step:1013/2330 train_time:58636ms step_avg:57.88ms
step:1014/2330 train_time:58695ms step_avg:57.88ms
step:1015/2330 train_time:58752ms step_avg:57.88ms
step:1016/2330 train_time:58810ms step_avg:57.88ms
step:1017/2330 train_time:58868ms step_avg:57.88ms
step:1018/2330 train_time:58928ms step_avg:57.89ms
step:1019/2330 train_time:58986ms step_avg:57.89ms
step:1020/2330 train_time:59048ms step_avg:57.89ms
step:1021/2330 train_time:59105ms step_avg:57.89ms
step:1022/2330 train_time:59167ms step_avg:57.89ms
step:1023/2330 train_time:59223ms step_avg:57.89ms
step:1024/2330 train_time:59284ms step_avg:57.89ms
step:1025/2330 train_time:59340ms step_avg:57.89ms
step:1026/2330 train_time:59400ms step_avg:57.89ms
step:1027/2330 train_time:59457ms step_avg:57.89ms
step:1028/2330 train_time:59517ms step_avg:57.90ms
step:1029/2330 train_time:59574ms step_avg:57.89ms
step:1030/2330 train_time:59633ms step_avg:57.90ms
step:1031/2330 train_time:59689ms step_avg:57.89ms
step:1032/2330 train_time:59749ms step_avg:57.90ms
step:1033/2330 train_time:59806ms step_avg:57.90ms
step:1034/2330 train_time:59866ms step_avg:57.90ms
step:1035/2330 train_time:59924ms step_avg:57.90ms
step:1036/2330 train_time:59984ms step_avg:57.90ms
step:1037/2330 train_time:60041ms step_avg:57.90ms
step:1038/2330 train_time:60102ms step_avg:57.90ms
step:1039/2330 train_time:60160ms step_avg:57.90ms
step:1040/2330 train_time:60220ms step_avg:57.90ms
step:1041/2330 train_time:60278ms step_avg:57.90ms
step:1042/2330 train_time:60338ms step_avg:57.91ms
step:1043/2330 train_time:60395ms step_avg:57.91ms
step:1044/2330 train_time:60456ms step_avg:57.91ms
step:1045/2330 train_time:60512ms step_avg:57.91ms
step:1046/2330 train_time:60572ms step_avg:57.91ms
step:1047/2330 train_time:60628ms step_avg:57.91ms
step:1048/2330 train_time:60688ms step_avg:57.91ms
step:1049/2330 train_time:60745ms step_avg:57.91ms
step:1050/2330 train_time:60804ms step_avg:57.91ms
step:1051/2330 train_time:60862ms step_avg:57.91ms
step:1052/2330 train_time:60921ms step_avg:57.91ms
step:1053/2330 train_time:60979ms step_avg:57.91ms
step:1054/2330 train_time:61040ms step_avg:57.91ms
step:1055/2330 train_time:61098ms step_avg:57.91ms
step:1056/2330 train_time:61158ms step_avg:57.92ms
step:1057/2330 train_time:61216ms step_avg:57.91ms
step:1058/2330 train_time:61276ms step_avg:57.92ms
step:1059/2330 train_time:61333ms step_avg:57.92ms
step:1060/2330 train_time:61393ms step_avg:57.92ms
step:1061/2330 train_time:61449ms step_avg:57.92ms
step:1062/2330 train_time:61510ms step_avg:57.92ms
step:1063/2330 train_time:61566ms step_avg:57.92ms
step:1064/2330 train_time:61625ms step_avg:57.92ms
step:1065/2330 train_time:61682ms step_avg:57.92ms
step:1066/2330 train_time:61742ms step_avg:57.92ms
step:1067/2330 train_time:61799ms step_avg:57.92ms
step:1068/2330 train_time:61859ms step_avg:57.92ms
step:1069/2330 train_time:61915ms step_avg:57.92ms
step:1070/2330 train_time:61976ms step_avg:57.92ms
step:1071/2330 train_time:62032ms step_avg:57.92ms
step:1072/2330 train_time:62093ms step_avg:57.92ms
step:1073/2330 train_time:62149ms step_avg:57.92ms
step:1074/2330 train_time:62209ms step_avg:57.92ms
step:1075/2330 train_time:62266ms step_avg:57.92ms
step:1076/2330 train_time:62327ms step_avg:57.92ms
step:1077/2330 train_time:62383ms step_avg:57.92ms
step:1078/2330 train_time:62443ms step_avg:57.93ms
step:1079/2330 train_time:62501ms step_avg:57.92ms
step:1080/2330 train_time:62561ms step_avg:57.93ms
step:1081/2330 train_time:62617ms step_avg:57.92ms
step:1082/2330 train_time:62677ms step_avg:57.93ms
step:1083/2330 train_time:62733ms step_avg:57.93ms
step:1084/2330 train_time:62794ms step_avg:57.93ms
step:1085/2330 train_time:62851ms step_avg:57.93ms
step:1086/2330 train_time:62910ms step_avg:57.93ms
step:1087/2330 train_time:62967ms step_avg:57.93ms
step:1088/2330 train_time:63027ms step_avg:57.93ms
step:1089/2330 train_time:63085ms step_avg:57.93ms
step:1090/2330 train_time:63145ms step_avg:57.93ms
step:1091/2330 train_time:63202ms step_avg:57.93ms
step:1092/2330 train_time:63263ms step_avg:57.93ms
step:1093/2330 train_time:63320ms step_avg:57.93ms
step:1094/2330 train_time:63380ms step_avg:57.93ms
step:1095/2330 train_time:63438ms step_avg:57.93ms
step:1096/2330 train_time:63498ms step_avg:57.94ms
step:1097/2330 train_time:63555ms step_avg:57.94ms
step:1098/2330 train_time:63615ms step_avg:57.94ms
step:1099/2330 train_time:63672ms step_avg:57.94ms
step:1100/2330 train_time:63731ms step_avg:57.94ms
step:1101/2330 train_time:63787ms step_avg:57.94ms
step:1102/2330 train_time:63848ms step_avg:57.94ms
step:1103/2330 train_time:63904ms step_avg:57.94ms
step:1104/2330 train_time:63964ms step_avg:57.94ms
step:1105/2330 train_time:64021ms step_avg:57.94ms
step:1106/2330 train_time:64081ms step_avg:57.94ms
step:1107/2330 train_time:64139ms step_avg:57.94ms
step:1108/2330 train_time:64199ms step_avg:57.94ms
step:1109/2330 train_time:64256ms step_avg:57.94ms
step:1110/2330 train_time:64316ms step_avg:57.94ms
step:1111/2330 train_time:64374ms step_avg:57.94ms
step:1112/2330 train_time:64433ms step_avg:57.94ms
step:1113/2330 train_time:64489ms step_avg:57.94ms
step:1114/2330 train_time:64550ms step_avg:57.94ms
step:1115/2330 train_time:64606ms step_avg:57.94ms
step:1116/2330 train_time:64666ms step_avg:57.94ms
step:1117/2330 train_time:64723ms step_avg:57.94ms
step:1118/2330 train_time:64783ms step_avg:57.95ms
step:1119/2330 train_time:64840ms step_avg:57.94ms
step:1120/2330 train_time:64900ms step_avg:57.95ms
step:1121/2330 train_time:64956ms step_avg:57.94ms
step:1122/2330 train_time:65016ms step_avg:57.95ms
step:1123/2330 train_time:65072ms step_avg:57.95ms
step:1124/2330 train_time:65134ms step_avg:57.95ms
step:1125/2330 train_time:65190ms step_avg:57.95ms
step:1126/2330 train_time:65251ms step_avg:57.95ms
step:1127/2330 train_time:65308ms step_avg:57.95ms
step:1128/2330 train_time:65369ms step_avg:57.95ms
step:1129/2330 train_time:65426ms step_avg:57.95ms
step:1130/2330 train_time:65486ms step_avg:57.95ms
step:1131/2330 train_time:65542ms step_avg:57.95ms
step:1132/2330 train_time:65602ms step_avg:57.95ms
step:1133/2330 train_time:65659ms step_avg:57.95ms
step:1134/2330 train_time:65720ms step_avg:57.95ms
step:1135/2330 train_time:65777ms step_avg:57.95ms
step:1136/2330 train_time:65837ms step_avg:57.96ms
step:1137/2330 train_time:65894ms step_avg:57.95ms
step:1138/2330 train_time:65953ms step_avg:57.96ms
step:1139/2330 train_time:66009ms step_avg:57.95ms
step:1140/2330 train_time:66070ms step_avg:57.96ms
step:1141/2330 train_time:66127ms step_avg:57.96ms
step:1142/2330 train_time:66187ms step_avg:57.96ms
step:1143/2330 train_time:66244ms step_avg:57.96ms
step:1144/2330 train_time:66304ms step_avg:57.96ms
step:1145/2330 train_time:66362ms step_avg:57.96ms
step:1146/2330 train_time:66421ms step_avg:57.96ms
step:1147/2330 train_time:66478ms step_avg:57.96ms
step:1148/2330 train_time:66540ms step_avg:57.96ms
step:1149/2330 train_time:66597ms step_avg:57.96ms
step:1150/2330 train_time:66657ms step_avg:57.96ms
step:1151/2330 train_time:66714ms step_avg:57.96ms
step:1152/2330 train_time:66773ms step_avg:57.96ms
step:1153/2330 train_time:66830ms step_avg:57.96ms
step:1154/2330 train_time:66890ms step_avg:57.96ms
step:1155/2330 train_time:66947ms step_avg:57.96ms
step:1156/2330 train_time:67006ms step_avg:57.96ms
step:1157/2330 train_time:67063ms step_avg:57.96ms
step:1158/2330 train_time:67123ms step_avg:57.96ms
step:1159/2330 train_time:67180ms step_avg:57.96ms
step:1160/2330 train_time:67241ms step_avg:57.97ms
step:1161/2330 train_time:67298ms step_avg:57.97ms
step:1162/2330 train_time:67359ms step_avg:57.97ms
step:1163/2330 train_time:67416ms step_avg:57.97ms
step:1164/2330 train_time:67476ms step_avg:57.97ms
step:1165/2330 train_time:67533ms step_avg:57.97ms
step:1166/2330 train_time:67592ms step_avg:57.97ms
step:1167/2330 train_time:67650ms step_avg:57.97ms
step:1168/2330 train_time:67710ms step_avg:57.97ms
step:1169/2330 train_time:67766ms step_avg:57.97ms
step:1170/2330 train_time:67826ms step_avg:57.97ms
step:1171/2330 train_time:67883ms step_avg:57.97ms
step:1172/2330 train_time:67943ms step_avg:57.97ms
step:1173/2330 train_time:68000ms step_avg:57.97ms
step:1174/2330 train_time:68060ms step_avg:57.97ms
step:1175/2330 train_time:68117ms step_avg:57.97ms
step:1176/2330 train_time:68177ms step_avg:57.97ms
step:1177/2330 train_time:68234ms step_avg:57.97ms
step:1178/2330 train_time:68293ms step_avg:57.97ms
step:1179/2330 train_time:68350ms step_avg:57.97ms
step:1180/2330 train_time:68409ms step_avg:57.97ms
step:1181/2330 train_time:68466ms step_avg:57.97ms
step:1182/2330 train_time:68526ms step_avg:57.97ms
step:1183/2330 train_time:68583ms step_avg:57.97ms
step:1184/2330 train_time:68644ms step_avg:57.98ms
step:1185/2330 train_time:68701ms step_avg:57.98ms
step:1186/2330 train_time:68761ms step_avg:57.98ms
step:1187/2330 train_time:68818ms step_avg:57.98ms
step:1188/2330 train_time:68878ms step_avg:57.98ms
step:1189/2330 train_time:68935ms step_avg:57.98ms
step:1190/2330 train_time:68994ms step_avg:57.98ms
step:1191/2330 train_time:69051ms step_avg:57.98ms
step:1192/2330 train_time:69111ms step_avg:57.98ms
step:1193/2330 train_time:69168ms step_avg:57.98ms
step:1194/2330 train_time:69228ms step_avg:57.98ms
step:1195/2330 train_time:69285ms step_avg:57.98ms
step:1196/2330 train_time:69345ms step_avg:57.98ms
step:1197/2330 train_time:69402ms step_avg:57.98ms
step:1198/2330 train_time:69462ms step_avg:57.98ms
step:1199/2330 train_time:69520ms step_avg:57.98ms
step:1200/2330 train_time:69580ms step_avg:57.98ms
step:1201/2330 train_time:69637ms step_avg:57.98ms
step:1202/2330 train_time:69697ms step_avg:57.98ms
step:1203/2330 train_time:69754ms step_avg:57.98ms
step:1204/2330 train_time:69814ms step_avg:57.98ms
step:1205/2330 train_time:69870ms step_avg:57.98ms
step:1206/2330 train_time:69931ms step_avg:57.99ms
step:1207/2330 train_time:69988ms step_avg:57.98ms
step:1208/2330 train_time:70047ms step_avg:57.99ms
step:1209/2330 train_time:70104ms step_avg:57.99ms
step:1210/2330 train_time:70164ms step_avg:57.99ms
step:1211/2330 train_time:70221ms step_avg:57.99ms
step:1212/2330 train_time:70282ms step_avg:57.99ms
step:1213/2330 train_time:70339ms step_avg:57.99ms
step:1214/2330 train_time:70399ms step_avg:57.99ms
step:1215/2330 train_time:70456ms step_avg:57.99ms
step:1216/2330 train_time:70517ms step_avg:57.99ms
step:1217/2330 train_time:70574ms step_avg:57.99ms
step:1218/2330 train_time:70634ms step_avg:57.99ms
step:1219/2330 train_time:70691ms step_avg:57.99ms
step:1220/2330 train_time:70751ms step_avg:57.99ms
step:1221/2330 train_time:70808ms step_avg:57.99ms
step:1222/2330 train_time:70868ms step_avg:57.99ms
step:1223/2330 train_time:70924ms step_avg:57.99ms
step:1224/2330 train_time:70985ms step_avg:57.99ms
step:1225/2330 train_time:71042ms step_avg:57.99ms
step:1226/2330 train_time:71102ms step_avg:57.99ms
step:1227/2330 train_time:71159ms step_avg:57.99ms
step:1228/2330 train_time:71219ms step_avg:58.00ms
step:1229/2330 train_time:71276ms step_avg:58.00ms
step:1230/2330 train_time:71336ms step_avg:58.00ms
step:1231/2330 train_time:71393ms step_avg:58.00ms
step:1232/2330 train_time:71453ms step_avg:58.00ms
step:1233/2330 train_time:71509ms step_avg:58.00ms
step:1234/2330 train_time:71570ms step_avg:58.00ms
step:1235/2330 train_time:71626ms step_avg:58.00ms
step:1236/2330 train_time:71686ms step_avg:58.00ms
step:1237/2330 train_time:71743ms step_avg:58.00ms
step:1238/2330 train_time:71803ms step_avg:58.00ms
step:1239/2330 train_time:71861ms step_avg:58.00ms
step:1240/2330 train_time:71921ms step_avg:58.00ms
step:1241/2330 train_time:71978ms step_avg:58.00ms
step:1242/2330 train_time:72038ms step_avg:58.00ms
step:1243/2330 train_time:72095ms step_avg:58.00ms
step:1244/2330 train_time:72155ms step_avg:58.00ms
step:1245/2330 train_time:72212ms step_avg:58.00ms
step:1246/2330 train_time:72272ms step_avg:58.00ms
step:1247/2330 train_time:72329ms step_avg:58.00ms
step:1248/2330 train_time:72389ms step_avg:58.00ms
step:1249/2330 train_time:72447ms step_avg:58.00ms
step:1250/2330 train_time:72506ms step_avg:58.00ms
step:1250/2330 val_loss:3.9933 train_time:72586ms step_avg:58.07ms
step:1251/2330 train_time:72605ms step_avg:58.04ms
step:1252/2330 train_time:72625ms step_avg:58.01ms
step:1253/2330 train_time:72684ms step_avg:58.01ms
step:1254/2330 train_time:72751ms step_avg:58.01ms
step:1255/2330 train_time:72807ms step_avg:58.01ms
step:1256/2330 train_time:72870ms step_avg:58.02ms
step:1257/2330 train_time:72926ms step_avg:58.02ms
step:1258/2330 train_time:72987ms step_avg:58.02ms
step:1259/2330 train_time:73043ms step_avg:58.02ms
step:1260/2330 train_time:73103ms step_avg:58.02ms
step:1261/2330 train_time:73158ms step_avg:58.02ms
step:1262/2330 train_time:73218ms step_avg:58.02ms
step:1263/2330 train_time:73274ms step_avg:58.02ms
step:1264/2330 train_time:73333ms step_avg:58.02ms
step:1265/2330 train_time:73390ms step_avg:58.02ms
step:1266/2330 train_time:73448ms step_avg:58.02ms
step:1267/2330 train_time:73505ms step_avg:58.01ms
step:1268/2330 train_time:73566ms step_avg:58.02ms
step:1269/2330 train_time:73624ms step_avg:58.02ms
step:1270/2330 train_time:73686ms step_avg:58.02ms
step:1271/2330 train_time:73744ms step_avg:58.02ms
step:1272/2330 train_time:73804ms step_avg:58.02ms
step:1273/2330 train_time:73861ms step_avg:58.02ms
step:1274/2330 train_time:73921ms step_avg:58.02ms
step:1275/2330 train_time:73978ms step_avg:58.02ms
step:1276/2330 train_time:74038ms step_avg:58.02ms
step:1277/2330 train_time:74095ms step_avg:58.02ms
step:1278/2330 train_time:74155ms step_avg:58.02ms
step:1279/2330 train_time:74210ms step_avg:58.02ms
step:1280/2330 train_time:74271ms step_avg:58.02ms
step:1281/2330 train_time:74327ms step_avg:58.02ms
step:1282/2330 train_time:74387ms step_avg:58.02ms
step:1283/2330 train_time:74443ms step_avg:58.02ms
step:1284/2330 train_time:74503ms step_avg:58.02ms
step:1285/2330 train_time:74561ms step_avg:58.02ms
step:1286/2330 train_time:74621ms step_avg:58.03ms
step:1287/2330 train_time:74679ms step_avg:58.03ms
step:1288/2330 train_time:74739ms step_avg:58.03ms
step:1289/2330 train_time:74796ms step_avg:58.03ms
step:1290/2330 train_time:74857ms step_avg:58.03ms
step:1291/2330 train_time:74914ms step_avg:58.03ms
step:1292/2330 train_time:74975ms step_avg:58.03ms
step:1293/2330 train_time:75033ms step_avg:58.03ms
step:1294/2330 train_time:75093ms step_avg:58.03ms
step:1295/2330 train_time:75149ms step_avg:58.03ms
step:1296/2330 train_time:75209ms step_avg:58.03ms
step:1297/2330 train_time:75266ms step_avg:58.03ms
step:1298/2330 train_time:75325ms step_avg:58.03ms
step:1299/2330 train_time:75382ms step_avg:58.03ms
step:1300/2330 train_time:75441ms step_avg:58.03ms
step:1301/2330 train_time:75498ms step_avg:58.03ms
step:1302/2330 train_time:75559ms step_avg:58.03ms
step:1303/2330 train_time:75616ms step_avg:58.03ms
step:1304/2330 train_time:75678ms step_avg:58.03ms
step:1305/2330 train_time:75735ms step_avg:58.03ms
step:1306/2330 train_time:75795ms step_avg:58.04ms
step:1307/2330 train_time:75852ms step_avg:58.04ms
step:1308/2330 train_time:75912ms step_avg:58.04ms
step:1309/2330 train_time:75969ms step_avg:58.04ms
step:1310/2330 train_time:76031ms step_avg:58.04ms
step:1311/2330 train_time:76087ms step_avg:58.04ms
step:1312/2330 train_time:76147ms step_avg:58.04ms
step:1313/2330 train_time:76203ms step_avg:58.04ms
step:1314/2330 train_time:76264ms step_avg:58.04ms
step:1315/2330 train_time:76321ms step_avg:58.04ms
step:1316/2330 train_time:76381ms step_avg:58.04ms
step:1317/2330 train_time:76438ms step_avg:58.04ms
step:1318/2330 train_time:76498ms step_avg:58.04ms
step:1319/2330 train_time:76555ms step_avg:58.04ms
step:1320/2330 train_time:76615ms step_avg:58.04ms
step:1321/2330 train_time:76672ms step_avg:58.04ms
step:1322/2330 train_time:76733ms step_avg:58.04ms
step:1323/2330 train_time:76790ms step_avg:58.04ms
step:1324/2330 train_time:76850ms step_avg:58.04ms
step:1325/2330 train_time:76907ms step_avg:58.04ms
step:1326/2330 train_time:76967ms step_avg:58.04ms
step:1327/2330 train_time:77025ms step_avg:58.04ms
step:1328/2330 train_time:77086ms step_avg:58.05ms
step:1329/2330 train_time:77142ms step_avg:58.05ms
step:1330/2330 train_time:77202ms step_avg:58.05ms
step:1331/2330 train_time:77258ms step_avg:58.05ms
step:1332/2330 train_time:77319ms step_avg:58.05ms
step:1333/2330 train_time:77375ms step_avg:58.05ms
step:1334/2330 train_time:77435ms step_avg:58.05ms
step:1335/2330 train_time:77492ms step_avg:58.05ms
step:1336/2330 train_time:77553ms step_avg:58.05ms
step:1337/2330 train_time:77609ms step_avg:58.05ms
step:1338/2330 train_time:77669ms step_avg:58.05ms
step:1339/2330 train_time:77726ms step_avg:58.05ms
step:1340/2330 train_time:77787ms step_avg:58.05ms
step:1341/2330 train_time:77844ms step_avg:58.05ms
step:1342/2330 train_time:77905ms step_avg:58.05ms
step:1343/2330 train_time:77961ms step_avg:58.05ms
step:1344/2330 train_time:78022ms step_avg:58.05ms
step:1345/2330 train_time:78079ms step_avg:58.05ms
step:1346/2330 train_time:78138ms step_avg:58.05ms
step:1347/2330 train_time:78195ms step_avg:58.05ms
step:1348/2330 train_time:78255ms step_avg:58.05ms
step:1349/2330 train_time:78312ms step_avg:58.05ms
step:1350/2330 train_time:78372ms step_avg:58.05ms
step:1351/2330 train_time:78429ms step_avg:58.05ms
step:1352/2330 train_time:78489ms step_avg:58.05ms
step:1353/2330 train_time:78545ms step_avg:58.05ms
step:1354/2330 train_time:78606ms step_avg:58.05ms
step:1355/2330 train_time:78663ms step_avg:58.05ms
step:1356/2330 train_time:78722ms step_avg:58.05ms
step:1357/2330 train_time:78780ms step_avg:58.05ms
step:1358/2330 train_time:78840ms step_avg:58.06ms
step:1359/2330 train_time:78897ms step_avg:58.06ms
step:1360/2330 train_time:78958ms step_avg:58.06ms
step:1361/2330 train_time:79014ms step_avg:58.06ms
step:1362/2330 train_time:79075ms step_avg:58.06ms
step:1363/2330 train_time:79133ms step_avg:58.06ms
step:1364/2330 train_time:79193ms step_avg:58.06ms
step:1365/2330 train_time:79250ms step_avg:58.06ms
step:1366/2330 train_time:79310ms step_avg:58.06ms
step:1367/2330 train_time:79366ms step_avg:58.06ms
step:1368/2330 train_time:79426ms step_avg:58.06ms
step:1369/2330 train_time:79483ms step_avg:58.06ms
step:1370/2330 train_time:79543ms step_avg:58.06ms
step:1371/2330 train_time:79599ms step_avg:58.06ms
step:1372/2330 train_time:79660ms step_avg:58.06ms
step:1373/2330 train_time:79717ms step_avg:58.06ms
step:1374/2330 train_time:79777ms step_avg:58.06ms
step:1375/2330 train_time:79834ms step_avg:58.06ms
step:1376/2330 train_time:79894ms step_avg:58.06ms
step:1377/2330 train_time:79952ms step_avg:58.06ms
step:1378/2330 train_time:80011ms step_avg:58.06ms
step:1379/2330 train_time:80068ms step_avg:58.06ms
step:1380/2330 train_time:80128ms step_avg:58.06ms
step:1381/2330 train_time:80185ms step_avg:58.06ms
step:1382/2330 train_time:80245ms step_avg:58.06ms
step:1383/2330 train_time:80301ms step_avg:58.06ms
step:1384/2330 train_time:80362ms step_avg:58.07ms
step:1385/2330 train_time:80419ms step_avg:58.06ms
step:1386/2330 train_time:80480ms step_avg:58.07ms
step:1387/2330 train_time:80537ms step_avg:58.07ms
step:1388/2330 train_time:80597ms step_avg:58.07ms
step:1389/2330 train_time:80654ms step_avg:58.07ms
step:1390/2330 train_time:80715ms step_avg:58.07ms
step:1391/2330 train_time:80772ms step_avg:58.07ms
step:1392/2330 train_time:80832ms step_avg:58.07ms
step:1393/2330 train_time:80888ms step_avg:58.07ms
step:1394/2330 train_time:80949ms step_avg:58.07ms
step:1395/2330 train_time:81006ms step_avg:58.07ms
step:1396/2330 train_time:81066ms step_avg:58.07ms
step:1397/2330 train_time:81122ms step_avg:58.07ms
step:1398/2330 train_time:81183ms step_avg:58.07ms
step:1399/2330 train_time:81240ms step_avg:58.07ms
step:1400/2330 train_time:81300ms step_avg:58.07ms
step:1401/2330 train_time:81357ms step_avg:58.07ms
step:1402/2330 train_time:81417ms step_avg:58.07ms
step:1403/2330 train_time:81474ms step_avg:58.07ms
step:1404/2330 train_time:81535ms step_avg:58.07ms
step:1405/2330 train_time:81591ms step_avg:58.07ms
step:1406/2330 train_time:81652ms step_avg:58.07ms
step:1407/2330 train_time:81708ms step_avg:58.07ms
step:1408/2330 train_time:81769ms step_avg:58.07ms
step:1409/2330 train_time:81826ms step_avg:58.07ms
step:1410/2330 train_time:81885ms step_avg:58.07ms
step:1411/2330 train_time:81943ms step_avg:58.07ms
step:1412/2330 train_time:82002ms step_avg:58.08ms
step:1413/2330 train_time:82060ms step_avg:58.07ms
step:1414/2330 train_time:82120ms step_avg:58.08ms
step:1415/2330 train_time:82177ms step_avg:58.08ms
step:1416/2330 train_time:82237ms step_avg:58.08ms
step:1417/2330 train_time:82295ms step_avg:58.08ms
step:1418/2330 train_time:82354ms step_avg:58.08ms
step:1419/2330 train_time:82411ms step_avg:58.08ms
step:1420/2330 train_time:82471ms step_avg:58.08ms
step:1421/2330 train_time:82529ms step_avg:58.08ms
step:1422/2330 train_time:82588ms step_avg:58.08ms
step:1423/2330 train_time:82645ms step_avg:58.08ms
step:1424/2330 train_time:82705ms step_avg:58.08ms
step:1425/2330 train_time:82762ms step_avg:58.08ms
step:1426/2330 train_time:82822ms step_avg:58.08ms
step:1427/2330 train_time:82879ms step_avg:58.08ms
step:1428/2330 train_time:82939ms step_avg:58.08ms
step:1429/2330 train_time:82996ms step_avg:58.08ms
step:1430/2330 train_time:83056ms step_avg:58.08ms
step:1431/2330 train_time:83112ms step_avg:58.08ms
step:1432/2330 train_time:83173ms step_avg:58.08ms
step:1433/2330 train_time:83229ms step_avg:58.08ms
step:1434/2330 train_time:83291ms step_avg:58.08ms
step:1435/2330 train_time:83348ms step_avg:58.08ms
step:1436/2330 train_time:83408ms step_avg:58.08ms
step:1437/2330 train_time:83464ms step_avg:58.08ms
step:1438/2330 train_time:83525ms step_avg:58.08ms
step:1439/2330 train_time:83582ms step_avg:58.08ms
step:1440/2330 train_time:83642ms step_avg:58.08ms
step:1441/2330 train_time:83698ms step_avg:58.08ms
step:1442/2330 train_time:83759ms step_avg:58.08ms
step:1443/2330 train_time:83816ms step_avg:58.08ms
step:1444/2330 train_time:83876ms step_avg:58.09ms
step:1445/2330 train_time:83933ms step_avg:58.09ms
step:1446/2330 train_time:83993ms step_avg:58.09ms
step:1447/2330 train_time:84050ms step_avg:58.09ms
step:1448/2330 train_time:84110ms step_avg:58.09ms
step:1449/2330 train_time:84166ms step_avg:58.09ms
step:1450/2330 train_time:84228ms step_avg:58.09ms
step:1451/2330 train_time:84284ms step_avg:58.09ms
step:1452/2330 train_time:84344ms step_avg:58.09ms
step:1453/2330 train_time:84401ms step_avg:58.09ms
step:1454/2330 train_time:84461ms step_avg:58.09ms
step:1455/2330 train_time:84518ms step_avg:58.09ms
step:1456/2330 train_time:84579ms step_avg:58.09ms
step:1457/2330 train_time:84636ms step_avg:58.09ms
step:1458/2330 train_time:84697ms step_avg:58.09ms
step:1459/2330 train_time:84753ms step_avg:58.09ms
step:1460/2330 train_time:84814ms step_avg:58.09ms
step:1461/2330 train_time:84871ms step_avg:58.09ms
step:1462/2330 train_time:84931ms step_avg:58.09ms
step:1463/2330 train_time:84987ms step_avg:58.09ms
step:1464/2330 train_time:85048ms step_avg:58.09ms
step:1465/2330 train_time:85104ms step_avg:58.09ms
step:1466/2330 train_time:85165ms step_avg:58.09ms
step:1467/2330 train_time:85221ms step_avg:58.09ms
step:1468/2330 train_time:85281ms step_avg:58.09ms
step:1469/2330 train_time:85338ms step_avg:58.09ms
step:1470/2330 train_time:85398ms step_avg:58.09ms
step:1471/2330 train_time:85455ms step_avg:58.09ms
step:1472/2330 train_time:85515ms step_avg:58.09ms
step:1473/2330 train_time:85573ms step_avg:58.09ms
step:1474/2330 train_time:85632ms step_avg:58.10ms
step:1475/2330 train_time:85689ms step_avg:58.09ms
step:1476/2330 train_time:85749ms step_avg:58.10ms
step:1477/2330 train_time:85805ms step_avg:58.09ms
step:1478/2330 train_time:85865ms step_avg:58.10ms
step:1479/2330 train_time:85922ms step_avg:58.09ms
step:1480/2330 train_time:85982ms step_avg:58.10ms
step:1481/2330 train_time:86039ms step_avg:58.10ms
step:1482/2330 train_time:86099ms step_avg:58.10ms
step:1483/2330 train_time:86156ms step_avg:58.10ms
step:1484/2330 train_time:86218ms step_avg:58.10ms
step:1485/2330 train_time:86274ms step_avg:58.10ms
step:1486/2330 train_time:86334ms step_avg:58.10ms
step:1487/2330 train_time:86391ms step_avg:58.10ms
step:1488/2330 train_time:86451ms step_avg:58.10ms
step:1489/2330 train_time:86508ms step_avg:58.10ms
step:1490/2330 train_time:86568ms step_avg:58.10ms
step:1491/2330 train_time:86625ms step_avg:58.10ms
step:1492/2330 train_time:86684ms step_avg:58.10ms
step:1493/2330 train_time:86740ms step_avg:58.10ms
step:1494/2330 train_time:86801ms step_avg:58.10ms
step:1495/2330 train_time:86858ms step_avg:58.10ms
step:1496/2330 train_time:86919ms step_avg:58.10ms
step:1497/2330 train_time:86977ms step_avg:58.10ms
step:1498/2330 train_time:87036ms step_avg:58.10ms
step:1499/2330 train_time:87094ms step_avg:58.10ms
step:1500/2330 train_time:87154ms step_avg:58.10ms
step:1500/2330 val_loss:3.9077 train_time:87235ms step_avg:58.16ms
step:1501/2330 train_time:87254ms step_avg:58.13ms
step:1502/2330 train_time:87273ms step_avg:58.10ms
step:1503/2330 train_time:87334ms step_avg:58.11ms
step:1504/2330 train_time:87398ms step_avg:58.11ms
step:1505/2330 train_time:87455ms step_avg:58.11ms
step:1506/2330 train_time:87515ms step_avg:58.11ms
step:1507/2330 train_time:87572ms step_avg:58.11ms
step:1508/2330 train_time:87631ms step_avg:58.11ms
step:1509/2330 train_time:87688ms step_avg:58.11ms
step:1510/2330 train_time:87747ms step_avg:58.11ms
step:1511/2330 train_time:87803ms step_avg:58.11ms
step:1512/2330 train_time:87864ms step_avg:58.11ms
step:1513/2330 train_time:87919ms step_avg:58.11ms
step:1514/2330 train_time:87980ms step_avg:58.11ms
step:1515/2330 train_time:88035ms step_avg:58.11ms
step:1516/2330 train_time:88096ms step_avg:58.11ms
step:1517/2330 train_time:88152ms step_avg:58.11ms
step:1518/2330 train_time:88213ms step_avg:58.11ms
step:1519/2330 train_time:88270ms step_avg:58.11ms
step:1520/2330 train_time:88332ms step_avg:58.11ms
step:1521/2330 train_time:88390ms step_avg:58.11ms
step:1522/2330 train_time:88452ms step_avg:58.12ms
step:1523/2330 train_time:88509ms step_avg:58.11ms
step:1524/2330 train_time:88570ms step_avg:58.12ms
step:1525/2330 train_time:88627ms step_avg:58.12ms
step:1526/2330 train_time:88688ms step_avg:58.12ms
step:1527/2330 train_time:88745ms step_avg:58.12ms
step:1528/2330 train_time:88805ms step_avg:58.12ms
step:1529/2330 train_time:88863ms step_avg:58.12ms
step:1530/2330 train_time:88921ms step_avg:58.12ms
step:1531/2330 train_time:88978ms step_avg:58.12ms
step:1532/2330 train_time:89038ms step_avg:58.12ms
step:1533/2330 train_time:89095ms step_avg:58.12ms
step:1534/2330 train_time:89155ms step_avg:58.12ms
step:1535/2330 train_time:89212ms step_avg:58.12ms
step:1536/2330 train_time:89274ms step_avg:58.12ms
step:1537/2330 train_time:89331ms step_avg:58.12ms
step:1538/2330 train_time:89393ms step_avg:58.12ms
step:1539/2330 train_time:89451ms step_avg:58.12ms
step:1540/2330 train_time:89512ms step_avg:58.12ms
step:1541/2330 train_time:89570ms step_avg:58.12ms
step:1542/2330 train_time:89630ms step_avg:58.13ms
step:1543/2330 train_time:89688ms step_avg:58.13ms
step:1544/2330 train_time:89749ms step_avg:58.13ms
step:1545/2330 train_time:89806ms step_avg:58.13ms
step:1546/2330 train_time:89868ms step_avg:58.13ms
step:1547/2330 train_time:89926ms step_avg:58.13ms
step:1548/2330 train_time:89987ms step_avg:58.13ms
step:1549/2330 train_time:90044ms step_avg:58.13ms
step:1550/2330 train_time:90105ms step_avg:58.13ms
step:1551/2330 train_time:90162ms step_avg:58.13ms
step:1552/2330 train_time:90222ms step_avg:58.13ms
step:1553/2330 train_time:90280ms step_avg:58.13ms
step:1554/2330 train_time:90341ms step_avg:58.13ms
step:1555/2330 train_time:90399ms step_avg:58.13ms
step:1556/2330 train_time:90461ms step_avg:58.14ms
step:1557/2330 train_time:90518ms step_avg:58.14ms
step:1558/2330 train_time:90580ms step_avg:58.14ms
step:1559/2330 train_time:90636ms step_avg:58.14ms
step:1560/2330 train_time:90697ms step_avg:58.14ms
step:1561/2330 train_time:90754ms step_avg:58.14ms
step:1562/2330 train_time:90815ms step_avg:58.14ms
step:1563/2330 train_time:90872ms step_avg:58.14ms
step:1564/2330 train_time:90932ms step_avg:58.14ms
step:1565/2330 train_time:90989ms step_avg:58.14ms
step:1566/2330 train_time:91050ms step_avg:58.14ms
step:1567/2330 train_time:91108ms step_avg:58.14ms
step:1568/2330 train_time:91168ms step_avg:58.14ms
step:1569/2330 train_time:91225ms step_avg:58.14ms
step:1570/2330 train_time:91288ms step_avg:58.14ms
step:1571/2330 train_time:91346ms step_avg:58.15ms
step:1572/2330 train_time:91407ms step_avg:58.15ms
step:1573/2330 train_time:91466ms step_avg:58.15ms
step:1574/2330 train_time:91527ms step_avg:58.15ms
step:1575/2330 train_time:91584ms step_avg:58.15ms
step:1576/2330 train_time:91646ms step_avg:58.15ms
step:1577/2330 train_time:91703ms step_avg:58.15ms
step:1578/2330 train_time:91765ms step_avg:58.15ms
step:1579/2330 train_time:91822ms step_avg:58.15ms
step:1580/2330 train_time:91882ms step_avg:58.15ms
step:1581/2330 train_time:91938ms step_avg:58.15ms
step:1582/2330 train_time:91999ms step_avg:58.15ms
step:1583/2330 train_time:92055ms step_avg:58.15ms
step:1584/2330 train_time:92117ms step_avg:58.15ms
step:1585/2330 train_time:92173ms step_avg:58.15ms
step:1586/2330 train_time:92235ms step_avg:58.16ms
step:1587/2330 train_time:92290ms step_avg:58.15ms
step:1588/2330 train_time:92352ms step_avg:58.16ms
step:1589/2330 train_time:92409ms step_avg:58.16ms
step:1590/2330 train_time:92471ms step_avg:58.16ms
step:1591/2330 train_time:92529ms step_avg:58.16ms
step:1592/2330 train_time:92590ms step_avg:58.16ms
step:1593/2330 train_time:92648ms step_avg:58.16ms
step:1594/2330 train_time:92711ms step_avg:58.16ms
step:1595/2330 train_time:92769ms step_avg:58.16ms
step:1596/2330 train_time:92829ms step_avg:58.16ms
step:1597/2330 train_time:92886ms step_avg:58.16ms
step:1598/2330 train_time:92947ms step_avg:58.16ms
step:1599/2330 train_time:93004ms step_avg:58.16ms
step:1600/2330 train_time:93066ms step_avg:58.17ms
step:1601/2330 train_time:93124ms step_avg:58.17ms
step:1602/2330 train_time:93185ms step_avg:58.17ms
step:1603/2330 train_time:93242ms step_avg:58.17ms
step:1604/2330 train_time:93304ms step_avg:58.17ms
step:1605/2330 train_time:93360ms step_avg:58.17ms
step:1606/2330 train_time:93421ms step_avg:58.17ms
step:1607/2330 train_time:93477ms step_avg:58.17ms
step:1608/2330 train_time:93539ms step_avg:58.17ms
step:1609/2330 train_time:93596ms step_avg:58.17ms
step:1610/2330 train_time:93658ms step_avg:58.17ms
step:1611/2330 train_time:93716ms step_avg:58.17ms
step:1612/2330 train_time:93775ms step_avg:58.17ms
step:1613/2330 train_time:93832ms step_avg:58.17ms
step:1614/2330 train_time:93893ms step_avg:58.17ms
step:1615/2330 train_time:93951ms step_avg:58.17ms
step:1616/2330 train_time:94011ms step_avg:58.18ms
step:1617/2330 train_time:94069ms step_avg:58.18ms
step:1618/2330 train_time:94129ms step_avg:58.18ms
step:1619/2330 train_time:94187ms step_avg:58.18ms
step:1620/2330 train_time:94247ms step_avg:58.18ms
step:1621/2330 train_time:94304ms step_avg:58.18ms
step:1622/2330 train_time:94365ms step_avg:58.18ms
step:1623/2330 train_time:94424ms step_avg:58.18ms
step:1624/2330 train_time:94485ms step_avg:58.18ms
step:1625/2330 train_time:94542ms step_avg:58.18ms
step:1626/2330 train_time:94603ms step_avg:58.18ms
step:1627/2330 train_time:94660ms step_avg:58.18ms
step:1628/2330 train_time:94721ms step_avg:58.18ms
step:1629/2330 train_time:94779ms step_avg:58.18ms
step:1630/2330 train_time:94839ms step_avg:58.18ms
step:1631/2330 train_time:94896ms step_avg:58.18ms
step:1632/2330 train_time:94957ms step_avg:58.18ms
step:1633/2330 train_time:95014ms step_avg:58.18ms
step:1634/2330 train_time:95076ms step_avg:58.19ms
step:1635/2330 train_time:95132ms step_avg:58.19ms
step:1636/2330 train_time:95194ms step_avg:58.19ms
step:1637/2330 train_time:95250ms step_avg:58.19ms
step:1638/2330 train_time:95311ms step_avg:58.19ms
step:1639/2330 train_time:95369ms step_avg:58.19ms
step:1640/2330 train_time:95430ms step_avg:58.19ms
step:1641/2330 train_time:95487ms step_avg:58.19ms
step:1642/2330 train_time:95549ms step_avg:58.19ms
step:1643/2330 train_time:95606ms step_avg:58.19ms
step:1644/2330 train_time:95667ms step_avg:58.19ms
step:1645/2330 train_time:95725ms step_avg:58.19ms
step:1646/2330 train_time:95786ms step_avg:58.19ms
step:1647/2330 train_time:95844ms step_avg:58.19ms
step:1648/2330 train_time:95906ms step_avg:58.20ms
step:1649/2330 train_time:95964ms step_avg:58.20ms
step:1650/2330 train_time:96025ms step_avg:58.20ms
step:1651/2330 train_time:96081ms step_avg:58.20ms
step:1652/2330 train_time:96143ms step_avg:58.20ms
step:1653/2330 train_time:96200ms step_avg:58.20ms
step:1654/2330 train_time:96260ms step_avg:58.20ms
step:1655/2330 train_time:96317ms step_avg:58.20ms
step:1656/2330 train_time:96379ms step_avg:58.20ms
step:1657/2330 train_time:96435ms step_avg:58.20ms
step:1658/2330 train_time:96498ms step_avg:58.20ms
step:1659/2330 train_time:96555ms step_avg:58.20ms
step:1660/2330 train_time:96615ms step_avg:58.20ms
step:1661/2330 train_time:96672ms step_avg:58.20ms
step:1662/2330 train_time:96733ms step_avg:58.20ms
step:1663/2330 train_time:96791ms step_avg:58.20ms
step:1664/2330 train_time:96852ms step_avg:58.20ms
step:1665/2330 train_time:96909ms step_avg:58.20ms
step:1666/2330 train_time:96970ms step_avg:58.21ms
step:1667/2330 train_time:97028ms step_avg:58.20ms
step:1668/2330 train_time:97089ms step_avg:58.21ms
step:1669/2330 train_time:97147ms step_avg:58.21ms
step:1670/2330 train_time:97208ms step_avg:58.21ms
step:1671/2330 train_time:97266ms step_avg:58.21ms
step:1672/2330 train_time:97327ms step_avg:58.21ms
step:1673/2330 train_time:97385ms step_avg:58.21ms
step:1674/2330 train_time:97447ms step_avg:58.21ms
step:1675/2330 train_time:97505ms step_avg:58.21ms
step:1676/2330 train_time:97565ms step_avg:58.21ms
step:1677/2330 train_time:97622ms step_avg:58.21ms
step:1678/2330 train_time:97683ms step_avg:58.21ms
step:1679/2330 train_time:97740ms step_avg:58.21ms
step:1680/2330 train_time:97803ms step_avg:58.22ms
step:1681/2330 train_time:97860ms step_avg:58.22ms
step:1682/2330 train_time:97922ms step_avg:58.22ms
step:1683/2330 train_time:97978ms step_avg:58.22ms
step:1684/2330 train_time:98041ms step_avg:58.22ms
step:1685/2330 train_time:98098ms step_avg:58.22ms
step:1686/2330 train_time:98158ms step_avg:58.22ms
step:1687/2330 train_time:98215ms step_avg:58.22ms
step:1688/2330 train_time:98276ms step_avg:58.22ms
step:1689/2330 train_time:98332ms step_avg:58.22ms
step:1690/2330 train_time:98394ms step_avg:58.22ms
step:1691/2330 train_time:98451ms step_avg:58.22ms
step:1692/2330 train_time:98512ms step_avg:58.22ms
step:1693/2330 train_time:98569ms step_avg:58.22ms
step:1694/2330 train_time:98630ms step_avg:58.22ms
step:1695/2330 train_time:98688ms step_avg:58.22ms
step:1696/2330 train_time:98748ms step_avg:58.22ms
step:1697/2330 train_time:98807ms step_avg:58.22ms
step:1698/2330 train_time:98868ms step_avg:58.23ms
step:1699/2330 train_time:98925ms step_avg:58.23ms
step:1700/2330 train_time:98987ms step_avg:58.23ms
step:1701/2330 train_time:99045ms step_avg:58.23ms
step:1702/2330 train_time:99105ms step_avg:58.23ms
step:1703/2330 train_time:99162ms step_avg:58.23ms
step:1704/2330 train_time:99223ms step_avg:58.23ms
step:1705/2330 train_time:99279ms step_avg:58.23ms
step:1706/2330 train_time:99342ms step_avg:58.23ms
step:1707/2330 train_time:99399ms step_avg:58.23ms
step:1708/2330 train_time:99460ms step_avg:58.23ms
step:1709/2330 train_time:99517ms step_avg:58.23ms
step:1710/2330 train_time:99578ms step_avg:58.23ms
step:1711/2330 train_time:99634ms step_avg:58.23ms
step:1712/2330 train_time:99696ms step_avg:58.23ms
step:1713/2330 train_time:99753ms step_avg:58.23ms
step:1714/2330 train_time:99815ms step_avg:58.24ms
step:1715/2330 train_time:99872ms step_avg:58.23ms
step:1716/2330 train_time:99933ms step_avg:58.24ms
step:1717/2330 train_time:99991ms step_avg:58.24ms
step:1718/2330 train_time:100051ms step_avg:58.24ms
step:1719/2330 train_time:100110ms step_avg:58.24ms
step:1720/2330 train_time:100170ms step_avg:58.24ms
step:1721/2330 train_time:100228ms step_avg:58.24ms
step:1722/2330 train_time:100289ms step_avg:58.24ms
step:1723/2330 train_time:100346ms step_avg:58.24ms
step:1724/2330 train_time:100408ms step_avg:58.24ms
step:1725/2330 train_time:100465ms step_avg:58.24ms
step:1726/2330 train_time:100527ms step_avg:58.24ms
step:1727/2330 train_time:100584ms step_avg:58.24ms
step:1728/2330 train_time:100646ms step_avg:58.24ms
step:1729/2330 train_time:100703ms step_avg:58.24ms
step:1730/2330 train_time:100765ms step_avg:58.25ms
step:1731/2330 train_time:100822ms step_avg:58.24ms
step:1732/2330 train_time:100883ms step_avg:58.25ms
step:1733/2330 train_time:100939ms step_avg:58.25ms
step:1734/2330 train_time:101001ms step_avg:58.25ms
step:1735/2330 train_time:101059ms step_avg:58.25ms
step:1736/2330 train_time:101120ms step_avg:58.25ms
step:1737/2330 train_time:101176ms step_avg:58.25ms
step:1738/2330 train_time:101238ms step_avg:58.25ms
step:1739/2330 train_time:101295ms step_avg:58.25ms
step:1740/2330 train_time:101355ms step_avg:58.25ms
step:1741/2330 train_time:101413ms step_avg:58.25ms
step:1742/2330 train_time:101472ms step_avg:58.25ms
step:1743/2330 train_time:101529ms step_avg:58.25ms
step:1744/2330 train_time:101591ms step_avg:58.25ms
step:1745/2330 train_time:101649ms step_avg:58.25ms
step:1746/2330 train_time:101711ms step_avg:58.25ms
step:1747/2330 train_time:101768ms step_avg:58.25ms
step:1748/2330 train_time:101829ms step_avg:58.25ms
step:1749/2330 train_time:101888ms step_avg:58.25ms
step:1750/2330 train_time:101948ms step_avg:58.26ms
step:1750/2330 val_loss:3.8218 train_time:102031ms step_avg:58.30ms
step:1751/2330 train_time:102049ms step_avg:58.28ms
step:1752/2330 train_time:102069ms step_avg:58.26ms
step:1753/2330 train_time:102124ms step_avg:58.26ms
step:1754/2330 train_time:102195ms step_avg:58.26ms
step:1755/2330 train_time:102251ms step_avg:58.26ms
step:1756/2330 train_time:102313ms step_avg:58.26ms
step:1757/2330 train_time:102370ms step_avg:58.26ms
step:1758/2330 train_time:102429ms step_avg:58.26ms
step:1759/2330 train_time:102485ms step_avg:58.26ms
step:1760/2330 train_time:102545ms step_avg:58.26ms
step:1761/2330 train_time:102601ms step_avg:58.26ms
step:1762/2330 train_time:102661ms step_avg:58.26ms
step:1763/2330 train_time:102717ms step_avg:58.26ms
step:1764/2330 train_time:102777ms step_avg:58.26ms
step:1765/2330 train_time:102833ms step_avg:58.26ms
step:1766/2330 train_time:102892ms step_avg:58.26ms
step:1767/2330 train_time:102953ms step_avg:58.26ms
step:1768/2330 train_time:103019ms step_avg:58.27ms
step:1769/2330 train_time:103077ms step_avg:58.27ms
step:1770/2330 train_time:103139ms step_avg:58.27ms
step:1771/2330 train_time:103196ms step_avg:58.27ms
step:1772/2330 train_time:103257ms step_avg:58.27ms
step:1773/2330 train_time:103313ms step_avg:58.27ms
step:1774/2330 train_time:103376ms step_avg:58.27ms
step:1775/2330 train_time:103432ms step_avg:58.27ms
step:1776/2330 train_time:103493ms step_avg:58.27ms
step:1777/2330 train_time:103549ms step_avg:58.27ms
step:1778/2330 train_time:103611ms step_avg:58.27ms
step:1779/2330 train_time:103669ms step_avg:58.27ms
step:1780/2330 train_time:103729ms step_avg:58.27ms
step:1781/2330 train_time:103786ms step_avg:58.27ms
step:1782/2330 train_time:103846ms step_avg:58.27ms
step:1783/2330 train_time:103904ms step_avg:58.27ms
step:1784/2330 train_time:103966ms step_avg:58.28ms
step:1785/2330 train_time:104024ms step_avg:58.28ms
step:1786/2330 train_time:104088ms step_avg:58.28ms
step:1787/2330 train_time:104146ms step_avg:58.28ms
step:1788/2330 train_time:104208ms step_avg:58.28ms
step:1789/2330 train_time:104266ms step_avg:58.28ms
step:1790/2330 train_time:104327ms step_avg:58.28ms
step:1791/2330 train_time:104384ms step_avg:58.28ms
step:1792/2330 train_time:104444ms step_avg:58.28ms
step:1793/2330 train_time:104501ms step_avg:58.28ms
step:1794/2330 train_time:104561ms step_avg:58.28ms
step:1795/2330 train_time:104617ms step_avg:58.28ms
step:1796/2330 train_time:104678ms step_avg:58.28ms
step:1797/2330 train_time:104734ms step_avg:58.28ms
step:1798/2330 train_time:104795ms step_avg:58.28ms
step:1799/2330 train_time:104852ms step_avg:58.28ms
step:1800/2330 train_time:104913ms step_avg:58.28ms
step:1801/2330 train_time:104972ms step_avg:58.29ms
step:1802/2330 train_time:105033ms step_avg:58.29ms
step:1803/2330 train_time:105091ms step_avg:58.29ms
step:1804/2330 train_time:105151ms step_avg:58.29ms
step:1805/2330 train_time:105209ms step_avg:58.29ms
step:1806/2330 train_time:105271ms step_avg:58.29ms
step:1807/2330 train_time:105328ms step_avg:58.29ms
step:1808/2330 train_time:105389ms step_avg:58.29ms
step:1809/2330 train_time:105446ms step_avg:58.29ms
step:1810/2330 train_time:105508ms step_avg:58.29ms
step:1811/2330 train_time:105565ms step_avg:58.29ms
step:1812/2330 train_time:105626ms step_avg:58.29ms
step:1813/2330 train_time:105683ms step_avg:58.29ms
step:1814/2330 train_time:105745ms step_avg:58.29ms
step:1815/2330 train_time:105801ms step_avg:58.29ms
step:1816/2330 train_time:105862ms step_avg:58.29ms
step:1817/2330 train_time:105919ms step_avg:58.29ms
step:1818/2330 train_time:105982ms step_avg:58.30ms
step:1819/2330 train_time:106039ms step_avg:58.30ms
step:1820/2330 train_time:106100ms step_avg:58.30ms
step:1821/2330 train_time:106157ms step_avg:58.30ms
step:1822/2330 train_time:106219ms step_avg:58.30ms
step:1823/2330 train_time:106275ms step_avg:58.30ms
step:1824/2330 train_time:106337ms step_avg:58.30ms
step:1825/2330 train_time:106393ms step_avg:58.30ms
step:1826/2330 train_time:106456ms step_avg:58.30ms
step:1827/2330 train_time:106512ms step_avg:58.30ms
step:1828/2330 train_time:106573ms step_avg:58.30ms
step:1829/2330 train_time:106630ms step_avg:58.30ms
step:1830/2330 train_time:106690ms step_avg:58.30ms
step:1831/2330 train_time:106747ms step_avg:58.30ms
step:1832/2330 train_time:106809ms step_avg:58.30ms
step:1833/2330 train_time:106866ms step_avg:58.30ms
step:1834/2330 train_time:106926ms step_avg:58.30ms
step:1835/2330 train_time:106983ms step_avg:58.30ms
step:1836/2330 train_time:107046ms step_avg:58.30ms
step:1837/2330 train_time:107104ms step_avg:58.30ms
step:1838/2330 train_time:107166ms step_avg:58.31ms
step:1839/2330 train_time:107223ms step_avg:58.31ms
step:1840/2330 train_time:107285ms step_avg:58.31ms
step:1841/2330 train_time:107341ms step_avg:58.31ms
step:1842/2330 train_time:107403ms step_avg:58.31ms
step:1843/2330 train_time:107459ms step_avg:58.31ms
step:1844/2330 train_time:107521ms step_avg:58.31ms
step:1845/2330 train_time:107578ms step_avg:58.31ms
step:1846/2330 train_time:107639ms step_avg:58.31ms
step:1847/2330 train_time:107695ms step_avg:58.31ms
step:1848/2330 train_time:107756ms step_avg:58.31ms
step:1849/2330 train_time:107812ms step_avg:58.31ms
step:1850/2330 train_time:107873ms step_avg:58.31ms
step:1851/2330 train_time:107931ms step_avg:58.31ms
step:1852/2330 train_time:107991ms step_avg:58.31ms
step:1853/2330 train_time:108049ms step_avg:58.31ms
step:1854/2330 train_time:108111ms step_avg:58.31ms
step:1855/2330 train_time:108169ms step_avg:58.31ms
step:1856/2330 train_time:108229ms step_avg:58.31ms
step:1857/2330 train_time:108287ms step_avg:58.31ms
step:1858/2330 train_time:108348ms step_avg:58.31ms
step:1859/2330 train_time:108406ms step_avg:58.31ms
step:1860/2330 train_time:108467ms step_avg:58.32ms
step:1861/2330 train_time:108524ms step_avg:58.32ms
step:1862/2330 train_time:108585ms step_avg:58.32ms
step:1863/2330 train_time:108642ms step_avg:58.32ms
step:1864/2330 train_time:108701ms step_avg:58.32ms
step:1865/2330 train_time:108758ms step_avg:58.32ms
step:1866/2330 train_time:108819ms step_avg:58.32ms
step:1867/2330 train_time:108875ms step_avg:58.32ms
step:1868/2330 train_time:108939ms step_avg:58.32ms
step:1869/2330 train_time:108995ms step_avg:58.32ms
step:1870/2330 train_time:109057ms step_avg:58.32ms
step:1871/2330 train_time:109114ms step_avg:58.32ms
step:1872/2330 train_time:109176ms step_avg:58.32ms
step:1873/2330 train_time:109233ms step_avg:58.32ms
step:1874/2330 train_time:109295ms step_avg:58.32ms
step:1875/2330 train_time:109353ms step_avg:58.32ms
step:1876/2330 train_time:109413ms step_avg:58.32ms
step:1877/2330 train_time:109469ms step_avg:58.32ms
step:1878/2330 train_time:109530ms step_avg:58.32ms
step:1879/2330 train_time:109588ms step_avg:58.32ms
step:1880/2330 train_time:109648ms step_avg:58.32ms
step:1881/2330 train_time:109706ms step_avg:58.32ms
step:1882/2330 train_time:109766ms step_avg:58.32ms
step:1883/2330 train_time:109824ms step_avg:58.32ms
step:1884/2330 train_time:109884ms step_avg:58.32ms
step:1885/2330 train_time:109942ms step_avg:58.32ms
step:1886/2330 train_time:110002ms step_avg:58.33ms
step:1887/2330 train_time:110060ms step_avg:58.33ms
step:1888/2330 train_time:110120ms step_avg:58.33ms
step:1889/2330 train_time:110177ms step_avg:58.33ms
step:1890/2330 train_time:110239ms step_avg:58.33ms
step:1891/2330 train_time:110295ms step_avg:58.33ms
step:1892/2330 train_time:110358ms step_avg:58.33ms
step:1893/2330 train_time:110416ms step_avg:58.33ms
step:1894/2330 train_time:110477ms step_avg:58.33ms
step:1895/2330 train_time:110533ms step_avg:58.33ms
step:1896/2330 train_time:110595ms step_avg:58.33ms
step:1897/2330 train_time:110652ms step_avg:58.33ms
step:1898/2330 train_time:110712ms step_avg:58.33ms
step:1899/2330 train_time:110770ms step_avg:58.33ms
step:1900/2330 train_time:110830ms step_avg:58.33ms
step:1901/2330 train_time:110888ms step_avg:58.33ms
step:1902/2330 train_time:110949ms step_avg:58.33ms
step:1903/2330 train_time:111008ms step_avg:58.33ms
step:1904/2330 train_time:111068ms step_avg:58.33ms
step:1905/2330 train_time:111126ms step_avg:58.33ms
step:1906/2330 train_time:111187ms step_avg:58.34ms
step:1907/2330 train_time:111244ms step_avg:58.33ms
step:1908/2330 train_time:111306ms step_avg:58.34ms
step:1909/2330 train_time:111364ms step_avg:58.34ms
step:1910/2330 train_time:111425ms step_avg:58.34ms
step:1911/2330 train_time:111482ms step_avg:58.34ms
step:1912/2330 train_time:111542ms step_avg:58.34ms
step:1913/2330 train_time:111598ms step_avg:58.34ms
step:1914/2330 train_time:111660ms step_avg:58.34ms
step:1915/2330 train_time:111717ms step_avg:58.34ms
step:1916/2330 train_time:111778ms step_avg:58.34ms
step:1917/2330 train_time:111835ms step_avg:58.34ms
step:1918/2330 train_time:111896ms step_avg:58.34ms
step:1919/2330 train_time:111953ms step_avg:58.34ms
step:1920/2330 train_time:112014ms step_avg:58.34ms
step:1921/2330 train_time:112072ms step_avg:58.34ms
step:1922/2330 train_time:112131ms step_avg:58.34ms
step:1923/2330 train_time:112189ms step_avg:58.34ms
step:1924/2330 train_time:112250ms step_avg:58.34ms
step:1925/2330 train_time:112308ms step_avg:58.34ms
step:1926/2330 train_time:112369ms step_avg:58.34ms
step:1927/2330 train_time:112428ms step_avg:58.34ms
step:1928/2330 train_time:112488ms step_avg:58.34ms
step:1929/2330 train_time:112546ms step_avg:58.34ms
step:1930/2330 train_time:112608ms step_avg:58.35ms
step:1931/2330 train_time:112667ms step_avg:58.35ms
step:1932/2330 train_time:112727ms step_avg:58.35ms
step:1933/2330 train_time:112784ms step_avg:58.35ms
step:1934/2330 train_time:112844ms step_avg:58.35ms
step:1935/2330 train_time:112901ms step_avg:58.35ms
step:1936/2330 train_time:112962ms step_avg:58.35ms
step:1937/2330 train_time:113019ms step_avg:58.35ms
step:1938/2330 train_time:113081ms step_avg:58.35ms
step:1939/2330 train_time:113137ms step_avg:58.35ms
step:1940/2330 train_time:113199ms step_avg:58.35ms
step:1941/2330 train_time:113256ms step_avg:58.35ms
step:1942/2330 train_time:113317ms step_avg:58.35ms
step:1943/2330 train_time:113374ms step_avg:58.35ms
step:1944/2330 train_time:113435ms step_avg:58.35ms
step:1945/2330 train_time:113491ms step_avg:58.35ms
step:1946/2330 train_time:113553ms step_avg:58.35ms
step:1947/2330 train_time:113610ms step_avg:58.35ms
step:1948/2330 train_time:113671ms step_avg:58.35ms
step:1949/2330 train_time:113728ms step_avg:58.35ms
step:1950/2330 train_time:113790ms step_avg:58.35ms
step:1951/2330 train_time:113847ms step_avg:58.35ms
step:1952/2330 train_time:113909ms step_avg:58.35ms
step:1953/2330 train_time:113968ms step_avg:58.36ms
step:1954/2330 train_time:114028ms step_avg:58.36ms
step:1955/2330 train_time:114086ms step_avg:58.36ms
step:1956/2330 train_time:114147ms step_avg:58.36ms
step:1957/2330 train_time:114206ms step_avg:58.36ms
step:1958/2330 train_time:114266ms step_avg:58.36ms
step:1959/2330 train_time:114324ms step_avg:58.36ms
step:1960/2330 train_time:114384ms step_avg:58.36ms
step:1961/2330 train_time:114441ms step_avg:58.36ms
step:1962/2330 train_time:114502ms step_avg:58.36ms
step:1963/2330 train_time:114559ms step_avg:58.36ms
step:1964/2330 train_time:114621ms step_avg:58.36ms
step:1965/2330 train_time:114678ms step_avg:58.36ms
step:1966/2330 train_time:114739ms step_avg:58.36ms
step:1967/2330 train_time:114796ms step_avg:58.36ms
step:1968/2330 train_time:114856ms step_avg:58.36ms
step:1969/2330 train_time:114913ms step_avg:58.36ms
step:1970/2330 train_time:114974ms step_avg:58.36ms
step:1971/2330 train_time:115032ms step_avg:58.36ms
step:1972/2330 train_time:115092ms step_avg:58.36ms
step:1973/2330 train_time:115149ms step_avg:58.36ms
step:1974/2330 train_time:115211ms step_avg:58.36ms
step:1975/2330 train_time:115269ms step_avg:58.36ms
step:1976/2330 train_time:115329ms step_avg:58.36ms
step:1977/2330 train_time:115387ms step_avg:58.36ms
step:1978/2330 train_time:115448ms step_avg:58.37ms
step:1979/2330 train_time:115506ms step_avg:58.37ms
step:1980/2330 train_time:115568ms step_avg:58.37ms
step:1981/2330 train_time:115626ms step_avg:58.37ms
step:1982/2330 train_time:115687ms step_avg:58.37ms
step:1983/2330 train_time:115745ms step_avg:58.37ms
step:1984/2330 train_time:115806ms step_avg:58.37ms
step:1985/2330 train_time:115863ms step_avg:58.37ms
step:1986/2330 train_time:115925ms step_avg:58.37ms
step:1987/2330 train_time:115981ms step_avg:58.37ms
step:1988/2330 train_time:116043ms step_avg:58.37ms
step:1989/2330 train_time:116099ms step_avg:58.37ms
step:1990/2330 train_time:116161ms step_avg:58.37ms
step:1991/2330 train_time:116218ms step_avg:58.37ms
step:1992/2330 train_time:116279ms step_avg:58.37ms
step:1993/2330 train_time:116336ms step_avg:58.37ms
step:1994/2330 train_time:116397ms step_avg:58.37ms
step:1995/2330 train_time:116454ms step_avg:58.37ms
step:1996/2330 train_time:116514ms step_avg:58.37ms
step:1997/2330 train_time:116571ms step_avg:58.37ms
step:1998/2330 train_time:116632ms step_avg:58.37ms
step:1999/2330 train_time:116689ms step_avg:58.37ms
step:2000/2330 train_time:116751ms step_avg:58.38ms
step:2000/2330 val_loss:3.7558 train_time:116832ms step_avg:58.42ms
step:2001/2330 train_time:116852ms step_avg:58.40ms
step:2002/2330 train_time:116873ms step_avg:58.38ms
step:2003/2330 train_time:116932ms step_avg:58.38ms
step:2004/2330 train_time:116996ms step_avg:58.38ms
step:2005/2330 train_time:117052ms step_avg:58.38ms
step:2006/2330 train_time:117117ms step_avg:58.38ms
step:2007/2330 train_time:117174ms step_avg:58.38ms
step:2008/2330 train_time:117234ms step_avg:58.38ms
step:2009/2330 train_time:117290ms step_avg:58.38ms
step:2010/2330 train_time:117351ms step_avg:58.38ms
step:2011/2330 train_time:117408ms step_avg:58.38ms
step:2012/2330 train_time:117468ms step_avg:58.38ms
step:2013/2330 train_time:117524ms step_avg:58.38ms
step:2014/2330 train_time:117584ms step_avg:58.38ms
step:2015/2330 train_time:117640ms step_avg:58.38ms
step:2016/2330 train_time:117700ms step_avg:58.38ms
step:2017/2330 train_time:117757ms step_avg:58.38ms
step:2018/2330 train_time:117819ms step_avg:58.38ms
step:2019/2330 train_time:117878ms step_avg:58.38ms
step:2020/2330 train_time:117942ms step_avg:58.39ms
step:2021/2330 train_time:118001ms step_avg:58.39ms
step:2022/2330 train_time:118063ms step_avg:58.39ms
step:2023/2330 train_time:118120ms step_avg:58.39ms
step:2024/2330 train_time:118181ms step_avg:58.39ms
step:2025/2330 train_time:118239ms step_avg:58.39ms
step:2026/2330 train_time:118300ms step_avg:58.39ms
step:2027/2330 train_time:118357ms step_avg:58.39ms
step:2028/2330 train_time:118417ms step_avg:58.39ms
step:2029/2330 train_time:118474ms step_avg:58.39ms
step:2030/2330 train_time:118535ms step_avg:58.39ms
step:2031/2330 train_time:118592ms step_avg:58.39ms
step:2032/2330 train_time:118651ms step_avg:58.39ms
step:2033/2330 train_time:118708ms step_avg:58.39ms
step:2034/2330 train_time:118768ms step_avg:58.39ms
step:2035/2330 train_time:118825ms step_avg:58.39ms
step:2036/2330 train_time:118888ms step_avg:58.39ms
step:2037/2330 train_time:118946ms step_avg:58.39ms
step:2038/2330 train_time:119008ms step_avg:58.39ms
step:2039/2330 train_time:119065ms step_avg:58.39ms
step:2040/2330 train_time:119127ms step_avg:58.40ms
step:2041/2330 train_time:119184ms step_avg:58.40ms
step:2042/2330 train_time:119246ms step_avg:58.40ms
step:2043/2330 train_time:119303ms step_avg:58.40ms
step:2044/2330 train_time:119363ms step_avg:58.40ms
step:2045/2330 train_time:119420ms step_avg:58.40ms
step:2046/2330 train_time:119481ms step_avg:58.40ms
step:2047/2330 train_time:119538ms step_avg:58.40ms
step:2048/2330 train_time:119599ms step_avg:58.40ms
step:2049/2330 train_time:119658ms step_avg:58.40ms
step:2050/2330 train_time:119718ms step_avg:58.40ms
step:2051/2330 train_time:119775ms step_avg:58.40ms
step:2052/2330 train_time:119837ms step_avg:58.40ms
step:2053/2330 train_time:119894ms step_avg:58.40ms
step:2054/2330 train_time:119956ms step_avg:58.40ms
step:2055/2330 train_time:120014ms step_avg:58.40ms
step:2056/2330 train_time:120076ms step_avg:58.40ms
step:2057/2330 train_time:120133ms step_avg:58.40ms
step:2058/2330 train_time:120196ms step_avg:58.40ms
step:2059/2330 train_time:120252ms step_avg:58.40ms
step:2060/2330 train_time:120314ms step_avg:58.40ms
step:2061/2330 train_time:120371ms step_avg:58.40ms
step:2062/2330 train_time:120431ms step_avg:58.41ms
step:2063/2330 train_time:120488ms step_avg:58.40ms
step:2064/2330 train_time:120548ms step_avg:58.41ms
step:2065/2330 train_time:120605ms step_avg:58.40ms
step:2066/2330 train_time:120665ms step_avg:58.41ms
step:2067/2330 train_time:120723ms step_avg:58.40ms
step:2068/2330 train_time:120784ms step_avg:58.41ms
step:2069/2330 train_time:120842ms step_avg:58.41ms
step:2070/2330 train_time:120902ms step_avg:58.41ms
step:2071/2330 train_time:120961ms step_avg:58.41ms
step:2072/2330 train_time:121023ms step_avg:58.41ms
step:2073/2330 train_time:121081ms step_avg:58.41ms
step:2074/2330 train_time:121141ms step_avg:58.41ms
step:2075/2330 train_time:121200ms step_avg:58.41ms
step:2076/2330 train_time:121262ms step_avg:58.41ms
step:2077/2330 train_time:121320ms step_avg:58.41ms
step:2078/2330 train_time:121379ms step_avg:58.41ms
step:2079/2330 train_time:121437ms step_avg:58.41ms
step:2080/2330 train_time:121497ms step_avg:58.41ms
step:2081/2330 train_time:121555ms step_avg:58.41ms
step:2082/2330 train_time:121615ms step_avg:58.41ms
step:2083/2330 train_time:121672ms step_avg:58.41ms
step:2084/2330 train_time:121733ms step_avg:58.41ms
step:2085/2330 train_time:121790ms step_avg:58.41ms
step:2086/2330 train_time:121850ms step_avg:58.41ms
step:2087/2330 train_time:121907ms step_avg:58.41ms
step:2088/2330 train_time:121968ms step_avg:58.41ms
step:2089/2330 train_time:122026ms step_avg:58.41ms
step:2090/2330 train_time:122087ms step_avg:58.41ms
step:2091/2330 train_time:122144ms step_avg:58.41ms
step:2092/2330 train_time:122205ms step_avg:58.42ms
step:2093/2330 train_time:122263ms step_avg:58.42ms
step:2094/2330 train_time:122324ms step_avg:58.42ms
step:2095/2330 train_time:122382ms step_avg:58.42ms
step:2096/2330 train_time:122443ms step_avg:58.42ms
step:2097/2330 train_time:122500ms step_avg:58.42ms
step:2098/2330 train_time:122560ms step_avg:58.42ms
step:2099/2330 train_time:122618ms step_avg:58.42ms
step:2100/2330 train_time:122679ms step_avg:58.42ms
step:2101/2330 train_time:122737ms step_avg:58.42ms
step:2102/2330 train_time:122798ms step_avg:58.42ms
step:2103/2330 train_time:122856ms step_avg:58.42ms
step:2104/2330 train_time:122917ms step_avg:58.42ms
step:2105/2330 train_time:122975ms step_avg:58.42ms
step:2106/2330 train_time:123036ms step_avg:58.42ms
step:2107/2330 train_time:123092ms step_avg:58.42ms
step:2108/2330 train_time:123154ms step_avg:58.42ms
step:2109/2330 train_time:123211ms step_avg:58.42ms
step:2110/2330 train_time:123273ms step_avg:58.42ms
step:2111/2330 train_time:123330ms step_avg:58.42ms
step:2112/2330 train_time:123392ms step_avg:58.42ms
step:2113/2330 train_time:123449ms step_avg:58.42ms
step:2114/2330 train_time:123510ms step_avg:58.42ms
step:2115/2330 train_time:123567ms step_avg:58.42ms
step:2116/2330 train_time:123627ms step_avg:58.43ms
step:2117/2330 train_time:123685ms step_avg:58.42ms
step:2118/2330 train_time:123744ms step_avg:58.43ms
step:2119/2330 train_time:123802ms step_avg:58.42ms
step:2120/2330 train_time:123862ms step_avg:58.43ms
step:2121/2330 train_time:123919ms step_avg:58.42ms
step:2122/2330 train_time:123981ms step_avg:58.43ms
step:2123/2330 train_time:124039ms step_avg:58.43ms
step:2124/2330 train_time:124100ms step_avg:58.43ms
step:2125/2330 train_time:124158ms step_avg:58.43ms
step:2126/2330 train_time:124219ms step_avg:58.43ms
step:2127/2330 train_time:124276ms step_avg:58.43ms
step:2128/2330 train_time:124339ms step_avg:58.43ms
step:2129/2330 train_time:124397ms step_avg:58.43ms
step:2130/2330 train_time:124458ms step_avg:58.43ms
step:2131/2330 train_time:124515ms step_avg:58.43ms
step:2132/2330 train_time:124576ms step_avg:58.43ms
step:2133/2330 train_time:124633ms step_avg:58.43ms
step:2134/2330 train_time:124694ms step_avg:58.43ms
step:2135/2330 train_time:124751ms step_avg:58.43ms
step:2136/2330 train_time:124812ms step_avg:58.43ms
step:2137/2330 train_time:124868ms step_avg:58.43ms
step:2138/2330 train_time:124929ms step_avg:58.43ms
step:2139/2330 train_time:124986ms step_avg:58.43ms
step:2140/2330 train_time:125047ms step_avg:58.43ms
step:2141/2330 train_time:125105ms step_avg:58.43ms
step:2142/2330 train_time:125165ms step_avg:58.43ms
step:2143/2330 train_time:125223ms step_avg:58.43ms
step:2144/2330 train_time:125283ms step_avg:58.43ms
step:2145/2330 train_time:125342ms step_avg:58.43ms
step:2146/2330 train_time:125402ms step_avg:58.44ms
step:2147/2330 train_time:125459ms step_avg:58.43ms
step:2148/2330 train_time:125519ms step_avg:58.44ms
step:2149/2330 train_time:125577ms step_avg:58.44ms
step:2150/2330 train_time:125638ms step_avg:58.44ms
step:2151/2330 train_time:125696ms step_avg:58.44ms
step:2152/2330 train_time:125757ms step_avg:58.44ms
step:2153/2330 train_time:125814ms step_avg:58.44ms
step:2154/2330 train_time:125875ms step_avg:58.44ms
step:2155/2330 train_time:125933ms step_avg:58.44ms
step:2156/2330 train_time:125995ms step_avg:58.44ms
step:2157/2330 train_time:126052ms step_avg:58.44ms
step:2158/2330 train_time:126113ms step_avg:58.44ms
step:2159/2330 train_time:126170ms step_avg:58.44ms
step:2160/2330 train_time:126232ms step_avg:58.44ms
step:2161/2330 train_time:126288ms step_avg:58.44ms
step:2162/2330 train_time:126350ms step_avg:58.44ms
step:2163/2330 train_time:126407ms step_avg:58.44ms
step:2164/2330 train_time:126467ms step_avg:58.44ms
step:2165/2330 train_time:126524ms step_avg:58.44ms
step:2166/2330 train_time:126584ms step_avg:58.44ms
step:2167/2330 train_time:126643ms step_avg:58.44ms
step:2168/2330 train_time:126703ms step_avg:58.44ms
step:2169/2330 train_time:126761ms step_avg:58.44ms
step:2170/2330 train_time:126821ms step_avg:58.44ms
step:2171/2330 train_time:126879ms step_avg:58.44ms
step:2172/2330 train_time:126941ms step_avg:58.44ms
step:2173/2330 train_time:126999ms step_avg:58.44ms
step:2174/2330 train_time:127059ms step_avg:58.44ms
step:2175/2330 train_time:127118ms step_avg:58.44ms
step:2176/2330 train_time:127178ms step_avg:58.45ms
step:2177/2330 train_time:127236ms step_avg:58.45ms
step:2178/2330 train_time:127298ms step_avg:58.45ms
step:2179/2330 train_time:127356ms step_avg:58.45ms
step:2180/2330 train_time:127418ms step_avg:58.45ms
step:2181/2330 train_time:127475ms step_avg:58.45ms
step:2182/2330 train_time:127536ms step_avg:58.45ms
step:2183/2330 train_time:127593ms step_avg:58.45ms
step:2184/2330 train_time:127654ms step_avg:58.45ms
step:2185/2330 train_time:127711ms step_avg:58.45ms
step:2186/2330 train_time:127773ms step_avg:58.45ms
step:2187/2330 train_time:127830ms step_avg:58.45ms
step:2188/2330 train_time:127891ms step_avg:58.45ms
step:2189/2330 train_time:127947ms step_avg:58.45ms
step:2190/2330 train_time:128008ms step_avg:58.45ms
step:2191/2330 train_time:128065ms step_avg:58.45ms
step:2192/2330 train_time:128126ms step_avg:58.45ms
step:2193/2330 train_time:128183ms step_avg:58.45ms
step:2194/2330 train_time:128244ms step_avg:58.45ms
step:2195/2330 train_time:128302ms step_avg:58.45ms
step:2196/2330 train_time:128362ms step_avg:58.45ms
step:2197/2330 train_time:128419ms step_avg:58.45ms
step:2198/2330 train_time:128481ms step_avg:58.45ms
step:2199/2330 train_time:128539ms step_avg:58.45ms
step:2200/2330 train_time:128599ms step_avg:58.45ms
step:2201/2330 train_time:128657ms step_avg:58.45ms
step:2202/2330 train_time:128718ms step_avg:58.46ms
step:2203/2330 train_time:128777ms step_avg:58.46ms
step:2204/2330 train_time:128838ms step_avg:58.46ms
step:2205/2330 train_time:128896ms step_avg:58.46ms
step:2206/2330 train_time:128957ms step_avg:58.46ms
step:2207/2330 train_time:129014ms step_avg:58.46ms
step:2208/2330 train_time:129075ms step_avg:58.46ms
step:2209/2330 train_time:129132ms step_avg:58.46ms
step:2210/2330 train_time:129194ms step_avg:58.46ms
step:2211/2330 train_time:129251ms step_avg:58.46ms
step:2212/2330 train_time:129311ms step_avg:58.46ms
step:2213/2330 train_time:129368ms step_avg:58.46ms
step:2214/2330 train_time:129429ms step_avg:58.46ms
step:2215/2330 train_time:129485ms step_avg:58.46ms
step:2216/2330 train_time:129546ms step_avg:58.46ms
step:2217/2330 train_time:129607ms step_avg:58.46ms
step:2218/2330 train_time:129663ms step_avg:58.46ms
step:2219/2330 train_time:129721ms step_avg:58.46ms
step:2220/2330 train_time:129782ms step_avg:58.46ms
step:2221/2330 train_time:129839ms step_avg:58.46ms
step:2222/2330 train_time:129901ms step_avg:58.46ms
step:2223/2330 train_time:129958ms step_avg:58.46ms
step:2224/2330 train_time:130019ms step_avg:58.46ms
step:2225/2330 train_time:130077ms step_avg:58.46ms
step:2226/2330 train_time:130138ms step_avg:58.46ms
step:2227/2330 train_time:130196ms step_avg:58.46ms
step:2228/2330 train_time:130257ms step_avg:58.46ms
step:2229/2330 train_time:130315ms step_avg:58.46ms
step:2230/2330 train_time:130376ms step_avg:58.46ms
step:2231/2330 train_time:130433ms step_avg:58.46ms
step:2232/2330 train_time:130494ms step_avg:58.47ms
step:2233/2330 train_time:130552ms step_avg:58.46ms
step:2234/2330 train_time:130612ms step_avg:58.47ms
step:2235/2330 train_time:130669ms step_avg:58.46ms
step:2236/2330 train_time:130730ms step_avg:58.47ms
step:2237/2330 train_time:130787ms step_avg:58.47ms
step:2238/2330 train_time:130848ms step_avg:58.47ms
step:2239/2330 train_time:130905ms step_avg:58.47ms
step:2240/2330 train_time:130967ms step_avg:58.47ms
step:2241/2330 train_time:131024ms step_avg:58.47ms
step:2242/2330 train_time:131084ms step_avg:58.47ms
step:2243/2330 train_time:131142ms step_avg:58.47ms
step:2244/2330 train_time:131202ms step_avg:58.47ms
step:2245/2330 train_time:131260ms step_avg:58.47ms
step:2246/2330 train_time:131320ms step_avg:58.47ms
step:2247/2330 train_time:131378ms step_avg:58.47ms
step:2248/2330 train_time:131440ms step_avg:58.47ms
step:2249/2330 train_time:131498ms step_avg:58.47ms
step:2250/2330 train_time:131559ms step_avg:58.47ms
step:2250/2330 val_loss:3.7045 train_time:131641ms step_avg:58.51ms
step:2251/2330 train_time:131660ms step_avg:58.49ms
step:2252/2330 train_time:131682ms step_avg:58.47ms
step:2253/2330 train_time:131739ms step_avg:58.47ms
step:2254/2330 train_time:131807ms step_avg:58.48ms
step:2255/2330 train_time:131864ms step_avg:58.48ms
step:2256/2330 train_time:131929ms step_avg:58.48ms
step:2257/2330 train_time:131986ms step_avg:58.48ms
step:2258/2330 train_time:132048ms step_avg:58.48ms
step:2259/2330 train_time:132105ms step_avg:58.48ms
step:2260/2330 train_time:132165ms step_avg:58.48ms
step:2261/2330 train_time:132223ms step_avg:58.48ms
step:2262/2330 train_time:132283ms step_avg:58.48ms
step:2263/2330 train_time:132339ms step_avg:58.48ms
step:2264/2330 train_time:132399ms step_avg:58.48ms
step:2265/2330 train_time:132455ms step_avg:58.48ms
step:2266/2330 train_time:132515ms step_avg:58.48ms
step:2267/2330 train_time:132572ms step_avg:58.48ms
step:2268/2330 train_time:132634ms step_avg:58.48ms
step:2269/2330 train_time:132694ms step_avg:58.48ms
step:2270/2330 train_time:132755ms step_avg:58.48ms
step:2271/2330 train_time:132813ms step_avg:58.48ms
step:2272/2330 train_time:132875ms step_avg:58.48ms
step:2273/2330 train_time:132931ms step_avg:58.48ms
step:2274/2330 train_time:132993ms step_avg:58.48ms
step:2275/2330 train_time:133049ms step_avg:58.48ms
step:2276/2330 train_time:133109ms step_avg:58.48ms
step:2277/2330 train_time:133166ms step_avg:58.48ms
step:2278/2330 train_time:133227ms step_avg:58.48ms
step:2279/2330 train_time:133284ms step_avg:58.48ms
step:2280/2330 train_time:133344ms step_avg:58.48ms
step:2281/2330 train_time:133401ms step_avg:58.48ms
step:2282/2330 train_time:133462ms step_avg:58.48ms
step:2283/2330 train_time:133519ms step_avg:58.48ms
step:2284/2330 train_time:133581ms step_avg:58.49ms
step:2285/2330 train_time:133639ms step_avg:58.49ms
step:2286/2330 train_time:133700ms step_avg:58.49ms
step:2287/2330 train_time:133758ms step_avg:58.49ms
step:2288/2330 train_time:133820ms step_avg:58.49ms
step:2289/2330 train_time:133877ms step_avg:58.49ms
step:2290/2330 train_time:133941ms step_avg:58.49ms
step:2291/2330 train_time:133997ms step_avg:58.49ms
step:2292/2330 train_time:134059ms step_avg:58.49ms
step:2293/2330 train_time:134115ms step_avg:58.49ms
step:2294/2330 train_time:134176ms step_avg:58.49ms
step:2295/2330 train_time:134234ms step_avg:58.49ms
step:2296/2330 train_time:134294ms step_avg:58.49ms
step:2297/2330 train_time:134350ms step_avg:58.49ms
step:2298/2330 train_time:134410ms step_avg:58.49ms
step:2299/2330 train_time:134468ms step_avg:58.49ms
step:2300/2330 train_time:134528ms step_avg:58.49ms
step:2301/2330 train_time:134586ms step_avg:58.49ms
step:2302/2330 train_time:134647ms step_avg:58.49ms
step:2303/2330 train_time:134706ms step_avg:58.49ms
step:2304/2330 train_time:134768ms step_avg:58.49ms
step:2305/2330 train_time:134826ms step_avg:58.49ms
step:2306/2330 train_time:134888ms step_avg:58.49ms
step:2307/2330 train_time:134946ms step_avg:58.49ms
step:2308/2330 train_time:135008ms step_avg:58.50ms
step:2309/2330 train_time:135066ms step_avg:58.50ms
step:2310/2330 train_time:135127ms step_avg:58.50ms
step:2311/2330 train_time:135185ms step_avg:58.50ms
step:2312/2330 train_time:135245ms step_avg:58.50ms
step:2313/2330 train_time:135302ms step_avg:58.50ms
step:2314/2330 train_time:135362ms step_avg:58.50ms
step:2315/2330 train_time:135419ms step_avg:58.50ms
step:2316/2330 train_time:135479ms step_avg:58.50ms
step:2317/2330 train_time:135536ms step_avg:58.50ms
step:2318/2330 train_time:135597ms step_avg:58.50ms
step:2319/2330 train_time:135654ms step_avg:58.50ms
step:2320/2330 train_time:135716ms step_avg:58.50ms
step:2321/2330 train_time:135773ms step_avg:58.50ms
step:2322/2330 train_time:135836ms step_avg:58.50ms
step:2323/2330 train_time:135893ms step_avg:58.50ms
step:2324/2330 train_time:135956ms step_avg:58.50ms
step:2325/2330 train_time:136013ms step_avg:58.50ms
step:2326/2330 train_time:136074ms step_avg:58.50ms
step:2327/2330 train_time:136131ms step_avg:58.50ms
step:2328/2330 train_time:136191ms step_avg:58.50ms
step:2329/2330 train_time:136248ms step_avg:58.50ms
step:2330/2330 train_time:136308ms step_avg:58.50ms
step:2330/2330 val_loss:3.6876 train_time:136390ms step_avg:58.54ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
