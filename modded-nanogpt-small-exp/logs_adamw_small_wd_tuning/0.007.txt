import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:03:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:90ms step_avg:90.13ms
step:2/2330 train_time:180ms step_avg:89.89ms
step:3/2330 train_time:199ms step_avg:66.18ms
step:4/2330 train_time:218ms step_avg:54.44ms
step:5/2330 train_time:271ms step_avg:54.17ms
step:6/2330 train_time:329ms step_avg:54.80ms
step:7/2330 train_time:384ms step_avg:54.79ms
step:8/2330 train_time:442ms step_avg:55.27ms
step:9/2330 train_time:498ms step_avg:55.31ms
step:10/2330 train_time:556ms step_avg:55.60ms
step:11/2330 train_time:611ms step_avg:55.58ms
step:12/2330 train_time:670ms step_avg:55.81ms
step:13/2330 train_time:725ms step_avg:55.80ms
step:14/2330 train_time:784ms step_avg:55.99ms
step:15/2330 train_time:839ms step_avg:55.97ms
step:16/2330 train_time:897ms step_avg:56.08ms
step:17/2330 train_time:953ms step_avg:56.03ms
step:18/2330 train_time:1012ms step_avg:56.20ms
step:19/2330 train_time:1068ms step_avg:56.21ms
step:20/2330 train_time:1131ms step_avg:56.55ms
step:21/2330 train_time:1188ms step_avg:56.58ms
step:22/2330 train_time:1252ms step_avg:56.91ms
step:23/2330 train_time:1308ms step_avg:56.88ms
step:24/2330 train_time:1367ms step_avg:56.98ms
step:25/2330 train_time:1423ms step_avg:56.92ms
step:26/2330 train_time:1482ms step_avg:56.99ms
step:27/2330 train_time:1538ms step_avg:56.96ms
step:28/2330 train_time:1596ms step_avg:57.00ms
step:29/2330 train_time:1652ms step_avg:56.95ms
step:30/2330 train_time:1710ms step_avg:56.99ms
step:31/2330 train_time:1765ms step_avg:56.94ms
step:32/2330 train_time:1824ms step_avg:56.99ms
step:33/2330 train_time:1879ms step_avg:56.94ms
step:34/2330 train_time:1938ms step_avg:56.99ms
step:35/2330 train_time:1993ms step_avg:56.95ms
step:36/2330 train_time:2054ms step_avg:57.05ms
step:37/2330 train_time:2110ms step_avg:57.02ms
step:38/2330 train_time:2172ms step_avg:57.16ms
step:39/2330 train_time:2228ms step_avg:57.12ms
step:40/2330 train_time:2289ms step_avg:57.23ms
step:41/2330 train_time:2345ms step_avg:57.20ms
step:42/2330 train_time:2405ms step_avg:57.26ms
step:43/2330 train_time:2460ms step_avg:57.22ms
step:44/2330 train_time:2520ms step_avg:57.27ms
step:45/2330 train_time:2575ms step_avg:57.23ms
step:46/2330 train_time:2634ms step_avg:57.27ms
step:47/2330 train_time:2690ms step_avg:57.24ms
step:48/2330 train_time:2748ms step_avg:57.26ms
step:49/2330 train_time:2804ms step_avg:57.23ms
step:50/2330 train_time:2863ms step_avg:57.25ms
step:51/2330 train_time:2919ms step_avg:57.23ms
step:52/2330 train_time:2977ms step_avg:57.25ms
step:53/2330 train_time:3033ms step_avg:57.23ms
step:54/2330 train_time:3092ms step_avg:57.26ms
step:55/2330 train_time:3149ms step_avg:57.25ms
step:56/2330 train_time:3209ms step_avg:57.30ms
step:57/2330 train_time:3266ms step_avg:57.29ms
step:58/2330 train_time:3325ms step_avg:57.32ms
step:59/2330 train_time:3381ms step_avg:57.30ms
step:60/2330 train_time:3440ms step_avg:57.33ms
step:61/2330 train_time:3496ms step_avg:57.31ms
step:62/2330 train_time:3555ms step_avg:57.33ms
step:63/2330 train_time:3610ms step_avg:57.31ms
step:64/2330 train_time:3670ms step_avg:57.34ms
step:65/2330 train_time:3725ms step_avg:57.31ms
step:66/2330 train_time:3784ms step_avg:57.33ms
step:67/2330 train_time:3840ms step_avg:57.31ms
step:68/2330 train_time:3898ms step_avg:57.33ms
step:69/2330 train_time:3955ms step_avg:57.32ms
step:70/2330 train_time:4013ms step_avg:57.33ms
step:71/2330 train_time:4068ms step_avg:57.30ms
step:72/2330 train_time:4129ms step_avg:57.35ms
step:73/2330 train_time:4185ms step_avg:57.33ms
step:74/2330 train_time:4245ms step_avg:57.37ms
step:75/2330 train_time:4302ms step_avg:57.35ms
step:76/2330 train_time:4361ms step_avg:57.38ms
step:77/2330 train_time:4417ms step_avg:57.36ms
step:78/2330 train_time:4476ms step_avg:57.38ms
step:79/2330 train_time:4532ms step_avg:57.36ms
step:80/2330 train_time:4590ms step_avg:57.38ms
step:81/2330 train_time:4646ms step_avg:57.36ms
step:82/2330 train_time:4704ms step_avg:57.37ms
step:83/2330 train_time:4760ms step_avg:57.35ms
step:84/2330 train_time:4819ms step_avg:57.37ms
step:85/2330 train_time:4875ms step_avg:57.35ms
step:86/2330 train_time:4934ms step_avg:57.37ms
step:87/2330 train_time:4989ms step_avg:57.35ms
step:88/2330 train_time:5049ms step_avg:57.37ms
step:89/2330 train_time:5105ms step_avg:57.36ms
step:90/2330 train_time:5164ms step_avg:57.38ms
step:91/2330 train_time:5220ms step_avg:57.36ms
step:92/2330 train_time:5279ms step_avg:57.38ms
step:93/2330 train_time:5335ms step_avg:57.36ms
step:94/2330 train_time:5395ms step_avg:57.39ms
step:95/2330 train_time:5452ms step_avg:57.39ms
step:96/2330 train_time:5510ms step_avg:57.40ms
step:97/2330 train_time:5566ms step_avg:57.38ms
step:98/2330 train_time:5626ms step_avg:57.41ms
step:99/2330 train_time:5682ms step_avg:57.39ms
step:100/2330 train_time:5741ms step_avg:57.41ms
step:101/2330 train_time:5797ms step_avg:57.39ms
step:102/2330 train_time:5856ms step_avg:57.41ms
step:103/2330 train_time:5912ms step_avg:57.40ms
step:104/2330 train_time:5970ms step_avg:57.41ms
step:105/2330 train_time:6026ms step_avg:57.39ms
step:106/2330 train_time:6085ms step_avg:57.41ms
step:107/2330 train_time:6142ms step_avg:57.41ms
step:108/2330 train_time:6201ms step_avg:57.42ms
step:109/2330 train_time:6257ms step_avg:57.41ms
step:110/2330 train_time:6316ms step_avg:57.42ms
step:111/2330 train_time:6372ms step_avg:57.41ms
step:112/2330 train_time:6431ms step_avg:57.42ms
step:113/2330 train_time:6487ms step_avg:57.40ms
step:114/2330 train_time:6546ms step_avg:57.42ms
step:115/2330 train_time:6602ms step_avg:57.41ms
step:116/2330 train_time:6660ms step_avg:57.41ms
step:117/2330 train_time:6715ms step_avg:57.40ms
step:118/2330 train_time:6774ms step_avg:57.41ms
step:119/2330 train_time:6831ms step_avg:57.40ms
step:120/2330 train_time:6890ms step_avg:57.42ms
step:121/2330 train_time:6946ms step_avg:57.41ms
step:122/2330 train_time:7005ms step_avg:57.42ms
step:123/2330 train_time:7061ms step_avg:57.40ms
step:124/2330 train_time:7119ms step_avg:57.41ms
step:125/2330 train_time:7175ms step_avg:57.40ms
step:126/2330 train_time:7235ms step_avg:57.42ms
step:127/2330 train_time:7291ms step_avg:57.41ms
step:128/2330 train_time:7350ms step_avg:57.42ms
step:129/2330 train_time:7406ms step_avg:57.41ms
step:130/2330 train_time:7465ms step_avg:57.42ms
step:131/2330 train_time:7521ms step_avg:57.41ms
step:132/2330 train_time:7580ms step_avg:57.42ms
step:133/2330 train_time:7636ms step_avg:57.41ms
step:134/2330 train_time:7694ms step_avg:57.42ms
step:135/2330 train_time:7750ms step_avg:57.41ms
step:136/2330 train_time:7810ms step_avg:57.43ms
step:137/2330 train_time:7866ms step_avg:57.42ms
step:138/2330 train_time:7925ms step_avg:57.42ms
step:139/2330 train_time:7980ms step_avg:57.41ms
step:140/2330 train_time:8039ms step_avg:57.42ms
step:141/2330 train_time:8095ms step_avg:57.41ms
step:142/2330 train_time:8154ms step_avg:57.42ms
step:143/2330 train_time:8210ms step_avg:57.41ms
step:144/2330 train_time:8269ms step_avg:57.42ms
step:145/2330 train_time:8325ms step_avg:57.42ms
step:146/2330 train_time:8384ms step_avg:57.42ms
step:147/2330 train_time:8440ms step_avg:57.41ms
step:148/2330 train_time:8499ms step_avg:57.42ms
step:149/2330 train_time:8555ms step_avg:57.42ms
step:150/2330 train_time:8614ms step_avg:57.43ms
step:151/2330 train_time:8670ms step_avg:57.41ms
step:152/2330 train_time:8729ms step_avg:57.43ms
step:153/2330 train_time:8785ms step_avg:57.42ms
step:154/2330 train_time:8844ms step_avg:57.43ms
step:155/2330 train_time:8900ms step_avg:57.42ms
step:156/2330 train_time:8959ms step_avg:57.43ms
step:157/2330 train_time:9015ms step_avg:57.42ms
step:158/2330 train_time:9073ms step_avg:57.43ms
step:159/2330 train_time:9129ms step_avg:57.42ms
step:160/2330 train_time:9189ms step_avg:57.43ms
step:161/2330 train_time:9245ms step_avg:57.42ms
step:162/2330 train_time:9303ms step_avg:57.43ms
step:163/2330 train_time:9360ms step_avg:57.42ms
step:164/2330 train_time:9418ms step_avg:57.43ms
step:165/2330 train_time:9473ms step_avg:57.41ms
step:166/2330 train_time:9532ms step_avg:57.42ms
step:167/2330 train_time:9588ms step_avg:57.42ms
step:168/2330 train_time:9648ms step_avg:57.43ms
step:169/2330 train_time:9704ms step_avg:57.42ms
step:170/2330 train_time:9762ms step_avg:57.43ms
step:171/2330 train_time:9818ms step_avg:57.42ms
step:172/2330 train_time:9877ms step_avg:57.42ms
step:173/2330 train_time:9933ms step_avg:57.41ms
step:174/2330 train_time:9992ms step_avg:57.43ms
step:175/2330 train_time:10048ms step_avg:57.42ms
step:176/2330 train_time:10108ms step_avg:57.43ms
step:177/2330 train_time:10164ms step_avg:57.42ms
step:178/2330 train_time:10223ms step_avg:57.43ms
step:179/2330 train_time:10279ms step_avg:57.42ms
step:180/2330 train_time:10338ms step_avg:57.43ms
step:181/2330 train_time:10394ms step_avg:57.42ms
step:182/2330 train_time:10453ms step_avg:57.43ms
step:183/2330 train_time:10509ms step_avg:57.43ms
step:184/2330 train_time:10569ms step_avg:57.44ms
step:185/2330 train_time:10624ms step_avg:57.43ms
step:186/2330 train_time:10683ms step_avg:57.44ms
step:187/2330 train_time:10740ms step_avg:57.43ms
step:188/2330 train_time:10798ms step_avg:57.44ms
step:189/2330 train_time:10855ms step_avg:57.43ms
step:190/2330 train_time:10913ms step_avg:57.44ms
step:191/2330 train_time:10969ms step_avg:57.43ms
step:192/2330 train_time:11028ms step_avg:57.44ms
step:193/2330 train_time:11084ms step_avg:57.43ms
step:194/2330 train_time:11143ms step_avg:57.44ms
step:195/2330 train_time:11199ms step_avg:57.43ms
step:196/2330 train_time:11258ms step_avg:57.44ms
step:197/2330 train_time:11315ms step_avg:57.43ms
step:198/2330 train_time:11373ms step_avg:57.44ms
step:199/2330 train_time:11429ms step_avg:57.43ms
step:200/2330 train_time:11488ms step_avg:57.44ms
step:201/2330 train_time:11544ms step_avg:57.43ms
step:202/2330 train_time:11603ms step_avg:57.44ms
step:203/2330 train_time:11659ms step_avg:57.43ms
step:204/2330 train_time:11718ms step_avg:57.44ms
step:205/2330 train_time:11773ms step_avg:57.43ms
step:206/2330 train_time:11832ms step_avg:57.44ms
step:207/2330 train_time:11888ms step_avg:57.43ms
step:208/2330 train_time:11948ms step_avg:57.44ms
step:209/2330 train_time:12004ms step_avg:57.44ms
step:210/2330 train_time:12063ms step_avg:57.44ms
step:211/2330 train_time:12119ms step_avg:57.44ms
step:212/2330 train_time:12178ms step_avg:57.44ms
step:213/2330 train_time:12234ms step_avg:57.44ms
step:214/2330 train_time:12293ms step_avg:57.44ms
step:215/2330 train_time:12349ms step_avg:57.44ms
step:216/2330 train_time:12408ms step_avg:57.45ms
step:217/2330 train_time:12465ms step_avg:57.44ms
step:218/2330 train_time:12524ms step_avg:57.45ms
step:219/2330 train_time:12579ms step_avg:57.44ms
step:220/2330 train_time:12638ms step_avg:57.44ms
step:221/2330 train_time:12694ms step_avg:57.44ms
step:222/2330 train_time:12752ms step_avg:57.44ms
step:223/2330 train_time:12808ms step_avg:57.43ms
step:224/2330 train_time:12868ms step_avg:57.45ms
step:225/2330 train_time:12924ms step_avg:57.44ms
step:226/2330 train_time:12983ms step_avg:57.45ms
step:227/2330 train_time:13039ms step_avg:57.44ms
step:228/2330 train_time:13099ms step_avg:57.45ms
step:229/2330 train_time:13155ms step_avg:57.44ms
step:230/2330 train_time:13213ms step_avg:57.45ms
step:231/2330 train_time:13269ms step_avg:57.44ms
step:232/2330 train_time:13328ms step_avg:57.45ms
step:233/2330 train_time:13384ms step_avg:57.44ms
step:234/2330 train_time:13444ms step_avg:57.45ms
step:235/2330 train_time:13500ms step_avg:57.45ms
step:236/2330 train_time:13559ms step_avg:57.45ms
step:237/2330 train_time:13615ms step_avg:57.45ms
step:238/2330 train_time:13673ms step_avg:57.45ms
step:239/2330 train_time:13730ms step_avg:57.45ms
step:240/2330 train_time:13789ms step_avg:57.45ms
step:241/2330 train_time:13845ms step_avg:57.45ms
step:242/2330 train_time:13904ms step_avg:57.45ms
step:243/2330 train_time:13960ms step_avg:57.45ms
step:244/2330 train_time:14019ms step_avg:57.45ms
step:245/2330 train_time:14074ms step_avg:57.44ms
step:246/2330 train_time:14134ms step_avg:57.45ms
step:247/2330 train_time:14189ms step_avg:57.45ms
step:248/2330 train_time:14249ms step_avg:57.46ms
step:249/2330 train_time:14305ms step_avg:57.45ms
step:250/2330 train_time:14364ms step_avg:57.45ms
step:250/2330 val_loss:4.8880 train_time:14443ms step_avg:57.77ms
step:251/2330 train_time:14462ms step_avg:57.62ms
step:252/2330 train_time:14481ms step_avg:57.47ms
step:253/2330 train_time:14535ms step_avg:57.45ms
step:254/2330 train_time:14598ms step_avg:57.47ms
step:255/2330 train_time:14654ms step_avg:57.47ms
step:256/2330 train_time:14717ms step_avg:57.49ms
step:257/2330 train_time:14773ms step_avg:57.48ms
step:258/2330 train_time:14834ms step_avg:57.50ms
step:259/2330 train_time:14889ms step_avg:57.49ms
step:260/2330 train_time:14949ms step_avg:57.49ms
step:261/2330 train_time:15004ms step_avg:57.49ms
step:262/2330 train_time:15062ms step_avg:57.49ms
step:263/2330 train_time:15117ms step_avg:57.48ms
step:264/2330 train_time:15176ms step_avg:57.49ms
step:265/2330 train_time:15232ms step_avg:57.48ms
step:266/2330 train_time:15290ms step_avg:57.48ms
step:267/2330 train_time:15349ms step_avg:57.49ms
step:268/2330 train_time:15408ms step_avg:57.49ms
step:269/2330 train_time:15465ms step_avg:57.49ms
step:270/2330 train_time:15524ms step_avg:57.49ms
step:271/2330 train_time:15580ms step_avg:57.49ms
step:272/2330 train_time:15642ms step_avg:57.51ms
step:273/2330 train_time:15698ms step_avg:57.50ms
step:274/2330 train_time:15758ms step_avg:57.51ms
step:275/2330 train_time:15814ms step_avg:57.50ms
step:276/2330 train_time:15874ms step_avg:57.51ms
step:277/2330 train_time:15929ms step_avg:57.51ms
step:278/2330 train_time:15988ms step_avg:57.51ms
step:279/2330 train_time:16044ms step_avg:57.50ms
step:280/2330 train_time:16102ms step_avg:57.51ms
step:281/2330 train_time:16157ms step_avg:57.50ms
step:282/2330 train_time:16216ms step_avg:57.50ms
step:283/2330 train_time:16273ms step_avg:57.50ms
step:284/2330 train_time:16332ms step_avg:57.51ms
step:285/2330 train_time:16388ms step_avg:57.50ms
step:286/2330 train_time:16447ms step_avg:57.51ms
step:287/2330 train_time:16504ms step_avg:57.50ms
step:288/2330 train_time:16563ms step_avg:57.51ms
step:289/2330 train_time:16619ms step_avg:57.51ms
step:290/2330 train_time:16680ms step_avg:57.52ms
step:291/2330 train_time:16737ms step_avg:57.51ms
step:292/2330 train_time:16796ms step_avg:57.52ms
step:293/2330 train_time:16852ms step_avg:57.52ms
step:294/2330 train_time:16911ms step_avg:57.52ms
step:295/2330 train_time:16967ms step_avg:57.51ms
step:296/2330 train_time:17026ms step_avg:57.52ms
step:297/2330 train_time:17081ms step_avg:57.51ms
step:298/2330 train_time:17141ms step_avg:57.52ms
step:299/2330 train_time:17197ms step_avg:57.51ms
step:300/2330 train_time:17256ms step_avg:57.52ms
step:301/2330 train_time:17312ms step_avg:57.51ms
step:302/2330 train_time:17371ms step_avg:57.52ms
step:303/2330 train_time:17427ms step_avg:57.51ms
step:304/2330 train_time:17486ms step_avg:57.52ms
step:305/2330 train_time:17542ms step_avg:57.52ms
step:306/2330 train_time:17602ms step_avg:57.52ms
step:307/2330 train_time:17658ms step_avg:57.52ms
step:308/2330 train_time:17718ms step_avg:57.53ms
step:309/2330 train_time:17774ms step_avg:57.52ms
step:310/2330 train_time:17834ms step_avg:57.53ms
step:311/2330 train_time:17890ms step_avg:57.53ms
step:312/2330 train_time:17949ms step_avg:57.53ms
step:313/2330 train_time:18005ms step_avg:57.52ms
step:314/2330 train_time:18064ms step_avg:57.53ms
step:315/2330 train_time:18120ms step_avg:57.52ms
step:316/2330 train_time:18181ms step_avg:57.53ms
step:317/2330 train_time:18236ms step_avg:57.53ms
step:318/2330 train_time:18295ms step_avg:57.53ms
step:319/2330 train_time:18351ms step_avg:57.53ms
step:320/2330 train_time:18409ms step_avg:57.53ms
step:321/2330 train_time:18465ms step_avg:57.52ms
step:322/2330 train_time:18523ms step_avg:57.53ms
step:323/2330 train_time:18579ms step_avg:57.52ms
step:324/2330 train_time:18639ms step_avg:57.53ms
step:325/2330 train_time:18695ms step_avg:57.52ms
step:326/2330 train_time:18754ms step_avg:57.53ms
step:327/2330 train_time:18810ms step_avg:57.52ms
step:328/2330 train_time:18869ms step_avg:57.53ms
step:329/2330 train_time:18925ms step_avg:57.52ms
step:330/2330 train_time:18985ms step_avg:57.53ms
step:331/2330 train_time:19040ms step_avg:57.52ms
step:332/2330 train_time:19100ms step_avg:57.53ms
step:333/2330 train_time:19156ms step_avg:57.52ms
step:334/2330 train_time:19214ms step_avg:57.53ms
step:335/2330 train_time:19270ms step_avg:57.52ms
step:336/2330 train_time:19329ms step_avg:57.53ms
step:337/2330 train_time:19385ms step_avg:57.52ms
step:338/2330 train_time:19444ms step_avg:57.53ms
step:339/2330 train_time:19499ms step_avg:57.52ms
step:340/2330 train_time:19560ms step_avg:57.53ms
step:341/2330 train_time:19617ms step_avg:57.53ms
step:342/2330 train_time:19676ms step_avg:57.53ms
step:343/2330 train_time:19732ms step_avg:57.53ms
step:344/2330 train_time:19792ms step_avg:57.53ms
step:345/2330 train_time:19848ms step_avg:57.53ms
step:346/2330 train_time:19907ms step_avg:57.53ms
step:347/2330 train_time:19963ms step_avg:57.53ms
step:348/2330 train_time:20022ms step_avg:57.54ms
step:349/2330 train_time:20079ms step_avg:57.53ms
step:350/2330 train_time:20137ms step_avg:57.53ms
step:351/2330 train_time:20193ms step_avg:57.53ms
step:352/2330 train_time:20252ms step_avg:57.53ms
step:353/2330 train_time:20308ms step_avg:57.53ms
step:354/2330 train_time:20367ms step_avg:57.53ms
step:355/2330 train_time:20423ms step_avg:57.53ms
step:356/2330 train_time:20482ms step_avg:57.53ms
step:357/2330 train_time:20539ms step_avg:57.53ms
step:358/2330 train_time:20597ms step_avg:57.53ms
step:359/2330 train_time:20654ms step_avg:57.53ms
step:360/2330 train_time:20713ms step_avg:57.54ms
step:361/2330 train_time:20768ms step_avg:57.53ms
step:362/2330 train_time:20828ms step_avg:57.54ms
step:363/2330 train_time:20884ms step_avg:57.53ms
step:364/2330 train_time:20944ms step_avg:57.54ms
step:365/2330 train_time:21000ms step_avg:57.53ms
step:366/2330 train_time:21059ms step_avg:57.54ms
step:367/2330 train_time:21115ms step_avg:57.53ms
step:368/2330 train_time:21174ms step_avg:57.54ms
step:369/2330 train_time:21230ms step_avg:57.53ms
step:370/2330 train_time:21288ms step_avg:57.54ms
step:371/2330 train_time:21344ms step_avg:57.53ms
step:372/2330 train_time:21403ms step_avg:57.53ms
step:373/2330 train_time:21459ms step_avg:57.53ms
step:374/2330 train_time:21519ms step_avg:57.54ms
step:375/2330 train_time:21576ms step_avg:57.54ms
step:376/2330 train_time:21635ms step_avg:57.54ms
step:377/2330 train_time:21691ms step_avg:57.54ms
step:378/2330 train_time:21750ms step_avg:57.54ms
step:379/2330 train_time:21806ms step_avg:57.54ms
step:380/2330 train_time:21865ms step_avg:57.54ms
step:381/2330 train_time:21921ms step_avg:57.54ms
step:382/2330 train_time:21981ms step_avg:57.54ms
step:383/2330 train_time:22037ms step_avg:57.54ms
step:384/2330 train_time:22095ms step_avg:57.54ms
step:385/2330 train_time:22151ms step_avg:57.54ms
step:386/2330 train_time:22210ms step_avg:57.54ms
step:387/2330 train_time:22266ms step_avg:57.53ms
step:388/2330 train_time:22325ms step_avg:57.54ms
step:389/2330 train_time:22381ms step_avg:57.53ms
step:390/2330 train_time:22440ms step_avg:57.54ms
step:391/2330 train_time:22497ms step_avg:57.54ms
step:392/2330 train_time:22557ms step_avg:57.54ms
step:393/2330 train_time:22612ms step_avg:57.54ms
step:394/2330 train_time:22672ms step_avg:57.54ms
step:395/2330 train_time:22727ms step_avg:57.54ms
step:396/2330 train_time:22786ms step_avg:57.54ms
step:397/2330 train_time:22842ms step_avg:57.54ms
step:398/2330 train_time:22901ms step_avg:57.54ms
step:399/2330 train_time:22958ms step_avg:57.54ms
step:400/2330 train_time:23016ms step_avg:57.54ms
step:401/2330 train_time:23073ms step_avg:57.54ms
step:402/2330 train_time:23132ms step_avg:57.54ms
step:403/2330 train_time:23188ms step_avg:57.54ms
step:404/2330 train_time:23246ms step_avg:57.54ms
step:405/2330 train_time:23302ms step_avg:57.54ms
step:406/2330 train_time:23361ms step_avg:57.54ms
step:407/2330 train_time:23417ms step_avg:57.54ms
step:408/2330 train_time:23477ms step_avg:57.54ms
step:409/2330 train_time:23534ms step_avg:57.54ms
step:410/2330 train_time:23593ms step_avg:57.54ms
step:411/2330 train_time:23649ms step_avg:57.54ms
step:412/2330 train_time:23708ms step_avg:57.54ms
step:413/2330 train_time:23764ms step_avg:57.54ms
step:414/2330 train_time:23823ms step_avg:57.54ms
step:415/2330 train_time:23879ms step_avg:57.54ms
step:416/2330 train_time:23939ms step_avg:57.55ms
step:417/2330 train_time:23995ms step_avg:57.54ms
step:418/2330 train_time:24053ms step_avg:57.54ms
step:419/2330 train_time:24109ms step_avg:57.54ms
step:420/2330 train_time:24167ms step_avg:57.54ms
step:421/2330 train_time:24223ms step_avg:57.54ms
step:422/2330 train_time:24283ms step_avg:57.54ms
step:423/2330 train_time:24339ms step_avg:57.54ms
step:424/2330 train_time:24398ms step_avg:57.54ms
step:425/2330 train_time:24454ms step_avg:57.54ms
step:426/2330 train_time:24513ms step_avg:57.54ms
step:427/2330 train_time:24569ms step_avg:57.54ms
step:428/2330 train_time:24627ms step_avg:57.54ms
step:429/2330 train_time:24685ms step_avg:57.54ms
step:430/2330 train_time:24743ms step_avg:57.54ms
step:431/2330 train_time:24799ms step_avg:57.54ms
step:432/2330 train_time:24859ms step_avg:57.54ms
step:433/2330 train_time:24915ms step_avg:57.54ms
step:434/2330 train_time:24974ms step_avg:57.54ms
step:435/2330 train_time:25030ms step_avg:57.54ms
step:436/2330 train_time:25090ms step_avg:57.54ms
step:437/2330 train_time:25145ms step_avg:57.54ms
step:438/2330 train_time:25204ms step_avg:57.54ms
step:439/2330 train_time:25260ms step_avg:57.54ms
step:440/2330 train_time:25319ms step_avg:57.54ms
step:441/2330 train_time:25376ms step_avg:57.54ms
step:442/2330 train_time:25435ms step_avg:57.55ms
step:443/2330 train_time:25492ms step_avg:57.54ms
step:444/2330 train_time:25550ms step_avg:57.55ms
step:445/2330 train_time:25606ms step_avg:57.54ms
step:446/2330 train_time:25665ms step_avg:57.54ms
step:447/2330 train_time:25721ms step_avg:57.54ms
step:448/2330 train_time:25781ms step_avg:57.55ms
step:449/2330 train_time:25837ms step_avg:57.54ms
step:450/2330 train_time:25896ms step_avg:57.55ms
step:451/2330 train_time:25952ms step_avg:57.54ms
step:452/2330 train_time:26011ms step_avg:57.55ms
step:453/2330 train_time:26067ms step_avg:57.54ms
step:454/2330 train_time:26126ms step_avg:57.55ms
step:455/2330 train_time:26182ms step_avg:57.54ms
step:456/2330 train_time:26241ms step_avg:57.55ms
step:457/2330 train_time:26297ms step_avg:57.54ms
step:458/2330 train_time:26356ms step_avg:57.55ms
step:459/2330 train_time:26412ms step_avg:57.54ms
step:460/2330 train_time:26471ms step_avg:57.55ms
step:461/2330 train_time:26527ms step_avg:57.54ms
step:462/2330 train_time:26587ms step_avg:57.55ms
step:463/2330 train_time:26643ms step_avg:57.54ms
step:464/2330 train_time:26702ms step_avg:57.55ms
step:465/2330 train_time:26758ms step_avg:57.54ms
step:466/2330 train_time:26817ms step_avg:57.55ms
step:467/2330 train_time:26873ms step_avg:57.54ms
step:468/2330 train_time:26933ms step_avg:57.55ms
step:469/2330 train_time:26988ms step_avg:57.54ms
step:470/2330 train_time:27048ms step_avg:57.55ms
step:471/2330 train_time:27104ms step_avg:57.55ms
step:472/2330 train_time:27163ms step_avg:57.55ms
step:473/2330 train_time:27219ms step_avg:57.55ms
step:474/2330 train_time:27278ms step_avg:57.55ms
step:475/2330 train_time:27334ms step_avg:57.55ms
step:476/2330 train_time:27393ms step_avg:57.55ms
step:477/2330 train_time:27449ms step_avg:57.55ms
step:478/2330 train_time:27509ms step_avg:57.55ms
step:479/2330 train_time:27565ms step_avg:57.55ms
step:480/2330 train_time:27624ms step_avg:57.55ms
step:481/2330 train_time:27680ms step_avg:57.55ms
step:482/2330 train_time:27740ms step_avg:57.55ms
step:483/2330 train_time:27795ms step_avg:57.55ms
step:484/2330 train_time:27855ms step_avg:57.55ms
step:485/2330 train_time:27911ms step_avg:57.55ms
step:486/2330 train_time:27970ms step_avg:57.55ms
step:487/2330 train_time:28026ms step_avg:57.55ms
step:488/2330 train_time:28086ms step_avg:57.55ms
step:489/2330 train_time:28141ms step_avg:57.55ms
step:490/2330 train_time:28201ms step_avg:57.55ms
step:491/2330 train_time:28256ms step_avg:57.55ms
step:492/2330 train_time:28316ms step_avg:57.55ms
step:493/2330 train_time:28372ms step_avg:57.55ms
step:494/2330 train_time:28431ms step_avg:57.55ms
step:495/2330 train_time:28488ms step_avg:57.55ms
step:496/2330 train_time:28547ms step_avg:57.55ms
step:497/2330 train_time:28602ms step_avg:57.55ms
step:498/2330 train_time:28662ms step_avg:57.55ms
step:499/2330 train_time:28718ms step_avg:57.55ms
step:500/2330 train_time:28778ms step_avg:57.56ms
step:500/2330 val_loss:4.4011 train_time:28857ms step_avg:57.71ms
step:501/2330 train_time:28875ms step_avg:57.64ms
step:502/2330 train_time:28896ms step_avg:57.56ms
step:503/2330 train_time:28952ms step_avg:57.56ms
step:504/2330 train_time:29015ms step_avg:57.57ms
step:505/2330 train_time:29071ms step_avg:57.57ms
step:506/2330 train_time:29134ms step_avg:57.58ms
step:507/2330 train_time:29189ms step_avg:57.57ms
step:508/2330 train_time:29249ms step_avg:57.58ms
step:509/2330 train_time:29305ms step_avg:57.57ms
step:510/2330 train_time:29365ms step_avg:57.58ms
step:511/2330 train_time:29421ms step_avg:57.58ms
step:512/2330 train_time:29479ms step_avg:57.58ms
step:513/2330 train_time:29535ms step_avg:57.57ms
step:514/2330 train_time:29594ms step_avg:57.58ms
step:515/2330 train_time:29649ms step_avg:57.57ms
step:516/2330 train_time:29707ms step_avg:57.57ms
step:517/2330 train_time:29764ms step_avg:57.57ms
step:518/2330 train_time:29824ms step_avg:57.58ms
step:519/2330 train_time:29882ms step_avg:57.58ms
step:520/2330 train_time:29942ms step_avg:57.58ms
step:521/2330 train_time:30000ms step_avg:57.58ms
step:522/2330 train_time:30060ms step_avg:57.59ms
step:523/2330 train_time:30116ms step_avg:57.58ms
step:524/2330 train_time:30176ms step_avg:57.59ms
step:525/2330 train_time:30232ms step_avg:57.58ms
step:526/2330 train_time:30293ms step_avg:57.59ms
step:527/2330 train_time:30348ms step_avg:57.59ms
step:528/2330 train_time:30407ms step_avg:57.59ms
step:529/2330 train_time:30462ms step_avg:57.58ms
step:530/2330 train_time:30522ms step_avg:57.59ms
step:531/2330 train_time:30578ms step_avg:57.58ms
step:532/2330 train_time:30637ms step_avg:57.59ms
step:533/2330 train_time:30692ms step_avg:57.58ms
step:534/2330 train_time:30751ms step_avg:57.59ms
step:535/2330 train_time:30807ms step_avg:57.58ms
step:536/2330 train_time:30867ms step_avg:57.59ms
step:537/2330 train_time:30923ms step_avg:57.58ms
step:538/2330 train_time:30984ms step_avg:57.59ms
step:539/2330 train_time:31041ms step_avg:57.59ms
step:540/2330 train_time:31101ms step_avg:57.59ms
step:541/2330 train_time:31158ms step_avg:57.59ms
step:542/2330 train_time:31218ms step_avg:57.60ms
step:543/2330 train_time:31275ms step_avg:57.60ms
step:544/2330 train_time:31334ms step_avg:57.60ms
step:545/2330 train_time:31390ms step_avg:57.60ms
step:546/2330 train_time:31449ms step_avg:57.60ms
step:547/2330 train_time:31505ms step_avg:57.60ms
step:548/2330 train_time:31564ms step_avg:57.60ms
step:549/2330 train_time:31621ms step_avg:57.60ms
step:550/2330 train_time:31680ms step_avg:57.60ms
step:551/2330 train_time:31735ms step_avg:57.60ms
step:552/2330 train_time:31794ms step_avg:57.60ms
step:553/2330 train_time:31850ms step_avg:57.59ms
step:554/2330 train_time:31910ms step_avg:57.60ms
step:555/2330 train_time:31966ms step_avg:57.60ms
step:556/2330 train_time:32028ms step_avg:57.60ms
step:557/2330 train_time:32084ms step_avg:57.60ms
step:558/2330 train_time:32144ms step_avg:57.61ms
step:559/2330 train_time:32200ms step_avg:57.60ms
step:560/2330 train_time:32259ms step_avg:57.61ms
step:561/2330 train_time:32316ms step_avg:57.60ms
step:562/2330 train_time:32376ms step_avg:57.61ms
step:563/2330 train_time:32432ms step_avg:57.61ms
step:564/2330 train_time:32491ms step_avg:57.61ms
step:565/2330 train_time:32546ms step_avg:57.60ms
step:566/2330 train_time:32606ms step_avg:57.61ms
step:567/2330 train_time:32663ms step_avg:57.61ms
step:568/2330 train_time:32721ms step_avg:57.61ms
step:569/2330 train_time:32777ms step_avg:57.60ms
step:570/2330 train_time:32837ms step_avg:57.61ms
step:571/2330 train_time:32893ms step_avg:57.61ms
step:572/2330 train_time:32953ms step_avg:57.61ms
step:573/2330 train_time:33009ms step_avg:57.61ms
step:574/2330 train_time:33070ms step_avg:57.61ms
step:575/2330 train_time:33126ms step_avg:57.61ms
step:576/2330 train_time:33188ms step_avg:57.62ms
step:577/2330 train_time:33243ms step_avg:57.61ms
step:578/2330 train_time:33302ms step_avg:57.62ms
step:579/2330 train_time:33359ms step_avg:57.61ms
step:580/2330 train_time:33418ms step_avg:57.62ms
step:581/2330 train_time:33474ms step_avg:57.61ms
step:582/2330 train_time:33533ms step_avg:57.62ms
step:583/2330 train_time:33589ms step_avg:57.61ms
step:584/2330 train_time:33650ms step_avg:57.62ms
step:585/2330 train_time:33706ms step_avg:57.62ms
step:586/2330 train_time:33766ms step_avg:57.62ms
step:587/2330 train_time:33822ms step_avg:57.62ms
step:588/2330 train_time:33881ms step_avg:57.62ms
step:589/2330 train_time:33937ms step_avg:57.62ms
step:590/2330 train_time:33997ms step_avg:57.62ms
step:591/2330 train_time:34053ms step_avg:57.62ms
step:592/2330 train_time:34113ms step_avg:57.62ms
step:593/2330 train_time:34169ms step_avg:57.62ms
step:594/2330 train_time:34229ms step_avg:57.62ms
step:595/2330 train_time:34284ms step_avg:57.62ms
step:596/2330 train_time:34344ms step_avg:57.62ms
step:597/2330 train_time:34400ms step_avg:57.62ms
step:598/2330 train_time:34460ms step_avg:57.63ms
step:599/2330 train_time:34516ms step_avg:57.62ms
step:600/2330 train_time:34576ms step_avg:57.63ms
step:601/2330 train_time:34632ms step_avg:57.62ms
step:602/2330 train_time:34691ms step_avg:57.63ms
step:603/2330 train_time:34746ms step_avg:57.62ms
step:604/2330 train_time:34807ms step_avg:57.63ms
step:605/2330 train_time:34864ms step_avg:57.63ms
step:606/2330 train_time:34923ms step_avg:57.63ms
step:607/2330 train_time:34980ms step_avg:57.63ms
step:608/2330 train_time:35040ms step_avg:57.63ms
step:609/2330 train_time:35095ms step_avg:57.63ms
step:610/2330 train_time:35155ms step_avg:57.63ms
step:611/2330 train_time:35211ms step_avg:57.63ms
step:612/2330 train_time:35273ms step_avg:57.64ms
step:613/2330 train_time:35329ms step_avg:57.63ms
step:614/2330 train_time:35388ms step_avg:57.64ms
step:615/2330 train_time:35444ms step_avg:57.63ms
step:616/2330 train_time:35503ms step_avg:57.64ms
step:617/2330 train_time:35560ms step_avg:57.63ms
step:618/2330 train_time:35619ms step_avg:57.64ms
step:619/2330 train_time:35674ms step_avg:57.63ms
step:620/2330 train_time:35735ms step_avg:57.64ms
step:621/2330 train_time:35791ms step_avg:57.63ms
step:622/2330 train_time:35850ms step_avg:57.64ms
step:623/2330 train_time:35906ms step_avg:57.63ms
step:624/2330 train_time:35967ms step_avg:57.64ms
step:625/2330 train_time:36024ms step_avg:57.64ms
step:626/2330 train_time:36082ms step_avg:57.64ms
step:627/2330 train_time:36139ms step_avg:57.64ms
step:628/2330 train_time:36198ms step_avg:57.64ms
step:629/2330 train_time:36254ms step_avg:57.64ms
step:630/2330 train_time:36314ms step_avg:57.64ms
step:631/2330 train_time:36369ms step_avg:57.64ms
step:632/2330 train_time:36430ms step_avg:57.64ms
step:633/2330 train_time:36485ms step_avg:57.64ms
step:634/2330 train_time:36546ms step_avg:57.64ms
step:635/2330 train_time:36601ms step_avg:57.64ms
step:636/2330 train_time:36660ms step_avg:57.64ms
step:637/2330 train_time:36717ms step_avg:57.64ms
step:638/2330 train_time:36776ms step_avg:57.64ms
step:639/2330 train_time:36831ms step_avg:57.64ms
step:640/2330 train_time:36892ms step_avg:57.64ms
step:641/2330 train_time:36947ms step_avg:57.64ms
step:642/2330 train_time:37009ms step_avg:57.65ms
step:643/2330 train_time:37064ms step_avg:57.64ms
step:644/2330 train_time:37125ms step_avg:57.65ms
step:645/2330 train_time:37181ms step_avg:57.65ms
step:646/2330 train_time:37240ms step_avg:57.65ms
step:647/2330 train_time:37296ms step_avg:57.65ms
step:648/2330 train_time:37356ms step_avg:57.65ms
step:649/2330 train_time:37412ms step_avg:57.65ms
step:650/2330 train_time:37472ms step_avg:57.65ms
step:651/2330 train_time:37528ms step_avg:57.65ms
step:652/2330 train_time:37587ms step_avg:57.65ms
step:653/2330 train_time:37643ms step_avg:57.65ms
step:654/2330 train_time:37702ms step_avg:57.65ms
step:655/2330 train_time:37759ms step_avg:57.65ms
step:656/2330 train_time:37817ms step_avg:57.65ms
step:657/2330 train_time:37873ms step_avg:57.65ms
step:658/2330 train_time:37933ms step_avg:57.65ms
step:659/2330 train_time:37988ms step_avg:57.65ms
step:660/2330 train_time:38049ms step_avg:57.65ms
step:661/2330 train_time:38105ms step_avg:57.65ms
step:662/2330 train_time:38165ms step_avg:57.65ms
step:663/2330 train_time:38221ms step_avg:57.65ms
step:664/2330 train_time:38281ms step_avg:57.65ms
step:665/2330 train_time:38337ms step_avg:57.65ms
step:666/2330 train_time:38397ms step_avg:57.65ms
step:667/2330 train_time:38453ms step_avg:57.65ms
step:668/2330 train_time:38512ms step_avg:57.65ms
step:669/2330 train_time:38568ms step_avg:57.65ms
step:670/2330 train_time:38630ms step_avg:57.66ms
step:671/2330 train_time:38685ms step_avg:57.65ms
step:672/2330 train_time:38745ms step_avg:57.66ms
step:673/2330 train_time:38801ms step_avg:57.65ms
step:674/2330 train_time:38860ms step_avg:57.66ms
step:675/2330 train_time:38917ms step_avg:57.65ms
step:676/2330 train_time:38976ms step_avg:57.66ms
step:677/2330 train_time:39032ms step_avg:57.65ms
step:678/2330 train_time:39092ms step_avg:57.66ms
step:679/2330 train_time:39147ms step_avg:57.65ms
step:680/2330 train_time:39208ms step_avg:57.66ms
step:681/2330 train_time:39264ms step_avg:57.66ms
step:682/2330 train_time:39323ms step_avg:57.66ms
step:683/2330 train_time:39380ms step_avg:57.66ms
step:684/2330 train_time:39439ms step_avg:57.66ms
step:685/2330 train_time:39495ms step_avg:57.66ms
step:686/2330 train_time:39555ms step_avg:57.66ms
step:687/2330 train_time:39611ms step_avg:57.66ms
step:688/2330 train_time:39672ms step_avg:57.66ms
step:689/2330 train_time:39727ms step_avg:57.66ms
step:690/2330 train_time:39787ms step_avg:57.66ms
step:691/2330 train_time:39844ms step_avg:57.66ms
step:692/2330 train_time:39903ms step_avg:57.66ms
step:693/2330 train_time:39959ms step_avg:57.66ms
step:694/2330 train_time:40018ms step_avg:57.66ms
step:695/2330 train_time:40074ms step_avg:57.66ms
step:696/2330 train_time:40134ms step_avg:57.66ms
step:697/2330 train_time:40189ms step_avg:57.66ms
step:698/2330 train_time:40250ms step_avg:57.66ms
step:699/2330 train_time:40305ms step_avg:57.66ms
step:700/2330 train_time:40366ms step_avg:57.67ms
step:701/2330 train_time:40423ms step_avg:57.66ms
step:702/2330 train_time:40482ms step_avg:57.67ms
step:703/2330 train_time:40539ms step_avg:57.67ms
step:704/2330 train_time:40599ms step_avg:57.67ms
step:705/2330 train_time:40655ms step_avg:57.67ms
step:706/2330 train_time:40714ms step_avg:57.67ms
step:707/2330 train_time:40770ms step_avg:57.67ms
step:708/2330 train_time:40830ms step_avg:57.67ms
step:709/2330 train_time:40886ms step_avg:57.67ms
step:710/2330 train_time:40946ms step_avg:57.67ms
step:711/2330 train_time:41002ms step_avg:57.67ms
step:712/2330 train_time:41061ms step_avg:57.67ms
step:713/2330 train_time:41117ms step_avg:57.67ms
step:714/2330 train_time:41176ms step_avg:57.67ms
step:715/2330 train_time:41232ms step_avg:57.67ms
step:716/2330 train_time:41292ms step_avg:57.67ms
step:717/2330 train_time:41347ms step_avg:57.67ms
step:718/2330 train_time:41408ms step_avg:57.67ms
step:719/2330 train_time:41464ms step_avg:57.67ms
step:720/2330 train_time:41523ms step_avg:57.67ms
step:721/2330 train_time:41581ms step_avg:57.67ms
step:722/2330 train_time:41640ms step_avg:57.67ms
step:723/2330 train_time:41697ms step_avg:57.67ms
step:724/2330 train_time:41756ms step_avg:57.67ms
step:725/2330 train_time:41811ms step_avg:57.67ms
step:726/2330 train_time:41871ms step_avg:57.67ms
step:727/2330 train_time:41926ms step_avg:57.67ms
step:728/2330 train_time:41987ms step_avg:57.67ms
step:729/2330 train_time:42043ms step_avg:57.67ms
step:730/2330 train_time:42102ms step_avg:57.67ms
step:731/2330 train_time:42158ms step_avg:57.67ms
step:732/2330 train_time:42218ms step_avg:57.67ms
step:733/2330 train_time:42274ms step_avg:57.67ms
step:734/2330 train_time:42333ms step_avg:57.67ms
step:735/2330 train_time:42389ms step_avg:57.67ms
step:736/2330 train_time:42449ms step_avg:57.67ms
step:737/2330 train_time:42505ms step_avg:57.67ms
step:738/2330 train_time:42565ms step_avg:57.68ms
step:739/2330 train_time:42622ms step_avg:57.68ms
step:740/2330 train_time:42682ms step_avg:57.68ms
step:741/2330 train_time:42738ms step_avg:57.68ms
step:742/2330 train_time:42797ms step_avg:57.68ms
step:743/2330 train_time:42853ms step_avg:57.68ms
step:744/2330 train_time:42912ms step_avg:57.68ms
step:745/2330 train_time:42969ms step_avg:57.68ms
step:746/2330 train_time:43029ms step_avg:57.68ms
step:747/2330 train_time:43085ms step_avg:57.68ms
step:748/2330 train_time:43144ms step_avg:57.68ms
step:749/2330 train_time:43200ms step_avg:57.68ms
step:750/2330 train_time:43260ms step_avg:57.68ms
step:750/2330 val_loss:4.2070 train_time:43340ms step_avg:57.79ms
step:751/2330 train_time:43359ms step_avg:57.74ms
step:752/2330 train_time:43379ms step_avg:57.68ms
step:753/2330 train_time:43439ms step_avg:57.69ms
step:754/2330 train_time:43502ms step_avg:57.69ms
step:755/2330 train_time:43559ms step_avg:57.69ms
step:756/2330 train_time:43619ms step_avg:57.70ms
step:757/2330 train_time:43675ms step_avg:57.70ms
step:758/2330 train_time:43734ms step_avg:57.70ms
step:759/2330 train_time:43790ms step_avg:57.69ms
step:760/2330 train_time:43848ms step_avg:57.70ms
step:761/2330 train_time:43904ms step_avg:57.69ms
step:762/2330 train_time:43963ms step_avg:57.69ms
step:763/2330 train_time:44018ms step_avg:57.69ms
step:764/2330 train_time:44077ms step_avg:57.69ms
step:765/2330 train_time:44134ms step_avg:57.69ms
step:766/2330 train_time:44191ms step_avg:57.69ms
step:767/2330 train_time:44248ms step_avg:57.69ms
step:768/2330 train_time:44308ms step_avg:57.69ms
step:769/2330 train_time:44366ms step_avg:57.69ms
step:770/2330 train_time:44428ms step_avg:57.70ms
step:771/2330 train_time:44486ms step_avg:57.70ms
step:772/2330 train_time:44547ms step_avg:57.70ms
step:773/2330 train_time:44605ms step_avg:57.70ms
step:774/2330 train_time:44666ms step_avg:57.71ms
step:775/2330 train_time:44722ms step_avg:57.71ms
step:776/2330 train_time:44782ms step_avg:57.71ms
step:777/2330 train_time:44839ms step_avg:57.71ms
step:778/2330 train_time:44899ms step_avg:57.71ms
step:779/2330 train_time:44955ms step_avg:57.71ms
step:780/2330 train_time:45014ms step_avg:57.71ms
step:781/2330 train_time:45071ms step_avg:57.71ms
step:782/2330 train_time:45130ms step_avg:57.71ms
step:783/2330 train_time:45186ms step_avg:57.71ms
step:784/2330 train_time:45246ms step_avg:57.71ms
step:785/2330 train_time:45303ms step_avg:57.71ms
step:786/2330 train_time:45364ms step_avg:57.71ms
step:787/2330 train_time:45421ms step_avg:57.71ms
step:788/2330 train_time:45482ms step_avg:57.72ms
step:789/2330 train_time:45538ms step_avg:57.72ms
step:790/2330 train_time:45600ms step_avg:57.72ms
step:791/2330 train_time:45658ms step_avg:57.72ms
step:792/2330 train_time:45718ms step_avg:57.73ms
step:793/2330 train_time:45775ms step_avg:57.72ms
step:794/2330 train_time:45835ms step_avg:57.73ms
step:795/2330 train_time:45892ms step_avg:57.73ms
step:796/2330 train_time:45952ms step_avg:57.73ms
step:797/2330 train_time:46008ms step_avg:57.73ms
step:798/2330 train_time:46068ms step_avg:57.73ms
step:799/2330 train_time:46124ms step_avg:57.73ms
step:800/2330 train_time:46184ms step_avg:57.73ms
step:801/2330 train_time:46240ms step_avg:57.73ms
step:802/2330 train_time:46301ms step_avg:57.73ms
step:803/2330 train_time:46357ms step_avg:57.73ms
step:804/2330 train_time:46419ms step_avg:57.73ms
step:805/2330 train_time:46476ms step_avg:57.73ms
step:806/2330 train_time:46536ms step_avg:57.74ms
step:807/2330 train_time:46594ms step_avg:57.74ms
step:808/2330 train_time:46655ms step_avg:57.74ms
step:809/2330 train_time:46712ms step_avg:57.74ms
step:810/2330 train_time:46772ms step_avg:57.74ms
step:811/2330 train_time:46829ms step_avg:57.74ms
step:812/2330 train_time:46888ms step_avg:57.74ms
step:813/2330 train_time:46944ms step_avg:57.74ms
step:814/2330 train_time:47006ms step_avg:57.75ms
step:815/2330 train_time:47062ms step_avg:57.74ms
step:816/2330 train_time:47121ms step_avg:57.75ms
step:817/2330 train_time:47177ms step_avg:57.74ms
step:818/2330 train_time:47239ms step_avg:57.75ms
step:819/2330 train_time:47295ms step_avg:57.75ms
step:820/2330 train_time:47356ms step_avg:57.75ms
step:821/2330 train_time:47414ms step_avg:57.75ms
step:822/2330 train_time:47473ms step_avg:57.75ms
step:823/2330 train_time:47531ms step_avg:57.75ms
step:824/2330 train_time:47590ms step_avg:57.76ms
step:825/2330 train_time:47648ms step_avg:57.76ms
step:826/2330 train_time:47707ms step_avg:57.76ms
step:827/2330 train_time:47765ms step_avg:57.76ms
step:828/2330 train_time:47825ms step_avg:57.76ms
step:829/2330 train_time:47881ms step_avg:57.76ms
step:830/2330 train_time:47942ms step_avg:57.76ms
step:831/2330 train_time:47999ms step_avg:57.76ms
step:832/2330 train_time:48059ms step_avg:57.76ms
step:833/2330 train_time:48115ms step_avg:57.76ms
step:834/2330 train_time:48175ms step_avg:57.76ms
step:835/2330 train_time:48232ms step_avg:57.76ms
step:836/2330 train_time:48292ms step_avg:57.77ms
step:837/2330 train_time:48349ms step_avg:57.76ms
step:838/2330 train_time:48409ms step_avg:57.77ms
step:839/2330 train_time:48466ms step_avg:57.77ms
step:840/2330 train_time:48525ms step_avg:57.77ms
step:841/2330 train_time:48582ms step_avg:57.77ms
step:842/2330 train_time:48643ms step_avg:57.77ms
step:843/2330 train_time:48700ms step_avg:57.77ms
step:844/2330 train_time:48760ms step_avg:57.77ms
step:845/2330 train_time:48817ms step_avg:57.77ms
step:846/2330 train_time:48877ms step_avg:57.77ms
step:847/2330 train_time:48934ms step_avg:57.77ms
step:848/2330 train_time:48994ms step_avg:57.78ms
step:849/2330 train_time:49051ms step_avg:57.78ms
step:850/2330 train_time:49111ms step_avg:57.78ms
step:851/2330 train_time:49168ms step_avg:57.78ms
step:852/2330 train_time:49228ms step_avg:57.78ms
step:853/2330 train_time:49285ms step_avg:57.78ms
step:854/2330 train_time:49345ms step_avg:57.78ms
step:855/2330 train_time:49401ms step_avg:57.78ms
step:856/2330 train_time:49462ms step_avg:57.78ms
step:857/2330 train_time:49519ms step_avg:57.78ms
step:858/2330 train_time:49580ms step_avg:57.79ms
step:859/2330 train_time:49637ms step_avg:57.78ms
step:860/2330 train_time:49697ms step_avg:57.79ms
step:861/2330 train_time:49754ms step_avg:57.79ms
step:862/2330 train_time:49815ms step_avg:57.79ms
step:863/2330 train_time:49872ms step_avg:57.79ms
step:864/2330 train_time:49932ms step_avg:57.79ms
step:865/2330 train_time:49990ms step_avg:57.79ms
step:866/2330 train_time:50049ms step_avg:57.79ms
step:867/2330 train_time:50106ms step_avg:57.79ms
step:868/2330 train_time:50166ms step_avg:57.80ms
step:869/2330 train_time:50223ms step_avg:57.79ms
step:870/2330 train_time:50283ms step_avg:57.80ms
step:871/2330 train_time:50340ms step_avg:57.80ms
step:872/2330 train_time:50400ms step_avg:57.80ms
step:873/2330 train_time:50458ms step_avg:57.80ms
step:874/2330 train_time:50518ms step_avg:57.80ms
step:875/2330 train_time:50575ms step_avg:57.80ms
step:876/2330 train_time:50635ms step_avg:57.80ms
step:877/2330 train_time:50692ms step_avg:57.80ms
step:878/2330 train_time:50752ms step_avg:57.80ms
step:879/2330 train_time:50810ms step_avg:57.80ms
step:880/2330 train_time:50870ms step_avg:57.81ms
step:881/2330 train_time:50926ms step_avg:57.81ms
step:882/2330 train_time:50985ms step_avg:57.81ms
step:883/2330 train_time:51042ms step_avg:57.81ms
step:884/2330 train_time:51102ms step_avg:57.81ms
step:885/2330 train_time:51159ms step_avg:57.81ms
step:886/2330 train_time:51220ms step_avg:57.81ms
step:887/2330 train_time:51276ms step_avg:57.81ms
step:888/2330 train_time:51337ms step_avg:57.81ms
step:889/2330 train_time:51393ms step_avg:57.81ms
step:890/2330 train_time:51454ms step_avg:57.81ms
step:891/2330 train_time:51511ms step_avg:57.81ms
step:892/2330 train_time:51570ms step_avg:57.81ms
step:893/2330 train_time:51626ms step_avg:57.81ms
step:894/2330 train_time:51687ms step_avg:57.82ms
step:895/2330 train_time:51744ms step_avg:57.81ms
step:896/2330 train_time:51805ms step_avg:57.82ms
step:897/2330 train_time:51861ms step_avg:57.82ms
step:898/2330 train_time:51922ms step_avg:57.82ms
step:899/2330 train_time:51978ms step_avg:57.82ms
step:900/2330 train_time:52039ms step_avg:57.82ms
step:901/2330 train_time:52096ms step_avg:57.82ms
step:902/2330 train_time:52158ms step_avg:57.82ms
step:903/2330 train_time:52214ms step_avg:57.82ms
step:904/2330 train_time:52275ms step_avg:57.83ms
step:905/2330 train_time:52332ms step_avg:57.83ms
step:906/2330 train_time:52393ms step_avg:57.83ms
step:907/2330 train_time:52450ms step_avg:57.83ms
step:908/2330 train_time:52510ms step_avg:57.83ms
step:909/2330 train_time:52568ms step_avg:57.83ms
step:910/2330 train_time:52627ms step_avg:57.83ms
step:911/2330 train_time:52684ms step_avg:57.83ms
step:912/2330 train_time:52745ms step_avg:57.83ms
step:913/2330 train_time:52803ms step_avg:57.83ms
step:914/2330 train_time:52862ms step_avg:57.84ms
step:915/2330 train_time:52919ms step_avg:57.83ms
step:916/2330 train_time:52980ms step_avg:57.84ms
step:917/2330 train_time:53037ms step_avg:57.84ms
step:918/2330 train_time:53098ms step_avg:57.84ms
step:919/2330 train_time:53155ms step_avg:57.84ms
step:920/2330 train_time:53215ms step_avg:57.84ms
step:921/2330 train_time:53272ms step_avg:57.84ms
step:922/2330 train_time:53332ms step_avg:57.84ms
step:923/2330 train_time:53389ms step_avg:57.84ms
step:924/2330 train_time:53449ms step_avg:57.84ms
step:925/2330 train_time:53505ms step_avg:57.84ms
step:926/2330 train_time:53565ms step_avg:57.85ms
step:927/2330 train_time:53623ms step_avg:57.85ms
step:928/2330 train_time:53682ms step_avg:57.85ms
step:929/2330 train_time:53739ms step_avg:57.85ms
step:930/2330 train_time:53799ms step_avg:57.85ms
step:931/2330 train_time:53856ms step_avg:57.85ms
step:932/2330 train_time:53918ms step_avg:57.85ms
step:933/2330 train_time:53975ms step_avg:57.85ms
step:934/2330 train_time:54035ms step_avg:57.85ms
step:935/2330 train_time:54092ms step_avg:57.85ms
step:936/2330 train_time:54152ms step_avg:57.85ms
step:937/2330 train_time:54210ms step_avg:57.86ms
step:938/2330 train_time:54270ms step_avg:57.86ms
step:939/2330 train_time:54326ms step_avg:57.86ms
step:940/2330 train_time:54387ms step_avg:57.86ms
step:941/2330 train_time:54444ms step_avg:57.86ms
step:942/2330 train_time:54503ms step_avg:57.86ms
step:943/2330 train_time:54560ms step_avg:57.86ms
step:944/2330 train_time:54620ms step_avg:57.86ms
step:945/2330 train_time:54676ms step_avg:57.86ms
step:946/2330 train_time:54738ms step_avg:57.86ms
step:947/2330 train_time:54795ms step_avg:57.86ms
step:948/2330 train_time:54855ms step_avg:57.86ms
step:949/2330 train_time:54913ms step_avg:57.86ms
step:950/2330 train_time:54973ms step_avg:57.87ms
step:951/2330 train_time:55030ms step_avg:57.87ms
step:952/2330 train_time:55090ms step_avg:57.87ms
step:953/2330 train_time:55147ms step_avg:57.87ms
step:954/2330 train_time:55207ms step_avg:57.87ms
step:955/2330 train_time:55264ms step_avg:57.87ms
step:956/2330 train_time:55324ms step_avg:57.87ms
step:957/2330 train_time:55381ms step_avg:57.87ms
step:958/2330 train_time:55442ms step_avg:57.87ms
step:959/2330 train_time:55500ms step_avg:57.87ms
step:960/2330 train_time:55559ms step_avg:57.87ms
step:961/2330 train_time:55616ms step_avg:57.87ms
step:962/2330 train_time:55676ms step_avg:57.88ms
step:963/2330 train_time:55733ms step_avg:57.87ms
step:964/2330 train_time:55793ms step_avg:57.88ms
step:965/2330 train_time:55850ms step_avg:57.88ms
step:966/2330 train_time:55910ms step_avg:57.88ms
step:967/2330 train_time:55967ms step_avg:57.88ms
step:968/2330 train_time:56027ms step_avg:57.88ms
step:969/2330 train_time:56084ms step_avg:57.88ms
step:970/2330 train_time:56144ms step_avg:57.88ms
step:971/2330 train_time:56201ms step_avg:57.88ms
step:972/2330 train_time:56261ms step_avg:57.88ms
step:973/2330 train_time:56318ms step_avg:57.88ms
step:974/2330 train_time:56379ms step_avg:57.88ms
step:975/2330 train_time:56435ms step_avg:57.88ms
step:976/2330 train_time:56496ms step_avg:57.89ms
step:977/2330 train_time:56553ms step_avg:57.88ms
step:978/2330 train_time:56614ms step_avg:57.89ms
step:979/2330 train_time:56670ms step_avg:57.89ms
step:980/2330 train_time:56731ms step_avg:57.89ms
step:981/2330 train_time:56787ms step_avg:57.89ms
step:982/2330 train_time:56848ms step_avg:57.89ms
step:983/2330 train_time:56905ms step_avg:57.89ms
step:984/2330 train_time:56966ms step_avg:57.89ms
step:985/2330 train_time:57023ms step_avg:57.89ms
step:986/2330 train_time:57083ms step_avg:57.89ms
step:987/2330 train_time:57140ms step_avg:57.89ms
step:988/2330 train_time:57200ms step_avg:57.89ms
step:989/2330 train_time:57257ms step_avg:57.89ms
step:990/2330 train_time:57317ms step_avg:57.90ms
step:991/2330 train_time:57374ms step_avg:57.89ms
step:992/2330 train_time:57433ms step_avg:57.90ms
step:993/2330 train_time:57491ms step_avg:57.90ms
step:994/2330 train_time:57551ms step_avg:57.90ms
step:995/2330 train_time:57608ms step_avg:57.90ms
step:996/2330 train_time:57668ms step_avg:57.90ms
step:997/2330 train_time:57724ms step_avg:57.90ms
step:998/2330 train_time:57784ms step_avg:57.90ms
step:999/2330 train_time:57840ms step_avg:57.90ms
step:1000/2330 train_time:57901ms step_avg:57.90ms
step:1000/2330 val_loss:4.0747 train_time:57982ms step_avg:57.98ms
step:1001/2330 train_time:58002ms step_avg:57.94ms
step:1002/2330 train_time:58022ms step_avg:57.91ms
step:1003/2330 train_time:58075ms step_avg:57.90ms
step:1004/2330 train_time:58144ms step_avg:57.91ms
step:1005/2330 train_time:58200ms step_avg:57.91ms
step:1006/2330 train_time:58261ms step_avg:57.91ms
step:1007/2330 train_time:58317ms step_avg:57.91ms
step:1008/2330 train_time:58377ms step_avg:57.91ms
step:1009/2330 train_time:58433ms step_avg:57.91ms
step:1010/2330 train_time:58493ms step_avg:57.91ms
step:1011/2330 train_time:58549ms step_avg:57.91ms
step:1012/2330 train_time:58609ms step_avg:57.91ms
step:1013/2330 train_time:58665ms step_avg:57.91ms
step:1014/2330 train_time:58724ms step_avg:57.91ms
step:1015/2330 train_time:58780ms step_avg:57.91ms
step:1016/2330 train_time:58840ms step_avg:57.91ms
step:1017/2330 train_time:58901ms step_avg:57.92ms
step:1018/2330 train_time:58963ms step_avg:57.92ms
step:1019/2330 train_time:59021ms step_avg:57.92ms
step:1020/2330 train_time:59081ms step_avg:57.92ms
step:1021/2330 train_time:59138ms step_avg:57.92ms
step:1022/2330 train_time:59200ms step_avg:57.93ms
step:1023/2330 train_time:59257ms step_avg:57.92ms
step:1024/2330 train_time:59317ms step_avg:57.93ms
step:1025/2330 train_time:59374ms step_avg:57.93ms
step:1026/2330 train_time:59434ms step_avg:57.93ms
step:1027/2330 train_time:59490ms step_avg:57.93ms
step:1028/2330 train_time:59551ms step_avg:57.93ms
step:1029/2330 train_time:59607ms step_avg:57.93ms
step:1030/2330 train_time:59667ms step_avg:57.93ms
step:1031/2330 train_time:59723ms step_avg:57.93ms
step:1032/2330 train_time:59783ms step_avg:57.93ms
step:1033/2330 train_time:59841ms step_avg:57.93ms
step:1034/2330 train_time:59901ms step_avg:57.93ms
step:1035/2330 train_time:59960ms step_avg:57.93ms
step:1036/2330 train_time:60021ms step_avg:57.94ms
step:1037/2330 train_time:60079ms step_avg:57.94ms
step:1038/2330 train_time:60139ms step_avg:57.94ms
step:1039/2330 train_time:60196ms step_avg:57.94ms
step:1040/2330 train_time:60257ms step_avg:57.94ms
step:1041/2330 train_time:60314ms step_avg:57.94ms
step:1042/2330 train_time:60374ms step_avg:57.94ms
step:1043/2330 train_time:60431ms step_avg:57.94ms
step:1044/2330 train_time:60491ms step_avg:57.94ms
step:1045/2330 train_time:60548ms step_avg:57.94ms
step:1046/2330 train_time:60608ms step_avg:57.94ms
step:1047/2330 train_time:60663ms step_avg:57.94ms
step:1048/2330 train_time:60724ms step_avg:57.94ms
step:1049/2330 train_time:60780ms step_avg:57.94ms
step:1050/2330 train_time:60842ms step_avg:57.94ms
step:1051/2330 train_time:60900ms step_avg:57.94ms
step:1052/2330 train_time:60960ms step_avg:57.95ms
step:1053/2330 train_time:61017ms step_avg:57.95ms
step:1054/2330 train_time:61078ms step_avg:57.95ms
step:1055/2330 train_time:61136ms step_avg:57.95ms
step:1056/2330 train_time:61196ms step_avg:57.95ms
step:1057/2330 train_time:61253ms step_avg:57.95ms
step:1058/2330 train_time:61313ms step_avg:57.95ms
step:1059/2330 train_time:61370ms step_avg:57.95ms
step:1060/2330 train_time:61430ms step_avg:57.95ms
step:1061/2330 train_time:61487ms step_avg:57.95ms
step:1062/2330 train_time:61547ms step_avg:57.95ms
step:1063/2330 train_time:61604ms step_avg:57.95ms
step:1064/2330 train_time:61664ms step_avg:57.95ms
step:1065/2330 train_time:61720ms step_avg:57.95ms
step:1066/2330 train_time:61781ms step_avg:57.96ms
step:1067/2330 train_time:61837ms step_avg:57.95ms
step:1068/2330 train_time:61898ms step_avg:57.96ms
step:1069/2330 train_time:61956ms step_avg:57.96ms
step:1070/2330 train_time:62016ms step_avg:57.96ms
step:1071/2330 train_time:62073ms step_avg:57.96ms
step:1072/2330 train_time:62133ms step_avg:57.96ms
step:1073/2330 train_time:62190ms step_avg:57.96ms
step:1074/2330 train_time:62251ms step_avg:57.96ms
step:1075/2330 train_time:62309ms step_avg:57.96ms
step:1076/2330 train_time:62368ms step_avg:57.96ms
step:1077/2330 train_time:62424ms step_avg:57.96ms
step:1078/2330 train_time:62485ms step_avg:57.96ms
step:1079/2330 train_time:62541ms step_avg:57.96ms
step:1080/2330 train_time:62601ms step_avg:57.96ms
step:1081/2330 train_time:62658ms step_avg:57.96ms
step:1082/2330 train_time:62718ms step_avg:57.97ms
step:1083/2330 train_time:62775ms step_avg:57.96ms
step:1084/2330 train_time:62835ms step_avg:57.97ms
step:1085/2330 train_time:62892ms step_avg:57.97ms
step:1086/2330 train_time:62952ms step_avg:57.97ms
step:1087/2330 train_time:63009ms step_avg:57.97ms
step:1088/2330 train_time:63070ms step_avg:57.97ms
step:1089/2330 train_time:63127ms step_avg:57.97ms
step:1090/2330 train_time:63187ms step_avg:57.97ms
step:1091/2330 train_time:63245ms step_avg:57.97ms
step:1092/2330 train_time:63306ms step_avg:57.97ms
step:1093/2330 train_time:63363ms step_avg:57.97ms
step:1094/2330 train_time:63423ms step_avg:57.97ms
step:1095/2330 train_time:63480ms step_avg:57.97ms
step:1096/2330 train_time:63540ms step_avg:57.97ms
step:1097/2330 train_time:63597ms step_avg:57.97ms
step:1098/2330 train_time:63658ms step_avg:57.98ms
step:1099/2330 train_time:63715ms step_avg:57.98ms
step:1100/2330 train_time:63775ms step_avg:57.98ms
step:1101/2330 train_time:63832ms step_avg:57.98ms
step:1102/2330 train_time:63892ms step_avg:57.98ms
step:1103/2330 train_time:63950ms step_avg:57.98ms
step:1104/2330 train_time:64009ms step_avg:57.98ms
step:1105/2330 train_time:64066ms step_avg:57.98ms
step:1106/2330 train_time:64126ms step_avg:57.98ms
step:1107/2330 train_time:64183ms step_avg:57.98ms
step:1108/2330 train_time:64244ms step_avg:57.98ms
step:1109/2330 train_time:64301ms step_avg:57.98ms
step:1110/2330 train_time:64361ms step_avg:57.98ms
step:1111/2330 train_time:64418ms step_avg:57.98ms
step:1112/2330 train_time:64478ms step_avg:57.98ms
step:1113/2330 train_time:64535ms step_avg:57.98ms
step:1114/2330 train_time:64595ms step_avg:57.98ms
step:1115/2330 train_time:64652ms step_avg:57.98ms
step:1116/2330 train_time:64712ms step_avg:57.99ms
step:1117/2330 train_time:64769ms step_avg:57.98ms
step:1118/2330 train_time:64829ms step_avg:57.99ms
step:1119/2330 train_time:64887ms step_avg:57.99ms
step:1120/2330 train_time:64946ms step_avg:57.99ms
step:1121/2330 train_time:65004ms step_avg:57.99ms
step:1122/2330 train_time:65064ms step_avg:57.99ms
step:1123/2330 train_time:65121ms step_avg:57.99ms
step:1124/2330 train_time:65181ms step_avg:57.99ms
step:1125/2330 train_time:65239ms step_avg:57.99ms
step:1126/2330 train_time:65299ms step_avg:57.99ms
step:1127/2330 train_time:65357ms step_avg:57.99ms
step:1128/2330 train_time:65417ms step_avg:57.99ms
step:1129/2330 train_time:65474ms step_avg:57.99ms
step:1130/2330 train_time:65534ms step_avg:57.99ms
step:1131/2330 train_time:65591ms step_avg:57.99ms
step:1132/2330 train_time:65651ms step_avg:58.00ms
step:1133/2330 train_time:65709ms step_avg:58.00ms
step:1134/2330 train_time:65768ms step_avg:58.00ms
step:1135/2330 train_time:65825ms step_avg:58.00ms
step:1136/2330 train_time:65885ms step_avg:58.00ms
step:1137/2330 train_time:65942ms step_avg:58.00ms
step:1138/2330 train_time:66003ms step_avg:58.00ms
step:1139/2330 train_time:66060ms step_avg:58.00ms
step:1140/2330 train_time:66120ms step_avg:58.00ms
step:1141/2330 train_time:66177ms step_avg:58.00ms
step:1142/2330 train_time:66237ms step_avg:58.00ms
step:1143/2330 train_time:66294ms step_avg:58.00ms
step:1144/2330 train_time:66354ms step_avg:58.00ms
step:1145/2330 train_time:66410ms step_avg:58.00ms
step:1146/2330 train_time:66471ms step_avg:58.00ms
step:1147/2330 train_time:66527ms step_avg:58.00ms
step:1148/2330 train_time:66589ms step_avg:58.00ms
step:1149/2330 train_time:66646ms step_avg:58.00ms
step:1150/2330 train_time:66706ms step_avg:58.01ms
step:1151/2330 train_time:66763ms step_avg:58.00ms
step:1152/2330 train_time:66823ms step_avg:58.01ms
step:1153/2330 train_time:66880ms step_avg:58.01ms
step:1154/2330 train_time:66940ms step_avg:58.01ms
step:1155/2330 train_time:66997ms step_avg:58.01ms
step:1156/2330 train_time:67058ms step_avg:58.01ms
step:1157/2330 train_time:67114ms step_avg:58.01ms
step:1158/2330 train_time:67175ms step_avg:58.01ms
step:1159/2330 train_time:67232ms step_avg:58.01ms
step:1160/2330 train_time:67292ms step_avg:58.01ms
step:1161/2330 train_time:67349ms step_avg:58.01ms
step:1162/2330 train_time:67410ms step_avg:58.01ms
step:1163/2330 train_time:67467ms step_avg:58.01ms
step:1164/2330 train_time:67527ms step_avg:58.01ms
step:1165/2330 train_time:67584ms step_avg:58.01ms
step:1166/2330 train_time:67645ms step_avg:58.01ms
step:1167/2330 train_time:67701ms step_avg:58.01ms
step:1168/2330 train_time:67762ms step_avg:58.02ms
step:1169/2330 train_time:67819ms step_avg:58.01ms
step:1170/2330 train_time:67880ms step_avg:58.02ms
step:1171/2330 train_time:67937ms step_avg:58.02ms
step:1172/2330 train_time:67997ms step_avg:58.02ms
step:1173/2330 train_time:68054ms step_avg:58.02ms
step:1174/2330 train_time:68114ms step_avg:58.02ms
step:1175/2330 train_time:68171ms step_avg:58.02ms
step:1176/2330 train_time:68231ms step_avg:58.02ms
step:1177/2330 train_time:68288ms step_avg:58.02ms
step:1178/2330 train_time:68347ms step_avg:58.02ms
step:1179/2330 train_time:68405ms step_avg:58.02ms
step:1180/2330 train_time:68465ms step_avg:58.02ms
step:1181/2330 train_time:68522ms step_avg:58.02ms
step:1182/2330 train_time:68582ms step_avg:58.02ms
step:1183/2330 train_time:68639ms step_avg:58.02ms
step:1184/2330 train_time:68700ms step_avg:58.02ms
step:1185/2330 train_time:68757ms step_avg:58.02ms
step:1186/2330 train_time:68817ms step_avg:58.02ms
step:1187/2330 train_time:68875ms step_avg:58.02ms
step:1188/2330 train_time:68935ms step_avg:58.03ms
step:1189/2330 train_time:68992ms step_avg:58.03ms
step:1190/2330 train_time:69051ms step_avg:58.03ms
step:1191/2330 train_time:69108ms step_avg:58.03ms
step:1192/2330 train_time:69169ms step_avg:58.03ms
step:1193/2330 train_time:69226ms step_avg:58.03ms
step:1194/2330 train_time:69286ms step_avg:58.03ms
step:1195/2330 train_time:69343ms step_avg:58.03ms
step:1196/2330 train_time:69404ms step_avg:58.03ms
step:1197/2330 train_time:69461ms step_avg:58.03ms
step:1198/2330 train_time:69521ms step_avg:58.03ms
step:1199/2330 train_time:69578ms step_avg:58.03ms
step:1200/2330 train_time:69639ms step_avg:58.03ms
step:1201/2330 train_time:69696ms step_avg:58.03ms
step:1202/2330 train_time:69756ms step_avg:58.03ms
step:1203/2330 train_time:69813ms step_avg:58.03ms
step:1204/2330 train_time:69874ms step_avg:58.03ms
step:1205/2330 train_time:69930ms step_avg:58.03ms
step:1206/2330 train_time:69990ms step_avg:58.04ms
step:1207/2330 train_time:70047ms step_avg:58.03ms
step:1208/2330 train_time:70108ms step_avg:58.04ms
step:1209/2330 train_time:70164ms step_avg:58.03ms
step:1210/2330 train_time:70225ms step_avg:58.04ms
step:1211/2330 train_time:70282ms step_avg:58.04ms
step:1212/2330 train_time:70343ms step_avg:58.04ms
step:1213/2330 train_time:70400ms step_avg:58.04ms
step:1214/2330 train_time:70460ms step_avg:58.04ms
step:1215/2330 train_time:70517ms step_avg:58.04ms
step:1216/2330 train_time:70577ms step_avg:58.04ms
step:1217/2330 train_time:70633ms step_avg:58.04ms
step:1218/2330 train_time:70694ms step_avg:58.04ms
step:1219/2330 train_time:70750ms step_avg:58.04ms
step:1220/2330 train_time:70812ms step_avg:58.04ms
step:1221/2330 train_time:70869ms step_avg:58.04ms
step:1222/2330 train_time:70928ms step_avg:58.04ms
step:1223/2330 train_time:70985ms step_avg:58.04ms
step:1224/2330 train_time:71046ms step_avg:58.04ms
step:1225/2330 train_time:71103ms step_avg:58.04ms
step:1226/2330 train_time:71163ms step_avg:58.04ms
step:1227/2330 train_time:71220ms step_avg:58.04ms
step:1228/2330 train_time:71281ms step_avg:58.05ms
step:1229/2330 train_time:71337ms step_avg:58.04ms
step:1230/2330 train_time:71397ms step_avg:58.05ms
step:1231/2330 train_time:71454ms step_avg:58.05ms
step:1232/2330 train_time:71515ms step_avg:58.05ms
step:1233/2330 train_time:71571ms step_avg:58.05ms
step:1234/2330 train_time:71632ms step_avg:58.05ms
step:1235/2330 train_time:71689ms step_avg:58.05ms
step:1236/2330 train_time:71750ms step_avg:58.05ms
step:1237/2330 train_time:71806ms step_avg:58.05ms
step:1238/2330 train_time:71867ms step_avg:58.05ms
step:1239/2330 train_time:71923ms step_avg:58.05ms
step:1240/2330 train_time:71983ms step_avg:58.05ms
step:1241/2330 train_time:72040ms step_avg:58.05ms
step:1242/2330 train_time:72101ms step_avg:58.05ms
step:1243/2330 train_time:72157ms step_avg:58.05ms
step:1244/2330 train_time:72217ms step_avg:58.05ms
step:1245/2330 train_time:72274ms step_avg:58.05ms
step:1246/2330 train_time:72334ms step_avg:58.05ms
step:1247/2330 train_time:72391ms step_avg:58.05ms
step:1248/2330 train_time:72451ms step_avg:58.05ms
step:1249/2330 train_time:72509ms step_avg:58.05ms
step:1250/2330 train_time:72568ms step_avg:58.05ms
step:1250/2330 val_loss:3.9906 train_time:72649ms step_avg:58.12ms
step:1251/2330 train_time:72668ms step_avg:58.09ms
step:1252/2330 train_time:72689ms step_avg:58.06ms
step:1253/2330 train_time:72748ms step_avg:58.06ms
step:1254/2330 train_time:72812ms step_avg:58.06ms
step:1255/2330 train_time:72868ms step_avg:58.06ms
step:1256/2330 train_time:72932ms step_avg:58.07ms
step:1257/2330 train_time:72988ms step_avg:58.07ms
step:1258/2330 train_time:73049ms step_avg:58.07ms
step:1259/2330 train_time:73105ms step_avg:58.07ms
step:1260/2330 train_time:73165ms step_avg:58.07ms
step:1261/2330 train_time:73221ms step_avg:58.07ms
step:1262/2330 train_time:73282ms step_avg:58.07ms
step:1263/2330 train_time:73338ms step_avg:58.07ms
step:1264/2330 train_time:73399ms step_avg:58.07ms
step:1265/2330 train_time:73454ms step_avg:58.07ms
step:1266/2330 train_time:73514ms step_avg:58.07ms
step:1267/2330 train_time:73571ms step_avg:58.07ms
step:1268/2330 train_time:73631ms step_avg:58.07ms
step:1269/2330 train_time:73690ms step_avg:58.07ms
step:1270/2330 train_time:73752ms step_avg:58.07ms
step:1271/2330 train_time:73810ms step_avg:58.07ms
step:1272/2330 train_time:73871ms step_avg:58.08ms
step:1273/2330 train_time:73928ms step_avg:58.07ms
step:1274/2330 train_time:73989ms step_avg:58.08ms
step:1275/2330 train_time:74046ms step_avg:58.08ms
step:1276/2330 train_time:74107ms step_avg:58.08ms
step:1277/2330 train_time:74163ms step_avg:58.08ms
step:1278/2330 train_time:74224ms step_avg:58.08ms
step:1279/2330 train_time:74281ms step_avg:58.08ms
step:1280/2330 train_time:74340ms step_avg:58.08ms
step:1281/2330 train_time:74396ms step_avg:58.08ms
step:1282/2330 train_time:74457ms step_avg:58.08ms
step:1283/2330 train_time:74514ms step_avg:58.08ms
step:1284/2330 train_time:74574ms step_avg:58.08ms
step:1285/2330 train_time:74631ms step_avg:58.08ms
step:1286/2330 train_time:74691ms step_avg:58.08ms
step:1287/2330 train_time:74748ms step_avg:58.08ms
step:1288/2330 train_time:74810ms step_avg:58.08ms
step:1289/2330 train_time:74866ms step_avg:58.08ms
step:1290/2330 train_time:74929ms step_avg:58.08ms
step:1291/2330 train_time:74985ms step_avg:58.08ms
step:1292/2330 train_time:75046ms step_avg:58.09ms
step:1293/2330 train_time:75103ms step_avg:58.08ms
step:1294/2330 train_time:75544ms step_avg:58.38ms
step:1295/2330 train_time:75600ms step_avg:58.38ms
step:1296/2330 train_time:75659ms step_avg:58.38ms
step:1297/2330 train_time:75715ms step_avg:58.38ms
step:1298/2330 train_time:75774ms step_avg:58.38ms
step:1299/2330 train_time:75830ms step_avg:58.38ms
step:1300/2330 train_time:75889ms step_avg:58.38ms
step:1301/2330 train_time:75946ms step_avg:58.38ms
step:1302/2330 train_time:76005ms step_avg:58.38ms
step:1303/2330 train_time:76061ms step_avg:58.37ms
step:1304/2330 train_time:76120ms step_avg:58.37ms
step:1305/2330 train_time:76176ms step_avg:58.37ms
step:1306/2330 train_time:76236ms step_avg:58.37ms
step:1307/2330 train_time:76292ms step_avg:58.37ms
step:1308/2330 train_time:76351ms step_avg:58.37ms
step:1309/2330 train_time:76410ms step_avg:58.37ms
step:1310/2330 train_time:76479ms step_avg:58.38ms
step:1311/2330 train_time:76536ms step_avg:58.38ms
step:1312/2330 train_time:76597ms step_avg:58.38ms
step:1313/2330 train_time:76654ms step_avg:58.38ms
step:1314/2330 train_time:76714ms step_avg:58.38ms
step:1315/2330 train_time:76771ms step_avg:58.38ms
step:1316/2330 train_time:76831ms step_avg:58.38ms
step:1317/2330 train_time:76887ms step_avg:58.38ms
step:1318/2330 train_time:76947ms step_avg:58.38ms
step:1319/2330 train_time:77003ms step_avg:58.38ms
step:1320/2330 train_time:77062ms step_avg:58.38ms
step:1321/2330 train_time:77118ms step_avg:58.38ms
step:1322/2330 train_time:77178ms step_avg:58.38ms
step:1323/2330 train_time:77235ms step_avg:58.38ms
step:1324/2330 train_time:77294ms step_avg:58.38ms
step:1325/2330 train_time:77351ms step_avg:58.38ms
step:1326/2330 train_time:77414ms step_avg:58.38ms
step:1327/2330 train_time:77472ms step_avg:58.38ms
step:1328/2330 train_time:77534ms step_avg:58.38ms
step:1329/2330 train_time:77591ms step_avg:58.38ms
step:1330/2330 train_time:77651ms step_avg:58.38ms
step:1331/2330 train_time:77708ms step_avg:58.38ms
step:1332/2330 train_time:77769ms step_avg:58.38ms
step:1333/2330 train_time:77826ms step_avg:58.38ms
step:1334/2330 train_time:77885ms step_avg:58.38ms
step:1335/2330 train_time:77942ms step_avg:58.38ms
step:1336/2330 train_time:78001ms step_avg:58.38ms
step:1337/2330 train_time:78058ms step_avg:58.38ms
step:1338/2330 train_time:78118ms step_avg:58.38ms
step:1339/2330 train_time:78174ms step_avg:58.38ms
step:1340/2330 train_time:78233ms step_avg:58.38ms
step:1341/2330 train_time:78290ms step_avg:58.38ms
step:1342/2330 train_time:78352ms step_avg:58.38ms
step:1343/2330 train_time:78409ms step_avg:58.38ms
step:1344/2330 train_time:78471ms step_avg:58.39ms
step:1345/2330 train_time:78528ms step_avg:58.38ms
step:1346/2330 train_time:78590ms step_avg:58.39ms
step:1347/2330 train_time:78647ms step_avg:58.39ms
step:1348/2330 train_time:78708ms step_avg:58.39ms
step:1349/2330 train_time:78765ms step_avg:58.39ms
step:1350/2330 train_time:78825ms step_avg:58.39ms
step:1351/2330 train_time:78882ms step_avg:58.39ms
step:1352/2330 train_time:78942ms step_avg:58.39ms
step:1353/2330 train_time:78999ms step_avg:58.39ms
step:1354/2330 train_time:79058ms step_avg:58.39ms
step:1355/2330 train_time:79116ms step_avg:58.39ms
step:1356/2330 train_time:79175ms step_avg:58.39ms
step:1357/2330 train_time:79233ms step_avg:58.39ms
step:1358/2330 train_time:79292ms step_avg:58.39ms
step:1359/2330 train_time:79348ms step_avg:58.39ms
step:1360/2330 train_time:79409ms step_avg:58.39ms
step:1361/2330 train_time:79467ms step_avg:58.39ms
step:1362/2330 train_time:79528ms step_avg:58.39ms
step:1363/2330 train_time:79586ms step_avg:58.39ms
step:1364/2330 train_time:79646ms step_avg:58.39ms
step:1365/2330 train_time:79703ms step_avg:58.39ms
step:1366/2330 train_time:79764ms step_avg:58.39ms
step:1367/2330 train_time:79821ms step_avg:58.39ms
step:1368/2330 train_time:79880ms step_avg:58.39ms
step:1369/2330 train_time:79937ms step_avg:58.39ms
step:1370/2330 train_time:79996ms step_avg:58.39ms
step:1371/2330 train_time:80053ms step_avg:58.39ms
step:1372/2330 train_time:80113ms step_avg:58.39ms
step:1373/2330 train_time:80170ms step_avg:58.39ms
step:1374/2330 train_time:80231ms step_avg:58.39ms
step:1375/2330 train_time:80288ms step_avg:58.39ms
step:1376/2330 train_time:80347ms step_avg:58.39ms
step:1377/2330 train_time:80405ms step_avg:58.39ms
step:1378/2330 train_time:80465ms step_avg:58.39ms
step:1379/2330 train_time:80523ms step_avg:58.39ms
step:1380/2330 train_time:80583ms step_avg:58.39ms
step:1381/2330 train_time:80640ms step_avg:58.39ms
step:1382/2330 train_time:80701ms step_avg:58.39ms
step:1383/2330 train_time:80758ms step_avg:58.39ms
step:1384/2330 train_time:80818ms step_avg:58.39ms
step:1385/2330 train_time:80875ms step_avg:58.39ms
step:1386/2330 train_time:80935ms step_avg:58.39ms
step:1387/2330 train_time:80992ms step_avg:58.39ms
step:1388/2330 train_time:81051ms step_avg:58.39ms
step:1389/2330 train_time:81108ms step_avg:58.39ms
step:1390/2330 train_time:81168ms step_avg:58.39ms
step:1391/2330 train_time:81225ms step_avg:58.39ms
step:1392/2330 train_time:81284ms step_avg:58.39ms
step:1393/2330 train_time:81342ms step_avg:58.39ms
step:1394/2330 train_time:81402ms step_avg:58.39ms
step:1395/2330 train_time:81460ms step_avg:58.39ms
step:1396/2330 train_time:81519ms step_avg:58.39ms
step:1397/2330 train_time:81577ms step_avg:58.39ms
step:1398/2330 train_time:81637ms step_avg:58.40ms
step:1399/2330 train_time:81694ms step_avg:58.39ms
step:1400/2330 train_time:81755ms step_avg:58.40ms
step:1401/2330 train_time:81812ms step_avg:58.40ms
step:1402/2330 train_time:81871ms step_avg:58.40ms
step:1403/2330 train_time:81928ms step_avg:58.40ms
step:1404/2330 train_time:81989ms step_avg:58.40ms
step:1405/2330 train_time:82046ms step_avg:58.40ms
step:1406/2330 train_time:82105ms step_avg:58.40ms
step:1407/2330 train_time:82162ms step_avg:58.40ms
step:1408/2330 train_time:82222ms step_avg:58.40ms
step:1409/2330 train_time:82280ms step_avg:58.40ms
step:1410/2330 train_time:82339ms step_avg:58.40ms
step:1411/2330 train_time:82397ms step_avg:58.40ms
step:1412/2330 train_time:82456ms step_avg:58.40ms
step:1413/2330 train_time:82514ms step_avg:58.40ms
step:1414/2330 train_time:82574ms step_avg:58.40ms
step:1415/2330 train_time:82631ms step_avg:58.40ms
step:1416/2330 train_time:82691ms step_avg:58.40ms
step:1417/2330 train_time:82748ms step_avg:58.40ms
step:1418/2330 train_time:82809ms step_avg:58.40ms
step:1419/2330 train_time:82866ms step_avg:58.40ms
step:1420/2330 train_time:82926ms step_avg:58.40ms
step:1421/2330 train_time:82984ms step_avg:58.40ms
step:1422/2330 train_time:83043ms step_avg:58.40ms
step:1423/2330 train_time:83100ms step_avg:58.40ms
step:1424/2330 train_time:83160ms step_avg:58.40ms
step:1425/2330 train_time:83216ms step_avg:58.40ms
step:1426/2330 train_time:83277ms step_avg:58.40ms
step:1427/2330 train_time:83335ms step_avg:58.40ms
step:1428/2330 train_time:83395ms step_avg:58.40ms
step:1429/2330 train_time:83452ms step_avg:58.40ms
step:1430/2330 train_time:83512ms step_avg:58.40ms
step:1431/2330 train_time:83569ms step_avg:58.40ms
step:1432/2330 train_time:83630ms step_avg:58.40ms
step:1433/2330 train_time:83687ms step_avg:58.40ms
step:1434/2330 train_time:83748ms step_avg:58.40ms
step:1435/2330 train_time:83805ms step_avg:58.40ms
step:1436/2330 train_time:83865ms step_avg:58.40ms
step:1437/2330 train_time:83922ms step_avg:58.40ms
step:1438/2330 train_time:83982ms step_avg:58.40ms
step:1439/2330 train_time:84040ms step_avg:58.40ms
step:1440/2330 train_time:84100ms step_avg:58.40ms
step:1441/2330 train_time:84157ms step_avg:58.40ms
step:1442/2330 train_time:84216ms step_avg:58.40ms
step:1443/2330 train_time:84274ms step_avg:58.40ms
step:1444/2330 train_time:84334ms step_avg:58.40ms
step:1445/2330 train_time:84391ms step_avg:58.40ms
step:1446/2330 train_time:84450ms step_avg:58.40ms
step:1447/2330 train_time:84507ms step_avg:58.40ms
step:1448/2330 train_time:84567ms step_avg:58.40ms
step:1449/2330 train_time:84624ms step_avg:58.40ms
step:1450/2330 train_time:84686ms step_avg:58.40ms
step:1451/2330 train_time:84743ms step_avg:58.40ms
step:1452/2330 train_time:84802ms step_avg:58.40ms
step:1453/2330 train_time:84859ms step_avg:58.40ms
step:1454/2330 train_time:84919ms step_avg:58.40ms
step:1455/2330 train_time:84976ms step_avg:58.40ms
step:1456/2330 train_time:85037ms step_avg:58.40ms
step:1457/2330 train_time:85093ms step_avg:58.40ms
step:1458/2330 train_time:85153ms step_avg:58.40ms
step:1459/2330 train_time:85210ms step_avg:58.40ms
step:1460/2330 train_time:85271ms step_avg:58.40ms
step:1461/2330 train_time:85328ms step_avg:58.40ms
step:1462/2330 train_time:85388ms step_avg:58.40ms
step:1463/2330 train_time:85444ms step_avg:58.40ms
step:1464/2330 train_time:85505ms step_avg:58.40ms
step:1465/2330 train_time:85562ms step_avg:58.40ms
step:1466/2330 train_time:85623ms step_avg:58.41ms
step:1467/2330 train_time:85680ms step_avg:58.41ms
step:1468/2330 train_time:85739ms step_avg:58.41ms
step:1469/2330 train_time:85796ms step_avg:58.40ms
step:1470/2330 train_time:85856ms step_avg:58.41ms
step:1471/2330 train_time:85913ms step_avg:58.40ms
step:1472/2330 train_time:85974ms step_avg:58.41ms
step:1473/2330 train_time:86031ms step_avg:58.41ms
step:1474/2330 train_time:86091ms step_avg:58.41ms
step:1475/2330 train_time:86148ms step_avg:58.41ms
step:1476/2330 train_time:86209ms step_avg:58.41ms
step:1477/2330 train_time:86266ms step_avg:58.41ms
step:1478/2330 train_time:86326ms step_avg:58.41ms
step:1479/2330 train_time:86383ms step_avg:58.41ms
step:1480/2330 train_time:86443ms step_avg:58.41ms
step:1481/2330 train_time:86499ms step_avg:58.41ms
step:1482/2330 train_time:86560ms step_avg:58.41ms
step:1483/2330 train_time:86617ms step_avg:58.41ms
step:1484/2330 train_time:86677ms step_avg:58.41ms
step:1485/2330 train_time:86734ms step_avg:58.41ms
step:1486/2330 train_time:86794ms step_avg:58.41ms
step:1487/2330 train_time:86850ms step_avg:58.41ms
step:1488/2330 train_time:86911ms step_avg:58.41ms
step:1489/2330 train_time:86967ms step_avg:58.41ms
step:1490/2330 train_time:87028ms step_avg:58.41ms
step:1491/2330 train_time:87085ms step_avg:58.41ms
step:1492/2330 train_time:87145ms step_avg:58.41ms
step:1493/2330 train_time:87201ms step_avg:58.41ms
step:1494/2330 train_time:87263ms step_avg:58.41ms
step:1495/2330 train_time:87320ms step_avg:58.41ms
step:1496/2330 train_time:87381ms step_avg:58.41ms
step:1497/2330 train_time:87438ms step_avg:58.41ms
step:1498/2330 train_time:87498ms step_avg:58.41ms
step:1499/2330 train_time:87555ms step_avg:58.41ms
step:1500/2330 train_time:87615ms step_avg:58.41ms
step:1500/2330 val_loss:3.9068 train_time:87696ms step_avg:58.46ms
step:1501/2330 train_time:87717ms step_avg:58.44ms
step:1502/2330 train_time:87738ms step_avg:58.41ms
step:1503/2330 train_time:87797ms step_avg:58.41ms
step:1504/2330 train_time:87861ms step_avg:58.42ms
step:1505/2330 train_time:87919ms step_avg:58.42ms
step:1506/2330 train_time:87979ms step_avg:58.42ms
step:1507/2330 train_time:88037ms step_avg:58.42ms
step:1508/2330 train_time:88096ms step_avg:58.42ms
step:1509/2330 train_time:88153ms step_avg:58.42ms
step:1510/2330 train_time:88211ms step_avg:58.42ms
step:1511/2330 train_time:88268ms step_avg:58.42ms
step:1512/2330 train_time:88327ms step_avg:58.42ms
step:1513/2330 train_time:88383ms step_avg:58.42ms
step:1514/2330 train_time:88442ms step_avg:58.42ms
step:1515/2330 train_time:88499ms step_avg:58.41ms
step:1516/2330 train_time:88559ms step_avg:58.42ms
step:1517/2330 train_time:88616ms step_avg:58.42ms
step:1518/2330 train_time:88676ms step_avg:58.42ms
step:1519/2330 train_time:88734ms step_avg:58.42ms
step:1520/2330 train_time:88797ms step_avg:58.42ms
step:1521/2330 train_time:88856ms step_avg:58.42ms
step:1522/2330 train_time:88916ms step_avg:58.42ms
step:1523/2330 train_time:88974ms step_avg:58.42ms
step:1524/2330 train_time:89034ms step_avg:58.42ms
step:1525/2330 train_time:89091ms step_avg:58.42ms
step:1526/2330 train_time:89151ms step_avg:58.42ms
step:1527/2330 train_time:89209ms step_avg:58.42ms
step:1528/2330 train_time:89268ms step_avg:58.42ms
step:1529/2330 train_time:89327ms step_avg:58.42ms
step:1530/2330 train_time:89384ms step_avg:58.42ms
step:1531/2330 train_time:89441ms step_avg:58.42ms
step:1532/2330 train_time:89501ms step_avg:58.42ms
step:1533/2330 train_time:89557ms step_avg:58.42ms
step:1534/2330 train_time:89619ms step_avg:58.42ms
step:1535/2330 train_time:89677ms step_avg:58.42ms
step:1536/2330 train_time:89739ms step_avg:58.42ms
step:1537/2330 train_time:89797ms step_avg:58.42ms
step:1538/2330 train_time:89860ms step_avg:58.43ms
step:1539/2330 train_time:89918ms step_avg:58.43ms
step:1540/2330 train_time:89981ms step_avg:58.43ms
step:1541/2330 train_time:90039ms step_avg:58.43ms
step:1542/2330 train_time:90100ms step_avg:58.43ms
step:1543/2330 train_time:90158ms step_avg:58.43ms
step:1544/2330 train_time:90219ms step_avg:58.43ms
step:1545/2330 train_time:90277ms step_avg:58.43ms
step:1546/2330 train_time:90337ms step_avg:58.43ms
step:1547/2330 train_time:90395ms step_avg:58.43ms
step:1548/2330 train_time:90455ms step_avg:58.43ms
step:1549/2330 train_time:90512ms step_avg:58.43ms
step:1550/2330 train_time:90572ms step_avg:58.43ms
step:1551/2330 train_time:90629ms step_avg:58.43ms
step:1552/2330 train_time:90690ms step_avg:58.43ms
step:1553/2330 train_time:90748ms step_avg:58.43ms
step:1554/2330 train_time:90810ms step_avg:58.44ms
step:1555/2330 train_time:90867ms step_avg:58.44ms
step:1556/2330 train_time:90930ms step_avg:58.44ms
step:1557/2330 train_time:90987ms step_avg:58.44ms
step:1558/2330 train_time:91050ms step_avg:58.44ms
step:1559/2330 train_time:91107ms step_avg:58.44ms
step:1560/2330 train_time:91169ms step_avg:58.44ms
step:1561/2330 train_time:91227ms step_avg:58.44ms
step:1562/2330 train_time:91289ms step_avg:58.44ms
step:1563/2330 train_time:91345ms step_avg:58.44ms
step:1564/2330 train_time:91406ms step_avg:58.44ms
step:1565/2330 train_time:91462ms step_avg:58.44ms
step:1566/2330 train_time:91524ms step_avg:58.44ms
step:1567/2330 train_time:91580ms step_avg:58.44ms
step:1568/2330 train_time:91642ms step_avg:58.44ms
step:1569/2330 train_time:91699ms step_avg:58.44ms
step:1570/2330 train_time:91761ms step_avg:58.45ms
step:1571/2330 train_time:91819ms step_avg:58.45ms
step:1572/2330 train_time:91880ms step_avg:58.45ms
step:1573/2330 train_time:91939ms step_avg:58.45ms
step:1574/2330 train_time:92001ms step_avg:58.45ms
step:1575/2330 train_time:92059ms step_avg:58.45ms
step:1576/2330 train_time:92121ms step_avg:58.45ms
step:1577/2330 train_time:92179ms step_avg:58.45ms
step:1578/2330 train_time:92240ms step_avg:58.45ms
step:1579/2330 train_time:92299ms step_avg:58.45ms
step:1580/2330 train_time:92359ms step_avg:58.46ms
step:1581/2330 train_time:92417ms step_avg:58.46ms
step:1582/2330 train_time:92478ms step_avg:58.46ms
step:1583/2330 train_time:92535ms step_avg:58.46ms
step:1584/2330 train_time:92596ms step_avg:58.46ms
step:1585/2330 train_time:92653ms step_avg:58.46ms
step:1586/2330 train_time:92713ms step_avg:58.46ms
step:1587/2330 train_time:92770ms step_avg:58.46ms
step:1588/2330 train_time:92831ms step_avg:58.46ms
step:1589/2330 train_time:92889ms step_avg:58.46ms
step:1590/2330 train_time:92952ms step_avg:58.46ms
step:1591/2330 train_time:93009ms step_avg:58.46ms
step:1592/2330 train_time:93070ms step_avg:58.46ms
step:1593/2330 train_time:93127ms step_avg:58.46ms
step:1594/2330 train_time:93189ms step_avg:58.46ms
step:1595/2330 train_time:93246ms step_avg:58.46ms
step:1596/2330 train_time:93307ms step_avg:58.46ms
step:1597/2330 train_time:93364ms step_avg:58.46ms
step:1598/2330 train_time:93424ms step_avg:58.46ms
step:1599/2330 train_time:93481ms step_avg:58.46ms
step:1600/2330 train_time:93543ms step_avg:58.46ms
step:1601/2330 train_time:93600ms step_avg:58.46ms
step:1602/2330 train_time:93661ms step_avg:58.47ms
step:1603/2330 train_time:93718ms step_avg:58.46ms
step:1604/2330 train_time:93779ms step_avg:58.47ms
step:1605/2330 train_time:93837ms step_avg:58.47ms
step:1606/2330 train_time:93898ms step_avg:58.47ms
step:1607/2330 train_time:93955ms step_avg:58.47ms
step:1608/2330 train_time:94015ms step_avg:58.47ms
step:1609/2330 train_time:94074ms step_avg:58.47ms
step:1610/2330 train_time:94134ms step_avg:58.47ms
step:1611/2330 train_time:94192ms step_avg:58.47ms
step:1612/2330 train_time:94252ms step_avg:58.47ms
step:1613/2330 train_time:94311ms step_avg:58.47ms
step:1614/2330 train_time:94371ms step_avg:58.47ms
step:1615/2330 train_time:94428ms step_avg:58.47ms
step:1616/2330 train_time:94491ms step_avg:58.47ms
step:1617/2330 train_time:94547ms step_avg:58.47ms
step:1618/2330 train_time:94610ms step_avg:58.47ms
step:1619/2330 train_time:94667ms step_avg:58.47ms
step:1620/2330 train_time:94728ms step_avg:58.47ms
step:1621/2330 train_time:94784ms step_avg:58.47ms
step:1622/2330 train_time:94847ms step_avg:58.48ms
step:1623/2330 train_time:94903ms step_avg:58.47ms
step:1624/2330 train_time:94965ms step_avg:58.48ms
step:1625/2330 train_time:95022ms step_avg:58.48ms
step:1626/2330 train_time:95083ms step_avg:58.48ms
step:1627/2330 train_time:95140ms step_avg:58.48ms
step:1628/2330 train_time:95202ms step_avg:58.48ms
step:1629/2330 train_time:95259ms step_avg:58.48ms
step:1630/2330 train_time:95321ms step_avg:58.48ms
step:1631/2330 train_time:95378ms step_avg:58.48ms
step:1632/2330 train_time:95439ms step_avg:58.48ms
step:1633/2330 train_time:95497ms step_avg:58.48ms
step:1634/2330 train_time:95559ms step_avg:58.48ms
step:1635/2330 train_time:95617ms step_avg:58.48ms
step:1636/2330 train_time:95677ms step_avg:58.48ms
step:1637/2330 train_time:95735ms step_avg:58.48ms
step:1638/2330 train_time:95795ms step_avg:58.48ms
step:1639/2330 train_time:95853ms step_avg:58.48ms
step:1640/2330 train_time:95913ms step_avg:58.48ms
step:1641/2330 train_time:95971ms step_avg:58.48ms
step:1642/2330 train_time:96034ms step_avg:58.49ms
step:1643/2330 train_time:96091ms step_avg:58.49ms
step:1644/2330 train_time:96153ms step_avg:58.49ms
step:1645/2330 train_time:96211ms step_avg:58.49ms
step:1646/2330 train_time:96272ms step_avg:58.49ms
step:1647/2330 train_time:96330ms step_avg:58.49ms
step:1648/2330 train_time:96390ms step_avg:58.49ms
step:1649/2330 train_time:96447ms step_avg:58.49ms
step:1650/2330 train_time:96508ms step_avg:58.49ms
step:1651/2330 train_time:96566ms step_avg:58.49ms
step:1652/2330 train_time:96626ms step_avg:58.49ms
step:1653/2330 train_time:96683ms step_avg:58.49ms
step:1654/2330 train_time:96745ms step_avg:58.49ms
step:1655/2330 train_time:96802ms step_avg:58.49ms
step:1656/2330 train_time:96863ms step_avg:58.49ms
step:1657/2330 train_time:96920ms step_avg:58.49ms
step:1658/2330 train_time:96983ms step_avg:58.49ms
step:1659/2330 train_time:97040ms step_avg:58.49ms
step:1660/2330 train_time:97101ms step_avg:58.49ms
step:1661/2330 train_time:97159ms step_avg:58.49ms
step:1662/2330 train_time:97220ms step_avg:58.50ms
step:1663/2330 train_time:97277ms step_avg:58.49ms
step:1664/2330 train_time:97338ms step_avg:58.50ms
step:1665/2330 train_time:97395ms step_avg:58.50ms
step:1666/2330 train_time:97456ms step_avg:58.50ms
step:1667/2330 train_time:97515ms step_avg:58.50ms
step:1668/2330 train_time:97575ms step_avg:58.50ms
step:1669/2330 train_time:97632ms step_avg:58.50ms
step:1670/2330 train_time:97694ms step_avg:58.50ms
step:1671/2330 train_time:97753ms step_avg:58.50ms
step:1672/2330 train_time:97814ms step_avg:58.50ms
step:1673/2330 train_time:97873ms step_avg:58.50ms
step:1674/2330 train_time:97934ms step_avg:58.50ms
step:1675/2330 train_time:97991ms step_avg:58.50ms
step:1676/2330 train_time:98052ms step_avg:58.50ms
step:1677/2330 train_time:98109ms step_avg:58.50ms
step:1678/2330 train_time:98170ms step_avg:58.50ms
step:1679/2330 train_time:98227ms step_avg:58.50ms
step:1680/2330 train_time:98288ms step_avg:58.50ms
step:1681/2330 train_time:98345ms step_avg:58.50ms
step:1682/2330 train_time:98405ms step_avg:58.50ms
step:1683/2330 train_time:98463ms step_avg:58.50ms
step:1684/2330 train_time:98525ms step_avg:58.51ms
step:1685/2330 train_time:98582ms step_avg:58.51ms
step:1686/2330 train_time:98643ms step_avg:58.51ms
step:1687/2330 train_time:98700ms step_avg:58.51ms
step:1688/2330 train_time:98763ms step_avg:58.51ms
step:1689/2330 train_time:98820ms step_avg:58.51ms
step:1690/2330 train_time:98881ms step_avg:58.51ms
step:1691/2330 train_time:98939ms step_avg:58.51ms
step:1692/2330 train_time:99000ms step_avg:58.51ms
step:1693/2330 train_time:99057ms step_avg:58.51ms
step:1694/2330 train_time:99118ms step_avg:58.51ms
step:1695/2330 train_time:99177ms step_avg:58.51ms
step:1696/2330 train_time:99236ms step_avg:58.51ms
step:1697/2330 train_time:99294ms step_avg:58.51ms
step:1698/2330 train_time:99355ms step_avg:58.51ms
step:1699/2330 train_time:99414ms step_avg:58.51ms
step:1700/2330 train_time:99474ms step_avg:58.51ms
step:1701/2330 train_time:99532ms step_avg:58.51ms
step:1702/2330 train_time:99593ms step_avg:58.52ms
step:1703/2330 train_time:99650ms step_avg:58.51ms
step:1704/2330 train_time:99711ms step_avg:58.52ms
step:1705/2330 train_time:99768ms step_avg:58.52ms
step:1706/2330 train_time:99830ms step_avg:58.52ms
step:1707/2330 train_time:99887ms step_avg:58.52ms
step:1708/2330 train_time:99948ms step_avg:58.52ms
step:1709/2330 train_time:100005ms step_avg:58.52ms
step:1710/2330 train_time:100066ms step_avg:58.52ms
step:1711/2330 train_time:100123ms step_avg:58.52ms
step:1712/2330 train_time:100185ms step_avg:58.52ms
step:1713/2330 train_time:100242ms step_avg:58.52ms
step:1714/2330 train_time:100304ms step_avg:58.52ms
step:1715/2330 train_time:100361ms step_avg:58.52ms
step:1716/2330 train_time:100423ms step_avg:58.52ms
step:1717/2330 train_time:100480ms step_avg:58.52ms
step:1718/2330 train_time:100542ms step_avg:58.52ms
step:1719/2330 train_time:100599ms step_avg:58.52ms
step:1720/2330 train_time:100661ms step_avg:58.52ms
step:1721/2330 train_time:100718ms step_avg:58.52ms
step:1722/2330 train_time:100779ms step_avg:58.52ms
step:1723/2330 train_time:100838ms step_avg:58.52ms
step:1724/2330 train_time:100899ms step_avg:58.53ms
step:1725/2330 train_time:100957ms step_avg:58.53ms
step:1726/2330 train_time:101017ms step_avg:58.53ms
step:1727/2330 train_time:101075ms step_avg:58.53ms
step:1728/2330 train_time:101136ms step_avg:58.53ms
step:1729/2330 train_time:101193ms step_avg:58.53ms
step:1730/2330 train_time:101256ms step_avg:58.53ms
step:1731/2330 train_time:101314ms step_avg:58.53ms
step:1732/2330 train_time:101374ms step_avg:58.53ms
step:1733/2330 train_time:101432ms step_avg:58.53ms
step:1734/2330 train_time:101493ms step_avg:58.53ms
step:1735/2330 train_time:101550ms step_avg:58.53ms
step:1736/2330 train_time:101611ms step_avg:58.53ms
step:1737/2330 train_time:101669ms step_avg:58.53ms
step:1738/2330 train_time:101729ms step_avg:58.53ms
step:1739/2330 train_time:101786ms step_avg:58.53ms
step:1740/2330 train_time:101847ms step_avg:58.53ms
step:1741/2330 train_time:101904ms step_avg:58.53ms
step:1742/2330 train_time:101966ms step_avg:58.53ms
step:1743/2330 train_time:102023ms step_avg:58.53ms
step:1744/2330 train_time:102085ms step_avg:58.54ms
step:1745/2330 train_time:102143ms step_avg:58.53ms
step:1746/2330 train_time:102203ms step_avg:58.54ms
step:1747/2330 train_time:102260ms step_avg:58.53ms
step:1748/2330 train_time:102322ms step_avg:58.54ms
step:1749/2330 train_time:102379ms step_avg:58.54ms
step:1750/2330 train_time:102441ms step_avg:58.54ms
step:1750/2330 val_loss:3.8231 train_time:102524ms step_avg:58.58ms
step:1751/2330 train_time:102544ms step_avg:58.56ms
step:1752/2330 train_time:102565ms step_avg:58.54ms
step:1753/2330 train_time:102617ms step_avg:58.54ms
step:1754/2330 train_time:102686ms step_avg:58.54ms
step:1755/2330 train_time:102742ms step_avg:58.54ms
step:1756/2330 train_time:102804ms step_avg:58.54ms
step:1757/2330 train_time:102861ms step_avg:58.54ms
step:1758/2330 train_time:102921ms step_avg:58.54ms
step:1759/2330 train_time:102978ms step_avg:58.54ms
step:1760/2330 train_time:103037ms step_avg:58.54ms
step:1761/2330 train_time:103094ms step_avg:58.54ms
step:1762/2330 train_time:103153ms step_avg:58.54ms
step:1763/2330 train_time:103210ms step_avg:58.54ms
step:1764/2330 train_time:103270ms step_avg:58.54ms
step:1765/2330 train_time:103327ms step_avg:58.54ms
step:1766/2330 train_time:103387ms step_avg:58.54ms
step:1767/2330 train_time:103450ms step_avg:58.55ms
step:1768/2330 train_time:103513ms step_avg:58.55ms
step:1769/2330 train_time:103572ms step_avg:58.55ms
step:1770/2330 train_time:103632ms step_avg:58.55ms
step:1771/2330 train_time:103689ms step_avg:58.55ms
step:1772/2330 train_time:103752ms step_avg:58.55ms
step:1773/2330 train_time:103809ms step_avg:58.55ms
step:1774/2330 train_time:103870ms step_avg:58.55ms
step:1775/2330 train_time:103927ms step_avg:58.55ms
step:1776/2330 train_time:103988ms step_avg:58.55ms
step:1777/2330 train_time:104045ms step_avg:58.55ms
step:1778/2330 train_time:104105ms step_avg:58.55ms
step:1779/2330 train_time:104162ms step_avg:58.55ms
step:1780/2330 train_time:104221ms step_avg:58.55ms
step:1781/2330 train_time:104278ms step_avg:58.55ms
step:1782/2330 train_time:104338ms step_avg:58.55ms
step:1783/2330 train_time:104397ms step_avg:58.55ms
step:1784/2330 train_time:104458ms step_avg:58.55ms
step:1785/2330 train_time:104517ms step_avg:58.55ms
step:1786/2330 train_time:104578ms step_avg:58.55ms
step:1787/2330 train_time:104636ms step_avg:58.55ms
step:1788/2330 train_time:104698ms step_avg:58.56ms
step:1789/2330 train_time:104755ms step_avg:58.56ms
step:1790/2330 train_time:104817ms step_avg:58.56ms
step:1791/2330 train_time:104873ms step_avg:58.56ms
step:1792/2330 train_time:104935ms step_avg:58.56ms
step:1793/2330 train_time:104992ms step_avg:58.56ms
step:1794/2330 train_time:105053ms step_avg:58.56ms
step:1795/2330 train_time:105109ms step_avg:58.56ms
step:1796/2330 train_time:105171ms step_avg:58.56ms
step:1797/2330 train_time:105228ms step_avg:58.56ms
step:1798/2330 train_time:105289ms step_avg:58.56ms
step:1799/2330 train_time:105346ms step_avg:58.56ms
step:1800/2330 train_time:105406ms step_avg:58.56ms
step:1801/2330 train_time:105464ms step_avg:58.56ms
step:1802/2330 train_time:105526ms step_avg:58.56ms
step:1803/2330 train_time:105585ms step_avg:58.56ms
step:1804/2330 train_time:105646ms step_avg:58.56ms
step:1805/2330 train_time:105705ms step_avg:58.56ms
step:1806/2330 train_time:105765ms step_avg:58.56ms
step:1807/2330 train_time:105824ms step_avg:58.56ms
step:1808/2330 train_time:105885ms step_avg:58.56ms
step:1809/2330 train_time:105942ms step_avg:58.56ms
step:1810/2330 train_time:106003ms step_avg:58.56ms
step:1811/2330 train_time:106061ms step_avg:58.57ms
step:1812/2330 train_time:106121ms step_avg:58.57ms
step:1813/2330 train_time:106178ms step_avg:58.56ms
step:1814/2330 train_time:106238ms step_avg:58.57ms
step:1815/2330 train_time:106295ms step_avg:58.56ms
step:1816/2330 train_time:106355ms step_avg:58.57ms
step:1817/2330 train_time:106413ms step_avg:58.57ms
step:1818/2330 train_time:106474ms step_avg:58.57ms
step:1819/2330 train_time:106531ms step_avg:58.57ms
step:1820/2330 train_time:106592ms step_avg:58.57ms
step:1821/2330 train_time:106650ms step_avg:58.57ms
step:1822/2330 train_time:106712ms step_avg:58.57ms
step:1823/2330 train_time:106770ms step_avg:58.57ms
step:1824/2330 train_time:106830ms step_avg:58.57ms
step:1825/2330 train_time:106888ms step_avg:58.57ms
step:1826/2330 train_time:106948ms step_avg:58.57ms
step:1827/2330 train_time:107006ms step_avg:58.57ms
step:1828/2330 train_time:107067ms step_avg:58.57ms
step:1829/2330 train_time:107125ms step_avg:58.57ms
step:1830/2330 train_time:107184ms step_avg:58.57ms
step:1831/2330 train_time:107242ms step_avg:58.57ms
step:1832/2330 train_time:107303ms step_avg:58.57ms
step:1833/2330 train_time:107361ms step_avg:58.57ms
step:1834/2330 train_time:107421ms step_avg:58.57ms
step:1835/2330 train_time:107479ms step_avg:58.57ms
step:1836/2330 train_time:107540ms step_avg:58.57ms
step:1837/2330 train_time:107598ms step_avg:58.57ms
step:1838/2330 train_time:107659ms step_avg:58.57ms
step:1839/2330 train_time:107716ms step_avg:58.57ms
step:1840/2330 train_time:107778ms step_avg:58.57ms
step:1841/2330 train_time:107835ms step_avg:58.57ms
step:1842/2330 train_time:107897ms step_avg:58.58ms
step:1843/2330 train_time:107954ms step_avg:58.58ms
step:1844/2330 train_time:108016ms step_avg:58.58ms
step:1845/2330 train_time:108073ms step_avg:58.58ms
step:1846/2330 train_time:108135ms step_avg:58.58ms
step:1847/2330 train_time:108192ms step_avg:58.58ms
step:1848/2330 train_time:108253ms step_avg:58.58ms
step:1849/2330 train_time:108310ms step_avg:58.58ms
step:1850/2330 train_time:108371ms step_avg:58.58ms
step:1851/2330 train_time:108429ms step_avg:58.58ms
step:1852/2330 train_time:108490ms step_avg:58.58ms
step:1853/2330 train_time:108548ms step_avg:58.58ms
step:1854/2330 train_time:108608ms step_avg:58.58ms
step:1855/2330 train_time:108666ms step_avg:58.58ms
step:1856/2330 train_time:108726ms step_avg:58.58ms
step:1857/2330 train_time:108784ms step_avg:58.58ms
step:1858/2330 train_time:108845ms step_avg:58.58ms
step:1859/2330 train_time:108904ms step_avg:58.58ms
step:1860/2330 train_time:108966ms step_avg:58.58ms
step:1861/2330 train_time:109024ms step_avg:58.58ms
step:1862/2330 train_time:109085ms step_avg:58.58ms
step:1863/2330 train_time:109143ms step_avg:58.58ms
step:1864/2330 train_time:109203ms step_avg:58.59ms
step:1865/2330 train_time:109260ms step_avg:58.58ms
step:1866/2330 train_time:109321ms step_avg:58.59ms
step:1867/2330 train_time:109379ms step_avg:58.59ms
step:1868/2330 train_time:109440ms step_avg:58.59ms
step:1869/2330 train_time:109497ms step_avg:58.59ms
step:1870/2330 train_time:109557ms step_avg:58.59ms
step:1871/2330 train_time:109614ms step_avg:58.59ms
step:1872/2330 train_time:109675ms step_avg:58.59ms
step:1873/2330 train_time:109733ms step_avg:58.59ms
step:1874/2330 train_time:109794ms step_avg:58.59ms
step:1875/2330 train_time:109852ms step_avg:58.59ms
step:1876/2330 train_time:109913ms step_avg:58.59ms
step:1877/2330 train_time:109970ms step_avg:58.59ms
step:1878/2330 train_time:110032ms step_avg:58.59ms
step:1879/2330 train_time:110089ms step_avg:58.59ms
step:1880/2330 train_time:110151ms step_avg:58.59ms
step:1881/2330 train_time:110208ms step_avg:58.59ms
step:1882/2330 train_time:110269ms step_avg:58.59ms
step:1883/2330 train_time:110326ms step_avg:58.59ms
step:1884/2330 train_time:110387ms step_avg:58.59ms
step:1885/2330 train_time:110445ms step_avg:58.59ms
step:1886/2330 train_time:110505ms step_avg:58.59ms
step:1887/2330 train_time:110564ms step_avg:58.59ms
step:1888/2330 train_time:110623ms step_avg:58.59ms
step:1889/2330 train_time:110681ms step_avg:58.59ms
step:1890/2330 train_time:110742ms step_avg:58.59ms
step:1891/2330 train_time:110800ms step_avg:58.59ms
step:1892/2330 train_time:110861ms step_avg:58.59ms
step:1893/2330 train_time:110918ms step_avg:58.59ms
step:1894/2330 train_time:110979ms step_avg:58.60ms
step:1895/2330 train_time:111037ms step_avg:58.59ms
step:1896/2330 train_time:111098ms step_avg:58.60ms
step:1897/2330 train_time:111156ms step_avg:58.60ms
step:1898/2330 train_time:111217ms step_avg:58.60ms
step:1899/2330 train_time:111273ms step_avg:58.60ms
step:1900/2330 train_time:111335ms step_avg:58.60ms
step:1901/2330 train_time:111392ms step_avg:58.60ms
step:1902/2330 train_time:111454ms step_avg:58.60ms
step:1903/2330 train_time:111510ms step_avg:58.60ms
step:1904/2330 train_time:111572ms step_avg:58.60ms
step:1905/2330 train_time:111629ms step_avg:58.60ms
step:1906/2330 train_time:111690ms step_avg:58.60ms
step:1907/2330 train_time:111748ms step_avg:58.60ms
step:1908/2330 train_time:111809ms step_avg:58.60ms
step:1909/2330 train_time:111867ms step_avg:58.60ms
step:1910/2330 train_time:111927ms step_avg:58.60ms
step:1911/2330 train_time:111985ms step_avg:58.60ms
step:1912/2330 train_time:112045ms step_avg:58.60ms
step:1913/2330 train_time:112104ms step_avg:58.60ms
step:1914/2330 train_time:112165ms step_avg:58.60ms
step:1915/2330 train_time:112224ms step_avg:58.60ms
step:1916/2330 train_time:112284ms step_avg:58.60ms
step:1917/2330 train_time:112343ms step_avg:58.60ms
step:1918/2330 train_time:112403ms step_avg:58.60ms
step:1919/2330 train_time:112462ms step_avg:58.60ms
step:1920/2330 train_time:112522ms step_avg:58.60ms
step:1921/2330 train_time:112580ms step_avg:58.60ms
step:1922/2330 train_time:112639ms step_avg:58.61ms
step:1923/2330 train_time:112697ms step_avg:58.60ms
step:1924/2330 train_time:112757ms step_avg:58.61ms
step:1925/2330 train_time:112814ms step_avg:58.60ms
step:1926/2330 train_time:112875ms step_avg:58.61ms
step:1927/2330 train_time:112932ms step_avg:58.61ms
step:1928/2330 train_time:112994ms step_avg:58.61ms
step:1929/2330 train_time:113052ms step_avg:58.61ms
step:1930/2330 train_time:113114ms step_avg:58.61ms
step:1931/2330 train_time:113171ms step_avg:58.61ms
step:1932/2330 train_time:113232ms step_avg:58.61ms
step:1933/2330 train_time:113289ms step_avg:58.61ms
step:1934/2330 train_time:113351ms step_avg:58.61ms
step:1935/2330 train_time:113409ms step_avg:58.61ms
step:1936/2330 train_time:113470ms step_avg:58.61ms
step:1937/2330 train_time:113528ms step_avg:58.61ms
step:1938/2330 train_time:113588ms step_avg:58.61ms
step:1939/2330 train_time:113647ms step_avg:58.61ms
step:1940/2330 train_time:113706ms step_avg:58.61ms
step:1941/2330 train_time:113764ms step_avg:58.61ms
step:1942/2330 train_time:113824ms step_avg:58.61ms
step:1943/2330 train_time:113882ms step_avg:58.61ms
step:1944/2330 train_time:113944ms step_avg:58.61ms
step:1945/2330 train_time:114002ms step_avg:58.61ms
step:1946/2330 train_time:114062ms step_avg:58.61ms
step:1947/2330 train_time:114120ms step_avg:58.61ms
step:1948/2330 train_time:114180ms step_avg:58.61ms
step:1949/2330 train_time:114237ms step_avg:58.61ms
step:1950/2330 train_time:114298ms step_avg:58.61ms
step:1951/2330 train_time:114354ms step_avg:58.61ms
step:1952/2330 train_time:114416ms step_avg:58.61ms
step:1953/2330 train_time:114472ms step_avg:58.61ms
step:1954/2330 train_time:114534ms step_avg:58.62ms
step:1955/2330 train_time:114591ms step_avg:58.61ms
step:1956/2330 train_time:114654ms step_avg:58.62ms
step:1957/2330 train_time:114711ms step_avg:58.62ms
step:1958/2330 train_time:114771ms step_avg:58.62ms
step:1959/2330 train_time:114828ms step_avg:58.62ms
step:1960/2330 train_time:114891ms step_avg:58.62ms
step:1961/2330 train_time:114948ms step_avg:58.62ms
step:1962/2330 train_time:115009ms step_avg:58.62ms
step:1963/2330 train_time:115067ms step_avg:58.62ms
step:1964/2330 train_time:115128ms step_avg:58.62ms
step:1965/2330 train_time:115186ms step_avg:58.62ms
step:1966/2330 train_time:115247ms step_avg:58.62ms
step:1967/2330 train_time:115305ms step_avg:58.62ms
step:1968/2330 train_time:115365ms step_avg:58.62ms
step:1969/2330 train_time:115423ms step_avg:58.62ms
step:1970/2330 train_time:115484ms step_avg:58.62ms
step:1971/2330 train_time:115541ms step_avg:58.62ms
step:1972/2330 train_time:115601ms step_avg:58.62ms
step:1973/2330 train_time:115658ms step_avg:58.62ms
step:1974/2330 train_time:115719ms step_avg:58.62ms
step:1975/2330 train_time:115776ms step_avg:58.62ms
step:1976/2330 train_time:115837ms step_avg:58.62ms
step:1977/2330 train_time:115893ms step_avg:58.62ms
step:1978/2330 train_time:115955ms step_avg:58.62ms
step:1979/2330 train_time:116012ms step_avg:58.62ms
step:1980/2330 train_time:116074ms step_avg:58.62ms
step:1981/2330 train_time:116131ms step_avg:58.62ms
step:1982/2330 train_time:116193ms step_avg:58.62ms
step:1983/2330 train_time:116250ms step_avg:58.62ms
step:1984/2330 train_time:116311ms step_avg:58.62ms
step:1985/2330 train_time:116368ms step_avg:58.62ms
step:1986/2330 train_time:116430ms step_avg:58.63ms
step:1987/2330 train_time:116488ms step_avg:58.62ms
step:1988/2330 train_time:116547ms step_avg:58.63ms
step:1989/2330 train_time:116605ms step_avg:58.62ms
step:1990/2330 train_time:116665ms step_avg:58.63ms
step:1991/2330 train_time:116723ms step_avg:58.63ms
step:1992/2330 train_time:116785ms step_avg:58.63ms
step:1993/2330 train_time:116843ms step_avg:58.63ms
step:1994/2330 train_time:116904ms step_avg:58.63ms
step:1995/2330 train_time:116962ms step_avg:58.63ms
step:1996/2330 train_time:117022ms step_avg:58.63ms
step:1997/2330 train_time:117079ms step_avg:58.63ms
step:1998/2330 train_time:117140ms step_avg:58.63ms
step:1999/2330 train_time:117197ms step_avg:58.63ms
step:2000/2330 train_time:117261ms step_avg:58.63ms
step:2000/2330 val_loss:3.7612 train_time:117343ms step_avg:58.67ms
step:2001/2330 train_time:117363ms step_avg:58.65ms
step:2002/2330 train_time:117384ms step_avg:58.63ms
step:2003/2330 train_time:117445ms step_avg:58.63ms
step:2004/2330 train_time:117508ms step_avg:58.64ms
step:2005/2330 train_time:117565ms step_avg:58.64ms
step:2006/2330 train_time:117629ms step_avg:58.64ms
step:2007/2330 train_time:117686ms step_avg:58.64ms
step:2008/2330 train_time:117747ms step_avg:58.64ms
step:2009/2330 train_time:117803ms step_avg:58.64ms
step:2010/2330 train_time:117865ms step_avg:58.64ms
step:2011/2330 train_time:117922ms step_avg:58.64ms
step:2012/2330 train_time:117982ms step_avg:58.64ms
step:2013/2330 train_time:118039ms step_avg:58.64ms
step:2014/2330 train_time:118099ms step_avg:58.64ms
step:2015/2330 train_time:118156ms step_avg:58.64ms
step:2016/2330 train_time:118215ms step_avg:58.64ms
step:2017/2330 train_time:118273ms step_avg:58.64ms
step:2018/2330 train_time:118333ms step_avg:58.64ms
step:2019/2330 train_time:118393ms step_avg:58.64ms
step:2020/2330 train_time:118455ms step_avg:58.64ms
step:2021/2330 train_time:118514ms step_avg:58.64ms
step:2022/2330 train_time:118577ms step_avg:58.64ms
step:2023/2330 train_time:118635ms step_avg:58.64ms
step:2024/2330 train_time:118696ms step_avg:58.64ms
step:2025/2330 train_time:118753ms step_avg:58.64ms
step:2026/2330 train_time:118815ms step_avg:58.65ms
step:2027/2330 train_time:118872ms step_avg:58.64ms
step:2028/2330 train_time:118932ms step_avg:58.65ms
step:2029/2330 train_time:118989ms step_avg:58.64ms
step:2030/2330 train_time:119049ms step_avg:58.64ms
step:2031/2330 train_time:119105ms step_avg:58.64ms
step:2032/2330 train_time:119166ms step_avg:58.64ms
step:2033/2330 train_time:119222ms step_avg:58.64ms
step:2034/2330 train_time:119283ms step_avg:58.64ms
step:2035/2330 train_time:119342ms step_avg:58.64ms
step:2036/2330 train_time:119403ms step_avg:58.65ms
step:2037/2330 train_time:119462ms step_avg:58.65ms
step:2038/2330 train_time:119523ms step_avg:58.65ms
step:2039/2330 train_time:119580ms step_avg:58.65ms
step:2040/2330 train_time:119643ms step_avg:58.65ms
step:2041/2330 train_time:119701ms step_avg:58.65ms
step:2042/2330 train_time:119763ms step_avg:58.65ms
step:2043/2330 train_time:119821ms step_avg:58.65ms
step:2044/2330 train_time:119880ms step_avg:58.65ms
step:2045/2330 train_time:119938ms step_avg:58.65ms
step:2046/2330 train_time:119998ms step_avg:58.65ms
step:2047/2330 train_time:120056ms step_avg:58.65ms
step:2048/2330 train_time:120117ms step_avg:58.65ms
step:2049/2330 train_time:120174ms step_avg:58.65ms
step:2050/2330 train_time:120234ms step_avg:58.65ms
step:2051/2330 train_time:120291ms step_avg:58.65ms
step:2052/2330 train_time:120352ms step_avg:58.65ms
step:2053/2330 train_time:120411ms step_avg:58.65ms
step:2054/2330 train_time:120471ms step_avg:58.65ms
step:2055/2330 train_time:120528ms step_avg:58.65ms
step:2056/2330 train_time:120592ms step_avg:58.65ms
step:2057/2330 train_time:120649ms step_avg:58.65ms
step:2058/2330 train_time:120711ms step_avg:58.65ms
step:2059/2330 train_time:120768ms step_avg:58.65ms
step:2060/2330 train_time:120830ms step_avg:58.66ms
step:2061/2330 train_time:120886ms step_avg:58.65ms
step:2062/2330 train_time:120948ms step_avg:58.66ms
step:2063/2330 train_time:121005ms step_avg:58.65ms
step:2064/2330 train_time:121066ms step_avg:58.66ms
step:2065/2330 train_time:121123ms step_avg:58.66ms
step:2066/2330 train_time:121184ms step_avg:58.66ms
step:2067/2330 train_time:121241ms step_avg:58.66ms
step:2068/2330 train_time:121302ms step_avg:58.66ms
step:2069/2330 train_time:121360ms step_avg:58.66ms
step:2070/2330 train_time:121420ms step_avg:58.66ms
step:2071/2330 train_time:121478ms step_avg:58.66ms
step:2072/2330 train_time:121539ms step_avg:58.66ms
step:2073/2330 train_time:121598ms step_avg:58.66ms
step:2074/2330 train_time:121658ms step_avg:58.66ms
step:2075/2330 train_time:121718ms step_avg:58.66ms
step:2076/2330 train_time:121778ms step_avg:58.66ms
step:2077/2330 train_time:121836ms step_avg:58.66ms
step:2078/2330 train_time:121896ms step_avg:58.66ms
step:2079/2330 train_time:121954ms step_avg:58.66ms
step:2080/2330 train_time:122015ms step_avg:58.66ms
step:2081/2330 train_time:122072ms step_avg:58.66ms
step:2082/2330 train_time:122133ms step_avg:58.66ms
step:2083/2330 train_time:122191ms step_avg:58.66ms
step:2084/2330 train_time:122251ms step_avg:58.66ms
step:2085/2330 train_time:122307ms step_avg:58.66ms
step:2086/2330 train_time:122369ms step_avg:58.66ms
step:2087/2330 train_time:122426ms step_avg:58.66ms
step:2088/2330 train_time:122489ms step_avg:58.66ms
step:2089/2330 train_time:122546ms step_avg:58.66ms
step:2090/2330 train_time:122608ms step_avg:58.66ms
step:2091/2330 train_time:122666ms step_avg:58.66ms
step:2092/2330 train_time:122727ms step_avg:58.66ms
step:2093/2330 train_time:122783ms step_avg:58.66ms
step:2094/2330 train_time:122846ms step_avg:58.67ms
step:2095/2330 train_time:122904ms step_avg:58.67ms
step:2096/2330 train_time:122965ms step_avg:58.67ms
step:2097/2330 train_time:123022ms step_avg:58.67ms
step:2098/2330 train_time:123082ms step_avg:58.67ms
step:2099/2330 train_time:123139ms step_avg:58.67ms
step:2100/2330 train_time:123199ms step_avg:58.67ms
step:2101/2330 train_time:123257ms step_avg:58.67ms
step:2102/2330 train_time:123317ms step_avg:58.67ms
step:2103/2330 train_time:123375ms step_avg:58.67ms
step:2104/2330 train_time:123435ms step_avg:58.67ms
step:2105/2330 train_time:123493ms step_avg:58.67ms
step:2106/2330 train_time:123554ms step_avg:58.67ms
step:2107/2330 train_time:123612ms step_avg:58.67ms
step:2108/2330 train_time:123674ms step_avg:58.67ms
step:2109/2330 train_time:123731ms step_avg:58.67ms
step:2110/2330 train_time:123792ms step_avg:58.67ms
step:2111/2330 train_time:123849ms step_avg:58.67ms
step:2112/2330 train_time:123911ms step_avg:58.67ms
step:2113/2330 train_time:123969ms step_avg:58.67ms
step:2114/2330 train_time:124029ms step_avg:58.67ms
step:2115/2330 train_time:124086ms step_avg:58.67ms
step:2116/2330 train_time:124146ms step_avg:58.67ms
step:2117/2330 train_time:124203ms step_avg:58.67ms
step:2118/2330 train_time:124264ms step_avg:58.67ms
step:2119/2330 train_time:124322ms step_avg:58.67ms
step:2120/2330 train_time:124383ms step_avg:58.67ms
step:2121/2330 train_time:124440ms step_avg:58.67ms
step:2122/2330 train_time:124502ms step_avg:58.67ms
step:2123/2330 train_time:124559ms step_avg:58.67ms
step:2124/2330 train_time:124620ms step_avg:58.67ms
step:2125/2330 train_time:124679ms step_avg:58.67ms
step:2126/2330 train_time:124739ms step_avg:58.67ms
step:2127/2330 train_time:124796ms step_avg:58.67ms
step:2128/2330 train_time:124858ms step_avg:58.67ms
step:2129/2330 train_time:124917ms step_avg:58.67ms
step:2130/2330 train_time:124978ms step_avg:58.68ms
step:2131/2330 train_time:125037ms step_avg:58.68ms
step:2132/2330 train_time:125098ms step_avg:58.68ms
step:2133/2330 train_time:125156ms step_avg:58.68ms
step:2134/2330 train_time:125216ms step_avg:58.68ms
step:2135/2330 train_time:125273ms step_avg:58.68ms
step:2136/2330 train_time:125334ms step_avg:58.68ms
step:2137/2330 train_time:125391ms step_avg:58.68ms
step:2138/2330 train_time:125451ms step_avg:58.68ms
step:2139/2330 train_time:125509ms step_avg:58.68ms
step:2140/2330 train_time:125571ms step_avg:58.68ms
step:2141/2330 train_time:125627ms step_avg:58.68ms
step:2142/2330 train_time:125689ms step_avg:58.68ms
step:2143/2330 train_time:125746ms step_avg:58.68ms
step:2144/2330 train_time:125808ms step_avg:58.68ms
step:2145/2330 train_time:125865ms step_avg:58.68ms
step:2146/2330 train_time:125927ms step_avg:58.68ms
step:2147/2330 train_time:125983ms step_avg:58.68ms
step:2148/2330 train_time:126045ms step_avg:58.68ms
step:2149/2330 train_time:126102ms step_avg:58.68ms
step:2150/2330 train_time:126165ms step_avg:58.68ms
step:2151/2330 train_time:126222ms step_avg:58.68ms
step:2152/2330 train_time:126283ms step_avg:58.68ms
step:2153/2330 train_time:126341ms step_avg:58.68ms
step:2154/2330 train_time:126401ms step_avg:58.68ms
step:2155/2330 train_time:126459ms step_avg:58.68ms
step:2156/2330 train_time:126520ms step_avg:58.68ms
step:2157/2330 train_time:126578ms step_avg:58.68ms
step:2158/2330 train_time:126638ms step_avg:58.68ms
step:2159/2330 train_time:126696ms step_avg:58.68ms
step:2160/2330 train_time:126758ms step_avg:58.68ms
step:2161/2330 train_time:126816ms step_avg:58.68ms
step:2162/2330 train_time:126876ms step_avg:58.68ms
step:2163/2330 train_time:126933ms step_avg:58.68ms
step:2164/2330 train_time:126995ms step_avg:58.69ms
step:2165/2330 train_time:127051ms step_avg:58.68ms
step:2166/2330 train_time:127114ms step_avg:58.69ms
step:2167/2330 train_time:127170ms step_avg:58.69ms
step:2168/2330 train_time:127232ms step_avg:58.69ms
step:2169/2330 train_time:127288ms step_avg:58.69ms
step:2170/2330 train_time:127351ms step_avg:58.69ms
step:2171/2330 train_time:127408ms step_avg:58.69ms
step:2172/2330 train_time:127470ms step_avg:58.69ms
step:2173/2330 train_time:127527ms step_avg:58.69ms
step:2174/2330 train_time:127589ms step_avg:58.69ms
step:2175/2330 train_time:127646ms step_avg:58.69ms
step:2176/2330 train_time:127708ms step_avg:58.69ms
step:2177/2330 train_time:127765ms step_avg:58.69ms
step:2178/2330 train_time:127827ms step_avg:58.69ms
step:2179/2330 train_time:127884ms step_avg:58.69ms
step:2180/2330 train_time:127946ms step_avg:58.69ms
step:2181/2330 train_time:128002ms step_avg:58.69ms
step:2182/2330 train_time:128064ms step_avg:58.69ms
step:2183/2330 train_time:128121ms step_avg:58.69ms
step:2184/2330 train_time:128182ms step_avg:58.69ms
step:2185/2330 train_time:128240ms step_avg:58.69ms
step:2186/2330 train_time:128300ms step_avg:58.69ms
step:2187/2330 train_time:128358ms step_avg:58.69ms
step:2188/2330 train_time:128419ms step_avg:58.69ms
step:2189/2330 train_time:128476ms step_avg:58.69ms
step:2190/2330 train_time:128537ms step_avg:58.69ms
step:2191/2330 train_time:128595ms step_avg:58.69ms
step:2192/2330 train_time:128656ms step_avg:58.69ms
step:2193/2330 train_time:128713ms step_avg:58.69ms
step:2194/2330 train_time:128775ms step_avg:58.69ms
step:2195/2330 train_time:128831ms step_avg:58.69ms
step:2196/2330 train_time:128894ms step_avg:58.69ms
step:2197/2330 train_time:128950ms step_avg:58.69ms
step:2198/2330 train_time:129012ms step_avg:58.70ms
step:2199/2330 train_time:129069ms step_avg:58.69ms
step:2200/2330 train_time:129130ms step_avg:58.70ms
step:2201/2330 train_time:129187ms step_avg:58.69ms
step:2202/2330 train_time:129248ms step_avg:58.70ms
step:2203/2330 train_time:129305ms step_avg:58.70ms
step:2204/2330 train_time:129368ms step_avg:58.70ms
step:2205/2330 train_time:129425ms step_avg:58.70ms
step:2206/2330 train_time:129486ms step_avg:58.70ms
step:2207/2330 train_time:129543ms step_avg:58.70ms
step:2208/2330 train_time:129605ms step_avg:58.70ms
step:2209/2330 train_time:129663ms step_avg:58.70ms
step:2210/2330 train_time:129724ms step_avg:58.70ms
step:2211/2330 train_time:129782ms step_avg:58.70ms
step:2212/2330 train_time:129843ms step_avg:58.70ms
step:2213/2330 train_time:129901ms step_avg:58.70ms
step:2214/2330 train_time:129961ms step_avg:58.70ms
step:2215/2330 train_time:130019ms step_avg:58.70ms
step:2216/2330 train_time:130079ms step_avg:58.70ms
step:2217/2330 train_time:130137ms step_avg:58.70ms
step:2218/2330 train_time:130198ms step_avg:58.70ms
step:2219/2330 train_time:130255ms step_avg:58.70ms
step:2220/2330 train_time:130316ms step_avg:58.70ms
step:2221/2330 train_time:130374ms step_avg:58.70ms
step:2222/2330 train_time:130434ms step_avg:58.70ms
step:2223/2330 train_time:130491ms step_avg:58.70ms
step:2224/2330 train_time:130553ms step_avg:58.70ms
step:2225/2330 train_time:130611ms step_avg:58.70ms
step:2226/2330 train_time:130672ms step_avg:58.70ms
step:2227/2330 train_time:130728ms step_avg:58.70ms
step:2228/2330 train_time:130789ms step_avg:58.70ms
step:2229/2330 train_time:130846ms step_avg:58.70ms
step:2230/2330 train_time:130907ms step_avg:58.70ms
step:2231/2330 train_time:130964ms step_avg:58.70ms
step:2232/2330 train_time:131027ms step_avg:58.70ms
step:2233/2330 train_time:131083ms step_avg:58.70ms
step:2234/2330 train_time:131146ms step_avg:58.70ms
step:2235/2330 train_time:131202ms step_avg:58.70ms
step:2236/2330 train_time:131265ms step_avg:58.71ms
step:2237/2330 train_time:131321ms step_avg:58.70ms
step:2238/2330 train_time:131382ms step_avg:58.70ms
step:2239/2330 train_time:131439ms step_avg:58.70ms
step:2240/2330 train_time:131500ms step_avg:58.71ms
step:2241/2330 train_time:131558ms step_avg:58.70ms
step:2242/2330 train_time:131618ms step_avg:58.71ms
step:2243/2330 train_time:131678ms step_avg:58.71ms
step:2244/2330 train_time:131738ms step_avg:58.71ms
step:2245/2330 train_time:131796ms step_avg:58.71ms
step:2246/2330 train_time:131856ms step_avg:58.71ms
step:2247/2330 train_time:131914ms step_avg:58.71ms
step:2248/2330 train_time:131976ms step_avg:58.71ms
step:2249/2330 train_time:132033ms step_avg:58.71ms
step:2250/2330 train_time:132094ms step_avg:58.71ms
step:2250/2330 val_loss:3.7132 train_time:132176ms step_avg:58.74ms
step:2251/2330 train_time:132197ms step_avg:58.73ms
step:2252/2330 train_time:132216ms step_avg:58.71ms
step:2253/2330 train_time:132278ms step_avg:58.71ms
step:2254/2330 train_time:132344ms step_avg:58.71ms
step:2255/2330 train_time:132402ms step_avg:58.71ms
step:2256/2330 train_time:132463ms step_avg:58.72ms
step:2257/2330 train_time:132521ms step_avg:58.72ms
step:2258/2330 train_time:132581ms step_avg:58.72ms
step:2259/2330 train_time:132638ms step_avg:58.72ms
step:2260/2330 train_time:132698ms step_avg:58.72ms
step:2261/2330 train_time:132755ms step_avg:58.72ms
step:2262/2330 train_time:132814ms step_avg:58.72ms
step:2263/2330 train_time:132871ms step_avg:58.71ms
step:2264/2330 train_time:132932ms step_avg:58.72ms
step:2265/2330 train_time:132988ms step_avg:58.71ms
step:2266/2330 train_time:133048ms step_avg:58.72ms
step:2267/2330 train_time:133105ms step_avg:58.71ms
step:2268/2330 train_time:133168ms step_avg:58.72ms
step:2269/2330 train_time:133226ms step_avg:58.72ms
step:2270/2330 train_time:133291ms step_avg:58.72ms
step:2271/2330 train_time:133349ms step_avg:58.72ms
step:2272/2330 train_time:133411ms step_avg:58.72ms
step:2273/2330 train_time:133468ms step_avg:58.72ms
step:2274/2330 train_time:133532ms step_avg:58.72ms
step:2275/2330 train_time:133589ms step_avg:58.72ms
step:2276/2330 train_time:133650ms step_avg:58.72ms
step:2277/2330 train_time:133706ms step_avg:58.72ms
step:2278/2330 train_time:133768ms step_avg:58.72ms
step:2279/2330 train_time:133825ms step_avg:58.72ms
step:2280/2330 train_time:133886ms step_avg:58.72ms
step:2281/2330 train_time:133942ms step_avg:58.72ms
step:2282/2330 train_time:134002ms step_avg:58.72ms
step:2283/2330 train_time:134060ms step_avg:58.72ms
step:2284/2330 train_time:134121ms step_avg:58.72ms
step:2285/2330 train_time:134179ms step_avg:58.72ms
step:2286/2330 train_time:134241ms step_avg:58.72ms
step:2287/2330 train_time:134299ms step_avg:58.72ms
step:2288/2330 train_time:134361ms step_avg:58.72ms
step:2289/2330 train_time:134419ms step_avg:58.72ms
step:2290/2330 train_time:134481ms step_avg:58.73ms
step:2291/2330 train_time:134540ms step_avg:58.73ms
step:2292/2330 train_time:134600ms step_avg:58.73ms
step:2293/2330 train_time:134658ms step_avg:58.73ms
step:2294/2330 train_time:134718ms step_avg:58.73ms
step:2295/2330 train_time:134776ms step_avg:58.73ms
step:2296/2330 train_time:134835ms step_avg:58.73ms
step:2297/2330 train_time:134892ms step_avg:58.73ms
step:2298/2330 train_time:134951ms step_avg:58.73ms
step:2299/2330 train_time:135008ms step_avg:58.72ms
step:2300/2330 train_time:135068ms step_avg:58.73ms
step:2301/2330 train_time:135125ms step_avg:58.72ms
step:2302/2330 train_time:135187ms step_avg:58.73ms
step:2303/2330 train_time:135245ms step_avg:58.73ms
step:2304/2330 train_time:135307ms step_avg:58.73ms
step:2305/2330 train_time:135365ms step_avg:58.73ms
step:2306/2330 train_time:135426ms step_avg:58.73ms
step:2307/2330 train_time:135484ms step_avg:58.73ms
step:2308/2330 train_time:135546ms step_avg:58.73ms
step:2309/2330 train_time:135604ms step_avg:58.73ms
step:2310/2330 train_time:135665ms step_avg:58.73ms
step:2311/2330 train_time:135723ms step_avg:58.73ms
step:2312/2330 train_time:135783ms step_avg:58.73ms
step:2313/2330 train_time:135841ms step_avg:58.73ms
step:2314/2330 train_time:135900ms step_avg:58.73ms
step:2315/2330 train_time:135958ms step_avg:58.73ms
step:2316/2330 train_time:136018ms step_avg:58.73ms
step:2317/2330 train_time:136075ms step_avg:58.73ms
step:2318/2330 train_time:136135ms step_avg:58.73ms
step:2319/2330 train_time:136192ms step_avg:58.73ms
step:2320/2330 train_time:136254ms step_avg:58.73ms
step:2321/2330 train_time:136312ms step_avg:58.73ms
step:2322/2330 train_time:136374ms step_avg:58.73ms
step:2323/2330 train_time:136431ms step_avg:58.73ms
step:2324/2330 train_time:136495ms step_avg:58.73ms
step:2325/2330 train_time:136552ms step_avg:58.73ms
step:2326/2330 train_time:136613ms step_avg:58.73ms
step:2327/2330 train_time:136670ms step_avg:58.73ms
step:2328/2330 train_time:136732ms step_avg:58.73ms
step:2329/2330 train_time:136789ms step_avg:58.73ms
step:2330/2330 train_time:136850ms step_avg:58.73ms
step:2330/2330 val_loss:3.6981 train_time:136931ms step_avg:58.77ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
