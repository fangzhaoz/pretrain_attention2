import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 07:18:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:82.91ms
step:2/2330 train_time:176ms step_avg:87.99ms
step:3/2330 train_time:194ms step_avg:64.67ms
step:4/2330 train_time:213ms step_avg:53.22ms
step:5/2330 train_time:267ms step_avg:53.30ms
step:6/2330 train_time:325ms step_avg:54.20ms
step:7/2330 train_time:380ms step_avg:54.35ms
step:8/2330 train_time:438ms step_avg:54.81ms
step:9/2330 train_time:494ms step_avg:54.92ms
step:10/2330 train_time:553ms step_avg:55.27ms
step:11/2330 train_time:608ms step_avg:55.27ms
step:12/2330 train_time:666ms step_avg:55.49ms
step:13/2330 train_time:721ms step_avg:55.49ms
step:14/2330 train_time:780ms step_avg:55.70ms
step:15/2330 train_time:835ms step_avg:55.67ms
step:16/2330 train_time:893ms step_avg:55.81ms
step:17/2330 train_time:948ms step_avg:55.77ms
step:18/2330 train_time:1007ms step_avg:55.95ms
step:19/2330 train_time:1065ms step_avg:56.04ms
step:20/2330 train_time:1127ms step_avg:56.35ms
step:21/2330 train_time:1184ms step_avg:56.38ms
step:22/2330 train_time:1245ms step_avg:56.61ms
step:23/2330 train_time:1301ms step_avg:56.57ms
step:24/2330 train_time:1362ms step_avg:56.74ms
step:25/2330 train_time:1418ms step_avg:56.71ms
step:26/2330 train_time:1477ms step_avg:56.82ms
step:27/2330 train_time:1534ms step_avg:56.80ms
step:28/2330 train_time:1592ms step_avg:56.86ms
step:29/2330 train_time:1647ms step_avg:56.79ms
step:30/2330 train_time:1706ms step_avg:56.85ms
step:31/2330 train_time:1761ms step_avg:56.81ms
step:32/2330 train_time:1820ms step_avg:56.87ms
step:33/2330 train_time:1876ms step_avg:56.84ms
step:34/2330 train_time:1934ms step_avg:56.88ms
step:35/2330 train_time:1990ms step_avg:56.87ms
step:36/2330 train_time:2049ms step_avg:56.92ms
step:37/2330 train_time:2106ms step_avg:56.92ms
step:38/2330 train_time:2166ms step_avg:57.01ms
step:39/2330 train_time:2223ms step_avg:56.99ms
step:40/2330 train_time:2284ms step_avg:57.09ms
step:41/2330 train_time:2339ms step_avg:57.06ms
step:42/2330 train_time:2400ms step_avg:57.14ms
step:43/2330 train_time:2456ms step_avg:57.12ms
step:44/2330 train_time:2515ms step_avg:57.16ms
step:45/2330 train_time:2571ms step_avg:57.13ms
step:46/2330 train_time:2629ms step_avg:57.16ms
step:47/2330 train_time:2685ms step_avg:57.13ms
step:48/2330 train_time:2743ms step_avg:57.14ms
step:49/2330 train_time:2798ms step_avg:57.11ms
step:50/2330 train_time:2857ms step_avg:57.14ms
step:51/2330 train_time:2913ms step_avg:57.12ms
step:52/2330 train_time:2972ms step_avg:57.15ms
step:53/2330 train_time:3028ms step_avg:57.13ms
step:54/2330 train_time:3087ms step_avg:57.17ms
step:55/2330 train_time:3143ms step_avg:57.15ms
step:56/2330 train_time:3204ms step_avg:57.21ms
step:57/2330 train_time:3259ms step_avg:57.18ms
step:58/2330 train_time:3320ms step_avg:57.24ms
step:59/2330 train_time:3377ms step_avg:57.24ms
step:60/2330 train_time:3436ms step_avg:57.27ms
step:61/2330 train_time:3492ms step_avg:57.25ms
step:62/2330 train_time:3551ms step_avg:57.27ms
step:63/2330 train_time:3606ms step_avg:57.24ms
step:64/2330 train_time:3665ms step_avg:57.26ms
step:65/2330 train_time:3720ms step_avg:57.23ms
step:66/2330 train_time:3779ms step_avg:57.26ms
step:67/2330 train_time:3835ms step_avg:57.24ms
step:68/2330 train_time:3893ms step_avg:57.25ms
step:69/2330 train_time:3949ms step_avg:57.23ms
step:70/2330 train_time:4007ms step_avg:57.25ms
step:71/2330 train_time:4064ms step_avg:57.24ms
step:72/2330 train_time:4123ms step_avg:57.26ms
step:73/2330 train_time:4179ms step_avg:57.25ms
step:74/2330 train_time:4238ms step_avg:57.27ms
step:75/2330 train_time:4294ms step_avg:57.25ms
step:76/2330 train_time:4353ms step_avg:57.28ms
step:77/2330 train_time:4409ms step_avg:57.26ms
step:78/2330 train_time:4468ms step_avg:57.29ms
step:79/2330 train_time:4524ms step_avg:57.27ms
step:80/2330 train_time:4583ms step_avg:57.29ms
step:81/2330 train_time:4639ms step_avg:57.27ms
step:82/2330 train_time:4698ms step_avg:57.30ms
step:83/2330 train_time:4754ms step_avg:57.28ms
step:84/2330 train_time:4813ms step_avg:57.30ms
step:85/2330 train_time:4870ms step_avg:57.29ms
step:86/2330 train_time:4928ms step_avg:57.30ms
step:87/2330 train_time:4985ms step_avg:57.30ms
step:88/2330 train_time:5043ms step_avg:57.31ms
step:89/2330 train_time:5100ms step_avg:57.30ms
step:90/2330 train_time:5160ms step_avg:57.33ms
step:91/2330 train_time:5216ms step_avg:57.32ms
step:92/2330 train_time:5275ms step_avg:57.34ms
step:93/2330 train_time:5331ms step_avg:57.32ms
step:94/2330 train_time:5390ms step_avg:57.34ms
step:95/2330 train_time:5446ms step_avg:57.33ms
step:96/2330 train_time:5505ms step_avg:57.35ms
step:97/2330 train_time:5561ms step_avg:57.32ms
step:98/2330 train_time:5621ms step_avg:57.36ms
step:99/2330 train_time:5677ms step_avg:57.35ms
step:100/2330 train_time:5736ms step_avg:57.36ms
step:101/2330 train_time:5791ms step_avg:57.34ms
step:102/2330 train_time:5850ms step_avg:57.35ms
step:103/2330 train_time:5906ms step_avg:57.34ms
step:104/2330 train_time:5965ms step_avg:57.35ms
step:105/2330 train_time:6021ms step_avg:57.34ms
step:106/2330 train_time:6079ms step_avg:57.35ms
step:107/2330 train_time:6136ms step_avg:57.35ms
step:108/2330 train_time:6195ms step_avg:57.36ms
step:109/2330 train_time:6251ms step_avg:57.35ms
step:110/2330 train_time:6310ms step_avg:57.36ms
step:111/2330 train_time:6366ms step_avg:57.35ms
step:112/2330 train_time:6426ms step_avg:57.37ms
step:113/2330 train_time:6481ms step_avg:57.35ms
step:114/2330 train_time:6541ms step_avg:57.38ms
step:115/2330 train_time:6597ms step_avg:57.37ms
step:116/2330 train_time:6656ms step_avg:57.38ms
step:117/2330 train_time:6712ms step_avg:57.37ms
step:118/2330 train_time:6771ms step_avg:57.38ms
step:119/2330 train_time:6827ms step_avg:57.37ms
step:120/2330 train_time:6886ms step_avg:57.38ms
step:121/2330 train_time:6941ms step_avg:57.36ms
step:122/2330 train_time:7001ms step_avg:57.39ms
step:123/2330 train_time:7057ms step_avg:57.37ms
step:124/2330 train_time:7116ms step_avg:57.39ms
step:125/2330 train_time:7173ms step_avg:57.38ms
step:126/2330 train_time:7232ms step_avg:57.40ms
step:127/2330 train_time:7288ms step_avg:57.38ms
step:128/2330 train_time:7346ms step_avg:57.39ms
step:129/2330 train_time:7402ms step_avg:57.38ms
step:130/2330 train_time:7462ms step_avg:57.40ms
step:131/2330 train_time:7518ms step_avg:57.39ms
step:132/2330 train_time:7577ms step_avg:57.40ms
step:133/2330 train_time:7632ms step_avg:57.39ms
step:134/2330 train_time:7692ms step_avg:57.40ms
step:135/2330 train_time:7747ms step_avg:57.39ms
step:136/2330 train_time:7806ms step_avg:57.40ms
step:137/2330 train_time:7862ms step_avg:57.39ms
step:138/2330 train_time:7921ms step_avg:57.40ms
step:139/2330 train_time:7977ms step_avg:57.39ms
step:140/2330 train_time:8035ms step_avg:57.39ms
step:141/2330 train_time:8091ms step_avg:57.38ms
step:142/2330 train_time:8151ms step_avg:57.40ms
step:143/2330 train_time:8206ms step_avg:57.38ms
step:144/2330 train_time:8265ms step_avg:57.40ms
step:145/2330 train_time:8321ms step_avg:57.39ms
step:146/2330 train_time:8381ms step_avg:57.40ms
step:147/2330 train_time:8437ms step_avg:57.40ms
step:148/2330 train_time:8496ms step_avg:57.41ms
step:149/2330 train_time:8552ms step_avg:57.39ms
step:150/2330 train_time:8610ms step_avg:57.40ms
step:151/2330 train_time:8666ms step_avg:57.39ms
step:152/2330 train_time:8725ms step_avg:57.40ms
step:153/2330 train_time:8781ms step_avg:57.39ms
step:154/2330 train_time:8840ms step_avg:57.41ms
step:155/2330 train_time:8896ms step_avg:57.40ms
step:156/2330 train_time:8955ms step_avg:57.41ms
step:157/2330 train_time:9011ms step_avg:57.39ms
step:158/2330 train_time:9070ms step_avg:57.41ms
step:159/2330 train_time:9125ms step_avg:57.39ms
step:160/2330 train_time:9185ms step_avg:57.40ms
step:161/2330 train_time:9240ms step_avg:57.39ms
step:162/2330 train_time:9299ms step_avg:57.40ms
step:163/2330 train_time:9355ms step_avg:57.39ms
step:164/2330 train_time:9414ms step_avg:57.40ms
step:165/2330 train_time:9470ms step_avg:57.39ms
step:166/2330 train_time:9529ms step_avg:57.40ms
step:167/2330 train_time:9585ms step_avg:57.39ms
step:168/2330 train_time:9643ms step_avg:57.40ms
step:169/2330 train_time:9700ms step_avg:57.39ms
step:170/2330 train_time:9759ms step_avg:57.40ms
step:171/2330 train_time:9815ms step_avg:57.39ms
step:172/2330 train_time:9873ms step_avg:57.40ms
step:173/2330 train_time:9929ms step_avg:57.39ms
step:174/2330 train_time:9988ms step_avg:57.40ms
step:175/2330 train_time:10044ms step_avg:57.39ms
step:176/2330 train_time:10103ms step_avg:57.40ms
step:177/2330 train_time:10158ms step_avg:57.39ms
step:178/2330 train_time:10218ms step_avg:57.40ms
step:179/2330 train_time:10274ms step_avg:57.40ms
step:180/2330 train_time:10333ms step_avg:57.40ms
step:181/2330 train_time:10388ms step_avg:57.39ms
step:182/2330 train_time:10448ms step_avg:57.40ms
step:183/2330 train_time:10503ms step_avg:57.39ms
step:184/2330 train_time:10562ms step_avg:57.40ms
step:185/2330 train_time:10618ms step_avg:57.39ms
step:186/2330 train_time:10678ms step_avg:57.41ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10793ms step_avg:57.41ms
step:189/2330 train_time:10849ms step_avg:57.40ms
step:190/2330 train_time:10907ms step_avg:57.41ms
step:191/2330 train_time:10963ms step_avg:57.40ms
step:192/2330 train_time:11022ms step_avg:57.41ms
step:193/2330 train_time:11078ms step_avg:57.40ms
step:194/2330 train_time:11137ms step_avg:57.41ms
step:195/2330 train_time:11193ms step_avg:57.40ms
step:196/2330 train_time:11252ms step_avg:57.41ms
step:197/2330 train_time:11308ms step_avg:57.40ms
step:198/2330 train_time:11366ms step_avg:57.40ms
step:199/2330 train_time:11422ms step_avg:57.40ms
step:200/2330 train_time:11481ms step_avg:57.41ms
step:201/2330 train_time:11538ms step_avg:57.40ms
step:202/2330 train_time:11597ms step_avg:57.41ms
step:203/2330 train_time:11653ms step_avg:57.40ms
step:204/2330 train_time:11712ms step_avg:57.41ms
step:205/2330 train_time:11767ms step_avg:57.40ms
step:206/2330 train_time:11826ms step_avg:57.41ms
step:207/2330 train_time:11882ms step_avg:57.40ms
step:208/2330 train_time:11941ms step_avg:57.41ms
step:209/2330 train_time:11997ms step_avg:57.40ms
step:210/2330 train_time:12056ms step_avg:57.41ms
step:211/2330 train_time:12112ms step_avg:57.40ms
step:212/2330 train_time:12171ms step_avg:57.41ms
step:213/2330 train_time:12226ms step_avg:57.40ms
step:214/2330 train_time:12285ms step_avg:57.41ms
step:215/2330 train_time:12340ms step_avg:57.40ms
step:216/2330 train_time:12400ms step_avg:57.41ms
step:217/2330 train_time:12456ms step_avg:57.40ms
step:218/2330 train_time:12516ms step_avg:57.41ms
step:219/2330 train_time:12572ms step_avg:57.40ms
step:220/2330 train_time:12631ms step_avg:57.41ms
step:221/2330 train_time:12687ms step_avg:57.41ms
step:222/2330 train_time:12745ms step_avg:57.41ms
step:223/2330 train_time:12801ms step_avg:57.40ms
step:224/2330 train_time:12860ms step_avg:57.41ms
step:225/2330 train_time:12917ms step_avg:57.41ms
step:226/2330 train_time:12976ms step_avg:57.42ms
step:227/2330 train_time:13032ms step_avg:57.41ms
step:228/2330 train_time:13091ms step_avg:57.42ms
step:229/2330 train_time:13147ms step_avg:57.41ms
step:230/2330 train_time:13205ms step_avg:57.41ms
step:231/2330 train_time:13260ms step_avg:57.40ms
step:232/2330 train_time:13321ms step_avg:57.42ms
step:233/2330 train_time:13376ms step_avg:57.41ms
step:234/2330 train_time:13435ms step_avg:57.41ms
step:235/2330 train_time:13490ms step_avg:57.41ms
step:236/2330 train_time:13549ms step_avg:57.41ms
step:237/2330 train_time:13605ms step_avg:57.40ms
step:238/2330 train_time:13664ms step_avg:57.41ms
step:239/2330 train_time:13720ms step_avg:57.40ms
step:240/2330 train_time:13780ms step_avg:57.42ms
step:241/2330 train_time:13836ms step_avg:57.41ms
step:242/2330 train_time:13896ms step_avg:57.42ms
step:243/2330 train_time:13951ms step_avg:57.41ms
step:244/2330 train_time:14011ms step_avg:57.42ms
step:245/2330 train_time:14067ms step_avg:57.42ms
step:246/2330 train_time:14126ms step_avg:57.42ms
step:247/2330 train_time:14181ms step_avg:57.41ms
step:248/2330 train_time:14241ms step_avg:57.43ms
step:249/2330 train_time:14298ms step_avg:57.42ms
step:250/2330 train_time:14356ms step_avg:57.42ms
step:250/2330 val_loss:4.9021 train_time:14436ms step_avg:57.74ms
step:251/2330 train_time:14455ms step_avg:57.59ms
step:252/2330 train_time:14473ms step_avg:57.43ms
step:253/2330 train_time:14528ms step_avg:57.42ms
step:254/2330 train_time:14593ms step_avg:57.45ms
step:255/2330 train_time:14648ms step_avg:57.44ms
step:256/2330 train_time:14710ms step_avg:57.46ms
step:257/2330 train_time:14765ms step_avg:57.45ms
step:258/2330 train_time:14826ms step_avg:57.47ms
step:259/2330 train_time:14882ms step_avg:57.46ms
step:260/2330 train_time:14943ms step_avg:57.47ms
step:261/2330 train_time:14998ms step_avg:57.46ms
step:262/2330 train_time:15057ms step_avg:57.47ms
step:263/2330 train_time:15112ms step_avg:57.46ms
step:264/2330 train_time:15170ms step_avg:57.46ms
step:265/2330 train_time:15226ms step_avg:57.46ms
step:266/2330 train_time:15285ms step_avg:57.46ms
step:267/2330 train_time:15341ms step_avg:57.46ms
step:268/2330 train_time:15400ms step_avg:57.46ms
step:269/2330 train_time:15457ms step_avg:57.46ms
step:270/2330 train_time:15516ms step_avg:57.47ms
step:271/2330 train_time:15573ms step_avg:57.47ms
step:272/2330 train_time:15633ms step_avg:57.48ms
step:273/2330 train_time:15689ms step_avg:57.47ms
step:274/2330 train_time:15749ms step_avg:57.48ms
step:275/2330 train_time:15804ms step_avg:57.47ms
step:276/2330 train_time:15865ms step_avg:57.48ms
step:277/2330 train_time:15920ms step_avg:57.47ms
step:278/2330 train_time:15979ms step_avg:57.48ms
step:279/2330 train_time:16035ms step_avg:57.47ms
step:280/2330 train_time:16093ms step_avg:57.48ms
step:281/2330 train_time:16149ms step_avg:57.47ms
step:282/2330 train_time:16206ms step_avg:57.47ms
step:283/2330 train_time:16262ms step_avg:57.46ms
step:284/2330 train_time:16321ms step_avg:57.47ms
step:285/2330 train_time:16379ms step_avg:57.47ms
step:286/2330 train_time:16438ms step_avg:57.48ms
step:287/2330 train_time:16494ms step_avg:57.47ms
step:288/2330 train_time:16554ms step_avg:57.48ms
step:289/2330 train_time:16610ms step_avg:57.47ms
step:290/2330 train_time:16671ms step_avg:57.49ms
step:291/2330 train_time:16727ms step_avg:57.48ms
step:292/2330 train_time:16787ms step_avg:57.49ms
step:293/2330 train_time:16842ms step_avg:57.48ms
step:294/2330 train_time:16903ms step_avg:57.49ms
step:295/2330 train_time:16959ms step_avg:57.49ms
step:296/2330 train_time:17018ms step_avg:57.49ms
step:297/2330 train_time:17073ms step_avg:57.49ms
step:298/2330 train_time:17133ms step_avg:57.49ms
step:299/2330 train_time:17189ms step_avg:57.49ms
step:300/2330 train_time:17247ms step_avg:57.49ms
step:301/2330 train_time:17302ms step_avg:57.48ms
step:302/2330 train_time:17362ms step_avg:57.49ms
step:303/2330 train_time:17419ms step_avg:57.49ms
step:304/2330 train_time:17478ms step_avg:57.49ms
step:305/2330 train_time:17534ms step_avg:57.49ms
step:306/2330 train_time:17593ms step_avg:57.49ms
step:307/2330 train_time:17649ms step_avg:57.49ms
step:308/2330 train_time:17708ms step_avg:57.49ms
step:309/2330 train_time:17764ms step_avg:57.49ms
step:310/2330 train_time:17823ms step_avg:57.49ms
step:311/2330 train_time:17879ms step_avg:57.49ms
step:312/2330 train_time:17938ms step_avg:57.49ms
step:313/2330 train_time:17994ms step_avg:57.49ms
step:314/2330 train_time:18053ms step_avg:57.49ms
step:315/2330 train_time:18108ms step_avg:57.49ms
step:316/2330 train_time:18167ms step_avg:57.49ms
step:317/2330 train_time:18223ms step_avg:57.48ms
step:318/2330 train_time:18282ms step_avg:57.49ms
step:319/2330 train_time:18337ms step_avg:57.48ms
step:320/2330 train_time:18397ms step_avg:57.49ms
step:321/2330 train_time:18452ms step_avg:57.48ms
step:322/2330 train_time:18512ms step_avg:57.49ms
step:323/2330 train_time:18568ms step_avg:57.49ms
step:324/2330 train_time:18628ms step_avg:57.49ms
step:325/2330 train_time:18684ms step_avg:57.49ms
step:326/2330 train_time:18744ms step_avg:57.50ms
step:327/2330 train_time:18800ms step_avg:57.49ms
step:328/2330 train_time:18859ms step_avg:57.50ms
step:329/2330 train_time:18915ms step_avg:57.49ms
step:330/2330 train_time:18975ms step_avg:57.50ms
step:331/2330 train_time:19030ms step_avg:57.49ms
step:332/2330 train_time:19089ms step_avg:57.50ms
step:333/2330 train_time:19145ms step_avg:57.49ms
step:334/2330 train_time:19204ms step_avg:57.50ms
step:335/2330 train_time:19260ms step_avg:57.49ms
step:336/2330 train_time:19318ms step_avg:57.49ms
step:337/2330 train_time:19374ms step_avg:57.49ms
step:338/2330 train_time:19433ms step_avg:57.49ms
step:339/2330 train_time:19489ms step_avg:57.49ms
step:340/2330 train_time:19547ms step_avg:57.49ms
step:341/2330 train_time:19603ms step_avg:57.49ms
step:342/2330 train_time:19663ms step_avg:57.49ms
step:343/2330 train_time:19720ms step_avg:57.49ms
step:344/2330 train_time:19779ms step_avg:57.50ms
step:345/2330 train_time:19835ms step_avg:57.49ms
step:346/2330 train_time:19894ms step_avg:57.50ms
step:347/2330 train_time:19950ms step_avg:57.49ms
step:348/2330 train_time:20008ms step_avg:57.49ms
step:349/2330 train_time:20063ms step_avg:57.49ms
step:350/2330 train_time:20123ms step_avg:57.49ms
step:351/2330 train_time:20179ms step_avg:57.49ms
step:352/2330 train_time:20238ms step_avg:57.50ms
step:353/2330 train_time:20294ms step_avg:57.49ms
step:354/2330 train_time:20353ms step_avg:57.49ms
step:355/2330 train_time:20409ms step_avg:57.49ms
step:356/2330 train_time:20467ms step_avg:57.49ms
step:357/2330 train_time:20523ms step_avg:57.49ms
step:358/2330 train_time:20583ms step_avg:57.49ms
step:359/2330 train_time:20639ms step_avg:57.49ms
step:360/2330 train_time:20698ms step_avg:57.49ms
step:361/2330 train_time:20754ms step_avg:57.49ms
step:362/2330 train_time:20813ms step_avg:57.49ms
step:363/2330 train_time:20869ms step_avg:57.49ms
step:364/2330 train_time:20928ms step_avg:57.49ms
step:365/2330 train_time:20983ms step_avg:57.49ms
step:366/2330 train_time:21044ms step_avg:57.50ms
step:367/2330 train_time:21099ms step_avg:57.49ms
step:368/2330 train_time:21159ms step_avg:57.50ms
step:369/2330 train_time:21214ms step_avg:57.49ms
step:370/2330 train_time:21273ms step_avg:57.50ms
step:371/2330 train_time:21329ms step_avg:57.49ms
step:372/2330 train_time:21388ms step_avg:57.49ms
step:373/2330 train_time:21444ms step_avg:57.49ms
step:374/2330 train_time:21503ms step_avg:57.49ms
step:375/2330 train_time:21559ms step_avg:57.49ms
step:376/2330 train_time:21618ms step_avg:57.49ms
step:377/2330 train_time:21674ms step_avg:57.49ms
step:378/2330 train_time:21733ms step_avg:57.49ms
step:379/2330 train_time:21789ms step_avg:57.49ms
step:380/2330 train_time:21847ms step_avg:57.49ms
step:381/2330 train_time:21903ms step_avg:57.49ms
step:382/2330 train_time:21962ms step_avg:57.49ms
step:383/2330 train_time:22018ms step_avg:57.49ms
step:384/2330 train_time:22078ms step_avg:57.49ms
step:385/2330 train_time:22134ms step_avg:57.49ms
step:386/2330 train_time:22193ms step_avg:57.50ms
step:387/2330 train_time:22249ms step_avg:57.49ms
step:388/2330 train_time:22308ms step_avg:57.49ms
step:389/2330 train_time:22364ms step_avg:57.49ms
step:390/2330 train_time:22423ms step_avg:57.49ms
step:391/2330 train_time:22478ms step_avg:57.49ms
step:392/2330 train_time:22538ms step_avg:57.50ms
step:393/2330 train_time:22594ms step_avg:57.49ms
step:394/2330 train_time:22653ms step_avg:57.50ms
step:395/2330 train_time:22709ms step_avg:57.49ms
step:396/2330 train_time:22769ms step_avg:57.50ms
step:397/2330 train_time:22824ms step_avg:57.49ms
step:398/2330 train_time:22884ms step_avg:57.50ms
step:399/2330 train_time:22941ms step_avg:57.50ms
step:400/2330 train_time:23000ms step_avg:57.50ms
step:401/2330 train_time:23055ms step_avg:57.49ms
step:402/2330 train_time:23115ms step_avg:57.50ms
step:403/2330 train_time:23171ms step_avg:57.50ms
step:404/2330 train_time:23231ms step_avg:57.50ms
step:405/2330 train_time:23286ms step_avg:57.50ms
step:406/2330 train_time:23344ms step_avg:57.50ms
step:407/2330 train_time:23400ms step_avg:57.49ms
step:408/2330 train_time:23460ms step_avg:57.50ms
step:409/2330 train_time:23516ms step_avg:57.50ms
step:410/2330 train_time:23575ms step_avg:57.50ms
step:411/2330 train_time:23631ms step_avg:57.50ms
step:412/2330 train_time:23690ms step_avg:57.50ms
step:413/2330 train_time:23746ms step_avg:57.50ms
step:414/2330 train_time:23805ms step_avg:57.50ms
step:415/2330 train_time:23861ms step_avg:57.50ms
step:416/2330 train_time:23921ms step_avg:57.50ms
step:417/2330 train_time:23977ms step_avg:57.50ms
step:418/2330 train_time:24036ms step_avg:57.50ms
step:419/2330 train_time:24092ms step_avg:57.50ms
step:420/2330 train_time:24151ms step_avg:57.50ms
step:421/2330 train_time:24207ms step_avg:57.50ms
step:422/2330 train_time:24266ms step_avg:57.50ms
step:423/2330 train_time:24321ms step_avg:57.50ms
step:424/2330 train_time:24382ms step_avg:57.50ms
step:425/2330 train_time:24438ms step_avg:57.50ms
step:426/2330 train_time:24497ms step_avg:57.50ms
step:427/2330 train_time:24553ms step_avg:57.50ms
step:428/2330 train_time:24611ms step_avg:57.50ms
step:429/2330 train_time:24668ms step_avg:57.50ms
step:430/2330 train_time:24727ms step_avg:57.50ms
step:431/2330 train_time:24783ms step_avg:57.50ms
step:432/2330 train_time:24842ms step_avg:57.51ms
step:433/2330 train_time:24898ms step_avg:57.50ms
step:434/2330 train_time:24957ms step_avg:57.51ms
step:435/2330 train_time:25013ms step_avg:57.50ms
step:436/2330 train_time:25073ms step_avg:57.51ms
step:437/2330 train_time:25129ms step_avg:57.50ms
step:438/2330 train_time:25188ms step_avg:57.51ms
step:439/2330 train_time:25243ms step_avg:57.50ms
step:440/2330 train_time:25304ms step_avg:57.51ms
step:441/2330 train_time:25361ms step_avg:57.51ms
step:442/2330 train_time:25419ms step_avg:57.51ms
step:443/2330 train_time:25475ms step_avg:57.51ms
step:444/2330 train_time:25533ms step_avg:57.51ms
step:445/2330 train_time:25589ms step_avg:57.50ms
step:446/2330 train_time:25648ms step_avg:57.51ms
step:447/2330 train_time:25704ms step_avg:57.50ms
step:448/2330 train_time:25764ms step_avg:57.51ms
step:449/2330 train_time:25821ms step_avg:57.51ms
step:450/2330 train_time:25880ms step_avg:57.51ms
step:451/2330 train_time:25937ms step_avg:57.51ms
step:452/2330 train_time:25995ms step_avg:57.51ms
step:453/2330 train_time:26052ms step_avg:57.51ms
step:454/2330 train_time:26111ms step_avg:57.51ms
step:455/2330 train_time:26167ms step_avg:57.51ms
step:456/2330 train_time:26226ms step_avg:57.51ms
step:457/2330 train_time:26282ms step_avg:57.51ms
step:458/2330 train_time:26342ms step_avg:57.51ms
step:459/2330 train_time:26398ms step_avg:57.51ms
step:460/2330 train_time:26457ms step_avg:57.52ms
step:461/2330 train_time:26513ms step_avg:57.51ms
step:462/2330 train_time:26572ms step_avg:57.51ms
step:463/2330 train_time:26628ms step_avg:57.51ms
step:464/2330 train_time:26687ms step_avg:57.52ms
step:465/2330 train_time:26742ms step_avg:57.51ms
step:466/2330 train_time:26803ms step_avg:57.52ms
step:467/2330 train_time:26859ms step_avg:57.51ms
step:468/2330 train_time:26918ms step_avg:57.52ms
step:469/2330 train_time:26974ms step_avg:57.51ms
step:470/2330 train_time:27034ms step_avg:57.52ms
step:471/2330 train_time:27089ms step_avg:57.51ms
step:472/2330 train_time:27148ms step_avg:57.52ms
step:473/2330 train_time:27204ms step_avg:57.51ms
step:474/2330 train_time:27263ms step_avg:57.52ms
step:475/2330 train_time:27319ms step_avg:57.51ms
step:476/2330 train_time:27378ms step_avg:57.52ms
step:477/2330 train_time:27434ms step_avg:57.51ms
step:478/2330 train_time:27493ms step_avg:57.52ms
step:479/2330 train_time:27550ms step_avg:57.51ms
step:480/2330 train_time:27608ms step_avg:57.52ms
step:481/2330 train_time:27663ms step_avg:57.51ms
step:482/2330 train_time:27723ms step_avg:57.52ms
step:483/2330 train_time:27780ms step_avg:57.52ms
step:484/2330 train_time:27839ms step_avg:57.52ms
step:485/2330 train_time:27895ms step_avg:57.52ms
step:486/2330 train_time:27954ms step_avg:57.52ms
step:487/2330 train_time:28010ms step_avg:57.52ms
step:488/2330 train_time:28069ms step_avg:57.52ms
step:489/2330 train_time:28125ms step_avg:57.52ms
step:490/2330 train_time:28184ms step_avg:57.52ms
step:491/2330 train_time:28240ms step_avg:57.51ms
step:492/2330 train_time:28300ms step_avg:57.52ms
step:493/2330 train_time:28356ms step_avg:57.52ms
step:494/2330 train_time:28415ms step_avg:57.52ms
step:495/2330 train_time:28471ms step_avg:57.52ms
step:496/2330 train_time:28530ms step_avg:57.52ms
step:497/2330 train_time:28586ms step_avg:57.52ms
step:498/2330 train_time:28646ms step_avg:57.52ms
step:499/2330 train_time:28702ms step_avg:57.52ms
step:500/2330 train_time:28762ms step_avg:57.52ms
step:500/2330 val_loss:4.4152 train_time:28842ms step_avg:57.68ms
step:501/2330 train_time:28860ms step_avg:57.60ms
step:502/2330 train_time:28880ms step_avg:57.53ms
step:503/2330 train_time:28938ms step_avg:57.53ms
step:504/2330 train_time:28999ms step_avg:57.54ms
step:505/2330 train_time:29056ms step_avg:57.54ms
step:506/2330 train_time:29118ms step_avg:57.54ms
step:507/2330 train_time:29173ms step_avg:57.54ms
step:508/2330 train_time:29233ms step_avg:57.54ms
step:509/2330 train_time:29288ms step_avg:57.54ms
step:510/2330 train_time:29348ms step_avg:57.54ms
step:511/2330 train_time:29403ms step_avg:57.54ms
step:512/2330 train_time:29461ms step_avg:57.54ms
step:513/2330 train_time:29517ms step_avg:57.54ms
step:514/2330 train_time:29575ms step_avg:57.54ms
step:515/2330 train_time:29631ms step_avg:57.54ms
step:516/2330 train_time:29689ms step_avg:57.54ms
step:517/2330 train_time:29745ms step_avg:57.53ms
step:518/2330 train_time:29804ms step_avg:57.54ms
step:519/2330 train_time:29860ms step_avg:57.53ms
step:520/2330 train_time:29920ms step_avg:57.54ms
step:521/2330 train_time:29978ms step_avg:57.54ms
step:522/2330 train_time:30038ms step_avg:57.54ms
step:523/2330 train_time:30095ms step_avg:57.54ms
step:524/2330 train_time:30155ms step_avg:57.55ms
step:525/2330 train_time:30211ms step_avg:57.54ms
step:526/2330 train_time:30270ms step_avg:57.55ms
step:527/2330 train_time:30325ms step_avg:57.54ms
step:528/2330 train_time:30385ms step_avg:57.55ms
step:529/2330 train_time:30440ms step_avg:57.54ms
step:530/2330 train_time:30499ms step_avg:57.55ms
step:531/2330 train_time:30555ms step_avg:57.54ms
step:532/2330 train_time:30613ms step_avg:57.54ms
step:533/2330 train_time:30669ms step_avg:57.54ms
step:534/2330 train_time:30728ms step_avg:57.54ms
step:535/2330 train_time:30784ms step_avg:57.54ms
step:536/2330 train_time:30843ms step_avg:57.54ms
step:537/2330 train_time:30901ms step_avg:57.54ms
step:538/2330 train_time:30960ms step_avg:57.55ms
step:539/2330 train_time:31017ms step_avg:57.54ms
step:540/2330 train_time:31075ms step_avg:57.55ms
step:541/2330 train_time:31132ms step_avg:57.55ms
step:542/2330 train_time:31192ms step_avg:57.55ms
step:543/2330 train_time:31248ms step_avg:57.55ms
step:544/2330 train_time:31308ms step_avg:57.55ms
step:545/2330 train_time:31364ms step_avg:57.55ms
step:546/2330 train_time:31423ms step_avg:57.55ms
step:547/2330 train_time:31479ms step_avg:57.55ms
step:548/2330 train_time:31537ms step_avg:57.55ms
step:549/2330 train_time:31592ms step_avg:57.55ms
step:550/2330 train_time:31650ms step_avg:57.55ms
step:551/2330 train_time:31705ms step_avg:57.54ms
step:552/2330 train_time:31764ms step_avg:57.54ms
step:553/2330 train_time:31820ms step_avg:57.54ms
step:554/2330 train_time:31881ms step_avg:57.55ms
step:555/2330 train_time:31937ms step_avg:57.54ms
step:556/2330 train_time:31996ms step_avg:57.55ms
step:557/2330 train_time:32052ms step_avg:57.54ms
step:558/2330 train_time:32112ms step_avg:57.55ms
step:559/2330 train_time:32168ms step_avg:57.54ms
step:560/2330 train_time:32228ms step_avg:57.55ms
step:561/2330 train_time:32284ms step_avg:57.55ms
step:562/2330 train_time:32343ms step_avg:57.55ms
step:563/2330 train_time:32400ms step_avg:57.55ms
step:564/2330 train_time:32459ms step_avg:57.55ms
step:565/2330 train_time:32514ms step_avg:57.55ms
step:566/2330 train_time:32573ms step_avg:57.55ms
step:567/2330 train_time:32628ms step_avg:57.55ms
step:568/2330 train_time:32687ms step_avg:57.55ms
step:569/2330 train_time:32743ms step_avg:57.54ms
step:570/2330 train_time:32802ms step_avg:57.55ms
step:571/2330 train_time:32858ms step_avg:57.54ms
step:572/2330 train_time:32917ms step_avg:57.55ms
step:573/2330 train_time:32973ms step_avg:57.54ms
step:574/2330 train_time:33033ms step_avg:57.55ms
step:575/2330 train_time:33089ms step_avg:57.55ms
step:576/2330 train_time:33149ms step_avg:57.55ms
step:577/2330 train_time:33205ms step_avg:57.55ms
step:578/2330 train_time:33264ms step_avg:57.55ms
step:579/2330 train_time:33320ms step_avg:57.55ms
step:580/2330 train_time:33379ms step_avg:57.55ms
step:581/2330 train_time:33435ms step_avg:57.55ms
step:582/2330 train_time:33495ms step_avg:57.55ms
step:583/2330 train_time:33551ms step_avg:57.55ms
step:584/2330 train_time:33609ms step_avg:57.55ms
step:585/2330 train_time:33665ms step_avg:57.55ms
step:586/2330 train_time:33724ms step_avg:57.55ms
step:587/2330 train_time:33780ms step_avg:57.55ms
step:588/2330 train_time:33839ms step_avg:57.55ms
step:589/2330 train_time:33895ms step_avg:57.55ms
step:590/2330 train_time:33953ms step_avg:57.55ms
step:591/2330 train_time:34010ms step_avg:57.55ms
step:592/2330 train_time:34069ms step_avg:57.55ms
step:593/2330 train_time:34125ms step_avg:57.55ms
step:594/2330 train_time:34185ms step_avg:57.55ms
step:595/2330 train_time:34241ms step_avg:57.55ms
step:596/2330 train_time:34300ms step_avg:57.55ms
step:597/2330 train_time:34356ms step_avg:57.55ms
step:598/2330 train_time:34416ms step_avg:57.55ms
step:599/2330 train_time:34472ms step_avg:57.55ms
step:600/2330 train_time:34531ms step_avg:57.55ms
step:601/2330 train_time:34587ms step_avg:57.55ms
step:602/2330 train_time:34646ms step_avg:57.55ms
step:603/2330 train_time:34702ms step_avg:57.55ms
step:604/2330 train_time:34761ms step_avg:57.55ms
step:605/2330 train_time:34817ms step_avg:57.55ms
step:606/2330 train_time:34876ms step_avg:57.55ms
step:607/2330 train_time:34932ms step_avg:57.55ms
step:608/2330 train_time:34990ms step_avg:57.55ms
step:609/2330 train_time:35046ms step_avg:57.55ms
step:610/2330 train_time:35105ms step_avg:57.55ms
step:611/2330 train_time:35162ms step_avg:57.55ms
step:612/2330 train_time:35221ms step_avg:57.55ms
step:613/2330 train_time:35277ms step_avg:57.55ms
step:614/2330 train_time:35336ms step_avg:57.55ms
step:615/2330 train_time:35393ms step_avg:57.55ms
step:616/2330 train_time:35451ms step_avg:57.55ms
step:617/2330 train_time:35507ms step_avg:57.55ms
step:618/2330 train_time:35567ms step_avg:57.55ms
step:619/2330 train_time:35624ms step_avg:57.55ms
step:620/2330 train_time:35682ms step_avg:57.55ms
step:621/2330 train_time:35738ms step_avg:57.55ms
step:622/2330 train_time:35796ms step_avg:57.55ms
step:623/2330 train_time:35852ms step_avg:57.55ms
step:624/2330 train_time:35912ms step_avg:57.55ms
step:625/2330 train_time:35967ms step_avg:57.55ms
step:626/2330 train_time:36027ms step_avg:57.55ms
step:627/2330 train_time:36084ms step_avg:57.55ms
step:628/2330 train_time:36143ms step_avg:57.55ms
step:629/2330 train_time:36198ms step_avg:57.55ms
step:630/2330 train_time:36258ms step_avg:57.55ms
step:631/2330 train_time:36314ms step_avg:57.55ms
step:632/2330 train_time:36373ms step_avg:57.55ms
step:633/2330 train_time:36429ms step_avg:57.55ms
step:634/2330 train_time:36489ms step_avg:57.55ms
step:635/2330 train_time:36545ms step_avg:57.55ms
step:636/2330 train_time:36604ms step_avg:57.55ms
step:637/2330 train_time:36660ms step_avg:57.55ms
step:638/2330 train_time:36718ms step_avg:57.55ms
step:639/2330 train_time:36774ms step_avg:57.55ms
step:640/2330 train_time:36833ms step_avg:57.55ms
step:641/2330 train_time:36889ms step_avg:57.55ms
step:642/2330 train_time:36947ms step_avg:57.55ms
step:643/2330 train_time:37005ms step_avg:57.55ms
step:644/2330 train_time:37063ms step_avg:57.55ms
step:645/2330 train_time:37119ms step_avg:57.55ms
step:646/2330 train_time:37178ms step_avg:57.55ms
step:647/2330 train_time:37235ms step_avg:57.55ms
step:648/2330 train_time:37293ms step_avg:57.55ms
step:649/2330 train_time:37349ms step_avg:57.55ms
step:650/2330 train_time:37408ms step_avg:57.55ms
step:651/2330 train_time:37464ms step_avg:57.55ms
step:652/2330 train_time:37523ms step_avg:57.55ms
step:653/2330 train_time:37580ms step_avg:57.55ms
step:654/2330 train_time:37639ms step_avg:57.55ms
step:655/2330 train_time:37695ms step_avg:57.55ms
step:656/2330 train_time:37754ms step_avg:57.55ms
step:657/2330 train_time:37810ms step_avg:57.55ms
step:658/2330 train_time:37869ms step_avg:57.55ms
step:659/2330 train_time:37925ms step_avg:57.55ms
step:660/2330 train_time:37984ms step_avg:57.55ms
step:661/2330 train_time:38040ms step_avg:57.55ms
step:662/2330 train_time:38099ms step_avg:57.55ms
step:663/2330 train_time:38155ms step_avg:57.55ms
step:664/2330 train_time:38215ms step_avg:57.55ms
step:665/2330 train_time:38271ms step_avg:57.55ms
step:666/2330 train_time:38330ms step_avg:57.55ms
step:667/2330 train_time:38386ms step_avg:57.55ms
step:668/2330 train_time:38445ms step_avg:57.55ms
step:669/2330 train_time:38501ms step_avg:57.55ms
step:670/2330 train_time:38561ms step_avg:57.55ms
step:671/2330 train_time:38617ms step_avg:57.55ms
step:672/2330 train_time:38676ms step_avg:57.55ms
step:673/2330 train_time:38731ms step_avg:57.55ms
step:674/2330 train_time:38790ms step_avg:57.55ms
step:675/2330 train_time:38846ms step_avg:57.55ms
step:676/2330 train_time:38906ms step_avg:57.55ms
step:677/2330 train_time:38962ms step_avg:57.55ms
step:678/2330 train_time:39021ms step_avg:57.55ms
step:679/2330 train_time:39077ms step_avg:57.55ms
step:680/2330 train_time:39136ms step_avg:57.55ms
step:681/2330 train_time:39192ms step_avg:57.55ms
step:682/2330 train_time:39251ms step_avg:57.55ms
step:683/2330 train_time:39307ms step_avg:57.55ms
step:684/2330 train_time:39366ms step_avg:57.55ms
step:685/2330 train_time:39422ms step_avg:57.55ms
step:686/2330 train_time:39482ms step_avg:57.55ms
step:687/2330 train_time:39538ms step_avg:57.55ms
step:688/2330 train_time:39596ms step_avg:57.55ms
step:689/2330 train_time:39652ms step_avg:57.55ms
step:690/2330 train_time:39711ms step_avg:57.55ms
step:691/2330 train_time:39767ms step_avg:57.55ms
step:692/2330 train_time:39828ms step_avg:57.55ms
step:693/2330 train_time:39884ms step_avg:57.55ms
step:694/2330 train_time:39943ms step_avg:57.55ms
step:695/2330 train_time:39998ms step_avg:57.55ms
step:696/2330 train_time:40057ms step_avg:57.55ms
step:697/2330 train_time:40114ms step_avg:57.55ms
step:698/2330 train_time:40173ms step_avg:57.56ms
step:699/2330 train_time:40229ms step_avg:57.55ms
step:700/2330 train_time:40288ms step_avg:57.55ms
step:701/2330 train_time:40345ms step_avg:57.55ms
step:702/2330 train_time:40404ms step_avg:57.56ms
step:703/2330 train_time:40460ms step_avg:57.55ms
step:704/2330 train_time:40518ms step_avg:57.55ms
step:705/2330 train_time:40575ms step_avg:57.55ms
step:706/2330 train_time:40633ms step_avg:57.55ms
step:707/2330 train_time:40688ms step_avg:57.55ms
step:708/2330 train_time:40748ms step_avg:57.55ms
step:709/2330 train_time:40803ms step_avg:57.55ms
step:710/2330 train_time:40863ms step_avg:57.55ms
step:711/2330 train_time:40919ms step_avg:57.55ms
step:712/2330 train_time:40978ms step_avg:57.55ms
step:713/2330 train_time:41035ms step_avg:57.55ms
step:714/2330 train_time:41093ms step_avg:57.55ms
step:715/2330 train_time:41148ms step_avg:57.55ms
step:716/2330 train_time:41209ms step_avg:57.55ms
step:717/2330 train_time:41265ms step_avg:57.55ms
step:718/2330 train_time:41325ms step_avg:57.56ms
step:719/2330 train_time:41382ms step_avg:57.55ms
step:720/2330 train_time:41441ms step_avg:57.56ms
step:721/2330 train_time:41497ms step_avg:57.56ms
step:722/2330 train_time:41556ms step_avg:57.56ms
step:723/2330 train_time:41612ms step_avg:57.55ms
step:724/2330 train_time:41670ms step_avg:57.56ms
step:725/2330 train_time:41726ms step_avg:57.55ms
step:726/2330 train_time:41786ms step_avg:57.56ms
step:727/2330 train_time:41842ms step_avg:57.55ms
step:728/2330 train_time:41901ms step_avg:57.56ms
step:729/2330 train_time:41958ms step_avg:57.56ms
step:730/2330 train_time:42017ms step_avg:57.56ms
step:731/2330 train_time:42072ms step_avg:57.55ms
step:732/2330 train_time:42131ms step_avg:57.56ms
step:733/2330 train_time:42187ms step_avg:57.55ms
step:734/2330 train_time:42246ms step_avg:57.56ms
step:735/2330 train_time:42302ms step_avg:57.55ms
step:736/2330 train_time:42362ms step_avg:57.56ms
step:737/2330 train_time:42418ms step_avg:57.55ms
step:738/2330 train_time:42477ms step_avg:57.56ms
step:739/2330 train_time:42534ms step_avg:57.56ms
step:740/2330 train_time:42592ms step_avg:57.56ms
step:741/2330 train_time:42647ms step_avg:57.55ms
step:742/2330 train_time:42708ms step_avg:57.56ms
step:743/2330 train_time:42765ms step_avg:57.56ms
step:744/2330 train_time:42823ms step_avg:57.56ms
step:745/2330 train_time:42879ms step_avg:57.56ms
step:746/2330 train_time:42939ms step_avg:57.56ms
step:747/2330 train_time:42995ms step_avg:57.56ms
step:748/2330 train_time:43053ms step_avg:57.56ms
step:749/2330 train_time:43110ms step_avg:57.56ms
step:750/2330 train_time:43169ms step_avg:57.56ms
step:750/2330 val_loss:4.2130 train_time:43249ms step_avg:57.67ms
step:751/2330 train_time:43267ms step_avg:57.61ms
step:752/2330 train_time:43289ms step_avg:57.56ms
step:753/2330 train_time:43346ms step_avg:57.56ms
step:754/2330 train_time:43408ms step_avg:57.57ms
step:755/2330 train_time:43465ms step_avg:57.57ms
step:756/2330 train_time:43524ms step_avg:57.57ms
step:757/2330 train_time:43580ms step_avg:57.57ms
step:758/2330 train_time:43639ms step_avg:57.57ms
step:759/2330 train_time:43695ms step_avg:57.57ms
step:760/2330 train_time:43754ms step_avg:57.57ms
step:761/2330 train_time:43810ms step_avg:57.57ms
step:762/2330 train_time:43869ms step_avg:57.57ms
step:763/2330 train_time:43925ms step_avg:57.57ms
step:764/2330 train_time:43983ms step_avg:57.57ms
step:765/2330 train_time:44040ms step_avg:57.57ms
step:766/2330 train_time:44097ms step_avg:57.57ms
step:767/2330 train_time:44153ms step_avg:57.57ms
step:768/2330 train_time:44214ms step_avg:57.57ms
step:769/2330 train_time:44272ms step_avg:57.57ms
step:770/2330 train_time:44333ms step_avg:57.58ms
step:771/2330 train_time:44391ms step_avg:57.58ms
step:772/2330 train_time:44453ms step_avg:57.58ms
step:773/2330 train_time:44510ms step_avg:57.58ms
step:774/2330 train_time:44572ms step_avg:57.59ms
step:775/2330 train_time:44629ms step_avg:57.59ms
step:776/2330 train_time:44689ms step_avg:57.59ms
step:777/2330 train_time:44746ms step_avg:57.59ms
step:778/2330 train_time:44805ms step_avg:57.59ms
step:779/2330 train_time:44862ms step_avg:57.59ms
step:780/2330 train_time:44920ms step_avg:57.59ms
step:781/2330 train_time:44978ms step_avg:57.59ms
step:782/2330 train_time:45036ms step_avg:57.59ms
step:783/2330 train_time:45092ms step_avg:57.59ms
step:784/2330 train_time:45152ms step_avg:57.59ms
step:785/2330 train_time:45209ms step_avg:57.59ms
step:786/2330 train_time:45269ms step_avg:57.59ms
step:787/2330 train_time:45327ms step_avg:57.59ms
step:788/2330 train_time:45387ms step_avg:57.60ms
step:789/2330 train_time:45443ms step_avg:57.60ms
step:790/2330 train_time:45505ms step_avg:57.60ms
step:791/2330 train_time:45561ms step_avg:57.60ms
step:792/2330 train_time:45621ms step_avg:57.60ms
step:793/2330 train_time:45677ms step_avg:57.60ms
step:794/2330 train_time:45738ms step_avg:57.60ms
step:795/2330 train_time:45794ms step_avg:57.60ms
step:796/2330 train_time:45855ms step_avg:57.61ms
step:797/2330 train_time:45911ms step_avg:57.61ms
step:798/2330 train_time:45972ms step_avg:57.61ms
step:799/2330 train_time:46028ms step_avg:57.61ms
step:800/2330 train_time:46087ms step_avg:57.61ms
step:801/2330 train_time:46145ms step_avg:57.61ms
step:802/2330 train_time:46204ms step_avg:57.61ms
step:803/2330 train_time:46261ms step_avg:57.61ms
step:804/2330 train_time:46321ms step_avg:57.61ms
step:805/2330 train_time:46378ms step_avg:57.61ms
step:806/2330 train_time:46437ms step_avg:57.61ms
step:807/2330 train_time:46494ms step_avg:57.61ms
step:808/2330 train_time:46556ms step_avg:57.62ms
step:809/2330 train_time:46613ms step_avg:57.62ms
step:810/2330 train_time:46673ms step_avg:57.62ms
step:811/2330 train_time:46731ms step_avg:57.62ms
step:812/2330 train_time:46790ms step_avg:57.62ms
step:813/2330 train_time:46847ms step_avg:57.62ms
step:814/2330 train_time:46908ms step_avg:57.63ms
step:815/2330 train_time:46964ms step_avg:57.62ms
step:816/2330 train_time:47024ms step_avg:57.63ms
step:817/2330 train_time:47080ms step_avg:57.63ms
step:818/2330 train_time:47139ms step_avg:57.63ms
step:819/2330 train_time:47197ms step_avg:57.63ms
step:820/2330 train_time:47256ms step_avg:57.63ms
step:821/2330 train_time:47313ms step_avg:57.63ms
step:822/2330 train_time:47373ms step_avg:57.63ms
step:823/2330 train_time:47430ms step_avg:57.63ms
step:824/2330 train_time:47491ms step_avg:57.63ms
step:825/2330 train_time:47549ms step_avg:57.63ms
step:826/2330 train_time:47608ms step_avg:57.64ms
step:827/2330 train_time:47666ms step_avg:57.64ms
step:828/2330 train_time:47726ms step_avg:57.64ms
step:829/2330 train_time:47784ms step_avg:57.64ms
step:830/2330 train_time:47843ms step_avg:57.64ms
step:831/2330 train_time:47899ms step_avg:57.64ms
step:832/2330 train_time:47959ms step_avg:57.64ms
step:833/2330 train_time:48015ms step_avg:57.64ms
step:834/2330 train_time:48075ms step_avg:57.64ms
step:835/2330 train_time:48132ms step_avg:57.64ms
step:836/2330 train_time:48192ms step_avg:57.65ms
step:837/2330 train_time:48250ms step_avg:57.65ms
step:838/2330 train_time:48309ms step_avg:57.65ms
step:839/2330 train_time:48366ms step_avg:57.65ms
step:840/2330 train_time:48426ms step_avg:57.65ms
step:841/2330 train_time:48483ms step_avg:57.65ms
step:842/2330 train_time:48544ms step_avg:57.65ms
step:843/2330 train_time:48600ms step_avg:57.65ms
step:844/2330 train_time:48661ms step_avg:57.65ms
step:845/2330 train_time:48717ms step_avg:57.65ms
step:846/2330 train_time:48777ms step_avg:57.66ms
step:847/2330 train_time:48834ms step_avg:57.65ms
step:848/2330 train_time:48894ms step_avg:57.66ms
step:849/2330 train_time:48951ms step_avg:57.66ms
step:850/2330 train_time:49011ms step_avg:57.66ms
step:851/2330 train_time:49067ms step_avg:57.66ms
step:852/2330 train_time:49129ms step_avg:57.66ms
step:853/2330 train_time:49186ms step_avg:57.66ms
step:854/2330 train_time:49246ms step_avg:57.67ms
step:855/2330 train_time:49303ms step_avg:57.66ms
step:856/2330 train_time:49362ms step_avg:57.67ms
step:857/2330 train_time:49418ms step_avg:57.66ms
step:858/2330 train_time:49479ms step_avg:57.67ms
step:859/2330 train_time:49536ms step_avg:57.67ms
step:860/2330 train_time:49597ms step_avg:57.67ms
step:861/2330 train_time:49654ms step_avg:57.67ms
step:862/2330 train_time:49714ms step_avg:57.67ms
step:863/2330 train_time:49771ms step_avg:57.67ms
step:864/2330 train_time:49831ms step_avg:57.67ms
step:865/2330 train_time:49888ms step_avg:57.67ms
step:866/2330 train_time:49948ms step_avg:57.68ms
step:867/2330 train_time:50005ms step_avg:57.68ms
step:868/2330 train_time:50065ms step_avg:57.68ms
step:869/2330 train_time:50122ms step_avg:57.68ms
step:870/2330 train_time:50182ms step_avg:57.68ms
step:871/2330 train_time:50238ms step_avg:57.68ms
step:872/2330 train_time:50298ms step_avg:57.68ms
step:873/2330 train_time:50355ms step_avg:57.68ms
step:874/2330 train_time:50415ms step_avg:57.68ms
step:875/2330 train_time:50472ms step_avg:57.68ms
step:876/2330 train_time:50532ms step_avg:57.69ms
step:877/2330 train_time:50589ms step_avg:57.68ms
step:878/2330 train_time:50650ms step_avg:57.69ms
step:879/2330 train_time:50707ms step_avg:57.69ms
step:880/2330 train_time:50767ms step_avg:57.69ms
step:881/2330 train_time:50824ms step_avg:57.69ms
step:882/2330 train_time:50884ms step_avg:57.69ms
step:883/2330 train_time:50940ms step_avg:57.69ms
step:884/2330 train_time:51000ms step_avg:57.69ms
step:885/2330 train_time:51057ms step_avg:57.69ms
step:886/2330 train_time:51117ms step_avg:57.69ms
step:887/2330 train_time:51174ms step_avg:57.69ms
step:888/2330 train_time:51234ms step_avg:57.70ms
step:889/2330 train_time:51291ms step_avg:57.70ms
step:890/2330 train_time:51352ms step_avg:57.70ms
step:891/2330 train_time:51409ms step_avg:57.70ms
step:892/2330 train_time:51470ms step_avg:57.70ms
step:893/2330 train_time:51528ms step_avg:57.70ms
step:894/2330 train_time:51587ms step_avg:57.70ms
step:895/2330 train_time:51645ms step_avg:57.70ms
step:896/2330 train_time:51704ms step_avg:57.71ms
step:897/2330 train_time:51761ms step_avg:57.70ms
step:898/2330 train_time:51820ms step_avg:57.71ms
step:899/2330 train_time:51877ms step_avg:57.71ms
step:900/2330 train_time:51938ms step_avg:57.71ms
step:901/2330 train_time:51994ms step_avg:57.71ms
step:902/2330 train_time:52055ms step_avg:57.71ms
step:903/2330 train_time:52111ms step_avg:57.71ms
step:904/2330 train_time:52172ms step_avg:57.71ms
step:905/2330 train_time:52229ms step_avg:57.71ms
step:906/2330 train_time:52289ms step_avg:57.71ms
step:907/2330 train_time:52347ms step_avg:57.71ms
step:908/2330 train_time:52407ms step_avg:57.72ms
step:909/2330 train_time:52463ms step_avg:57.71ms
step:910/2330 train_time:52522ms step_avg:57.72ms
step:911/2330 train_time:52578ms step_avg:57.72ms
step:912/2330 train_time:52639ms step_avg:57.72ms
step:913/2330 train_time:52695ms step_avg:57.72ms
step:914/2330 train_time:52756ms step_avg:57.72ms
step:915/2330 train_time:52813ms step_avg:57.72ms
step:916/2330 train_time:52873ms step_avg:57.72ms
step:917/2330 train_time:52930ms step_avg:57.72ms
step:918/2330 train_time:52991ms step_avg:57.72ms
step:919/2330 train_time:53048ms step_avg:57.72ms
step:920/2330 train_time:53108ms step_avg:57.73ms
step:921/2330 train_time:53164ms step_avg:57.72ms
step:922/2330 train_time:53224ms step_avg:57.73ms
step:923/2330 train_time:53280ms step_avg:57.72ms
step:924/2330 train_time:53340ms step_avg:57.73ms
step:925/2330 train_time:53397ms step_avg:57.73ms
step:926/2330 train_time:53457ms step_avg:57.73ms
step:927/2330 train_time:53514ms step_avg:57.73ms
step:928/2330 train_time:53575ms step_avg:57.73ms
step:929/2330 train_time:53631ms step_avg:57.73ms
step:930/2330 train_time:53692ms step_avg:57.73ms
step:931/2330 train_time:53749ms step_avg:57.73ms
step:932/2330 train_time:53809ms step_avg:57.74ms
step:933/2330 train_time:53866ms step_avg:57.73ms
step:934/2330 train_time:53926ms step_avg:57.74ms
step:935/2330 train_time:53983ms step_avg:57.74ms
step:936/2330 train_time:54043ms step_avg:57.74ms
step:937/2330 train_time:54099ms step_avg:57.74ms
step:938/2330 train_time:54159ms step_avg:57.74ms
step:939/2330 train_time:54216ms step_avg:57.74ms
step:940/2330 train_time:54277ms step_avg:57.74ms
step:941/2330 train_time:54333ms step_avg:57.74ms
step:942/2330 train_time:54394ms step_avg:57.74ms
step:943/2330 train_time:54451ms step_avg:57.74ms
step:944/2330 train_time:54512ms step_avg:57.75ms
step:945/2330 train_time:54569ms step_avg:57.75ms
step:946/2330 train_time:54630ms step_avg:57.75ms
step:947/2330 train_time:54687ms step_avg:57.75ms
step:948/2330 train_time:54747ms step_avg:57.75ms
step:949/2330 train_time:54804ms step_avg:57.75ms
step:950/2330 train_time:54863ms step_avg:57.75ms
step:951/2330 train_time:54920ms step_avg:57.75ms
step:952/2330 train_time:54980ms step_avg:57.75ms
step:953/2330 train_time:55037ms step_avg:57.75ms
step:954/2330 train_time:55096ms step_avg:57.75ms
step:955/2330 train_time:55153ms step_avg:57.75ms
step:956/2330 train_time:55214ms step_avg:57.76ms
step:957/2330 train_time:55271ms step_avg:57.75ms
step:958/2330 train_time:55331ms step_avg:57.76ms
step:959/2330 train_time:55389ms step_avg:57.76ms
step:960/2330 train_time:55449ms step_avg:57.76ms
step:961/2330 train_time:55506ms step_avg:57.76ms
step:962/2330 train_time:55565ms step_avg:57.76ms
step:963/2330 train_time:55622ms step_avg:57.76ms
step:964/2330 train_time:55681ms step_avg:57.76ms
step:965/2330 train_time:55738ms step_avg:57.76ms
step:966/2330 train_time:55797ms step_avg:57.76ms
step:967/2330 train_time:55854ms step_avg:57.76ms
step:968/2330 train_time:55915ms step_avg:57.76ms
step:969/2330 train_time:55972ms step_avg:57.76ms
step:970/2330 train_time:56032ms step_avg:57.77ms
step:971/2330 train_time:56089ms step_avg:57.76ms
step:972/2330 train_time:56150ms step_avg:57.77ms
step:973/2330 train_time:56207ms step_avg:57.77ms
step:974/2330 train_time:56267ms step_avg:57.77ms
step:975/2330 train_time:56324ms step_avg:57.77ms
step:976/2330 train_time:56383ms step_avg:57.77ms
step:977/2330 train_time:56440ms step_avg:57.77ms
step:978/2330 train_time:56500ms step_avg:57.77ms
step:979/2330 train_time:56557ms step_avg:57.77ms
step:980/2330 train_time:56618ms step_avg:57.77ms
step:981/2330 train_time:56674ms step_avg:57.77ms
step:982/2330 train_time:56735ms step_avg:57.77ms
step:983/2330 train_time:56791ms step_avg:57.77ms
step:984/2330 train_time:56851ms step_avg:57.78ms
step:985/2330 train_time:56908ms step_avg:57.77ms
step:986/2330 train_time:56968ms step_avg:57.78ms
step:987/2330 train_time:57025ms step_avg:57.78ms
step:988/2330 train_time:57085ms step_avg:57.78ms
step:989/2330 train_time:57142ms step_avg:57.78ms
step:990/2330 train_time:57201ms step_avg:57.78ms
step:991/2330 train_time:57258ms step_avg:57.78ms
step:992/2330 train_time:57318ms step_avg:57.78ms
step:993/2330 train_time:57374ms step_avg:57.78ms
step:994/2330 train_time:57435ms step_avg:57.78ms
step:995/2330 train_time:57492ms step_avg:57.78ms
step:996/2330 train_time:57553ms step_avg:57.78ms
step:997/2330 train_time:57610ms step_avg:57.78ms
step:998/2330 train_time:57670ms step_avg:57.79ms
step:999/2330 train_time:57728ms step_avg:57.79ms
step:1000/2330 train_time:57787ms step_avg:57.79ms
step:1000/2330 val_loss:4.0681 train_time:57867ms step_avg:57.87ms
step:1001/2330 train_time:57886ms step_avg:57.83ms
step:1002/2330 train_time:57905ms step_avg:57.79ms
step:1003/2330 train_time:57960ms step_avg:57.79ms
step:1004/2330 train_time:58024ms step_avg:57.79ms
step:1005/2330 train_time:58080ms step_avg:57.79ms
step:1006/2330 train_time:58141ms step_avg:57.79ms
step:1007/2330 train_time:58197ms step_avg:57.79ms
step:1008/2330 train_time:58256ms step_avg:57.79ms
step:1009/2330 train_time:58312ms step_avg:57.79ms
step:1010/2330 train_time:58371ms step_avg:57.79ms
step:1011/2330 train_time:58428ms step_avg:57.79ms
step:1012/2330 train_time:58487ms step_avg:57.79ms
step:1013/2330 train_time:58543ms step_avg:57.79ms
step:1014/2330 train_time:58602ms step_avg:57.79ms
step:1015/2330 train_time:58658ms step_avg:57.79ms
step:1016/2330 train_time:58718ms step_avg:57.79ms
step:1017/2330 train_time:58776ms step_avg:57.79ms
step:1018/2330 train_time:58840ms step_avg:57.80ms
step:1019/2330 train_time:58897ms step_avg:57.80ms
step:1020/2330 train_time:58959ms step_avg:57.80ms
step:1021/2330 train_time:59016ms step_avg:57.80ms
step:1022/2330 train_time:59077ms step_avg:57.81ms
step:1023/2330 train_time:59133ms step_avg:57.80ms
step:1024/2330 train_time:59195ms step_avg:57.81ms
step:1025/2330 train_time:59251ms step_avg:57.81ms
step:1026/2330 train_time:59310ms step_avg:57.81ms
step:1027/2330 train_time:59367ms step_avg:57.81ms
step:1028/2330 train_time:59426ms step_avg:57.81ms
step:1029/2330 train_time:59483ms step_avg:57.81ms
step:1030/2330 train_time:59541ms step_avg:57.81ms
step:1031/2330 train_time:59598ms step_avg:57.81ms
step:1032/2330 train_time:59657ms step_avg:57.81ms
step:1033/2330 train_time:59714ms step_avg:57.81ms
step:1034/2330 train_time:59775ms step_avg:57.81ms
step:1035/2330 train_time:59833ms step_avg:57.81ms
step:1036/2330 train_time:59893ms step_avg:57.81ms
step:1037/2330 train_time:59951ms step_avg:57.81ms
step:1038/2330 train_time:60013ms step_avg:57.82ms
step:1039/2330 train_time:60070ms step_avg:57.82ms
step:1040/2330 train_time:60130ms step_avg:57.82ms
step:1041/2330 train_time:60187ms step_avg:57.82ms
step:1042/2330 train_time:60248ms step_avg:57.82ms
step:1043/2330 train_time:60304ms step_avg:57.82ms
step:1044/2330 train_time:60364ms step_avg:57.82ms
step:1045/2330 train_time:60420ms step_avg:57.82ms
step:1046/2330 train_time:60479ms step_avg:57.82ms
step:1047/2330 train_time:60535ms step_avg:57.82ms
step:1048/2330 train_time:60595ms step_avg:57.82ms
step:1049/2330 train_time:60652ms step_avg:57.82ms
step:1050/2330 train_time:60712ms step_avg:57.82ms
step:1051/2330 train_time:60769ms step_avg:57.82ms
step:1052/2330 train_time:60830ms step_avg:57.82ms
step:1053/2330 train_time:60888ms step_avg:57.82ms
step:1054/2330 train_time:60949ms step_avg:57.83ms
step:1055/2330 train_time:61006ms step_avg:57.83ms
step:1056/2330 train_time:61066ms step_avg:57.83ms
step:1057/2330 train_time:61123ms step_avg:57.83ms
step:1058/2330 train_time:61184ms step_avg:57.83ms
step:1059/2330 train_time:61240ms step_avg:57.83ms
step:1060/2330 train_time:61301ms step_avg:57.83ms
step:1061/2330 train_time:61357ms step_avg:57.83ms
step:1062/2330 train_time:61417ms step_avg:57.83ms
step:1063/2330 train_time:61474ms step_avg:57.83ms
step:1064/2330 train_time:61532ms step_avg:57.83ms
step:1065/2330 train_time:61589ms step_avg:57.83ms
step:1066/2330 train_time:61650ms step_avg:57.83ms
step:1067/2330 train_time:61706ms step_avg:57.83ms
step:1068/2330 train_time:61767ms step_avg:57.83ms
step:1069/2330 train_time:61825ms step_avg:57.83ms
step:1070/2330 train_time:61885ms step_avg:57.84ms
step:1071/2330 train_time:61942ms step_avg:57.84ms
step:1072/2330 train_time:62002ms step_avg:57.84ms
step:1073/2330 train_time:62059ms step_avg:57.84ms
step:1074/2330 train_time:62119ms step_avg:57.84ms
step:1075/2330 train_time:62176ms step_avg:57.84ms
step:1076/2330 train_time:62237ms step_avg:57.84ms
step:1077/2330 train_time:62293ms step_avg:57.84ms
step:1078/2330 train_time:62353ms step_avg:57.84ms
step:1079/2330 train_time:62409ms step_avg:57.84ms
step:1080/2330 train_time:62471ms step_avg:57.84ms
step:1081/2330 train_time:62527ms step_avg:57.84ms
step:1082/2330 train_time:62587ms step_avg:57.84ms
step:1083/2330 train_time:62644ms step_avg:57.84ms
step:1084/2330 train_time:62704ms step_avg:57.84ms
step:1085/2330 train_time:62761ms step_avg:57.84ms
step:1086/2330 train_time:62822ms step_avg:57.85ms
step:1087/2330 train_time:62879ms step_avg:57.85ms
step:1088/2330 train_time:62939ms step_avg:57.85ms
step:1089/2330 train_time:62995ms step_avg:57.85ms
step:1090/2330 train_time:63056ms step_avg:57.85ms
step:1091/2330 train_time:63112ms step_avg:57.85ms
step:1092/2330 train_time:63174ms step_avg:57.85ms
step:1093/2330 train_time:63230ms step_avg:57.85ms
step:1094/2330 train_time:63291ms step_avg:57.85ms
step:1095/2330 train_time:63347ms step_avg:57.85ms
step:1096/2330 train_time:63408ms step_avg:57.85ms
step:1097/2330 train_time:63465ms step_avg:57.85ms
step:1098/2330 train_time:63524ms step_avg:57.85ms
step:1099/2330 train_time:63582ms step_avg:57.85ms
step:1100/2330 train_time:63642ms step_avg:57.86ms
step:1101/2330 train_time:63698ms step_avg:57.86ms
step:1102/2330 train_time:63759ms step_avg:57.86ms
step:1103/2330 train_time:63817ms step_avg:57.86ms
step:1104/2330 train_time:63876ms step_avg:57.86ms
step:1105/2330 train_time:63933ms step_avg:57.86ms
step:1106/2330 train_time:63993ms step_avg:57.86ms
step:1107/2330 train_time:64050ms step_avg:57.86ms
step:1108/2330 train_time:64110ms step_avg:57.86ms
step:1109/2330 train_time:64167ms step_avg:57.86ms
step:1110/2330 train_time:64227ms step_avg:57.86ms
step:1111/2330 train_time:64284ms step_avg:57.86ms
step:1112/2330 train_time:64344ms step_avg:57.86ms
step:1113/2330 train_time:64400ms step_avg:57.86ms
step:1114/2330 train_time:64460ms step_avg:57.86ms
step:1115/2330 train_time:64517ms step_avg:57.86ms
step:1116/2330 train_time:64577ms step_avg:57.87ms
step:1117/2330 train_time:64634ms step_avg:57.86ms
step:1118/2330 train_time:64695ms step_avg:57.87ms
step:1119/2330 train_time:64751ms step_avg:57.87ms
step:1120/2330 train_time:64812ms step_avg:57.87ms
step:1121/2330 train_time:64869ms step_avg:57.87ms
step:1122/2330 train_time:64929ms step_avg:57.87ms
step:1123/2330 train_time:64986ms step_avg:57.87ms
step:1124/2330 train_time:65046ms step_avg:57.87ms
step:1125/2330 train_time:65102ms step_avg:57.87ms
step:1126/2330 train_time:65163ms step_avg:57.87ms
step:1127/2330 train_time:65219ms step_avg:57.87ms
step:1128/2330 train_time:65280ms step_avg:57.87ms
step:1129/2330 train_time:65338ms step_avg:57.87ms
step:1130/2330 train_time:65397ms step_avg:57.87ms
step:1131/2330 train_time:65454ms step_avg:57.87ms
step:1132/2330 train_time:65515ms step_avg:57.88ms
step:1133/2330 train_time:65572ms step_avg:57.87ms
step:1134/2330 train_time:65633ms step_avg:57.88ms
step:1135/2330 train_time:65690ms step_avg:57.88ms
step:1136/2330 train_time:65749ms step_avg:57.88ms
step:1137/2330 train_time:65806ms step_avg:57.88ms
step:1138/2330 train_time:65866ms step_avg:57.88ms
step:1139/2330 train_time:65924ms step_avg:57.88ms
step:1140/2330 train_time:65984ms step_avg:57.88ms
step:1141/2330 train_time:66041ms step_avg:57.88ms
step:1142/2330 train_time:66101ms step_avg:57.88ms
step:1143/2330 train_time:66157ms step_avg:57.88ms
step:1144/2330 train_time:66217ms step_avg:57.88ms
step:1145/2330 train_time:66274ms step_avg:57.88ms
step:1146/2330 train_time:66334ms step_avg:57.88ms
step:1147/2330 train_time:66390ms step_avg:57.88ms
step:1148/2330 train_time:67041ms step_avg:58.40ms
step:1149/2330 train_time:67096ms step_avg:58.40ms
step:1150/2330 train_time:67155ms step_avg:58.40ms
step:1151/2330 train_time:67211ms step_avg:58.39ms
step:1152/2330 train_time:67271ms step_avg:58.40ms
step:1153/2330 train_time:67327ms step_avg:58.39ms
step:1154/2330 train_time:67386ms step_avg:58.39ms
step:1155/2330 train_time:67442ms step_avg:58.39ms
step:1156/2330 train_time:67502ms step_avg:58.39ms
step:1157/2330 train_time:67558ms step_avg:58.39ms
step:1158/2330 train_time:67616ms step_avg:58.39ms
step:1159/2330 train_time:67672ms step_avg:58.39ms
step:1160/2330 train_time:67732ms step_avg:58.39ms
step:1161/2330 train_time:67788ms step_avg:58.39ms
step:1162/2330 train_time:67847ms step_avg:58.39ms
step:1163/2330 train_time:67908ms step_avg:58.39ms
step:1164/2330 train_time:67974ms step_avg:58.40ms
step:1165/2330 train_time:68031ms step_avg:58.40ms
step:1166/2330 train_time:68092ms step_avg:58.40ms
step:1167/2330 train_time:68148ms step_avg:58.40ms
step:1168/2330 train_time:68209ms step_avg:58.40ms
step:1169/2330 train_time:68266ms step_avg:58.40ms
step:1170/2330 train_time:68325ms step_avg:58.40ms
step:1171/2330 train_time:68381ms step_avg:58.40ms
step:1172/2330 train_time:68441ms step_avg:58.40ms
step:1173/2330 train_time:68496ms step_avg:58.39ms
step:1174/2330 train_time:68556ms step_avg:58.40ms
step:1175/2330 train_time:68613ms step_avg:58.39ms
step:1176/2330 train_time:68673ms step_avg:58.40ms
step:1177/2330 train_time:68729ms step_avg:58.39ms
step:1178/2330 train_time:68788ms step_avg:58.39ms
step:1179/2330 train_time:68846ms step_avg:58.39ms
step:1180/2330 train_time:68908ms step_avg:58.40ms
step:1181/2330 train_time:68965ms step_avg:58.40ms
step:1182/2330 train_time:69026ms step_avg:58.40ms
step:1183/2330 train_time:69085ms step_avg:58.40ms
step:1184/2330 train_time:69145ms step_avg:58.40ms
step:1185/2330 train_time:69202ms step_avg:58.40ms
step:1186/2330 train_time:69261ms step_avg:58.40ms
step:1187/2330 train_time:69318ms step_avg:58.40ms
step:1188/2330 train_time:69377ms step_avg:58.40ms
step:1189/2330 train_time:69433ms step_avg:58.40ms
step:1190/2330 train_time:69492ms step_avg:58.40ms
step:1191/2330 train_time:69549ms step_avg:58.40ms
step:1192/2330 train_time:69608ms step_avg:58.40ms
step:1193/2330 train_time:69665ms step_avg:58.39ms
step:1194/2330 train_time:69724ms step_avg:58.40ms
step:1195/2330 train_time:69782ms step_avg:58.39ms
step:1196/2330 train_time:69841ms step_avg:58.40ms
step:1197/2330 train_time:69898ms step_avg:58.39ms
step:1198/2330 train_time:69959ms step_avg:58.40ms
step:1199/2330 train_time:70016ms step_avg:58.40ms
step:1200/2330 train_time:70078ms step_avg:58.40ms
step:1201/2330 train_time:70134ms step_avg:58.40ms
step:1202/2330 train_time:70196ms step_avg:58.40ms
step:1203/2330 train_time:70252ms step_avg:58.40ms
step:1204/2330 train_time:70315ms step_avg:58.40ms
step:1205/2330 train_time:70372ms step_avg:58.40ms
step:1206/2330 train_time:70431ms step_avg:58.40ms
step:1207/2330 train_time:70487ms step_avg:58.40ms
step:1208/2330 train_time:70547ms step_avg:58.40ms
step:1209/2330 train_time:70603ms step_avg:58.40ms
step:1210/2330 train_time:70662ms step_avg:58.40ms
step:1211/2330 train_time:70719ms step_avg:58.40ms
step:1212/2330 train_time:70779ms step_avg:58.40ms
step:1213/2330 train_time:70836ms step_avg:58.40ms
step:1214/2330 train_time:70895ms step_avg:58.40ms
step:1215/2330 train_time:70953ms step_avg:58.40ms
step:1216/2330 train_time:71014ms step_avg:58.40ms
step:1217/2330 train_time:71072ms step_avg:58.40ms
step:1218/2330 train_time:71132ms step_avg:58.40ms
step:1219/2330 train_time:71189ms step_avg:58.40ms
step:1220/2330 train_time:71250ms step_avg:58.40ms
step:1221/2330 train_time:71307ms step_avg:58.40ms
step:1222/2330 train_time:71367ms step_avg:58.40ms
step:1223/2330 train_time:71425ms step_avg:58.40ms
step:1224/2330 train_time:71484ms step_avg:58.40ms
step:1225/2330 train_time:71540ms step_avg:58.40ms
step:1226/2330 train_time:71600ms step_avg:58.40ms
step:1227/2330 train_time:71656ms step_avg:58.40ms
step:1228/2330 train_time:71716ms step_avg:58.40ms
step:1229/2330 train_time:71773ms step_avg:58.40ms
step:1230/2330 train_time:71833ms step_avg:58.40ms
step:1231/2330 train_time:71890ms step_avg:58.40ms
step:1232/2330 train_time:71951ms step_avg:58.40ms
step:1233/2330 train_time:72007ms step_avg:58.40ms
step:1234/2330 train_time:72068ms step_avg:58.40ms
step:1235/2330 train_time:72126ms step_avg:58.40ms
step:1236/2330 train_time:72186ms step_avg:58.40ms
step:1237/2330 train_time:72243ms step_avg:58.40ms
step:1238/2330 train_time:72303ms step_avg:58.40ms
step:1239/2330 train_time:72359ms step_avg:58.40ms
step:1240/2330 train_time:72419ms step_avg:58.40ms
step:1241/2330 train_time:72476ms step_avg:58.40ms
step:1242/2330 train_time:72536ms step_avg:58.40ms
step:1243/2330 train_time:72592ms step_avg:58.40ms
step:1244/2330 train_time:72652ms step_avg:58.40ms
step:1245/2330 train_time:72708ms step_avg:58.40ms
step:1246/2330 train_time:72769ms step_avg:58.40ms
step:1247/2330 train_time:72827ms step_avg:58.40ms
step:1248/2330 train_time:72886ms step_avg:58.40ms
step:1249/2330 train_time:72943ms step_avg:58.40ms
step:1250/2330 train_time:73003ms step_avg:58.40ms
step:1250/2330 val_loss:3.9903 train_time:73084ms step_avg:58.47ms
step:1251/2330 train_time:73102ms step_avg:58.43ms
step:1252/2330 train_time:73123ms step_avg:58.40ms
step:1253/2330 train_time:73181ms step_avg:58.40ms
step:1254/2330 train_time:73245ms step_avg:58.41ms
step:1255/2330 train_time:73301ms step_avg:58.41ms
step:1256/2330 train_time:73363ms step_avg:58.41ms
step:1257/2330 train_time:73419ms step_avg:58.41ms
step:1258/2330 train_time:73479ms step_avg:58.41ms
step:1259/2330 train_time:73534ms step_avg:58.41ms
step:1260/2330 train_time:73595ms step_avg:58.41ms
step:1261/2330 train_time:73651ms step_avg:58.41ms
step:1262/2330 train_time:73711ms step_avg:58.41ms
step:1263/2330 train_time:73767ms step_avg:58.41ms
step:1264/2330 train_time:73826ms step_avg:58.41ms
step:1265/2330 train_time:73882ms step_avg:58.40ms
step:1266/2330 train_time:73941ms step_avg:58.41ms
step:1267/2330 train_time:73998ms step_avg:58.40ms
step:1268/2330 train_time:74058ms step_avg:58.41ms
step:1269/2330 train_time:74116ms step_avg:58.40ms
step:1270/2330 train_time:74179ms step_avg:58.41ms
step:1271/2330 train_time:74237ms step_avg:58.41ms
step:1272/2330 train_time:74299ms step_avg:58.41ms
step:1273/2330 train_time:74355ms step_avg:58.41ms
step:1274/2330 train_time:74416ms step_avg:58.41ms
step:1275/2330 train_time:74472ms step_avg:58.41ms
step:1276/2330 train_time:74533ms step_avg:58.41ms
step:1277/2330 train_time:74588ms step_avg:58.41ms
step:1278/2330 train_time:74649ms step_avg:58.41ms
step:1279/2330 train_time:74705ms step_avg:58.41ms
step:1280/2330 train_time:74765ms step_avg:58.41ms
step:1281/2330 train_time:74820ms step_avg:58.41ms
step:1282/2330 train_time:74881ms step_avg:58.41ms
step:1283/2330 train_time:74937ms step_avg:58.41ms
step:1284/2330 train_time:74998ms step_avg:58.41ms
step:1285/2330 train_time:75055ms step_avg:58.41ms
step:1286/2330 train_time:75117ms step_avg:58.41ms
step:1287/2330 train_time:75175ms step_avg:58.41ms
step:1288/2330 train_time:75237ms step_avg:58.41ms
step:1289/2330 train_time:75294ms step_avg:58.41ms
step:1290/2330 train_time:75356ms step_avg:58.42ms
step:1291/2330 train_time:75413ms step_avg:58.41ms
step:1292/2330 train_time:75473ms step_avg:58.42ms
step:1293/2330 train_time:75530ms step_avg:58.41ms
step:1294/2330 train_time:75590ms step_avg:58.42ms
step:1295/2330 train_time:75646ms step_avg:58.41ms
step:1296/2330 train_time:75706ms step_avg:58.42ms
step:1297/2330 train_time:75763ms step_avg:58.41ms
step:1298/2330 train_time:75822ms step_avg:58.41ms
step:1299/2330 train_time:75879ms step_avg:58.41ms
step:1300/2330 train_time:75938ms step_avg:58.41ms
step:1301/2330 train_time:75995ms step_avg:58.41ms
step:1302/2330 train_time:76055ms step_avg:58.41ms
step:1303/2330 train_time:76112ms step_avg:58.41ms
step:1304/2330 train_time:76173ms step_avg:58.41ms
step:1305/2330 train_time:76230ms step_avg:58.41ms
step:1306/2330 train_time:76291ms step_avg:58.42ms
step:1307/2330 train_time:76348ms step_avg:58.42ms
step:1308/2330 train_time:76409ms step_avg:58.42ms
step:1309/2330 train_time:76465ms step_avg:58.41ms
step:1310/2330 train_time:76525ms step_avg:58.42ms
step:1311/2330 train_time:76582ms step_avg:58.41ms
step:1312/2330 train_time:76642ms step_avg:58.42ms
step:1313/2330 train_time:76698ms step_avg:58.41ms
step:1314/2330 train_time:76758ms step_avg:58.42ms
step:1315/2330 train_time:76814ms step_avg:58.41ms
step:1316/2330 train_time:76875ms step_avg:58.42ms
step:1317/2330 train_time:76931ms step_avg:58.41ms
step:1318/2330 train_time:76993ms step_avg:58.42ms
step:1319/2330 train_time:77050ms step_avg:58.42ms
step:1320/2330 train_time:77110ms step_avg:58.42ms
step:1321/2330 train_time:77167ms step_avg:58.42ms
step:1322/2330 train_time:77228ms step_avg:58.42ms
step:1323/2330 train_time:77286ms step_avg:58.42ms
step:1324/2330 train_time:77345ms step_avg:58.42ms
step:1325/2330 train_time:77402ms step_avg:58.42ms
step:1326/2330 train_time:77463ms step_avg:58.42ms
step:1327/2330 train_time:77519ms step_avg:58.42ms
step:1328/2330 train_time:77580ms step_avg:58.42ms
step:1329/2330 train_time:77637ms step_avg:58.42ms
step:1330/2330 train_time:77697ms step_avg:58.42ms
step:1331/2330 train_time:77753ms step_avg:58.42ms
step:1332/2330 train_time:77813ms step_avg:58.42ms
step:1333/2330 train_time:77870ms step_avg:58.42ms
step:1334/2330 train_time:77930ms step_avg:58.42ms
step:1335/2330 train_time:77987ms step_avg:58.42ms
step:1336/2330 train_time:78047ms step_avg:58.42ms
step:1337/2330 train_time:78103ms step_avg:58.42ms
step:1338/2330 train_time:78163ms step_avg:58.42ms
step:1339/2330 train_time:78220ms step_avg:58.42ms
step:1340/2330 train_time:78280ms step_avg:58.42ms
step:1341/2330 train_time:78338ms step_avg:58.42ms
step:1342/2330 train_time:78398ms step_avg:58.42ms
step:1343/2330 train_time:78455ms step_avg:58.42ms
step:1344/2330 train_time:78515ms step_avg:58.42ms
step:1345/2330 train_time:78572ms step_avg:58.42ms
step:1346/2330 train_time:78632ms step_avg:58.42ms
step:1347/2330 train_time:78689ms step_avg:58.42ms
step:1348/2330 train_time:78749ms step_avg:58.42ms
step:1349/2330 train_time:78805ms step_avg:58.42ms
step:1350/2330 train_time:78865ms step_avg:58.42ms
step:1351/2330 train_time:78922ms step_avg:58.42ms
step:1352/2330 train_time:78981ms step_avg:58.42ms
step:1353/2330 train_time:79038ms step_avg:58.42ms
step:1354/2330 train_time:79098ms step_avg:58.42ms
step:1355/2330 train_time:79155ms step_avg:58.42ms
step:1356/2330 train_time:79216ms step_avg:58.42ms
step:1357/2330 train_time:79273ms step_avg:58.42ms
step:1358/2330 train_time:79334ms step_avg:58.42ms
step:1359/2330 train_time:79392ms step_avg:58.42ms
step:1360/2330 train_time:79452ms step_avg:58.42ms
step:1361/2330 train_time:79509ms step_avg:58.42ms
step:1362/2330 train_time:79569ms step_avg:58.42ms
step:1363/2330 train_time:79626ms step_avg:58.42ms
step:1364/2330 train_time:79686ms step_avg:58.42ms
step:1365/2330 train_time:79743ms step_avg:58.42ms
step:1366/2330 train_time:79801ms step_avg:58.42ms
step:1367/2330 train_time:79859ms step_avg:58.42ms
step:1368/2330 train_time:79918ms step_avg:58.42ms
step:1369/2330 train_time:79975ms step_avg:58.42ms
step:1370/2330 train_time:80035ms step_avg:58.42ms
step:1371/2330 train_time:80092ms step_avg:58.42ms
step:1372/2330 train_time:80153ms step_avg:58.42ms
step:1373/2330 train_time:80210ms step_avg:58.42ms
step:1374/2330 train_time:80270ms step_avg:58.42ms
step:1375/2330 train_time:80328ms step_avg:58.42ms
step:1376/2330 train_time:80388ms step_avg:58.42ms
step:1377/2330 train_time:80445ms step_avg:58.42ms
step:1378/2330 train_time:80505ms step_avg:58.42ms
step:1379/2330 train_time:80562ms step_avg:58.42ms
step:1380/2330 train_time:80621ms step_avg:58.42ms
step:1381/2330 train_time:80678ms step_avg:58.42ms
step:1382/2330 train_time:80738ms step_avg:58.42ms
step:1383/2330 train_time:80795ms step_avg:58.42ms
step:1384/2330 train_time:80854ms step_avg:58.42ms
step:1385/2330 train_time:80911ms step_avg:58.42ms
step:1386/2330 train_time:80971ms step_avg:58.42ms
step:1387/2330 train_time:81028ms step_avg:58.42ms
step:1388/2330 train_time:81088ms step_avg:58.42ms
step:1389/2330 train_time:81145ms step_avg:58.42ms
step:1390/2330 train_time:81205ms step_avg:58.42ms
step:1391/2330 train_time:81262ms step_avg:58.42ms
step:1392/2330 train_time:81323ms step_avg:58.42ms
step:1393/2330 train_time:81379ms step_avg:58.42ms
step:1394/2330 train_time:81439ms step_avg:58.42ms
step:1395/2330 train_time:81495ms step_avg:58.42ms
step:1396/2330 train_time:81556ms step_avg:58.42ms
step:1397/2330 train_time:81613ms step_avg:58.42ms
step:1398/2330 train_time:81673ms step_avg:58.42ms
step:1399/2330 train_time:81730ms step_avg:58.42ms
step:1400/2330 train_time:81791ms step_avg:58.42ms
step:1401/2330 train_time:81848ms step_avg:58.42ms
step:1402/2330 train_time:81907ms step_avg:58.42ms
step:1403/2330 train_time:81964ms step_avg:58.42ms
step:1404/2330 train_time:82023ms step_avg:58.42ms
step:1405/2330 train_time:82080ms step_avg:58.42ms
step:1406/2330 train_time:82141ms step_avg:58.42ms
step:1407/2330 train_time:82198ms step_avg:58.42ms
step:1408/2330 train_time:82257ms step_avg:58.42ms
step:1409/2330 train_time:82313ms step_avg:58.42ms
step:1410/2330 train_time:82374ms step_avg:58.42ms
step:1411/2330 train_time:82432ms step_avg:58.42ms
step:1412/2330 train_time:82492ms step_avg:58.42ms
step:1413/2330 train_time:82550ms step_avg:58.42ms
step:1414/2330 train_time:82610ms step_avg:58.42ms
step:1415/2330 train_time:82668ms step_avg:58.42ms
step:1416/2330 train_time:82727ms step_avg:58.42ms
step:1417/2330 train_time:82785ms step_avg:58.42ms
step:1418/2330 train_time:82844ms step_avg:58.42ms
step:1419/2330 train_time:82901ms step_avg:58.42ms
step:1420/2330 train_time:82960ms step_avg:58.42ms
step:1421/2330 train_time:83017ms step_avg:58.42ms
step:1422/2330 train_time:83078ms step_avg:58.42ms
step:1423/2330 train_time:83135ms step_avg:58.42ms
step:1424/2330 train_time:83195ms step_avg:58.42ms
step:1425/2330 train_time:83252ms step_avg:58.42ms
step:1426/2330 train_time:83312ms step_avg:58.42ms
step:1427/2330 train_time:83369ms step_avg:58.42ms
step:1428/2330 train_time:83429ms step_avg:58.42ms
step:1429/2330 train_time:83486ms step_avg:58.42ms
step:1430/2330 train_time:83546ms step_avg:58.42ms
step:1431/2330 train_time:83604ms step_avg:58.42ms
step:1432/2330 train_time:83663ms step_avg:58.42ms
step:1433/2330 train_time:83719ms step_avg:58.42ms
step:1434/2330 train_time:83780ms step_avg:58.42ms
step:1435/2330 train_time:83836ms step_avg:58.42ms
step:1436/2330 train_time:83897ms step_avg:58.42ms
step:1437/2330 train_time:83954ms step_avg:58.42ms
step:1438/2330 train_time:84014ms step_avg:58.42ms
step:1439/2330 train_time:84071ms step_avg:58.42ms
step:1440/2330 train_time:84131ms step_avg:58.42ms
step:1441/2330 train_time:84188ms step_avg:58.42ms
step:1442/2330 train_time:84248ms step_avg:58.42ms
step:1443/2330 train_time:84305ms step_avg:58.42ms
step:1444/2330 train_time:84364ms step_avg:58.42ms
step:1445/2330 train_time:84421ms step_avg:58.42ms
step:1446/2330 train_time:84481ms step_avg:58.42ms
step:1447/2330 train_time:84537ms step_avg:58.42ms
step:1448/2330 train_time:84598ms step_avg:58.42ms
step:1449/2330 train_time:84655ms step_avg:58.42ms
step:1450/2330 train_time:84716ms step_avg:58.42ms
step:1451/2330 train_time:84772ms step_avg:58.42ms
step:1452/2330 train_time:84833ms step_avg:58.43ms
step:1453/2330 train_time:84890ms step_avg:58.42ms
step:1454/2330 train_time:84950ms step_avg:58.42ms
step:1455/2330 train_time:85008ms step_avg:58.42ms
step:1456/2330 train_time:85067ms step_avg:58.43ms
step:1457/2330 train_time:85124ms step_avg:58.42ms
step:1458/2330 train_time:85183ms step_avg:58.42ms
step:1459/2330 train_time:85239ms step_avg:58.42ms
step:1460/2330 train_time:85300ms step_avg:58.42ms
step:1461/2330 train_time:85357ms step_avg:58.42ms
step:1462/2330 train_time:85418ms step_avg:58.43ms
step:1463/2330 train_time:85474ms step_avg:58.42ms
step:1464/2330 train_time:85536ms step_avg:58.43ms
step:1465/2330 train_time:85592ms step_avg:58.42ms
step:1466/2330 train_time:85653ms step_avg:58.43ms
step:1467/2330 train_time:85710ms step_avg:58.43ms
step:1468/2330 train_time:85770ms step_avg:58.43ms
step:1469/2330 train_time:85827ms step_avg:58.43ms
step:1470/2330 train_time:85887ms step_avg:58.43ms
step:1471/2330 train_time:85943ms step_avg:58.43ms
step:1472/2330 train_time:86003ms step_avg:58.43ms
step:1473/2330 train_time:86061ms step_avg:58.43ms
step:1474/2330 train_time:86120ms step_avg:58.43ms
step:1475/2330 train_time:86177ms step_avg:58.43ms
step:1476/2330 train_time:86237ms step_avg:58.43ms
step:1477/2330 train_time:86294ms step_avg:58.43ms
step:1478/2330 train_time:86354ms step_avg:58.43ms
step:1479/2330 train_time:86412ms step_avg:58.43ms
step:1480/2330 train_time:86471ms step_avg:58.43ms
step:1481/2330 train_time:86528ms step_avg:58.43ms
step:1482/2330 train_time:86588ms step_avg:58.43ms
step:1483/2330 train_time:86646ms step_avg:58.43ms
step:1484/2330 train_time:86706ms step_avg:58.43ms
step:1485/2330 train_time:86763ms step_avg:58.43ms
step:1486/2330 train_time:86822ms step_avg:58.43ms
step:1487/2330 train_time:86879ms step_avg:58.43ms
step:1488/2330 train_time:86939ms step_avg:58.43ms
step:1489/2330 train_time:86996ms step_avg:58.43ms
step:1490/2330 train_time:87056ms step_avg:58.43ms
step:1491/2330 train_time:87112ms step_avg:58.43ms
step:1492/2330 train_time:87173ms step_avg:58.43ms
step:1493/2330 train_time:87230ms step_avg:58.43ms
step:1494/2330 train_time:87291ms step_avg:58.43ms
step:1495/2330 train_time:87349ms step_avg:58.43ms
step:1496/2330 train_time:87409ms step_avg:58.43ms
step:1497/2330 train_time:87467ms step_avg:58.43ms
step:1498/2330 train_time:87526ms step_avg:58.43ms
step:1499/2330 train_time:87584ms step_avg:58.43ms
step:1500/2330 train_time:87643ms step_avg:58.43ms
step:1500/2330 val_loss:3.9053 train_time:87723ms step_avg:58.48ms
step:1501/2330 train_time:87741ms step_avg:58.46ms
step:1502/2330 train_time:87762ms step_avg:58.43ms
step:1503/2330 train_time:87821ms step_avg:58.43ms
step:1504/2330 train_time:87887ms step_avg:58.44ms
step:1505/2330 train_time:87945ms step_avg:58.44ms
step:1506/2330 train_time:88005ms step_avg:58.44ms
step:1507/2330 train_time:88062ms step_avg:58.44ms
step:1508/2330 train_time:88122ms step_avg:58.44ms
step:1509/2330 train_time:88179ms step_avg:58.44ms
step:1510/2330 train_time:88239ms step_avg:58.44ms
step:1511/2330 train_time:88295ms step_avg:58.43ms
step:1512/2330 train_time:88354ms step_avg:58.44ms
step:1513/2330 train_time:88410ms step_avg:58.43ms
step:1514/2330 train_time:88469ms step_avg:58.43ms
step:1515/2330 train_time:88525ms step_avg:58.43ms
step:1516/2330 train_time:88586ms step_avg:58.43ms
step:1517/2330 train_time:88642ms step_avg:58.43ms
step:1518/2330 train_time:88702ms step_avg:58.43ms
step:1519/2330 train_time:88761ms step_avg:58.43ms
step:1520/2330 train_time:88823ms step_avg:58.44ms
step:1521/2330 train_time:88882ms step_avg:58.44ms
step:1522/2330 train_time:88942ms step_avg:58.44ms
step:1523/2330 train_time:89000ms step_avg:58.44ms
step:1524/2330 train_time:89060ms step_avg:58.44ms
step:1525/2330 train_time:89119ms step_avg:58.44ms
step:1526/2330 train_time:89179ms step_avg:58.44ms
step:1527/2330 train_time:89236ms step_avg:58.44ms
step:1528/2330 train_time:89296ms step_avg:58.44ms
step:1529/2330 train_time:89354ms step_avg:58.44ms
step:1530/2330 train_time:89412ms step_avg:58.44ms
step:1531/2330 train_time:89469ms step_avg:58.44ms
step:1532/2330 train_time:89529ms step_avg:58.44ms
step:1533/2330 train_time:89586ms step_avg:58.44ms
step:1534/2330 train_time:89646ms step_avg:58.44ms
step:1535/2330 train_time:89703ms step_avg:58.44ms
step:1536/2330 train_time:89765ms step_avg:58.44ms
step:1537/2330 train_time:89823ms step_avg:58.44ms
step:1538/2330 train_time:89886ms step_avg:58.44ms
step:1539/2330 train_time:89944ms step_avg:58.44ms
step:1540/2330 train_time:90005ms step_avg:58.45ms
step:1541/2330 train_time:90064ms step_avg:58.44ms
step:1542/2330 train_time:90125ms step_avg:58.45ms
step:1543/2330 train_time:90183ms step_avg:58.45ms
step:1544/2330 train_time:90243ms step_avg:58.45ms
step:1545/2330 train_time:90301ms step_avg:58.45ms
step:1546/2330 train_time:90362ms step_avg:58.45ms
step:1547/2330 train_time:90419ms step_avg:58.45ms
step:1548/2330 train_time:90479ms step_avg:58.45ms
step:1549/2330 train_time:90536ms step_avg:58.45ms
step:1550/2330 train_time:90596ms step_avg:58.45ms
step:1551/2330 train_time:90653ms step_avg:58.45ms
step:1552/2330 train_time:90714ms step_avg:58.45ms
step:1553/2330 train_time:90771ms step_avg:58.45ms
step:1554/2330 train_time:90832ms step_avg:58.45ms
step:1555/2330 train_time:90889ms step_avg:58.45ms
step:1556/2330 train_time:90950ms step_avg:58.45ms
step:1557/2330 train_time:91007ms step_avg:58.45ms
step:1558/2330 train_time:91070ms step_avg:58.45ms
step:1559/2330 train_time:91127ms step_avg:58.45ms
step:1560/2330 train_time:91189ms step_avg:58.45ms
step:1561/2330 train_time:91245ms step_avg:58.45ms
step:1562/2330 train_time:91307ms step_avg:58.46ms
step:1563/2330 train_time:91364ms step_avg:58.45ms
step:1564/2330 train_time:91426ms step_avg:58.46ms
step:1565/2330 train_time:91482ms step_avg:58.46ms
step:1566/2330 train_time:91544ms step_avg:58.46ms
step:1567/2330 train_time:91602ms step_avg:58.46ms
step:1568/2330 train_time:91662ms step_avg:58.46ms
step:1569/2330 train_time:91720ms step_avg:58.46ms
step:1570/2330 train_time:91781ms step_avg:58.46ms
step:1571/2330 train_time:91838ms step_avg:58.46ms
step:1572/2330 train_time:91899ms step_avg:58.46ms
step:1573/2330 train_time:91958ms step_avg:58.46ms
step:1574/2330 train_time:92019ms step_avg:58.46ms
step:1575/2330 train_time:92078ms step_avg:58.46ms
step:1576/2330 train_time:92139ms step_avg:58.46ms
step:1577/2330 train_time:92197ms step_avg:58.46ms
step:1578/2330 train_time:92258ms step_avg:58.47ms
step:1579/2330 train_time:92316ms step_avg:58.46ms
step:1580/2330 train_time:92376ms step_avg:58.47ms
step:1581/2330 train_time:92432ms step_avg:58.46ms
step:1582/2330 train_time:92493ms step_avg:58.47ms
step:1583/2330 train_time:92549ms step_avg:58.46ms
step:1584/2330 train_time:92611ms step_avg:58.47ms
step:1585/2330 train_time:92668ms step_avg:58.47ms
step:1586/2330 train_time:92730ms step_avg:58.47ms
step:1587/2330 train_time:92786ms step_avg:58.47ms
step:1588/2330 train_time:92848ms step_avg:58.47ms
step:1589/2330 train_time:92904ms step_avg:58.47ms
step:1590/2330 train_time:92967ms step_avg:58.47ms
step:1591/2330 train_time:93024ms step_avg:58.47ms
step:1592/2330 train_time:93085ms step_avg:58.47ms
step:1593/2330 train_time:93143ms step_avg:58.47ms
step:1594/2330 train_time:93204ms step_avg:58.47ms
step:1595/2330 train_time:93263ms step_avg:58.47ms
step:1596/2330 train_time:93323ms step_avg:58.47ms
step:1597/2330 train_time:93380ms step_avg:58.47ms
step:1598/2330 train_time:93441ms step_avg:58.47ms
step:1599/2330 train_time:93499ms step_avg:58.47ms
step:1600/2330 train_time:93559ms step_avg:58.47ms
step:1601/2330 train_time:93617ms step_avg:58.47ms
step:1602/2330 train_time:93678ms step_avg:58.48ms
step:1603/2330 train_time:93735ms step_avg:58.47ms
step:1604/2330 train_time:93795ms step_avg:58.48ms
step:1605/2330 train_time:93853ms step_avg:58.48ms
step:1606/2330 train_time:93913ms step_avg:58.48ms
step:1607/2330 train_time:93969ms step_avg:58.47ms
step:1608/2330 train_time:94031ms step_avg:58.48ms
step:1609/2330 train_time:94087ms step_avg:58.48ms
step:1610/2330 train_time:94149ms step_avg:58.48ms
step:1611/2330 train_time:94206ms step_avg:58.48ms
step:1612/2330 train_time:94269ms step_avg:58.48ms
step:1613/2330 train_time:94326ms step_avg:58.48ms
step:1614/2330 train_time:94388ms step_avg:58.48ms
step:1615/2330 train_time:94444ms step_avg:58.48ms
step:1616/2330 train_time:94506ms step_avg:58.48ms
step:1617/2330 train_time:94563ms step_avg:58.48ms
step:1618/2330 train_time:94625ms step_avg:58.48ms
step:1619/2330 train_time:94682ms step_avg:58.48ms
step:1620/2330 train_time:94742ms step_avg:58.48ms
step:1621/2330 train_time:94800ms step_avg:58.48ms
step:1622/2330 train_time:94861ms step_avg:58.48ms
step:1623/2330 train_time:94920ms step_avg:58.48ms
step:1624/2330 train_time:94980ms step_avg:58.49ms
step:1625/2330 train_time:95038ms step_avg:58.49ms
step:1626/2330 train_time:95098ms step_avg:58.49ms
step:1627/2330 train_time:95155ms step_avg:58.49ms
step:1628/2330 train_time:95217ms step_avg:58.49ms
step:1629/2330 train_time:95274ms step_avg:58.49ms
step:1630/2330 train_time:95334ms step_avg:58.49ms
step:1631/2330 train_time:95390ms step_avg:58.49ms
step:1632/2330 train_time:95451ms step_avg:58.49ms
step:1633/2330 train_time:95508ms step_avg:58.49ms
step:1634/2330 train_time:95569ms step_avg:58.49ms
step:1635/2330 train_time:95626ms step_avg:58.49ms
step:1636/2330 train_time:95687ms step_avg:58.49ms
step:1637/2330 train_time:95744ms step_avg:58.49ms
step:1638/2330 train_time:95806ms step_avg:58.49ms
step:1639/2330 train_time:95863ms step_avg:58.49ms
step:1640/2330 train_time:95927ms step_avg:58.49ms
step:1641/2330 train_time:95984ms step_avg:58.49ms
step:1642/2330 train_time:96045ms step_avg:58.49ms
step:1643/2330 train_time:96103ms step_avg:58.49ms
step:1644/2330 train_time:96164ms step_avg:58.49ms
step:1645/2330 train_time:96222ms step_avg:58.49ms
step:1646/2330 train_time:96283ms step_avg:58.49ms
step:1647/2330 train_time:96340ms step_avg:58.49ms
step:1648/2330 train_time:96401ms step_avg:58.50ms
step:1649/2330 train_time:96459ms step_avg:58.50ms
step:1650/2330 train_time:96519ms step_avg:58.50ms
step:1651/2330 train_time:96576ms step_avg:58.50ms
step:1652/2330 train_time:96636ms step_avg:58.50ms
step:1653/2330 train_time:96694ms step_avg:58.50ms
step:1654/2330 train_time:96754ms step_avg:58.50ms
step:1655/2330 train_time:96811ms step_avg:58.50ms
step:1656/2330 train_time:96872ms step_avg:58.50ms
step:1657/2330 train_time:96929ms step_avg:58.50ms
step:1658/2330 train_time:96990ms step_avg:58.50ms
step:1659/2330 train_time:97047ms step_avg:58.50ms
step:1660/2330 train_time:97108ms step_avg:58.50ms
step:1661/2330 train_time:97165ms step_avg:58.50ms
step:1662/2330 train_time:97228ms step_avg:58.50ms
step:1663/2330 train_time:97285ms step_avg:58.50ms
step:1664/2330 train_time:97346ms step_avg:58.50ms
step:1665/2330 train_time:97403ms step_avg:58.50ms
step:1666/2330 train_time:97465ms step_avg:58.50ms
step:1667/2330 train_time:97523ms step_avg:58.50ms
step:1668/2330 train_time:97584ms step_avg:58.50ms
step:1669/2330 train_time:97642ms step_avg:58.50ms
step:1670/2330 train_time:97702ms step_avg:58.50ms
step:1671/2330 train_time:97760ms step_avg:58.50ms
step:1672/2330 train_time:97821ms step_avg:58.51ms
step:1673/2330 train_time:97879ms step_avg:58.51ms
step:1674/2330 train_time:97939ms step_avg:58.51ms
step:1675/2330 train_time:97997ms step_avg:58.51ms
step:1676/2330 train_time:98057ms step_avg:58.51ms
step:1677/2330 train_time:98114ms step_avg:58.51ms
step:1678/2330 train_time:98175ms step_avg:58.51ms
step:1679/2330 train_time:98232ms step_avg:58.51ms
step:1680/2330 train_time:98294ms step_avg:58.51ms
step:1681/2330 train_time:98351ms step_avg:58.51ms
step:1682/2330 train_time:98412ms step_avg:58.51ms
step:1683/2330 train_time:98469ms step_avg:58.51ms
step:1684/2330 train_time:98530ms step_avg:58.51ms
step:1685/2330 train_time:98587ms step_avg:58.51ms
step:1686/2330 train_time:98648ms step_avg:58.51ms
step:1687/2330 train_time:98705ms step_avg:58.51ms
step:1688/2330 train_time:98767ms step_avg:58.51ms
step:1689/2330 train_time:98824ms step_avg:58.51ms
step:1690/2330 train_time:98885ms step_avg:58.51ms
step:1691/2330 train_time:98944ms step_avg:58.51ms
step:1692/2330 train_time:99004ms step_avg:58.51ms
step:1693/2330 train_time:99061ms step_avg:58.51ms
step:1694/2330 train_time:99122ms step_avg:58.51ms
step:1695/2330 train_time:99181ms step_avg:58.51ms
step:1696/2330 train_time:99241ms step_avg:58.51ms
step:1697/2330 train_time:99300ms step_avg:58.52ms
step:1698/2330 train_time:99361ms step_avg:58.52ms
step:1699/2330 train_time:99420ms step_avg:58.52ms
step:1700/2330 train_time:99480ms step_avg:58.52ms
step:1701/2330 train_time:99537ms step_avg:58.52ms
step:1702/2330 train_time:99597ms step_avg:58.52ms
step:1703/2330 train_time:99654ms step_avg:58.52ms
step:1704/2330 train_time:99714ms step_avg:58.52ms
step:1705/2330 train_time:99771ms step_avg:58.52ms
step:1706/2330 train_time:99832ms step_avg:58.52ms
step:1707/2330 train_time:99889ms step_avg:58.52ms
step:1708/2330 train_time:99950ms step_avg:58.52ms
step:1709/2330 train_time:100008ms step_avg:58.52ms
step:1710/2330 train_time:100070ms step_avg:58.52ms
step:1711/2330 train_time:100126ms step_avg:58.52ms
step:1712/2330 train_time:100188ms step_avg:58.52ms
step:1713/2330 train_time:100244ms step_avg:58.52ms
step:1714/2330 train_time:100307ms step_avg:58.52ms
step:1715/2330 train_time:100364ms step_avg:58.52ms
step:1716/2330 train_time:100427ms step_avg:58.52ms
step:1717/2330 train_time:100484ms step_avg:58.52ms
step:1718/2330 train_time:100545ms step_avg:58.52ms
step:1719/2330 train_time:100602ms step_avg:58.52ms
step:1720/2330 train_time:100663ms step_avg:58.53ms
step:1721/2330 train_time:100721ms step_avg:58.52ms
step:1722/2330 train_time:100781ms step_avg:58.53ms
step:1723/2330 train_time:100839ms step_avg:58.53ms
step:1724/2330 train_time:100899ms step_avg:58.53ms
step:1725/2330 train_time:100957ms step_avg:58.53ms
step:1726/2330 train_time:101019ms step_avg:58.53ms
step:1727/2330 train_time:101076ms step_avg:58.53ms
step:1728/2330 train_time:101137ms step_avg:58.53ms
step:1729/2330 train_time:101194ms step_avg:58.53ms
step:1730/2330 train_time:101255ms step_avg:58.53ms
step:1731/2330 train_time:101312ms step_avg:58.53ms
step:1732/2330 train_time:101374ms step_avg:58.53ms
step:1733/2330 train_time:101430ms step_avg:58.53ms
step:1734/2330 train_time:101493ms step_avg:58.53ms
step:1735/2330 train_time:101549ms step_avg:58.53ms
step:1736/2330 train_time:101610ms step_avg:58.53ms
step:1737/2330 train_time:101667ms step_avg:58.53ms
step:1738/2330 train_time:101729ms step_avg:58.53ms
step:1739/2330 train_time:101785ms step_avg:58.53ms
step:1740/2330 train_time:101847ms step_avg:58.53ms
step:1741/2330 train_time:101904ms step_avg:58.53ms
step:1742/2330 train_time:101966ms step_avg:58.53ms
step:1743/2330 train_time:102023ms step_avg:58.53ms
step:1744/2330 train_time:102085ms step_avg:58.54ms
step:1745/2330 train_time:102143ms step_avg:58.53ms
step:1746/2330 train_time:102204ms step_avg:58.54ms
step:1747/2330 train_time:102262ms step_avg:58.54ms
step:1748/2330 train_time:102323ms step_avg:58.54ms
step:1749/2330 train_time:102382ms step_avg:58.54ms
step:1750/2330 train_time:102442ms step_avg:58.54ms
step:1750/2330 val_loss:3.8220 train_time:102524ms step_avg:58.58ms
step:1751/2330 train_time:102542ms step_avg:58.56ms
step:1752/2330 train_time:102562ms step_avg:58.54ms
step:1753/2330 train_time:102617ms step_avg:58.54ms
step:1754/2330 train_time:102686ms step_avg:58.54ms
step:1755/2330 train_time:102742ms step_avg:58.54ms
step:1756/2330 train_time:102809ms step_avg:58.55ms
step:1757/2330 train_time:102865ms step_avg:58.55ms
step:1758/2330 train_time:102926ms step_avg:58.55ms
step:1759/2330 train_time:102982ms step_avg:58.55ms
step:1760/2330 train_time:103043ms step_avg:58.55ms
step:1761/2330 train_time:103100ms step_avg:58.55ms
step:1762/2330 train_time:103160ms step_avg:58.55ms
step:1763/2330 train_time:103216ms step_avg:58.55ms
step:1764/2330 train_time:103276ms step_avg:58.55ms
step:1765/2330 train_time:103332ms step_avg:58.55ms
step:1766/2330 train_time:103392ms step_avg:58.55ms
step:1767/2330 train_time:103452ms step_avg:58.55ms
step:1768/2330 train_time:103515ms step_avg:58.55ms
step:1769/2330 train_time:103572ms step_avg:58.55ms
step:1770/2330 train_time:103634ms step_avg:58.55ms
step:1771/2330 train_time:103692ms step_avg:58.55ms
step:1772/2330 train_time:103754ms step_avg:58.55ms
step:1773/2330 train_time:103811ms step_avg:58.55ms
step:1774/2330 train_time:103873ms step_avg:58.55ms
step:1775/2330 train_time:103929ms step_avg:58.55ms
step:1776/2330 train_time:103991ms step_avg:58.55ms
step:1777/2330 train_time:104048ms step_avg:58.55ms
step:1778/2330 train_time:104109ms step_avg:58.55ms
step:1779/2330 train_time:104165ms step_avg:58.55ms
step:1780/2330 train_time:104226ms step_avg:58.55ms
step:1781/2330 train_time:104283ms step_avg:58.55ms
step:1782/2330 train_time:104344ms step_avg:58.55ms
step:1783/2330 train_time:104402ms step_avg:58.55ms
step:1784/2330 train_time:104463ms step_avg:58.56ms
step:1785/2330 train_time:104522ms step_avg:58.56ms
step:1786/2330 train_time:104583ms step_avg:58.56ms
step:1787/2330 train_time:104641ms step_avg:58.56ms
step:1788/2330 train_time:104704ms step_avg:58.56ms
step:1789/2330 train_time:104761ms step_avg:58.56ms
step:1790/2330 train_time:104823ms step_avg:58.56ms
step:1791/2330 train_time:104881ms step_avg:58.56ms
step:1792/2330 train_time:104942ms step_avg:58.56ms
step:1793/2330 train_time:104998ms step_avg:58.56ms
step:1794/2330 train_time:105060ms step_avg:58.56ms
step:1795/2330 train_time:105117ms step_avg:58.56ms
step:1796/2330 train_time:105177ms step_avg:58.56ms
step:1797/2330 train_time:105234ms step_avg:58.56ms
step:1798/2330 train_time:105295ms step_avg:58.56ms
step:1799/2330 train_time:105352ms step_avg:58.56ms
step:1800/2330 train_time:105414ms step_avg:58.56ms
step:1801/2330 train_time:105471ms step_avg:58.56ms
step:1802/2330 train_time:105532ms step_avg:58.56ms
step:1803/2330 train_time:105590ms step_avg:58.56ms
step:1804/2330 train_time:105651ms step_avg:58.56ms
step:1805/2330 train_time:105708ms step_avg:58.56ms
step:1806/2330 train_time:105769ms step_avg:58.57ms
step:1807/2330 train_time:105826ms step_avg:58.56ms
step:1808/2330 train_time:105888ms step_avg:58.57ms
step:1809/2330 train_time:105944ms step_avg:58.57ms
step:1810/2330 train_time:106006ms step_avg:58.57ms
step:1811/2330 train_time:106063ms step_avg:58.57ms
step:1812/2330 train_time:106124ms step_avg:58.57ms
step:1813/2330 train_time:106180ms step_avg:58.57ms
step:1814/2330 train_time:106243ms step_avg:58.57ms
step:1815/2330 train_time:106299ms step_avg:58.57ms
step:1816/2330 train_time:106361ms step_avg:58.57ms
step:1817/2330 train_time:106418ms step_avg:58.57ms
step:1818/2330 train_time:106480ms step_avg:58.57ms
step:1819/2330 train_time:106537ms step_avg:58.57ms
step:1820/2330 train_time:106599ms step_avg:58.57ms
step:1821/2330 train_time:106658ms step_avg:58.57ms
step:1822/2330 train_time:106719ms step_avg:58.57ms
step:1823/2330 train_time:106777ms step_avg:58.57ms
step:1824/2330 train_time:106838ms step_avg:58.57ms
step:1825/2330 train_time:106897ms step_avg:58.57ms
step:1826/2330 train_time:106957ms step_avg:58.57ms
step:1827/2330 train_time:107015ms step_avg:58.57ms
step:1828/2330 train_time:107076ms step_avg:58.58ms
step:1829/2330 train_time:107133ms step_avg:58.57ms
step:1830/2330 train_time:107194ms step_avg:58.58ms
step:1831/2330 train_time:107251ms step_avg:58.57ms
step:1832/2330 train_time:107312ms step_avg:58.58ms
step:1833/2330 train_time:107369ms step_avg:58.58ms
step:1834/2330 train_time:107431ms step_avg:58.58ms
step:1835/2330 train_time:107487ms step_avg:58.58ms
step:1836/2330 train_time:107549ms step_avg:58.58ms
step:1837/2330 train_time:107606ms step_avg:58.58ms
step:1838/2330 train_time:107666ms step_avg:58.58ms
step:1839/2330 train_time:107723ms step_avg:58.58ms
step:1840/2330 train_time:107786ms step_avg:58.58ms
step:1841/2330 train_time:107843ms step_avg:58.58ms
step:1842/2330 train_time:107905ms step_avg:58.58ms
step:1843/2330 train_time:107963ms step_avg:58.58ms
step:1844/2330 train_time:108024ms step_avg:58.58ms
step:1845/2330 train_time:108081ms step_avg:58.58ms
step:1846/2330 train_time:108142ms step_avg:58.58ms
step:1847/2330 train_time:108200ms step_avg:58.58ms
step:1848/2330 train_time:108260ms step_avg:58.58ms
step:1849/2330 train_time:108318ms step_avg:58.58ms
step:1850/2330 train_time:108379ms step_avg:58.58ms
step:1851/2330 train_time:108437ms step_avg:58.58ms
step:1852/2330 train_time:108498ms step_avg:58.58ms
step:1853/2330 train_time:108554ms step_avg:58.58ms
step:1854/2330 train_time:108615ms step_avg:58.58ms
step:1855/2330 train_time:108672ms step_avg:58.58ms
step:1856/2330 train_time:108734ms step_avg:58.59ms
step:1857/2330 train_time:108791ms step_avg:58.58ms
step:1858/2330 train_time:108853ms step_avg:58.59ms
step:1859/2330 train_time:108910ms step_avg:58.59ms
step:1860/2330 train_time:108973ms step_avg:58.59ms
step:1861/2330 train_time:109029ms step_avg:58.59ms
step:1862/2330 train_time:109092ms step_avg:58.59ms
step:1863/2330 train_time:109147ms step_avg:58.59ms
step:1864/2330 train_time:109209ms step_avg:58.59ms
step:1865/2330 train_time:109266ms step_avg:58.59ms
step:1866/2330 train_time:109326ms step_avg:58.59ms
step:1867/2330 train_time:109383ms step_avg:58.59ms
step:1868/2330 train_time:109445ms step_avg:58.59ms
step:1869/2330 train_time:109501ms step_avg:58.59ms
step:1870/2330 train_time:109563ms step_avg:58.59ms
step:1871/2330 train_time:109621ms step_avg:58.59ms
step:1872/2330 train_time:109682ms step_avg:58.59ms
step:1873/2330 train_time:109740ms step_avg:58.59ms
step:1874/2330 train_time:109802ms step_avg:58.59ms
step:1875/2330 train_time:109860ms step_avg:58.59ms
step:1876/2330 train_time:109921ms step_avg:58.59ms
step:1877/2330 train_time:109978ms step_avg:58.59ms
step:1878/2330 train_time:110039ms step_avg:58.59ms
step:1879/2330 train_time:110096ms step_avg:58.59ms
step:1880/2330 train_time:110156ms step_avg:58.59ms
step:1881/2330 train_time:110213ms step_avg:58.59ms
step:1882/2330 train_time:110275ms step_avg:58.59ms
step:1883/2330 train_time:110332ms step_avg:58.59ms
step:1884/2330 train_time:110394ms step_avg:58.60ms
step:1885/2330 train_time:110450ms step_avg:58.59ms
step:1886/2330 train_time:110512ms step_avg:58.60ms
step:1887/2330 train_time:110568ms step_avg:58.59ms
step:1888/2330 train_time:110630ms step_avg:58.60ms
step:1889/2330 train_time:110687ms step_avg:58.60ms
step:1890/2330 train_time:110748ms step_avg:58.60ms
step:1891/2330 train_time:110806ms step_avg:58.60ms
step:1892/2330 train_time:110868ms step_avg:58.60ms
step:1893/2330 train_time:110924ms step_avg:58.60ms
step:1894/2330 train_time:110986ms step_avg:58.60ms
step:1895/2330 train_time:111042ms step_avg:58.60ms
step:1896/2330 train_time:111106ms step_avg:58.60ms
step:1897/2330 train_time:111162ms step_avg:58.60ms
step:1898/2330 train_time:111224ms step_avg:58.60ms
step:1899/2330 train_time:111281ms step_avg:58.60ms
step:1900/2330 train_time:111343ms step_avg:58.60ms
step:1901/2330 train_time:111401ms step_avg:58.60ms
step:1902/2330 train_time:111461ms step_avg:58.60ms
step:1903/2330 train_time:111518ms step_avg:58.60ms
step:1904/2330 train_time:111580ms step_avg:58.60ms
step:1905/2330 train_time:111637ms step_avg:58.60ms
step:1906/2330 train_time:111698ms step_avg:58.60ms
step:1907/2330 train_time:111757ms step_avg:58.60ms
step:1908/2330 train_time:111818ms step_avg:58.60ms
step:1909/2330 train_time:111876ms step_avg:58.60ms
step:1910/2330 train_time:111936ms step_avg:58.61ms
step:1911/2330 train_time:111995ms step_avg:58.61ms
step:1912/2330 train_time:112055ms step_avg:58.61ms
step:1913/2330 train_time:112112ms step_avg:58.61ms
step:1914/2330 train_time:112173ms step_avg:58.61ms
step:1915/2330 train_time:112230ms step_avg:58.61ms
step:1916/2330 train_time:112292ms step_avg:58.61ms
step:1917/2330 train_time:112349ms step_avg:58.61ms
step:1918/2330 train_time:112409ms step_avg:58.61ms
step:1919/2330 train_time:112467ms step_avg:58.61ms
step:1920/2330 train_time:112528ms step_avg:58.61ms
step:1921/2330 train_time:112584ms step_avg:58.61ms
step:1922/2330 train_time:112647ms step_avg:58.61ms
step:1923/2330 train_time:112704ms step_avg:58.61ms
step:1924/2330 train_time:112766ms step_avg:58.61ms
step:1925/2330 train_time:112823ms step_avg:58.61ms
step:1926/2330 train_time:112885ms step_avg:58.61ms
step:1927/2330 train_time:112942ms step_avg:58.61ms
step:1928/2330 train_time:113004ms step_avg:58.61ms
step:1929/2330 train_time:113061ms step_avg:58.61ms
step:1930/2330 train_time:113123ms step_avg:58.61ms
step:1931/2330 train_time:113181ms step_avg:58.61ms
step:1932/2330 train_time:113242ms step_avg:58.61ms
step:1933/2330 train_time:113300ms step_avg:58.61ms
step:1934/2330 train_time:113361ms step_avg:58.61ms
step:1935/2330 train_time:113418ms step_avg:58.61ms
step:1936/2330 train_time:113478ms step_avg:58.61ms
step:1937/2330 train_time:113536ms step_avg:58.61ms
step:1938/2330 train_time:113596ms step_avg:58.62ms
step:1939/2330 train_time:113654ms step_avg:58.61ms
step:1940/2330 train_time:113715ms step_avg:58.62ms
step:1941/2330 train_time:113772ms step_avg:58.62ms
step:1942/2330 train_time:113833ms step_avg:58.62ms
step:1943/2330 train_time:113890ms step_avg:58.62ms
step:1944/2330 train_time:113952ms step_avg:58.62ms
step:1945/2330 train_time:114009ms step_avg:58.62ms
step:1946/2330 train_time:114071ms step_avg:58.62ms
step:1947/2330 train_time:114128ms step_avg:58.62ms
step:1948/2330 train_time:114189ms step_avg:58.62ms
step:1949/2330 train_time:114246ms step_avg:58.62ms
step:1950/2330 train_time:114307ms step_avg:58.62ms
step:1951/2330 train_time:114363ms step_avg:58.62ms
step:1952/2330 train_time:114425ms step_avg:58.62ms
step:1953/2330 train_time:114482ms step_avg:58.62ms
step:1954/2330 train_time:114543ms step_avg:58.62ms
step:1955/2330 train_time:114601ms step_avg:58.62ms
step:1956/2330 train_time:114661ms step_avg:58.62ms
step:1957/2330 train_time:114720ms step_avg:58.62ms
step:1958/2330 train_time:114780ms step_avg:58.62ms
step:1959/2330 train_time:114838ms step_avg:58.62ms
step:1960/2330 train_time:114899ms step_avg:58.62ms
step:1961/2330 train_time:114958ms step_avg:58.62ms
step:1962/2330 train_time:115018ms step_avg:58.62ms
step:1963/2330 train_time:115077ms step_avg:58.62ms
step:1964/2330 train_time:115137ms step_avg:58.62ms
step:1965/2330 train_time:115195ms step_avg:58.62ms
step:1966/2330 train_time:115255ms step_avg:58.62ms
step:1967/2330 train_time:115312ms step_avg:58.62ms
step:1968/2330 train_time:115372ms step_avg:58.62ms
step:1969/2330 train_time:115429ms step_avg:58.62ms
step:1970/2330 train_time:115491ms step_avg:58.62ms
step:1971/2330 train_time:115547ms step_avg:58.62ms
step:1972/2330 train_time:115608ms step_avg:58.62ms
step:1973/2330 train_time:115665ms step_avg:58.62ms
step:1974/2330 train_time:115726ms step_avg:58.63ms
step:1975/2330 train_time:115783ms step_avg:58.62ms
step:1976/2330 train_time:115845ms step_avg:58.63ms
step:1977/2330 train_time:115902ms step_avg:58.63ms
step:1978/2330 train_time:115963ms step_avg:58.63ms
step:1979/2330 train_time:116021ms step_avg:58.63ms
step:1980/2330 train_time:116083ms step_avg:58.63ms
step:1981/2330 train_time:116141ms step_avg:58.63ms
step:1982/2330 train_time:116203ms step_avg:58.63ms
step:1983/2330 train_time:116261ms step_avg:58.63ms
step:1984/2330 train_time:116321ms step_avg:58.63ms
step:1985/2330 train_time:116379ms step_avg:58.63ms
step:1986/2330 train_time:116440ms step_avg:58.63ms
step:1987/2330 train_time:116498ms step_avg:58.63ms
step:1988/2330 train_time:116558ms step_avg:58.63ms
step:1989/2330 train_time:116615ms step_avg:58.63ms
step:1990/2330 train_time:116676ms step_avg:58.63ms
step:1991/2330 train_time:116733ms step_avg:58.63ms
step:1992/2330 train_time:116794ms step_avg:58.63ms
step:1993/2330 train_time:116852ms step_avg:58.63ms
step:1994/2330 train_time:116913ms step_avg:58.63ms
step:1995/2330 train_time:116970ms step_avg:58.63ms
step:1996/2330 train_time:117031ms step_avg:58.63ms
step:1997/2330 train_time:117088ms step_avg:58.63ms
step:1998/2330 train_time:117149ms step_avg:58.63ms
step:1999/2330 train_time:117205ms step_avg:58.63ms
step:2000/2330 train_time:117266ms step_avg:58.63ms
step:2000/2330 val_loss:3.7590 train_time:117348ms step_avg:58.67ms
step:2001/2330 train_time:117368ms step_avg:58.65ms
step:2002/2330 train_time:117389ms step_avg:58.64ms
step:2003/2330 train_time:117448ms step_avg:58.64ms
step:2004/2330 train_time:117512ms step_avg:58.64ms
step:2005/2330 train_time:117569ms step_avg:58.64ms
step:2006/2330 train_time:117634ms step_avg:58.64ms
step:2007/2330 train_time:117690ms step_avg:58.64ms
step:2008/2330 train_time:117750ms step_avg:58.64ms
step:2009/2330 train_time:117807ms step_avg:58.64ms
step:2010/2330 train_time:117868ms step_avg:58.64ms
step:2011/2330 train_time:117924ms step_avg:58.64ms
step:2012/2330 train_time:117984ms step_avg:58.64ms
step:2013/2330 train_time:118040ms step_avg:58.64ms
step:2014/2330 train_time:118100ms step_avg:58.64ms
step:2015/2330 train_time:118157ms step_avg:58.64ms
step:2016/2330 train_time:118216ms step_avg:58.64ms
step:2017/2330 train_time:118273ms step_avg:58.64ms
step:2018/2330 train_time:118335ms step_avg:58.64ms
step:2019/2330 train_time:118394ms step_avg:58.64ms
step:2020/2330 train_time:118457ms step_avg:58.64ms
step:2021/2330 train_time:118515ms step_avg:58.64ms
step:2022/2330 train_time:118579ms step_avg:58.64ms
step:2023/2330 train_time:118638ms step_avg:58.64ms
step:2024/2330 train_time:118700ms step_avg:58.65ms
step:2025/2330 train_time:118757ms step_avg:58.65ms
step:2026/2330 train_time:118818ms step_avg:58.65ms
step:2027/2330 train_time:118875ms step_avg:58.65ms
step:2028/2330 train_time:118935ms step_avg:58.65ms
step:2029/2330 train_time:118992ms step_avg:58.65ms
step:2030/2330 train_time:119051ms step_avg:58.65ms
step:2031/2330 train_time:119109ms step_avg:58.65ms
step:2032/2330 train_time:119168ms step_avg:58.65ms
step:2033/2330 train_time:119225ms step_avg:58.64ms
step:2034/2330 train_time:119284ms step_avg:58.65ms
step:2035/2330 train_time:119342ms step_avg:58.64ms
step:2036/2330 train_time:119404ms step_avg:58.65ms
step:2037/2330 train_time:119462ms step_avg:58.65ms
step:2038/2330 train_time:119526ms step_avg:58.65ms
step:2039/2330 train_time:119584ms step_avg:58.65ms
step:2040/2330 train_time:119647ms step_avg:58.65ms
step:2041/2330 train_time:119704ms step_avg:58.65ms
step:2042/2330 train_time:119767ms step_avg:58.65ms
step:2043/2330 train_time:119824ms step_avg:58.65ms
step:2044/2330 train_time:119884ms step_avg:58.65ms
step:2045/2330 train_time:119942ms step_avg:58.65ms
step:2046/2330 train_time:120003ms step_avg:58.65ms
step:2047/2330 train_time:120060ms step_avg:58.65ms
step:2048/2330 train_time:120120ms step_avg:58.65ms
step:2049/2330 train_time:120177ms step_avg:58.65ms
step:2050/2330 train_time:120238ms step_avg:58.65ms
step:2051/2330 train_time:120295ms step_avg:58.65ms
step:2052/2330 train_time:120357ms step_avg:58.65ms
step:2053/2330 train_time:120416ms step_avg:58.65ms
step:2054/2330 train_time:120477ms step_avg:58.66ms
step:2055/2330 train_time:120535ms step_avg:58.65ms
step:2056/2330 train_time:120598ms step_avg:58.66ms
step:2057/2330 train_time:120655ms step_avg:58.66ms
step:2058/2330 train_time:120718ms step_avg:58.66ms
step:2059/2330 train_time:120776ms step_avg:58.66ms
step:2060/2330 train_time:120837ms step_avg:58.66ms
step:2061/2330 train_time:120894ms step_avg:58.66ms
step:2062/2330 train_time:120955ms step_avg:58.66ms
step:2063/2330 train_time:121012ms step_avg:58.66ms
step:2064/2330 train_time:121072ms step_avg:58.66ms
step:2065/2330 train_time:121128ms step_avg:58.66ms
step:2066/2330 train_time:121188ms step_avg:58.66ms
step:2067/2330 train_time:121245ms step_avg:58.66ms
step:2068/2330 train_time:121306ms step_avg:58.66ms
step:2069/2330 train_time:121365ms step_avg:58.66ms
step:2070/2330 train_time:121427ms step_avg:58.66ms
step:2071/2330 train_time:121485ms step_avg:58.66ms
step:2072/2330 train_time:121547ms step_avg:58.66ms
step:2073/2330 train_time:121605ms step_avg:58.66ms
step:2074/2330 train_time:121666ms step_avg:58.66ms
step:2075/2330 train_time:121724ms step_avg:58.66ms
step:2076/2330 train_time:121786ms step_avg:58.66ms
step:2077/2330 train_time:121843ms step_avg:58.66ms
step:2078/2330 train_time:121904ms step_avg:58.66ms
step:2079/2330 train_time:121961ms step_avg:58.66ms
step:2080/2330 train_time:122022ms step_avg:58.66ms
step:2081/2330 train_time:122079ms step_avg:58.66ms
step:2082/2330 train_time:122138ms step_avg:58.66ms
step:2083/2330 train_time:122195ms step_avg:58.66ms
step:2084/2330 train_time:122255ms step_avg:58.66ms
step:2085/2330 train_time:122313ms step_avg:58.66ms
step:2086/2330 train_time:122374ms step_avg:58.66ms
step:2087/2330 train_time:122432ms step_avg:58.66ms
step:2088/2330 train_time:122495ms step_avg:58.67ms
step:2089/2330 train_time:122552ms step_avg:58.67ms
step:2090/2330 train_time:122615ms step_avg:58.67ms
step:2091/2330 train_time:122671ms step_avg:58.67ms
step:2092/2330 train_time:122734ms step_avg:58.67ms
step:2093/2330 train_time:122790ms step_avg:58.67ms
step:2094/2330 train_time:122852ms step_avg:58.67ms
step:2095/2330 train_time:122909ms step_avg:58.67ms
step:2096/2330 train_time:122970ms step_avg:58.67ms
step:2097/2330 train_time:123026ms step_avg:58.67ms
step:2098/2330 train_time:123087ms step_avg:58.67ms
step:2099/2330 train_time:123144ms step_avg:58.67ms
step:2100/2330 train_time:123205ms step_avg:58.67ms
step:2101/2330 train_time:123263ms step_avg:58.67ms
step:2102/2330 train_time:123325ms step_avg:58.67ms
step:2103/2330 train_time:123383ms step_avg:58.67ms
step:2104/2330 train_time:123444ms step_avg:58.67ms
step:2105/2330 train_time:123502ms step_avg:58.67ms
step:2106/2330 train_time:123564ms step_avg:58.67ms
step:2107/2330 train_time:123623ms step_avg:58.67ms
step:2108/2330 train_time:123684ms step_avg:58.67ms
step:2109/2330 train_time:123742ms step_avg:58.67ms
step:2110/2330 train_time:123803ms step_avg:58.67ms
step:2111/2330 train_time:123861ms step_avg:58.67ms
step:2112/2330 train_time:123921ms step_avg:58.67ms
step:2113/2330 train_time:123978ms step_avg:58.67ms
step:2114/2330 train_time:124039ms step_avg:58.67ms
step:2115/2330 train_time:124096ms step_avg:58.67ms
step:2116/2330 train_time:124156ms step_avg:58.67ms
step:2117/2330 train_time:124213ms step_avg:58.67ms
step:2118/2330 train_time:124274ms step_avg:58.68ms
step:2119/2330 train_time:124331ms step_avg:58.67ms
step:2120/2330 train_time:124393ms step_avg:58.68ms
step:2121/2330 train_time:124450ms step_avg:58.68ms
step:2122/2330 train_time:124513ms step_avg:58.68ms
step:2123/2330 train_time:124570ms step_avg:58.68ms
step:2124/2330 train_time:124632ms step_avg:58.68ms
step:2125/2330 train_time:124689ms step_avg:58.68ms
step:2126/2330 train_time:124750ms step_avg:58.68ms
step:2127/2330 train_time:124806ms step_avg:58.68ms
step:2128/2330 train_time:124869ms step_avg:58.68ms
step:2129/2330 train_time:124926ms step_avg:58.68ms
step:2130/2330 train_time:124987ms step_avg:58.68ms
step:2131/2330 train_time:125044ms step_avg:58.68ms
step:2132/2330 train_time:125106ms step_avg:58.68ms
step:2133/2330 train_time:125164ms step_avg:58.68ms
step:2134/2330 train_time:125224ms step_avg:58.68ms
step:2135/2330 train_time:125282ms step_avg:58.68ms
step:2136/2330 train_time:125343ms step_avg:58.68ms
step:2137/2330 train_time:125400ms step_avg:58.68ms
step:2138/2330 train_time:125461ms step_avg:58.68ms
step:2139/2330 train_time:125520ms step_avg:58.68ms
step:2140/2330 train_time:125580ms step_avg:58.68ms
step:2141/2330 train_time:125639ms step_avg:58.68ms
step:2142/2330 train_time:125699ms step_avg:58.68ms
step:2143/2330 train_time:125758ms step_avg:58.68ms
step:2144/2330 train_time:125819ms step_avg:58.68ms
step:2145/2330 train_time:125877ms step_avg:58.68ms
step:2146/2330 train_time:125937ms step_avg:58.68ms
step:2147/2330 train_time:125995ms step_avg:58.68ms
step:2148/2330 train_time:126056ms step_avg:58.69ms
step:2149/2330 train_time:126114ms step_avg:58.68ms
step:2150/2330 train_time:126173ms step_avg:58.69ms
step:2151/2330 train_time:126230ms step_avg:58.68ms
step:2152/2330 train_time:126291ms step_avg:58.69ms
step:2153/2330 train_time:126348ms step_avg:58.68ms
step:2154/2330 train_time:126409ms step_avg:58.69ms
step:2155/2330 train_time:126466ms step_avg:58.68ms
step:2156/2330 train_time:126529ms step_avg:58.69ms
step:2157/2330 train_time:126586ms step_avg:58.69ms
step:2158/2330 train_time:126647ms step_avg:58.69ms
step:2159/2330 train_time:126705ms step_avg:58.69ms
step:2160/2330 train_time:126766ms step_avg:58.69ms
step:2161/2330 train_time:126825ms step_avg:58.69ms
step:2162/2330 train_time:126885ms step_avg:58.69ms
step:2163/2330 train_time:126943ms step_avg:58.69ms
step:2164/2330 train_time:127004ms step_avg:58.69ms
step:2165/2330 train_time:127062ms step_avg:58.69ms
step:2166/2330 train_time:127123ms step_avg:58.69ms
step:2167/2330 train_time:127181ms step_avg:58.69ms
step:2168/2330 train_time:127241ms step_avg:58.69ms
step:2169/2330 train_time:127299ms step_avg:58.69ms
step:2170/2330 train_time:127360ms step_avg:58.69ms
step:2171/2330 train_time:127419ms step_avg:58.69ms
step:2172/2330 train_time:127479ms step_avg:58.69ms
step:2173/2330 train_time:127538ms step_avg:58.69ms
step:2174/2330 train_time:127599ms step_avg:58.69ms
step:2175/2330 train_time:127657ms step_avg:58.69ms
step:2176/2330 train_time:127718ms step_avg:58.69ms
step:2177/2330 train_time:127776ms step_avg:58.69ms
step:2178/2330 train_time:127835ms step_avg:58.69ms
step:2179/2330 train_time:127892ms step_avg:58.69ms
step:2180/2330 train_time:127955ms step_avg:58.69ms
step:2181/2330 train_time:128012ms step_avg:58.69ms
step:2182/2330 train_time:128074ms step_avg:58.70ms
step:2183/2330 train_time:128131ms step_avg:58.69ms
step:2184/2330 train_time:128191ms step_avg:58.70ms
step:2185/2330 train_time:128248ms step_avg:58.69ms
step:2186/2330 train_time:128309ms step_avg:58.70ms
step:2187/2330 train_time:128366ms step_avg:58.70ms
step:2188/2330 train_time:128429ms step_avg:58.70ms
step:2189/2330 train_time:128486ms step_avg:58.70ms
step:2190/2330 train_time:128547ms step_avg:58.70ms
step:2191/2330 train_time:128604ms step_avg:58.70ms
step:2192/2330 train_time:128665ms step_avg:58.70ms
step:2193/2330 train_time:128722ms step_avg:58.70ms
step:2194/2330 train_time:128784ms step_avg:58.70ms
step:2195/2330 train_time:128842ms step_avg:58.70ms
step:2196/2330 train_time:128902ms step_avg:58.70ms
step:2197/2330 train_time:128960ms step_avg:58.70ms
step:2198/2330 train_time:129021ms step_avg:58.70ms
step:2199/2330 train_time:129079ms step_avg:58.70ms
step:2200/2330 train_time:129140ms step_avg:58.70ms
step:2201/2330 train_time:129199ms step_avg:58.70ms
step:2202/2330 train_time:129259ms step_avg:58.70ms
step:2203/2330 train_time:129316ms step_avg:58.70ms
step:2204/2330 train_time:129377ms step_avg:58.70ms
step:2205/2330 train_time:129434ms step_avg:58.70ms
step:2206/2330 train_time:129497ms step_avg:58.70ms
step:2207/2330 train_time:129553ms step_avg:58.70ms
step:2208/2330 train_time:129615ms step_avg:58.70ms
step:2209/2330 train_time:129672ms step_avg:58.70ms
step:2210/2330 train_time:129734ms step_avg:58.70ms
step:2211/2330 train_time:129791ms step_avg:58.70ms
step:2212/2330 train_time:129853ms step_avg:58.70ms
step:2213/2330 train_time:129910ms step_avg:58.70ms
step:2214/2330 train_time:129971ms step_avg:58.70ms
step:2215/2330 train_time:130029ms step_avg:58.70ms
step:2216/2330 train_time:130090ms step_avg:58.70ms
step:2217/2330 train_time:130146ms step_avg:58.70ms
step:2218/2330 train_time:130208ms step_avg:58.71ms
step:2219/2330 train_time:130265ms step_avg:58.70ms
step:2220/2330 train_time:130327ms step_avg:58.71ms
step:2221/2330 train_time:130383ms step_avg:58.70ms
step:2222/2330 train_time:130446ms step_avg:58.71ms
step:2223/2330 train_time:130504ms step_avg:58.71ms
step:2224/2330 train_time:130565ms step_avg:58.71ms
step:2225/2330 train_time:130623ms step_avg:58.71ms
step:2226/2330 train_time:130685ms step_avg:58.71ms
step:2227/2330 train_time:130743ms step_avg:58.71ms
step:2228/2330 train_time:130803ms step_avg:58.71ms
step:2229/2330 train_time:130861ms step_avg:58.71ms
step:2230/2330 train_time:130921ms step_avg:58.71ms
step:2231/2330 train_time:130978ms step_avg:58.71ms
step:2232/2330 train_time:131039ms step_avg:58.71ms
step:2233/2330 train_time:131096ms step_avg:58.71ms
step:2234/2330 train_time:131157ms step_avg:58.71ms
step:2235/2330 train_time:131215ms step_avg:58.71ms
step:2236/2330 train_time:131275ms step_avg:58.71ms
step:2237/2330 train_time:131332ms step_avg:58.71ms
step:2238/2330 train_time:131394ms step_avg:58.71ms
step:2239/2330 train_time:131451ms step_avg:58.71ms
step:2240/2330 train_time:131513ms step_avg:58.71ms
step:2241/2330 train_time:131570ms step_avg:58.71ms
step:2242/2330 train_time:131632ms step_avg:58.71ms
step:2243/2330 train_time:131688ms step_avg:58.71ms
step:2244/2330 train_time:131749ms step_avg:58.71ms
step:2245/2330 train_time:131806ms step_avg:58.71ms
step:2246/2330 train_time:131869ms step_avg:58.71ms
step:2247/2330 train_time:131926ms step_avg:58.71ms
step:2248/2330 train_time:131988ms step_avg:58.71ms
step:2249/2330 train_time:132044ms step_avg:58.71ms
step:2250/2330 train_time:132106ms step_avg:58.71ms
step:2250/2330 val_loss:3.7106 train_time:132189ms step_avg:58.75ms
step:2251/2330 train_time:132207ms step_avg:58.73ms
step:2252/2330 train_time:132228ms step_avg:58.72ms
step:2253/2330 train_time:132288ms step_avg:58.72ms
step:2254/2330 train_time:132356ms step_avg:58.72ms
step:2255/2330 train_time:132415ms step_avg:58.72ms
step:2256/2330 train_time:132475ms step_avg:58.72ms
step:2257/2330 train_time:132532ms step_avg:58.72ms
step:2258/2330 train_time:132592ms step_avg:58.72ms
step:2259/2330 train_time:132649ms step_avg:58.72ms
step:2260/2330 train_time:132709ms step_avg:58.72ms
step:2261/2330 train_time:132766ms step_avg:58.72ms
step:2262/2330 train_time:132826ms step_avg:58.72ms
step:2263/2330 train_time:132882ms step_avg:58.72ms
step:2264/2330 train_time:132944ms step_avg:58.72ms
step:2265/2330 train_time:133001ms step_avg:58.72ms
step:2266/2330 train_time:133061ms step_avg:58.72ms
step:2267/2330 train_time:133118ms step_avg:58.72ms
step:2268/2330 train_time:133179ms step_avg:58.72ms
step:2269/2330 train_time:133240ms step_avg:58.72ms
step:2270/2330 train_time:133302ms step_avg:58.72ms
step:2271/2330 train_time:133361ms step_avg:58.72ms
step:2272/2330 train_time:133423ms step_avg:58.72ms
step:2273/2330 train_time:133481ms step_avg:58.72ms
step:2274/2330 train_time:133542ms step_avg:58.73ms
step:2275/2330 train_time:133600ms step_avg:58.73ms
step:2276/2330 train_time:133660ms step_avg:58.73ms
step:2277/2330 train_time:133718ms step_avg:58.73ms
step:2278/2330 train_time:133778ms step_avg:58.73ms
step:2279/2330 train_time:133834ms step_avg:58.73ms
step:2280/2330 train_time:133896ms step_avg:58.73ms
step:2281/2330 train_time:133952ms step_avg:58.73ms
step:2282/2330 train_time:134013ms step_avg:58.73ms
step:2283/2330 train_time:134070ms step_avg:58.73ms
step:2284/2330 train_time:134131ms step_avg:58.73ms
step:2285/2330 train_time:134189ms step_avg:58.73ms
step:2286/2330 train_time:134252ms step_avg:58.73ms
step:2287/2330 train_time:134309ms step_avg:58.73ms
step:2288/2330 train_time:134370ms step_avg:58.73ms
step:2289/2330 train_time:134428ms step_avg:58.73ms
step:2290/2330 train_time:134491ms step_avg:58.73ms
step:2291/2330 train_time:134548ms step_avg:58.73ms
step:2292/2330 train_time:134609ms step_avg:58.73ms
step:2293/2330 train_time:134666ms step_avg:58.73ms
step:2294/2330 train_time:134728ms step_avg:58.73ms
step:2295/2330 train_time:134785ms step_avg:58.73ms
step:2296/2330 train_time:134846ms step_avg:58.73ms
step:2297/2330 train_time:134903ms step_avg:58.73ms
step:2298/2330 train_time:134963ms step_avg:58.73ms
step:2299/2330 train_time:135021ms step_avg:58.73ms
step:2300/2330 train_time:135081ms step_avg:58.73ms
step:2301/2330 train_time:135139ms step_avg:58.73ms
step:2302/2330 train_time:135199ms step_avg:58.73ms
step:2303/2330 train_time:135258ms step_avg:58.73ms
step:2304/2330 train_time:135319ms step_avg:58.73ms
step:2305/2330 train_time:135377ms step_avg:58.73ms
step:2306/2330 train_time:135439ms step_avg:58.73ms
step:2307/2330 train_time:135496ms step_avg:58.73ms
step:2308/2330 train_time:135559ms step_avg:58.73ms
step:2309/2330 train_time:135616ms step_avg:58.73ms
step:2310/2330 train_time:135679ms step_avg:58.74ms
step:2311/2330 train_time:135735ms step_avg:58.73ms
step:2312/2330 train_time:135796ms step_avg:58.74ms
step:2313/2330 train_time:135853ms step_avg:58.73ms
step:2314/2330 train_time:135914ms step_avg:58.74ms
step:2315/2330 train_time:135970ms step_avg:58.73ms
step:2316/2330 train_time:136031ms step_avg:58.74ms
step:2317/2330 train_time:136087ms step_avg:58.73ms
step:2318/2330 train_time:136150ms step_avg:58.74ms
step:2319/2330 train_time:136206ms step_avg:58.73ms
step:2320/2330 train_time:136269ms step_avg:58.74ms
step:2321/2330 train_time:136327ms step_avg:58.74ms
step:2322/2330 train_time:136387ms step_avg:58.74ms
step:2323/2330 train_time:136446ms step_avg:58.74ms
step:2324/2330 train_time:136507ms step_avg:58.74ms
step:2325/2330 train_time:136565ms step_avg:58.74ms
step:2326/2330 train_time:136626ms step_avg:58.74ms
step:2327/2330 train_time:136684ms step_avg:58.74ms
step:2328/2330 train_time:136745ms step_avg:58.74ms
step:2329/2330 train_time:136802ms step_avg:58.74ms
step:2330/2330 train_time:136862ms step_avg:58.74ms
step:2330/2330 val_loss:3.6952 train_time:136945ms step_avg:58.77ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
