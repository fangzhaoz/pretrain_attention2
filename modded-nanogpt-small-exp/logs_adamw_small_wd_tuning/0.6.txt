import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 08:25:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:77ms step_avg:77.18ms
step:2/2330 train_time:180ms step_avg:90.03ms
step:3/2330 train_time:198ms step_avg:65.86ms
step:4/2330 train_time:217ms step_avg:54.24ms
step:5/2330 train_time:271ms step_avg:54.21ms
step:6/2330 train_time:329ms step_avg:54.87ms
step:7/2330 train_time:384ms step_avg:54.89ms
step:8/2330 train_time:443ms step_avg:55.35ms
step:9/2330 train_time:498ms step_avg:55.32ms
step:10/2330 train_time:556ms step_avg:55.64ms
step:11/2330 train_time:612ms step_avg:55.67ms
step:12/2330 train_time:671ms step_avg:55.90ms
step:13/2330 train_time:726ms step_avg:55.86ms
step:14/2330 train_time:784ms step_avg:55.97ms
step:15/2330 train_time:839ms step_avg:55.93ms
step:16/2330 train_time:898ms step_avg:56.10ms
step:17/2330 train_time:953ms step_avg:56.08ms
step:18/2330 train_time:1012ms step_avg:56.24ms
step:19/2330 train_time:1071ms step_avg:56.35ms
step:20/2330 train_time:1134ms step_avg:56.70ms
step:21/2330 train_time:1192ms step_avg:56.74ms
step:22/2330 train_time:1252ms step_avg:56.92ms
step:23/2330 train_time:1309ms step_avg:56.92ms
step:24/2330 train_time:1368ms step_avg:56.99ms
step:25/2330 train_time:1423ms step_avg:56.92ms
step:26/2330 train_time:1483ms step_avg:57.02ms
step:27/2330 train_time:1538ms step_avg:56.95ms
step:28/2330 train_time:1596ms step_avg:57.01ms
step:29/2330 train_time:1652ms step_avg:56.96ms
step:30/2330 train_time:1710ms step_avg:57.01ms
step:31/2330 train_time:1766ms step_avg:56.96ms
step:32/2330 train_time:1824ms step_avg:57.00ms
step:33/2330 train_time:1879ms step_avg:56.95ms
step:34/2330 train_time:1938ms step_avg:57.00ms
step:35/2330 train_time:1994ms step_avg:56.97ms
step:36/2330 train_time:2054ms step_avg:57.05ms
step:37/2330 train_time:2112ms step_avg:57.08ms
step:38/2330 train_time:2172ms step_avg:57.15ms
step:39/2330 train_time:2229ms step_avg:57.16ms
step:40/2330 train_time:2288ms step_avg:57.21ms
step:41/2330 train_time:2346ms step_avg:57.21ms
step:42/2330 train_time:2404ms step_avg:57.25ms
step:43/2330 train_time:2460ms step_avg:57.20ms
step:44/2330 train_time:2519ms step_avg:57.24ms
step:45/2330 train_time:2574ms step_avg:57.20ms
step:46/2330 train_time:2632ms step_avg:57.23ms
step:47/2330 train_time:2688ms step_avg:57.18ms
step:48/2330 train_time:2747ms step_avg:57.23ms
step:49/2330 train_time:2803ms step_avg:57.19ms
step:50/2330 train_time:2861ms step_avg:57.23ms
step:51/2330 train_time:2917ms step_avg:57.19ms
step:52/2330 train_time:2975ms step_avg:57.22ms
step:53/2330 train_time:3031ms step_avg:57.20ms
step:54/2330 train_time:3091ms step_avg:57.24ms
step:55/2330 train_time:3147ms step_avg:57.22ms
step:56/2330 train_time:3209ms step_avg:57.30ms
step:57/2330 train_time:3265ms step_avg:57.28ms
step:58/2330 train_time:3325ms step_avg:57.32ms
step:59/2330 train_time:3381ms step_avg:57.31ms
step:60/2330 train_time:3440ms step_avg:57.33ms
step:61/2330 train_time:3495ms step_avg:57.30ms
step:62/2330 train_time:3554ms step_avg:57.33ms
step:63/2330 train_time:3610ms step_avg:57.30ms
step:64/2330 train_time:3668ms step_avg:57.32ms
step:65/2330 train_time:3724ms step_avg:57.29ms
step:66/2330 train_time:3783ms step_avg:57.31ms
step:67/2330 train_time:3838ms step_avg:57.28ms
step:68/2330 train_time:3897ms step_avg:57.31ms
step:69/2330 train_time:3953ms step_avg:57.29ms
step:70/2330 train_time:4012ms step_avg:57.31ms
step:71/2330 train_time:4068ms step_avg:57.29ms
step:72/2330 train_time:4127ms step_avg:57.31ms
step:73/2330 train_time:4183ms step_avg:57.30ms
step:74/2330 train_time:4243ms step_avg:57.34ms
step:75/2330 train_time:4300ms step_avg:57.33ms
step:76/2330 train_time:4359ms step_avg:57.36ms
step:77/2330 train_time:4415ms step_avg:57.34ms
step:78/2330 train_time:4474ms step_avg:57.36ms
step:79/2330 train_time:4529ms step_avg:57.33ms
step:80/2330 train_time:4588ms step_avg:57.36ms
step:81/2330 train_time:4644ms step_avg:57.33ms
step:82/2330 train_time:4702ms step_avg:57.35ms
step:83/2330 train_time:4758ms step_avg:57.32ms
step:84/2330 train_time:4817ms step_avg:57.34ms
step:85/2330 train_time:4872ms step_avg:57.32ms
step:86/2330 train_time:4931ms step_avg:57.34ms
step:87/2330 train_time:4987ms step_avg:57.32ms
step:88/2330 train_time:5047ms step_avg:57.35ms
step:89/2330 train_time:5103ms step_avg:57.34ms
step:90/2330 train_time:5161ms step_avg:57.35ms
step:91/2330 train_time:5217ms step_avg:57.33ms
step:92/2330 train_time:5277ms step_avg:57.36ms
step:93/2330 train_time:5334ms step_avg:57.35ms
step:94/2330 train_time:5392ms step_avg:57.36ms
step:95/2330 train_time:5448ms step_avg:57.35ms
step:96/2330 train_time:5507ms step_avg:57.37ms
step:97/2330 train_time:5563ms step_avg:57.35ms
step:98/2330 train_time:5622ms step_avg:57.37ms
step:99/2330 train_time:5677ms step_avg:57.35ms
step:100/2330 train_time:5736ms step_avg:57.36ms
step:101/2330 train_time:5792ms step_avg:57.35ms
step:102/2330 train_time:5850ms step_avg:57.35ms
step:103/2330 train_time:5906ms step_avg:57.34ms
step:104/2330 train_time:5964ms step_avg:57.35ms
step:105/2330 train_time:6020ms step_avg:57.34ms
step:106/2330 train_time:6079ms step_avg:57.35ms
step:107/2330 train_time:6135ms step_avg:57.34ms
step:108/2330 train_time:6193ms step_avg:57.35ms
step:109/2330 train_time:6249ms step_avg:57.33ms
step:110/2330 train_time:6309ms step_avg:57.35ms
step:111/2330 train_time:6364ms step_avg:57.33ms
step:112/2330 train_time:6424ms step_avg:57.36ms
step:113/2330 train_time:6479ms step_avg:57.34ms
step:114/2330 train_time:6540ms step_avg:57.37ms
step:115/2330 train_time:6595ms step_avg:57.35ms
step:116/2330 train_time:6654ms step_avg:57.36ms
step:117/2330 train_time:6710ms step_avg:57.35ms
step:118/2330 train_time:6768ms step_avg:57.36ms
step:119/2330 train_time:6824ms step_avg:57.34ms
step:120/2330 train_time:6883ms step_avg:57.36ms
step:121/2330 train_time:6939ms step_avg:57.34ms
step:122/2330 train_time:6997ms step_avg:57.35ms
step:123/2330 train_time:7053ms step_avg:57.34ms
step:124/2330 train_time:7112ms step_avg:57.35ms
step:125/2330 train_time:7167ms step_avg:57.34ms
step:126/2330 train_time:7227ms step_avg:57.36ms
step:127/2330 train_time:7283ms step_avg:57.34ms
step:128/2330 train_time:7342ms step_avg:57.36ms
step:129/2330 train_time:7398ms step_avg:57.35ms
step:130/2330 train_time:7458ms step_avg:57.37ms
step:131/2330 train_time:7513ms step_avg:57.35ms
step:132/2330 train_time:7573ms step_avg:57.37ms
step:133/2330 train_time:7628ms step_avg:57.35ms
step:134/2330 train_time:7687ms step_avg:57.37ms
step:135/2330 train_time:7743ms step_avg:57.36ms
step:136/2330 train_time:7803ms step_avg:57.38ms
step:137/2330 train_time:7858ms step_avg:57.36ms
step:138/2330 train_time:7918ms step_avg:57.38ms
step:139/2330 train_time:7974ms step_avg:57.36ms
step:140/2330 train_time:8033ms step_avg:57.38ms
step:141/2330 train_time:8089ms step_avg:57.37ms
step:142/2330 train_time:8148ms step_avg:57.38ms
step:143/2330 train_time:8204ms step_avg:57.37ms
step:144/2330 train_time:8263ms step_avg:57.38ms
step:145/2330 train_time:8319ms step_avg:57.37ms
step:146/2330 train_time:8379ms step_avg:57.39ms
step:147/2330 train_time:8434ms step_avg:57.38ms
step:148/2330 train_time:8494ms step_avg:57.39ms
step:149/2330 train_time:8549ms step_avg:57.38ms
step:150/2330 train_time:8608ms step_avg:57.39ms
step:151/2330 train_time:8664ms step_avg:57.38ms
step:152/2330 train_time:8723ms step_avg:57.39ms
step:153/2330 train_time:8779ms step_avg:57.38ms
step:154/2330 train_time:8838ms step_avg:57.39ms
step:155/2330 train_time:8894ms step_avg:57.38ms
step:156/2330 train_time:8953ms step_avg:57.39ms
step:157/2330 train_time:9009ms step_avg:57.38ms
step:158/2330 train_time:9067ms step_avg:57.39ms
step:159/2330 train_time:9123ms step_avg:57.38ms
step:160/2330 train_time:9182ms step_avg:57.39ms
step:161/2330 train_time:9238ms step_avg:57.38ms
step:162/2330 train_time:9297ms step_avg:57.39ms
step:163/2330 train_time:9354ms step_avg:57.38ms
step:164/2330 train_time:9413ms step_avg:57.39ms
step:165/2330 train_time:9470ms step_avg:57.39ms
step:166/2330 train_time:9528ms step_avg:57.40ms
step:167/2330 train_time:9584ms step_avg:57.39ms
step:168/2330 train_time:9643ms step_avg:57.40ms
step:169/2330 train_time:9699ms step_avg:57.39ms
step:170/2330 train_time:9758ms step_avg:57.40ms
step:171/2330 train_time:9814ms step_avg:57.39ms
step:172/2330 train_time:9872ms step_avg:57.40ms
step:173/2330 train_time:9929ms step_avg:57.39ms
step:174/2330 train_time:9987ms step_avg:57.40ms
step:175/2330 train_time:10043ms step_avg:57.39ms
step:176/2330 train_time:10102ms step_avg:57.40ms
step:177/2330 train_time:10157ms step_avg:57.38ms
step:178/2330 train_time:10217ms step_avg:57.40ms
step:179/2330 train_time:10273ms step_avg:57.39ms
step:180/2330 train_time:10332ms step_avg:57.40ms
step:181/2330 train_time:10387ms step_avg:57.39ms
step:182/2330 train_time:10447ms step_avg:57.40ms
step:183/2330 train_time:10503ms step_avg:57.39ms
step:184/2330 train_time:10562ms step_avg:57.40ms
step:185/2330 train_time:10617ms step_avg:57.39ms
step:186/2330 train_time:10677ms step_avg:57.40ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10793ms step_avg:57.41ms
step:189/2330 train_time:10849ms step_avg:57.40ms
step:190/2330 train_time:10907ms step_avg:57.41ms
step:191/2330 train_time:10963ms step_avg:57.40ms
step:192/2330 train_time:11022ms step_avg:57.41ms
step:193/2330 train_time:11078ms step_avg:57.40ms
step:194/2330 train_time:11137ms step_avg:57.41ms
step:195/2330 train_time:11194ms step_avg:57.40ms
step:196/2330 train_time:11252ms step_avg:57.41ms
step:197/2330 train_time:11307ms step_avg:57.40ms
step:198/2330 train_time:11368ms step_avg:57.41ms
step:199/2330 train_time:11423ms step_avg:57.40ms
step:200/2330 train_time:11483ms step_avg:57.41ms
step:201/2330 train_time:11538ms step_avg:57.40ms
step:202/2330 train_time:11597ms step_avg:57.41ms
step:203/2330 train_time:11652ms step_avg:57.40ms
step:204/2330 train_time:11712ms step_avg:57.41ms
step:205/2330 train_time:11767ms step_avg:57.40ms
step:206/2330 train_time:11827ms step_avg:57.41ms
step:207/2330 train_time:11883ms step_avg:57.41ms
step:208/2330 train_time:11943ms step_avg:57.42ms
step:209/2330 train_time:11999ms step_avg:57.41ms
step:210/2330 train_time:12057ms step_avg:57.42ms
step:211/2330 train_time:12113ms step_avg:57.41ms
step:212/2330 train_time:12172ms step_avg:57.42ms
step:213/2330 train_time:12228ms step_avg:57.41ms
step:214/2330 train_time:12288ms step_avg:57.42ms
step:215/2330 train_time:12344ms step_avg:57.41ms
step:216/2330 train_time:12403ms step_avg:57.42ms
step:217/2330 train_time:12458ms step_avg:57.41ms
step:218/2330 train_time:12517ms step_avg:57.42ms
step:219/2330 train_time:12573ms step_avg:57.41ms
step:220/2330 train_time:12632ms step_avg:57.42ms
step:221/2330 train_time:12688ms step_avg:57.41ms
step:222/2330 train_time:12747ms step_avg:57.42ms
step:223/2330 train_time:12803ms step_avg:57.41ms
step:224/2330 train_time:12862ms step_avg:57.42ms
step:225/2330 train_time:12918ms step_avg:57.41ms
step:226/2330 train_time:12977ms step_avg:57.42ms
step:227/2330 train_time:13032ms step_avg:57.41ms
step:228/2330 train_time:13091ms step_avg:57.42ms
step:229/2330 train_time:13147ms step_avg:57.41ms
step:230/2330 train_time:13206ms step_avg:57.42ms
step:231/2330 train_time:13262ms step_avg:57.41ms
step:232/2330 train_time:13322ms step_avg:57.42ms
step:233/2330 train_time:13378ms step_avg:57.41ms
step:234/2330 train_time:13437ms step_avg:57.42ms
step:235/2330 train_time:13492ms step_avg:57.41ms
step:236/2330 train_time:13552ms step_avg:57.42ms
step:237/2330 train_time:13608ms step_avg:57.42ms
step:238/2330 train_time:13667ms step_avg:57.43ms
step:239/2330 train_time:13723ms step_avg:57.42ms
step:240/2330 train_time:13783ms step_avg:57.43ms
step:241/2330 train_time:13838ms step_avg:57.42ms
step:242/2330 train_time:13898ms step_avg:57.43ms
step:243/2330 train_time:13954ms step_avg:57.42ms
step:244/2330 train_time:14013ms step_avg:57.43ms
step:245/2330 train_time:14068ms step_avg:57.42ms
step:246/2330 train_time:14128ms step_avg:57.43ms
step:247/2330 train_time:14184ms step_avg:57.42ms
step:248/2330 train_time:14243ms step_avg:57.43ms
step:249/2330 train_time:14299ms step_avg:57.42ms
step:250/2330 train_time:14359ms step_avg:57.43ms
step:250/2330 val_loss:4.8948 train_time:14438ms step_avg:57.75ms
step:251/2330 train_time:14455ms step_avg:57.59ms
step:252/2330 train_time:14475ms step_avg:57.44ms
step:253/2330 train_time:14531ms step_avg:57.44ms
step:254/2330 train_time:14595ms step_avg:57.46ms
step:255/2330 train_time:14650ms step_avg:57.45ms
step:256/2330 train_time:14715ms step_avg:57.48ms
step:257/2330 train_time:14770ms step_avg:57.47ms
step:258/2330 train_time:14830ms step_avg:57.48ms
step:259/2330 train_time:14885ms step_avg:57.47ms
step:260/2330 train_time:14944ms step_avg:57.48ms
step:261/2330 train_time:14998ms step_avg:57.47ms
step:262/2330 train_time:15058ms step_avg:57.47ms
step:263/2330 train_time:15113ms step_avg:57.46ms
step:264/2330 train_time:15172ms step_avg:57.47ms
step:265/2330 train_time:15227ms step_avg:57.46ms
step:266/2330 train_time:15285ms step_avg:57.46ms
step:267/2330 train_time:15340ms step_avg:57.45ms
step:268/2330 train_time:15399ms step_avg:57.46ms
step:269/2330 train_time:15456ms step_avg:57.46ms
step:270/2330 train_time:15517ms step_avg:57.47ms
step:271/2330 train_time:15573ms step_avg:57.46ms
step:272/2330 train_time:15634ms step_avg:57.48ms
step:273/2330 train_time:15690ms step_avg:57.47ms
step:274/2330 train_time:15750ms step_avg:57.48ms
step:275/2330 train_time:15807ms step_avg:57.48ms
step:276/2330 train_time:15866ms step_avg:57.48ms
step:277/2330 train_time:15921ms step_avg:57.48ms
step:278/2330 train_time:15980ms step_avg:57.48ms
step:279/2330 train_time:16035ms step_avg:57.47ms
step:280/2330 train_time:16094ms step_avg:57.48ms
step:281/2330 train_time:16149ms step_avg:57.47ms
step:282/2330 train_time:16209ms step_avg:57.48ms
step:283/2330 train_time:16264ms step_avg:57.47ms
step:284/2330 train_time:16323ms step_avg:57.48ms
step:285/2330 train_time:16379ms step_avg:57.47ms
step:286/2330 train_time:16438ms step_avg:57.48ms
step:287/2330 train_time:16494ms step_avg:57.47ms
step:288/2330 train_time:16555ms step_avg:57.48ms
step:289/2330 train_time:16610ms step_avg:57.48ms
step:290/2330 train_time:16672ms step_avg:57.49ms
step:291/2330 train_time:16729ms step_avg:57.49ms
step:292/2330 train_time:16787ms step_avg:57.49ms
step:293/2330 train_time:16844ms step_avg:57.49ms
step:294/2330 train_time:16903ms step_avg:57.49ms
step:295/2330 train_time:16959ms step_avg:57.49ms
step:296/2330 train_time:17018ms step_avg:57.49ms
step:297/2330 train_time:17073ms step_avg:57.48ms
step:298/2330 train_time:17132ms step_avg:57.49ms
step:299/2330 train_time:17188ms step_avg:57.48ms
step:300/2330 train_time:17246ms step_avg:57.49ms
step:301/2330 train_time:17302ms step_avg:57.48ms
step:302/2330 train_time:17361ms step_avg:57.49ms
step:303/2330 train_time:17417ms step_avg:57.48ms
step:304/2330 train_time:17476ms step_avg:57.49ms
step:305/2330 train_time:17531ms step_avg:57.48ms
step:306/2330 train_time:17592ms step_avg:57.49ms
step:307/2330 train_time:17647ms step_avg:57.48ms
step:308/2330 train_time:17708ms step_avg:57.49ms
step:309/2330 train_time:17764ms step_avg:57.49ms
step:310/2330 train_time:17823ms step_avg:57.49ms
step:311/2330 train_time:17879ms step_avg:57.49ms
step:312/2330 train_time:17938ms step_avg:57.49ms
step:313/2330 train_time:17994ms step_avg:57.49ms
step:314/2330 train_time:18053ms step_avg:57.49ms
step:315/2330 train_time:18108ms step_avg:57.49ms
step:316/2330 train_time:18167ms step_avg:57.49ms
step:317/2330 train_time:18223ms step_avg:57.48ms
step:318/2330 train_time:18282ms step_avg:57.49ms
step:319/2330 train_time:18337ms step_avg:57.48ms
step:320/2330 train_time:18398ms step_avg:57.49ms
step:321/2330 train_time:18453ms step_avg:57.49ms
step:322/2330 train_time:18513ms step_avg:57.49ms
step:323/2330 train_time:18568ms step_avg:57.49ms
step:324/2330 train_time:18628ms step_avg:57.49ms
step:325/2330 train_time:18684ms step_avg:57.49ms
step:326/2330 train_time:18743ms step_avg:57.49ms
step:327/2330 train_time:18799ms step_avg:57.49ms
step:328/2330 train_time:18859ms step_avg:57.50ms
step:329/2330 train_time:18915ms step_avg:57.49ms
step:330/2330 train_time:18976ms step_avg:57.50ms
step:331/2330 train_time:19031ms step_avg:57.49ms
step:332/2330 train_time:19090ms step_avg:57.50ms
step:333/2330 train_time:19145ms step_avg:57.49ms
step:334/2330 train_time:19205ms step_avg:57.50ms
step:335/2330 train_time:19260ms step_avg:57.49ms
step:336/2330 train_time:19319ms step_avg:57.50ms
step:337/2330 train_time:19376ms step_avg:57.49ms
step:338/2330 train_time:19434ms step_avg:57.50ms
step:339/2330 train_time:19490ms step_avg:57.49ms
step:340/2330 train_time:19549ms step_avg:57.50ms
step:341/2330 train_time:19605ms step_avg:57.49ms
step:342/2330 train_time:19664ms step_avg:57.50ms
step:343/2330 train_time:19720ms step_avg:57.49ms
step:344/2330 train_time:19779ms step_avg:57.50ms
step:345/2330 train_time:19835ms step_avg:57.49ms
step:346/2330 train_time:19894ms step_avg:57.50ms
step:347/2330 train_time:19950ms step_avg:57.49ms
step:348/2330 train_time:20009ms step_avg:57.50ms
step:349/2330 train_time:20065ms step_avg:57.49ms
step:350/2330 train_time:20124ms step_avg:57.50ms
step:351/2330 train_time:20179ms step_avg:57.49ms
step:352/2330 train_time:20239ms step_avg:57.50ms
step:353/2330 train_time:20295ms step_avg:57.49ms
step:354/2330 train_time:20353ms step_avg:57.50ms
step:355/2330 train_time:20409ms step_avg:57.49ms
step:356/2330 train_time:20468ms step_avg:57.50ms
step:357/2330 train_time:20524ms step_avg:57.49ms
step:358/2330 train_time:20583ms step_avg:57.49ms
step:359/2330 train_time:20639ms step_avg:57.49ms
step:360/2330 train_time:20698ms step_avg:57.50ms
step:361/2330 train_time:20754ms step_avg:57.49ms
step:362/2330 train_time:20814ms step_avg:57.50ms
step:363/2330 train_time:20869ms step_avg:57.49ms
step:364/2330 train_time:20928ms step_avg:57.50ms
step:365/2330 train_time:20985ms step_avg:57.49ms
step:366/2330 train_time:21044ms step_avg:57.50ms
step:367/2330 train_time:21100ms step_avg:57.49ms
step:368/2330 train_time:21159ms step_avg:57.50ms
step:369/2330 train_time:21214ms step_avg:57.49ms
step:370/2330 train_time:21274ms step_avg:57.50ms
step:371/2330 train_time:21329ms step_avg:57.49ms
step:372/2330 train_time:21389ms step_avg:57.50ms
step:373/2330 train_time:21445ms step_avg:57.49ms
step:374/2330 train_time:21504ms step_avg:57.50ms
step:375/2330 train_time:21560ms step_avg:57.49ms
step:376/2330 train_time:21619ms step_avg:57.50ms
step:377/2330 train_time:21675ms step_avg:57.49ms
step:378/2330 train_time:21735ms step_avg:57.50ms
step:379/2330 train_time:21791ms step_avg:57.50ms
step:380/2330 train_time:21850ms step_avg:57.50ms
step:381/2330 train_time:21908ms step_avg:57.50ms
step:382/2330 train_time:21966ms step_avg:57.50ms
step:383/2330 train_time:22023ms step_avg:57.50ms
step:384/2330 train_time:22081ms step_avg:57.50ms
step:385/2330 train_time:22137ms step_avg:57.50ms
step:386/2330 train_time:22196ms step_avg:57.50ms
step:387/2330 train_time:22252ms step_avg:57.50ms
step:388/2330 train_time:22311ms step_avg:57.50ms
step:389/2330 train_time:22367ms step_avg:57.50ms
step:390/2330 train_time:22426ms step_avg:57.50ms
step:391/2330 train_time:22482ms step_avg:57.50ms
step:392/2330 train_time:22541ms step_avg:57.50ms
step:393/2330 train_time:22597ms step_avg:57.50ms
step:394/2330 train_time:22656ms step_avg:57.50ms
step:395/2330 train_time:22712ms step_avg:57.50ms
step:396/2330 train_time:22771ms step_avg:57.50ms
step:397/2330 train_time:22827ms step_avg:57.50ms
step:398/2330 train_time:22886ms step_avg:57.50ms
step:399/2330 train_time:22942ms step_avg:57.50ms
step:400/2330 train_time:23002ms step_avg:57.51ms
step:401/2330 train_time:23058ms step_avg:57.50ms
step:402/2330 train_time:23117ms step_avg:57.51ms
step:403/2330 train_time:23173ms step_avg:57.50ms
step:404/2330 train_time:23232ms step_avg:57.51ms
step:405/2330 train_time:23288ms step_avg:57.50ms
step:406/2330 train_time:23347ms step_avg:57.51ms
step:407/2330 train_time:23403ms step_avg:57.50ms
step:408/2330 train_time:23462ms step_avg:57.50ms
step:409/2330 train_time:23518ms step_avg:57.50ms
step:410/2330 train_time:23577ms step_avg:57.50ms
step:411/2330 train_time:23633ms step_avg:57.50ms
step:412/2330 train_time:23692ms step_avg:57.51ms
step:413/2330 train_time:23748ms step_avg:57.50ms
step:414/2330 train_time:23806ms step_avg:57.50ms
step:415/2330 train_time:23863ms step_avg:57.50ms
step:416/2330 train_time:23922ms step_avg:57.50ms
step:417/2330 train_time:23978ms step_avg:57.50ms
step:418/2330 train_time:24037ms step_avg:57.50ms
step:419/2330 train_time:24093ms step_avg:57.50ms
step:420/2330 train_time:24152ms step_avg:57.51ms
step:421/2330 train_time:24209ms step_avg:57.50ms
step:422/2330 train_time:24267ms step_avg:57.50ms
step:423/2330 train_time:24323ms step_avg:57.50ms
step:424/2330 train_time:24382ms step_avg:57.50ms
step:425/2330 train_time:24437ms step_avg:57.50ms
step:426/2330 train_time:24497ms step_avg:57.50ms
step:427/2330 train_time:24553ms step_avg:57.50ms
step:428/2330 train_time:24612ms step_avg:57.50ms
step:429/2330 train_time:24667ms step_avg:57.50ms
step:430/2330 train_time:24726ms step_avg:57.50ms
step:431/2330 train_time:24783ms step_avg:57.50ms
step:432/2330 train_time:24842ms step_avg:57.50ms
step:433/2330 train_time:24898ms step_avg:57.50ms
step:434/2330 train_time:24956ms step_avg:57.50ms
step:435/2330 train_time:25012ms step_avg:57.50ms
step:436/2330 train_time:25072ms step_avg:57.50ms
step:437/2330 train_time:25128ms step_avg:57.50ms
step:438/2330 train_time:25187ms step_avg:57.51ms
step:439/2330 train_time:25243ms step_avg:57.50ms
step:440/2330 train_time:25302ms step_avg:57.50ms
step:441/2330 train_time:25358ms step_avg:57.50ms
step:442/2330 train_time:25417ms step_avg:57.50ms
step:443/2330 train_time:25472ms step_avg:57.50ms
step:444/2330 train_time:25532ms step_avg:57.50ms
step:445/2330 train_time:25588ms step_avg:57.50ms
step:446/2330 train_time:25647ms step_avg:57.51ms
step:447/2330 train_time:25703ms step_avg:57.50ms
step:448/2330 train_time:25762ms step_avg:57.50ms
step:449/2330 train_time:25818ms step_avg:57.50ms
step:450/2330 train_time:25878ms step_avg:57.51ms
step:451/2330 train_time:25934ms step_avg:57.50ms
step:452/2330 train_time:25993ms step_avg:57.51ms
step:453/2330 train_time:26049ms step_avg:57.50ms
step:454/2330 train_time:26108ms step_avg:57.51ms
step:455/2330 train_time:26164ms step_avg:57.50ms
step:456/2330 train_time:26223ms step_avg:57.51ms
step:457/2330 train_time:26280ms step_avg:57.50ms
step:458/2330 train_time:26338ms step_avg:57.51ms
step:459/2330 train_time:26394ms step_avg:57.50ms
step:460/2330 train_time:26453ms step_avg:57.51ms
step:461/2330 train_time:26510ms step_avg:57.50ms
step:462/2330 train_time:26569ms step_avg:57.51ms
step:463/2330 train_time:26625ms step_avg:57.51ms
step:464/2330 train_time:26684ms step_avg:57.51ms
step:465/2330 train_time:26740ms step_avg:57.51ms
step:466/2330 train_time:26800ms step_avg:57.51ms
step:467/2330 train_time:26856ms step_avg:57.51ms
step:468/2330 train_time:26916ms step_avg:57.51ms
step:469/2330 train_time:26971ms step_avg:57.51ms
step:470/2330 train_time:27030ms step_avg:57.51ms
step:471/2330 train_time:27086ms step_avg:57.51ms
step:472/2330 train_time:27145ms step_avg:57.51ms
step:473/2330 train_time:27201ms step_avg:57.51ms
step:474/2330 train_time:27260ms step_avg:57.51ms
step:475/2330 train_time:27317ms step_avg:57.51ms
step:476/2330 train_time:27375ms step_avg:57.51ms
step:477/2330 train_time:27431ms step_avg:57.51ms
step:478/2330 train_time:27491ms step_avg:57.51ms
step:479/2330 train_time:27547ms step_avg:57.51ms
step:480/2330 train_time:27606ms step_avg:57.51ms
step:481/2330 train_time:27662ms step_avg:57.51ms
step:482/2330 train_time:27721ms step_avg:57.51ms
step:483/2330 train_time:27777ms step_avg:57.51ms
step:484/2330 train_time:27836ms step_avg:57.51ms
step:485/2330 train_time:27892ms step_avg:57.51ms
step:486/2330 train_time:27952ms step_avg:57.51ms
step:487/2330 train_time:28007ms step_avg:57.51ms
step:488/2330 train_time:28066ms step_avg:57.51ms
step:489/2330 train_time:28121ms step_avg:57.51ms
step:490/2330 train_time:28181ms step_avg:57.51ms
step:491/2330 train_time:28236ms step_avg:57.51ms
step:492/2330 train_time:28297ms step_avg:57.51ms
step:493/2330 train_time:28353ms step_avg:57.51ms
step:494/2330 train_time:28412ms step_avg:57.51ms
step:495/2330 train_time:28468ms step_avg:57.51ms
step:496/2330 train_time:28527ms step_avg:57.51ms
step:497/2330 train_time:28582ms step_avg:57.51ms
step:498/2330 train_time:28642ms step_avg:57.51ms
step:499/2330 train_time:28699ms step_avg:57.51ms
step:500/2330 train_time:28757ms step_avg:57.51ms
step:500/2330 val_loss:4.4106 train_time:28836ms step_avg:57.67ms
step:501/2330 train_time:28853ms step_avg:57.59ms
step:502/2330 train_time:28875ms step_avg:57.52ms
step:503/2330 train_time:28933ms step_avg:57.52ms
step:504/2330 train_time:28997ms step_avg:57.53ms
step:505/2330 train_time:29054ms step_avg:57.53ms
step:506/2330 train_time:29114ms step_avg:57.54ms
step:507/2330 train_time:29170ms step_avg:57.53ms
step:508/2330 train_time:29229ms step_avg:57.54ms
step:509/2330 train_time:29285ms step_avg:57.53ms
step:510/2330 train_time:29344ms step_avg:57.54ms
step:511/2330 train_time:29399ms step_avg:57.53ms
step:512/2330 train_time:29458ms step_avg:57.54ms
step:513/2330 train_time:29514ms step_avg:57.53ms
step:514/2330 train_time:29572ms step_avg:57.53ms
step:515/2330 train_time:29627ms step_avg:57.53ms
step:516/2330 train_time:29686ms step_avg:57.53ms
step:517/2330 train_time:29741ms step_avg:57.53ms
step:518/2330 train_time:29800ms step_avg:57.53ms
step:519/2330 train_time:29857ms step_avg:57.53ms
step:520/2330 train_time:29917ms step_avg:57.53ms
step:521/2330 train_time:29974ms step_avg:57.53ms
step:522/2330 train_time:30035ms step_avg:57.54ms
step:523/2330 train_time:30091ms step_avg:57.54ms
step:524/2330 train_time:30151ms step_avg:57.54ms
step:525/2330 train_time:30207ms step_avg:57.54ms
step:526/2330 train_time:30266ms step_avg:57.54ms
step:527/2330 train_time:30322ms step_avg:57.54ms
step:528/2330 train_time:30381ms step_avg:57.54ms
step:529/2330 train_time:30436ms step_avg:57.53ms
step:530/2330 train_time:30495ms step_avg:57.54ms
step:531/2330 train_time:30551ms step_avg:57.53ms
step:532/2330 train_time:30609ms step_avg:57.54ms
step:533/2330 train_time:30664ms step_avg:57.53ms
step:534/2330 train_time:30723ms step_avg:57.53ms
step:535/2330 train_time:30779ms step_avg:57.53ms
step:536/2330 train_time:30838ms step_avg:57.53ms
step:537/2330 train_time:30895ms step_avg:57.53ms
step:538/2330 train_time:30954ms step_avg:57.54ms
step:539/2330 train_time:31011ms step_avg:57.53ms
step:540/2330 train_time:31070ms step_avg:57.54ms
step:541/2330 train_time:31127ms step_avg:57.54ms
step:542/2330 train_time:31186ms step_avg:57.54ms
step:543/2330 train_time:31242ms step_avg:57.54ms
step:544/2330 train_time:31301ms step_avg:57.54ms
step:545/2330 train_time:31357ms step_avg:57.54ms
step:546/2330 train_time:31416ms step_avg:57.54ms
step:547/2330 train_time:31472ms step_avg:57.54ms
step:548/2330 train_time:31531ms step_avg:57.54ms
step:549/2330 train_time:31586ms step_avg:57.53ms
step:550/2330 train_time:31645ms step_avg:57.54ms
step:551/2330 train_time:31700ms step_avg:57.53ms
step:552/2330 train_time:31760ms step_avg:57.54ms
step:553/2330 train_time:31816ms step_avg:57.53ms
step:554/2330 train_time:31876ms step_avg:57.54ms
step:555/2330 train_time:31932ms step_avg:57.53ms
step:556/2330 train_time:31991ms step_avg:57.54ms
step:557/2330 train_time:32047ms step_avg:57.54ms
step:558/2330 train_time:32107ms step_avg:57.54ms
step:559/2330 train_time:32164ms step_avg:57.54ms
step:560/2330 train_time:32223ms step_avg:57.54ms
step:561/2330 train_time:32279ms step_avg:57.54ms
step:562/2330 train_time:32340ms step_avg:57.54ms
step:563/2330 train_time:32395ms step_avg:57.54ms
step:564/2330 train_time:32454ms step_avg:57.54ms
step:565/2330 train_time:32510ms step_avg:57.54ms
step:566/2330 train_time:32568ms step_avg:57.54ms
step:567/2330 train_time:32624ms step_avg:57.54ms
step:568/2330 train_time:32683ms step_avg:57.54ms
step:569/2330 train_time:32739ms step_avg:57.54ms
step:570/2330 train_time:32799ms step_avg:57.54ms
step:571/2330 train_time:32854ms step_avg:57.54ms
step:572/2330 train_time:32914ms step_avg:57.54ms
step:573/2330 train_time:32970ms step_avg:57.54ms
step:574/2330 train_time:33029ms step_avg:57.54ms
step:575/2330 train_time:33085ms step_avg:57.54ms
step:576/2330 train_time:33144ms step_avg:57.54ms
step:577/2330 train_time:33200ms step_avg:57.54ms
step:578/2330 train_time:33260ms step_avg:57.54ms
step:579/2330 train_time:33316ms step_avg:57.54ms
step:580/2330 train_time:33375ms step_avg:57.54ms
step:581/2330 train_time:33431ms step_avg:57.54ms
step:582/2330 train_time:33490ms step_avg:57.54ms
step:583/2330 train_time:33546ms step_avg:57.54ms
step:584/2330 train_time:33605ms step_avg:57.54ms
step:585/2330 train_time:33661ms step_avg:57.54ms
step:586/2330 train_time:33719ms step_avg:57.54ms
step:587/2330 train_time:33774ms step_avg:57.54ms
step:588/2330 train_time:33834ms step_avg:57.54ms
step:589/2330 train_time:33890ms step_avg:57.54ms
step:590/2330 train_time:33950ms step_avg:57.54ms
step:591/2330 train_time:34006ms step_avg:57.54ms
step:592/2330 train_time:34065ms step_avg:57.54ms
step:593/2330 train_time:34121ms step_avg:57.54ms
step:594/2330 train_time:34180ms step_avg:57.54ms
step:595/2330 train_time:34236ms step_avg:57.54ms
step:596/2330 train_time:34295ms step_avg:57.54ms
step:597/2330 train_time:34351ms step_avg:57.54ms
step:598/2330 train_time:34411ms step_avg:57.54ms
step:599/2330 train_time:34466ms step_avg:57.54ms
step:600/2330 train_time:34526ms step_avg:57.54ms
step:601/2330 train_time:34582ms step_avg:57.54ms
step:602/2330 train_time:34641ms step_avg:57.54ms
step:603/2330 train_time:34697ms step_avg:57.54ms
step:604/2330 train_time:34757ms step_avg:57.54ms
step:605/2330 train_time:34812ms step_avg:57.54ms
step:606/2330 train_time:34871ms step_avg:57.54ms
step:607/2330 train_time:34927ms step_avg:57.54ms
step:608/2330 train_time:34986ms step_avg:57.54ms
step:609/2330 train_time:35042ms step_avg:57.54ms
step:610/2330 train_time:35101ms step_avg:57.54ms
step:611/2330 train_time:35157ms step_avg:57.54ms
step:612/2330 train_time:35216ms step_avg:57.54ms
step:613/2330 train_time:35273ms step_avg:57.54ms
step:614/2330 train_time:35331ms step_avg:57.54ms
step:615/2330 train_time:35387ms step_avg:57.54ms
step:616/2330 train_time:35446ms step_avg:57.54ms
step:617/2330 train_time:35502ms step_avg:57.54ms
step:618/2330 train_time:35562ms step_avg:57.54ms
step:619/2330 train_time:35618ms step_avg:57.54ms
step:620/2330 train_time:35677ms step_avg:57.54ms
step:621/2330 train_time:35734ms step_avg:57.54ms
step:622/2330 train_time:35792ms step_avg:57.54ms
step:623/2330 train_time:35849ms step_avg:57.54ms
step:624/2330 train_time:35907ms step_avg:57.54ms
step:625/2330 train_time:35963ms step_avg:57.54ms
step:626/2330 train_time:36022ms step_avg:57.54ms
step:627/2330 train_time:36077ms step_avg:57.54ms
step:628/2330 train_time:36137ms step_avg:57.54ms
step:629/2330 train_time:36194ms step_avg:57.54ms
step:630/2330 train_time:36252ms step_avg:57.54ms
step:631/2330 train_time:36308ms step_avg:57.54ms
step:632/2330 train_time:36367ms step_avg:57.54ms
step:633/2330 train_time:36423ms step_avg:57.54ms
step:634/2330 train_time:36483ms step_avg:57.54ms
step:635/2330 train_time:36538ms step_avg:57.54ms
step:636/2330 train_time:36598ms step_avg:57.54ms
step:637/2330 train_time:36654ms step_avg:57.54ms
step:638/2330 train_time:36713ms step_avg:57.54ms
step:639/2330 train_time:36769ms step_avg:57.54ms
step:640/2330 train_time:36828ms step_avg:57.54ms
step:641/2330 train_time:36884ms step_avg:57.54ms
step:642/2330 train_time:36944ms step_avg:57.54ms
step:643/2330 train_time:37000ms step_avg:57.54ms
step:644/2330 train_time:37059ms step_avg:57.55ms
step:645/2330 train_time:37115ms step_avg:57.54ms
step:646/2330 train_time:37174ms step_avg:57.54ms
step:647/2330 train_time:37230ms step_avg:57.54ms
step:648/2330 train_time:37289ms step_avg:57.54ms
step:649/2330 train_time:37345ms step_avg:57.54ms
step:650/2330 train_time:37404ms step_avg:57.54ms
step:651/2330 train_time:37460ms step_avg:57.54ms
step:652/2330 train_time:37519ms step_avg:57.54ms
step:653/2330 train_time:37575ms step_avg:57.54ms
step:654/2330 train_time:37635ms step_avg:57.55ms
step:655/2330 train_time:37691ms step_avg:57.54ms
step:656/2330 train_time:37750ms step_avg:57.55ms
step:657/2330 train_time:37806ms step_avg:57.54ms
step:658/2330 train_time:37866ms step_avg:57.55ms
step:659/2330 train_time:37922ms step_avg:57.54ms
step:660/2330 train_time:37981ms step_avg:57.55ms
step:661/2330 train_time:38037ms step_avg:57.54ms
step:662/2330 train_time:38097ms step_avg:57.55ms
step:663/2330 train_time:38153ms step_avg:57.55ms
step:664/2330 train_time:38211ms step_avg:57.55ms
step:665/2330 train_time:38268ms step_avg:57.55ms
step:666/2330 train_time:38326ms step_avg:57.55ms
step:667/2330 train_time:38383ms step_avg:57.55ms
step:668/2330 train_time:38442ms step_avg:57.55ms
step:669/2330 train_time:38497ms step_avg:57.54ms
step:670/2330 train_time:38558ms step_avg:57.55ms
step:671/2330 train_time:38613ms step_avg:57.55ms
step:672/2330 train_time:38672ms step_avg:57.55ms
step:673/2330 train_time:38729ms step_avg:57.55ms
step:674/2330 train_time:38787ms step_avg:57.55ms
step:675/2330 train_time:38843ms step_avg:57.55ms
step:676/2330 train_time:38903ms step_avg:57.55ms
step:677/2330 train_time:38959ms step_avg:57.55ms
step:678/2330 train_time:39018ms step_avg:57.55ms
step:679/2330 train_time:39074ms step_avg:57.55ms
step:680/2330 train_time:39133ms step_avg:57.55ms
step:681/2330 train_time:39189ms step_avg:57.55ms
step:682/2330 train_time:39248ms step_avg:57.55ms
step:683/2330 train_time:39304ms step_avg:57.55ms
step:684/2330 train_time:39364ms step_avg:57.55ms
step:685/2330 train_time:39419ms step_avg:57.55ms
step:686/2330 train_time:39478ms step_avg:57.55ms
step:687/2330 train_time:39535ms step_avg:57.55ms
step:688/2330 train_time:39594ms step_avg:57.55ms
step:689/2330 train_time:39650ms step_avg:57.55ms
step:690/2330 train_time:39709ms step_avg:57.55ms
step:691/2330 train_time:39765ms step_avg:57.55ms
step:692/2330 train_time:39825ms step_avg:57.55ms
step:693/2330 train_time:39881ms step_avg:57.55ms
step:694/2330 train_time:39941ms step_avg:57.55ms
step:695/2330 train_time:39997ms step_avg:57.55ms
step:696/2330 train_time:40056ms step_avg:57.55ms
step:697/2330 train_time:40113ms step_avg:57.55ms
step:698/2330 train_time:40172ms step_avg:57.55ms
step:699/2330 train_time:40228ms step_avg:57.55ms
step:700/2330 train_time:40287ms step_avg:57.55ms
step:701/2330 train_time:40343ms step_avg:57.55ms
step:702/2330 train_time:40403ms step_avg:57.55ms
step:703/2330 train_time:40458ms step_avg:57.55ms
step:704/2330 train_time:40519ms step_avg:57.56ms
step:705/2330 train_time:40575ms step_avg:57.55ms
step:706/2330 train_time:40635ms step_avg:57.56ms
step:707/2330 train_time:40691ms step_avg:57.55ms
step:708/2330 train_time:40749ms step_avg:57.56ms
step:709/2330 train_time:40806ms step_avg:57.55ms
step:710/2330 train_time:40865ms step_avg:57.56ms
step:711/2330 train_time:40921ms step_avg:57.55ms
step:712/2330 train_time:40980ms step_avg:57.56ms
step:713/2330 train_time:41036ms step_avg:57.55ms
step:714/2330 train_time:41096ms step_avg:57.56ms
step:715/2330 train_time:41152ms step_avg:57.56ms
step:716/2330 train_time:41211ms step_avg:57.56ms
step:717/2330 train_time:41267ms step_avg:57.56ms
step:718/2330 train_time:41327ms step_avg:57.56ms
step:719/2330 train_time:41382ms step_avg:57.56ms
step:720/2330 train_time:41442ms step_avg:57.56ms
step:721/2330 train_time:41498ms step_avg:57.56ms
step:722/2330 train_time:41558ms step_avg:57.56ms
step:723/2330 train_time:41613ms step_avg:57.56ms
step:724/2330 train_time:41672ms step_avg:57.56ms
step:725/2330 train_time:41728ms step_avg:57.56ms
step:726/2330 train_time:41788ms step_avg:57.56ms
step:727/2330 train_time:41844ms step_avg:57.56ms
step:728/2330 train_time:41904ms step_avg:57.56ms
step:729/2330 train_time:41959ms step_avg:57.56ms
step:730/2330 train_time:42019ms step_avg:57.56ms
step:731/2330 train_time:42075ms step_avg:57.56ms
step:732/2330 train_time:42134ms step_avg:57.56ms
step:733/2330 train_time:42190ms step_avg:57.56ms
step:734/2330 train_time:42249ms step_avg:57.56ms
step:735/2330 train_time:42305ms step_avg:57.56ms
step:736/2330 train_time:42364ms step_avg:57.56ms
step:737/2330 train_time:42420ms step_avg:57.56ms
step:738/2330 train_time:42480ms step_avg:57.56ms
step:739/2330 train_time:42536ms step_avg:57.56ms
step:740/2330 train_time:42595ms step_avg:57.56ms
step:741/2330 train_time:42651ms step_avg:57.56ms
step:742/2330 train_time:42711ms step_avg:57.56ms
step:743/2330 train_time:42767ms step_avg:57.56ms
step:744/2330 train_time:42825ms step_avg:57.56ms
step:745/2330 train_time:42882ms step_avg:57.56ms
step:746/2330 train_time:42941ms step_avg:57.56ms
step:747/2330 train_time:42997ms step_avg:57.56ms
step:748/2330 train_time:43057ms step_avg:57.56ms
step:749/2330 train_time:43113ms step_avg:57.56ms
step:750/2330 train_time:43172ms step_avg:57.56ms
step:750/2330 val_loss:4.2116 train_time:43251ms step_avg:57.67ms
step:751/2330 train_time:43269ms step_avg:57.62ms
step:752/2330 train_time:43290ms step_avg:57.57ms
step:753/2330 train_time:43347ms step_avg:57.57ms
step:754/2330 train_time:43409ms step_avg:57.57ms
step:755/2330 train_time:43467ms step_avg:57.57ms
step:756/2330 train_time:43527ms step_avg:57.58ms
step:757/2330 train_time:43584ms step_avg:57.58ms
step:758/2330 train_time:43643ms step_avg:57.58ms
step:759/2330 train_time:43699ms step_avg:57.57ms
step:760/2330 train_time:43758ms step_avg:57.58ms
step:761/2330 train_time:43813ms step_avg:57.57ms
step:762/2330 train_time:43872ms step_avg:57.57ms
step:763/2330 train_time:43927ms step_avg:57.57ms
step:764/2330 train_time:43986ms step_avg:57.57ms
step:765/2330 train_time:44043ms step_avg:57.57ms
step:766/2330 train_time:44101ms step_avg:57.57ms
step:767/2330 train_time:44158ms step_avg:57.57ms
step:768/2330 train_time:44218ms step_avg:57.57ms
step:769/2330 train_time:44275ms step_avg:57.58ms
step:770/2330 train_time:44338ms step_avg:57.58ms
step:771/2330 train_time:44394ms step_avg:57.58ms
step:772/2330 train_time:44456ms step_avg:57.59ms
step:773/2330 train_time:44513ms step_avg:57.58ms
step:774/2330 train_time:44574ms step_avg:57.59ms
step:775/2330 train_time:44631ms step_avg:57.59ms
step:776/2330 train_time:44691ms step_avg:57.59ms
step:777/2330 train_time:44748ms step_avg:57.59ms
step:778/2330 train_time:44808ms step_avg:57.59ms
step:779/2330 train_time:44864ms step_avg:57.59ms
step:780/2330 train_time:44923ms step_avg:57.59ms
step:781/2330 train_time:44980ms step_avg:57.59ms
step:782/2330 train_time:45039ms step_avg:57.59ms
step:783/2330 train_time:45096ms step_avg:57.59ms
step:784/2330 train_time:45155ms step_avg:57.60ms
step:785/2330 train_time:45211ms step_avg:57.59ms
step:786/2330 train_time:45272ms step_avg:57.60ms
step:787/2330 train_time:45330ms step_avg:57.60ms
step:788/2330 train_time:45390ms step_avg:57.60ms
step:789/2330 train_time:45448ms step_avg:57.60ms
step:790/2330 train_time:45508ms step_avg:57.61ms
step:791/2330 train_time:45565ms step_avg:57.60ms
step:792/2330 train_time:45626ms step_avg:57.61ms
step:793/2330 train_time:45683ms step_avg:57.61ms
step:794/2330 train_time:45743ms step_avg:57.61ms
step:795/2330 train_time:45799ms step_avg:57.61ms
step:796/2330 train_time:45859ms step_avg:57.61ms
step:797/2330 train_time:45916ms step_avg:57.61ms
step:798/2330 train_time:45975ms step_avg:57.61ms
step:799/2330 train_time:46032ms step_avg:57.61ms
step:800/2330 train_time:46092ms step_avg:57.62ms
step:801/2330 train_time:46150ms step_avg:57.61ms
step:802/2330 train_time:46209ms step_avg:57.62ms
step:803/2330 train_time:46266ms step_avg:57.62ms
step:804/2330 train_time:46326ms step_avg:57.62ms
step:805/2330 train_time:46384ms step_avg:57.62ms
step:806/2330 train_time:46444ms step_avg:57.62ms
step:807/2330 train_time:46501ms step_avg:57.62ms
step:808/2330 train_time:46562ms step_avg:57.63ms
step:809/2330 train_time:46618ms step_avg:57.62ms
step:810/2330 train_time:46680ms step_avg:57.63ms
step:811/2330 train_time:46737ms step_avg:57.63ms
step:812/2330 train_time:46796ms step_avg:57.63ms
step:813/2330 train_time:46853ms step_avg:57.63ms
step:814/2330 train_time:46913ms step_avg:57.63ms
step:815/2330 train_time:46970ms step_avg:57.63ms
step:816/2330 train_time:47029ms step_avg:57.63ms
step:817/2330 train_time:47086ms step_avg:57.63ms
step:818/2330 train_time:47145ms step_avg:57.63ms
step:819/2330 train_time:47202ms step_avg:57.63ms
step:820/2330 train_time:47263ms step_avg:57.64ms
step:821/2330 train_time:47320ms step_avg:57.64ms
step:822/2330 train_time:47381ms step_avg:57.64ms
step:823/2330 train_time:47438ms step_avg:57.64ms
step:824/2330 train_time:47498ms step_avg:57.64ms
step:825/2330 train_time:47555ms step_avg:57.64ms
step:826/2330 train_time:47615ms step_avg:57.65ms
step:827/2330 train_time:47672ms step_avg:57.64ms
step:828/2330 train_time:47732ms step_avg:57.65ms
step:829/2330 train_time:47790ms step_avg:57.65ms
step:830/2330 train_time:47850ms step_avg:57.65ms
step:831/2330 train_time:47907ms step_avg:57.65ms
step:832/2330 train_time:47966ms step_avg:57.65ms
step:833/2330 train_time:48024ms step_avg:57.65ms
step:834/2330 train_time:48082ms step_avg:57.65ms
step:835/2330 train_time:48139ms step_avg:57.65ms
step:836/2330 train_time:48200ms step_avg:57.66ms
step:837/2330 train_time:48257ms step_avg:57.65ms
step:838/2330 train_time:48317ms step_avg:57.66ms
step:839/2330 train_time:48374ms step_avg:57.66ms
step:840/2330 train_time:48434ms step_avg:57.66ms
step:841/2330 train_time:48490ms step_avg:57.66ms
step:842/2330 train_time:48550ms step_avg:57.66ms
step:843/2330 train_time:48607ms step_avg:57.66ms
step:844/2330 train_time:48667ms step_avg:57.66ms
step:845/2330 train_time:48725ms step_avg:57.66ms
step:846/2330 train_time:48784ms step_avg:57.66ms
step:847/2330 train_time:48841ms step_avg:57.66ms
step:848/2330 train_time:48902ms step_avg:57.67ms
step:849/2330 train_time:48959ms step_avg:57.67ms
step:850/2330 train_time:49019ms step_avg:57.67ms
step:851/2330 train_time:49075ms step_avg:57.67ms
step:852/2330 train_time:49135ms step_avg:57.67ms
step:853/2330 train_time:49191ms step_avg:57.67ms
step:854/2330 train_time:49251ms step_avg:57.67ms
step:855/2330 train_time:49309ms step_avg:57.67ms
step:856/2330 train_time:49369ms step_avg:57.67ms
step:857/2330 train_time:49426ms step_avg:57.67ms
step:858/2330 train_time:49486ms step_avg:57.68ms
step:859/2330 train_time:49543ms step_avg:57.68ms
step:860/2330 train_time:49603ms step_avg:57.68ms
step:861/2330 train_time:49660ms step_avg:57.68ms
step:862/2330 train_time:49720ms step_avg:57.68ms
step:863/2330 train_time:49776ms step_avg:57.68ms
step:864/2330 train_time:49837ms step_avg:57.68ms
step:865/2330 train_time:49894ms step_avg:57.68ms
step:866/2330 train_time:49954ms step_avg:57.68ms
step:867/2330 train_time:50011ms step_avg:57.68ms
step:868/2330 train_time:50070ms step_avg:57.68ms
step:869/2330 train_time:50127ms step_avg:57.68ms
step:870/2330 train_time:50188ms step_avg:57.69ms
step:871/2330 train_time:50245ms step_avg:57.69ms
step:872/2330 train_time:50305ms step_avg:57.69ms
step:873/2330 train_time:50362ms step_avg:57.69ms
step:874/2330 train_time:50422ms step_avg:57.69ms
step:875/2330 train_time:50479ms step_avg:57.69ms
step:876/2330 train_time:50538ms step_avg:57.69ms
step:877/2330 train_time:50595ms step_avg:57.69ms
step:878/2330 train_time:50656ms step_avg:57.69ms
step:879/2330 train_time:50712ms step_avg:57.69ms
step:880/2330 train_time:50773ms step_avg:57.70ms
step:881/2330 train_time:50829ms step_avg:57.70ms
step:882/2330 train_time:50890ms step_avg:57.70ms
step:883/2330 train_time:50948ms step_avg:57.70ms
step:884/2330 train_time:51007ms step_avg:57.70ms
step:885/2330 train_time:51064ms step_avg:57.70ms
step:886/2330 train_time:51125ms step_avg:57.70ms
step:887/2330 train_time:51181ms step_avg:57.70ms
step:888/2330 train_time:51241ms step_avg:57.70ms
step:889/2330 train_time:51298ms step_avg:57.70ms
step:890/2330 train_time:51359ms step_avg:57.71ms
step:891/2330 train_time:51416ms step_avg:57.71ms
step:892/2330 train_time:51475ms step_avg:57.71ms
step:893/2330 train_time:51531ms step_avg:57.71ms
step:894/2330 train_time:51592ms step_avg:57.71ms
step:895/2330 train_time:51650ms step_avg:57.71ms
step:896/2330 train_time:51710ms step_avg:57.71ms
step:897/2330 train_time:51767ms step_avg:57.71ms
step:898/2330 train_time:51827ms step_avg:57.71ms
step:899/2330 train_time:51884ms step_avg:57.71ms
step:900/2330 train_time:51944ms step_avg:57.72ms
step:901/2330 train_time:52000ms step_avg:57.71ms
step:902/2330 train_time:52060ms step_avg:57.72ms
step:903/2330 train_time:52117ms step_avg:57.71ms
step:904/2330 train_time:52177ms step_avg:57.72ms
step:905/2330 train_time:52234ms step_avg:57.72ms
step:906/2330 train_time:52294ms step_avg:57.72ms
step:907/2330 train_time:52351ms step_avg:57.72ms
step:908/2330 train_time:52411ms step_avg:57.72ms
step:909/2330 train_time:52467ms step_avg:57.72ms
step:910/2330 train_time:52527ms step_avg:57.72ms
step:911/2330 train_time:52584ms step_avg:57.72ms
step:912/2330 train_time:52644ms step_avg:57.72ms
step:913/2330 train_time:52701ms step_avg:57.72ms
step:914/2330 train_time:52761ms step_avg:57.73ms
step:915/2330 train_time:52817ms step_avg:57.72ms
step:916/2330 train_time:52878ms step_avg:57.73ms
step:917/2330 train_time:52935ms step_avg:57.73ms
step:918/2330 train_time:52996ms step_avg:57.73ms
step:919/2330 train_time:53053ms step_avg:57.73ms
step:920/2330 train_time:53112ms step_avg:57.73ms
step:921/2330 train_time:53170ms step_avg:57.73ms
step:922/2330 train_time:53230ms step_avg:57.73ms
step:923/2330 train_time:53287ms step_avg:57.73ms
step:924/2330 train_time:53347ms step_avg:57.74ms
step:925/2330 train_time:53404ms step_avg:57.73ms
step:926/2330 train_time:53463ms step_avg:57.74ms
step:927/2330 train_time:53520ms step_avg:57.73ms
step:928/2330 train_time:53580ms step_avg:57.74ms
step:929/2330 train_time:53636ms step_avg:57.74ms
step:930/2330 train_time:53696ms step_avg:57.74ms
step:931/2330 train_time:53752ms step_avg:57.74ms
step:932/2330 train_time:53813ms step_avg:57.74ms
step:933/2330 train_time:53870ms step_avg:57.74ms
step:934/2330 train_time:53929ms step_avg:57.74ms
step:935/2330 train_time:53986ms step_avg:57.74ms
step:936/2330 train_time:54046ms step_avg:57.74ms
step:937/2330 train_time:54103ms step_avg:57.74ms
step:938/2330 train_time:54163ms step_avg:57.74ms
step:939/2330 train_time:54219ms step_avg:57.74ms
step:940/2330 train_time:54279ms step_avg:57.74ms
step:941/2330 train_time:54336ms step_avg:57.74ms
step:942/2330 train_time:54395ms step_avg:57.74ms
step:943/2330 train_time:54452ms step_avg:57.74ms
step:944/2330 train_time:54513ms step_avg:57.75ms
step:945/2330 train_time:54571ms step_avg:57.75ms
step:946/2330 train_time:54631ms step_avg:57.75ms
step:947/2330 train_time:54688ms step_avg:57.75ms
step:948/2330 train_time:54748ms step_avg:57.75ms
step:949/2330 train_time:54805ms step_avg:57.75ms
step:950/2330 train_time:54866ms step_avg:57.75ms
step:951/2330 train_time:54922ms step_avg:57.75ms
step:952/2330 train_time:54982ms step_avg:57.75ms
step:953/2330 train_time:55039ms step_avg:57.75ms
step:954/2330 train_time:55100ms step_avg:57.76ms
step:955/2330 train_time:55156ms step_avg:57.76ms
step:956/2330 train_time:55217ms step_avg:57.76ms
step:957/2330 train_time:55274ms step_avg:57.76ms
step:958/2330 train_time:55335ms step_avg:57.76ms
step:959/2330 train_time:55392ms step_avg:57.76ms
step:960/2330 train_time:55451ms step_avg:57.76ms
step:961/2330 train_time:55508ms step_avg:57.76ms
step:962/2330 train_time:55568ms step_avg:57.76ms
step:963/2330 train_time:55624ms step_avg:57.76ms
step:964/2330 train_time:55685ms step_avg:57.76ms
step:965/2330 train_time:55742ms step_avg:57.76ms
step:966/2330 train_time:55802ms step_avg:57.77ms
step:967/2330 train_time:55858ms step_avg:57.76ms
step:968/2330 train_time:55919ms step_avg:57.77ms
step:969/2330 train_time:55976ms step_avg:57.77ms
step:970/2330 train_time:56036ms step_avg:57.77ms
step:971/2330 train_time:56094ms step_avg:57.77ms
step:972/2330 train_time:56154ms step_avg:57.77ms
step:973/2330 train_time:56211ms step_avg:57.77ms
step:974/2330 train_time:56271ms step_avg:57.77ms
step:975/2330 train_time:56328ms step_avg:57.77ms
step:976/2330 train_time:56389ms step_avg:57.78ms
step:977/2330 train_time:56447ms step_avg:57.78ms
step:978/2330 train_time:56506ms step_avg:57.78ms
step:979/2330 train_time:56563ms step_avg:57.78ms
step:980/2330 train_time:56623ms step_avg:57.78ms
step:981/2330 train_time:56680ms step_avg:57.78ms
step:982/2330 train_time:56739ms step_avg:57.78ms
step:983/2330 train_time:56797ms step_avg:57.78ms
step:984/2330 train_time:56856ms step_avg:57.78ms
step:985/2330 train_time:56913ms step_avg:57.78ms
step:986/2330 train_time:56973ms step_avg:57.78ms
step:987/2330 train_time:57030ms step_avg:57.78ms
step:988/2330 train_time:57090ms step_avg:57.78ms
step:989/2330 train_time:57148ms step_avg:57.78ms
step:990/2330 train_time:57208ms step_avg:57.79ms
step:991/2330 train_time:57265ms step_avg:57.79ms
step:992/2330 train_time:57325ms step_avg:57.79ms
step:993/2330 train_time:57382ms step_avg:57.79ms
step:994/2330 train_time:57442ms step_avg:57.79ms
step:995/2330 train_time:57499ms step_avg:57.79ms
step:996/2330 train_time:57559ms step_avg:57.79ms
step:997/2330 train_time:57615ms step_avg:57.79ms
step:998/2330 train_time:57676ms step_avg:57.79ms
step:999/2330 train_time:57733ms step_avg:57.79ms
step:1000/2330 train_time:57793ms step_avg:57.79ms
step:1000/2330 val_loss:4.0883 train_time:57873ms step_avg:57.87ms
step:1001/2330 train_time:57892ms step_avg:57.83ms
step:1002/2330 train_time:57912ms step_avg:57.80ms
step:1003/2330 train_time:57966ms step_avg:57.79ms
step:1004/2330 train_time:58029ms step_avg:57.80ms
step:1005/2330 train_time:58085ms step_avg:57.80ms
step:1006/2330 train_time:58148ms step_avg:57.80ms
step:1007/2330 train_time:58204ms step_avg:57.80ms
step:1008/2330 train_time:58264ms step_avg:57.80ms
step:1009/2330 train_time:58319ms step_avg:57.80ms
step:1010/2330 train_time:58379ms step_avg:57.80ms
step:1011/2330 train_time:58435ms step_avg:57.80ms
step:1012/2330 train_time:58493ms step_avg:57.80ms
step:1013/2330 train_time:58549ms step_avg:57.80ms
step:1014/2330 train_time:58609ms step_avg:57.80ms
step:1015/2330 train_time:58665ms step_avg:57.80ms
step:1016/2330 train_time:58725ms step_avg:57.80ms
step:1017/2330 train_time:58787ms step_avg:57.80ms
step:1018/2330 train_time:58851ms step_avg:57.81ms
step:1019/2330 train_time:58909ms step_avg:57.81ms
step:1020/2330 train_time:58971ms step_avg:57.81ms
step:1021/2330 train_time:59028ms step_avg:57.81ms
step:1022/2330 train_time:59088ms step_avg:57.82ms
step:1023/2330 train_time:59144ms step_avg:57.81ms
step:1024/2330 train_time:59204ms step_avg:57.82ms
step:1025/2330 train_time:59260ms step_avg:57.81ms
step:1026/2330 train_time:59320ms step_avg:57.82ms
step:1027/2330 train_time:59376ms step_avg:57.82ms
step:1028/2330 train_time:59435ms step_avg:57.82ms
step:1029/2330 train_time:59491ms step_avg:57.81ms
step:1030/2330 train_time:59551ms step_avg:57.82ms
step:1031/2330 train_time:59607ms step_avg:57.81ms
step:1032/2330 train_time:59667ms step_avg:57.82ms
step:1033/2330 train_time:59725ms step_avg:57.82ms
step:1034/2330 train_time:59786ms step_avg:57.82ms
step:1035/2330 train_time:59844ms step_avg:57.82ms
step:1036/2330 train_time:59905ms step_avg:57.82ms
step:1037/2330 train_time:59963ms step_avg:57.82ms
step:1038/2330 train_time:60024ms step_avg:57.83ms
step:1039/2330 train_time:60081ms step_avg:57.83ms
step:1040/2330 train_time:60142ms step_avg:57.83ms
step:1041/2330 train_time:60198ms step_avg:57.83ms
step:1042/2330 train_time:60258ms step_avg:57.83ms
step:1043/2330 train_time:60315ms step_avg:57.83ms
step:1044/2330 train_time:60374ms step_avg:57.83ms
step:1045/2330 train_time:60431ms step_avg:57.83ms
step:1046/2330 train_time:60490ms step_avg:57.83ms
step:1047/2330 train_time:60547ms step_avg:57.83ms
step:1048/2330 train_time:60606ms step_avg:57.83ms
step:1049/2330 train_time:60663ms step_avg:57.83ms
step:1050/2330 train_time:60723ms step_avg:57.83ms
step:1051/2330 train_time:60780ms step_avg:57.83ms
step:1052/2330 train_time:60842ms step_avg:57.83ms
step:1053/2330 train_time:60898ms step_avg:57.83ms
step:1054/2330 train_time:60961ms step_avg:57.84ms
step:1055/2330 train_time:61017ms step_avg:57.84ms
step:1056/2330 train_time:61080ms step_avg:57.84ms
step:1057/2330 train_time:61136ms step_avg:57.84ms
step:1058/2330 train_time:61196ms step_avg:57.84ms
step:1059/2330 train_time:61253ms step_avg:57.84ms
step:1060/2330 train_time:61313ms step_avg:57.84ms
step:1061/2330 train_time:61369ms step_avg:57.84ms
step:1062/2330 train_time:61429ms step_avg:57.84ms
step:1063/2330 train_time:61485ms step_avg:57.84ms
step:1064/2330 train_time:61545ms step_avg:57.84ms
step:1065/2330 train_time:61601ms step_avg:57.84ms
step:1066/2330 train_time:61661ms step_avg:57.84ms
step:1067/2330 train_time:61718ms step_avg:57.84ms
step:1068/2330 train_time:61778ms step_avg:57.84ms
step:1069/2330 train_time:61835ms step_avg:57.84ms
step:1070/2330 train_time:61897ms step_avg:57.85ms
step:1071/2330 train_time:61954ms step_avg:57.85ms
step:1072/2330 train_time:62014ms step_avg:57.85ms
step:1073/2330 train_time:62072ms step_avg:57.85ms
step:1074/2330 train_time:62132ms step_avg:57.85ms
step:1075/2330 train_time:62189ms step_avg:57.85ms
step:1076/2330 train_time:62248ms step_avg:57.85ms
step:1077/2330 train_time:62304ms step_avg:57.85ms
step:1078/2330 train_time:62364ms step_avg:57.85ms
step:1079/2330 train_time:62420ms step_avg:57.85ms
step:1080/2330 train_time:62480ms step_avg:57.85ms
step:1081/2330 train_time:62537ms step_avg:57.85ms
step:1082/2330 train_time:62597ms step_avg:57.85ms
step:1083/2330 train_time:62654ms step_avg:57.85ms
step:1084/2330 train_time:62714ms step_avg:57.85ms
step:1085/2330 train_time:62772ms step_avg:57.85ms
step:1086/2330 train_time:62832ms step_avg:57.86ms
step:1087/2330 train_time:62891ms step_avg:57.86ms
step:1088/2330 train_time:62950ms step_avg:57.86ms
step:1089/2330 train_time:63008ms step_avg:57.86ms
step:1090/2330 train_time:63069ms step_avg:57.86ms
step:1091/2330 train_time:63126ms step_avg:57.86ms
step:1092/2330 train_time:63186ms step_avg:57.86ms
step:1093/2330 train_time:63243ms step_avg:57.86ms
step:1094/2330 train_time:63303ms step_avg:57.86ms
step:1095/2330 train_time:63358ms step_avg:57.86ms
step:1096/2330 train_time:63420ms step_avg:57.86ms
step:1097/2330 train_time:63477ms step_avg:57.86ms
step:1098/2330 train_time:63536ms step_avg:57.87ms
step:1099/2330 train_time:63593ms step_avg:57.86ms
step:1100/2330 train_time:63653ms step_avg:57.87ms
step:1101/2330 train_time:63711ms step_avg:57.87ms
step:1102/2330 train_time:63771ms step_avg:57.87ms
step:1103/2330 train_time:63828ms step_avg:57.87ms
step:1104/2330 train_time:63888ms step_avg:57.87ms
step:1105/2330 train_time:63946ms step_avg:57.87ms
step:1106/2330 train_time:64006ms step_avg:57.87ms
step:1107/2330 train_time:64063ms step_avg:57.87ms
step:1108/2330 train_time:64123ms step_avg:57.87ms
step:1109/2330 train_time:64180ms step_avg:57.87ms
step:1110/2330 train_time:64241ms step_avg:57.87ms
step:1111/2330 train_time:64298ms step_avg:57.87ms
step:1112/2330 train_time:64358ms step_avg:57.88ms
step:1113/2330 train_time:64414ms step_avg:57.87ms
step:1114/2330 train_time:64475ms step_avg:57.88ms
step:1115/2330 train_time:64531ms step_avg:57.88ms
step:1116/2330 train_time:64591ms step_avg:57.88ms
step:1117/2330 train_time:64649ms step_avg:57.88ms
step:1118/2330 train_time:64709ms step_avg:57.88ms
step:1119/2330 train_time:64766ms step_avg:57.88ms
step:1120/2330 train_time:64826ms step_avg:57.88ms
step:1121/2330 train_time:64883ms step_avg:57.88ms
step:1122/2330 train_time:64943ms step_avg:57.88ms
step:1123/2330 train_time:65000ms step_avg:57.88ms
step:1124/2330 train_time:65060ms step_avg:57.88ms
step:1125/2330 train_time:65118ms step_avg:57.88ms
step:1126/2330 train_time:65178ms step_avg:57.88ms
step:1127/2330 train_time:65234ms step_avg:57.88ms
step:1128/2330 train_time:65295ms step_avg:57.89ms
step:1129/2330 train_time:65351ms step_avg:57.88ms
step:1130/2330 train_time:65412ms step_avg:57.89ms
step:1131/2330 train_time:65469ms step_avg:57.89ms
step:1132/2330 train_time:65529ms step_avg:57.89ms
step:1133/2330 train_time:65586ms step_avg:57.89ms
step:1134/2330 train_time:65645ms step_avg:57.89ms
step:1135/2330 train_time:65702ms step_avg:57.89ms
step:1136/2330 train_time:65762ms step_avg:57.89ms
step:1137/2330 train_time:65819ms step_avg:57.89ms
step:1138/2330 train_time:65879ms step_avg:57.89ms
step:1139/2330 train_time:65936ms step_avg:57.89ms
step:1140/2330 train_time:65996ms step_avg:57.89ms
step:1141/2330 train_time:66054ms step_avg:57.89ms
step:1142/2330 train_time:66114ms step_avg:57.89ms
step:1143/2330 train_time:66172ms step_avg:57.89ms
step:1144/2330 train_time:66232ms step_avg:57.89ms
step:1145/2330 train_time:66289ms step_avg:57.89ms
step:1146/2330 train_time:66348ms step_avg:57.90ms
step:1147/2330 train_time:66404ms step_avg:57.89ms
step:1148/2330 train_time:66465ms step_avg:57.90ms
step:1149/2330 train_time:66522ms step_avg:57.90ms
step:1150/2330 train_time:66582ms step_avg:57.90ms
step:1151/2330 train_time:66638ms step_avg:57.90ms
step:1152/2330 train_time:66699ms step_avg:57.90ms
step:1153/2330 train_time:66756ms step_avg:57.90ms
step:1154/2330 train_time:66816ms step_avg:57.90ms
step:1155/2330 train_time:66873ms step_avg:57.90ms
step:1156/2330 train_time:66934ms step_avg:57.90ms
step:1157/2330 train_time:66991ms step_avg:57.90ms
step:1158/2330 train_time:67052ms step_avg:57.90ms
step:1159/2330 train_time:67109ms step_avg:57.90ms
step:1160/2330 train_time:67169ms step_avg:57.90ms
step:1161/2330 train_time:67226ms step_avg:57.90ms
step:1162/2330 train_time:67287ms step_avg:57.91ms
step:1163/2330 train_time:67344ms step_avg:57.91ms
step:1164/2330 train_time:67404ms step_avg:57.91ms
step:1165/2330 train_time:67461ms step_avg:57.91ms
step:1166/2330 train_time:67521ms step_avg:57.91ms
step:1167/2330 train_time:67578ms step_avg:57.91ms
step:1168/2330 train_time:67638ms step_avg:57.91ms
step:1169/2330 train_time:67694ms step_avg:57.91ms
step:1170/2330 train_time:67756ms step_avg:57.91ms
step:1171/2330 train_time:67813ms step_avg:57.91ms
step:1172/2330 train_time:67873ms step_avg:57.91ms
step:1173/2330 train_time:67930ms step_avg:57.91ms
step:1174/2330 train_time:67990ms step_avg:57.91ms
step:1175/2330 train_time:68047ms step_avg:57.91ms
step:1176/2330 train_time:68107ms step_avg:57.91ms
step:1177/2330 train_time:68164ms step_avg:57.91ms
step:1178/2330 train_time:68224ms step_avg:57.92ms
step:1179/2330 train_time:68280ms step_avg:57.91ms
step:1180/2330 train_time:68342ms step_avg:57.92ms
step:1181/2330 train_time:68398ms step_avg:57.92ms
step:1182/2330 train_time:68458ms step_avg:57.92ms
step:1183/2330 train_time:68516ms step_avg:57.92ms
step:1184/2330 train_time:68575ms step_avg:57.92ms
step:1185/2330 train_time:68632ms step_avg:57.92ms
step:1186/2330 train_time:68692ms step_avg:57.92ms
step:1187/2330 train_time:68749ms step_avg:57.92ms
step:1188/2330 train_time:68809ms step_avg:57.92ms
step:1189/2330 train_time:68867ms step_avg:57.92ms
step:1190/2330 train_time:68926ms step_avg:57.92ms
step:1191/2330 train_time:68984ms step_avg:57.92ms
step:1192/2330 train_time:69044ms step_avg:57.92ms
step:1193/2330 train_time:69101ms step_avg:57.92ms
step:1194/2330 train_time:69161ms step_avg:57.92ms
step:1195/2330 train_time:69218ms step_avg:57.92ms
step:1196/2330 train_time:69278ms step_avg:57.92ms
step:1197/2330 train_time:69335ms step_avg:57.92ms
step:1198/2330 train_time:69395ms step_avg:57.93ms
step:1199/2330 train_time:69451ms step_avg:57.92ms
step:1200/2330 train_time:69511ms step_avg:57.93ms
step:1201/2330 train_time:69568ms step_avg:57.93ms
step:1202/2330 train_time:69629ms step_avg:57.93ms
step:1203/2330 train_time:69687ms step_avg:57.93ms
step:1204/2330 train_time:69747ms step_avg:57.93ms
step:1205/2330 train_time:69804ms step_avg:57.93ms
step:1206/2330 train_time:69863ms step_avg:57.93ms
step:1207/2330 train_time:69920ms step_avg:57.93ms
step:1208/2330 train_time:69981ms step_avg:57.93ms
step:1209/2330 train_time:70038ms step_avg:57.93ms
step:1210/2330 train_time:70098ms step_avg:57.93ms
step:1211/2330 train_time:70155ms step_avg:57.93ms
step:1212/2330 train_time:70216ms step_avg:57.93ms
step:1213/2330 train_time:70272ms step_avg:57.93ms
step:1214/2330 train_time:70333ms step_avg:57.94ms
step:1215/2330 train_time:70390ms step_avg:57.93ms
step:1216/2330 train_time:70449ms step_avg:57.94ms
step:1217/2330 train_time:70506ms step_avg:57.93ms
step:1218/2330 train_time:70566ms step_avg:57.94ms
step:1219/2330 train_time:70623ms step_avg:57.94ms
step:1220/2330 train_time:70683ms step_avg:57.94ms
step:1221/2330 train_time:70740ms step_avg:57.94ms
step:1222/2330 train_time:70800ms step_avg:57.94ms
step:1223/2330 train_time:70857ms step_avg:57.94ms
step:1224/2330 train_time:70918ms step_avg:57.94ms
step:1225/2330 train_time:70976ms step_avg:57.94ms
step:1226/2330 train_time:71035ms step_avg:57.94ms
step:1227/2330 train_time:71092ms step_avg:57.94ms
step:1228/2330 train_time:71153ms step_avg:57.94ms
step:1229/2330 train_time:71210ms step_avg:57.94ms
step:1230/2330 train_time:71270ms step_avg:57.94ms
step:1231/2330 train_time:71328ms step_avg:57.94ms
step:1232/2330 train_time:71388ms step_avg:57.94ms
step:1233/2330 train_time:71445ms step_avg:57.94ms
step:1234/2330 train_time:71505ms step_avg:57.95ms
step:1235/2330 train_time:71563ms step_avg:57.95ms
step:1236/2330 train_time:71622ms step_avg:57.95ms
step:1237/2330 train_time:71680ms step_avg:57.95ms
step:1238/2330 train_time:71740ms step_avg:57.95ms
step:1239/2330 train_time:71796ms step_avg:57.95ms
step:1240/2330 train_time:71857ms step_avg:57.95ms
step:1241/2330 train_time:71914ms step_avg:57.95ms
step:1242/2330 train_time:71974ms step_avg:57.95ms
step:1243/2330 train_time:72030ms step_avg:57.95ms
step:1244/2330 train_time:72091ms step_avg:57.95ms
step:1245/2330 train_time:72147ms step_avg:57.95ms
step:1246/2330 train_time:72209ms step_avg:57.95ms
step:1247/2330 train_time:72266ms step_avg:57.95ms
step:1248/2330 train_time:72326ms step_avg:57.95ms
step:1249/2330 train_time:72383ms step_avg:57.95ms
step:1250/2330 train_time:72442ms step_avg:57.95ms
step:1250/2330 val_loss:3.9960 train_time:72523ms step_avg:58.02ms
step:1251/2330 train_time:72541ms step_avg:57.99ms
step:1252/2330 train_time:72561ms step_avg:57.96ms
step:1253/2330 train_time:72618ms step_avg:57.96ms
step:1254/2330 train_time:72683ms step_avg:57.96ms
step:1255/2330 train_time:72740ms step_avg:57.96ms
step:1256/2330 train_time:72805ms step_avg:57.97ms
step:1257/2330 train_time:72862ms step_avg:57.96ms
step:1258/2330 train_time:72922ms step_avg:57.97ms
step:1259/2330 train_time:72978ms step_avg:57.96ms
step:1260/2330 train_time:73038ms step_avg:57.97ms
step:1261/2330 train_time:73094ms step_avg:57.97ms
step:1262/2330 train_time:73154ms step_avg:57.97ms
step:1263/2330 train_time:73210ms step_avg:57.97ms
step:1264/2330 train_time:73270ms step_avg:57.97ms
step:1265/2330 train_time:73326ms step_avg:57.97ms
step:1266/2330 train_time:73386ms step_avg:57.97ms
step:1267/2330 train_time:73443ms step_avg:57.97ms
step:1268/2330 train_time:73504ms step_avg:57.97ms
step:1269/2330 train_time:73562ms step_avg:57.97ms
step:1270/2330 train_time:73625ms step_avg:57.97ms
step:1271/2330 train_time:73683ms step_avg:57.97ms
step:1272/2330 train_time:73745ms step_avg:57.98ms
step:1273/2330 train_time:73801ms step_avg:57.97ms
step:1274/2330 train_time:73864ms step_avg:57.98ms
step:1275/2330 train_time:73920ms step_avg:57.98ms
step:1276/2330 train_time:73981ms step_avg:57.98ms
step:1277/2330 train_time:74037ms step_avg:57.98ms
step:1278/2330 train_time:74097ms step_avg:57.98ms
step:1279/2330 train_time:74153ms step_avg:57.98ms
step:1280/2330 train_time:74214ms step_avg:57.98ms
step:1281/2330 train_time:74270ms step_avg:57.98ms
step:1282/2330 train_time:74331ms step_avg:57.98ms
step:1283/2330 train_time:74387ms step_avg:57.98ms
step:1284/2330 train_time:74447ms step_avg:57.98ms
step:1285/2330 train_time:74505ms step_avg:57.98ms
step:1286/2330 train_time:74565ms step_avg:57.98ms
step:1287/2330 train_time:74622ms step_avg:57.98ms
step:1288/2330 train_time:74683ms step_avg:57.98ms
step:1289/2330 train_time:74739ms step_avg:57.98ms
step:1290/2330 train_time:74801ms step_avg:57.99ms
step:1291/2330 train_time:74859ms step_avg:57.99ms
step:1292/2330 train_time:74919ms step_avg:57.99ms
step:1293/2330 train_time:74977ms step_avg:57.99ms
step:1294/2330 train_time:75036ms step_avg:57.99ms
step:1295/2330 train_time:75093ms step_avg:57.99ms
step:1296/2330 train_time:75153ms step_avg:57.99ms
step:1297/2330 train_time:75209ms step_avg:57.99ms
step:1298/2330 train_time:75269ms step_avg:57.99ms
step:1299/2330 train_time:75326ms step_avg:57.99ms
step:1300/2330 train_time:75385ms step_avg:57.99ms
step:1301/2330 train_time:75442ms step_avg:57.99ms
step:1302/2330 train_time:75501ms step_avg:57.99ms
step:1303/2330 train_time:75558ms step_avg:57.99ms
step:1304/2330 train_time:75620ms step_avg:57.99ms
step:1305/2330 train_time:75678ms step_avg:57.99ms
step:1306/2330 train_time:75739ms step_avg:57.99ms
step:1307/2330 train_time:75796ms step_avg:57.99ms
step:1308/2330 train_time:75857ms step_avg:57.99ms
step:1309/2330 train_time:75913ms step_avg:57.99ms
step:1310/2330 train_time:75974ms step_avg:58.00ms
step:1311/2330 train_time:76032ms step_avg:58.00ms
step:1312/2330 train_time:76092ms step_avg:58.00ms
step:1313/2330 train_time:76149ms step_avg:58.00ms
step:1314/2330 train_time:76208ms step_avg:58.00ms
step:1315/2330 train_time:76265ms step_avg:58.00ms
step:1316/2330 train_time:76324ms step_avg:58.00ms
step:1317/2330 train_time:76382ms step_avg:58.00ms
step:1318/2330 train_time:76441ms step_avg:58.00ms
step:1319/2330 train_time:76497ms step_avg:58.00ms
step:1320/2330 train_time:76558ms step_avg:58.00ms
step:1321/2330 train_time:76615ms step_avg:58.00ms
step:1322/2330 train_time:76676ms step_avg:58.00ms
step:1323/2330 train_time:76733ms step_avg:58.00ms
step:1324/2330 train_time:76794ms step_avg:58.00ms
step:1325/2330 train_time:76851ms step_avg:58.00ms
step:1326/2330 train_time:76911ms step_avg:58.00ms
step:1327/2330 train_time:76968ms step_avg:58.00ms
step:1328/2330 train_time:77029ms step_avg:58.00ms
step:1329/2330 train_time:77086ms step_avg:58.00ms
step:1330/2330 train_time:77146ms step_avg:58.00ms
step:1331/2330 train_time:77203ms step_avg:58.00ms
step:1332/2330 train_time:77262ms step_avg:58.00ms
step:1333/2330 train_time:77318ms step_avg:58.00ms
step:1334/2330 train_time:77379ms step_avg:58.01ms
step:1335/2330 train_time:77436ms step_avg:58.00ms
step:1336/2330 train_time:77496ms step_avg:58.01ms
step:1337/2330 train_time:77553ms step_avg:58.01ms
step:1338/2330 train_time:77615ms step_avg:58.01ms
step:1339/2330 train_time:77673ms step_avg:58.01ms
step:1340/2330 train_time:77733ms step_avg:58.01ms
step:1341/2330 train_time:77789ms step_avg:58.01ms
step:1342/2330 train_time:77851ms step_avg:58.01ms
step:1343/2330 train_time:77908ms step_avg:58.01ms
step:1344/2330 train_time:77968ms step_avg:58.01ms
step:1345/2330 train_time:78025ms step_avg:58.01ms
step:1346/2330 train_time:78085ms step_avg:58.01ms
step:1347/2330 train_time:78141ms step_avg:58.01ms
step:1348/2330 train_time:78202ms step_avg:58.01ms
step:1349/2330 train_time:78259ms step_avg:58.01ms
step:1350/2330 train_time:78319ms step_avg:58.01ms
step:1351/2330 train_time:78376ms step_avg:58.01ms
step:1352/2330 train_time:78436ms step_avg:58.01ms
step:1353/2330 train_time:78493ms step_avg:58.01ms
step:1354/2330 train_time:78553ms step_avg:58.02ms
step:1355/2330 train_time:78610ms step_avg:58.01ms
step:1356/2330 train_time:78671ms step_avg:58.02ms
step:1357/2330 train_time:78728ms step_avg:58.02ms
step:1358/2330 train_time:78788ms step_avg:58.02ms
step:1359/2330 train_time:78846ms step_avg:58.02ms
step:1360/2330 train_time:78906ms step_avg:58.02ms
step:1361/2330 train_time:78962ms step_avg:58.02ms
step:1362/2330 train_time:79023ms step_avg:58.02ms
step:1363/2330 train_time:79080ms step_avg:58.02ms
step:1364/2330 train_time:79140ms step_avg:58.02ms
step:1365/2330 train_time:79197ms step_avg:58.02ms
step:1366/2330 train_time:79257ms step_avg:58.02ms
step:1367/2330 train_time:79313ms step_avg:58.02ms
step:1368/2330 train_time:79373ms step_avg:58.02ms
step:1369/2330 train_time:79430ms step_avg:58.02ms
step:1370/2330 train_time:79490ms step_avg:58.02ms
step:1371/2330 train_time:79548ms step_avg:58.02ms
step:1372/2330 train_time:79608ms step_avg:58.02ms
step:1373/2330 train_time:79665ms step_avg:58.02ms
step:1374/2330 train_time:79725ms step_avg:58.02ms
step:1375/2330 train_time:79783ms step_avg:58.02ms
step:1376/2330 train_time:79843ms step_avg:58.03ms
step:1377/2330 train_time:79900ms step_avg:58.02ms
step:1378/2330 train_time:79960ms step_avg:58.03ms
step:1379/2330 train_time:80016ms step_avg:58.02ms
step:1380/2330 train_time:80078ms step_avg:58.03ms
step:1381/2330 train_time:80134ms step_avg:58.03ms
step:1382/2330 train_time:80196ms step_avg:58.03ms
step:1383/2330 train_time:80252ms step_avg:58.03ms
step:1384/2330 train_time:80312ms step_avg:58.03ms
step:1385/2330 train_time:80369ms step_avg:58.03ms
step:1386/2330 train_time:80429ms step_avg:58.03ms
step:1387/2330 train_time:80487ms step_avg:58.03ms
step:1388/2330 train_time:80547ms step_avg:58.03ms
step:1389/2330 train_time:80604ms step_avg:58.03ms
step:1390/2330 train_time:80664ms step_avg:58.03ms
step:1391/2330 train_time:80721ms step_avg:58.03ms
step:1392/2330 train_time:80781ms step_avg:58.03ms
step:1393/2330 train_time:80840ms step_avg:58.03ms
step:1394/2330 train_time:80900ms step_avg:58.03ms
step:1395/2330 train_time:80957ms step_avg:58.03ms
step:1396/2330 train_time:81017ms step_avg:58.04ms
step:1397/2330 train_time:81074ms step_avg:58.03ms
step:1398/2330 train_time:81134ms step_avg:58.04ms
step:1399/2330 train_time:81191ms step_avg:58.04ms
step:1400/2330 train_time:81251ms step_avg:58.04ms
step:1401/2330 train_time:81308ms step_avg:58.04ms
step:1402/2330 train_time:81368ms step_avg:58.04ms
step:1403/2330 train_time:81425ms step_avg:58.04ms
step:1404/2330 train_time:81485ms step_avg:58.04ms
step:1405/2330 train_time:81542ms step_avg:58.04ms
step:1406/2330 train_time:81602ms step_avg:58.04ms
step:1407/2330 train_time:81659ms step_avg:58.04ms
step:1408/2330 train_time:81720ms step_avg:58.04ms
step:1409/2330 train_time:81777ms step_avg:58.04ms
step:1410/2330 train_time:81837ms step_avg:58.04ms
step:1411/2330 train_time:81894ms step_avg:58.04ms
step:1412/2330 train_time:81955ms step_avg:58.04ms
step:1413/2330 train_time:82012ms step_avg:58.04ms
step:1414/2330 train_time:82073ms step_avg:58.04ms
step:1415/2330 train_time:82130ms step_avg:58.04ms
step:1416/2330 train_time:82190ms step_avg:58.04ms
step:1417/2330 train_time:82247ms step_avg:58.04ms
step:1418/2330 train_time:82306ms step_avg:58.04ms
step:1419/2330 train_time:82363ms step_avg:58.04ms
step:1420/2330 train_time:82423ms step_avg:58.04ms
step:1421/2330 train_time:82480ms step_avg:58.04ms
step:1422/2330 train_time:82540ms step_avg:58.05ms
step:1423/2330 train_time:82598ms step_avg:58.05ms
step:1424/2330 train_time:82658ms step_avg:58.05ms
step:1425/2330 train_time:82715ms step_avg:58.05ms
step:1426/2330 train_time:82775ms step_avg:58.05ms
step:1427/2330 train_time:82832ms step_avg:58.05ms
step:1428/2330 train_time:82892ms step_avg:58.05ms
step:1429/2330 train_time:82950ms step_avg:58.05ms
step:1430/2330 train_time:83010ms step_avg:58.05ms
step:1431/2330 train_time:83067ms step_avg:58.05ms
step:1432/2330 train_time:83127ms step_avg:58.05ms
step:1433/2330 train_time:83184ms step_avg:58.05ms
step:1434/2330 train_time:83244ms step_avg:58.05ms
step:1435/2330 train_time:83302ms step_avg:58.05ms
step:1436/2330 train_time:83362ms step_avg:58.05ms
step:1437/2330 train_time:83418ms step_avg:58.05ms
step:1438/2330 train_time:83479ms step_avg:58.05ms
step:1439/2330 train_time:83536ms step_avg:58.05ms
step:1440/2330 train_time:83596ms step_avg:58.05ms
step:1441/2330 train_time:83652ms step_avg:58.05ms
step:1442/2330 train_time:83712ms step_avg:58.05ms
step:1443/2330 train_time:83769ms step_avg:58.05ms
step:1444/2330 train_time:83829ms step_avg:58.05ms
step:1445/2330 train_time:83887ms step_avg:58.05ms
step:1446/2330 train_time:83947ms step_avg:58.05ms
step:1447/2330 train_time:84004ms step_avg:58.05ms
step:1448/2330 train_time:84064ms step_avg:58.06ms
step:1449/2330 train_time:84120ms step_avg:58.05ms
step:1450/2330 train_time:84181ms step_avg:58.06ms
step:1451/2330 train_time:84237ms step_avg:58.05ms
step:1452/2330 train_time:84299ms step_avg:58.06ms
step:1453/2330 train_time:84356ms step_avg:58.06ms
step:1454/2330 train_time:84416ms step_avg:58.06ms
step:1455/2330 train_time:84473ms step_avg:58.06ms
step:1456/2330 train_time:84534ms step_avg:58.06ms
step:1457/2330 train_time:84591ms step_avg:58.06ms
step:1458/2330 train_time:84650ms step_avg:58.06ms
step:1459/2330 train_time:84707ms step_avg:58.06ms
step:1460/2330 train_time:84767ms step_avg:58.06ms
step:1461/2330 train_time:84824ms step_avg:58.06ms
step:1462/2330 train_time:84883ms step_avg:58.06ms
step:1463/2330 train_time:84941ms step_avg:58.06ms
step:1464/2330 train_time:85001ms step_avg:58.06ms
step:1465/2330 train_time:85058ms step_avg:58.06ms
step:1466/2330 train_time:85119ms step_avg:58.06ms
step:1467/2330 train_time:85176ms step_avg:58.06ms
step:1468/2330 train_time:85237ms step_avg:58.06ms
step:1469/2330 train_time:85293ms step_avg:58.06ms
step:1470/2330 train_time:85355ms step_avg:58.06ms
step:1471/2330 train_time:85412ms step_avg:58.06ms
step:1472/2330 train_time:85472ms step_avg:58.07ms
step:1473/2330 train_time:85529ms step_avg:58.06ms
step:1474/2330 train_time:85589ms step_avg:58.07ms
step:1475/2330 train_time:85646ms step_avg:58.07ms
step:1476/2330 train_time:85706ms step_avg:58.07ms
step:1477/2330 train_time:85763ms step_avg:58.07ms
step:1478/2330 train_time:85822ms step_avg:58.07ms
step:1479/2330 train_time:85879ms step_avg:58.07ms
step:1480/2330 train_time:85940ms step_avg:58.07ms
step:1481/2330 train_time:85997ms step_avg:58.07ms
step:1482/2330 train_time:86057ms step_avg:58.07ms
step:1483/2330 train_time:86114ms step_avg:58.07ms
step:1484/2330 train_time:86174ms step_avg:58.07ms
step:1485/2330 train_time:86231ms step_avg:58.07ms
step:1486/2330 train_time:86291ms step_avg:58.07ms
step:1487/2330 train_time:86348ms step_avg:58.07ms
step:1488/2330 train_time:86408ms step_avg:58.07ms
step:1489/2330 train_time:86466ms step_avg:58.07ms
step:1490/2330 train_time:86526ms step_avg:58.07ms
step:1491/2330 train_time:86583ms step_avg:58.07ms
step:1492/2330 train_time:86643ms step_avg:58.07ms
step:1493/2330 train_time:86700ms step_avg:58.07ms
step:1494/2330 train_time:86760ms step_avg:58.07ms
step:1495/2330 train_time:86817ms step_avg:58.07ms
step:1496/2330 train_time:86878ms step_avg:58.07ms
step:1497/2330 train_time:86935ms step_avg:58.07ms
step:1498/2330 train_time:86996ms step_avg:58.07ms
step:1499/2330 train_time:87053ms step_avg:58.07ms
step:1500/2330 train_time:87113ms step_avg:58.08ms
step:1500/2330 val_loss:3.9164 train_time:87194ms step_avg:58.13ms
step:1501/2330 train_time:87213ms step_avg:58.10ms
step:1502/2330 train_time:87235ms step_avg:58.08ms
step:1503/2330 train_time:87294ms step_avg:58.08ms
step:1504/2330 train_time:87359ms step_avg:58.08ms
step:1505/2330 train_time:87416ms step_avg:58.08ms
step:1506/2330 train_time:87478ms step_avg:58.09ms
step:1507/2330 train_time:87535ms step_avg:58.09ms
step:1508/2330 train_time:87596ms step_avg:58.09ms
step:1509/2330 train_time:87652ms step_avg:58.09ms
step:1510/2330 train_time:87714ms step_avg:58.09ms
step:1511/2330 train_time:87769ms step_avg:58.09ms
step:1512/2330 train_time:87830ms step_avg:58.09ms
step:1513/2330 train_time:87885ms step_avg:58.09ms
step:1514/2330 train_time:87946ms step_avg:58.09ms
step:1515/2330 train_time:88002ms step_avg:58.09ms
step:1516/2330 train_time:88062ms step_avg:58.09ms
step:1517/2330 train_time:88118ms step_avg:58.09ms
step:1518/2330 train_time:88179ms step_avg:58.09ms
step:1519/2330 train_time:88238ms step_avg:58.09ms
step:1520/2330 train_time:88299ms step_avg:58.09ms
step:1521/2330 train_time:88357ms step_avg:58.09ms
step:1522/2330 train_time:88418ms step_avg:58.09ms
step:1523/2330 train_time:88475ms step_avg:58.09ms
step:1524/2330 train_time:88537ms step_avg:58.09ms
step:1525/2330 train_time:88594ms step_avg:58.09ms
step:1526/2330 train_time:88655ms step_avg:58.10ms
step:1527/2330 train_time:88712ms step_avg:58.10ms
step:1528/2330 train_time:88771ms step_avg:58.10ms
step:1529/2330 train_time:88830ms step_avg:58.10ms
step:1530/2330 train_time:88888ms step_avg:58.10ms
step:1531/2330 train_time:88944ms step_avg:58.10ms
step:1532/2330 train_time:89005ms step_avg:58.10ms
step:1533/2330 train_time:89062ms step_avg:58.10ms
step:1534/2330 train_time:89123ms step_avg:58.10ms
step:1535/2330 train_time:89181ms step_avg:58.10ms
step:1536/2330 train_time:89241ms step_avg:58.10ms
step:1537/2330 train_time:89300ms step_avg:58.10ms
step:1538/2330 train_time:89360ms step_avg:58.10ms
step:1539/2330 train_time:89419ms step_avg:58.10ms
step:1540/2330 train_time:89479ms step_avg:58.10ms
step:1541/2330 train_time:89536ms step_avg:58.10ms
step:1542/2330 train_time:89598ms step_avg:58.11ms
step:1543/2330 train_time:89655ms step_avg:58.10ms
step:1544/2330 train_time:89717ms step_avg:58.11ms
step:1545/2330 train_time:89775ms step_avg:58.11ms
step:1546/2330 train_time:89835ms step_avg:58.11ms
step:1547/2330 train_time:89893ms step_avg:58.11ms
step:1548/2330 train_time:89952ms step_avg:58.11ms
step:1549/2330 train_time:90009ms step_avg:58.11ms
step:1550/2330 train_time:90071ms step_avg:58.11ms
step:1551/2330 train_time:90129ms step_avg:58.11ms
step:1552/2330 train_time:90190ms step_avg:58.11ms
step:1553/2330 train_time:90247ms step_avg:58.11ms
step:1554/2330 train_time:90309ms step_avg:58.11ms
step:1555/2330 train_time:90366ms step_avg:58.11ms
step:1556/2330 train_time:90429ms step_avg:58.12ms
step:1557/2330 train_time:90485ms step_avg:58.12ms
step:1558/2330 train_time:90550ms step_avg:58.12ms
step:1559/2330 train_time:90606ms step_avg:58.12ms
step:1560/2330 train_time:90669ms step_avg:58.12ms
step:1561/2330 train_time:90725ms step_avg:58.12ms
step:1562/2330 train_time:90787ms step_avg:58.12ms
step:1563/2330 train_time:90843ms step_avg:58.12ms
step:1564/2330 train_time:90904ms step_avg:58.12ms
step:1565/2330 train_time:90961ms step_avg:58.12ms
step:1566/2330 train_time:91022ms step_avg:58.12ms
step:1567/2330 train_time:91080ms step_avg:58.12ms
step:1568/2330 train_time:91140ms step_avg:58.12ms
step:1569/2330 train_time:91197ms step_avg:58.12ms
step:1570/2330 train_time:91258ms step_avg:58.13ms
step:1571/2330 train_time:91316ms step_avg:58.13ms
step:1572/2330 train_time:91377ms step_avg:58.13ms
step:1573/2330 train_time:91436ms step_avg:58.13ms
step:1574/2330 train_time:91497ms step_avg:58.13ms
step:1575/2330 train_time:91555ms step_avg:58.13ms
step:1576/2330 train_time:91616ms step_avg:58.13ms
step:1577/2330 train_time:91674ms step_avg:58.13ms
step:1578/2330 train_time:91735ms step_avg:58.13ms
step:1579/2330 train_time:91792ms step_avg:58.13ms
step:1580/2330 train_time:91853ms step_avg:58.13ms
step:1581/2330 train_time:91909ms step_avg:58.13ms
step:1582/2330 train_time:91970ms step_avg:58.14ms
step:1583/2330 train_time:92027ms step_avg:58.13ms
step:1584/2330 train_time:92089ms step_avg:58.14ms
step:1585/2330 train_time:92145ms step_avg:58.14ms
step:1586/2330 train_time:92207ms step_avg:58.14ms
step:1587/2330 train_time:92263ms step_avg:58.14ms
step:1588/2330 train_time:92325ms step_avg:58.14ms
step:1589/2330 train_time:92382ms step_avg:58.14ms
step:1590/2330 train_time:92446ms step_avg:58.14ms
step:1591/2330 train_time:92503ms step_avg:58.14ms
step:1592/2330 train_time:92565ms step_avg:58.14ms
step:1593/2330 train_time:92622ms step_avg:58.14ms
step:1594/2330 train_time:92683ms step_avg:58.15ms
step:1595/2330 train_time:92740ms step_avg:58.14ms
step:1596/2330 train_time:92801ms step_avg:58.15ms
step:1597/2330 train_time:92858ms step_avg:58.15ms
step:1598/2330 train_time:92918ms step_avg:58.15ms
step:1599/2330 train_time:92975ms step_avg:58.15ms
step:1600/2330 train_time:93037ms step_avg:58.15ms
step:1601/2330 train_time:93095ms step_avg:58.15ms
step:1602/2330 train_time:93155ms step_avg:58.15ms
step:1603/2330 train_time:93213ms step_avg:58.15ms
step:1604/2330 train_time:93274ms step_avg:58.15ms
step:1605/2330 train_time:93332ms step_avg:58.15ms
step:1606/2330 train_time:93393ms step_avg:58.15ms
step:1607/2330 train_time:93450ms step_avg:58.15ms
step:1608/2330 train_time:93513ms step_avg:58.15ms
step:1609/2330 train_time:93570ms step_avg:58.15ms
step:1610/2330 train_time:93632ms step_avg:58.16ms
step:1611/2330 train_time:93689ms step_avg:58.16ms
step:1612/2330 train_time:93751ms step_avg:58.16ms
step:1613/2330 train_time:93808ms step_avg:58.16ms
step:1614/2330 train_time:93870ms step_avg:58.16ms
step:1615/2330 train_time:93926ms step_avg:58.16ms
step:1616/2330 train_time:93987ms step_avg:58.16ms
step:1617/2330 train_time:94043ms step_avg:58.16ms
step:1618/2330 train_time:94106ms step_avg:58.16ms
step:1619/2330 train_time:94163ms step_avg:58.16ms
step:1620/2330 train_time:94224ms step_avg:58.16ms
step:1621/2330 train_time:94281ms step_avg:58.16ms
step:1622/2330 train_time:94343ms step_avg:58.16ms
step:1623/2330 train_time:94400ms step_avg:58.16ms
step:1624/2330 train_time:94461ms step_avg:58.17ms
step:1625/2330 train_time:94518ms step_avg:58.17ms
step:1626/2330 train_time:94578ms step_avg:58.17ms
step:1627/2330 train_time:94636ms step_avg:58.17ms
step:1628/2330 train_time:94697ms step_avg:58.17ms
step:1629/2330 train_time:94755ms step_avg:58.17ms
step:1630/2330 train_time:94816ms step_avg:58.17ms
step:1631/2330 train_time:94873ms step_avg:58.17ms
step:1632/2330 train_time:94934ms step_avg:58.17ms
step:1633/2330 train_time:94990ms step_avg:58.17ms
step:1634/2330 train_time:95051ms step_avg:58.17ms
step:1635/2330 train_time:95108ms step_avg:58.17ms
step:1636/2330 train_time:95170ms step_avg:58.17ms
step:1637/2330 train_time:95227ms step_avg:58.17ms
step:1638/2330 train_time:95289ms step_avg:58.17ms
step:1639/2330 train_time:95345ms step_avg:58.17ms
step:1640/2330 train_time:95409ms step_avg:58.18ms
step:1641/2330 train_time:95465ms step_avg:58.17ms
step:1642/2330 train_time:95528ms step_avg:58.18ms
step:1643/2330 train_time:95584ms step_avg:58.18ms
step:1644/2330 train_time:95647ms step_avg:58.18ms
step:1645/2330 train_time:95704ms step_avg:58.18ms
step:1646/2330 train_time:95766ms step_avg:58.18ms
step:1647/2330 train_time:95822ms step_avg:58.18ms
step:1648/2330 train_time:95884ms step_avg:58.18ms
step:1649/2330 train_time:95941ms step_avg:58.18ms
step:1650/2330 train_time:96002ms step_avg:58.18ms
step:1651/2330 train_time:96059ms step_avg:58.18ms
step:1652/2330 train_time:96120ms step_avg:58.18ms
step:1653/2330 train_time:96179ms step_avg:58.18ms
step:1654/2330 train_time:96239ms step_avg:58.19ms
step:1655/2330 train_time:96296ms step_avg:58.18ms
step:1656/2330 train_time:96358ms step_avg:58.19ms
step:1657/2330 train_time:96416ms step_avg:58.19ms
step:1658/2330 train_time:96478ms step_avg:58.19ms
step:1659/2330 train_time:96535ms step_avg:58.19ms
step:1660/2330 train_time:96595ms step_avg:58.19ms
step:1661/2330 train_time:96652ms step_avg:58.19ms
step:1662/2330 train_time:96714ms step_avg:58.19ms
step:1663/2330 train_time:96770ms step_avg:58.19ms
step:1664/2330 train_time:96832ms step_avg:58.19ms
step:1665/2330 train_time:96889ms step_avg:58.19ms
step:1666/2330 train_time:96950ms step_avg:58.19ms
step:1667/2330 train_time:97007ms step_avg:58.19ms
step:1668/2330 train_time:97068ms step_avg:58.19ms
step:1669/2330 train_time:97125ms step_avg:58.19ms
step:1670/2330 train_time:97187ms step_avg:58.20ms
step:1671/2330 train_time:97243ms step_avg:58.19ms
step:1672/2330 train_time:97307ms step_avg:58.20ms
step:1673/2330 train_time:97364ms step_avg:58.20ms
step:1674/2330 train_time:97426ms step_avg:58.20ms
step:1675/2330 train_time:97482ms step_avg:58.20ms
step:1676/2330 train_time:97545ms step_avg:58.20ms
step:1677/2330 train_time:97601ms step_avg:58.20ms
step:1678/2330 train_time:97664ms step_avg:58.20ms
step:1679/2330 train_time:97721ms step_avg:58.20ms
step:1680/2330 train_time:97783ms step_avg:58.20ms
step:1681/2330 train_time:97840ms step_avg:58.20ms
step:1682/2330 train_time:97900ms step_avg:58.20ms
step:1683/2330 train_time:97958ms step_avg:58.20ms
step:1684/2330 train_time:98018ms step_avg:58.21ms
step:1685/2330 train_time:98076ms step_avg:58.21ms
step:1686/2330 train_time:98136ms step_avg:58.21ms
step:1687/2330 train_time:98195ms step_avg:58.21ms
step:1688/2330 train_time:98256ms step_avg:58.21ms
step:1689/2330 train_time:98313ms step_avg:58.21ms
step:1690/2330 train_time:98373ms step_avg:58.21ms
step:1691/2330 train_time:98432ms step_avg:58.21ms
step:1692/2330 train_time:98492ms step_avg:58.21ms
step:1693/2330 train_time:98549ms step_avg:58.21ms
step:1694/2330 train_time:98611ms step_avg:58.21ms
step:1695/2330 train_time:98668ms step_avg:58.21ms
step:1696/2330 train_time:98730ms step_avg:58.21ms
step:1697/2330 train_time:98787ms step_avg:58.21ms
step:1698/2330 train_time:98849ms step_avg:58.21ms
step:1699/2330 train_time:98905ms step_avg:58.21ms
step:1700/2330 train_time:98967ms step_avg:58.22ms
step:1701/2330 train_time:99023ms step_avg:58.21ms
step:1702/2330 train_time:99086ms step_avg:58.22ms
step:1703/2330 train_time:99142ms step_avg:58.22ms
step:1704/2330 train_time:99205ms step_avg:58.22ms
step:1705/2330 train_time:99261ms step_avg:58.22ms
step:1706/2330 train_time:99324ms step_avg:58.22ms
step:1707/2330 train_time:99381ms step_avg:58.22ms
step:1708/2330 train_time:99442ms step_avg:58.22ms
step:1709/2330 train_time:99500ms step_avg:58.22ms
step:1710/2330 train_time:99560ms step_avg:58.22ms
step:1711/2330 train_time:99618ms step_avg:58.22ms
step:1712/2330 train_time:99679ms step_avg:58.22ms
step:1713/2330 train_time:99737ms step_avg:58.22ms
step:1714/2330 train_time:99797ms step_avg:58.22ms
step:1715/2330 train_time:99854ms step_avg:58.22ms
step:1716/2330 train_time:99917ms step_avg:58.23ms
step:1717/2330 train_time:99974ms step_avg:58.23ms
step:1718/2330 train_time:100035ms step_avg:58.23ms
step:1719/2330 train_time:100092ms step_avg:58.23ms
step:1720/2330 train_time:100152ms step_avg:58.23ms
step:1721/2330 train_time:100209ms step_avg:58.23ms
step:1722/2330 train_time:100270ms step_avg:58.23ms
step:1723/2330 train_time:100327ms step_avg:58.23ms
step:1724/2330 train_time:100389ms step_avg:58.23ms
step:1725/2330 train_time:100445ms step_avg:58.23ms
step:1726/2330 train_time:100508ms step_avg:58.23ms
step:1727/2330 train_time:100564ms step_avg:58.23ms
step:1728/2330 train_time:100627ms step_avg:58.23ms
step:1729/2330 train_time:100684ms step_avg:58.23ms
step:1730/2330 train_time:100746ms step_avg:58.23ms
step:1731/2330 train_time:100803ms step_avg:58.23ms
step:1732/2330 train_time:100865ms step_avg:58.24ms
step:1733/2330 train_time:100922ms step_avg:58.24ms
step:1734/2330 train_time:100983ms step_avg:58.24ms
step:1735/2330 train_time:101040ms step_avg:58.24ms
step:1736/2330 train_time:101102ms step_avg:58.24ms
step:1737/2330 train_time:101159ms step_avg:58.24ms
step:1738/2330 train_time:101219ms step_avg:58.24ms
step:1739/2330 train_time:101277ms step_avg:58.24ms
step:1740/2330 train_time:101337ms step_avg:58.24ms
step:1741/2330 train_time:101396ms step_avg:58.24ms
step:1742/2330 train_time:101456ms step_avg:58.24ms
step:1743/2330 train_time:101515ms step_avg:58.24ms
step:1744/2330 train_time:101575ms step_avg:58.24ms
step:1745/2330 train_time:101633ms step_avg:58.24ms
step:1746/2330 train_time:101693ms step_avg:58.24ms
step:1747/2330 train_time:101750ms step_avg:58.24ms
step:1748/2330 train_time:101812ms step_avg:58.24ms
step:1749/2330 train_time:101869ms step_avg:58.24ms
step:1750/2330 train_time:101932ms step_avg:58.25ms
step:1750/2330 val_loss:3.8287 train_time:102014ms step_avg:58.29ms
step:1751/2330 train_time:102033ms step_avg:58.27ms
step:1752/2330 train_time:102052ms step_avg:58.25ms
step:1753/2330 train_time:102107ms step_avg:58.25ms
step:1754/2330 train_time:102176ms step_avg:58.25ms
step:1755/2330 train_time:102232ms step_avg:58.25ms
step:1756/2330 train_time:102295ms step_avg:58.25ms
step:1757/2330 train_time:102352ms step_avg:58.25ms
step:1758/2330 train_time:102414ms step_avg:58.26ms
step:1759/2330 train_time:102470ms step_avg:58.25ms
step:1760/2330 train_time:102530ms step_avg:58.26ms
step:1761/2330 train_time:102587ms step_avg:58.25ms
step:1762/2330 train_time:102646ms step_avg:58.26ms
step:1763/2330 train_time:102702ms step_avg:58.25ms
step:1764/2330 train_time:102762ms step_avg:58.26ms
step:1765/2330 train_time:102819ms step_avg:58.25ms
step:1766/2330 train_time:102879ms step_avg:58.26ms
step:1767/2330 train_time:102939ms step_avg:58.26ms
step:1768/2330 train_time:103002ms step_avg:58.26ms
step:1769/2330 train_time:103060ms step_avg:58.26ms
step:1770/2330 train_time:103121ms step_avg:58.26ms
step:1771/2330 train_time:103178ms step_avg:58.26ms
step:1772/2330 train_time:103240ms step_avg:58.26ms
step:1773/2330 train_time:103296ms step_avg:58.26ms
step:1774/2330 train_time:103360ms step_avg:58.26ms
step:1775/2330 train_time:103416ms step_avg:58.26ms
step:1776/2330 train_time:103478ms step_avg:58.26ms
step:1777/2330 train_time:103535ms step_avg:58.26ms
step:1778/2330 train_time:103595ms step_avg:58.26ms
step:1779/2330 train_time:103652ms step_avg:58.26ms
step:1780/2330 train_time:103712ms step_avg:58.26ms
step:1781/2330 train_time:103769ms step_avg:58.26ms
step:1782/2330 train_time:103828ms step_avg:58.26ms
step:1783/2330 train_time:103887ms step_avg:58.27ms
step:1784/2330 train_time:103948ms step_avg:58.27ms
step:1785/2330 train_time:104008ms step_avg:58.27ms
step:1786/2330 train_time:104069ms step_avg:58.27ms
step:1787/2330 train_time:104130ms step_avg:58.27ms
step:1788/2330 train_time:104190ms step_avg:58.27ms
step:1789/2330 train_time:104249ms step_avg:58.27ms
step:1790/2330 train_time:104309ms step_avg:58.27ms
step:1791/2330 train_time:104367ms step_avg:58.27ms
step:1792/2330 train_time:104427ms step_avg:58.27ms
step:1793/2330 train_time:104484ms step_avg:58.27ms
step:1794/2330 train_time:104545ms step_avg:58.27ms
step:1795/2330 train_time:104601ms step_avg:58.27ms
step:1796/2330 train_time:104662ms step_avg:58.27ms
step:1797/2330 train_time:104718ms step_avg:58.27ms
step:1798/2330 train_time:104779ms step_avg:58.28ms
step:1799/2330 train_time:104836ms step_avg:58.27ms
step:1800/2330 train_time:104897ms step_avg:58.28ms
step:1801/2330 train_time:104955ms step_avg:58.28ms
step:1802/2330 train_time:105016ms step_avg:58.28ms
step:1803/2330 train_time:105074ms step_avg:58.28ms
step:1804/2330 train_time:105135ms step_avg:58.28ms
step:1805/2330 train_time:105194ms step_avg:58.28ms
step:1806/2330 train_time:105254ms step_avg:58.28ms
step:1807/2330 train_time:105311ms step_avg:58.28ms
step:1808/2330 train_time:105372ms step_avg:58.28ms
step:1809/2330 train_time:105431ms step_avg:58.28ms
step:1810/2330 train_time:105491ms step_avg:58.28ms
step:1811/2330 train_time:105549ms step_avg:58.28ms
step:1812/2330 train_time:105609ms step_avg:58.28ms
step:1813/2330 train_time:105666ms step_avg:58.28ms
step:1814/2330 train_time:105727ms step_avg:58.28ms
step:1815/2330 train_time:105785ms step_avg:58.28ms
step:1816/2330 train_time:105844ms step_avg:58.28ms
step:1817/2330 train_time:105902ms step_avg:58.28ms
step:1818/2330 train_time:105963ms step_avg:58.29ms
step:1819/2330 train_time:106020ms step_avg:58.28ms
step:1820/2330 train_time:106082ms step_avg:58.29ms
step:1821/2330 train_time:106139ms step_avg:58.29ms
step:1822/2330 train_time:106202ms step_avg:58.29ms
step:1823/2330 train_time:106259ms step_avg:58.29ms
step:1824/2330 train_time:106320ms step_avg:58.29ms
step:1825/2330 train_time:106377ms step_avg:58.29ms
step:1826/2330 train_time:106438ms step_avg:58.29ms
step:1827/2330 train_time:106494ms step_avg:58.29ms
step:1828/2330 train_time:106555ms step_avg:58.29ms
step:1829/2330 train_time:106612ms step_avg:58.29ms
step:1830/2330 train_time:106673ms step_avg:58.29ms
step:1831/2330 train_time:106730ms step_avg:58.29ms
step:1832/2330 train_time:106789ms step_avg:58.29ms
step:1833/2330 train_time:106847ms step_avg:58.29ms
step:1834/2330 train_time:106908ms step_avg:58.29ms
step:1835/2330 train_time:106967ms step_avg:58.29ms
step:1836/2330 train_time:107028ms step_avg:58.29ms
step:1837/2330 train_time:107086ms step_avg:58.29ms
step:1838/2330 train_time:107147ms step_avg:58.30ms
step:1839/2330 train_time:107204ms step_avg:58.29ms
step:1840/2330 train_time:107267ms step_avg:58.30ms
step:1841/2330 train_time:107325ms step_avg:58.30ms
step:1842/2330 train_time:107387ms step_avg:58.30ms
step:1843/2330 train_time:107444ms step_avg:58.30ms
step:1844/2330 train_time:107505ms step_avg:58.30ms
step:1845/2330 train_time:107562ms step_avg:58.30ms
step:1846/2330 train_time:107623ms step_avg:58.30ms
step:1847/2330 train_time:107680ms step_avg:58.30ms
step:1848/2330 train_time:107741ms step_avg:58.30ms
step:1849/2330 train_time:107798ms step_avg:58.30ms
step:1850/2330 train_time:107858ms step_avg:58.30ms
step:1851/2330 train_time:107915ms step_avg:58.30ms
step:1852/2330 train_time:107976ms step_avg:58.30ms
step:1853/2330 train_time:108034ms step_avg:58.30ms
step:1854/2330 train_time:108094ms step_avg:58.30ms
step:1855/2330 train_time:108152ms step_avg:58.30ms
step:1856/2330 train_time:108213ms step_avg:58.30ms
step:1857/2330 train_time:108271ms step_avg:58.30ms
step:1858/2330 train_time:108331ms step_avg:58.30ms
step:1859/2330 train_time:108388ms step_avg:58.30ms
step:1860/2330 train_time:108449ms step_avg:58.31ms
step:1861/2330 train_time:108506ms step_avg:58.31ms
step:1862/2330 train_time:108567ms step_avg:58.31ms
step:1863/2330 train_time:108625ms step_avg:58.31ms
step:1864/2330 train_time:108685ms step_avg:58.31ms
step:1865/2330 train_time:108743ms step_avg:58.31ms
step:1866/2330 train_time:108804ms step_avg:58.31ms
step:1867/2330 train_time:108861ms step_avg:58.31ms
step:1868/2330 train_time:108921ms step_avg:58.31ms
step:1869/2330 train_time:108979ms step_avg:58.31ms
step:1870/2330 train_time:109040ms step_avg:58.31ms
step:1871/2330 train_time:109097ms step_avg:58.31ms
step:1872/2330 train_time:109159ms step_avg:58.31ms
step:1873/2330 train_time:109216ms step_avg:58.31ms
step:1874/2330 train_time:109278ms step_avg:58.31ms
step:1875/2330 train_time:109334ms step_avg:58.31ms
step:1876/2330 train_time:109396ms step_avg:58.31ms
step:1877/2330 train_time:109453ms step_avg:58.31ms
step:1878/2330 train_time:109514ms step_avg:58.31ms
step:1879/2330 train_time:109572ms step_avg:58.31ms
step:1880/2330 train_time:109632ms step_avg:58.31ms
step:1881/2330 train_time:109689ms step_avg:58.31ms
step:1882/2330 train_time:109750ms step_avg:58.32ms
step:1883/2330 train_time:109808ms step_avg:58.32ms
step:1884/2330 train_time:109868ms step_avg:58.32ms
step:1885/2330 train_time:109926ms step_avg:58.32ms
step:1886/2330 train_time:109987ms step_avg:58.32ms
step:1887/2330 train_time:110046ms step_avg:58.32ms
step:1888/2330 train_time:110106ms step_avg:58.32ms
step:1889/2330 train_time:110165ms step_avg:58.32ms
step:1890/2330 train_time:110225ms step_avg:58.32ms
step:1891/2330 train_time:110283ms step_avg:58.32ms
step:1892/2330 train_time:110343ms step_avg:58.32ms
step:1893/2330 train_time:110400ms step_avg:58.32ms
step:1894/2330 train_time:110461ms step_avg:58.32ms
step:1895/2330 train_time:110517ms step_avg:58.32ms
step:1896/2330 train_time:110580ms step_avg:58.32ms
step:1897/2330 train_time:110637ms step_avg:58.32ms
step:1898/2330 train_time:110700ms step_avg:58.32ms
step:1899/2330 train_time:110756ms step_avg:58.32ms
step:1900/2330 train_time:110817ms step_avg:58.32ms
step:1901/2330 train_time:110875ms step_avg:58.32ms
step:1902/2330 train_time:110935ms step_avg:58.33ms
step:1903/2330 train_time:110992ms step_avg:58.32ms
step:1904/2330 train_time:111054ms step_avg:58.33ms
step:1905/2330 train_time:111111ms step_avg:58.33ms
step:1906/2330 train_time:111171ms step_avg:58.33ms
step:1907/2330 train_time:111230ms step_avg:58.33ms
step:1908/2330 train_time:111290ms step_avg:58.33ms
step:1909/2330 train_time:111348ms step_avg:58.33ms
step:1910/2330 train_time:111409ms step_avg:58.33ms
step:1911/2330 train_time:111467ms step_avg:58.33ms
step:1912/2330 train_time:111528ms step_avg:58.33ms
step:1913/2330 train_time:111586ms step_avg:58.33ms
step:1914/2330 train_time:111647ms step_avg:58.33ms
step:1915/2330 train_time:111704ms step_avg:58.33ms
step:1916/2330 train_time:111766ms step_avg:58.33ms
step:1917/2330 train_time:111823ms step_avg:58.33ms
step:1918/2330 train_time:111885ms step_avg:58.33ms
step:1919/2330 train_time:111942ms step_avg:58.33ms
step:1920/2330 train_time:112003ms step_avg:58.33ms
step:1921/2330 train_time:112060ms step_avg:58.33ms
step:1922/2330 train_time:112121ms step_avg:58.34ms
step:1923/2330 train_time:112178ms step_avg:58.33ms
step:1924/2330 train_time:112240ms step_avg:58.34ms
step:1925/2330 train_time:112296ms step_avg:58.34ms
step:1926/2330 train_time:112358ms step_avg:58.34ms
step:1927/2330 train_time:112415ms step_avg:58.34ms
step:1928/2330 train_time:112476ms step_avg:58.34ms
step:1929/2330 train_time:112533ms step_avg:58.34ms
step:1930/2330 train_time:112594ms step_avg:58.34ms
step:1931/2330 train_time:112652ms step_avg:58.34ms
step:1932/2330 train_time:112711ms step_avg:58.34ms
step:1933/2330 train_time:112770ms step_avg:58.34ms
step:1934/2330 train_time:112830ms step_avg:58.34ms
step:1935/2330 train_time:112889ms step_avg:58.34ms
step:1936/2330 train_time:112949ms step_avg:58.34ms
step:1937/2330 train_time:113007ms step_avg:58.34ms
step:1938/2330 train_time:113067ms step_avg:58.34ms
step:1939/2330 train_time:113125ms step_avg:58.34ms
step:1940/2330 train_time:113186ms step_avg:58.34ms
step:1941/2330 train_time:113244ms step_avg:58.34ms
step:1942/2330 train_time:113304ms step_avg:58.34ms
step:1943/2330 train_time:113361ms step_avg:58.34ms
step:1944/2330 train_time:113423ms step_avg:58.35ms
step:1945/2330 train_time:113480ms step_avg:58.34ms
step:1946/2330 train_time:113542ms step_avg:58.35ms
step:1947/2330 train_time:113598ms step_avg:58.35ms
step:1948/2330 train_time:113660ms step_avg:58.35ms
step:1949/2330 train_time:113717ms step_avg:58.35ms
step:1950/2330 train_time:113778ms step_avg:58.35ms
step:1951/2330 train_time:113835ms step_avg:58.35ms
step:1952/2330 train_time:113897ms step_avg:58.35ms
step:1953/2330 train_time:113954ms step_avg:58.35ms
step:1954/2330 train_time:114014ms step_avg:58.35ms
step:1955/2330 train_time:114071ms step_avg:58.35ms
step:1956/2330 train_time:114132ms step_avg:58.35ms
step:1957/2330 train_time:114191ms step_avg:58.35ms
step:1958/2330 train_time:114252ms step_avg:58.35ms
step:1959/2330 train_time:114309ms step_avg:58.35ms
step:1960/2330 train_time:114370ms step_avg:58.35ms
step:1961/2330 train_time:114428ms step_avg:58.35ms
step:1962/2330 train_time:114489ms step_avg:58.35ms
step:1963/2330 train_time:114547ms step_avg:58.35ms
step:1964/2330 train_time:114608ms step_avg:58.35ms
step:1965/2330 train_time:114666ms step_avg:58.35ms
step:1966/2330 train_time:114726ms step_avg:58.35ms
step:1967/2330 train_time:114783ms step_avg:58.35ms
step:1968/2330 train_time:114844ms step_avg:58.36ms
step:1969/2330 train_time:114901ms step_avg:58.35ms
step:1970/2330 train_time:114962ms step_avg:58.36ms
step:1971/2330 train_time:115019ms step_avg:58.36ms
step:1972/2330 train_time:115080ms step_avg:58.36ms
step:1973/2330 train_time:115137ms step_avg:58.36ms
step:1974/2330 train_time:115199ms step_avg:58.36ms
step:1975/2330 train_time:115255ms step_avg:58.36ms
step:1976/2330 train_time:115317ms step_avg:58.36ms
step:1977/2330 train_time:115374ms step_avg:58.36ms
step:1978/2330 train_time:115436ms step_avg:58.36ms
step:1979/2330 train_time:115493ms step_avg:58.36ms
step:1980/2330 train_time:115554ms step_avg:58.36ms
step:1981/2330 train_time:115611ms step_avg:58.36ms
step:1982/2330 train_time:115672ms step_avg:58.36ms
step:1983/2330 train_time:115730ms step_avg:58.36ms
step:1984/2330 train_time:115790ms step_avg:58.36ms
step:1985/2330 train_time:115849ms step_avg:58.36ms
step:1986/2330 train_time:115910ms step_avg:58.36ms
step:1987/2330 train_time:115967ms step_avg:58.36ms
step:1988/2330 train_time:116028ms step_avg:58.36ms
step:1989/2330 train_time:116085ms step_avg:58.36ms
step:1990/2330 train_time:116147ms step_avg:58.37ms
step:1991/2330 train_time:116204ms step_avg:58.36ms
step:1992/2330 train_time:116267ms step_avg:58.37ms
step:1993/2330 train_time:116324ms step_avg:58.37ms
step:1994/2330 train_time:116386ms step_avg:58.37ms
step:1995/2330 train_time:116443ms step_avg:58.37ms
step:1996/2330 train_time:116504ms step_avg:58.37ms
step:1997/2330 train_time:116561ms step_avg:58.37ms
step:1998/2330 train_time:116622ms step_avg:58.37ms
step:1999/2330 train_time:116679ms step_avg:58.37ms
step:2000/2330 train_time:116741ms step_avg:58.37ms
step:2000/2330 val_loss:3.7624 train_time:116823ms step_avg:58.41ms
step:2001/2330 train_time:116841ms step_avg:58.39ms
step:2002/2330 train_time:116863ms step_avg:58.37ms
step:2003/2330 train_time:116921ms step_avg:58.37ms
step:2004/2330 train_time:116984ms step_avg:58.38ms
step:2005/2330 train_time:117042ms step_avg:58.38ms
step:2006/2330 train_time:117104ms step_avg:58.38ms
step:2007/2330 train_time:117161ms step_avg:58.38ms
step:2008/2330 train_time:117222ms step_avg:58.38ms
step:2009/2330 train_time:117279ms step_avg:58.38ms
step:2010/2330 train_time:117339ms step_avg:58.38ms
step:2011/2330 train_time:117396ms step_avg:58.38ms
step:2012/2330 train_time:117455ms step_avg:58.38ms
step:2013/2330 train_time:117512ms step_avg:58.38ms
step:2014/2330 train_time:117572ms step_avg:58.38ms
step:2015/2330 train_time:117629ms step_avg:58.38ms
step:2016/2330 train_time:117689ms step_avg:58.38ms
step:2017/2330 train_time:117747ms step_avg:58.38ms
step:2018/2330 train_time:117808ms step_avg:58.38ms
step:2019/2330 train_time:117868ms step_avg:58.38ms
step:2020/2330 train_time:117931ms step_avg:58.38ms
step:2021/2330 train_time:117990ms step_avg:58.38ms
step:2022/2330 train_time:118051ms step_avg:58.38ms
step:2023/2330 train_time:118110ms step_avg:58.38ms
step:2024/2330 train_time:118170ms step_avg:58.38ms
step:2025/2330 train_time:118227ms step_avg:58.38ms
step:2026/2330 train_time:118290ms step_avg:58.39ms
step:2027/2330 train_time:118347ms step_avg:58.39ms
step:2028/2330 train_time:118407ms step_avg:58.39ms
step:2029/2330 train_time:118464ms step_avg:58.39ms
step:2030/2330 train_time:118524ms step_avg:58.39ms
step:2031/2330 train_time:118580ms step_avg:58.39ms
step:2032/2330 train_time:118640ms step_avg:58.39ms
step:2033/2330 train_time:118697ms step_avg:58.39ms
step:2034/2330 train_time:118758ms step_avg:58.39ms
step:2035/2330 train_time:118815ms step_avg:58.39ms
step:2036/2330 train_time:118876ms step_avg:58.39ms
step:2037/2330 train_time:118934ms step_avg:58.39ms
step:2038/2330 train_time:118996ms step_avg:58.39ms
step:2039/2330 train_time:119054ms step_avg:58.39ms
step:2040/2330 train_time:119114ms step_avg:58.39ms
step:2041/2330 train_time:119173ms step_avg:58.39ms
step:2042/2330 train_time:119234ms step_avg:58.39ms
step:2043/2330 train_time:119292ms step_avg:58.39ms
step:2044/2330 train_time:119351ms step_avg:58.39ms
step:2045/2330 train_time:119408ms step_avg:58.39ms
step:2046/2330 train_time:119470ms step_avg:58.39ms
step:2047/2330 train_time:119528ms step_avg:58.39ms
step:2048/2330 train_time:119588ms step_avg:58.39ms
step:2049/2330 train_time:119646ms step_avg:58.39ms
step:2050/2330 train_time:119706ms step_avg:58.39ms
step:2051/2330 train_time:119763ms step_avg:58.39ms
step:2052/2330 train_time:119825ms step_avg:58.39ms
step:2053/2330 train_time:119882ms step_avg:58.39ms
step:2054/2330 train_time:119945ms step_avg:58.40ms
step:2055/2330 train_time:120003ms step_avg:58.40ms
step:2056/2330 train_time:120065ms step_avg:58.40ms
step:2057/2330 train_time:120122ms step_avg:58.40ms
step:2058/2330 train_time:120184ms step_avg:58.40ms
step:2059/2330 train_time:120241ms step_avg:58.40ms
step:2060/2330 train_time:120303ms step_avg:58.40ms
step:2061/2330 train_time:120360ms step_avg:58.40ms
step:2062/2330 train_time:120421ms step_avg:58.40ms
step:2063/2330 train_time:120478ms step_avg:58.40ms
step:2064/2330 train_time:120538ms step_avg:58.40ms
step:2065/2330 train_time:120595ms step_avg:58.40ms
step:2066/2330 train_time:120656ms step_avg:58.40ms
step:2067/2330 train_time:120713ms step_avg:58.40ms
step:2068/2330 train_time:120773ms step_avg:58.40ms
step:2069/2330 train_time:120831ms step_avg:58.40ms
step:2070/2330 train_time:120892ms step_avg:58.40ms
step:2071/2330 train_time:120950ms step_avg:58.40ms
step:2072/2330 train_time:121011ms step_avg:58.40ms
step:2073/2330 train_time:121070ms step_avg:58.40ms
step:2074/2330 train_time:121131ms step_avg:58.40ms
step:2075/2330 train_time:121189ms step_avg:58.40ms
step:2076/2330 train_time:121250ms step_avg:58.41ms
step:2077/2330 train_time:121308ms step_avg:58.41ms
step:2078/2330 train_time:121368ms step_avg:58.41ms
step:2079/2330 train_time:121425ms step_avg:58.41ms
step:2080/2330 train_time:121487ms step_avg:58.41ms
step:2081/2330 train_time:121544ms step_avg:58.41ms
step:2082/2330 train_time:121606ms step_avg:58.41ms
step:2083/2330 train_time:121662ms step_avg:58.41ms
step:2084/2330 train_time:121723ms step_avg:58.41ms
step:2085/2330 train_time:121780ms step_avg:58.41ms
step:2086/2330 train_time:121842ms step_avg:58.41ms
step:2087/2330 train_time:121898ms step_avg:58.41ms
step:2088/2330 train_time:121960ms step_avg:58.41ms
step:2089/2330 train_time:122016ms step_avg:58.41ms
step:2090/2330 train_time:122079ms step_avg:58.41ms
step:2091/2330 train_time:122135ms step_avg:58.41ms
step:2092/2330 train_time:122197ms step_avg:58.41ms
step:2093/2330 train_time:122254ms step_avg:58.41ms
step:2094/2330 train_time:122316ms step_avg:58.41ms
step:2095/2330 train_time:122373ms step_avg:58.41ms
step:2096/2330 train_time:122434ms step_avg:58.41ms
step:2097/2330 train_time:122491ms step_avg:58.41ms
step:2098/2330 train_time:122551ms step_avg:58.41ms
step:2099/2330 train_time:122609ms step_avg:58.41ms
step:2100/2330 train_time:122669ms step_avg:58.41ms
step:2101/2330 train_time:122727ms step_avg:58.41ms
step:2102/2330 train_time:122787ms step_avg:58.41ms
step:2103/2330 train_time:122845ms step_avg:58.41ms
step:2104/2330 train_time:122906ms step_avg:58.42ms
step:2105/2330 train_time:122963ms step_avg:58.41ms
step:2106/2330 train_time:123024ms step_avg:58.42ms
step:2107/2330 train_time:123081ms step_avg:58.42ms
step:2108/2330 train_time:123143ms step_avg:58.42ms
step:2109/2330 train_time:123200ms step_avg:58.42ms
step:2110/2330 train_time:123260ms step_avg:58.42ms
step:2111/2330 train_time:123317ms step_avg:58.42ms
step:2112/2330 train_time:123380ms step_avg:58.42ms
step:2113/2330 train_time:123437ms step_avg:58.42ms
step:2114/2330 train_time:123498ms step_avg:58.42ms
step:2115/2330 train_time:123555ms step_avg:58.42ms
step:2116/2330 train_time:123616ms step_avg:58.42ms
step:2117/2330 train_time:123672ms step_avg:58.42ms
step:2118/2330 train_time:123732ms step_avg:58.42ms
step:2119/2330 train_time:123790ms step_avg:58.42ms
step:2120/2330 train_time:123850ms step_avg:58.42ms
step:2121/2330 train_time:123909ms step_avg:58.42ms
step:2122/2330 train_time:123969ms step_avg:58.42ms
step:2123/2330 train_time:124028ms step_avg:58.42ms
step:2124/2330 train_time:124089ms step_avg:58.42ms
step:2125/2330 train_time:124147ms step_avg:58.42ms
step:2126/2330 train_time:124209ms step_avg:58.42ms
step:2127/2330 train_time:124266ms step_avg:58.42ms
step:2128/2330 train_time:124328ms step_avg:58.42ms
step:2129/2330 train_time:124386ms step_avg:58.42ms
step:2130/2330 train_time:124447ms step_avg:58.43ms
step:2131/2330 train_time:124504ms step_avg:58.43ms
step:2132/2330 train_time:124566ms step_avg:58.43ms
step:2133/2330 train_time:124624ms step_avg:58.43ms
step:2134/2330 train_time:124684ms step_avg:58.43ms
step:2135/2330 train_time:124741ms step_avg:58.43ms
step:2136/2330 train_time:124802ms step_avg:58.43ms
step:2137/2330 train_time:124859ms step_avg:58.43ms
step:2138/2330 train_time:124920ms step_avg:58.43ms
step:2139/2330 train_time:124978ms step_avg:58.43ms
step:2140/2330 train_time:125038ms step_avg:58.43ms
step:2141/2330 train_time:125095ms step_avg:58.43ms
step:2142/2330 train_time:125156ms step_avg:58.43ms
step:2143/2330 train_time:125214ms step_avg:58.43ms
step:2144/2330 train_time:125274ms step_avg:58.43ms
step:2145/2330 train_time:125331ms step_avg:58.43ms
step:2146/2330 train_time:125392ms step_avg:58.43ms
step:2147/2330 train_time:125450ms step_avg:58.43ms
step:2148/2330 train_time:125510ms step_avg:58.43ms
step:2149/2330 train_time:125568ms step_avg:58.43ms
step:2150/2330 train_time:125629ms step_avg:58.43ms
step:2151/2330 train_time:125687ms step_avg:58.43ms
step:2152/2330 train_time:125748ms step_avg:58.43ms
step:2153/2330 train_time:125806ms step_avg:58.43ms
step:2154/2330 train_time:125867ms step_avg:58.43ms
step:2155/2330 train_time:125924ms step_avg:58.43ms
step:2156/2330 train_time:125986ms step_avg:58.43ms
step:2157/2330 train_time:126043ms step_avg:58.43ms
step:2158/2330 train_time:126106ms step_avg:58.44ms
step:2159/2330 train_time:126163ms step_avg:58.44ms
step:2160/2330 train_time:126225ms step_avg:58.44ms
step:2161/2330 train_time:126282ms step_avg:58.44ms
step:2162/2330 train_time:126345ms step_avg:58.44ms
step:2163/2330 train_time:126401ms step_avg:58.44ms
step:2164/2330 train_time:126464ms step_avg:58.44ms
step:2165/2330 train_time:126521ms step_avg:58.44ms
step:2166/2330 train_time:126582ms step_avg:58.44ms
step:2167/2330 train_time:126639ms step_avg:58.44ms
step:2168/2330 train_time:126699ms step_avg:58.44ms
step:2169/2330 train_time:126756ms step_avg:58.44ms
step:2170/2330 train_time:126817ms step_avg:58.44ms
step:2171/2330 train_time:126875ms step_avg:58.44ms
step:2172/2330 train_time:126935ms step_avg:58.44ms
step:2173/2330 train_time:126993ms step_avg:58.44ms
step:2174/2330 train_time:127053ms step_avg:58.44ms
step:2175/2330 train_time:127111ms step_avg:58.44ms
step:2176/2330 train_time:127173ms step_avg:58.44ms
step:2177/2330 train_time:127230ms step_avg:58.44ms
step:2178/2330 train_time:127291ms step_avg:58.44ms
step:2179/2330 train_time:127349ms step_avg:58.44ms
step:2180/2330 train_time:127410ms step_avg:58.44ms
step:2181/2330 train_time:127467ms step_avg:58.44ms
step:2182/2330 train_time:127528ms step_avg:58.45ms
step:2183/2330 train_time:127585ms step_avg:58.44ms
step:2184/2330 train_time:127646ms step_avg:58.45ms
step:2185/2330 train_time:127703ms step_avg:58.45ms
step:2186/2330 train_time:127764ms step_avg:58.45ms
step:2187/2330 train_time:127821ms step_avg:58.45ms
step:2188/2330 train_time:127882ms step_avg:58.45ms
step:2189/2330 train_time:127939ms step_avg:58.45ms
step:2190/2330 train_time:128002ms step_avg:58.45ms
step:2191/2330 train_time:128058ms step_avg:58.45ms
step:2192/2330 train_time:128120ms step_avg:58.45ms
step:2193/2330 train_time:128177ms step_avg:58.45ms
step:2194/2330 train_time:128238ms step_avg:58.45ms
step:2195/2330 train_time:128296ms step_avg:58.45ms
step:2196/2330 train_time:128357ms step_avg:58.45ms
step:2197/2330 train_time:128413ms step_avg:58.45ms
step:2198/2330 train_time:128474ms step_avg:58.45ms
step:2199/2330 train_time:128532ms step_avg:58.45ms
step:2200/2330 train_time:128593ms step_avg:58.45ms
step:2201/2330 train_time:128651ms step_avg:58.45ms
step:2202/2330 train_time:128712ms step_avg:58.45ms
step:2203/2330 train_time:128769ms step_avg:58.45ms
step:2204/2330 train_time:128831ms step_avg:58.45ms
step:2205/2330 train_time:128890ms step_avg:58.45ms
step:2206/2330 train_time:128950ms step_avg:58.45ms
step:2207/2330 train_time:129008ms step_avg:58.45ms
step:2208/2330 train_time:129068ms step_avg:58.45ms
step:2209/2330 train_time:129126ms step_avg:58.45ms
step:2210/2330 train_time:129186ms step_avg:58.46ms
step:2211/2330 train_time:129245ms step_avg:58.46ms
step:2212/2330 train_time:129305ms step_avg:58.46ms
step:2213/2330 train_time:129362ms step_avg:58.46ms
step:2214/2330 train_time:129423ms step_avg:58.46ms
step:2215/2330 train_time:129480ms step_avg:58.46ms
step:2216/2330 train_time:129541ms step_avg:58.46ms
step:2217/2330 train_time:129598ms step_avg:58.46ms
step:2218/2330 train_time:129660ms step_avg:58.46ms
step:2219/2330 train_time:129716ms step_avg:58.46ms
step:2220/2330 train_time:129778ms step_avg:58.46ms
step:2221/2330 train_time:129835ms step_avg:58.46ms
step:2222/2330 train_time:129896ms step_avg:58.46ms
step:2223/2330 train_time:129954ms step_avg:58.46ms
step:2224/2330 train_time:130014ms step_avg:58.46ms
step:2225/2330 train_time:130072ms step_avg:58.46ms
step:2226/2330 train_time:130133ms step_avg:58.46ms
step:2227/2330 train_time:130191ms step_avg:58.46ms
step:2228/2330 train_time:130251ms step_avg:58.46ms
step:2229/2330 train_time:130309ms step_avg:58.46ms
step:2230/2330 train_time:130370ms step_avg:58.46ms
step:2231/2330 train_time:130427ms step_avg:58.46ms
step:2232/2330 train_time:130488ms step_avg:58.46ms
step:2233/2330 train_time:130546ms step_avg:58.46ms
step:2234/2330 train_time:130607ms step_avg:58.46ms
step:2235/2330 train_time:130664ms step_avg:58.46ms
step:2236/2330 train_time:130725ms step_avg:58.46ms
step:2237/2330 train_time:130782ms step_avg:58.46ms
step:2238/2330 train_time:130843ms step_avg:58.46ms
step:2239/2330 train_time:130900ms step_avg:58.46ms
step:2240/2330 train_time:130961ms step_avg:58.46ms
step:2241/2330 train_time:131018ms step_avg:58.46ms
step:2242/2330 train_time:131079ms step_avg:58.47ms
step:2243/2330 train_time:131136ms step_avg:58.46ms
step:2244/2330 train_time:131198ms step_avg:58.47ms
step:2245/2330 train_time:131255ms step_avg:58.47ms
step:2246/2330 train_time:131314ms step_avg:58.47ms
step:2247/2330 train_time:131372ms step_avg:58.47ms
step:2248/2330 train_time:131432ms step_avg:58.47ms
step:2249/2330 train_time:131490ms step_avg:58.47ms
step:2250/2330 train_time:131550ms step_avg:58.47ms
step:2250/2330 val_loss:3.7069 train_time:131631ms step_avg:58.50ms
step:2251/2330 train_time:131649ms step_avg:58.48ms
step:2252/2330 train_time:131670ms step_avg:58.47ms
step:2253/2330 train_time:131734ms step_avg:58.47ms
step:2254/2330 train_time:131797ms step_avg:58.47ms
step:2255/2330 train_time:131855ms step_avg:58.47ms
step:2256/2330 train_time:131917ms step_avg:58.47ms
step:2257/2330 train_time:131974ms step_avg:58.47ms
step:2258/2330 train_time:132034ms step_avg:58.47ms
step:2259/2330 train_time:132092ms step_avg:58.47ms
step:2260/2330 train_time:132151ms step_avg:58.47ms
step:2261/2330 train_time:132209ms step_avg:58.47ms
step:2262/2330 train_time:132268ms step_avg:58.47ms
step:2263/2330 train_time:132325ms step_avg:58.47ms
step:2264/2330 train_time:132385ms step_avg:58.47ms
step:2265/2330 train_time:132441ms step_avg:58.47ms
step:2266/2330 train_time:132502ms step_avg:58.47ms
step:2267/2330 train_time:132558ms step_avg:58.47ms
step:2268/2330 train_time:132620ms step_avg:58.47ms
step:2269/2330 train_time:132680ms step_avg:58.47ms
step:2270/2330 train_time:132741ms step_avg:58.48ms
step:2271/2330 train_time:132800ms step_avg:58.48ms
step:2272/2330 train_time:132861ms step_avg:58.48ms
step:2273/2330 train_time:132919ms step_avg:58.48ms
step:2274/2330 train_time:132979ms step_avg:58.48ms
step:2275/2330 train_time:133036ms step_avg:58.48ms
step:2276/2330 train_time:133096ms step_avg:58.48ms
step:2277/2330 train_time:133153ms step_avg:58.48ms
step:2278/2330 train_time:133213ms step_avg:58.48ms
step:2279/2330 train_time:133271ms step_avg:58.48ms
step:2280/2330 train_time:133331ms step_avg:58.48ms
step:2281/2330 train_time:133387ms step_avg:58.48ms
step:2282/2330 train_time:133449ms step_avg:58.48ms
step:2283/2330 train_time:133506ms step_avg:58.48ms
step:2284/2330 train_time:133567ms step_avg:58.48ms
step:2285/2330 train_time:133625ms step_avg:58.48ms
step:2286/2330 train_time:133687ms step_avg:58.48ms
step:2287/2330 train_time:133744ms step_avg:58.48ms
step:2288/2330 train_time:133807ms step_avg:58.48ms
step:2289/2330 train_time:133865ms step_avg:58.48ms
step:2290/2330 train_time:133926ms step_avg:58.48ms
step:2291/2330 train_time:133983ms step_avg:58.48ms
step:2292/2330 train_time:134045ms step_avg:58.48ms
step:2293/2330 train_time:134102ms step_avg:58.48ms
step:2294/2330 train_time:134164ms step_avg:58.48ms
step:2295/2330 train_time:134221ms step_avg:58.48ms
step:2296/2330 train_time:134281ms step_avg:58.48ms
step:2297/2330 train_time:134338ms step_avg:58.48ms
step:2298/2330 train_time:134398ms step_avg:58.48ms
step:2299/2330 train_time:134456ms step_avg:58.48ms
step:2300/2330 train_time:134516ms step_avg:58.49ms
step:2301/2330 train_time:134574ms step_avg:58.49ms
step:2302/2330 train_time:134635ms step_avg:58.49ms
step:2303/2330 train_time:134693ms step_avg:58.49ms
step:2304/2330 train_time:134754ms step_avg:58.49ms
step:2305/2330 train_time:134813ms step_avg:58.49ms
step:2306/2330 train_time:134874ms step_avg:58.49ms
step:2307/2330 train_time:134933ms step_avg:58.49ms
step:2308/2330 train_time:134993ms step_avg:58.49ms
step:2309/2330 train_time:135051ms step_avg:58.49ms
step:2310/2330 train_time:135112ms step_avg:58.49ms
step:2311/2330 train_time:135170ms step_avg:58.49ms
step:2312/2330 train_time:135230ms step_avg:58.49ms
step:2313/2330 train_time:135287ms step_avg:58.49ms
step:2314/2330 train_time:135348ms step_avg:58.49ms
step:2315/2330 train_time:135405ms step_avg:58.49ms
step:2316/2330 train_time:135465ms step_avg:58.49ms
step:2317/2330 train_time:135522ms step_avg:58.49ms
step:2318/2330 train_time:135582ms step_avg:58.49ms
step:2319/2330 train_time:135639ms step_avg:58.49ms
step:2320/2330 train_time:135700ms step_avg:58.49ms
step:2321/2330 train_time:135758ms step_avg:58.49ms
step:2322/2330 train_time:135819ms step_avg:58.49ms
step:2323/2330 train_time:135877ms step_avg:58.49ms
step:2324/2330 train_time:135937ms step_avg:58.49ms
step:2325/2330 train_time:135995ms step_avg:58.49ms
step:2326/2330 train_time:136056ms step_avg:58.49ms
step:2327/2330 train_time:136114ms step_avg:58.49ms
step:2328/2330 train_time:136174ms step_avg:58.49ms
step:2329/2330 train_time:136232ms step_avg:58.49ms
step:2330/2330 train_time:136292ms step_avg:58.49ms
step:2330/2330 val_loss:3.6892 train_time:136374ms step_avg:58.53ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
