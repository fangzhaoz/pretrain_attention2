import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:31:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:78ms step_avg:78.00ms
step:2/2330 train_time:194ms step_avg:97.13ms
step:3/2330 train_time:212ms step_avg:70.78ms
step:4/2330 train_time:231ms step_avg:57.76ms
step:5/2330 train_time:285ms step_avg:56.99ms
step:6/2330 train_time:343ms step_avg:57.12ms
step:7/2330 train_time:398ms step_avg:56.84ms
step:8/2330 train_time:457ms step_avg:57.11ms
step:9/2330 train_time:513ms step_avg:56.95ms
step:10/2330 train_time:571ms step_avg:57.07ms
step:11/2330 train_time:626ms step_avg:56.91ms
step:12/2330 train_time:685ms step_avg:57.05ms
step:13/2330 train_time:740ms step_avg:56.93ms
step:14/2330 train_time:798ms step_avg:57.00ms
step:15/2330 train_time:853ms step_avg:56.86ms
step:16/2330 train_time:912ms step_avg:56.99ms
step:17/2330 train_time:967ms step_avg:56.91ms
step:18/2330 train_time:1026ms step_avg:56.99ms
step:19/2330 train_time:1082ms step_avg:56.97ms
step:20/2330 train_time:1147ms step_avg:57.36ms
step:21/2330 train_time:1206ms step_avg:57.42ms
step:22/2330 train_time:1266ms step_avg:57.57ms
step:23/2330 train_time:1323ms step_avg:57.52ms
step:24/2330 train_time:1382ms step_avg:57.58ms
step:25/2330 train_time:1438ms step_avg:57.53ms
step:26/2330 train_time:1497ms step_avg:57.57ms
step:27/2330 train_time:1552ms step_avg:57.49ms
step:28/2330 train_time:1611ms step_avg:57.52ms
step:29/2330 train_time:1666ms step_avg:57.44ms
step:30/2330 train_time:1724ms step_avg:57.47ms
step:31/2330 train_time:1780ms step_avg:57.42ms
step:32/2330 train_time:1839ms step_avg:57.46ms
step:33/2330 train_time:1894ms step_avg:57.39ms
step:34/2330 train_time:1952ms step_avg:57.42ms
step:35/2330 train_time:2008ms step_avg:57.36ms
step:36/2330 train_time:2068ms step_avg:57.44ms
step:37/2330 train_time:2125ms step_avg:57.43ms
step:38/2330 train_time:2185ms step_avg:57.50ms
step:39/2330 train_time:2242ms step_avg:57.49ms
step:40/2330 train_time:2303ms step_avg:57.58ms
step:41/2330 train_time:2359ms step_avg:57.54ms
step:42/2330 train_time:2419ms step_avg:57.59ms
step:43/2330 train_time:2475ms step_avg:57.56ms
step:44/2330 train_time:2533ms step_avg:57.58ms
step:45/2330 train_time:2589ms step_avg:57.53ms
step:46/2330 train_time:2648ms step_avg:57.56ms
step:47/2330 train_time:2704ms step_avg:57.53ms
step:48/2330 train_time:2762ms step_avg:57.55ms
step:49/2330 train_time:2818ms step_avg:57.51ms
step:50/2330 train_time:2876ms step_avg:57.52ms
step:51/2330 train_time:2932ms step_avg:57.49ms
step:52/2330 train_time:2991ms step_avg:57.51ms
step:53/2330 train_time:3046ms step_avg:57.47ms
step:54/2330 train_time:3106ms step_avg:57.53ms
step:55/2330 train_time:3163ms step_avg:57.50ms
step:56/2330 train_time:3223ms step_avg:57.55ms
step:57/2330 train_time:3279ms step_avg:57.53ms
step:58/2330 train_time:3339ms step_avg:57.57ms
step:59/2330 train_time:3394ms step_avg:57.53ms
step:60/2330 train_time:3454ms step_avg:57.57ms
step:61/2330 train_time:3510ms step_avg:57.55ms
step:62/2330 train_time:3569ms step_avg:57.56ms
step:63/2330 train_time:3624ms step_avg:57.53ms
step:64/2330 train_time:3683ms step_avg:57.55ms
step:65/2330 train_time:3740ms step_avg:57.53ms
step:66/2330 train_time:3798ms step_avg:57.55ms
step:67/2330 train_time:3854ms step_avg:57.52ms
step:68/2330 train_time:3912ms step_avg:57.53ms
step:69/2330 train_time:3967ms step_avg:57.50ms
step:70/2330 train_time:4026ms step_avg:57.52ms
step:71/2330 train_time:4083ms step_avg:57.50ms
step:72/2330 train_time:4142ms step_avg:57.52ms
step:73/2330 train_time:4198ms step_avg:57.50ms
step:74/2330 train_time:4257ms step_avg:57.53ms
step:75/2330 train_time:4314ms step_avg:57.52ms
step:76/2330 train_time:4373ms step_avg:57.54ms
step:77/2330 train_time:4429ms step_avg:57.52ms
step:78/2330 train_time:4488ms step_avg:57.54ms
step:79/2330 train_time:4545ms step_avg:57.53ms
step:80/2330 train_time:4604ms step_avg:57.55ms
step:81/2330 train_time:4660ms step_avg:57.53ms
step:82/2330 train_time:4719ms step_avg:57.55ms
step:83/2330 train_time:4775ms step_avg:57.53ms
step:84/2330 train_time:4833ms step_avg:57.54ms
step:85/2330 train_time:4889ms step_avg:57.52ms
step:86/2330 train_time:4948ms step_avg:57.54ms
step:87/2330 train_time:5004ms step_avg:57.52ms
step:88/2330 train_time:5063ms step_avg:57.54ms
step:89/2330 train_time:5120ms step_avg:57.53ms
step:90/2330 train_time:5178ms step_avg:57.54ms
step:91/2330 train_time:5235ms step_avg:57.53ms
step:92/2330 train_time:5294ms step_avg:57.54ms
step:93/2330 train_time:5350ms step_avg:57.52ms
step:94/2330 train_time:5409ms step_avg:57.54ms
step:95/2330 train_time:5465ms step_avg:57.53ms
step:96/2330 train_time:5524ms step_avg:57.55ms
step:97/2330 train_time:5581ms step_avg:57.54ms
step:98/2330 train_time:5640ms step_avg:57.55ms
step:99/2330 train_time:5696ms step_avg:57.53ms
step:100/2330 train_time:5755ms step_avg:57.55ms
step:101/2330 train_time:5810ms step_avg:57.53ms
step:102/2330 train_time:5870ms step_avg:57.55ms
step:103/2330 train_time:5925ms step_avg:57.52ms
step:104/2330 train_time:5985ms step_avg:57.54ms
step:105/2330 train_time:6041ms step_avg:57.53ms
step:106/2330 train_time:6099ms step_avg:57.54ms
step:107/2330 train_time:6155ms step_avg:57.52ms
step:108/2330 train_time:6214ms step_avg:57.54ms
step:109/2330 train_time:6270ms step_avg:57.52ms
step:110/2330 train_time:6329ms step_avg:57.54ms
step:111/2330 train_time:6385ms step_avg:57.53ms
step:112/2330 train_time:6444ms step_avg:57.54ms
step:113/2330 train_time:6500ms step_avg:57.52ms
step:114/2330 train_time:6559ms step_avg:57.54ms
step:115/2330 train_time:6616ms step_avg:57.53ms
step:116/2330 train_time:6674ms step_avg:57.54ms
step:117/2330 train_time:6730ms step_avg:57.52ms
step:118/2330 train_time:6789ms step_avg:57.53ms
step:119/2330 train_time:6845ms step_avg:57.52ms
step:120/2330 train_time:6904ms step_avg:57.53ms
step:121/2330 train_time:6960ms step_avg:57.52ms
step:122/2330 train_time:7018ms step_avg:57.53ms
step:123/2330 train_time:7075ms step_avg:57.52ms
step:124/2330 train_time:7134ms step_avg:57.53ms
step:125/2330 train_time:7190ms step_avg:57.52ms
step:126/2330 train_time:7249ms step_avg:57.53ms
step:127/2330 train_time:7306ms step_avg:57.53ms
step:128/2330 train_time:7364ms step_avg:57.53ms
step:129/2330 train_time:7421ms step_avg:57.53ms
step:130/2330 train_time:7480ms step_avg:57.54ms
step:131/2330 train_time:7536ms step_avg:57.53ms
step:132/2330 train_time:7595ms step_avg:57.54ms
step:133/2330 train_time:7651ms step_avg:57.52ms
step:134/2330 train_time:7710ms step_avg:57.54ms
step:135/2330 train_time:7765ms step_avg:57.52ms
step:136/2330 train_time:7826ms step_avg:57.54ms
step:137/2330 train_time:7881ms step_avg:57.53ms
step:138/2330 train_time:7940ms step_avg:57.53ms
step:139/2330 train_time:7995ms step_avg:57.52ms
step:140/2330 train_time:8055ms step_avg:57.54ms
step:141/2330 train_time:8112ms step_avg:57.53ms
step:142/2330 train_time:8171ms step_avg:57.54ms
step:143/2330 train_time:8226ms step_avg:57.52ms
step:144/2330 train_time:8286ms step_avg:57.54ms
step:145/2330 train_time:8342ms step_avg:57.53ms
step:146/2330 train_time:8401ms step_avg:57.54ms
step:147/2330 train_time:8458ms step_avg:57.53ms
step:148/2330 train_time:8516ms step_avg:57.54ms
step:149/2330 train_time:8572ms step_avg:57.53ms
step:150/2330 train_time:8632ms step_avg:57.55ms
step:151/2330 train_time:8688ms step_avg:57.54ms
step:152/2330 train_time:8748ms step_avg:57.55ms
step:153/2330 train_time:8803ms step_avg:57.54ms
step:154/2330 train_time:8863ms step_avg:57.55ms
step:155/2330 train_time:8919ms step_avg:57.54ms
step:156/2330 train_time:8979ms step_avg:57.56ms
step:157/2330 train_time:9034ms step_avg:57.54ms
step:158/2330 train_time:9094ms step_avg:57.56ms
step:159/2330 train_time:9149ms step_avg:57.54ms
step:160/2330 train_time:9209ms step_avg:57.55ms
step:161/2330 train_time:9265ms step_avg:57.54ms
step:162/2330 train_time:9324ms step_avg:57.55ms
step:163/2330 train_time:9380ms step_avg:57.54ms
step:164/2330 train_time:9440ms step_avg:57.56ms
step:165/2330 train_time:9496ms step_avg:57.55ms
step:166/2330 train_time:9554ms step_avg:57.56ms
step:167/2330 train_time:9611ms step_avg:57.55ms
step:168/2330 train_time:9670ms step_avg:57.56ms
step:169/2330 train_time:9726ms step_avg:57.55ms
step:170/2330 train_time:9785ms step_avg:57.56ms
step:171/2330 train_time:9841ms step_avg:57.55ms
step:172/2330 train_time:9899ms step_avg:57.55ms
step:173/2330 train_time:9955ms step_avg:57.54ms
step:174/2330 train_time:10014ms step_avg:57.55ms
step:175/2330 train_time:10070ms step_avg:57.54ms
step:176/2330 train_time:10129ms step_avg:57.55ms
step:177/2330 train_time:10185ms step_avg:57.54ms
step:178/2330 train_time:10244ms step_avg:57.55ms
step:179/2330 train_time:10299ms step_avg:57.54ms
step:180/2330 train_time:10359ms step_avg:57.55ms
step:181/2330 train_time:10414ms step_avg:57.54ms
step:182/2330 train_time:10474ms step_avg:57.55ms
step:183/2330 train_time:10530ms step_avg:57.54ms
step:184/2330 train_time:10588ms step_avg:57.54ms
step:185/2330 train_time:10644ms step_avg:57.53ms
step:186/2330 train_time:10704ms step_avg:57.55ms
step:187/2330 train_time:10760ms step_avg:57.54ms
step:188/2330 train_time:10819ms step_avg:57.55ms
step:189/2330 train_time:10875ms step_avg:57.54ms
step:190/2330 train_time:10934ms step_avg:57.55ms
step:191/2330 train_time:10990ms step_avg:57.54ms
step:192/2330 train_time:11049ms step_avg:57.55ms
step:193/2330 train_time:11104ms step_avg:57.54ms
step:194/2330 train_time:11164ms step_avg:57.55ms
step:195/2330 train_time:11220ms step_avg:57.54ms
step:196/2330 train_time:11279ms step_avg:57.55ms
step:197/2330 train_time:11335ms step_avg:57.54ms
step:198/2330 train_time:11394ms step_avg:57.55ms
step:199/2330 train_time:11450ms step_avg:57.54ms
step:200/2330 train_time:11510ms step_avg:57.55ms
step:201/2330 train_time:11565ms step_avg:57.54ms
step:202/2330 train_time:11624ms step_avg:57.55ms
step:203/2330 train_time:11681ms step_avg:57.54ms
step:204/2330 train_time:11740ms step_avg:57.55ms
step:205/2330 train_time:11796ms step_avg:57.54ms
step:206/2330 train_time:11854ms step_avg:57.54ms
step:207/2330 train_time:11911ms step_avg:57.54ms
step:208/2330 train_time:11970ms step_avg:57.55ms
step:209/2330 train_time:12026ms step_avg:57.54ms
step:210/2330 train_time:12085ms step_avg:57.55ms
step:211/2330 train_time:12141ms step_avg:57.54ms
step:212/2330 train_time:12200ms step_avg:57.55ms
step:213/2330 train_time:12256ms step_avg:57.54ms
step:214/2330 train_time:12315ms step_avg:57.55ms
step:215/2330 train_time:12371ms step_avg:57.54ms
step:216/2330 train_time:12431ms step_avg:57.55ms
step:217/2330 train_time:12486ms step_avg:57.54ms
step:218/2330 train_time:12545ms step_avg:57.55ms
step:219/2330 train_time:12601ms step_avg:57.54ms
step:220/2330 train_time:12661ms step_avg:57.55ms
step:221/2330 train_time:12717ms step_avg:57.54ms
step:222/2330 train_time:12776ms step_avg:57.55ms
step:223/2330 train_time:12832ms step_avg:57.54ms
step:224/2330 train_time:12891ms step_avg:57.55ms
step:225/2330 train_time:12946ms step_avg:57.54ms
step:226/2330 train_time:13006ms step_avg:57.55ms
step:227/2330 train_time:13062ms step_avg:57.54ms
step:228/2330 train_time:13121ms step_avg:57.55ms
step:229/2330 train_time:13177ms step_avg:57.54ms
step:230/2330 train_time:13236ms step_avg:57.55ms
step:231/2330 train_time:13292ms step_avg:57.54ms
step:232/2330 train_time:13351ms step_avg:57.55ms
step:233/2330 train_time:13407ms step_avg:57.54ms
step:234/2330 train_time:13467ms step_avg:57.55ms
step:235/2330 train_time:13523ms step_avg:57.54ms
step:236/2330 train_time:13582ms step_avg:57.55ms
step:237/2330 train_time:13639ms step_avg:57.55ms
step:238/2330 train_time:13698ms step_avg:57.55ms
step:239/2330 train_time:13754ms step_avg:57.55ms
step:240/2330 train_time:13813ms step_avg:57.55ms
step:241/2330 train_time:13869ms step_avg:57.55ms
step:242/2330 train_time:13930ms step_avg:57.56ms
step:243/2330 train_time:13985ms step_avg:57.55ms
step:244/2330 train_time:14043ms step_avg:57.56ms
step:245/2330 train_time:14099ms step_avg:57.55ms
step:246/2330 train_time:14158ms step_avg:57.55ms
step:247/2330 train_time:14214ms step_avg:57.55ms
step:248/2330 train_time:14273ms step_avg:57.55ms
step:249/2330 train_time:14330ms step_avg:57.55ms
step:250/2330 train_time:14388ms step_avg:57.55ms
step:250/2330 val_loss:4.8843 train_time:14467ms step_avg:57.87ms
step:251/2330 train_time:14484ms step_avg:57.70ms
step:252/2330 train_time:14505ms step_avg:57.56ms
step:253/2330 train_time:14561ms step_avg:57.55ms
step:254/2330 train_time:14626ms step_avg:57.58ms
step:255/2330 train_time:14683ms step_avg:57.58ms
step:256/2330 train_time:14749ms step_avg:57.61ms
step:257/2330 train_time:14804ms step_avg:57.60ms
step:258/2330 train_time:14865ms step_avg:57.62ms
step:259/2330 train_time:14920ms step_avg:57.61ms
step:260/2330 train_time:14979ms step_avg:57.61ms
step:261/2330 train_time:15035ms step_avg:57.60ms
step:262/2330 train_time:15093ms step_avg:57.61ms
step:263/2330 train_time:15148ms step_avg:57.60ms
step:264/2330 train_time:15206ms step_avg:57.60ms
step:265/2330 train_time:15261ms step_avg:57.59ms
step:266/2330 train_time:15320ms step_avg:57.59ms
step:267/2330 train_time:15377ms step_avg:57.59ms
step:268/2330 train_time:15436ms step_avg:57.60ms
step:269/2330 train_time:15493ms step_avg:57.59ms
step:270/2330 train_time:15552ms step_avg:57.60ms
step:271/2330 train_time:15609ms step_avg:57.60ms
step:272/2330 train_time:15670ms step_avg:57.61ms
step:273/2330 train_time:15726ms step_avg:57.60ms
step:274/2330 train_time:15788ms step_avg:57.62ms
step:275/2330 train_time:15843ms step_avg:57.61ms
step:276/2330 train_time:15903ms step_avg:57.62ms
step:277/2330 train_time:15959ms step_avg:57.61ms
step:278/2330 train_time:16018ms step_avg:57.62ms
step:279/2330 train_time:16073ms step_avg:57.61ms
step:280/2330 train_time:16132ms step_avg:57.61ms
step:281/2330 train_time:16187ms step_avg:57.61ms
step:282/2330 train_time:16246ms step_avg:57.61ms
step:283/2330 train_time:16301ms step_avg:57.60ms
step:284/2330 train_time:16360ms step_avg:57.61ms
step:285/2330 train_time:16417ms step_avg:57.60ms
step:286/2330 train_time:16476ms step_avg:57.61ms
step:287/2330 train_time:16532ms step_avg:57.60ms
step:288/2330 train_time:16592ms step_avg:57.61ms
step:289/2330 train_time:16649ms step_avg:57.61ms
step:290/2330 train_time:16709ms step_avg:57.62ms
step:291/2330 train_time:16765ms step_avg:57.61ms
step:292/2330 train_time:16826ms step_avg:57.62ms
step:293/2330 train_time:16882ms step_avg:57.62ms
step:294/2330 train_time:16941ms step_avg:57.62ms
step:295/2330 train_time:16997ms step_avg:57.62ms
step:296/2330 train_time:17055ms step_avg:57.62ms
step:297/2330 train_time:17111ms step_avg:57.61ms
step:298/2330 train_time:17170ms step_avg:57.62ms
step:299/2330 train_time:17226ms step_avg:57.61ms
step:300/2330 train_time:17285ms step_avg:57.62ms
step:301/2330 train_time:17341ms step_avg:57.61ms
step:302/2330 train_time:17400ms step_avg:57.62ms
step:303/2330 train_time:17456ms step_avg:57.61ms
step:304/2330 train_time:17516ms step_avg:57.62ms
step:305/2330 train_time:17572ms step_avg:57.61ms
step:306/2330 train_time:17633ms step_avg:57.62ms
step:307/2330 train_time:17689ms step_avg:57.62ms
step:308/2330 train_time:17749ms step_avg:57.63ms
step:309/2330 train_time:17805ms step_avg:57.62ms
step:310/2330 train_time:17866ms step_avg:57.63ms
step:311/2330 train_time:17922ms step_avg:57.63ms
step:312/2330 train_time:17981ms step_avg:57.63ms
step:313/2330 train_time:18038ms step_avg:57.63ms
step:314/2330 train_time:18097ms step_avg:57.63ms
step:315/2330 train_time:18153ms step_avg:57.63ms
step:316/2330 train_time:18211ms step_avg:57.63ms
step:317/2330 train_time:18266ms step_avg:57.62ms
step:318/2330 train_time:18326ms step_avg:57.63ms
step:319/2330 train_time:18382ms step_avg:57.62ms
step:320/2330 train_time:18441ms step_avg:57.63ms
step:321/2330 train_time:18497ms step_avg:57.62ms
step:322/2330 train_time:18557ms step_avg:57.63ms
step:323/2330 train_time:18615ms step_avg:57.63ms
step:324/2330 train_time:18674ms step_avg:57.63ms
step:325/2330 train_time:18729ms step_avg:57.63ms
step:326/2330 train_time:18790ms step_avg:57.64ms
step:327/2330 train_time:18845ms step_avg:57.63ms
step:328/2330 train_time:18906ms step_avg:57.64ms
step:329/2330 train_time:18962ms step_avg:57.64ms
step:330/2330 train_time:19022ms step_avg:57.64ms
step:331/2330 train_time:19078ms step_avg:57.64ms
step:332/2330 train_time:19136ms step_avg:57.64ms
step:333/2330 train_time:19192ms step_avg:57.63ms
step:334/2330 train_time:19251ms step_avg:57.64ms
step:335/2330 train_time:19307ms step_avg:57.63ms
step:336/2330 train_time:19366ms step_avg:57.64ms
step:337/2330 train_time:19422ms step_avg:57.63ms
step:338/2330 train_time:19482ms step_avg:57.64ms
step:339/2330 train_time:19539ms step_avg:57.64ms
step:340/2330 train_time:19599ms step_avg:57.64ms
step:341/2330 train_time:19656ms step_avg:57.64ms
step:342/2330 train_time:19716ms step_avg:57.65ms
step:343/2330 train_time:19772ms step_avg:57.64ms
step:344/2330 train_time:19831ms step_avg:57.65ms
step:345/2330 train_time:19887ms step_avg:57.64ms
step:346/2330 train_time:19947ms step_avg:57.65ms
step:347/2330 train_time:20002ms step_avg:57.64ms
step:348/2330 train_time:20062ms step_avg:57.65ms
step:349/2330 train_time:20119ms step_avg:57.65ms
step:350/2330 train_time:20177ms step_avg:57.65ms
step:351/2330 train_time:20233ms step_avg:57.65ms
step:352/2330 train_time:20292ms step_avg:57.65ms
step:353/2330 train_time:20348ms step_avg:57.64ms
step:354/2330 train_time:20407ms step_avg:57.65ms
step:355/2330 train_time:20463ms step_avg:57.64ms
step:356/2330 train_time:20523ms step_avg:57.65ms
step:357/2330 train_time:20579ms step_avg:57.65ms
step:358/2330 train_time:20639ms step_avg:57.65ms
step:359/2330 train_time:20696ms step_avg:57.65ms
step:360/2330 train_time:20756ms step_avg:57.65ms
step:361/2330 train_time:20812ms step_avg:57.65ms
step:362/2330 train_time:20871ms step_avg:57.65ms
step:363/2330 train_time:20927ms step_avg:57.65ms
step:364/2330 train_time:20988ms step_avg:57.66ms
step:365/2330 train_time:21043ms step_avg:57.65ms
step:366/2330 train_time:21103ms step_avg:57.66ms
step:367/2330 train_time:21160ms step_avg:57.66ms
step:368/2330 train_time:21219ms step_avg:57.66ms
step:369/2330 train_time:21276ms step_avg:57.66ms
step:370/2330 train_time:21334ms step_avg:57.66ms
step:371/2330 train_time:21391ms step_avg:57.66ms
step:372/2330 train_time:21449ms step_avg:57.66ms
step:373/2330 train_time:21505ms step_avg:57.65ms
step:374/2330 train_time:21564ms step_avg:57.66ms
step:375/2330 train_time:21620ms step_avg:57.65ms
step:376/2330 train_time:21681ms step_avg:57.66ms
step:377/2330 train_time:21737ms step_avg:57.66ms
step:378/2330 train_time:21797ms step_avg:57.66ms
step:379/2330 train_time:21853ms step_avg:57.66ms
step:380/2330 train_time:21912ms step_avg:57.66ms
step:381/2330 train_time:21968ms step_avg:57.66ms
step:382/2330 train_time:22027ms step_avg:57.66ms
step:383/2330 train_time:22083ms step_avg:57.66ms
step:384/2330 train_time:22144ms step_avg:57.67ms
step:385/2330 train_time:22200ms step_avg:57.66ms
step:386/2330 train_time:22259ms step_avg:57.67ms
step:387/2330 train_time:22315ms step_avg:57.66ms
step:388/2330 train_time:22374ms step_avg:57.66ms
step:389/2330 train_time:22430ms step_avg:57.66ms
step:390/2330 train_time:22490ms step_avg:57.67ms
step:391/2330 train_time:22545ms step_avg:57.66ms
step:392/2330 train_time:22605ms step_avg:57.67ms
step:393/2330 train_time:22661ms step_avg:57.66ms
step:394/2330 train_time:22721ms step_avg:57.67ms
step:395/2330 train_time:22777ms step_avg:57.66ms
step:396/2330 train_time:22837ms step_avg:57.67ms
step:397/2330 train_time:22893ms step_avg:57.67ms
step:398/2330 train_time:22952ms step_avg:57.67ms
step:399/2330 train_time:23008ms step_avg:57.66ms
step:400/2330 train_time:23067ms step_avg:57.67ms
step:401/2330 train_time:23122ms step_avg:57.66ms
step:402/2330 train_time:23182ms step_avg:57.67ms
step:403/2330 train_time:23239ms step_avg:57.66ms
step:404/2330 train_time:23297ms step_avg:57.67ms
step:405/2330 train_time:23354ms step_avg:57.66ms
step:406/2330 train_time:23413ms step_avg:57.67ms
step:407/2330 train_time:23468ms step_avg:57.66ms
step:408/2330 train_time:23528ms step_avg:57.67ms
step:409/2330 train_time:23584ms step_avg:57.66ms
step:410/2330 train_time:23645ms step_avg:57.67ms
step:411/2330 train_time:23701ms step_avg:57.67ms
step:412/2330 train_time:23761ms step_avg:57.67ms
step:413/2330 train_time:23818ms step_avg:57.67ms
step:414/2330 train_time:23877ms step_avg:57.67ms
step:415/2330 train_time:23933ms step_avg:57.67ms
step:416/2330 train_time:23992ms step_avg:57.67ms
step:417/2330 train_time:24048ms step_avg:57.67ms
step:418/2330 train_time:24108ms step_avg:57.67ms
step:419/2330 train_time:24163ms step_avg:57.67ms
step:420/2330 train_time:24224ms step_avg:57.68ms
step:421/2330 train_time:24280ms step_avg:57.67ms
step:422/2330 train_time:24338ms step_avg:57.67ms
step:423/2330 train_time:24395ms step_avg:57.67ms
step:424/2330 train_time:24454ms step_avg:57.67ms
step:425/2330 train_time:24510ms step_avg:57.67ms
step:426/2330 train_time:24570ms step_avg:57.68ms
step:427/2330 train_time:24626ms step_avg:57.67ms
step:428/2330 train_time:24686ms step_avg:57.68ms
step:429/2330 train_time:24742ms step_avg:57.67ms
step:430/2330 train_time:24802ms step_avg:57.68ms
step:431/2330 train_time:24858ms step_avg:57.67ms
step:432/2330 train_time:24919ms step_avg:57.68ms
step:433/2330 train_time:24975ms step_avg:57.68ms
step:434/2330 train_time:25035ms step_avg:57.68ms
step:435/2330 train_time:25090ms step_avg:57.68ms
step:436/2330 train_time:25150ms step_avg:57.68ms
step:437/2330 train_time:25205ms step_avg:57.68ms
step:438/2330 train_time:25265ms step_avg:57.68ms
step:439/2330 train_time:25321ms step_avg:57.68ms
step:440/2330 train_time:25381ms step_avg:57.68ms
step:441/2330 train_time:25437ms step_avg:57.68ms
step:442/2330 train_time:25496ms step_avg:57.68ms
step:443/2330 train_time:25552ms step_avg:57.68ms
step:444/2330 train_time:25611ms step_avg:57.68ms
step:445/2330 train_time:25667ms step_avg:57.68ms
step:446/2330 train_time:25727ms step_avg:57.68ms
step:447/2330 train_time:25783ms step_avg:57.68ms
step:448/2330 train_time:25843ms step_avg:57.68ms
step:449/2330 train_time:25898ms step_avg:57.68ms
step:450/2330 train_time:25958ms step_avg:57.68ms
step:451/2330 train_time:26014ms step_avg:57.68ms
step:452/2330 train_time:26074ms step_avg:57.69ms
step:453/2330 train_time:26129ms step_avg:57.68ms
step:454/2330 train_time:26190ms step_avg:57.69ms
step:455/2330 train_time:26246ms step_avg:57.68ms
step:456/2330 train_time:26305ms step_avg:57.69ms
step:457/2330 train_time:26361ms step_avg:57.68ms
step:458/2330 train_time:26420ms step_avg:57.69ms
step:459/2330 train_time:26477ms step_avg:57.69ms
step:460/2330 train_time:26537ms step_avg:57.69ms
step:461/2330 train_time:26593ms step_avg:57.69ms
step:462/2330 train_time:26652ms step_avg:57.69ms
step:463/2330 train_time:26708ms step_avg:57.68ms
step:464/2330 train_time:26767ms step_avg:57.69ms
step:465/2330 train_time:26823ms step_avg:57.68ms
step:466/2330 train_time:26882ms step_avg:57.69ms
step:467/2330 train_time:26939ms step_avg:57.68ms
step:468/2330 train_time:26998ms step_avg:57.69ms
step:469/2330 train_time:27054ms step_avg:57.68ms
step:470/2330 train_time:27113ms step_avg:57.69ms
step:471/2330 train_time:27169ms step_avg:57.68ms
step:472/2330 train_time:27229ms step_avg:57.69ms
step:473/2330 train_time:27284ms step_avg:57.68ms
step:474/2330 train_time:27343ms step_avg:57.69ms
step:475/2330 train_time:27400ms step_avg:57.68ms
step:476/2330 train_time:27459ms step_avg:57.69ms
step:477/2330 train_time:27515ms step_avg:57.68ms
step:478/2330 train_time:27574ms step_avg:57.69ms
step:479/2330 train_time:27631ms step_avg:57.68ms
step:480/2330 train_time:27690ms step_avg:57.69ms
step:481/2330 train_time:27746ms step_avg:57.68ms
step:482/2330 train_time:27805ms step_avg:57.69ms
step:483/2330 train_time:27861ms step_avg:57.68ms
step:484/2330 train_time:27921ms step_avg:57.69ms
step:485/2330 train_time:27977ms step_avg:57.69ms
step:486/2330 train_time:28036ms step_avg:57.69ms
step:487/2330 train_time:28092ms step_avg:57.68ms
step:488/2330 train_time:28152ms step_avg:57.69ms
step:489/2330 train_time:28208ms step_avg:57.68ms
step:490/2330 train_time:28267ms step_avg:57.69ms
step:491/2330 train_time:28322ms step_avg:57.68ms
step:492/2330 train_time:28383ms step_avg:57.69ms
step:493/2330 train_time:28439ms step_avg:57.69ms
step:494/2330 train_time:28498ms step_avg:57.69ms
step:495/2330 train_time:28554ms step_avg:57.68ms
step:496/2330 train_time:28613ms step_avg:57.69ms
step:497/2330 train_time:28669ms step_avg:57.68ms
step:498/2330 train_time:28729ms step_avg:57.69ms
step:499/2330 train_time:28784ms step_avg:57.68ms
step:500/2330 train_time:28845ms step_avg:57.69ms
step:500/2330 val_loss:4.3924 train_time:28925ms step_avg:57.85ms
step:501/2330 train_time:28943ms step_avg:57.77ms
step:502/2330 train_time:28963ms step_avg:57.69ms
step:503/2330 train_time:29021ms step_avg:57.70ms
step:504/2330 train_time:29085ms step_avg:57.71ms
step:505/2330 train_time:29141ms step_avg:57.70ms
step:506/2330 train_time:29203ms step_avg:57.71ms
step:507/2330 train_time:29258ms step_avg:57.71ms
step:508/2330 train_time:29318ms step_avg:57.71ms
step:509/2330 train_time:29374ms step_avg:57.71ms
step:510/2330 train_time:29434ms step_avg:57.71ms
step:511/2330 train_time:29490ms step_avg:57.71ms
step:512/2330 train_time:29548ms step_avg:57.71ms
step:513/2330 train_time:29603ms step_avg:57.71ms
step:514/2330 train_time:29663ms step_avg:57.71ms
step:515/2330 train_time:29718ms step_avg:57.70ms
step:516/2330 train_time:29777ms step_avg:57.71ms
step:517/2330 train_time:29832ms step_avg:57.70ms
step:518/2330 train_time:29892ms step_avg:57.71ms
step:519/2330 train_time:29948ms step_avg:57.70ms
step:520/2330 train_time:30012ms step_avg:57.72ms
step:521/2330 train_time:30068ms step_avg:57.71ms
step:522/2330 train_time:30131ms step_avg:57.72ms
step:523/2330 train_time:30186ms step_avg:57.72ms
step:524/2330 train_time:30247ms step_avg:57.72ms
step:525/2330 train_time:30303ms step_avg:57.72ms
step:526/2330 train_time:30362ms step_avg:57.72ms
step:527/2330 train_time:30418ms step_avg:57.72ms
step:528/2330 train_time:30477ms step_avg:57.72ms
step:529/2330 train_time:30533ms step_avg:57.72ms
step:530/2330 train_time:30592ms step_avg:57.72ms
step:531/2330 train_time:30647ms step_avg:57.72ms
step:532/2330 train_time:30705ms step_avg:57.72ms
step:533/2330 train_time:30761ms step_avg:57.71ms
step:534/2330 train_time:30820ms step_avg:57.72ms
step:535/2330 train_time:30876ms step_avg:57.71ms
step:536/2330 train_time:30936ms step_avg:57.72ms
step:537/2330 train_time:30994ms step_avg:57.72ms
step:538/2330 train_time:31054ms step_avg:57.72ms
step:539/2330 train_time:31111ms step_avg:57.72ms
step:540/2330 train_time:31171ms step_avg:57.72ms
step:541/2330 train_time:31228ms step_avg:57.72ms
step:542/2330 train_time:31288ms step_avg:57.73ms
step:543/2330 train_time:31344ms step_avg:57.72ms
step:544/2330 train_time:31403ms step_avg:57.73ms
step:545/2330 train_time:31459ms step_avg:57.72ms
step:546/2330 train_time:31519ms step_avg:57.73ms
step:547/2330 train_time:31574ms step_avg:57.72ms
step:548/2330 train_time:31633ms step_avg:57.72ms
step:549/2330 train_time:31689ms step_avg:57.72ms
step:550/2330 train_time:31749ms step_avg:57.72ms
step:551/2330 train_time:31804ms step_avg:57.72ms
step:552/2330 train_time:31865ms step_avg:57.73ms
step:553/2330 train_time:31921ms step_avg:57.72ms
step:554/2330 train_time:31981ms step_avg:57.73ms
step:555/2330 train_time:32037ms step_avg:57.72ms
step:556/2330 train_time:32097ms step_avg:57.73ms
step:557/2330 train_time:32154ms step_avg:57.73ms
step:558/2330 train_time:32214ms step_avg:57.73ms
step:559/2330 train_time:32272ms step_avg:57.73ms
step:560/2330 train_time:32331ms step_avg:57.73ms
step:561/2330 train_time:32387ms step_avg:57.73ms
step:562/2330 train_time:32447ms step_avg:57.73ms
step:563/2330 train_time:32503ms step_avg:57.73ms
step:564/2330 train_time:32563ms step_avg:57.74ms
step:565/2330 train_time:32619ms step_avg:57.73ms
step:566/2330 train_time:32677ms step_avg:57.73ms
step:567/2330 train_time:32733ms step_avg:57.73ms
step:568/2330 train_time:32792ms step_avg:57.73ms
step:569/2330 train_time:32848ms step_avg:57.73ms
step:570/2330 train_time:32908ms step_avg:57.73ms
step:571/2330 train_time:32964ms step_avg:57.73ms
step:572/2330 train_time:33024ms step_avg:57.73ms
step:573/2330 train_time:33079ms step_avg:57.73ms
step:574/2330 train_time:33140ms step_avg:57.73ms
step:575/2330 train_time:33196ms step_avg:57.73ms
step:576/2330 train_time:33256ms step_avg:57.74ms
step:577/2330 train_time:33313ms step_avg:57.73ms
step:578/2330 train_time:33373ms step_avg:57.74ms
step:579/2330 train_time:33430ms step_avg:57.74ms
step:580/2330 train_time:33489ms step_avg:57.74ms
step:581/2330 train_time:33544ms step_avg:57.74ms
step:582/2330 train_time:33603ms step_avg:57.74ms
step:583/2330 train_time:33659ms step_avg:57.73ms
step:584/2330 train_time:33719ms step_avg:57.74ms
step:585/2330 train_time:33775ms step_avg:57.74ms
step:586/2330 train_time:33834ms step_avg:57.74ms
step:587/2330 train_time:33890ms step_avg:57.73ms
step:588/2330 train_time:33949ms step_avg:57.74ms
step:589/2330 train_time:34006ms step_avg:57.73ms
step:590/2330 train_time:34065ms step_avg:57.74ms
step:591/2330 train_time:34121ms step_avg:57.73ms
step:592/2330 train_time:34181ms step_avg:57.74ms
step:593/2330 train_time:34238ms step_avg:57.74ms
step:594/2330 train_time:34298ms step_avg:57.74ms
step:595/2330 train_time:34355ms step_avg:57.74ms
step:596/2330 train_time:34414ms step_avg:57.74ms
step:597/2330 train_time:34472ms step_avg:57.74ms
step:598/2330 train_time:34530ms step_avg:57.74ms
step:599/2330 train_time:34586ms step_avg:57.74ms
step:600/2330 train_time:34644ms step_avg:57.74ms
step:601/2330 train_time:34699ms step_avg:57.74ms
step:602/2330 train_time:34760ms step_avg:57.74ms
step:603/2330 train_time:34816ms step_avg:57.74ms
step:604/2330 train_time:34876ms step_avg:57.74ms
step:605/2330 train_time:34932ms step_avg:57.74ms
step:606/2330 train_time:34992ms step_avg:57.74ms
step:607/2330 train_time:35048ms step_avg:57.74ms
step:608/2330 train_time:35107ms step_avg:57.74ms
step:609/2330 train_time:35164ms step_avg:57.74ms
step:610/2330 train_time:35224ms step_avg:57.74ms
step:611/2330 train_time:35279ms step_avg:57.74ms
step:612/2330 train_time:35341ms step_avg:57.75ms
step:613/2330 train_time:35397ms step_avg:57.74ms
step:614/2330 train_time:35456ms step_avg:57.75ms
step:615/2330 train_time:35513ms step_avg:57.74ms
step:616/2330 train_time:35572ms step_avg:57.75ms
step:617/2330 train_time:35627ms step_avg:57.74ms
step:618/2330 train_time:35686ms step_avg:57.74ms
step:619/2330 train_time:35742ms step_avg:57.74ms
step:620/2330 train_time:35802ms step_avg:57.75ms
step:621/2330 train_time:35858ms step_avg:57.74ms
step:622/2330 train_time:35918ms step_avg:57.75ms
step:623/2330 train_time:35974ms step_avg:57.74ms
step:624/2330 train_time:36034ms step_avg:57.75ms
step:625/2330 train_time:36091ms step_avg:57.75ms
step:626/2330 train_time:36150ms step_avg:57.75ms
step:627/2330 train_time:36206ms step_avg:57.75ms
step:628/2330 train_time:36266ms step_avg:57.75ms
step:629/2330 train_time:36322ms step_avg:57.75ms
step:630/2330 train_time:36382ms step_avg:57.75ms
step:631/2330 train_time:36437ms step_avg:57.75ms
step:632/2330 train_time:36497ms step_avg:57.75ms
step:633/2330 train_time:36555ms step_avg:57.75ms
step:634/2330 train_time:36614ms step_avg:57.75ms
step:635/2330 train_time:36670ms step_avg:57.75ms
step:636/2330 train_time:36729ms step_avg:57.75ms
step:637/2330 train_time:36784ms step_avg:57.75ms
step:638/2330 train_time:36843ms step_avg:57.75ms
step:639/2330 train_time:36899ms step_avg:57.75ms
step:640/2330 train_time:36960ms step_avg:57.75ms
step:641/2330 train_time:37016ms step_avg:57.75ms
step:642/2330 train_time:37077ms step_avg:57.75ms
step:643/2330 train_time:37133ms step_avg:57.75ms
step:644/2330 train_time:37195ms step_avg:57.76ms
step:645/2330 train_time:37251ms step_avg:57.75ms
step:646/2330 train_time:37311ms step_avg:57.76ms
step:647/2330 train_time:37367ms step_avg:57.75ms
step:648/2330 train_time:37426ms step_avg:57.76ms
step:649/2330 train_time:37482ms step_avg:57.75ms
step:650/2330 train_time:37541ms step_avg:57.76ms
step:651/2330 train_time:37597ms step_avg:57.75ms
step:652/2330 train_time:37657ms step_avg:57.76ms
step:653/2330 train_time:37714ms step_avg:57.76ms
step:654/2330 train_time:37773ms step_avg:57.76ms
step:655/2330 train_time:37828ms step_avg:57.75ms
step:656/2330 train_time:37887ms step_avg:57.76ms
step:657/2330 train_time:37943ms step_avg:57.75ms
step:658/2330 train_time:38004ms step_avg:57.76ms
step:659/2330 train_time:38059ms step_avg:57.75ms
step:660/2330 train_time:38119ms step_avg:57.76ms
step:661/2330 train_time:38175ms step_avg:57.75ms
step:662/2330 train_time:38235ms step_avg:57.76ms
step:663/2330 train_time:38292ms step_avg:57.76ms
step:664/2330 train_time:38353ms step_avg:57.76ms
step:665/2330 train_time:38409ms step_avg:57.76ms
step:666/2330 train_time:38468ms step_avg:57.76ms
step:667/2330 train_time:38525ms step_avg:57.76ms
step:668/2330 train_time:38584ms step_avg:57.76ms
step:669/2330 train_time:38640ms step_avg:57.76ms
step:670/2330 train_time:38699ms step_avg:57.76ms
step:671/2330 train_time:38755ms step_avg:57.76ms
step:672/2330 train_time:38815ms step_avg:57.76ms
step:673/2330 train_time:38871ms step_avg:57.76ms
step:674/2330 train_time:38930ms step_avg:57.76ms
step:675/2330 train_time:38987ms step_avg:57.76ms
step:676/2330 train_time:39046ms step_avg:57.76ms
step:677/2330 train_time:39102ms step_avg:57.76ms
step:678/2330 train_time:39163ms step_avg:57.76ms
step:679/2330 train_time:39218ms step_avg:57.76ms
step:680/2330 train_time:39279ms step_avg:57.76ms
step:681/2330 train_time:39335ms step_avg:57.76ms
step:682/2330 train_time:39394ms step_avg:57.76ms
step:683/2330 train_time:39451ms step_avg:57.76ms
step:684/2330 train_time:39511ms step_avg:57.76ms
step:685/2330 train_time:39566ms step_avg:57.76ms
step:686/2330 train_time:39626ms step_avg:57.76ms
step:687/2330 train_time:39682ms step_avg:57.76ms
step:688/2330 train_time:39742ms step_avg:57.76ms
step:689/2330 train_time:39798ms step_avg:57.76ms
step:690/2330 train_time:39857ms step_avg:57.76ms
step:691/2330 train_time:39913ms step_avg:57.76ms
step:692/2330 train_time:39973ms step_avg:57.76ms
step:693/2330 train_time:40029ms step_avg:57.76ms
step:694/2330 train_time:40090ms step_avg:57.77ms
step:695/2330 train_time:40146ms step_avg:57.76ms
step:696/2330 train_time:40205ms step_avg:57.77ms
step:697/2330 train_time:40261ms step_avg:57.76ms
step:698/2330 train_time:40321ms step_avg:57.77ms
step:699/2330 train_time:40377ms step_avg:57.76ms
step:700/2330 train_time:40436ms step_avg:57.77ms
step:701/2330 train_time:40493ms step_avg:57.76ms
step:702/2330 train_time:40553ms step_avg:57.77ms
step:703/2330 train_time:40609ms step_avg:57.77ms
step:704/2330 train_time:40669ms step_avg:57.77ms
step:705/2330 train_time:40724ms step_avg:57.77ms
step:706/2330 train_time:40784ms step_avg:57.77ms
step:707/2330 train_time:40840ms step_avg:57.77ms
step:708/2330 train_time:40899ms step_avg:57.77ms
step:709/2330 train_time:40956ms step_avg:57.77ms
step:710/2330 train_time:41017ms step_avg:57.77ms
step:711/2330 train_time:41073ms step_avg:57.77ms
step:712/2330 train_time:41132ms step_avg:57.77ms
step:713/2330 train_time:41188ms step_avg:57.77ms
step:714/2330 train_time:41249ms step_avg:57.77ms
step:715/2330 train_time:41305ms step_avg:57.77ms
step:716/2330 train_time:41364ms step_avg:57.77ms
step:717/2330 train_time:41420ms step_avg:57.77ms
step:718/2330 train_time:41480ms step_avg:57.77ms
step:719/2330 train_time:41536ms step_avg:57.77ms
step:720/2330 train_time:41597ms step_avg:57.77ms
step:721/2330 train_time:41653ms step_avg:57.77ms
step:722/2330 train_time:41712ms step_avg:57.77ms
step:723/2330 train_time:41769ms step_avg:57.77ms
step:724/2330 train_time:41828ms step_avg:57.77ms
step:725/2330 train_time:41884ms step_avg:57.77ms
step:726/2330 train_time:41945ms step_avg:57.77ms
step:727/2330 train_time:42000ms step_avg:57.77ms
step:728/2330 train_time:42061ms step_avg:57.78ms
step:729/2330 train_time:42117ms step_avg:57.77ms
step:730/2330 train_time:42177ms step_avg:57.78ms
step:731/2330 train_time:42234ms step_avg:57.77ms
step:732/2330 train_time:42293ms step_avg:57.78ms
step:733/2330 train_time:42349ms step_avg:57.78ms
step:734/2330 train_time:42409ms step_avg:57.78ms
step:735/2330 train_time:42465ms step_avg:57.77ms
step:736/2330 train_time:42525ms step_avg:57.78ms
step:737/2330 train_time:42580ms step_avg:57.77ms
step:738/2330 train_time:42640ms step_avg:57.78ms
step:739/2330 train_time:42696ms step_avg:57.78ms
step:740/2330 train_time:42756ms step_avg:57.78ms
step:741/2330 train_time:42812ms step_avg:57.78ms
step:742/2330 train_time:42872ms step_avg:57.78ms
step:743/2330 train_time:42928ms step_avg:57.78ms
step:744/2330 train_time:42987ms step_avg:57.78ms
step:745/2330 train_time:43043ms step_avg:57.78ms
step:746/2330 train_time:43103ms step_avg:57.78ms
step:747/2330 train_time:43158ms step_avg:57.78ms
step:748/2330 train_time:43219ms step_avg:57.78ms
step:749/2330 train_time:43276ms step_avg:57.78ms
step:750/2330 train_time:43335ms step_avg:57.78ms
step:750/2330 val_loss:4.2092 train_time:43414ms step_avg:57.89ms
step:751/2330 train_time:43432ms step_avg:57.83ms
step:752/2330 train_time:43452ms step_avg:57.78ms
step:753/2330 train_time:43510ms step_avg:57.78ms
step:754/2330 train_time:43574ms step_avg:57.79ms
step:755/2330 train_time:43630ms step_avg:57.79ms
step:756/2330 train_time:43691ms step_avg:57.79ms
step:757/2330 train_time:43747ms step_avg:57.79ms
step:758/2330 train_time:43806ms step_avg:57.79ms
step:759/2330 train_time:43861ms step_avg:57.79ms
step:760/2330 train_time:43920ms step_avg:57.79ms
step:761/2330 train_time:43975ms step_avg:57.79ms
step:762/2330 train_time:44034ms step_avg:57.79ms
step:763/2330 train_time:44090ms step_avg:57.78ms
step:764/2330 train_time:44148ms step_avg:57.78ms
step:765/2330 train_time:44204ms step_avg:57.78ms
step:766/2330 train_time:44263ms step_avg:57.78ms
step:767/2330 train_time:44319ms step_avg:57.78ms
step:768/2330 train_time:44379ms step_avg:57.79ms
step:769/2330 train_time:44437ms step_avg:57.78ms
step:770/2330 train_time:44499ms step_avg:57.79ms
step:771/2330 train_time:44557ms step_avg:57.79ms
step:772/2330 train_time:44619ms step_avg:57.80ms
step:773/2330 train_time:44676ms step_avg:57.80ms
step:774/2330 train_time:44737ms step_avg:57.80ms
step:775/2330 train_time:44794ms step_avg:57.80ms
step:776/2330 train_time:44854ms step_avg:57.80ms
step:777/2330 train_time:44911ms step_avg:57.80ms
step:778/2330 train_time:44970ms step_avg:57.80ms
step:779/2330 train_time:45026ms step_avg:57.80ms
step:780/2330 train_time:45085ms step_avg:57.80ms
step:781/2330 train_time:45141ms step_avg:57.80ms
step:782/2330 train_time:45201ms step_avg:57.80ms
step:783/2330 train_time:45257ms step_avg:57.80ms
step:784/2330 train_time:45317ms step_avg:57.80ms
step:785/2330 train_time:45374ms step_avg:57.80ms
step:786/2330 train_time:45434ms step_avg:57.80ms
step:787/2330 train_time:45492ms step_avg:57.80ms
step:788/2330 train_time:45553ms step_avg:57.81ms
step:789/2330 train_time:45610ms step_avg:57.81ms
step:790/2330 train_time:45671ms step_avg:57.81ms
step:791/2330 train_time:45728ms step_avg:57.81ms
step:792/2330 train_time:45789ms step_avg:57.81ms
step:793/2330 train_time:45845ms step_avg:57.81ms
step:794/2330 train_time:45906ms step_avg:57.82ms
step:795/2330 train_time:45963ms step_avg:57.81ms
step:796/2330 train_time:46022ms step_avg:57.82ms
step:797/2330 train_time:46078ms step_avg:57.81ms
step:798/2330 train_time:46138ms step_avg:57.82ms
step:799/2330 train_time:46194ms step_avg:57.81ms
step:800/2330 train_time:46254ms step_avg:57.82ms
step:801/2330 train_time:46311ms step_avg:57.82ms
step:802/2330 train_time:46370ms step_avg:57.82ms
step:803/2330 train_time:46427ms step_avg:57.82ms
step:804/2330 train_time:46488ms step_avg:57.82ms
step:805/2330 train_time:46545ms step_avg:57.82ms
step:806/2330 train_time:46606ms step_avg:57.82ms
step:807/2330 train_time:46663ms step_avg:57.82ms
step:808/2330 train_time:46724ms step_avg:57.83ms
step:809/2330 train_time:46781ms step_avg:57.83ms
step:810/2330 train_time:46842ms step_avg:57.83ms
step:811/2330 train_time:46899ms step_avg:57.83ms
step:812/2330 train_time:46960ms step_avg:57.83ms
step:813/2330 train_time:47016ms step_avg:57.83ms
step:814/2330 train_time:47077ms step_avg:57.83ms
step:815/2330 train_time:47133ms step_avg:57.83ms
step:816/2330 train_time:47192ms step_avg:57.83ms
step:817/2330 train_time:47249ms step_avg:57.83ms
step:818/2330 train_time:47309ms step_avg:57.83ms
step:819/2330 train_time:47365ms step_avg:57.83ms
step:820/2330 train_time:47425ms step_avg:57.84ms
step:821/2330 train_time:47482ms step_avg:57.83ms
step:822/2330 train_time:47543ms step_avg:57.84ms
step:823/2330 train_time:47600ms step_avg:57.84ms
step:824/2330 train_time:47661ms step_avg:57.84ms
step:825/2330 train_time:47718ms step_avg:57.84ms
step:826/2330 train_time:47779ms step_avg:57.84ms
step:827/2330 train_time:47835ms step_avg:57.84ms
step:828/2330 train_time:47897ms step_avg:57.85ms
step:829/2330 train_time:47954ms step_avg:57.85ms
step:830/2330 train_time:48013ms step_avg:57.85ms
step:831/2330 train_time:48070ms step_avg:57.85ms
step:832/2330 train_time:48131ms step_avg:57.85ms
step:833/2330 train_time:48187ms step_avg:57.85ms
step:834/2330 train_time:48246ms step_avg:57.85ms
step:835/2330 train_time:48303ms step_avg:57.85ms
step:836/2330 train_time:48363ms step_avg:57.85ms
step:837/2330 train_time:48420ms step_avg:57.85ms
step:838/2330 train_time:48480ms step_avg:57.85ms
step:839/2330 train_time:48537ms step_avg:57.85ms
step:840/2330 train_time:48598ms step_avg:57.85ms
step:841/2330 train_time:48655ms step_avg:57.85ms
step:842/2330 train_time:48716ms step_avg:57.86ms
step:843/2330 train_time:48773ms step_avg:57.86ms
step:844/2330 train_time:48834ms step_avg:57.86ms
step:845/2330 train_time:48890ms step_avg:57.86ms
step:846/2330 train_time:48950ms step_avg:57.86ms
step:847/2330 train_time:49007ms step_avg:57.86ms
step:848/2330 train_time:49067ms step_avg:57.86ms
step:849/2330 train_time:49124ms step_avg:57.86ms
step:850/2330 train_time:49184ms step_avg:57.86ms
step:851/2330 train_time:49240ms step_avg:57.86ms
step:852/2330 train_time:49301ms step_avg:57.87ms
step:853/2330 train_time:49358ms step_avg:57.86ms
step:854/2330 train_time:49418ms step_avg:57.87ms
step:855/2330 train_time:49475ms step_avg:57.87ms
step:856/2330 train_time:49535ms step_avg:57.87ms
step:857/2330 train_time:49592ms step_avg:57.87ms
step:858/2330 train_time:49652ms step_avg:57.87ms
step:859/2330 train_time:49710ms step_avg:57.87ms
step:860/2330 train_time:49770ms step_avg:57.87ms
step:861/2330 train_time:49827ms step_avg:57.87ms
step:862/2330 train_time:49886ms step_avg:57.87ms
step:863/2330 train_time:49943ms step_avg:57.87ms
step:864/2330 train_time:50003ms step_avg:57.87ms
step:865/2330 train_time:50060ms step_avg:57.87ms
step:866/2330 train_time:50119ms step_avg:57.87ms
step:867/2330 train_time:50176ms step_avg:57.87ms
step:868/2330 train_time:50236ms step_avg:57.88ms
step:869/2330 train_time:50293ms step_avg:57.87ms
step:870/2330 train_time:50352ms step_avg:57.88ms
step:871/2330 train_time:50410ms step_avg:57.88ms
step:872/2330 train_time:50470ms step_avg:57.88ms
step:873/2330 train_time:50527ms step_avg:57.88ms
step:874/2330 train_time:50588ms step_avg:57.88ms
step:875/2330 train_time:50644ms step_avg:57.88ms
step:876/2330 train_time:50706ms step_avg:57.88ms
step:877/2330 train_time:50763ms step_avg:57.88ms
step:878/2330 train_time:50822ms step_avg:57.88ms
step:879/2330 train_time:50878ms step_avg:57.88ms
step:880/2330 train_time:50939ms step_avg:57.89ms
step:881/2330 train_time:50996ms step_avg:57.88ms
step:882/2330 train_time:51056ms step_avg:57.89ms
step:883/2330 train_time:51112ms step_avg:57.88ms
step:884/2330 train_time:51173ms step_avg:57.89ms
step:885/2330 train_time:51230ms step_avg:57.89ms
step:886/2330 train_time:51289ms step_avg:57.89ms
step:887/2330 train_time:51346ms step_avg:57.89ms
step:888/2330 train_time:51407ms step_avg:57.89ms
step:889/2330 train_time:51464ms step_avg:57.89ms
step:890/2330 train_time:51523ms step_avg:57.89ms
step:891/2330 train_time:51580ms step_avg:57.89ms
step:892/2330 train_time:51642ms step_avg:57.89ms
step:893/2330 train_time:51698ms step_avg:57.89ms
step:894/2330 train_time:51759ms step_avg:57.90ms
step:895/2330 train_time:51816ms step_avg:57.89ms
step:896/2330 train_time:51876ms step_avg:57.90ms
step:897/2330 train_time:51932ms step_avg:57.90ms
step:898/2330 train_time:51993ms step_avg:57.90ms
step:899/2330 train_time:52050ms step_avg:57.90ms
step:900/2330 train_time:52110ms step_avg:57.90ms
step:901/2330 train_time:52166ms step_avg:57.90ms
step:902/2330 train_time:52226ms step_avg:57.90ms
step:903/2330 train_time:52282ms step_avg:57.90ms
step:904/2330 train_time:52342ms step_avg:57.90ms
step:905/2330 train_time:52399ms step_avg:57.90ms
step:906/2330 train_time:52460ms step_avg:57.90ms
step:907/2330 train_time:52517ms step_avg:57.90ms
step:908/2330 train_time:52577ms step_avg:57.90ms
step:909/2330 train_time:52634ms step_avg:57.90ms
step:910/2330 train_time:52695ms step_avg:57.91ms
step:911/2330 train_time:52752ms step_avg:57.91ms
step:912/2330 train_time:52812ms step_avg:57.91ms
step:913/2330 train_time:52869ms step_avg:57.91ms
step:914/2330 train_time:52929ms step_avg:57.91ms
step:915/2330 train_time:52986ms step_avg:57.91ms
step:916/2330 train_time:53045ms step_avg:57.91ms
step:917/2330 train_time:53102ms step_avg:57.91ms
step:918/2330 train_time:53162ms step_avg:57.91ms
step:919/2330 train_time:53219ms step_avg:57.91ms
step:920/2330 train_time:53278ms step_avg:57.91ms
step:921/2330 train_time:53335ms step_avg:57.91ms
step:922/2330 train_time:53395ms step_avg:57.91ms
step:923/2330 train_time:53452ms step_avg:57.91ms
step:924/2330 train_time:53512ms step_avg:57.91ms
step:925/2330 train_time:53569ms step_avg:57.91ms
step:926/2330 train_time:53629ms step_avg:57.91ms
step:927/2330 train_time:53685ms step_avg:57.91ms
step:928/2330 train_time:53746ms step_avg:57.92ms
step:929/2330 train_time:53802ms step_avg:57.91ms
step:930/2330 train_time:53863ms step_avg:57.92ms
step:931/2330 train_time:53920ms step_avg:57.92ms
step:932/2330 train_time:53980ms step_avg:57.92ms
step:933/2330 train_time:54037ms step_avg:57.92ms
step:934/2330 train_time:54098ms step_avg:57.92ms
step:935/2330 train_time:54154ms step_avg:57.92ms
step:936/2330 train_time:54215ms step_avg:57.92ms
step:937/2330 train_time:54271ms step_avg:57.92ms
step:938/2330 train_time:54331ms step_avg:57.92ms
step:939/2330 train_time:54387ms step_avg:57.92ms
step:940/2330 train_time:54448ms step_avg:57.92ms
step:941/2330 train_time:54505ms step_avg:57.92ms
step:942/2330 train_time:54565ms step_avg:57.92ms
step:943/2330 train_time:54622ms step_avg:57.92ms
step:944/2330 train_time:54682ms step_avg:57.93ms
step:945/2330 train_time:54739ms step_avg:57.93ms
step:946/2330 train_time:54800ms step_avg:57.93ms
step:947/2330 train_time:54857ms step_avg:57.93ms
step:948/2330 train_time:54917ms step_avg:57.93ms
step:949/2330 train_time:54974ms step_avg:57.93ms
step:950/2330 train_time:55035ms step_avg:57.93ms
step:951/2330 train_time:55091ms step_avg:57.93ms
step:952/2330 train_time:55152ms step_avg:57.93ms
step:953/2330 train_time:55209ms step_avg:57.93ms
step:954/2330 train_time:55268ms step_avg:57.93ms
step:955/2330 train_time:55324ms step_avg:57.93ms
step:956/2330 train_time:55386ms step_avg:57.93ms
step:957/2330 train_time:55443ms step_avg:57.93ms
step:958/2330 train_time:55503ms step_avg:57.94ms
step:959/2330 train_time:55561ms step_avg:57.94ms
step:960/2330 train_time:55620ms step_avg:57.94ms
step:961/2330 train_time:55676ms step_avg:57.94ms
step:962/2330 train_time:55736ms step_avg:57.94ms
step:963/2330 train_time:55793ms step_avg:57.94ms
step:964/2330 train_time:55853ms step_avg:57.94ms
step:965/2330 train_time:55911ms step_avg:57.94ms
step:966/2330 train_time:55971ms step_avg:57.94ms
step:967/2330 train_time:56028ms step_avg:57.94ms
step:968/2330 train_time:56087ms step_avg:57.94ms
step:969/2330 train_time:56144ms step_avg:57.94ms
step:970/2330 train_time:56203ms step_avg:57.94ms
step:971/2330 train_time:56260ms step_avg:57.94ms
step:972/2330 train_time:56321ms step_avg:57.94ms
step:973/2330 train_time:56377ms step_avg:57.94ms
step:974/2330 train_time:56438ms step_avg:57.94ms
step:975/2330 train_time:56495ms step_avg:57.94ms
step:976/2330 train_time:56555ms step_avg:57.95ms
step:977/2330 train_time:56612ms step_avg:57.95ms
step:978/2330 train_time:56672ms step_avg:57.95ms
step:979/2330 train_time:56729ms step_avg:57.95ms
step:980/2330 train_time:56789ms step_avg:57.95ms
step:981/2330 train_time:56844ms step_avg:57.95ms
step:982/2330 train_time:56906ms step_avg:57.95ms
step:983/2330 train_time:56963ms step_avg:57.95ms
step:984/2330 train_time:57024ms step_avg:57.95ms
step:985/2330 train_time:57081ms step_avg:57.95ms
step:986/2330 train_time:57141ms step_avg:57.95ms
step:987/2330 train_time:57198ms step_avg:57.95ms
step:988/2330 train_time:57259ms step_avg:57.95ms
step:989/2330 train_time:57316ms step_avg:57.95ms
step:990/2330 train_time:57376ms step_avg:57.96ms
step:991/2330 train_time:57433ms step_avg:57.95ms
step:992/2330 train_time:57492ms step_avg:57.96ms
step:993/2330 train_time:57549ms step_avg:57.96ms
step:994/2330 train_time:57610ms step_avg:57.96ms
step:995/2330 train_time:57667ms step_avg:57.96ms
step:996/2330 train_time:57727ms step_avg:57.96ms
step:997/2330 train_time:57784ms step_avg:57.96ms
step:998/2330 train_time:57845ms step_avg:57.96ms
step:999/2330 train_time:57903ms step_avg:57.96ms
step:1000/2330 train_time:57962ms step_avg:57.96ms
step:1000/2330 val_loss:4.0629 train_time:58043ms step_avg:58.04ms
step:1001/2330 train_time:58062ms step_avg:58.00ms
step:1002/2330 train_time:58083ms step_avg:57.97ms
step:1003/2330 train_time:58136ms step_avg:57.96ms
step:1004/2330 train_time:58205ms step_avg:57.97ms
step:1005/2330 train_time:58261ms step_avg:57.97ms
step:1006/2330 train_time:58324ms step_avg:57.98ms
step:1007/2330 train_time:58381ms step_avg:57.98ms
step:1008/2330 train_time:58441ms step_avg:57.98ms
step:1009/2330 train_time:58496ms step_avg:57.97ms
step:1010/2330 train_time:58556ms step_avg:57.98ms
step:1011/2330 train_time:58612ms step_avg:57.97ms
step:1012/2330 train_time:58671ms step_avg:57.98ms
step:1013/2330 train_time:58727ms step_avg:57.97ms
step:1014/2330 train_time:58787ms step_avg:57.98ms
step:1015/2330 train_time:58843ms step_avg:57.97ms
step:1016/2330 train_time:58902ms step_avg:57.97ms
step:1017/2330 train_time:58959ms step_avg:57.97ms
step:1018/2330 train_time:59022ms step_avg:57.98ms
step:1019/2330 train_time:59081ms step_avg:57.98ms
step:1020/2330 train_time:59142ms step_avg:57.98ms
step:1021/2330 train_time:59199ms step_avg:57.98ms
step:1022/2330 train_time:59261ms step_avg:57.99ms
step:1023/2330 train_time:59318ms step_avg:57.98ms
step:1024/2330 train_time:59378ms step_avg:57.99ms
step:1025/2330 train_time:59435ms step_avg:57.99ms
step:1026/2330 train_time:59495ms step_avg:57.99ms
step:1027/2330 train_time:59551ms step_avg:57.99ms
step:1028/2330 train_time:59611ms step_avg:57.99ms
step:1029/2330 train_time:59668ms step_avg:57.99ms
step:1030/2330 train_time:59728ms step_avg:57.99ms
step:1031/2330 train_time:59784ms step_avg:57.99ms
step:1032/2330 train_time:59843ms step_avg:57.99ms
step:1033/2330 train_time:59900ms step_avg:57.99ms
step:1034/2330 train_time:59960ms step_avg:57.99ms
step:1035/2330 train_time:60018ms step_avg:57.99ms
step:1036/2330 train_time:60079ms step_avg:57.99ms
step:1037/2330 train_time:60137ms step_avg:57.99ms
step:1038/2330 train_time:60198ms step_avg:57.99ms
step:1039/2330 train_time:60255ms step_avg:57.99ms
step:1040/2330 train_time:60316ms step_avg:58.00ms
step:1041/2330 train_time:60373ms step_avg:58.00ms
step:1042/2330 train_time:60434ms step_avg:58.00ms
step:1043/2330 train_time:60491ms step_avg:58.00ms
step:1044/2330 train_time:60551ms step_avg:58.00ms
step:1045/2330 train_time:60608ms step_avg:58.00ms
step:1046/2330 train_time:60667ms step_avg:58.00ms
step:1047/2330 train_time:60724ms step_avg:58.00ms
step:1048/2330 train_time:60783ms step_avg:58.00ms
step:1049/2330 train_time:60839ms step_avg:58.00ms
step:1050/2330 train_time:60899ms step_avg:58.00ms
step:1051/2330 train_time:60955ms step_avg:58.00ms
step:1052/2330 train_time:61016ms step_avg:58.00ms
step:1053/2330 train_time:61074ms step_avg:58.00ms
step:1054/2330 train_time:61135ms step_avg:58.00ms
step:1055/2330 train_time:61192ms step_avg:58.00ms
step:1056/2330 train_time:61254ms step_avg:58.01ms
step:1057/2330 train_time:61311ms step_avg:58.00ms
step:1058/2330 train_time:61371ms step_avg:58.01ms
step:1059/2330 train_time:61428ms step_avg:58.01ms
step:1060/2330 train_time:61489ms step_avg:58.01ms
step:1061/2330 train_time:61546ms step_avg:58.01ms
step:1062/2330 train_time:61606ms step_avg:58.01ms
step:1063/2330 train_time:61662ms step_avg:58.01ms
step:1064/2330 train_time:61722ms step_avg:58.01ms
step:1065/2330 train_time:61778ms step_avg:58.01ms
step:1066/2330 train_time:61838ms step_avg:58.01ms
step:1067/2330 train_time:61895ms step_avg:58.01ms
step:1068/2330 train_time:61955ms step_avg:58.01ms
step:1069/2330 train_time:62012ms step_avg:58.01ms
step:1070/2330 train_time:62072ms step_avg:58.01ms
step:1071/2330 train_time:62130ms step_avg:58.01ms
step:1072/2330 train_time:62190ms step_avg:58.01ms
step:1073/2330 train_time:62247ms step_avg:58.01ms
step:1074/2330 train_time:62307ms step_avg:58.01ms
step:1075/2330 train_time:62364ms step_avg:58.01ms
step:1076/2330 train_time:62425ms step_avg:58.02ms
step:1077/2330 train_time:62482ms step_avg:58.02ms
step:1078/2330 train_time:62543ms step_avg:58.02ms
step:1079/2330 train_time:62599ms step_avg:58.02ms
step:1080/2330 train_time:62660ms step_avg:58.02ms
step:1081/2330 train_time:62716ms step_avg:58.02ms
step:1082/2330 train_time:62776ms step_avg:58.02ms
step:1083/2330 train_time:62833ms step_avg:58.02ms
step:1084/2330 train_time:62893ms step_avg:58.02ms
step:1085/2330 train_time:62949ms step_avg:58.02ms
step:1086/2330 train_time:63010ms step_avg:58.02ms
step:1087/2330 train_time:63067ms step_avg:58.02ms
step:1088/2330 train_time:63128ms step_avg:58.02ms
step:1089/2330 train_time:63184ms step_avg:58.02ms
step:1090/2330 train_time:63245ms step_avg:58.02ms
step:1091/2330 train_time:63301ms step_avg:58.02ms
step:1092/2330 train_time:63363ms step_avg:58.02ms
step:1093/2330 train_time:63420ms step_avg:58.02ms
step:1094/2330 train_time:63480ms step_avg:58.03ms
step:1095/2330 train_time:63537ms step_avg:58.02ms
step:1096/2330 train_time:63598ms step_avg:58.03ms
step:1097/2330 train_time:63655ms step_avg:58.03ms
step:1098/2330 train_time:63714ms step_avg:58.03ms
step:1099/2330 train_time:63771ms step_avg:58.03ms
step:1100/2330 train_time:63831ms step_avg:58.03ms
step:1101/2330 train_time:63887ms step_avg:58.03ms
step:1102/2330 train_time:63947ms step_avg:58.03ms
step:1103/2330 train_time:64004ms step_avg:58.03ms
step:1104/2330 train_time:64064ms step_avg:58.03ms
step:1105/2330 train_time:64122ms step_avg:58.03ms
step:1106/2330 train_time:64182ms step_avg:58.03ms
step:1107/2330 train_time:64239ms step_avg:58.03ms
step:1108/2330 train_time:64300ms step_avg:58.03ms
step:1109/2330 train_time:64357ms step_avg:58.03ms
step:1110/2330 train_time:64418ms step_avg:58.03ms
step:1111/2330 train_time:64475ms step_avg:58.03ms
step:1112/2330 train_time:64536ms step_avg:58.04ms
step:1113/2330 train_time:64593ms step_avg:58.03ms
step:1114/2330 train_time:64653ms step_avg:58.04ms
step:1115/2330 train_time:64710ms step_avg:58.04ms
step:1116/2330 train_time:64770ms step_avg:58.04ms
step:1117/2330 train_time:64827ms step_avg:58.04ms
step:1118/2330 train_time:64887ms step_avg:58.04ms
step:1119/2330 train_time:64944ms step_avg:58.04ms
step:1120/2330 train_time:65003ms step_avg:58.04ms
step:1121/2330 train_time:65061ms step_avg:58.04ms
step:1122/2330 train_time:65120ms step_avg:58.04ms
step:1123/2330 train_time:65177ms step_avg:58.04ms
step:1124/2330 train_time:65238ms step_avg:58.04ms
step:1125/2330 train_time:65296ms step_avg:58.04ms
step:1126/2330 train_time:65356ms step_avg:58.04ms
step:1127/2330 train_time:65413ms step_avg:58.04ms
step:1128/2330 train_time:65473ms step_avg:58.04ms
step:1129/2330 train_time:65531ms step_avg:58.04ms
step:1130/2330 train_time:65591ms step_avg:58.04ms
step:1131/2330 train_time:65647ms step_avg:58.04ms
step:1132/2330 train_time:65708ms step_avg:58.05ms
step:1133/2330 train_time:65766ms step_avg:58.05ms
step:1134/2330 train_time:65825ms step_avg:58.05ms
step:1135/2330 train_time:65882ms step_avg:58.05ms
step:1136/2330 train_time:65941ms step_avg:58.05ms
step:1137/2330 train_time:65997ms step_avg:58.05ms
step:1138/2330 train_time:66058ms step_avg:58.05ms
step:1139/2330 train_time:66115ms step_avg:58.05ms
step:1140/2330 train_time:66176ms step_avg:58.05ms
step:1141/2330 train_time:66234ms step_avg:58.05ms
step:1142/2330 train_time:66293ms step_avg:58.05ms
step:1143/2330 train_time:66351ms step_avg:58.05ms
step:1144/2330 train_time:66412ms step_avg:58.05ms
step:1145/2330 train_time:66470ms step_avg:58.05ms
step:1146/2330 train_time:66529ms step_avg:58.05ms
step:1147/2330 train_time:66586ms step_avg:58.05ms
step:1148/2330 train_time:67030ms step_avg:58.39ms
step:1149/2330 train_time:67085ms step_avg:58.39ms
step:1150/2330 train_time:67144ms step_avg:58.39ms
step:1151/2330 train_time:67201ms step_avg:58.38ms
step:1152/2330 train_time:67260ms step_avg:58.39ms
step:1153/2330 train_time:67315ms step_avg:58.38ms
step:1154/2330 train_time:67375ms step_avg:58.38ms
step:1155/2330 train_time:67431ms step_avg:58.38ms
step:1156/2330 train_time:67490ms step_avg:58.38ms
step:1157/2330 train_time:67546ms step_avg:58.38ms
step:1158/2330 train_time:67605ms step_avg:58.38ms
step:1159/2330 train_time:67662ms step_avg:58.38ms
step:1160/2330 train_time:67720ms step_avg:58.38ms
step:1161/2330 train_time:67776ms step_avg:58.38ms
step:1162/2330 train_time:67836ms step_avg:58.38ms
step:1163/2330 train_time:67894ms step_avg:58.38ms
step:1164/2330 train_time:67963ms step_avg:58.39ms
step:1165/2330 train_time:68021ms step_avg:58.39ms
step:1166/2330 train_time:68083ms step_avg:58.39ms
step:1167/2330 train_time:68140ms step_avg:58.39ms
step:1168/2330 train_time:68200ms step_avg:58.39ms
step:1169/2330 train_time:68256ms step_avg:58.39ms
step:1170/2330 train_time:68318ms step_avg:58.39ms
step:1171/2330 train_time:68374ms step_avg:58.39ms
step:1172/2330 train_time:68433ms step_avg:58.39ms
step:1173/2330 train_time:68490ms step_avg:58.39ms
step:1174/2330 train_time:68549ms step_avg:58.39ms
step:1175/2330 train_time:68606ms step_avg:58.39ms
step:1176/2330 train_time:68665ms step_avg:58.39ms
step:1177/2330 train_time:68721ms step_avg:58.39ms
step:1178/2330 train_time:68781ms step_avg:58.39ms
step:1179/2330 train_time:68838ms step_avg:58.39ms
step:1180/2330 train_time:68899ms step_avg:58.39ms
step:1181/2330 train_time:68957ms step_avg:58.39ms
step:1182/2330 train_time:69020ms step_avg:58.39ms
step:1183/2330 train_time:69076ms step_avg:58.39ms
step:1184/2330 train_time:69138ms step_avg:58.39ms
step:1185/2330 train_time:69195ms step_avg:58.39ms
step:1186/2330 train_time:69256ms step_avg:58.39ms
step:1187/2330 train_time:69312ms step_avg:58.39ms
step:1188/2330 train_time:69372ms step_avg:58.39ms
step:1189/2330 train_time:69429ms step_avg:58.39ms
step:1190/2330 train_time:69488ms step_avg:58.39ms
step:1191/2330 train_time:69545ms step_avg:58.39ms
step:1192/2330 train_time:69604ms step_avg:58.39ms
step:1193/2330 train_time:69660ms step_avg:58.39ms
step:1194/2330 train_time:69720ms step_avg:58.39ms
step:1195/2330 train_time:69777ms step_avg:58.39ms
step:1196/2330 train_time:69838ms step_avg:58.39ms
step:1197/2330 train_time:69896ms step_avg:58.39ms
step:1198/2330 train_time:69957ms step_avg:58.39ms
step:1199/2330 train_time:70014ms step_avg:58.39ms
step:1200/2330 train_time:70075ms step_avg:58.40ms
step:1201/2330 train_time:70132ms step_avg:58.39ms
step:1202/2330 train_time:70192ms step_avg:58.40ms
step:1203/2330 train_time:70249ms step_avg:58.40ms
step:1204/2330 train_time:70309ms step_avg:58.40ms
step:1205/2330 train_time:70366ms step_avg:58.39ms
step:1206/2330 train_time:70426ms step_avg:58.40ms
step:1207/2330 train_time:70482ms step_avg:58.39ms
step:1208/2330 train_time:70542ms step_avg:58.40ms
step:1209/2330 train_time:70598ms step_avg:58.39ms
step:1210/2330 train_time:70658ms step_avg:58.40ms
step:1211/2330 train_time:70715ms step_avg:58.39ms
step:1212/2330 train_time:70775ms step_avg:58.40ms
step:1213/2330 train_time:70832ms step_avg:58.39ms
step:1214/2330 train_time:70892ms step_avg:58.40ms
step:1215/2330 train_time:70949ms step_avg:58.39ms
step:1216/2330 train_time:71010ms step_avg:58.40ms
step:1217/2330 train_time:71067ms step_avg:58.40ms
step:1218/2330 train_time:71129ms step_avg:58.40ms
step:1219/2330 train_time:71186ms step_avg:58.40ms
step:1220/2330 train_time:71246ms step_avg:58.40ms
step:1221/2330 train_time:71302ms step_avg:58.40ms
step:1222/2330 train_time:71363ms step_avg:58.40ms
step:1223/2330 train_time:71420ms step_avg:58.40ms
step:1224/2330 train_time:71480ms step_avg:58.40ms
step:1225/2330 train_time:71537ms step_avg:58.40ms
step:1226/2330 train_time:71596ms step_avg:58.40ms
step:1227/2330 train_time:71653ms step_avg:58.40ms
step:1228/2330 train_time:71713ms step_avg:58.40ms
step:1229/2330 train_time:71770ms step_avg:58.40ms
step:1230/2330 train_time:71830ms step_avg:58.40ms
step:1231/2330 train_time:71887ms step_avg:58.40ms
step:1232/2330 train_time:71948ms step_avg:58.40ms
step:1233/2330 train_time:72004ms step_avg:58.40ms
step:1234/2330 train_time:72066ms step_avg:58.40ms
step:1235/2330 train_time:72124ms step_avg:58.40ms
step:1236/2330 train_time:72184ms step_avg:58.40ms
step:1237/2330 train_time:72241ms step_avg:58.40ms
step:1238/2330 train_time:72301ms step_avg:58.40ms
step:1239/2330 train_time:72357ms step_avg:58.40ms
step:1240/2330 train_time:72418ms step_avg:58.40ms
step:1241/2330 train_time:72474ms step_avg:58.40ms
step:1242/2330 train_time:72535ms step_avg:58.40ms
step:1243/2330 train_time:72591ms step_avg:58.40ms
step:1244/2330 train_time:72652ms step_avg:58.40ms
step:1245/2330 train_time:72709ms step_avg:58.40ms
step:1246/2330 train_time:72769ms step_avg:58.40ms
step:1247/2330 train_time:72826ms step_avg:58.40ms
step:1248/2330 train_time:72886ms step_avg:58.40ms
step:1249/2330 train_time:72942ms step_avg:58.40ms
step:1250/2330 train_time:73004ms step_avg:58.40ms
step:1250/2330 val_loss:3.9875 train_time:73087ms step_avg:58.47ms
step:1251/2330 train_time:73105ms step_avg:58.44ms
step:1252/2330 train_time:73126ms step_avg:58.41ms
step:1253/2330 train_time:73186ms step_avg:58.41ms
step:1254/2330 train_time:73252ms step_avg:58.41ms
step:1255/2330 train_time:73310ms step_avg:58.41ms
step:1256/2330 train_time:73371ms step_avg:58.42ms
step:1257/2330 train_time:73427ms step_avg:58.41ms
step:1258/2330 train_time:73487ms step_avg:58.42ms
step:1259/2330 train_time:73543ms step_avg:58.41ms
step:1260/2330 train_time:73603ms step_avg:58.42ms
step:1261/2330 train_time:73660ms step_avg:58.41ms
step:1262/2330 train_time:73719ms step_avg:58.41ms
step:1263/2330 train_time:73775ms step_avg:58.41ms
step:1264/2330 train_time:73834ms step_avg:58.41ms
step:1265/2330 train_time:73890ms step_avg:58.41ms
step:1266/2330 train_time:73950ms step_avg:58.41ms
step:1267/2330 train_time:74006ms step_avg:58.41ms
step:1268/2330 train_time:74066ms step_avg:58.41ms
step:1269/2330 train_time:74124ms step_avg:58.41ms
step:1270/2330 train_time:74187ms step_avg:58.42ms
step:1271/2330 train_time:74245ms step_avg:58.41ms
step:1272/2330 train_time:74308ms step_avg:58.42ms
step:1273/2330 train_time:74366ms step_avg:58.42ms
step:1274/2330 train_time:74427ms step_avg:58.42ms
step:1275/2330 train_time:74484ms step_avg:58.42ms
step:1276/2330 train_time:74543ms step_avg:58.42ms
step:1277/2330 train_time:74600ms step_avg:58.42ms
step:1278/2330 train_time:74659ms step_avg:58.42ms
step:1279/2330 train_time:74716ms step_avg:58.42ms
step:1280/2330 train_time:74775ms step_avg:58.42ms
step:1281/2330 train_time:74830ms step_avg:58.42ms
step:1282/2330 train_time:74890ms step_avg:58.42ms
step:1283/2330 train_time:74946ms step_avg:58.41ms
step:1284/2330 train_time:75006ms step_avg:58.42ms
step:1285/2330 train_time:75062ms step_avg:58.41ms
step:1286/2330 train_time:75123ms step_avg:58.42ms
step:1287/2330 train_time:75182ms step_avg:58.42ms
step:1288/2330 train_time:75243ms step_avg:58.42ms
step:1289/2330 train_time:75301ms step_avg:58.42ms
step:1290/2330 train_time:75362ms step_avg:58.42ms
step:1291/2330 train_time:75419ms step_avg:58.42ms
step:1292/2330 train_time:75479ms step_avg:58.42ms
step:1293/2330 train_time:75536ms step_avg:58.42ms
step:1294/2330 train_time:75595ms step_avg:58.42ms
step:1295/2330 train_time:75653ms step_avg:58.42ms
step:1296/2330 train_time:75712ms step_avg:58.42ms
step:1297/2330 train_time:75768ms step_avg:58.42ms
step:1298/2330 train_time:75828ms step_avg:58.42ms
step:1299/2330 train_time:75885ms step_avg:58.42ms
step:1300/2330 train_time:75944ms step_avg:58.42ms
step:1301/2330 train_time:76001ms step_avg:58.42ms
step:1302/2330 train_time:76061ms step_avg:58.42ms
step:1303/2330 train_time:76117ms step_avg:58.42ms
step:1304/2330 train_time:76179ms step_avg:58.42ms
step:1305/2330 train_time:76236ms step_avg:58.42ms
step:1306/2330 train_time:76297ms step_avg:58.42ms
step:1307/2330 train_time:76354ms step_avg:58.42ms
step:1308/2330 train_time:76416ms step_avg:58.42ms
step:1309/2330 train_time:76473ms step_avg:58.42ms
step:1310/2330 train_time:76533ms step_avg:58.42ms
step:1311/2330 train_time:76590ms step_avg:58.42ms
step:1312/2330 train_time:76650ms step_avg:58.42ms
step:1313/2330 train_time:76706ms step_avg:58.42ms
step:1314/2330 train_time:76767ms step_avg:58.42ms
step:1315/2330 train_time:76823ms step_avg:58.42ms
step:1316/2330 train_time:76882ms step_avg:58.42ms
step:1317/2330 train_time:76939ms step_avg:58.42ms
step:1318/2330 train_time:76998ms step_avg:58.42ms
step:1319/2330 train_time:77055ms step_avg:58.42ms
step:1320/2330 train_time:77118ms step_avg:58.42ms
step:1321/2330 train_time:77174ms step_avg:58.42ms
step:1322/2330 train_time:77235ms step_avg:58.42ms
step:1323/2330 train_time:77293ms step_avg:58.42ms
step:1324/2330 train_time:77354ms step_avg:58.42ms
step:1325/2330 train_time:77410ms step_avg:58.42ms
step:1326/2330 train_time:77471ms step_avg:58.42ms
step:1327/2330 train_time:77527ms step_avg:58.42ms
step:1328/2330 train_time:77589ms step_avg:58.43ms
step:1329/2330 train_time:77646ms step_avg:58.42ms
step:1330/2330 train_time:77706ms step_avg:58.43ms
step:1331/2330 train_time:77762ms step_avg:58.42ms
step:1332/2330 train_time:77821ms step_avg:58.42ms
step:1333/2330 train_time:77878ms step_avg:58.42ms
step:1334/2330 train_time:77938ms step_avg:58.42ms
step:1335/2330 train_time:77995ms step_avg:58.42ms
step:1336/2330 train_time:78055ms step_avg:58.42ms
step:1337/2330 train_time:78112ms step_avg:58.42ms
step:1338/2330 train_time:78172ms step_avg:58.42ms
step:1339/2330 train_time:78228ms step_avg:58.42ms
step:1340/2330 train_time:78290ms step_avg:58.43ms
step:1341/2330 train_time:78348ms step_avg:58.43ms
step:1342/2330 train_time:78409ms step_avg:58.43ms
step:1343/2330 train_time:78466ms step_avg:58.43ms
step:1344/2330 train_time:78525ms step_avg:58.43ms
step:1345/2330 train_time:78582ms step_avg:58.43ms
step:1346/2330 train_time:78642ms step_avg:58.43ms
step:1347/2330 train_time:78699ms step_avg:58.43ms
step:1348/2330 train_time:78759ms step_avg:58.43ms
step:1349/2330 train_time:78815ms step_avg:58.42ms
step:1350/2330 train_time:78875ms step_avg:58.43ms
step:1351/2330 train_time:78932ms step_avg:58.43ms
step:1352/2330 train_time:78992ms step_avg:58.43ms
step:1353/2330 train_time:79049ms step_avg:58.42ms
step:1354/2330 train_time:79109ms step_avg:58.43ms
step:1355/2330 train_time:79166ms step_avg:58.43ms
step:1356/2330 train_time:79227ms step_avg:58.43ms
step:1357/2330 train_time:79285ms step_avg:58.43ms
step:1358/2330 train_time:79345ms step_avg:58.43ms
step:1359/2330 train_time:79403ms step_avg:58.43ms
step:1360/2330 train_time:79463ms step_avg:58.43ms
step:1361/2330 train_time:79520ms step_avg:58.43ms
step:1362/2330 train_time:79579ms step_avg:58.43ms
step:1363/2330 train_time:79636ms step_avg:58.43ms
step:1364/2330 train_time:79696ms step_avg:58.43ms
step:1365/2330 train_time:79753ms step_avg:58.43ms
step:1366/2330 train_time:79814ms step_avg:58.43ms
step:1367/2330 train_time:79871ms step_avg:58.43ms
step:1368/2330 train_time:79931ms step_avg:58.43ms
step:1369/2330 train_time:79987ms step_avg:58.43ms
step:1370/2330 train_time:80047ms step_avg:58.43ms
step:1371/2330 train_time:80105ms step_avg:58.43ms
step:1372/2330 train_time:80165ms step_avg:58.43ms
step:1373/2330 train_time:80222ms step_avg:58.43ms
step:1374/2330 train_time:80282ms step_avg:58.43ms
step:1375/2330 train_time:80339ms step_avg:58.43ms
step:1376/2330 train_time:80399ms step_avg:58.43ms
step:1377/2330 train_time:80455ms step_avg:58.43ms
step:1378/2330 train_time:80517ms step_avg:58.43ms
step:1379/2330 train_time:80574ms step_avg:58.43ms
step:1380/2330 train_time:80634ms step_avg:58.43ms
step:1381/2330 train_time:80690ms step_avg:58.43ms
step:1382/2330 train_time:80752ms step_avg:58.43ms
step:1383/2330 train_time:80808ms step_avg:58.43ms
step:1384/2330 train_time:80869ms step_avg:58.43ms
step:1385/2330 train_time:80926ms step_avg:58.43ms
step:1386/2330 train_time:80986ms step_avg:58.43ms
step:1387/2330 train_time:81043ms step_avg:58.43ms
step:1388/2330 train_time:81103ms step_avg:58.43ms
step:1389/2330 train_time:81160ms step_avg:58.43ms
step:1390/2330 train_time:81220ms step_avg:58.43ms
step:1391/2330 train_time:81278ms step_avg:58.43ms
step:1392/2330 train_time:81337ms step_avg:58.43ms
step:1393/2330 train_time:81394ms step_avg:58.43ms
step:1394/2330 train_time:81454ms step_avg:58.43ms
step:1395/2330 train_time:81511ms step_avg:58.43ms
step:1396/2330 train_time:81571ms step_avg:58.43ms
step:1397/2330 train_time:81628ms step_avg:58.43ms
step:1398/2330 train_time:81688ms step_avg:58.43ms
step:1399/2330 train_time:81746ms step_avg:58.43ms
step:1400/2330 train_time:81806ms step_avg:58.43ms
step:1401/2330 train_time:81863ms step_avg:58.43ms
step:1402/2330 train_time:81922ms step_avg:58.43ms
step:1403/2330 train_time:81979ms step_avg:58.43ms
step:1404/2330 train_time:82039ms step_avg:58.43ms
step:1405/2330 train_time:82095ms step_avg:58.43ms
step:1406/2330 train_time:82156ms step_avg:58.43ms
step:1407/2330 train_time:82212ms step_avg:58.43ms
step:1408/2330 train_time:82274ms step_avg:58.43ms
step:1409/2330 train_time:82331ms step_avg:58.43ms
step:1410/2330 train_time:82391ms step_avg:58.43ms
step:1411/2330 train_time:82448ms step_avg:58.43ms
step:1412/2330 train_time:82510ms step_avg:58.43ms
step:1413/2330 train_time:82566ms step_avg:58.43ms
step:1414/2330 train_time:82626ms step_avg:58.43ms
step:1415/2330 train_time:82684ms step_avg:58.43ms
step:1416/2330 train_time:82744ms step_avg:58.43ms
step:1417/2330 train_time:82801ms step_avg:58.43ms
step:1418/2330 train_time:82861ms step_avg:58.44ms
step:1419/2330 train_time:82918ms step_avg:58.43ms
step:1420/2330 train_time:82978ms step_avg:58.44ms
step:1421/2330 train_time:83035ms step_avg:58.43ms
step:1422/2330 train_time:83095ms step_avg:58.44ms
step:1423/2330 train_time:83152ms step_avg:58.43ms
step:1424/2330 train_time:83214ms step_avg:58.44ms
step:1425/2330 train_time:83270ms step_avg:58.44ms
step:1426/2330 train_time:83331ms step_avg:58.44ms
step:1427/2330 train_time:83388ms step_avg:58.44ms
step:1428/2330 train_time:83448ms step_avg:58.44ms
step:1429/2330 train_time:83505ms step_avg:58.44ms
step:1430/2330 train_time:83565ms step_avg:58.44ms
step:1431/2330 train_time:83622ms step_avg:58.44ms
step:1432/2330 train_time:83683ms step_avg:58.44ms
step:1433/2330 train_time:83740ms step_avg:58.44ms
step:1434/2330 train_time:83800ms step_avg:58.44ms
step:1435/2330 train_time:83857ms step_avg:58.44ms
step:1436/2330 train_time:83917ms step_avg:58.44ms
step:1437/2330 train_time:83974ms step_avg:58.44ms
step:1438/2330 train_time:84033ms step_avg:58.44ms
step:1439/2330 train_time:84090ms step_avg:58.44ms
step:1440/2330 train_time:84150ms step_avg:58.44ms
step:1441/2330 train_time:84207ms step_avg:58.44ms
step:1442/2330 train_time:84268ms step_avg:58.44ms
step:1443/2330 train_time:84326ms step_avg:58.44ms
step:1444/2330 train_time:84386ms step_avg:58.44ms
step:1445/2330 train_time:84443ms step_avg:58.44ms
step:1446/2330 train_time:84503ms step_avg:58.44ms
step:1447/2330 train_time:84560ms step_avg:58.44ms
step:1448/2330 train_time:84620ms step_avg:58.44ms
step:1449/2330 train_time:84677ms step_avg:58.44ms
step:1450/2330 train_time:84736ms step_avg:58.44ms
step:1451/2330 train_time:84793ms step_avg:58.44ms
step:1452/2330 train_time:84854ms step_avg:58.44ms
step:1453/2330 train_time:84911ms step_avg:58.44ms
step:1454/2330 train_time:84971ms step_avg:58.44ms
step:1455/2330 train_time:85027ms step_avg:58.44ms
step:1456/2330 train_time:85088ms step_avg:58.44ms
step:1457/2330 train_time:85145ms step_avg:58.44ms
step:1458/2330 train_time:85205ms step_avg:58.44ms
step:1459/2330 train_time:85263ms step_avg:58.44ms
step:1460/2330 train_time:85323ms step_avg:58.44ms
step:1461/2330 train_time:85380ms step_avg:58.44ms
step:1462/2330 train_time:85440ms step_avg:58.44ms
step:1463/2330 train_time:85497ms step_avg:58.44ms
step:1464/2330 train_time:85557ms step_avg:58.44ms
step:1465/2330 train_time:85614ms step_avg:58.44ms
step:1466/2330 train_time:85674ms step_avg:58.44ms
step:1467/2330 train_time:85731ms step_avg:58.44ms
step:1468/2330 train_time:85792ms step_avg:58.44ms
step:1469/2330 train_time:85848ms step_avg:58.44ms
step:1470/2330 train_time:85908ms step_avg:58.44ms
step:1471/2330 train_time:85966ms step_avg:58.44ms
step:1472/2330 train_time:86026ms step_avg:58.44ms
step:1473/2330 train_time:86084ms step_avg:58.44ms
step:1474/2330 train_time:86144ms step_avg:58.44ms
step:1475/2330 train_time:86202ms step_avg:58.44ms
step:1476/2330 train_time:86262ms step_avg:58.44ms
step:1477/2330 train_time:86319ms step_avg:58.44ms
step:1478/2330 train_time:86378ms step_avg:58.44ms
step:1479/2330 train_time:86435ms step_avg:58.44ms
step:1480/2330 train_time:86495ms step_avg:58.44ms
step:1481/2330 train_time:86551ms step_avg:58.44ms
step:1482/2330 train_time:86613ms step_avg:58.44ms
step:1483/2330 train_time:86669ms step_avg:58.44ms
step:1484/2330 train_time:86730ms step_avg:58.44ms
step:1485/2330 train_time:86786ms step_avg:58.44ms
step:1486/2330 train_time:86847ms step_avg:58.44ms
step:1487/2330 train_time:86904ms step_avg:58.44ms
step:1488/2330 train_time:86964ms step_avg:58.44ms
step:1489/2330 train_time:87021ms step_avg:58.44ms
step:1490/2330 train_time:87082ms step_avg:58.44ms
step:1491/2330 train_time:87139ms step_avg:58.44ms
step:1492/2330 train_time:87199ms step_avg:58.44ms
step:1493/2330 train_time:87255ms step_avg:58.44ms
step:1494/2330 train_time:87317ms step_avg:58.44ms
step:1495/2330 train_time:87373ms step_avg:58.44ms
step:1496/2330 train_time:87433ms step_avg:58.44ms
step:1497/2330 train_time:87491ms step_avg:58.44ms
step:1498/2330 train_time:87551ms step_avg:58.45ms
step:1499/2330 train_time:87608ms step_avg:58.44ms
step:1500/2330 train_time:87668ms step_avg:58.45ms
step:1500/2330 val_loss:3.9001 train_time:87748ms step_avg:58.50ms
step:1501/2330 train_time:87767ms step_avg:58.47ms
step:1502/2330 train_time:87787ms step_avg:58.45ms
step:1503/2330 train_time:87851ms step_avg:58.45ms
step:1504/2330 train_time:87915ms step_avg:58.45ms
step:1505/2330 train_time:87973ms step_avg:58.45ms
step:1506/2330 train_time:88033ms step_avg:58.46ms
step:1507/2330 train_time:88090ms step_avg:58.45ms
step:1508/2330 train_time:88150ms step_avg:58.45ms
step:1509/2330 train_time:88207ms step_avg:58.45ms
step:1510/2330 train_time:88266ms step_avg:58.45ms
step:1511/2330 train_time:88323ms step_avg:58.45ms
step:1512/2330 train_time:88382ms step_avg:58.45ms
step:1513/2330 train_time:88438ms step_avg:58.45ms
step:1514/2330 train_time:88497ms step_avg:58.45ms
step:1515/2330 train_time:88553ms step_avg:58.45ms
step:1516/2330 train_time:88612ms step_avg:58.45ms
step:1517/2330 train_time:88668ms step_avg:58.45ms
step:1518/2330 train_time:88729ms step_avg:58.45ms
step:1519/2330 train_time:88788ms step_avg:58.45ms
step:1520/2330 train_time:88850ms step_avg:58.45ms
step:1521/2330 train_time:88909ms step_avg:58.45ms
step:1522/2330 train_time:88970ms step_avg:58.46ms
step:1523/2330 train_time:89028ms step_avg:58.46ms
step:1524/2330 train_time:89087ms step_avg:58.46ms
step:1525/2330 train_time:89144ms step_avg:58.46ms
step:1526/2330 train_time:89204ms step_avg:58.46ms
step:1527/2330 train_time:89261ms step_avg:58.46ms
step:1528/2330 train_time:89321ms step_avg:58.46ms
step:1529/2330 train_time:89379ms step_avg:58.46ms
step:1530/2330 train_time:89438ms step_avg:58.46ms
step:1531/2330 train_time:89494ms step_avg:58.45ms
step:1532/2330 train_time:89556ms step_avg:58.46ms
step:1533/2330 train_time:89612ms step_avg:58.46ms
step:1534/2330 train_time:89672ms step_avg:58.46ms
step:1535/2330 train_time:89730ms step_avg:58.46ms
step:1536/2330 train_time:89792ms step_avg:58.46ms
step:1537/2330 train_time:89850ms step_avg:58.46ms
step:1538/2330 train_time:89912ms step_avg:58.46ms
step:1539/2330 train_time:89969ms step_avg:58.46ms
step:1540/2330 train_time:90032ms step_avg:58.46ms
step:1541/2330 train_time:90090ms step_avg:58.46ms
step:1542/2330 train_time:90151ms step_avg:58.46ms
step:1543/2330 train_time:90209ms step_avg:58.46ms
step:1544/2330 train_time:90270ms step_avg:58.46ms
step:1545/2330 train_time:90327ms step_avg:58.46ms
step:1546/2330 train_time:90387ms step_avg:58.47ms
step:1547/2330 train_time:90446ms step_avg:58.47ms
step:1548/2330 train_time:90506ms step_avg:58.47ms
step:1549/2330 train_time:90563ms step_avg:58.47ms
step:1550/2330 train_time:90623ms step_avg:58.47ms
step:1551/2330 train_time:90680ms step_avg:58.47ms
step:1552/2330 train_time:90740ms step_avg:58.47ms
step:1553/2330 train_time:90798ms step_avg:58.47ms
step:1554/2330 train_time:90859ms step_avg:58.47ms
step:1555/2330 train_time:90917ms step_avg:58.47ms
step:1556/2330 train_time:90979ms step_avg:58.47ms
step:1557/2330 train_time:91036ms step_avg:58.47ms
step:1558/2330 train_time:91099ms step_avg:58.47ms
step:1559/2330 train_time:91155ms step_avg:58.47ms
step:1560/2330 train_time:91218ms step_avg:58.47ms
step:1561/2330 train_time:91274ms step_avg:58.47ms
step:1562/2330 train_time:91336ms step_avg:58.47ms
step:1563/2330 train_time:91393ms step_avg:58.47ms
step:1564/2330 train_time:91454ms step_avg:58.47ms
step:1565/2330 train_time:91510ms step_avg:58.47ms
step:1566/2330 train_time:91573ms step_avg:58.48ms
step:1567/2330 train_time:91631ms step_avg:58.48ms
step:1568/2330 train_time:91690ms step_avg:58.48ms
step:1569/2330 train_time:91747ms step_avg:58.48ms
step:1570/2330 train_time:91810ms step_avg:58.48ms
step:1571/2330 train_time:91868ms step_avg:58.48ms
step:1572/2330 train_time:91929ms step_avg:58.48ms
step:1573/2330 train_time:91987ms step_avg:58.48ms
step:1574/2330 train_time:92047ms step_avg:58.48ms
step:1575/2330 train_time:92105ms step_avg:58.48ms
step:1576/2330 train_time:92166ms step_avg:58.48ms
step:1577/2330 train_time:92224ms step_avg:58.48ms
step:1578/2330 train_time:92285ms step_avg:58.48ms
step:1579/2330 train_time:92342ms step_avg:58.48ms
step:1580/2330 train_time:92402ms step_avg:58.48ms
step:1581/2330 train_time:92459ms step_avg:58.48ms
step:1582/2330 train_time:92519ms step_avg:58.48ms
step:1583/2330 train_time:92576ms step_avg:58.48ms
step:1584/2330 train_time:92638ms step_avg:58.48ms
step:1585/2330 train_time:92694ms step_avg:58.48ms
step:1586/2330 train_time:92756ms step_avg:58.48ms
step:1587/2330 train_time:92812ms step_avg:58.48ms
step:1588/2330 train_time:92875ms step_avg:58.49ms
step:1589/2330 train_time:92932ms step_avg:58.48ms
step:1590/2330 train_time:92994ms step_avg:58.49ms
step:1591/2330 train_time:93051ms step_avg:58.49ms
step:1592/2330 train_time:93113ms step_avg:58.49ms
step:1593/2330 train_time:93171ms step_avg:58.49ms
step:1594/2330 train_time:93233ms step_avg:58.49ms
step:1595/2330 train_time:93291ms step_avg:58.49ms
step:1596/2330 train_time:93351ms step_avg:58.49ms
step:1597/2330 train_time:93408ms step_avg:58.49ms
step:1598/2330 train_time:93468ms step_avg:58.49ms
step:1599/2330 train_time:93525ms step_avg:58.49ms
step:1600/2330 train_time:93586ms step_avg:58.49ms
step:1601/2330 train_time:93645ms step_avg:58.49ms
step:1602/2330 train_time:93705ms step_avg:58.49ms
step:1603/2330 train_time:93762ms step_avg:58.49ms
step:1604/2330 train_time:93822ms step_avg:58.49ms
step:1605/2330 train_time:93880ms step_avg:58.49ms
step:1606/2330 train_time:93942ms step_avg:58.49ms
step:1607/2330 train_time:93999ms step_avg:58.49ms
step:1608/2330 train_time:94060ms step_avg:58.50ms
step:1609/2330 train_time:94117ms step_avg:58.49ms
step:1610/2330 train_time:94179ms step_avg:58.50ms
step:1611/2330 train_time:94236ms step_avg:58.50ms
step:1612/2330 train_time:94299ms step_avg:58.50ms
step:1613/2330 train_time:94355ms step_avg:58.50ms
step:1614/2330 train_time:94417ms step_avg:58.50ms
step:1615/2330 train_time:94474ms step_avg:58.50ms
step:1616/2330 train_time:94536ms step_avg:58.50ms
step:1617/2330 train_time:94593ms step_avg:58.50ms
step:1618/2330 train_time:94654ms step_avg:58.50ms
step:1619/2330 train_time:94711ms step_avg:58.50ms
step:1620/2330 train_time:94773ms step_avg:58.50ms
step:1621/2330 train_time:94830ms step_avg:58.50ms
step:1622/2330 train_time:94891ms step_avg:58.50ms
step:1623/2330 train_time:94950ms step_avg:58.50ms
step:1624/2330 train_time:95010ms step_avg:58.50ms
step:1625/2330 train_time:95068ms step_avg:58.50ms
step:1626/2330 train_time:95129ms step_avg:58.51ms
step:1627/2330 train_time:95188ms step_avg:58.51ms
step:1628/2330 train_time:95248ms step_avg:58.51ms
step:1629/2330 train_time:95305ms step_avg:58.51ms
step:1630/2330 train_time:95366ms step_avg:58.51ms
step:1631/2330 train_time:95423ms step_avg:58.51ms
step:1632/2330 train_time:95484ms step_avg:58.51ms
step:1633/2330 train_time:95541ms step_avg:58.51ms
step:1634/2330 train_time:95603ms step_avg:58.51ms
step:1635/2330 train_time:95660ms step_avg:58.51ms
step:1636/2330 train_time:95721ms step_avg:58.51ms
step:1637/2330 train_time:95778ms step_avg:58.51ms
step:1638/2330 train_time:95841ms step_avg:58.51ms
step:1639/2330 train_time:95898ms step_avg:58.51ms
step:1640/2330 train_time:95960ms step_avg:58.51ms
step:1641/2330 train_time:96017ms step_avg:58.51ms
step:1642/2330 train_time:96079ms step_avg:58.51ms
step:1643/2330 train_time:96136ms step_avg:58.51ms
step:1644/2330 train_time:96198ms step_avg:58.51ms
step:1645/2330 train_time:96254ms step_avg:58.51ms
step:1646/2330 train_time:96317ms step_avg:58.52ms
step:1647/2330 train_time:96373ms step_avg:58.51ms
step:1648/2330 train_time:96434ms step_avg:58.52ms
step:1649/2330 train_time:96491ms step_avg:58.52ms
step:1650/2330 train_time:96552ms step_avg:58.52ms
step:1651/2330 train_time:96610ms step_avg:58.52ms
step:1652/2330 train_time:96671ms step_avg:58.52ms
step:1653/2330 train_time:96728ms step_avg:58.52ms
step:1654/2330 train_time:96789ms step_avg:58.52ms
step:1655/2330 train_time:96848ms step_avg:58.52ms
step:1656/2330 train_time:96908ms step_avg:58.52ms
step:1657/2330 train_time:96965ms step_avg:58.52ms
step:1658/2330 train_time:97026ms step_avg:58.52ms
step:1659/2330 train_time:97085ms step_avg:58.52ms
step:1660/2330 train_time:97145ms step_avg:58.52ms
step:1661/2330 train_time:97203ms step_avg:58.52ms
step:1662/2330 train_time:97263ms step_avg:58.52ms
step:1663/2330 train_time:97320ms step_avg:58.52ms
step:1664/2330 train_time:97381ms step_avg:58.52ms
step:1665/2330 train_time:97438ms step_avg:58.52ms
step:1666/2330 train_time:97499ms step_avg:58.52ms
step:1667/2330 train_time:97556ms step_avg:58.52ms
step:1668/2330 train_time:97617ms step_avg:58.52ms
step:1669/2330 train_time:97674ms step_avg:58.52ms
step:1670/2330 train_time:97736ms step_avg:58.52ms
step:1671/2330 train_time:97793ms step_avg:58.52ms
step:1672/2330 train_time:97855ms step_avg:58.53ms
step:1673/2330 train_time:97913ms step_avg:58.53ms
step:1674/2330 train_time:97973ms step_avg:58.53ms
step:1675/2330 train_time:98031ms step_avg:58.53ms
step:1676/2330 train_time:98091ms step_avg:58.53ms
step:1677/2330 train_time:98148ms step_avg:58.53ms
step:1678/2330 train_time:98210ms step_avg:58.53ms
step:1679/2330 train_time:98268ms step_avg:58.53ms
step:1680/2330 train_time:98329ms step_avg:58.53ms
step:1681/2330 train_time:98387ms step_avg:58.53ms
step:1682/2330 train_time:98447ms step_avg:58.53ms
step:1683/2330 train_time:98506ms step_avg:58.53ms
step:1684/2330 train_time:98567ms step_avg:58.53ms
step:1685/2330 train_time:98625ms step_avg:58.53ms
step:1686/2330 train_time:98685ms step_avg:58.53ms
step:1687/2330 train_time:98743ms step_avg:58.53ms
step:1688/2330 train_time:98804ms step_avg:58.53ms
step:1689/2330 train_time:98862ms step_avg:58.53ms
step:1690/2330 train_time:98922ms step_avg:58.53ms
step:1691/2330 train_time:98980ms step_avg:58.53ms
step:1692/2330 train_time:99042ms step_avg:58.54ms
step:1693/2330 train_time:99099ms step_avg:58.53ms
step:1694/2330 train_time:99160ms step_avg:58.54ms
step:1695/2330 train_time:99217ms step_avg:58.54ms
step:1696/2330 train_time:99279ms step_avg:58.54ms
step:1697/2330 train_time:99336ms step_avg:58.54ms
step:1698/2330 train_time:99397ms step_avg:58.54ms
step:1699/2330 train_time:99454ms step_avg:58.54ms
step:1700/2330 train_time:99515ms step_avg:58.54ms
step:1701/2330 train_time:99572ms step_avg:58.54ms
step:1702/2330 train_time:99633ms step_avg:58.54ms
step:1703/2330 train_time:99690ms step_avg:58.54ms
step:1704/2330 train_time:99751ms step_avg:58.54ms
step:1705/2330 train_time:99808ms step_avg:58.54ms
step:1706/2330 train_time:99870ms step_avg:58.54ms
step:1707/2330 train_time:99929ms step_avg:58.54ms
step:1708/2330 train_time:99990ms step_avg:58.54ms
step:1709/2330 train_time:100049ms step_avg:58.54ms
step:1710/2330 train_time:100109ms step_avg:58.54ms
step:1711/2330 train_time:100167ms step_avg:58.54ms
step:1712/2330 train_time:100228ms step_avg:58.54ms
step:1713/2330 train_time:100287ms step_avg:58.54ms
step:1714/2330 train_time:100347ms step_avg:58.55ms
step:1715/2330 train_time:100405ms step_avg:58.55ms
step:1716/2330 train_time:100466ms step_avg:58.55ms
step:1717/2330 train_time:100523ms step_avg:58.55ms
step:1718/2330 train_time:100584ms step_avg:58.55ms
step:1719/2330 train_time:100640ms step_avg:58.55ms
step:1720/2330 train_time:100701ms step_avg:58.55ms
step:1721/2330 train_time:100758ms step_avg:58.55ms
step:1722/2330 train_time:100819ms step_avg:58.55ms
step:1723/2330 train_time:100876ms step_avg:58.55ms
step:1724/2330 train_time:100939ms step_avg:58.55ms
step:1725/2330 train_time:100997ms step_avg:58.55ms
step:1726/2330 train_time:101058ms step_avg:58.55ms
step:1727/2330 train_time:101115ms step_avg:58.55ms
step:1728/2330 train_time:101177ms step_avg:58.55ms
step:1729/2330 train_time:101234ms step_avg:58.55ms
step:1730/2330 train_time:101296ms step_avg:58.55ms
step:1731/2330 train_time:101353ms step_avg:58.55ms
step:1732/2330 train_time:101414ms step_avg:58.55ms
step:1733/2330 train_time:101471ms step_avg:58.55ms
step:1734/2330 train_time:101533ms step_avg:58.55ms
step:1735/2330 train_time:101590ms step_avg:58.55ms
step:1736/2330 train_time:101651ms step_avg:58.55ms
step:1737/2330 train_time:101709ms step_avg:58.55ms
step:1738/2330 train_time:101769ms step_avg:58.56ms
step:1739/2330 train_time:101827ms step_avg:58.56ms
step:1740/2330 train_time:101887ms step_avg:58.56ms
step:1741/2330 train_time:101946ms step_avg:58.56ms
step:1742/2330 train_time:102006ms step_avg:58.56ms
step:1743/2330 train_time:102064ms step_avg:58.56ms
step:1744/2330 train_time:102125ms step_avg:58.56ms
step:1745/2330 train_time:102183ms step_avg:58.56ms
step:1746/2330 train_time:102243ms step_avg:58.56ms
step:1747/2330 train_time:102301ms step_avg:58.56ms
step:1748/2330 train_time:102360ms step_avg:58.56ms
step:1749/2330 train_time:102418ms step_avg:58.56ms
step:1750/2330 train_time:102479ms step_avg:58.56ms
step:1750/2330 val_loss:3.8174 train_time:102561ms step_avg:58.61ms
step:1751/2330 train_time:102581ms step_avg:58.58ms
step:1752/2330 train_time:102601ms step_avg:58.56ms
step:1753/2330 train_time:102657ms step_avg:58.56ms
step:1754/2330 train_time:102724ms step_avg:58.57ms
step:1755/2330 train_time:102780ms step_avg:58.56ms
step:1756/2330 train_time:102844ms step_avg:58.57ms
step:1757/2330 train_time:102901ms step_avg:58.57ms
step:1758/2330 train_time:102960ms step_avg:58.57ms
step:1759/2330 train_time:103017ms step_avg:58.57ms
step:1760/2330 train_time:103076ms step_avg:58.57ms
step:1761/2330 train_time:103132ms step_avg:58.56ms
step:1762/2330 train_time:103192ms step_avg:58.57ms
step:1763/2330 train_time:103249ms step_avg:58.56ms
step:1764/2330 train_time:103309ms step_avg:58.57ms
step:1765/2330 train_time:103365ms step_avg:58.56ms
step:1766/2330 train_time:103425ms step_avg:58.56ms
step:1767/2330 train_time:103484ms step_avg:58.56ms
step:1768/2330 train_time:103549ms step_avg:58.57ms
step:1769/2330 train_time:103608ms step_avg:58.57ms
step:1770/2330 train_time:103669ms step_avg:58.57ms
step:1771/2330 train_time:103727ms step_avg:58.57ms
step:1772/2330 train_time:103788ms step_avg:58.57ms
step:1773/2330 train_time:103845ms step_avg:58.57ms
step:1774/2330 train_time:103907ms step_avg:58.57ms
step:1775/2330 train_time:103964ms step_avg:58.57ms
step:1776/2330 train_time:104025ms step_avg:58.57ms
step:1777/2330 train_time:104081ms step_avg:58.57ms
step:1778/2330 train_time:104142ms step_avg:58.57ms
step:1779/2330 train_time:104199ms step_avg:58.57ms
step:1780/2330 train_time:104259ms step_avg:58.57ms
step:1781/2330 train_time:104316ms step_avg:58.57ms
step:1782/2330 train_time:104375ms step_avg:58.57ms
step:1783/2330 train_time:104433ms step_avg:58.57ms
step:1784/2330 train_time:104494ms step_avg:58.57ms
step:1785/2330 train_time:104553ms step_avg:58.57ms
step:1786/2330 train_time:104615ms step_avg:58.57ms
step:1787/2330 train_time:104672ms step_avg:58.57ms
step:1788/2330 train_time:104734ms step_avg:58.58ms
step:1789/2330 train_time:104792ms step_avg:58.58ms
step:1790/2330 train_time:104855ms step_avg:58.58ms
step:1791/2330 train_time:104912ms step_avg:58.58ms
step:1792/2330 train_time:104972ms step_avg:58.58ms
step:1793/2330 train_time:105029ms step_avg:58.58ms
step:1794/2330 train_time:105090ms step_avg:58.58ms
step:1795/2330 train_time:105146ms step_avg:58.58ms
step:1796/2330 train_time:105208ms step_avg:58.58ms
step:1797/2330 train_time:105264ms step_avg:58.58ms
step:1798/2330 train_time:105325ms step_avg:58.58ms
step:1799/2330 train_time:105382ms step_avg:58.58ms
step:1800/2330 train_time:105443ms step_avg:58.58ms
step:1801/2330 train_time:105501ms step_avg:58.58ms
step:1802/2330 train_time:105563ms step_avg:58.58ms
step:1803/2330 train_time:105621ms step_avg:58.58ms
step:1804/2330 train_time:105682ms step_avg:58.58ms
step:1805/2330 train_time:105740ms step_avg:58.58ms
step:1806/2330 train_time:105801ms step_avg:58.58ms
step:1807/2330 train_time:105859ms step_avg:58.58ms
step:1808/2330 train_time:105920ms step_avg:58.58ms
step:1809/2330 train_time:105978ms step_avg:58.58ms
step:1810/2330 train_time:106038ms step_avg:58.58ms
step:1811/2330 train_time:106095ms step_avg:58.58ms
step:1812/2330 train_time:106155ms step_avg:58.58ms
step:1813/2330 train_time:106211ms step_avg:58.58ms
step:1814/2330 train_time:106273ms step_avg:58.58ms
step:1815/2330 train_time:106329ms step_avg:58.58ms
step:1816/2330 train_time:106391ms step_avg:58.59ms
step:1817/2330 train_time:106447ms step_avg:58.58ms
step:1818/2330 train_time:106510ms step_avg:58.59ms
step:1819/2330 train_time:106567ms step_avg:58.59ms
step:1820/2330 train_time:106629ms step_avg:58.59ms
step:1821/2330 train_time:106687ms step_avg:58.59ms
step:1822/2330 train_time:106748ms step_avg:58.59ms
step:1823/2330 train_time:106806ms step_avg:58.59ms
step:1824/2330 train_time:106866ms step_avg:58.59ms
step:1825/2330 train_time:106923ms step_avg:58.59ms
step:1826/2330 train_time:106985ms step_avg:58.59ms
step:1827/2330 train_time:107043ms step_avg:58.59ms
step:1828/2330 train_time:107104ms step_avg:58.59ms
step:1829/2330 train_time:107161ms step_avg:58.59ms
step:1830/2330 train_time:107223ms step_avg:58.59ms
step:1831/2330 train_time:107280ms step_avg:58.59ms
step:1832/2330 train_time:107341ms step_avg:58.59ms
step:1833/2330 train_time:107398ms step_avg:58.59ms
step:1834/2330 train_time:107459ms step_avg:58.59ms
step:1835/2330 train_time:107516ms step_avg:58.59ms
step:1836/2330 train_time:107577ms step_avg:58.59ms
step:1837/2330 train_time:107635ms step_avg:58.59ms
step:1838/2330 train_time:107697ms step_avg:58.59ms
step:1839/2330 train_time:107754ms step_avg:58.59ms
step:1840/2330 train_time:107816ms step_avg:58.60ms
step:1841/2330 train_time:107873ms step_avg:58.59ms
step:1842/2330 train_time:107934ms step_avg:58.60ms
step:1843/2330 train_time:107992ms step_avg:58.60ms
step:1844/2330 train_time:108053ms step_avg:58.60ms
step:1845/2330 train_time:108109ms step_avg:58.60ms
step:1846/2330 train_time:108171ms step_avg:58.60ms
step:1847/2330 train_time:108227ms step_avg:58.60ms
step:1848/2330 train_time:108288ms step_avg:58.60ms
step:1849/2330 train_time:108345ms step_avg:58.60ms
step:1850/2330 train_time:108405ms step_avg:58.60ms
step:1851/2330 train_time:108463ms step_avg:58.60ms
step:1852/2330 train_time:108524ms step_avg:58.60ms
step:1853/2330 train_time:108581ms step_avg:58.60ms
step:1854/2330 train_time:108643ms step_avg:58.60ms
step:1855/2330 train_time:108701ms step_avg:58.60ms
step:1856/2330 train_time:108761ms step_avg:58.60ms
step:1857/2330 train_time:108819ms step_avg:58.60ms
step:1858/2330 train_time:108880ms step_avg:58.60ms
step:1859/2330 train_time:108937ms step_avg:58.60ms
step:1860/2330 train_time:108999ms step_avg:58.60ms
step:1861/2330 train_time:109056ms step_avg:58.60ms
step:1862/2330 train_time:109118ms step_avg:58.60ms
step:1863/2330 train_time:109175ms step_avg:58.60ms
step:1864/2330 train_time:109235ms step_avg:58.60ms
step:1865/2330 train_time:109291ms step_avg:58.60ms
step:1866/2330 train_time:109353ms step_avg:58.60ms
step:1867/2330 train_time:109409ms step_avg:58.60ms
step:1868/2330 train_time:109471ms step_avg:58.60ms
step:1869/2330 train_time:109528ms step_avg:58.60ms
step:1870/2330 train_time:109591ms step_avg:58.60ms
step:1871/2330 train_time:109648ms step_avg:58.60ms
step:1872/2330 train_time:109709ms step_avg:58.61ms
step:1873/2330 train_time:109766ms step_avg:58.60ms
step:1874/2330 train_time:109828ms step_avg:58.61ms
step:1875/2330 train_time:109886ms step_avg:58.61ms
step:1876/2330 train_time:109947ms step_avg:58.61ms
step:1877/2330 train_time:110004ms step_avg:58.61ms
step:1878/2330 train_time:110066ms step_avg:58.61ms
step:1879/2330 train_time:110123ms step_avg:58.61ms
step:1880/2330 train_time:110184ms step_avg:58.61ms
step:1881/2330 train_time:110241ms step_avg:58.61ms
step:1882/2330 train_time:110303ms step_avg:58.61ms
step:1883/2330 train_time:110361ms step_avg:58.61ms
step:1884/2330 train_time:110420ms step_avg:58.61ms
step:1885/2330 train_time:110478ms step_avg:58.61ms
step:1886/2330 train_time:110539ms step_avg:58.61ms
step:1887/2330 train_time:110596ms step_avg:58.61ms
step:1888/2330 train_time:110657ms step_avg:58.61ms
step:1889/2330 train_time:110714ms step_avg:58.61ms
step:1890/2330 train_time:110775ms step_avg:58.61ms
step:1891/2330 train_time:110832ms step_avg:58.61ms
step:1892/2330 train_time:110895ms step_avg:58.61ms
step:1893/2330 train_time:110952ms step_avg:58.61ms
step:1894/2330 train_time:111013ms step_avg:58.61ms
step:1895/2330 train_time:111069ms step_avg:58.61ms
step:1896/2330 train_time:111130ms step_avg:58.61ms
step:1897/2330 train_time:111187ms step_avg:58.61ms
step:1898/2330 train_time:111249ms step_avg:58.61ms
step:1899/2330 train_time:111306ms step_avg:58.61ms
step:1900/2330 train_time:111367ms step_avg:58.61ms
step:1901/2330 train_time:111424ms step_avg:58.61ms
step:1902/2330 train_time:111484ms step_avg:58.61ms
step:1903/2330 train_time:111541ms step_avg:58.61ms
step:1904/2330 train_time:111603ms step_avg:58.61ms
step:1905/2330 train_time:111661ms step_avg:58.61ms
step:1906/2330 train_time:111722ms step_avg:58.62ms
step:1907/2330 train_time:111779ms step_avg:58.62ms
step:1908/2330 train_time:111840ms step_avg:58.62ms
step:1909/2330 train_time:111898ms step_avg:58.62ms
step:1910/2330 train_time:111959ms step_avg:58.62ms
step:1911/2330 train_time:112016ms step_avg:58.62ms
step:1912/2330 train_time:112077ms step_avg:58.62ms
step:1913/2330 train_time:112134ms step_avg:58.62ms
step:1914/2330 train_time:112194ms step_avg:58.62ms
step:1915/2330 train_time:112251ms step_avg:58.62ms
step:1916/2330 train_time:112313ms step_avg:58.62ms
step:1917/2330 train_time:112369ms step_avg:58.62ms
step:1918/2330 train_time:112431ms step_avg:58.62ms
step:1919/2330 train_time:112488ms step_avg:58.62ms
step:1920/2330 train_time:112550ms step_avg:58.62ms
step:1921/2330 train_time:112607ms step_avg:58.62ms
step:1922/2330 train_time:112669ms step_avg:58.62ms
step:1923/2330 train_time:112725ms step_avg:58.62ms
step:1924/2330 train_time:112788ms step_avg:58.62ms
step:1925/2330 train_time:112845ms step_avg:58.62ms
step:1926/2330 train_time:112906ms step_avg:58.62ms
step:1927/2330 train_time:112963ms step_avg:58.62ms
step:1928/2330 train_time:113024ms step_avg:58.62ms
step:1929/2330 train_time:113082ms step_avg:58.62ms
step:1930/2330 train_time:113143ms step_avg:58.62ms
step:1931/2330 train_time:113201ms step_avg:58.62ms
step:1932/2330 train_time:113260ms step_avg:58.62ms
step:1933/2330 train_time:113318ms step_avg:58.62ms
step:1934/2330 train_time:113379ms step_avg:58.62ms
step:1935/2330 train_time:113436ms step_avg:58.62ms
step:1936/2330 train_time:113498ms step_avg:58.62ms
step:1937/2330 train_time:113555ms step_avg:58.62ms
step:1938/2330 train_time:113615ms step_avg:58.62ms
step:1939/2330 train_time:113672ms step_avg:58.62ms
step:1940/2330 train_time:113733ms step_avg:58.63ms
step:1941/2330 train_time:113790ms step_avg:58.62ms
step:1942/2330 train_time:113853ms step_avg:58.63ms
step:1943/2330 train_time:113909ms step_avg:58.63ms
step:1944/2330 train_time:113970ms step_avg:58.63ms
step:1945/2330 train_time:114027ms step_avg:58.63ms
step:1946/2330 train_time:114089ms step_avg:58.63ms
step:1947/2330 train_time:114146ms step_avg:58.63ms
step:1948/2330 train_time:114208ms step_avg:58.63ms
step:1949/2330 train_time:114265ms step_avg:58.63ms
step:1950/2330 train_time:114327ms step_avg:58.63ms
step:1951/2330 train_time:114384ms step_avg:58.63ms
step:1952/2330 train_time:114445ms step_avg:58.63ms
step:1953/2330 train_time:114503ms step_avg:58.63ms
step:1954/2330 train_time:114563ms step_avg:58.63ms
step:1955/2330 train_time:114622ms step_avg:58.63ms
step:1956/2330 train_time:114683ms step_avg:58.63ms
step:1957/2330 train_time:114742ms step_avg:58.63ms
step:1958/2330 train_time:114802ms step_avg:58.63ms
step:1959/2330 train_time:114860ms step_avg:58.63ms
step:1960/2330 train_time:114921ms step_avg:58.63ms
step:1961/2330 train_time:114978ms step_avg:58.63ms
step:1962/2330 train_time:115039ms step_avg:58.63ms
step:1963/2330 train_time:115096ms step_avg:58.63ms
step:1964/2330 train_time:115157ms step_avg:58.63ms
step:1965/2330 train_time:115215ms step_avg:58.63ms
step:1966/2330 train_time:115276ms step_avg:58.63ms
step:1967/2330 train_time:115333ms step_avg:58.63ms
step:1968/2330 train_time:115395ms step_avg:58.64ms
step:1969/2330 train_time:115452ms step_avg:58.63ms
step:1970/2330 train_time:115512ms step_avg:58.64ms
step:1971/2330 train_time:115569ms step_avg:58.63ms
step:1972/2330 train_time:115630ms step_avg:58.64ms
step:1973/2330 train_time:115687ms step_avg:58.64ms
step:1974/2330 train_time:115750ms step_avg:58.64ms
step:1975/2330 train_time:115807ms step_avg:58.64ms
step:1976/2330 train_time:115869ms step_avg:58.64ms
step:1977/2330 train_time:115926ms step_avg:58.64ms
step:1978/2330 train_time:115987ms step_avg:58.64ms
step:1979/2330 train_time:116045ms step_avg:58.64ms
step:1980/2330 train_time:116106ms step_avg:58.64ms
step:1981/2330 train_time:116164ms step_avg:58.64ms
step:1982/2330 train_time:116226ms step_avg:58.64ms
step:1983/2330 train_time:116284ms step_avg:58.64ms
step:1984/2330 train_time:116345ms step_avg:58.64ms
step:1985/2330 train_time:116404ms step_avg:58.64ms
step:1986/2330 train_time:116464ms step_avg:58.64ms
step:1987/2330 train_time:116521ms step_avg:58.64ms
step:1988/2330 train_time:116581ms step_avg:58.64ms
step:1989/2330 train_time:116639ms step_avg:58.64ms
step:1990/2330 train_time:116699ms step_avg:58.64ms
step:1991/2330 train_time:116756ms step_avg:58.64ms
step:1992/2330 train_time:116818ms step_avg:58.64ms
step:1993/2330 train_time:116876ms step_avg:58.64ms
step:1994/2330 train_time:116937ms step_avg:58.64ms
step:1995/2330 train_time:116995ms step_avg:58.64ms
step:1996/2330 train_time:117057ms step_avg:58.65ms
step:1997/2330 train_time:117115ms step_avg:58.65ms
step:1998/2330 train_time:117175ms step_avg:58.65ms
step:1999/2330 train_time:117232ms step_avg:58.65ms
step:2000/2330 train_time:117294ms step_avg:58.65ms
step:2000/2330 val_loss:3.7555 train_time:117375ms step_avg:58.69ms
step:2001/2330 train_time:117395ms step_avg:58.67ms
step:2002/2330 train_time:117416ms step_avg:58.65ms
step:2003/2330 train_time:117475ms step_avg:58.65ms
step:2004/2330 train_time:117540ms step_avg:58.65ms
step:2005/2330 train_time:117598ms step_avg:58.65ms
step:2006/2330 train_time:117658ms step_avg:58.65ms
step:2007/2330 train_time:117715ms step_avg:58.65ms
step:2008/2330 train_time:117775ms step_avg:58.65ms
step:2009/2330 train_time:117832ms step_avg:58.65ms
step:2010/2330 train_time:117892ms step_avg:58.65ms
step:2011/2330 train_time:117949ms step_avg:58.65ms
step:2012/2330 train_time:118008ms step_avg:58.65ms
step:2013/2330 train_time:118065ms step_avg:58.65ms
step:2014/2330 train_time:118125ms step_avg:58.65ms
step:2015/2330 train_time:118182ms step_avg:58.65ms
step:2016/2330 train_time:118241ms step_avg:58.65ms
step:2017/2330 train_time:118299ms step_avg:58.65ms
step:2018/2330 train_time:118361ms step_avg:58.65ms
step:2019/2330 train_time:118420ms step_avg:58.65ms
step:2020/2330 train_time:118481ms step_avg:58.65ms
step:2021/2330 train_time:118540ms step_avg:58.65ms
step:2022/2330 train_time:118603ms step_avg:58.66ms
step:2023/2330 train_time:118660ms step_avg:58.66ms
step:2024/2330 train_time:118721ms step_avg:58.66ms
step:2025/2330 train_time:118777ms step_avg:58.66ms
step:2026/2330 train_time:118839ms step_avg:58.66ms
step:2027/2330 train_time:118896ms step_avg:58.66ms
step:2028/2330 train_time:118957ms step_avg:58.66ms
step:2029/2330 train_time:119014ms step_avg:58.66ms
step:2030/2330 train_time:119074ms step_avg:58.66ms
step:2031/2330 train_time:119131ms step_avg:58.66ms
step:2032/2330 train_time:119191ms step_avg:58.66ms
step:2033/2330 train_time:119248ms step_avg:58.66ms
step:2034/2330 train_time:119309ms step_avg:58.66ms
step:2035/2330 train_time:119367ms step_avg:58.66ms
step:2036/2330 train_time:119428ms step_avg:58.66ms
step:2037/2330 train_time:119486ms step_avg:58.66ms
step:2038/2330 train_time:119548ms step_avg:58.66ms
step:2039/2330 train_time:119606ms step_avg:58.66ms
step:2040/2330 train_time:119668ms step_avg:58.66ms
step:2041/2330 train_time:119725ms step_avg:58.66ms
step:2042/2330 train_time:119786ms step_avg:58.66ms
step:2043/2330 train_time:119843ms step_avg:58.66ms
step:2044/2330 train_time:119904ms step_avg:58.66ms
step:2045/2330 train_time:119961ms step_avg:58.66ms
step:2046/2330 train_time:120023ms step_avg:58.66ms
step:2047/2330 train_time:120079ms step_avg:58.66ms
step:2048/2330 train_time:120140ms step_avg:58.66ms
step:2049/2330 train_time:120197ms step_avg:58.66ms
step:2050/2330 train_time:120258ms step_avg:58.66ms
step:2051/2330 train_time:120315ms step_avg:58.66ms
step:2052/2330 train_time:120377ms step_avg:58.66ms
step:2053/2330 train_time:120436ms step_avg:58.66ms
step:2054/2330 train_time:120496ms step_avg:58.66ms
step:2055/2330 train_time:120554ms step_avg:58.66ms
step:2056/2330 train_time:120616ms step_avg:58.67ms
step:2057/2330 train_time:120674ms step_avg:58.67ms
step:2058/2330 train_time:120735ms step_avg:58.67ms
step:2059/2330 train_time:120793ms step_avg:58.67ms
step:2060/2330 train_time:120853ms step_avg:58.67ms
step:2061/2330 train_time:120912ms step_avg:58.67ms
step:2062/2330 train_time:120972ms step_avg:58.67ms
step:2063/2330 train_time:121030ms step_avg:58.67ms
step:2064/2330 train_time:121089ms step_avg:58.67ms
step:2065/2330 train_time:121146ms step_avg:58.67ms
step:2066/2330 train_time:121206ms step_avg:58.67ms
step:2067/2330 train_time:121263ms step_avg:58.67ms
step:2068/2330 train_time:121323ms step_avg:58.67ms
step:2069/2330 train_time:121380ms step_avg:58.67ms
step:2070/2330 train_time:121442ms step_avg:58.67ms
step:2071/2330 train_time:121500ms step_avg:58.67ms
step:2072/2330 train_time:121562ms step_avg:58.67ms
step:2073/2330 train_time:121621ms step_avg:58.67ms
step:2074/2330 train_time:121681ms step_avg:58.67ms
step:2075/2330 train_time:121739ms step_avg:58.67ms
step:2076/2330 train_time:121800ms step_avg:58.67ms
step:2077/2330 train_time:121857ms step_avg:58.67ms
step:2078/2330 train_time:121918ms step_avg:58.67ms
step:2079/2330 train_time:121975ms step_avg:58.67ms
step:2080/2330 train_time:122036ms step_avg:58.67ms
step:2081/2330 train_time:122094ms step_avg:58.67ms
step:2082/2330 train_time:122155ms step_avg:58.67ms
step:2083/2330 train_time:122212ms step_avg:58.67ms
step:2084/2330 train_time:122273ms step_avg:58.67ms
step:2085/2330 train_time:122330ms step_avg:58.67ms
step:2086/2330 train_time:122391ms step_avg:58.67ms
step:2087/2330 train_time:122449ms step_avg:58.67ms
step:2088/2330 train_time:122510ms step_avg:58.67ms
step:2089/2330 train_time:122568ms step_avg:58.67ms
step:2090/2330 train_time:122630ms step_avg:58.67ms
step:2091/2330 train_time:122686ms step_avg:58.67ms
step:2092/2330 train_time:122748ms step_avg:58.67ms
step:2093/2330 train_time:122804ms step_avg:58.67ms
step:2094/2330 train_time:122867ms step_avg:58.68ms
step:2095/2330 train_time:122924ms step_avg:58.67ms
step:2096/2330 train_time:122985ms step_avg:58.68ms
step:2097/2330 train_time:123041ms step_avg:58.67ms
step:2098/2330 train_time:123103ms step_avg:58.68ms
step:2099/2330 train_time:123160ms step_avg:58.68ms
step:2100/2330 train_time:123221ms step_avg:58.68ms
step:2101/2330 train_time:123278ms step_avg:58.68ms
step:2102/2330 train_time:123340ms step_avg:58.68ms
step:2103/2330 train_time:123397ms step_avg:58.68ms
step:2104/2330 train_time:123459ms step_avg:58.68ms
step:2105/2330 train_time:123518ms step_avg:58.68ms
step:2106/2330 train_time:123578ms step_avg:58.68ms
step:2107/2330 train_time:123636ms step_avg:58.68ms
step:2108/2330 train_time:123697ms step_avg:58.68ms
step:2109/2330 train_time:123755ms step_avg:58.68ms
step:2110/2330 train_time:123815ms step_avg:58.68ms
step:2111/2330 train_time:123872ms step_avg:58.68ms
step:2112/2330 train_time:123933ms step_avg:58.68ms
step:2113/2330 train_time:123991ms step_avg:58.68ms
step:2114/2330 train_time:124052ms step_avg:58.68ms
step:2115/2330 train_time:124110ms step_avg:58.68ms
step:2116/2330 train_time:124169ms step_avg:58.68ms
step:2117/2330 train_time:124226ms step_avg:58.68ms
step:2118/2330 train_time:124286ms step_avg:58.68ms
step:2119/2330 train_time:124343ms step_avg:58.68ms
step:2120/2330 train_time:124404ms step_avg:58.68ms
step:2121/2330 train_time:124462ms step_avg:58.68ms
step:2122/2330 train_time:124524ms step_avg:58.68ms
step:2123/2330 train_time:124580ms step_avg:58.68ms
step:2124/2330 train_time:124643ms step_avg:58.68ms
step:2125/2330 train_time:124700ms step_avg:58.68ms
step:2126/2330 train_time:124762ms step_avg:58.68ms
step:2127/2330 train_time:124819ms step_avg:58.68ms
step:2128/2330 train_time:124882ms step_avg:58.69ms
step:2129/2330 train_time:124938ms step_avg:58.68ms
step:2130/2330 train_time:125001ms step_avg:58.69ms
step:2131/2330 train_time:125058ms step_avg:58.69ms
step:2132/2330 train_time:125119ms step_avg:58.69ms
step:2133/2330 train_time:125177ms step_avg:58.69ms
step:2134/2330 train_time:125237ms step_avg:58.69ms
step:2135/2330 train_time:125294ms step_avg:58.69ms
step:2136/2330 train_time:125356ms step_avg:58.69ms
step:2137/2330 train_time:125415ms step_avg:58.69ms
step:2138/2330 train_time:125475ms step_avg:58.69ms
step:2139/2330 train_time:125532ms step_avg:58.69ms
step:2140/2330 train_time:125593ms step_avg:58.69ms
step:2141/2330 train_time:125651ms step_avg:58.69ms
step:2142/2330 train_time:125711ms step_avg:58.69ms
step:2143/2330 train_time:125770ms step_avg:58.69ms
step:2144/2330 train_time:125830ms step_avg:58.69ms
step:2145/2330 train_time:125887ms step_avg:58.69ms
step:2146/2330 train_time:125948ms step_avg:58.69ms
step:2147/2330 train_time:126005ms step_avg:58.69ms
step:2148/2330 train_time:126066ms step_avg:58.69ms
step:2149/2330 train_time:126123ms step_avg:58.69ms
step:2150/2330 train_time:126184ms step_avg:58.69ms
step:2151/2330 train_time:126241ms step_avg:58.69ms
step:2152/2330 train_time:126302ms step_avg:58.69ms
step:2153/2330 train_time:126359ms step_avg:58.69ms
step:2154/2330 train_time:126420ms step_avg:58.69ms
step:2155/2330 train_time:126478ms step_avg:58.69ms
step:2156/2330 train_time:126539ms step_avg:58.69ms
step:2157/2330 train_time:126597ms step_avg:58.69ms
step:2158/2330 train_time:126658ms step_avg:58.69ms
step:2159/2330 train_time:126715ms step_avg:58.69ms
step:2160/2330 train_time:126777ms step_avg:58.69ms
step:2161/2330 train_time:126834ms step_avg:58.69ms
step:2162/2330 train_time:126895ms step_avg:58.69ms
step:2163/2330 train_time:126953ms step_avg:58.69ms
step:2164/2330 train_time:127014ms step_avg:58.69ms
step:2165/2330 train_time:127073ms step_avg:58.69ms
step:2166/2330 train_time:127133ms step_avg:58.69ms
step:2167/2330 train_time:127191ms step_avg:58.69ms
step:2168/2330 train_time:127250ms step_avg:58.69ms
step:2169/2330 train_time:127307ms step_avg:58.69ms
step:2170/2330 train_time:127368ms step_avg:58.69ms
step:2171/2330 train_time:127425ms step_avg:58.69ms
step:2172/2330 train_time:127487ms step_avg:58.70ms
step:2173/2330 train_time:127544ms step_avg:58.69ms
step:2174/2330 train_time:127606ms step_avg:58.70ms
step:2175/2330 train_time:127663ms step_avg:58.70ms
step:2176/2330 train_time:127725ms step_avg:58.70ms
step:2177/2330 train_time:127782ms step_avg:58.70ms
step:2178/2330 train_time:127843ms step_avg:58.70ms
step:2179/2330 train_time:127900ms step_avg:58.70ms
step:2180/2330 train_time:127963ms step_avg:58.70ms
step:2181/2330 train_time:128019ms step_avg:58.70ms
step:2182/2330 train_time:128081ms step_avg:58.70ms
step:2183/2330 train_time:128138ms step_avg:58.70ms
step:2184/2330 train_time:128200ms step_avg:58.70ms
step:2185/2330 train_time:128257ms step_avg:58.70ms
step:2186/2330 train_time:128318ms step_avg:58.70ms
step:2187/2330 train_time:128376ms step_avg:58.70ms
step:2188/2330 train_time:128437ms step_avg:58.70ms
step:2189/2330 train_time:128495ms step_avg:58.70ms
step:2190/2330 train_time:128555ms step_avg:58.70ms
step:2191/2330 train_time:128614ms step_avg:58.70ms
step:2192/2330 train_time:128674ms step_avg:58.70ms
step:2193/2330 train_time:128731ms step_avg:58.70ms
step:2194/2330 train_time:128793ms step_avg:58.70ms
step:2195/2330 train_time:128850ms step_avg:58.70ms
step:2196/2330 train_time:128910ms step_avg:58.70ms
step:2197/2330 train_time:128967ms step_avg:58.70ms
step:2198/2330 train_time:129027ms step_avg:58.70ms
step:2199/2330 train_time:129084ms step_avg:58.70ms
step:2200/2330 train_time:129146ms step_avg:58.70ms
step:2201/2330 train_time:129202ms step_avg:58.70ms
step:2202/2330 train_time:129264ms step_avg:58.70ms
step:2203/2330 train_time:129321ms step_avg:58.70ms
step:2204/2330 train_time:129385ms step_avg:58.70ms
step:2205/2330 train_time:129442ms step_avg:58.70ms
step:2206/2330 train_time:129504ms step_avg:58.71ms
step:2207/2330 train_time:129561ms step_avg:58.70ms
step:2208/2330 train_time:129622ms step_avg:58.71ms
step:2209/2330 train_time:129678ms step_avg:58.70ms
step:2210/2330 train_time:129741ms step_avg:58.71ms
step:2211/2330 train_time:129798ms step_avg:58.71ms
step:2212/2330 train_time:129859ms step_avg:58.71ms
step:2213/2330 train_time:129917ms step_avg:58.71ms
step:2214/2330 train_time:129977ms step_avg:58.71ms
step:2215/2330 train_time:130035ms step_avg:58.71ms
step:2216/2330 train_time:130095ms step_avg:58.71ms
step:2217/2330 train_time:130153ms step_avg:58.71ms
step:2218/2330 train_time:130214ms step_avg:58.71ms
step:2219/2330 train_time:130271ms step_avg:58.71ms
step:2220/2330 train_time:130332ms step_avg:58.71ms
step:2221/2330 train_time:130388ms step_avg:58.71ms
step:2222/2330 train_time:130450ms step_avg:58.71ms
step:2223/2330 train_time:130507ms step_avg:58.71ms
step:2224/2330 train_time:130569ms step_avg:58.71ms
step:2225/2330 train_time:130626ms step_avg:58.71ms
step:2226/2330 train_time:130687ms step_avg:58.71ms
step:2227/2330 train_time:130744ms step_avg:58.71ms
step:2228/2330 train_time:130806ms step_avg:58.71ms
step:2229/2330 train_time:130863ms step_avg:58.71ms
step:2230/2330 train_time:130925ms step_avg:58.71ms
step:2231/2330 train_time:130981ms step_avg:58.71ms
step:2232/2330 train_time:131044ms step_avg:58.71ms
step:2233/2330 train_time:131101ms step_avg:58.71ms
step:2234/2330 train_time:131163ms step_avg:58.71ms
step:2235/2330 train_time:131220ms step_avg:58.71ms
step:2236/2330 train_time:131281ms step_avg:58.71ms
step:2237/2330 train_time:131338ms step_avg:58.71ms
step:2238/2330 train_time:131400ms step_avg:58.71ms
step:2239/2330 train_time:131457ms step_avg:58.71ms
step:2240/2330 train_time:131518ms step_avg:58.71ms
step:2241/2330 train_time:131576ms step_avg:58.71ms
step:2242/2330 train_time:131636ms step_avg:58.71ms
step:2243/2330 train_time:131694ms step_avg:58.71ms
step:2244/2330 train_time:131755ms step_avg:58.71ms
step:2245/2330 train_time:131813ms step_avg:58.71ms
step:2246/2330 train_time:131873ms step_avg:58.71ms
step:2247/2330 train_time:131930ms step_avg:58.71ms
step:2248/2330 train_time:131991ms step_avg:58.71ms
step:2249/2330 train_time:132049ms step_avg:58.71ms
step:2250/2330 train_time:132110ms step_avg:58.72ms
step:2250/2330 val_loss:3.7078 train_time:132191ms step_avg:58.75ms
step:2251/2330 train_time:132210ms step_avg:58.73ms
step:2252/2330 train_time:132230ms step_avg:58.72ms
step:2253/2330 train_time:132291ms step_avg:58.72ms
step:2254/2330 train_time:132355ms step_avg:58.72ms
step:2255/2330 train_time:132413ms step_avg:58.72ms
step:2256/2330 train_time:132475ms step_avg:58.72ms
step:2257/2330 train_time:132532ms step_avg:58.72ms
step:2258/2330 train_time:132592ms step_avg:58.72ms
step:2259/2330 train_time:132649ms step_avg:58.72ms
step:2260/2330 train_time:132709ms step_avg:58.72ms
step:2261/2330 train_time:132765ms step_avg:58.72ms
step:2262/2330 train_time:132826ms step_avg:58.72ms
step:2263/2330 train_time:132882ms step_avg:58.72ms
step:2264/2330 train_time:132943ms step_avg:58.72ms
step:2265/2330 train_time:132999ms step_avg:58.72ms
step:2266/2330 train_time:133061ms step_avg:58.72ms
step:2267/2330 train_time:133117ms step_avg:58.72ms
step:2268/2330 train_time:133179ms step_avg:58.72ms
step:2269/2330 train_time:133238ms step_avg:58.72ms
step:2270/2330 train_time:133301ms step_avg:58.72ms
step:2271/2330 train_time:133359ms step_avg:58.72ms
step:2272/2330 train_time:133421ms step_avg:58.72ms
step:2273/2330 train_time:133478ms step_avg:58.72ms
step:2274/2330 train_time:133540ms step_avg:58.72ms
step:2275/2330 train_time:133598ms step_avg:58.72ms
step:2276/2330 train_time:133658ms step_avg:58.73ms
step:2277/2330 train_time:133716ms step_avg:58.72ms
step:2278/2330 train_time:133777ms step_avg:58.73ms
step:2279/2330 train_time:133834ms step_avg:58.72ms
step:2280/2330 train_time:133894ms step_avg:58.73ms
step:2281/2330 train_time:133951ms step_avg:58.72ms
step:2282/2330 train_time:134011ms step_avg:58.73ms
step:2283/2330 train_time:134067ms step_avg:58.72ms
step:2284/2330 train_time:134129ms step_avg:58.73ms
step:2285/2330 train_time:134186ms step_avg:58.72ms
step:2286/2330 train_time:134248ms step_avg:58.73ms
step:2287/2330 train_time:134306ms step_avg:58.73ms
step:2288/2330 train_time:134369ms step_avg:58.73ms
step:2289/2330 train_time:134426ms step_avg:58.73ms
step:2290/2330 train_time:134489ms step_avg:58.73ms
step:2291/2330 train_time:134546ms step_avg:58.73ms
step:2292/2330 train_time:134608ms step_avg:58.73ms
step:2293/2330 train_time:134664ms step_avg:58.73ms
step:2294/2330 train_time:134726ms step_avg:58.73ms
step:2295/2330 train_time:134783ms step_avg:58.73ms
step:2296/2330 train_time:134843ms step_avg:58.73ms
step:2297/2330 train_time:134900ms step_avg:58.73ms
step:2298/2330 train_time:134961ms step_avg:58.73ms
step:2299/2330 train_time:135018ms step_avg:58.73ms
step:2300/2330 train_time:135079ms step_avg:58.73ms
step:2301/2330 train_time:135136ms step_avg:58.73ms
step:2302/2330 train_time:135198ms step_avg:58.73ms
step:2303/2330 train_time:135257ms step_avg:58.73ms
step:2304/2330 train_time:135318ms step_avg:58.73ms
step:2305/2330 train_time:135377ms step_avg:58.73ms
step:2306/2330 train_time:135437ms step_avg:58.73ms
step:2307/2330 train_time:135495ms step_avg:58.73ms
step:2308/2330 train_time:135557ms step_avg:58.73ms
step:2309/2330 train_time:135615ms step_avg:58.73ms
step:2310/2330 train_time:135676ms step_avg:58.73ms
step:2311/2330 train_time:135734ms step_avg:58.73ms
step:2312/2330 train_time:135794ms step_avg:58.73ms
step:2313/2330 train_time:135850ms step_avg:58.73ms
step:2314/2330 train_time:135912ms step_avg:58.73ms
step:2315/2330 train_time:135968ms step_avg:58.73ms
step:2316/2330 train_time:136029ms step_avg:58.73ms
step:2317/2330 train_time:136086ms step_avg:58.73ms
step:2318/2330 train_time:136147ms step_avg:58.73ms
step:2319/2330 train_time:136204ms step_avg:58.73ms
step:2320/2330 train_time:136266ms step_avg:58.74ms
step:2321/2330 train_time:136323ms step_avg:58.73ms
step:2322/2330 train_time:136385ms step_avg:58.74ms
step:2323/2330 train_time:136442ms step_avg:58.74ms
step:2324/2330 train_time:136505ms step_avg:58.74ms
step:2325/2330 train_time:136562ms step_avg:58.74ms
step:2326/2330 train_time:136624ms step_avg:58.74ms
step:2327/2330 train_time:136681ms step_avg:58.74ms
step:2328/2330 train_time:136743ms step_avg:58.74ms
step:2329/2330 train_time:136800ms step_avg:58.74ms
step:2330/2330 train_time:136862ms step_avg:58.74ms
step:2330/2330 val_loss:3.6923 train_time:136944ms step_avg:58.77ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
