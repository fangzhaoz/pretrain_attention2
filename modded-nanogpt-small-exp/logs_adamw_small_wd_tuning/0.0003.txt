import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:03:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:93ms step_avg:92.63ms
step:2/2330 train_time:202ms step_avg:100.98ms
step:3/2330 train_time:220ms step_avg:73.32ms
step:4/2330 train_time:239ms step_avg:59.75ms
step:5/2330 train_time:293ms step_avg:58.59ms
step:6/2330 train_time:351ms step_avg:58.50ms
step:7/2330 train_time:406ms step_avg:58.00ms
step:8/2330 train_time:465ms step_avg:58.06ms
step:9/2330 train_time:520ms step_avg:57.78ms
step:10/2330 train_time:578ms step_avg:57.82ms
step:11/2330 train_time:633ms step_avg:57.58ms
step:12/2330 train_time:692ms step_avg:57.66ms
step:13/2330 train_time:747ms step_avg:57.47ms
step:14/2330 train_time:805ms step_avg:57.52ms
step:15/2330 train_time:861ms step_avg:57.40ms
step:16/2330 train_time:919ms step_avg:57.44ms
step:17/2330 train_time:974ms step_avg:57.30ms
step:18/2330 train_time:1033ms step_avg:57.42ms
step:19/2330 train_time:1091ms step_avg:57.40ms
step:20/2330 train_time:1153ms step_avg:57.66ms
step:21/2330 train_time:1211ms step_avg:57.67ms
step:22/2330 train_time:1271ms step_avg:57.77ms
step:23/2330 train_time:1327ms step_avg:57.70ms
step:24/2330 train_time:1387ms step_avg:57.78ms
step:25/2330 train_time:1442ms step_avg:57.68ms
step:26/2330 train_time:1502ms step_avg:57.75ms
step:27/2330 train_time:1557ms step_avg:57.66ms
step:28/2330 train_time:1616ms step_avg:57.71ms
step:29/2330 train_time:1671ms step_avg:57.63ms
step:30/2330 train_time:1730ms step_avg:57.66ms
step:31/2330 train_time:1785ms step_avg:57.59ms
step:32/2330 train_time:1844ms step_avg:57.61ms
step:33/2330 train_time:1900ms step_avg:57.57ms
step:34/2330 train_time:1959ms step_avg:57.61ms
step:35/2330 train_time:2015ms step_avg:57.56ms
step:36/2330 train_time:2075ms step_avg:57.63ms
step:37/2330 train_time:2132ms step_avg:57.61ms
step:38/2330 train_time:2191ms step_avg:57.65ms
step:39/2330 train_time:2247ms step_avg:57.62ms
step:40/2330 train_time:2307ms step_avg:57.67ms
step:41/2330 train_time:2363ms step_avg:57.64ms
step:42/2330 train_time:2422ms step_avg:57.67ms
step:43/2330 train_time:2479ms step_avg:57.64ms
step:44/2330 train_time:2538ms step_avg:57.69ms
step:45/2330 train_time:2594ms step_avg:57.64ms
step:46/2330 train_time:2652ms step_avg:57.65ms
step:47/2330 train_time:2707ms step_avg:57.60ms
step:48/2330 train_time:2766ms step_avg:57.62ms
step:49/2330 train_time:2822ms step_avg:57.59ms
step:50/2330 train_time:2880ms step_avg:57.61ms
step:51/2330 train_time:2936ms step_avg:57.58ms
step:52/2330 train_time:2995ms step_avg:57.59ms
step:53/2330 train_time:3051ms step_avg:57.57ms
step:54/2330 train_time:3110ms step_avg:57.60ms
step:55/2330 train_time:3167ms step_avg:57.59ms
step:56/2330 train_time:3227ms step_avg:57.62ms
step:57/2330 train_time:3283ms step_avg:57.59ms
step:58/2330 train_time:3342ms step_avg:57.62ms
step:59/2330 train_time:3399ms step_avg:57.61ms
step:60/2330 train_time:3457ms step_avg:57.62ms
step:61/2330 train_time:3513ms step_avg:57.60ms
step:62/2330 train_time:3572ms step_avg:57.62ms
step:63/2330 train_time:3628ms step_avg:57.58ms
step:64/2330 train_time:3686ms step_avg:57.60ms
step:65/2330 train_time:3742ms step_avg:57.57ms
step:66/2330 train_time:3801ms step_avg:57.59ms
step:67/2330 train_time:3856ms step_avg:57.56ms
step:68/2330 train_time:3915ms step_avg:57.57ms
step:69/2330 train_time:3971ms step_avg:57.55ms
step:70/2330 train_time:4030ms step_avg:57.57ms
step:71/2330 train_time:4086ms step_avg:57.56ms
step:72/2330 train_time:4145ms step_avg:57.57ms
step:73/2330 train_time:4202ms step_avg:57.57ms
step:74/2330 train_time:4261ms step_avg:57.58ms
step:75/2330 train_time:4317ms step_avg:57.56ms
step:76/2330 train_time:4377ms step_avg:57.59ms
step:77/2330 train_time:4433ms step_avg:57.57ms
step:78/2330 train_time:4492ms step_avg:57.59ms
step:79/2330 train_time:4547ms step_avg:57.56ms
step:80/2330 train_time:4606ms step_avg:57.58ms
step:81/2330 train_time:4662ms step_avg:57.55ms
step:82/2330 train_time:4721ms step_avg:57.57ms
step:83/2330 train_time:4777ms step_avg:57.55ms
step:84/2330 train_time:4836ms step_avg:57.57ms
step:85/2330 train_time:4891ms step_avg:57.55ms
step:86/2330 train_time:4951ms step_avg:57.57ms
step:87/2330 train_time:5007ms step_avg:57.55ms
step:88/2330 train_time:5066ms step_avg:57.57ms
step:89/2330 train_time:5122ms step_avg:57.56ms
step:90/2330 train_time:5181ms step_avg:57.57ms
step:91/2330 train_time:5237ms step_avg:57.55ms
step:92/2330 train_time:5296ms step_avg:57.57ms
step:93/2330 train_time:5352ms step_avg:57.55ms
step:94/2330 train_time:5411ms step_avg:57.57ms
step:95/2330 train_time:5467ms step_avg:57.54ms
step:96/2330 train_time:5526ms step_avg:57.57ms
step:97/2330 train_time:5582ms step_avg:57.55ms
step:98/2330 train_time:5641ms step_avg:57.57ms
step:99/2330 train_time:5697ms step_avg:57.54ms
step:100/2330 train_time:5756ms step_avg:57.56ms
step:101/2330 train_time:5812ms step_avg:57.54ms
step:102/2330 train_time:5871ms step_avg:57.56ms
step:103/2330 train_time:5927ms step_avg:57.54ms
step:104/2330 train_time:5985ms step_avg:57.55ms
step:105/2330 train_time:6041ms step_avg:57.53ms
step:106/2330 train_time:6100ms step_avg:57.55ms
step:107/2330 train_time:6157ms step_avg:57.54ms
step:108/2330 train_time:6216ms step_avg:57.56ms
step:109/2330 train_time:6272ms step_avg:57.54ms
step:110/2330 train_time:6331ms step_avg:57.55ms
step:111/2330 train_time:6387ms step_avg:57.54ms
step:112/2330 train_time:6446ms step_avg:57.55ms
step:113/2330 train_time:6502ms step_avg:57.54ms
step:114/2330 train_time:6561ms step_avg:57.55ms
step:115/2330 train_time:6617ms step_avg:57.54ms
step:116/2330 train_time:6675ms step_avg:57.55ms
step:117/2330 train_time:6731ms step_avg:57.53ms
step:118/2330 train_time:6789ms step_avg:57.54ms
step:119/2330 train_time:6845ms step_avg:57.52ms
step:120/2330 train_time:6905ms step_avg:57.54ms
step:121/2330 train_time:6961ms step_avg:57.53ms
step:122/2330 train_time:7019ms step_avg:57.53ms
step:123/2330 train_time:7075ms step_avg:57.52ms
step:124/2330 train_time:7134ms step_avg:57.53ms
step:125/2330 train_time:7190ms step_avg:57.52ms
step:126/2330 train_time:7249ms step_avg:57.53ms
step:127/2330 train_time:7305ms step_avg:57.52ms
step:128/2330 train_time:7364ms step_avg:57.53ms
step:129/2330 train_time:7421ms step_avg:57.52ms
step:130/2330 train_time:7480ms step_avg:57.54ms
step:131/2330 train_time:7536ms step_avg:57.53ms
step:132/2330 train_time:7595ms step_avg:57.53ms
step:133/2330 train_time:7650ms step_avg:57.52ms
step:134/2330 train_time:7709ms step_avg:57.53ms
step:135/2330 train_time:7765ms step_avg:57.52ms
step:136/2330 train_time:7825ms step_avg:57.53ms
step:137/2330 train_time:7881ms step_avg:57.52ms
step:138/2330 train_time:7939ms step_avg:57.53ms
step:139/2330 train_time:7995ms step_avg:57.52ms
step:140/2330 train_time:8055ms step_avg:57.53ms
step:141/2330 train_time:8110ms step_avg:57.52ms
step:142/2330 train_time:8169ms step_avg:57.53ms
step:143/2330 train_time:8225ms step_avg:57.52ms
step:144/2330 train_time:8285ms step_avg:57.53ms
step:145/2330 train_time:8341ms step_avg:57.52ms
step:146/2330 train_time:8400ms step_avg:57.53ms
step:147/2330 train_time:8456ms step_avg:57.52ms
step:148/2330 train_time:8515ms step_avg:57.53ms
step:149/2330 train_time:8571ms step_avg:57.52ms
step:150/2330 train_time:8630ms step_avg:57.54ms
step:151/2330 train_time:8686ms step_avg:57.52ms
step:152/2330 train_time:8745ms step_avg:57.53ms
step:153/2330 train_time:8801ms step_avg:57.52ms
step:154/2330 train_time:8860ms step_avg:57.53ms
step:155/2330 train_time:8915ms step_avg:57.52ms
step:156/2330 train_time:8976ms step_avg:57.54ms
step:157/2330 train_time:9032ms step_avg:57.53ms
step:158/2330 train_time:9091ms step_avg:57.54ms
step:159/2330 train_time:9146ms step_avg:57.52ms
step:160/2330 train_time:9206ms step_avg:57.54ms
step:161/2330 train_time:9262ms step_avg:57.53ms
step:162/2330 train_time:9321ms step_avg:57.53ms
step:163/2330 train_time:9377ms step_avg:57.53ms
step:164/2330 train_time:9437ms step_avg:57.54ms
step:165/2330 train_time:9493ms step_avg:57.53ms
step:166/2330 train_time:9552ms step_avg:57.54ms
step:167/2330 train_time:9607ms step_avg:57.53ms
step:168/2330 train_time:9666ms step_avg:57.54ms
step:169/2330 train_time:9723ms step_avg:57.53ms
step:170/2330 train_time:9782ms step_avg:57.54ms
step:171/2330 train_time:9838ms step_avg:57.53ms
step:172/2330 train_time:9897ms step_avg:57.54ms
step:173/2330 train_time:9952ms step_avg:57.53ms
step:174/2330 train_time:10012ms step_avg:57.54ms
step:175/2330 train_time:10067ms step_avg:57.53ms
step:176/2330 train_time:10126ms step_avg:57.54ms
step:177/2330 train_time:10183ms step_avg:57.53ms
step:178/2330 train_time:10242ms step_avg:57.54ms
step:179/2330 train_time:10298ms step_avg:57.53ms
step:180/2330 train_time:10356ms step_avg:57.54ms
step:181/2330 train_time:10412ms step_avg:57.53ms
step:182/2330 train_time:10471ms step_avg:57.53ms
step:183/2330 train_time:10528ms step_avg:57.53ms
step:184/2330 train_time:10586ms step_avg:57.53ms
step:185/2330 train_time:10641ms step_avg:57.52ms
step:186/2330 train_time:10700ms step_avg:57.53ms
step:187/2330 train_time:10756ms step_avg:57.52ms
step:188/2330 train_time:10815ms step_avg:57.53ms
step:189/2330 train_time:10871ms step_avg:57.52ms
step:190/2330 train_time:10929ms step_avg:57.52ms
step:191/2330 train_time:10985ms step_avg:57.51ms
step:192/2330 train_time:11044ms step_avg:57.52ms
step:193/2330 train_time:11100ms step_avg:57.51ms
step:194/2330 train_time:11158ms step_avg:57.52ms
step:195/2330 train_time:11214ms step_avg:57.51ms
step:196/2330 train_time:11274ms step_avg:57.52ms
step:197/2330 train_time:11330ms step_avg:57.51ms
step:198/2330 train_time:11389ms step_avg:57.52ms
step:199/2330 train_time:11445ms step_avg:57.51ms
step:200/2330 train_time:11504ms step_avg:57.52ms
step:201/2330 train_time:11560ms step_avg:57.51ms
step:202/2330 train_time:11619ms step_avg:57.52ms
step:203/2330 train_time:11674ms step_avg:57.51ms
step:204/2330 train_time:11734ms step_avg:57.52ms
step:205/2330 train_time:11790ms step_avg:57.51ms
step:206/2330 train_time:11849ms step_avg:57.52ms
step:207/2330 train_time:11905ms step_avg:57.51ms
step:208/2330 train_time:11964ms step_avg:57.52ms
step:209/2330 train_time:12020ms step_avg:57.51ms
step:210/2330 train_time:12079ms step_avg:57.52ms
step:211/2330 train_time:12135ms step_avg:57.51ms
step:212/2330 train_time:12194ms step_avg:57.52ms
step:213/2330 train_time:12250ms step_avg:57.51ms
step:214/2330 train_time:12309ms step_avg:57.52ms
step:215/2330 train_time:12365ms step_avg:57.51ms
step:216/2330 train_time:12424ms step_avg:57.52ms
step:217/2330 train_time:12481ms step_avg:57.51ms
step:218/2330 train_time:12540ms step_avg:57.52ms
step:219/2330 train_time:12596ms step_avg:57.51ms
step:220/2330 train_time:12655ms step_avg:57.52ms
step:221/2330 train_time:12711ms step_avg:57.52ms
step:222/2330 train_time:12770ms step_avg:57.52ms
step:223/2330 train_time:12826ms step_avg:57.52ms
step:224/2330 train_time:12885ms step_avg:57.52ms
step:225/2330 train_time:12941ms step_avg:57.51ms
step:226/2330 train_time:13000ms step_avg:57.52ms
step:227/2330 train_time:13056ms step_avg:57.51ms
step:228/2330 train_time:13115ms step_avg:57.52ms
step:229/2330 train_time:13171ms step_avg:57.52ms
step:230/2330 train_time:13230ms step_avg:57.52ms
step:231/2330 train_time:13286ms step_avg:57.52ms
step:232/2330 train_time:13345ms step_avg:57.52ms
step:233/2330 train_time:13401ms step_avg:57.52ms
step:234/2330 train_time:13460ms step_avg:57.52ms
step:235/2330 train_time:13516ms step_avg:57.52ms
step:236/2330 train_time:13575ms step_avg:57.52ms
step:237/2330 train_time:13631ms step_avg:57.51ms
step:238/2330 train_time:13690ms step_avg:57.52ms
step:239/2330 train_time:13745ms step_avg:57.51ms
step:240/2330 train_time:13805ms step_avg:57.52ms
step:241/2330 train_time:13860ms step_avg:57.51ms
step:242/2330 train_time:13920ms step_avg:57.52ms
step:243/2330 train_time:13976ms step_avg:57.51ms
step:244/2330 train_time:14036ms step_avg:57.52ms
step:245/2330 train_time:14091ms step_avg:57.51ms
step:246/2330 train_time:14150ms step_avg:57.52ms
step:247/2330 train_time:14206ms step_avg:57.51ms
step:248/2330 train_time:14266ms step_avg:57.52ms
step:249/2330 train_time:14322ms step_avg:57.52ms
step:250/2330 train_time:14381ms step_avg:57.52ms
step:250/2330 val_loss:4.8996 train_time:14459ms step_avg:57.84ms
step:251/2330 train_time:14478ms step_avg:57.68ms
step:252/2330 train_time:14498ms step_avg:57.53ms
step:253/2330 train_time:14555ms step_avg:57.53ms
step:254/2330 train_time:14617ms step_avg:57.55ms
step:255/2330 train_time:14673ms step_avg:57.54ms
step:256/2330 train_time:14737ms step_avg:57.57ms
step:257/2330 train_time:14792ms step_avg:57.56ms
step:258/2330 train_time:14853ms step_avg:57.57ms
step:259/2330 train_time:14908ms step_avg:57.56ms
step:260/2330 train_time:14967ms step_avg:57.56ms
step:261/2330 train_time:15023ms step_avg:57.56ms
step:262/2330 train_time:15081ms step_avg:57.56ms
step:263/2330 train_time:15136ms step_avg:57.55ms
step:264/2330 train_time:15195ms step_avg:57.56ms
step:265/2330 train_time:15250ms step_avg:57.55ms
step:266/2330 train_time:15308ms step_avg:57.55ms
step:267/2330 train_time:15363ms step_avg:57.54ms
step:268/2330 train_time:15423ms step_avg:57.55ms
step:269/2330 train_time:15480ms step_avg:57.55ms
step:270/2330 train_time:15540ms step_avg:57.55ms
step:271/2330 train_time:15598ms step_avg:57.56ms
step:272/2330 train_time:15657ms step_avg:57.56ms
step:273/2330 train_time:15714ms step_avg:57.56ms
step:274/2330 train_time:15774ms step_avg:57.57ms
step:275/2330 train_time:15830ms step_avg:57.56ms
step:276/2330 train_time:15889ms step_avg:57.57ms
step:277/2330 train_time:15945ms step_avg:57.56ms
step:278/2330 train_time:16004ms step_avg:57.57ms
step:279/2330 train_time:16059ms step_avg:57.56ms
step:280/2330 train_time:16119ms step_avg:57.57ms
step:281/2330 train_time:16175ms step_avg:57.56ms
step:282/2330 train_time:16233ms step_avg:57.56ms
step:283/2330 train_time:16289ms step_avg:57.56ms
step:284/2330 train_time:16347ms step_avg:57.56ms
step:285/2330 train_time:16404ms step_avg:57.56ms
step:286/2330 train_time:16462ms step_avg:57.56ms
step:287/2330 train_time:16519ms step_avg:57.56ms
step:288/2330 train_time:16578ms step_avg:57.56ms
step:289/2330 train_time:16635ms step_avg:57.56ms
step:290/2330 train_time:16696ms step_avg:57.57ms
step:291/2330 train_time:16752ms step_avg:57.57ms
step:292/2330 train_time:16812ms step_avg:57.58ms
step:293/2330 train_time:16868ms step_avg:57.57ms
step:294/2330 train_time:16926ms step_avg:57.57ms
step:295/2330 train_time:16982ms step_avg:57.57ms
step:296/2330 train_time:17041ms step_avg:57.57ms
step:297/2330 train_time:17097ms step_avg:57.57ms
step:298/2330 train_time:17156ms step_avg:57.57ms
step:299/2330 train_time:17211ms step_avg:57.56ms
step:300/2330 train_time:17269ms step_avg:57.56ms
step:301/2330 train_time:17325ms step_avg:57.56ms
step:302/2330 train_time:17384ms step_avg:57.56ms
step:303/2330 train_time:17440ms step_avg:57.56ms
step:304/2330 train_time:17498ms step_avg:57.56ms
step:305/2330 train_time:17555ms step_avg:57.56ms
step:306/2330 train_time:17615ms step_avg:57.56ms
step:307/2330 train_time:17671ms step_avg:57.56ms
step:308/2330 train_time:17731ms step_avg:57.57ms
step:309/2330 train_time:17787ms step_avg:57.56ms
step:310/2330 train_time:17846ms step_avg:57.57ms
step:311/2330 train_time:17902ms step_avg:57.56ms
step:312/2330 train_time:17961ms step_avg:57.57ms
step:313/2330 train_time:18017ms step_avg:57.56ms
step:314/2330 train_time:18076ms step_avg:57.57ms
step:315/2330 train_time:18132ms step_avg:57.56ms
step:316/2330 train_time:18190ms step_avg:57.56ms
step:317/2330 train_time:18246ms step_avg:57.56ms
step:318/2330 train_time:18305ms step_avg:57.56ms
step:319/2330 train_time:18361ms step_avg:57.56ms
step:320/2330 train_time:18420ms step_avg:57.56ms
step:321/2330 train_time:18476ms step_avg:57.56ms
step:322/2330 train_time:18535ms step_avg:57.56ms
step:323/2330 train_time:18591ms step_avg:57.56ms
step:324/2330 train_time:18650ms step_avg:57.56ms
step:325/2330 train_time:18706ms step_avg:57.56ms
step:326/2330 train_time:18766ms step_avg:57.56ms
step:327/2330 train_time:18822ms step_avg:57.56ms
step:328/2330 train_time:18881ms step_avg:57.56ms
step:329/2330 train_time:18936ms step_avg:57.56ms
step:330/2330 train_time:18997ms step_avg:57.57ms
step:331/2330 train_time:19053ms step_avg:57.56ms
step:332/2330 train_time:19112ms step_avg:57.57ms
step:333/2330 train_time:19167ms step_avg:57.56ms
step:334/2330 train_time:19227ms step_avg:57.56ms
step:335/2330 train_time:19282ms step_avg:57.56ms
step:336/2330 train_time:19342ms step_avg:57.56ms
step:337/2330 train_time:19397ms step_avg:57.56ms
step:338/2330 train_time:19458ms step_avg:57.57ms
step:339/2330 train_time:19514ms step_avg:57.56ms
step:340/2330 train_time:19573ms step_avg:57.57ms
step:341/2330 train_time:19628ms step_avg:57.56ms
step:342/2330 train_time:19688ms step_avg:57.57ms
step:343/2330 train_time:19744ms step_avg:57.56ms
step:344/2330 train_time:19803ms step_avg:57.57ms
step:345/2330 train_time:19860ms step_avg:57.56ms
step:346/2330 train_time:19920ms step_avg:57.57ms
step:347/2330 train_time:19976ms step_avg:57.57ms
step:348/2330 train_time:20035ms step_avg:57.57ms
step:349/2330 train_time:20091ms step_avg:57.57ms
step:350/2330 train_time:20149ms step_avg:57.57ms
step:351/2330 train_time:20205ms step_avg:57.56ms
step:352/2330 train_time:20263ms step_avg:57.57ms
step:353/2330 train_time:20319ms step_avg:57.56ms
step:354/2330 train_time:20378ms step_avg:57.57ms
step:355/2330 train_time:20435ms step_avg:57.56ms
step:356/2330 train_time:20493ms step_avg:57.57ms
step:357/2330 train_time:20550ms step_avg:57.56ms
step:358/2330 train_time:20608ms step_avg:57.57ms
step:359/2330 train_time:20664ms step_avg:57.56ms
step:360/2330 train_time:20724ms step_avg:57.57ms
step:361/2330 train_time:20779ms step_avg:57.56ms
step:362/2330 train_time:20838ms step_avg:57.56ms
step:363/2330 train_time:20894ms step_avg:57.56ms
step:364/2330 train_time:20953ms step_avg:57.56ms
step:365/2330 train_time:21009ms step_avg:57.56ms
step:366/2330 train_time:21068ms step_avg:57.56ms
step:367/2330 train_time:21124ms step_avg:57.56ms
step:368/2330 train_time:21183ms step_avg:57.56ms
step:369/2330 train_time:21239ms step_avg:57.56ms
step:370/2330 train_time:21298ms step_avg:57.56ms
step:371/2330 train_time:21354ms step_avg:57.56ms
step:372/2330 train_time:21414ms step_avg:57.56ms
step:373/2330 train_time:21469ms step_avg:57.56ms
step:374/2330 train_time:21528ms step_avg:57.56ms
step:375/2330 train_time:21584ms step_avg:57.56ms
step:376/2330 train_time:21643ms step_avg:57.56ms
step:377/2330 train_time:21699ms step_avg:57.56ms
step:378/2330 train_time:21758ms step_avg:57.56ms
step:379/2330 train_time:21814ms step_avg:57.56ms
step:380/2330 train_time:21874ms step_avg:57.56ms
step:381/2330 train_time:21930ms step_avg:57.56ms
step:382/2330 train_time:21989ms step_avg:57.56ms
step:383/2330 train_time:22045ms step_avg:57.56ms
step:384/2330 train_time:22104ms step_avg:57.56ms
step:385/2330 train_time:22160ms step_avg:57.56ms
step:386/2330 train_time:22219ms step_avg:57.56ms
step:387/2330 train_time:22276ms step_avg:57.56ms
step:388/2330 train_time:22334ms step_avg:57.56ms
step:389/2330 train_time:22391ms step_avg:57.56ms
step:390/2330 train_time:22449ms step_avg:57.56ms
step:391/2330 train_time:22506ms step_avg:57.56ms
step:392/2330 train_time:22564ms step_avg:57.56ms
step:393/2330 train_time:22620ms step_avg:57.56ms
step:394/2330 train_time:22680ms step_avg:57.56ms
step:395/2330 train_time:22736ms step_avg:57.56ms
step:396/2330 train_time:22796ms step_avg:57.56ms
step:397/2330 train_time:22852ms step_avg:57.56ms
step:398/2330 train_time:22910ms step_avg:57.56ms
step:399/2330 train_time:22966ms step_avg:57.56ms
step:400/2330 train_time:23025ms step_avg:57.56ms
step:401/2330 train_time:23081ms step_avg:57.56ms
step:402/2330 train_time:23140ms step_avg:57.56ms
step:403/2330 train_time:23196ms step_avg:57.56ms
step:404/2330 train_time:23257ms step_avg:57.57ms
step:405/2330 train_time:23314ms step_avg:57.56ms
step:406/2330 train_time:23372ms step_avg:57.57ms
step:407/2330 train_time:23428ms step_avg:57.56ms
step:408/2330 train_time:23487ms step_avg:57.57ms
step:409/2330 train_time:23543ms step_avg:57.56ms
step:410/2330 train_time:23602ms step_avg:57.56ms
step:411/2330 train_time:23658ms step_avg:57.56ms
step:412/2330 train_time:23717ms step_avg:57.57ms
step:413/2330 train_time:23773ms step_avg:57.56ms
step:414/2330 train_time:23831ms step_avg:57.56ms
step:415/2330 train_time:23887ms step_avg:57.56ms
step:416/2330 train_time:23947ms step_avg:57.56ms
step:417/2330 train_time:24003ms step_avg:57.56ms
step:418/2330 train_time:24062ms step_avg:57.57ms
step:419/2330 train_time:24118ms step_avg:57.56ms
step:420/2330 train_time:24177ms step_avg:57.57ms
step:421/2330 train_time:24234ms step_avg:57.56ms
step:422/2330 train_time:24292ms step_avg:57.56ms
step:423/2330 train_time:24348ms step_avg:57.56ms
step:424/2330 train_time:24407ms step_avg:57.56ms
step:425/2330 train_time:24463ms step_avg:57.56ms
step:426/2330 train_time:24523ms step_avg:57.57ms
step:427/2330 train_time:24579ms step_avg:57.56ms
step:428/2330 train_time:24639ms step_avg:57.57ms
step:429/2330 train_time:24696ms step_avg:57.57ms
step:430/2330 train_time:24754ms step_avg:57.57ms
step:431/2330 train_time:24811ms step_avg:57.57ms
step:432/2330 train_time:24870ms step_avg:57.57ms
step:433/2330 train_time:24926ms step_avg:57.56ms
step:434/2330 train_time:24985ms step_avg:57.57ms
step:435/2330 train_time:25041ms step_avg:57.57ms
step:436/2330 train_time:25100ms step_avg:57.57ms
step:437/2330 train_time:25156ms step_avg:57.56ms
step:438/2330 train_time:25215ms step_avg:57.57ms
step:439/2330 train_time:25272ms step_avg:57.57ms
step:440/2330 train_time:25330ms step_avg:57.57ms
step:441/2330 train_time:25386ms step_avg:57.56ms
step:442/2330 train_time:25445ms step_avg:57.57ms
step:443/2330 train_time:25502ms step_avg:57.57ms
step:444/2330 train_time:25562ms step_avg:57.57ms
step:445/2330 train_time:25618ms step_avg:57.57ms
step:446/2330 train_time:25677ms step_avg:57.57ms
step:447/2330 train_time:25733ms step_avg:57.57ms
step:448/2330 train_time:25793ms step_avg:57.57ms
step:449/2330 train_time:25849ms step_avg:57.57ms
step:450/2330 train_time:25908ms step_avg:57.57ms
step:451/2330 train_time:25964ms step_avg:57.57ms
step:452/2330 train_time:26023ms step_avg:57.57ms
step:453/2330 train_time:26079ms step_avg:57.57ms
step:454/2330 train_time:26139ms step_avg:57.57ms
step:455/2330 train_time:26195ms step_avg:57.57ms
step:456/2330 train_time:26255ms step_avg:57.58ms
step:457/2330 train_time:26311ms step_avg:57.57ms
step:458/2330 train_time:26370ms step_avg:57.58ms
step:459/2330 train_time:26425ms step_avg:57.57ms
step:460/2330 train_time:26485ms step_avg:57.58ms
step:461/2330 train_time:26541ms step_avg:57.57ms
step:462/2330 train_time:26601ms step_avg:57.58ms
step:463/2330 train_time:26656ms step_avg:57.57ms
step:464/2330 train_time:26716ms step_avg:57.58ms
step:465/2330 train_time:26772ms step_avg:57.57ms
step:466/2330 train_time:26831ms step_avg:57.58ms
step:467/2330 train_time:26887ms step_avg:57.57ms
step:468/2330 train_time:26946ms step_avg:57.58ms
step:469/2330 train_time:27003ms step_avg:57.58ms
step:470/2330 train_time:27062ms step_avg:57.58ms
step:471/2330 train_time:27119ms step_avg:57.58ms
step:472/2330 train_time:27178ms step_avg:57.58ms
step:473/2330 train_time:27234ms step_avg:57.58ms
step:474/2330 train_time:27293ms step_avg:57.58ms
step:475/2330 train_time:27349ms step_avg:57.58ms
step:476/2330 train_time:27408ms step_avg:57.58ms
step:477/2330 train_time:27465ms step_avg:57.58ms
step:478/2330 train_time:27524ms step_avg:57.58ms
step:479/2330 train_time:27580ms step_avg:57.58ms
step:480/2330 train_time:27639ms step_avg:57.58ms
step:481/2330 train_time:27695ms step_avg:57.58ms
step:482/2330 train_time:27753ms step_avg:57.58ms
step:483/2330 train_time:27810ms step_avg:57.58ms
step:484/2330 train_time:27869ms step_avg:57.58ms
step:485/2330 train_time:27925ms step_avg:57.58ms
step:486/2330 train_time:27985ms step_avg:57.58ms
step:487/2330 train_time:28040ms step_avg:57.58ms
step:488/2330 train_time:28101ms step_avg:57.58ms
step:489/2330 train_time:28157ms step_avg:57.58ms
step:490/2330 train_time:28217ms step_avg:57.59ms
step:491/2330 train_time:28274ms step_avg:57.58ms
step:492/2330 train_time:28332ms step_avg:57.59ms
step:493/2330 train_time:28388ms step_avg:57.58ms
step:494/2330 train_time:28447ms step_avg:57.59ms
step:495/2330 train_time:28504ms step_avg:57.58ms
step:496/2330 train_time:28563ms step_avg:57.59ms
step:497/2330 train_time:28620ms step_avg:57.59ms
step:498/2330 train_time:28679ms step_avg:57.59ms
step:499/2330 train_time:28734ms step_avg:57.58ms
step:500/2330 train_time:28794ms step_avg:57.59ms
step:500/2330 val_loss:4.3909 train_time:28873ms step_avg:57.75ms
step:501/2330 train_time:28891ms step_avg:57.67ms
step:502/2330 train_time:28911ms step_avg:57.59ms
step:503/2330 train_time:28967ms step_avg:57.59ms
step:504/2330 train_time:29034ms step_avg:57.61ms
step:505/2330 train_time:29090ms step_avg:57.60ms
step:506/2330 train_time:29151ms step_avg:57.61ms
step:507/2330 train_time:29206ms step_avg:57.61ms
step:508/2330 train_time:29265ms step_avg:57.61ms
step:509/2330 train_time:29321ms step_avg:57.60ms
step:510/2330 train_time:29379ms step_avg:57.61ms
step:511/2330 train_time:29434ms step_avg:57.60ms
step:512/2330 train_time:29493ms step_avg:57.60ms
step:513/2330 train_time:29549ms step_avg:57.60ms
step:514/2330 train_time:29607ms step_avg:57.60ms
step:515/2330 train_time:29662ms step_avg:57.60ms
step:516/2330 train_time:29721ms step_avg:57.60ms
step:517/2330 train_time:29777ms step_avg:57.60ms
step:518/2330 train_time:29836ms step_avg:57.60ms
step:519/2330 train_time:29894ms step_avg:57.60ms
step:520/2330 train_time:29953ms step_avg:57.60ms
step:521/2330 train_time:30011ms step_avg:57.60ms
step:522/2330 train_time:30071ms step_avg:57.61ms
step:523/2330 train_time:30128ms step_avg:57.61ms
step:524/2330 train_time:30187ms step_avg:57.61ms
step:525/2330 train_time:30243ms step_avg:57.61ms
step:526/2330 train_time:30302ms step_avg:57.61ms
step:527/2330 train_time:30358ms step_avg:57.61ms
step:528/2330 train_time:30417ms step_avg:57.61ms
step:529/2330 train_time:30472ms step_avg:57.60ms
step:530/2330 train_time:30531ms step_avg:57.61ms
step:531/2330 train_time:30587ms step_avg:57.60ms
step:532/2330 train_time:30645ms step_avg:57.60ms
step:533/2330 train_time:30701ms step_avg:57.60ms
step:534/2330 train_time:30759ms step_avg:57.60ms
step:535/2330 train_time:30815ms step_avg:57.60ms
step:536/2330 train_time:30876ms step_avg:57.60ms
step:537/2330 train_time:30932ms step_avg:57.60ms
step:538/2330 train_time:30992ms step_avg:57.61ms
step:539/2330 train_time:31048ms step_avg:57.60ms
step:540/2330 train_time:31108ms step_avg:57.61ms
step:541/2330 train_time:31165ms step_avg:57.61ms
step:542/2330 train_time:31224ms step_avg:57.61ms
step:543/2330 train_time:31280ms step_avg:57.61ms
step:544/2330 train_time:31341ms step_avg:57.61ms
step:545/2330 train_time:31397ms step_avg:57.61ms
step:546/2330 train_time:31456ms step_avg:57.61ms
step:547/2330 train_time:31511ms step_avg:57.61ms
step:548/2330 train_time:31570ms step_avg:57.61ms
step:549/2330 train_time:31626ms step_avg:57.61ms
step:550/2330 train_time:31685ms step_avg:57.61ms
step:551/2330 train_time:31742ms step_avg:57.61ms
step:552/2330 train_time:31801ms step_avg:57.61ms
step:553/2330 train_time:31858ms step_avg:57.61ms
step:554/2330 train_time:31917ms step_avg:57.61ms
step:555/2330 train_time:31973ms step_avg:57.61ms
step:556/2330 train_time:32033ms step_avg:57.61ms
step:557/2330 train_time:32089ms step_avg:57.61ms
step:558/2330 train_time:32149ms step_avg:57.61ms
step:559/2330 train_time:32205ms step_avg:57.61ms
step:560/2330 train_time:32264ms step_avg:57.61ms
step:561/2330 train_time:32320ms step_avg:57.61ms
step:562/2330 train_time:32380ms step_avg:57.62ms
step:563/2330 train_time:32437ms step_avg:57.61ms
step:564/2330 train_time:32496ms step_avg:57.62ms
step:565/2330 train_time:32551ms step_avg:57.61ms
step:566/2330 train_time:32610ms step_avg:57.61ms
step:567/2330 train_time:32665ms step_avg:57.61ms
step:568/2330 train_time:32725ms step_avg:57.61ms
step:569/2330 train_time:32782ms step_avg:57.61ms
step:570/2330 train_time:32841ms step_avg:57.62ms
step:571/2330 train_time:32898ms step_avg:57.61ms
step:572/2330 train_time:32958ms step_avg:57.62ms
step:573/2330 train_time:33014ms step_avg:57.62ms
step:574/2330 train_time:33076ms step_avg:57.62ms
step:575/2330 train_time:33131ms step_avg:57.62ms
step:576/2330 train_time:33192ms step_avg:57.62ms
step:577/2330 train_time:33248ms step_avg:57.62ms
step:578/2330 train_time:33307ms step_avg:57.62ms
step:579/2330 train_time:33363ms step_avg:57.62ms
step:580/2330 train_time:33423ms step_avg:57.63ms
step:581/2330 train_time:33479ms step_avg:57.62ms
step:582/2330 train_time:33537ms step_avg:57.62ms
step:583/2330 train_time:33593ms step_avg:57.62ms
step:584/2330 train_time:33652ms step_avg:57.62ms
step:585/2330 train_time:33707ms step_avg:57.62ms
step:586/2330 train_time:33767ms step_avg:57.62ms
step:587/2330 train_time:33823ms step_avg:57.62ms
step:588/2330 train_time:33882ms step_avg:57.62ms
step:589/2330 train_time:33939ms step_avg:57.62ms
step:590/2330 train_time:33998ms step_avg:57.62ms
step:591/2330 train_time:34054ms step_avg:57.62ms
step:592/2330 train_time:34115ms step_avg:57.63ms
step:593/2330 train_time:34171ms step_avg:57.62ms
step:594/2330 train_time:34230ms step_avg:57.63ms
step:595/2330 train_time:34285ms step_avg:57.62ms
step:596/2330 train_time:34345ms step_avg:57.63ms
step:597/2330 train_time:34401ms step_avg:57.62ms
step:598/2330 train_time:34461ms step_avg:57.63ms
step:599/2330 train_time:34517ms step_avg:57.62ms
step:600/2330 train_time:34576ms step_avg:57.63ms
step:601/2330 train_time:34632ms step_avg:57.62ms
step:602/2330 train_time:34691ms step_avg:57.63ms
step:603/2330 train_time:34747ms step_avg:57.62ms
step:604/2330 train_time:34807ms step_avg:57.63ms
step:605/2330 train_time:34863ms step_avg:57.63ms
step:606/2330 train_time:34922ms step_avg:57.63ms
step:607/2330 train_time:34979ms step_avg:57.63ms
step:608/2330 train_time:35040ms step_avg:57.63ms
step:609/2330 train_time:35096ms step_avg:57.63ms
step:610/2330 train_time:35156ms step_avg:57.63ms
step:611/2330 train_time:35212ms step_avg:57.63ms
step:612/2330 train_time:35270ms step_avg:57.63ms
step:613/2330 train_time:35326ms step_avg:57.63ms
step:614/2330 train_time:35386ms step_avg:57.63ms
step:615/2330 train_time:35443ms step_avg:57.63ms
step:616/2330 train_time:35502ms step_avg:57.63ms
step:617/2330 train_time:35558ms step_avg:57.63ms
step:618/2330 train_time:35618ms step_avg:57.63ms
step:619/2330 train_time:35673ms step_avg:57.63ms
step:620/2330 train_time:35732ms step_avg:57.63ms
step:621/2330 train_time:35789ms step_avg:57.63ms
step:622/2330 train_time:35847ms step_avg:57.63ms
step:623/2330 train_time:35903ms step_avg:57.63ms
step:624/2330 train_time:35964ms step_avg:57.63ms
step:625/2330 train_time:36021ms step_avg:57.63ms
step:626/2330 train_time:36081ms step_avg:57.64ms
step:627/2330 train_time:36137ms step_avg:57.63ms
step:628/2330 train_time:36197ms step_avg:57.64ms
step:629/2330 train_time:36253ms step_avg:57.64ms
step:630/2330 train_time:36313ms step_avg:57.64ms
step:631/2330 train_time:36369ms step_avg:57.64ms
step:632/2330 train_time:36428ms step_avg:57.64ms
step:633/2330 train_time:36484ms step_avg:57.64ms
step:634/2330 train_time:36544ms step_avg:57.64ms
step:635/2330 train_time:36600ms step_avg:57.64ms
step:636/2330 train_time:36659ms step_avg:57.64ms
step:637/2330 train_time:36715ms step_avg:57.64ms
step:638/2330 train_time:36774ms step_avg:57.64ms
step:639/2330 train_time:36830ms step_avg:57.64ms
step:640/2330 train_time:36889ms step_avg:57.64ms
step:641/2330 train_time:36945ms step_avg:57.64ms
step:642/2330 train_time:37005ms step_avg:57.64ms
step:643/2330 train_time:37062ms step_avg:57.64ms
step:644/2330 train_time:37121ms step_avg:57.64ms
step:645/2330 train_time:37177ms step_avg:57.64ms
step:646/2330 train_time:37238ms step_avg:57.64ms
step:647/2330 train_time:37294ms step_avg:57.64ms
step:648/2330 train_time:37354ms step_avg:57.64ms
step:649/2330 train_time:37410ms step_avg:57.64ms
step:650/2330 train_time:37469ms step_avg:57.64ms
step:651/2330 train_time:37525ms step_avg:57.64ms
step:652/2330 train_time:37584ms step_avg:57.64ms
step:653/2330 train_time:37640ms step_avg:57.64ms
step:654/2330 train_time:37700ms step_avg:57.65ms
step:655/2330 train_time:37757ms step_avg:57.64ms
step:656/2330 train_time:37816ms step_avg:57.65ms
step:657/2330 train_time:37871ms step_avg:57.64ms
step:658/2330 train_time:37932ms step_avg:57.65ms
step:659/2330 train_time:37987ms step_avg:57.64ms
step:660/2330 train_time:38047ms step_avg:57.65ms
step:661/2330 train_time:38103ms step_avg:57.65ms
step:662/2330 train_time:38162ms step_avg:57.65ms
step:663/2330 train_time:38219ms step_avg:57.65ms
step:664/2330 train_time:38278ms step_avg:57.65ms
step:665/2330 train_time:38334ms step_avg:57.64ms
step:666/2330 train_time:38394ms step_avg:57.65ms
step:667/2330 train_time:38450ms step_avg:57.65ms
step:668/2330 train_time:38509ms step_avg:57.65ms
step:669/2330 train_time:38565ms step_avg:57.65ms
step:670/2330 train_time:38625ms step_avg:57.65ms
step:671/2330 train_time:38681ms step_avg:57.65ms
step:672/2330 train_time:38740ms step_avg:57.65ms
step:673/2330 train_time:38796ms step_avg:57.65ms
step:674/2330 train_time:38857ms step_avg:57.65ms
step:675/2330 train_time:38912ms step_avg:57.65ms
step:676/2330 train_time:38972ms step_avg:57.65ms
step:677/2330 train_time:39027ms step_avg:57.65ms
step:678/2330 train_time:39088ms step_avg:57.65ms
step:679/2330 train_time:39144ms step_avg:57.65ms
step:680/2330 train_time:39203ms step_avg:57.65ms
step:681/2330 train_time:39259ms step_avg:57.65ms
step:682/2330 train_time:39319ms step_avg:57.65ms
step:683/2330 train_time:39375ms step_avg:57.65ms
step:684/2330 train_time:39435ms step_avg:57.65ms
step:685/2330 train_time:39491ms step_avg:57.65ms
step:686/2330 train_time:39550ms step_avg:57.65ms
step:687/2330 train_time:39606ms step_avg:57.65ms
step:688/2330 train_time:39665ms step_avg:57.65ms
step:689/2330 train_time:39722ms step_avg:57.65ms
step:690/2330 train_time:39781ms step_avg:57.65ms
step:691/2330 train_time:39837ms step_avg:57.65ms
step:692/2330 train_time:39897ms step_avg:57.65ms
step:693/2330 train_time:39952ms step_avg:57.65ms
step:694/2330 train_time:40012ms step_avg:57.65ms
step:695/2330 train_time:40068ms step_avg:57.65ms
step:696/2330 train_time:40128ms step_avg:57.65ms
step:697/2330 train_time:40184ms step_avg:57.65ms
step:698/2330 train_time:40243ms step_avg:57.66ms
step:699/2330 train_time:40300ms step_avg:57.65ms
step:700/2330 train_time:40360ms step_avg:57.66ms
step:701/2330 train_time:40416ms step_avg:57.66ms
step:702/2330 train_time:40475ms step_avg:57.66ms
step:703/2330 train_time:40530ms step_avg:57.65ms
step:704/2330 train_time:40590ms step_avg:57.66ms
step:705/2330 train_time:40646ms step_avg:57.65ms
step:706/2330 train_time:40706ms step_avg:57.66ms
step:707/2330 train_time:40761ms step_avg:57.65ms
step:708/2330 train_time:40822ms step_avg:57.66ms
step:709/2330 train_time:40878ms step_avg:57.66ms
step:710/2330 train_time:40938ms step_avg:57.66ms
step:711/2330 train_time:40994ms step_avg:57.66ms
step:712/2330 train_time:41054ms step_avg:57.66ms
step:713/2330 train_time:41109ms step_avg:57.66ms
step:714/2330 train_time:41170ms step_avg:57.66ms
step:715/2330 train_time:41225ms step_avg:57.66ms
step:716/2330 train_time:41286ms step_avg:57.66ms
step:717/2330 train_time:41342ms step_avg:57.66ms
step:718/2330 train_time:41401ms step_avg:57.66ms
step:719/2330 train_time:41458ms step_avg:57.66ms
step:720/2330 train_time:41517ms step_avg:57.66ms
step:721/2330 train_time:41572ms step_avg:57.66ms
step:722/2330 train_time:41631ms step_avg:57.66ms
step:723/2330 train_time:41687ms step_avg:57.66ms
step:724/2330 train_time:41747ms step_avg:57.66ms
step:725/2330 train_time:41804ms step_avg:57.66ms
step:726/2330 train_time:41863ms step_avg:57.66ms
step:727/2330 train_time:41920ms step_avg:57.66ms
step:728/2330 train_time:41979ms step_avg:57.66ms
step:729/2330 train_time:42036ms step_avg:57.66ms
step:730/2330 train_time:42096ms step_avg:57.67ms
step:731/2330 train_time:42152ms step_avg:57.66ms
step:732/2330 train_time:42211ms step_avg:57.67ms
step:733/2330 train_time:42267ms step_avg:57.66ms
step:734/2330 train_time:42327ms step_avg:57.67ms
step:735/2330 train_time:42384ms step_avg:57.66ms
step:736/2330 train_time:42443ms step_avg:57.67ms
step:737/2330 train_time:42499ms step_avg:57.67ms
step:738/2330 train_time:42559ms step_avg:57.67ms
step:739/2330 train_time:42615ms step_avg:57.67ms
step:740/2330 train_time:42674ms step_avg:57.67ms
step:741/2330 train_time:42730ms step_avg:57.67ms
step:742/2330 train_time:42789ms step_avg:57.67ms
step:743/2330 train_time:42845ms step_avg:57.66ms
step:744/2330 train_time:42904ms step_avg:57.67ms
step:745/2330 train_time:42960ms step_avg:57.66ms
step:746/2330 train_time:43020ms step_avg:57.67ms
step:747/2330 train_time:43076ms step_avg:57.67ms
step:748/2330 train_time:43136ms step_avg:57.67ms
step:749/2330 train_time:43193ms step_avg:57.67ms
step:750/2330 train_time:43252ms step_avg:57.67ms
step:750/2330 val_loss:4.2133 train_time:43333ms step_avg:57.78ms
step:751/2330 train_time:43351ms step_avg:57.72ms
step:752/2330 train_time:43373ms step_avg:57.68ms
step:753/2330 train_time:43431ms step_avg:57.68ms
step:754/2330 train_time:43494ms step_avg:57.68ms
step:755/2330 train_time:43551ms step_avg:57.68ms
step:756/2330 train_time:43611ms step_avg:57.69ms
step:757/2330 train_time:43667ms step_avg:57.68ms
step:758/2330 train_time:43725ms step_avg:57.69ms
step:759/2330 train_time:43781ms step_avg:57.68ms
step:760/2330 train_time:43840ms step_avg:57.68ms
step:761/2330 train_time:43895ms step_avg:57.68ms
step:762/2330 train_time:43954ms step_avg:57.68ms
step:763/2330 train_time:44009ms step_avg:57.68ms
step:764/2330 train_time:44067ms step_avg:57.68ms
step:765/2330 train_time:44124ms step_avg:57.68ms
step:766/2330 train_time:44183ms step_avg:57.68ms
step:767/2330 train_time:44239ms step_avg:57.68ms
step:768/2330 train_time:44298ms step_avg:57.68ms
step:769/2330 train_time:44356ms step_avg:57.68ms
step:770/2330 train_time:44418ms step_avg:57.69ms
step:771/2330 train_time:44477ms step_avg:57.69ms
step:772/2330 train_time:44539ms step_avg:57.69ms
step:773/2330 train_time:44597ms step_avg:57.69ms
step:774/2330 train_time:44656ms step_avg:57.70ms
step:775/2330 train_time:44713ms step_avg:57.69ms
step:776/2330 train_time:44773ms step_avg:57.70ms
step:777/2330 train_time:44829ms step_avg:57.70ms
step:778/2330 train_time:44889ms step_avg:57.70ms
step:779/2330 train_time:44945ms step_avg:57.70ms
step:780/2330 train_time:45004ms step_avg:57.70ms
step:781/2330 train_time:45060ms step_avg:57.70ms
step:782/2330 train_time:45120ms step_avg:57.70ms
step:783/2330 train_time:45177ms step_avg:57.70ms
step:784/2330 train_time:45236ms step_avg:57.70ms
step:785/2330 train_time:45293ms step_avg:57.70ms
step:786/2330 train_time:45354ms step_avg:57.70ms
step:787/2330 train_time:45411ms step_avg:57.70ms
step:788/2330 train_time:45473ms step_avg:57.71ms
step:789/2330 train_time:45531ms step_avg:57.71ms
step:790/2330 train_time:45592ms step_avg:57.71ms
step:791/2330 train_time:45649ms step_avg:57.71ms
step:792/2330 train_time:45708ms step_avg:57.71ms
step:793/2330 train_time:45765ms step_avg:57.71ms
step:794/2330 train_time:45825ms step_avg:57.71ms
step:795/2330 train_time:45882ms step_avg:57.71ms
step:796/2330 train_time:45941ms step_avg:57.72ms
step:797/2330 train_time:45998ms step_avg:57.71ms
step:798/2330 train_time:46058ms step_avg:57.72ms
step:799/2330 train_time:46114ms step_avg:57.71ms
step:800/2330 train_time:46173ms step_avg:57.72ms
step:801/2330 train_time:46230ms step_avg:57.72ms
step:802/2330 train_time:46290ms step_avg:57.72ms
step:803/2330 train_time:46347ms step_avg:57.72ms
step:804/2330 train_time:46407ms step_avg:57.72ms
step:805/2330 train_time:46464ms step_avg:57.72ms
step:806/2330 train_time:46526ms step_avg:57.72ms
step:807/2330 train_time:46584ms step_avg:57.73ms
step:808/2330 train_time:46644ms step_avg:57.73ms
step:809/2330 train_time:46702ms step_avg:57.73ms
step:810/2330 train_time:46763ms step_avg:57.73ms
step:811/2330 train_time:46820ms step_avg:57.73ms
step:812/2330 train_time:46879ms step_avg:57.73ms
step:813/2330 train_time:46936ms step_avg:57.73ms
step:814/2330 train_time:46996ms step_avg:57.73ms
step:815/2330 train_time:47053ms step_avg:57.73ms
step:816/2330 train_time:47112ms step_avg:57.74ms
step:817/2330 train_time:47169ms step_avg:57.73ms
step:818/2330 train_time:47228ms step_avg:57.74ms
step:819/2330 train_time:47284ms step_avg:57.73ms
step:820/2330 train_time:47345ms step_avg:57.74ms
step:821/2330 train_time:47403ms step_avg:57.74ms
step:822/2330 train_time:47464ms step_avg:57.74ms
step:823/2330 train_time:47521ms step_avg:57.74ms
step:824/2330 train_time:47582ms step_avg:57.74ms
step:825/2330 train_time:47639ms step_avg:57.74ms
step:826/2330 train_time:47700ms step_avg:57.75ms
step:827/2330 train_time:47756ms step_avg:57.75ms
step:828/2330 train_time:47817ms step_avg:57.75ms
step:829/2330 train_time:47874ms step_avg:57.75ms
step:830/2330 train_time:47934ms step_avg:57.75ms
step:831/2330 train_time:47991ms step_avg:57.75ms
step:832/2330 train_time:48050ms step_avg:57.75ms
step:833/2330 train_time:48106ms step_avg:57.75ms
step:834/2330 train_time:48167ms step_avg:57.75ms
step:835/2330 train_time:48224ms step_avg:57.75ms
step:836/2330 train_time:48284ms step_avg:57.76ms
step:837/2330 train_time:48342ms step_avg:57.76ms
step:838/2330 train_time:48401ms step_avg:57.76ms
step:839/2330 train_time:48459ms step_avg:57.76ms
step:840/2330 train_time:48520ms step_avg:57.76ms
step:841/2330 train_time:48577ms step_avg:57.76ms
step:842/2330 train_time:48637ms step_avg:57.76ms
step:843/2330 train_time:48695ms step_avg:57.76ms
step:844/2330 train_time:48755ms step_avg:57.77ms
step:845/2330 train_time:48812ms step_avg:57.77ms
step:846/2330 train_time:48872ms step_avg:57.77ms
step:847/2330 train_time:48929ms step_avg:57.77ms
step:848/2330 train_time:48989ms step_avg:57.77ms
step:849/2330 train_time:49046ms step_avg:57.77ms
step:850/2330 train_time:49106ms step_avg:57.77ms
step:851/2330 train_time:49162ms step_avg:57.77ms
step:852/2330 train_time:49223ms step_avg:57.77ms
step:853/2330 train_time:49280ms step_avg:57.77ms
step:854/2330 train_time:49340ms step_avg:57.78ms
step:855/2330 train_time:49398ms step_avg:57.78ms
step:856/2330 train_time:49457ms step_avg:57.78ms
step:857/2330 train_time:49515ms step_avg:57.78ms
step:858/2330 train_time:49574ms step_avg:57.78ms
step:859/2330 train_time:49632ms step_avg:57.78ms
step:860/2330 train_time:49693ms step_avg:57.78ms
step:861/2330 train_time:49750ms step_avg:57.78ms
step:862/2330 train_time:49810ms step_avg:57.78ms
step:863/2330 train_time:49867ms step_avg:57.78ms
step:864/2330 train_time:49928ms step_avg:57.79ms
step:865/2330 train_time:49985ms step_avg:57.79ms
step:866/2330 train_time:50046ms step_avg:57.79ms
step:867/2330 train_time:50103ms step_avg:57.79ms
step:868/2330 train_time:50162ms step_avg:57.79ms
step:869/2330 train_time:50219ms step_avg:57.79ms
step:870/2330 train_time:50279ms step_avg:57.79ms
step:871/2330 train_time:50337ms step_avg:57.79ms
step:872/2330 train_time:50396ms step_avg:57.79ms
step:873/2330 train_time:50453ms step_avg:57.79ms
step:874/2330 train_time:50513ms step_avg:57.80ms
step:875/2330 train_time:50571ms step_avg:57.80ms
step:876/2330 train_time:50631ms step_avg:57.80ms
step:877/2330 train_time:50688ms step_avg:57.80ms
step:878/2330 train_time:50748ms step_avg:57.80ms
step:879/2330 train_time:50805ms step_avg:57.80ms
step:880/2330 train_time:50865ms step_avg:57.80ms
step:881/2330 train_time:50922ms step_avg:57.80ms
step:882/2330 train_time:50982ms step_avg:57.80ms
step:883/2330 train_time:51040ms step_avg:57.80ms
step:884/2330 train_time:51100ms step_avg:57.80ms
step:885/2330 train_time:51157ms step_avg:57.80ms
step:886/2330 train_time:51216ms step_avg:57.81ms
step:887/2330 train_time:51273ms step_avg:57.81ms
step:888/2330 train_time:51334ms step_avg:57.81ms
step:889/2330 train_time:51390ms step_avg:57.81ms
step:890/2330 train_time:51450ms step_avg:57.81ms
step:891/2330 train_time:51507ms step_avg:57.81ms
step:892/2330 train_time:51568ms step_avg:57.81ms
step:893/2330 train_time:51625ms step_avg:57.81ms
step:894/2330 train_time:51685ms step_avg:57.81ms
step:895/2330 train_time:51743ms step_avg:57.81ms
step:896/2330 train_time:51802ms step_avg:57.81ms
step:897/2330 train_time:51859ms step_avg:57.81ms
step:898/2330 train_time:51919ms step_avg:57.82ms
step:899/2330 train_time:51976ms step_avg:57.81ms
step:900/2330 train_time:52038ms step_avg:57.82ms
step:901/2330 train_time:52095ms step_avg:57.82ms
step:902/2330 train_time:52154ms step_avg:57.82ms
step:903/2330 train_time:52211ms step_avg:57.82ms
step:904/2330 train_time:52271ms step_avg:57.82ms
step:905/2330 train_time:52327ms step_avg:57.82ms
step:906/2330 train_time:52388ms step_avg:57.82ms
step:907/2330 train_time:52444ms step_avg:57.82ms
step:908/2330 train_time:52505ms step_avg:57.82ms
step:909/2330 train_time:52562ms step_avg:57.82ms
step:910/2330 train_time:52622ms step_avg:57.83ms
step:911/2330 train_time:52680ms step_avg:57.83ms
step:912/2330 train_time:52740ms step_avg:57.83ms
step:913/2330 train_time:52798ms step_avg:57.83ms
step:914/2330 train_time:52858ms step_avg:57.83ms
step:915/2330 train_time:52915ms step_avg:57.83ms
step:916/2330 train_time:52975ms step_avg:57.83ms
step:917/2330 train_time:53031ms step_avg:57.83ms
step:918/2330 train_time:53092ms step_avg:57.83ms
step:919/2330 train_time:53149ms step_avg:57.83ms
step:920/2330 train_time:53209ms step_avg:57.84ms
step:921/2330 train_time:53266ms step_avg:57.83ms
step:922/2330 train_time:53325ms step_avg:57.84ms
step:923/2330 train_time:53382ms step_avg:57.84ms
step:924/2330 train_time:53443ms step_avg:57.84ms
step:925/2330 train_time:53499ms step_avg:57.84ms
step:926/2330 train_time:53560ms step_avg:57.84ms
step:927/2330 train_time:53617ms step_avg:57.84ms
step:928/2330 train_time:53677ms step_avg:57.84ms
step:929/2330 train_time:53734ms step_avg:57.84ms
step:930/2330 train_time:53794ms step_avg:57.84ms
step:931/2330 train_time:53850ms step_avg:57.84ms
step:932/2330 train_time:53911ms step_avg:57.84ms
step:933/2330 train_time:53968ms step_avg:57.84ms
step:934/2330 train_time:54028ms step_avg:57.85ms
step:935/2330 train_time:54085ms step_avg:57.85ms
step:936/2330 train_time:54145ms step_avg:57.85ms
step:937/2330 train_time:54202ms step_avg:57.85ms
step:938/2330 train_time:54262ms step_avg:57.85ms
step:939/2330 train_time:54319ms step_avg:57.85ms
step:940/2330 train_time:54379ms step_avg:57.85ms
step:941/2330 train_time:54436ms step_avg:57.85ms
step:942/2330 train_time:54496ms step_avg:57.85ms
step:943/2330 train_time:54552ms step_avg:57.85ms
step:944/2330 train_time:54613ms step_avg:57.85ms
step:945/2330 train_time:54670ms step_avg:57.85ms
step:946/2330 train_time:54730ms step_avg:57.85ms
step:947/2330 train_time:54787ms step_avg:57.85ms
step:948/2330 train_time:54847ms step_avg:57.86ms
step:949/2330 train_time:54904ms step_avg:57.85ms
step:950/2330 train_time:54965ms step_avg:57.86ms
step:951/2330 train_time:55022ms step_avg:57.86ms
step:952/2330 train_time:55082ms step_avg:57.86ms
step:953/2330 train_time:55140ms step_avg:57.86ms
step:954/2330 train_time:55200ms step_avg:57.86ms
step:955/2330 train_time:55257ms step_avg:57.86ms
step:956/2330 train_time:55317ms step_avg:57.86ms
step:957/2330 train_time:55374ms step_avg:57.86ms
step:958/2330 train_time:55434ms step_avg:57.86ms
step:959/2330 train_time:55490ms step_avg:57.86ms
step:960/2330 train_time:55551ms step_avg:57.87ms
step:961/2330 train_time:55608ms step_avg:57.87ms
step:962/2330 train_time:55669ms step_avg:57.87ms
step:963/2330 train_time:55726ms step_avg:57.87ms
step:964/2330 train_time:55786ms step_avg:57.87ms
step:965/2330 train_time:55843ms step_avg:57.87ms
step:966/2330 train_time:55904ms step_avg:57.87ms
step:967/2330 train_time:55961ms step_avg:57.87ms
step:968/2330 train_time:56021ms step_avg:57.87ms
step:969/2330 train_time:56079ms step_avg:57.87ms
step:970/2330 train_time:56139ms step_avg:57.87ms
step:971/2330 train_time:56196ms step_avg:57.87ms
step:972/2330 train_time:56255ms step_avg:57.88ms
step:973/2330 train_time:56312ms step_avg:57.88ms
step:974/2330 train_time:56372ms step_avg:57.88ms
step:975/2330 train_time:56429ms step_avg:57.88ms
step:976/2330 train_time:56489ms step_avg:57.88ms
step:977/2330 train_time:56545ms step_avg:57.88ms
step:978/2330 train_time:56606ms step_avg:57.88ms
step:979/2330 train_time:56663ms step_avg:57.88ms
step:980/2330 train_time:56724ms step_avg:57.88ms
step:981/2330 train_time:56781ms step_avg:57.88ms
step:982/2330 train_time:56843ms step_avg:57.88ms
step:983/2330 train_time:56900ms step_avg:57.88ms
step:984/2330 train_time:56961ms step_avg:57.89ms
step:985/2330 train_time:57017ms step_avg:57.89ms
step:986/2330 train_time:57078ms step_avg:57.89ms
step:987/2330 train_time:57135ms step_avg:57.89ms
step:988/2330 train_time:57195ms step_avg:57.89ms
step:989/2330 train_time:57252ms step_avg:57.89ms
step:990/2330 train_time:57312ms step_avg:57.89ms
step:991/2330 train_time:57369ms step_avg:57.89ms
step:992/2330 train_time:57428ms step_avg:57.89ms
step:993/2330 train_time:57485ms step_avg:57.89ms
step:994/2330 train_time:57545ms step_avg:57.89ms
step:995/2330 train_time:57602ms step_avg:57.89ms
step:996/2330 train_time:57662ms step_avg:57.89ms
step:997/2330 train_time:57719ms step_avg:57.89ms
step:998/2330 train_time:57781ms step_avg:57.90ms
step:999/2330 train_time:57839ms step_avg:57.90ms
step:1000/2330 train_time:57899ms step_avg:57.90ms
step:1000/2330 val_loss:4.0634 train_time:57979ms step_avg:57.98ms
step:1001/2330 train_time:57999ms step_avg:57.94ms
step:1002/2330 train_time:58019ms step_avg:57.90ms
step:1003/2330 train_time:58079ms step_avg:57.91ms
step:1004/2330 train_time:58143ms step_avg:57.91ms
step:1005/2330 train_time:58201ms step_avg:57.91ms
step:1006/2330 train_time:58261ms step_avg:57.91ms
step:1007/2330 train_time:58319ms step_avg:57.91ms
step:1008/2330 train_time:58377ms step_avg:57.91ms
step:1009/2330 train_time:58434ms step_avg:57.91ms
step:1010/2330 train_time:58493ms step_avg:57.91ms
step:1011/2330 train_time:58549ms step_avg:57.91ms
step:1012/2330 train_time:58608ms step_avg:57.91ms
step:1013/2330 train_time:58664ms step_avg:57.91ms
step:1014/2330 train_time:58724ms step_avg:57.91ms
step:1015/2330 train_time:58780ms step_avg:57.91ms
step:1016/2330 train_time:58840ms step_avg:57.91ms
step:1017/2330 train_time:58896ms step_avg:57.91ms
step:1018/2330 train_time:58960ms step_avg:57.92ms
step:1019/2330 train_time:59018ms step_avg:57.92ms
step:1020/2330 train_time:59080ms step_avg:57.92ms
step:1021/2330 train_time:59139ms step_avg:57.92ms
step:1022/2330 train_time:59199ms step_avg:57.92ms
step:1023/2330 train_time:59255ms step_avg:57.92ms
step:1024/2330 train_time:59315ms step_avg:57.92ms
step:1025/2330 train_time:59372ms step_avg:57.92ms
step:1026/2330 train_time:59431ms step_avg:57.93ms
step:1027/2330 train_time:59488ms step_avg:57.92ms
step:1028/2330 train_time:59546ms step_avg:57.92ms
step:1029/2330 train_time:59603ms step_avg:57.92ms
step:1030/2330 train_time:59662ms step_avg:57.92ms
step:1031/2330 train_time:59718ms step_avg:57.92ms
step:1032/2330 train_time:59778ms step_avg:57.92ms
step:1033/2330 train_time:59834ms step_avg:57.92ms
step:1034/2330 train_time:59895ms step_avg:57.93ms
step:1035/2330 train_time:59952ms step_avg:57.92ms
step:1036/2330 train_time:60015ms step_avg:57.93ms
step:1037/2330 train_time:60071ms step_avg:57.93ms
step:1038/2330 train_time:60134ms step_avg:57.93ms
step:1039/2330 train_time:60191ms step_avg:57.93ms
step:1040/2330 train_time:60251ms step_avg:57.93ms
step:1041/2330 train_time:60309ms step_avg:57.93ms
step:1042/2330 train_time:60368ms step_avg:57.93ms
step:1043/2330 train_time:60425ms step_avg:57.93ms
step:1044/2330 train_time:60485ms step_avg:57.94ms
step:1045/2330 train_time:60541ms step_avg:57.93ms
step:1046/2330 train_time:60601ms step_avg:57.94ms
step:1047/2330 train_time:60658ms step_avg:57.93ms
step:1048/2330 train_time:60717ms step_avg:57.94ms
step:1049/2330 train_time:60774ms step_avg:57.94ms
step:1050/2330 train_time:60834ms step_avg:57.94ms
step:1051/2330 train_time:60891ms step_avg:57.94ms
step:1052/2330 train_time:60951ms step_avg:57.94ms
step:1053/2330 train_time:61008ms step_avg:57.94ms
step:1054/2330 train_time:61069ms step_avg:57.94ms
step:1055/2330 train_time:61128ms step_avg:57.94ms
step:1056/2330 train_time:61188ms step_avg:57.94ms
step:1057/2330 train_time:61246ms step_avg:57.94ms
step:1058/2330 train_time:61307ms step_avg:57.95ms
step:1059/2330 train_time:61364ms step_avg:57.94ms
step:1060/2330 train_time:61423ms step_avg:57.95ms
step:1061/2330 train_time:61480ms step_avg:57.95ms
step:1062/2330 train_time:61540ms step_avg:57.95ms
step:1063/2330 train_time:61596ms step_avg:57.95ms
step:1064/2330 train_time:61655ms step_avg:57.95ms
step:1065/2330 train_time:61712ms step_avg:57.95ms
step:1066/2330 train_time:61772ms step_avg:57.95ms
step:1067/2330 train_time:61828ms step_avg:57.95ms
step:1068/2330 train_time:61888ms step_avg:57.95ms
step:1069/2330 train_time:61945ms step_avg:57.95ms
step:1070/2330 train_time:62006ms step_avg:57.95ms
step:1071/2330 train_time:62063ms step_avg:57.95ms
step:1072/2330 train_time:62125ms step_avg:57.95ms
step:1073/2330 train_time:62182ms step_avg:57.95ms
step:1074/2330 train_time:62242ms step_avg:57.95ms
step:1075/2330 train_time:62300ms step_avg:57.95ms
step:1076/2330 train_time:62360ms step_avg:57.96ms
step:1077/2330 train_time:62417ms step_avg:57.95ms
step:1078/2330 train_time:62476ms step_avg:57.96ms
step:1079/2330 train_time:62533ms step_avg:57.95ms
step:1080/2330 train_time:62592ms step_avg:57.96ms
step:1081/2330 train_time:62648ms step_avg:57.95ms
step:1082/2330 train_time:62709ms step_avg:57.96ms
step:1083/2330 train_time:62766ms step_avg:57.96ms
step:1084/2330 train_time:62826ms step_avg:57.96ms
step:1085/2330 train_time:62884ms step_avg:57.96ms
step:1086/2330 train_time:62945ms step_avg:57.96ms
step:1087/2330 train_time:63002ms step_avg:57.96ms
step:1088/2330 train_time:63063ms step_avg:57.96ms
step:1089/2330 train_time:63121ms step_avg:57.96ms
step:1090/2330 train_time:63182ms step_avg:57.97ms
step:1091/2330 train_time:63239ms step_avg:57.96ms
step:1092/2330 train_time:63299ms step_avg:57.97ms
step:1093/2330 train_time:63356ms step_avg:57.96ms
step:1094/2330 train_time:63416ms step_avg:57.97ms
step:1095/2330 train_time:63472ms step_avg:57.97ms
step:1096/2330 train_time:63532ms step_avg:57.97ms
step:1097/2330 train_time:63589ms step_avg:57.97ms
step:1098/2330 train_time:63648ms step_avg:57.97ms
step:1099/2330 train_time:63706ms step_avg:57.97ms
step:1100/2330 train_time:63766ms step_avg:57.97ms
step:1101/2330 train_time:63824ms step_avg:57.97ms
step:1102/2330 train_time:63884ms step_avg:57.97ms
step:1103/2330 train_time:63941ms step_avg:57.97ms
step:1104/2330 train_time:64001ms step_avg:57.97ms
step:1105/2330 train_time:64058ms step_avg:57.97ms
step:1106/2330 train_time:64119ms step_avg:57.97ms
step:1107/2330 train_time:64176ms step_avg:57.97ms
step:1108/2330 train_time:64237ms step_avg:57.98ms
step:1109/2330 train_time:64293ms step_avg:57.97ms
step:1110/2330 train_time:64354ms step_avg:57.98ms
step:1111/2330 train_time:64412ms step_avg:57.98ms
step:1112/2330 train_time:64471ms step_avg:57.98ms
step:1113/2330 train_time:64527ms step_avg:57.98ms
step:1114/2330 train_time:64587ms step_avg:57.98ms
step:1115/2330 train_time:64644ms step_avg:57.98ms
step:1116/2330 train_time:64703ms step_avg:57.98ms
step:1117/2330 train_time:64760ms step_avg:57.98ms
step:1118/2330 train_time:64820ms step_avg:57.98ms
step:1119/2330 train_time:64878ms step_avg:57.98ms
step:1120/2330 train_time:64938ms step_avg:57.98ms
step:1121/2330 train_time:64995ms step_avg:57.98ms
step:1122/2330 train_time:65054ms step_avg:57.98ms
step:1123/2330 train_time:65112ms step_avg:57.98ms
step:1124/2330 train_time:65172ms step_avg:57.98ms
step:1125/2330 train_time:65229ms step_avg:57.98ms
step:1126/2330 train_time:65289ms step_avg:57.98ms
step:1127/2330 train_time:65346ms step_avg:57.98ms
step:1128/2330 train_time:65407ms step_avg:57.98ms
step:1129/2330 train_time:65464ms step_avg:57.98ms
step:1130/2330 train_time:65524ms step_avg:57.99ms
step:1131/2330 train_time:65580ms step_avg:57.98ms
step:1132/2330 train_time:65641ms step_avg:57.99ms
step:1133/2330 train_time:65697ms step_avg:57.99ms
step:1134/2330 train_time:65758ms step_avg:57.99ms
step:1135/2330 train_time:65814ms step_avg:57.99ms
step:1136/2330 train_time:65875ms step_avg:57.99ms
step:1137/2330 train_time:65932ms step_avg:57.99ms
step:1138/2330 train_time:65992ms step_avg:57.99ms
step:1139/2330 train_time:66049ms step_avg:57.99ms
step:1140/2330 train_time:66110ms step_avg:57.99ms
step:1141/2330 train_time:66167ms step_avg:57.99ms
step:1142/2330 train_time:66227ms step_avg:57.99ms
step:1143/2330 train_time:66285ms step_avg:57.99ms
step:1144/2330 train_time:66344ms step_avg:57.99ms
step:1145/2330 train_time:66402ms step_avg:57.99ms
step:1146/2330 train_time:66462ms step_avg:57.99ms
step:1147/2330 train_time:66520ms step_avg:57.99ms
step:1148/2330 train_time:66579ms step_avg:58.00ms
step:1149/2330 train_time:66637ms step_avg:58.00ms
step:1150/2330 train_time:66696ms step_avg:58.00ms
step:1151/2330 train_time:66753ms step_avg:58.00ms
step:1152/2330 train_time:66813ms step_avg:58.00ms
step:1153/2330 train_time:66870ms step_avg:58.00ms
step:1154/2330 train_time:66930ms step_avg:58.00ms
step:1155/2330 train_time:66986ms step_avg:58.00ms
step:1156/2330 train_time:67047ms step_avg:58.00ms
step:1157/2330 train_time:67104ms step_avg:58.00ms
step:1158/2330 train_time:67164ms step_avg:58.00ms
step:1159/2330 train_time:67222ms step_avg:58.00ms
step:1160/2330 train_time:67283ms step_avg:58.00ms
step:1161/2330 train_time:67339ms step_avg:58.00ms
step:1162/2330 train_time:67400ms step_avg:58.00ms
step:1163/2330 train_time:67457ms step_avg:58.00ms
step:1164/2330 train_time:67517ms step_avg:58.00ms
step:1165/2330 train_time:67574ms step_avg:58.00ms
step:1166/2330 train_time:67633ms step_avg:58.00ms
step:1167/2330 train_time:67690ms step_avg:58.00ms
step:1168/2330 train_time:67750ms step_avg:58.01ms
step:1169/2330 train_time:67807ms step_avg:58.00ms
step:1170/2330 train_time:67868ms step_avg:58.01ms
step:1171/2330 train_time:67925ms step_avg:58.01ms
step:1172/2330 train_time:67985ms step_avg:58.01ms
step:1173/2330 train_time:68042ms step_avg:58.01ms
step:1174/2330 train_time:68102ms step_avg:58.01ms
step:1175/2330 train_time:68160ms step_avg:58.01ms
step:1176/2330 train_time:68220ms step_avg:58.01ms
step:1177/2330 train_time:68278ms step_avg:58.01ms
step:1178/2330 train_time:68338ms step_avg:58.01ms
step:1179/2330 train_time:68395ms step_avg:58.01ms
step:1180/2330 train_time:68455ms step_avg:58.01ms
step:1181/2330 train_time:68512ms step_avg:58.01ms
step:1182/2330 train_time:68571ms step_avg:58.01ms
step:1183/2330 train_time:68628ms step_avg:58.01ms
step:1184/2330 train_time:68688ms step_avg:58.01ms
step:1185/2330 train_time:68744ms step_avg:58.01ms
step:1186/2330 train_time:68804ms step_avg:58.01ms
step:1187/2330 train_time:68862ms step_avg:58.01ms
step:1188/2330 train_time:68922ms step_avg:58.02ms
step:1189/2330 train_time:68979ms step_avg:58.01ms
step:1190/2330 train_time:69039ms step_avg:58.02ms
step:1191/2330 train_time:69095ms step_avg:58.01ms
step:1192/2330 train_time:69156ms step_avg:58.02ms
step:1193/2330 train_time:69213ms step_avg:58.02ms
step:1194/2330 train_time:69273ms step_avg:58.02ms
step:1195/2330 train_time:69331ms step_avg:58.02ms
step:1196/2330 train_time:69390ms step_avg:58.02ms
step:1197/2330 train_time:69447ms step_avg:58.02ms
step:1198/2330 train_time:69508ms step_avg:58.02ms
step:1199/2330 train_time:69566ms step_avg:58.02ms
step:1200/2330 train_time:69625ms step_avg:58.02ms
step:1201/2330 train_time:69682ms step_avg:58.02ms
step:1202/2330 train_time:69742ms step_avg:58.02ms
step:1203/2330 train_time:69799ms step_avg:58.02ms
step:1204/2330 train_time:69859ms step_avg:58.02ms
step:1205/2330 train_time:69916ms step_avg:58.02ms
step:1206/2330 train_time:69976ms step_avg:58.02ms
step:1207/2330 train_time:70034ms step_avg:58.02ms
step:1208/2330 train_time:70093ms step_avg:58.02ms
step:1209/2330 train_time:70150ms step_avg:58.02ms
step:1210/2330 train_time:70210ms step_avg:58.03ms
step:1211/2330 train_time:70268ms step_avg:58.02ms
step:1212/2330 train_time:70329ms step_avg:58.03ms
step:1213/2330 train_time:70386ms step_avg:58.03ms
step:1214/2330 train_time:70446ms step_avg:58.03ms
step:1215/2330 train_time:70504ms step_avg:58.03ms
step:1216/2330 train_time:70564ms step_avg:58.03ms
step:1217/2330 train_time:70621ms step_avg:58.03ms
step:1218/2330 train_time:70681ms step_avg:58.03ms
step:1219/2330 train_time:70738ms step_avg:58.03ms
step:1220/2330 train_time:70798ms step_avg:58.03ms
step:1221/2330 train_time:70855ms step_avg:58.03ms
step:1222/2330 train_time:70915ms step_avg:58.03ms
step:1223/2330 train_time:70971ms step_avg:58.03ms
step:1224/2330 train_time:71032ms step_avg:58.03ms
step:1225/2330 train_time:71089ms step_avg:58.03ms
step:1226/2330 train_time:71149ms step_avg:58.03ms
step:1227/2330 train_time:71206ms step_avg:58.03ms
step:1228/2330 train_time:71267ms step_avg:58.03ms
step:1229/2330 train_time:71324ms step_avg:58.03ms
step:1230/2330 train_time:71384ms step_avg:58.04ms
step:1231/2330 train_time:71440ms step_avg:58.03ms
step:1232/2330 train_time:71500ms step_avg:58.04ms
step:1233/2330 train_time:71557ms step_avg:58.03ms
step:1234/2330 train_time:71618ms step_avg:58.04ms
step:1235/2330 train_time:71675ms step_avg:58.04ms
step:1236/2330 train_time:71735ms step_avg:58.04ms
step:1237/2330 train_time:71792ms step_avg:58.04ms
step:1238/2330 train_time:71852ms step_avg:58.04ms
step:1239/2330 train_time:71909ms step_avg:58.04ms
step:1240/2330 train_time:71969ms step_avg:58.04ms
step:1241/2330 train_time:72026ms step_avg:58.04ms
step:1242/2330 train_time:72086ms step_avg:58.04ms
step:1243/2330 train_time:72143ms step_avg:58.04ms
step:1244/2330 train_time:72203ms step_avg:58.04ms
step:1245/2330 train_time:72260ms step_avg:58.04ms
step:1246/2330 train_time:72321ms step_avg:58.04ms
step:1247/2330 train_time:72377ms step_avg:58.04ms
step:1248/2330 train_time:72438ms step_avg:58.04ms
step:1249/2330 train_time:72496ms step_avg:58.04ms
step:1250/2330 train_time:72556ms step_avg:58.04ms
step:1250/2330 val_loss:3.9850 train_time:72637ms step_avg:58.11ms
step:1251/2330 train_time:72656ms step_avg:58.08ms
step:1252/2330 train_time:72676ms step_avg:58.05ms
step:1253/2330 train_time:72734ms step_avg:58.05ms
step:1254/2330 train_time:72799ms step_avg:58.05ms
step:1255/2330 train_time:72855ms step_avg:58.05ms
step:1256/2330 train_time:72916ms step_avg:58.05ms
step:1257/2330 train_time:72972ms step_avg:58.05ms
step:1258/2330 train_time:73032ms step_avg:58.05ms
step:1259/2330 train_time:73088ms step_avg:58.05ms
step:1260/2330 train_time:73151ms step_avg:58.06ms
step:1261/2330 train_time:73208ms step_avg:58.06ms
step:1262/2330 train_time:73267ms step_avg:58.06ms
step:1263/2330 train_time:73324ms step_avg:58.06ms
step:1264/2330 train_time:73384ms step_avg:58.06ms
step:1265/2330 train_time:73440ms step_avg:58.06ms
step:1266/2330 train_time:73499ms step_avg:58.06ms
step:1267/2330 train_time:73555ms step_avg:58.05ms
step:1268/2330 train_time:73616ms step_avg:58.06ms
step:1269/2330 train_time:73674ms step_avg:58.06ms
step:1270/2330 train_time:73737ms step_avg:58.06ms
step:1271/2330 train_time:73794ms step_avg:58.06ms
step:1272/2330 train_time:73854ms step_avg:58.06ms
step:1273/2330 train_time:73910ms step_avg:58.06ms
step:1274/2330 train_time:73971ms step_avg:58.06ms
step:1275/2330 train_time:74027ms step_avg:58.06ms
step:1276/2330 train_time:74090ms step_avg:58.06ms
step:1277/2330 train_time:74146ms step_avg:58.06ms
step:1278/2330 train_time:74207ms step_avg:58.06ms
step:1279/2330 train_time:74263ms step_avg:58.06ms
step:1280/2330 train_time:74323ms step_avg:58.06ms
step:1281/2330 train_time:74379ms step_avg:58.06ms
step:1282/2330 train_time:74439ms step_avg:58.06ms
step:1283/2330 train_time:74496ms step_avg:58.06ms
step:1284/2330 train_time:74556ms step_avg:58.07ms
step:1285/2330 train_time:74613ms step_avg:58.06ms
step:1286/2330 train_time:74674ms step_avg:58.07ms
step:1287/2330 train_time:74732ms step_avg:58.07ms
step:1288/2330 train_time:74793ms step_avg:58.07ms
step:1289/2330 train_time:74850ms step_avg:58.07ms
step:1290/2330 train_time:74911ms step_avg:58.07ms
step:1291/2330 train_time:74968ms step_avg:58.07ms
step:1292/2330 train_time:75028ms step_avg:58.07ms
step:1293/2330 train_time:75085ms step_avg:58.07ms
step:1294/2330 train_time:75146ms step_avg:58.07ms
step:1295/2330 train_time:75202ms step_avg:58.07ms
step:1296/2330 train_time:75264ms step_avg:58.07ms
step:1297/2330 train_time:75320ms step_avg:58.07ms
step:1298/2330 train_time:75381ms step_avg:58.07ms
step:1299/2330 train_time:75437ms step_avg:58.07ms
step:1300/2330 train_time:75497ms step_avg:58.07ms
step:1301/2330 train_time:75554ms step_avg:58.07ms
step:1302/2330 train_time:75613ms step_avg:58.07ms
step:1303/2330 train_time:75670ms step_avg:58.07ms
step:1304/2330 train_time:75731ms step_avg:58.08ms
step:1305/2330 train_time:75788ms step_avg:58.08ms
step:1306/2330 train_time:75850ms step_avg:58.08ms
step:1307/2330 train_time:75907ms step_avg:58.08ms
step:1308/2330 train_time:75967ms step_avg:58.08ms
step:1309/2330 train_time:76023ms step_avg:58.08ms
step:1310/2330 train_time:76084ms step_avg:58.08ms
step:1311/2330 train_time:76141ms step_avg:58.08ms
step:1312/2330 train_time:76201ms step_avg:58.08ms
step:1313/2330 train_time:76258ms step_avg:58.08ms
step:1314/2330 train_time:76317ms step_avg:58.08ms
step:1315/2330 train_time:76373ms step_avg:58.08ms
step:1316/2330 train_time:76434ms step_avg:58.08ms
step:1317/2330 train_time:76491ms step_avg:58.08ms
step:1318/2330 train_time:76550ms step_avg:58.08ms
step:1319/2330 train_time:76607ms step_avg:58.08ms
step:1320/2330 train_time:76668ms step_avg:58.08ms
step:1321/2330 train_time:76725ms step_avg:58.08ms
step:1322/2330 train_time:76786ms step_avg:58.08ms
step:1323/2330 train_time:76843ms step_avg:58.08ms
step:1324/2330 train_time:76904ms step_avg:58.08ms
step:1325/2330 train_time:76961ms step_avg:58.08ms
step:1326/2330 train_time:77021ms step_avg:58.08ms
step:1327/2330 train_time:77077ms step_avg:58.08ms
step:1328/2330 train_time:77138ms step_avg:58.09ms
step:1329/2330 train_time:77196ms step_avg:58.09ms
step:1330/2330 train_time:77255ms step_avg:58.09ms
step:1331/2330 train_time:77312ms step_avg:58.09ms
step:1332/2330 train_time:77372ms step_avg:58.09ms
step:1333/2330 train_time:77428ms step_avg:58.09ms
step:1334/2330 train_time:77489ms step_avg:58.09ms
step:1335/2330 train_time:77546ms step_avg:58.09ms
step:1336/2330 train_time:77605ms step_avg:58.09ms
step:1337/2330 train_time:77661ms step_avg:58.09ms
step:1338/2330 train_time:77723ms step_avg:58.09ms
step:1339/2330 train_time:77779ms step_avg:58.09ms
step:1340/2330 train_time:77841ms step_avg:58.09ms
step:1341/2330 train_time:77897ms step_avg:58.09ms
step:1342/2330 train_time:77958ms step_avg:58.09ms
step:1343/2330 train_time:78014ms step_avg:58.09ms
step:1344/2330 train_time:78074ms step_avg:58.09ms
step:1345/2330 train_time:78131ms step_avg:58.09ms
step:1346/2330 train_time:78191ms step_avg:58.09ms
step:1347/2330 train_time:78248ms step_avg:58.09ms
step:1348/2330 train_time:78309ms step_avg:58.09ms
step:1349/2330 train_time:78365ms step_avg:58.09ms
step:1350/2330 train_time:78426ms step_avg:58.09ms
step:1351/2330 train_time:78482ms step_avg:58.09ms
step:1352/2330 train_time:78542ms step_avg:58.09ms
step:1353/2330 train_time:78600ms step_avg:58.09ms
step:1354/2330 train_time:78659ms step_avg:58.09ms
step:1355/2330 train_time:78717ms step_avg:58.09ms
step:1356/2330 train_time:78776ms step_avg:58.09ms
step:1357/2330 train_time:78834ms step_avg:58.09ms
step:1358/2330 train_time:78894ms step_avg:58.10ms
step:1359/2330 train_time:78950ms step_avg:58.09ms
step:1360/2330 train_time:79011ms step_avg:58.10ms
step:1361/2330 train_time:79068ms step_avg:58.10ms
step:1362/2330 train_time:79128ms step_avg:58.10ms
step:1363/2330 train_time:79185ms step_avg:58.10ms
step:1364/2330 train_time:79245ms step_avg:58.10ms
step:1365/2330 train_time:79302ms step_avg:58.10ms
step:1366/2330 train_time:79362ms step_avg:58.10ms
step:1367/2330 train_time:79419ms step_avg:58.10ms
step:1368/2330 train_time:79479ms step_avg:58.10ms
step:1369/2330 train_time:79536ms step_avg:58.10ms
step:1370/2330 train_time:79596ms step_avg:58.10ms
step:1371/2330 train_time:79653ms step_avg:58.10ms
step:1372/2330 train_time:79714ms step_avg:58.10ms
step:1373/2330 train_time:79770ms step_avg:58.10ms
step:1374/2330 train_time:79832ms step_avg:58.10ms
step:1375/2330 train_time:79888ms step_avg:58.10ms
step:1376/2330 train_time:79950ms step_avg:58.10ms
step:1377/2330 train_time:80006ms step_avg:58.10ms
step:1378/2330 train_time:80066ms step_avg:58.10ms
step:1379/2330 train_time:80123ms step_avg:58.10ms
step:1380/2330 train_time:80183ms step_avg:58.10ms
step:1381/2330 train_time:80239ms step_avg:58.10ms
step:1382/2330 train_time:80300ms step_avg:58.10ms
step:1383/2330 train_time:80356ms step_avg:58.10ms
step:1384/2330 train_time:80416ms step_avg:58.10ms
step:1385/2330 train_time:80473ms step_avg:58.10ms
step:1386/2330 train_time:80533ms step_avg:58.10ms
step:1387/2330 train_time:80590ms step_avg:58.10ms
step:1388/2330 train_time:80651ms step_avg:58.11ms
step:1389/2330 train_time:80709ms step_avg:58.11ms
step:1390/2330 train_time:80768ms step_avg:58.11ms
step:1391/2330 train_time:80826ms step_avg:58.11ms
step:1392/2330 train_time:80886ms step_avg:58.11ms
step:1393/2330 train_time:80942ms step_avg:58.11ms
step:1394/2330 train_time:81003ms step_avg:58.11ms
step:1395/2330 train_time:81060ms step_avg:58.11ms
step:1396/2330 train_time:81120ms step_avg:58.11ms
step:1397/2330 train_time:81176ms step_avg:58.11ms
step:1398/2330 train_time:81236ms step_avg:58.11ms
step:1399/2330 train_time:81293ms step_avg:58.11ms
step:1400/2330 train_time:81354ms step_avg:58.11ms
step:1401/2330 train_time:81411ms step_avg:58.11ms
step:1402/2330 train_time:81471ms step_avg:58.11ms
step:1403/2330 train_time:81527ms step_avg:58.11ms
step:1404/2330 train_time:81589ms step_avg:58.11ms
step:1405/2330 train_time:81646ms step_avg:58.11ms
step:1406/2330 train_time:81707ms step_avg:58.11ms
step:1407/2330 train_time:81764ms step_avg:58.11ms
step:1408/2330 train_time:81824ms step_avg:58.11ms
step:1409/2330 train_time:81880ms step_avg:58.11ms
step:1410/2330 train_time:81941ms step_avg:58.11ms
step:1411/2330 train_time:81998ms step_avg:58.11ms
step:1412/2330 train_time:82058ms step_avg:58.11ms
step:1413/2330 train_time:82115ms step_avg:58.11ms
step:1414/2330 train_time:82175ms step_avg:58.12ms
step:1415/2330 train_time:82231ms step_avg:58.11ms
step:1416/2330 train_time:82293ms step_avg:58.12ms
step:1417/2330 train_time:82350ms step_avg:58.12ms
step:1418/2330 train_time:82410ms step_avg:58.12ms
step:1419/2330 train_time:82467ms step_avg:58.12ms
step:1420/2330 train_time:82527ms step_avg:58.12ms
step:1421/2330 train_time:82584ms step_avg:58.12ms
step:1422/2330 train_time:82644ms step_avg:58.12ms
step:1423/2330 train_time:82701ms step_avg:58.12ms
step:1424/2330 train_time:82761ms step_avg:58.12ms
step:1425/2330 train_time:82818ms step_avg:58.12ms
step:1426/2330 train_time:82878ms step_avg:58.12ms
step:1427/2330 train_time:82935ms step_avg:58.12ms
step:1428/2330 train_time:82995ms step_avg:58.12ms
step:1429/2330 train_time:83052ms step_avg:58.12ms
step:1430/2330 train_time:83112ms step_avg:58.12ms
step:1431/2330 train_time:83170ms step_avg:58.12ms
step:1432/2330 train_time:83229ms step_avg:58.12ms
step:1433/2330 train_time:83286ms step_avg:58.12ms
step:1434/2330 train_time:83346ms step_avg:58.12ms
step:1435/2330 train_time:83403ms step_avg:58.12ms
step:1436/2330 train_time:83464ms step_avg:58.12ms
step:1437/2330 train_time:83521ms step_avg:58.12ms
step:1438/2330 train_time:83581ms step_avg:58.12ms
step:1439/2330 train_time:83638ms step_avg:58.12ms
step:1440/2330 train_time:83698ms step_avg:58.12ms
step:1441/2330 train_time:83754ms step_avg:58.12ms
step:1442/2330 train_time:83815ms step_avg:58.12ms
step:1443/2330 train_time:83872ms step_avg:58.12ms
step:1444/2330 train_time:83932ms step_avg:58.12ms
step:1445/2330 train_time:83989ms step_avg:58.12ms
step:1446/2330 train_time:84049ms step_avg:58.13ms
step:1447/2330 train_time:84106ms step_avg:58.12ms
step:1448/2330 train_time:84166ms step_avg:58.13ms
step:1449/2330 train_time:84223ms step_avg:58.13ms
step:1450/2330 train_time:84283ms step_avg:58.13ms
step:1451/2330 train_time:84339ms step_avg:58.12ms
step:1452/2330 train_time:84400ms step_avg:58.13ms
step:1453/2330 train_time:84457ms step_avg:58.13ms
step:1454/2330 train_time:84516ms step_avg:58.13ms
step:1455/2330 train_time:84573ms step_avg:58.13ms
step:1456/2330 train_time:84633ms step_avg:58.13ms
step:1457/2330 train_time:84690ms step_avg:58.13ms
step:1458/2330 train_time:84751ms step_avg:58.13ms
step:1459/2330 train_time:84808ms step_avg:58.13ms
step:1460/2330 train_time:84869ms step_avg:58.13ms
step:1461/2330 train_time:84926ms step_avg:58.13ms
step:1462/2330 train_time:84987ms step_avg:58.13ms
step:1463/2330 train_time:85044ms step_avg:58.13ms
step:1464/2330 train_time:85104ms step_avg:58.13ms
step:1465/2330 train_time:85160ms step_avg:58.13ms
step:1466/2330 train_time:85220ms step_avg:58.13ms
step:1467/2330 train_time:85277ms step_avg:58.13ms
step:1468/2330 train_time:85337ms step_avg:58.13ms
step:1469/2330 train_time:85395ms step_avg:58.13ms
step:1470/2330 train_time:85454ms step_avg:58.13ms
step:1471/2330 train_time:85511ms step_avg:58.13ms
step:1472/2330 train_time:85571ms step_avg:58.13ms
step:1473/2330 train_time:85628ms step_avg:58.13ms
step:1474/2330 train_time:85689ms step_avg:58.13ms
step:1475/2330 train_time:85746ms step_avg:58.13ms
step:1476/2330 train_time:85808ms step_avg:58.14ms
step:1477/2330 train_time:85865ms step_avg:58.13ms
step:1478/2330 train_time:85925ms step_avg:58.14ms
step:1479/2330 train_time:85982ms step_avg:58.13ms
step:1480/2330 train_time:86042ms step_avg:58.14ms
step:1481/2330 train_time:86098ms step_avg:58.14ms
step:1482/2330 train_time:86158ms step_avg:58.14ms
step:1483/2330 train_time:86215ms step_avg:58.14ms
step:1484/2330 train_time:86275ms step_avg:58.14ms
step:1485/2330 train_time:86332ms step_avg:58.14ms
step:1486/2330 train_time:86393ms step_avg:58.14ms
step:1487/2330 train_time:86450ms step_avg:58.14ms
step:1488/2330 train_time:86510ms step_avg:58.14ms
step:1489/2330 train_time:86567ms step_avg:58.14ms
step:1490/2330 train_time:86627ms step_avg:58.14ms
step:1491/2330 train_time:86684ms step_avg:58.14ms
step:1492/2330 train_time:86744ms step_avg:58.14ms
step:1493/2330 train_time:86801ms step_avg:58.14ms
step:1494/2330 train_time:86861ms step_avg:58.14ms
step:1495/2330 train_time:86918ms step_avg:58.14ms
step:1496/2330 train_time:86978ms step_avg:58.14ms
step:1497/2330 train_time:87035ms step_avg:58.14ms
step:1498/2330 train_time:87095ms step_avg:58.14ms
step:1499/2330 train_time:87152ms step_avg:58.14ms
step:1500/2330 train_time:87212ms step_avg:58.14ms
step:1500/2330 val_loss:3.9013 train_time:87293ms step_avg:58.20ms
step:1501/2330 train_time:87312ms step_avg:58.17ms
step:1502/2330 train_time:87332ms step_avg:58.14ms
step:1503/2330 train_time:87390ms step_avg:58.14ms
step:1504/2330 train_time:87457ms step_avg:58.15ms
step:1505/2330 train_time:87514ms step_avg:58.15ms
step:1506/2330 train_time:87575ms step_avg:58.15ms
step:1507/2330 train_time:87631ms step_avg:58.15ms
step:1508/2330 train_time:87691ms step_avg:58.15ms
step:1509/2330 train_time:87747ms step_avg:58.15ms
step:1510/2330 train_time:87807ms step_avg:58.15ms
step:1511/2330 train_time:87863ms step_avg:58.15ms
step:1512/2330 train_time:87923ms step_avg:58.15ms
step:1513/2330 train_time:87979ms step_avg:58.15ms
step:1514/2330 train_time:88038ms step_avg:58.15ms
step:1515/2330 train_time:88095ms step_avg:58.15ms
step:1516/2330 train_time:88154ms step_avg:58.15ms
step:1517/2330 train_time:88210ms step_avg:58.15ms
step:1518/2330 train_time:88271ms step_avg:58.15ms
step:1519/2330 train_time:88329ms step_avg:58.15ms
step:1520/2330 train_time:88391ms step_avg:58.15ms
step:1521/2330 train_time:88448ms step_avg:58.15ms
step:1522/2330 train_time:88511ms step_avg:58.15ms
step:1523/2330 train_time:88567ms step_avg:58.15ms
step:1524/2330 train_time:88629ms step_avg:58.16ms
step:1525/2330 train_time:88685ms step_avg:58.15ms
step:1526/2330 train_time:88745ms step_avg:58.16ms
step:1527/2330 train_time:88801ms step_avg:58.15ms
step:1528/2330 train_time:88861ms step_avg:58.16ms
step:1529/2330 train_time:88918ms step_avg:58.15ms
step:1530/2330 train_time:88978ms step_avg:58.16ms
step:1531/2330 train_time:89035ms step_avg:58.15ms
step:1532/2330 train_time:89096ms step_avg:58.16ms
step:1533/2330 train_time:89153ms step_avg:58.16ms
step:1534/2330 train_time:89214ms step_avg:58.16ms
step:1535/2330 train_time:89272ms step_avg:58.16ms
step:1536/2330 train_time:89334ms step_avg:58.16ms
step:1537/2330 train_time:89391ms step_avg:58.16ms
step:1538/2330 train_time:89454ms step_avg:58.16ms
step:1539/2330 train_time:89511ms step_avg:58.16ms
step:1540/2330 train_time:89574ms step_avg:58.16ms
step:1541/2330 train_time:89631ms step_avg:58.16ms
step:1542/2330 train_time:89693ms step_avg:58.17ms
step:1543/2330 train_time:89749ms step_avg:58.17ms
step:1544/2330 train_time:89811ms step_avg:58.17ms
step:1545/2330 train_time:89867ms step_avg:58.17ms
step:1546/2330 train_time:89928ms step_avg:58.17ms
step:1547/2330 train_time:89985ms step_avg:58.17ms
step:1548/2330 train_time:90045ms step_avg:58.17ms
step:1549/2330 train_time:90101ms step_avg:58.17ms
step:1550/2330 train_time:90163ms step_avg:58.17ms
step:1551/2330 train_time:90219ms step_avg:58.17ms
step:1552/2330 train_time:90281ms step_avg:58.17ms
step:1553/2330 train_time:90340ms step_avg:58.17ms
step:1554/2330 train_time:90401ms step_avg:58.17ms
step:1555/2330 train_time:90459ms step_avg:58.17ms
step:1556/2330 train_time:90522ms step_avg:58.18ms
step:1557/2330 train_time:90580ms step_avg:58.18ms
step:1558/2330 train_time:90641ms step_avg:58.18ms
step:1559/2330 train_time:90699ms step_avg:58.18ms
step:1560/2330 train_time:90759ms step_avg:58.18ms
step:1561/2330 train_time:90817ms step_avg:58.18ms
step:1562/2330 train_time:90879ms step_avg:58.18ms
step:1563/2330 train_time:90937ms step_avg:58.18ms
step:1564/2330 train_time:90997ms step_avg:58.18ms
step:1565/2330 train_time:91054ms step_avg:58.18ms
step:1566/2330 train_time:91115ms step_avg:58.18ms
step:1567/2330 train_time:91171ms step_avg:58.18ms
step:1568/2330 train_time:91232ms step_avg:58.18ms
step:1569/2330 train_time:91289ms step_avg:58.18ms
step:1570/2330 train_time:91351ms step_avg:58.19ms
step:1571/2330 train_time:91408ms step_avg:58.18ms
step:1572/2330 train_time:91469ms step_avg:58.19ms
step:1573/2330 train_time:91527ms step_avg:58.19ms
step:1574/2330 train_time:91587ms step_avg:58.19ms
step:1575/2330 train_time:91644ms step_avg:58.19ms
step:1576/2330 train_time:91706ms step_avg:58.19ms
step:1577/2330 train_time:91763ms step_avg:58.19ms
step:1578/2330 train_time:91825ms step_avg:58.19ms
step:1579/2330 train_time:91882ms step_avg:58.19ms
step:1580/2330 train_time:91942ms step_avg:58.19ms
step:1581/2330 train_time:91999ms step_avg:58.19ms
step:1582/2330 train_time:92060ms step_avg:58.19ms
step:1583/2330 train_time:92117ms step_avg:58.19ms
step:1584/2330 train_time:92178ms step_avg:58.19ms
step:1585/2330 train_time:92236ms step_avg:58.19ms
step:1586/2330 train_time:92297ms step_avg:58.20ms
step:1587/2330 train_time:92355ms step_avg:58.19ms
step:1588/2330 train_time:92417ms step_avg:58.20ms
step:1589/2330 train_time:92473ms step_avg:58.20ms
step:1590/2330 train_time:92536ms step_avg:58.20ms
step:1591/2330 train_time:92594ms step_avg:58.20ms
step:1592/2330 train_time:92655ms step_avg:58.20ms
step:1593/2330 train_time:92711ms step_avg:58.20ms
step:1594/2330 train_time:92774ms step_avg:58.20ms
step:1595/2330 train_time:92831ms step_avg:58.20ms
step:1596/2330 train_time:92893ms step_avg:58.20ms
step:1597/2330 train_time:92950ms step_avg:58.20ms
step:1598/2330 train_time:93011ms step_avg:58.20ms
step:1599/2330 train_time:93067ms step_avg:58.20ms
step:1600/2330 train_time:93129ms step_avg:58.21ms
step:1601/2330 train_time:93185ms step_avg:58.20ms
step:1602/2330 train_time:93246ms step_avg:58.21ms
step:1603/2330 train_time:93302ms step_avg:58.20ms
step:1604/2330 train_time:93363ms step_avg:58.21ms
step:1605/2330 train_time:93421ms step_avg:58.21ms
step:1606/2330 train_time:93484ms step_avg:58.21ms
step:1607/2330 train_time:93542ms step_avg:58.21ms
step:1608/2330 train_time:93602ms step_avg:58.21ms
step:1609/2330 train_time:93660ms step_avg:58.21ms
step:1610/2330 train_time:93721ms step_avg:58.21ms
step:1611/2330 train_time:93780ms step_avg:58.21ms
step:1612/2330 train_time:93840ms step_avg:58.21ms
step:1613/2330 train_time:93897ms step_avg:58.21ms
step:1614/2330 train_time:93959ms step_avg:58.21ms
step:1615/2330 train_time:94017ms step_avg:58.21ms
step:1616/2330 train_time:94077ms step_avg:58.22ms
step:1617/2330 train_time:94135ms step_avg:58.22ms
step:1618/2330 train_time:94195ms step_avg:58.22ms
step:1619/2330 train_time:94252ms step_avg:58.22ms
step:1620/2330 train_time:94314ms step_avg:58.22ms
step:1621/2330 train_time:94370ms step_avg:58.22ms
step:1622/2330 train_time:94432ms step_avg:58.22ms
step:1623/2330 train_time:94488ms step_avg:58.22ms
step:1624/2330 train_time:94551ms step_avg:58.22ms
step:1625/2330 train_time:94608ms step_avg:58.22ms
step:1626/2330 train_time:94670ms step_avg:58.22ms
step:1627/2330 train_time:94727ms step_avg:58.22ms
step:1628/2330 train_time:94788ms step_avg:58.22ms
step:1629/2330 train_time:94845ms step_avg:58.22ms
step:1630/2330 train_time:94906ms step_avg:58.22ms
step:1631/2330 train_time:94962ms step_avg:58.22ms
step:1632/2330 train_time:95024ms step_avg:58.23ms
step:1633/2330 train_time:95082ms step_avg:58.23ms
step:1634/2330 train_time:95143ms step_avg:58.23ms
step:1635/2330 train_time:95200ms step_avg:58.23ms
step:1636/2330 train_time:95262ms step_avg:58.23ms
step:1637/2330 train_time:95319ms step_avg:58.23ms
step:1638/2330 train_time:95381ms step_avg:58.23ms
step:1639/2330 train_time:95438ms step_avg:58.23ms
step:1640/2330 train_time:95499ms step_avg:58.23ms
step:1641/2330 train_time:95556ms step_avg:58.23ms
step:1642/2330 train_time:95619ms step_avg:58.23ms
step:1643/2330 train_time:95677ms step_avg:58.23ms
step:1644/2330 train_time:95738ms step_avg:58.23ms
step:1645/2330 train_time:95795ms step_avg:58.23ms
step:1646/2330 train_time:95857ms step_avg:58.24ms
step:1647/2330 train_time:95914ms step_avg:58.24ms
step:1648/2330 train_time:95976ms step_avg:58.24ms
step:1649/2330 train_time:96033ms step_avg:58.24ms
step:1650/2330 train_time:96094ms step_avg:58.24ms
step:1651/2330 train_time:96150ms step_avg:58.24ms
step:1652/2330 train_time:96212ms step_avg:58.24ms
step:1653/2330 train_time:96269ms step_avg:58.24ms
step:1654/2330 train_time:96331ms step_avg:58.24ms
step:1655/2330 train_time:96388ms step_avg:58.24ms
step:1656/2330 train_time:96448ms step_avg:58.24ms
step:1657/2330 train_time:96504ms step_avg:58.24ms
step:1658/2330 train_time:96567ms step_avg:58.24ms
step:1659/2330 train_time:96624ms step_avg:58.24ms
step:1660/2330 train_time:96685ms step_avg:58.24ms
step:1661/2330 train_time:96741ms step_avg:58.24ms
step:1662/2330 train_time:96804ms step_avg:58.25ms
step:1663/2330 train_time:96861ms step_avg:58.24ms
step:1664/2330 train_time:96923ms step_avg:58.25ms
step:1665/2330 train_time:96981ms step_avg:58.25ms
step:1666/2330 train_time:97043ms step_avg:58.25ms
step:1667/2330 train_time:97100ms step_avg:58.25ms
step:1668/2330 train_time:97162ms step_avg:58.25ms
step:1669/2330 train_time:97220ms step_avg:58.25ms
step:1670/2330 train_time:97280ms step_avg:58.25ms
step:1671/2330 train_time:97339ms step_avg:58.25ms
step:1672/2330 train_time:97399ms step_avg:58.25ms
step:1673/2330 train_time:97457ms step_avg:58.25ms
step:1674/2330 train_time:97517ms step_avg:58.25ms
step:1675/2330 train_time:97574ms step_avg:58.25ms
step:1676/2330 train_time:97635ms step_avg:58.25ms
step:1677/2330 train_time:97692ms step_avg:58.25ms
step:1678/2330 train_time:97753ms step_avg:58.26ms
step:1679/2330 train_time:97810ms step_avg:58.25ms
step:1680/2330 train_time:97873ms step_avg:58.26ms
step:1681/2330 train_time:97929ms step_avg:58.26ms
step:1682/2330 train_time:97990ms step_avg:58.26ms
step:1683/2330 train_time:98047ms step_avg:58.26ms
step:1684/2330 train_time:98107ms step_avg:58.26ms
step:1685/2330 train_time:98165ms step_avg:58.26ms
step:1686/2330 train_time:98227ms step_avg:58.26ms
step:1687/2330 train_time:98284ms step_avg:58.26ms
step:1688/2330 train_time:98344ms step_avg:58.26ms
step:1689/2330 train_time:98402ms step_avg:58.26ms
step:1690/2330 train_time:98463ms step_avg:58.26ms
step:1691/2330 train_time:98521ms step_avg:58.26ms
step:1692/2330 train_time:98581ms step_avg:58.26ms
step:1693/2330 train_time:98639ms step_avg:58.26ms
step:1694/2330 train_time:98701ms step_avg:58.26ms
step:1695/2330 train_time:98759ms step_avg:58.26ms
step:1696/2330 train_time:98821ms step_avg:58.27ms
step:1697/2330 train_time:98879ms step_avg:58.27ms
step:1698/2330 train_time:98939ms step_avg:58.27ms
step:1699/2330 train_time:98997ms step_avg:58.27ms
step:1700/2330 train_time:99057ms step_avg:58.27ms
step:1701/2330 train_time:99114ms step_avg:58.27ms
step:1702/2330 train_time:99175ms step_avg:58.27ms
step:1703/2330 train_time:99232ms step_avg:58.27ms
step:1704/2330 train_time:99294ms step_avg:58.27ms
step:1705/2330 train_time:99350ms step_avg:58.27ms
step:1706/2330 train_time:99411ms step_avg:58.27ms
step:1707/2330 train_time:99467ms step_avg:58.27ms
step:1708/2330 train_time:99530ms step_avg:58.27ms
step:1709/2330 train_time:99587ms step_avg:58.27ms
step:1710/2330 train_time:99648ms step_avg:58.27ms
step:1711/2330 train_time:99705ms step_avg:58.27ms
step:1712/2330 train_time:99767ms step_avg:58.28ms
step:1713/2330 train_time:99824ms step_avg:58.27ms
step:1714/2330 train_time:99886ms step_avg:58.28ms
step:1715/2330 train_time:99942ms step_avg:58.28ms
step:1716/2330 train_time:100004ms step_avg:58.28ms
step:1717/2330 train_time:100062ms step_avg:58.28ms
step:1718/2330 train_time:100124ms step_avg:58.28ms
step:1719/2330 train_time:100182ms step_avg:58.28ms
step:1720/2330 train_time:100242ms step_avg:58.28ms
step:1721/2330 train_time:100300ms step_avg:58.28ms
step:1722/2330 train_time:100361ms step_avg:58.28ms
step:1723/2330 train_time:100418ms step_avg:58.28ms
step:1724/2330 train_time:100480ms step_avg:58.28ms
step:1725/2330 train_time:100537ms step_avg:58.28ms
step:1726/2330 train_time:100599ms step_avg:58.28ms
step:1727/2330 train_time:100656ms step_avg:58.28ms
step:1728/2330 train_time:100717ms step_avg:58.29ms
step:1729/2330 train_time:100775ms step_avg:58.29ms
step:1730/2330 train_time:100836ms step_avg:58.29ms
step:1731/2330 train_time:100894ms step_avg:58.29ms
step:1732/2330 train_time:100955ms step_avg:58.29ms
step:1733/2330 train_time:101012ms step_avg:58.29ms
step:1734/2330 train_time:101074ms step_avg:58.29ms
step:1735/2330 train_time:101130ms step_avg:58.29ms
step:1736/2330 train_time:101193ms step_avg:58.29ms
step:1737/2330 train_time:101249ms step_avg:58.29ms
step:1738/2330 train_time:101311ms step_avg:58.29ms
step:1739/2330 train_time:101367ms step_avg:58.29ms
step:1740/2330 train_time:101428ms step_avg:58.29ms
step:1741/2330 train_time:101485ms step_avg:58.29ms
step:1742/2330 train_time:101546ms step_avg:58.29ms
step:1743/2330 train_time:101603ms step_avg:58.29ms
step:1744/2330 train_time:101665ms step_avg:58.29ms
step:1745/2330 train_time:101722ms step_avg:58.29ms
step:1746/2330 train_time:101782ms step_avg:58.29ms
step:1747/2330 train_time:101840ms step_avg:58.29ms
step:1748/2330 train_time:101900ms step_avg:58.30ms
step:1749/2330 train_time:101958ms step_avg:58.30ms
step:1750/2330 train_time:102021ms step_avg:58.30ms
step:1750/2330 val_loss:3.8182 train_time:102103ms step_avg:58.34ms
step:1751/2330 train_time:102122ms step_avg:58.32ms
step:1752/2330 train_time:102142ms step_avg:58.30ms
step:1753/2330 train_time:102197ms step_avg:58.30ms
step:1754/2330 train_time:102266ms step_avg:58.30ms
step:1755/2330 train_time:102322ms step_avg:58.30ms
step:1756/2330 train_time:102388ms step_avg:58.31ms
step:1757/2330 train_time:102445ms step_avg:58.31ms
step:1758/2330 train_time:102505ms step_avg:58.31ms
step:1759/2330 train_time:102561ms step_avg:58.31ms
step:1760/2330 train_time:102623ms step_avg:58.31ms
step:1761/2330 train_time:102680ms step_avg:58.31ms
step:1762/2330 train_time:102740ms step_avg:58.31ms
step:1763/2330 train_time:102796ms step_avg:58.31ms
step:1764/2330 train_time:102857ms step_avg:58.31ms
step:1765/2330 train_time:102914ms step_avg:58.31ms
step:1766/2330 train_time:102974ms step_avg:58.31ms
step:1767/2330 train_time:103034ms step_avg:58.31ms
step:1768/2330 train_time:103095ms step_avg:58.31ms
step:1769/2330 train_time:103154ms step_avg:58.31ms
step:1770/2330 train_time:103217ms step_avg:58.31ms
step:1771/2330 train_time:103275ms step_avg:58.31ms
step:1772/2330 train_time:103335ms step_avg:58.32ms
step:1773/2330 train_time:103391ms step_avg:58.31ms
step:1774/2330 train_time:103454ms step_avg:58.32ms
step:1775/2330 train_time:103510ms step_avg:58.32ms
step:1776/2330 train_time:103572ms step_avg:58.32ms
step:1777/2330 train_time:103629ms step_avg:58.32ms
step:1778/2330 train_time:103689ms step_avg:58.32ms
step:1779/2330 train_time:103747ms step_avg:58.32ms
step:1780/2330 train_time:103807ms step_avg:58.32ms
step:1781/2330 train_time:103865ms step_avg:58.32ms
step:1782/2330 train_time:103925ms step_avg:58.32ms
step:1783/2330 train_time:103983ms step_avg:58.32ms
step:1784/2330 train_time:104044ms step_avg:58.32ms
step:1785/2330 train_time:104104ms step_avg:58.32ms
step:1786/2330 train_time:104164ms step_avg:58.32ms
step:1787/2330 train_time:104223ms step_avg:58.32ms
step:1788/2330 train_time:104285ms step_avg:58.32ms
step:1789/2330 train_time:104343ms step_avg:58.32ms
step:1790/2330 train_time:104403ms step_avg:58.33ms
step:1791/2330 train_time:104460ms step_avg:58.32ms
step:1792/2330 train_time:104521ms step_avg:58.33ms
step:1793/2330 train_time:104578ms step_avg:58.33ms
step:1794/2330 train_time:104639ms step_avg:58.33ms
step:1795/2330 train_time:104695ms step_avg:58.33ms
step:1796/2330 train_time:104757ms step_avg:58.33ms
step:1797/2330 train_time:104813ms step_avg:58.33ms
step:1798/2330 train_time:104874ms step_avg:58.33ms
step:1799/2330 train_time:104930ms step_avg:58.33ms
step:1800/2330 train_time:104991ms step_avg:58.33ms
step:1801/2330 train_time:105049ms step_avg:58.33ms
step:1802/2330 train_time:105110ms step_avg:58.33ms
step:1803/2330 train_time:105169ms step_avg:58.33ms
step:1804/2330 train_time:105229ms step_avg:58.33ms
step:1805/2330 train_time:105288ms step_avg:58.33ms
step:1806/2330 train_time:105348ms step_avg:58.33ms
step:1807/2330 train_time:105405ms step_avg:58.33ms
step:1808/2330 train_time:105467ms step_avg:58.33ms
step:1809/2330 train_time:105524ms step_avg:58.33ms
step:1810/2330 train_time:105586ms step_avg:58.33ms
step:1811/2330 train_time:105643ms step_avg:58.33ms
step:1812/2330 train_time:105704ms step_avg:58.34ms
step:1813/2330 train_time:105761ms step_avg:58.34ms
step:1814/2330 train_time:105822ms step_avg:58.34ms
step:1815/2330 train_time:105879ms step_avg:58.34ms
step:1816/2330 train_time:105940ms step_avg:58.34ms
step:1817/2330 train_time:105997ms step_avg:58.34ms
step:1818/2330 train_time:106058ms step_avg:58.34ms
step:1819/2330 train_time:106115ms step_avg:58.34ms
step:1820/2330 train_time:106176ms step_avg:58.34ms
step:1821/2330 train_time:106234ms step_avg:58.34ms
step:1822/2330 train_time:106294ms step_avg:58.34ms
step:1823/2330 train_time:106352ms step_avg:58.34ms
step:1824/2330 train_time:106413ms step_avg:58.34ms
step:1825/2330 train_time:106470ms step_avg:58.34ms
step:1826/2330 train_time:106531ms step_avg:58.34ms
step:1827/2330 train_time:106588ms step_avg:58.34ms
step:1828/2330 train_time:106649ms step_avg:58.34ms
step:1829/2330 train_time:106707ms step_avg:58.34ms
step:1830/2330 train_time:106767ms step_avg:58.34ms
step:1831/2330 train_time:106825ms step_avg:58.34ms
step:1832/2330 train_time:106885ms step_avg:58.34ms
step:1833/2330 train_time:106943ms step_avg:58.34ms
step:1834/2330 train_time:107003ms step_avg:58.34ms
step:1835/2330 train_time:107060ms step_avg:58.34ms
step:1836/2330 train_time:107123ms step_avg:58.35ms
step:1837/2330 train_time:107180ms step_avg:58.35ms
step:1838/2330 train_time:107241ms step_avg:58.35ms
step:1839/2330 train_time:107298ms step_avg:58.35ms
step:1840/2330 train_time:107361ms step_avg:58.35ms
step:1841/2330 train_time:107418ms step_avg:58.35ms
step:1842/2330 train_time:107479ms step_avg:58.35ms
step:1843/2330 train_time:107537ms step_avg:58.35ms
step:1844/2330 train_time:107597ms step_avg:58.35ms
step:1845/2330 train_time:107654ms step_avg:58.35ms
step:1846/2330 train_time:107715ms step_avg:58.35ms
step:1847/2330 train_time:107772ms step_avg:58.35ms
step:1848/2330 train_time:107832ms step_avg:58.35ms
step:1849/2330 train_time:107890ms step_avg:58.35ms
step:1850/2330 train_time:107951ms step_avg:58.35ms
step:1851/2330 train_time:108010ms step_avg:58.35ms
step:1852/2330 train_time:108071ms step_avg:58.35ms
step:1853/2330 train_time:108129ms step_avg:58.35ms
step:1854/2330 train_time:108189ms step_avg:58.35ms
step:1855/2330 train_time:108248ms step_avg:58.35ms
step:1856/2330 train_time:108308ms step_avg:58.36ms
step:1857/2330 train_time:108366ms step_avg:58.36ms
step:1858/2330 train_time:108427ms step_avg:58.36ms
step:1859/2330 train_time:108485ms step_avg:58.36ms
step:1860/2330 train_time:108546ms step_avg:58.36ms
step:1861/2330 train_time:108604ms step_avg:58.36ms
step:1862/2330 train_time:108664ms step_avg:58.36ms
step:1863/2330 train_time:108721ms step_avg:58.36ms
step:1864/2330 train_time:108782ms step_avg:58.36ms
step:1865/2330 train_time:108839ms step_avg:58.36ms
step:1866/2330 train_time:108900ms step_avg:58.36ms
step:1867/2330 train_time:108957ms step_avg:58.36ms
step:1868/2330 train_time:109018ms step_avg:58.36ms
step:1869/2330 train_time:109075ms step_avg:58.36ms
step:1870/2330 train_time:109135ms step_avg:58.36ms
step:1871/2330 train_time:109192ms step_avg:58.36ms
step:1872/2330 train_time:109253ms step_avg:58.36ms
step:1873/2330 train_time:109310ms step_avg:58.36ms
step:1874/2330 train_time:109372ms step_avg:58.36ms
step:1875/2330 train_time:109429ms step_avg:58.36ms
step:1876/2330 train_time:109490ms step_avg:58.36ms
step:1877/2330 train_time:109548ms step_avg:58.36ms
step:1878/2330 train_time:109608ms step_avg:58.36ms
step:1879/2330 train_time:109667ms step_avg:58.36ms
step:1880/2330 train_time:109727ms step_avg:58.37ms
step:1881/2330 train_time:109785ms step_avg:58.37ms
step:1882/2330 train_time:109846ms step_avg:58.37ms
step:1883/2330 train_time:109903ms step_avg:58.37ms
step:1884/2330 train_time:109964ms step_avg:58.37ms
step:1885/2330 train_time:110022ms step_avg:58.37ms
step:1886/2330 train_time:110082ms step_avg:58.37ms
step:1887/2330 train_time:110140ms step_avg:58.37ms
step:1888/2330 train_time:110201ms step_avg:58.37ms
step:1889/2330 train_time:110259ms step_avg:58.37ms
step:1890/2330 train_time:110320ms step_avg:58.37ms
step:1891/2330 train_time:110377ms step_avg:58.37ms
step:1892/2330 train_time:110438ms step_avg:58.37ms
step:1893/2330 train_time:110495ms step_avg:58.37ms
step:1894/2330 train_time:110557ms step_avg:58.37ms
step:1895/2330 train_time:110614ms step_avg:58.37ms
step:1896/2330 train_time:110675ms step_avg:58.37ms
step:1897/2330 train_time:110732ms step_avg:58.37ms
step:1898/2330 train_time:110793ms step_avg:58.37ms
step:1899/2330 train_time:110850ms step_avg:58.37ms
step:1900/2330 train_time:110911ms step_avg:58.37ms
step:1901/2330 train_time:110969ms step_avg:58.37ms
step:1902/2330 train_time:111029ms step_avg:58.37ms
step:1903/2330 train_time:111087ms step_avg:58.37ms
step:1904/2330 train_time:111148ms step_avg:58.38ms
step:1905/2330 train_time:111206ms step_avg:58.38ms
step:1906/2330 train_time:111268ms step_avg:58.38ms
step:1907/2330 train_time:111327ms step_avg:58.38ms
step:1908/2330 train_time:111387ms step_avg:58.38ms
step:1909/2330 train_time:111445ms step_avg:58.38ms
step:1910/2330 train_time:111505ms step_avg:58.38ms
step:1911/2330 train_time:111562ms step_avg:58.38ms
step:1912/2330 train_time:111624ms step_avg:58.38ms
step:1913/2330 train_time:111681ms step_avg:58.38ms
step:1914/2330 train_time:111742ms step_avg:58.38ms
step:1915/2330 train_time:111799ms step_avg:58.38ms
step:1916/2330 train_time:111860ms step_avg:58.38ms
step:1917/2330 train_time:111917ms step_avg:58.38ms
step:1918/2330 train_time:111978ms step_avg:58.38ms
step:1919/2330 train_time:112036ms step_avg:58.38ms
step:1920/2330 train_time:112096ms step_avg:58.38ms
step:1921/2330 train_time:112154ms step_avg:58.38ms
step:1922/2330 train_time:112214ms step_avg:58.38ms
step:1923/2330 train_time:112271ms step_avg:58.38ms
step:1924/2330 train_time:112333ms step_avg:58.38ms
step:1925/2330 train_time:112390ms step_avg:58.38ms
step:1926/2330 train_time:112450ms step_avg:58.39ms
step:1927/2330 train_time:112508ms step_avg:58.39ms
step:1928/2330 train_time:112569ms step_avg:58.39ms
step:1929/2330 train_time:112626ms step_avg:58.39ms
step:1930/2330 train_time:112687ms step_avg:58.39ms
step:1931/2330 train_time:112746ms step_avg:58.39ms
step:1932/2330 train_time:112806ms step_avg:58.39ms
step:1933/2330 train_time:112864ms step_avg:58.39ms
step:1934/2330 train_time:112925ms step_avg:58.39ms
step:1935/2330 train_time:112984ms step_avg:58.39ms
step:1936/2330 train_time:113044ms step_avg:58.39ms
step:1937/2330 train_time:113103ms step_avg:58.39ms
step:1938/2330 train_time:113163ms step_avg:58.39ms
step:1939/2330 train_time:113220ms step_avg:58.39ms
step:1940/2330 train_time:113282ms step_avg:58.39ms
step:1941/2330 train_time:113339ms step_avg:58.39ms
step:1942/2330 train_time:113399ms step_avg:58.39ms
step:1943/2330 train_time:113456ms step_avg:58.39ms
step:1944/2330 train_time:113518ms step_avg:58.39ms
step:1945/2330 train_time:113574ms step_avg:58.39ms
step:1946/2330 train_time:113636ms step_avg:58.39ms
step:1947/2330 train_time:113693ms step_avg:58.39ms
step:1948/2330 train_time:113754ms step_avg:58.40ms
step:1949/2330 train_time:113811ms step_avg:58.39ms
step:1950/2330 train_time:113872ms step_avg:58.40ms
step:1951/2330 train_time:113930ms step_avg:58.40ms
step:1952/2330 train_time:113990ms step_avg:58.40ms
step:1953/2330 train_time:114047ms step_avg:58.40ms
step:1954/2330 train_time:114108ms step_avg:58.40ms
step:1955/2330 train_time:114167ms step_avg:58.40ms
step:1956/2330 train_time:114227ms step_avg:58.40ms
step:1957/2330 train_time:114286ms step_avg:58.40ms
step:1958/2330 train_time:114346ms step_avg:58.40ms
step:1959/2330 train_time:114405ms step_avg:58.40ms
step:1960/2330 train_time:114465ms step_avg:58.40ms
step:1961/2330 train_time:114524ms step_avg:58.40ms
step:1962/2330 train_time:114584ms step_avg:58.40ms
step:1963/2330 train_time:114642ms step_avg:58.40ms
step:1964/2330 train_time:114702ms step_avg:58.40ms
step:1965/2330 train_time:114759ms step_avg:58.40ms
step:1966/2330 train_time:114821ms step_avg:58.40ms
step:1967/2330 train_time:114878ms step_avg:58.40ms
step:1968/2330 train_time:114940ms step_avg:58.40ms
step:1969/2330 train_time:114997ms step_avg:58.40ms
step:1970/2330 train_time:115057ms step_avg:58.40ms
step:1971/2330 train_time:115114ms step_avg:58.40ms
step:1972/2330 train_time:115174ms step_avg:58.40ms
step:1973/2330 train_time:115231ms step_avg:58.40ms
step:1974/2330 train_time:115293ms step_avg:58.41ms
step:1975/2330 train_time:115350ms step_avg:58.41ms
step:1976/2330 train_time:115411ms step_avg:58.41ms
step:1977/2330 train_time:115469ms step_avg:58.41ms
step:1978/2330 train_time:115529ms step_avg:58.41ms
step:1979/2330 train_time:115587ms step_avg:58.41ms
step:1980/2330 train_time:115647ms step_avg:58.41ms
step:1981/2330 train_time:115704ms step_avg:58.41ms
step:1982/2330 train_time:115765ms step_avg:58.41ms
step:1983/2330 train_time:115823ms step_avg:58.41ms
step:1984/2330 train_time:115884ms step_avg:58.41ms
step:1985/2330 train_time:115942ms step_avg:58.41ms
step:1986/2330 train_time:116003ms step_avg:58.41ms
step:1987/2330 train_time:116060ms step_avg:58.41ms
step:1988/2330 train_time:116120ms step_avg:58.41ms
step:1989/2330 train_time:116177ms step_avg:58.41ms
step:1990/2330 train_time:116239ms step_avg:58.41ms
step:1991/2330 train_time:116296ms step_avg:58.41ms
step:1992/2330 train_time:116359ms step_avg:58.41ms
step:1993/2330 train_time:116417ms step_avg:58.41ms
step:1994/2330 train_time:116479ms step_avg:58.41ms
step:1995/2330 train_time:116536ms step_avg:58.41ms
step:1996/2330 train_time:116597ms step_avg:58.42ms
step:1997/2330 train_time:116653ms step_avg:58.41ms
step:1998/2330 train_time:116716ms step_avg:58.42ms
step:1999/2330 train_time:116773ms step_avg:58.42ms
step:2000/2330 train_time:116834ms step_avg:58.42ms
step:2000/2330 val_loss:3.7561 train_time:116917ms step_avg:58.46ms
step:2001/2330 train_time:116935ms step_avg:58.44ms
step:2002/2330 train_time:116957ms step_avg:58.42ms
step:2003/2330 train_time:117016ms step_avg:58.42ms
step:2004/2330 train_time:117080ms step_avg:58.42ms
step:2005/2330 train_time:117139ms step_avg:58.42ms
step:2006/2330 train_time:117202ms step_avg:58.43ms
step:2007/2330 train_time:117259ms step_avg:58.43ms
step:2008/2330 train_time:117320ms step_avg:58.43ms
step:2009/2330 train_time:117377ms step_avg:58.43ms
step:2010/2330 train_time:117438ms step_avg:58.43ms
step:2011/2330 train_time:117495ms step_avg:58.43ms
step:2012/2330 train_time:117554ms step_avg:58.43ms
step:2013/2330 train_time:117610ms step_avg:58.43ms
step:2014/2330 train_time:117671ms step_avg:58.43ms
step:2015/2330 train_time:117727ms step_avg:58.43ms
step:2016/2330 train_time:117787ms step_avg:58.43ms
step:2017/2330 train_time:117844ms step_avg:58.43ms
step:2018/2330 train_time:117905ms step_avg:58.43ms
step:2019/2330 train_time:117964ms step_avg:58.43ms
step:2020/2330 train_time:118026ms step_avg:58.43ms
step:2021/2330 train_time:118086ms step_avg:58.43ms
step:2022/2330 train_time:118148ms step_avg:58.43ms
step:2023/2330 train_time:118206ms step_avg:58.43ms
step:2024/2330 train_time:118267ms step_avg:58.43ms
step:2025/2330 train_time:118325ms step_avg:58.43ms
step:2026/2330 train_time:118386ms step_avg:58.43ms
step:2027/2330 train_time:118444ms step_avg:58.43ms
step:2028/2330 train_time:118504ms step_avg:58.43ms
step:2029/2330 train_time:118562ms step_avg:58.43ms
step:2030/2330 train_time:118621ms step_avg:58.43ms
step:2031/2330 train_time:118678ms step_avg:58.43ms
step:2032/2330 train_time:118738ms step_avg:58.43ms
step:2033/2330 train_time:118795ms step_avg:58.43ms
step:2034/2330 train_time:118855ms step_avg:58.43ms
step:2035/2330 train_time:118912ms step_avg:58.43ms
step:2036/2330 train_time:118973ms step_avg:58.43ms
step:2037/2330 train_time:119031ms step_avg:58.43ms
step:2038/2330 train_time:119093ms step_avg:58.44ms
step:2039/2330 train_time:119151ms step_avg:58.44ms
step:2040/2330 train_time:119212ms step_avg:58.44ms
step:2041/2330 train_time:119270ms step_avg:58.44ms
step:2042/2330 train_time:119331ms step_avg:58.44ms
step:2043/2330 train_time:119390ms step_avg:58.44ms
step:2044/2330 train_time:119450ms step_avg:58.44ms
step:2045/2330 train_time:119507ms step_avg:58.44ms
step:2046/2330 train_time:119567ms step_avg:58.44ms
step:2047/2330 train_time:119624ms step_avg:58.44ms
step:2048/2330 train_time:119686ms step_avg:58.44ms
step:2049/2330 train_time:119743ms step_avg:58.44ms
step:2050/2330 train_time:119803ms step_avg:58.44ms
step:2051/2330 train_time:119860ms step_avg:58.44ms
step:2052/2330 train_time:119921ms step_avg:58.44ms
step:2053/2330 train_time:119979ms step_avg:58.44ms
step:2054/2330 train_time:120041ms step_avg:58.44ms
step:2055/2330 train_time:120099ms step_avg:58.44ms
step:2056/2330 train_time:120160ms step_avg:58.44ms
step:2057/2330 train_time:120218ms step_avg:58.44ms
step:2058/2330 train_time:120280ms step_avg:58.45ms
step:2059/2330 train_time:120337ms step_avg:58.44ms
step:2060/2330 train_time:120400ms step_avg:58.45ms
step:2061/2330 train_time:120457ms step_avg:58.45ms
step:2062/2330 train_time:120517ms step_avg:58.45ms
step:2063/2330 train_time:120575ms step_avg:58.45ms
step:2064/2330 train_time:120635ms step_avg:58.45ms
step:2065/2330 train_time:120692ms step_avg:58.45ms
step:2066/2330 train_time:120752ms step_avg:58.45ms
step:2067/2330 train_time:120809ms step_avg:58.45ms
step:2068/2330 train_time:120870ms step_avg:58.45ms
step:2069/2330 train_time:120929ms step_avg:58.45ms
step:2070/2330 train_time:120990ms step_avg:58.45ms
step:2071/2330 train_time:121048ms step_avg:58.45ms
step:2072/2330 train_time:121108ms step_avg:58.45ms
step:2073/2330 train_time:121166ms step_avg:58.45ms
step:2074/2330 train_time:121227ms step_avg:58.45ms
step:2075/2330 train_time:121285ms step_avg:58.45ms
step:2076/2330 train_time:121346ms step_avg:58.45ms
step:2077/2330 train_time:121405ms step_avg:58.45ms
step:2078/2330 train_time:121465ms step_avg:58.45ms
step:2079/2330 train_time:121522ms step_avg:58.45ms
step:2080/2330 train_time:121583ms step_avg:58.45ms
step:2081/2330 train_time:121641ms step_avg:58.45ms
step:2082/2330 train_time:121702ms step_avg:58.45ms
step:2083/2330 train_time:121759ms step_avg:58.45ms
step:2084/2330 train_time:121819ms step_avg:58.45ms
step:2085/2330 train_time:121877ms step_avg:58.45ms
step:2086/2330 train_time:121938ms step_avg:58.46ms
step:2087/2330 train_time:121995ms step_avg:58.45ms
step:2088/2330 train_time:122056ms step_avg:58.46ms
step:2089/2330 train_time:122113ms step_avg:58.46ms
step:2090/2330 train_time:122175ms step_avg:58.46ms
step:2091/2330 train_time:122233ms step_avg:58.46ms
step:2092/2330 train_time:122294ms step_avg:58.46ms
step:2093/2330 train_time:122351ms step_avg:58.46ms
step:2094/2330 train_time:122412ms step_avg:58.46ms
step:2095/2330 train_time:122469ms step_avg:58.46ms
step:2096/2330 train_time:122530ms step_avg:58.46ms
step:2097/2330 train_time:122588ms step_avg:58.46ms
step:2098/2330 train_time:122648ms step_avg:58.46ms
step:2099/2330 train_time:122705ms step_avg:58.46ms
step:2100/2330 train_time:122766ms step_avg:58.46ms
step:2101/2330 train_time:122824ms step_avg:58.46ms
step:2102/2330 train_time:122885ms step_avg:58.46ms
step:2103/2330 train_time:122943ms step_avg:58.46ms
step:2104/2330 train_time:123004ms step_avg:58.46ms
step:2105/2330 train_time:123062ms step_avg:58.46ms
step:2106/2330 train_time:123123ms step_avg:58.46ms
step:2107/2330 train_time:123181ms step_avg:58.46ms
step:2108/2330 train_time:123242ms step_avg:58.46ms
step:2109/2330 train_time:123300ms step_avg:58.46ms
step:2110/2330 train_time:123361ms step_avg:58.46ms
step:2111/2330 train_time:123418ms step_avg:58.46ms
step:2112/2330 train_time:123480ms step_avg:58.47ms
step:2113/2330 train_time:123537ms step_avg:58.47ms
step:2114/2330 train_time:123600ms step_avg:58.47ms
step:2115/2330 train_time:123657ms step_avg:58.47ms
step:2116/2330 train_time:123717ms step_avg:58.47ms
step:2117/2330 train_time:123774ms step_avg:58.47ms
step:2118/2330 train_time:123835ms step_avg:58.47ms
step:2119/2330 train_time:123892ms step_avg:58.47ms
step:2120/2330 train_time:123952ms step_avg:58.47ms
step:2121/2330 train_time:124009ms step_avg:58.47ms
step:2122/2330 train_time:124070ms step_avg:58.47ms
step:2123/2330 train_time:124128ms step_avg:58.47ms
step:2124/2330 train_time:124189ms step_avg:58.47ms
step:2125/2330 train_time:124248ms step_avg:58.47ms
step:2126/2330 train_time:124308ms step_avg:58.47ms
step:2127/2330 train_time:124366ms step_avg:58.47ms
step:2128/2330 train_time:124427ms step_avg:58.47ms
step:2129/2330 train_time:124485ms step_avg:58.47ms
step:2130/2330 train_time:124546ms step_avg:58.47ms
step:2131/2330 train_time:124605ms step_avg:58.47ms
step:2132/2330 train_time:124665ms step_avg:58.47ms
step:2133/2330 train_time:124724ms step_avg:58.47ms
step:2134/2330 train_time:124784ms step_avg:58.47ms
step:2135/2330 train_time:124841ms step_avg:58.47ms
step:2136/2330 train_time:124902ms step_avg:58.47ms
step:2137/2330 train_time:124960ms step_avg:58.47ms
step:2138/2330 train_time:125021ms step_avg:58.48ms
step:2139/2330 train_time:125078ms step_avg:58.48ms
step:2140/2330 train_time:125139ms step_avg:58.48ms
step:2141/2330 train_time:125196ms step_avg:58.48ms
step:2142/2330 train_time:125257ms step_avg:58.48ms
step:2143/2330 train_time:125315ms step_avg:58.48ms
step:2144/2330 train_time:125375ms step_avg:58.48ms
step:2145/2330 train_time:125432ms step_avg:58.48ms
step:2146/2330 train_time:125494ms step_avg:58.48ms
step:2147/2330 train_time:125551ms step_avg:58.48ms
step:2148/2330 train_time:125613ms step_avg:58.48ms
step:2149/2330 train_time:125669ms step_avg:58.48ms
step:2150/2330 train_time:125731ms step_avg:58.48ms
step:2151/2330 train_time:125789ms step_avg:58.48ms
step:2152/2330 train_time:125849ms step_avg:58.48ms
step:2153/2330 train_time:125908ms step_avg:58.48ms
step:2154/2330 train_time:125968ms step_avg:58.48ms
step:2155/2330 train_time:126026ms step_avg:58.48ms
step:2156/2330 train_time:126086ms step_avg:58.48ms
step:2157/2330 train_time:126145ms step_avg:58.48ms
step:2158/2330 train_time:126206ms step_avg:58.48ms
step:2159/2330 train_time:126264ms step_avg:58.48ms
step:2160/2330 train_time:126325ms step_avg:58.48ms
step:2161/2330 train_time:126382ms step_avg:58.48ms
step:2162/2330 train_time:126444ms step_avg:58.48ms
step:2163/2330 train_time:126502ms step_avg:58.48ms
step:2164/2330 train_time:126562ms step_avg:58.49ms
step:2165/2330 train_time:126620ms step_avg:58.48ms
step:2166/2330 train_time:126680ms step_avg:58.49ms
step:2167/2330 train_time:126738ms step_avg:58.49ms
step:2168/2330 train_time:126798ms step_avg:58.49ms
step:2169/2330 train_time:126855ms step_avg:58.49ms
step:2170/2330 train_time:126917ms step_avg:58.49ms
step:2171/2330 train_time:126975ms step_avg:58.49ms
step:2172/2330 train_time:127036ms step_avg:58.49ms
step:2173/2330 train_time:127094ms step_avg:58.49ms
step:2174/2330 train_time:127154ms step_avg:58.49ms
step:2175/2330 train_time:127211ms step_avg:58.49ms
step:2176/2330 train_time:127273ms step_avg:58.49ms
step:2177/2330 train_time:127330ms step_avg:58.49ms
step:2178/2330 train_time:127390ms step_avg:58.49ms
step:2179/2330 train_time:127447ms step_avg:58.49ms
step:2180/2330 train_time:127508ms step_avg:58.49ms
step:2181/2330 train_time:127566ms step_avg:58.49ms
step:2182/2330 train_time:127627ms step_avg:58.49ms
step:2183/2330 train_time:127684ms step_avg:58.49ms
step:2184/2330 train_time:127746ms step_avg:58.49ms
step:2185/2330 train_time:127804ms step_avg:58.49ms
step:2186/2330 train_time:127864ms step_avg:58.49ms
step:2187/2330 train_time:127923ms step_avg:58.49ms
step:2188/2330 train_time:127984ms step_avg:58.49ms
step:2189/2330 train_time:128042ms step_avg:58.49ms
step:2190/2330 train_time:128103ms step_avg:58.49ms
step:2191/2330 train_time:128162ms step_avg:58.49ms
step:2192/2330 train_time:128222ms step_avg:58.50ms
step:2193/2330 train_time:128279ms step_avg:58.49ms
step:2194/2330 train_time:128340ms step_avg:58.50ms
step:2195/2330 train_time:128397ms step_avg:58.50ms
step:2196/2330 train_time:128458ms step_avg:58.50ms
step:2197/2330 train_time:128515ms step_avg:58.50ms
step:2198/2330 train_time:128576ms step_avg:58.50ms
step:2199/2330 train_time:128633ms step_avg:58.50ms
step:2200/2330 train_time:128693ms step_avg:58.50ms
step:2201/2330 train_time:128750ms step_avg:58.50ms
step:2202/2330 train_time:128811ms step_avg:58.50ms
step:2203/2330 train_time:128868ms step_avg:58.50ms
step:2204/2330 train_time:128930ms step_avg:58.50ms
step:2205/2330 train_time:128988ms step_avg:58.50ms
step:2206/2330 train_time:129049ms step_avg:58.50ms
step:2207/2330 train_time:129106ms step_avg:58.50ms
step:2208/2330 train_time:129167ms step_avg:58.50ms
step:2209/2330 train_time:129225ms step_avg:58.50ms
step:2210/2330 train_time:129287ms step_avg:58.50ms
step:2211/2330 train_time:129346ms step_avg:58.50ms
step:2212/2330 train_time:129406ms step_avg:58.50ms
step:2213/2330 train_time:129464ms step_avg:58.50ms
step:2214/2330 train_time:129525ms step_avg:58.50ms
step:2215/2330 train_time:129584ms step_avg:58.50ms
step:2216/2330 train_time:129644ms step_avg:58.50ms
step:2217/2330 train_time:129702ms step_avg:58.50ms
step:2218/2330 train_time:129763ms step_avg:58.50ms
step:2219/2330 train_time:129819ms step_avg:58.50ms
step:2220/2330 train_time:129880ms step_avg:58.50ms
step:2221/2330 train_time:129937ms step_avg:58.50ms
step:2222/2330 train_time:129999ms step_avg:58.51ms
step:2223/2330 train_time:130056ms step_avg:58.50ms
step:2224/2330 train_time:130118ms step_avg:58.51ms
step:2225/2330 train_time:130174ms step_avg:58.51ms
step:2226/2330 train_time:130238ms step_avg:58.51ms
step:2227/2330 train_time:130295ms step_avg:58.51ms
step:2228/2330 train_time:130356ms step_avg:58.51ms
step:2229/2330 train_time:130413ms step_avg:58.51ms
step:2230/2330 train_time:130474ms step_avg:58.51ms
step:2231/2330 train_time:130531ms step_avg:58.51ms
step:2232/2330 train_time:130591ms step_avg:58.51ms
step:2233/2330 train_time:130649ms step_avg:58.51ms
step:2234/2330 train_time:130709ms step_avg:58.51ms
step:2235/2330 train_time:130766ms step_avg:58.51ms
step:2236/2330 train_time:130827ms step_avg:58.51ms
step:2237/2330 train_time:130885ms step_avg:58.51ms
step:2238/2330 train_time:130947ms step_avg:58.51ms
step:2239/2330 train_time:131004ms step_avg:58.51ms
step:2240/2330 train_time:131065ms step_avg:58.51ms
step:2241/2330 train_time:131124ms step_avg:58.51ms
step:2242/2330 train_time:131185ms step_avg:58.51ms
step:2243/2330 train_time:131244ms step_avg:58.51ms
step:2244/2330 train_time:131304ms step_avg:58.51ms
step:2245/2330 train_time:131361ms step_avg:58.51ms
step:2246/2330 train_time:131423ms step_avg:58.51ms
step:2247/2330 train_time:131480ms step_avg:58.51ms
step:2248/2330 train_time:131541ms step_avg:58.51ms
step:2249/2330 train_time:131599ms step_avg:58.51ms
step:2250/2330 train_time:131659ms step_avg:58.52ms
step:2250/2330 val_loss:3.7091 train_time:131740ms step_avg:58.55ms
step:2251/2330 train_time:131758ms step_avg:58.53ms
step:2252/2330 train_time:131779ms step_avg:58.52ms
step:2253/2330 train_time:131837ms step_avg:58.52ms
step:2254/2330 train_time:131901ms step_avg:58.52ms
step:2255/2330 train_time:131958ms step_avg:58.52ms
step:2256/2330 train_time:132020ms step_avg:58.52ms
step:2257/2330 train_time:132076ms step_avg:58.52ms
step:2258/2330 train_time:132138ms step_avg:58.52ms
step:2259/2330 train_time:132194ms step_avg:58.52ms
step:2260/2330 train_time:132255ms step_avg:58.52ms
step:2261/2330 train_time:132311ms step_avg:58.52ms
step:2262/2330 train_time:132371ms step_avg:58.52ms
step:2263/2330 train_time:132427ms step_avg:58.52ms
step:2264/2330 train_time:132487ms step_avg:58.52ms
step:2265/2330 train_time:132544ms step_avg:58.52ms
step:2266/2330 train_time:132605ms step_avg:58.52ms
step:2267/2330 train_time:132663ms step_avg:58.52ms
step:2268/2330 train_time:132725ms step_avg:58.52ms
step:2269/2330 train_time:132784ms step_avg:58.52ms
step:2270/2330 train_time:132845ms step_avg:58.52ms
step:2271/2330 train_time:132905ms step_avg:58.52ms
step:2272/2330 train_time:132967ms step_avg:58.52ms
step:2273/2330 train_time:133025ms step_avg:58.52ms
step:2274/2330 train_time:133086ms step_avg:58.53ms
step:2275/2330 train_time:133143ms step_avg:58.52ms
step:2276/2330 train_time:133204ms step_avg:58.53ms
step:2277/2330 train_time:133261ms step_avg:58.52ms
step:2278/2330 train_time:133321ms step_avg:58.53ms
step:2279/2330 train_time:133378ms step_avg:58.52ms
step:2280/2330 train_time:133439ms step_avg:58.53ms
step:2281/2330 train_time:133495ms step_avg:58.52ms
step:2282/2330 train_time:133556ms step_avg:58.53ms
step:2283/2330 train_time:133613ms step_avg:58.53ms
step:2284/2330 train_time:133673ms step_avg:58.53ms
step:2285/2330 train_time:133730ms step_avg:58.53ms
step:2286/2330 train_time:133791ms step_avg:58.53ms
step:2287/2330 train_time:133849ms step_avg:58.53ms
step:2288/2330 train_time:133912ms step_avg:58.53ms
step:2289/2330 train_time:133970ms step_avg:58.53ms
step:2290/2330 train_time:134030ms step_avg:58.53ms
step:2291/2330 train_time:134088ms step_avg:58.53ms
step:2292/2330 train_time:134149ms step_avg:58.53ms
step:2293/2330 train_time:134207ms step_avg:58.53ms
step:2294/2330 train_time:134268ms step_avg:58.53ms
step:2295/2330 train_time:134326ms step_avg:58.53ms
step:2296/2330 train_time:134387ms step_avg:58.53ms
step:2297/2330 train_time:134445ms step_avg:58.53ms
step:2298/2330 train_time:134506ms step_avg:58.53ms
step:2299/2330 train_time:134564ms step_avg:58.53ms
step:2300/2330 train_time:134625ms step_avg:58.53ms
step:2301/2330 train_time:134683ms step_avg:58.53ms
step:2302/2330 train_time:134744ms step_avg:58.53ms
step:2303/2330 train_time:134801ms step_avg:58.53ms
step:2304/2330 train_time:134863ms step_avg:58.53ms
step:2305/2330 train_time:134920ms step_avg:58.53ms
step:2306/2330 train_time:134983ms step_avg:58.54ms
step:2307/2330 train_time:135040ms step_avg:58.53ms
step:2308/2330 train_time:135102ms step_avg:58.54ms
step:2309/2330 train_time:135159ms step_avg:58.54ms
step:2310/2330 train_time:135220ms step_avg:58.54ms
step:2311/2330 train_time:135277ms step_avg:58.54ms
step:2312/2330 train_time:135338ms step_avg:58.54ms
step:2313/2330 train_time:135396ms step_avg:58.54ms
step:2314/2330 train_time:135457ms step_avg:58.54ms
step:2315/2330 train_time:135514ms step_avg:58.54ms
step:2316/2330 train_time:135574ms step_avg:58.54ms
step:2317/2330 train_time:135631ms step_avg:58.54ms
step:2318/2330 train_time:135691ms step_avg:58.54ms
step:2319/2330 train_time:135748ms step_avg:58.54ms
step:2320/2330 train_time:135810ms step_avg:58.54ms
step:2321/2330 train_time:135869ms step_avg:58.54ms
step:2322/2330 train_time:135929ms step_avg:58.54ms
step:2323/2330 train_time:135986ms step_avg:58.54ms
step:2324/2330 train_time:136047ms step_avg:58.54ms
step:2325/2330 train_time:136106ms step_avg:58.54ms
step:2326/2330 train_time:136166ms step_avg:58.54ms
step:2327/2330 train_time:136226ms step_avg:58.54ms
step:2328/2330 train_time:136286ms step_avg:58.54ms
step:2329/2330 train_time:136344ms step_avg:58.54ms
step:2330/2330 train_time:136404ms step_avg:58.54ms
step:2330/2330 val_loss:3.6937 train_time:136487ms step_avg:58.58ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
