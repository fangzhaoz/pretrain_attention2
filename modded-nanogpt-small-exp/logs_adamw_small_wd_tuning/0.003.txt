import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:44:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:81ms step_avg:80.55ms
step:2/2330 train_time:192ms step_avg:95.90ms
step:3/2330 train_time:209ms step_avg:69.68ms
step:4/2330 train_time:229ms step_avg:57.15ms
step:5/2330 train_time:283ms step_avg:56.50ms
step:6/2330 train_time:341ms step_avg:56.85ms
step:7/2330 train_time:396ms step_avg:56.61ms
step:8/2330 train_time:454ms step_avg:56.80ms
step:9/2330 train_time:510ms step_avg:56.66ms
step:10/2330 train_time:568ms step_avg:56.81ms
step:11/2330 train_time:624ms step_avg:56.69ms
step:12/2330 train_time:682ms step_avg:56.81ms
step:13/2330 train_time:738ms step_avg:56.73ms
step:14/2330 train_time:796ms step_avg:56.82ms
step:15/2330 train_time:851ms step_avg:56.72ms
step:16/2330 train_time:909ms step_avg:56.81ms
step:17/2330 train_time:964ms step_avg:56.72ms
step:18/2330 train_time:1023ms step_avg:56.81ms
step:19/2330 train_time:1080ms step_avg:56.84ms
step:20/2330 train_time:1143ms step_avg:57.14ms
step:21/2330 train_time:1201ms step_avg:57.18ms
step:22/2330 train_time:1261ms step_avg:57.34ms
step:23/2330 train_time:1318ms step_avg:57.28ms
step:24/2330 train_time:1377ms step_avg:57.37ms
step:25/2330 train_time:1433ms step_avg:57.32ms
step:26/2330 train_time:1492ms step_avg:57.40ms
step:27/2330 train_time:1547ms step_avg:57.31ms
step:28/2330 train_time:1606ms step_avg:57.37ms
step:29/2330 train_time:1663ms step_avg:57.33ms
step:30/2330 train_time:1721ms step_avg:57.37ms
step:31/2330 train_time:1776ms step_avg:57.30ms
step:32/2330 train_time:1835ms step_avg:57.35ms
step:33/2330 train_time:1891ms step_avg:57.30ms
step:34/2330 train_time:1949ms step_avg:57.32ms
step:35/2330 train_time:2004ms step_avg:57.27ms
step:36/2330 train_time:2064ms step_avg:57.33ms
step:37/2330 train_time:2121ms step_avg:57.34ms
step:38/2330 train_time:2182ms step_avg:57.42ms
step:39/2330 train_time:2239ms step_avg:57.42ms
step:40/2330 train_time:2298ms step_avg:57.46ms
step:41/2330 train_time:2354ms step_avg:57.42ms
step:42/2330 train_time:2414ms step_avg:57.47ms
step:43/2330 train_time:2470ms step_avg:57.43ms
step:44/2330 train_time:2529ms step_avg:57.47ms
step:45/2330 train_time:2584ms step_avg:57.42ms
step:46/2330 train_time:2643ms step_avg:57.46ms
step:47/2330 train_time:2698ms step_avg:57.41ms
step:48/2330 train_time:2758ms step_avg:57.46ms
step:49/2330 train_time:2813ms step_avg:57.40ms
step:50/2330 train_time:2872ms step_avg:57.43ms
step:51/2330 train_time:2927ms step_avg:57.40ms
step:52/2330 train_time:2986ms step_avg:57.42ms
step:53/2330 train_time:3042ms step_avg:57.40ms
step:54/2330 train_time:3101ms step_avg:57.43ms
step:55/2330 train_time:3159ms step_avg:57.43ms
step:56/2330 train_time:3218ms step_avg:57.47ms
step:57/2330 train_time:3275ms step_avg:57.45ms
step:58/2330 train_time:3335ms step_avg:57.49ms
step:59/2330 train_time:3390ms step_avg:57.46ms
step:60/2330 train_time:3449ms step_avg:57.48ms
step:61/2330 train_time:3504ms step_avg:57.45ms
step:62/2330 train_time:3563ms step_avg:57.47ms
step:63/2330 train_time:3618ms step_avg:57.44ms
step:64/2330 train_time:3677ms step_avg:57.46ms
step:65/2330 train_time:3733ms step_avg:57.44ms
step:66/2330 train_time:3792ms step_avg:57.46ms
step:67/2330 train_time:3848ms step_avg:57.43ms
step:68/2330 train_time:3906ms step_avg:57.45ms
step:69/2330 train_time:3962ms step_avg:57.42ms
step:70/2330 train_time:4021ms step_avg:57.44ms
step:71/2330 train_time:4077ms step_avg:57.42ms
step:72/2330 train_time:4136ms step_avg:57.44ms
step:73/2330 train_time:4191ms step_avg:57.41ms
step:74/2330 train_time:4252ms step_avg:57.46ms
step:75/2330 train_time:4308ms step_avg:57.44ms
step:76/2330 train_time:4368ms step_avg:57.47ms
step:77/2330 train_time:4423ms step_avg:57.45ms
step:78/2330 train_time:4483ms step_avg:57.47ms
step:79/2330 train_time:4539ms step_avg:57.45ms
step:80/2330 train_time:4598ms step_avg:57.48ms
step:81/2330 train_time:4654ms step_avg:57.45ms
step:82/2330 train_time:4713ms step_avg:57.47ms
step:83/2330 train_time:4769ms step_avg:57.45ms
step:84/2330 train_time:4827ms step_avg:57.47ms
step:85/2330 train_time:4883ms step_avg:57.44ms
step:86/2330 train_time:4941ms step_avg:57.46ms
step:87/2330 train_time:4997ms step_avg:57.43ms
step:88/2330 train_time:5057ms step_avg:57.47ms
step:89/2330 train_time:5113ms step_avg:57.45ms
step:90/2330 train_time:5173ms step_avg:57.47ms
step:91/2330 train_time:5229ms step_avg:57.46ms
step:92/2330 train_time:5288ms step_avg:57.48ms
step:93/2330 train_time:5344ms step_avg:57.47ms
step:94/2330 train_time:5403ms step_avg:57.48ms
step:95/2330 train_time:5459ms step_avg:57.47ms
step:96/2330 train_time:5518ms step_avg:57.48ms
step:97/2330 train_time:5574ms step_avg:57.47ms
step:98/2330 train_time:5633ms step_avg:57.48ms
step:99/2330 train_time:5689ms step_avg:57.46ms
step:100/2330 train_time:5748ms step_avg:57.48ms
step:101/2330 train_time:5803ms step_avg:57.46ms
step:102/2330 train_time:5862ms step_avg:57.48ms
step:103/2330 train_time:5918ms step_avg:57.46ms
step:104/2330 train_time:5977ms step_avg:57.47ms
step:105/2330 train_time:6033ms step_avg:57.46ms
step:106/2330 train_time:6092ms step_avg:57.47ms
step:107/2330 train_time:6148ms step_avg:57.46ms
step:108/2330 train_time:6207ms step_avg:57.47ms
step:109/2330 train_time:6264ms step_avg:57.46ms
step:110/2330 train_time:6323ms step_avg:57.48ms
step:111/2330 train_time:6379ms step_avg:57.47ms
step:112/2330 train_time:6438ms step_avg:57.48ms
step:113/2330 train_time:6494ms step_avg:57.47ms
step:114/2330 train_time:6552ms step_avg:57.47ms
step:115/2330 train_time:6608ms step_avg:57.46ms
step:116/2330 train_time:6668ms step_avg:57.48ms
step:117/2330 train_time:6723ms step_avg:57.46ms
step:118/2330 train_time:6782ms step_avg:57.48ms
step:119/2330 train_time:6838ms step_avg:57.46ms
step:120/2330 train_time:6898ms step_avg:57.48ms
step:121/2330 train_time:6953ms step_avg:57.46ms
step:122/2330 train_time:7013ms step_avg:57.48ms
step:123/2330 train_time:7068ms step_avg:57.46ms
step:124/2330 train_time:7127ms step_avg:57.48ms
step:125/2330 train_time:7183ms step_avg:57.47ms
step:126/2330 train_time:7242ms step_avg:57.48ms
step:127/2330 train_time:7298ms step_avg:57.47ms
step:128/2330 train_time:7358ms step_avg:57.48ms
step:129/2330 train_time:7414ms step_avg:57.47ms
step:130/2330 train_time:7473ms step_avg:57.49ms
step:131/2330 train_time:7530ms step_avg:57.48ms
step:132/2330 train_time:7589ms step_avg:57.49ms
step:133/2330 train_time:7644ms step_avg:57.47ms
step:134/2330 train_time:7703ms step_avg:57.49ms
step:135/2330 train_time:7759ms step_avg:57.47ms
step:136/2330 train_time:7818ms step_avg:57.48ms
step:137/2330 train_time:7873ms step_avg:57.47ms
step:138/2330 train_time:7932ms step_avg:57.48ms
step:139/2330 train_time:7989ms step_avg:57.47ms
step:140/2330 train_time:8048ms step_avg:57.48ms
step:141/2330 train_time:8104ms step_avg:57.47ms
step:142/2330 train_time:8163ms step_avg:57.48ms
step:143/2330 train_time:8219ms step_avg:57.47ms
step:144/2330 train_time:8278ms step_avg:57.49ms
step:145/2330 train_time:8334ms step_avg:57.47ms
step:146/2330 train_time:8393ms step_avg:57.49ms
step:147/2330 train_time:8449ms step_avg:57.48ms
step:148/2330 train_time:8508ms step_avg:57.49ms
step:149/2330 train_time:8564ms step_avg:57.48ms
step:150/2330 train_time:8623ms step_avg:57.49ms
step:151/2330 train_time:8679ms step_avg:57.48ms
step:152/2330 train_time:8737ms step_avg:57.48ms
step:153/2330 train_time:8793ms step_avg:57.47ms
step:154/2330 train_time:8851ms step_avg:57.48ms
step:155/2330 train_time:8907ms step_avg:57.46ms
step:156/2330 train_time:8966ms step_avg:57.48ms
step:157/2330 train_time:9022ms step_avg:57.47ms
step:158/2330 train_time:9081ms step_avg:57.47ms
step:159/2330 train_time:9137ms step_avg:57.47ms
step:160/2330 train_time:9196ms step_avg:57.47ms
step:161/2330 train_time:9252ms step_avg:57.46ms
step:162/2330 train_time:9311ms step_avg:57.47ms
step:163/2330 train_time:9366ms step_avg:57.46ms
step:164/2330 train_time:9426ms step_avg:57.47ms
step:165/2330 train_time:9481ms step_avg:57.46ms
step:166/2330 train_time:9541ms step_avg:57.48ms
step:167/2330 train_time:9596ms step_avg:57.46ms
step:168/2330 train_time:9656ms step_avg:57.48ms
step:169/2330 train_time:9712ms step_avg:57.47ms
step:170/2330 train_time:9771ms step_avg:57.48ms
step:171/2330 train_time:9827ms step_avg:57.47ms
step:172/2330 train_time:9886ms step_avg:57.48ms
step:173/2330 train_time:9942ms step_avg:57.47ms
step:174/2330 train_time:10001ms step_avg:57.48ms
step:175/2330 train_time:10057ms step_avg:57.47ms
step:176/2330 train_time:10117ms step_avg:57.48ms
step:177/2330 train_time:10172ms step_avg:57.47ms
step:178/2330 train_time:10232ms step_avg:57.48ms
step:179/2330 train_time:10288ms step_avg:57.48ms
step:180/2330 train_time:10346ms step_avg:57.48ms
step:181/2330 train_time:10402ms step_avg:57.47ms
step:182/2330 train_time:10462ms step_avg:57.48ms
step:183/2330 train_time:10518ms step_avg:57.47ms
step:184/2330 train_time:10576ms step_avg:57.48ms
step:185/2330 train_time:10632ms step_avg:57.47ms
step:186/2330 train_time:10691ms step_avg:57.48ms
step:187/2330 train_time:10747ms step_avg:57.47ms
step:188/2330 train_time:10806ms step_avg:57.48ms
step:189/2330 train_time:10862ms step_avg:57.47ms
step:190/2330 train_time:10922ms step_avg:57.48ms
step:191/2330 train_time:10977ms step_avg:57.47ms
step:192/2330 train_time:11037ms step_avg:57.48ms
step:193/2330 train_time:11093ms step_avg:57.47ms
step:194/2330 train_time:11152ms step_avg:57.48ms
step:195/2330 train_time:11207ms step_avg:57.47ms
step:196/2330 train_time:11266ms step_avg:57.48ms
step:197/2330 train_time:11322ms step_avg:57.47ms
step:198/2330 train_time:11381ms step_avg:57.48ms
step:199/2330 train_time:11437ms step_avg:57.47ms
step:200/2330 train_time:11498ms step_avg:57.49ms
step:201/2330 train_time:11553ms step_avg:57.48ms
step:202/2330 train_time:11613ms step_avg:57.49ms
step:203/2330 train_time:11669ms step_avg:57.48ms
step:204/2330 train_time:11728ms step_avg:57.49ms
step:205/2330 train_time:11783ms step_avg:57.48ms
step:206/2330 train_time:11842ms step_avg:57.49ms
step:207/2330 train_time:11898ms step_avg:57.48ms
step:208/2330 train_time:11957ms step_avg:57.49ms
step:209/2330 train_time:12013ms step_avg:57.48ms
step:210/2330 train_time:12072ms step_avg:57.49ms
step:211/2330 train_time:12128ms step_avg:57.48ms
step:212/2330 train_time:12187ms step_avg:57.49ms
step:213/2330 train_time:12244ms step_avg:57.48ms
step:214/2330 train_time:12302ms step_avg:57.49ms
step:215/2330 train_time:12359ms step_avg:57.48ms
step:216/2330 train_time:12418ms step_avg:57.49ms
step:217/2330 train_time:12474ms step_avg:57.49ms
step:218/2330 train_time:12533ms step_avg:57.49ms
step:219/2330 train_time:12589ms step_avg:57.49ms
step:220/2330 train_time:12648ms step_avg:57.49ms
step:221/2330 train_time:12703ms step_avg:57.48ms
step:222/2330 train_time:12763ms step_avg:57.49ms
step:223/2330 train_time:12819ms step_avg:57.48ms
step:224/2330 train_time:12878ms step_avg:57.49ms
step:225/2330 train_time:12934ms step_avg:57.48ms
step:226/2330 train_time:12993ms step_avg:57.49ms
step:227/2330 train_time:13049ms step_avg:57.48ms
step:228/2330 train_time:13109ms step_avg:57.49ms
step:229/2330 train_time:13165ms step_avg:57.49ms
step:230/2330 train_time:13223ms step_avg:57.49ms
step:231/2330 train_time:13280ms step_avg:57.49ms
step:232/2330 train_time:13340ms step_avg:57.50ms
step:233/2330 train_time:13396ms step_avg:57.49ms
step:234/2330 train_time:13456ms step_avg:57.50ms
step:235/2330 train_time:13511ms step_avg:57.49ms
step:236/2330 train_time:13570ms step_avg:57.50ms
step:237/2330 train_time:13626ms step_avg:57.50ms
step:238/2330 train_time:13685ms step_avg:57.50ms
step:239/2330 train_time:13742ms step_avg:57.50ms
step:240/2330 train_time:13800ms step_avg:57.50ms
step:241/2330 train_time:13856ms step_avg:57.49ms
step:242/2330 train_time:13915ms step_avg:57.50ms
step:243/2330 train_time:13971ms step_avg:57.49ms
step:244/2330 train_time:14030ms step_avg:57.50ms
step:245/2330 train_time:14087ms step_avg:57.50ms
step:246/2330 train_time:14145ms step_avg:57.50ms
step:247/2330 train_time:14201ms step_avg:57.49ms
step:248/2330 train_time:14260ms step_avg:57.50ms
step:249/2330 train_time:14315ms step_avg:57.49ms
step:250/2330 train_time:14376ms step_avg:57.50ms
step:250/2330 val_loss:4.8903 train_time:14455ms step_avg:57.82ms
step:251/2330 train_time:14473ms step_avg:57.66ms
step:252/2330 train_time:14493ms step_avg:57.51ms
step:253/2330 train_time:14547ms step_avg:57.50ms
step:254/2330 train_time:14611ms step_avg:57.52ms
step:255/2330 train_time:14666ms step_avg:57.51ms
step:256/2330 train_time:14731ms step_avg:57.54ms
step:257/2330 train_time:14786ms step_avg:57.53ms
step:258/2330 train_time:14846ms step_avg:57.54ms
step:259/2330 train_time:14902ms step_avg:57.54ms
step:260/2330 train_time:14961ms step_avg:57.54ms
step:261/2330 train_time:15017ms step_avg:57.54ms
step:262/2330 train_time:15075ms step_avg:57.54ms
step:263/2330 train_time:15130ms step_avg:57.53ms
step:264/2330 train_time:15189ms step_avg:57.53ms
step:265/2330 train_time:15244ms step_avg:57.53ms
step:266/2330 train_time:15303ms step_avg:57.53ms
step:267/2330 train_time:15359ms step_avg:57.53ms
step:268/2330 train_time:15419ms step_avg:57.53ms
step:269/2330 train_time:15476ms step_avg:57.53ms
step:270/2330 train_time:15535ms step_avg:57.54ms
step:271/2330 train_time:15592ms step_avg:57.53ms
step:272/2330 train_time:15652ms step_avg:57.55ms
step:273/2330 train_time:15708ms step_avg:57.54ms
step:274/2330 train_time:15770ms step_avg:57.55ms
step:275/2330 train_time:15826ms step_avg:57.55ms
step:276/2330 train_time:15886ms step_avg:57.56ms
step:277/2330 train_time:15941ms step_avg:57.55ms
step:278/2330 train_time:16001ms step_avg:57.56ms
step:279/2330 train_time:16057ms step_avg:57.55ms
step:280/2330 train_time:16115ms step_avg:57.55ms
step:281/2330 train_time:16170ms step_avg:57.55ms
step:282/2330 train_time:16229ms step_avg:57.55ms
step:283/2330 train_time:16284ms step_avg:57.54ms
step:284/2330 train_time:16343ms step_avg:57.55ms
step:285/2330 train_time:16399ms step_avg:57.54ms
step:286/2330 train_time:16459ms step_avg:57.55ms
step:287/2330 train_time:16515ms step_avg:57.54ms
step:288/2330 train_time:16575ms step_avg:57.55ms
step:289/2330 train_time:16631ms step_avg:57.55ms
step:290/2330 train_time:16692ms step_avg:57.56ms
step:291/2330 train_time:16748ms step_avg:57.55ms
step:292/2330 train_time:16809ms step_avg:57.56ms
step:293/2330 train_time:16864ms step_avg:57.56ms
step:294/2330 train_time:16923ms step_avg:57.56ms
step:295/2330 train_time:16979ms step_avg:57.56ms
step:296/2330 train_time:17038ms step_avg:57.56ms
step:297/2330 train_time:17094ms step_avg:57.56ms
step:298/2330 train_time:17153ms step_avg:57.56ms
step:299/2330 train_time:17208ms step_avg:57.55ms
step:300/2330 train_time:17267ms step_avg:57.56ms
step:301/2330 train_time:17322ms step_avg:57.55ms
step:302/2330 train_time:17381ms step_avg:57.55ms
step:303/2330 train_time:17438ms step_avg:57.55ms
step:304/2330 train_time:17497ms step_avg:57.55ms
step:305/2330 train_time:17553ms step_avg:57.55ms
step:306/2330 train_time:17613ms step_avg:57.56ms
step:307/2330 train_time:17669ms step_avg:57.55ms
step:308/2330 train_time:17728ms step_avg:57.56ms
step:309/2330 train_time:17784ms step_avg:57.55ms
step:310/2330 train_time:17844ms step_avg:57.56ms
step:311/2330 train_time:17899ms step_avg:57.55ms
step:312/2330 train_time:17959ms step_avg:57.56ms
step:313/2330 train_time:18016ms step_avg:57.56ms
step:314/2330 train_time:18074ms step_avg:57.56ms
step:315/2330 train_time:18130ms step_avg:57.56ms
step:316/2330 train_time:18188ms step_avg:57.56ms
step:317/2330 train_time:18244ms step_avg:57.55ms
step:318/2330 train_time:18303ms step_avg:57.56ms
step:319/2330 train_time:18359ms step_avg:57.55ms
step:320/2330 train_time:18417ms step_avg:57.55ms
step:321/2330 train_time:18474ms step_avg:57.55ms
step:322/2330 train_time:18533ms step_avg:57.56ms
step:323/2330 train_time:18589ms step_avg:57.55ms
step:324/2330 train_time:18649ms step_avg:57.56ms
step:325/2330 train_time:18705ms step_avg:57.55ms
step:326/2330 train_time:18764ms step_avg:57.56ms
step:327/2330 train_time:18819ms step_avg:57.55ms
step:328/2330 train_time:18879ms step_avg:57.56ms
step:329/2330 train_time:18935ms step_avg:57.55ms
step:330/2330 train_time:18995ms step_avg:57.56ms
step:331/2330 train_time:19051ms step_avg:57.56ms
step:332/2330 train_time:19110ms step_avg:57.56ms
step:333/2330 train_time:19166ms step_avg:57.56ms
step:334/2330 train_time:19225ms step_avg:57.56ms
step:335/2330 train_time:19280ms step_avg:57.55ms
step:336/2330 train_time:19339ms step_avg:57.56ms
step:337/2330 train_time:19395ms step_avg:57.55ms
step:338/2330 train_time:19454ms step_avg:57.56ms
step:339/2330 train_time:19510ms step_avg:57.55ms
step:340/2330 train_time:19569ms step_avg:57.56ms
step:341/2330 train_time:19625ms step_avg:57.55ms
step:342/2330 train_time:19685ms step_avg:57.56ms
step:343/2330 train_time:19740ms step_avg:57.55ms
step:344/2330 train_time:19801ms step_avg:57.56ms
step:345/2330 train_time:19858ms step_avg:57.56ms
step:346/2330 train_time:19917ms step_avg:57.56ms
step:347/2330 train_time:19973ms step_avg:57.56ms
step:348/2330 train_time:20032ms step_avg:57.56ms
step:349/2330 train_time:20088ms step_avg:57.56ms
step:350/2330 train_time:20147ms step_avg:57.56ms
step:351/2330 train_time:20202ms step_avg:57.56ms
step:352/2330 train_time:20262ms step_avg:57.56ms
step:353/2330 train_time:20318ms step_avg:57.56ms
step:354/2330 train_time:20377ms step_avg:57.56ms
step:355/2330 train_time:20433ms step_avg:57.56ms
step:356/2330 train_time:20492ms step_avg:57.56ms
step:357/2330 train_time:20548ms step_avg:57.56ms
step:358/2330 train_time:20607ms step_avg:57.56ms
step:359/2330 train_time:20664ms step_avg:57.56ms
step:360/2330 train_time:20722ms step_avg:57.56ms
step:361/2330 train_time:20778ms step_avg:57.56ms
step:362/2330 train_time:20839ms step_avg:57.57ms
step:363/2330 train_time:20895ms step_avg:57.56ms
step:364/2330 train_time:20954ms step_avg:57.57ms
step:365/2330 train_time:21010ms step_avg:57.56ms
step:366/2330 train_time:21069ms step_avg:57.57ms
step:367/2330 train_time:21125ms step_avg:57.56ms
step:368/2330 train_time:21184ms step_avg:57.56ms
step:369/2330 train_time:21239ms step_avg:57.56ms
step:370/2330 train_time:21298ms step_avg:57.56ms
step:371/2330 train_time:21354ms step_avg:57.56ms
step:372/2330 train_time:21414ms step_avg:57.56ms
step:373/2330 train_time:21470ms step_avg:57.56ms
step:374/2330 train_time:21530ms step_avg:57.57ms
step:375/2330 train_time:21585ms step_avg:57.56ms
step:376/2330 train_time:21645ms step_avg:57.57ms
step:377/2330 train_time:21702ms step_avg:57.56ms
step:378/2330 train_time:21760ms step_avg:57.57ms
step:379/2330 train_time:21817ms step_avg:57.56ms
step:380/2330 train_time:21875ms step_avg:57.57ms
step:381/2330 train_time:21932ms step_avg:57.56ms
step:382/2330 train_time:21990ms step_avg:57.57ms
step:383/2330 train_time:22047ms step_avg:57.56ms
step:384/2330 train_time:22106ms step_avg:57.57ms
step:385/2330 train_time:22161ms step_avg:57.56ms
step:386/2330 train_time:22221ms step_avg:57.57ms
step:387/2330 train_time:22277ms step_avg:57.56ms
step:388/2330 train_time:22336ms step_avg:57.57ms
step:389/2330 train_time:22393ms step_avg:57.56ms
step:390/2330 train_time:22452ms step_avg:57.57ms
step:391/2330 train_time:22508ms step_avg:57.56ms
step:392/2330 train_time:22567ms step_avg:57.57ms
step:393/2330 train_time:22623ms step_avg:57.56ms
step:394/2330 train_time:22682ms step_avg:57.57ms
step:395/2330 train_time:22738ms step_avg:57.56ms
step:396/2330 train_time:22797ms step_avg:57.57ms
step:397/2330 train_time:22853ms step_avg:57.56ms
step:398/2330 train_time:22911ms step_avg:57.57ms
step:399/2330 train_time:22968ms step_avg:57.56ms
step:400/2330 train_time:23027ms step_avg:57.57ms
step:401/2330 train_time:23084ms step_avg:57.57ms
step:402/2330 train_time:23142ms step_avg:57.57ms
step:403/2330 train_time:23198ms step_avg:57.56ms
step:404/2330 train_time:23257ms step_avg:57.57ms
step:405/2330 train_time:23313ms step_avg:57.56ms
step:406/2330 train_time:23373ms step_avg:57.57ms
step:407/2330 train_time:23428ms step_avg:57.56ms
step:408/2330 train_time:23488ms step_avg:57.57ms
step:409/2330 train_time:23544ms step_avg:57.57ms
step:410/2330 train_time:23603ms step_avg:57.57ms
step:411/2330 train_time:23658ms step_avg:57.56ms
step:412/2330 train_time:23718ms step_avg:57.57ms
step:413/2330 train_time:23775ms step_avg:57.57ms
step:414/2330 train_time:23834ms step_avg:57.57ms
step:415/2330 train_time:23890ms step_avg:57.57ms
step:416/2330 train_time:23949ms step_avg:57.57ms
step:417/2330 train_time:24006ms step_avg:57.57ms
step:418/2330 train_time:24065ms step_avg:57.57ms
step:419/2330 train_time:24120ms step_avg:57.57ms
step:420/2330 train_time:24179ms step_avg:57.57ms
step:421/2330 train_time:24235ms step_avg:57.57ms
step:422/2330 train_time:24294ms step_avg:57.57ms
step:423/2330 train_time:24350ms step_avg:57.57ms
step:424/2330 train_time:24410ms step_avg:57.57ms
step:425/2330 train_time:24466ms step_avg:57.57ms
step:426/2330 train_time:24525ms step_avg:57.57ms
step:427/2330 train_time:24581ms step_avg:57.57ms
step:428/2330 train_time:24640ms step_avg:57.57ms
step:429/2330 train_time:24696ms step_avg:57.57ms
step:430/2330 train_time:24756ms step_avg:57.57ms
step:431/2330 train_time:24813ms step_avg:57.57ms
step:432/2330 train_time:24872ms step_avg:57.57ms
step:433/2330 train_time:24929ms step_avg:57.57ms
step:434/2330 train_time:24987ms step_avg:57.57ms
step:435/2330 train_time:25043ms step_avg:57.57ms
step:436/2330 train_time:25102ms step_avg:57.57ms
step:437/2330 train_time:25158ms step_avg:57.57ms
step:438/2330 train_time:25218ms step_avg:57.57ms
step:439/2330 train_time:25275ms step_avg:57.57ms
step:440/2330 train_time:25334ms step_avg:57.58ms
step:441/2330 train_time:25390ms step_avg:57.57ms
step:442/2330 train_time:25448ms step_avg:57.58ms
step:443/2330 train_time:25504ms step_avg:57.57ms
step:444/2330 train_time:25564ms step_avg:57.58ms
step:445/2330 train_time:25619ms step_avg:57.57ms
step:446/2330 train_time:25680ms step_avg:57.58ms
step:447/2330 train_time:25736ms step_avg:57.58ms
step:448/2330 train_time:25795ms step_avg:57.58ms
step:449/2330 train_time:25851ms step_avg:57.58ms
step:450/2330 train_time:25910ms step_avg:57.58ms
step:451/2330 train_time:25966ms step_avg:57.58ms
step:452/2330 train_time:26025ms step_avg:57.58ms
step:453/2330 train_time:26081ms step_avg:57.57ms
step:454/2330 train_time:26139ms step_avg:57.58ms
step:455/2330 train_time:26196ms step_avg:57.57ms
step:456/2330 train_time:26255ms step_avg:57.58ms
step:457/2330 train_time:26311ms step_avg:57.57ms
step:458/2330 train_time:26370ms step_avg:57.58ms
step:459/2330 train_time:26427ms step_avg:57.57ms
step:460/2330 train_time:26485ms step_avg:57.58ms
step:461/2330 train_time:26540ms step_avg:57.57ms
step:462/2330 train_time:26600ms step_avg:57.58ms
step:463/2330 train_time:26656ms step_avg:57.57ms
step:464/2330 train_time:26716ms step_avg:57.58ms
step:465/2330 train_time:26771ms step_avg:57.57ms
step:466/2330 train_time:26832ms step_avg:57.58ms
step:467/2330 train_time:26887ms step_avg:57.57ms
step:468/2330 train_time:26947ms step_avg:57.58ms
step:469/2330 train_time:27003ms step_avg:57.57ms
step:470/2330 train_time:27063ms step_avg:57.58ms
step:471/2330 train_time:27119ms step_avg:57.58ms
step:472/2330 train_time:27178ms step_avg:57.58ms
step:473/2330 train_time:27233ms step_avg:57.58ms
step:474/2330 train_time:27293ms step_avg:57.58ms
step:475/2330 train_time:27349ms step_avg:57.58ms
step:476/2330 train_time:27408ms step_avg:57.58ms
step:477/2330 train_time:27464ms step_avg:57.58ms
step:478/2330 train_time:27524ms step_avg:57.58ms
step:479/2330 train_time:27579ms step_avg:57.58ms
step:480/2330 train_time:27640ms step_avg:57.58ms
step:481/2330 train_time:27696ms step_avg:57.58ms
step:482/2330 train_time:27756ms step_avg:57.58ms
step:483/2330 train_time:27813ms step_avg:57.58ms
step:484/2330 train_time:27871ms step_avg:57.59ms
step:485/2330 train_time:27928ms step_avg:57.58ms
step:486/2330 train_time:27987ms step_avg:57.59ms
step:487/2330 train_time:28042ms step_avg:57.58ms
step:488/2330 train_time:28103ms step_avg:57.59ms
step:489/2330 train_time:28159ms step_avg:57.59ms
step:490/2330 train_time:28218ms step_avg:57.59ms
step:491/2330 train_time:28274ms step_avg:57.58ms
step:492/2330 train_time:28334ms step_avg:57.59ms
step:493/2330 train_time:28390ms step_avg:57.59ms
step:494/2330 train_time:28449ms step_avg:57.59ms
step:495/2330 train_time:28504ms step_avg:57.58ms
step:496/2330 train_time:28565ms step_avg:57.59ms
step:497/2330 train_time:28620ms step_avg:57.59ms
step:498/2330 train_time:28680ms step_avg:57.59ms
step:499/2330 train_time:28737ms step_avg:57.59ms
step:500/2330 train_time:28796ms step_avg:57.59ms
step:500/2330 val_loss:4.4077 train_time:28877ms step_avg:57.75ms
step:501/2330 train_time:28896ms step_avg:57.68ms
step:502/2330 train_time:28915ms step_avg:57.60ms
step:503/2330 train_time:28972ms step_avg:57.60ms
step:504/2330 train_time:29040ms step_avg:57.62ms
step:505/2330 train_time:29097ms step_avg:57.62ms
step:506/2330 train_time:29157ms step_avg:57.62ms
step:507/2330 train_time:29213ms step_avg:57.62ms
step:508/2330 train_time:29273ms step_avg:57.62ms
step:509/2330 train_time:29328ms step_avg:57.62ms
step:510/2330 train_time:29387ms step_avg:57.62ms
step:511/2330 train_time:29443ms step_avg:57.62ms
step:512/2330 train_time:29501ms step_avg:57.62ms
step:513/2330 train_time:29557ms step_avg:57.62ms
step:514/2330 train_time:29615ms step_avg:57.62ms
step:515/2330 train_time:29671ms step_avg:57.61ms
step:516/2330 train_time:29729ms step_avg:57.62ms
step:517/2330 train_time:29785ms step_avg:57.61ms
step:518/2330 train_time:29844ms step_avg:57.61ms
step:519/2330 train_time:29900ms step_avg:57.61ms
step:520/2330 train_time:29962ms step_avg:57.62ms
step:521/2330 train_time:30018ms step_avg:57.62ms
step:522/2330 train_time:30080ms step_avg:57.62ms
step:523/2330 train_time:30136ms step_avg:57.62ms
step:524/2330 train_time:30196ms step_avg:57.63ms
step:525/2330 train_time:30252ms step_avg:57.62ms
step:526/2330 train_time:30311ms step_avg:57.63ms
step:527/2330 train_time:30367ms step_avg:57.62ms
step:528/2330 train_time:30426ms step_avg:57.63ms
step:529/2330 train_time:30482ms step_avg:57.62ms
step:530/2330 train_time:30540ms step_avg:57.62ms
step:531/2330 train_time:30596ms step_avg:57.62ms
step:532/2330 train_time:30654ms step_avg:57.62ms
step:533/2330 train_time:30710ms step_avg:57.62ms
step:534/2330 train_time:30769ms step_avg:57.62ms
step:535/2330 train_time:30824ms step_avg:57.62ms
step:536/2330 train_time:30883ms step_avg:57.62ms
step:537/2330 train_time:30940ms step_avg:57.62ms
step:538/2330 train_time:31001ms step_avg:57.62ms
step:539/2330 train_time:31057ms step_avg:57.62ms
step:540/2330 train_time:31117ms step_avg:57.62ms
step:541/2330 train_time:31174ms step_avg:57.62ms
step:542/2330 train_time:31233ms step_avg:57.63ms
step:543/2330 train_time:31290ms step_avg:57.62ms
step:544/2330 train_time:31348ms step_avg:57.63ms
step:545/2330 train_time:31405ms step_avg:57.62ms
step:546/2330 train_time:31463ms step_avg:57.62ms
step:547/2330 train_time:31518ms step_avg:57.62ms
step:548/2330 train_time:31577ms step_avg:57.62ms
step:549/2330 train_time:31633ms step_avg:57.62ms
step:550/2330 train_time:31692ms step_avg:57.62ms
step:551/2330 train_time:31748ms step_avg:57.62ms
step:552/2330 train_time:31807ms step_avg:57.62ms
step:553/2330 train_time:31864ms step_avg:57.62ms
step:554/2330 train_time:31923ms step_avg:57.62ms
step:555/2330 train_time:31980ms step_avg:57.62ms
step:556/2330 train_time:32039ms step_avg:57.62ms
step:557/2330 train_time:32095ms step_avg:57.62ms
step:558/2330 train_time:32155ms step_avg:57.63ms
step:559/2330 train_time:32211ms step_avg:57.62ms
step:560/2330 train_time:32270ms step_avg:57.63ms
step:561/2330 train_time:32326ms step_avg:57.62ms
step:562/2330 train_time:32387ms step_avg:57.63ms
step:563/2330 train_time:32443ms step_avg:57.62ms
step:564/2330 train_time:32502ms step_avg:57.63ms
step:565/2330 train_time:32557ms step_avg:57.62ms
step:566/2330 train_time:32617ms step_avg:57.63ms
step:567/2330 train_time:32673ms step_avg:57.62ms
step:568/2330 train_time:32732ms step_avg:57.63ms
step:569/2330 train_time:32788ms step_avg:57.62ms
step:570/2330 train_time:32847ms step_avg:57.63ms
step:571/2330 train_time:32903ms step_avg:57.62ms
step:572/2330 train_time:32963ms step_avg:57.63ms
step:573/2330 train_time:33020ms step_avg:57.63ms
step:574/2330 train_time:33079ms step_avg:57.63ms
step:575/2330 train_time:33135ms step_avg:57.63ms
step:576/2330 train_time:33196ms step_avg:57.63ms
step:577/2330 train_time:33252ms step_avg:57.63ms
step:578/2330 train_time:33311ms step_avg:57.63ms
step:579/2330 train_time:33367ms step_avg:57.63ms
step:580/2330 train_time:33428ms step_avg:57.63ms
step:581/2330 train_time:33483ms step_avg:57.63ms
step:582/2330 train_time:33543ms step_avg:57.63ms
step:583/2330 train_time:33598ms step_avg:57.63ms
step:584/2330 train_time:33658ms step_avg:57.63ms
step:585/2330 train_time:33713ms step_avg:57.63ms
step:586/2330 train_time:33773ms step_avg:57.63ms
step:587/2330 train_time:33829ms step_avg:57.63ms
step:588/2330 train_time:33889ms step_avg:57.63ms
step:589/2330 train_time:33945ms step_avg:57.63ms
step:590/2330 train_time:34005ms step_avg:57.64ms
step:591/2330 train_time:34061ms step_avg:57.63ms
step:592/2330 train_time:34120ms step_avg:57.64ms
step:593/2330 train_time:34175ms step_avg:57.63ms
step:594/2330 train_time:34236ms step_avg:57.64ms
step:595/2330 train_time:34293ms step_avg:57.63ms
step:596/2330 train_time:34352ms step_avg:57.64ms
step:597/2330 train_time:34408ms step_avg:57.64ms
step:598/2330 train_time:34467ms step_avg:57.64ms
step:599/2330 train_time:34523ms step_avg:57.63ms
step:600/2330 train_time:34583ms step_avg:57.64ms
step:601/2330 train_time:34638ms step_avg:57.63ms
step:602/2330 train_time:34699ms step_avg:57.64ms
step:603/2330 train_time:34755ms step_avg:57.64ms
step:604/2330 train_time:34814ms step_avg:57.64ms
step:605/2330 train_time:34870ms step_avg:57.64ms
step:606/2330 train_time:34929ms step_avg:57.64ms
step:607/2330 train_time:34985ms step_avg:57.64ms
step:608/2330 train_time:35045ms step_avg:57.64ms
step:609/2330 train_time:35101ms step_avg:57.64ms
step:610/2330 train_time:35160ms step_avg:57.64ms
step:611/2330 train_time:35215ms step_avg:57.64ms
step:612/2330 train_time:35276ms step_avg:57.64ms
step:613/2330 train_time:35332ms step_avg:57.64ms
step:614/2330 train_time:35391ms step_avg:57.64ms
step:615/2330 train_time:35448ms step_avg:57.64ms
step:616/2330 train_time:35508ms step_avg:57.64ms
step:617/2330 train_time:35563ms step_avg:57.64ms
step:618/2330 train_time:35622ms step_avg:57.64ms
step:619/2330 train_time:35679ms step_avg:57.64ms
step:620/2330 train_time:35737ms step_avg:57.64ms
step:621/2330 train_time:35793ms step_avg:57.64ms
step:622/2330 train_time:35852ms step_avg:57.64ms
step:623/2330 train_time:35907ms step_avg:57.64ms
step:624/2330 train_time:35966ms step_avg:57.64ms
step:625/2330 train_time:36022ms step_avg:57.64ms
step:626/2330 train_time:36082ms step_avg:57.64ms
step:627/2330 train_time:36138ms step_avg:57.64ms
step:628/2330 train_time:36198ms step_avg:57.64ms
step:629/2330 train_time:36254ms step_avg:57.64ms
step:630/2330 train_time:36313ms step_avg:57.64ms
step:631/2330 train_time:36369ms step_avg:57.64ms
step:632/2330 train_time:36429ms step_avg:57.64ms
step:633/2330 train_time:36485ms step_avg:57.64ms
step:634/2330 train_time:36544ms step_avg:57.64ms
step:635/2330 train_time:36600ms step_avg:57.64ms
step:636/2330 train_time:36659ms step_avg:57.64ms
step:637/2330 train_time:36714ms step_avg:57.64ms
step:638/2330 train_time:36775ms step_avg:57.64ms
step:639/2330 train_time:36831ms step_avg:57.64ms
step:640/2330 train_time:36890ms step_avg:57.64ms
step:641/2330 train_time:36946ms step_avg:57.64ms
step:642/2330 train_time:37006ms step_avg:57.64ms
step:643/2330 train_time:37062ms step_avg:57.64ms
step:644/2330 train_time:37120ms step_avg:57.64ms
step:645/2330 train_time:37176ms step_avg:57.64ms
step:646/2330 train_time:37237ms step_avg:57.64ms
step:647/2330 train_time:37292ms step_avg:57.64ms
step:648/2330 train_time:37352ms step_avg:57.64ms
step:649/2330 train_time:37409ms step_avg:57.64ms
step:650/2330 train_time:37467ms step_avg:57.64ms
step:651/2330 train_time:37523ms step_avg:57.64ms
step:652/2330 train_time:37583ms step_avg:57.64ms
step:653/2330 train_time:37640ms step_avg:57.64ms
step:654/2330 train_time:37699ms step_avg:57.64ms
step:655/2330 train_time:37754ms step_avg:57.64ms
step:656/2330 train_time:37813ms step_avg:57.64ms
step:657/2330 train_time:37870ms step_avg:57.64ms
step:658/2330 train_time:37929ms step_avg:57.64ms
step:659/2330 train_time:37985ms step_avg:57.64ms
step:660/2330 train_time:38046ms step_avg:57.64ms
step:661/2330 train_time:38101ms step_avg:57.64ms
step:662/2330 train_time:38161ms step_avg:57.64ms
step:663/2330 train_time:38216ms step_avg:57.64ms
step:664/2330 train_time:38277ms step_avg:57.65ms
step:665/2330 train_time:38332ms step_avg:57.64ms
step:666/2330 train_time:38392ms step_avg:57.65ms
step:667/2330 train_time:38448ms step_avg:57.64ms
step:668/2330 train_time:38508ms step_avg:57.65ms
step:669/2330 train_time:38565ms step_avg:57.65ms
step:670/2330 train_time:38624ms step_avg:57.65ms
step:671/2330 train_time:38679ms step_avg:57.64ms
step:672/2330 train_time:38739ms step_avg:57.65ms
step:673/2330 train_time:38795ms step_avg:57.64ms
step:674/2330 train_time:38855ms step_avg:57.65ms
step:675/2330 train_time:38911ms step_avg:57.65ms
step:676/2330 train_time:38970ms step_avg:57.65ms
step:677/2330 train_time:39027ms step_avg:57.65ms
step:678/2330 train_time:39086ms step_avg:57.65ms
step:679/2330 train_time:39143ms step_avg:57.65ms
step:680/2330 train_time:39202ms step_avg:57.65ms
step:681/2330 train_time:39258ms step_avg:57.65ms
step:682/2330 train_time:39317ms step_avg:57.65ms
step:683/2330 train_time:39373ms step_avg:57.65ms
step:684/2330 train_time:39433ms step_avg:57.65ms
step:685/2330 train_time:39490ms step_avg:57.65ms
step:686/2330 train_time:39549ms step_avg:57.65ms
step:687/2330 train_time:39606ms step_avg:57.65ms
step:688/2330 train_time:39665ms step_avg:57.65ms
step:689/2330 train_time:39720ms step_avg:57.65ms
step:690/2330 train_time:39780ms step_avg:57.65ms
step:691/2330 train_time:39836ms step_avg:57.65ms
step:692/2330 train_time:39895ms step_avg:57.65ms
step:693/2330 train_time:39952ms step_avg:57.65ms
step:694/2330 train_time:40011ms step_avg:57.65ms
step:695/2330 train_time:40068ms step_avg:57.65ms
step:696/2330 train_time:40128ms step_avg:57.65ms
step:697/2330 train_time:40184ms step_avg:57.65ms
step:698/2330 train_time:40243ms step_avg:57.65ms
step:699/2330 train_time:40299ms step_avg:57.65ms
step:700/2330 train_time:40359ms step_avg:57.66ms
step:701/2330 train_time:40415ms step_avg:57.65ms
step:702/2330 train_time:40474ms step_avg:57.66ms
step:703/2330 train_time:40530ms step_avg:57.65ms
step:704/2330 train_time:40589ms step_avg:57.65ms
step:705/2330 train_time:40645ms step_avg:57.65ms
step:706/2330 train_time:40704ms step_avg:57.65ms
step:707/2330 train_time:40760ms step_avg:57.65ms
step:708/2330 train_time:40820ms step_avg:57.65ms
step:709/2330 train_time:40875ms step_avg:57.65ms
step:710/2330 train_time:40935ms step_avg:57.65ms
step:711/2330 train_time:40992ms step_avg:57.65ms
step:712/2330 train_time:41051ms step_avg:57.66ms
step:713/2330 train_time:41107ms step_avg:57.65ms
step:714/2330 train_time:41167ms step_avg:57.66ms
step:715/2330 train_time:41224ms step_avg:57.66ms
step:716/2330 train_time:41283ms step_avg:57.66ms
step:717/2330 train_time:41338ms step_avg:57.65ms
step:718/2330 train_time:41398ms step_avg:57.66ms
step:719/2330 train_time:41454ms step_avg:57.65ms
step:720/2330 train_time:41513ms step_avg:57.66ms
step:721/2330 train_time:41570ms step_avg:57.66ms
step:722/2330 train_time:41630ms step_avg:57.66ms
step:723/2330 train_time:41686ms step_avg:57.66ms
step:724/2330 train_time:41745ms step_avg:57.66ms
step:725/2330 train_time:41801ms step_avg:57.66ms
step:726/2330 train_time:41861ms step_avg:57.66ms
step:727/2330 train_time:41916ms step_avg:57.66ms
step:728/2330 train_time:41977ms step_avg:57.66ms
step:729/2330 train_time:42033ms step_avg:57.66ms
step:730/2330 train_time:42093ms step_avg:57.66ms
step:731/2330 train_time:42150ms step_avg:57.66ms
step:732/2330 train_time:42210ms step_avg:57.66ms
step:733/2330 train_time:42266ms step_avg:57.66ms
step:734/2330 train_time:42325ms step_avg:57.66ms
step:735/2330 train_time:42381ms step_avg:57.66ms
step:736/2330 train_time:42441ms step_avg:57.66ms
step:737/2330 train_time:42497ms step_avg:57.66ms
step:738/2330 train_time:42556ms step_avg:57.66ms
step:739/2330 train_time:42613ms step_avg:57.66ms
step:740/2330 train_time:42673ms step_avg:57.67ms
step:741/2330 train_time:42729ms step_avg:57.66ms
step:742/2330 train_time:42789ms step_avg:57.67ms
step:743/2330 train_time:42845ms step_avg:57.67ms
step:744/2330 train_time:42904ms step_avg:57.67ms
step:745/2330 train_time:42960ms step_avg:57.66ms
step:746/2330 train_time:43020ms step_avg:57.67ms
step:747/2330 train_time:43075ms step_avg:57.66ms
step:748/2330 train_time:43136ms step_avg:57.67ms
step:749/2330 train_time:43194ms step_avg:57.67ms
step:750/2330 train_time:43253ms step_avg:57.67ms
step:750/2330 val_loss:4.2100 train_time:43332ms step_avg:57.78ms
step:751/2330 train_time:43350ms step_avg:57.72ms
step:752/2330 train_time:43371ms step_avg:57.67ms
step:753/2330 train_time:43429ms step_avg:57.68ms
step:754/2330 train_time:43495ms step_avg:57.69ms
step:755/2330 train_time:43552ms step_avg:57.68ms
step:756/2330 train_time:43612ms step_avg:57.69ms
step:757/2330 train_time:43669ms step_avg:57.69ms
step:758/2330 train_time:43727ms step_avg:57.69ms
step:759/2330 train_time:43783ms step_avg:57.69ms
step:760/2330 train_time:43842ms step_avg:57.69ms
step:761/2330 train_time:43897ms step_avg:57.68ms
step:762/2330 train_time:43956ms step_avg:57.68ms
step:763/2330 train_time:44011ms step_avg:57.68ms
step:764/2330 train_time:44070ms step_avg:57.68ms
step:765/2330 train_time:44127ms step_avg:57.68ms
step:766/2330 train_time:44185ms step_avg:57.68ms
step:767/2330 train_time:44241ms step_avg:57.68ms
step:768/2330 train_time:44301ms step_avg:57.68ms
step:769/2330 train_time:44358ms step_avg:57.68ms
step:770/2330 train_time:44420ms step_avg:57.69ms
step:771/2330 train_time:44477ms step_avg:57.69ms
step:772/2330 train_time:44539ms step_avg:57.69ms
step:773/2330 train_time:44597ms step_avg:57.69ms
step:774/2330 train_time:44658ms step_avg:57.70ms
step:775/2330 train_time:44715ms step_avg:57.70ms
step:776/2330 train_time:44774ms step_avg:57.70ms
step:777/2330 train_time:44831ms step_avg:57.70ms
step:778/2330 train_time:44890ms step_avg:57.70ms
step:779/2330 train_time:44947ms step_avg:57.70ms
step:780/2330 train_time:45006ms step_avg:57.70ms
step:781/2330 train_time:45062ms step_avg:57.70ms
step:782/2330 train_time:45121ms step_avg:57.70ms
step:783/2330 train_time:45177ms step_avg:57.70ms
step:784/2330 train_time:45238ms step_avg:57.70ms
step:785/2330 train_time:45294ms step_avg:57.70ms
step:786/2330 train_time:45354ms step_avg:57.70ms
step:787/2330 train_time:45411ms step_avg:57.70ms
step:788/2330 train_time:45472ms step_avg:57.71ms
step:789/2330 train_time:45531ms step_avg:57.71ms
step:790/2330 train_time:45591ms step_avg:57.71ms
step:791/2330 train_time:45649ms step_avg:57.71ms
step:792/2330 train_time:45709ms step_avg:57.71ms
step:793/2330 train_time:45766ms step_avg:57.71ms
step:794/2330 train_time:45826ms step_avg:57.72ms
step:795/2330 train_time:45884ms step_avg:57.72ms
step:796/2330 train_time:45943ms step_avg:57.72ms
step:797/2330 train_time:46000ms step_avg:57.72ms
step:798/2330 train_time:46059ms step_avg:57.72ms
step:799/2330 train_time:46116ms step_avg:57.72ms
step:800/2330 train_time:46175ms step_avg:57.72ms
step:801/2330 train_time:46232ms step_avg:57.72ms
step:802/2330 train_time:46291ms step_avg:57.72ms
step:803/2330 train_time:46349ms step_avg:57.72ms
step:804/2330 train_time:46409ms step_avg:57.72ms
step:805/2330 train_time:46466ms step_avg:57.72ms
step:806/2330 train_time:46527ms step_avg:57.73ms
step:807/2330 train_time:46585ms step_avg:57.73ms
step:808/2330 train_time:46645ms step_avg:57.73ms
step:809/2330 train_time:46703ms step_avg:57.73ms
step:810/2330 train_time:46763ms step_avg:57.73ms
step:811/2330 train_time:46819ms step_avg:57.73ms
step:812/2330 train_time:46879ms step_avg:57.73ms
step:813/2330 train_time:46936ms step_avg:57.73ms
step:814/2330 train_time:46996ms step_avg:57.73ms
step:815/2330 train_time:47052ms step_avg:57.73ms
step:816/2330 train_time:47112ms step_avg:57.73ms
step:817/2330 train_time:47168ms step_avg:57.73ms
step:818/2330 train_time:47228ms step_avg:57.74ms
step:819/2330 train_time:47285ms step_avg:57.73ms
step:820/2330 train_time:47344ms step_avg:57.74ms
step:821/2330 train_time:47401ms step_avg:57.74ms
step:822/2330 train_time:47462ms step_avg:57.74ms
step:823/2330 train_time:47519ms step_avg:57.74ms
step:824/2330 train_time:47579ms step_avg:57.74ms
step:825/2330 train_time:47636ms step_avg:57.74ms
step:826/2330 train_time:47697ms step_avg:57.74ms
step:827/2330 train_time:47754ms step_avg:57.74ms
step:828/2330 train_time:47815ms step_avg:57.75ms
step:829/2330 train_time:47872ms step_avg:57.75ms
step:830/2330 train_time:47932ms step_avg:57.75ms
step:831/2330 train_time:47989ms step_avg:57.75ms
step:832/2330 train_time:48048ms step_avg:57.75ms
step:833/2330 train_time:48105ms step_avg:57.75ms
step:834/2330 train_time:48164ms step_avg:57.75ms
step:835/2330 train_time:48221ms step_avg:57.75ms
step:836/2330 train_time:48281ms step_avg:57.75ms
step:837/2330 train_time:48337ms step_avg:57.75ms
step:838/2330 train_time:48398ms step_avg:57.75ms
step:839/2330 train_time:48454ms step_avg:57.75ms
step:840/2330 train_time:48515ms step_avg:57.76ms
step:841/2330 train_time:48573ms step_avg:57.76ms
step:842/2330 train_time:48634ms step_avg:57.76ms
step:843/2330 train_time:48691ms step_avg:57.76ms
step:844/2330 train_time:48751ms step_avg:57.76ms
step:845/2330 train_time:48809ms step_avg:57.76ms
step:846/2330 train_time:48868ms step_avg:57.76ms
step:847/2330 train_time:48925ms step_avg:57.76ms
step:848/2330 train_time:48985ms step_avg:57.77ms
step:849/2330 train_time:49042ms step_avg:57.76ms
step:850/2330 train_time:49101ms step_avg:57.77ms
step:851/2330 train_time:49158ms step_avg:57.76ms
step:852/2330 train_time:49218ms step_avg:57.77ms
step:853/2330 train_time:49274ms step_avg:57.77ms
step:854/2330 train_time:49335ms step_avg:57.77ms
step:855/2330 train_time:49392ms step_avg:57.77ms
step:856/2330 train_time:49453ms step_avg:57.77ms
step:857/2330 train_time:49510ms step_avg:57.77ms
step:858/2330 train_time:49570ms step_avg:57.77ms
step:859/2330 train_time:49628ms step_avg:57.77ms
step:860/2330 train_time:49688ms step_avg:57.78ms
step:861/2330 train_time:49745ms step_avg:57.78ms
step:862/2330 train_time:49805ms step_avg:57.78ms
step:863/2330 train_time:49862ms step_avg:57.78ms
step:864/2330 train_time:49922ms step_avg:57.78ms
step:865/2330 train_time:49978ms step_avg:57.78ms
step:866/2330 train_time:50038ms step_avg:57.78ms
step:867/2330 train_time:50095ms step_avg:57.78ms
step:868/2330 train_time:50155ms step_avg:57.78ms
step:869/2330 train_time:50211ms step_avg:57.78ms
step:870/2330 train_time:50272ms step_avg:57.78ms
step:871/2330 train_time:50329ms step_avg:57.78ms
step:872/2330 train_time:50388ms step_avg:57.78ms
step:873/2330 train_time:50445ms step_avg:57.78ms
step:874/2330 train_time:50505ms step_avg:57.79ms
step:875/2330 train_time:50562ms step_avg:57.78ms
step:876/2330 train_time:50622ms step_avg:57.79ms
step:877/2330 train_time:50679ms step_avg:57.79ms
step:878/2330 train_time:50739ms step_avg:57.79ms
step:879/2330 train_time:50797ms step_avg:57.79ms
step:880/2330 train_time:50856ms step_avg:57.79ms
step:881/2330 train_time:50913ms step_avg:57.79ms
step:882/2330 train_time:50974ms step_avg:57.79ms
step:883/2330 train_time:51032ms step_avg:57.79ms
step:884/2330 train_time:51091ms step_avg:57.80ms
step:885/2330 train_time:51147ms step_avg:57.79ms
step:886/2330 train_time:51208ms step_avg:57.80ms
step:887/2330 train_time:51265ms step_avg:57.80ms
step:888/2330 train_time:51325ms step_avg:57.80ms
step:889/2330 train_time:51382ms step_avg:57.80ms
step:890/2330 train_time:51442ms step_avg:57.80ms
step:891/2330 train_time:51499ms step_avg:57.80ms
step:892/2330 train_time:51558ms step_avg:57.80ms
step:893/2330 train_time:51615ms step_avg:57.80ms
step:894/2330 train_time:51675ms step_avg:57.80ms
step:895/2330 train_time:51732ms step_avg:57.80ms
step:896/2330 train_time:51793ms step_avg:57.80ms
step:897/2330 train_time:51850ms step_avg:57.80ms
step:898/2330 train_time:51909ms step_avg:57.81ms
step:899/2330 train_time:51966ms step_avg:57.80ms
step:900/2330 train_time:52027ms step_avg:57.81ms
step:901/2330 train_time:52084ms step_avg:57.81ms
step:902/2330 train_time:52143ms step_avg:57.81ms
step:903/2330 train_time:52200ms step_avg:57.81ms
step:904/2330 train_time:52260ms step_avg:57.81ms
step:905/2330 train_time:52316ms step_avg:57.81ms
step:906/2330 train_time:52376ms step_avg:57.81ms
step:907/2330 train_time:52433ms step_avg:57.81ms
step:908/2330 train_time:52493ms step_avg:57.81ms
step:909/2330 train_time:52550ms step_avg:57.81ms
step:910/2330 train_time:52610ms step_avg:57.81ms
step:911/2330 train_time:52668ms step_avg:57.81ms
step:912/2330 train_time:52728ms step_avg:57.82ms
step:913/2330 train_time:52786ms step_avg:57.82ms
step:914/2330 train_time:52845ms step_avg:57.82ms
step:915/2330 train_time:52903ms step_avg:57.82ms
step:916/2330 train_time:52962ms step_avg:57.82ms
step:917/2330 train_time:53019ms step_avg:57.82ms
step:918/2330 train_time:53079ms step_avg:57.82ms
step:919/2330 train_time:53137ms step_avg:57.82ms
step:920/2330 train_time:53196ms step_avg:57.82ms
step:921/2330 train_time:53253ms step_avg:57.82ms
step:922/2330 train_time:53313ms step_avg:57.82ms
step:923/2330 train_time:53370ms step_avg:57.82ms
step:924/2330 train_time:53430ms step_avg:57.82ms
step:925/2330 train_time:53488ms step_avg:57.82ms
step:926/2330 train_time:53548ms step_avg:57.83ms
step:927/2330 train_time:53605ms step_avg:57.83ms
step:928/2330 train_time:53665ms step_avg:57.83ms
step:929/2330 train_time:53721ms step_avg:57.83ms
step:930/2330 train_time:53782ms step_avg:57.83ms
step:931/2330 train_time:53839ms step_avg:57.83ms
step:932/2330 train_time:53899ms step_avg:57.83ms
step:933/2330 train_time:53955ms step_avg:57.83ms
step:934/2330 train_time:54015ms step_avg:57.83ms
step:935/2330 train_time:54072ms step_avg:57.83ms
step:936/2330 train_time:54133ms step_avg:57.83ms
step:937/2330 train_time:54190ms step_avg:57.83ms
step:938/2330 train_time:54249ms step_avg:57.83ms
step:939/2330 train_time:54306ms step_avg:57.83ms
step:940/2330 train_time:54366ms step_avg:57.84ms
step:941/2330 train_time:54424ms step_avg:57.84ms
step:942/2330 train_time:54483ms step_avg:57.84ms
step:943/2330 train_time:54540ms step_avg:57.84ms
step:944/2330 train_time:54600ms step_avg:57.84ms
step:945/2330 train_time:54657ms step_avg:57.84ms
step:946/2330 train_time:54717ms step_avg:57.84ms
step:947/2330 train_time:54773ms step_avg:57.84ms
step:948/2330 train_time:54834ms step_avg:57.84ms
step:949/2330 train_time:54891ms step_avg:57.84ms
step:950/2330 train_time:54950ms step_avg:57.84ms
step:951/2330 train_time:55007ms step_avg:57.84ms
step:952/2330 train_time:55068ms step_avg:57.84ms
step:953/2330 train_time:55125ms step_avg:57.84ms
step:954/2330 train_time:55185ms step_avg:57.85ms
step:955/2330 train_time:55243ms step_avg:57.85ms
step:956/2330 train_time:55302ms step_avg:57.85ms
step:957/2330 train_time:55359ms step_avg:57.85ms
step:958/2330 train_time:55418ms step_avg:57.85ms
step:959/2330 train_time:55475ms step_avg:57.85ms
step:960/2330 train_time:55536ms step_avg:57.85ms
step:961/2330 train_time:55593ms step_avg:57.85ms
step:962/2330 train_time:55652ms step_avg:57.85ms
step:963/2330 train_time:55709ms step_avg:57.85ms
step:964/2330 train_time:55769ms step_avg:57.85ms
step:965/2330 train_time:55827ms step_avg:57.85ms
step:966/2330 train_time:55887ms step_avg:57.85ms
step:967/2330 train_time:55944ms step_avg:57.85ms
step:968/2330 train_time:56003ms step_avg:57.85ms
step:969/2330 train_time:56060ms step_avg:57.85ms
step:970/2330 train_time:56120ms step_avg:57.86ms
step:971/2330 train_time:56177ms step_avg:57.85ms
step:972/2330 train_time:56238ms step_avg:57.86ms
step:973/2330 train_time:56294ms step_avg:57.86ms
step:974/2330 train_time:56355ms step_avg:57.86ms
step:975/2330 train_time:56412ms step_avg:57.86ms
step:976/2330 train_time:56472ms step_avg:57.86ms
step:977/2330 train_time:56530ms step_avg:57.86ms
step:978/2330 train_time:56589ms step_avg:57.86ms
step:979/2330 train_time:56646ms step_avg:57.86ms
step:980/2330 train_time:56707ms step_avg:57.86ms
step:981/2330 train_time:56764ms step_avg:57.86ms
step:982/2330 train_time:56824ms step_avg:57.87ms
step:983/2330 train_time:56881ms step_avg:57.86ms
step:984/2330 train_time:56941ms step_avg:57.87ms
step:985/2330 train_time:56998ms step_avg:57.87ms
step:986/2330 train_time:57058ms step_avg:57.87ms
step:987/2330 train_time:57115ms step_avg:57.87ms
step:988/2330 train_time:57175ms step_avg:57.87ms
step:989/2330 train_time:57232ms step_avg:57.87ms
step:990/2330 train_time:57293ms step_avg:57.87ms
step:991/2330 train_time:57349ms step_avg:57.87ms
step:992/2330 train_time:57409ms step_avg:57.87ms
step:993/2330 train_time:57466ms step_avg:57.87ms
step:994/2330 train_time:57526ms step_avg:57.87ms
step:995/2330 train_time:57584ms step_avg:57.87ms
step:996/2330 train_time:57644ms step_avg:57.88ms
step:997/2330 train_time:57701ms step_avg:57.87ms
step:998/2330 train_time:57760ms step_avg:57.88ms
step:999/2330 train_time:57817ms step_avg:57.88ms
step:1000/2330 train_time:57878ms step_avg:57.88ms
step:1000/2330 val_loss:4.0647 train_time:57958ms step_avg:57.96ms
step:1001/2330 train_time:57978ms step_avg:57.92ms
step:1002/2330 train_time:57998ms step_avg:57.88ms
step:1003/2330 train_time:58051ms step_avg:57.88ms
step:1004/2330 train_time:58118ms step_avg:57.89ms
step:1005/2330 train_time:58174ms step_avg:57.88ms
step:1006/2330 train_time:58236ms step_avg:57.89ms
step:1007/2330 train_time:58292ms step_avg:57.89ms
step:1008/2330 train_time:58351ms step_avg:57.89ms
step:1009/2330 train_time:58407ms step_avg:57.89ms
step:1010/2330 train_time:58467ms step_avg:57.89ms
step:1011/2330 train_time:58524ms step_avg:57.89ms
step:1012/2330 train_time:58583ms step_avg:57.89ms
step:1013/2330 train_time:58639ms step_avg:57.89ms
step:1014/2330 train_time:58698ms step_avg:57.89ms
step:1015/2330 train_time:58754ms step_avg:57.89ms
step:1016/2330 train_time:58814ms step_avg:57.89ms
step:1017/2330 train_time:58875ms step_avg:57.89ms
step:1018/2330 train_time:58938ms step_avg:57.90ms
step:1019/2330 train_time:58996ms step_avg:57.90ms
step:1020/2330 train_time:59058ms step_avg:57.90ms
step:1021/2330 train_time:59114ms step_avg:57.90ms
step:1022/2330 train_time:59175ms step_avg:57.90ms
step:1023/2330 train_time:59231ms step_avg:57.90ms
step:1024/2330 train_time:59291ms step_avg:57.90ms
step:1025/2330 train_time:59347ms step_avg:57.90ms
step:1026/2330 train_time:59406ms step_avg:57.90ms
step:1027/2330 train_time:59463ms step_avg:57.90ms
step:1028/2330 train_time:59522ms step_avg:57.90ms
step:1029/2330 train_time:59579ms step_avg:57.90ms
step:1030/2330 train_time:59638ms step_avg:57.90ms
step:1031/2330 train_time:59694ms step_avg:57.90ms
step:1032/2330 train_time:59755ms step_avg:57.90ms
step:1033/2330 train_time:59812ms step_avg:57.90ms
step:1034/2330 train_time:59873ms step_avg:57.90ms
step:1035/2330 train_time:59931ms step_avg:57.90ms
step:1036/2330 train_time:59994ms step_avg:57.91ms
step:1037/2330 train_time:60051ms step_avg:57.91ms
step:1038/2330 train_time:60111ms step_avg:57.91ms
step:1039/2330 train_time:60168ms step_avg:57.91ms
step:1040/2330 train_time:60228ms step_avg:57.91ms
step:1041/2330 train_time:60285ms step_avg:57.91ms
step:1042/2330 train_time:60344ms step_avg:57.91ms
step:1043/2330 train_time:60400ms step_avg:57.91ms
step:1044/2330 train_time:60461ms step_avg:57.91ms
step:1045/2330 train_time:60517ms step_avg:57.91ms
step:1046/2330 train_time:60577ms step_avg:57.91ms
step:1047/2330 train_time:60634ms step_avg:57.91ms
step:1048/2330 train_time:60694ms step_avg:57.91ms
step:1049/2330 train_time:60750ms step_avg:57.91ms
step:1050/2330 train_time:60811ms step_avg:57.92ms
step:1051/2330 train_time:60868ms step_avg:57.91ms
step:1052/2330 train_time:60930ms step_avg:57.92ms
step:1053/2330 train_time:60987ms step_avg:57.92ms
step:1054/2330 train_time:61048ms step_avg:57.92ms
step:1055/2330 train_time:61104ms step_avg:57.92ms
step:1056/2330 train_time:61166ms step_avg:57.92ms
step:1057/2330 train_time:61223ms step_avg:57.92ms
step:1058/2330 train_time:61283ms step_avg:57.92ms
step:1059/2330 train_time:61339ms step_avg:57.92ms
step:1060/2330 train_time:61399ms step_avg:57.92ms
step:1061/2330 train_time:61455ms step_avg:57.92ms
step:1062/2330 train_time:61517ms step_avg:57.93ms
step:1063/2330 train_time:61573ms step_avg:57.92ms
step:1064/2330 train_time:61632ms step_avg:57.93ms
step:1065/2330 train_time:61689ms step_avg:57.92ms
step:1066/2330 train_time:61749ms step_avg:57.93ms
step:1067/2330 train_time:61806ms step_avg:57.92ms
step:1068/2330 train_time:61868ms step_avg:57.93ms
step:1069/2330 train_time:61924ms step_avg:57.93ms
step:1070/2330 train_time:61987ms step_avg:57.93ms
step:1071/2330 train_time:62045ms step_avg:57.93ms
step:1072/2330 train_time:62105ms step_avg:57.93ms
step:1073/2330 train_time:62163ms step_avg:57.93ms
step:1074/2330 train_time:62222ms step_avg:57.94ms
step:1075/2330 train_time:62279ms step_avg:57.93ms
step:1076/2330 train_time:62339ms step_avg:57.94ms
step:1077/2330 train_time:62396ms step_avg:57.93ms
step:1078/2330 train_time:62455ms step_avg:57.94ms
step:1079/2330 train_time:62512ms step_avg:57.94ms
step:1080/2330 train_time:62572ms step_avg:57.94ms
step:1081/2330 train_time:62628ms step_avg:57.94ms
step:1082/2330 train_time:62688ms step_avg:57.94ms
step:1083/2330 train_time:62745ms step_avg:57.94ms
step:1084/2330 train_time:62805ms step_avg:57.94ms
step:1085/2330 train_time:62862ms step_avg:57.94ms
step:1086/2330 train_time:62924ms step_avg:57.94ms
step:1087/2330 train_time:62982ms step_avg:57.94ms
step:1088/2330 train_time:63042ms step_avg:57.94ms
step:1089/2330 train_time:63099ms step_avg:57.94ms
step:1090/2330 train_time:63160ms step_avg:57.94ms
step:1091/2330 train_time:63217ms step_avg:57.94ms
step:1092/2330 train_time:63277ms step_avg:57.95ms
step:1093/2330 train_time:63334ms step_avg:57.94ms
step:1094/2330 train_time:63394ms step_avg:57.95ms
step:1095/2330 train_time:63450ms step_avg:57.95ms
step:1096/2330 train_time:63511ms step_avg:57.95ms
step:1097/2330 train_time:63568ms step_avg:57.95ms
step:1098/2330 train_time:63628ms step_avg:57.95ms
step:1099/2330 train_time:63686ms step_avg:57.95ms
step:1100/2330 train_time:63745ms step_avg:57.95ms
step:1101/2330 train_time:63802ms step_avg:57.95ms
step:1102/2330 train_time:63862ms step_avg:57.95ms
step:1103/2330 train_time:63920ms step_avg:57.95ms
step:1104/2330 train_time:63979ms step_avg:57.95ms
step:1105/2330 train_time:64036ms step_avg:57.95ms
step:1106/2330 train_time:64097ms step_avg:57.95ms
step:1107/2330 train_time:64154ms step_avg:57.95ms
step:1108/2330 train_time:64213ms step_avg:57.95ms
step:1109/2330 train_time:64271ms step_avg:57.95ms
step:1110/2330 train_time:64331ms step_avg:57.96ms
step:1111/2330 train_time:64388ms step_avg:57.96ms
step:1112/2330 train_time:64448ms step_avg:57.96ms
step:1113/2330 train_time:64504ms step_avg:57.96ms
step:1114/2330 train_time:64564ms step_avg:57.96ms
step:1115/2330 train_time:64621ms step_avg:57.96ms
step:1116/2330 train_time:64681ms step_avg:57.96ms
step:1117/2330 train_time:64738ms step_avg:57.96ms
step:1118/2330 train_time:64799ms step_avg:57.96ms
step:1119/2330 train_time:64856ms step_avg:57.96ms
step:1120/2330 train_time:64916ms step_avg:57.96ms
step:1121/2330 train_time:64973ms step_avg:57.96ms
step:1122/2330 train_time:65033ms step_avg:57.96ms
step:1123/2330 train_time:65090ms step_avg:57.96ms
step:1124/2330 train_time:65151ms step_avg:57.96ms
step:1125/2330 train_time:65208ms step_avg:57.96ms
step:1126/2330 train_time:65268ms step_avg:57.96ms
step:1127/2330 train_time:65325ms step_avg:57.96ms
step:1128/2330 train_time:65386ms step_avg:57.97ms
step:1129/2330 train_time:65444ms step_avg:57.97ms
step:1130/2330 train_time:65503ms step_avg:57.97ms
step:1131/2330 train_time:65559ms step_avg:57.97ms
step:1132/2330 train_time:65620ms step_avg:57.97ms
step:1133/2330 train_time:65677ms step_avg:57.97ms
step:1134/2330 train_time:65738ms step_avg:57.97ms
step:1135/2330 train_time:65795ms step_avg:57.97ms
step:1136/2330 train_time:65854ms step_avg:57.97ms
step:1137/2330 train_time:65911ms step_avg:57.97ms
step:1138/2330 train_time:65971ms step_avg:57.97ms
step:1139/2330 train_time:66028ms step_avg:57.97ms
step:1140/2330 train_time:66089ms step_avg:57.97ms
step:1141/2330 train_time:66147ms step_avg:57.97ms
step:1142/2330 train_time:66207ms step_avg:57.97ms
step:1143/2330 train_time:66264ms step_avg:57.97ms
step:1144/2330 train_time:66324ms step_avg:57.98ms
step:1145/2330 train_time:66382ms step_avg:57.98ms
step:1146/2330 train_time:66441ms step_avg:57.98ms
step:1147/2330 train_time:66498ms step_avg:57.98ms
step:1148/2330 train_time:66558ms step_avg:57.98ms
step:1149/2330 train_time:66615ms step_avg:57.98ms
step:1150/2330 train_time:66676ms step_avg:57.98ms
step:1151/2330 train_time:66733ms step_avg:57.98ms
step:1152/2330 train_time:66793ms step_avg:57.98ms
step:1153/2330 train_time:66849ms step_avg:57.98ms
step:1154/2330 train_time:66909ms step_avg:57.98ms
step:1155/2330 train_time:66967ms step_avg:57.98ms
step:1156/2330 train_time:67027ms step_avg:57.98ms
step:1157/2330 train_time:67084ms step_avg:57.98ms
step:1158/2330 train_time:67145ms step_avg:57.98ms
step:1159/2330 train_time:67203ms step_avg:57.98ms
step:1160/2330 train_time:67263ms step_avg:57.99ms
step:1161/2330 train_time:67320ms step_avg:57.98ms
step:1162/2330 train_time:67381ms step_avg:57.99ms
step:1163/2330 train_time:67438ms step_avg:57.99ms
step:1164/2330 train_time:67498ms step_avg:57.99ms
step:1165/2330 train_time:67555ms step_avg:57.99ms
step:1166/2330 train_time:67616ms step_avg:57.99ms
step:1167/2330 train_time:67673ms step_avg:57.99ms
step:1168/2330 train_time:67733ms step_avg:57.99ms
step:1169/2330 train_time:67790ms step_avg:57.99ms
step:1170/2330 train_time:67850ms step_avg:57.99ms
step:1171/2330 train_time:67906ms step_avg:57.99ms
step:1172/2330 train_time:67966ms step_avg:57.99ms
step:1173/2330 train_time:68023ms step_avg:57.99ms
step:1174/2330 train_time:68084ms step_avg:57.99ms
step:1175/2330 train_time:68140ms step_avg:57.99ms
step:1176/2330 train_time:68201ms step_avg:57.99ms
step:1177/2330 train_time:68257ms step_avg:57.99ms
step:1178/2330 train_time:68318ms step_avg:57.99ms
step:1179/2330 train_time:68374ms step_avg:57.99ms
step:1180/2330 train_time:68435ms step_avg:58.00ms
step:1181/2330 train_time:68493ms step_avg:58.00ms
step:1182/2330 train_time:68553ms step_avg:58.00ms
step:1183/2330 train_time:68610ms step_avg:58.00ms
step:1184/2330 train_time:68670ms step_avg:58.00ms
step:1185/2330 train_time:68727ms step_avg:58.00ms
step:1186/2330 train_time:68787ms step_avg:58.00ms
step:1187/2330 train_time:68843ms step_avg:58.00ms
step:1188/2330 train_time:68903ms step_avg:58.00ms
step:1189/2330 train_time:68960ms step_avg:58.00ms
step:1190/2330 train_time:69020ms step_avg:58.00ms
step:1191/2330 train_time:69077ms step_avg:58.00ms
step:1192/2330 train_time:69137ms step_avg:58.00ms
step:1193/2330 train_time:69195ms step_avg:58.00ms
step:1194/2330 train_time:69255ms step_avg:58.00ms
step:1195/2330 train_time:69311ms step_avg:58.00ms
step:1196/2330 train_time:69372ms step_avg:58.00ms
step:1197/2330 train_time:69429ms step_avg:58.00ms
step:1198/2330 train_time:69489ms step_avg:58.00ms
step:1199/2330 train_time:69547ms step_avg:58.00ms
step:1200/2330 train_time:69607ms step_avg:58.01ms
step:1201/2330 train_time:69664ms step_avg:58.00ms
step:1202/2330 train_time:69724ms step_avg:58.01ms
step:1203/2330 train_time:69781ms step_avg:58.01ms
step:1204/2330 train_time:69842ms step_avg:58.01ms
step:1205/2330 train_time:69898ms step_avg:58.01ms
step:1206/2330 train_time:69959ms step_avg:58.01ms
step:1207/2330 train_time:70016ms step_avg:58.01ms
step:1208/2330 train_time:70076ms step_avg:58.01ms
step:1209/2330 train_time:70133ms step_avg:58.01ms
step:1210/2330 train_time:70194ms step_avg:58.01ms
step:1211/2330 train_time:70250ms step_avg:58.01ms
step:1212/2330 train_time:70310ms step_avg:58.01ms
step:1213/2330 train_time:70367ms step_avg:58.01ms
step:1214/2330 train_time:70427ms step_avg:58.01ms
step:1215/2330 train_time:70485ms step_avg:58.01ms
step:1216/2330 train_time:70545ms step_avg:58.01ms
step:1217/2330 train_time:70601ms step_avg:58.01ms
step:1218/2330 train_time:70662ms step_avg:58.02ms
step:1219/2330 train_time:70719ms step_avg:58.01ms
step:1220/2330 train_time:70781ms step_avg:58.02ms
step:1221/2330 train_time:70838ms step_avg:58.02ms
step:1222/2330 train_time:70897ms step_avg:58.02ms
step:1223/2330 train_time:70954ms step_avg:58.02ms
step:1224/2330 train_time:71014ms step_avg:58.02ms
step:1225/2330 train_time:71071ms step_avg:58.02ms
step:1226/2330 train_time:71131ms step_avg:58.02ms
step:1227/2330 train_time:71188ms step_avg:58.02ms
step:1228/2330 train_time:71248ms step_avg:58.02ms
step:1229/2330 train_time:71305ms step_avg:58.02ms
step:1230/2330 train_time:71366ms step_avg:58.02ms
step:1231/2330 train_time:71423ms step_avg:58.02ms
step:1232/2330 train_time:71483ms step_avg:58.02ms
step:1233/2330 train_time:71540ms step_avg:58.02ms
step:1234/2330 train_time:71600ms step_avg:58.02ms
step:1235/2330 train_time:71658ms step_avg:58.02ms
step:1236/2330 train_time:71717ms step_avg:58.02ms
step:1237/2330 train_time:71774ms step_avg:58.02ms
step:1238/2330 train_time:71834ms step_avg:58.02ms
step:1239/2330 train_time:71891ms step_avg:58.02ms
step:1240/2330 train_time:71951ms step_avg:58.02ms
step:1241/2330 train_time:72007ms step_avg:58.02ms
step:1242/2330 train_time:72068ms step_avg:58.03ms
step:1243/2330 train_time:72125ms step_avg:58.03ms
step:1244/2330 train_time:72185ms step_avg:58.03ms
step:1245/2330 train_time:72242ms step_avg:58.03ms
step:1246/2330 train_time:72302ms step_avg:58.03ms
step:1247/2330 train_time:72360ms step_avg:58.03ms
step:1248/2330 train_time:72419ms step_avg:58.03ms
step:1249/2330 train_time:72477ms step_avg:58.03ms
step:1250/2330 train_time:72538ms step_avg:58.03ms
step:1250/2330 val_loss:3.9869 train_time:72618ms step_avg:58.09ms
step:1251/2330 train_time:72637ms step_avg:58.06ms
step:1252/2330 train_time:72657ms step_avg:58.03ms
step:1253/2330 train_time:72717ms step_avg:58.03ms
step:1254/2330 train_time:72783ms step_avg:58.04ms
step:1255/2330 train_time:72840ms step_avg:58.04ms
step:1256/2330 train_time:72900ms step_avg:58.04ms
step:1257/2330 train_time:72957ms step_avg:58.04ms
step:1258/2330 train_time:73016ms step_avg:58.04ms
step:1259/2330 train_time:73073ms step_avg:58.04ms
step:1260/2330 train_time:73133ms step_avg:58.04ms
step:1261/2330 train_time:73189ms step_avg:58.04ms
step:1262/2330 train_time:73248ms step_avg:58.04ms
step:1263/2330 train_time:73304ms step_avg:58.04ms
step:1264/2330 train_time:73363ms step_avg:58.04ms
step:1265/2330 train_time:73420ms step_avg:58.04ms
step:1266/2330 train_time:73478ms step_avg:58.04ms
step:1267/2330 train_time:73535ms step_avg:58.04ms
step:1268/2330 train_time:73596ms step_avg:58.04ms
step:1269/2330 train_time:73654ms step_avg:58.04ms
step:1270/2330 train_time:73719ms step_avg:58.05ms
step:1271/2330 train_time:73777ms step_avg:58.05ms
step:1272/2330 train_time:73839ms step_avg:58.05ms
step:1273/2330 train_time:73896ms step_avg:58.05ms
step:1274/2330 train_time:73957ms step_avg:58.05ms
step:1275/2330 train_time:74014ms step_avg:58.05ms
step:1276/2330 train_time:74074ms step_avg:58.05ms
step:1277/2330 train_time:74131ms step_avg:58.05ms
step:1278/2330 train_time:74191ms step_avg:58.05ms
step:1279/2330 train_time:74247ms step_avg:58.05ms
step:1280/2330 train_time:74308ms step_avg:58.05ms
step:1281/2330 train_time:74365ms step_avg:58.05ms
step:1282/2330 train_time:74424ms step_avg:58.05ms
step:1283/2330 train_time:74480ms step_avg:58.05ms
step:1284/2330 train_time:74540ms step_avg:58.05ms
step:1285/2330 train_time:74597ms step_avg:58.05ms
step:1286/2330 train_time:74659ms step_avg:58.06ms
step:1287/2330 train_time:74716ms step_avg:58.05ms
step:1288/2330 train_time:74778ms step_avg:58.06ms
step:1289/2330 train_time:74835ms step_avg:58.06ms
step:1290/2330 train_time:74897ms step_avg:58.06ms
step:1291/2330 train_time:74954ms step_avg:58.06ms
step:1292/2330 train_time:75014ms step_avg:58.06ms
step:1293/2330 train_time:75071ms step_avg:58.06ms
step:1294/2330 train_time:75130ms step_avg:58.06ms
step:1295/2330 train_time:75187ms step_avg:58.06ms
step:1296/2330 train_time:75246ms step_avg:58.06ms
step:1297/2330 train_time:75302ms step_avg:58.06ms
step:1298/2330 train_time:75362ms step_avg:58.06ms
step:1299/2330 train_time:75419ms step_avg:58.06ms
step:1300/2330 train_time:75478ms step_avg:58.06ms
step:1301/2330 train_time:75535ms step_avg:58.06ms
step:1302/2330 train_time:75596ms step_avg:58.06ms
step:1303/2330 train_time:75652ms step_avg:58.06ms
step:1304/2330 train_time:75714ms step_avg:58.06ms
step:1305/2330 train_time:75771ms step_avg:58.06ms
step:1306/2330 train_time:75832ms step_avg:58.06ms
step:1307/2330 train_time:75890ms step_avg:58.06ms
step:1308/2330 train_time:75950ms step_avg:58.07ms
step:1309/2330 train_time:76007ms step_avg:58.06ms
step:1310/2330 train_time:76068ms step_avg:58.07ms
step:1311/2330 train_time:76125ms step_avg:58.07ms
step:1312/2330 train_time:76184ms step_avg:58.07ms
step:1313/2330 train_time:76240ms step_avg:58.07ms
step:1314/2330 train_time:76301ms step_avg:58.07ms
step:1315/2330 train_time:76357ms step_avg:58.07ms
step:1316/2330 train_time:76417ms step_avg:58.07ms
step:1317/2330 train_time:76475ms step_avg:58.07ms
step:1318/2330 train_time:76534ms step_avg:58.07ms
step:1319/2330 train_time:76591ms step_avg:58.07ms
step:1320/2330 train_time:76651ms step_avg:58.07ms
step:1321/2330 train_time:76708ms step_avg:58.07ms
step:1322/2330 train_time:76769ms step_avg:58.07ms
step:1323/2330 train_time:76826ms step_avg:58.07ms
step:1324/2330 train_time:76886ms step_avg:58.07ms
step:1325/2330 train_time:76943ms step_avg:58.07ms
step:1326/2330 train_time:77005ms step_avg:58.07ms
step:1327/2330 train_time:77061ms step_avg:58.07ms
step:1328/2330 train_time:77122ms step_avg:58.07ms
step:1329/2330 train_time:77178ms step_avg:58.07ms
step:1330/2330 train_time:77239ms step_avg:58.07ms
step:1331/2330 train_time:77295ms step_avg:58.07ms
step:1332/2330 train_time:77355ms step_avg:58.07ms
step:1333/2330 train_time:77412ms step_avg:58.07ms
step:1334/2330 train_time:77472ms step_avg:58.07ms
step:1335/2330 train_time:77529ms step_avg:58.07ms
step:1336/2330 train_time:77589ms step_avg:58.08ms
step:1337/2330 train_time:77645ms step_avg:58.07ms
step:1338/2330 train_time:77706ms step_avg:58.08ms
step:1339/2330 train_time:77763ms step_avg:58.08ms
step:1340/2330 train_time:77823ms step_avg:58.08ms
step:1341/2330 train_time:77879ms step_avg:58.08ms
step:1342/2330 train_time:77941ms step_avg:58.08ms
step:1343/2330 train_time:77998ms step_avg:58.08ms
step:1344/2330 train_time:78059ms step_avg:58.08ms
step:1345/2330 train_time:78116ms step_avg:58.08ms
step:1346/2330 train_time:78176ms step_avg:58.08ms
step:1347/2330 train_time:78232ms step_avg:58.08ms
step:1348/2330 train_time:78292ms step_avg:58.08ms
step:1349/2330 train_time:78349ms step_avg:58.08ms
step:1350/2330 train_time:78409ms step_avg:58.08ms
step:1351/2330 train_time:78465ms step_avg:58.08ms
step:1352/2330 train_time:78526ms step_avg:58.08ms
step:1353/2330 train_time:78582ms step_avg:58.08ms
step:1354/2330 train_time:78643ms step_avg:58.08ms
step:1355/2330 train_time:78701ms step_avg:58.08ms
step:1356/2330 train_time:78761ms step_avg:58.08ms
step:1357/2330 train_time:78817ms step_avg:58.08ms
step:1358/2330 train_time:78878ms step_avg:58.08ms
step:1359/2330 train_time:78935ms step_avg:58.08ms
step:1360/2330 train_time:78996ms step_avg:58.09ms
step:1361/2330 train_time:79053ms step_avg:58.08ms
step:1362/2330 train_time:79113ms step_avg:58.09ms
step:1363/2330 train_time:79171ms step_avg:58.09ms
step:1364/2330 train_time:79230ms step_avg:58.09ms
step:1365/2330 train_time:79287ms step_avg:58.09ms
step:1366/2330 train_time:79347ms step_avg:58.09ms
step:1367/2330 train_time:79403ms step_avg:58.09ms
step:1368/2330 train_time:79463ms step_avg:58.09ms
step:1369/2330 train_time:79519ms step_avg:58.09ms
step:1370/2330 train_time:79580ms step_avg:58.09ms
step:1371/2330 train_time:79637ms step_avg:58.09ms
step:1372/2330 train_time:79696ms step_avg:58.09ms
step:1373/2330 train_time:79754ms step_avg:58.09ms
step:1374/2330 train_time:79814ms step_avg:58.09ms
step:1375/2330 train_time:79871ms step_avg:58.09ms
step:1376/2330 train_time:79932ms step_avg:58.09ms
step:1377/2330 train_time:79989ms step_avg:58.09ms
step:1378/2330 train_time:80050ms step_avg:58.09ms
step:1379/2330 train_time:80106ms step_avg:58.09ms
step:1380/2330 train_time:80167ms step_avg:58.09ms
step:1381/2330 train_time:80223ms step_avg:58.09ms
step:1382/2330 train_time:80284ms step_avg:58.09ms
step:1383/2330 train_time:80340ms step_avg:58.09ms
step:1384/2330 train_time:80400ms step_avg:58.09ms
step:1385/2330 train_time:80458ms step_avg:58.09ms
step:1386/2330 train_time:80517ms step_avg:58.09ms
step:1387/2330 train_time:80575ms step_avg:58.09ms
step:1388/2330 train_time:80636ms step_avg:58.09ms
step:1389/2330 train_time:80693ms step_avg:58.09ms
step:1390/2330 train_time:80753ms step_avg:58.10ms
step:1391/2330 train_time:80811ms step_avg:58.10ms
step:1392/2330 train_time:80871ms step_avg:58.10ms
step:1393/2330 train_time:80928ms step_avg:58.10ms
step:1394/2330 train_time:80989ms step_avg:58.10ms
step:1395/2330 train_time:81046ms step_avg:58.10ms
step:1396/2330 train_time:81106ms step_avg:58.10ms
step:1397/2330 train_time:81163ms step_avg:58.10ms
step:1398/2330 train_time:81224ms step_avg:58.10ms
step:1399/2330 train_time:81280ms step_avg:58.10ms
step:1400/2330 train_time:81340ms step_avg:58.10ms
step:1401/2330 train_time:81397ms step_avg:58.10ms
step:1402/2330 train_time:81457ms step_avg:58.10ms
step:1403/2330 train_time:81514ms step_avg:58.10ms
step:1404/2330 train_time:81574ms step_avg:58.10ms
step:1405/2330 train_time:81631ms step_avg:58.10ms
step:1406/2330 train_time:81691ms step_avg:58.10ms
step:1407/2330 train_time:81747ms step_avg:58.10ms
step:1408/2330 train_time:81808ms step_avg:58.10ms
step:1409/2330 train_time:81864ms step_avg:58.10ms
step:1410/2330 train_time:81926ms step_avg:58.10ms
step:1411/2330 train_time:81983ms step_avg:58.10ms
step:1412/2330 train_time:82043ms step_avg:58.10ms
step:1413/2330 train_time:82099ms step_avg:58.10ms
step:1414/2330 train_time:82160ms step_avg:58.10ms
step:1415/2330 train_time:82217ms step_avg:58.10ms
step:1416/2330 train_time:82277ms step_avg:58.11ms
step:1417/2330 train_time:82334ms step_avg:58.10ms
step:1418/2330 train_time:82394ms step_avg:58.11ms
step:1419/2330 train_time:82451ms step_avg:58.10ms
step:1420/2330 train_time:82511ms step_avg:58.11ms
step:1421/2330 train_time:82568ms step_avg:58.11ms
step:1422/2330 train_time:82628ms step_avg:58.11ms
step:1423/2330 train_time:82685ms step_avg:58.11ms
step:1424/2330 train_time:82745ms step_avg:58.11ms
step:1425/2330 train_time:82801ms step_avg:58.11ms
step:1426/2330 train_time:82862ms step_avg:58.11ms
step:1427/2330 train_time:82919ms step_avg:58.11ms
step:1428/2330 train_time:82979ms step_avg:58.11ms
step:1429/2330 train_time:83036ms step_avg:58.11ms
step:1430/2330 train_time:83097ms step_avg:58.11ms
step:1431/2330 train_time:83154ms step_avg:58.11ms
step:1432/2330 train_time:83214ms step_avg:58.11ms
step:1433/2330 train_time:83271ms step_avg:58.11ms
step:1434/2330 train_time:83331ms step_avg:58.11ms
step:1435/2330 train_time:83387ms step_avg:58.11ms
step:1436/2330 train_time:83448ms step_avg:58.11ms
step:1437/2330 train_time:83505ms step_avg:58.11ms
step:1438/2330 train_time:83565ms step_avg:58.11ms
step:1439/2330 train_time:83621ms step_avg:58.11ms
step:1440/2330 train_time:83682ms step_avg:58.11ms
step:1441/2330 train_time:83738ms step_avg:58.11ms
step:1442/2330 train_time:83800ms step_avg:58.11ms
step:1443/2330 train_time:83857ms step_avg:58.11ms
step:1444/2330 train_time:83917ms step_avg:58.11ms
step:1445/2330 train_time:83975ms step_avg:58.11ms
step:1446/2330 train_time:84034ms step_avg:58.11ms
step:1447/2330 train_time:84092ms step_avg:58.11ms
step:1448/2330 train_time:84151ms step_avg:58.12ms
step:1449/2330 train_time:84208ms step_avg:58.11ms
step:1450/2330 train_time:84268ms step_avg:58.12ms
step:1451/2330 train_time:84324ms step_avg:58.11ms
step:1452/2330 train_time:84385ms step_avg:58.12ms
step:1453/2330 train_time:84442ms step_avg:58.12ms
step:1454/2330 train_time:84502ms step_avg:58.12ms
step:1455/2330 train_time:84559ms step_avg:58.12ms
step:1456/2330 train_time:84619ms step_avg:58.12ms
step:1457/2330 train_time:84676ms step_avg:58.12ms
step:1458/2330 train_time:84736ms step_avg:58.12ms
step:1459/2330 train_time:84793ms step_avg:58.12ms
step:1460/2330 train_time:84853ms step_avg:58.12ms
step:1461/2330 train_time:84910ms step_avg:58.12ms
step:1462/2330 train_time:84971ms step_avg:58.12ms
step:1463/2330 train_time:85028ms step_avg:58.12ms
step:1464/2330 train_time:85089ms step_avg:58.12ms
step:1465/2330 train_time:85146ms step_avg:58.12ms
step:1466/2330 train_time:85207ms step_avg:58.12ms
step:1467/2330 train_time:85263ms step_avg:58.12ms
step:1468/2330 train_time:85323ms step_avg:58.12ms
step:1469/2330 train_time:85379ms step_avg:58.12ms
step:1470/2330 train_time:85441ms step_avg:58.12ms
step:1471/2330 train_time:85497ms step_avg:58.12ms
step:1472/2330 train_time:85558ms step_avg:58.12ms
step:1473/2330 train_time:85615ms step_avg:58.12ms
step:1474/2330 train_time:85676ms step_avg:58.12ms
step:1475/2330 train_time:85732ms step_avg:58.12ms
step:1476/2330 train_time:85793ms step_avg:58.13ms
step:1477/2330 train_time:85850ms step_avg:58.12ms
step:1478/2330 train_time:85911ms step_avg:58.13ms
step:1479/2330 train_time:85968ms step_avg:58.13ms
step:1480/2330 train_time:86029ms step_avg:58.13ms
step:1481/2330 train_time:86085ms step_avg:58.13ms
step:1482/2330 train_time:86146ms step_avg:58.13ms
step:1483/2330 train_time:86203ms step_avg:58.13ms
step:1484/2330 train_time:86264ms step_avg:58.13ms
step:1485/2330 train_time:86320ms step_avg:58.13ms
step:1486/2330 train_time:86382ms step_avg:58.13ms
step:1487/2330 train_time:86438ms step_avg:58.13ms
step:1488/2330 train_time:86499ms step_avg:58.13ms
step:1489/2330 train_time:86555ms step_avg:58.13ms
step:1490/2330 train_time:86615ms step_avg:58.13ms
step:1491/2330 train_time:86673ms step_avg:58.13ms
step:1492/2330 train_time:86732ms step_avg:58.13ms
step:1493/2330 train_time:86789ms step_avg:58.13ms
step:1494/2330 train_time:86850ms step_avg:58.13ms
step:1495/2330 train_time:86906ms step_avg:58.13ms
step:1496/2330 train_time:86967ms step_avg:58.13ms
step:1497/2330 train_time:87025ms step_avg:58.13ms
step:1498/2330 train_time:87084ms step_avg:58.13ms
step:1499/2330 train_time:87141ms step_avg:58.13ms
step:1500/2330 train_time:87203ms step_avg:58.14ms
step:1500/2330 val_loss:3.9051 train_time:87283ms step_avg:58.19ms
step:1501/2330 train_time:87303ms step_avg:58.16ms
step:1502/2330 train_time:87323ms step_avg:58.14ms
step:1503/2330 train_time:87380ms step_avg:58.14ms
step:1504/2330 train_time:87447ms step_avg:58.14ms
step:1505/2330 train_time:87503ms step_avg:58.14ms
step:1506/2330 train_time:87564ms step_avg:58.14ms
step:1507/2330 train_time:87621ms step_avg:58.14ms
step:1508/2330 train_time:87680ms step_avg:58.14ms
step:1509/2330 train_time:87737ms step_avg:58.14ms
step:1510/2330 train_time:87796ms step_avg:58.14ms
step:1511/2330 train_time:87853ms step_avg:58.14ms
step:1512/2330 train_time:87912ms step_avg:58.14ms
step:1513/2330 train_time:87968ms step_avg:58.14ms
step:1514/2330 train_time:88027ms step_avg:58.14ms
step:1515/2330 train_time:88084ms step_avg:58.14ms
step:1516/2330 train_time:88144ms step_avg:58.14ms
step:1517/2330 train_time:88201ms step_avg:58.14ms
step:1518/2330 train_time:88262ms step_avg:58.14ms
step:1519/2330 train_time:88320ms step_avg:58.14ms
step:1520/2330 train_time:88383ms step_avg:58.15ms
step:1521/2330 train_time:88441ms step_avg:58.15ms
step:1522/2330 train_time:88502ms step_avg:58.15ms
step:1523/2330 train_time:88559ms step_avg:58.15ms
step:1524/2330 train_time:88620ms step_avg:58.15ms
step:1525/2330 train_time:88677ms step_avg:58.15ms
step:1526/2330 train_time:88738ms step_avg:58.15ms
step:1527/2330 train_time:88795ms step_avg:58.15ms
step:1528/2330 train_time:88854ms step_avg:58.15ms
step:1529/2330 train_time:88911ms step_avg:58.15ms
step:1530/2330 train_time:88971ms step_avg:58.15ms
step:1531/2330 train_time:89028ms step_avg:58.15ms
step:1532/2330 train_time:89089ms step_avg:58.15ms
step:1533/2330 train_time:89146ms step_avg:58.15ms
step:1534/2330 train_time:89206ms step_avg:58.15ms
step:1535/2330 train_time:89263ms step_avg:58.15ms
step:1536/2330 train_time:89325ms step_avg:58.15ms
step:1537/2330 train_time:89382ms step_avg:58.15ms
step:1538/2330 train_time:89447ms step_avg:58.16ms
step:1539/2330 train_time:89503ms step_avg:58.16ms
step:1540/2330 train_time:89567ms step_avg:58.16ms
step:1541/2330 train_time:89623ms step_avg:58.16ms
step:1542/2330 train_time:89685ms step_avg:58.16ms
step:1543/2330 train_time:89743ms step_avg:58.16ms
step:1544/2330 train_time:89804ms step_avg:58.16ms
step:1545/2330 train_time:89861ms step_avg:58.16ms
step:1546/2330 train_time:89921ms step_avg:58.16ms
step:1547/2330 train_time:89978ms step_avg:58.16ms
step:1548/2330 train_time:90039ms step_avg:58.16ms
step:1549/2330 train_time:90097ms step_avg:58.16ms
step:1550/2330 train_time:90157ms step_avg:58.17ms
step:1551/2330 train_time:90215ms step_avg:58.17ms
step:1552/2330 train_time:90275ms step_avg:58.17ms
step:1553/2330 train_time:90334ms step_avg:58.17ms
step:1554/2330 train_time:90396ms step_avg:58.17ms
step:1555/2330 train_time:90453ms step_avg:58.17ms
step:1556/2330 train_time:90515ms step_avg:58.17ms
step:1557/2330 train_time:90572ms step_avg:58.17ms
step:1558/2330 train_time:90634ms step_avg:58.17ms
step:1559/2330 train_time:90691ms step_avg:58.17ms
step:1560/2330 train_time:90753ms step_avg:58.18ms
step:1561/2330 train_time:90810ms step_avg:58.17ms
step:1562/2330 train_time:90870ms step_avg:58.18ms
step:1563/2330 train_time:90927ms step_avg:58.17ms
step:1564/2330 train_time:90988ms step_avg:58.18ms
step:1565/2330 train_time:91044ms step_avg:58.18ms
step:1566/2330 train_time:91106ms step_avg:58.18ms
step:1567/2330 train_time:91163ms step_avg:58.18ms
step:1568/2330 train_time:91224ms step_avg:58.18ms
step:1569/2330 train_time:91281ms step_avg:58.18ms
step:1570/2330 train_time:91342ms step_avg:58.18ms
step:1571/2330 train_time:91400ms step_avg:58.18ms
step:1572/2330 train_time:91461ms step_avg:58.18ms
step:1573/2330 train_time:91519ms step_avg:58.18ms
step:1574/2330 train_time:91581ms step_avg:58.18ms
step:1575/2330 train_time:91639ms step_avg:58.18ms
step:1576/2330 train_time:91701ms step_avg:58.19ms
step:1577/2330 train_time:91758ms step_avg:58.19ms
step:1578/2330 train_time:91820ms step_avg:58.19ms
step:1579/2330 train_time:91878ms step_avg:58.19ms
step:1580/2330 train_time:91939ms step_avg:58.19ms
step:1581/2330 train_time:91996ms step_avg:58.19ms
step:1582/2330 train_time:92057ms step_avg:58.19ms
step:1583/2330 train_time:92115ms step_avg:58.19ms
step:1584/2330 train_time:92175ms step_avg:58.19ms
step:1585/2330 train_time:92232ms step_avg:58.19ms
step:1586/2330 train_time:92293ms step_avg:58.19ms
step:1587/2330 train_time:92350ms step_avg:58.19ms
step:1588/2330 train_time:92411ms step_avg:58.19ms
step:1589/2330 train_time:92468ms step_avg:58.19ms
step:1590/2330 train_time:92530ms step_avg:58.19ms
step:1591/2330 train_time:92587ms step_avg:58.19ms
step:1592/2330 train_time:92649ms step_avg:58.20ms
step:1593/2330 train_time:92706ms step_avg:58.20ms
step:1594/2330 train_time:92769ms step_avg:58.20ms
step:1595/2330 train_time:92826ms step_avg:58.20ms
step:1596/2330 train_time:92886ms step_avg:58.20ms
step:1597/2330 train_time:92943ms step_avg:58.20ms
step:1598/2330 train_time:93004ms step_avg:58.20ms
step:1599/2330 train_time:93060ms step_avg:58.20ms
step:1600/2330 train_time:93122ms step_avg:58.20ms
step:1601/2330 train_time:93179ms step_avg:58.20ms
step:1602/2330 train_time:93239ms step_avg:58.20ms
step:1603/2330 train_time:93297ms step_avg:58.20ms
step:1604/2330 train_time:93358ms step_avg:58.20ms
step:1605/2330 train_time:93417ms step_avg:58.20ms
step:1606/2330 train_time:93478ms step_avg:58.21ms
step:1607/2330 train_time:93537ms step_avg:58.21ms
step:1608/2330 train_time:93598ms step_avg:58.21ms
step:1609/2330 train_time:93656ms step_avg:58.21ms
step:1610/2330 train_time:93717ms step_avg:58.21ms
step:1611/2330 train_time:93774ms step_avg:58.21ms
step:1612/2330 train_time:93836ms step_avg:58.21ms
step:1613/2330 train_time:93893ms step_avg:58.21ms
step:1614/2330 train_time:93954ms step_avg:58.21ms
step:1615/2330 train_time:94011ms step_avg:58.21ms
step:1616/2330 train_time:94073ms step_avg:58.21ms
step:1617/2330 train_time:94130ms step_avg:58.21ms
step:1618/2330 train_time:94192ms step_avg:58.21ms
step:1619/2330 train_time:94248ms step_avg:58.21ms
step:1620/2330 train_time:94309ms step_avg:58.22ms
step:1621/2330 train_time:94366ms step_avg:58.21ms
step:1622/2330 train_time:94428ms step_avg:58.22ms
step:1623/2330 train_time:94485ms step_avg:58.22ms
step:1624/2330 train_time:94546ms step_avg:58.22ms
step:1625/2330 train_time:94604ms step_avg:58.22ms
step:1626/2330 train_time:94665ms step_avg:58.22ms
step:1627/2330 train_time:94722ms step_avg:58.22ms
step:1628/2330 train_time:94784ms step_avg:58.22ms
step:1629/2330 train_time:94842ms step_avg:58.22ms
step:1630/2330 train_time:94901ms step_avg:58.22ms
step:1631/2330 train_time:94959ms step_avg:58.22ms
step:1632/2330 train_time:95019ms step_avg:58.22ms
step:1633/2330 train_time:95077ms step_avg:58.22ms
step:1634/2330 train_time:95139ms step_avg:58.22ms
step:1635/2330 train_time:95196ms step_avg:58.22ms
step:1636/2330 train_time:95257ms step_avg:58.23ms
step:1637/2330 train_time:95315ms step_avg:58.23ms
step:1638/2330 train_time:95376ms step_avg:58.23ms
step:1639/2330 train_time:95433ms step_avg:58.23ms
step:1640/2330 train_time:95495ms step_avg:58.23ms
step:1641/2330 train_time:95553ms step_avg:58.23ms
step:1642/2330 train_time:95615ms step_avg:58.23ms
step:1643/2330 train_time:95671ms step_avg:58.23ms
step:1644/2330 train_time:95734ms step_avg:58.23ms
step:1645/2330 train_time:95790ms step_avg:58.23ms
step:1646/2330 train_time:95853ms step_avg:58.23ms
step:1647/2330 train_time:95910ms step_avg:58.23ms
step:1648/2330 train_time:95972ms step_avg:58.24ms
step:1649/2330 train_time:96029ms step_avg:58.23ms
step:1650/2330 train_time:96091ms step_avg:58.24ms
step:1651/2330 train_time:96147ms step_avg:58.24ms
step:1652/2330 train_time:96210ms step_avg:58.24ms
step:1653/2330 train_time:96266ms step_avg:58.24ms
step:1654/2330 train_time:96327ms step_avg:58.24ms
step:1655/2330 train_time:96383ms step_avg:58.24ms
step:1656/2330 train_time:96446ms step_avg:58.24ms
step:1657/2330 train_time:96502ms step_avg:58.24ms
step:1658/2330 train_time:96564ms step_avg:58.24ms
step:1659/2330 train_time:96623ms step_avg:58.24ms
step:1660/2330 train_time:96683ms step_avg:58.24ms
step:1661/2330 train_time:96741ms step_avg:58.24ms
step:1662/2330 train_time:96801ms step_avg:58.24ms
step:1663/2330 train_time:96858ms step_avg:58.24ms
step:1664/2330 train_time:96920ms step_avg:58.25ms
step:1665/2330 train_time:96977ms step_avg:58.24ms
step:1666/2330 train_time:97039ms step_avg:58.25ms
step:1667/2330 train_time:97097ms step_avg:58.25ms
step:1668/2330 train_time:97158ms step_avg:58.25ms
step:1669/2330 train_time:97216ms step_avg:58.25ms
step:1670/2330 train_time:97276ms step_avg:58.25ms
step:1671/2330 train_time:97334ms step_avg:58.25ms
step:1672/2330 train_time:97395ms step_avg:58.25ms
step:1673/2330 train_time:97452ms step_avg:58.25ms
step:1674/2330 train_time:97514ms step_avg:58.25ms
step:1675/2330 train_time:97571ms step_avg:58.25ms
step:1676/2330 train_time:97632ms step_avg:58.25ms
step:1677/2330 train_time:97689ms step_avg:58.25ms
step:1678/2330 train_time:97752ms step_avg:58.25ms
step:1679/2330 train_time:97809ms step_avg:58.25ms
step:1680/2330 train_time:97870ms step_avg:58.26ms
step:1681/2330 train_time:97927ms step_avg:58.25ms
step:1682/2330 train_time:97988ms step_avg:58.26ms
step:1683/2330 train_time:98046ms step_avg:58.26ms
step:1684/2330 train_time:98107ms step_avg:58.26ms
step:1685/2330 train_time:98164ms step_avg:58.26ms
step:1686/2330 train_time:98225ms step_avg:58.26ms
step:1687/2330 train_time:98282ms step_avg:58.26ms
step:1688/2330 train_time:98343ms step_avg:58.26ms
step:1689/2330 train_time:98401ms step_avg:58.26ms
step:1690/2330 train_time:98462ms step_avg:58.26ms
step:1691/2330 train_time:98519ms step_avg:58.26ms
step:1692/2330 train_time:98581ms step_avg:58.26ms
step:1693/2330 train_time:98639ms step_avg:58.26ms
step:1694/2330 train_time:98699ms step_avg:58.26ms
step:1695/2330 train_time:98757ms step_avg:58.26ms
step:1696/2330 train_time:98819ms step_avg:58.27ms
step:1697/2330 train_time:98877ms step_avg:58.27ms
step:1698/2330 train_time:98938ms step_avg:58.27ms
step:1699/2330 train_time:98996ms step_avg:58.27ms
step:1700/2330 train_time:99056ms step_avg:58.27ms
step:1701/2330 train_time:99113ms step_avg:58.27ms
step:1702/2330 train_time:99174ms step_avg:58.27ms
step:1703/2330 train_time:99231ms step_avg:58.27ms
step:1704/2330 train_time:99293ms step_avg:58.27ms
step:1705/2330 train_time:99350ms step_avg:58.27ms
step:1706/2330 train_time:99411ms step_avg:58.27ms
step:1707/2330 train_time:99467ms step_avg:58.27ms
step:1708/2330 train_time:99529ms step_avg:58.27ms
step:1709/2330 train_time:99586ms step_avg:58.27ms
step:1710/2330 train_time:99648ms step_avg:58.27ms
step:1711/2330 train_time:99704ms step_avg:58.27ms
step:1712/2330 train_time:99767ms step_avg:58.28ms
step:1713/2330 train_time:99824ms step_avg:58.27ms
step:1714/2330 train_time:99886ms step_avg:58.28ms
step:1715/2330 train_time:99943ms step_avg:58.28ms
step:1716/2330 train_time:100005ms step_avg:58.28ms
step:1717/2330 train_time:100062ms step_avg:58.28ms
step:1718/2330 train_time:100123ms step_avg:58.28ms
step:1719/2330 train_time:100180ms step_avg:58.28ms
step:1720/2330 train_time:100241ms step_avg:58.28ms
step:1721/2330 train_time:100299ms step_avg:58.28ms
step:1722/2330 train_time:100359ms step_avg:58.28ms
step:1723/2330 train_time:100416ms step_avg:58.28ms
step:1724/2330 train_time:100478ms step_avg:58.28ms
step:1725/2330 train_time:100536ms step_avg:58.28ms
step:1726/2330 train_time:100597ms step_avg:58.28ms
step:1727/2330 train_time:100656ms step_avg:58.28ms
step:1728/2330 train_time:100717ms step_avg:58.29ms
step:1729/2330 train_time:100773ms step_avg:58.28ms
step:1730/2330 train_time:100835ms step_avg:58.29ms
step:1731/2330 train_time:100893ms step_avg:58.29ms
step:1732/2330 train_time:100955ms step_avg:58.29ms
step:1733/2330 train_time:101011ms step_avg:58.29ms
step:1734/2330 train_time:101073ms step_avg:58.29ms
step:1735/2330 train_time:101130ms step_avg:58.29ms
step:1736/2330 train_time:101192ms step_avg:58.29ms
step:1737/2330 train_time:101249ms step_avg:58.29ms
step:1738/2330 train_time:101311ms step_avg:58.29ms
step:1739/2330 train_time:101368ms step_avg:58.29ms
step:1740/2330 train_time:101430ms step_avg:58.29ms
step:1741/2330 train_time:101487ms step_avg:58.29ms
step:1742/2330 train_time:101548ms step_avg:58.29ms
step:1743/2330 train_time:101604ms step_avg:58.29ms
step:1744/2330 train_time:101665ms step_avg:58.29ms
step:1745/2330 train_time:101723ms step_avg:58.29ms
step:1746/2330 train_time:101785ms step_avg:58.30ms
step:1747/2330 train_time:101842ms step_avg:58.30ms
step:1748/2330 train_time:101903ms step_avg:58.30ms
step:1749/2330 train_time:101961ms step_avg:58.30ms
step:1750/2330 train_time:102021ms step_avg:58.30ms
step:1750/2330 val_loss:3.8204 train_time:102102ms step_avg:58.34ms
step:1751/2330 train_time:102121ms step_avg:58.32ms
step:1752/2330 train_time:102140ms step_avg:58.30ms
step:1753/2330 train_time:102196ms step_avg:58.30ms
step:1754/2330 train_time:102265ms step_avg:58.30ms
step:1755/2330 train_time:102322ms step_avg:58.30ms
step:1756/2330 train_time:102384ms step_avg:58.31ms
step:1757/2330 train_time:102440ms step_avg:58.30ms
step:1758/2330 train_time:102501ms step_avg:58.31ms
step:1759/2330 train_time:102558ms step_avg:58.30ms
step:1760/2330 train_time:102618ms step_avg:58.31ms
step:1761/2330 train_time:102675ms step_avg:58.30ms
step:1762/2330 train_time:102735ms step_avg:58.31ms
step:1763/2330 train_time:102792ms step_avg:58.30ms
step:1764/2330 train_time:102851ms step_avg:58.31ms
step:1765/2330 train_time:102908ms step_avg:58.30ms
step:1766/2330 train_time:102967ms step_avg:58.31ms
step:1767/2330 train_time:103027ms step_avg:58.31ms
step:1768/2330 train_time:103091ms step_avg:58.31ms
step:1769/2330 train_time:103149ms step_avg:58.31ms
step:1770/2330 train_time:103211ms step_avg:58.31ms
step:1771/2330 train_time:103268ms step_avg:58.31ms
step:1772/2330 train_time:103330ms step_avg:58.31ms
step:1773/2330 train_time:103387ms step_avg:58.31ms
step:1774/2330 train_time:103449ms step_avg:58.31ms
step:1775/2330 train_time:103506ms step_avg:58.31ms
step:1776/2330 train_time:103567ms step_avg:58.31ms
step:1777/2330 train_time:103624ms step_avg:58.31ms
step:1778/2330 train_time:103685ms step_avg:58.32ms
step:1779/2330 train_time:103742ms step_avg:58.31ms
step:1780/2330 train_time:103802ms step_avg:58.32ms
step:1781/2330 train_time:103860ms step_avg:58.32ms
step:1782/2330 train_time:103920ms step_avg:58.32ms
step:1783/2330 train_time:103979ms step_avg:58.32ms
step:1784/2330 train_time:104039ms step_avg:58.32ms
step:1785/2330 train_time:104097ms step_avg:58.32ms
step:1786/2330 train_time:104161ms step_avg:58.32ms
step:1787/2330 train_time:104218ms step_avg:58.32ms
step:1788/2330 train_time:104281ms step_avg:58.32ms
step:1789/2330 train_time:104338ms step_avg:58.32ms
step:1790/2330 train_time:104399ms step_avg:58.32ms
step:1791/2330 train_time:104456ms step_avg:58.32ms
step:1792/2330 train_time:104518ms step_avg:58.32ms
step:1793/2330 train_time:104574ms step_avg:58.32ms
step:1794/2330 train_time:104635ms step_avg:58.33ms
step:1795/2330 train_time:104692ms step_avg:58.32ms
step:1796/2330 train_time:104753ms step_avg:58.33ms
step:1797/2330 train_time:104809ms step_avg:58.32ms
step:1798/2330 train_time:104870ms step_avg:58.33ms
step:1799/2330 train_time:104927ms step_avg:58.32ms
step:1800/2330 train_time:104988ms step_avg:58.33ms
step:1801/2330 train_time:105046ms step_avg:58.33ms
step:1802/2330 train_time:105107ms step_avg:58.33ms
step:1803/2330 train_time:105165ms step_avg:58.33ms
step:1804/2330 train_time:105226ms step_avg:58.33ms
step:1805/2330 train_time:105284ms step_avg:58.33ms
step:1806/2330 train_time:105346ms step_avg:58.33ms
step:1807/2330 train_time:105403ms step_avg:58.33ms
step:1808/2330 train_time:105464ms step_avg:58.33ms
step:1809/2330 train_time:105521ms step_avg:58.33ms
step:1810/2330 train_time:105583ms step_avg:58.33ms
step:1811/2330 train_time:105641ms step_avg:58.33ms
step:1812/2330 train_time:105701ms step_avg:58.33ms
step:1813/2330 train_time:105758ms step_avg:58.33ms
step:1814/2330 train_time:105819ms step_avg:58.33ms
step:1815/2330 train_time:105877ms step_avg:58.33ms
step:1816/2330 train_time:105938ms step_avg:58.34ms
step:1817/2330 train_time:105995ms step_avg:58.34ms
step:1818/2330 train_time:106055ms step_avg:58.34ms
step:1819/2330 train_time:106113ms step_avg:58.34ms
step:1820/2330 train_time:106174ms step_avg:58.34ms
step:1821/2330 train_time:106231ms step_avg:58.34ms
step:1822/2330 train_time:106292ms step_avg:58.34ms
step:1823/2330 train_time:106350ms step_avg:58.34ms
step:1824/2330 train_time:106412ms step_avg:58.34ms
step:1825/2330 train_time:106469ms step_avg:58.34ms
step:1826/2330 train_time:106530ms step_avg:58.34ms
step:1827/2330 train_time:106586ms step_avg:58.34ms
step:1828/2330 train_time:106649ms step_avg:58.34ms
step:1829/2330 train_time:106706ms step_avg:58.34ms
step:1830/2330 train_time:106767ms step_avg:58.34ms
step:1831/2330 train_time:106824ms step_avg:58.34ms
step:1832/2330 train_time:106885ms step_avg:58.34ms
step:1833/2330 train_time:106943ms step_avg:58.34ms
step:1834/2330 train_time:107004ms step_avg:58.34ms
step:1835/2330 train_time:107062ms step_avg:58.34ms
step:1836/2330 train_time:107123ms step_avg:58.35ms
step:1837/2330 train_time:107181ms step_avg:58.35ms
step:1838/2330 train_time:107242ms step_avg:58.35ms
step:1839/2330 train_time:107300ms step_avg:58.35ms
step:1840/2330 train_time:107362ms step_avg:58.35ms
step:1841/2330 train_time:107420ms step_avg:58.35ms
step:1842/2330 train_time:107482ms step_avg:58.35ms
step:1843/2330 train_time:107539ms step_avg:58.35ms
step:1844/2330 train_time:107600ms step_avg:58.35ms
step:1845/2330 train_time:107658ms step_avg:58.35ms
step:1846/2330 train_time:107718ms step_avg:58.35ms
step:1847/2330 train_time:107775ms step_avg:58.35ms
step:1848/2330 train_time:107835ms step_avg:58.35ms
step:1849/2330 train_time:107892ms step_avg:58.35ms
step:1850/2330 train_time:107952ms step_avg:58.35ms
step:1851/2330 train_time:108010ms step_avg:58.35ms
step:1852/2330 train_time:108071ms step_avg:58.35ms
step:1853/2330 train_time:108128ms step_avg:58.35ms
step:1854/2330 train_time:108190ms step_avg:58.35ms
step:1855/2330 train_time:108247ms step_avg:58.35ms
step:1856/2330 train_time:108308ms step_avg:58.36ms
step:1857/2330 train_time:108366ms step_avg:58.36ms
step:1858/2330 train_time:108426ms step_avg:58.36ms
step:1859/2330 train_time:108484ms step_avg:58.36ms
step:1860/2330 train_time:108545ms step_avg:58.36ms
step:1861/2330 train_time:108603ms step_avg:58.36ms
step:1862/2330 train_time:108664ms step_avg:58.36ms
step:1863/2330 train_time:108721ms step_avg:58.36ms
step:1864/2330 train_time:108783ms step_avg:58.36ms
step:1865/2330 train_time:108841ms step_avg:58.36ms
step:1866/2330 train_time:108901ms step_avg:58.36ms
step:1867/2330 train_time:108959ms step_avg:58.36ms
step:1868/2330 train_time:109020ms step_avg:58.36ms
step:1869/2330 train_time:109078ms step_avg:58.36ms
step:1870/2330 train_time:109139ms step_avg:58.36ms
step:1871/2330 train_time:109196ms step_avg:58.36ms
step:1872/2330 train_time:109258ms step_avg:58.36ms
step:1873/2330 train_time:109315ms step_avg:58.36ms
step:1874/2330 train_time:109375ms step_avg:58.36ms
step:1875/2330 train_time:109432ms step_avg:58.36ms
step:1876/2330 train_time:109494ms step_avg:58.37ms
step:1877/2330 train_time:109551ms step_avg:58.36ms
step:1878/2330 train_time:109613ms step_avg:58.37ms
step:1879/2330 train_time:109669ms step_avg:58.37ms
step:1880/2330 train_time:109731ms step_avg:58.37ms
step:1881/2330 train_time:109788ms step_avg:58.37ms
step:1882/2330 train_time:109850ms step_avg:58.37ms
step:1883/2330 train_time:109908ms step_avg:58.37ms
step:1884/2330 train_time:109969ms step_avg:58.37ms
step:1885/2330 train_time:110026ms step_avg:58.37ms
step:1886/2330 train_time:110087ms step_avg:58.37ms
step:1887/2330 train_time:110145ms step_avg:58.37ms
step:1888/2330 train_time:110205ms step_avg:58.37ms
step:1889/2330 train_time:110263ms step_avg:58.37ms
step:1890/2330 train_time:110324ms step_avg:58.37ms
step:1891/2330 train_time:110382ms step_avg:58.37ms
step:1892/2330 train_time:110444ms step_avg:58.37ms
step:1893/2330 train_time:110502ms step_avg:58.37ms
step:1894/2330 train_time:110562ms step_avg:58.38ms
step:1895/2330 train_time:110619ms step_avg:58.37ms
step:1896/2330 train_time:110681ms step_avg:58.38ms
step:1897/2330 train_time:110738ms step_avg:58.38ms
step:1898/2330 train_time:110800ms step_avg:58.38ms
step:1899/2330 train_time:110858ms step_avg:58.38ms
step:1900/2330 train_time:110919ms step_avg:58.38ms
step:1901/2330 train_time:110976ms step_avg:58.38ms
step:1902/2330 train_time:111037ms step_avg:58.38ms
step:1903/2330 train_time:111094ms step_avg:58.38ms
step:1904/2330 train_time:111156ms step_avg:58.38ms
step:1905/2330 train_time:111213ms step_avg:58.38ms
step:1906/2330 train_time:111274ms step_avg:58.38ms
step:1907/2330 train_time:111331ms step_avg:58.38ms
step:1908/2330 train_time:111393ms step_avg:58.38ms
step:1909/2330 train_time:111449ms step_avg:58.38ms
step:1910/2330 train_time:111511ms step_avg:58.38ms
step:1911/2330 train_time:111568ms step_avg:58.38ms
step:1912/2330 train_time:111630ms step_avg:58.38ms
step:1913/2330 train_time:111687ms step_avg:58.38ms
step:1914/2330 train_time:111750ms step_avg:58.39ms
step:1915/2330 train_time:111807ms step_avg:58.38ms
step:1916/2330 train_time:111868ms step_avg:58.39ms
step:1917/2330 train_time:111926ms step_avg:58.39ms
step:1918/2330 train_time:111987ms step_avg:58.39ms
step:1919/2330 train_time:112045ms step_avg:58.39ms
step:1920/2330 train_time:112106ms step_avg:58.39ms
step:1921/2330 train_time:112164ms step_avg:58.39ms
step:1922/2330 train_time:112225ms step_avg:58.39ms
step:1923/2330 train_time:112283ms step_avg:58.39ms
step:1924/2330 train_time:112344ms step_avg:58.39ms
step:1925/2330 train_time:112402ms step_avg:58.39ms
step:1926/2330 train_time:112462ms step_avg:58.39ms
step:1927/2330 train_time:112520ms step_avg:58.39ms
step:1928/2330 train_time:112581ms step_avg:58.39ms
step:1929/2330 train_time:112638ms step_avg:58.39ms
step:1930/2330 train_time:112700ms step_avg:58.39ms
step:1931/2330 train_time:112758ms step_avg:58.39ms
step:1932/2330 train_time:112818ms step_avg:58.39ms
step:1933/2330 train_time:112876ms step_avg:58.39ms
step:1934/2330 train_time:112936ms step_avg:58.39ms
step:1935/2330 train_time:112994ms step_avg:58.39ms
step:1936/2330 train_time:113054ms step_avg:58.40ms
step:1937/2330 train_time:113112ms step_avg:58.40ms
step:1938/2330 train_time:113172ms step_avg:58.40ms
step:1939/2330 train_time:113229ms step_avg:58.40ms
step:1940/2330 train_time:113290ms step_avg:58.40ms
step:1941/2330 train_time:113347ms step_avg:58.40ms
step:1942/2330 train_time:113408ms step_avg:58.40ms
step:1943/2330 train_time:113466ms step_avg:58.40ms
step:1944/2330 train_time:113527ms step_avg:58.40ms
step:1945/2330 train_time:113584ms step_avg:58.40ms
step:1946/2330 train_time:113646ms step_avg:58.40ms
step:1947/2330 train_time:113703ms step_avg:58.40ms
step:1948/2330 train_time:113764ms step_avg:58.40ms
step:1949/2330 train_time:113822ms step_avg:58.40ms
step:1950/2330 train_time:113882ms step_avg:58.40ms
step:1951/2330 train_time:113940ms step_avg:58.40ms
step:1952/2330 train_time:114001ms step_avg:58.40ms
step:1953/2330 train_time:114060ms step_avg:58.40ms
step:1954/2330 train_time:114120ms step_avg:58.40ms
step:1955/2330 train_time:114179ms step_avg:58.40ms
step:1956/2330 train_time:114240ms step_avg:58.40ms
step:1957/2330 train_time:114298ms step_avg:58.40ms
step:1958/2330 train_time:114359ms step_avg:58.41ms
step:1959/2330 train_time:114416ms step_avg:58.41ms
step:1960/2330 train_time:114477ms step_avg:58.41ms
step:1961/2330 train_time:114534ms step_avg:58.41ms
step:1962/2330 train_time:114596ms step_avg:58.41ms
step:1963/2330 train_time:114652ms step_avg:58.41ms
step:1964/2330 train_time:114716ms step_avg:58.41ms
step:1965/2330 train_time:114772ms step_avg:58.41ms
step:1966/2330 train_time:114834ms step_avg:58.41ms
step:1967/2330 train_time:114891ms step_avg:58.41ms
step:1968/2330 train_time:114952ms step_avg:58.41ms
step:1969/2330 train_time:115009ms step_avg:58.41ms
step:1970/2330 train_time:115070ms step_avg:58.41ms
step:1971/2330 train_time:115127ms step_avg:58.41ms
step:1972/2330 train_time:115189ms step_avg:58.41ms
step:1973/2330 train_time:115247ms step_avg:58.41ms
step:1974/2330 train_time:115307ms step_avg:58.41ms
step:1975/2330 train_time:115365ms step_avg:58.41ms
step:1976/2330 train_time:115425ms step_avg:58.41ms
step:1977/2330 train_time:115484ms step_avg:58.41ms
step:1978/2330 train_time:115544ms step_avg:58.41ms
step:1979/2330 train_time:115602ms step_avg:58.41ms
step:1980/2330 train_time:115664ms step_avg:58.42ms
step:1981/2330 train_time:115722ms step_avg:58.42ms
step:1982/2330 train_time:115782ms step_avg:58.42ms
step:1983/2330 train_time:115840ms step_avg:58.42ms
step:1984/2330 train_time:115901ms step_avg:58.42ms
step:1985/2330 train_time:115958ms step_avg:58.42ms
step:1986/2330 train_time:116019ms step_avg:58.42ms
step:1987/2330 train_time:116076ms step_avg:58.42ms
step:1988/2330 train_time:116137ms step_avg:58.42ms
step:1989/2330 train_time:116194ms step_avg:58.42ms
step:1990/2330 train_time:116255ms step_avg:58.42ms
step:1991/2330 train_time:116312ms step_avg:58.42ms
step:1992/2330 train_time:116374ms step_avg:58.42ms
step:1993/2330 train_time:116431ms step_avg:58.42ms
step:1994/2330 train_time:116493ms step_avg:58.42ms
step:1995/2330 train_time:116550ms step_avg:58.42ms
step:1996/2330 train_time:116613ms step_avg:58.42ms
step:1997/2330 train_time:116670ms step_avg:58.42ms
step:1998/2330 train_time:116732ms step_avg:58.42ms
step:1999/2330 train_time:116788ms step_avg:58.42ms
step:2000/2330 train_time:116850ms step_avg:58.43ms
step:2000/2330 val_loss:3.7577 train_time:116932ms step_avg:58.47ms
step:2001/2330 train_time:116950ms step_avg:58.45ms
step:2002/2330 train_time:116971ms step_avg:58.43ms
step:2003/2330 train_time:117032ms step_avg:58.43ms
step:2004/2330 train_time:117096ms step_avg:58.43ms
step:2005/2330 train_time:117155ms step_avg:58.43ms
step:2006/2330 train_time:117215ms step_avg:58.43ms
step:2007/2330 train_time:117273ms step_avg:58.43ms
step:2008/2330 train_time:117332ms step_avg:58.43ms
step:2009/2330 train_time:117389ms step_avg:58.43ms
step:2010/2330 train_time:117449ms step_avg:58.43ms
step:2011/2330 train_time:117505ms step_avg:58.43ms
step:2012/2330 train_time:117565ms step_avg:58.43ms
step:2013/2330 train_time:117622ms step_avg:58.43ms
step:2014/2330 train_time:117682ms step_avg:58.43ms
step:2015/2330 train_time:117739ms step_avg:58.43ms
step:2016/2330 train_time:117798ms step_avg:58.43ms
step:2017/2330 train_time:117855ms step_avg:58.43ms
step:2018/2330 train_time:117918ms step_avg:58.43ms
step:2019/2330 train_time:117976ms step_avg:58.43ms
step:2020/2330 train_time:118039ms step_avg:58.44ms
step:2021/2330 train_time:118097ms step_avg:58.43ms
step:2022/2330 train_time:118160ms step_avg:58.44ms
step:2023/2330 train_time:118217ms step_avg:58.44ms
step:2024/2330 train_time:118279ms step_avg:58.44ms
step:2025/2330 train_time:118335ms step_avg:58.44ms
step:2026/2330 train_time:118398ms step_avg:58.44ms
step:2027/2330 train_time:118454ms step_avg:58.44ms
step:2028/2330 train_time:118514ms step_avg:58.44ms
step:2029/2330 train_time:118571ms step_avg:58.44ms
step:2030/2330 train_time:118632ms step_avg:58.44ms
step:2031/2330 train_time:118689ms step_avg:58.44ms
step:2032/2330 train_time:118749ms step_avg:58.44ms
step:2033/2330 train_time:118806ms step_avg:58.44ms
step:2034/2330 train_time:118867ms step_avg:58.44ms
step:2035/2330 train_time:118924ms step_avg:58.44ms
step:2036/2330 train_time:118986ms step_avg:58.44ms
step:2037/2330 train_time:119045ms step_avg:58.44ms
step:2038/2330 train_time:119107ms step_avg:58.44ms
step:2039/2330 train_time:119165ms step_avg:58.44ms
step:2040/2330 train_time:119226ms step_avg:58.44ms
step:2041/2330 train_time:119284ms step_avg:58.44ms
step:2042/2330 train_time:119345ms step_avg:58.45ms
step:2043/2330 train_time:119402ms step_avg:58.44ms
step:2044/2330 train_time:119463ms step_avg:58.45ms
step:2045/2330 train_time:119519ms step_avg:58.44ms
step:2046/2330 train_time:119582ms step_avg:58.45ms
step:2047/2330 train_time:119639ms step_avg:58.45ms
step:2048/2330 train_time:119701ms step_avg:58.45ms
step:2049/2330 train_time:119757ms step_avg:58.45ms
step:2050/2330 train_time:119818ms step_avg:58.45ms
step:2051/2330 train_time:119875ms step_avg:58.45ms
step:2052/2330 train_time:119937ms step_avg:58.45ms
step:2053/2330 train_time:119994ms step_avg:58.45ms
step:2054/2330 train_time:120057ms step_avg:58.45ms
step:2055/2330 train_time:120114ms step_avg:58.45ms
step:2056/2330 train_time:120176ms step_avg:58.45ms
step:2057/2330 train_time:120234ms step_avg:58.45ms
step:2058/2330 train_time:120294ms step_avg:58.45ms
step:2059/2330 train_time:120352ms step_avg:58.45ms
step:2060/2330 train_time:120413ms step_avg:58.45ms
step:2061/2330 train_time:120470ms step_avg:58.45ms
step:2062/2330 train_time:120530ms step_avg:58.45ms
step:2063/2330 train_time:120587ms step_avg:58.45ms
step:2064/2330 train_time:120649ms step_avg:58.45ms
step:2065/2330 train_time:120707ms step_avg:58.45ms
step:2066/2330 train_time:120767ms step_avg:58.45ms
step:2067/2330 train_time:120824ms step_avg:58.45ms
step:2068/2330 train_time:120885ms step_avg:58.45ms
step:2069/2330 train_time:120943ms step_avg:58.45ms
step:2070/2330 train_time:121003ms step_avg:58.46ms
step:2071/2330 train_time:121060ms step_avg:58.46ms
step:2072/2330 train_time:121122ms step_avg:58.46ms
step:2073/2330 train_time:121180ms step_avg:58.46ms
step:2074/2330 train_time:121241ms step_avg:58.46ms
step:2075/2330 train_time:121298ms step_avg:58.46ms
step:2076/2330 train_time:121360ms step_avg:58.46ms
step:2077/2330 train_time:121417ms step_avg:58.46ms
step:2078/2330 train_time:121479ms step_avg:58.46ms
step:2079/2330 train_time:121536ms step_avg:58.46ms
step:2080/2330 train_time:121598ms step_avg:58.46ms
step:2081/2330 train_time:121654ms step_avg:58.46ms
step:2082/2330 train_time:121715ms step_avg:58.46ms
step:2083/2330 train_time:121772ms step_avg:58.46ms
step:2084/2330 train_time:121832ms step_avg:58.46ms
step:2085/2330 train_time:121890ms step_avg:58.46ms
step:2086/2330 train_time:121951ms step_avg:58.46ms
step:2087/2330 train_time:122009ms step_avg:58.46ms
step:2088/2330 train_time:122069ms step_avg:58.46ms
step:2089/2330 train_time:122127ms step_avg:58.46ms
step:2090/2330 train_time:122189ms step_avg:58.46ms
step:2091/2330 train_time:122247ms step_avg:58.46ms
step:2092/2330 train_time:122308ms step_avg:58.46ms
step:2093/2330 train_time:122366ms step_avg:58.46ms
step:2094/2330 train_time:122427ms step_avg:58.47ms
step:2095/2330 train_time:122486ms step_avg:58.47ms
step:2096/2330 train_time:122547ms step_avg:58.47ms
step:2097/2330 train_time:122605ms step_avg:58.47ms
step:2098/2330 train_time:122665ms step_avg:58.47ms
step:2099/2330 train_time:122722ms step_avg:58.47ms
step:2100/2330 train_time:122782ms step_avg:58.47ms
step:2101/2330 train_time:122840ms step_avg:58.47ms
step:2102/2330 train_time:122901ms step_avg:58.47ms
step:2103/2330 train_time:122958ms step_avg:58.47ms
step:2104/2330 train_time:123018ms step_avg:58.47ms
step:2105/2330 train_time:123075ms step_avg:58.47ms
step:2106/2330 train_time:123137ms step_avg:58.47ms
step:2107/2330 train_time:123194ms step_avg:58.47ms
step:2108/2330 train_time:123255ms step_avg:58.47ms
step:2109/2330 train_time:123313ms step_avg:58.47ms
step:2110/2330 train_time:123374ms step_avg:58.47ms
step:2111/2330 train_time:123432ms step_avg:58.47ms
step:2112/2330 train_time:123493ms step_avg:58.47ms
step:2113/2330 train_time:123552ms step_avg:58.47ms
step:2114/2330 train_time:123612ms step_avg:58.47ms
step:2115/2330 train_time:123669ms step_avg:58.47ms
step:2116/2330 train_time:123730ms step_avg:58.47ms
step:2117/2330 train_time:123788ms step_avg:58.47ms
step:2118/2330 train_time:123848ms step_avg:58.47ms
step:2119/2330 train_time:123906ms step_avg:58.47ms
step:2120/2330 train_time:123966ms step_avg:58.47ms
step:2121/2330 train_time:124024ms step_avg:58.47ms
step:2122/2330 train_time:124085ms step_avg:58.48ms
step:2123/2330 train_time:124143ms step_avg:58.48ms
step:2124/2330 train_time:124203ms step_avg:58.48ms
step:2125/2330 train_time:124260ms step_avg:58.48ms
step:2126/2330 train_time:124320ms step_avg:58.48ms
step:2127/2330 train_time:124377ms step_avg:58.48ms
step:2128/2330 train_time:124440ms step_avg:58.48ms
step:2129/2330 train_time:124497ms step_avg:58.48ms
step:2130/2330 train_time:124558ms step_avg:58.48ms
step:2131/2330 train_time:124615ms step_avg:58.48ms
step:2132/2330 train_time:124677ms step_avg:58.48ms
step:2133/2330 train_time:124734ms step_avg:58.48ms
step:2134/2330 train_time:124797ms step_avg:58.48ms
step:2135/2330 train_time:124854ms step_avg:58.48ms
step:2136/2330 train_time:124915ms step_avg:58.48ms
step:2137/2330 train_time:124972ms step_avg:58.48ms
step:2138/2330 train_time:125033ms step_avg:58.48ms
step:2139/2330 train_time:125090ms step_avg:58.48ms
step:2140/2330 train_time:125151ms step_avg:58.48ms
step:2141/2330 train_time:125208ms step_avg:58.48ms
step:2142/2330 train_time:125268ms step_avg:58.48ms
step:2143/2330 train_time:125326ms step_avg:58.48ms
step:2144/2330 train_time:125387ms step_avg:58.48ms
step:2145/2330 train_time:125446ms step_avg:58.48ms
step:2146/2330 train_time:125506ms step_avg:58.48ms
step:2147/2330 train_time:125564ms step_avg:58.48ms
step:2148/2330 train_time:125623ms step_avg:58.48ms
step:2149/2330 train_time:125682ms step_avg:58.48ms
step:2150/2330 train_time:125742ms step_avg:58.48ms
step:2151/2330 train_time:125799ms step_avg:58.48ms
step:2152/2330 train_time:125859ms step_avg:58.48ms
step:2153/2330 train_time:125916ms step_avg:58.48ms
step:2154/2330 train_time:125978ms step_avg:58.49ms
step:2155/2330 train_time:126034ms step_avg:58.48ms
step:2156/2330 train_time:126098ms step_avg:58.49ms
step:2157/2330 train_time:126154ms step_avg:58.49ms
step:2158/2330 train_time:126216ms step_avg:58.49ms
step:2159/2330 train_time:126273ms step_avg:58.49ms
step:2160/2330 train_time:126334ms step_avg:58.49ms
step:2161/2330 train_time:126392ms step_avg:58.49ms
step:2162/2330 train_time:126453ms step_avg:58.49ms
step:2163/2330 train_time:126510ms step_avg:58.49ms
step:2164/2330 train_time:126571ms step_avg:58.49ms
step:2165/2330 train_time:126629ms step_avg:58.49ms
step:2166/2330 train_time:126690ms step_avg:58.49ms
step:2167/2330 train_time:126749ms step_avg:58.49ms
step:2168/2330 train_time:126810ms step_avg:58.49ms
step:2169/2330 train_time:126868ms step_avg:58.49ms
step:2170/2330 train_time:126929ms step_avg:58.49ms
step:2171/2330 train_time:126987ms step_avg:58.49ms
step:2172/2330 train_time:127048ms step_avg:58.49ms
step:2173/2330 train_time:127106ms step_avg:58.49ms
step:2174/2330 train_time:127166ms step_avg:58.49ms
step:2175/2330 train_time:127224ms step_avg:58.49ms
step:2176/2330 train_time:127285ms step_avg:58.49ms
step:2177/2330 train_time:127342ms step_avg:58.49ms
step:2178/2330 train_time:127402ms step_avg:58.49ms
step:2179/2330 train_time:127459ms step_avg:58.49ms
step:2180/2330 train_time:127521ms step_avg:58.50ms
step:2181/2330 train_time:127578ms step_avg:58.50ms
step:2182/2330 train_time:127640ms step_avg:58.50ms
step:2183/2330 train_time:127698ms step_avg:58.50ms
step:2184/2330 train_time:127759ms step_avg:58.50ms
step:2185/2330 train_time:127817ms step_avg:58.50ms
step:2186/2330 train_time:127879ms step_avg:58.50ms
step:2187/2330 train_time:127936ms step_avg:58.50ms
step:2188/2330 train_time:127997ms step_avg:58.50ms
step:2189/2330 train_time:128055ms step_avg:58.50ms
step:2190/2330 train_time:128115ms step_avg:58.50ms
step:2191/2330 train_time:128172ms step_avg:58.50ms
step:2192/2330 train_time:128233ms step_avg:58.50ms
step:2193/2330 train_time:128291ms step_avg:58.50ms
step:2194/2330 train_time:128352ms step_avg:58.50ms
step:2195/2330 train_time:128410ms step_avg:58.50ms
step:2196/2330 train_time:128470ms step_avg:58.50ms
step:2197/2330 train_time:128528ms step_avg:58.50ms
step:2198/2330 train_time:128588ms step_avg:58.50ms
step:2199/2330 train_time:128647ms step_avg:58.50ms
step:2200/2330 train_time:128707ms step_avg:58.50ms
step:2201/2330 train_time:128765ms step_avg:58.50ms
step:2202/2330 train_time:128826ms step_avg:58.50ms
step:2203/2330 train_time:128884ms step_avg:58.50ms
step:2204/2330 train_time:128944ms step_avg:58.50ms
step:2205/2330 train_time:129001ms step_avg:58.50ms
step:2206/2330 train_time:129062ms step_avg:58.50ms
step:2207/2330 train_time:129118ms step_avg:58.50ms
step:2208/2330 train_time:129181ms step_avg:58.51ms
step:2209/2330 train_time:129238ms step_avg:58.51ms
step:2210/2330 train_time:129299ms step_avg:58.51ms
step:2211/2330 train_time:129356ms step_avg:58.51ms
step:2212/2330 train_time:129418ms step_avg:58.51ms
step:2213/2330 train_time:129475ms step_avg:58.51ms
step:2214/2330 train_time:129538ms step_avg:58.51ms
step:2215/2330 train_time:129595ms step_avg:58.51ms
step:2216/2330 train_time:129656ms step_avg:58.51ms
step:2217/2330 train_time:129713ms step_avg:58.51ms
step:2218/2330 train_time:129774ms step_avg:58.51ms
step:2219/2330 train_time:129831ms step_avg:58.51ms
step:2220/2330 train_time:129892ms step_avg:58.51ms
step:2221/2330 train_time:129949ms step_avg:58.51ms
step:2222/2330 train_time:130010ms step_avg:58.51ms
step:2223/2330 train_time:130068ms step_avg:58.51ms
step:2224/2330 train_time:130129ms step_avg:58.51ms
step:2225/2330 train_time:130186ms step_avg:58.51ms
step:2226/2330 train_time:130247ms step_avg:58.51ms
step:2227/2330 train_time:130305ms step_avg:58.51ms
step:2228/2330 train_time:130365ms step_avg:58.51ms
step:2229/2330 train_time:130423ms step_avg:58.51ms
step:2230/2330 train_time:130484ms step_avg:58.51ms
step:2231/2330 train_time:130541ms step_avg:58.51ms
step:2232/2330 train_time:130603ms step_avg:58.51ms
step:2233/2330 train_time:130660ms step_avg:58.51ms
step:2234/2330 train_time:130720ms step_avg:58.51ms
step:2235/2330 train_time:130777ms step_avg:58.51ms
step:2236/2330 train_time:130838ms step_avg:58.51ms
step:2237/2330 train_time:130895ms step_avg:58.51ms
step:2238/2330 train_time:130958ms step_avg:58.52ms
step:2239/2330 train_time:131015ms step_avg:58.51ms
step:2240/2330 train_time:131076ms step_avg:58.52ms
step:2241/2330 train_time:131133ms step_avg:58.52ms
step:2242/2330 train_time:131194ms step_avg:58.52ms
step:2243/2330 train_time:131252ms step_avg:58.52ms
step:2244/2330 train_time:131312ms step_avg:58.52ms
step:2245/2330 train_time:131369ms step_avg:58.52ms
step:2246/2330 train_time:131431ms step_avg:58.52ms
step:2247/2330 train_time:131489ms step_avg:58.52ms
step:2248/2330 train_time:131550ms step_avg:58.52ms
step:2249/2330 train_time:131608ms step_avg:58.52ms
step:2250/2330 train_time:131669ms step_avg:58.52ms
step:2250/2330 val_loss:3.7099 train_time:131752ms step_avg:58.56ms
step:2251/2330 train_time:131769ms step_avg:58.54ms
step:2252/2330 train_time:131791ms step_avg:58.52ms
step:2253/2330 train_time:131854ms step_avg:58.52ms
step:2254/2330 train_time:131918ms step_avg:58.53ms
step:2255/2330 train_time:131976ms step_avg:58.53ms
step:2256/2330 train_time:132036ms step_avg:58.53ms
step:2257/2330 train_time:132093ms step_avg:58.53ms
step:2258/2330 train_time:132155ms step_avg:58.53ms
step:2259/2330 train_time:132212ms step_avg:58.53ms
step:2260/2330 train_time:132272ms step_avg:58.53ms
step:2261/2330 train_time:132329ms step_avg:58.53ms
step:2262/2330 train_time:132389ms step_avg:58.53ms
step:2263/2330 train_time:132445ms step_avg:58.53ms
step:2264/2330 train_time:132506ms step_avg:58.53ms
step:2265/2330 train_time:132562ms step_avg:58.53ms
step:2266/2330 train_time:132622ms step_avg:58.53ms
step:2267/2330 train_time:132678ms step_avg:58.53ms
step:2268/2330 train_time:132740ms step_avg:58.53ms
step:2269/2330 train_time:132799ms step_avg:58.53ms
step:2270/2330 train_time:132863ms step_avg:58.53ms
step:2271/2330 train_time:132921ms step_avg:58.53ms
step:2272/2330 train_time:132984ms step_avg:58.53ms
step:2273/2330 train_time:133041ms step_avg:58.53ms
step:2274/2330 train_time:133105ms step_avg:58.53ms
step:2275/2330 train_time:133161ms step_avg:58.53ms
step:2276/2330 train_time:133222ms step_avg:58.53ms
step:2277/2330 train_time:133278ms step_avg:58.53ms
step:2278/2330 train_time:133339ms step_avg:58.53ms
step:2279/2330 train_time:133396ms step_avg:58.53ms
step:2280/2330 train_time:133457ms step_avg:58.53ms
step:2281/2330 train_time:133513ms step_avg:58.53ms
step:2282/2330 train_time:133573ms step_avg:58.53ms
step:2283/2330 train_time:133630ms step_avg:58.53ms
step:2284/2330 train_time:133691ms step_avg:58.53ms
step:2285/2330 train_time:133749ms step_avg:58.53ms
step:2286/2330 train_time:133811ms step_avg:58.53ms
step:2287/2330 train_time:133870ms step_avg:58.54ms
step:2288/2330 train_time:133932ms step_avg:58.54ms
step:2289/2330 train_time:133990ms step_avg:58.54ms
step:2290/2330 train_time:134052ms step_avg:58.54ms
step:2291/2330 train_time:134110ms step_avg:58.54ms
step:2292/2330 train_time:134170ms step_avg:58.54ms
step:2293/2330 train_time:134227ms step_avg:58.54ms
step:2294/2330 train_time:134288ms step_avg:58.54ms
step:2295/2330 train_time:134346ms step_avg:58.54ms
step:2296/2330 train_time:134407ms step_avg:58.54ms
step:2297/2330 train_time:134463ms step_avg:58.54ms
step:2298/2330 train_time:134523ms step_avg:58.54ms
step:2299/2330 train_time:134580ms step_avg:58.54ms
step:2300/2330 train_time:134640ms step_avg:58.54ms
step:2301/2330 train_time:134697ms step_avg:58.54ms
step:2302/2330 train_time:134760ms step_avg:58.54ms
step:2303/2330 train_time:134817ms step_avg:58.54ms
step:2304/2330 train_time:134880ms step_avg:58.54ms
step:2305/2330 train_time:134937ms step_avg:58.54ms
step:2306/2330 train_time:134999ms step_avg:58.54ms
step:2307/2330 train_time:135056ms step_avg:58.54ms
step:2308/2330 train_time:135119ms step_avg:58.54ms
step:2309/2330 train_time:135177ms step_avg:58.54ms
step:2310/2330 train_time:135238ms step_avg:58.54ms
step:2311/2330 train_time:135295ms step_avg:58.54ms
step:2312/2330 train_time:135356ms step_avg:58.54ms
step:2313/2330 train_time:135413ms step_avg:58.54ms
step:2314/2330 train_time:135474ms step_avg:58.55ms
step:2315/2330 train_time:135531ms step_avg:58.54ms
step:2316/2330 train_time:135592ms step_avg:58.55ms
step:2317/2330 train_time:135649ms step_avg:58.55ms
step:2318/2330 train_time:135710ms step_avg:58.55ms
step:2319/2330 train_time:135767ms step_avg:58.55ms
step:2320/2330 train_time:135828ms step_avg:58.55ms
step:2321/2330 train_time:135886ms step_avg:58.55ms
step:2322/2330 train_time:135948ms step_avg:58.55ms
step:2323/2330 train_time:136006ms step_avg:58.55ms
step:2324/2330 train_time:136068ms step_avg:58.55ms
step:2325/2330 train_time:136125ms step_avg:58.55ms
step:2326/2330 train_time:136186ms step_avg:58.55ms
step:2327/2330 train_time:136243ms step_avg:58.55ms
step:2328/2330 train_time:136304ms step_avg:58.55ms
step:2329/2330 train_time:136361ms step_avg:58.55ms
step:2330/2330 train_time:136423ms step_avg:58.55ms
step:2330/2330 val_loss:3.6948 train_time:136504ms step_avg:58.59ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
