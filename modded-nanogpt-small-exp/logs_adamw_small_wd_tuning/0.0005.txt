import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:13:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:80ms step_avg:80.00ms
step:2/2330 train_time:187ms step_avg:93.33ms
step:3/2330 train_time:206ms step_avg:68.65ms
step:4/2330 train_time:226ms step_avg:56.41ms
step:5/2330 train_time:278ms step_avg:55.59ms
step:6/2330 train_time:336ms step_avg:55.98ms
step:7/2330 train_time:391ms step_avg:55.89ms
step:8/2330 train_time:450ms step_avg:56.19ms
step:9/2330 train_time:505ms step_avg:56.16ms
step:10/2330 train_time:563ms step_avg:56.28ms
step:11/2330 train_time:618ms step_avg:56.23ms
step:12/2330 train_time:676ms step_avg:56.36ms
step:13/2330 train_time:731ms step_avg:56.26ms
step:14/2330 train_time:790ms step_avg:56.42ms
step:15/2330 train_time:846ms step_avg:56.37ms
step:16/2330 train_time:904ms step_avg:56.49ms
step:17/2330 train_time:959ms step_avg:56.44ms
step:18/2330 train_time:1018ms step_avg:56.54ms
step:19/2330 train_time:1076ms step_avg:56.64ms
step:20/2330 train_time:1138ms step_avg:56.88ms
step:21/2330 train_time:1195ms step_avg:56.92ms
step:22/2330 train_time:1255ms step_avg:57.03ms
step:23/2330 train_time:1312ms step_avg:57.03ms
step:24/2330 train_time:1370ms step_avg:57.08ms
step:25/2330 train_time:1426ms step_avg:57.02ms
step:26/2330 train_time:1484ms step_avg:57.09ms
step:27/2330 train_time:1540ms step_avg:57.03ms
step:28/2330 train_time:1598ms step_avg:57.06ms
step:29/2330 train_time:1653ms step_avg:57.00ms
step:30/2330 train_time:1711ms step_avg:57.04ms
step:31/2330 train_time:1767ms step_avg:57.00ms
step:32/2330 train_time:1825ms step_avg:57.03ms
step:33/2330 train_time:1880ms step_avg:56.98ms
step:34/2330 train_time:1938ms step_avg:57.01ms
step:35/2330 train_time:1994ms step_avg:56.98ms
step:36/2330 train_time:2054ms step_avg:57.05ms
step:37/2330 train_time:2112ms step_avg:57.08ms
step:38/2330 train_time:2170ms step_avg:57.12ms
step:39/2330 train_time:2227ms step_avg:57.09ms
step:40/2330 train_time:2287ms step_avg:57.17ms
step:41/2330 train_time:2344ms step_avg:57.17ms
step:42/2330 train_time:2403ms step_avg:57.21ms
step:43/2330 train_time:2459ms step_avg:57.18ms
step:44/2330 train_time:2517ms step_avg:57.21ms
step:45/2330 train_time:2573ms step_avg:57.19ms
step:46/2330 train_time:2631ms step_avg:57.21ms
step:47/2330 train_time:2688ms step_avg:57.19ms
step:48/2330 train_time:2746ms step_avg:57.20ms
step:49/2330 train_time:2801ms step_avg:57.17ms
step:50/2330 train_time:2859ms step_avg:57.18ms
step:51/2330 train_time:2915ms step_avg:57.16ms
step:52/2330 train_time:2973ms step_avg:57.18ms
step:53/2330 train_time:3029ms step_avg:57.16ms
step:54/2330 train_time:3088ms step_avg:57.19ms
step:55/2330 train_time:3145ms step_avg:57.18ms
step:56/2330 train_time:3205ms step_avg:57.23ms
step:57/2330 train_time:3261ms step_avg:57.21ms
step:58/2330 train_time:3320ms step_avg:57.25ms
step:59/2330 train_time:3376ms step_avg:57.22ms
step:60/2330 train_time:3435ms step_avg:57.26ms
step:61/2330 train_time:3491ms step_avg:57.23ms
step:62/2330 train_time:3551ms step_avg:57.28ms
step:63/2330 train_time:3608ms step_avg:57.26ms
step:64/2330 train_time:3665ms step_avg:57.27ms
step:65/2330 train_time:3721ms step_avg:57.24ms
step:66/2330 train_time:3778ms step_avg:57.25ms
step:67/2330 train_time:3834ms step_avg:57.22ms
step:68/2330 train_time:3893ms step_avg:57.25ms
step:69/2330 train_time:3948ms step_avg:57.22ms
step:70/2330 train_time:4008ms step_avg:57.25ms
step:71/2330 train_time:4064ms step_avg:57.24ms
step:72/2330 train_time:4123ms step_avg:57.26ms
step:73/2330 train_time:4179ms step_avg:57.25ms
step:74/2330 train_time:4238ms step_avg:57.27ms
step:75/2330 train_time:4296ms step_avg:57.27ms
step:76/2330 train_time:4354ms step_avg:57.29ms
step:77/2330 train_time:4411ms step_avg:57.28ms
step:78/2330 train_time:4469ms step_avg:57.30ms
step:79/2330 train_time:4525ms step_avg:57.28ms
step:80/2330 train_time:4584ms step_avg:57.30ms
step:81/2330 train_time:4639ms step_avg:57.28ms
step:82/2330 train_time:4699ms step_avg:57.30ms
step:83/2330 train_time:4755ms step_avg:57.29ms
step:84/2330 train_time:4813ms step_avg:57.30ms
step:85/2330 train_time:4869ms step_avg:57.28ms
step:86/2330 train_time:4928ms step_avg:57.30ms
step:87/2330 train_time:4984ms step_avg:57.29ms
step:88/2330 train_time:5042ms step_avg:57.30ms
step:89/2330 train_time:5098ms step_avg:57.28ms
step:90/2330 train_time:5157ms step_avg:57.30ms
step:91/2330 train_time:5213ms step_avg:57.29ms
step:92/2330 train_time:5272ms step_avg:57.30ms
step:93/2330 train_time:5327ms step_avg:57.28ms
step:94/2330 train_time:5388ms step_avg:57.31ms
step:95/2330 train_time:5444ms step_avg:57.31ms
step:96/2330 train_time:5503ms step_avg:57.32ms
step:97/2330 train_time:5559ms step_avg:57.31ms
step:98/2330 train_time:5617ms step_avg:57.32ms
step:99/2330 train_time:5673ms step_avg:57.31ms
step:100/2330 train_time:5732ms step_avg:57.32ms
step:101/2330 train_time:5788ms step_avg:57.31ms
step:102/2330 train_time:5846ms step_avg:57.32ms
step:103/2330 train_time:5903ms step_avg:57.31ms
step:104/2330 train_time:5961ms step_avg:57.32ms
step:105/2330 train_time:6017ms step_avg:57.31ms
step:106/2330 train_time:6076ms step_avg:57.32ms
step:107/2330 train_time:6132ms step_avg:57.31ms
step:108/2330 train_time:6191ms step_avg:57.33ms
step:109/2330 train_time:6247ms step_avg:57.32ms
step:110/2330 train_time:6306ms step_avg:57.33ms
step:111/2330 train_time:6362ms step_avg:57.32ms
step:112/2330 train_time:6421ms step_avg:57.33ms
step:113/2330 train_time:6477ms step_avg:57.32ms
step:114/2330 train_time:6536ms step_avg:57.33ms
step:115/2330 train_time:6592ms step_avg:57.32ms
step:116/2330 train_time:6650ms step_avg:57.33ms
step:117/2330 train_time:6706ms step_avg:57.32ms
step:118/2330 train_time:6765ms step_avg:57.33ms
step:119/2330 train_time:6821ms step_avg:57.32ms
step:120/2330 train_time:6879ms step_avg:57.33ms
step:121/2330 train_time:6936ms step_avg:57.32ms
step:122/2330 train_time:6994ms step_avg:57.33ms
step:123/2330 train_time:7050ms step_avg:57.32ms
step:124/2330 train_time:7109ms step_avg:57.33ms
step:125/2330 train_time:7164ms step_avg:57.31ms
step:126/2330 train_time:7224ms step_avg:57.33ms
step:127/2330 train_time:7280ms step_avg:57.32ms
step:128/2330 train_time:7339ms step_avg:57.33ms
step:129/2330 train_time:7396ms step_avg:57.33ms
step:130/2330 train_time:7454ms step_avg:57.34ms
step:131/2330 train_time:7511ms step_avg:57.33ms
step:132/2330 train_time:7569ms step_avg:57.34ms
step:133/2330 train_time:7625ms step_avg:57.33ms
step:134/2330 train_time:7683ms step_avg:57.34ms
step:135/2330 train_time:7739ms step_avg:57.33ms
step:136/2330 train_time:7797ms step_avg:57.33ms
step:137/2330 train_time:7854ms step_avg:57.33ms
step:138/2330 train_time:7913ms step_avg:57.34ms
step:139/2330 train_time:7969ms step_avg:57.33ms
step:140/2330 train_time:8027ms step_avg:57.34ms
step:141/2330 train_time:8084ms step_avg:57.33ms
step:142/2330 train_time:8142ms step_avg:57.34ms
step:143/2330 train_time:8198ms step_avg:57.33ms
step:144/2330 train_time:8257ms step_avg:57.34ms
step:145/2330 train_time:8313ms step_avg:57.33ms
step:146/2330 train_time:8371ms step_avg:57.34ms
step:147/2330 train_time:8427ms step_avg:57.33ms
step:148/2330 train_time:8487ms step_avg:57.34ms
step:149/2330 train_time:8542ms step_avg:57.33ms
step:150/2330 train_time:8601ms step_avg:57.34ms
step:151/2330 train_time:8658ms step_avg:57.33ms
step:152/2330 train_time:8715ms step_avg:57.34ms
step:153/2330 train_time:8771ms step_avg:57.33ms
step:154/2330 train_time:8830ms step_avg:57.34ms
step:155/2330 train_time:8886ms step_avg:57.33ms
step:156/2330 train_time:8944ms step_avg:57.34ms
step:157/2330 train_time:9001ms step_avg:57.33ms
step:158/2330 train_time:9059ms step_avg:57.33ms
step:159/2330 train_time:9116ms step_avg:57.33ms
step:160/2330 train_time:9173ms step_avg:57.33ms
step:161/2330 train_time:9229ms step_avg:57.32ms
step:162/2330 train_time:9288ms step_avg:57.33ms
step:163/2330 train_time:9344ms step_avg:57.33ms
step:164/2330 train_time:9403ms step_avg:57.34ms
step:165/2330 train_time:9459ms step_avg:57.33ms
step:166/2330 train_time:9517ms step_avg:57.33ms
step:167/2330 train_time:9574ms step_avg:57.33ms
step:168/2330 train_time:9633ms step_avg:57.34ms
step:169/2330 train_time:9689ms step_avg:57.33ms
step:170/2330 train_time:9750ms step_avg:57.35ms
step:171/2330 train_time:9805ms step_avg:57.34ms
step:172/2330 train_time:9864ms step_avg:57.35ms
step:173/2330 train_time:9919ms step_avg:57.34ms
step:174/2330 train_time:9979ms step_avg:57.35ms
step:175/2330 train_time:10035ms step_avg:57.34ms
step:176/2330 train_time:10093ms step_avg:57.35ms
step:177/2330 train_time:10149ms step_avg:57.34ms
step:178/2330 train_time:10208ms step_avg:57.35ms
step:179/2330 train_time:10264ms step_avg:57.34ms
step:180/2330 train_time:10323ms step_avg:57.35ms
step:181/2330 train_time:10379ms step_avg:57.34ms
step:182/2330 train_time:10438ms step_avg:57.35ms
step:183/2330 train_time:10494ms step_avg:57.35ms
step:184/2330 train_time:10553ms step_avg:57.35ms
step:185/2330 train_time:10609ms step_avg:57.35ms
step:186/2330 train_time:10667ms step_avg:57.35ms
step:187/2330 train_time:10723ms step_avg:57.34ms
step:188/2330 train_time:10782ms step_avg:57.35ms
step:189/2330 train_time:10837ms step_avg:57.34ms
step:190/2330 train_time:10898ms step_avg:57.36ms
step:191/2330 train_time:10954ms step_avg:57.35ms
step:192/2330 train_time:11012ms step_avg:57.36ms
step:193/2330 train_time:11068ms step_avg:57.35ms
step:194/2330 train_time:11127ms step_avg:57.36ms
step:195/2330 train_time:11183ms step_avg:57.35ms
step:196/2330 train_time:11242ms step_avg:57.36ms
step:197/2330 train_time:11297ms step_avg:57.35ms
step:198/2330 train_time:11356ms step_avg:57.36ms
step:199/2330 train_time:11412ms step_avg:57.35ms
step:200/2330 train_time:11471ms step_avg:57.35ms
step:201/2330 train_time:11526ms step_avg:57.35ms
step:202/2330 train_time:11585ms step_avg:57.35ms
step:203/2330 train_time:11641ms step_avg:57.35ms
step:204/2330 train_time:11700ms step_avg:57.35ms
step:205/2330 train_time:11757ms step_avg:57.35ms
step:206/2330 train_time:11814ms step_avg:57.35ms
step:207/2330 train_time:11870ms step_avg:57.35ms
step:208/2330 train_time:11930ms step_avg:57.36ms
step:209/2330 train_time:11986ms step_avg:57.35ms
step:210/2330 train_time:12045ms step_avg:57.36ms
step:211/2330 train_time:12101ms step_avg:57.35ms
step:212/2330 train_time:12160ms step_avg:57.36ms
step:213/2330 train_time:12216ms step_avg:57.35ms
step:214/2330 train_time:12275ms step_avg:57.36ms
step:215/2330 train_time:12331ms step_avg:57.35ms
step:216/2330 train_time:12389ms step_avg:57.36ms
step:217/2330 train_time:12445ms step_avg:57.35ms
step:218/2330 train_time:12504ms step_avg:57.36ms
step:219/2330 train_time:12560ms step_avg:57.35ms
step:220/2330 train_time:12619ms step_avg:57.36ms
step:221/2330 train_time:12675ms step_avg:57.35ms
step:222/2330 train_time:12734ms step_avg:57.36ms
step:223/2330 train_time:12790ms step_avg:57.35ms
step:224/2330 train_time:12849ms step_avg:57.36ms
step:225/2330 train_time:12905ms step_avg:57.36ms
step:226/2330 train_time:12966ms step_avg:57.37ms
step:227/2330 train_time:13022ms step_avg:57.36ms
step:228/2330 train_time:13080ms step_avg:57.37ms
step:229/2330 train_time:13137ms step_avg:57.37ms
step:230/2330 train_time:13195ms step_avg:57.37ms
step:231/2330 train_time:13251ms step_avg:57.36ms
step:232/2330 train_time:13310ms step_avg:57.37ms
step:233/2330 train_time:13366ms step_avg:57.36ms
step:234/2330 train_time:13424ms step_avg:57.37ms
step:235/2330 train_time:13480ms step_avg:57.36ms
step:236/2330 train_time:13539ms step_avg:57.37ms
step:237/2330 train_time:13596ms step_avg:57.37ms
step:238/2330 train_time:13655ms step_avg:57.37ms
step:239/2330 train_time:13711ms step_avg:57.37ms
step:240/2330 train_time:13770ms step_avg:57.37ms
step:241/2330 train_time:13827ms step_avg:57.37ms
step:242/2330 train_time:13885ms step_avg:57.38ms
step:243/2330 train_time:13941ms step_avg:57.37ms
step:244/2330 train_time:14000ms step_avg:57.38ms
step:245/2330 train_time:14056ms step_avg:57.37ms
step:246/2330 train_time:14114ms step_avg:57.38ms
step:247/2330 train_time:14170ms step_avg:57.37ms
step:248/2330 train_time:14230ms step_avg:57.38ms
step:249/2330 train_time:14286ms step_avg:57.37ms
step:250/2330 train_time:14344ms step_avg:57.38ms
step:250/2330 val_loss:4.8787 train_time:14423ms step_avg:57.69ms
step:251/2330 train_time:14443ms step_avg:57.54ms
step:252/2330 train_time:14463ms step_avg:57.39ms
step:253/2330 train_time:14520ms step_avg:57.39ms
step:254/2330 train_time:14585ms step_avg:57.42ms
step:255/2330 train_time:14643ms step_avg:57.42ms
step:256/2330 train_time:14704ms step_avg:57.44ms
step:257/2330 train_time:14760ms step_avg:57.43ms
step:258/2330 train_time:14819ms step_avg:57.44ms
step:259/2330 train_time:14874ms step_avg:57.43ms
step:260/2330 train_time:14932ms step_avg:57.43ms
step:261/2330 train_time:14988ms step_avg:57.43ms
step:262/2330 train_time:15046ms step_avg:57.43ms
step:263/2330 train_time:15101ms step_avg:57.42ms
step:264/2330 train_time:15159ms step_avg:57.42ms
step:265/2330 train_time:15215ms step_avg:57.41ms
step:266/2330 train_time:15272ms step_avg:57.42ms
step:267/2330 train_time:15329ms step_avg:57.41ms
step:268/2330 train_time:15387ms step_avg:57.42ms
step:269/2330 train_time:15443ms step_avg:57.41ms
step:270/2330 train_time:15503ms step_avg:57.42ms
step:271/2330 train_time:15560ms step_avg:57.42ms
step:272/2330 train_time:15622ms step_avg:57.43ms
step:273/2330 train_time:15678ms step_avg:57.43ms
step:274/2330 train_time:15738ms step_avg:57.44ms
step:275/2330 train_time:15794ms step_avg:57.43ms
step:276/2330 train_time:15853ms step_avg:57.44ms
step:277/2330 train_time:15908ms step_avg:57.43ms
step:278/2330 train_time:15967ms step_avg:57.44ms
step:279/2330 train_time:16022ms step_avg:57.43ms
step:280/2330 train_time:16081ms step_avg:57.43ms
step:281/2330 train_time:16137ms step_avg:57.43ms
step:282/2330 train_time:16195ms step_avg:57.43ms
step:283/2330 train_time:16251ms step_avg:57.42ms
step:284/2330 train_time:16309ms step_avg:57.43ms
step:285/2330 train_time:16365ms step_avg:57.42ms
step:286/2330 train_time:16423ms step_avg:57.42ms
step:287/2330 train_time:16480ms step_avg:57.42ms
step:288/2330 train_time:16540ms step_avg:57.43ms
step:289/2330 train_time:16596ms step_avg:57.43ms
step:290/2330 train_time:16656ms step_avg:57.43ms
step:291/2330 train_time:16713ms step_avg:57.43ms
step:292/2330 train_time:16771ms step_avg:57.44ms
step:293/2330 train_time:16828ms step_avg:57.43ms
step:294/2330 train_time:16887ms step_avg:57.44ms
step:295/2330 train_time:16942ms step_avg:57.43ms
step:296/2330 train_time:17001ms step_avg:57.44ms
step:297/2330 train_time:17057ms step_avg:57.43ms
step:298/2330 train_time:17115ms step_avg:57.43ms
step:299/2330 train_time:17171ms step_avg:57.43ms
step:300/2330 train_time:17229ms step_avg:57.43ms
step:301/2330 train_time:17285ms step_avg:57.42ms
step:302/2330 train_time:17343ms step_avg:57.43ms
step:303/2330 train_time:17399ms step_avg:57.42ms
step:304/2330 train_time:17458ms step_avg:57.43ms
step:305/2330 train_time:17515ms step_avg:57.43ms
step:306/2330 train_time:17574ms step_avg:57.43ms
step:307/2330 train_time:17630ms step_avg:57.43ms
step:308/2330 train_time:17690ms step_avg:57.44ms
step:309/2330 train_time:17747ms step_avg:57.43ms
step:310/2330 train_time:17805ms step_avg:57.44ms
step:311/2330 train_time:17861ms step_avg:57.43ms
step:312/2330 train_time:17920ms step_avg:57.44ms
step:313/2330 train_time:17975ms step_avg:57.43ms
step:314/2330 train_time:18034ms step_avg:57.43ms
step:315/2330 train_time:18090ms step_avg:57.43ms
step:316/2330 train_time:18149ms step_avg:57.43ms
step:317/2330 train_time:18206ms step_avg:57.43ms
step:318/2330 train_time:18264ms step_avg:57.43ms
step:319/2330 train_time:18320ms step_avg:57.43ms
step:320/2330 train_time:18379ms step_avg:57.43ms
step:321/2330 train_time:18435ms step_avg:57.43ms
step:322/2330 train_time:18494ms step_avg:57.43ms
step:323/2330 train_time:18551ms step_avg:57.43ms
step:324/2330 train_time:18610ms step_avg:57.44ms
step:325/2330 train_time:18666ms step_avg:57.44ms
step:326/2330 train_time:18725ms step_avg:57.44ms
step:327/2330 train_time:18781ms step_avg:57.44ms
step:328/2330 train_time:18840ms step_avg:57.44ms
step:329/2330 train_time:18897ms step_avg:57.44ms
step:330/2330 train_time:18956ms step_avg:57.44ms
step:331/2330 train_time:19013ms step_avg:57.44ms
step:332/2330 train_time:19071ms step_avg:57.44ms
step:333/2330 train_time:19126ms step_avg:57.44ms
step:334/2330 train_time:19187ms step_avg:57.45ms
step:335/2330 train_time:19243ms step_avg:57.44ms
step:336/2330 train_time:19301ms step_avg:57.44ms
step:337/2330 train_time:19358ms step_avg:57.44ms
step:338/2330 train_time:19416ms step_avg:57.44ms
step:339/2330 train_time:19473ms step_avg:57.44ms
step:340/2330 train_time:19531ms step_avg:57.45ms
step:341/2330 train_time:19588ms step_avg:57.44ms
step:342/2330 train_time:19647ms step_avg:57.45ms
step:343/2330 train_time:19704ms step_avg:57.45ms
step:344/2330 train_time:19763ms step_avg:57.45ms
step:345/2330 train_time:19819ms step_avg:57.45ms
step:346/2330 train_time:19879ms step_avg:57.46ms
step:347/2330 train_time:19935ms step_avg:57.45ms
step:348/2330 train_time:19994ms step_avg:57.45ms
step:349/2330 train_time:20050ms step_avg:57.45ms
step:350/2330 train_time:20108ms step_avg:57.45ms
step:351/2330 train_time:20164ms step_avg:57.45ms
step:352/2330 train_time:20223ms step_avg:57.45ms
step:353/2330 train_time:20279ms step_avg:57.45ms
step:354/2330 train_time:20337ms step_avg:57.45ms
step:355/2330 train_time:20393ms step_avg:57.45ms
step:356/2330 train_time:20452ms step_avg:57.45ms
step:357/2330 train_time:20509ms step_avg:57.45ms
step:358/2330 train_time:20568ms step_avg:57.45ms
step:359/2330 train_time:20625ms step_avg:57.45ms
step:360/2330 train_time:20683ms step_avg:57.45ms
step:361/2330 train_time:20739ms step_avg:57.45ms
step:362/2330 train_time:20798ms step_avg:57.45ms
step:363/2330 train_time:20854ms step_avg:57.45ms
step:364/2330 train_time:20913ms step_avg:57.45ms
step:365/2330 train_time:20969ms step_avg:57.45ms
step:366/2330 train_time:21028ms step_avg:57.45ms
step:367/2330 train_time:21084ms step_avg:57.45ms
step:368/2330 train_time:21143ms step_avg:57.45ms
step:369/2330 train_time:21199ms step_avg:57.45ms
step:370/2330 train_time:21257ms step_avg:57.45ms
step:371/2330 train_time:21313ms step_avg:57.45ms
step:372/2330 train_time:21372ms step_avg:57.45ms
step:373/2330 train_time:21429ms step_avg:57.45ms
step:374/2330 train_time:21488ms step_avg:57.45ms
step:375/2330 train_time:21544ms step_avg:57.45ms
step:376/2330 train_time:21603ms step_avg:57.46ms
step:377/2330 train_time:21660ms step_avg:57.45ms
step:378/2330 train_time:21718ms step_avg:57.46ms
step:379/2330 train_time:21774ms step_avg:57.45ms
step:380/2330 train_time:21833ms step_avg:57.45ms
step:381/2330 train_time:21889ms step_avg:57.45ms
step:382/2330 train_time:21948ms step_avg:57.46ms
step:383/2330 train_time:22005ms step_avg:57.45ms
step:384/2330 train_time:22063ms step_avg:57.46ms
step:385/2330 train_time:22120ms step_avg:57.45ms
step:386/2330 train_time:22177ms step_avg:57.45ms
step:387/2330 train_time:22234ms step_avg:57.45ms
step:388/2330 train_time:22292ms step_avg:57.45ms
step:389/2330 train_time:22348ms step_avg:57.45ms
step:390/2330 train_time:22406ms step_avg:57.45ms
step:391/2330 train_time:22463ms step_avg:57.45ms
step:392/2330 train_time:22522ms step_avg:57.45ms
step:393/2330 train_time:22578ms step_avg:57.45ms
step:394/2330 train_time:22638ms step_avg:57.46ms
step:395/2330 train_time:22694ms step_avg:57.45ms
step:396/2330 train_time:22753ms step_avg:57.46ms
step:397/2330 train_time:22809ms step_avg:57.45ms
step:398/2330 train_time:22868ms step_avg:57.46ms
step:399/2330 train_time:22925ms step_avg:57.46ms
step:400/2330 train_time:22983ms step_avg:57.46ms
step:401/2330 train_time:23040ms step_avg:57.46ms
step:402/2330 train_time:23098ms step_avg:57.46ms
step:403/2330 train_time:23154ms step_avg:57.45ms
step:404/2330 train_time:23212ms step_avg:57.46ms
step:405/2330 train_time:23269ms step_avg:57.45ms
step:406/2330 train_time:23328ms step_avg:57.46ms
step:407/2330 train_time:23384ms step_avg:57.45ms
step:408/2330 train_time:23442ms step_avg:57.46ms
step:409/2330 train_time:23498ms step_avg:57.45ms
step:410/2330 train_time:23558ms step_avg:57.46ms
step:411/2330 train_time:23614ms step_avg:57.45ms
step:412/2330 train_time:23672ms step_avg:57.46ms
step:413/2330 train_time:23728ms step_avg:57.45ms
step:414/2330 train_time:23787ms step_avg:57.46ms
step:415/2330 train_time:23843ms step_avg:57.45ms
step:416/2330 train_time:23903ms step_avg:57.46ms
step:417/2330 train_time:23959ms step_avg:57.46ms
step:418/2330 train_time:24018ms step_avg:57.46ms
step:419/2330 train_time:24074ms step_avg:57.46ms
step:420/2330 train_time:24132ms step_avg:57.46ms
step:421/2330 train_time:24188ms step_avg:57.45ms
step:422/2330 train_time:24246ms step_avg:57.46ms
step:423/2330 train_time:24303ms step_avg:57.45ms
step:424/2330 train_time:24361ms step_avg:57.46ms
step:425/2330 train_time:24417ms step_avg:57.45ms
step:426/2330 train_time:24477ms step_avg:57.46ms
step:427/2330 train_time:24533ms step_avg:57.45ms
step:428/2330 train_time:24592ms step_avg:57.46ms
step:429/2330 train_time:24649ms step_avg:57.46ms
step:430/2330 train_time:24708ms step_avg:57.46ms
step:431/2330 train_time:24764ms step_avg:57.46ms
step:432/2330 train_time:24823ms step_avg:57.46ms
step:433/2330 train_time:24879ms step_avg:57.46ms
step:434/2330 train_time:24938ms step_avg:57.46ms
step:435/2330 train_time:24993ms step_avg:57.46ms
step:436/2330 train_time:25053ms step_avg:57.46ms
step:437/2330 train_time:25108ms step_avg:57.46ms
step:438/2330 train_time:25167ms step_avg:57.46ms
step:439/2330 train_time:25224ms step_avg:57.46ms
step:440/2330 train_time:25282ms step_avg:57.46ms
step:441/2330 train_time:25337ms step_avg:57.45ms
step:442/2330 train_time:25397ms step_avg:57.46ms
step:443/2330 train_time:25453ms step_avg:57.46ms
step:444/2330 train_time:25512ms step_avg:57.46ms
step:445/2330 train_time:25569ms step_avg:57.46ms
step:446/2330 train_time:25627ms step_avg:57.46ms
step:447/2330 train_time:25683ms step_avg:57.46ms
step:448/2330 train_time:25742ms step_avg:57.46ms
step:449/2330 train_time:25799ms step_avg:57.46ms
step:450/2330 train_time:25858ms step_avg:57.46ms
step:451/2330 train_time:25914ms step_avg:57.46ms
step:452/2330 train_time:25973ms step_avg:57.46ms
step:453/2330 train_time:26030ms step_avg:57.46ms
step:454/2330 train_time:26088ms step_avg:57.46ms
step:455/2330 train_time:26144ms step_avg:57.46ms
step:456/2330 train_time:26203ms step_avg:57.46ms
step:457/2330 train_time:26259ms step_avg:57.46ms
step:458/2330 train_time:26317ms step_avg:57.46ms
step:459/2330 train_time:26373ms step_avg:57.46ms
step:460/2330 train_time:26432ms step_avg:57.46ms
step:461/2330 train_time:26488ms step_avg:57.46ms
step:462/2330 train_time:26547ms step_avg:57.46ms
step:463/2330 train_time:26603ms step_avg:57.46ms
step:464/2330 train_time:26662ms step_avg:57.46ms
step:465/2330 train_time:26719ms step_avg:57.46ms
step:466/2330 train_time:26777ms step_avg:57.46ms
step:467/2330 train_time:26833ms step_avg:57.46ms
step:468/2330 train_time:26893ms step_avg:57.46ms
step:469/2330 train_time:26949ms step_avg:57.46ms
step:470/2330 train_time:27008ms step_avg:57.46ms
step:471/2330 train_time:27064ms step_avg:57.46ms
step:472/2330 train_time:27123ms step_avg:57.46ms
step:473/2330 train_time:27179ms step_avg:57.46ms
step:474/2330 train_time:27237ms step_avg:57.46ms
step:475/2330 train_time:27293ms step_avg:57.46ms
step:476/2330 train_time:27352ms step_avg:57.46ms
step:477/2330 train_time:27408ms step_avg:57.46ms
step:478/2330 train_time:27468ms step_avg:57.46ms
step:479/2330 train_time:27524ms step_avg:57.46ms
step:480/2330 train_time:27583ms step_avg:57.46ms
step:481/2330 train_time:27640ms step_avg:57.46ms
step:482/2330 train_time:27699ms step_avg:57.47ms
step:483/2330 train_time:27754ms step_avg:57.46ms
step:484/2330 train_time:27815ms step_avg:57.47ms
step:485/2330 train_time:27871ms step_avg:57.47ms
step:486/2330 train_time:27929ms step_avg:57.47ms
step:487/2330 train_time:27985ms step_avg:57.46ms
step:488/2330 train_time:28044ms step_avg:57.47ms
step:489/2330 train_time:28100ms step_avg:57.47ms
step:490/2330 train_time:28160ms step_avg:57.47ms
step:491/2330 train_time:28216ms step_avg:57.47ms
step:492/2330 train_time:28275ms step_avg:57.47ms
step:493/2330 train_time:28331ms step_avg:57.47ms
step:494/2330 train_time:28390ms step_avg:57.47ms
step:495/2330 train_time:28447ms step_avg:57.47ms
step:496/2330 train_time:28506ms step_avg:57.47ms
step:497/2330 train_time:28562ms step_avg:57.47ms
step:498/2330 train_time:28621ms step_avg:57.47ms
step:499/2330 train_time:28677ms step_avg:57.47ms
step:500/2330 train_time:28736ms step_avg:57.47ms
step:500/2330 val_loss:4.3984 train_time:28816ms step_avg:57.63ms
step:501/2330 train_time:28835ms step_avg:57.56ms
step:502/2330 train_time:28857ms step_avg:57.48ms
step:503/2330 train_time:28914ms step_avg:57.48ms
step:504/2330 train_time:28977ms step_avg:57.49ms
step:505/2330 train_time:29035ms step_avg:57.50ms
step:506/2330 train_time:29094ms step_avg:57.50ms
step:507/2330 train_time:29151ms step_avg:57.50ms
step:508/2330 train_time:29210ms step_avg:57.50ms
step:509/2330 train_time:29266ms step_avg:57.50ms
step:510/2330 train_time:29324ms step_avg:57.50ms
step:511/2330 train_time:29380ms step_avg:57.50ms
step:512/2330 train_time:29438ms step_avg:57.50ms
step:513/2330 train_time:29494ms step_avg:57.49ms
step:514/2330 train_time:29552ms step_avg:57.49ms
step:515/2330 train_time:29608ms step_avg:57.49ms
step:516/2330 train_time:29666ms step_avg:57.49ms
step:517/2330 train_time:29722ms step_avg:57.49ms
step:518/2330 train_time:29780ms step_avg:57.49ms
step:519/2330 train_time:29837ms step_avg:57.49ms
step:520/2330 train_time:29897ms step_avg:57.49ms
step:521/2330 train_time:29955ms step_avg:57.50ms
step:522/2330 train_time:30015ms step_avg:57.50ms
step:523/2330 train_time:30073ms step_avg:57.50ms
step:524/2330 train_time:30132ms step_avg:57.50ms
step:525/2330 train_time:30189ms step_avg:57.50ms
step:526/2330 train_time:30248ms step_avg:57.51ms
step:527/2330 train_time:30304ms step_avg:57.50ms
step:528/2330 train_time:30363ms step_avg:57.51ms
step:529/2330 train_time:30418ms step_avg:57.50ms
step:530/2330 train_time:30477ms step_avg:57.50ms
step:531/2330 train_time:30533ms step_avg:57.50ms
step:532/2330 train_time:30591ms step_avg:57.50ms
step:533/2330 train_time:30647ms step_avg:57.50ms
step:534/2330 train_time:30706ms step_avg:57.50ms
step:535/2330 train_time:30762ms step_avg:57.50ms
step:536/2330 train_time:30820ms step_avg:57.50ms
step:537/2330 train_time:30877ms step_avg:57.50ms
step:538/2330 train_time:30936ms step_avg:57.50ms
step:539/2330 train_time:30994ms step_avg:57.50ms
step:540/2330 train_time:31053ms step_avg:57.51ms
step:541/2330 train_time:31111ms step_avg:57.51ms
step:542/2330 train_time:31170ms step_avg:57.51ms
step:543/2330 train_time:31227ms step_avg:57.51ms
step:544/2330 train_time:31286ms step_avg:57.51ms
step:545/2330 train_time:31342ms step_avg:57.51ms
step:546/2330 train_time:31402ms step_avg:57.51ms
step:547/2330 train_time:31458ms step_avg:57.51ms
step:548/2330 train_time:31516ms step_avg:57.51ms
step:549/2330 train_time:31571ms step_avg:57.51ms
step:550/2330 train_time:31631ms step_avg:57.51ms
step:551/2330 train_time:31686ms step_avg:57.51ms
step:552/2330 train_time:31745ms step_avg:57.51ms
step:553/2330 train_time:31801ms step_avg:57.51ms
step:554/2330 train_time:31861ms step_avg:57.51ms
step:555/2330 train_time:31916ms step_avg:57.51ms
step:556/2330 train_time:31977ms step_avg:57.51ms
step:557/2330 train_time:32033ms step_avg:57.51ms
step:558/2330 train_time:32094ms step_avg:57.52ms
step:559/2330 train_time:32151ms step_avg:57.51ms
step:560/2330 train_time:32210ms step_avg:57.52ms
step:561/2330 train_time:32266ms step_avg:57.51ms
step:562/2330 train_time:32325ms step_avg:57.52ms
step:563/2330 train_time:32382ms step_avg:57.52ms
step:564/2330 train_time:32440ms step_avg:57.52ms
step:565/2330 train_time:32496ms step_avg:57.51ms
step:566/2330 train_time:32555ms step_avg:57.52ms
step:567/2330 train_time:32611ms step_avg:57.52ms
step:568/2330 train_time:32669ms step_avg:57.52ms
step:569/2330 train_time:32725ms step_avg:57.51ms
step:570/2330 train_time:32784ms step_avg:57.52ms
step:571/2330 train_time:32840ms step_avg:57.51ms
step:572/2330 train_time:32898ms step_avg:57.51ms
step:573/2330 train_time:32954ms step_avg:57.51ms
step:574/2330 train_time:33014ms step_avg:57.52ms
step:575/2330 train_time:33071ms step_avg:57.51ms
step:576/2330 train_time:33131ms step_avg:57.52ms
step:577/2330 train_time:33188ms step_avg:57.52ms
step:578/2330 train_time:33248ms step_avg:57.52ms
step:579/2330 train_time:33304ms step_avg:57.52ms
step:580/2330 train_time:33362ms step_avg:57.52ms
step:581/2330 train_time:33418ms step_avg:57.52ms
step:582/2330 train_time:33477ms step_avg:57.52ms
step:583/2330 train_time:33533ms step_avg:57.52ms
step:584/2330 train_time:33592ms step_avg:57.52ms
step:585/2330 train_time:33648ms step_avg:57.52ms
step:586/2330 train_time:33707ms step_avg:57.52ms
step:587/2330 train_time:33762ms step_avg:57.52ms
step:588/2330 train_time:33822ms step_avg:57.52ms
step:589/2330 train_time:33878ms step_avg:57.52ms
step:590/2330 train_time:33938ms step_avg:57.52ms
step:591/2330 train_time:33994ms step_avg:57.52ms
step:592/2330 train_time:34053ms step_avg:57.52ms
step:593/2330 train_time:34109ms step_avg:57.52ms
step:594/2330 train_time:34169ms step_avg:57.52ms
step:595/2330 train_time:34226ms step_avg:57.52ms
step:596/2330 train_time:34284ms step_avg:57.52ms
step:597/2330 train_time:34341ms step_avg:57.52ms
step:598/2330 train_time:34400ms step_avg:57.53ms
step:599/2330 train_time:34456ms step_avg:57.52ms
step:600/2330 train_time:34515ms step_avg:57.52ms
step:601/2330 train_time:34572ms step_avg:57.52ms
step:602/2330 train_time:34630ms step_avg:57.53ms
step:603/2330 train_time:34686ms step_avg:57.52ms
step:604/2330 train_time:34745ms step_avg:57.52ms
step:605/2330 train_time:34801ms step_avg:57.52ms
step:606/2330 train_time:34860ms step_avg:57.52ms
step:607/2330 train_time:34915ms step_avg:57.52ms
step:608/2330 train_time:34976ms step_avg:57.53ms
step:609/2330 train_time:35033ms step_avg:57.52ms
step:610/2330 train_time:35091ms step_avg:57.53ms
step:611/2330 train_time:35148ms step_avg:57.53ms
step:612/2330 train_time:35208ms step_avg:57.53ms
step:613/2330 train_time:35264ms step_avg:57.53ms
step:614/2330 train_time:35323ms step_avg:57.53ms
step:615/2330 train_time:35379ms step_avg:57.53ms
step:616/2330 train_time:35438ms step_avg:57.53ms
step:617/2330 train_time:35494ms step_avg:57.53ms
step:618/2330 train_time:35553ms step_avg:57.53ms
step:619/2330 train_time:35609ms step_avg:57.53ms
step:620/2330 train_time:35668ms step_avg:57.53ms
step:621/2330 train_time:35724ms step_avg:57.53ms
step:622/2330 train_time:35783ms step_avg:57.53ms
step:623/2330 train_time:35839ms step_avg:57.53ms
step:624/2330 train_time:35898ms step_avg:57.53ms
step:625/2330 train_time:35954ms step_avg:57.53ms
step:626/2330 train_time:36013ms step_avg:57.53ms
step:627/2330 train_time:36069ms step_avg:57.53ms
step:628/2330 train_time:36129ms step_avg:57.53ms
step:629/2330 train_time:36185ms step_avg:57.53ms
step:630/2330 train_time:36244ms step_avg:57.53ms
step:631/2330 train_time:36301ms step_avg:57.53ms
step:632/2330 train_time:36359ms step_avg:57.53ms
step:633/2330 train_time:36415ms step_avg:57.53ms
step:634/2330 train_time:36474ms step_avg:57.53ms
step:635/2330 train_time:36530ms step_avg:57.53ms
step:636/2330 train_time:36590ms step_avg:57.53ms
step:637/2330 train_time:36646ms step_avg:57.53ms
step:638/2330 train_time:36705ms step_avg:57.53ms
step:639/2330 train_time:36761ms step_avg:57.53ms
step:640/2330 train_time:36820ms step_avg:57.53ms
step:641/2330 train_time:36875ms step_avg:57.53ms
step:642/2330 train_time:36934ms step_avg:57.53ms
step:643/2330 train_time:36991ms step_avg:57.53ms
step:644/2330 train_time:37050ms step_avg:57.53ms
step:645/2330 train_time:37107ms step_avg:57.53ms
step:646/2330 train_time:37166ms step_avg:57.53ms
step:647/2330 train_time:37223ms step_avg:57.53ms
step:648/2330 train_time:37282ms step_avg:57.53ms
step:649/2330 train_time:37338ms step_avg:57.53ms
step:650/2330 train_time:37397ms step_avg:57.53ms
step:651/2330 train_time:37453ms step_avg:57.53ms
step:652/2330 train_time:37511ms step_avg:57.53ms
step:653/2330 train_time:37569ms step_avg:57.53ms
step:654/2330 train_time:37627ms step_avg:57.53ms
step:655/2330 train_time:37684ms step_avg:57.53ms
step:656/2330 train_time:37743ms step_avg:57.53ms
step:657/2330 train_time:37799ms step_avg:57.53ms
step:658/2330 train_time:37858ms step_avg:57.53ms
step:659/2330 train_time:37914ms step_avg:57.53ms
step:660/2330 train_time:37972ms step_avg:57.53ms
step:661/2330 train_time:38028ms step_avg:57.53ms
step:662/2330 train_time:38088ms step_avg:57.54ms
step:663/2330 train_time:38144ms step_avg:57.53ms
step:664/2330 train_time:38204ms step_avg:57.54ms
step:665/2330 train_time:38260ms step_avg:57.53ms
step:666/2330 train_time:38319ms step_avg:57.54ms
step:667/2330 train_time:38375ms step_avg:57.53ms
step:668/2330 train_time:38434ms step_avg:57.54ms
step:669/2330 train_time:38490ms step_avg:57.53ms
step:670/2330 train_time:38550ms step_avg:57.54ms
step:671/2330 train_time:38606ms step_avg:57.53ms
step:672/2330 train_time:38665ms step_avg:57.54ms
step:673/2330 train_time:38721ms step_avg:57.53ms
step:674/2330 train_time:38779ms step_avg:57.54ms
step:675/2330 train_time:38836ms step_avg:57.53ms
step:676/2330 train_time:38894ms step_avg:57.54ms
step:677/2330 train_time:38951ms step_avg:57.53ms
step:678/2330 train_time:39009ms step_avg:57.54ms
step:679/2330 train_time:39065ms step_avg:57.53ms
step:680/2330 train_time:39125ms step_avg:57.54ms
step:681/2330 train_time:39181ms step_avg:57.53ms
step:682/2330 train_time:39240ms step_avg:57.54ms
step:683/2330 train_time:39297ms step_avg:57.54ms
step:684/2330 train_time:39356ms step_avg:57.54ms
step:685/2330 train_time:39412ms step_avg:57.54ms
step:686/2330 train_time:39471ms step_avg:57.54ms
step:687/2330 train_time:39527ms step_avg:57.54ms
step:688/2330 train_time:39587ms step_avg:57.54ms
step:689/2330 train_time:39643ms step_avg:57.54ms
step:690/2330 train_time:39703ms step_avg:57.54ms
step:691/2330 train_time:39759ms step_avg:57.54ms
step:692/2330 train_time:39818ms step_avg:57.54ms
step:693/2330 train_time:39874ms step_avg:57.54ms
step:694/2330 train_time:39933ms step_avg:57.54ms
step:695/2330 train_time:39990ms step_avg:57.54ms
step:696/2330 train_time:40048ms step_avg:57.54ms
step:697/2330 train_time:40104ms step_avg:57.54ms
step:698/2330 train_time:40164ms step_avg:57.54ms
step:699/2330 train_time:40220ms step_avg:57.54ms
step:700/2330 train_time:40279ms step_avg:57.54ms
step:701/2330 train_time:40335ms step_avg:57.54ms
step:702/2330 train_time:40395ms step_avg:57.54ms
step:703/2330 train_time:40451ms step_avg:57.54ms
step:704/2330 train_time:40511ms step_avg:57.54ms
step:705/2330 train_time:40568ms step_avg:57.54ms
step:706/2330 train_time:40627ms step_avg:57.55ms
step:707/2330 train_time:40683ms step_avg:57.54ms
step:708/2330 train_time:40742ms step_avg:57.54ms
step:709/2330 train_time:40798ms step_avg:57.54ms
step:710/2330 train_time:40857ms step_avg:57.55ms
step:711/2330 train_time:40914ms step_avg:57.54ms
step:712/2330 train_time:40973ms step_avg:57.55ms
step:713/2330 train_time:41029ms step_avg:57.54ms
step:714/2330 train_time:41088ms step_avg:57.55ms
step:715/2330 train_time:41144ms step_avg:57.54ms
step:716/2330 train_time:41204ms step_avg:57.55ms
step:717/2330 train_time:41260ms step_avg:57.54ms
step:718/2330 train_time:41318ms step_avg:57.55ms
step:719/2330 train_time:41374ms step_avg:57.54ms
step:720/2330 train_time:41434ms step_avg:57.55ms
step:721/2330 train_time:41491ms step_avg:57.55ms
step:722/2330 train_time:41551ms step_avg:57.55ms
step:723/2330 train_time:41607ms step_avg:57.55ms
step:724/2330 train_time:41666ms step_avg:57.55ms
step:725/2330 train_time:41723ms step_avg:57.55ms
step:726/2330 train_time:41781ms step_avg:57.55ms
step:727/2330 train_time:41837ms step_avg:57.55ms
step:728/2330 train_time:41896ms step_avg:57.55ms
step:729/2330 train_time:41953ms step_avg:57.55ms
step:730/2330 train_time:42012ms step_avg:57.55ms
step:731/2330 train_time:42069ms step_avg:57.55ms
step:732/2330 train_time:42128ms step_avg:57.55ms
step:733/2330 train_time:42184ms step_avg:57.55ms
step:734/2330 train_time:42243ms step_avg:57.55ms
step:735/2330 train_time:42300ms step_avg:57.55ms
step:736/2330 train_time:42358ms step_avg:57.55ms
step:737/2330 train_time:42414ms step_avg:57.55ms
step:738/2330 train_time:42473ms step_avg:57.55ms
step:739/2330 train_time:42529ms step_avg:57.55ms
step:740/2330 train_time:42589ms step_avg:57.55ms
step:741/2330 train_time:42645ms step_avg:57.55ms
step:742/2330 train_time:42705ms step_avg:57.55ms
step:743/2330 train_time:42761ms step_avg:57.55ms
step:744/2330 train_time:42820ms step_avg:57.55ms
step:745/2330 train_time:42876ms step_avg:57.55ms
step:746/2330 train_time:42934ms step_avg:57.55ms
step:747/2330 train_time:42991ms step_avg:57.55ms
step:748/2330 train_time:43051ms step_avg:57.55ms
step:749/2330 train_time:43107ms step_avg:57.55ms
step:750/2330 train_time:43167ms step_avg:57.56ms
step:750/2330 val_loss:4.2144 train_time:43246ms step_avg:57.66ms
step:751/2330 train_time:43266ms step_avg:57.61ms
step:752/2330 train_time:43285ms step_avg:57.56ms
step:753/2330 train_time:43339ms step_avg:57.56ms
step:754/2330 train_time:43406ms step_avg:57.57ms
step:755/2330 train_time:43463ms step_avg:57.57ms
step:756/2330 train_time:43523ms step_avg:57.57ms
step:757/2330 train_time:43580ms step_avg:57.57ms
step:758/2330 train_time:43638ms step_avg:57.57ms
step:759/2330 train_time:43694ms step_avg:57.57ms
step:760/2330 train_time:43752ms step_avg:57.57ms
step:761/2330 train_time:43808ms step_avg:57.57ms
step:762/2330 train_time:43867ms step_avg:57.57ms
step:763/2330 train_time:43923ms step_avg:57.57ms
step:764/2330 train_time:43981ms step_avg:57.57ms
step:765/2330 train_time:44038ms step_avg:57.57ms
step:766/2330 train_time:44096ms step_avg:57.57ms
step:767/2330 train_time:44153ms step_avg:57.57ms
step:768/2330 train_time:44213ms step_avg:57.57ms
step:769/2330 train_time:44271ms step_avg:57.57ms
step:770/2330 train_time:44333ms step_avg:57.58ms
step:771/2330 train_time:44392ms step_avg:57.58ms
step:772/2330 train_time:44454ms step_avg:57.58ms
step:773/2330 train_time:44512ms step_avg:57.58ms
step:774/2330 train_time:44573ms step_avg:57.59ms
step:775/2330 train_time:44631ms step_avg:57.59ms
step:776/2330 train_time:44690ms step_avg:57.59ms
step:777/2330 train_time:44747ms step_avg:57.59ms
step:778/2330 train_time:44807ms step_avg:57.59ms
step:779/2330 train_time:44863ms step_avg:57.59ms
step:780/2330 train_time:44923ms step_avg:57.59ms
step:781/2330 train_time:44980ms step_avg:57.59ms
step:782/2330 train_time:45039ms step_avg:57.59ms
step:783/2330 train_time:45096ms step_avg:57.59ms
step:784/2330 train_time:45155ms step_avg:57.60ms
step:785/2330 train_time:45212ms step_avg:57.60ms
step:786/2330 train_time:45272ms step_avg:57.60ms
step:787/2330 train_time:45330ms step_avg:57.60ms
step:788/2330 train_time:45390ms step_avg:57.60ms
step:789/2330 train_time:45449ms step_avg:57.60ms
step:790/2330 train_time:45510ms step_avg:57.61ms
step:791/2330 train_time:45567ms step_avg:57.61ms
step:792/2330 train_time:45627ms step_avg:57.61ms
step:793/2330 train_time:45685ms step_avg:57.61ms
step:794/2330 train_time:45744ms step_avg:57.61ms
step:795/2330 train_time:45801ms step_avg:57.61ms
step:796/2330 train_time:45860ms step_avg:57.61ms
step:797/2330 train_time:45917ms step_avg:57.61ms
step:798/2330 train_time:45976ms step_avg:57.61ms
step:799/2330 train_time:46032ms step_avg:57.61ms
step:800/2330 train_time:46092ms step_avg:57.62ms
step:801/2330 train_time:46150ms step_avg:57.62ms
step:802/2330 train_time:46210ms step_avg:57.62ms
step:803/2330 train_time:46267ms step_avg:57.62ms
step:804/2330 train_time:46326ms step_avg:57.62ms
step:805/2330 train_time:46384ms step_avg:57.62ms
step:806/2330 train_time:46444ms step_avg:57.62ms
step:807/2330 train_time:46502ms step_avg:57.62ms
step:808/2330 train_time:46562ms step_avg:57.63ms
step:809/2330 train_time:46619ms step_avg:57.62ms
step:810/2330 train_time:46679ms step_avg:57.63ms
step:811/2330 train_time:46736ms step_avg:57.63ms
step:812/2330 train_time:46796ms step_avg:57.63ms
step:813/2330 train_time:46853ms step_avg:57.63ms
step:814/2330 train_time:46913ms step_avg:57.63ms
step:815/2330 train_time:46970ms step_avg:57.63ms
step:816/2330 train_time:47029ms step_avg:57.63ms
step:817/2330 train_time:47086ms step_avg:57.63ms
step:818/2330 train_time:47146ms step_avg:57.64ms
step:819/2330 train_time:47203ms step_avg:57.63ms
step:820/2330 train_time:47263ms step_avg:57.64ms
step:821/2330 train_time:47321ms step_avg:57.64ms
step:822/2330 train_time:47380ms step_avg:57.64ms
step:823/2330 train_time:47438ms step_avg:57.64ms
step:824/2330 train_time:47498ms step_avg:57.64ms
step:825/2330 train_time:47555ms step_avg:57.64ms
step:826/2330 train_time:47616ms step_avg:57.65ms
step:827/2330 train_time:47673ms step_avg:57.65ms
step:828/2330 train_time:47733ms step_avg:57.65ms
step:829/2330 train_time:47791ms step_avg:57.65ms
step:830/2330 train_time:47851ms step_avg:57.65ms
step:831/2330 train_time:47908ms step_avg:57.65ms
step:832/2330 train_time:47966ms step_avg:57.65ms
step:833/2330 train_time:48023ms step_avg:57.65ms
step:834/2330 train_time:48083ms step_avg:57.65ms
step:835/2330 train_time:48140ms step_avg:57.65ms
step:836/2330 train_time:48200ms step_avg:57.66ms
step:837/2330 train_time:48257ms step_avg:57.65ms
step:838/2330 train_time:48317ms step_avg:57.66ms
step:839/2330 train_time:48374ms step_avg:57.66ms
step:840/2330 train_time:48434ms step_avg:57.66ms
step:841/2330 train_time:48492ms step_avg:57.66ms
step:842/2330 train_time:48552ms step_avg:57.66ms
step:843/2330 train_time:48610ms step_avg:57.66ms
step:844/2330 train_time:48669ms step_avg:57.66ms
step:845/2330 train_time:48726ms step_avg:57.66ms
step:846/2330 train_time:48786ms step_avg:57.67ms
step:847/2330 train_time:48844ms step_avg:57.67ms
step:848/2330 train_time:48902ms step_avg:57.67ms
step:849/2330 train_time:48959ms step_avg:57.67ms
step:850/2330 train_time:49019ms step_avg:57.67ms
step:851/2330 train_time:49076ms step_avg:57.67ms
step:852/2330 train_time:49136ms step_avg:57.67ms
step:853/2330 train_time:49193ms step_avg:57.67ms
step:854/2330 train_time:49253ms step_avg:57.67ms
step:855/2330 train_time:49311ms step_avg:57.67ms
step:856/2330 train_time:49371ms step_avg:57.68ms
step:857/2330 train_time:49428ms step_avg:57.68ms
step:858/2330 train_time:49489ms step_avg:57.68ms
step:859/2330 train_time:49547ms step_avg:57.68ms
step:860/2330 train_time:49606ms step_avg:57.68ms
step:861/2330 train_time:49663ms step_avg:57.68ms
step:862/2330 train_time:49724ms step_avg:57.68ms
step:863/2330 train_time:49781ms step_avg:57.68ms
step:864/2330 train_time:49841ms step_avg:57.69ms
step:865/2330 train_time:49897ms step_avg:57.68ms
step:866/2330 train_time:49957ms step_avg:57.69ms
step:867/2330 train_time:50014ms step_avg:57.69ms
step:868/2330 train_time:50075ms step_avg:57.69ms
step:869/2330 train_time:50132ms step_avg:57.69ms
step:870/2330 train_time:50192ms step_avg:57.69ms
step:871/2330 train_time:50249ms step_avg:57.69ms
step:872/2330 train_time:50309ms step_avg:57.69ms
step:873/2330 train_time:50366ms step_avg:57.69ms
step:874/2330 train_time:50426ms step_avg:57.70ms
step:875/2330 train_time:50483ms step_avg:57.70ms
step:876/2330 train_time:50543ms step_avg:57.70ms
step:877/2330 train_time:50600ms step_avg:57.70ms
step:878/2330 train_time:50660ms step_avg:57.70ms
step:879/2330 train_time:50717ms step_avg:57.70ms
step:880/2330 train_time:50778ms step_avg:57.70ms
step:881/2330 train_time:50835ms step_avg:57.70ms
step:882/2330 train_time:50895ms step_avg:57.70ms
step:883/2330 train_time:50952ms step_avg:57.70ms
step:884/2330 train_time:51012ms step_avg:57.71ms
step:885/2330 train_time:51069ms step_avg:57.71ms
step:886/2330 train_time:51129ms step_avg:57.71ms
step:887/2330 train_time:51186ms step_avg:57.71ms
step:888/2330 train_time:51246ms step_avg:57.71ms
step:889/2330 train_time:51303ms step_avg:57.71ms
step:890/2330 train_time:51363ms step_avg:57.71ms
step:891/2330 train_time:51421ms step_avg:57.71ms
step:892/2330 train_time:51480ms step_avg:57.71ms
step:893/2330 train_time:51537ms step_avg:57.71ms
step:894/2330 train_time:51597ms step_avg:57.71ms
step:895/2330 train_time:51655ms step_avg:57.71ms
step:896/2330 train_time:51715ms step_avg:57.72ms
step:897/2330 train_time:51772ms step_avg:57.72ms
step:898/2330 train_time:51832ms step_avg:57.72ms
step:899/2330 train_time:51889ms step_avg:57.72ms
step:900/2330 train_time:51949ms step_avg:57.72ms
step:901/2330 train_time:52006ms step_avg:57.72ms
step:902/2330 train_time:52065ms step_avg:57.72ms
step:903/2330 train_time:52122ms step_avg:57.72ms
step:904/2330 train_time:52182ms step_avg:57.72ms
step:905/2330 train_time:52239ms step_avg:57.72ms
step:906/2330 train_time:52299ms step_avg:57.73ms
step:907/2330 train_time:52357ms step_avg:57.73ms
step:908/2330 train_time:52417ms step_avg:57.73ms
step:909/2330 train_time:52474ms step_avg:57.73ms
step:910/2330 train_time:52534ms step_avg:57.73ms
step:911/2330 train_time:52591ms step_avg:57.73ms
step:912/2330 train_time:52651ms step_avg:57.73ms
step:913/2330 train_time:52707ms step_avg:57.73ms
step:914/2330 train_time:52767ms step_avg:57.73ms
step:915/2330 train_time:52824ms step_avg:57.73ms
step:916/2330 train_time:52885ms step_avg:57.73ms
step:917/2330 train_time:52942ms step_avg:57.73ms
step:918/2330 train_time:53001ms step_avg:57.74ms
step:919/2330 train_time:53058ms step_avg:57.73ms
step:920/2330 train_time:53119ms step_avg:57.74ms
step:921/2330 train_time:53176ms step_avg:57.74ms
step:922/2330 train_time:53236ms step_avg:57.74ms
step:923/2330 train_time:53293ms step_avg:57.74ms
step:924/2330 train_time:53353ms step_avg:57.74ms
step:925/2330 train_time:53410ms step_avg:57.74ms
step:926/2330 train_time:53469ms step_avg:57.74ms
step:927/2330 train_time:53527ms step_avg:57.74ms
step:928/2330 train_time:53586ms step_avg:57.74ms
step:929/2330 train_time:53643ms step_avg:57.74ms
step:930/2330 train_time:53704ms step_avg:57.75ms
step:931/2330 train_time:53760ms step_avg:57.74ms
step:932/2330 train_time:53821ms step_avg:57.75ms
step:933/2330 train_time:53878ms step_avg:57.75ms
step:934/2330 train_time:53939ms step_avg:57.75ms
step:935/2330 train_time:53996ms step_avg:57.75ms
step:936/2330 train_time:54055ms step_avg:57.75ms
step:937/2330 train_time:54112ms step_avg:57.75ms
step:938/2330 train_time:54172ms step_avg:57.75ms
step:939/2330 train_time:54230ms step_avg:57.75ms
step:940/2330 train_time:54289ms step_avg:57.75ms
step:941/2330 train_time:54346ms step_avg:57.75ms
step:942/2330 train_time:54406ms step_avg:57.76ms
step:943/2330 train_time:54462ms step_avg:57.75ms
step:944/2330 train_time:54522ms step_avg:57.76ms
step:945/2330 train_time:54580ms step_avg:57.76ms
step:946/2330 train_time:54640ms step_avg:57.76ms
step:947/2330 train_time:54697ms step_avg:57.76ms
step:948/2330 train_time:54757ms step_avg:57.76ms
step:949/2330 train_time:54814ms step_avg:57.76ms
step:950/2330 train_time:54875ms step_avg:57.76ms
step:951/2330 train_time:54933ms step_avg:57.76ms
step:952/2330 train_time:54992ms step_avg:57.76ms
step:953/2330 train_time:55050ms step_avg:57.76ms
step:954/2330 train_time:55110ms step_avg:57.77ms
step:955/2330 train_time:55166ms step_avg:57.77ms
step:956/2330 train_time:55228ms step_avg:57.77ms
step:957/2330 train_time:55284ms step_avg:57.77ms
step:958/2330 train_time:55345ms step_avg:57.77ms
step:959/2330 train_time:55402ms step_avg:57.77ms
step:960/2330 train_time:55463ms step_avg:57.77ms
step:961/2330 train_time:55519ms step_avg:57.77ms
step:962/2330 train_time:55580ms step_avg:57.78ms
step:963/2330 train_time:55637ms step_avg:57.77ms
step:964/2330 train_time:55696ms step_avg:57.78ms
step:965/2330 train_time:55754ms step_avg:57.78ms
step:966/2330 train_time:55814ms step_avg:57.78ms
step:967/2330 train_time:55871ms step_avg:57.78ms
step:968/2330 train_time:55931ms step_avg:57.78ms
step:969/2330 train_time:55988ms step_avg:57.78ms
step:970/2330 train_time:56048ms step_avg:57.78ms
step:971/2330 train_time:56106ms step_avg:57.78ms
step:972/2330 train_time:56166ms step_avg:57.78ms
step:973/2330 train_time:56223ms step_avg:57.78ms
step:974/2330 train_time:56282ms step_avg:57.78ms
step:975/2330 train_time:56340ms step_avg:57.78ms
step:976/2330 train_time:56400ms step_avg:57.79ms
step:977/2330 train_time:56456ms step_avg:57.79ms
step:978/2330 train_time:56516ms step_avg:57.79ms
step:979/2330 train_time:56573ms step_avg:57.79ms
step:980/2330 train_time:56633ms step_avg:57.79ms
step:981/2330 train_time:56691ms step_avg:57.79ms
step:982/2330 train_time:56751ms step_avg:57.79ms
step:983/2330 train_time:56809ms step_avg:57.79ms
step:984/2330 train_time:56868ms step_avg:57.79ms
step:985/2330 train_time:56926ms step_avg:57.79ms
step:986/2330 train_time:56985ms step_avg:57.79ms
step:987/2330 train_time:57042ms step_avg:57.79ms
step:988/2330 train_time:57102ms step_avg:57.80ms
step:989/2330 train_time:57160ms step_avg:57.80ms
step:990/2330 train_time:57220ms step_avg:57.80ms
step:991/2330 train_time:57277ms step_avg:57.80ms
step:992/2330 train_time:57336ms step_avg:57.80ms
step:993/2330 train_time:57393ms step_avg:57.80ms
step:994/2330 train_time:57453ms step_avg:57.80ms
step:995/2330 train_time:57511ms step_avg:57.80ms
step:996/2330 train_time:57572ms step_avg:57.80ms
step:997/2330 train_time:57628ms step_avg:57.80ms
step:998/2330 train_time:57690ms step_avg:57.81ms
step:999/2330 train_time:57747ms step_avg:57.80ms
step:1000/2330 train_time:57807ms step_avg:57.81ms
step:1000/2330 val_loss:4.0579 train_time:57888ms step_avg:57.89ms
step:1001/2330 train_time:57909ms step_avg:57.85ms
step:1002/2330 train_time:57930ms step_avg:57.81ms
step:1003/2330 train_time:57988ms step_avg:57.81ms
step:1004/2330 train_time:58052ms step_avg:57.82ms
step:1005/2330 train_time:58111ms step_avg:57.82ms
step:1006/2330 train_time:58170ms step_avg:57.82ms
step:1007/2330 train_time:58227ms step_avg:57.82ms
step:1008/2330 train_time:58285ms step_avg:57.82ms
step:1009/2330 train_time:58342ms step_avg:57.82ms
step:1010/2330 train_time:58401ms step_avg:57.82ms
step:1011/2330 train_time:58457ms step_avg:57.82ms
step:1012/2330 train_time:58517ms step_avg:57.82ms
step:1013/2330 train_time:58573ms step_avg:57.82ms
step:1014/2330 train_time:58632ms step_avg:57.82ms
step:1015/2330 train_time:58688ms step_avg:57.82ms
step:1016/2330 train_time:58747ms step_avg:57.82ms
step:1017/2330 train_time:58804ms step_avg:57.82ms
step:1018/2330 train_time:58870ms step_avg:57.83ms
step:1019/2330 train_time:58928ms step_avg:57.83ms
step:1020/2330 train_time:58992ms step_avg:57.84ms
step:1021/2330 train_time:59051ms step_avg:57.84ms
step:1022/2330 train_time:59111ms step_avg:57.84ms
step:1023/2330 train_time:59169ms step_avg:57.84ms
step:1024/2330 train_time:59228ms step_avg:57.84ms
step:1025/2330 train_time:59286ms step_avg:57.84ms
step:1026/2330 train_time:59346ms step_avg:57.84ms
step:1027/2330 train_time:59402ms step_avg:57.84ms
step:1028/2330 train_time:59462ms step_avg:57.84ms
step:1029/2330 train_time:59518ms step_avg:57.84ms
step:1030/2330 train_time:59577ms step_avg:57.84ms
step:1031/2330 train_time:59634ms step_avg:57.84ms
step:1032/2330 train_time:59693ms step_avg:57.84ms
step:1033/2330 train_time:59749ms step_avg:57.84ms
step:1034/2330 train_time:59811ms step_avg:57.84ms
step:1035/2330 train_time:59869ms step_avg:57.84ms
step:1036/2330 train_time:59931ms step_avg:57.85ms
step:1037/2330 train_time:59989ms step_avg:57.85ms
step:1038/2330 train_time:60049ms step_avg:57.85ms
step:1039/2330 train_time:60106ms step_avg:57.85ms
step:1040/2330 train_time:60168ms step_avg:57.85ms
step:1041/2330 train_time:60226ms step_avg:57.85ms
step:1042/2330 train_time:60286ms step_avg:57.86ms
step:1043/2330 train_time:60345ms step_avg:57.86ms
step:1044/2330 train_time:60405ms step_avg:57.86ms
step:1045/2330 train_time:60462ms step_avg:57.86ms
step:1046/2330 train_time:60521ms step_avg:57.86ms
step:1047/2330 train_time:60578ms step_avg:57.86ms
step:1048/2330 train_time:60637ms step_avg:57.86ms
step:1049/2330 train_time:60694ms step_avg:57.86ms
step:1050/2330 train_time:60754ms step_avg:57.86ms
step:1051/2330 train_time:60811ms step_avg:57.86ms
step:1052/2330 train_time:60872ms step_avg:57.86ms
step:1053/2330 train_time:60929ms step_avg:57.86ms
step:1054/2330 train_time:60990ms step_avg:57.87ms
step:1055/2330 train_time:61047ms step_avg:57.86ms
step:1056/2330 train_time:61108ms step_avg:57.87ms
step:1057/2330 train_time:61164ms step_avg:57.87ms
step:1058/2330 train_time:61225ms step_avg:57.87ms
step:1059/2330 train_time:61282ms step_avg:57.87ms
step:1060/2330 train_time:61342ms step_avg:57.87ms
step:1061/2330 train_time:61399ms step_avg:57.87ms
step:1062/2330 train_time:61459ms step_avg:57.87ms
step:1063/2330 train_time:61516ms step_avg:57.87ms
step:1064/2330 train_time:61575ms step_avg:57.87ms
step:1065/2330 train_time:61631ms step_avg:57.87ms
step:1066/2330 train_time:61692ms step_avg:57.87ms
step:1067/2330 train_time:61748ms step_avg:57.87ms
step:1068/2330 train_time:61810ms step_avg:57.87ms
step:1069/2330 train_time:61866ms step_avg:57.87ms
step:1070/2330 train_time:61927ms step_avg:57.88ms
step:1071/2330 train_time:61985ms step_avg:57.88ms
step:1072/2330 train_time:62045ms step_avg:57.88ms
step:1073/2330 train_time:62102ms step_avg:57.88ms
step:1074/2330 train_time:62163ms step_avg:57.88ms
step:1075/2330 train_time:62221ms step_avg:57.88ms
step:1076/2330 train_time:62280ms step_avg:57.88ms
step:1077/2330 train_time:62338ms step_avg:57.88ms
step:1078/2330 train_time:62397ms step_avg:57.88ms
step:1079/2330 train_time:62454ms step_avg:57.88ms
step:1080/2330 train_time:62514ms step_avg:57.88ms
step:1081/2330 train_time:62570ms step_avg:57.88ms
step:1082/2330 train_time:62630ms step_avg:57.88ms
step:1083/2330 train_time:62688ms step_avg:57.88ms
step:1084/2330 train_time:62748ms step_avg:57.89ms
step:1085/2330 train_time:62805ms step_avg:57.88ms
step:1086/2330 train_time:62865ms step_avg:57.89ms
step:1087/2330 train_time:62923ms step_avg:57.89ms
step:1088/2330 train_time:62982ms step_avg:57.89ms
step:1089/2330 train_time:63039ms step_avg:57.89ms
step:1090/2330 train_time:63099ms step_avg:57.89ms
step:1091/2330 train_time:63156ms step_avg:57.89ms
step:1092/2330 train_time:63217ms step_avg:57.89ms
step:1093/2330 train_time:63274ms step_avg:57.89ms
step:1094/2330 train_time:63334ms step_avg:57.89ms
step:1095/2330 train_time:63391ms step_avg:57.89ms
step:1096/2330 train_time:63451ms step_avg:57.89ms
step:1097/2330 train_time:63508ms step_avg:57.89ms
step:1098/2330 train_time:63569ms step_avg:57.90ms
step:1099/2330 train_time:63626ms step_avg:57.89ms
step:1100/2330 train_time:63686ms step_avg:57.90ms
step:1101/2330 train_time:63743ms step_avg:57.90ms
step:1102/2330 train_time:63802ms step_avg:57.90ms
step:1103/2330 train_time:63859ms step_avg:57.90ms
step:1104/2330 train_time:63919ms step_avg:57.90ms
step:1105/2330 train_time:63977ms step_avg:57.90ms
step:1106/2330 train_time:64036ms step_avg:57.90ms
step:1107/2330 train_time:64093ms step_avg:57.90ms
step:1108/2330 train_time:64153ms step_avg:57.90ms
step:1109/2330 train_time:64210ms step_avg:57.90ms
step:1110/2330 train_time:64271ms step_avg:57.90ms
step:1111/2330 train_time:64329ms step_avg:57.90ms
step:1112/2330 train_time:64389ms step_avg:57.90ms
step:1113/2330 train_time:64446ms step_avg:57.90ms
step:1114/2330 train_time:64506ms step_avg:57.90ms
step:1115/2330 train_time:64562ms step_avg:57.90ms
step:1116/2330 train_time:64622ms step_avg:57.90ms
step:1117/2330 train_time:64679ms step_avg:57.90ms
step:1118/2330 train_time:64739ms step_avg:57.91ms
step:1119/2330 train_time:64796ms step_avg:57.91ms
step:1120/2330 train_time:64855ms step_avg:57.91ms
step:1121/2330 train_time:64912ms step_avg:57.91ms
step:1122/2330 train_time:64972ms step_avg:57.91ms
step:1123/2330 train_time:65030ms step_avg:57.91ms
step:1124/2330 train_time:65090ms step_avg:57.91ms
step:1125/2330 train_time:65148ms step_avg:57.91ms
step:1126/2330 train_time:65208ms step_avg:57.91ms
step:1127/2330 train_time:65265ms step_avg:57.91ms
step:1128/2330 train_time:65325ms step_avg:57.91ms
step:1129/2330 train_time:65383ms step_avg:57.91ms
step:1130/2330 train_time:65442ms step_avg:57.91ms
step:1131/2330 train_time:65499ms step_avg:57.91ms
step:1132/2330 train_time:65559ms step_avg:57.91ms
step:1133/2330 train_time:65616ms step_avg:57.91ms
step:1134/2330 train_time:65675ms step_avg:57.91ms
step:1135/2330 train_time:65732ms step_avg:57.91ms
step:1136/2330 train_time:65793ms step_avg:57.92ms
step:1137/2330 train_time:65850ms step_avg:57.92ms
step:1138/2330 train_time:65910ms step_avg:57.92ms
step:1139/2330 train_time:65967ms step_avg:57.92ms
step:1140/2330 train_time:66027ms step_avg:57.92ms
step:1141/2330 train_time:66085ms step_avg:57.92ms
step:1142/2330 train_time:66145ms step_avg:57.92ms
step:1143/2330 train_time:66202ms step_avg:57.92ms
step:1144/2330 train_time:66262ms step_avg:57.92ms
step:1145/2330 train_time:66319ms step_avg:57.92ms
step:1146/2330 train_time:66379ms step_avg:57.92ms
step:1147/2330 train_time:66436ms step_avg:57.92ms
step:1148/2330 train_time:66496ms step_avg:57.92ms
step:1149/2330 train_time:66553ms step_avg:57.92ms
step:1150/2330 train_time:66613ms step_avg:57.92ms
step:1151/2330 train_time:66671ms step_avg:57.92ms
step:1152/2330 train_time:66730ms step_avg:57.93ms
step:1153/2330 train_time:66787ms step_avg:57.92ms
step:1154/2330 train_time:66847ms step_avg:57.93ms
step:1155/2330 train_time:66905ms step_avg:57.93ms
step:1156/2330 train_time:66965ms step_avg:57.93ms
step:1157/2330 train_time:67022ms step_avg:57.93ms
step:1158/2330 train_time:67082ms step_avg:57.93ms
step:1159/2330 train_time:67139ms step_avg:57.93ms
step:1160/2330 train_time:67199ms step_avg:57.93ms
step:1161/2330 train_time:67256ms step_avg:57.93ms
step:1162/2330 train_time:67316ms step_avg:57.93ms
step:1163/2330 train_time:67373ms step_avg:57.93ms
step:1164/2330 train_time:67432ms step_avg:57.93ms
step:1165/2330 train_time:67490ms step_avg:57.93ms
step:1166/2330 train_time:67549ms step_avg:57.93ms
step:1167/2330 train_time:67606ms step_avg:57.93ms
step:1168/2330 train_time:67667ms step_avg:57.93ms
step:1169/2330 train_time:67724ms step_avg:57.93ms
step:1170/2330 train_time:67784ms step_avg:57.93ms
step:1171/2330 train_time:67841ms step_avg:57.93ms
step:1172/2330 train_time:67901ms step_avg:57.94ms
step:1173/2330 train_time:67957ms step_avg:57.93ms
step:1174/2330 train_time:68017ms step_avg:57.94ms
step:1175/2330 train_time:68074ms step_avg:57.94ms
step:1176/2330 train_time:68134ms step_avg:57.94ms
step:1177/2330 train_time:68191ms step_avg:57.94ms
step:1178/2330 train_time:68251ms step_avg:57.94ms
step:1179/2330 train_time:68309ms step_avg:57.94ms
step:1180/2330 train_time:68369ms step_avg:57.94ms
step:1181/2330 train_time:68427ms step_avg:57.94ms
step:1182/2330 train_time:68487ms step_avg:57.94ms
step:1183/2330 train_time:68545ms step_avg:57.94ms
step:1184/2330 train_time:68605ms step_avg:57.94ms
step:1185/2330 train_time:68662ms step_avg:57.94ms
step:1186/2330 train_time:68721ms step_avg:57.94ms
step:1187/2330 train_time:68778ms step_avg:57.94ms
step:1188/2330 train_time:68839ms step_avg:57.95ms
step:1189/2330 train_time:68895ms step_avg:57.94ms
step:1190/2330 train_time:68956ms step_avg:57.95ms
step:1191/2330 train_time:69013ms step_avg:57.95ms
step:1192/2330 train_time:69074ms step_avg:57.95ms
step:1193/2330 train_time:69130ms step_avg:57.95ms
step:1194/2330 train_time:69190ms step_avg:57.95ms
step:1195/2330 train_time:69247ms step_avg:57.95ms
step:1196/2330 train_time:69307ms step_avg:57.95ms
step:1197/2330 train_time:69365ms step_avg:57.95ms
step:1198/2330 train_time:69425ms step_avg:57.95ms
step:1199/2330 train_time:69483ms step_avg:57.95ms
step:1200/2330 train_time:69543ms step_avg:57.95ms
step:1201/2330 train_time:69600ms step_avg:57.95ms
step:1202/2330 train_time:69660ms step_avg:57.95ms
step:1203/2330 train_time:69718ms step_avg:57.95ms
step:1204/2330 train_time:69777ms step_avg:57.95ms
step:1205/2330 train_time:69834ms step_avg:57.95ms
step:1206/2330 train_time:69894ms step_avg:57.96ms
step:1207/2330 train_time:69951ms step_avg:57.95ms
step:1208/2330 train_time:70012ms step_avg:57.96ms
step:1209/2330 train_time:70068ms step_avg:57.96ms
step:1210/2330 train_time:70129ms step_avg:57.96ms
step:1211/2330 train_time:70187ms step_avg:57.96ms
step:1212/2330 train_time:70246ms step_avg:57.96ms
step:1213/2330 train_time:70303ms step_avg:57.96ms
step:1214/2330 train_time:70364ms step_avg:57.96ms
step:1215/2330 train_time:70421ms step_avg:57.96ms
step:1216/2330 train_time:70481ms step_avg:57.96ms
step:1217/2330 train_time:70538ms step_avg:57.96ms
step:1218/2330 train_time:70598ms step_avg:57.96ms
step:1219/2330 train_time:70655ms step_avg:57.96ms
step:1220/2330 train_time:70715ms step_avg:57.96ms
step:1221/2330 train_time:70772ms step_avg:57.96ms
step:1222/2330 train_time:70833ms step_avg:57.96ms
step:1223/2330 train_time:70890ms step_avg:57.96ms
step:1224/2330 train_time:70950ms step_avg:57.97ms
step:1225/2330 train_time:71007ms step_avg:57.96ms
step:1226/2330 train_time:71067ms step_avg:57.97ms
step:1227/2330 train_time:71124ms step_avg:57.97ms
step:1228/2330 train_time:71185ms step_avg:57.97ms
step:1229/2330 train_time:71242ms step_avg:57.97ms
step:1230/2330 train_time:71302ms step_avg:57.97ms
step:1231/2330 train_time:71359ms step_avg:57.97ms
step:1232/2330 train_time:71419ms step_avg:57.97ms
step:1233/2330 train_time:71476ms step_avg:57.97ms
step:1234/2330 train_time:71536ms step_avg:57.97ms
step:1235/2330 train_time:71592ms step_avg:57.97ms
step:1236/2330 train_time:71654ms step_avg:57.97ms
step:1237/2330 train_time:71711ms step_avg:57.97ms
step:1238/2330 train_time:71772ms step_avg:57.97ms
step:1239/2330 train_time:71829ms step_avg:57.97ms
step:1240/2330 train_time:71889ms step_avg:57.97ms
step:1241/2330 train_time:71946ms step_avg:57.97ms
step:1242/2330 train_time:72006ms step_avg:57.98ms
step:1243/2330 train_time:72063ms step_avg:57.98ms
step:1244/2330 train_time:72123ms step_avg:57.98ms
step:1245/2330 train_time:72180ms step_avg:57.98ms
step:1246/2330 train_time:72241ms step_avg:57.98ms
step:1247/2330 train_time:72297ms step_avg:57.98ms
step:1248/2330 train_time:72357ms step_avg:57.98ms
step:1249/2330 train_time:72414ms step_avg:57.98ms
step:1250/2330 train_time:72475ms step_avg:57.98ms
step:1250/2330 val_loss:3.9790 train_time:72555ms step_avg:58.04ms
step:1251/2330 train_time:72575ms step_avg:58.01ms
step:1252/2330 train_time:72596ms step_avg:57.98ms
step:1253/2330 train_time:72656ms step_avg:57.99ms
step:1254/2330 train_time:72719ms step_avg:57.99ms
step:1255/2330 train_time:72776ms step_avg:57.99ms
step:1256/2330 train_time:72838ms step_avg:57.99ms
step:1257/2330 train_time:72894ms step_avg:57.99ms
step:1258/2330 train_time:72954ms step_avg:57.99ms
step:1259/2330 train_time:73011ms step_avg:57.99ms
step:1260/2330 train_time:73071ms step_avg:57.99ms
step:1261/2330 train_time:73127ms step_avg:57.99ms
step:1262/2330 train_time:73186ms step_avg:57.99ms
step:1263/2330 train_time:73242ms step_avg:57.99ms
step:1264/2330 train_time:73302ms step_avg:57.99ms
step:1265/2330 train_time:73358ms step_avg:57.99ms
step:1266/2330 train_time:73417ms step_avg:57.99ms
step:1267/2330 train_time:73474ms step_avg:57.99ms
step:1268/2330 train_time:73535ms step_avg:57.99ms
step:1269/2330 train_time:73594ms step_avg:57.99ms
step:1270/2330 train_time:73656ms step_avg:58.00ms
step:1271/2330 train_time:73713ms step_avg:58.00ms
step:1272/2330 train_time:73776ms step_avg:58.00ms
step:1273/2330 train_time:73832ms step_avg:58.00ms
step:1274/2330 train_time:73893ms step_avg:58.00ms
step:1275/2330 train_time:73950ms step_avg:58.00ms
step:1276/2330 train_time:74011ms step_avg:58.00ms
step:1277/2330 train_time:74068ms step_avg:58.00ms
step:1278/2330 train_time:74129ms step_avg:58.00ms
step:1279/2330 train_time:74186ms step_avg:58.00ms
step:1280/2330 train_time:74245ms step_avg:58.00ms
step:1281/2330 train_time:74301ms step_avg:58.00ms
step:1282/2330 train_time:74361ms step_avg:58.00ms
step:1283/2330 train_time:74418ms step_avg:58.00ms
step:1284/2330 train_time:74478ms step_avg:58.00ms
step:1285/2330 train_time:74535ms step_avg:58.00ms
step:1286/2330 train_time:74596ms step_avg:58.01ms
step:1287/2330 train_time:74654ms step_avg:58.01ms
step:1288/2330 train_time:74714ms step_avg:58.01ms
step:1289/2330 train_time:74771ms step_avg:58.01ms
step:1290/2330 train_time:74832ms step_avg:58.01ms
step:1291/2330 train_time:74889ms step_avg:58.01ms
step:1292/2330 train_time:74951ms step_avg:58.01ms
step:1293/2330 train_time:75007ms step_avg:58.01ms
step:1294/2330 train_time:75068ms step_avg:58.01ms
step:1295/2330 train_time:75125ms step_avg:58.01ms
step:1296/2330 train_time:75185ms step_avg:58.01ms
step:1297/2330 train_time:75242ms step_avg:58.01ms
step:1298/2330 train_time:75301ms step_avg:58.01ms
step:1299/2330 train_time:75358ms step_avg:58.01ms
step:1300/2330 train_time:75417ms step_avg:58.01ms
step:1301/2330 train_time:75474ms step_avg:58.01ms
step:1302/2330 train_time:75534ms step_avg:58.01ms
step:1303/2330 train_time:75592ms step_avg:58.01ms
step:1304/2330 train_time:75652ms step_avg:58.02ms
step:1305/2330 train_time:75709ms step_avg:58.01ms
step:1306/2330 train_time:75771ms step_avg:58.02ms
step:1307/2330 train_time:75829ms step_avg:58.02ms
step:1308/2330 train_time:75888ms step_avg:58.02ms
step:1309/2330 train_time:75945ms step_avg:58.02ms
step:1310/2330 train_time:76006ms step_avg:58.02ms
step:1311/2330 train_time:76063ms step_avg:58.02ms
step:1312/2330 train_time:76123ms step_avg:58.02ms
step:1313/2330 train_time:76179ms step_avg:58.02ms
step:1314/2330 train_time:76239ms step_avg:58.02ms
step:1315/2330 train_time:76295ms step_avg:58.02ms
step:1316/2330 train_time:76355ms step_avg:58.02ms
step:1317/2330 train_time:76413ms step_avg:58.02ms
step:1318/2330 train_time:76472ms step_avg:58.02ms
step:1319/2330 train_time:76530ms step_avg:58.02ms
step:1320/2330 train_time:76590ms step_avg:58.02ms
step:1321/2330 train_time:76647ms step_avg:58.02ms
step:1322/2330 train_time:76707ms step_avg:58.02ms
step:1323/2330 train_time:76765ms step_avg:58.02ms
step:1324/2330 train_time:76825ms step_avg:58.02ms
step:1325/2330 train_time:76881ms step_avg:58.02ms
step:1326/2330 train_time:76943ms step_avg:58.03ms
step:1327/2330 train_time:77000ms step_avg:58.03ms
step:1328/2330 train_time:77060ms step_avg:58.03ms
step:1329/2330 train_time:77117ms step_avg:58.03ms
step:1330/2330 train_time:77176ms step_avg:58.03ms
step:1331/2330 train_time:77234ms step_avg:58.03ms
step:1332/2330 train_time:77293ms step_avg:58.03ms
step:1333/2330 train_time:77350ms step_avg:58.03ms
step:1334/2330 train_time:77409ms step_avg:58.03ms
step:1335/2330 train_time:77467ms step_avg:58.03ms
step:1336/2330 train_time:77526ms step_avg:58.03ms
step:1337/2330 train_time:77583ms step_avg:58.03ms
step:1338/2330 train_time:77643ms step_avg:58.03ms
step:1339/2330 train_time:77701ms step_avg:58.03ms
step:1340/2330 train_time:77762ms step_avg:58.03ms
step:1341/2330 train_time:77819ms step_avg:58.03ms
step:1342/2330 train_time:77879ms step_avg:58.03ms
step:1343/2330 train_time:77936ms step_avg:58.03ms
step:1344/2330 train_time:77996ms step_avg:58.03ms
step:1345/2330 train_time:78053ms step_avg:58.03ms
step:1346/2330 train_time:78114ms step_avg:58.03ms
step:1347/2330 train_time:78171ms step_avg:58.03ms
step:1348/2330 train_time:78233ms step_avg:58.04ms
step:1349/2330 train_time:78289ms step_avg:58.03ms
step:1350/2330 train_time:78349ms step_avg:58.04ms
step:1351/2330 train_time:78406ms step_avg:58.04ms
step:1352/2330 train_time:78466ms step_avg:58.04ms
step:1353/2330 train_time:78523ms step_avg:58.04ms
step:1354/2330 train_time:78582ms step_avg:58.04ms
step:1355/2330 train_time:78640ms step_avg:58.04ms
step:1356/2330 train_time:78700ms step_avg:58.04ms
step:1357/2330 train_time:78757ms step_avg:58.04ms
step:1358/2330 train_time:78817ms step_avg:58.04ms
step:1359/2330 train_time:78875ms step_avg:58.04ms
step:1360/2330 train_time:78935ms step_avg:58.04ms
step:1361/2330 train_time:78992ms step_avg:58.04ms
step:1362/2330 train_time:79052ms step_avg:58.04ms
step:1363/2330 train_time:79110ms step_avg:58.04ms
step:1364/2330 train_time:79170ms step_avg:58.04ms
step:1365/2330 train_time:79227ms step_avg:58.04ms
step:1366/2330 train_time:79286ms step_avg:58.04ms
step:1367/2330 train_time:79342ms step_avg:58.04ms
step:1368/2330 train_time:79403ms step_avg:58.04ms
step:1369/2330 train_time:79460ms step_avg:58.04ms
step:1370/2330 train_time:79520ms step_avg:58.04ms
step:1371/2330 train_time:79576ms step_avg:58.04ms
step:1372/2330 train_time:79636ms step_avg:58.04ms
step:1373/2330 train_time:79693ms step_avg:58.04ms
step:1374/2330 train_time:79753ms step_avg:58.04ms
step:1375/2330 train_time:79811ms step_avg:58.04ms
step:1376/2330 train_time:79871ms step_avg:58.05ms
step:1377/2330 train_time:79928ms step_avg:58.05ms
step:1378/2330 train_time:79989ms step_avg:58.05ms
step:1379/2330 train_time:80046ms step_avg:58.05ms
step:1380/2330 train_time:80106ms step_avg:58.05ms
step:1381/2330 train_time:80163ms step_avg:58.05ms
step:1382/2330 train_time:80224ms step_avg:58.05ms
step:1383/2330 train_time:80280ms step_avg:58.05ms
step:1384/2330 train_time:80340ms step_avg:58.05ms
step:1385/2330 train_time:80397ms step_avg:58.05ms
step:1386/2330 train_time:80457ms step_avg:58.05ms
step:1387/2330 train_time:80515ms step_avg:58.05ms
step:1388/2330 train_time:80574ms step_avg:58.05ms
step:1389/2330 train_time:80631ms step_avg:58.05ms
step:1390/2330 train_time:80691ms step_avg:58.05ms
step:1391/2330 train_time:80749ms step_avg:58.05ms
step:1392/2330 train_time:80808ms step_avg:58.05ms
step:1393/2330 train_time:80866ms step_avg:58.05ms
step:1394/2330 train_time:80926ms step_avg:58.05ms
step:1395/2330 train_time:80983ms step_avg:58.05ms
step:1396/2330 train_time:81043ms step_avg:58.05ms
step:1397/2330 train_time:81100ms step_avg:58.05ms
step:1398/2330 train_time:81161ms step_avg:58.05ms
step:1399/2330 train_time:81217ms step_avg:58.05ms
step:1400/2330 train_time:81278ms step_avg:58.06ms
step:1401/2330 train_time:81334ms step_avg:58.05ms
step:1402/2330 train_time:81394ms step_avg:58.06ms
step:1403/2330 train_time:81451ms step_avg:58.05ms
step:1404/2330 train_time:81511ms step_avg:58.06ms
step:1405/2330 train_time:81569ms step_avg:58.06ms
step:1406/2330 train_time:81628ms step_avg:58.06ms
step:1407/2330 train_time:81685ms step_avg:58.06ms
step:1408/2330 train_time:81745ms step_avg:58.06ms
step:1409/2330 train_time:81802ms step_avg:58.06ms
step:1410/2330 train_time:81863ms step_avg:58.06ms
step:1411/2330 train_time:81920ms step_avg:58.06ms
step:1412/2330 train_time:81979ms step_avg:58.06ms
step:1413/2330 train_time:82037ms step_avg:58.06ms
step:1414/2330 train_time:82096ms step_avg:58.06ms
step:1415/2330 train_time:82154ms step_avg:58.06ms
step:1416/2330 train_time:82213ms step_avg:58.06ms
step:1417/2330 train_time:82270ms step_avg:58.06ms
step:1418/2330 train_time:82331ms step_avg:58.06ms
step:1419/2330 train_time:82388ms step_avg:58.06ms
step:1420/2330 train_time:82448ms step_avg:58.06ms
step:1421/2330 train_time:82506ms step_avg:58.06ms
step:1422/2330 train_time:82565ms step_avg:58.06ms
step:1423/2330 train_time:82622ms step_avg:58.06ms
step:1424/2330 train_time:82681ms step_avg:58.06ms
step:1425/2330 train_time:82738ms step_avg:58.06ms
step:1426/2330 train_time:82799ms step_avg:58.06ms
step:1427/2330 train_time:82855ms step_avg:58.06ms
step:1428/2330 train_time:82916ms step_avg:58.06ms
step:1429/2330 train_time:82973ms step_avg:58.06ms
step:1430/2330 train_time:83033ms step_avg:58.07ms
step:1431/2330 train_time:83091ms step_avg:58.07ms
step:1432/2330 train_time:83151ms step_avg:58.07ms
step:1433/2330 train_time:83208ms step_avg:58.07ms
step:1434/2330 train_time:83268ms step_avg:58.07ms
step:1435/2330 train_time:83325ms step_avg:58.07ms
step:1436/2330 train_time:83385ms step_avg:58.07ms
step:1437/2330 train_time:83441ms step_avg:58.07ms
step:1438/2330 train_time:83502ms step_avg:58.07ms
step:1439/2330 train_time:83559ms step_avg:58.07ms
step:1440/2330 train_time:83619ms step_avg:58.07ms
step:1441/2330 train_time:83675ms step_avg:58.07ms
step:1442/2330 train_time:83737ms step_avg:58.07ms
step:1443/2330 train_time:83794ms step_avg:58.07ms
step:1444/2330 train_time:83855ms step_avg:58.07ms
step:1445/2330 train_time:83912ms step_avg:58.07ms
step:1446/2330 train_time:83972ms step_avg:58.07ms
step:1447/2330 train_time:84029ms step_avg:58.07ms
step:1448/2330 train_time:84088ms step_avg:58.07ms
step:1449/2330 train_time:84146ms step_avg:58.07ms
step:1450/2330 train_time:84205ms step_avg:58.07ms
step:1451/2330 train_time:84262ms step_avg:58.07ms
step:1452/2330 train_time:84321ms step_avg:58.07ms
step:1453/2330 train_time:84378ms step_avg:58.07ms
step:1454/2330 train_time:84438ms step_avg:58.07ms
step:1455/2330 train_time:84495ms step_avg:58.07ms
step:1456/2330 train_time:84556ms step_avg:58.07ms
step:1457/2330 train_time:84613ms step_avg:58.07ms
step:1458/2330 train_time:84673ms step_avg:58.08ms
step:1459/2330 train_time:84731ms step_avg:58.08ms
step:1460/2330 train_time:84791ms step_avg:58.08ms
step:1461/2330 train_time:84848ms step_avg:58.08ms
step:1462/2330 train_time:84908ms step_avg:58.08ms
step:1463/2330 train_time:84965ms step_avg:58.08ms
step:1464/2330 train_time:85026ms step_avg:58.08ms
step:1465/2330 train_time:85082ms step_avg:58.08ms
step:1466/2330 train_time:85142ms step_avg:58.08ms
step:1467/2330 train_time:85199ms step_avg:58.08ms
step:1468/2330 train_time:85259ms step_avg:58.08ms
step:1469/2330 train_time:85315ms step_avg:58.08ms
step:1470/2330 train_time:85376ms step_avg:58.08ms
step:1471/2330 train_time:85433ms step_avg:58.08ms
step:1472/2330 train_time:85493ms step_avg:58.08ms
step:1473/2330 train_time:85551ms step_avg:58.08ms
step:1474/2330 train_time:85611ms step_avg:58.08ms
step:1475/2330 train_time:85668ms step_avg:58.08ms
step:1476/2330 train_time:85729ms step_avg:58.08ms
step:1477/2330 train_time:85785ms step_avg:58.08ms
step:1478/2330 train_time:85845ms step_avg:58.08ms
step:1479/2330 train_time:85903ms step_avg:58.08ms
step:1480/2330 train_time:85963ms step_avg:58.08ms
step:1481/2330 train_time:86020ms step_avg:58.08ms
step:1482/2330 train_time:86081ms step_avg:58.08ms
step:1483/2330 train_time:86138ms step_avg:58.08ms
step:1484/2330 train_time:86198ms step_avg:58.08ms
step:1485/2330 train_time:86256ms step_avg:58.08ms
step:1486/2330 train_time:86315ms step_avg:58.09ms
step:1487/2330 train_time:86373ms step_avg:58.09ms
step:1488/2330 train_time:86433ms step_avg:58.09ms
step:1489/2330 train_time:86490ms step_avg:58.09ms
step:1490/2330 train_time:86551ms step_avg:58.09ms
step:1491/2330 train_time:86607ms step_avg:58.09ms
step:1492/2330 train_time:86667ms step_avg:58.09ms
step:1493/2330 train_time:86725ms step_avg:58.09ms
step:1494/2330 train_time:86785ms step_avg:58.09ms
step:1495/2330 train_time:86842ms step_avg:58.09ms
step:1496/2330 train_time:86902ms step_avg:58.09ms
step:1497/2330 train_time:86959ms step_avg:58.09ms
step:1498/2330 train_time:87019ms step_avg:58.09ms
step:1499/2330 train_time:87076ms step_avg:58.09ms
step:1500/2330 train_time:87136ms step_avg:58.09ms
step:1500/2330 val_loss:3.9011 train_time:87217ms step_avg:58.14ms
step:1501/2330 train_time:87237ms step_avg:58.12ms
step:1502/2330 train_time:87258ms step_avg:58.09ms
step:1503/2330 train_time:87316ms step_avg:58.09ms
step:1504/2330 train_time:87379ms step_avg:58.10ms
step:1505/2330 train_time:87437ms step_avg:58.10ms
step:1506/2330 train_time:87500ms step_avg:58.10ms
step:1507/2330 train_time:87556ms step_avg:58.10ms
step:1508/2330 train_time:87617ms step_avg:58.10ms
step:1509/2330 train_time:87673ms step_avg:58.10ms
step:1510/2330 train_time:87733ms step_avg:58.10ms
step:1511/2330 train_time:87789ms step_avg:58.10ms
step:1512/2330 train_time:87848ms step_avg:58.10ms
step:1513/2330 train_time:87905ms step_avg:58.10ms
step:1514/2330 train_time:87965ms step_avg:58.10ms
step:1515/2330 train_time:88021ms step_avg:58.10ms
step:1516/2330 train_time:88081ms step_avg:58.10ms
step:1517/2330 train_time:88137ms step_avg:58.10ms
step:1518/2330 train_time:88197ms step_avg:58.10ms
step:1519/2330 train_time:88255ms step_avg:58.10ms
step:1520/2330 train_time:88318ms step_avg:58.10ms
step:1521/2330 train_time:88376ms step_avg:58.10ms
step:1522/2330 train_time:88439ms step_avg:58.11ms
step:1523/2330 train_time:88495ms step_avg:58.11ms
step:1524/2330 train_time:88557ms step_avg:58.11ms
step:1525/2330 train_time:88613ms step_avg:58.11ms
step:1526/2330 train_time:88675ms step_avg:58.11ms
step:1527/2330 train_time:88731ms step_avg:58.11ms
step:1528/2330 train_time:88792ms step_avg:58.11ms
step:1529/2330 train_time:88850ms step_avg:58.11ms
step:1530/2330 train_time:88908ms step_avg:58.11ms
step:1531/2330 train_time:88965ms step_avg:58.11ms
step:1532/2330 train_time:89025ms step_avg:58.11ms
step:1533/2330 train_time:89082ms step_avg:58.11ms
step:1534/2330 train_time:89143ms step_avg:58.11ms
step:1535/2330 train_time:89200ms step_avg:58.11ms
step:1536/2330 train_time:89262ms step_avg:58.11ms
step:1537/2330 train_time:89320ms step_avg:58.11ms
step:1538/2330 train_time:89384ms step_avg:58.12ms
step:1539/2330 train_time:89443ms step_avg:58.12ms
step:1540/2330 train_time:89506ms step_avg:58.12ms
step:1541/2330 train_time:89564ms step_avg:58.12ms
step:1542/2330 train_time:89624ms step_avg:58.12ms
step:1543/2330 train_time:89681ms step_avg:58.12ms
step:1544/2330 train_time:89742ms step_avg:58.12ms
step:1545/2330 train_time:89799ms step_avg:58.12ms
step:1546/2330 train_time:89860ms step_avg:58.12ms
step:1547/2330 train_time:89916ms step_avg:58.12ms
step:1548/2330 train_time:89977ms step_avg:58.12ms
step:1549/2330 train_time:90034ms step_avg:58.12ms
step:1550/2330 train_time:90095ms step_avg:58.13ms
step:1551/2330 train_time:90152ms step_avg:58.13ms
step:1552/2330 train_time:90213ms step_avg:58.13ms
step:1553/2330 train_time:90270ms step_avg:58.13ms
step:1554/2330 train_time:90331ms step_avg:58.13ms
step:1555/2330 train_time:90389ms step_avg:58.13ms
step:1556/2330 train_time:90450ms step_avg:58.13ms
step:1557/2330 train_time:90509ms step_avg:58.13ms
step:1558/2330 train_time:90569ms step_avg:58.13ms
step:1559/2330 train_time:90627ms step_avg:58.13ms
step:1560/2330 train_time:90689ms step_avg:58.13ms
step:1561/2330 train_time:90747ms step_avg:58.13ms
step:1562/2330 train_time:90807ms step_avg:58.14ms
step:1563/2330 train_time:90865ms step_avg:58.13ms
step:1564/2330 train_time:90925ms step_avg:58.14ms
step:1565/2330 train_time:90982ms step_avg:58.14ms
step:1566/2330 train_time:91043ms step_avg:58.14ms
step:1567/2330 train_time:91101ms step_avg:58.14ms
step:1568/2330 train_time:91161ms step_avg:58.14ms
step:1569/2330 train_time:91218ms step_avg:58.14ms
step:1570/2330 train_time:91279ms step_avg:58.14ms
step:1571/2330 train_time:91336ms step_avg:58.14ms
step:1572/2330 train_time:91398ms step_avg:58.14ms
step:1573/2330 train_time:91456ms step_avg:58.14ms
step:1574/2330 train_time:91516ms step_avg:58.14ms
step:1575/2330 train_time:91573ms step_avg:58.14ms
step:1576/2330 train_time:91635ms step_avg:58.14ms
step:1577/2330 train_time:91692ms step_avg:58.14ms
step:1578/2330 train_time:91755ms step_avg:58.15ms
step:1579/2330 train_time:91811ms step_avg:58.15ms
step:1580/2330 train_time:91872ms step_avg:58.15ms
step:1581/2330 train_time:91928ms step_avg:58.15ms
step:1582/2330 train_time:91990ms step_avg:58.15ms
step:1583/2330 train_time:92047ms step_avg:58.15ms
step:1584/2330 train_time:92107ms step_avg:58.15ms
step:1585/2330 train_time:92165ms step_avg:58.15ms
step:1586/2330 train_time:92225ms step_avg:58.15ms
step:1587/2330 train_time:92283ms step_avg:58.15ms
step:1588/2330 train_time:92345ms step_avg:58.15ms
step:1589/2330 train_time:92403ms step_avg:58.15ms
step:1590/2330 train_time:92465ms step_avg:58.15ms
step:1591/2330 train_time:92523ms step_avg:58.15ms
step:1592/2330 train_time:92585ms step_avg:58.16ms
step:1593/2330 train_time:92643ms step_avg:58.16ms
step:1594/2330 train_time:92704ms step_avg:58.16ms
step:1595/2330 train_time:92762ms step_avg:58.16ms
step:1596/2330 train_time:92823ms step_avg:58.16ms
step:1597/2330 train_time:92879ms step_avg:58.16ms
step:1598/2330 train_time:92940ms step_avg:58.16ms
step:1599/2330 train_time:92996ms step_avg:58.16ms
step:1600/2330 train_time:93058ms step_avg:58.16ms
step:1601/2330 train_time:93115ms step_avg:58.16ms
step:1602/2330 train_time:93175ms step_avg:58.16ms
step:1603/2330 train_time:93233ms step_avg:58.16ms
step:1604/2330 train_time:93293ms step_avg:58.16ms
step:1605/2330 train_time:93350ms step_avg:58.16ms
step:1606/2330 train_time:93410ms step_avg:58.16ms
step:1607/2330 train_time:93468ms step_avg:58.16ms
step:1608/2330 train_time:93530ms step_avg:58.17ms
step:1609/2330 train_time:93588ms step_avg:58.17ms
step:1610/2330 train_time:93648ms step_avg:58.17ms
step:1611/2330 train_time:93705ms step_avg:58.17ms
step:1612/2330 train_time:93766ms step_avg:58.17ms
step:1613/2330 train_time:93824ms step_avg:58.17ms
step:1614/2330 train_time:93886ms step_avg:58.17ms
step:1615/2330 train_time:93943ms step_avg:58.17ms
step:1616/2330 train_time:94004ms step_avg:58.17ms
step:1617/2330 train_time:94062ms step_avg:58.17ms
step:1618/2330 train_time:94123ms step_avg:58.17ms
step:1619/2330 train_time:94181ms step_avg:58.17ms
step:1620/2330 train_time:94241ms step_avg:58.17ms
step:1621/2330 train_time:94298ms step_avg:58.17ms
step:1622/2330 train_time:94358ms step_avg:58.17ms
step:1623/2330 train_time:94415ms step_avg:58.17ms
step:1624/2330 train_time:94478ms step_avg:58.18ms
step:1625/2330 train_time:94534ms step_avg:58.17ms
step:1626/2330 train_time:94597ms step_avg:58.18ms
step:1627/2330 train_time:94653ms step_avg:58.18ms
step:1628/2330 train_time:94715ms step_avg:58.18ms
step:1629/2330 train_time:94772ms step_avg:58.18ms
step:1630/2330 train_time:94834ms step_avg:58.18ms
step:1631/2330 train_time:94891ms step_avg:58.18ms
step:1632/2330 train_time:94952ms step_avg:58.18ms
step:1633/2330 train_time:95008ms step_avg:58.18ms
step:1634/2330 train_time:95069ms step_avg:58.18ms
step:1635/2330 train_time:95126ms step_avg:58.18ms
step:1636/2330 train_time:95188ms step_avg:58.18ms
step:1637/2330 train_time:95246ms step_avg:58.18ms
step:1638/2330 train_time:95306ms step_avg:58.18ms
step:1639/2330 train_time:95364ms step_avg:58.18ms
step:1640/2330 train_time:95425ms step_avg:58.19ms
step:1641/2330 train_time:95482ms step_avg:58.19ms
step:1642/2330 train_time:95545ms step_avg:58.19ms
step:1643/2330 train_time:95603ms step_avg:58.19ms
step:1644/2330 train_time:95665ms step_avg:58.19ms
step:1645/2330 train_time:95722ms step_avg:58.19ms
step:1646/2330 train_time:95783ms step_avg:58.19ms
step:1647/2330 train_time:95841ms step_avg:58.19ms
step:1648/2330 train_time:95902ms step_avg:58.19ms
step:1649/2330 train_time:95959ms step_avg:58.19ms
step:1650/2330 train_time:96019ms step_avg:58.19ms
step:1651/2330 train_time:96077ms step_avg:58.19ms
step:1652/2330 train_time:96138ms step_avg:58.20ms
step:1653/2330 train_time:96196ms step_avg:58.19ms
step:1654/2330 train_time:96257ms step_avg:58.20ms
step:1655/2330 train_time:96313ms step_avg:58.20ms
step:1656/2330 train_time:96375ms step_avg:58.20ms
step:1657/2330 train_time:96432ms step_avg:58.20ms
step:1658/2330 train_time:96494ms step_avg:58.20ms
step:1659/2330 train_time:96551ms step_avg:58.20ms
step:1660/2330 train_time:96612ms step_avg:58.20ms
step:1661/2330 train_time:96668ms step_avg:58.20ms
step:1662/2330 train_time:96730ms step_avg:58.20ms
step:1663/2330 train_time:96787ms step_avg:58.20ms
step:1664/2330 train_time:96848ms step_avg:58.20ms
step:1665/2330 train_time:96906ms step_avg:58.20ms
step:1666/2330 train_time:96968ms step_avg:58.20ms
step:1667/2330 train_time:97025ms step_avg:58.20ms
step:1668/2330 train_time:97086ms step_avg:58.21ms
step:1669/2330 train_time:97144ms step_avg:58.21ms
step:1670/2330 train_time:97204ms step_avg:58.21ms
step:1671/2330 train_time:97263ms step_avg:58.21ms
step:1672/2330 train_time:97324ms step_avg:58.21ms
step:1673/2330 train_time:97382ms step_avg:58.21ms
step:1674/2330 train_time:97442ms step_avg:58.21ms
step:1675/2330 train_time:97501ms step_avg:58.21ms
step:1676/2330 train_time:97561ms step_avg:58.21ms
step:1677/2330 train_time:97618ms step_avg:58.21ms
step:1678/2330 train_time:97680ms step_avg:58.21ms
step:1679/2330 train_time:97736ms step_avg:58.21ms
step:1680/2330 train_time:97799ms step_avg:58.21ms
step:1681/2330 train_time:97855ms step_avg:58.21ms
step:1682/2330 train_time:97917ms step_avg:58.21ms
step:1683/2330 train_time:97974ms step_avg:58.21ms
step:1684/2330 train_time:98035ms step_avg:58.22ms
step:1685/2330 train_time:98092ms step_avg:58.22ms
step:1686/2330 train_time:98152ms step_avg:58.22ms
step:1687/2330 train_time:98209ms step_avg:58.21ms
step:1688/2330 train_time:98270ms step_avg:58.22ms
step:1689/2330 train_time:98327ms step_avg:58.22ms
step:1690/2330 train_time:98388ms step_avg:58.22ms
step:1691/2330 train_time:98446ms step_avg:58.22ms
step:1692/2330 train_time:98506ms step_avg:58.22ms
step:1693/2330 train_time:98565ms step_avg:58.22ms
step:1694/2330 train_time:98626ms step_avg:58.22ms
step:1695/2330 train_time:98685ms step_avg:58.22ms
step:1696/2330 train_time:98746ms step_avg:58.22ms
step:1697/2330 train_time:98804ms step_avg:58.22ms
step:1698/2330 train_time:98864ms step_avg:58.22ms
step:1699/2330 train_time:98922ms step_avg:58.22ms
step:1700/2330 train_time:98982ms step_avg:58.22ms
step:1701/2330 train_time:99039ms step_avg:58.22ms
step:1702/2330 train_time:99100ms step_avg:58.23ms
step:1703/2330 train_time:99157ms step_avg:58.22ms
step:1704/2330 train_time:99218ms step_avg:58.23ms
step:1705/2330 train_time:99275ms step_avg:58.23ms
step:1706/2330 train_time:99337ms step_avg:58.23ms
step:1707/2330 train_time:99394ms step_avg:58.23ms
step:1708/2330 train_time:99455ms step_avg:58.23ms
step:1709/2330 train_time:99512ms step_avg:58.23ms
step:1710/2330 train_time:99574ms step_avg:58.23ms
step:1711/2330 train_time:99631ms step_avg:58.23ms
step:1712/2330 train_time:99692ms step_avg:58.23ms
step:1713/2330 train_time:99749ms step_avg:58.23ms
step:1714/2330 train_time:99810ms step_avg:58.23ms
step:1715/2330 train_time:99867ms step_avg:58.23ms
step:1716/2330 train_time:99928ms step_avg:58.23ms
step:1717/2330 train_time:99985ms step_avg:58.23ms
step:1718/2330 train_time:100045ms step_avg:58.23ms
step:1719/2330 train_time:100103ms step_avg:58.23ms
step:1720/2330 train_time:100164ms step_avg:58.23ms
step:1721/2330 train_time:100221ms step_avg:58.23ms
step:1722/2330 train_time:100283ms step_avg:58.24ms
step:1723/2330 train_time:100342ms step_avg:58.24ms
step:1724/2330 train_time:100402ms step_avg:58.24ms
step:1725/2330 train_time:100459ms step_avg:58.24ms
step:1726/2330 train_time:100520ms step_avg:58.24ms
step:1727/2330 train_time:100577ms step_avg:58.24ms
step:1728/2330 train_time:100638ms step_avg:58.24ms
step:1729/2330 train_time:100695ms step_avg:58.24ms
step:1730/2330 train_time:100757ms step_avg:58.24ms
step:1731/2330 train_time:100813ms step_avg:58.24ms
step:1732/2330 train_time:100875ms step_avg:58.24ms
step:1733/2330 train_time:100932ms step_avg:58.24ms
step:1734/2330 train_time:100995ms step_avg:58.24ms
step:1735/2330 train_time:101052ms step_avg:58.24ms
step:1736/2330 train_time:101113ms step_avg:58.24ms
step:1737/2330 train_time:101170ms step_avg:58.24ms
step:1738/2330 train_time:101231ms step_avg:58.25ms
step:1739/2330 train_time:101288ms step_avg:58.25ms
step:1740/2330 train_time:101348ms step_avg:58.25ms
step:1741/2330 train_time:101405ms step_avg:58.25ms
step:1742/2330 train_time:101467ms step_avg:58.25ms
step:1743/2330 train_time:101525ms step_avg:58.25ms
step:1744/2330 train_time:101586ms step_avg:58.25ms
step:1745/2330 train_time:101645ms step_avg:58.25ms
step:1746/2330 train_time:101704ms step_avg:58.25ms
step:1747/2330 train_time:101762ms step_avg:58.25ms
step:1748/2330 train_time:101824ms step_avg:58.25ms
step:1749/2330 train_time:101882ms step_avg:58.25ms
step:1750/2330 train_time:101943ms step_avg:58.25ms
step:1750/2330 val_loss:3.8174 train_time:102024ms step_avg:58.30ms
step:1751/2330 train_time:102044ms step_avg:58.28ms
step:1752/2330 train_time:102064ms step_avg:58.26ms
step:1753/2330 train_time:102118ms step_avg:58.25ms
step:1754/2330 train_time:102188ms step_avg:58.26ms
step:1755/2330 train_time:102244ms step_avg:58.26ms
step:1756/2330 train_time:102307ms step_avg:58.26ms
step:1757/2330 train_time:102364ms step_avg:58.26ms
step:1758/2330 train_time:102425ms step_avg:58.26ms
step:1759/2330 train_time:102481ms step_avg:58.26ms
step:1760/2330 train_time:102543ms step_avg:58.26ms
step:1761/2330 train_time:102600ms step_avg:58.26ms
step:1762/2330 train_time:102659ms step_avg:58.26ms
step:1763/2330 train_time:102716ms step_avg:58.26ms
step:1764/2330 train_time:102776ms step_avg:58.26ms
step:1765/2330 train_time:102832ms step_avg:58.26ms
step:1766/2330 train_time:102891ms step_avg:58.26ms
step:1767/2330 train_time:102951ms step_avg:58.26ms
step:1768/2330 train_time:103014ms step_avg:58.27ms
step:1769/2330 train_time:103073ms step_avg:58.27ms
step:1770/2330 train_time:103135ms step_avg:58.27ms
step:1771/2330 train_time:103194ms step_avg:58.27ms
step:1772/2330 train_time:103254ms step_avg:58.27ms
step:1773/2330 train_time:103314ms step_avg:58.27ms
step:1774/2330 train_time:103374ms step_avg:58.27ms
step:1775/2330 train_time:103431ms step_avg:58.27ms
step:1776/2330 train_time:103492ms step_avg:58.27ms
step:1777/2330 train_time:103549ms step_avg:58.27ms
step:1778/2330 train_time:103610ms step_avg:58.27ms
step:1779/2330 train_time:103667ms step_avg:58.27ms
step:1780/2330 train_time:103727ms step_avg:58.27ms
step:1781/2330 train_time:103783ms step_avg:58.27ms
step:1782/2330 train_time:103843ms step_avg:58.27ms
step:1783/2330 train_time:103901ms step_avg:58.27ms
step:1784/2330 train_time:103962ms step_avg:58.27ms
step:1785/2330 train_time:104020ms step_avg:58.27ms
step:1786/2330 train_time:104082ms step_avg:58.28ms
step:1787/2330 train_time:104140ms step_avg:58.28ms
step:1788/2330 train_time:104202ms step_avg:58.28ms
step:1789/2330 train_time:104260ms step_avg:58.28ms
step:1790/2330 train_time:104321ms step_avg:58.28ms
step:1791/2330 train_time:104379ms step_avg:58.28ms
step:1792/2330 train_time:104440ms step_avg:58.28ms
step:1793/2330 train_time:104497ms step_avg:58.28ms
step:1794/2330 train_time:104558ms step_avg:58.28ms
step:1795/2330 train_time:104615ms step_avg:58.28ms
step:1796/2330 train_time:104675ms step_avg:58.28ms
step:1797/2330 train_time:104733ms step_avg:58.28ms
step:1798/2330 train_time:104793ms step_avg:58.28ms
step:1799/2330 train_time:104851ms step_avg:58.28ms
step:1800/2330 train_time:104910ms step_avg:58.28ms
step:1801/2330 train_time:104969ms step_avg:58.28ms
step:1802/2330 train_time:105029ms step_avg:58.28ms
step:1803/2330 train_time:105087ms step_avg:58.28ms
step:1804/2330 train_time:105149ms step_avg:58.29ms
step:1805/2330 train_time:105206ms step_avg:58.29ms
step:1806/2330 train_time:105269ms step_avg:58.29ms
step:1807/2330 train_time:105326ms step_avg:58.29ms
step:1808/2330 train_time:105387ms step_avg:58.29ms
step:1809/2330 train_time:105444ms step_avg:58.29ms
step:1810/2330 train_time:105505ms step_avg:58.29ms
step:1811/2330 train_time:105562ms step_avg:58.29ms
step:1812/2330 train_time:105622ms step_avg:58.29ms
step:1813/2330 train_time:105679ms step_avg:58.29ms
step:1814/2330 train_time:105740ms step_avg:58.29ms
step:1815/2330 train_time:105798ms step_avg:58.29ms
step:1816/2330 train_time:105859ms step_avg:58.29ms
step:1817/2330 train_time:105918ms step_avg:58.29ms
step:1818/2330 train_time:105978ms step_avg:58.29ms
step:1819/2330 train_time:106037ms step_avg:58.29ms
step:1820/2330 train_time:106098ms step_avg:58.30ms
step:1821/2330 train_time:106157ms step_avg:58.30ms
step:1822/2330 train_time:106218ms step_avg:58.30ms
step:1823/2330 train_time:106276ms step_avg:58.30ms
step:1824/2330 train_time:106337ms step_avg:58.30ms
step:1825/2330 train_time:106395ms step_avg:58.30ms
step:1826/2330 train_time:106455ms step_avg:58.30ms
step:1827/2330 train_time:106513ms step_avg:58.30ms
step:1828/2330 train_time:106574ms step_avg:58.30ms
step:1829/2330 train_time:106631ms step_avg:58.30ms
step:1830/2330 train_time:106691ms step_avg:58.30ms
step:1831/2330 train_time:106748ms step_avg:58.30ms
step:1832/2330 train_time:106809ms step_avg:58.30ms
step:1833/2330 train_time:106867ms step_avg:58.30ms
step:1834/2330 train_time:106927ms step_avg:58.30ms
step:1835/2330 train_time:106984ms step_avg:58.30ms
step:1836/2330 train_time:107046ms step_avg:58.30ms
step:1837/2330 train_time:107104ms step_avg:58.30ms
step:1838/2330 train_time:107164ms step_avg:58.30ms
step:1839/2330 train_time:107222ms step_avg:58.30ms
step:1840/2330 train_time:107283ms step_avg:58.31ms
step:1841/2330 train_time:107340ms step_avg:58.31ms
step:1842/2330 train_time:107401ms step_avg:58.31ms
step:1843/2330 train_time:107459ms step_avg:58.31ms
step:1844/2330 train_time:107518ms step_avg:58.31ms
step:1845/2330 train_time:107576ms step_avg:58.31ms
step:1846/2330 train_time:107637ms step_avg:58.31ms
step:1847/2330 train_time:107695ms step_avg:58.31ms
step:1848/2330 train_time:107755ms step_avg:58.31ms
step:1849/2330 train_time:107813ms step_avg:58.31ms
step:1850/2330 train_time:107874ms step_avg:58.31ms
step:1851/2330 train_time:107933ms step_avg:58.31ms
step:1852/2330 train_time:107993ms step_avg:58.31ms
step:1853/2330 train_time:108051ms step_avg:58.31ms
step:1854/2330 train_time:108111ms step_avg:58.31ms
step:1855/2330 train_time:108168ms step_avg:58.31ms
step:1856/2330 train_time:108230ms step_avg:58.31ms
step:1857/2330 train_time:108287ms step_avg:58.31ms
step:1858/2330 train_time:108350ms step_avg:58.32ms
step:1859/2330 train_time:108406ms step_avg:58.31ms
step:1860/2330 train_time:108469ms step_avg:58.32ms
step:1861/2330 train_time:108526ms step_avg:58.32ms
step:1862/2330 train_time:108587ms step_avg:58.32ms
step:1863/2330 train_time:108643ms step_avg:58.32ms
step:1864/2330 train_time:108705ms step_avg:58.32ms
step:1865/2330 train_time:108762ms step_avg:58.32ms
step:1866/2330 train_time:108822ms step_avg:58.32ms
step:1867/2330 train_time:108879ms step_avg:58.32ms
step:1868/2330 train_time:108941ms step_avg:58.32ms
step:1869/2330 train_time:108998ms step_avg:58.32ms
step:1870/2330 train_time:109059ms step_avg:58.32ms
step:1871/2330 train_time:109119ms step_avg:58.32ms
step:1872/2330 train_time:109179ms step_avg:58.32ms
step:1873/2330 train_time:109237ms step_avg:58.32ms
step:1874/2330 train_time:109298ms step_avg:58.32ms
step:1875/2330 train_time:109357ms step_avg:58.32ms
step:1876/2330 train_time:109417ms step_avg:58.32ms
step:1877/2330 train_time:109475ms step_avg:58.32ms
step:1878/2330 train_time:109536ms step_avg:58.33ms
step:1879/2330 train_time:109594ms step_avg:58.33ms
step:1880/2330 train_time:109653ms step_avg:58.33ms
step:1881/2330 train_time:109711ms step_avg:58.33ms
step:1882/2330 train_time:109771ms step_avg:58.33ms
step:1883/2330 train_time:109828ms step_avg:58.33ms
step:1884/2330 train_time:109889ms step_avg:58.33ms
step:1885/2330 train_time:109946ms step_avg:58.33ms
step:1886/2330 train_time:110007ms step_avg:58.33ms
step:1887/2330 train_time:110064ms step_avg:58.33ms
step:1888/2330 train_time:110127ms step_avg:58.33ms
step:1889/2330 train_time:110184ms step_avg:58.33ms
step:1890/2330 train_time:110245ms step_avg:58.33ms
step:1891/2330 train_time:110303ms step_avg:58.33ms
step:1892/2330 train_time:110365ms step_avg:58.33ms
step:1893/2330 train_time:110422ms step_avg:58.33ms
step:1894/2330 train_time:110483ms step_avg:58.33ms
step:1895/2330 train_time:110540ms step_avg:58.33ms
step:1896/2330 train_time:110601ms step_avg:58.33ms
step:1897/2330 train_time:110659ms step_avg:58.33ms
step:1898/2330 train_time:110719ms step_avg:58.33ms
step:1899/2330 train_time:110777ms step_avg:58.33ms
step:1900/2330 train_time:110837ms step_avg:58.34ms
step:1901/2330 train_time:110895ms step_avg:58.34ms
step:1902/2330 train_time:110956ms step_avg:58.34ms
step:1903/2330 train_time:111013ms step_avg:58.34ms
step:1904/2330 train_time:111074ms step_avg:58.34ms
step:1905/2330 train_time:111133ms step_avg:58.34ms
step:1906/2330 train_time:111193ms step_avg:58.34ms
step:1907/2330 train_time:111250ms step_avg:58.34ms
step:1908/2330 train_time:111312ms step_avg:58.34ms
step:1909/2330 train_time:111369ms step_avg:58.34ms
step:1910/2330 train_time:111431ms step_avg:58.34ms
step:1911/2330 train_time:111488ms step_avg:58.34ms
step:1912/2330 train_time:111549ms step_avg:58.34ms
step:1913/2330 train_time:111606ms step_avg:58.34ms
step:1914/2330 train_time:111669ms step_avg:58.34ms
step:1915/2330 train_time:111725ms step_avg:58.34ms
step:1916/2330 train_time:111787ms step_avg:58.34ms
step:1917/2330 train_time:111844ms step_avg:58.34ms
step:1918/2330 train_time:111905ms step_avg:58.34ms
step:1919/2330 train_time:111962ms step_avg:58.34ms
step:1920/2330 train_time:112023ms step_avg:58.35ms
step:1921/2330 train_time:112080ms step_avg:58.34ms
step:1922/2330 train_time:112141ms step_avg:58.35ms
step:1923/2330 train_time:112199ms step_avg:58.35ms
step:1924/2330 train_time:112260ms step_avg:58.35ms
step:1925/2330 train_time:112318ms step_avg:58.35ms
step:1926/2330 train_time:112379ms step_avg:58.35ms
step:1927/2330 train_time:112438ms step_avg:58.35ms
step:1928/2330 train_time:112500ms step_avg:58.35ms
step:1929/2330 train_time:112558ms step_avg:58.35ms
step:1930/2330 train_time:112618ms step_avg:58.35ms
step:1931/2330 train_time:112677ms step_avg:58.35ms
step:1932/2330 train_time:112737ms step_avg:58.35ms
step:1933/2330 train_time:112796ms step_avg:58.35ms
step:1934/2330 train_time:112856ms step_avg:58.35ms
step:1935/2330 train_time:112915ms step_avg:58.35ms
step:1936/2330 train_time:112975ms step_avg:58.35ms
step:1937/2330 train_time:113032ms step_avg:58.35ms
step:1938/2330 train_time:113092ms step_avg:58.36ms
step:1939/2330 train_time:113149ms step_avg:58.35ms
step:1940/2330 train_time:113212ms step_avg:58.36ms
step:1941/2330 train_time:113269ms step_avg:58.36ms
step:1942/2330 train_time:113329ms step_avg:58.36ms
step:1943/2330 train_time:113386ms step_avg:58.36ms
step:1944/2330 train_time:113448ms step_avg:58.36ms
step:1945/2330 train_time:113505ms step_avg:58.36ms
step:1946/2330 train_time:113566ms step_avg:58.36ms
step:1947/2330 train_time:113623ms step_avg:58.36ms
step:1948/2330 train_time:113685ms step_avg:58.36ms
step:1949/2330 train_time:113742ms step_avg:58.36ms
step:1950/2330 train_time:113803ms step_avg:58.36ms
step:1951/2330 train_time:113861ms step_avg:58.36ms
step:1952/2330 train_time:113921ms step_avg:58.36ms
step:1953/2330 train_time:113978ms step_avg:58.36ms
step:1954/2330 train_time:114039ms step_avg:58.36ms
step:1955/2330 train_time:114097ms step_avg:58.36ms
step:1956/2330 train_time:114159ms step_avg:58.36ms
step:1957/2330 train_time:114218ms step_avg:58.36ms
step:1958/2330 train_time:114278ms step_avg:58.36ms
step:1959/2330 train_time:114335ms step_avg:58.36ms
step:1960/2330 train_time:114397ms step_avg:58.37ms
step:1961/2330 train_time:114455ms step_avg:58.37ms
step:1962/2330 train_time:114515ms step_avg:58.37ms
step:1963/2330 train_time:114573ms step_avg:58.37ms
step:1964/2330 train_time:114635ms step_avg:58.37ms
step:1965/2330 train_time:114691ms step_avg:58.37ms
step:1966/2330 train_time:114752ms step_avg:58.37ms
step:1967/2330 train_time:114810ms step_avg:58.37ms
step:1968/2330 train_time:114872ms step_avg:58.37ms
step:1969/2330 train_time:114929ms step_avg:58.37ms
step:1970/2330 train_time:114990ms step_avg:58.37ms
step:1971/2330 train_time:115048ms step_avg:58.37ms
step:1972/2330 train_time:115109ms step_avg:58.37ms
step:1973/2330 train_time:115166ms step_avg:58.37ms
step:1974/2330 train_time:115228ms step_avg:58.37ms
step:1975/2330 train_time:115284ms step_avg:58.37ms
step:1976/2330 train_time:115347ms step_avg:58.37ms
step:1977/2330 train_time:115404ms step_avg:58.37ms
step:1978/2330 train_time:115465ms step_avg:58.37ms
step:1979/2330 train_time:115522ms step_avg:58.37ms
step:1980/2330 train_time:115583ms step_avg:58.38ms
step:1981/2330 train_time:115639ms step_avg:58.37ms
step:1982/2330 train_time:115700ms step_avg:58.38ms
step:1983/2330 train_time:115759ms step_avg:58.38ms
step:1984/2330 train_time:115819ms step_avg:58.38ms
step:1985/2330 train_time:115878ms step_avg:58.38ms
step:1986/2330 train_time:115938ms step_avg:58.38ms
step:1987/2330 train_time:115996ms step_avg:58.38ms
step:1988/2330 train_time:116056ms step_avg:58.38ms
step:1989/2330 train_time:116114ms step_avg:58.38ms
step:1990/2330 train_time:116174ms step_avg:58.38ms
step:1991/2330 train_time:116233ms step_avg:58.38ms
step:1992/2330 train_time:116294ms step_avg:58.38ms
step:1993/2330 train_time:116351ms step_avg:58.38ms
step:1994/2330 train_time:116412ms step_avg:58.38ms
step:1995/2330 train_time:116469ms step_avg:58.38ms
step:1996/2330 train_time:116531ms step_avg:58.38ms
step:1997/2330 train_time:116587ms step_avg:58.38ms
step:1998/2330 train_time:116648ms step_avg:58.38ms
step:1999/2330 train_time:116705ms step_avg:58.38ms
step:2000/2330 train_time:116768ms step_avg:58.38ms
step:2000/2330 val_loss:3.7559 train_time:116850ms step_avg:58.43ms
step:2001/2330 train_time:116869ms step_avg:58.41ms
step:2002/2330 train_time:116890ms step_avg:58.39ms
step:2003/2330 train_time:116951ms step_avg:58.39ms
step:2004/2330 train_time:117018ms step_avg:58.39ms
step:2005/2330 train_time:117076ms step_avg:58.39ms
step:2006/2330 train_time:117138ms step_avg:58.39ms
step:2007/2330 train_time:117195ms step_avg:58.39ms
step:2008/2330 train_time:117254ms step_avg:58.39ms
step:2009/2330 train_time:117311ms step_avg:58.39ms
step:2010/2330 train_time:117372ms step_avg:58.39ms
step:2011/2330 train_time:117429ms step_avg:58.39ms
step:2012/2330 train_time:117489ms step_avg:58.39ms
step:2013/2330 train_time:117546ms step_avg:58.39ms
step:2014/2330 train_time:117605ms step_avg:58.39ms
step:2015/2330 train_time:117662ms step_avg:58.39ms
step:2016/2330 train_time:117722ms step_avg:58.39ms
step:2017/2330 train_time:117779ms step_avg:58.39ms
step:2018/2330 train_time:117840ms step_avg:58.39ms
step:2019/2330 train_time:117898ms step_avg:58.39ms
step:2020/2330 train_time:117961ms step_avg:58.40ms
step:2021/2330 train_time:118019ms step_avg:58.40ms
step:2022/2330 train_time:118084ms step_avg:58.40ms
step:2023/2330 train_time:118141ms step_avg:58.40ms
step:2024/2330 train_time:118202ms step_avg:58.40ms
step:2025/2330 train_time:118259ms step_avg:58.40ms
step:2026/2330 train_time:118321ms step_avg:58.40ms
step:2027/2330 train_time:118377ms step_avg:58.40ms
step:2028/2330 train_time:118438ms step_avg:58.40ms
step:2029/2330 train_time:118494ms step_avg:58.40ms
step:2030/2330 train_time:118554ms step_avg:58.40ms
step:2031/2330 train_time:118610ms step_avg:58.40ms
step:2032/2330 train_time:118671ms step_avg:58.40ms
step:2033/2330 train_time:118728ms step_avg:58.40ms
step:2034/2330 train_time:118789ms step_avg:58.40ms
step:2035/2330 train_time:118848ms step_avg:58.40ms
step:2036/2330 train_time:118910ms step_avg:58.40ms
step:2037/2330 train_time:118969ms step_avg:58.40ms
step:2038/2330 train_time:119030ms step_avg:58.41ms
step:2039/2330 train_time:119089ms step_avg:58.41ms
step:2040/2330 train_time:119150ms step_avg:58.41ms
step:2041/2330 train_time:119208ms step_avg:58.41ms
step:2042/2330 train_time:119269ms step_avg:58.41ms
step:2043/2330 train_time:119327ms step_avg:58.41ms
step:2044/2330 train_time:119387ms step_avg:58.41ms
step:2045/2330 train_time:119445ms step_avg:58.41ms
step:2046/2330 train_time:119505ms step_avg:58.41ms
step:2047/2330 train_time:119562ms step_avg:58.41ms
step:2048/2330 train_time:119622ms step_avg:58.41ms
step:2049/2330 train_time:119679ms step_avg:58.41ms
step:2050/2330 train_time:119740ms step_avg:58.41ms
step:2051/2330 train_time:119797ms step_avg:58.41ms
step:2052/2330 train_time:119860ms step_avg:58.41ms
step:2053/2330 train_time:119917ms step_avg:58.41ms
step:2054/2330 train_time:119978ms step_avg:58.41ms
step:2055/2330 train_time:120035ms step_avg:58.41ms
step:2056/2330 train_time:120097ms step_avg:58.41ms
step:2057/2330 train_time:120155ms step_avg:58.41ms
step:2058/2330 train_time:120216ms step_avg:58.41ms
step:2059/2330 train_time:120274ms step_avg:58.41ms
step:2060/2330 train_time:120334ms step_avg:58.41ms
step:2061/2330 train_time:120391ms step_avg:58.41ms
step:2062/2330 train_time:120452ms step_avg:58.42ms
step:2063/2330 train_time:120509ms step_avg:58.41ms
step:2064/2330 train_time:120570ms step_avg:58.42ms
step:2065/2330 train_time:120628ms step_avg:58.42ms
step:2066/2330 train_time:120688ms step_avg:58.42ms
step:2067/2330 train_time:120746ms step_avg:58.42ms
step:2068/2330 train_time:120807ms step_avg:58.42ms
step:2069/2330 train_time:120865ms step_avg:58.42ms
step:2070/2330 train_time:120926ms step_avg:58.42ms
step:2071/2330 train_time:120985ms step_avg:58.42ms
step:2072/2330 train_time:121045ms step_avg:58.42ms
step:2073/2330 train_time:121103ms step_avg:58.42ms
step:2074/2330 train_time:121165ms step_avg:58.42ms
step:2075/2330 train_time:121223ms step_avg:58.42ms
step:2076/2330 train_time:121285ms step_avg:58.42ms
step:2077/2330 train_time:121343ms step_avg:58.42ms
step:2078/2330 train_time:121403ms step_avg:58.42ms
step:2079/2330 train_time:121460ms step_avg:58.42ms
step:2080/2330 train_time:121521ms step_avg:58.42ms
step:2081/2330 train_time:121579ms step_avg:58.42ms
step:2082/2330 train_time:121639ms step_avg:58.42ms
step:2083/2330 train_time:121696ms step_avg:58.42ms
step:2084/2330 train_time:121756ms step_avg:58.42ms
step:2085/2330 train_time:121813ms step_avg:58.42ms
step:2086/2330 train_time:121874ms step_avg:58.42ms
step:2087/2330 train_time:121932ms step_avg:58.42ms
step:2088/2330 train_time:121993ms step_avg:58.43ms
step:2089/2330 train_time:122050ms step_avg:58.43ms
step:2090/2330 train_time:122112ms step_avg:58.43ms
step:2091/2330 train_time:122170ms step_avg:58.43ms
step:2092/2330 train_time:122231ms step_avg:58.43ms
step:2093/2330 train_time:122289ms step_avg:58.43ms
step:2094/2330 train_time:122351ms step_avg:58.43ms
step:2095/2330 train_time:122408ms step_avg:58.43ms
step:2096/2330 train_time:122469ms step_avg:58.43ms
step:2097/2330 train_time:122528ms step_avg:58.43ms
step:2098/2330 train_time:122587ms step_avg:58.43ms
step:2099/2330 train_time:122645ms step_avg:58.43ms
step:2100/2330 train_time:122706ms step_avg:58.43ms
step:2101/2330 train_time:122763ms step_avg:58.43ms
step:2102/2330 train_time:122825ms step_avg:58.43ms
step:2103/2330 train_time:122884ms step_avg:58.43ms
step:2104/2330 train_time:122944ms step_avg:58.43ms
step:2105/2330 train_time:123001ms step_avg:58.43ms
step:2106/2330 train_time:123062ms step_avg:58.43ms
step:2107/2330 train_time:123120ms step_avg:58.43ms
step:2108/2330 train_time:123181ms step_avg:58.44ms
step:2109/2330 train_time:123239ms step_avg:58.43ms
step:2110/2330 train_time:123301ms step_avg:58.44ms
step:2111/2330 train_time:123358ms step_avg:58.44ms
step:2112/2330 train_time:123419ms step_avg:58.44ms
step:2113/2330 train_time:123477ms step_avg:58.44ms
step:2114/2330 train_time:123537ms step_avg:58.44ms
step:2115/2330 train_time:123593ms step_avg:58.44ms
step:2116/2330 train_time:123654ms step_avg:58.44ms
step:2117/2330 train_time:123711ms step_avg:58.44ms
step:2118/2330 train_time:123772ms step_avg:58.44ms
step:2119/2330 train_time:123829ms step_avg:58.44ms
step:2120/2330 train_time:123890ms step_avg:58.44ms
step:2121/2330 train_time:123948ms step_avg:58.44ms
step:2122/2330 train_time:124008ms step_avg:58.44ms
step:2123/2330 train_time:124067ms step_avg:58.44ms
step:2124/2330 train_time:124128ms step_avg:58.44ms
step:2125/2330 train_time:124186ms step_avg:58.44ms
step:2126/2330 train_time:124248ms step_avg:58.44ms
step:2127/2330 train_time:124306ms step_avg:58.44ms
step:2128/2330 train_time:124367ms step_avg:58.44ms
step:2129/2330 train_time:124424ms step_avg:58.44ms
step:2130/2330 train_time:124485ms step_avg:58.44ms
step:2131/2330 train_time:124542ms step_avg:58.44ms
step:2132/2330 train_time:124604ms step_avg:58.44ms
step:2133/2330 train_time:124662ms step_avg:58.44ms
step:2134/2330 train_time:124722ms step_avg:58.45ms
step:2135/2330 train_time:124779ms step_avg:58.44ms
step:2136/2330 train_time:124840ms step_avg:58.45ms
step:2137/2330 train_time:124897ms step_avg:58.44ms
step:2138/2330 train_time:124958ms step_avg:58.45ms
step:2139/2330 train_time:125016ms step_avg:58.45ms
step:2140/2330 train_time:125077ms step_avg:58.45ms
step:2141/2330 train_time:125133ms step_avg:58.45ms
step:2142/2330 train_time:125195ms step_avg:58.45ms
step:2143/2330 train_time:125253ms step_avg:58.45ms
step:2144/2330 train_time:125313ms step_avg:58.45ms
step:2145/2330 train_time:125372ms step_avg:58.45ms
step:2146/2330 train_time:125432ms step_avg:58.45ms
step:2147/2330 train_time:125489ms step_avg:58.45ms
step:2148/2330 train_time:125550ms step_avg:58.45ms
step:2149/2330 train_time:125608ms step_avg:58.45ms
step:2150/2330 train_time:125669ms step_avg:58.45ms
step:2151/2330 train_time:125727ms step_avg:58.45ms
step:2152/2330 train_time:125788ms step_avg:58.45ms
step:2153/2330 train_time:125846ms step_avg:58.45ms
step:2154/2330 train_time:125907ms step_avg:58.45ms
step:2155/2330 train_time:125965ms step_avg:58.45ms
step:2156/2330 train_time:126026ms step_avg:58.45ms
step:2157/2330 train_time:126084ms step_avg:58.45ms
step:2158/2330 train_time:126145ms step_avg:58.45ms
step:2159/2330 train_time:126202ms step_avg:58.45ms
step:2160/2330 train_time:126264ms step_avg:58.46ms
step:2161/2330 train_time:126321ms step_avg:58.46ms
step:2162/2330 train_time:126382ms step_avg:58.46ms
step:2163/2330 train_time:126440ms step_avg:58.46ms
step:2164/2330 train_time:126500ms step_avg:58.46ms
step:2165/2330 train_time:126557ms step_avg:58.46ms
step:2166/2330 train_time:126619ms step_avg:58.46ms
step:2167/2330 train_time:126676ms step_avg:58.46ms
step:2168/2330 train_time:126737ms step_avg:58.46ms
step:2169/2330 train_time:126793ms step_avg:58.46ms
step:2170/2330 train_time:126855ms step_avg:58.46ms
step:2171/2330 train_time:126912ms step_avg:58.46ms
step:2172/2330 train_time:126973ms step_avg:58.46ms
step:2173/2330 train_time:127031ms step_avg:58.46ms
step:2174/2330 train_time:127092ms step_avg:58.46ms
step:2175/2330 train_time:127150ms step_avg:58.46ms
step:2176/2330 train_time:127211ms step_avg:58.46ms
step:2177/2330 train_time:127268ms step_avg:58.46ms
step:2178/2330 train_time:127329ms step_avg:58.46ms
step:2179/2330 train_time:127388ms step_avg:58.46ms
step:2180/2330 train_time:127449ms step_avg:58.46ms
step:2181/2330 train_time:127507ms step_avg:58.46ms
step:2182/2330 train_time:127568ms step_avg:58.46ms
step:2183/2330 train_time:127625ms step_avg:58.46ms
step:2184/2330 train_time:127686ms step_avg:58.46ms
step:2185/2330 train_time:127744ms step_avg:58.46ms
step:2186/2330 train_time:127804ms step_avg:58.46ms
step:2187/2330 train_time:127862ms step_avg:58.46ms
step:2188/2330 train_time:127924ms step_avg:58.47ms
step:2189/2330 train_time:127982ms step_avg:58.47ms
step:2190/2330 train_time:128042ms step_avg:58.47ms
step:2191/2330 train_time:128100ms step_avg:58.47ms
step:2192/2330 train_time:128161ms step_avg:58.47ms
step:2193/2330 train_time:128218ms step_avg:58.47ms
step:2194/2330 train_time:128279ms step_avg:58.47ms
step:2195/2330 train_time:128336ms step_avg:58.47ms
step:2196/2330 train_time:128397ms step_avg:58.47ms
step:2197/2330 train_time:128453ms step_avg:58.47ms
step:2198/2330 train_time:128515ms step_avg:58.47ms
step:2199/2330 train_time:128573ms step_avg:58.47ms
step:2200/2330 train_time:128634ms step_avg:58.47ms
step:2201/2330 train_time:128691ms step_avg:58.47ms
step:2202/2330 train_time:128751ms step_avg:58.47ms
step:2203/2330 train_time:128809ms step_avg:58.47ms
step:2204/2330 train_time:128870ms step_avg:58.47ms
step:2205/2330 train_time:128929ms step_avg:58.47ms
step:2206/2330 train_time:128989ms step_avg:58.47ms
step:2207/2330 train_time:129048ms step_avg:58.47ms
step:2208/2330 train_time:129109ms step_avg:58.47ms
step:2209/2330 train_time:129167ms step_avg:58.47ms
step:2210/2330 train_time:129228ms step_avg:58.47ms
step:2211/2330 train_time:129287ms step_avg:58.47ms
step:2212/2330 train_time:129347ms step_avg:58.48ms
step:2213/2330 train_time:129404ms step_avg:58.47ms
step:2214/2330 train_time:129465ms step_avg:58.48ms
step:2215/2330 train_time:129523ms step_avg:58.48ms
step:2216/2330 train_time:129585ms step_avg:58.48ms
step:2217/2330 train_time:129642ms step_avg:58.48ms
step:2218/2330 train_time:129702ms step_avg:58.48ms
step:2219/2330 train_time:129759ms step_avg:58.48ms
step:2220/2330 train_time:129821ms step_avg:58.48ms
step:2221/2330 train_time:129879ms step_avg:58.48ms
step:2222/2330 train_time:129940ms step_avg:58.48ms
step:2223/2330 train_time:129997ms step_avg:58.48ms
step:2224/2330 train_time:130057ms step_avg:58.48ms
step:2225/2330 train_time:130115ms step_avg:58.48ms
step:2226/2330 train_time:130175ms step_avg:58.48ms
step:2227/2330 train_time:130232ms step_avg:58.48ms
step:2228/2330 train_time:130293ms step_avg:58.48ms
step:2229/2330 train_time:130350ms step_avg:58.48ms
step:2230/2330 train_time:130411ms step_avg:58.48ms
step:2231/2330 train_time:130470ms step_avg:58.48ms
step:2232/2330 train_time:130530ms step_avg:58.48ms
step:2233/2330 train_time:130587ms step_avg:58.48ms
step:2234/2330 train_time:130649ms step_avg:58.48ms
step:2235/2330 train_time:130707ms step_avg:58.48ms
step:2236/2330 train_time:130768ms step_avg:58.48ms
step:2237/2330 train_time:130826ms step_avg:58.48ms
step:2238/2330 train_time:130887ms step_avg:58.48ms
step:2239/2330 train_time:130944ms step_avg:58.48ms
step:2240/2330 train_time:131006ms step_avg:58.48ms
step:2241/2330 train_time:131063ms step_avg:58.48ms
step:2242/2330 train_time:131125ms step_avg:58.49ms
step:2243/2330 train_time:131182ms step_avg:58.48ms
step:2244/2330 train_time:131243ms step_avg:58.49ms
step:2245/2330 train_time:131300ms step_avg:58.49ms
step:2246/2330 train_time:131362ms step_avg:58.49ms
step:2247/2330 train_time:131418ms step_avg:58.49ms
step:2248/2330 train_time:131482ms step_avg:58.49ms
step:2249/2330 train_time:131539ms step_avg:58.49ms
step:2250/2330 train_time:131600ms step_avg:58.49ms
step:2250/2330 val_loss:3.7083 train_time:131681ms step_avg:58.52ms
step:2251/2330 train_time:131701ms step_avg:58.51ms
step:2252/2330 train_time:131721ms step_avg:58.49ms
step:2253/2330 train_time:131782ms step_avg:58.49ms
step:2254/2330 train_time:131844ms step_avg:58.49ms
step:2255/2330 train_time:131901ms step_avg:58.49ms
step:2256/2330 train_time:131964ms step_avg:58.49ms
step:2257/2330 train_time:132021ms step_avg:58.49ms
step:2258/2330 train_time:132081ms step_avg:58.49ms
step:2259/2330 train_time:132138ms step_avg:58.49ms
step:2260/2330 train_time:132199ms step_avg:58.50ms
step:2261/2330 train_time:132256ms step_avg:58.49ms
step:2262/2330 train_time:132315ms step_avg:58.49ms
step:2263/2330 train_time:132372ms step_avg:58.49ms
step:2264/2330 train_time:132432ms step_avg:58.49ms
step:2265/2330 train_time:132489ms step_avg:58.49ms
step:2266/2330 train_time:132549ms step_avg:58.49ms
step:2267/2330 train_time:132607ms step_avg:58.49ms
step:2268/2330 train_time:132669ms step_avg:58.50ms
step:2269/2330 train_time:132729ms step_avg:58.50ms
step:2270/2330 train_time:132793ms step_avg:58.50ms
step:2271/2330 train_time:132852ms step_avg:58.50ms
step:2272/2330 train_time:132913ms step_avg:58.50ms
step:2273/2330 train_time:132971ms step_avg:58.50ms
step:2274/2330 train_time:133033ms step_avg:58.50ms
step:2275/2330 train_time:133090ms step_avg:58.50ms
step:2276/2330 train_time:133151ms step_avg:58.50ms
step:2277/2330 train_time:133208ms step_avg:58.50ms
step:2278/2330 train_time:133269ms step_avg:58.50ms
step:2279/2330 train_time:133326ms step_avg:58.50ms
step:2280/2330 train_time:133386ms step_avg:58.50ms
step:2281/2330 train_time:133442ms step_avg:58.50ms
step:2282/2330 train_time:133502ms step_avg:58.50ms
step:2283/2330 train_time:133559ms step_avg:58.50ms
step:2284/2330 train_time:133620ms step_avg:58.50ms
step:2285/2330 train_time:133677ms step_avg:58.50ms
step:2286/2330 train_time:133739ms step_avg:58.50ms
step:2287/2330 train_time:133797ms step_avg:58.50ms
step:2288/2330 train_time:133860ms step_avg:58.51ms
step:2289/2330 train_time:133917ms step_avg:58.50ms
step:2290/2330 train_time:133980ms step_avg:58.51ms
step:2291/2330 train_time:134037ms step_avg:58.51ms
step:2292/2330 train_time:134099ms step_avg:58.51ms
step:2293/2330 train_time:134156ms step_avg:58.51ms
step:2294/2330 train_time:134217ms step_avg:58.51ms
step:2295/2330 train_time:134274ms step_avg:58.51ms
step:2296/2330 train_time:134334ms step_avg:58.51ms
step:2297/2330 train_time:134391ms step_avg:58.51ms
step:2298/2330 train_time:134451ms step_avg:58.51ms
step:2299/2330 train_time:134509ms step_avg:58.51ms
step:2300/2330 train_time:134569ms step_avg:58.51ms
step:2301/2330 train_time:134627ms step_avg:58.51ms
step:2302/2330 train_time:134689ms step_avg:58.51ms
step:2303/2330 train_time:134748ms step_avg:58.51ms
step:2304/2330 train_time:134809ms step_avg:58.51ms
step:2305/2330 train_time:134868ms step_avg:58.51ms
step:2306/2330 train_time:134929ms step_avg:58.51ms
step:2307/2330 train_time:134988ms step_avg:58.51ms
step:2308/2330 train_time:135048ms step_avg:58.51ms
step:2309/2330 train_time:135107ms step_avg:58.51ms
step:2310/2330 train_time:135167ms step_avg:58.51ms
step:2311/2330 train_time:135225ms step_avg:58.51ms
step:2312/2330 train_time:135285ms step_avg:58.51ms
step:2313/2330 train_time:135341ms step_avg:58.51ms
step:2314/2330 train_time:135403ms step_avg:58.51ms
step:2315/2330 train_time:135460ms step_avg:58.51ms
step:2316/2330 train_time:135519ms step_avg:58.51ms
step:2317/2330 train_time:135577ms step_avg:58.51ms
step:2318/2330 train_time:135637ms step_avg:58.51ms
step:2319/2330 train_time:135695ms step_avg:58.51ms
step:2320/2330 train_time:135756ms step_avg:58.52ms
step:2321/2330 train_time:135814ms step_avg:58.52ms
step:2322/2330 train_time:135876ms step_avg:58.52ms
step:2323/2330 train_time:135934ms step_avg:58.52ms
step:2324/2330 train_time:135996ms step_avg:58.52ms
step:2325/2330 train_time:136053ms step_avg:58.52ms
step:2326/2330 train_time:136114ms step_avg:58.52ms
step:2327/2330 train_time:136171ms step_avg:58.52ms
step:2328/2330 train_time:136232ms step_avg:58.52ms
step:2329/2330 train_time:136290ms step_avg:58.52ms
step:2330/2330 train_time:136351ms step_avg:58.52ms
step:2330/2330 val_loss:3.6931 train_time:136432ms step_avg:58.55ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
