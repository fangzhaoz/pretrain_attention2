import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr7e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:08:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:81ms step_avg:80.67ms
step:2/2330 train_time:149ms step_avg:74.56ms
step:3/2330 train_time:162ms step_avg:53.86ms
step:4/2330 train_time:177ms step_avg:44.17ms
step:5/2330 train_time:189ms step_avg:37.89ms
step:6/2330 train_time:220ms step_avg:36.64ms
step:7/2330 train_time:253ms step_avg:36.19ms
step:8/2330 train_time:297ms step_avg:37.10ms
step:9/2330 train_time:331ms step_avg:36.78ms
step:10/2330 train_time:375ms step_avg:37.51ms
step:11/2330 train_time:410ms step_avg:37.24ms
step:12/2330 train_time:454ms step_avg:37.82ms
step:13/2330 train_time:489ms step_avg:37.59ms
step:14/2330 train_time:533ms step_avg:38.07ms
step:15/2330 train_time:568ms step_avg:37.83ms
step:16/2330 train_time:611ms step_avg:38.21ms
step:17/2330 train_time:646ms step_avg:38.02ms
step:18/2330 train_time:690ms step_avg:38.36ms
step:19/2330 train_time:725ms step_avg:38.16ms
step:20/2330 train_time:770ms step_avg:38.48ms
step:21/2330 train_time:804ms step_avg:38.30ms
step:22/2330 train_time:848ms step_avg:38.54ms
step:23/2330 train_time:882ms step_avg:38.37ms
step:24/2330 train_time:926ms step_avg:38.58ms
step:25/2330 train_time:961ms step_avg:38.44ms
step:26/2330 train_time:1009ms step_avg:38.79ms
step:27/2330 train_time:1048ms step_avg:38.81ms
step:28/2330 train_time:1097ms step_avg:39.17ms
step:29/2330 train_time:1134ms step_avg:39.10ms
step:30/2330 train_time:1180ms step_avg:39.34ms
step:31/2330 train_time:1215ms step_avg:39.21ms
step:32/2330 train_time:1260ms step_avg:39.37ms
step:33/2330 train_time:1295ms step_avg:39.24ms
step:34/2330 train_time:1338ms step_avg:39.37ms
step:35/2330 train_time:1373ms step_avg:39.24ms
step:36/2330 train_time:1417ms step_avg:39.37ms
step:37/2330 train_time:1452ms step_avg:39.24ms
step:38/2330 train_time:1496ms step_avg:39.37ms
step:39/2330 train_time:1530ms step_avg:39.24ms
step:40/2330 train_time:1575ms step_avg:39.38ms
step:41/2330 train_time:1611ms step_avg:39.28ms
step:42/2330 train_time:1656ms step_avg:39.42ms
step:43/2330 train_time:1691ms step_avg:39.33ms
step:44/2330 train_time:1735ms step_avg:39.43ms
step:45/2330 train_time:1769ms step_avg:39.32ms
step:46/2330 train_time:1814ms step_avg:39.43ms
step:47/2330 train_time:1849ms step_avg:39.34ms
step:48/2330 train_time:1893ms step_avg:39.45ms
step:49/2330 train_time:1929ms step_avg:39.36ms
step:50/2330 train_time:1974ms step_avg:39.48ms
step:51/2330 train_time:2010ms step_avg:39.41ms
step:52/2330 train_time:2056ms step_avg:39.54ms
step:53/2330 train_time:2093ms step_avg:39.48ms
step:54/2330 train_time:2139ms step_avg:39.60ms
step:55/2330 train_time:2175ms step_avg:39.54ms
step:56/2330 train_time:2220ms step_avg:39.64ms
step:57/2330 train_time:2255ms step_avg:39.56ms
step:58/2330 train_time:2299ms step_avg:39.64ms
step:59/2330 train_time:2334ms step_avg:39.56ms
step:60/2330 train_time:2378ms step_avg:39.64ms
step:61/2330 train_time:2413ms step_avg:39.56ms
step:62/2330 train_time:2457ms step_avg:39.63ms
step:63/2330 train_time:2492ms step_avg:39.56ms
step:64/2330 train_time:2536ms step_avg:39.63ms
step:65/2330 train_time:2571ms step_avg:39.56ms
step:66/2330 train_time:2616ms step_avg:39.64ms
step:67/2330 train_time:2651ms step_avg:39.57ms
step:68/2330 train_time:2695ms step_avg:39.63ms
step:69/2330 train_time:2729ms step_avg:39.55ms
step:70/2330 train_time:2773ms step_avg:39.61ms
step:71/2330 train_time:2808ms step_avg:39.55ms
step:72/2330 train_time:2853ms step_avg:39.62ms
step:73/2330 train_time:2888ms step_avg:39.56ms
step:74/2330 train_time:2932ms step_avg:39.63ms
step:75/2330 train_time:2968ms step_avg:39.57ms
step:76/2330 train_time:3013ms step_avg:39.65ms
step:77/2330 train_time:3050ms step_avg:39.61ms
step:78/2330 train_time:3095ms step_avg:39.68ms
step:79/2330 train_time:3132ms step_avg:39.64ms
step:80/2330 train_time:3177ms step_avg:39.71ms
step:81/2330 train_time:3213ms step_avg:39.66ms
step:82/2330 train_time:3257ms step_avg:39.72ms
step:83/2330 train_time:3293ms step_avg:39.67ms
step:84/2330 train_time:3337ms step_avg:39.73ms
step:85/2330 train_time:3373ms step_avg:39.68ms
step:86/2330 train_time:3416ms step_avg:39.73ms
step:87/2330 train_time:3451ms step_avg:39.66ms
step:88/2330 train_time:3495ms step_avg:39.72ms
step:89/2330 train_time:3530ms step_avg:39.66ms
step:90/2330 train_time:3575ms step_avg:39.72ms
step:91/2330 train_time:3610ms step_avg:39.67ms
step:92/2330 train_time:3654ms step_avg:39.72ms
step:93/2330 train_time:3689ms step_avg:39.67ms
step:94/2330 train_time:3732ms step_avg:39.71ms
step:95/2330 train_time:3768ms step_avg:39.66ms
step:96/2330 train_time:3812ms step_avg:39.71ms
step:97/2330 train_time:3848ms step_avg:39.67ms
step:98/2330 train_time:3893ms step_avg:39.72ms
step:99/2330 train_time:3928ms step_avg:39.68ms
step:100/2330 train_time:3973ms step_avg:39.73ms
step:101/2330 train_time:4008ms step_avg:39.68ms
step:102/2330 train_time:4053ms step_avg:39.74ms
step:103/2330 train_time:4089ms step_avg:39.70ms
step:104/2330 train_time:4134ms step_avg:39.75ms
step:105/2330 train_time:4169ms step_avg:39.71ms
step:106/2330 train_time:4215ms step_avg:39.77ms
step:107/2330 train_time:4251ms step_avg:39.73ms
step:108/2330 train_time:4296ms step_avg:39.78ms
step:109/2330 train_time:4331ms step_avg:39.73ms
step:110/2330 train_time:4375ms step_avg:39.77ms
step:111/2330 train_time:4410ms step_avg:39.73ms
step:112/2330 train_time:4454ms step_avg:39.77ms
step:113/2330 train_time:4489ms step_avg:39.73ms
step:114/2330 train_time:4534ms step_avg:39.77ms
step:115/2330 train_time:4568ms step_avg:39.72ms
step:116/2330 train_time:4611ms step_avg:39.75ms
step:117/2330 train_time:4647ms step_avg:39.72ms
step:118/2330 train_time:4691ms step_avg:39.75ms
step:119/2330 train_time:4725ms step_avg:39.70ms
step:120/2330 train_time:4768ms step_avg:39.74ms
step:121/2330 train_time:4803ms step_avg:39.70ms
step:122/2330 train_time:4847ms step_avg:39.73ms
step:123/2330 train_time:4883ms step_avg:39.70ms
step:124/2330 train_time:4926ms step_avg:39.73ms
step:125/2330 train_time:4961ms step_avg:39.69ms
step:126/2330 train_time:5006ms step_avg:39.73ms
step:127/2330 train_time:5041ms step_avg:39.69ms
step:128/2330 train_time:5087ms step_avg:39.74ms
step:129/2330 train_time:5122ms step_avg:39.70ms
step:130/2330 train_time:5167ms step_avg:39.74ms
step:131/2330 train_time:5202ms step_avg:39.71ms
step:132/2330 train_time:5246ms step_avg:39.74ms
step:133/2330 train_time:5282ms step_avg:39.72ms
step:134/2330 train_time:5326ms step_avg:39.75ms
step:135/2330 train_time:5362ms step_avg:39.72ms
step:136/2330 train_time:5406ms step_avg:39.75ms
step:137/2330 train_time:5441ms step_avg:39.71ms
step:138/2330 train_time:5485ms step_avg:39.75ms
step:139/2330 train_time:5520ms step_avg:39.71ms
step:140/2330 train_time:5563ms step_avg:39.74ms
step:141/2330 train_time:5598ms step_avg:39.70ms
step:142/2330 train_time:5641ms step_avg:39.73ms
step:143/2330 train_time:5676ms step_avg:39.69ms
step:144/2330 train_time:5721ms step_avg:39.73ms
step:145/2330 train_time:5756ms step_avg:39.69ms
step:146/2330 train_time:5800ms step_avg:39.72ms
step:147/2330 train_time:5835ms step_avg:39.69ms
step:148/2330 train_time:5879ms step_avg:39.72ms
step:149/2330 train_time:5914ms step_avg:39.69ms
step:150/2330 train_time:5959ms step_avg:39.72ms
step:151/2330 train_time:5994ms step_avg:39.69ms
step:152/2330 train_time:6038ms step_avg:39.73ms
step:153/2330 train_time:6074ms step_avg:39.70ms
step:154/2330 train_time:6119ms step_avg:39.73ms
step:155/2330 train_time:6155ms step_avg:39.71ms
step:156/2330 train_time:6199ms step_avg:39.74ms
step:157/2330 train_time:6234ms step_avg:39.71ms
step:158/2330 train_time:6279ms step_avg:39.74ms
step:159/2330 train_time:6314ms step_avg:39.71ms
step:160/2330 train_time:6358ms step_avg:39.74ms
step:161/2330 train_time:6393ms step_avg:39.71ms
step:162/2330 train_time:6438ms step_avg:39.74ms
step:163/2330 train_time:6473ms step_avg:39.71ms
step:164/2330 train_time:6517ms step_avg:39.74ms
step:165/2330 train_time:6552ms step_avg:39.71ms
step:166/2330 train_time:6596ms step_avg:39.74ms
step:167/2330 train_time:6631ms step_avg:39.71ms
step:168/2330 train_time:6675ms step_avg:39.73ms
step:169/2330 train_time:6710ms step_avg:39.70ms
step:170/2330 train_time:6755ms step_avg:39.73ms
step:171/2330 train_time:6791ms step_avg:39.71ms
step:172/2330 train_time:6835ms step_avg:39.74ms
step:173/2330 train_time:6870ms step_avg:39.71ms
step:174/2330 train_time:6914ms step_avg:39.74ms
step:175/2330 train_time:6950ms step_avg:39.71ms
step:176/2330 train_time:6994ms step_avg:39.74ms
step:177/2330 train_time:7030ms step_avg:39.72ms
step:178/2330 train_time:7075ms step_avg:39.75ms
step:179/2330 train_time:7110ms step_avg:39.72ms
step:180/2330 train_time:7154ms step_avg:39.74ms
step:181/2330 train_time:7189ms step_avg:39.72ms
step:182/2330 train_time:7235ms step_avg:39.75ms
step:183/2330 train_time:7270ms step_avg:39.73ms
step:184/2330 train_time:7315ms step_avg:39.75ms
step:185/2330 train_time:7349ms step_avg:39.72ms
step:186/2330 train_time:7393ms step_avg:39.75ms
step:187/2330 train_time:7429ms step_avg:39.73ms
step:188/2330 train_time:7473ms step_avg:39.75ms
step:189/2330 train_time:7508ms step_avg:39.72ms
step:190/2330 train_time:7553ms step_avg:39.75ms
step:191/2330 train_time:7588ms step_avg:39.73ms
step:192/2330 train_time:7632ms step_avg:39.75ms
step:193/2330 train_time:7667ms step_avg:39.72ms
step:194/2330 train_time:7711ms step_avg:39.75ms
step:195/2330 train_time:7746ms step_avg:39.73ms
step:196/2330 train_time:7791ms step_avg:39.75ms
step:197/2330 train_time:7826ms step_avg:39.72ms
step:198/2330 train_time:7870ms step_avg:39.75ms
step:199/2330 train_time:7905ms step_avg:39.72ms
step:200/2330 train_time:7949ms step_avg:39.74ms
step:201/2330 train_time:7984ms step_avg:39.72ms
step:202/2330 train_time:8028ms step_avg:39.74ms
step:203/2330 train_time:8063ms step_avg:39.72ms
step:204/2330 train_time:8107ms step_avg:39.74ms
step:205/2330 train_time:8142ms step_avg:39.71ms
step:206/2330 train_time:8186ms step_avg:39.74ms
step:207/2330 train_time:8221ms step_avg:39.71ms
step:208/2330 train_time:8265ms step_avg:39.73ms
step:209/2330 train_time:8300ms step_avg:39.71ms
step:210/2330 train_time:8345ms step_avg:39.74ms
step:211/2330 train_time:8380ms step_avg:39.72ms
step:212/2330 train_time:8424ms step_avg:39.74ms
step:213/2330 train_time:8459ms step_avg:39.72ms
step:214/2330 train_time:8503ms step_avg:39.74ms
step:215/2330 train_time:8538ms step_avg:39.71ms
step:216/2330 train_time:8582ms step_avg:39.73ms
step:217/2330 train_time:8617ms step_avg:39.71ms
step:218/2330 train_time:8661ms step_avg:39.73ms
step:219/2330 train_time:8696ms step_avg:39.71ms
step:220/2330 train_time:8740ms step_avg:39.73ms
step:221/2330 train_time:8776ms step_avg:39.71ms
step:222/2330 train_time:8821ms step_avg:39.73ms
step:223/2330 train_time:8856ms step_avg:39.71ms
step:224/2330 train_time:8900ms step_avg:39.73ms
step:225/2330 train_time:8935ms step_avg:39.71ms
step:226/2330 train_time:8979ms step_avg:39.73ms
step:227/2330 train_time:9015ms step_avg:39.71ms
step:228/2330 train_time:9059ms step_avg:39.73ms
step:229/2330 train_time:9094ms step_avg:39.71ms
step:230/2330 train_time:9138ms step_avg:39.73ms
step:231/2330 train_time:9173ms step_avg:39.71ms
step:232/2330 train_time:9217ms step_avg:39.73ms
step:233/2330 train_time:9253ms step_avg:39.71ms
step:234/2330 train_time:9298ms step_avg:39.73ms
step:235/2330 train_time:9333ms step_avg:39.72ms
step:236/2330 train_time:9377ms step_avg:39.73ms
step:237/2330 train_time:9413ms step_avg:39.72ms
step:238/2330 train_time:9457ms step_avg:39.74ms
step:239/2330 train_time:9492ms step_avg:39.72ms
step:240/2330 train_time:9537ms step_avg:39.74ms
step:241/2330 train_time:9572ms step_avg:39.72ms
step:242/2330 train_time:9616ms step_avg:39.74ms
step:243/2330 train_time:9652ms step_avg:39.72ms
step:244/2330 train_time:9696ms step_avg:39.74ms
step:245/2330 train_time:9731ms step_avg:39.72ms
step:246/2330 train_time:9776ms step_avg:39.74ms
step:247/2330 train_time:9811ms step_avg:39.72ms
step:248/2330 train_time:9856ms step_avg:39.74ms
step:249/2330 train_time:9891ms step_avg:39.72ms
step:250/2330 train_time:9935ms step_avg:39.74ms
step:250/2330 val_loss:5.4284 train_time:10021ms step_avg:40.09ms
step:251/2330 train_time:10035ms step_avg:39.98ms
step:252/2330 train_time:10048ms step_avg:39.87ms
step:253/2330 train_time:10059ms step_avg:39.76ms
step:254/2330 train_time:10094ms step_avg:39.74ms
step:255/2330 train_time:10127ms step_avg:39.72ms
step:256/2330 train_time:10171ms step_avg:39.73ms
step:257/2330 train_time:10205ms step_avg:39.71ms
step:258/2330 train_time:10249ms step_avg:39.72ms
step:259/2330 train_time:10283ms step_avg:39.70ms
step:260/2330 train_time:10328ms step_avg:39.72ms
step:261/2330 train_time:10363ms step_avg:39.70ms
step:262/2330 train_time:10409ms step_avg:39.73ms
step:263/2330 train_time:10446ms step_avg:39.72ms
step:264/2330 train_time:10491ms step_avg:39.74ms
step:265/2330 train_time:10527ms step_avg:39.73ms
step:266/2330 train_time:10571ms step_avg:39.74ms
step:267/2330 train_time:10606ms step_avg:39.72ms
step:268/2330 train_time:10650ms step_avg:39.74ms
step:269/2330 train_time:10685ms step_avg:39.72ms
step:270/2330 train_time:10729ms step_avg:39.74ms
step:271/2330 train_time:10764ms step_avg:39.72ms
step:272/2330 train_time:10807ms step_avg:39.73ms
step:273/2330 train_time:10842ms step_avg:39.72ms
step:274/2330 train_time:10886ms step_avg:39.73ms
step:275/2330 train_time:10926ms step_avg:39.73ms
step:276/2330 train_time:10975ms step_avg:39.76ms
step:277/2330 train_time:11013ms step_avg:39.76ms
step:278/2330 train_time:11059ms step_avg:39.78ms
step:279/2330 train_time:11094ms step_avg:39.77ms
step:280/2330 train_time:11138ms step_avg:39.78ms
step:281/2330 train_time:11173ms step_avg:39.76ms
step:282/2330 train_time:11217ms step_avg:39.78ms
step:283/2330 train_time:11251ms step_avg:39.76ms
step:284/2330 train_time:11294ms step_avg:39.77ms
step:285/2330 train_time:11329ms step_avg:39.75ms
step:286/2330 train_time:11373ms step_avg:39.77ms
step:287/2330 train_time:11408ms step_avg:39.75ms
step:288/2330 train_time:11452ms step_avg:39.76ms
step:289/2330 train_time:11487ms step_avg:39.75ms
step:290/2330 train_time:11531ms step_avg:39.76ms
step:291/2330 train_time:11565ms step_avg:39.74ms
step:292/2330 train_time:11609ms step_avg:39.76ms
step:293/2330 train_time:11643ms step_avg:39.74ms
step:294/2330 train_time:11688ms step_avg:39.75ms
step:295/2330 train_time:11722ms step_avg:39.74ms
step:296/2330 train_time:11765ms step_avg:39.75ms
step:297/2330 train_time:11800ms step_avg:39.73ms
step:298/2330 train_time:11845ms step_avg:39.75ms
step:299/2330 train_time:11881ms step_avg:39.74ms
step:300/2330 train_time:11927ms step_avg:39.76ms
step:301/2330 train_time:11964ms step_avg:39.75ms
step:302/2330 train_time:12009ms step_avg:39.76ms
step:303/2330 train_time:12045ms step_avg:39.75ms
step:304/2330 train_time:12090ms step_avg:39.77ms
step:305/2330 train_time:12126ms step_avg:39.76ms
step:306/2330 train_time:12169ms step_avg:39.77ms
step:307/2330 train_time:12204ms step_avg:39.75ms
step:308/2330 train_time:12250ms step_avg:39.77ms
step:309/2330 train_time:12285ms step_avg:39.76ms
step:310/2330 train_time:12329ms step_avg:39.77ms
step:311/2330 train_time:12364ms step_avg:39.76ms
step:312/2330 train_time:12408ms step_avg:39.77ms
step:313/2330 train_time:12443ms step_avg:39.75ms
step:314/2330 train_time:12487ms step_avg:39.77ms
step:315/2330 train_time:12522ms step_avg:39.75ms
step:316/2330 train_time:12565ms step_avg:39.76ms
step:317/2330 train_time:12600ms step_avg:39.75ms
step:318/2330 train_time:12643ms step_avg:39.76ms
step:319/2330 train_time:12678ms step_avg:39.74ms
step:320/2330 train_time:12722ms step_avg:39.76ms
step:321/2330 train_time:12757ms step_avg:39.74ms
step:322/2330 train_time:12802ms step_avg:39.76ms
step:323/2330 train_time:12836ms step_avg:39.74ms
step:324/2330 train_time:12881ms step_avg:39.76ms
step:325/2330 train_time:12916ms step_avg:39.74ms
step:326/2330 train_time:12961ms step_avg:39.76ms
step:327/2330 train_time:12996ms step_avg:39.74ms
step:328/2330 train_time:13042ms step_avg:39.76ms
step:329/2330 train_time:13077ms step_avg:39.75ms
step:330/2330 train_time:13122ms step_avg:39.76ms
step:331/2330 train_time:13158ms step_avg:39.75ms
step:332/2330 train_time:13202ms step_avg:39.76ms
step:333/2330 train_time:13237ms step_avg:39.75ms
step:334/2330 train_time:13281ms step_avg:39.76ms
step:335/2330 train_time:13316ms step_avg:39.75ms
step:336/2330 train_time:13360ms step_avg:39.76ms
step:337/2330 train_time:13396ms step_avg:39.75ms
step:338/2330 train_time:13440ms step_avg:39.76ms
step:339/2330 train_time:13474ms step_avg:39.75ms
step:340/2330 train_time:13519ms step_avg:39.76ms
step:341/2330 train_time:13553ms step_avg:39.75ms
step:342/2330 train_time:13596ms step_avg:39.76ms
step:343/2330 train_time:13630ms step_avg:39.74ms
step:344/2330 train_time:13674ms step_avg:39.75ms
step:345/2330 train_time:13708ms step_avg:39.73ms
step:346/2330 train_time:13752ms step_avg:39.74ms
step:347/2330 train_time:13786ms step_avg:39.73ms
step:348/2330 train_time:13831ms step_avg:39.74ms
step:349/2330 train_time:13866ms step_avg:39.73ms
step:350/2330 train_time:13911ms step_avg:39.75ms
step:351/2330 train_time:13947ms step_avg:39.73ms
step:352/2330 train_time:13992ms step_avg:39.75ms
step:353/2330 train_time:14027ms step_avg:39.74ms
step:354/2330 train_time:14072ms step_avg:39.75ms
step:355/2330 train_time:14107ms step_avg:39.74ms
step:356/2330 train_time:14153ms step_avg:39.75ms
step:357/2330 train_time:14188ms step_avg:39.74ms
step:358/2330 train_time:14232ms step_avg:39.75ms
step:359/2330 train_time:14267ms step_avg:39.74ms
step:360/2330 train_time:14313ms step_avg:39.76ms
step:361/2330 train_time:14348ms step_avg:39.75ms
step:362/2330 train_time:14392ms step_avg:39.76ms
step:363/2330 train_time:14428ms step_avg:39.75ms
step:364/2330 train_time:14471ms step_avg:39.76ms
step:365/2330 train_time:14506ms step_avg:39.74ms
step:366/2330 train_time:14550ms step_avg:39.75ms
step:367/2330 train_time:14584ms step_avg:39.74ms
step:368/2330 train_time:14628ms step_avg:39.75ms
step:369/2330 train_time:14663ms step_avg:39.74ms
step:370/2330 train_time:14706ms step_avg:39.75ms
step:371/2330 train_time:14741ms step_avg:39.73ms
step:372/2330 train_time:14786ms step_avg:39.75ms
step:373/2330 train_time:14821ms step_avg:39.73ms
step:374/2330 train_time:14865ms step_avg:39.75ms
step:375/2330 train_time:14901ms step_avg:39.74ms
step:376/2330 train_time:14946ms step_avg:39.75ms
step:377/2330 train_time:14982ms step_avg:39.74ms
step:378/2330 train_time:15027ms step_avg:39.75ms
step:379/2330 train_time:15061ms step_avg:39.74ms
step:380/2330 train_time:15106ms step_avg:39.75ms
step:381/2330 train_time:15142ms step_avg:39.74ms
step:382/2330 train_time:15187ms step_avg:39.76ms
step:383/2330 train_time:15223ms step_avg:39.75ms
step:384/2330 train_time:15267ms step_avg:39.76ms
step:385/2330 train_time:15302ms step_avg:39.74ms
step:386/2330 train_time:15347ms step_avg:39.76ms
step:387/2330 train_time:15381ms step_avg:39.75ms
step:388/2330 train_time:15426ms step_avg:39.76ms
step:389/2330 train_time:15461ms step_avg:39.75ms
step:390/2330 train_time:15505ms step_avg:39.76ms
step:391/2330 train_time:15540ms step_avg:39.74ms
step:392/2330 train_time:15584ms step_avg:39.75ms
step:393/2330 train_time:15618ms step_avg:39.74ms
step:394/2330 train_time:15662ms step_avg:39.75ms
step:395/2330 train_time:15697ms step_avg:39.74ms
step:396/2330 train_time:15742ms step_avg:39.75ms
step:397/2330 train_time:15776ms step_avg:39.74ms
step:398/2330 train_time:15821ms step_avg:39.75ms
step:399/2330 train_time:15855ms step_avg:39.74ms
step:400/2330 train_time:15900ms step_avg:39.75ms
step:401/2330 train_time:15935ms step_avg:39.74ms
step:402/2330 train_time:15979ms step_avg:39.75ms
step:403/2330 train_time:16014ms step_avg:39.74ms
step:404/2330 train_time:16058ms step_avg:39.75ms
step:405/2330 train_time:16093ms step_avg:39.74ms
step:406/2330 train_time:16138ms step_avg:39.75ms
step:407/2330 train_time:16174ms step_avg:39.74ms
step:408/2330 train_time:16218ms step_avg:39.75ms
step:409/2330 train_time:16252ms step_avg:39.74ms
step:410/2330 train_time:16296ms step_avg:39.75ms
step:411/2330 train_time:16331ms step_avg:39.74ms
step:412/2330 train_time:16375ms step_avg:39.74ms
step:413/2330 train_time:16410ms step_avg:39.73ms
step:414/2330 train_time:16454ms step_avg:39.74ms
step:415/2330 train_time:16489ms step_avg:39.73ms
step:416/2330 train_time:16533ms step_avg:39.74ms
step:417/2330 train_time:16568ms step_avg:39.73ms
step:418/2330 train_time:16612ms step_avg:39.74ms
step:419/2330 train_time:16648ms step_avg:39.73ms
step:420/2330 train_time:16692ms step_avg:39.74ms
step:421/2330 train_time:16726ms step_avg:39.73ms
step:422/2330 train_time:16770ms step_avg:39.74ms
step:423/2330 train_time:16805ms step_avg:39.73ms
step:424/2330 train_time:16849ms step_avg:39.74ms
step:425/2330 train_time:16885ms step_avg:39.73ms
step:426/2330 train_time:16930ms step_avg:39.74ms
step:427/2330 train_time:16965ms step_avg:39.73ms
step:428/2330 train_time:17009ms step_avg:39.74ms
step:429/2330 train_time:17044ms step_avg:39.73ms
step:430/2330 train_time:17089ms step_avg:39.74ms
step:431/2330 train_time:17125ms step_avg:39.73ms
step:432/2330 train_time:17170ms step_avg:39.74ms
step:433/2330 train_time:17204ms step_avg:39.73ms
step:434/2330 train_time:17248ms step_avg:39.74ms
step:435/2330 train_time:17283ms step_avg:39.73ms
step:436/2330 train_time:17328ms step_avg:39.74ms
step:437/2330 train_time:17364ms step_avg:39.73ms
step:438/2330 train_time:17409ms step_avg:39.75ms
step:439/2330 train_time:17443ms step_avg:39.73ms
step:440/2330 train_time:17488ms step_avg:39.75ms
step:441/2330 train_time:17524ms step_avg:39.74ms
step:442/2330 train_time:17568ms step_avg:39.75ms
step:443/2330 train_time:17602ms step_avg:39.73ms
step:444/2330 train_time:17647ms step_avg:39.74ms
step:445/2330 train_time:17682ms step_avg:39.73ms
step:446/2330 train_time:17726ms step_avg:39.74ms
step:447/2330 train_time:17761ms step_avg:39.73ms
step:448/2330 train_time:17805ms step_avg:39.74ms
step:449/2330 train_time:17840ms step_avg:39.73ms
step:450/2330 train_time:17884ms step_avg:39.74ms
step:451/2330 train_time:17919ms step_avg:39.73ms
step:452/2330 train_time:17963ms step_avg:39.74ms
step:453/2330 train_time:17999ms step_avg:39.73ms
step:454/2330 train_time:18043ms step_avg:39.74ms
step:455/2330 train_time:18078ms step_avg:39.73ms
step:456/2330 train_time:18122ms step_avg:39.74ms
step:457/2330 train_time:18157ms step_avg:39.73ms
step:458/2330 train_time:18201ms step_avg:39.74ms
step:459/2330 train_time:18236ms step_avg:39.73ms
step:460/2330 train_time:18281ms step_avg:39.74ms
step:461/2330 train_time:18316ms step_avg:39.73ms
step:462/2330 train_time:18360ms step_avg:39.74ms
step:463/2330 train_time:18396ms step_avg:39.73ms
step:464/2330 train_time:18440ms step_avg:39.74ms
step:465/2330 train_time:18476ms step_avg:39.73ms
step:466/2330 train_time:18521ms step_avg:39.74ms
step:467/2330 train_time:18555ms step_avg:39.73ms
step:468/2330 train_time:18599ms step_avg:39.74ms
step:469/2330 train_time:18633ms step_avg:39.73ms
step:470/2330 train_time:18677ms step_avg:39.74ms
step:471/2330 train_time:18711ms step_avg:39.73ms
step:472/2330 train_time:18754ms step_avg:39.73ms
step:473/2330 train_time:18788ms step_avg:39.72ms
step:474/2330 train_time:18833ms step_avg:39.73ms
step:475/2330 train_time:18868ms step_avg:39.72ms
step:476/2330 train_time:18912ms step_avg:39.73ms
step:477/2330 train_time:18947ms step_avg:39.72ms
step:478/2330 train_time:18991ms step_avg:39.73ms
step:479/2330 train_time:19026ms step_avg:39.72ms
step:480/2330 train_time:19070ms step_avg:39.73ms
step:481/2330 train_time:19106ms step_avg:39.72ms
step:482/2330 train_time:19150ms step_avg:39.73ms
step:483/2330 train_time:19185ms step_avg:39.72ms
step:484/2330 train_time:19230ms step_avg:39.73ms
step:485/2330 train_time:19265ms step_avg:39.72ms
step:486/2330 train_time:19310ms step_avg:39.73ms
step:487/2330 train_time:19346ms step_avg:39.72ms
step:488/2330 train_time:19390ms step_avg:39.73ms
step:489/2330 train_time:19426ms step_avg:39.72ms
step:490/2330 train_time:19470ms step_avg:39.73ms
step:491/2330 train_time:19506ms step_avg:39.73ms
step:492/2330 train_time:19551ms step_avg:39.74ms
step:493/2330 train_time:19586ms step_avg:39.73ms
step:494/2330 train_time:19631ms step_avg:39.74ms
step:495/2330 train_time:19666ms step_avg:39.73ms
step:496/2330 train_time:19710ms step_avg:39.74ms
step:497/2330 train_time:19745ms step_avg:39.73ms
step:498/2330 train_time:19790ms step_avg:39.74ms
step:499/2330 train_time:19826ms step_avg:39.73ms
step:500/2330 train_time:19870ms step_avg:39.74ms
step:500/2330 val_loss:5.3019 train_time:19958ms step_avg:39.92ms
step:501/2330 train_time:19971ms step_avg:39.86ms
step:502/2330 train_time:19985ms step_avg:39.81ms
step:503/2330 train_time:19996ms step_avg:39.75ms
step:504/2330 train_time:20030ms step_avg:39.74ms
step:505/2330 train_time:20065ms step_avg:39.73ms
step:506/2330 train_time:20108ms step_avg:39.74ms
step:507/2330 train_time:20142ms step_avg:39.73ms
step:508/2330 train_time:20186ms step_avg:39.74ms
step:509/2330 train_time:20220ms step_avg:39.73ms
step:510/2330 train_time:20265ms step_avg:39.73ms
step:511/2330 train_time:20304ms step_avg:39.73ms
step:512/2330 train_time:20352ms step_avg:39.75ms
step:513/2330 train_time:20387ms step_avg:39.74ms
step:514/2330 train_time:20432ms step_avg:39.75ms
step:515/2330 train_time:20469ms step_avg:39.75ms
step:516/2330 train_time:20513ms step_avg:39.75ms
step:517/2330 train_time:20548ms step_avg:39.75ms
step:518/2330 train_time:20593ms step_avg:39.75ms
step:519/2330 train_time:20627ms step_avg:39.74ms
step:520/2330 train_time:20671ms step_avg:39.75ms
step:521/2330 train_time:20706ms step_avg:39.74ms
step:522/2330 train_time:20749ms step_avg:39.75ms
step:523/2330 train_time:20785ms step_avg:39.74ms
step:524/2330 train_time:20829ms step_avg:39.75ms
step:525/2330 train_time:20863ms step_avg:39.74ms
step:526/2330 train_time:20908ms step_avg:39.75ms
step:527/2330 train_time:20944ms step_avg:39.74ms
step:528/2330 train_time:20988ms step_avg:39.75ms
step:529/2330 train_time:21023ms step_avg:39.74ms
step:530/2330 train_time:21067ms step_avg:39.75ms
step:531/2330 train_time:21101ms step_avg:39.74ms
step:532/2330 train_time:21145ms step_avg:39.75ms
step:533/2330 train_time:21180ms step_avg:39.74ms
step:534/2330 train_time:21225ms step_avg:39.75ms
step:535/2330 train_time:21261ms step_avg:39.74ms
step:536/2330 train_time:21306ms step_avg:39.75ms
step:537/2330 train_time:21343ms step_avg:39.74ms
step:538/2330 train_time:21388ms step_avg:39.75ms
step:539/2330 train_time:21423ms step_avg:39.74ms
step:540/2330 train_time:21467ms step_avg:39.75ms
step:541/2330 train_time:21503ms step_avg:39.75ms
step:542/2330 train_time:21547ms step_avg:39.75ms
step:543/2330 train_time:21583ms step_avg:39.75ms
step:544/2330 train_time:21627ms step_avg:39.76ms
step:545/2330 train_time:21663ms step_avg:39.75ms
step:546/2330 train_time:21706ms step_avg:39.76ms
step:547/2330 train_time:21742ms step_avg:39.75ms
step:548/2330 train_time:21786ms step_avg:39.76ms
step:549/2330 train_time:21822ms step_avg:39.75ms
step:550/2330 train_time:21866ms step_avg:39.76ms
step:551/2330 train_time:21901ms step_avg:39.75ms
step:552/2330 train_time:21945ms step_avg:39.76ms
step:553/2330 train_time:21980ms step_avg:39.75ms
step:554/2330 train_time:22024ms step_avg:39.75ms
step:555/2330 train_time:22059ms step_avg:39.75ms
step:556/2330 train_time:22102ms step_avg:39.75ms
step:557/2330 train_time:22137ms step_avg:39.74ms
step:558/2330 train_time:22181ms step_avg:39.75ms
step:559/2330 train_time:22215ms step_avg:39.74ms
step:560/2330 train_time:22262ms step_avg:39.75ms
step:561/2330 train_time:22296ms step_avg:39.74ms
step:562/2330 train_time:22341ms step_avg:39.75ms
step:563/2330 train_time:22376ms step_avg:39.74ms
step:564/2330 train_time:22421ms step_avg:39.75ms
step:565/2330 train_time:22457ms step_avg:39.75ms
step:566/2330 train_time:22501ms step_avg:39.75ms
step:567/2330 train_time:22536ms step_avg:39.75ms
step:568/2330 train_time:22581ms step_avg:39.76ms
step:569/2330 train_time:22617ms step_avg:39.75ms
step:570/2330 train_time:22661ms step_avg:39.76ms
step:571/2330 train_time:22696ms step_avg:39.75ms
step:572/2330 train_time:22740ms step_avg:39.76ms
step:573/2330 train_time:22774ms step_avg:39.75ms
step:574/2330 train_time:22819ms step_avg:39.75ms
step:575/2330 train_time:22853ms step_avg:39.74ms
step:576/2330 train_time:22897ms step_avg:39.75ms
step:577/2330 train_time:22932ms step_avg:39.74ms
step:578/2330 train_time:22976ms step_avg:39.75ms
step:579/2330 train_time:23011ms step_avg:39.74ms
step:580/2330 train_time:23055ms step_avg:39.75ms
step:581/2330 train_time:23090ms step_avg:39.74ms
step:582/2330 train_time:23133ms step_avg:39.75ms
step:583/2330 train_time:23169ms step_avg:39.74ms
step:584/2330 train_time:23213ms step_avg:39.75ms
step:585/2330 train_time:23248ms step_avg:39.74ms
step:586/2330 train_time:23293ms step_avg:39.75ms
step:587/2330 train_time:23329ms step_avg:39.74ms
step:588/2330 train_time:23374ms step_avg:39.75ms
step:589/2330 train_time:23409ms step_avg:39.74ms
step:590/2330 train_time:23454ms step_avg:39.75ms
step:591/2330 train_time:23489ms step_avg:39.74ms
step:592/2330 train_time:23534ms step_avg:39.75ms
step:593/2330 train_time:23569ms step_avg:39.75ms
step:594/2330 train_time:23613ms step_avg:39.75ms
step:595/2330 train_time:23649ms step_avg:39.75ms
step:596/2330 train_time:23693ms step_avg:39.75ms
step:597/2330 train_time:23728ms step_avg:39.75ms
step:598/2330 train_time:23773ms step_avg:39.75ms
step:599/2330 train_time:23808ms step_avg:39.75ms
step:600/2330 train_time:23853ms step_avg:39.75ms
step:601/2330 train_time:23887ms step_avg:39.75ms
step:602/2330 train_time:23931ms step_avg:39.75ms
step:603/2330 train_time:23966ms step_avg:39.75ms
step:604/2330 train_time:24011ms step_avg:39.75ms
step:605/2330 train_time:24047ms step_avg:39.75ms
step:606/2330 train_time:24091ms step_avg:39.75ms
step:607/2330 train_time:24126ms step_avg:39.75ms
step:608/2330 train_time:24171ms step_avg:39.75ms
step:609/2330 train_time:24206ms step_avg:39.75ms
step:610/2330 train_time:24250ms step_avg:39.75ms
step:611/2330 train_time:24286ms step_avg:39.75ms
step:612/2330 train_time:24331ms step_avg:39.76ms
step:613/2330 train_time:24366ms step_avg:39.75ms
step:614/2330 train_time:24411ms step_avg:39.76ms
step:615/2330 train_time:24446ms step_avg:39.75ms
step:616/2330 train_time:24492ms step_avg:39.76ms
step:617/2330 train_time:24527ms step_avg:39.75ms
step:618/2330 train_time:24571ms step_avg:39.76ms
step:619/2330 train_time:24607ms step_avg:39.75ms
step:620/2330 train_time:24651ms step_avg:39.76ms
step:621/2330 train_time:24686ms step_avg:39.75ms
step:622/2330 train_time:24731ms step_avg:39.76ms
step:623/2330 train_time:24766ms step_avg:39.75ms
step:624/2330 train_time:24810ms step_avg:39.76ms
step:625/2330 train_time:24845ms step_avg:39.75ms
step:626/2330 train_time:24889ms step_avg:39.76ms
step:627/2330 train_time:24925ms step_avg:39.75ms
step:628/2330 train_time:24969ms step_avg:39.76ms
step:629/2330 train_time:25004ms step_avg:39.75ms
step:630/2330 train_time:25049ms step_avg:39.76ms
step:631/2330 train_time:25084ms step_avg:39.75ms
step:632/2330 train_time:25128ms step_avg:39.76ms
step:633/2330 train_time:25164ms step_avg:39.75ms
step:634/2330 train_time:25208ms step_avg:39.76ms
step:635/2330 train_time:25243ms step_avg:39.75ms
step:636/2330 train_time:25288ms step_avg:39.76ms
step:637/2330 train_time:25323ms step_avg:39.75ms
step:638/2330 train_time:25368ms step_avg:39.76ms
step:639/2330 train_time:25404ms step_avg:39.76ms
step:640/2330 train_time:25448ms step_avg:39.76ms
step:641/2330 train_time:25484ms step_avg:39.76ms
step:642/2330 train_time:25529ms step_avg:39.76ms
step:643/2330 train_time:25564ms step_avg:39.76ms
step:644/2330 train_time:25608ms step_avg:39.76ms
step:645/2330 train_time:25644ms step_avg:39.76ms
step:646/2330 train_time:25688ms step_avg:39.76ms
step:647/2330 train_time:25724ms step_avg:39.76ms
step:648/2330 train_time:25769ms step_avg:39.77ms
step:649/2330 train_time:25804ms step_avg:39.76ms
step:650/2330 train_time:25848ms step_avg:39.77ms
step:651/2330 train_time:25883ms step_avg:39.76ms
step:652/2330 train_time:25929ms step_avg:39.77ms
step:653/2330 train_time:25964ms step_avg:39.76ms
step:654/2330 train_time:26008ms step_avg:39.77ms
step:655/2330 train_time:26042ms step_avg:39.76ms
step:656/2330 train_time:26086ms step_avg:39.77ms
step:657/2330 train_time:26122ms step_avg:39.76ms
step:658/2330 train_time:26166ms step_avg:39.77ms
step:659/2330 train_time:26200ms step_avg:39.76ms
step:660/2330 train_time:26245ms step_avg:39.77ms
step:661/2330 train_time:26280ms step_avg:39.76ms
step:662/2330 train_time:26324ms step_avg:39.77ms
step:663/2330 train_time:26359ms step_avg:39.76ms
step:664/2330 train_time:26404ms step_avg:39.77ms
step:665/2330 train_time:26440ms step_avg:39.76ms
step:666/2330 train_time:26484ms step_avg:39.77ms
step:667/2330 train_time:26519ms step_avg:39.76ms
step:668/2330 train_time:26564ms step_avg:39.77ms
step:669/2330 train_time:26598ms step_avg:39.76ms
step:670/2330 train_time:26642ms step_avg:39.76ms
step:671/2330 train_time:26678ms step_avg:39.76ms
step:672/2330 train_time:26723ms step_avg:39.77ms
step:673/2330 train_time:26758ms step_avg:39.76ms
step:674/2330 train_time:26802ms step_avg:39.77ms
step:675/2330 train_time:26837ms step_avg:39.76ms
step:676/2330 train_time:26881ms step_avg:39.77ms
step:677/2330 train_time:26916ms step_avg:39.76ms
step:678/2330 train_time:26960ms step_avg:39.76ms
step:679/2330 train_time:26995ms step_avg:39.76ms
step:680/2330 train_time:27039ms step_avg:39.76ms
step:681/2330 train_time:27073ms step_avg:39.76ms
step:682/2330 train_time:27118ms step_avg:39.76ms
step:683/2330 train_time:27153ms step_avg:39.75ms
step:684/2330 train_time:27197ms step_avg:39.76ms
step:685/2330 train_time:27232ms step_avg:39.75ms
step:686/2330 train_time:27276ms step_avg:39.76ms
step:687/2330 train_time:27311ms step_avg:39.75ms
step:688/2330 train_time:27355ms step_avg:39.76ms
step:689/2330 train_time:27391ms step_avg:39.75ms
step:690/2330 train_time:27435ms step_avg:39.76ms
step:691/2330 train_time:27471ms step_avg:39.76ms
step:692/2330 train_time:27515ms step_avg:39.76ms
step:693/2330 train_time:27551ms step_avg:39.76ms
step:694/2330 train_time:27595ms step_avg:39.76ms
step:695/2330 train_time:27630ms step_avg:39.76ms
step:696/2330 train_time:27675ms step_avg:39.76ms
step:697/2330 train_time:27710ms step_avg:39.76ms
step:698/2330 train_time:27754ms step_avg:39.76ms
step:699/2330 train_time:27789ms step_avg:39.76ms
step:700/2330 train_time:27834ms step_avg:39.76ms
step:701/2330 train_time:27869ms step_avg:39.76ms
step:702/2330 train_time:27913ms step_avg:39.76ms
step:703/2330 train_time:27948ms step_avg:39.76ms
step:704/2330 train_time:27993ms step_avg:39.76ms
step:705/2330 train_time:28029ms step_avg:39.76ms
step:706/2330 train_time:28073ms step_avg:39.76ms
step:707/2330 train_time:28109ms step_avg:39.76ms
step:708/2330 train_time:28153ms step_avg:39.76ms
step:709/2330 train_time:28189ms step_avg:39.76ms
step:710/2330 train_time:28233ms step_avg:39.76ms
step:711/2330 train_time:28268ms step_avg:39.76ms
step:712/2330 train_time:28314ms step_avg:39.77ms
step:713/2330 train_time:28349ms step_avg:39.76ms
step:714/2330 train_time:28394ms step_avg:39.77ms
step:715/2330 train_time:28430ms step_avg:39.76ms
step:716/2330 train_time:28473ms step_avg:39.77ms
step:717/2330 train_time:28509ms step_avg:39.76ms
step:718/2330 train_time:28554ms step_avg:39.77ms
step:719/2330 train_time:28589ms step_avg:39.76ms
step:720/2330 train_time:28633ms step_avg:39.77ms
step:721/2330 train_time:28668ms step_avg:39.76ms
step:722/2330 train_time:28713ms step_avg:39.77ms
step:723/2330 train_time:28749ms step_avg:39.76ms
step:724/2330 train_time:28793ms step_avg:39.77ms
step:725/2330 train_time:28828ms step_avg:39.76ms
step:726/2330 train_time:28873ms step_avg:39.77ms
step:727/2330 train_time:28908ms step_avg:39.76ms
step:728/2330 train_time:28952ms step_avg:39.77ms
step:729/2330 train_time:28986ms step_avg:39.76ms
step:730/2330 train_time:29030ms step_avg:39.77ms
step:731/2330 train_time:29066ms step_avg:39.76ms
step:732/2330 train_time:29110ms step_avg:39.77ms
step:733/2330 train_time:29145ms step_avg:39.76ms
step:734/2330 train_time:29190ms step_avg:39.77ms
step:735/2330 train_time:29225ms step_avg:39.76ms
step:736/2330 train_time:29270ms step_avg:39.77ms
step:737/2330 train_time:29305ms step_avg:39.76ms
step:738/2330 train_time:29349ms step_avg:39.77ms
step:739/2330 train_time:29386ms step_avg:39.76ms
step:740/2330 train_time:29430ms step_avg:39.77ms
step:741/2330 train_time:29465ms step_avg:39.76ms
step:742/2330 train_time:29509ms step_avg:39.77ms
step:743/2330 train_time:29544ms step_avg:39.76ms
step:744/2330 train_time:29589ms step_avg:39.77ms
step:745/2330 train_time:29624ms step_avg:39.76ms
step:746/2330 train_time:29669ms step_avg:39.77ms
step:747/2330 train_time:29704ms step_avg:39.76ms
step:748/2330 train_time:29748ms step_avg:39.77ms
step:749/2330 train_time:29783ms step_avg:39.76ms
step:750/2330 train_time:29828ms step_avg:39.77ms
step:750/2330 val_loss:5.2403 train_time:29914ms step_avg:39.88ms
step:751/2330 train_time:29927ms step_avg:39.85ms
step:752/2330 train_time:29939ms step_avg:39.81ms
step:753/2330 train_time:29950ms step_avg:39.77ms
step:754/2330 train_time:29986ms step_avg:39.77ms
step:755/2330 train_time:30021ms step_avg:39.76ms
step:756/2330 train_time:30064ms step_avg:39.77ms
step:757/2330 train_time:30098ms step_avg:39.76ms
step:758/2330 train_time:30141ms step_avg:39.76ms
step:759/2330 train_time:30175ms step_avg:39.76ms
step:760/2330 train_time:30223ms step_avg:39.77ms
step:761/2330 train_time:30263ms step_avg:39.77ms
step:762/2330 train_time:30311ms step_avg:39.78ms
step:763/2330 train_time:30346ms step_avg:39.77ms
step:764/2330 train_time:30391ms step_avg:39.78ms
step:765/2330 train_time:30427ms step_avg:39.77ms
step:766/2330 train_time:30470ms step_avg:39.78ms
step:767/2330 train_time:30505ms step_avg:39.77ms
step:768/2330 train_time:30550ms step_avg:39.78ms
step:769/2330 train_time:30584ms step_avg:39.77ms
step:770/2330 train_time:30628ms step_avg:39.78ms
step:771/2330 train_time:30663ms step_avg:39.77ms
step:772/2330 train_time:30707ms step_avg:39.78ms
step:773/2330 train_time:30743ms step_avg:39.77ms
step:774/2330 train_time:30786ms step_avg:39.78ms
step:775/2330 train_time:30821ms step_avg:39.77ms
step:776/2330 train_time:30866ms step_avg:39.78ms
step:777/2330 train_time:30901ms step_avg:39.77ms
step:778/2330 train_time:30945ms step_avg:39.78ms
step:779/2330 train_time:30979ms step_avg:39.77ms
step:780/2330 train_time:31023ms step_avg:39.77ms
step:781/2330 train_time:31057ms step_avg:39.77ms
step:782/2330 train_time:31100ms step_avg:39.77ms
step:783/2330 train_time:31136ms step_avg:39.76ms
step:784/2330 train_time:31180ms step_avg:39.77ms
step:785/2330 train_time:31217ms step_avg:39.77ms
step:786/2330 train_time:31262ms step_avg:39.77ms
step:787/2330 train_time:31298ms step_avg:39.77ms
step:788/2330 train_time:31342ms step_avg:39.77ms
step:789/2330 train_time:31378ms step_avg:39.77ms
step:790/2330 train_time:31422ms step_avg:39.77ms
step:791/2330 train_time:31457ms step_avg:39.77ms
step:792/2330 train_time:31501ms step_avg:39.77ms
step:793/2330 train_time:31537ms step_avg:39.77ms
step:794/2330 train_time:31581ms step_avg:39.77ms
step:795/2330 train_time:31616ms step_avg:39.77ms
step:796/2330 train_time:31660ms step_avg:39.77ms
step:797/2330 train_time:31695ms step_avg:39.77ms
step:798/2330 train_time:31740ms step_avg:39.77ms
step:799/2330 train_time:31774ms step_avg:39.77ms
step:800/2330 train_time:31818ms step_avg:39.77ms
step:801/2330 train_time:31852ms step_avg:39.77ms
step:802/2330 train_time:31896ms step_avg:39.77ms
step:803/2330 train_time:31931ms step_avg:39.76ms
step:804/2330 train_time:31975ms step_avg:39.77ms
step:805/2330 train_time:32010ms step_avg:39.76ms
step:806/2330 train_time:32054ms step_avg:39.77ms
step:807/2330 train_time:32089ms step_avg:39.76ms
step:808/2330 train_time:32133ms step_avg:39.77ms
step:809/2330 train_time:32169ms step_avg:39.76ms
step:810/2330 train_time:32214ms step_avg:39.77ms
step:811/2330 train_time:32250ms step_avg:39.77ms
step:812/2330 train_time:32295ms step_avg:39.77ms
step:813/2330 train_time:32330ms step_avg:39.77ms
step:814/2330 train_time:32375ms step_avg:39.77ms
step:815/2330 train_time:32410ms step_avg:39.77ms
step:816/2330 train_time:32455ms step_avg:39.77ms
step:817/2330 train_time:32490ms step_avg:39.77ms
step:818/2330 train_time:32534ms step_avg:39.77ms
step:819/2330 train_time:32570ms step_avg:39.77ms
step:820/2330 train_time:32614ms step_avg:39.77ms
step:821/2330 train_time:32649ms step_avg:39.77ms
step:822/2330 train_time:32693ms step_avg:39.77ms
step:823/2330 train_time:32728ms step_avg:39.77ms
step:824/2330 train_time:32773ms step_avg:39.77ms
step:825/2330 train_time:32808ms step_avg:39.77ms
step:826/2330 train_time:32852ms step_avg:39.77ms
step:827/2330 train_time:32886ms step_avg:39.77ms
step:828/2330 train_time:32930ms step_avg:39.77ms
step:829/2330 train_time:32966ms step_avg:39.77ms
step:830/2330 train_time:33009ms step_avg:39.77ms
step:831/2330 train_time:33044ms step_avg:39.76ms
step:832/2330 train_time:33089ms step_avg:39.77ms
step:833/2330 train_time:33124ms step_avg:39.76ms
step:834/2330 train_time:33170ms step_avg:39.77ms
step:835/2330 train_time:33205ms step_avg:39.77ms
step:836/2330 train_time:33250ms step_avg:39.77ms
step:837/2330 train_time:33286ms step_avg:39.77ms
step:838/2330 train_time:33330ms step_avg:39.77ms
step:839/2330 train_time:33366ms step_avg:39.77ms
step:840/2330 train_time:33411ms step_avg:39.77ms
step:841/2330 train_time:33446ms step_avg:39.77ms
step:842/2330 train_time:33490ms step_avg:39.77ms
step:843/2330 train_time:33525ms step_avg:39.77ms
step:844/2330 train_time:33569ms step_avg:39.77ms
step:845/2330 train_time:33604ms step_avg:39.77ms
step:846/2330 train_time:33649ms step_avg:39.77ms
step:847/2330 train_time:33684ms step_avg:39.77ms
step:848/2330 train_time:33727ms step_avg:39.77ms
step:849/2330 train_time:33763ms step_avg:39.77ms
step:850/2330 train_time:33807ms step_avg:39.77ms
step:851/2330 train_time:33842ms step_avg:39.77ms
step:852/2330 train_time:33887ms step_avg:39.77ms
step:853/2330 train_time:33922ms step_avg:39.77ms
step:854/2330 train_time:33967ms step_avg:39.77ms
step:855/2330 train_time:34001ms step_avg:39.77ms
step:856/2330 train_time:34045ms step_avg:39.77ms
step:857/2330 train_time:34080ms step_avg:39.77ms
step:858/2330 train_time:34124ms step_avg:39.77ms
step:859/2330 train_time:34159ms step_avg:39.77ms
step:860/2330 train_time:34204ms step_avg:39.77ms
step:861/2330 train_time:34239ms step_avg:39.77ms
step:862/2330 train_time:34285ms step_avg:39.77ms
step:863/2330 train_time:34319ms step_avg:39.77ms
step:864/2330 train_time:34364ms step_avg:39.77ms
step:865/2330 train_time:34398ms step_avg:39.77ms
step:866/2330 train_time:34444ms step_avg:39.77ms
step:867/2330 train_time:34478ms step_avg:39.77ms
step:868/2330 train_time:34522ms step_avg:39.77ms
step:869/2330 train_time:34557ms step_avg:39.77ms
step:870/2330 train_time:34602ms step_avg:39.77ms
step:871/2330 train_time:34637ms step_avg:39.77ms
step:872/2330 train_time:34681ms step_avg:39.77ms
step:873/2330 train_time:34715ms step_avg:39.77ms
step:874/2330 train_time:34759ms step_avg:39.77ms
step:875/2330 train_time:34794ms step_avg:39.76ms
step:876/2330 train_time:34838ms step_avg:39.77ms
step:877/2330 train_time:34873ms step_avg:39.76ms
step:878/2330 train_time:34917ms step_avg:39.77ms
step:879/2330 train_time:34952ms step_avg:39.76ms
step:880/2330 train_time:34996ms step_avg:39.77ms
step:881/2330 train_time:35031ms step_avg:39.76ms
step:882/2330 train_time:35075ms step_avg:39.77ms
step:883/2330 train_time:35110ms step_avg:39.76ms
step:884/2330 train_time:35154ms step_avg:39.77ms
step:885/2330 train_time:35190ms step_avg:39.76ms
step:886/2330 train_time:35234ms step_avg:39.77ms
step:887/2330 train_time:35270ms step_avg:39.76ms
step:888/2330 train_time:35315ms step_avg:39.77ms
step:889/2330 train_time:35350ms step_avg:39.76ms
step:890/2330 train_time:35395ms step_avg:39.77ms
step:891/2330 train_time:35430ms step_avg:39.76ms
step:892/2330 train_time:35474ms step_avg:39.77ms
step:893/2330 train_time:35509ms step_avg:39.76ms
step:894/2330 train_time:35554ms step_avg:39.77ms
step:895/2330 train_time:35589ms step_avg:39.76ms
step:896/2330 train_time:35633ms step_avg:39.77ms
step:897/2330 train_time:35669ms step_avg:39.76ms
step:898/2330 train_time:35713ms step_avg:39.77ms
step:899/2330 train_time:35748ms step_avg:39.76ms
step:900/2330 train_time:35792ms step_avg:39.77ms
step:901/2330 train_time:35827ms step_avg:39.76ms
step:902/2330 train_time:35871ms step_avg:39.77ms
step:903/2330 train_time:35906ms step_avg:39.76ms
step:904/2330 train_time:35951ms step_avg:39.77ms
step:905/2330 train_time:35986ms step_avg:39.76ms
step:906/2330 train_time:36030ms step_avg:39.77ms
step:907/2330 train_time:36066ms step_avg:39.76ms
step:908/2330 train_time:36110ms step_avg:39.77ms
step:909/2330 train_time:36146ms step_avg:39.76ms
step:910/2330 train_time:36190ms step_avg:39.77ms
step:911/2330 train_time:36225ms step_avg:39.76ms
step:912/2330 train_time:36270ms step_avg:39.77ms
step:913/2330 train_time:36305ms step_avg:39.76ms
step:914/2330 train_time:36350ms step_avg:39.77ms
step:915/2330 train_time:36385ms step_avg:39.77ms
step:916/2330 train_time:36429ms step_avg:39.77ms
step:917/2330 train_time:36464ms step_avg:39.76ms
step:918/2330 train_time:36509ms step_avg:39.77ms
step:919/2330 train_time:36543ms step_avg:39.76ms
step:920/2330 train_time:36588ms step_avg:39.77ms
step:921/2330 train_time:36624ms step_avg:39.77ms
step:922/2330 train_time:36668ms step_avg:39.77ms
step:923/2330 train_time:36704ms step_avg:39.77ms
step:924/2330 train_time:36748ms step_avg:39.77ms
step:925/2330 train_time:36783ms step_avg:39.77ms
step:926/2330 train_time:36828ms step_avg:39.77ms
step:927/2330 train_time:36863ms step_avg:39.77ms
step:928/2330 train_time:36907ms step_avg:39.77ms
step:929/2330 train_time:36942ms step_avg:39.77ms
step:930/2330 train_time:36987ms step_avg:39.77ms
step:931/2330 train_time:37022ms step_avg:39.77ms
step:932/2330 train_time:37066ms step_avg:39.77ms
step:933/2330 train_time:37101ms step_avg:39.76ms
step:934/2330 train_time:37145ms step_avg:39.77ms
step:935/2330 train_time:37181ms step_avg:39.77ms
step:936/2330 train_time:37225ms step_avg:39.77ms
step:937/2330 train_time:37261ms step_avg:39.77ms
step:938/2330 train_time:37306ms step_avg:39.77ms
step:939/2330 train_time:37341ms step_avg:39.77ms
step:940/2330 train_time:37385ms step_avg:39.77ms
step:941/2330 train_time:37419ms step_avg:39.77ms
step:942/2330 train_time:37463ms step_avg:39.77ms
step:943/2330 train_time:37498ms step_avg:39.76ms
step:944/2330 train_time:37542ms step_avg:39.77ms
step:945/2330 train_time:37578ms step_avg:39.77ms
step:946/2330 train_time:37622ms step_avg:39.77ms
step:947/2330 train_time:37657ms step_avg:39.76ms
step:948/2330 train_time:37701ms step_avg:39.77ms
step:949/2330 train_time:37736ms step_avg:39.76ms
step:950/2330 train_time:37780ms step_avg:39.77ms
step:951/2330 train_time:37815ms step_avg:39.76ms
step:952/2330 train_time:37859ms step_avg:39.77ms
step:953/2330 train_time:37893ms step_avg:39.76ms
step:954/2330 train_time:37937ms step_avg:39.77ms
step:955/2330 train_time:37972ms step_avg:39.76ms
step:956/2330 train_time:38015ms step_avg:39.76ms
step:957/2330 train_time:38051ms step_avg:39.76ms
step:958/2330 train_time:38095ms step_avg:39.76ms
step:959/2330 train_time:38130ms step_avg:39.76ms
step:960/2330 train_time:38175ms step_avg:39.77ms
step:961/2330 train_time:38210ms step_avg:39.76ms
step:962/2330 train_time:38254ms step_avg:39.77ms
step:963/2330 train_time:38290ms step_avg:39.76ms
step:964/2330 train_time:38335ms step_avg:39.77ms
step:965/2330 train_time:38370ms step_avg:39.76ms
step:966/2330 train_time:38415ms step_avg:39.77ms
step:967/2330 train_time:38450ms step_avg:39.76ms
step:968/2330 train_time:38494ms step_avg:39.77ms
step:969/2330 train_time:38529ms step_avg:39.76ms
step:970/2330 train_time:38574ms step_avg:39.77ms
step:971/2330 train_time:38609ms step_avg:39.76ms
step:972/2330 train_time:38654ms step_avg:39.77ms
step:973/2330 train_time:38689ms step_avg:39.76ms
step:974/2330 train_time:38733ms step_avg:39.77ms
step:975/2330 train_time:38769ms step_avg:39.76ms
step:976/2330 train_time:38813ms step_avg:39.77ms
step:977/2330 train_time:38848ms step_avg:39.76ms
step:978/2330 train_time:38892ms step_avg:39.77ms
step:979/2330 train_time:38927ms step_avg:39.76ms
step:980/2330 train_time:38971ms step_avg:39.77ms
step:981/2330 train_time:39006ms step_avg:39.76ms
step:982/2330 train_time:39051ms step_avg:39.77ms
step:983/2330 train_time:39086ms step_avg:39.76ms
step:984/2330 train_time:39131ms step_avg:39.77ms
step:985/2330 train_time:39166ms step_avg:39.76ms
step:986/2330 train_time:39211ms step_avg:39.77ms
step:987/2330 train_time:39246ms step_avg:39.76ms
step:988/2330 train_time:39290ms step_avg:39.77ms
step:989/2330 train_time:39324ms step_avg:39.76ms
step:990/2330 train_time:39369ms step_avg:39.77ms
step:991/2330 train_time:39404ms step_avg:39.76ms
step:992/2330 train_time:39449ms step_avg:39.77ms
step:993/2330 train_time:39485ms step_avg:39.76ms
step:994/2330 train_time:39529ms step_avg:39.77ms
step:995/2330 train_time:39564ms step_avg:39.76ms
step:996/2330 train_time:39609ms step_avg:39.77ms
step:997/2330 train_time:39644ms step_avg:39.76ms
step:998/2330 train_time:39688ms step_avg:39.77ms
step:999/2330 train_time:39723ms step_avg:39.76ms
step:1000/2330 train_time:39768ms step_avg:39.77ms
step:1000/2330 val_loss:5.2010 train_time:39855ms step_avg:39.86ms
step:1001/2330 train_time:39869ms step_avg:39.83ms
step:1002/2330 train_time:39882ms step_avg:39.80ms
step:1003/2330 train_time:39893ms step_avg:39.77ms
step:1004/2330 train_time:39928ms step_avg:39.77ms
step:1005/2330 train_time:39962ms step_avg:39.76ms
step:1006/2330 train_time:40005ms step_avg:39.77ms
step:1007/2330 train_time:40038ms step_avg:39.76ms
step:1008/2330 train_time:40082ms step_avg:39.76ms
step:1009/2330 train_time:40116ms step_avg:39.76ms
step:1010/2330 train_time:40161ms step_avg:39.76ms
step:1011/2330 train_time:40198ms step_avg:39.76ms
step:1012/2330 train_time:40244ms step_avg:39.77ms
step:1013/2330 train_time:40281ms step_avg:39.76ms
step:1014/2330 train_time:40326ms step_avg:39.77ms
step:1015/2330 train_time:40360ms step_avg:39.76ms
step:1016/2330 train_time:40404ms step_avg:39.77ms
step:1017/2330 train_time:40438ms step_avg:39.76ms
step:1018/2330 train_time:40481ms step_avg:39.77ms
step:1019/2330 train_time:40516ms step_avg:39.76ms
step:1020/2330 train_time:40559ms step_avg:39.76ms
step:1021/2330 train_time:40594ms step_avg:39.76ms
step:1022/2330 train_time:40638ms step_avg:39.76ms
step:1023/2330 train_time:40673ms step_avg:39.76ms
step:1024/2330 train_time:40716ms step_avg:39.76ms
step:1025/2330 train_time:40751ms step_avg:39.76ms
step:1026/2330 train_time:40797ms step_avg:39.76ms
step:1027/2330 train_time:40832ms step_avg:39.76ms
step:1028/2330 train_time:40877ms step_avg:39.76ms
step:1029/2330 train_time:40912ms step_avg:39.76ms
step:1030/2330 train_time:40957ms step_avg:39.76ms
step:1031/2330 train_time:40991ms step_avg:39.76ms
step:1032/2330 train_time:41035ms step_avg:39.76ms
step:1033/2330 train_time:41071ms step_avg:39.76ms
step:1034/2330 train_time:41116ms step_avg:39.76ms
step:1035/2330 train_time:41151ms step_avg:39.76ms
step:1036/2330 train_time:41196ms step_avg:39.76ms
step:1037/2330 train_time:41232ms step_avg:39.76ms
step:1038/2330 train_time:41278ms step_avg:39.77ms
step:1039/2330 train_time:41313ms step_avg:39.76ms
step:1040/2330 train_time:41358ms step_avg:39.77ms
step:1041/2330 train_time:41394ms step_avg:39.76ms
step:1042/2330 train_time:41438ms step_avg:39.77ms
step:1043/2330 train_time:41472ms step_avg:39.76ms
step:1044/2330 train_time:41516ms step_avg:39.77ms
step:1045/2330 train_time:41551ms step_avg:39.76ms
step:1046/2330 train_time:41596ms step_avg:39.77ms
step:1047/2330 train_time:41630ms step_avg:39.76ms
step:1048/2330 train_time:41674ms step_avg:39.77ms
step:1049/2330 train_time:41709ms step_avg:39.76ms
step:1050/2330 train_time:41753ms step_avg:39.77ms
step:1051/2330 train_time:41789ms step_avg:39.76ms
step:1052/2330 train_time:41833ms step_avg:39.77ms
step:1053/2330 train_time:41869ms step_avg:39.76ms
step:1054/2330 train_time:41914ms step_avg:39.77ms
step:1055/2330 train_time:41949ms step_avg:39.76ms
step:1056/2330 train_time:41993ms step_avg:39.77ms
step:1057/2330 train_time:42028ms step_avg:39.76ms
step:1058/2330 train_time:42072ms step_avg:39.77ms
step:1059/2330 train_time:42108ms step_avg:39.76ms
step:1060/2330 train_time:42152ms step_avg:39.77ms
step:1061/2330 train_time:42187ms step_avg:39.76ms
step:1062/2330 train_time:42233ms step_avg:39.77ms
step:1063/2330 train_time:42267ms step_avg:39.76ms
step:1064/2330 train_time:42312ms step_avg:39.77ms
step:1065/2330 train_time:42347ms step_avg:39.76ms
step:1066/2330 train_time:42392ms step_avg:39.77ms
step:1067/2330 train_time:42427ms step_avg:39.76ms
step:1068/2330 train_time:42471ms step_avg:39.77ms
step:1069/2330 train_time:42505ms step_avg:39.76ms
step:1070/2330 train_time:42549ms step_avg:39.77ms
step:1071/2330 train_time:42583ms step_avg:39.76ms
step:1072/2330 train_time:42627ms step_avg:39.76ms
step:1073/2330 train_time:42662ms step_avg:39.76ms
step:1074/2330 train_time:42705ms step_avg:39.76ms
step:1075/2330 train_time:42739ms step_avg:39.76ms
step:1076/2330 train_time:42783ms step_avg:39.76ms
step:1077/2330 train_time:42819ms step_avg:39.76ms
step:1078/2330 train_time:42863ms step_avg:39.76ms
step:1079/2330 train_time:42897ms step_avg:39.76ms
step:1080/2330 train_time:42941ms step_avg:39.76ms
step:1081/2330 train_time:42977ms step_avg:39.76ms
step:1082/2330 train_time:43022ms step_avg:39.76ms
step:1083/2330 train_time:43058ms step_avg:39.76ms
step:1084/2330 train_time:43102ms step_avg:39.76ms
step:1085/2330 train_time:43138ms step_avg:39.76ms
step:1086/2330 train_time:43182ms step_avg:39.76ms
step:1087/2330 train_time:43218ms step_avg:39.76ms
step:1088/2330 train_time:43262ms step_avg:39.76ms
step:1089/2330 train_time:43297ms step_avg:39.76ms
step:1090/2330 train_time:43342ms step_avg:39.76ms
step:1091/2330 train_time:43377ms step_avg:39.76ms
step:1092/2330 train_time:43421ms step_avg:39.76ms
step:1093/2330 train_time:43456ms step_avg:39.76ms
step:1094/2330 train_time:43500ms step_avg:39.76ms
step:1095/2330 train_time:43535ms step_avg:39.76ms
step:1096/2330 train_time:43579ms step_avg:39.76ms
step:1097/2330 train_time:43614ms step_avg:39.76ms
step:1098/2330 train_time:43658ms step_avg:39.76ms
step:1099/2330 train_time:43694ms step_avg:39.76ms
step:1100/2330 train_time:43738ms step_avg:39.76ms
step:1101/2330 train_time:43773ms step_avg:39.76ms
step:1102/2330 train_time:43817ms step_avg:39.76ms
step:1103/2330 train_time:43852ms step_avg:39.76ms
step:1104/2330 train_time:43896ms step_avg:39.76ms
step:1105/2330 train_time:43931ms step_avg:39.76ms
step:1106/2330 train_time:43976ms step_avg:39.76ms
step:1107/2330 train_time:44011ms step_avg:39.76ms
step:1108/2330 train_time:44056ms step_avg:39.76ms
step:1109/2330 train_time:44092ms step_avg:39.76ms
step:1110/2330 train_time:44136ms step_avg:39.76ms
step:1111/2330 train_time:44172ms step_avg:39.76ms
step:1112/2330 train_time:44217ms step_avg:39.76ms
step:1113/2330 train_time:44252ms step_avg:39.76ms
step:1114/2330 train_time:44297ms step_avg:39.76ms
step:1115/2330 train_time:44332ms step_avg:39.76ms
step:1116/2330 train_time:44376ms step_avg:39.76ms
step:1117/2330 train_time:44411ms step_avg:39.76ms
step:1118/2330 train_time:44456ms step_avg:39.76ms
step:1119/2330 train_time:44490ms step_avg:39.76ms
step:1120/2330 train_time:44534ms step_avg:39.76ms
step:1121/2330 train_time:44569ms step_avg:39.76ms
step:1122/2330 train_time:44614ms step_avg:39.76ms
step:1123/2330 train_time:44649ms step_avg:39.76ms
step:1124/2330 train_time:44693ms step_avg:39.76ms
step:1125/2330 train_time:44727ms step_avg:39.76ms
step:1126/2330 train_time:44772ms step_avg:39.76ms
step:1127/2330 train_time:44808ms step_avg:39.76ms
step:1128/2330 train_time:44852ms step_avg:39.76ms
step:1129/2330 train_time:44887ms step_avg:39.76ms
step:1130/2330 train_time:44930ms step_avg:39.76ms
step:1131/2330 train_time:44966ms step_avg:39.76ms
step:1132/2330 train_time:45009ms step_avg:39.76ms
step:1133/2330 train_time:45044ms step_avg:39.76ms
step:1134/2330 train_time:45088ms step_avg:39.76ms
step:1135/2330 train_time:45123ms step_avg:39.76ms
step:1136/2330 train_time:45168ms step_avg:39.76ms
step:1137/2330 train_time:45204ms step_avg:39.76ms
step:1138/2330 train_time:45247ms step_avg:39.76ms
step:1139/2330 train_time:45282ms step_avg:39.76ms
step:1140/2330 train_time:45327ms step_avg:39.76ms
step:1141/2330 train_time:45362ms step_avg:39.76ms
step:1142/2330 train_time:45406ms step_avg:39.76ms
step:1143/2330 train_time:45440ms step_avg:39.76ms
step:1144/2330 train_time:45484ms step_avg:39.76ms
step:1145/2330 train_time:45519ms step_avg:39.75ms
step:1146/2330 train_time:45563ms step_avg:39.76ms
step:1147/2330 train_time:45598ms step_avg:39.75ms
step:1148/2330 train_time:45641ms step_avg:39.76ms
step:1149/2330 train_time:45676ms step_avg:39.75ms
step:1150/2330 train_time:45721ms step_avg:39.76ms
step:1151/2330 train_time:45756ms step_avg:39.75ms
step:1152/2330 train_time:45800ms step_avg:39.76ms
step:1153/2330 train_time:45835ms step_avg:39.75ms
step:1154/2330 train_time:45880ms step_avg:39.76ms
step:1155/2330 train_time:45916ms step_avg:39.75ms
step:1156/2330 train_time:45960ms step_avg:39.76ms
step:1157/2330 train_time:45995ms step_avg:39.75ms
step:1158/2330 train_time:46040ms step_avg:39.76ms
step:1159/2330 train_time:46075ms step_avg:39.75ms
step:1160/2330 train_time:46120ms step_avg:39.76ms
step:1161/2330 train_time:46155ms step_avg:39.75ms
step:1162/2330 train_time:46200ms step_avg:39.76ms
step:1163/2330 train_time:46236ms step_avg:39.76ms
step:1164/2330 train_time:46280ms step_avg:39.76ms
step:1165/2330 train_time:46316ms step_avg:39.76ms
step:1166/2330 train_time:46360ms step_avg:39.76ms
step:1167/2330 train_time:46395ms step_avg:39.76ms
step:1168/2330 train_time:46440ms step_avg:39.76ms
step:1169/2330 train_time:46475ms step_avg:39.76ms
step:1170/2330 train_time:46520ms step_avg:39.76ms
step:1171/2330 train_time:46555ms step_avg:39.76ms
step:1172/2330 train_time:46600ms step_avg:39.76ms
step:1173/2330 train_time:46635ms step_avg:39.76ms
step:1174/2330 train_time:46679ms step_avg:39.76ms
step:1175/2330 train_time:46714ms step_avg:39.76ms
step:1176/2330 train_time:46758ms step_avg:39.76ms
step:1177/2330 train_time:46793ms step_avg:39.76ms
step:1178/2330 train_time:46838ms step_avg:39.76ms
step:1179/2330 train_time:46873ms step_avg:39.76ms
step:1180/2330 train_time:46917ms step_avg:39.76ms
step:1181/2330 train_time:46953ms step_avg:39.76ms
step:1182/2330 train_time:46997ms step_avg:39.76ms
step:1183/2330 train_time:47032ms step_avg:39.76ms
step:1184/2330 train_time:47077ms step_avg:39.76ms
step:1185/2330 train_time:47111ms step_avg:39.76ms
step:1186/2330 train_time:47157ms step_avg:39.76ms
step:1187/2330 train_time:47192ms step_avg:39.76ms
step:1188/2330 train_time:47236ms step_avg:39.76ms
step:1189/2330 train_time:47271ms step_avg:39.76ms
step:1190/2330 train_time:47315ms step_avg:39.76ms
step:1191/2330 train_time:47350ms step_avg:39.76ms
step:1192/2330 train_time:47395ms step_avg:39.76ms
step:1193/2330 train_time:47430ms step_avg:39.76ms
step:1194/2330 train_time:47475ms step_avg:39.76ms
step:1195/2330 train_time:47510ms step_avg:39.76ms
step:1196/2330 train_time:47555ms step_avg:39.76ms
step:1197/2330 train_time:47590ms step_avg:39.76ms
step:1198/2330 train_time:47635ms step_avg:39.76ms
step:1199/2330 train_time:47670ms step_avg:39.76ms
step:1200/2330 train_time:47714ms step_avg:39.76ms
step:1201/2330 train_time:47750ms step_avg:39.76ms
step:1202/2330 train_time:47794ms step_avg:39.76ms
step:1203/2330 train_time:47829ms step_avg:39.76ms
step:1204/2330 train_time:47874ms step_avg:39.76ms
step:1205/2330 train_time:47910ms step_avg:39.76ms
step:1206/2330 train_time:47954ms step_avg:39.76ms
step:1207/2330 train_time:47989ms step_avg:39.76ms
step:1208/2330 train_time:48034ms step_avg:39.76ms
step:1209/2330 train_time:48069ms step_avg:39.76ms
step:1210/2330 train_time:48113ms step_avg:39.76ms
step:1211/2330 train_time:48148ms step_avg:39.76ms
step:1212/2330 train_time:48193ms step_avg:39.76ms
step:1213/2330 train_time:48228ms step_avg:39.76ms
step:1214/2330 train_time:48272ms step_avg:39.76ms
step:1215/2330 train_time:48307ms step_avg:39.76ms
step:1216/2330 train_time:48352ms step_avg:39.76ms
step:1217/2330 train_time:48386ms step_avg:39.76ms
step:1218/2330 train_time:48431ms step_avg:39.76ms
step:1219/2330 train_time:48465ms step_avg:39.76ms
step:1220/2330 train_time:48509ms step_avg:39.76ms
step:1221/2330 train_time:48545ms step_avg:39.76ms
step:1222/2330 train_time:48588ms step_avg:39.76ms
step:1223/2330 train_time:48623ms step_avg:39.76ms
step:1224/2330 train_time:48666ms step_avg:39.76ms
step:1225/2330 train_time:48701ms step_avg:39.76ms
step:1226/2330 train_time:48744ms step_avg:39.76ms
step:1227/2330 train_time:48779ms step_avg:39.75ms
step:1228/2330 train_time:48823ms step_avg:39.76ms
step:1229/2330 train_time:48858ms step_avg:39.75ms
step:1230/2330 train_time:48902ms step_avg:39.76ms
step:1231/2330 train_time:48937ms step_avg:39.75ms
step:1232/2330 train_time:48981ms step_avg:39.76ms
step:1233/2330 train_time:49017ms step_avg:39.75ms
step:1234/2330 train_time:49061ms step_avg:39.76ms
step:1235/2330 train_time:49097ms step_avg:39.75ms
step:1236/2330 train_time:49141ms step_avg:39.76ms
step:1237/2330 train_time:49177ms step_avg:39.75ms
step:1238/2330 train_time:49222ms step_avg:39.76ms
step:1239/2330 train_time:49258ms step_avg:39.76ms
step:1240/2330 train_time:49302ms step_avg:39.76ms
step:1241/2330 train_time:49338ms step_avg:39.76ms
step:1242/2330 train_time:49382ms step_avg:39.76ms
step:1243/2330 train_time:49417ms step_avg:39.76ms
step:1244/2330 train_time:49462ms step_avg:39.76ms
step:1245/2330 train_time:49497ms step_avg:39.76ms
step:1246/2330 train_time:49541ms step_avg:39.76ms
step:1247/2330 train_time:49576ms step_avg:39.76ms
step:1248/2330 train_time:49620ms step_avg:39.76ms
step:1249/2330 train_time:49655ms step_avg:39.76ms
step:1250/2330 train_time:49699ms step_avg:39.76ms
step:1250/2330 val_loss:5.1743 train_time:49786ms step_avg:39.83ms
step:1251/2330 train_time:49801ms step_avg:39.81ms
step:1252/2330 train_time:49816ms step_avg:39.79ms
step:1253/2330 train_time:49826ms step_avg:39.77ms
step:1254/2330 train_time:49860ms step_avg:39.76ms
step:1255/2330 train_time:49894ms step_avg:39.76ms
step:1256/2330 train_time:49938ms step_avg:39.76ms
step:1257/2330 train_time:49972ms step_avg:39.75ms
step:1258/2330 train_time:50015ms step_avg:39.76ms
step:1259/2330 train_time:50049ms step_avg:39.75ms
step:1260/2330 train_time:50093ms step_avg:39.76ms
step:1261/2330 train_time:50133ms step_avg:39.76ms
step:1262/2330 train_time:50181ms step_avg:39.76ms
step:1263/2330 train_time:50217ms step_avg:39.76ms
step:1264/2330 train_time:50262ms step_avg:39.76ms
step:1265/2330 train_time:50296ms step_avg:39.76ms
step:1266/2330 train_time:50339ms step_avg:39.76ms
step:1267/2330 train_time:50375ms step_avg:39.76ms
step:1268/2330 train_time:50419ms step_avg:39.76ms
step:1269/2330 train_time:50738ms step_avg:39.98ms
step:1270/2330 train_time:50752ms step_avg:39.96ms
step:1271/2330 train_time:50764ms step_avg:39.94ms
step:1272/2330 train_time:50790ms step_avg:39.93ms
step:1273/2330 train_time:50824ms step_avg:39.92ms
step:1274/2330 train_time:50866ms step_avg:39.93ms
step:1275/2330 train_time:50900ms step_avg:39.92ms
step:1276/2330 train_time:50943ms step_avg:39.92ms
step:1277/2330 train_time:50978ms step_avg:39.92ms
step:1278/2330 train_time:51022ms step_avg:39.92ms
step:1279/2330 train_time:51056ms step_avg:39.92ms
step:1280/2330 train_time:51100ms step_avg:39.92ms
step:1281/2330 train_time:51134ms step_avg:39.92ms
step:1282/2330 train_time:51178ms step_avg:39.92ms
step:1283/2330 train_time:51212ms step_avg:39.92ms
step:1284/2330 train_time:51255ms step_avg:39.92ms
step:1285/2330 train_time:51289ms step_avg:39.91ms
step:1286/2330 train_time:51333ms step_avg:39.92ms
step:1287/2330 train_time:51368ms step_avg:39.91ms
step:1288/2330 train_time:51411ms step_avg:39.92ms
step:1289/2330 train_time:51444ms step_avg:39.91ms
step:1290/2330 train_time:51488ms step_avg:39.91ms
step:1291/2330 train_time:51522ms step_avg:39.91ms
step:1292/2330 train_time:51565ms step_avg:39.91ms
step:1293/2330 train_time:51600ms step_avg:39.91ms
step:1294/2330 train_time:51647ms step_avg:39.91ms
step:1295/2330 train_time:51684ms step_avg:39.91ms
step:1296/2330 train_time:51732ms step_avg:39.92ms
step:1297/2330 train_time:51769ms step_avg:39.91ms
step:1298/2330 train_time:51815ms step_avg:39.92ms
step:1299/2330 train_time:51851ms step_avg:39.92ms
step:1300/2330 train_time:51895ms step_avg:39.92ms
step:1301/2330 train_time:51930ms step_avg:39.92ms
step:1302/2330 train_time:51974ms step_avg:39.92ms
step:1303/2330 train_time:52009ms step_avg:39.91ms
step:1304/2330 train_time:52053ms step_avg:39.92ms
step:1305/2330 train_time:52088ms step_avg:39.91ms
step:1306/2330 train_time:52131ms step_avg:39.92ms
step:1307/2330 train_time:52165ms step_avg:39.91ms
step:1308/2330 train_time:52208ms step_avg:39.91ms
step:1309/2330 train_time:52242ms step_avg:39.91ms
step:1310/2330 train_time:52286ms step_avg:39.91ms
step:1311/2330 train_time:52320ms step_avg:39.91ms
step:1312/2330 train_time:52364ms step_avg:39.91ms
step:1313/2330 train_time:52399ms step_avg:39.91ms
step:1314/2330 train_time:52443ms step_avg:39.91ms
step:1315/2330 train_time:52477ms step_avg:39.91ms
step:1316/2330 train_time:52521ms step_avg:39.91ms
step:1317/2330 train_time:52555ms step_avg:39.91ms
step:1318/2330 train_time:52600ms step_avg:39.91ms
step:1319/2330 train_time:52637ms step_avg:39.91ms
step:1320/2330 train_time:52683ms step_avg:39.91ms
step:1321/2330 train_time:52719ms step_avg:39.91ms
step:1322/2330 train_time:52765ms step_avg:39.91ms
step:1323/2330 train_time:52801ms step_avg:39.91ms
step:1324/2330 train_time:52846ms step_avg:39.91ms
step:1325/2330 train_time:52882ms step_avg:39.91ms
step:1326/2330 train_time:52927ms step_avg:39.91ms
step:1327/2330 train_time:52962ms step_avg:39.91ms
step:1328/2330 train_time:53006ms step_avg:39.91ms
step:1329/2330 train_time:53042ms step_avg:39.91ms
step:1330/2330 train_time:53086ms step_avg:39.91ms
step:1331/2330 train_time:53121ms step_avg:39.91ms
step:1332/2330 train_time:53165ms step_avg:39.91ms
step:1333/2330 train_time:53200ms step_avg:39.91ms
step:1334/2330 train_time:53244ms step_avg:39.91ms
step:1335/2330 train_time:53279ms step_avg:39.91ms
step:1336/2330 train_time:53323ms step_avg:39.91ms
step:1337/2330 train_time:53357ms step_avg:39.91ms
step:1338/2330 train_time:53401ms step_avg:39.91ms
step:1339/2330 train_time:53435ms step_avg:39.91ms
step:1340/2330 train_time:53479ms step_avg:39.91ms
step:1341/2330 train_time:53514ms step_avg:39.91ms
step:1342/2330 train_time:53558ms step_avg:39.91ms
step:1343/2330 train_time:53593ms step_avg:39.91ms
step:1344/2330 train_time:53638ms step_avg:39.91ms
step:1345/2330 train_time:53674ms step_avg:39.91ms
step:1346/2330 train_time:53719ms step_avg:39.91ms
step:1347/2330 train_time:53754ms step_avg:39.91ms
step:1348/2330 train_time:53799ms step_avg:39.91ms
step:1349/2330 train_time:53834ms step_avg:39.91ms
step:1350/2330 train_time:53881ms step_avg:39.91ms
step:1351/2330 train_time:53916ms step_avg:39.91ms
step:1352/2330 train_time:53962ms step_avg:39.91ms
step:1353/2330 train_time:53997ms step_avg:39.91ms
step:1354/2330 train_time:54040ms step_avg:39.91ms
step:1355/2330 train_time:54076ms step_avg:39.91ms
step:1356/2330 train_time:54121ms step_avg:39.91ms
step:1357/2330 train_time:54155ms step_avg:39.91ms
step:1358/2330 train_time:54201ms step_avg:39.91ms
step:1359/2330 train_time:54235ms step_avg:39.91ms
step:1360/2330 train_time:54279ms step_avg:39.91ms
step:1361/2330 train_time:54314ms step_avg:39.91ms
step:1362/2330 train_time:54357ms step_avg:39.91ms
step:1363/2330 train_time:54391ms step_avg:39.91ms
step:1364/2330 train_time:54435ms step_avg:39.91ms
step:1365/2330 train_time:54470ms step_avg:39.90ms
step:1366/2330 train_time:54514ms step_avg:39.91ms
step:1367/2330 train_time:54548ms step_avg:39.90ms
step:1368/2330 train_time:54592ms step_avg:39.91ms
step:1369/2330 train_time:54627ms step_avg:39.90ms
step:1370/2330 train_time:54672ms step_avg:39.91ms
step:1371/2330 train_time:54706ms step_avg:39.90ms
step:1372/2330 train_time:54752ms step_avg:39.91ms
step:1373/2330 train_time:54787ms step_avg:39.90ms
step:1374/2330 train_time:54832ms step_avg:39.91ms
step:1375/2330 train_time:54867ms step_avg:39.90ms
step:1376/2330 train_time:54911ms step_avg:39.91ms
step:1377/2330 train_time:54946ms step_avg:39.90ms
step:1378/2330 train_time:54990ms step_avg:39.91ms
step:1379/2330 train_time:55025ms step_avg:39.90ms
step:1380/2330 train_time:55068ms step_avg:39.90ms
step:1381/2330 train_time:55103ms step_avg:39.90ms
step:1382/2330 train_time:55147ms step_avg:39.90ms
step:1383/2330 train_time:55183ms step_avg:39.90ms
step:1384/2330 train_time:55227ms step_avg:39.90ms
step:1385/2330 train_time:55262ms step_avg:39.90ms
step:1386/2330 train_time:55306ms step_avg:39.90ms
step:1387/2330 train_time:55341ms step_avg:39.90ms
step:1388/2330 train_time:55385ms step_avg:39.90ms
step:1389/2330 train_time:55420ms step_avg:39.90ms
step:1390/2330 train_time:55465ms step_avg:39.90ms
step:1391/2330 train_time:55500ms step_avg:39.90ms
step:1392/2330 train_time:55544ms step_avg:39.90ms
step:1393/2330 train_time:55580ms step_avg:39.90ms
step:1394/2330 train_time:55624ms step_avg:39.90ms
step:1395/2330 train_time:55661ms step_avg:39.90ms
step:1396/2330 train_time:55705ms step_avg:39.90ms
step:1397/2330 train_time:55740ms step_avg:39.90ms
step:1398/2330 train_time:55785ms step_avg:39.90ms
step:1399/2330 train_time:55821ms step_avg:39.90ms
step:1400/2330 train_time:55866ms step_avg:39.90ms
step:1401/2330 train_time:55901ms step_avg:39.90ms
step:1402/2330 train_time:55945ms step_avg:39.90ms
step:1403/2330 train_time:55980ms step_avg:39.90ms
step:1404/2330 train_time:56025ms step_avg:39.90ms
step:1405/2330 train_time:56060ms step_avg:39.90ms
step:1406/2330 train_time:56105ms step_avg:39.90ms
step:1407/2330 train_time:56139ms step_avg:39.90ms
step:1408/2330 train_time:56184ms step_avg:39.90ms
step:1409/2330 train_time:56219ms step_avg:39.90ms
step:1410/2330 train_time:56264ms step_avg:39.90ms
step:1411/2330 train_time:56299ms step_avg:39.90ms
step:1412/2330 train_time:56344ms step_avg:39.90ms
step:1413/2330 train_time:56379ms step_avg:39.90ms
step:1414/2330 train_time:56423ms step_avg:39.90ms
step:1415/2330 train_time:56458ms step_avg:39.90ms
step:1416/2330 train_time:56502ms step_avg:39.90ms
step:1417/2330 train_time:56537ms step_avg:39.90ms
step:1418/2330 train_time:56582ms step_avg:39.90ms
step:1419/2330 train_time:56616ms step_avg:39.90ms
step:1420/2330 train_time:56661ms step_avg:39.90ms
step:1421/2330 train_time:56696ms step_avg:39.90ms
step:1422/2330 train_time:56741ms step_avg:39.90ms
step:1423/2330 train_time:56777ms step_avg:39.90ms
step:1424/2330 train_time:56822ms step_avg:39.90ms
step:1425/2330 train_time:56858ms step_avg:39.90ms
step:1426/2330 train_time:56902ms step_avg:39.90ms
step:1427/2330 train_time:56937ms step_avg:39.90ms
step:1428/2330 train_time:56980ms step_avg:39.90ms
step:1429/2330 train_time:57016ms step_avg:39.90ms
step:1430/2330 train_time:57061ms step_avg:39.90ms
step:1431/2330 train_time:57096ms step_avg:39.90ms
step:1432/2330 train_time:57141ms step_avg:39.90ms
step:1433/2330 train_time:57176ms step_avg:39.90ms
step:1434/2330 train_time:57220ms step_avg:39.90ms
step:1435/2330 train_time:57254ms step_avg:39.90ms
step:1436/2330 train_time:57298ms step_avg:39.90ms
step:1437/2330 train_time:57333ms step_avg:39.90ms
step:1438/2330 train_time:57377ms step_avg:39.90ms
step:1439/2330 train_time:57412ms step_avg:39.90ms
step:1440/2330 train_time:57456ms step_avg:39.90ms
step:1441/2330 train_time:57492ms step_avg:39.90ms
step:1442/2330 train_time:57536ms step_avg:39.90ms
step:1443/2330 train_time:57570ms step_avg:39.90ms
step:1444/2330 train_time:57614ms step_avg:39.90ms
step:1445/2330 train_time:57649ms step_avg:39.90ms
step:1446/2330 train_time:57694ms step_avg:39.90ms
step:1447/2330 train_time:57729ms step_avg:39.90ms
step:1448/2330 train_time:57774ms step_avg:39.90ms
step:1449/2330 train_time:57809ms step_avg:39.90ms
step:1450/2330 train_time:57854ms step_avg:39.90ms
step:1451/2330 train_time:57888ms step_avg:39.90ms
step:1452/2330 train_time:57933ms step_avg:39.90ms
step:1453/2330 train_time:57968ms step_avg:39.90ms
step:1454/2330 train_time:58012ms step_avg:39.90ms
step:1455/2330 train_time:58046ms step_avg:39.89ms
step:1456/2330 train_time:58090ms step_avg:39.90ms
step:1457/2330 train_time:58125ms step_avg:39.89ms
step:1458/2330 train_time:58168ms step_avg:39.90ms
step:1459/2330 train_time:58203ms step_avg:39.89ms
step:1460/2330 train_time:58247ms step_avg:39.90ms
step:1461/2330 train_time:58282ms step_avg:39.89ms
step:1462/2330 train_time:58327ms step_avg:39.90ms
step:1463/2330 train_time:58362ms step_avg:39.89ms
step:1464/2330 train_time:58406ms step_avg:39.89ms
step:1465/2330 train_time:58441ms step_avg:39.89ms
step:1466/2330 train_time:58486ms step_avg:39.89ms
step:1467/2330 train_time:58521ms step_avg:39.89ms
step:1468/2330 train_time:58565ms step_avg:39.89ms
step:1469/2330 train_time:58600ms step_avg:39.89ms
step:1470/2330 train_time:58645ms step_avg:39.89ms
step:1471/2330 train_time:58681ms step_avg:39.89ms
step:1472/2330 train_time:58725ms step_avg:39.89ms
step:1473/2330 train_time:58760ms step_avg:39.89ms
step:1474/2330 train_time:58804ms step_avg:39.89ms
step:1475/2330 train_time:58841ms step_avg:39.89ms
step:1476/2330 train_time:58885ms step_avg:39.89ms
step:1477/2330 train_time:58920ms step_avg:39.89ms
step:1478/2330 train_time:58965ms step_avg:39.89ms
step:1479/2330 train_time:59000ms step_avg:39.89ms
step:1480/2330 train_time:59044ms step_avg:39.89ms
step:1481/2330 train_time:59079ms step_avg:39.89ms
step:1482/2330 train_time:59124ms step_avg:39.89ms
step:1483/2330 train_time:59158ms step_avg:39.89ms
step:1484/2330 train_time:59203ms step_avg:39.89ms
step:1485/2330 train_time:59238ms step_avg:39.89ms
step:1486/2330 train_time:59282ms step_avg:39.89ms
step:1487/2330 train_time:59317ms step_avg:39.89ms
step:1488/2330 train_time:59362ms step_avg:39.89ms
step:1489/2330 train_time:59397ms step_avg:39.89ms
step:1490/2330 train_time:59441ms step_avg:39.89ms
step:1491/2330 train_time:59477ms step_avg:39.89ms
step:1492/2330 train_time:59522ms step_avg:39.89ms
step:1493/2330 train_time:59557ms step_avg:39.89ms
step:1494/2330 train_time:59601ms step_avg:39.89ms
step:1495/2330 train_time:59636ms step_avg:39.89ms
step:1496/2330 train_time:59680ms step_avg:39.89ms
step:1497/2330 train_time:59715ms step_avg:39.89ms
step:1498/2330 train_time:59760ms step_avg:39.89ms
step:1499/2330 train_time:59795ms step_avg:39.89ms
step:1500/2330 train_time:59840ms step_avg:39.89ms
step:1500/2330 val_loss:5.1562 train_time:59928ms step_avg:39.95ms
step:1501/2330 train_time:59941ms step_avg:39.93ms
step:1502/2330 train_time:59955ms step_avg:39.92ms
step:1503/2330 train_time:59966ms step_avg:39.90ms
step:1504/2330 train_time:60001ms step_avg:39.89ms
step:1505/2330 train_time:60035ms step_avg:39.89ms
step:1506/2330 train_time:60078ms step_avg:39.89ms
step:1507/2330 train_time:60112ms step_avg:39.89ms
step:1508/2330 train_time:60156ms step_avg:39.89ms
step:1509/2330 train_time:60190ms step_avg:39.89ms
step:1510/2330 train_time:60235ms step_avg:39.89ms
step:1511/2330 train_time:60275ms step_avg:39.89ms
step:1512/2330 train_time:60323ms step_avg:39.90ms
step:1513/2330 train_time:60358ms step_avg:39.89ms
step:1514/2330 train_time:60403ms step_avg:39.90ms
step:1515/2330 train_time:60437ms step_avg:39.89ms
step:1516/2330 train_time:60481ms step_avg:39.90ms
step:1517/2330 train_time:60517ms step_avg:39.89ms
step:1518/2330 train_time:60560ms step_avg:39.89ms
step:1519/2330 train_time:60594ms step_avg:39.89ms
step:1520/2330 train_time:60639ms step_avg:39.89ms
step:1521/2330 train_time:60674ms step_avg:39.89ms
step:1522/2330 train_time:60718ms step_avg:39.89ms
step:1523/2330 train_time:60752ms step_avg:39.89ms
step:1524/2330 train_time:60796ms step_avg:39.89ms
step:1525/2330 train_time:60830ms step_avg:39.89ms
step:1526/2330 train_time:60874ms step_avg:39.89ms
step:1527/2330 train_time:60909ms step_avg:39.89ms
step:1528/2330 train_time:60953ms step_avg:39.89ms
step:1529/2330 train_time:61164ms step_avg:40.00ms
step:1530/2330 train_time:61206ms step_avg:40.00ms
step:1531/2330 train_time:61239ms step_avg:40.00ms
step:1532/2330 train_time:61283ms step_avg:40.00ms
step:1533/2330 train_time:61317ms step_avg:40.00ms
step:1534/2330 train_time:61361ms step_avg:40.00ms
step:1535/2330 train_time:61395ms step_avg:40.00ms
step:1536/2330 train_time:61439ms step_avg:40.00ms
step:1537/2330 train_time:61473ms step_avg:40.00ms
step:1538/2330 train_time:61516ms step_avg:40.00ms
step:1539/2330 train_time:61550ms step_avg:39.99ms
step:1540/2330 train_time:61593ms step_avg:40.00ms
step:1541/2330 train_time:61627ms step_avg:39.99ms
step:1542/2330 train_time:61670ms step_avg:39.99ms
step:1543/2330 train_time:61704ms step_avg:39.99ms
step:1544/2330 train_time:61748ms step_avg:39.99ms
step:1545/2330 train_time:61782ms step_avg:39.99ms
step:1546/2330 train_time:61826ms step_avg:39.99ms
step:1547/2330 train_time:61860ms step_avg:39.99ms
step:1548/2330 train_time:61903ms step_avg:39.99ms
step:1549/2330 train_time:61938ms step_avg:39.99ms
step:1550/2330 train_time:61982ms step_avg:39.99ms
step:1551/2330 train_time:62020ms step_avg:39.99ms
step:1552/2330 train_time:62069ms step_avg:39.99ms
step:1553/2330 train_time:62107ms step_avg:39.99ms
step:1554/2330 train_time:62154ms step_avg:40.00ms
step:1555/2330 train_time:62190ms step_avg:39.99ms
step:1556/2330 train_time:62234ms step_avg:40.00ms
step:1557/2330 train_time:62270ms step_avg:39.99ms
step:1558/2330 train_time:62314ms step_avg:40.00ms
step:1559/2330 train_time:62348ms step_avg:39.99ms
step:1560/2330 train_time:62392ms step_avg:39.99ms
step:1561/2330 train_time:62426ms step_avg:39.99ms
step:1562/2330 train_time:62470ms step_avg:39.99ms
step:1563/2330 train_time:62503ms step_avg:39.99ms
step:1564/2330 train_time:62547ms step_avg:39.99ms
step:1565/2330 train_time:62582ms step_avg:39.99ms
step:1566/2330 train_time:62626ms step_avg:39.99ms
step:1567/2330 train_time:62660ms step_avg:39.99ms
step:1568/2330 train_time:62704ms step_avg:39.99ms
step:1569/2330 train_time:62739ms step_avg:39.99ms
step:1570/2330 train_time:62783ms step_avg:39.99ms
step:1571/2330 train_time:62818ms step_avg:39.99ms
step:1572/2330 train_time:62862ms step_avg:39.99ms
step:1573/2330 train_time:62896ms step_avg:39.98ms
step:1574/2330 train_time:62941ms step_avg:39.99ms
step:1575/2330 train_time:62976ms step_avg:39.98ms
step:1576/2330 train_time:63024ms step_avg:39.99ms
step:1577/2330 train_time:63059ms step_avg:39.99ms
step:1578/2330 train_time:63105ms step_avg:39.99ms
step:1579/2330 train_time:63142ms step_avg:39.99ms
step:1580/2330 train_time:63187ms step_avg:39.99ms
step:1581/2330 train_time:63223ms step_avg:39.99ms
step:1582/2330 train_time:63268ms step_avg:39.99ms
step:1583/2330 train_time:63303ms step_avg:39.99ms
step:1584/2330 train_time:63348ms step_avg:39.99ms
step:1585/2330 train_time:63383ms step_avg:39.99ms
step:1586/2330 train_time:63427ms step_avg:39.99ms
step:1587/2330 train_time:63462ms step_avg:39.99ms
step:1588/2330 train_time:63505ms step_avg:39.99ms
step:1589/2330 train_time:63540ms step_avg:39.99ms
step:1590/2330 train_time:63584ms step_avg:39.99ms
step:1591/2330 train_time:63619ms step_avg:39.99ms
step:1592/2330 train_time:63662ms step_avg:39.99ms
step:1593/2330 train_time:63697ms step_avg:39.99ms
step:1594/2330 train_time:63741ms step_avg:39.99ms
step:1595/2330 train_time:63776ms step_avg:39.98ms
step:1596/2330 train_time:63819ms step_avg:39.99ms
step:1597/2330 train_time:63854ms step_avg:39.98ms
step:1598/2330 train_time:63898ms step_avg:39.99ms
step:1599/2330 train_time:63933ms step_avg:39.98ms
step:1600/2330 train_time:63977ms step_avg:39.99ms
step:1601/2330 train_time:64013ms step_avg:39.98ms
step:1602/2330 train_time:64058ms step_avg:39.99ms
step:1603/2330 train_time:64094ms step_avg:39.98ms
step:1604/2330 train_time:64139ms step_avg:39.99ms
step:1605/2330 train_time:64175ms step_avg:39.98ms
step:1606/2330 train_time:64220ms step_avg:39.99ms
step:1607/2330 train_time:64256ms step_avg:39.99ms
step:1608/2330 train_time:64301ms step_avg:39.99ms
step:1609/2330 train_time:64336ms step_avg:39.99ms
step:1610/2330 train_time:64381ms step_avg:39.99ms
step:1611/2330 train_time:64416ms step_avg:39.99ms
step:1612/2330 train_time:64460ms step_avg:39.99ms
step:1613/2330 train_time:64495ms step_avg:39.98ms
step:1614/2330 train_time:64540ms step_avg:39.99ms
step:1615/2330 train_time:64574ms step_avg:39.98ms
step:1616/2330 train_time:64618ms step_avg:39.99ms
step:1617/2330 train_time:64653ms step_avg:39.98ms
step:1618/2330 train_time:64697ms step_avg:39.99ms
step:1619/2330 train_time:64731ms step_avg:39.98ms
step:1620/2330 train_time:64775ms step_avg:39.98ms
step:1621/2330 train_time:64809ms step_avg:39.98ms
step:1622/2330 train_time:64853ms step_avg:39.98ms
step:1623/2330 train_time:64888ms step_avg:39.98ms
step:1624/2330 train_time:64933ms step_avg:39.98ms
step:1625/2330 train_time:64968ms step_avg:39.98ms
step:1626/2330 train_time:65013ms step_avg:39.98ms
step:1627/2330 train_time:65049ms step_avg:39.98ms
step:1628/2330 train_time:65094ms step_avg:39.98ms
step:1629/2330 train_time:65129ms step_avg:39.98ms
step:1630/2330 train_time:65173ms step_avg:39.98ms
step:1631/2330 train_time:65209ms step_avg:39.98ms
step:1632/2330 train_time:65254ms step_avg:39.98ms
step:1633/2330 train_time:65289ms step_avg:39.98ms
step:1634/2330 train_time:65333ms step_avg:39.98ms
step:1635/2330 train_time:65368ms step_avg:39.98ms
step:1636/2330 train_time:65412ms step_avg:39.98ms
step:1637/2330 train_time:65447ms step_avg:39.98ms
step:1638/2330 train_time:65490ms step_avg:39.98ms
step:1639/2330 train_time:65524ms step_avg:39.98ms
step:1640/2330 train_time:65568ms step_avg:39.98ms
step:1641/2330 train_time:65603ms step_avg:39.98ms
step:1642/2330 train_time:65647ms step_avg:39.98ms
step:1643/2330 train_time:65682ms step_avg:39.98ms
step:1644/2330 train_time:65726ms step_avg:39.98ms
step:1645/2330 train_time:65761ms step_avg:39.98ms
step:1646/2330 train_time:65805ms step_avg:39.98ms
step:1647/2330 train_time:65840ms step_avg:39.98ms
step:1648/2330 train_time:65885ms step_avg:39.98ms
step:1649/2330 train_time:65920ms step_avg:39.98ms
step:1650/2330 train_time:65964ms step_avg:39.98ms
step:1651/2330 train_time:66001ms step_avg:39.98ms
step:1652/2330 train_time:66045ms step_avg:39.98ms
step:1653/2330 train_time:66081ms step_avg:39.98ms
step:1654/2330 train_time:66126ms step_avg:39.98ms
step:1655/2330 train_time:66162ms step_avg:39.98ms
step:1656/2330 train_time:66207ms step_avg:39.98ms
step:1657/2330 train_time:66242ms step_avg:39.98ms
step:1658/2330 train_time:66287ms step_avg:39.98ms
step:1659/2330 train_time:66322ms step_avg:39.98ms
step:1660/2330 train_time:66366ms step_avg:39.98ms
step:1661/2330 train_time:66402ms step_avg:39.98ms
step:1662/2330 train_time:66446ms step_avg:39.98ms
step:1663/2330 train_time:66481ms step_avg:39.98ms
step:1664/2330 train_time:66526ms step_avg:39.98ms
step:1665/2330 train_time:66561ms step_avg:39.98ms
step:1666/2330 train_time:66605ms step_avg:39.98ms
step:1667/2330 train_time:66640ms step_avg:39.98ms
step:1668/2330 train_time:66685ms step_avg:39.98ms
step:1669/2330 train_time:66720ms step_avg:39.98ms
step:1670/2330 train_time:66764ms step_avg:39.98ms
step:1671/2330 train_time:66799ms step_avg:39.98ms
step:1672/2330 train_time:66843ms step_avg:39.98ms
step:1673/2330 train_time:66878ms step_avg:39.97ms
step:1674/2330 train_time:66923ms step_avg:39.98ms
step:1675/2330 train_time:66958ms step_avg:39.97ms
step:1676/2330 train_time:67002ms step_avg:39.98ms
step:1677/2330 train_time:67038ms step_avg:39.97ms
step:1678/2330 train_time:67082ms step_avg:39.98ms
step:1679/2330 train_time:67117ms step_avg:39.97ms
step:1680/2330 train_time:67162ms step_avg:39.98ms
step:1681/2330 train_time:67198ms step_avg:39.97ms
step:1682/2330 train_time:67242ms step_avg:39.98ms
step:1683/2330 train_time:67277ms step_avg:39.97ms
step:1684/2330 train_time:67322ms step_avg:39.98ms
step:1685/2330 train_time:67358ms step_avg:39.97ms
step:1686/2330 train_time:67403ms step_avg:39.98ms
step:1687/2330 train_time:67438ms step_avg:39.98ms
step:1688/2330 train_time:67482ms step_avg:39.98ms
step:1689/2330 train_time:67518ms step_avg:39.98ms
step:1690/2330 train_time:67562ms step_avg:39.98ms
step:1691/2330 train_time:67597ms step_avg:39.97ms
step:1692/2330 train_time:67640ms step_avg:39.98ms
step:1693/2330 train_time:67675ms step_avg:39.97ms
step:1694/2330 train_time:67720ms step_avg:39.98ms
step:1695/2330 train_time:67755ms step_avg:39.97ms
step:1696/2330 train_time:67800ms step_avg:39.98ms
step:1697/2330 train_time:67834ms step_avg:39.97ms
step:1698/2330 train_time:67879ms step_avg:39.98ms
step:1699/2330 train_time:67914ms step_avg:39.97ms
step:1700/2330 train_time:67958ms step_avg:39.98ms
step:1701/2330 train_time:67993ms step_avg:39.97ms
step:1702/2330 train_time:68037ms step_avg:39.98ms
step:1703/2330 train_time:68073ms step_avg:39.97ms
step:1704/2330 train_time:68118ms step_avg:39.98ms
step:1705/2330 train_time:68153ms step_avg:39.97ms
step:1706/2330 train_time:68198ms step_avg:39.98ms
step:1707/2330 train_time:68232ms step_avg:39.97ms
step:1708/2330 train_time:68277ms step_avg:39.97ms
step:1709/2330 train_time:68311ms step_avg:39.97ms
step:1710/2330 train_time:68357ms step_avg:39.97ms
step:1711/2330 train_time:68392ms step_avg:39.97ms
step:1712/2330 train_time:68436ms step_avg:39.97ms
step:1713/2330 train_time:68471ms step_avg:39.97ms
step:1714/2330 train_time:68517ms step_avg:39.97ms
step:1715/2330 train_time:68551ms step_avg:39.97ms
step:1716/2330 train_time:68595ms step_avg:39.97ms
step:1717/2330 train_time:68630ms step_avg:39.97ms
step:1718/2330 train_time:68674ms step_avg:39.97ms
step:1719/2330 train_time:68709ms step_avg:39.97ms
step:1720/2330 train_time:68752ms step_avg:39.97ms
step:1721/2330 train_time:68787ms step_avg:39.97ms
step:1722/2330 train_time:68831ms step_avg:39.97ms
step:1723/2330 train_time:68866ms step_avg:39.97ms
step:1724/2330 train_time:68910ms step_avg:39.97ms
step:1725/2330 train_time:68945ms step_avg:39.97ms
step:1726/2330 train_time:68989ms step_avg:39.97ms
step:1727/2330 train_time:69024ms step_avg:39.97ms
step:1728/2330 train_time:69068ms step_avg:39.97ms
step:1729/2330 train_time:69103ms step_avg:39.97ms
step:1730/2330 train_time:69148ms step_avg:39.97ms
step:1731/2330 train_time:69184ms step_avg:39.97ms
step:1732/2330 train_time:69228ms step_avg:39.97ms
step:1733/2330 train_time:69263ms step_avg:39.97ms
step:1734/2330 train_time:69308ms step_avg:39.97ms
step:1735/2330 train_time:69343ms step_avg:39.97ms
step:1736/2330 train_time:69387ms step_avg:39.97ms
step:1737/2330 train_time:69422ms step_avg:39.97ms
step:1738/2330 train_time:69466ms step_avg:39.97ms
step:1739/2330 train_time:69501ms step_avg:39.97ms
step:1740/2330 train_time:69546ms step_avg:39.97ms
step:1741/2330 train_time:69581ms step_avg:39.97ms
step:1742/2330 train_time:69626ms step_avg:39.97ms
step:1743/2330 train_time:69661ms step_avg:39.97ms
step:1744/2330 train_time:69705ms step_avg:39.97ms
step:1745/2330 train_time:69740ms step_avg:39.97ms
step:1746/2330 train_time:69785ms step_avg:39.97ms
step:1747/2330 train_time:69819ms step_avg:39.97ms
step:1748/2330 train_time:69864ms step_avg:39.97ms
step:1749/2330 train_time:69899ms step_avg:39.97ms
step:1750/2330 train_time:69943ms step_avg:39.97ms
step:1750/2330 val_loss:5.1637 train_time:70030ms step_avg:40.02ms
step:1751/2330 train_time:70045ms step_avg:40.00ms
step:1752/2330 train_time:70058ms step_avg:39.99ms
step:1753/2330 train_time:70070ms step_avg:39.97ms
step:1754/2330 train_time:70103ms step_avg:39.97ms
step:1755/2330 train_time:70136ms step_avg:39.96ms
step:1756/2330 train_time:70179ms step_avg:39.97ms
step:1757/2330 train_time:70213ms step_avg:39.96ms
step:1758/2330 train_time:70256ms step_avg:39.96ms
step:1759/2330 train_time:70290ms step_avg:39.96ms
step:1760/2330 train_time:70334ms step_avg:39.96ms
step:1761/2330 train_time:70371ms step_avg:39.96ms
step:1762/2330 train_time:70418ms step_avg:39.96ms
step:1763/2330 train_time:70455ms step_avg:39.96ms
step:1764/2330 train_time:70500ms step_avg:39.97ms
step:1765/2330 train_time:70535ms step_avg:39.96ms
step:1766/2330 train_time:70579ms step_avg:39.97ms
step:1767/2330 train_time:70614ms step_avg:39.96ms
step:1768/2330 train_time:70658ms step_avg:39.97ms
step:1769/2330 train_time:70693ms step_avg:39.96ms
step:1770/2330 train_time:70737ms step_avg:39.96ms
step:1771/2330 train_time:70772ms step_avg:39.96ms
step:1772/2330 train_time:70816ms step_avg:39.96ms
step:1773/2330 train_time:70850ms step_avg:39.96ms
step:1774/2330 train_time:70894ms step_avg:39.96ms
step:1775/2330 train_time:70929ms step_avg:39.96ms
step:1776/2330 train_time:70974ms step_avg:39.96ms
step:1777/2330 train_time:71011ms step_avg:39.96ms
step:1778/2330 train_time:71057ms step_avg:39.96ms
step:1779/2330 train_time:71092ms step_avg:39.96ms
step:1780/2330 train_time:71137ms step_avg:39.96ms
step:1781/2330 train_time:71172ms step_avg:39.96ms
step:1782/2330 train_time:71216ms step_avg:39.96ms
step:1783/2330 train_time:71251ms step_avg:39.96ms
step:1784/2330 train_time:71295ms step_avg:39.96ms
step:1785/2330 train_time:71331ms step_avg:39.96ms
step:1786/2330 train_time:71376ms step_avg:39.96ms
step:1787/2330 train_time:71412ms step_avg:39.96ms
step:1788/2330 train_time:71456ms step_avg:39.96ms
step:1789/2330 train_time:71492ms step_avg:39.96ms
step:1790/2330 train_time:71536ms step_avg:39.96ms
step:1791/2330 train_time:71572ms step_avg:39.96ms
step:1792/2330 train_time:71616ms step_avg:39.96ms
step:1793/2330 train_time:71651ms step_avg:39.96ms
step:1794/2330 train_time:71695ms step_avg:39.96ms
step:1795/2330 train_time:71730ms step_avg:39.96ms
step:1796/2330 train_time:71774ms step_avg:39.96ms
step:1797/2330 train_time:71809ms step_avg:39.96ms
step:1798/2330 train_time:71852ms step_avg:39.96ms
step:1799/2330 train_time:71888ms step_avg:39.96ms
step:1800/2330 train_time:71932ms step_avg:39.96ms
step:1801/2330 train_time:71969ms step_avg:39.96ms
step:1802/2330 train_time:72013ms step_avg:39.96ms
step:1803/2330 train_time:72049ms step_avg:39.96ms
step:1804/2330 train_time:72093ms step_avg:39.96ms
step:1805/2330 train_time:72128ms step_avg:39.96ms
step:1806/2330 train_time:72172ms step_avg:39.96ms
step:1807/2330 train_time:72207ms step_avg:39.96ms
step:1808/2330 train_time:72252ms step_avg:39.96ms
step:1809/2330 train_time:72288ms step_avg:39.96ms
step:1810/2330 train_time:72332ms step_avg:39.96ms
step:1811/2330 train_time:72368ms step_avg:39.96ms
step:1812/2330 train_time:72412ms step_avg:39.96ms
step:1813/2330 train_time:72448ms step_avg:39.96ms
step:1814/2330 train_time:72492ms step_avg:39.96ms
step:1815/2330 train_time:72528ms step_avg:39.96ms
step:1816/2330 train_time:72572ms step_avg:39.96ms
step:1817/2330 train_time:72607ms step_avg:39.96ms
step:1818/2330 train_time:72651ms step_avg:39.96ms
step:1819/2330 train_time:72685ms step_avg:39.96ms
step:1820/2330 train_time:72729ms step_avg:39.96ms
step:1821/2330 train_time:72763ms step_avg:39.96ms
step:1822/2330 train_time:72808ms step_avg:39.96ms
step:1823/2330 train_time:72842ms step_avg:39.96ms
step:1824/2330 train_time:72887ms step_avg:39.96ms
step:1825/2330 train_time:72921ms step_avg:39.96ms
step:1826/2330 train_time:72965ms step_avg:39.96ms
step:1827/2330 train_time:73000ms step_avg:39.96ms
step:1828/2330 train_time:73045ms step_avg:39.96ms
step:1829/2330 train_time:73079ms step_avg:39.96ms
step:1830/2330 train_time:73123ms step_avg:39.96ms
step:1831/2330 train_time:73158ms step_avg:39.96ms
step:1832/2330 train_time:73202ms step_avg:39.96ms
step:1833/2330 train_time:73237ms step_avg:39.95ms
step:1834/2330 train_time:73281ms step_avg:39.96ms
step:1835/2330 train_time:73316ms step_avg:39.95ms
step:1836/2330 train_time:73360ms step_avg:39.96ms
step:1837/2330 train_time:73395ms step_avg:39.95ms
step:1838/2330 train_time:73439ms step_avg:39.96ms
step:1839/2330 train_time:73474ms step_avg:39.95ms
step:1840/2330 train_time:73518ms step_avg:39.96ms
step:1841/2330 train_time:73553ms step_avg:39.95ms
step:1842/2330 train_time:73597ms step_avg:39.96ms
step:1843/2330 train_time:73632ms step_avg:39.95ms
step:1844/2330 train_time:73677ms step_avg:39.95ms
step:1845/2330 train_time:73712ms step_avg:39.95ms
step:1846/2330 train_time:73756ms step_avg:39.95ms
step:1847/2330 train_time:73791ms step_avg:39.95ms
step:1848/2330 train_time:73836ms step_avg:39.95ms
step:1849/2330 train_time:73871ms step_avg:39.95ms
step:1850/2330 train_time:73915ms step_avg:39.95ms
step:1851/2330 train_time:73950ms step_avg:39.95ms
step:1852/2330 train_time:73994ms step_avg:39.95ms
step:1853/2330 train_time:74030ms step_avg:39.95ms
step:1854/2330 train_time:74074ms step_avg:39.95ms
step:1855/2330 train_time:74109ms step_avg:39.95ms
step:1856/2330 train_time:74154ms step_avg:39.95ms
step:1857/2330 train_time:74189ms step_avg:39.95ms
step:1858/2330 train_time:74234ms step_avg:39.95ms
step:1859/2330 train_time:74269ms step_avg:39.95ms
step:1860/2330 train_time:74314ms step_avg:39.95ms
step:1861/2330 train_time:74348ms step_avg:39.95ms
step:1862/2330 train_time:74393ms step_avg:39.95ms
step:1863/2330 train_time:74428ms step_avg:39.95ms
step:1864/2330 train_time:74472ms step_avg:39.95ms
step:1865/2330 train_time:74508ms step_avg:39.95ms
step:1866/2330 train_time:74552ms step_avg:39.95ms
step:1867/2330 train_time:74587ms step_avg:39.95ms
step:1868/2330 train_time:74632ms step_avg:39.95ms
step:1869/2330 train_time:74666ms step_avg:39.95ms
step:1870/2330 train_time:74711ms step_avg:39.95ms
step:1871/2330 train_time:74745ms step_avg:39.95ms
step:1872/2330 train_time:74790ms step_avg:39.95ms
step:1873/2330 train_time:74825ms step_avg:39.95ms
step:1874/2330 train_time:74869ms step_avg:39.95ms
step:1875/2330 train_time:74904ms step_avg:39.95ms
step:1876/2330 train_time:74949ms step_avg:39.95ms
step:1877/2330 train_time:74983ms step_avg:39.95ms
step:1878/2330 train_time:75027ms step_avg:39.95ms
step:1879/2330 train_time:75063ms step_avg:39.95ms
step:1880/2330 train_time:75107ms step_avg:39.95ms
step:1881/2330 train_time:75141ms step_avg:39.95ms
step:1882/2330 train_time:75185ms step_avg:39.95ms
step:1883/2330 train_time:75220ms step_avg:39.95ms
step:1884/2330 train_time:75264ms step_avg:39.95ms
step:1885/2330 train_time:75300ms step_avg:39.95ms
step:1886/2330 train_time:75343ms step_avg:39.95ms
step:1887/2330 train_time:75378ms step_avg:39.95ms
step:1888/2330 train_time:75422ms step_avg:39.95ms
step:1889/2330 train_time:75456ms step_avg:39.94ms
step:1890/2330 train_time:75500ms step_avg:39.95ms
step:1891/2330 train_time:75535ms step_avg:39.94ms
step:1892/2330 train_time:75579ms step_avg:39.95ms
step:1893/2330 train_time:75614ms step_avg:39.94ms
step:1894/2330 train_time:75658ms step_avg:39.95ms
step:1895/2330 train_time:75693ms step_avg:39.94ms
step:1896/2330 train_time:75738ms step_avg:39.95ms
step:1897/2330 train_time:75773ms step_avg:39.94ms
step:1898/2330 train_time:75817ms step_avg:39.95ms
step:1899/2330 train_time:75853ms step_avg:39.94ms
step:1900/2330 train_time:75897ms step_avg:39.95ms
step:1901/2330 train_time:75932ms step_avg:39.94ms
step:1902/2330 train_time:75978ms step_avg:39.95ms
step:1903/2330 train_time:76013ms step_avg:39.94ms
step:1904/2330 train_time:76058ms step_avg:39.95ms
step:1905/2330 train_time:76093ms step_avg:39.94ms
step:1906/2330 train_time:76137ms step_avg:39.95ms
step:1907/2330 train_time:76172ms step_avg:39.94ms
step:1908/2330 train_time:76216ms step_avg:39.95ms
step:1909/2330 train_time:76252ms step_avg:39.94ms
step:1910/2330 train_time:76296ms step_avg:39.95ms
step:1911/2330 train_time:76332ms step_avg:39.94ms
step:1912/2330 train_time:76376ms step_avg:39.95ms
step:1913/2330 train_time:76412ms step_avg:39.94ms
step:1914/2330 train_time:76456ms step_avg:39.95ms
step:1915/2330 train_time:76492ms step_avg:39.94ms
step:1916/2330 train_time:76536ms step_avg:39.95ms
step:1917/2330 train_time:76571ms step_avg:39.94ms
step:1918/2330 train_time:76615ms step_avg:39.95ms
step:1919/2330 train_time:76650ms step_avg:39.94ms
step:1920/2330 train_time:76695ms step_avg:39.95ms
step:1921/2330 train_time:76731ms step_avg:39.94ms
step:1922/2330 train_time:76775ms step_avg:39.95ms
step:1923/2330 train_time:76810ms step_avg:39.94ms
step:1924/2330 train_time:76854ms step_avg:39.95ms
step:1925/2330 train_time:76889ms step_avg:39.94ms
step:1926/2330 train_time:76934ms step_avg:39.94ms
step:1927/2330 train_time:76969ms step_avg:39.94ms
step:1928/2330 train_time:77014ms step_avg:39.94ms
step:1929/2330 train_time:77049ms step_avg:39.94ms
step:1930/2330 train_time:77093ms step_avg:39.94ms
step:1931/2330 train_time:77128ms step_avg:39.94ms
step:1932/2330 train_time:77173ms step_avg:39.94ms
step:1933/2330 train_time:77208ms step_avg:39.94ms
step:1934/2330 train_time:77252ms step_avg:39.94ms
step:1935/2330 train_time:77287ms step_avg:39.94ms
step:1936/2330 train_time:77332ms step_avg:39.94ms
step:1937/2330 train_time:77367ms step_avg:39.94ms
step:1938/2330 train_time:77411ms step_avg:39.94ms
step:1939/2330 train_time:77446ms step_avg:39.94ms
step:1940/2330 train_time:77491ms step_avg:39.94ms
step:1941/2330 train_time:77526ms step_avg:39.94ms
step:1942/2330 train_time:77570ms step_avg:39.94ms
step:1943/2330 train_time:77605ms step_avg:39.94ms
step:1944/2330 train_time:77650ms step_avg:39.94ms
step:1945/2330 train_time:77684ms step_avg:39.94ms
step:1946/2330 train_time:77728ms step_avg:39.94ms
step:1947/2330 train_time:77763ms step_avg:39.94ms
step:1948/2330 train_time:77808ms step_avg:39.94ms
step:1949/2330 train_time:77843ms step_avg:39.94ms
step:1950/2330 train_time:77888ms step_avg:39.94ms
step:1951/2330 train_time:77922ms step_avg:39.94ms
step:1952/2330 train_time:77967ms step_avg:39.94ms
step:1953/2330 train_time:78002ms step_avg:39.94ms
step:1954/2330 train_time:78046ms step_avg:39.94ms
step:1955/2330 train_time:78081ms step_avg:39.94ms
step:1956/2330 train_time:78125ms step_avg:39.94ms
step:1957/2330 train_time:78160ms step_avg:39.94ms
step:1958/2330 train_time:78204ms step_avg:39.94ms
step:1959/2330 train_time:78238ms step_avg:39.94ms
step:1960/2330 train_time:78282ms step_avg:39.94ms
step:1961/2330 train_time:78317ms step_avg:39.94ms
step:1962/2330 train_time:78361ms step_avg:39.94ms
step:1963/2330 train_time:78395ms step_avg:39.94ms
step:1964/2330 train_time:78440ms step_avg:39.94ms
step:1965/2330 train_time:78475ms step_avg:39.94ms
step:1966/2330 train_time:78519ms step_avg:39.94ms
step:1967/2330 train_time:78554ms step_avg:39.94ms
step:1968/2330 train_time:78599ms step_avg:39.94ms
step:1969/2330 train_time:78634ms step_avg:39.94ms
step:1970/2330 train_time:78678ms step_avg:39.94ms
step:1971/2330 train_time:78713ms step_avg:39.94ms
step:1972/2330 train_time:78757ms step_avg:39.94ms
step:1973/2330 train_time:78793ms step_avg:39.94ms
step:1974/2330 train_time:78837ms step_avg:39.94ms
step:1975/2330 train_time:78873ms step_avg:39.94ms
step:1976/2330 train_time:78917ms step_avg:39.94ms
step:1977/2330 train_time:78952ms step_avg:39.94ms
step:1978/2330 train_time:78996ms step_avg:39.94ms
step:1979/2330 train_time:79032ms step_avg:39.94ms
step:1980/2330 train_time:79076ms step_avg:39.94ms
step:1981/2330 train_time:79112ms step_avg:39.94ms
step:1982/2330 train_time:79156ms step_avg:39.94ms
step:1983/2330 train_time:79191ms step_avg:39.93ms
step:1984/2330 train_time:79235ms step_avg:39.94ms
step:1985/2330 train_time:79270ms step_avg:39.93ms
step:1986/2330 train_time:79314ms step_avg:39.94ms
step:1987/2330 train_time:79349ms step_avg:39.93ms
step:1988/2330 train_time:79394ms step_avg:39.94ms
step:1989/2330 train_time:79429ms step_avg:39.93ms
step:1990/2330 train_time:79474ms step_avg:39.94ms
step:1991/2330 train_time:79509ms step_avg:39.93ms
step:1992/2330 train_time:79553ms step_avg:39.94ms
step:1993/2330 train_time:79588ms step_avg:39.93ms
step:1994/2330 train_time:79633ms step_avg:39.94ms
step:1995/2330 train_time:79668ms step_avg:39.93ms
step:1996/2330 train_time:79712ms step_avg:39.94ms
step:1997/2330 train_time:79747ms step_avg:39.93ms
step:1998/2330 train_time:79792ms step_avg:39.94ms
step:1999/2330 train_time:79828ms step_avg:39.93ms
step:2000/2330 train_time:79872ms step_avg:39.94ms
step:2000/2330 val_loss:5.1744 train_time:79959ms step_avg:39.98ms
step:2001/2330 train_time:79974ms step_avg:39.97ms
step:2002/2330 train_time:79987ms step_avg:39.95ms
step:2003/2330 train_time:79998ms step_avg:39.94ms
step:2004/2330 train_time:80032ms step_avg:39.94ms
step:2005/2330 train_time:80066ms step_avg:39.93ms
step:2006/2330 train_time:80110ms step_avg:39.94ms
step:2007/2330 train_time:80144ms step_avg:39.93ms
step:2008/2330 train_time:80187ms step_avg:39.93ms
step:2009/2330 train_time:80221ms step_avg:39.93ms
step:2010/2330 train_time:80266ms step_avg:39.93ms
step:2011/2330 train_time:80304ms step_avg:39.93ms
step:2012/2330 train_time:80354ms step_avg:39.94ms
step:2013/2330 train_time:80390ms step_avg:39.94ms
step:2014/2330 train_time:80435ms step_avg:39.94ms
step:2015/2330 train_time:80470ms step_avg:39.94ms
step:2016/2330 train_time:80515ms step_avg:39.94ms
step:2017/2330 train_time:80550ms step_avg:39.94ms
step:2018/2330 train_time:80594ms step_avg:39.94ms
step:2019/2330 train_time:80628ms step_avg:39.93ms
step:2020/2330 train_time:80672ms step_avg:39.94ms
step:2021/2330 train_time:80707ms step_avg:39.93ms
step:2022/2330 train_time:80750ms step_avg:39.94ms
step:2023/2330 train_time:80784ms step_avg:39.93ms
step:2024/2330 train_time:80828ms step_avg:39.93ms
step:2025/2330 train_time:80862ms step_avg:39.93ms
step:2026/2330 train_time:80906ms step_avg:39.93ms
step:2027/2330 train_time:80941ms step_avg:39.93ms
step:2028/2330 train_time:80984ms step_avg:39.93ms
step:2029/2330 train_time:81019ms step_avg:39.93ms
step:2030/2330 train_time:81063ms step_avg:39.93ms
step:2031/2330 train_time:81098ms step_avg:39.93ms
step:2032/2330 train_time:81141ms step_avg:39.93ms
step:2033/2330 train_time:81176ms step_avg:39.93ms
step:2034/2330 train_time:81221ms step_avg:39.93ms
step:2035/2330 train_time:81257ms step_avg:39.93ms
step:2036/2330 train_time:81303ms step_avg:39.93ms
step:2037/2330 train_time:81339ms step_avg:39.93ms
step:2038/2330 train_time:81383ms step_avg:39.93ms
step:2039/2330 train_time:81419ms step_avg:39.93ms
step:2040/2330 train_time:81464ms step_avg:39.93ms
step:2041/2330 train_time:81499ms step_avg:39.93ms
step:2042/2330 train_time:81543ms step_avg:39.93ms
step:2043/2330 train_time:81578ms step_avg:39.93ms
step:2044/2330 train_time:81622ms step_avg:39.93ms
step:2045/2330 train_time:81657ms step_avg:39.93ms
step:2046/2330 train_time:81701ms step_avg:39.93ms
step:2047/2330 train_time:81736ms step_avg:39.93ms
step:2048/2330 train_time:81780ms step_avg:39.93ms
step:2049/2330 train_time:81815ms step_avg:39.93ms
step:2050/2330 train_time:81859ms step_avg:39.93ms
step:2051/2330 train_time:81894ms step_avg:39.93ms
step:2052/2330 train_time:81938ms step_avg:39.93ms
step:2053/2330 train_time:81973ms step_avg:39.93ms
step:2054/2330 train_time:82017ms step_avg:39.93ms
step:2055/2330 train_time:82052ms step_avg:39.93ms
step:2056/2330 train_time:82096ms step_avg:39.93ms
step:2057/2330 train_time:82131ms step_avg:39.93ms
step:2058/2330 train_time:82176ms step_avg:39.93ms
step:2059/2330 train_time:82211ms step_avg:39.93ms
step:2060/2330 train_time:82256ms step_avg:39.93ms
step:2061/2330 train_time:82292ms step_avg:39.93ms
step:2062/2330 train_time:82337ms step_avg:39.93ms
step:2063/2330 train_time:82373ms step_avg:39.93ms
step:2064/2330 train_time:82418ms step_avg:39.93ms
step:2065/2330 train_time:82453ms step_avg:39.93ms
step:2066/2330 train_time:82498ms step_avg:39.93ms
step:2067/2330 train_time:82534ms step_avg:39.93ms
step:2068/2330 train_time:82578ms step_avg:39.93ms
step:2069/2330 train_time:82612ms step_avg:39.93ms
step:2070/2330 train_time:82657ms step_avg:39.93ms
step:2071/2330 train_time:82692ms step_avg:39.93ms
step:2072/2330 train_time:82735ms step_avg:39.93ms
step:2073/2330 train_time:82771ms step_avg:39.93ms
step:2074/2330 train_time:82815ms step_avg:39.93ms
step:2075/2330 train_time:82850ms step_avg:39.93ms
step:2076/2330 train_time:82894ms step_avg:39.93ms
step:2077/2330 train_time:82929ms step_avg:39.93ms
step:2078/2330 train_time:82972ms step_avg:39.93ms
step:2079/2330 train_time:83007ms step_avg:39.93ms
step:2080/2330 train_time:83051ms step_avg:39.93ms
step:2081/2330 train_time:83085ms step_avg:39.93ms
step:2082/2330 train_time:83130ms step_avg:39.93ms
step:2083/2330 train_time:83164ms step_avg:39.93ms
step:2084/2330 train_time:83208ms step_avg:39.93ms
step:2085/2330 train_time:83243ms step_avg:39.92ms
step:2086/2330 train_time:83288ms step_avg:39.93ms
step:2087/2330 train_time:83324ms step_avg:39.93ms
step:2088/2330 train_time:83369ms step_avg:39.93ms
step:2089/2330 train_time:83404ms step_avg:39.93ms
step:2090/2330 train_time:83448ms step_avg:39.93ms
step:2091/2330 train_time:83483ms step_avg:39.92ms
step:2092/2330 train_time:83528ms step_avg:39.93ms
step:2093/2330 train_time:83563ms step_avg:39.93ms
step:2094/2330 train_time:83608ms step_avg:39.93ms
step:2095/2330 train_time:83643ms step_avg:39.92ms
step:2096/2330 train_time:83687ms step_avg:39.93ms
step:2097/2330 train_time:83721ms step_avg:39.92ms
step:2098/2330 train_time:83765ms step_avg:39.93ms
step:2099/2330 train_time:83800ms step_avg:39.92ms
step:2100/2330 train_time:83843ms step_avg:39.93ms
step:2101/2330 train_time:83878ms step_avg:39.92ms
step:2102/2330 train_time:83922ms step_avg:39.92ms
step:2103/2330 train_time:83957ms step_avg:39.92ms
step:2104/2330 train_time:84001ms step_avg:39.92ms
step:2105/2330 train_time:84036ms step_avg:39.92ms
step:2106/2330 train_time:84081ms step_avg:39.92ms
step:2107/2330 train_time:84116ms step_avg:39.92ms
step:2108/2330 train_time:84160ms step_avg:39.92ms
step:2109/2330 train_time:84195ms step_avg:39.92ms
step:2110/2330 train_time:84240ms step_avg:39.92ms
step:2111/2330 train_time:84275ms step_avg:39.92ms
step:2112/2330 train_time:84320ms step_avg:39.92ms
step:2113/2330 train_time:84355ms step_avg:39.92ms
step:2114/2330 train_time:84400ms step_avg:39.92ms
step:2115/2330 train_time:84435ms step_avg:39.92ms
step:2116/2330 train_time:84480ms step_avg:39.92ms
step:2117/2330 train_time:84516ms step_avg:39.92ms
step:2118/2330 train_time:84560ms step_avg:39.92ms
step:2119/2330 train_time:84596ms step_avg:39.92ms
step:2120/2330 train_time:84641ms step_avg:39.92ms
step:2121/2330 train_time:84676ms step_avg:39.92ms
step:2122/2330 train_time:84721ms step_avg:39.92ms
step:2123/2330 train_time:84756ms step_avg:39.92ms
step:2124/2330 train_time:84801ms step_avg:39.92ms
step:2125/2330 train_time:84836ms step_avg:39.92ms
step:2126/2330 train_time:84880ms step_avg:39.92ms
step:2127/2330 train_time:84915ms step_avg:39.92ms
step:2128/2330 train_time:84959ms step_avg:39.92ms
step:2129/2330 train_time:84994ms step_avg:39.92ms
step:2130/2330 train_time:85038ms step_avg:39.92ms
step:2131/2330 train_time:85073ms step_avg:39.92ms
step:2132/2330 train_time:85118ms step_avg:39.92ms
step:2133/2330 train_time:85153ms step_avg:39.92ms
step:2134/2330 train_time:85197ms step_avg:39.92ms
step:2135/2330 train_time:85233ms step_avg:39.92ms
step:2136/2330 train_time:85277ms step_avg:39.92ms
step:2137/2330 train_time:85312ms step_avg:39.92ms
step:2138/2330 train_time:85356ms step_avg:39.92ms
step:2139/2330 train_time:85392ms step_avg:39.92ms
step:2140/2330 train_time:85436ms step_avg:39.92ms
step:2141/2330 train_time:85471ms step_avg:39.92ms
step:2142/2330 train_time:85516ms step_avg:39.92ms
step:2143/2330 train_time:85551ms step_avg:39.92ms
step:2144/2330 train_time:85596ms step_avg:39.92ms
step:2145/2330 train_time:85632ms step_avg:39.92ms
step:2146/2330 train_time:85677ms step_avg:39.92ms
step:2147/2330 train_time:85712ms step_avg:39.92ms
step:2148/2330 train_time:85756ms step_avg:39.92ms
step:2149/2330 train_time:85792ms step_avg:39.92ms
step:2150/2330 train_time:85836ms step_avg:39.92ms
step:2151/2330 train_time:85871ms step_avg:39.92ms
step:2152/2330 train_time:85916ms step_avg:39.92ms
step:2153/2330 train_time:85951ms step_avg:39.92ms
step:2154/2330 train_time:85994ms step_avg:39.92ms
step:2155/2330 train_time:86029ms step_avg:39.92ms
step:2156/2330 train_time:86073ms step_avg:39.92ms
step:2157/2330 train_time:86109ms step_avg:39.92ms
step:2158/2330 train_time:86154ms step_avg:39.92ms
step:2159/2330 train_time:86188ms step_avg:39.92ms
step:2160/2330 train_time:86232ms step_avg:39.92ms
step:2161/2330 train_time:86267ms step_avg:39.92ms
step:2162/2330 train_time:86311ms step_avg:39.92ms
step:2163/2330 train_time:86346ms step_avg:39.92ms
step:2164/2330 train_time:86390ms step_avg:39.92ms
step:2165/2330 train_time:86425ms step_avg:39.92ms
step:2166/2330 train_time:86469ms step_avg:39.92ms
step:2167/2330 train_time:86504ms step_avg:39.92ms
step:2168/2330 train_time:86548ms step_avg:39.92ms
step:2169/2330 train_time:86584ms step_avg:39.92ms
step:2170/2330 train_time:86629ms step_avg:39.92ms
step:2171/2330 train_time:86664ms step_avg:39.92ms
step:2172/2330 train_time:86708ms step_avg:39.92ms
step:2173/2330 train_time:86743ms step_avg:39.92ms
step:2174/2330 train_time:86787ms step_avg:39.92ms
step:2175/2330 train_time:86822ms step_avg:39.92ms
step:2176/2330 train_time:86865ms step_avg:39.92ms
step:2177/2330 train_time:86900ms step_avg:39.92ms
step:2178/2330 train_time:86944ms step_avg:39.92ms
step:2179/2330 train_time:86979ms step_avg:39.92ms
step:2180/2330 train_time:87024ms step_avg:39.92ms
step:2181/2330 train_time:87059ms step_avg:39.92ms
step:2182/2330 train_time:87103ms step_avg:39.92ms
step:2183/2330 train_time:87138ms step_avg:39.92ms
step:2184/2330 train_time:87181ms step_avg:39.92ms
step:2185/2330 train_time:87216ms step_avg:39.92ms
step:2186/2330 train_time:87261ms step_avg:39.92ms
step:2187/2330 train_time:87296ms step_avg:39.92ms
step:2188/2330 train_time:87340ms step_avg:39.92ms
step:2189/2330 train_time:87376ms step_avg:39.92ms
step:2190/2330 train_time:87421ms step_avg:39.92ms
step:2191/2330 train_time:87456ms step_avg:39.92ms
step:2192/2330 train_time:87500ms step_avg:39.92ms
step:2193/2330 train_time:87536ms step_avg:39.92ms
step:2194/2330 train_time:87581ms step_avg:39.92ms
step:2195/2330 train_time:87616ms step_avg:39.92ms
step:2196/2330 train_time:87661ms step_avg:39.92ms
step:2197/2330 train_time:87696ms step_avg:39.92ms
step:2198/2330 train_time:87741ms step_avg:39.92ms
step:2199/2330 train_time:87776ms step_avg:39.92ms
step:2200/2330 train_time:87821ms step_avg:39.92ms
step:2201/2330 train_time:87856ms step_avg:39.92ms
step:2202/2330 train_time:87900ms step_avg:39.92ms
step:2203/2330 train_time:87936ms step_avg:39.92ms
step:2204/2330 train_time:87980ms step_avg:39.92ms
step:2205/2330 train_time:88015ms step_avg:39.92ms
step:2206/2330 train_time:88059ms step_avg:39.92ms
step:2207/2330 train_time:88094ms step_avg:39.92ms
step:2208/2330 train_time:88139ms step_avg:39.92ms
step:2209/2330 train_time:88173ms step_avg:39.92ms
step:2210/2330 train_time:88217ms step_avg:39.92ms
step:2211/2330 train_time:88252ms step_avg:39.92ms
step:2212/2330 train_time:88297ms step_avg:39.92ms
step:2213/2330 train_time:88332ms step_avg:39.92ms
step:2214/2330 train_time:88376ms step_avg:39.92ms
step:2215/2330 train_time:88411ms step_avg:39.91ms
step:2216/2330 train_time:88456ms step_avg:39.92ms
step:2217/2330 train_time:88522ms step_avg:39.93ms
step:2218/2330 train_time:88537ms step_avg:39.92ms
step:2219/2330 train_time:88572ms step_avg:39.92ms
step:2220/2330 train_time:88616ms step_avg:39.92ms
step:2221/2330 train_time:88652ms step_avg:39.92ms
step:2222/2330 train_time:88696ms step_avg:39.92ms
step:2223/2330 train_time:88731ms step_avg:39.91ms
step:2224/2330 train_time:88776ms step_avg:39.92ms
step:2225/2330 train_time:88811ms step_avg:39.92ms
step:2226/2330 train_time:88856ms step_avg:39.92ms
step:2227/2330 train_time:88891ms step_avg:39.92ms
step:2228/2330 train_time:88936ms step_avg:39.92ms
step:2229/2330 train_time:88970ms step_avg:39.91ms
step:2230/2330 train_time:89014ms step_avg:39.92ms
step:2231/2330 train_time:89050ms step_avg:39.91ms
step:2232/2330 train_time:89094ms step_avg:39.92ms
step:2233/2330 train_time:89129ms step_avg:39.91ms
step:2234/2330 train_time:89174ms step_avg:39.92ms
step:2235/2330 train_time:89209ms step_avg:39.91ms
step:2236/2330 train_time:89252ms step_avg:39.92ms
step:2237/2330 train_time:89287ms step_avg:39.91ms
step:2238/2330 train_time:89330ms step_avg:39.92ms
step:2239/2330 train_time:89365ms step_avg:39.91ms
step:2240/2330 train_time:89409ms step_avg:39.91ms
step:2241/2330 train_time:89445ms step_avg:39.91ms
step:2242/2330 train_time:89489ms step_avg:39.91ms
step:2243/2330 train_time:89524ms step_avg:39.91ms
step:2244/2330 train_time:89568ms step_avg:39.91ms
step:2245/2330 train_time:89604ms step_avg:39.91ms
step:2246/2330 train_time:89648ms step_avg:39.91ms
step:2247/2330 train_time:89683ms step_avg:39.91ms
step:2248/2330 train_time:89728ms step_avg:39.91ms
step:2249/2330 train_time:89763ms step_avg:39.91ms
step:2250/2330 train_time:89808ms step_avg:39.91ms
step:2250/2330 val_loss:5.1212 train_time:89894ms step_avg:39.95ms
step:2251/2330 train_time:89908ms step_avg:39.94ms
step:2252/2330 train_time:89922ms step_avg:39.93ms
step:2253/2330 train_time:89935ms step_avg:39.92ms
step:2254/2330 train_time:89967ms step_avg:39.91ms
step:2255/2330 train_time:90000ms step_avg:39.91ms
step:2256/2330 train_time:90044ms step_avg:39.91ms
step:2257/2330 train_time:90078ms step_avg:39.91ms
step:2258/2330 train_time:90121ms step_avg:39.91ms
step:2259/2330 train_time:90156ms step_avg:39.91ms
step:2260/2330 train_time:90200ms step_avg:39.91ms
step:2261/2330 train_time:90239ms step_avg:39.91ms
step:2262/2330 train_time:90288ms step_avg:39.92ms
step:2263/2330 train_time:90324ms step_avg:39.91ms
step:2264/2330 train_time:90369ms step_avg:39.92ms
step:2265/2330 train_time:90404ms step_avg:39.91ms
step:2266/2330 train_time:90447ms step_avg:39.92ms
step:2267/2330 train_time:90482ms step_avg:39.91ms
step:2268/2330 train_time:90526ms step_avg:39.91ms
step:2269/2330 train_time:90561ms step_avg:39.91ms
step:2270/2330 train_time:90604ms step_avg:39.91ms
step:2271/2330 train_time:90638ms step_avg:39.91ms
step:2272/2330 train_time:90681ms step_avg:39.91ms
step:2273/2330 train_time:90716ms step_avg:39.91ms
step:2274/2330 train_time:90760ms step_avg:39.91ms
step:2275/2330 train_time:90794ms step_avg:39.91ms
step:2276/2330 train_time:90839ms step_avg:39.91ms
step:2277/2330 train_time:90874ms step_avg:39.91ms
step:2278/2330 train_time:90919ms step_avg:39.91ms
step:2279/2330 train_time:90954ms step_avg:39.91ms
step:2280/2330 train_time:90998ms step_avg:39.91ms
step:2281/2330 train_time:91032ms step_avg:39.91ms
step:2282/2330 train_time:91076ms step_avg:39.91ms
step:2283/2330 train_time:91111ms step_avg:39.91ms
step:2284/2330 train_time:91156ms step_avg:39.91ms
step:2285/2330 train_time:91193ms step_avg:39.91ms
step:2286/2330 train_time:91239ms step_avg:39.91ms
step:2287/2330 train_time:91274ms step_avg:39.91ms
step:2288/2330 train_time:91320ms step_avg:39.91ms
step:2289/2330 train_time:91356ms step_avg:39.91ms
step:2290/2330 train_time:91401ms step_avg:39.91ms
step:2291/2330 train_time:91436ms step_avg:39.91ms
step:2292/2330 train_time:91481ms step_avg:39.91ms
step:2293/2330 train_time:91516ms step_avg:39.91ms
step:2294/2330 train_time:91560ms step_avg:39.91ms
step:2295/2330 train_time:91595ms step_avg:39.91ms
step:2296/2330 train_time:91638ms step_avg:39.91ms
step:2297/2330 train_time:91672ms step_avg:39.91ms
step:2298/2330 train_time:91716ms step_avg:39.91ms
step:2299/2330 train_time:91751ms step_avg:39.91ms
step:2300/2330 train_time:91794ms step_avg:39.91ms
step:2301/2330 train_time:91828ms step_avg:39.91ms
step:2302/2330 train_time:91873ms step_avg:39.91ms
step:2303/2330 train_time:91907ms step_avg:39.91ms
step:2304/2330 train_time:91951ms step_avg:39.91ms
step:2305/2330 train_time:91987ms step_avg:39.91ms
step:2306/2330 train_time:92030ms step_avg:39.91ms
step:2307/2330 train_time:92065ms step_avg:39.91ms
step:2308/2330 train_time:92108ms step_avg:39.91ms
step:2309/2330 train_time:92143ms step_avg:39.91ms
step:2310/2330 train_time:92188ms step_avg:39.91ms
step:2311/2330 train_time:92223ms step_avg:39.91ms
step:2312/2330 train_time:92268ms step_avg:39.91ms
step:2313/2330 train_time:92304ms step_avg:39.91ms
step:2314/2330 train_time:92349ms step_avg:39.91ms
step:2315/2330 train_time:92384ms step_avg:39.91ms
step:2316/2330 train_time:92429ms step_avg:39.91ms
step:2317/2330 train_time:92464ms step_avg:39.91ms
step:2318/2330 train_time:92507ms step_avg:39.91ms
step:2319/2330 train_time:92542ms step_avg:39.91ms
step:2320/2330 train_time:92586ms step_avg:39.91ms
step:2321/2330 train_time:92621ms step_avg:39.91ms
step:2322/2330 train_time:92665ms step_avg:39.91ms
step:2323/2330 train_time:92700ms step_avg:39.91ms
step:2324/2330 train_time:92743ms step_avg:39.91ms
step:2325/2330 train_time:92778ms step_avg:39.90ms
step:2326/2330 train_time:92822ms step_avg:39.91ms
step:2327/2330 train_time:92856ms step_avg:39.90ms
step:2328/2330 train_time:92900ms step_avg:39.91ms
step:2329/2330 train_time:92935ms step_avg:39.90ms
step:2330/2330 train_time:92979ms step_avg:39.90ms
step:2330/2330 val_loss:5.1127 train_time:93065ms step_avg:39.94ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
