import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr1e-1"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:17:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:65ms step_avg:64.61ms
step:2/2330 train_time:135ms step_avg:67.66ms
step:3/2330 train_time:148ms step_avg:49.19ms
step:4/2330 train_time:159ms step_avg:39.86ms
step:5/2330 train_time:170ms step_avg:34.05ms
step:6/2330 train_time:195ms step_avg:32.56ms
step:7/2330 train_time:216ms step_avg:30.92ms
step:8/2330 train_time:272ms step_avg:34.02ms
step:9/2330 train_time:294ms step_avg:32.63ms
step:10/2330 train_time:349ms step_avg:34.93ms
step:11/2330 train_time:371ms step_avg:33.71ms
step:12/2330 train_time:426ms step_avg:35.50ms
step:13/2330 train_time:449ms step_avg:34.51ms
step:14/2330 train_time:504ms step_avg:36.01ms
step:15/2330 train_time:526ms step_avg:35.08ms
step:16/2330 train_time:582ms step_avg:36.35ms
step:17/2330 train_time:604ms step_avg:35.52ms
step:18/2330 train_time:659ms step_avg:36.63ms
step:19/2330 train_time:682ms step_avg:35.87ms
step:20/2330 train_time:737ms step_avg:36.86ms
step:21/2330 train_time:759ms step_avg:36.13ms
step:22/2330 train_time:815ms step_avg:37.03ms
step:23/2330 train_time:836ms step_avg:36.37ms
step:24/2330 train_time:892ms step_avg:37.15ms
step:25/2330 train_time:914ms step_avg:36.55ms
step:26/2330 train_time:972ms step_avg:37.38ms
step:27/2330 train_time:996ms step_avg:36.90ms
step:28/2330 train_time:1056ms step_avg:37.73ms
step:29/2330 train_time:1082ms step_avg:37.30ms
step:30/2330 train_time:1141ms step_avg:38.03ms
step:31/2330 train_time:1165ms step_avg:37.57ms
step:32/2330 train_time:1222ms step_avg:38.17ms
step:33/2330 train_time:1244ms step_avg:37.71ms
step:34/2330 train_time:1301ms step_avg:38.26ms
step:35/2330 train_time:1323ms step_avg:37.80ms
step:36/2330 train_time:1379ms step_avg:38.30ms
step:37/2330 train_time:1401ms step_avg:37.86ms
step:38/2330 train_time:1457ms step_avg:38.33ms
step:39/2330 train_time:1478ms step_avg:37.90ms
step:40/2330 train_time:1534ms step_avg:38.36ms
step:41/2330 train_time:1556ms step_avg:37.94ms
step:42/2330 train_time:1611ms step_avg:38.36ms
step:43/2330 train_time:1632ms step_avg:37.96ms
step:44/2330 train_time:1688ms step_avg:38.35ms
step:45/2330 train_time:1710ms step_avg:37.99ms
step:46/2330 train_time:1765ms step_avg:38.37ms
step:47/2330 train_time:1787ms step_avg:38.03ms
step:48/2330 train_time:1843ms step_avg:38.39ms
step:49/2330 train_time:1865ms step_avg:38.07ms
step:50/2330 train_time:1922ms step_avg:38.43ms
step:51/2330 train_time:1945ms step_avg:38.14ms
step:52/2330 train_time:2002ms step_avg:38.50ms
step:53/2330 train_time:2026ms step_avg:38.22ms
step:54/2330 train_time:2083ms step_avg:38.57ms
step:55/2330 train_time:2106ms step_avg:38.29ms
step:56/2330 train_time:2163ms step_avg:38.62ms
step:57/2330 train_time:2186ms step_avg:38.35ms
step:58/2330 train_time:2242ms step_avg:38.66ms
step:59/2330 train_time:2265ms step_avg:38.39ms
step:60/2330 train_time:2321ms step_avg:38.68ms
step:61/2330 train_time:2343ms step_avg:38.42ms
step:62/2330 train_time:2399ms step_avg:38.70ms
step:63/2330 train_time:2422ms step_avg:38.44ms
step:64/2330 train_time:2478ms step_avg:38.72ms
step:65/2330 train_time:2501ms step_avg:38.47ms
step:66/2330 train_time:2556ms step_avg:38.73ms
step:67/2330 train_time:2579ms step_avg:38.49ms
step:68/2330 train_time:2635ms step_avg:38.75ms
step:69/2330 train_time:2657ms step_avg:38.51ms
step:70/2330 train_time:2713ms step_avg:38.75ms
step:71/2330 train_time:2735ms step_avg:38.52ms
step:72/2330 train_time:2791ms step_avg:38.76ms
step:73/2330 train_time:2813ms step_avg:38.53ms
step:74/2330 train_time:2869ms step_avg:38.77ms
step:75/2330 train_time:2891ms step_avg:38.54ms
step:76/2330 train_time:2947ms step_avg:38.77ms
step:77/2330 train_time:2969ms step_avg:38.56ms
step:78/2330 train_time:3026ms step_avg:38.79ms
step:79/2330 train_time:3049ms step_avg:38.59ms
step:80/2330 train_time:3105ms step_avg:38.81ms
step:81/2330 train_time:3128ms step_avg:38.62ms
step:82/2330 train_time:3185ms step_avg:38.84ms
step:83/2330 train_time:3208ms step_avg:38.65ms
step:84/2330 train_time:3265ms step_avg:38.86ms
step:85/2330 train_time:3288ms step_avg:38.68ms
step:86/2330 train_time:3344ms step_avg:38.88ms
step:87/2330 train_time:3366ms step_avg:38.69ms
step:88/2330 train_time:3423ms step_avg:38.90ms
step:89/2330 train_time:3446ms step_avg:38.72ms
step:90/2330 train_time:3502ms step_avg:38.91ms
step:91/2330 train_time:3524ms step_avg:38.73ms
step:92/2330 train_time:3580ms step_avg:38.91ms
step:93/2330 train_time:3603ms step_avg:38.74ms
step:94/2330 train_time:3658ms step_avg:38.92ms
step:95/2330 train_time:3681ms step_avg:38.75ms
step:96/2330 train_time:3738ms step_avg:38.93ms
step:97/2330 train_time:3760ms step_avg:38.77ms
step:98/2330 train_time:3817ms step_avg:38.95ms
step:99/2330 train_time:3840ms step_avg:38.79ms
step:100/2330 train_time:3896ms step_avg:38.96ms
step:101/2330 train_time:3919ms step_avg:38.80ms
step:102/2330 train_time:3975ms step_avg:38.98ms
step:103/2330 train_time:3998ms step_avg:38.81ms
step:104/2330 train_time:4055ms step_avg:38.99ms
step:105/2330 train_time:4078ms step_avg:38.84ms
step:106/2330 train_time:4135ms step_avg:39.01ms
step:107/2330 train_time:4157ms step_avg:38.85ms
step:108/2330 train_time:4215ms step_avg:39.03ms
step:109/2330 train_time:4238ms step_avg:38.88ms
step:110/2330 train_time:4295ms step_avg:39.05ms
step:111/2330 train_time:4317ms step_avg:38.89ms
step:112/2330 train_time:4374ms step_avg:39.06ms
step:113/2330 train_time:4396ms step_avg:38.91ms
step:114/2330 train_time:4453ms step_avg:39.06ms
step:115/2330 train_time:4475ms step_avg:38.91ms
step:116/2330 train_time:4531ms step_avg:39.06ms
step:117/2330 train_time:4553ms step_avg:38.92ms
step:118/2330 train_time:4609ms step_avg:39.06ms
step:119/2330 train_time:4631ms step_avg:38.92ms
step:120/2330 train_time:4687ms step_avg:39.06ms
step:121/2330 train_time:4710ms step_avg:38.92ms
step:122/2330 train_time:4765ms step_avg:39.05ms
step:123/2330 train_time:4787ms step_avg:38.92ms
step:124/2330 train_time:4843ms step_avg:39.06ms
step:125/2330 train_time:4866ms step_avg:38.93ms
step:126/2330 train_time:4923ms step_avg:39.07ms
step:127/2330 train_time:4946ms step_avg:38.95ms
step:128/2330 train_time:5002ms step_avg:39.08ms
step:129/2330 train_time:5025ms step_avg:38.95ms
step:130/2330 train_time:5081ms step_avg:39.09ms
step:131/2330 train_time:5104ms step_avg:38.96ms
step:132/2330 train_time:5160ms step_avg:39.09ms
step:133/2330 train_time:5183ms step_avg:38.97ms
step:134/2330 train_time:5239ms step_avg:39.10ms
step:135/2330 train_time:5262ms step_avg:38.98ms
step:136/2330 train_time:5319ms step_avg:39.11ms
step:137/2330 train_time:5343ms step_avg:39.00ms
step:138/2330 train_time:5399ms step_avg:39.12ms
step:139/2330 train_time:5421ms step_avg:39.00ms
step:140/2330 train_time:5478ms step_avg:39.13ms
step:141/2330 train_time:5500ms step_avg:39.01ms
step:142/2330 train_time:5557ms step_avg:39.13ms
step:143/2330 train_time:5579ms step_avg:39.01ms
step:144/2330 train_time:5635ms step_avg:39.13ms
step:145/2330 train_time:5657ms step_avg:39.01ms
step:146/2330 train_time:5714ms step_avg:39.14ms
step:147/2330 train_time:5736ms step_avg:39.02ms
step:148/2330 train_time:5793ms step_avg:39.14ms
step:149/2330 train_time:5815ms step_avg:39.03ms
step:150/2330 train_time:5872ms step_avg:39.15ms
step:151/2330 train_time:5894ms step_avg:39.03ms
step:152/2330 train_time:5950ms step_avg:39.15ms
step:153/2330 train_time:5972ms step_avg:39.04ms
step:154/2330 train_time:6029ms step_avg:39.15ms
step:155/2330 train_time:6051ms step_avg:39.04ms
step:156/2330 train_time:6106ms step_avg:39.14ms
step:157/2330 train_time:6129ms step_avg:39.04ms
step:158/2330 train_time:6185ms step_avg:39.15ms
step:159/2330 train_time:6208ms step_avg:39.05ms
step:160/2330 train_time:6264ms step_avg:39.15ms
step:161/2330 train_time:6286ms step_avg:39.05ms
step:162/2330 train_time:6342ms step_avg:39.15ms
step:163/2330 train_time:6365ms step_avg:39.05ms
step:164/2330 train_time:6422ms step_avg:39.16ms
step:165/2330 train_time:6445ms step_avg:39.06ms
step:166/2330 train_time:6502ms step_avg:39.17ms
step:167/2330 train_time:6524ms step_avg:39.07ms
step:168/2330 train_time:6581ms step_avg:39.17ms
step:169/2330 train_time:6603ms step_avg:39.07ms
step:170/2330 train_time:6660ms step_avg:39.18ms
step:171/2330 train_time:6682ms step_avg:39.08ms
step:172/2330 train_time:6739ms step_avg:39.18ms
step:173/2330 train_time:6762ms step_avg:39.09ms
step:174/2330 train_time:6818ms step_avg:39.18ms
step:175/2330 train_time:6841ms step_avg:39.09ms
step:176/2330 train_time:6898ms step_avg:39.19ms
step:177/2330 train_time:6920ms step_avg:39.10ms
step:178/2330 train_time:6977ms step_avg:39.20ms
step:179/2330 train_time:7000ms step_avg:39.11ms
step:180/2330 train_time:7058ms step_avg:39.21ms
step:181/2330 train_time:7080ms step_avg:39.12ms
step:182/2330 train_time:7137ms step_avg:39.21ms
step:183/2330 train_time:7159ms step_avg:39.12ms
step:184/2330 train_time:7216ms step_avg:39.22ms
step:185/2330 train_time:7238ms step_avg:39.12ms
step:186/2330 train_time:7295ms step_avg:39.22ms
step:187/2330 train_time:7318ms step_avg:39.13ms
step:188/2330 train_time:7374ms step_avg:39.22ms
step:189/2330 train_time:7397ms step_avg:39.14ms
step:190/2330 train_time:7454ms step_avg:39.23ms
step:191/2330 train_time:7476ms step_avg:39.14ms
step:192/2330 train_time:7533ms step_avg:39.23ms
step:193/2330 train_time:7554ms step_avg:39.14ms
step:194/2330 train_time:7611ms step_avg:39.23ms
step:195/2330 train_time:7633ms step_avg:39.14ms
step:196/2330 train_time:7689ms step_avg:39.23ms
step:197/2330 train_time:7711ms step_avg:39.14ms
step:198/2330 train_time:7767ms step_avg:39.23ms
step:199/2330 train_time:7790ms step_avg:39.14ms
step:200/2330 train_time:7845ms step_avg:39.23ms
step:201/2330 train_time:7869ms step_avg:39.15ms
step:202/2330 train_time:7924ms step_avg:39.23ms
step:203/2330 train_time:7947ms step_avg:39.15ms
step:204/2330 train_time:8003ms step_avg:39.23ms
step:205/2330 train_time:8026ms step_avg:39.15ms
step:206/2330 train_time:8083ms step_avg:39.24ms
step:207/2330 train_time:8106ms step_avg:39.16ms
step:208/2330 train_time:8162ms step_avg:39.24ms
step:209/2330 train_time:8184ms step_avg:39.16ms
step:210/2330 train_time:8240ms step_avg:39.24ms
step:211/2330 train_time:8263ms step_avg:39.16ms
step:212/2330 train_time:8320ms step_avg:39.25ms
step:213/2330 train_time:8342ms step_avg:39.17ms
step:214/2330 train_time:8399ms step_avg:39.25ms
step:215/2330 train_time:8421ms step_avg:39.17ms
step:216/2330 train_time:8478ms step_avg:39.25ms
step:217/2330 train_time:8501ms step_avg:39.17ms
step:218/2330 train_time:8558ms step_avg:39.26ms
step:219/2330 train_time:8580ms step_avg:39.18ms
step:220/2330 train_time:8636ms step_avg:39.26ms
step:221/2330 train_time:8659ms step_avg:39.18ms
step:222/2330 train_time:8715ms step_avg:39.26ms
step:223/2330 train_time:8737ms step_avg:39.18ms
step:224/2330 train_time:8794ms step_avg:39.26ms
step:225/2330 train_time:8817ms step_avg:39.19ms
step:226/2330 train_time:8875ms step_avg:39.27ms
step:227/2330 train_time:8897ms step_avg:39.19ms
step:228/2330 train_time:8953ms step_avg:39.27ms
step:229/2330 train_time:8976ms step_avg:39.19ms
step:230/2330 train_time:9033ms step_avg:39.27ms
step:231/2330 train_time:9055ms step_avg:39.20ms
step:232/2330 train_time:9111ms step_avg:39.27ms
step:233/2330 train_time:9134ms step_avg:39.20ms
step:234/2330 train_time:9190ms step_avg:39.27ms
step:235/2330 train_time:9212ms step_avg:39.20ms
step:236/2330 train_time:9268ms step_avg:39.27ms
step:237/2330 train_time:9290ms step_avg:39.20ms
step:238/2330 train_time:9346ms step_avg:39.27ms
step:239/2330 train_time:9369ms step_avg:39.20ms
step:240/2330 train_time:9425ms step_avg:39.27ms
step:241/2330 train_time:9449ms step_avg:39.21ms
step:242/2330 train_time:9505ms step_avg:39.28ms
step:243/2330 train_time:9528ms step_avg:39.21ms
step:244/2330 train_time:9584ms step_avg:39.28ms
step:245/2330 train_time:9607ms step_avg:39.21ms
step:246/2330 train_time:9663ms step_avg:39.28ms
step:247/2330 train_time:9687ms step_avg:39.22ms
step:248/2330 train_time:9743ms step_avg:39.28ms
step:249/2330 train_time:9766ms step_avg:39.22ms
step:250/2330 train_time:9821ms step_avg:39.29ms
step:250/2330 val_loss:5.5090 train_time:9918ms step_avg:39.67ms
step:251/2330 train_time:9929ms step_avg:39.56ms
step:252/2330 train_time:9940ms step_avg:39.45ms
step:253/2330 train_time:9949ms step_avg:39.33ms
step:254/2330 train_time:9981ms step_avg:39.30ms
step:255/2330 train_time:10003ms step_avg:39.23ms
step:256/2330 train_time:10058ms step_avg:39.29ms
step:257/2330 train_time:10081ms step_avg:39.22ms
step:258/2330 train_time:10135ms step_avg:39.28ms
step:259/2330 train_time:10157ms step_avg:39.22ms
step:260/2330 train_time:10213ms step_avg:39.28ms
step:261/2330 train_time:10238ms step_avg:39.23ms
step:262/2330 train_time:10299ms step_avg:39.31ms
step:263/2330 train_time:10324ms step_avg:39.25ms
step:264/2330 train_time:10381ms step_avg:39.32ms
step:265/2330 train_time:10405ms step_avg:39.27ms
step:266/2330 train_time:10461ms step_avg:39.33ms
step:267/2330 train_time:10484ms step_avg:39.27ms
step:268/2330 train_time:10541ms step_avg:39.33ms
step:269/2330 train_time:10564ms step_avg:39.27ms
step:270/2330 train_time:10620ms step_avg:39.33ms
step:271/2330 train_time:10644ms step_avg:39.27ms
step:272/2330 train_time:10700ms step_avg:39.34ms
step:273/2330 train_time:10722ms step_avg:39.28ms
step:274/2330 train_time:10778ms step_avg:39.33ms
step:275/2330 train_time:10800ms step_avg:39.27ms
step:276/2330 train_time:10855ms step_avg:39.33ms
step:277/2330 train_time:10878ms step_avg:39.27ms
step:278/2330 train_time:10934ms step_avg:39.33ms
step:279/2330 train_time:10956ms step_avg:39.27ms
step:280/2330 train_time:11012ms step_avg:39.33ms
step:281/2330 train_time:11034ms step_avg:39.27ms
step:282/2330 train_time:11090ms step_avg:39.32ms
step:283/2330 train_time:11111ms step_avg:39.26ms
step:284/2330 train_time:11168ms step_avg:39.32ms
step:285/2330 train_time:11191ms step_avg:39.27ms
step:286/2330 train_time:11248ms step_avg:39.33ms
step:287/2330 train_time:11271ms step_avg:39.27ms
step:288/2330 train_time:11329ms step_avg:39.34ms
step:289/2330 train_time:11352ms step_avg:39.28ms
step:290/2330 train_time:11409ms step_avg:39.34ms
step:291/2330 train_time:11432ms step_avg:39.28ms
step:292/2330 train_time:11489ms step_avg:39.34ms
step:293/2330 train_time:11511ms step_avg:39.29ms
step:294/2330 train_time:11567ms step_avg:39.34ms
step:295/2330 train_time:11589ms step_avg:39.29ms
step:296/2330 train_time:11646ms step_avg:39.34ms
step:297/2330 train_time:11667ms step_avg:39.28ms
step:298/2330 train_time:11724ms step_avg:39.34ms
step:299/2330 train_time:11746ms step_avg:39.28ms
step:300/2330 train_time:11801ms step_avg:39.34ms
step:301/2330 train_time:11823ms step_avg:39.28ms
step:302/2330 train_time:11880ms step_avg:39.34ms
step:303/2330 train_time:11902ms step_avg:39.28ms
step:304/2330 train_time:11958ms step_avg:39.34ms
step:305/2330 train_time:11981ms step_avg:39.28ms
step:306/2330 train_time:12036ms step_avg:39.33ms
step:307/2330 train_time:12059ms step_avg:39.28ms
step:308/2330 train_time:12115ms step_avg:39.34ms
step:309/2330 train_time:12138ms step_avg:39.28ms
step:310/2330 train_time:12195ms step_avg:39.34ms
step:311/2330 train_time:12217ms step_avg:39.28ms
step:312/2330 train_time:12274ms step_avg:39.34ms
step:313/2330 train_time:12297ms step_avg:39.29ms
step:314/2330 train_time:12355ms step_avg:39.35ms
step:315/2330 train_time:12379ms step_avg:39.30ms
step:316/2330 train_time:12436ms step_avg:39.35ms
step:317/2330 train_time:12458ms step_avg:39.30ms
step:318/2330 train_time:12516ms step_avg:39.36ms
step:319/2330 train_time:12539ms step_avg:39.31ms
step:320/2330 train_time:12596ms step_avg:39.36ms
step:321/2330 train_time:12619ms step_avg:39.31ms
step:322/2330 train_time:12675ms step_avg:39.36ms
step:323/2330 train_time:12697ms step_avg:39.31ms
step:324/2330 train_time:12754ms step_avg:39.36ms
step:325/2330 train_time:12776ms step_avg:39.31ms
step:326/2330 train_time:12833ms step_avg:39.36ms
step:327/2330 train_time:12855ms step_avg:39.31ms
step:328/2330 train_time:12911ms step_avg:39.36ms
step:329/2330 train_time:12933ms step_avg:39.31ms
step:330/2330 train_time:12989ms step_avg:39.36ms
step:331/2330 train_time:13011ms step_avg:39.31ms
step:332/2330 train_time:13067ms step_avg:39.36ms
step:333/2330 train_time:13089ms step_avg:39.31ms
step:334/2330 train_time:13145ms step_avg:39.36ms
step:335/2330 train_time:13167ms step_avg:39.30ms
step:336/2330 train_time:13222ms step_avg:39.35ms
step:337/2330 train_time:13245ms step_avg:39.30ms
step:338/2330 train_time:13301ms step_avg:39.35ms
step:339/2330 train_time:13324ms step_avg:39.30ms
step:340/2330 train_time:13380ms step_avg:39.35ms
step:341/2330 train_time:13402ms step_avg:39.30ms
step:342/2330 train_time:13459ms step_avg:39.35ms
step:343/2330 train_time:13482ms step_avg:39.31ms
step:344/2330 train_time:13538ms step_avg:39.36ms
step:345/2330 train_time:13560ms step_avg:39.31ms
step:346/2330 train_time:13617ms step_avg:39.35ms
step:347/2330 train_time:13639ms step_avg:39.31ms
step:348/2330 train_time:13695ms step_avg:39.35ms
step:349/2330 train_time:13718ms step_avg:39.31ms
step:350/2330 train_time:13774ms step_avg:39.35ms
step:351/2330 train_time:13796ms step_avg:39.31ms
step:352/2330 train_time:13852ms step_avg:39.35ms
step:353/2330 train_time:13874ms step_avg:39.30ms
step:354/2330 train_time:13931ms step_avg:39.35ms
step:355/2330 train_time:13953ms step_avg:39.30ms
step:356/2330 train_time:14010ms step_avg:39.35ms
step:357/2330 train_time:14032ms step_avg:39.30ms
step:358/2330 train_time:14087ms step_avg:39.35ms
step:359/2330 train_time:14109ms step_avg:39.30ms
step:360/2330 train_time:14166ms step_avg:39.35ms
step:361/2330 train_time:14188ms step_avg:39.30ms
step:362/2330 train_time:14245ms step_avg:39.35ms
step:363/2330 train_time:14267ms step_avg:39.30ms
step:364/2330 train_time:14325ms step_avg:39.35ms
step:365/2330 train_time:14347ms step_avg:39.31ms
step:366/2330 train_time:14403ms step_avg:39.35ms
step:367/2330 train_time:14426ms step_avg:39.31ms
step:368/2330 train_time:14482ms step_avg:39.35ms
step:369/2330 train_time:14504ms step_avg:39.31ms
step:370/2330 train_time:14560ms step_avg:39.35ms
step:371/2330 train_time:14583ms step_avg:39.31ms
step:372/2330 train_time:14639ms step_avg:39.35ms
step:373/2330 train_time:14662ms step_avg:39.31ms
step:374/2330 train_time:14719ms step_avg:39.36ms
step:375/2330 train_time:14742ms step_avg:39.31ms
step:376/2330 train_time:14798ms step_avg:39.36ms
step:377/2330 train_time:14821ms step_avg:39.31ms
step:378/2330 train_time:14878ms step_avg:39.36ms
step:379/2330 train_time:14901ms step_avg:39.32ms
step:380/2330 train_time:14957ms step_avg:39.36ms
step:381/2330 train_time:14980ms step_avg:39.32ms
step:382/2330 train_time:15035ms step_avg:39.36ms
step:383/2330 train_time:15058ms step_avg:39.32ms
step:384/2330 train_time:15114ms step_avg:39.36ms
step:385/2330 train_time:15137ms step_avg:39.32ms
step:386/2330 train_time:15194ms step_avg:39.36ms
step:387/2330 train_time:15217ms step_avg:39.32ms
step:388/2330 train_time:15273ms step_avg:39.36ms
step:389/2330 train_time:15296ms step_avg:39.32ms
step:390/2330 train_time:15353ms step_avg:39.37ms
step:391/2330 train_time:15375ms step_avg:39.32ms
step:392/2330 train_time:15432ms step_avg:39.37ms
step:393/2330 train_time:15454ms step_avg:39.32ms
step:394/2330 train_time:15511ms step_avg:39.37ms
step:395/2330 train_time:15533ms step_avg:39.32ms
step:396/2330 train_time:15590ms step_avg:39.37ms
step:397/2330 train_time:15612ms step_avg:39.33ms
step:398/2330 train_time:15669ms step_avg:39.37ms
step:399/2330 train_time:15691ms step_avg:39.33ms
step:400/2330 train_time:15748ms step_avg:39.37ms
step:401/2330 train_time:15770ms step_avg:39.33ms
step:402/2330 train_time:15826ms step_avg:39.37ms
step:403/2330 train_time:15848ms step_avg:39.33ms
step:404/2330 train_time:15904ms step_avg:39.37ms
step:405/2330 train_time:15926ms step_avg:39.32ms
step:406/2330 train_time:15981ms step_avg:39.36ms
step:407/2330 train_time:16004ms step_avg:39.32ms
step:408/2330 train_time:16060ms step_avg:39.36ms
step:409/2330 train_time:16083ms step_avg:39.32ms
step:410/2330 train_time:16138ms step_avg:39.36ms
step:411/2330 train_time:16161ms step_avg:39.32ms
step:412/2330 train_time:16218ms step_avg:39.36ms
step:413/2330 train_time:16240ms step_avg:39.32ms
step:414/2330 train_time:16296ms step_avg:39.36ms
step:415/2330 train_time:16318ms step_avg:39.32ms
step:416/2330 train_time:16374ms step_avg:39.36ms
step:417/2330 train_time:16397ms step_avg:39.32ms
step:418/2330 train_time:16453ms step_avg:39.36ms
step:419/2330 train_time:16476ms step_avg:39.32ms
step:420/2330 train_time:16533ms step_avg:39.36ms
step:421/2330 train_time:16555ms step_avg:39.32ms
step:422/2330 train_time:16611ms step_avg:39.36ms
step:423/2330 train_time:16633ms step_avg:39.32ms
step:424/2330 train_time:16690ms step_avg:39.36ms
step:425/2330 train_time:16712ms step_avg:39.32ms
step:426/2330 train_time:16769ms step_avg:39.36ms
step:427/2330 train_time:16791ms step_avg:39.32ms
step:428/2330 train_time:16848ms step_avg:39.36ms
step:429/2330 train_time:16870ms step_avg:39.32ms
step:430/2330 train_time:16926ms step_avg:39.36ms
step:431/2330 train_time:16948ms step_avg:39.32ms
step:432/2330 train_time:17005ms step_avg:39.36ms
step:433/2330 train_time:17027ms step_avg:39.32ms
step:434/2330 train_time:17083ms step_avg:39.36ms
step:435/2330 train_time:17105ms step_avg:39.32ms
step:436/2330 train_time:17161ms step_avg:39.36ms
step:437/2330 train_time:17184ms step_avg:39.32ms
step:438/2330 train_time:17240ms step_avg:39.36ms
step:439/2330 train_time:17263ms step_avg:39.32ms
step:440/2330 train_time:17319ms step_avg:39.36ms
step:441/2330 train_time:17342ms step_avg:39.32ms
step:442/2330 train_time:17399ms step_avg:39.36ms
step:443/2330 train_time:17421ms step_avg:39.33ms
step:444/2330 train_time:17478ms step_avg:39.36ms
step:445/2330 train_time:17501ms step_avg:39.33ms
step:446/2330 train_time:17557ms step_avg:39.37ms
step:447/2330 train_time:17580ms step_avg:39.33ms
step:448/2330 train_time:17636ms step_avg:39.37ms
step:449/2330 train_time:17658ms step_avg:39.33ms
step:450/2330 train_time:17715ms step_avg:39.37ms
step:451/2330 train_time:17737ms step_avg:39.33ms
step:452/2330 train_time:17794ms step_avg:39.37ms
step:453/2330 train_time:17817ms step_avg:39.33ms
step:454/2330 train_time:17873ms step_avg:39.37ms
step:455/2330 train_time:17896ms step_avg:39.33ms
step:456/2330 train_time:17953ms step_avg:39.37ms
step:457/2330 train_time:17975ms step_avg:39.33ms
step:458/2330 train_time:18031ms step_avg:39.37ms
step:459/2330 train_time:18053ms step_avg:39.33ms
step:460/2330 train_time:18110ms step_avg:39.37ms
step:461/2330 train_time:18132ms step_avg:39.33ms
step:462/2330 train_time:18188ms step_avg:39.37ms
step:463/2330 train_time:18211ms step_avg:39.33ms
step:464/2330 train_time:18267ms step_avg:39.37ms
step:465/2330 train_time:18289ms step_avg:39.33ms
step:466/2330 train_time:18346ms step_avg:39.37ms
step:467/2330 train_time:18368ms step_avg:39.33ms
step:468/2330 train_time:18425ms step_avg:39.37ms
step:469/2330 train_time:18446ms step_avg:39.33ms
step:470/2330 train_time:18502ms step_avg:39.37ms
step:471/2330 train_time:18524ms step_avg:39.33ms
step:472/2330 train_time:18580ms step_avg:39.36ms
step:473/2330 train_time:18603ms step_avg:39.33ms
step:474/2330 train_time:18659ms step_avg:39.37ms
step:475/2330 train_time:18682ms step_avg:39.33ms
step:476/2330 train_time:18738ms step_avg:39.36ms
step:477/2330 train_time:18760ms step_avg:39.33ms
step:478/2330 train_time:18817ms step_avg:39.37ms
step:479/2330 train_time:18839ms step_avg:39.33ms
step:480/2330 train_time:18896ms step_avg:39.37ms
step:481/2330 train_time:18919ms step_avg:39.33ms
step:482/2330 train_time:18975ms step_avg:39.37ms
step:483/2330 train_time:18998ms step_avg:39.33ms
step:484/2330 train_time:19054ms step_avg:39.37ms
step:485/2330 train_time:19077ms step_avg:39.33ms
step:486/2330 train_time:19133ms step_avg:39.37ms
step:487/2330 train_time:19156ms step_avg:39.33ms
step:488/2330 train_time:19211ms step_avg:39.37ms
step:489/2330 train_time:19234ms step_avg:39.33ms
step:490/2330 train_time:19291ms step_avg:39.37ms
step:491/2330 train_time:19313ms step_avg:39.33ms
step:492/2330 train_time:19371ms step_avg:39.37ms
step:493/2330 train_time:19392ms step_avg:39.34ms
step:494/2330 train_time:19449ms step_avg:39.37ms
step:495/2330 train_time:19471ms step_avg:39.34ms
step:496/2330 train_time:19527ms step_avg:39.37ms
step:497/2330 train_time:19550ms step_avg:39.34ms
step:498/2330 train_time:19606ms step_avg:39.37ms
step:499/2330 train_time:19630ms step_avg:39.34ms
step:500/2330 train_time:19686ms step_avg:39.37ms
step:500/2330 val_loss:5.3754 train_time:19781ms step_avg:39.56ms
step:501/2330 train_time:19795ms step_avg:39.51ms
step:502/2330 train_time:19806ms step_avg:39.45ms
step:503/2330 train_time:19818ms step_avg:39.40ms
step:504/2330 train_time:19844ms step_avg:39.37ms
step:505/2330 train_time:19865ms step_avg:39.34ms
step:506/2330 train_time:19920ms step_avg:39.37ms
step:507/2330 train_time:19942ms step_avg:39.33ms
step:508/2330 train_time:19997ms step_avg:39.36ms
step:509/2330 train_time:20018ms step_avg:39.33ms
step:510/2330 train_time:20074ms step_avg:39.36ms
step:511/2330 train_time:20100ms step_avg:39.33ms
step:512/2330 train_time:20161ms step_avg:39.38ms
step:513/2330 train_time:20185ms step_avg:39.35ms
step:514/2330 train_time:20242ms step_avg:39.38ms
step:515/2330 train_time:20266ms step_avg:39.35ms
step:516/2330 train_time:20321ms step_avg:39.38ms
step:517/2330 train_time:20344ms step_avg:39.35ms
step:518/2330 train_time:20399ms step_avg:39.38ms
step:519/2330 train_time:20421ms step_avg:39.35ms
step:520/2330 train_time:20477ms step_avg:39.38ms
step:521/2330 train_time:20499ms step_avg:39.35ms
step:522/2330 train_time:20555ms step_avg:39.38ms
step:523/2330 train_time:20576ms step_avg:39.34ms
step:524/2330 train_time:20631ms step_avg:39.37ms
step:525/2330 train_time:20653ms step_avg:39.34ms
step:526/2330 train_time:20709ms step_avg:39.37ms
step:527/2330 train_time:20733ms step_avg:39.34ms
step:528/2330 train_time:20790ms step_avg:39.37ms
step:529/2330 train_time:20812ms step_avg:39.34ms
step:530/2330 train_time:20868ms step_avg:39.37ms
step:531/2330 train_time:20890ms step_avg:39.34ms
step:532/2330 train_time:20946ms step_avg:39.37ms
step:533/2330 train_time:20968ms step_avg:39.34ms
step:534/2330 train_time:21025ms step_avg:39.37ms
step:535/2330 train_time:21047ms step_avg:39.34ms
step:536/2330 train_time:21106ms step_avg:39.38ms
step:537/2330 train_time:21129ms step_avg:39.35ms
step:538/2330 train_time:21186ms step_avg:39.38ms
step:539/2330 train_time:21208ms step_avg:39.35ms
step:540/2330 train_time:21264ms step_avg:39.38ms
step:541/2330 train_time:21287ms step_avg:39.35ms
step:542/2330 train_time:21343ms step_avg:39.38ms
step:543/2330 train_time:21364ms step_avg:39.35ms
step:544/2330 train_time:21420ms step_avg:39.37ms
step:545/2330 train_time:21443ms step_avg:39.34ms
step:546/2330 train_time:21499ms step_avg:39.37ms
step:547/2330 train_time:21521ms step_avg:39.34ms
step:548/2330 train_time:21577ms step_avg:39.37ms
step:549/2330 train_time:21600ms step_avg:39.34ms
step:550/2330 train_time:21655ms step_avg:39.37ms
step:551/2330 train_time:21678ms step_avg:39.34ms
step:552/2330 train_time:21734ms step_avg:39.37ms
step:553/2330 train_time:21756ms step_avg:39.34ms
step:554/2330 train_time:21812ms step_avg:39.37ms
step:555/2330 train_time:21835ms step_avg:39.34ms
step:556/2330 train_time:21891ms step_avg:39.37ms
step:557/2330 train_time:21913ms step_avg:39.34ms
step:558/2330 train_time:21969ms step_avg:39.37ms
step:559/2330 train_time:21992ms step_avg:39.34ms
step:560/2330 train_time:22049ms step_avg:39.37ms
step:561/2330 train_time:22072ms step_avg:39.34ms
step:562/2330 train_time:22130ms step_avg:39.38ms
step:563/2330 train_time:22152ms step_avg:39.35ms
step:564/2330 train_time:22209ms step_avg:39.38ms
step:565/2330 train_time:22231ms step_avg:39.35ms
step:566/2330 train_time:22288ms step_avg:39.38ms
step:567/2330 train_time:22311ms step_avg:39.35ms
step:568/2330 train_time:22367ms step_avg:39.38ms
step:569/2330 train_time:22389ms step_avg:39.35ms
step:570/2330 train_time:22446ms step_avg:39.38ms
step:571/2330 train_time:22468ms step_avg:39.35ms
step:572/2330 train_time:22525ms step_avg:39.38ms
step:573/2330 train_time:22547ms step_avg:39.35ms
step:574/2330 train_time:22602ms step_avg:39.38ms
step:575/2330 train_time:22625ms step_avg:39.35ms
step:576/2330 train_time:22680ms step_avg:39.37ms
step:577/2330 train_time:22703ms step_avg:39.35ms
step:578/2330 train_time:22759ms step_avg:39.37ms
step:579/2330 train_time:22781ms step_avg:39.35ms
step:580/2330 train_time:22837ms step_avg:39.37ms
step:581/2330 train_time:22860ms step_avg:39.35ms
step:582/2330 train_time:22916ms step_avg:39.38ms
step:583/2330 train_time:22939ms step_avg:39.35ms
step:584/2330 train_time:22995ms step_avg:39.38ms
step:585/2330 train_time:23019ms step_avg:39.35ms
step:586/2330 train_time:23075ms step_avg:39.38ms
step:587/2330 train_time:23097ms step_avg:39.35ms
step:588/2330 train_time:23153ms step_avg:39.38ms
step:589/2330 train_time:23176ms step_avg:39.35ms
step:590/2330 train_time:23233ms step_avg:39.38ms
step:591/2330 train_time:23256ms step_avg:39.35ms
step:592/2330 train_time:23313ms step_avg:39.38ms
step:593/2330 train_time:23336ms step_avg:39.35ms
step:594/2330 train_time:23393ms step_avg:39.38ms
step:595/2330 train_time:23417ms step_avg:39.36ms
step:596/2330 train_time:23474ms step_avg:39.39ms
step:597/2330 train_time:23496ms step_avg:39.36ms
step:598/2330 train_time:23553ms step_avg:39.39ms
step:599/2330 train_time:23575ms step_avg:39.36ms
step:600/2330 train_time:23632ms step_avg:39.39ms
step:601/2330 train_time:23653ms step_avg:39.36ms
step:602/2330 train_time:23710ms step_avg:39.38ms
step:603/2330 train_time:23732ms step_avg:39.36ms
step:604/2330 train_time:23788ms step_avg:39.38ms
step:605/2330 train_time:23810ms step_avg:39.36ms
step:606/2330 train_time:23866ms step_avg:39.38ms
step:607/2330 train_time:23888ms step_avg:39.35ms
step:608/2330 train_time:23945ms step_avg:39.38ms
step:609/2330 train_time:23967ms step_avg:39.36ms
step:610/2330 train_time:24024ms step_avg:39.38ms
step:611/2330 train_time:24045ms step_avg:39.35ms
step:612/2330 train_time:24101ms step_avg:39.38ms
step:613/2330 train_time:24124ms step_avg:39.35ms
step:614/2330 train_time:24179ms step_avg:39.38ms
step:615/2330 train_time:24203ms step_avg:39.35ms
step:616/2330 train_time:24259ms step_avg:39.38ms
step:617/2330 train_time:24282ms step_avg:39.36ms
step:618/2330 train_time:24339ms step_avg:39.38ms
step:619/2330 train_time:24361ms step_avg:39.36ms
step:620/2330 train_time:24417ms step_avg:39.38ms
step:621/2330 train_time:24440ms step_avg:39.36ms
step:622/2330 train_time:24496ms step_avg:39.38ms
step:623/2330 train_time:24518ms step_avg:39.36ms
step:624/2330 train_time:24575ms step_avg:39.38ms
step:625/2330 train_time:24597ms step_avg:39.36ms
step:626/2330 train_time:24653ms step_avg:39.38ms
step:627/2330 train_time:24675ms step_avg:39.35ms
step:628/2330 train_time:24731ms step_avg:39.38ms
step:629/2330 train_time:24754ms step_avg:39.35ms
step:630/2330 train_time:24810ms step_avg:39.38ms
step:631/2330 train_time:24832ms step_avg:39.35ms
step:632/2330 train_time:24889ms step_avg:39.38ms
step:633/2330 train_time:24910ms step_avg:39.35ms
step:634/2330 train_time:24967ms step_avg:39.38ms
step:635/2330 train_time:24989ms step_avg:39.35ms
step:636/2330 train_time:25046ms step_avg:39.38ms
step:637/2330 train_time:25067ms step_avg:39.35ms
step:638/2330 train_time:25124ms step_avg:39.38ms
step:639/2330 train_time:25146ms step_avg:39.35ms
step:640/2330 train_time:25202ms step_avg:39.38ms
step:641/2330 train_time:25224ms step_avg:39.35ms
step:642/2330 train_time:25280ms step_avg:39.38ms
step:643/2330 train_time:25302ms step_avg:39.35ms
step:644/2330 train_time:25359ms step_avg:39.38ms
step:645/2330 train_time:25381ms step_avg:39.35ms
step:646/2330 train_time:25437ms step_avg:39.38ms
step:647/2330 train_time:25459ms step_avg:39.35ms
step:648/2330 train_time:25516ms step_avg:39.38ms
step:649/2330 train_time:25538ms step_avg:39.35ms
step:650/2330 train_time:25594ms step_avg:39.37ms
step:651/2330 train_time:25617ms step_avg:39.35ms
step:652/2330 train_time:25672ms step_avg:39.37ms
step:653/2330 train_time:25695ms step_avg:39.35ms
step:654/2330 train_time:25752ms step_avg:39.38ms
step:655/2330 train_time:25774ms step_avg:39.35ms
step:656/2330 train_time:25830ms step_avg:39.37ms
step:657/2330 train_time:25852ms step_avg:39.35ms
step:658/2330 train_time:25908ms step_avg:39.37ms
step:659/2330 train_time:25930ms step_avg:39.35ms
step:660/2330 train_time:25987ms step_avg:39.37ms
step:661/2330 train_time:26009ms step_avg:39.35ms
step:662/2330 train_time:26065ms step_avg:39.37ms
step:663/2330 train_time:26087ms step_avg:39.35ms
step:664/2330 train_time:26144ms step_avg:39.37ms
step:665/2330 train_time:26166ms step_avg:39.35ms
step:666/2330 train_time:26223ms step_avg:39.37ms
step:667/2330 train_time:26245ms step_avg:39.35ms
step:668/2330 train_time:26301ms step_avg:39.37ms
step:669/2330 train_time:26323ms step_avg:39.35ms
step:670/2330 train_time:26379ms step_avg:39.37ms
step:671/2330 train_time:26402ms step_avg:39.35ms
step:672/2330 train_time:26458ms step_avg:39.37ms
step:673/2330 train_time:26481ms step_avg:39.35ms
step:674/2330 train_time:26539ms step_avg:39.37ms
step:675/2330 train_time:26562ms step_avg:39.35ms
step:676/2330 train_time:26618ms step_avg:39.38ms
step:677/2330 train_time:26641ms step_avg:39.35ms
step:678/2330 train_time:26697ms step_avg:39.38ms
step:679/2330 train_time:26719ms step_avg:39.35ms
step:680/2330 train_time:26776ms step_avg:39.38ms
step:681/2330 train_time:26799ms step_avg:39.35ms
step:682/2330 train_time:26855ms step_avg:39.38ms
step:683/2330 train_time:26877ms step_avg:39.35ms
step:684/2330 train_time:26933ms step_avg:39.38ms
step:685/2330 train_time:26956ms step_avg:39.35ms
step:686/2330 train_time:27013ms step_avg:39.38ms
step:687/2330 train_time:27036ms step_avg:39.35ms
step:688/2330 train_time:27092ms step_avg:39.38ms
step:689/2330 train_time:27116ms step_avg:39.36ms
step:690/2330 train_time:27172ms step_avg:39.38ms
step:691/2330 train_time:27195ms step_avg:39.36ms
step:692/2330 train_time:27251ms step_avg:39.38ms
step:693/2330 train_time:27273ms step_avg:39.36ms
step:694/2330 train_time:27330ms step_avg:39.38ms
step:695/2330 train_time:27352ms step_avg:39.35ms
step:696/2330 train_time:27409ms step_avg:39.38ms
step:697/2330 train_time:27431ms step_avg:39.36ms
step:698/2330 train_time:27490ms step_avg:39.38ms
step:699/2330 train_time:27513ms step_avg:39.36ms
step:700/2330 train_time:27569ms step_avg:39.38ms
step:701/2330 train_time:27591ms step_avg:39.36ms
step:702/2330 train_time:27648ms step_avg:39.38ms
step:703/2330 train_time:27670ms step_avg:39.36ms
step:704/2330 train_time:27726ms step_avg:39.38ms
step:705/2330 train_time:27748ms step_avg:39.36ms
step:706/2330 train_time:27805ms step_avg:39.38ms
step:707/2330 train_time:27826ms step_avg:39.36ms
step:708/2330 train_time:27882ms step_avg:39.38ms
step:709/2330 train_time:27904ms step_avg:39.36ms
step:710/2330 train_time:27960ms step_avg:39.38ms
step:711/2330 train_time:27982ms step_avg:39.36ms
step:712/2330 train_time:28038ms step_avg:39.38ms
step:713/2330 train_time:28061ms step_avg:39.36ms
step:714/2330 train_time:28117ms step_avg:39.38ms
step:715/2330 train_time:28140ms step_avg:39.36ms
step:716/2330 train_time:28196ms step_avg:39.38ms
step:717/2330 train_time:28218ms step_avg:39.36ms
step:718/2330 train_time:28274ms step_avg:39.38ms
step:719/2330 train_time:28297ms step_avg:39.36ms
step:720/2330 train_time:28353ms step_avg:39.38ms
step:721/2330 train_time:28376ms step_avg:39.36ms
step:722/2330 train_time:28432ms step_avg:39.38ms
step:723/2330 train_time:28455ms step_avg:39.36ms
step:724/2330 train_time:28511ms step_avg:39.38ms
step:725/2330 train_time:28533ms step_avg:39.36ms
step:726/2330 train_time:28589ms step_avg:39.38ms
step:727/2330 train_time:28611ms step_avg:39.36ms
step:728/2330 train_time:28668ms step_avg:39.38ms
step:729/2330 train_time:28689ms step_avg:39.35ms
step:730/2330 train_time:28746ms step_avg:39.38ms
step:731/2330 train_time:28768ms step_avg:39.35ms
step:732/2330 train_time:28824ms step_avg:39.38ms
step:733/2330 train_time:28846ms step_avg:39.35ms
step:734/2330 train_time:28902ms step_avg:39.38ms
step:735/2330 train_time:28924ms step_avg:39.35ms
step:736/2330 train_time:28980ms step_avg:39.37ms
step:737/2330 train_time:29002ms step_avg:39.35ms
step:738/2330 train_time:29058ms step_avg:39.37ms
step:739/2330 train_time:29081ms step_avg:39.35ms
step:740/2330 train_time:29137ms step_avg:39.37ms
step:741/2330 train_time:29160ms step_avg:39.35ms
step:742/2330 train_time:29216ms step_avg:39.37ms
step:743/2330 train_time:29238ms step_avg:39.35ms
step:744/2330 train_time:29295ms step_avg:39.37ms
step:745/2330 train_time:29317ms step_avg:39.35ms
step:746/2330 train_time:29374ms step_avg:39.38ms
step:747/2330 train_time:29397ms step_avg:39.35ms
step:748/2330 train_time:29452ms step_avg:39.37ms
step:749/2330 train_time:29475ms step_avg:39.35ms
step:750/2330 train_time:29531ms step_avg:39.37ms
step:750/2330 val_loss:5.2796 train_time:29626ms step_avg:39.50ms
step:751/2330 train_time:29639ms step_avg:39.47ms
step:752/2330 train_time:29651ms step_avg:39.43ms
step:753/2330 train_time:29662ms step_avg:39.39ms
step:754/2330 train_time:29690ms step_avg:39.38ms
step:755/2330 train_time:29711ms step_avg:39.35ms
step:756/2330 train_time:29765ms step_avg:39.37ms
step:757/2330 train_time:29787ms step_avg:39.35ms
step:758/2330 train_time:29842ms step_avg:39.37ms
step:759/2330 train_time:29864ms step_avg:39.35ms
step:760/2330 train_time:29920ms step_avg:39.37ms
step:761/2330 train_time:29944ms step_avg:39.35ms
step:762/2330 train_time:30004ms step_avg:39.38ms
step:763/2330 train_time:30029ms step_avg:39.36ms
step:764/2330 train_time:30086ms step_avg:39.38ms
step:765/2330 train_time:30110ms step_avg:39.36ms
step:766/2330 train_time:30166ms step_avg:39.38ms
step:767/2330 train_time:30188ms step_avg:39.36ms
step:768/2330 train_time:30244ms step_avg:39.38ms
step:769/2330 train_time:30266ms step_avg:39.36ms
step:770/2330 train_time:30322ms step_avg:39.38ms
step:771/2330 train_time:30343ms step_avg:39.36ms
step:772/2330 train_time:30399ms step_avg:39.38ms
step:773/2330 train_time:30421ms step_avg:39.35ms
step:774/2330 train_time:30477ms step_avg:39.38ms
step:775/2330 train_time:30499ms step_avg:39.35ms
step:776/2330 train_time:30555ms step_avg:39.37ms
step:777/2330 train_time:30577ms step_avg:39.35ms
step:778/2330 train_time:30634ms step_avg:39.38ms
step:779/2330 train_time:30656ms step_avg:39.35ms
step:780/2330 train_time:30712ms step_avg:39.37ms
step:781/2330 train_time:30734ms step_avg:39.35ms
step:782/2330 train_time:30790ms step_avg:39.37ms
step:783/2330 train_time:30811ms step_avg:39.35ms
step:784/2330 train_time:30868ms step_avg:39.37ms
step:785/2330 train_time:30890ms step_avg:39.35ms
step:786/2330 train_time:30946ms step_avg:39.37ms
step:787/2330 train_time:30970ms step_avg:39.35ms
step:788/2330 train_time:31026ms step_avg:39.37ms
step:789/2330 train_time:31048ms step_avg:39.35ms
step:790/2330 train_time:31104ms step_avg:39.37ms
step:791/2330 train_time:31127ms step_avg:39.35ms
step:792/2330 train_time:31184ms step_avg:39.37ms
step:793/2330 train_time:31207ms step_avg:39.35ms
step:794/2330 train_time:31264ms step_avg:39.37ms
step:795/2330 train_time:31286ms step_avg:39.35ms
step:796/2330 train_time:31342ms step_avg:39.37ms
step:797/2330 train_time:31365ms step_avg:39.35ms
step:798/2330 train_time:31420ms step_avg:39.37ms
step:799/2330 train_time:31443ms step_avg:39.35ms
step:800/2330 train_time:31499ms step_avg:39.37ms
step:801/2330 train_time:31522ms step_avg:39.35ms
step:802/2330 train_time:31578ms step_avg:39.37ms
step:803/2330 train_time:31600ms step_avg:39.35ms
step:804/2330 train_time:31656ms step_avg:39.37ms
step:805/2330 train_time:31679ms step_avg:39.35ms
step:806/2330 train_time:31734ms step_avg:39.37ms
step:807/2330 train_time:31757ms step_avg:39.35ms
step:808/2330 train_time:31813ms step_avg:39.37ms
step:809/2330 train_time:31836ms step_avg:39.35ms
step:810/2330 train_time:31893ms step_avg:39.37ms
step:811/2330 train_time:31915ms step_avg:39.35ms
step:812/2330 train_time:31972ms step_avg:39.37ms
step:813/2330 train_time:31994ms step_avg:39.35ms
step:814/2330 train_time:32051ms step_avg:39.37ms
step:815/2330 train_time:32073ms step_avg:39.35ms
step:816/2330 train_time:32130ms step_avg:39.38ms
step:817/2330 train_time:32152ms step_avg:39.35ms
step:818/2330 train_time:32209ms step_avg:39.38ms
step:819/2330 train_time:32231ms step_avg:39.35ms
step:820/2330 train_time:32287ms step_avg:39.37ms
step:821/2330 train_time:32309ms step_avg:39.35ms
step:822/2330 train_time:32364ms step_avg:39.37ms
step:823/2330 train_time:32387ms step_avg:39.35ms
step:824/2330 train_time:32442ms step_avg:39.37ms
step:825/2330 train_time:32465ms step_avg:39.35ms
step:826/2330 train_time:32522ms step_avg:39.37ms
step:827/2330 train_time:32545ms step_avg:39.35ms
step:828/2330 train_time:32601ms step_avg:39.37ms
step:829/2330 train_time:32623ms step_avg:39.35ms
step:830/2330 train_time:32679ms step_avg:39.37ms
step:831/2330 train_time:32702ms step_avg:39.35ms
step:832/2330 train_time:32758ms step_avg:39.37ms
step:833/2330 train_time:32781ms step_avg:39.35ms
step:834/2330 train_time:32837ms step_avg:39.37ms
step:835/2330 train_time:32859ms step_avg:39.35ms
step:836/2330 train_time:32916ms step_avg:39.37ms
step:837/2330 train_time:32939ms step_avg:39.35ms
step:838/2330 train_time:32995ms step_avg:39.37ms
step:839/2330 train_time:33019ms step_avg:39.35ms
step:840/2330 train_time:33076ms step_avg:39.38ms
step:841/2330 train_time:33098ms step_avg:39.36ms
step:842/2330 train_time:33156ms step_avg:39.38ms
step:843/2330 train_time:33178ms step_avg:39.36ms
step:844/2330 train_time:33235ms step_avg:39.38ms
step:845/2330 train_time:33257ms step_avg:39.36ms
step:846/2330 train_time:33314ms step_avg:39.38ms
step:847/2330 train_time:33336ms step_avg:39.36ms
step:848/2330 train_time:33394ms step_avg:39.38ms
step:849/2330 train_time:33416ms step_avg:39.36ms
step:850/2330 train_time:33472ms step_avg:39.38ms
step:851/2330 train_time:33494ms step_avg:39.36ms
step:852/2330 train_time:33550ms step_avg:39.38ms
step:853/2330 train_time:33572ms step_avg:39.36ms
step:854/2330 train_time:33628ms step_avg:39.38ms
step:855/2330 train_time:33650ms step_avg:39.36ms
step:856/2330 train_time:33705ms step_avg:39.38ms
step:857/2330 train_time:33728ms step_avg:39.36ms
step:858/2330 train_time:33784ms step_avg:39.38ms
step:859/2330 train_time:33807ms step_avg:39.36ms
step:860/2330 train_time:33863ms step_avg:39.38ms
step:861/2330 train_time:33887ms step_avg:39.36ms
step:862/2330 train_time:33944ms step_avg:39.38ms
step:863/2330 train_time:33967ms step_avg:39.36ms
step:864/2330 train_time:34024ms step_avg:39.38ms
step:865/2330 train_time:34047ms step_avg:39.36ms
step:866/2330 train_time:34103ms step_avg:39.38ms
step:867/2330 train_time:34126ms step_avg:39.36ms
step:868/2330 train_time:34182ms step_avg:39.38ms
step:869/2330 train_time:34205ms step_avg:39.36ms
step:870/2330 train_time:34260ms step_avg:39.38ms
step:871/2330 train_time:34283ms step_avg:39.36ms
step:872/2330 train_time:34339ms step_avg:39.38ms
step:873/2330 train_time:34361ms step_avg:39.36ms
step:874/2330 train_time:34417ms step_avg:39.38ms
step:875/2330 train_time:34440ms step_avg:39.36ms
step:876/2330 train_time:34495ms step_avg:39.38ms
step:877/2330 train_time:34518ms step_avg:39.36ms
step:878/2330 train_time:34574ms step_avg:39.38ms
step:879/2330 train_time:34596ms step_avg:39.36ms
step:880/2330 train_time:34652ms step_avg:39.38ms
step:881/2330 train_time:34675ms step_avg:39.36ms
step:882/2330 train_time:34731ms step_avg:39.38ms
step:883/2330 train_time:34753ms step_avg:39.36ms
step:884/2330 train_time:34809ms step_avg:39.38ms
step:885/2330 train_time:34832ms step_avg:39.36ms
step:886/2330 train_time:34889ms step_avg:39.38ms
step:887/2330 train_time:34911ms step_avg:39.36ms
step:888/2330 train_time:34967ms step_avg:39.38ms
step:889/2330 train_time:34989ms step_avg:39.36ms
step:890/2330 train_time:35045ms step_avg:39.38ms
step:891/2330 train_time:35068ms step_avg:39.36ms
step:892/2330 train_time:35124ms step_avg:39.38ms
step:893/2330 train_time:35147ms step_avg:39.36ms
step:894/2330 train_time:35203ms step_avg:39.38ms
step:895/2330 train_time:35225ms step_avg:39.36ms
step:896/2330 train_time:35282ms step_avg:39.38ms
step:897/2330 train_time:35305ms step_avg:39.36ms
step:898/2330 train_time:35361ms step_avg:39.38ms
step:899/2330 train_time:35384ms step_avg:39.36ms
step:900/2330 train_time:35440ms step_avg:39.38ms
step:901/2330 train_time:35463ms step_avg:39.36ms
step:902/2330 train_time:35518ms step_avg:39.38ms
step:903/2330 train_time:35540ms step_avg:39.36ms
step:904/2330 train_time:35596ms step_avg:39.38ms
step:905/2330 train_time:35619ms step_avg:39.36ms
step:906/2330 train_time:35676ms step_avg:39.38ms
step:907/2330 train_time:35699ms step_avg:39.36ms
step:908/2330 train_time:35755ms step_avg:39.38ms
step:909/2330 train_time:35778ms step_avg:39.36ms
step:910/2330 train_time:35835ms step_avg:39.38ms
step:911/2330 train_time:35857ms step_avg:39.36ms
step:912/2330 train_time:35913ms step_avg:39.38ms
step:913/2330 train_time:35936ms step_avg:39.36ms
step:914/2330 train_time:35993ms step_avg:39.38ms
step:915/2330 train_time:36015ms step_avg:39.36ms
step:916/2330 train_time:36072ms step_avg:39.38ms
step:917/2330 train_time:36094ms step_avg:39.36ms
step:918/2330 train_time:36151ms step_avg:39.38ms
step:919/2330 train_time:36173ms step_avg:39.36ms
step:920/2330 train_time:36230ms step_avg:39.38ms
step:921/2330 train_time:36252ms step_avg:39.36ms
step:922/2330 train_time:36309ms step_avg:39.38ms
step:923/2330 train_time:36331ms step_avg:39.36ms
step:924/2330 train_time:36386ms step_avg:39.38ms
step:925/2330 train_time:36408ms step_avg:39.36ms
step:926/2330 train_time:36464ms step_avg:39.38ms
step:927/2330 train_time:36487ms step_avg:39.36ms
step:928/2330 train_time:36543ms step_avg:39.38ms
step:929/2330 train_time:36566ms step_avg:39.36ms
step:930/2330 train_time:36622ms step_avg:39.38ms
step:931/2330 train_time:36645ms step_avg:39.36ms
step:932/2330 train_time:36701ms step_avg:39.38ms
step:933/2330 train_time:36724ms step_avg:39.36ms
step:934/2330 train_time:36780ms step_avg:39.38ms
step:935/2330 train_time:36802ms step_avg:39.36ms
step:936/2330 train_time:36859ms step_avg:39.38ms
step:937/2330 train_time:36882ms step_avg:39.36ms
step:938/2330 train_time:36938ms step_avg:39.38ms
step:939/2330 train_time:36960ms step_avg:39.36ms
step:940/2330 train_time:37017ms step_avg:39.38ms
step:941/2330 train_time:37039ms step_avg:39.36ms
step:942/2330 train_time:37097ms step_avg:39.38ms
step:943/2330 train_time:37119ms step_avg:39.36ms
step:944/2330 train_time:37176ms step_avg:39.38ms
step:945/2330 train_time:37198ms step_avg:39.36ms
step:946/2330 train_time:37255ms step_avg:39.38ms
step:947/2330 train_time:37277ms step_avg:39.36ms
step:948/2330 train_time:37333ms step_avg:39.38ms
step:949/2330 train_time:37356ms step_avg:39.36ms
step:950/2330 train_time:37412ms step_avg:39.38ms
step:951/2330 train_time:37434ms step_avg:39.36ms
step:952/2330 train_time:37492ms step_avg:39.38ms
step:953/2330 train_time:37514ms step_avg:39.36ms
step:954/2330 train_time:37571ms step_avg:39.38ms
step:955/2330 train_time:37592ms step_avg:39.36ms
step:956/2330 train_time:37648ms step_avg:39.38ms
step:957/2330 train_time:37671ms step_avg:39.36ms
step:958/2330 train_time:37726ms step_avg:39.38ms
step:959/2330 train_time:37748ms step_avg:39.36ms
step:960/2330 train_time:37804ms step_avg:39.38ms
step:961/2330 train_time:37827ms step_avg:39.36ms
step:962/2330 train_time:37883ms step_avg:39.38ms
step:963/2330 train_time:37906ms step_avg:39.36ms
step:964/2330 train_time:37962ms step_avg:39.38ms
step:965/2330 train_time:37985ms step_avg:39.36ms
step:966/2330 train_time:38041ms step_avg:39.38ms
step:967/2330 train_time:38064ms step_avg:39.36ms
step:968/2330 train_time:38120ms step_avg:39.38ms
step:969/2330 train_time:38142ms step_avg:39.36ms
step:970/2330 train_time:38199ms step_avg:39.38ms
step:971/2330 train_time:38221ms step_avg:39.36ms
step:972/2330 train_time:38277ms step_avg:39.38ms
step:973/2330 train_time:38299ms step_avg:39.36ms
step:974/2330 train_time:38356ms step_avg:39.38ms
step:975/2330 train_time:38378ms step_avg:39.36ms
step:976/2330 train_time:38435ms step_avg:39.38ms
step:977/2330 train_time:38457ms step_avg:39.36ms
step:978/2330 train_time:38514ms step_avg:39.38ms
step:979/2330 train_time:38535ms step_avg:39.36ms
step:980/2330 train_time:38592ms step_avg:39.38ms
step:981/2330 train_time:38613ms step_avg:39.36ms
step:982/2330 train_time:38671ms step_avg:39.38ms
step:983/2330 train_time:38692ms step_avg:39.36ms
step:984/2330 train_time:38749ms step_avg:39.38ms
step:985/2330 train_time:38771ms step_avg:39.36ms
step:986/2330 train_time:38828ms step_avg:39.38ms
step:987/2330 train_time:38850ms step_avg:39.36ms
step:988/2330 train_time:38905ms step_avg:39.38ms
step:989/2330 train_time:38928ms step_avg:39.36ms
step:990/2330 train_time:38984ms step_avg:39.38ms
step:991/2330 train_time:39007ms step_avg:39.36ms
step:992/2330 train_time:39064ms step_avg:39.38ms
step:993/2330 train_time:39087ms step_avg:39.36ms
step:994/2330 train_time:39144ms step_avg:39.38ms
step:995/2330 train_time:39167ms step_avg:39.36ms
step:996/2330 train_time:39224ms step_avg:39.38ms
step:997/2330 train_time:39247ms step_avg:39.36ms
step:998/2330 train_time:39303ms step_avg:39.38ms
step:999/2330 train_time:39326ms step_avg:39.37ms
step:1000/2330 train_time:39383ms step_avg:39.38ms
step:1000/2330 val_loss:5.2348 train_time:39479ms step_avg:39.48ms
step:1001/2330 train_time:39492ms step_avg:39.45ms
step:1002/2330 train_time:39505ms step_avg:39.43ms
step:1003/2330 train_time:39515ms step_avg:39.40ms
step:1004/2330 train_time:39541ms step_avg:39.38ms
step:1005/2330 train_time:39563ms step_avg:39.37ms
step:1006/2330 train_time:39617ms step_avg:39.38ms
step:1007/2330 train_time:39639ms step_avg:39.36ms
step:1008/2330 train_time:39694ms step_avg:39.38ms
step:1009/2330 train_time:39716ms step_avg:39.36ms
step:1010/2330 train_time:39771ms step_avg:39.38ms
step:1011/2330 train_time:39795ms step_avg:39.36ms
step:1012/2330 train_time:39855ms step_avg:39.38ms
step:1013/2330 train_time:39879ms step_avg:39.37ms
step:1014/2330 train_time:39936ms step_avg:39.38ms
step:1015/2330 train_time:39958ms step_avg:39.37ms
step:1016/2330 train_time:40014ms step_avg:39.38ms
step:1017/2330 train_time:40036ms step_avg:39.37ms
step:1018/2330 train_time:40091ms step_avg:39.38ms
step:1019/2330 train_time:40113ms step_avg:39.37ms
step:1020/2330 train_time:40168ms step_avg:39.38ms
step:1021/2330 train_time:40190ms step_avg:39.36ms
step:1022/2330 train_time:40246ms step_avg:39.38ms
step:1023/2330 train_time:40268ms step_avg:39.36ms
step:1024/2330 train_time:40323ms step_avg:39.38ms
step:1025/2330 train_time:40345ms step_avg:39.36ms
step:1026/2330 train_time:40402ms step_avg:39.38ms
step:1027/2330 train_time:40425ms step_avg:39.36ms
step:1028/2330 train_time:40481ms step_avg:39.38ms
step:1029/2330 train_time:40503ms step_avg:39.36ms
step:1030/2330 train_time:40559ms step_avg:39.38ms
step:1031/2330 train_time:40581ms step_avg:39.36ms
step:1032/2330 train_time:40637ms step_avg:39.38ms
step:1033/2330 train_time:40658ms step_avg:39.36ms
step:1034/2330 train_time:40715ms step_avg:39.38ms
step:1035/2330 train_time:40737ms step_avg:39.36ms
step:1036/2330 train_time:40794ms step_avg:39.38ms
step:1037/2330 train_time:40816ms step_avg:39.36ms
step:1038/2330 train_time:40872ms step_avg:39.38ms
step:1039/2330 train_time:40895ms step_avg:39.36ms
step:1040/2330 train_time:40951ms step_avg:39.38ms
step:1041/2330 train_time:40974ms step_avg:39.36ms
step:1042/2330 train_time:41030ms step_avg:39.38ms
step:1043/2330 train_time:41052ms step_avg:39.36ms
step:1044/2330 train_time:41108ms step_avg:39.38ms
step:1045/2330 train_time:41130ms step_avg:39.36ms
step:1046/2330 train_time:41186ms step_avg:39.37ms
step:1047/2330 train_time:41208ms step_avg:39.36ms
step:1048/2330 train_time:41264ms step_avg:39.37ms
step:1049/2330 train_time:41286ms step_avg:39.36ms
step:1050/2330 train_time:41341ms step_avg:39.37ms
step:1051/2330 train_time:41364ms step_avg:39.36ms
step:1052/2330 train_time:41419ms step_avg:39.37ms
step:1053/2330 train_time:41441ms step_avg:39.36ms
step:1054/2330 train_time:41497ms step_avg:39.37ms
step:1055/2330 train_time:41520ms step_avg:39.36ms
step:1056/2330 train_time:41575ms step_avg:39.37ms
step:1057/2330 train_time:41597ms step_avg:39.35ms
step:1058/2330 train_time:41652ms step_avg:39.37ms
step:1059/2330 train_time:41674ms step_avg:39.35ms
step:1060/2330 train_time:41730ms step_avg:39.37ms
step:1061/2330 train_time:41753ms step_avg:39.35ms
step:1062/2330 train_time:41809ms step_avg:39.37ms
step:1063/2330 train_time:41832ms step_avg:39.35ms
step:1064/2330 train_time:41888ms step_avg:39.37ms
step:1065/2330 train_time:41911ms step_avg:39.35ms
step:1066/2330 train_time:41967ms step_avg:39.37ms
step:1067/2330 train_time:41990ms step_avg:39.35ms
step:1068/2330 train_time:42045ms step_avg:39.37ms
step:1069/2330 train_time:42068ms step_avg:39.35ms
step:1070/2330 train_time:42123ms step_avg:39.37ms
step:1071/2330 train_time:42146ms step_avg:39.35ms
step:1072/2330 train_time:42202ms step_avg:39.37ms
step:1073/2330 train_time:42224ms step_avg:39.35ms
step:1074/2330 train_time:42279ms step_avg:39.37ms
step:1075/2330 train_time:42302ms step_avg:39.35ms
step:1076/2330 train_time:42357ms step_avg:39.37ms
step:1077/2330 train_time:42379ms step_avg:39.35ms
step:1078/2330 train_time:42435ms step_avg:39.36ms
step:1079/2330 train_time:42457ms step_avg:39.35ms
step:1080/2330 train_time:42513ms step_avg:39.36ms
step:1081/2330 train_time:42535ms step_avg:39.35ms
step:1082/2330 train_time:42590ms step_avg:39.36ms
step:1083/2330 train_time:42613ms step_avg:39.35ms
step:1084/2330 train_time:42669ms step_avg:39.36ms
step:1085/2330 train_time:42691ms step_avg:39.35ms
step:1086/2330 train_time:42747ms step_avg:39.36ms
step:1087/2330 train_time:42770ms step_avg:39.35ms
step:1088/2330 train_time:42827ms step_avg:39.36ms
step:1089/2330 train_time:42850ms step_avg:39.35ms
step:1090/2330 train_time:42906ms step_avg:39.36ms
step:1091/2330 train_time:42928ms step_avg:39.35ms
step:1092/2330 train_time:42984ms step_avg:39.36ms
step:1093/2330 train_time:43007ms step_avg:39.35ms
step:1094/2330 train_time:43063ms step_avg:39.36ms
step:1095/2330 train_time:43085ms step_avg:39.35ms
step:1096/2330 train_time:43141ms step_avg:39.36ms
step:1097/2330 train_time:43164ms step_avg:39.35ms
step:1098/2330 train_time:43219ms step_avg:39.36ms
step:1099/2330 train_time:43241ms step_avg:39.35ms
step:1100/2330 train_time:43297ms step_avg:39.36ms
step:1101/2330 train_time:43319ms step_avg:39.35ms
step:1102/2330 train_time:43375ms step_avg:39.36ms
step:1103/2330 train_time:43397ms step_avg:39.34ms
step:1104/2330 train_time:43453ms step_avg:39.36ms
step:1105/2330 train_time:43475ms step_avg:39.34ms
step:1106/2330 train_time:43531ms step_avg:39.36ms
step:1107/2330 train_time:43553ms step_avg:39.34ms
step:1108/2330 train_time:43608ms step_avg:39.36ms
step:1109/2330 train_time:43631ms step_avg:39.34ms
step:1110/2330 train_time:43686ms step_avg:39.36ms
step:1111/2330 train_time:43710ms step_avg:39.34ms
step:1112/2330 train_time:43767ms step_avg:39.36ms
step:1113/2330 train_time:43789ms step_avg:39.34ms
step:1114/2330 train_time:43845ms step_avg:39.36ms
step:1115/2330 train_time:43868ms step_avg:39.34ms
step:1116/2330 train_time:43924ms step_avg:39.36ms
step:1117/2330 train_time:43946ms step_avg:39.34ms
step:1118/2330 train_time:44002ms step_avg:39.36ms
step:1119/2330 train_time:44026ms step_avg:39.34ms
step:1120/2330 train_time:44082ms step_avg:39.36ms
step:1121/2330 train_time:44105ms step_avg:39.34ms
step:1122/2330 train_time:44161ms step_avg:39.36ms
step:1123/2330 train_time:44183ms step_avg:39.34ms
step:1124/2330 train_time:44239ms step_avg:39.36ms
step:1125/2330 train_time:44261ms step_avg:39.34ms
step:1126/2330 train_time:44317ms step_avg:39.36ms
step:1127/2330 train_time:44340ms step_avg:39.34ms
step:1128/2330 train_time:44395ms step_avg:39.36ms
step:1129/2330 train_time:44418ms step_avg:39.34ms
step:1130/2330 train_time:44474ms step_avg:39.36ms
step:1131/2330 train_time:44496ms step_avg:39.34ms
step:1132/2330 train_time:44552ms step_avg:39.36ms
step:1133/2330 train_time:44575ms step_avg:39.34ms
step:1134/2330 train_time:44632ms step_avg:39.36ms
step:1135/2330 train_time:44654ms step_avg:39.34ms
step:1136/2330 train_time:44710ms step_avg:39.36ms
step:1137/2330 train_time:44732ms step_avg:39.34ms
step:1138/2330 train_time:44788ms step_avg:39.36ms
step:1139/2330 train_time:44811ms step_avg:39.34ms
step:1140/2330 train_time:44867ms step_avg:39.36ms
step:1141/2330 train_time:44889ms step_avg:39.34ms
step:1142/2330 train_time:44945ms step_avg:39.36ms
step:1143/2330 train_time:44968ms step_avg:39.34ms
step:1144/2330 train_time:45025ms step_avg:39.36ms
step:1145/2330 train_time:45048ms step_avg:39.34ms
step:1146/2330 train_time:45104ms step_avg:39.36ms
step:1147/2330 train_time:45126ms step_avg:39.34ms
step:1148/2330 train_time:45182ms step_avg:39.36ms
step:1149/2330 train_time:45205ms step_avg:39.34ms
step:1150/2330 train_time:45261ms step_avg:39.36ms
step:1151/2330 train_time:45284ms step_avg:39.34ms
step:1152/2330 train_time:45340ms step_avg:39.36ms
step:1153/2330 train_time:45363ms step_avg:39.34ms
step:1154/2330 train_time:45419ms step_avg:39.36ms
step:1155/2330 train_time:45443ms step_avg:39.34ms
step:1156/2330 train_time:45499ms step_avg:39.36ms
step:1157/2330 train_time:45521ms step_avg:39.34ms
step:1158/2330 train_time:45577ms step_avg:39.36ms
step:1159/2330 train_time:45600ms step_avg:39.34ms
step:1160/2330 train_time:45656ms step_avg:39.36ms
step:1161/2330 train_time:45678ms step_avg:39.34ms
step:1162/2330 train_time:45736ms step_avg:39.36ms
step:1163/2330 train_time:45758ms step_avg:39.35ms
step:1164/2330 train_time:45815ms step_avg:39.36ms
step:1165/2330 train_time:45837ms step_avg:39.35ms
step:1166/2330 train_time:45895ms step_avg:39.36ms
step:1167/2330 train_time:45917ms step_avg:39.35ms
step:1168/2330 train_time:45973ms step_avg:39.36ms
step:1169/2330 train_time:45996ms step_avg:39.35ms
step:1170/2330 train_time:46052ms step_avg:39.36ms
step:1171/2330 train_time:46073ms step_avg:39.35ms
step:1172/2330 train_time:46129ms step_avg:39.36ms
step:1173/2330 train_time:46152ms step_avg:39.35ms
step:1174/2330 train_time:46208ms step_avg:39.36ms
step:1175/2330 train_time:46230ms step_avg:39.35ms
step:1176/2330 train_time:46286ms step_avg:39.36ms
step:1177/2330 train_time:46309ms step_avg:39.35ms
step:1178/2330 train_time:46366ms step_avg:39.36ms
step:1179/2330 train_time:46388ms step_avg:39.35ms
step:1180/2330 train_time:46444ms step_avg:39.36ms
step:1181/2330 train_time:46467ms step_avg:39.35ms
step:1182/2330 train_time:46523ms step_avg:39.36ms
step:1183/2330 train_time:46546ms step_avg:39.35ms
step:1184/2330 train_time:46602ms step_avg:39.36ms
step:1185/2330 train_time:46624ms step_avg:39.35ms
step:1186/2330 train_time:46681ms step_avg:39.36ms
step:1187/2330 train_time:46704ms step_avg:39.35ms
step:1188/2330 train_time:46761ms step_avg:39.36ms
step:1189/2330 train_time:46783ms step_avg:39.35ms
step:1190/2330 train_time:46840ms step_avg:39.36ms
step:1191/2330 train_time:46862ms step_avg:39.35ms
step:1192/2330 train_time:46919ms step_avg:39.36ms
step:1193/2330 train_time:46941ms step_avg:39.35ms
step:1194/2330 train_time:46998ms step_avg:39.36ms
step:1195/2330 train_time:47021ms step_avg:39.35ms
step:1196/2330 train_time:47077ms step_avg:39.36ms
step:1197/2330 train_time:47099ms step_avg:39.35ms
step:1198/2330 train_time:47155ms step_avg:39.36ms
step:1199/2330 train_time:47178ms step_avg:39.35ms
step:1200/2330 train_time:47234ms step_avg:39.36ms
step:1201/2330 train_time:47257ms step_avg:39.35ms
step:1202/2330 train_time:47313ms step_avg:39.36ms
step:1203/2330 train_time:47336ms step_avg:39.35ms
step:1204/2330 train_time:47392ms step_avg:39.36ms
step:1205/2330 train_time:47414ms step_avg:39.35ms
step:1206/2330 train_time:47470ms step_avg:39.36ms
step:1207/2330 train_time:47492ms step_avg:39.35ms
step:1208/2330 train_time:47547ms step_avg:39.36ms
step:1209/2330 train_time:47570ms step_avg:39.35ms
step:1210/2330 train_time:47626ms step_avg:39.36ms
step:1211/2330 train_time:47649ms step_avg:39.35ms
step:1212/2330 train_time:47705ms step_avg:39.36ms
step:1213/2330 train_time:47729ms step_avg:39.35ms
step:1214/2330 train_time:47785ms step_avg:39.36ms
step:1215/2330 train_time:47807ms step_avg:39.35ms
step:1216/2330 train_time:47863ms step_avg:39.36ms
step:1217/2330 train_time:47886ms step_avg:39.35ms
step:1218/2330 train_time:47942ms step_avg:39.36ms
step:1219/2330 train_time:47965ms step_avg:39.35ms
step:1220/2330 train_time:48021ms step_avg:39.36ms
step:1221/2330 train_time:48044ms step_avg:39.35ms
step:1222/2330 train_time:48100ms step_avg:39.36ms
step:1223/2330 train_time:48123ms step_avg:39.35ms
step:1224/2330 train_time:48179ms step_avg:39.36ms
step:1225/2330 train_time:48201ms step_avg:39.35ms
step:1226/2330 train_time:48258ms step_avg:39.36ms
step:1227/2330 train_time:48280ms step_avg:39.35ms
step:1228/2330 train_time:48336ms step_avg:39.36ms
step:1229/2330 train_time:48359ms step_avg:39.35ms
step:1230/2330 train_time:48414ms step_avg:39.36ms
step:1231/2330 train_time:48436ms step_avg:39.35ms
step:1232/2330 train_time:48493ms step_avg:39.36ms
step:1233/2330 train_time:48515ms step_avg:39.35ms
step:1234/2330 train_time:48571ms step_avg:39.36ms
step:1235/2330 train_time:48594ms step_avg:39.35ms
step:1236/2330 train_time:48649ms step_avg:39.36ms
step:1237/2330 train_time:48672ms step_avg:39.35ms
step:1238/2330 train_time:48727ms step_avg:39.36ms
step:1239/2330 train_time:48750ms step_avg:39.35ms
step:1240/2330 train_time:48806ms step_avg:39.36ms
step:1241/2330 train_time:48829ms step_avg:39.35ms
step:1242/2330 train_time:48885ms step_avg:39.36ms
step:1243/2330 train_time:48908ms step_avg:39.35ms
step:1244/2330 train_time:48964ms step_avg:39.36ms
step:1245/2330 train_time:48987ms step_avg:39.35ms
step:1246/2330 train_time:49042ms step_avg:39.36ms
step:1247/2330 train_time:49065ms step_avg:39.35ms
step:1248/2330 train_time:49122ms step_avg:39.36ms
step:1249/2330 train_time:49144ms step_avg:39.35ms
step:1250/2330 train_time:49200ms step_avg:39.36ms
step:1250/2330 val_loss:5.2001 train_time:49296ms step_avg:39.44ms
step:1251/2330 train_time:49309ms step_avg:39.42ms
step:1252/2330 train_time:49322ms step_avg:39.39ms
step:1253/2330 train_time:49332ms step_avg:39.37ms
step:1254/2330 train_time:49359ms step_avg:39.36ms
step:1255/2330 train_time:49381ms step_avg:39.35ms
step:1256/2330 train_time:49436ms step_avg:39.36ms
step:1257/2330 train_time:49458ms step_avg:39.35ms
step:1258/2330 train_time:49513ms step_avg:39.36ms
step:1259/2330 train_time:49536ms step_avg:39.35ms
step:1260/2330 train_time:49592ms step_avg:39.36ms
step:1261/2330 train_time:49617ms step_avg:39.35ms
step:1262/2330 train_time:49678ms step_avg:39.36ms
step:1263/2330 train_time:49703ms step_avg:39.35ms
step:1264/2330 train_time:49759ms step_avg:39.37ms
step:1265/2330 train_time:49783ms step_avg:39.35ms
step:1266/2330 train_time:49839ms step_avg:39.37ms
step:1267/2330 train_time:49862ms step_avg:39.35ms
step:1268/2330 train_time:49917ms step_avg:39.37ms
step:1269/2330 train_time:49939ms step_avg:39.35ms
step:1270/2330 train_time:49994ms step_avg:39.37ms
step:1271/2330 train_time:50017ms step_avg:39.35ms
step:1272/2330 train_time:50072ms step_avg:39.36ms
step:1273/2330 train_time:50094ms step_avg:39.35ms
step:1274/2330 train_time:50150ms step_avg:39.36ms
step:1275/2330 train_time:50171ms step_avg:39.35ms
step:1276/2330 train_time:50228ms step_avg:39.36ms
step:1277/2330 train_time:50250ms step_avg:39.35ms
step:1278/2330 train_time:50306ms step_avg:39.36ms
step:1279/2330 train_time:50328ms step_avg:39.35ms
step:1280/2330 train_time:50384ms step_avg:39.36ms
step:1281/2330 train_time:50406ms step_avg:39.35ms
step:1282/2330 train_time:50462ms step_avg:39.36ms
step:1283/2330 train_time:50484ms step_avg:39.35ms
step:1284/2330 train_time:50541ms step_avg:39.36ms
step:1285/2330 train_time:50564ms step_avg:39.35ms
step:1286/2330 train_time:50621ms step_avg:39.36ms
step:1287/2330 train_time:50645ms step_avg:39.35ms
step:1288/2330 train_time:50701ms step_avg:39.36ms
step:1289/2330 train_time:50724ms step_avg:39.35ms
step:1290/2330 train_time:50780ms step_avg:39.36ms
step:1291/2330 train_time:50804ms step_avg:39.35ms
step:1292/2330 train_time:50860ms step_avg:39.37ms
step:1293/2330 train_time:50883ms step_avg:39.35ms
step:1294/2330 train_time:50939ms step_avg:39.37ms
step:1295/2330 train_time:50961ms step_avg:39.35ms
step:1296/2330 train_time:51017ms step_avg:39.37ms
step:1297/2330 train_time:51040ms step_avg:39.35ms
step:1298/2330 train_time:51096ms step_avg:39.36ms
step:1299/2330 train_time:51118ms step_avg:39.35ms
step:1300/2330 train_time:51174ms step_avg:39.36ms
step:1301/2330 train_time:51196ms step_avg:39.35ms
step:1302/2330 train_time:51252ms step_avg:39.36ms
step:1303/2330 train_time:51275ms step_avg:39.35ms
step:1304/2330 train_time:51330ms step_avg:39.36ms
step:1305/2330 train_time:51352ms step_avg:39.35ms
step:1306/2330 train_time:51409ms step_avg:39.36ms
step:1307/2330 train_time:51431ms step_avg:39.35ms
step:1308/2330 train_time:51487ms step_avg:39.36ms
step:1309/2330 train_time:51511ms step_avg:39.35ms
step:1310/2330 train_time:51567ms step_avg:39.36ms
step:1311/2330 train_time:51590ms step_avg:39.35ms
step:1312/2330 train_time:51647ms step_avg:39.37ms
step:1313/2330 train_time:51670ms step_avg:39.35ms
step:1314/2330 train_time:51727ms step_avg:39.37ms
step:1315/2330 train_time:51749ms step_avg:39.35ms
step:1316/2330 train_time:51806ms step_avg:39.37ms
step:1317/2330 train_time:51828ms step_avg:39.35ms
step:1318/2330 train_time:51885ms step_avg:39.37ms
step:1319/2330 train_time:51907ms step_avg:39.35ms
step:1320/2330 train_time:51963ms step_avg:39.37ms
step:1321/2330 train_time:51985ms step_avg:39.35ms
step:1322/2330 train_time:52040ms step_avg:39.36ms
step:1323/2330 train_time:52062ms step_avg:39.35ms
step:1324/2330 train_time:52118ms step_avg:39.36ms
step:1325/2330 train_time:52142ms step_avg:39.35ms
step:1326/2330 train_time:52198ms step_avg:39.36ms
step:1327/2330 train_time:52220ms step_avg:39.35ms
step:1328/2330 train_time:52276ms step_avg:39.36ms
step:1329/2330 train_time:52300ms step_avg:39.35ms
step:1330/2330 train_time:52356ms step_avg:39.37ms
step:1331/2330 train_time:52379ms step_avg:39.35ms
step:1332/2330 train_time:52435ms step_avg:39.37ms
step:1333/2330 train_time:52458ms step_avg:39.35ms
step:1334/2330 train_time:52513ms step_avg:39.37ms
step:1335/2330 train_time:52536ms step_avg:39.35ms
step:1336/2330 train_time:52594ms step_avg:39.37ms
step:1337/2330 train_time:52616ms step_avg:39.35ms
step:1338/2330 train_time:52673ms step_avg:39.37ms
step:1339/2330 train_time:52695ms step_avg:39.35ms
step:1340/2330 train_time:52752ms step_avg:39.37ms
step:1341/2330 train_time:52775ms step_avg:39.35ms
step:1342/2330 train_time:52831ms step_avg:39.37ms
step:1343/2330 train_time:52854ms step_avg:39.35ms
step:1344/2330 train_time:52911ms step_avg:39.37ms
step:1345/2330 train_time:52933ms step_avg:39.36ms
step:1346/2330 train_time:52989ms step_avg:39.37ms
step:1347/2330 train_time:53011ms step_avg:39.35ms
step:1348/2330 train_time:53067ms step_avg:39.37ms
step:1349/2330 train_time:53089ms step_avg:39.35ms
step:1350/2330 train_time:53145ms step_avg:39.37ms
step:1351/2330 train_time:53167ms step_avg:39.35ms
step:1352/2330 train_time:53223ms step_avg:39.37ms
step:1353/2330 train_time:53246ms step_avg:39.35ms
step:1354/2330 train_time:53302ms step_avg:39.37ms
step:1355/2330 train_time:53325ms step_avg:39.35ms
step:1356/2330 train_time:53380ms step_avg:39.37ms
step:1357/2330 train_time:53404ms step_avg:39.35ms
step:1358/2330 train_time:53459ms step_avg:39.37ms
step:1359/2330 train_time:53482ms step_avg:39.35ms
step:1360/2330 train_time:53538ms step_avg:39.37ms
step:1361/2330 train_time:53561ms step_avg:39.35ms
step:1362/2330 train_time:53617ms step_avg:39.37ms
step:1363/2330 train_time:53640ms step_avg:39.35ms
step:1364/2330 train_time:53696ms step_avg:39.37ms
step:1365/2330 train_time:53719ms step_avg:39.35ms
step:1366/2330 train_time:53775ms step_avg:39.37ms
step:1367/2330 train_time:53798ms step_avg:39.35ms
step:1368/2330 train_time:53854ms step_avg:39.37ms
step:1369/2330 train_time:53878ms step_avg:39.36ms
step:1370/2330 train_time:53934ms step_avg:39.37ms
step:1371/2330 train_time:53957ms step_avg:39.36ms
step:1372/2330 train_time:54013ms step_avg:39.37ms
step:1373/2330 train_time:54035ms step_avg:39.36ms
step:1374/2330 train_time:54092ms step_avg:39.37ms
step:1375/2330 train_time:54114ms step_avg:39.36ms
step:1376/2330 train_time:54171ms step_avg:39.37ms
step:1377/2330 train_time:54193ms step_avg:39.36ms
step:1378/2330 train_time:54249ms step_avg:39.37ms
step:1379/2330 train_time:54271ms step_avg:39.36ms
step:1380/2330 train_time:54328ms step_avg:39.37ms
step:1381/2330 train_time:54350ms step_avg:39.36ms
step:1382/2330 train_time:54406ms step_avg:39.37ms
step:1383/2330 train_time:54428ms step_avg:39.36ms
step:1384/2330 train_time:54485ms step_avg:39.37ms
step:1385/2330 train_time:54507ms step_avg:39.35ms
step:1386/2330 train_time:54563ms step_avg:39.37ms
step:1387/2330 train_time:54585ms step_avg:39.35ms
step:1388/2330 train_time:54640ms step_avg:39.37ms
step:1389/2330 train_time:54663ms step_avg:39.35ms
step:1390/2330 train_time:54719ms step_avg:39.37ms
step:1391/2330 train_time:54743ms step_avg:39.35ms
step:1392/2330 train_time:54799ms step_avg:39.37ms
step:1393/2330 train_time:54822ms step_avg:39.36ms
step:1394/2330 train_time:54878ms step_avg:39.37ms
step:1395/2330 train_time:54901ms step_avg:39.36ms
step:1396/2330 train_time:54958ms step_avg:39.37ms
step:1397/2330 train_time:54982ms step_avg:39.36ms
step:1398/2330 train_time:55038ms step_avg:39.37ms
step:1399/2330 train_time:55061ms step_avg:39.36ms
step:1400/2330 train_time:55117ms step_avg:39.37ms
step:1401/2330 train_time:55140ms step_avg:39.36ms
step:1402/2330 train_time:55195ms step_avg:39.37ms
step:1403/2330 train_time:55218ms step_avg:39.36ms
step:1404/2330 train_time:55275ms step_avg:39.37ms
step:1405/2330 train_time:55297ms step_avg:39.36ms
step:1406/2330 train_time:55353ms step_avg:39.37ms
step:1407/2330 train_time:55375ms step_avg:39.36ms
step:1408/2330 train_time:55432ms step_avg:39.37ms
step:1409/2330 train_time:55454ms step_avg:39.36ms
step:1410/2330 train_time:55510ms step_avg:39.37ms
step:1411/2330 train_time:55532ms step_avg:39.36ms
step:1412/2330 train_time:55588ms step_avg:39.37ms
step:1413/2330 train_time:55610ms step_avg:39.36ms
step:1414/2330 train_time:55667ms step_avg:39.37ms
step:1415/2330 train_time:55689ms step_avg:39.36ms
step:1416/2330 train_time:55745ms step_avg:39.37ms
step:1417/2330 train_time:55767ms step_avg:39.36ms
step:1418/2330 train_time:55825ms step_avg:39.37ms
step:1419/2330 train_time:55847ms step_avg:39.36ms
step:1420/2330 train_time:55903ms step_avg:39.37ms
step:1421/2330 train_time:55925ms step_avg:39.36ms
step:1422/2330 train_time:55980ms step_avg:39.37ms
step:1423/2330 train_time:56003ms step_avg:39.36ms
step:1424/2330 train_time:56060ms step_avg:39.37ms
step:1425/2330 train_time:56083ms step_avg:39.36ms
step:1426/2330 train_time:56140ms step_avg:39.37ms
step:1427/2330 train_time:56163ms step_avg:39.36ms
step:1428/2330 train_time:56219ms step_avg:39.37ms
step:1429/2330 train_time:56242ms step_avg:39.36ms
step:1430/2330 train_time:56299ms step_avg:39.37ms
step:1431/2330 train_time:56323ms step_avg:39.36ms
step:1432/2330 train_time:56378ms step_avg:39.37ms
step:1433/2330 train_time:56402ms step_avg:39.36ms
step:1434/2330 train_time:56459ms step_avg:39.37ms
step:1435/2330 train_time:56482ms step_avg:39.36ms
step:1436/2330 train_time:56538ms step_avg:39.37ms
step:1437/2330 train_time:56561ms step_avg:39.36ms
step:1438/2330 train_time:56616ms step_avg:39.37ms
step:1439/2330 train_time:56638ms step_avg:39.36ms
step:1440/2330 train_time:56694ms step_avg:39.37ms
step:1441/2330 train_time:56717ms step_avg:39.36ms
step:1442/2330 train_time:56773ms step_avg:39.37ms
step:1443/2330 train_time:56795ms step_avg:39.36ms
step:1444/2330 train_time:56852ms step_avg:39.37ms
step:1445/2330 train_time:56874ms step_avg:39.36ms
step:1446/2330 train_time:56930ms step_avg:39.37ms
step:1447/2330 train_time:56953ms step_avg:39.36ms
step:1448/2330 train_time:57009ms step_avg:39.37ms
step:1449/2330 train_time:57031ms step_avg:39.36ms
step:1450/2330 train_time:57087ms step_avg:39.37ms
step:1451/2330 train_time:57110ms step_avg:39.36ms
step:1452/2330 train_time:57167ms step_avg:39.37ms
step:1453/2330 train_time:57189ms step_avg:39.36ms
step:1454/2330 train_time:57245ms step_avg:39.37ms
step:1455/2330 train_time:57267ms step_avg:39.36ms
step:1456/2330 train_time:57323ms step_avg:39.37ms
step:1457/2330 train_time:57345ms step_avg:39.36ms
step:1458/2330 train_time:57402ms step_avg:39.37ms
step:1459/2330 train_time:57424ms step_avg:39.36ms
step:1460/2330 train_time:57480ms step_avg:39.37ms
step:1461/2330 train_time:57503ms step_avg:39.36ms
step:1462/2330 train_time:57559ms step_avg:39.37ms
step:1463/2330 train_time:57582ms step_avg:39.36ms
step:1464/2330 train_time:57637ms step_avg:39.37ms
step:1465/2330 train_time:57660ms step_avg:39.36ms
step:1466/2330 train_time:57716ms step_avg:39.37ms
step:1467/2330 train_time:57739ms step_avg:39.36ms
step:1468/2330 train_time:57795ms step_avg:39.37ms
step:1469/2330 train_time:57818ms step_avg:39.36ms
step:1470/2330 train_time:57874ms step_avg:39.37ms
step:1471/2330 train_time:57896ms step_avg:39.36ms
step:1472/2330 train_time:57952ms step_avg:39.37ms
step:1473/2330 train_time:57975ms step_avg:39.36ms
step:1474/2330 train_time:58030ms step_avg:39.37ms
step:1475/2330 train_time:58053ms step_avg:39.36ms
step:1476/2330 train_time:58110ms step_avg:39.37ms
step:1477/2330 train_time:58132ms step_avg:39.36ms
step:1478/2330 train_time:58189ms step_avg:39.37ms
step:1479/2330 train_time:58211ms step_avg:39.36ms
step:1480/2330 train_time:58267ms step_avg:39.37ms
step:1481/2330 train_time:58290ms step_avg:39.36ms
step:1482/2330 train_time:58346ms step_avg:39.37ms
step:1483/2330 train_time:58369ms step_avg:39.36ms
step:1484/2330 train_time:58426ms step_avg:39.37ms
step:1485/2330 train_time:58448ms step_avg:39.36ms
step:1486/2330 train_time:58505ms step_avg:39.37ms
step:1487/2330 train_time:58527ms step_avg:39.36ms
step:1488/2330 train_time:58583ms step_avg:39.37ms
step:1489/2330 train_time:58606ms step_avg:39.36ms
step:1490/2330 train_time:58662ms step_avg:39.37ms
step:1491/2330 train_time:58684ms step_avg:39.36ms
step:1492/2330 train_time:58740ms step_avg:39.37ms
step:1493/2330 train_time:58763ms step_avg:39.36ms
step:1494/2330 train_time:58819ms step_avg:39.37ms
step:1495/2330 train_time:58842ms step_avg:39.36ms
step:1496/2330 train_time:58898ms step_avg:39.37ms
step:1497/2330 train_time:58921ms step_avg:39.36ms
step:1498/2330 train_time:58977ms step_avg:39.37ms
step:1499/2330 train_time:59000ms step_avg:39.36ms
step:1500/2330 train_time:59056ms step_avg:39.37ms
step:1500/2330 val_loss:5.1974 train_time:59152ms step_avg:39.43ms
step:1501/2330 train_time:59165ms step_avg:39.42ms
step:1502/2330 train_time:59178ms step_avg:39.40ms
step:1503/2330 train_time:59189ms step_avg:39.38ms
step:1504/2330 train_time:59215ms step_avg:39.37ms
step:1505/2330 train_time:59236ms step_avg:39.36ms
step:1506/2330 train_time:59291ms step_avg:39.37ms
step:1507/2330 train_time:59312ms step_avg:39.36ms
step:1508/2330 train_time:59368ms step_avg:39.37ms
step:1509/2330 train_time:59389ms step_avg:39.36ms
step:1510/2330 train_time:59444ms step_avg:39.37ms
step:1511/2330 train_time:59469ms step_avg:39.36ms
step:1512/2330 train_time:59529ms step_avg:39.37ms
step:1513/2330 train_time:59551ms step_avg:39.36ms
step:1514/2330 train_time:59608ms step_avg:39.37ms
step:1515/2330 train_time:59630ms step_avg:39.36ms
step:1516/2330 train_time:59686ms step_avg:39.37ms
step:1517/2330 train_time:59708ms step_avg:39.36ms
step:1518/2330 train_time:59763ms step_avg:39.37ms
step:1519/2330 train_time:59785ms step_avg:39.36ms
step:1520/2330 train_time:59841ms step_avg:39.37ms
step:1521/2330 train_time:59863ms step_avg:39.36ms
step:1522/2330 train_time:59918ms step_avg:39.37ms
step:1523/2330 train_time:59940ms step_avg:39.36ms
step:1524/2330 train_time:59996ms step_avg:39.37ms
step:1525/2330 train_time:60018ms step_avg:39.36ms
step:1526/2330 train_time:60074ms step_avg:39.37ms
step:1527/2330 train_time:60097ms step_avg:39.36ms
step:1528/2330 train_time:60153ms step_avg:39.37ms
step:1529/2330 train_time:60176ms step_avg:39.36ms
step:1530/2330 train_time:60232ms step_avg:39.37ms
step:1531/2330 train_time:60253ms step_avg:39.36ms
step:1532/2330 train_time:60308ms step_avg:39.37ms
step:1533/2330 train_time:60331ms step_avg:39.35ms
step:1534/2330 train_time:60388ms step_avg:39.37ms
step:1535/2330 train_time:60410ms step_avg:39.35ms
step:1536/2330 train_time:60467ms step_avg:39.37ms
step:1537/2330 train_time:60490ms step_avg:39.36ms
step:1538/2330 train_time:60547ms step_avg:39.37ms
step:1539/2330 train_time:60569ms step_avg:39.36ms
step:1540/2330 train_time:60626ms step_avg:39.37ms
step:1541/2330 train_time:60648ms step_avg:39.36ms
step:1542/2330 train_time:60703ms step_avg:39.37ms
step:1543/2330 train_time:60726ms step_avg:39.36ms
step:1544/2330 train_time:60781ms step_avg:39.37ms
step:1545/2330 train_time:60803ms step_avg:39.35ms
step:1546/2330 train_time:60859ms step_avg:39.37ms
step:1547/2330 train_time:60881ms step_avg:39.35ms
step:1548/2330 train_time:60936ms step_avg:39.36ms
step:1549/2330 train_time:60958ms step_avg:39.35ms
step:1550/2330 train_time:61014ms step_avg:39.36ms
step:1551/2330 train_time:61036ms step_avg:39.35ms
step:1552/2330 train_time:61092ms step_avg:39.36ms
step:1553/2330 train_time:61114ms step_avg:39.35ms
step:1554/2330 train_time:61170ms step_avg:39.36ms
step:1555/2330 train_time:61192ms step_avg:39.35ms
step:1556/2330 train_time:61248ms step_avg:39.36ms
step:1557/2330 train_time:61270ms step_avg:39.35ms
step:1558/2330 train_time:61326ms step_avg:39.36ms
step:1559/2330 train_time:61348ms step_avg:39.35ms
step:1560/2330 train_time:61404ms step_avg:39.36ms
step:1561/2330 train_time:61427ms step_avg:39.35ms
step:1562/2330 train_time:61483ms step_avg:39.36ms
step:1563/2330 train_time:61506ms step_avg:39.35ms
step:1564/2330 train_time:61563ms step_avg:39.36ms
step:1565/2330 train_time:61586ms step_avg:39.35ms
step:1566/2330 train_time:61642ms step_avg:39.36ms
step:1567/2330 train_time:61666ms step_avg:39.35ms
step:1568/2330 train_time:61722ms step_avg:39.36ms
step:1569/2330 train_time:61744ms step_avg:39.35ms
step:1570/2330 train_time:61800ms step_avg:39.36ms
step:1571/2330 train_time:61822ms step_avg:39.35ms
step:1572/2330 train_time:61878ms step_avg:39.36ms
step:1573/2330 train_time:61901ms step_avg:39.35ms
step:1574/2330 train_time:61957ms step_avg:39.36ms
step:1575/2330 train_time:61979ms step_avg:39.35ms
step:1576/2330 train_time:62035ms step_avg:39.36ms
step:1577/2330 train_time:62057ms step_avg:39.35ms
step:1578/2330 train_time:62113ms step_avg:39.36ms
step:1579/2330 train_time:62135ms step_avg:39.35ms
step:1580/2330 train_time:62191ms step_avg:39.36ms
step:1581/2330 train_time:62214ms step_avg:39.35ms
step:1582/2330 train_time:62270ms step_avg:39.36ms
step:1583/2330 train_time:62293ms step_avg:39.35ms
step:1584/2330 train_time:62350ms step_avg:39.36ms
step:1585/2330 train_time:62372ms step_avg:39.35ms
step:1586/2330 train_time:62428ms step_avg:39.36ms
step:1587/2330 train_time:62450ms step_avg:39.35ms
step:1588/2330 train_time:62507ms step_avg:39.36ms
step:1589/2330 train_time:62529ms step_avg:39.35ms
step:1590/2330 train_time:62585ms step_avg:39.36ms
step:1591/2330 train_time:62607ms step_avg:39.35ms
step:1592/2330 train_time:62663ms step_avg:39.36ms
step:1593/2330 train_time:62686ms step_avg:39.35ms
step:1594/2330 train_time:62741ms step_avg:39.36ms
step:1595/2330 train_time:62764ms step_avg:39.35ms
step:1596/2330 train_time:62820ms step_avg:39.36ms
step:1597/2330 train_time:62842ms step_avg:39.35ms
step:1598/2330 train_time:62898ms step_avg:39.36ms
step:1599/2330 train_time:62921ms step_avg:39.35ms
step:1600/2330 train_time:62977ms step_avg:39.36ms
step:1601/2330 train_time:62999ms step_avg:39.35ms
step:1602/2330 train_time:63056ms step_avg:39.36ms
step:1603/2330 train_time:63079ms step_avg:39.35ms
step:1604/2330 train_time:63134ms step_avg:39.36ms
step:1605/2330 train_time:63157ms step_avg:39.35ms
step:1606/2330 train_time:63212ms step_avg:39.36ms
step:1607/2330 train_time:63235ms step_avg:39.35ms
step:1608/2330 train_time:63291ms step_avg:39.36ms
step:1609/2330 train_time:63314ms step_avg:39.35ms
step:1610/2330 train_time:63372ms step_avg:39.36ms
step:1611/2330 train_time:63394ms step_avg:39.35ms
step:1612/2330 train_time:63451ms step_avg:39.36ms
step:1613/2330 train_time:63473ms step_avg:39.35ms
step:1614/2330 train_time:63531ms step_avg:39.36ms
step:1615/2330 train_time:63553ms step_avg:39.35ms
step:1616/2330 train_time:63611ms step_avg:39.36ms
step:1617/2330 train_time:63633ms step_avg:39.35ms
step:1618/2330 train_time:63691ms step_avg:39.36ms
step:1619/2330 train_time:63714ms step_avg:39.35ms
step:1620/2330 train_time:63771ms step_avg:39.36ms
step:1621/2330 train_time:63793ms step_avg:39.35ms
step:1622/2330 train_time:63849ms step_avg:39.36ms
step:1623/2330 train_time:63871ms step_avg:39.35ms
step:1624/2330 train_time:63927ms step_avg:39.36ms
step:1625/2330 train_time:63950ms step_avg:39.35ms
step:1626/2330 train_time:64005ms step_avg:39.36ms
step:1627/2330 train_time:64027ms step_avg:39.35ms
step:1628/2330 train_time:64083ms step_avg:39.36ms
step:1629/2330 train_time:64106ms step_avg:39.35ms
step:1630/2330 train_time:64161ms step_avg:39.36ms
step:1631/2330 train_time:64184ms step_avg:39.35ms
step:1632/2330 train_time:64240ms step_avg:39.36ms
step:1633/2330 train_time:64264ms step_avg:39.35ms
step:1634/2330 train_time:64321ms step_avg:39.36ms
step:1635/2330 train_time:64344ms step_avg:39.35ms
step:1636/2330 train_time:64401ms step_avg:39.36ms
step:1637/2330 train_time:64423ms step_avg:39.35ms
step:1638/2330 train_time:64479ms step_avg:39.36ms
step:1639/2330 train_time:64502ms step_avg:39.35ms
step:1640/2330 train_time:64558ms step_avg:39.36ms
step:1641/2330 train_time:64581ms step_avg:39.35ms
step:1642/2330 train_time:64637ms step_avg:39.36ms
step:1643/2330 train_time:64659ms step_avg:39.35ms
step:1644/2330 train_time:64715ms step_avg:39.36ms
step:1645/2330 train_time:64738ms step_avg:39.35ms
step:1646/2330 train_time:64795ms step_avg:39.36ms
step:1647/2330 train_time:64817ms step_avg:39.35ms
step:1648/2330 train_time:64874ms step_avg:39.37ms
step:1649/2330 train_time:64896ms step_avg:39.35ms
step:1650/2330 train_time:64953ms step_avg:39.37ms
step:1651/2330 train_time:64975ms step_avg:39.35ms
step:1652/2330 train_time:65032ms step_avg:39.37ms
step:1653/2330 train_time:65053ms step_avg:39.35ms
step:1654/2330 train_time:65110ms step_avg:39.36ms
step:1655/2330 train_time:65133ms step_avg:39.35ms
step:1656/2330 train_time:65189ms step_avg:39.37ms
step:1657/2330 train_time:65212ms step_avg:39.36ms
step:1658/2330 train_time:65268ms step_avg:39.37ms
step:1659/2330 train_time:65290ms step_avg:39.36ms
step:1660/2330 train_time:65347ms step_avg:39.37ms
step:1661/2330 train_time:65369ms step_avg:39.36ms
step:1662/2330 train_time:65425ms step_avg:39.37ms
step:1663/2330 train_time:65447ms step_avg:39.35ms
step:1664/2330 train_time:65503ms step_avg:39.36ms
step:1665/2330 train_time:65525ms step_avg:39.35ms
step:1666/2330 train_time:65581ms step_avg:39.36ms
step:1667/2330 train_time:65604ms step_avg:39.35ms
step:1668/2330 train_time:65661ms step_avg:39.36ms
step:1669/2330 train_time:65684ms step_avg:39.36ms
step:1670/2330 train_time:65740ms step_avg:39.37ms
step:1671/2330 train_time:65762ms step_avg:39.35ms
step:1672/2330 train_time:65818ms step_avg:39.36ms
step:1673/2330 train_time:65841ms step_avg:39.35ms
step:1674/2330 train_time:65896ms step_avg:39.36ms
step:1675/2330 train_time:65919ms step_avg:39.35ms
step:1676/2330 train_time:65975ms step_avg:39.36ms
step:1677/2330 train_time:65997ms step_avg:39.35ms
step:1678/2330 train_time:66053ms step_avg:39.36ms
step:1679/2330 train_time:66075ms step_avg:39.35ms
step:1680/2330 train_time:66132ms step_avg:39.36ms
step:1681/2330 train_time:66154ms step_avg:39.35ms
step:1682/2330 train_time:66211ms step_avg:39.36ms
step:1683/2330 train_time:66234ms step_avg:39.35ms
step:1684/2330 train_time:66291ms step_avg:39.36ms
step:1685/2330 train_time:66313ms step_avg:39.35ms
step:1686/2330 train_time:66370ms step_avg:39.37ms
step:1687/2330 train_time:66392ms step_avg:39.36ms
step:1688/2330 train_time:66448ms step_avg:39.37ms
step:1689/2330 train_time:66471ms step_avg:39.36ms
step:1690/2330 train_time:66530ms step_avg:39.37ms
step:1691/2330 train_time:66552ms step_avg:39.36ms
step:1692/2330 train_time:66608ms step_avg:39.37ms
step:1693/2330 train_time:66630ms step_avg:39.36ms
step:1694/2330 train_time:66686ms step_avg:39.37ms
step:1695/2330 train_time:66708ms step_avg:39.36ms
step:1696/2330 train_time:66764ms step_avg:39.37ms
step:1697/2330 train_time:66786ms step_avg:39.36ms
step:1698/2330 train_time:66842ms step_avg:39.36ms
step:1699/2330 train_time:66865ms step_avg:39.36ms
step:1700/2330 train_time:66921ms step_avg:39.37ms
step:1701/2330 train_time:66944ms step_avg:39.36ms
step:1702/2330 train_time:67002ms step_avg:39.37ms
step:1703/2330 train_time:67025ms step_avg:39.36ms
step:1704/2330 train_time:67082ms step_avg:39.37ms
step:1705/2330 train_time:67105ms step_avg:39.36ms
step:1706/2330 train_time:67162ms step_avg:39.37ms
step:1707/2330 train_time:67185ms step_avg:39.36ms
step:1708/2330 train_time:67242ms step_avg:39.37ms
step:1709/2330 train_time:67265ms step_avg:39.36ms
step:1710/2330 train_time:67320ms step_avg:39.37ms
step:1711/2330 train_time:67343ms step_avg:39.36ms
step:1712/2330 train_time:67399ms step_avg:39.37ms
step:1713/2330 train_time:67422ms step_avg:39.36ms
step:1714/2330 train_time:67478ms step_avg:39.37ms
step:1715/2330 train_time:67500ms step_avg:39.36ms
step:1716/2330 train_time:67556ms step_avg:39.37ms
step:1717/2330 train_time:67579ms step_avg:39.36ms
step:1718/2330 train_time:67636ms step_avg:39.37ms
step:1719/2330 train_time:67658ms step_avg:39.36ms
step:1720/2330 train_time:67714ms step_avg:39.37ms
step:1721/2330 train_time:67737ms step_avg:39.36ms
step:1722/2330 train_time:67793ms step_avg:39.37ms
step:1723/2330 train_time:67815ms step_avg:39.36ms
step:1724/2330 train_time:67871ms step_avg:39.37ms
step:1725/2330 train_time:67893ms step_avg:39.36ms
step:1726/2330 train_time:67950ms step_avg:39.37ms
step:1727/2330 train_time:67972ms step_avg:39.36ms
step:1728/2330 train_time:68029ms step_avg:39.37ms
step:1729/2330 train_time:68051ms step_avg:39.36ms
step:1730/2330 train_time:68107ms step_avg:39.37ms
step:1731/2330 train_time:68129ms step_avg:39.36ms
step:1732/2330 train_time:68186ms step_avg:39.37ms
step:1733/2330 train_time:68208ms step_avg:39.36ms
step:1734/2330 train_time:68264ms step_avg:39.37ms
step:1735/2330 train_time:68287ms step_avg:39.36ms
step:1736/2330 train_time:68342ms step_avg:39.37ms
step:1737/2330 train_time:68365ms step_avg:39.36ms
step:1738/2330 train_time:68421ms step_avg:39.37ms
step:1739/2330 train_time:68444ms step_avg:39.36ms
step:1740/2330 train_time:68500ms step_avg:39.37ms
step:1741/2330 train_time:68523ms step_avg:39.36ms
step:1742/2330 train_time:68579ms step_avg:39.37ms
step:1743/2330 train_time:68602ms step_avg:39.36ms
step:1744/2330 train_time:68658ms step_avg:39.37ms
step:1745/2330 train_time:68680ms step_avg:39.36ms
step:1746/2330 train_time:68736ms step_avg:39.37ms
step:1747/2330 train_time:68759ms step_avg:39.36ms
step:1748/2330 train_time:68814ms step_avg:39.37ms
step:1749/2330 train_time:68837ms step_avg:39.36ms
step:1750/2330 train_time:68893ms step_avg:39.37ms
step:1750/2330 val_loss:5.1624 train_time:68989ms step_avg:39.42ms
step:1751/2330 train_time:69003ms step_avg:39.41ms
step:1752/2330 train_time:69015ms step_avg:39.39ms
step:1753/2330 train_time:69026ms step_avg:39.38ms
step:1754/2330 train_time:69051ms step_avg:39.37ms
step:1755/2330 train_time:69072ms step_avg:39.36ms
step:1756/2330 train_time:69127ms step_avg:39.37ms
step:1757/2330 train_time:69148ms step_avg:39.36ms
step:1758/2330 train_time:69203ms step_avg:39.36ms
step:1759/2330 train_time:69225ms step_avg:39.35ms
step:1760/2330 train_time:69279ms step_avg:39.36ms
step:1761/2330 train_time:69303ms step_avg:39.35ms
step:1762/2330 train_time:69361ms step_avg:39.37ms
step:1763/2330 train_time:69386ms step_avg:39.36ms
step:1764/2330 train_time:69443ms step_avg:39.37ms
step:1765/2330 train_time:69465ms step_avg:39.36ms
step:1766/2330 train_time:69521ms step_avg:39.37ms
step:1767/2330 train_time:69544ms step_avg:39.36ms
step:1768/2330 train_time:69600ms step_avg:39.37ms
step:1769/2330 train_time:69622ms step_avg:39.36ms
step:1770/2330 train_time:69677ms step_avg:39.37ms
step:1771/2330 train_time:69700ms step_avg:39.36ms
step:1772/2330 train_time:69755ms step_avg:39.36ms
step:1773/2330 train_time:69776ms step_avg:39.35ms
step:1774/2330 train_time:69832ms step_avg:39.36ms
step:1775/2330 train_time:69855ms step_avg:39.35ms
step:1776/2330 train_time:69912ms step_avg:39.36ms
step:1777/2330 train_time:69935ms step_avg:39.36ms
step:1778/2330 train_time:69992ms step_avg:39.37ms
step:1779/2330 train_time:70013ms step_avg:39.36ms
step:1780/2330 train_time:70069ms step_avg:39.36ms
step:1781/2330 train_time:70091ms step_avg:39.35ms
step:1782/2330 train_time:70147ms step_avg:39.36ms
step:1783/2330 train_time:70169ms step_avg:39.35ms
step:1784/2330 train_time:70225ms step_avg:39.36ms
step:1785/2330 train_time:70246ms step_avg:39.35ms
step:1786/2330 train_time:70303ms step_avg:39.36ms
step:1787/2330 train_time:70326ms step_avg:39.35ms
step:1788/2330 train_time:70382ms step_avg:39.36ms
step:1789/2330 train_time:70405ms step_avg:39.35ms
step:1790/2330 train_time:70461ms step_avg:39.36ms
step:1791/2330 train_time:70483ms step_avg:39.35ms
step:1792/2330 train_time:70539ms step_avg:39.36ms
step:1793/2330 train_time:70561ms step_avg:39.35ms
step:1794/2330 train_time:70617ms step_avg:39.36ms
step:1795/2330 train_time:70639ms step_avg:39.35ms
step:1796/2330 train_time:70694ms step_avg:39.36ms
step:1797/2330 train_time:70716ms step_avg:39.35ms
step:1798/2330 train_time:70771ms step_avg:39.36ms
step:1799/2330 train_time:70793ms step_avg:39.35ms
step:1800/2330 train_time:70849ms step_avg:39.36ms
step:1801/2330 train_time:70871ms step_avg:39.35ms
step:1802/2330 train_time:70928ms step_avg:39.36ms
step:1803/2330 train_time:70950ms step_avg:39.35ms
step:1804/2330 train_time:71006ms step_avg:39.36ms
step:1805/2330 train_time:71028ms step_avg:39.35ms
step:1806/2330 train_time:71084ms step_avg:39.36ms
step:1807/2330 train_time:71106ms step_avg:39.35ms
step:1808/2330 train_time:71162ms step_avg:39.36ms
step:1809/2330 train_time:71183ms step_avg:39.35ms
step:1810/2330 train_time:71239ms step_avg:39.36ms
step:1811/2330 train_time:71262ms step_avg:39.35ms
step:1812/2330 train_time:71318ms step_avg:39.36ms
step:1813/2330 train_time:71341ms step_avg:39.35ms
step:1814/2330 train_time:71397ms step_avg:39.36ms
step:1815/2330 train_time:71420ms step_avg:39.35ms
step:1816/2330 train_time:71476ms step_avg:39.36ms
step:1817/2330 train_time:71499ms step_avg:39.35ms
step:1818/2330 train_time:71554ms step_avg:39.36ms
step:1819/2330 train_time:71577ms step_avg:39.35ms
step:1820/2330 train_time:71632ms step_avg:39.36ms
step:1821/2330 train_time:71655ms step_avg:39.35ms
step:1822/2330 train_time:71711ms step_avg:39.36ms
step:1823/2330 train_time:71733ms step_avg:39.35ms
step:1824/2330 train_time:71789ms step_avg:39.36ms
step:1825/2330 train_time:71810ms step_avg:39.35ms
step:1826/2330 train_time:71866ms step_avg:39.36ms
step:1827/2330 train_time:71888ms step_avg:39.35ms
step:1828/2330 train_time:71944ms step_avg:39.36ms
step:1829/2330 train_time:71966ms step_avg:39.35ms
step:1830/2330 train_time:72022ms step_avg:39.36ms
step:1831/2330 train_time:72044ms step_avg:39.35ms
step:1832/2330 train_time:72099ms step_avg:39.36ms
step:1833/2330 train_time:72121ms step_avg:39.35ms
step:1834/2330 train_time:72177ms step_avg:39.35ms
step:1835/2330 train_time:72199ms step_avg:39.35ms
step:1836/2330 train_time:72255ms step_avg:39.35ms
step:1837/2330 train_time:72277ms step_avg:39.35ms
step:1838/2330 train_time:72333ms step_avg:39.35ms
step:1839/2330 train_time:72356ms step_avg:39.35ms
step:1840/2330 train_time:72413ms step_avg:39.35ms
step:1841/2330 train_time:72435ms step_avg:39.35ms
step:1842/2330 train_time:72491ms step_avg:39.35ms
step:1843/2330 train_time:72513ms step_avg:39.35ms
step:1844/2330 train_time:72570ms step_avg:39.35ms
step:1845/2330 train_time:72592ms step_avg:39.35ms
step:1846/2330 train_time:72648ms step_avg:39.35ms
step:1847/2330 train_time:72670ms step_avg:39.34ms
step:1848/2330 train_time:72725ms step_avg:39.35ms
step:1849/2330 train_time:72747ms step_avg:39.34ms
step:1850/2330 train_time:72804ms step_avg:39.35ms
step:1851/2330 train_time:72825ms step_avg:39.34ms
step:1852/2330 train_time:72881ms step_avg:39.35ms
step:1853/2330 train_time:72903ms step_avg:39.34ms
step:1854/2330 train_time:72959ms step_avg:39.35ms
step:1855/2330 train_time:72981ms step_avg:39.34ms
step:1856/2330 train_time:73037ms step_avg:39.35ms
step:1857/2330 train_time:73060ms step_avg:39.34ms
step:1858/2330 train_time:73115ms step_avg:39.35ms
step:1859/2330 train_time:73138ms step_avg:39.34ms
step:1860/2330 train_time:73193ms step_avg:39.35ms
step:1861/2330 train_time:73216ms step_avg:39.34ms
step:1862/2330 train_time:73272ms step_avg:39.35ms
step:1863/2330 train_time:73295ms step_avg:39.34ms
step:1864/2330 train_time:73351ms step_avg:39.35ms
step:1865/2330 train_time:73373ms step_avg:39.34ms
step:1866/2330 train_time:73429ms step_avg:39.35ms
step:1867/2330 train_time:73452ms step_avg:39.34ms
step:1868/2330 train_time:73507ms step_avg:39.35ms
step:1869/2330 train_time:73530ms step_avg:39.34ms
step:1870/2330 train_time:73585ms step_avg:39.35ms
step:1871/2330 train_time:73607ms step_avg:39.34ms
step:1872/2330 train_time:73663ms step_avg:39.35ms
step:1873/2330 train_time:73685ms step_avg:39.34ms
step:1874/2330 train_time:73740ms step_avg:39.35ms
step:1875/2330 train_time:73763ms step_avg:39.34ms
step:1876/2330 train_time:73818ms step_avg:39.35ms
step:1877/2330 train_time:73841ms step_avg:39.34ms
step:1878/2330 train_time:73897ms step_avg:39.35ms
step:1879/2330 train_time:73919ms step_avg:39.34ms
step:1880/2330 train_time:73975ms step_avg:39.35ms
step:1881/2330 train_time:73997ms step_avg:39.34ms
step:1882/2330 train_time:74053ms step_avg:39.35ms
step:1883/2330 train_time:74075ms step_avg:39.34ms
step:1884/2330 train_time:74132ms step_avg:39.35ms
step:1885/2330 train_time:74153ms step_avg:39.34ms
step:1886/2330 train_time:74209ms step_avg:39.35ms
step:1887/2330 train_time:74231ms step_avg:39.34ms
step:1888/2330 train_time:74288ms step_avg:39.35ms
step:1889/2330 train_time:74310ms step_avg:39.34ms
step:1890/2330 train_time:74365ms step_avg:39.35ms
step:1891/2330 train_time:74388ms step_avg:39.34ms
step:1892/2330 train_time:74444ms step_avg:39.35ms
step:1893/2330 train_time:74466ms step_avg:39.34ms
step:1894/2330 train_time:74521ms step_avg:39.35ms
step:1895/2330 train_time:74544ms step_avg:39.34ms
step:1896/2330 train_time:74599ms step_avg:39.35ms
step:1897/2330 train_time:74621ms step_avg:39.34ms
step:1898/2330 train_time:74677ms step_avg:39.35ms
step:1899/2330 train_time:74700ms step_avg:39.34ms
step:1900/2330 train_time:74755ms step_avg:39.34ms
step:1901/2330 train_time:74778ms step_avg:39.34ms
step:1902/2330 train_time:74834ms step_avg:39.34ms
step:1903/2330 train_time:74856ms step_avg:39.34ms
step:1904/2330 train_time:74913ms step_avg:39.35ms
step:1905/2330 train_time:74936ms step_avg:39.34ms
step:1906/2330 train_time:74992ms step_avg:39.35ms
step:1907/2330 train_time:75014ms step_avg:39.34ms
step:1908/2330 train_time:75070ms step_avg:39.34ms
step:1909/2330 train_time:75092ms step_avg:39.34ms
step:1910/2330 train_time:75148ms step_avg:39.34ms
step:1911/2330 train_time:75170ms step_avg:39.34ms
step:1912/2330 train_time:75226ms step_avg:39.34ms
step:1913/2330 train_time:75248ms step_avg:39.34ms
step:1914/2330 train_time:75304ms step_avg:39.34ms
step:1915/2330 train_time:75325ms step_avg:39.33ms
step:1916/2330 train_time:75380ms step_avg:39.34ms
step:1917/2330 train_time:75402ms step_avg:39.33ms
step:1918/2330 train_time:75458ms step_avg:39.34ms
step:1919/2330 train_time:75480ms step_avg:39.33ms
step:1920/2330 train_time:75536ms step_avg:39.34ms
step:1921/2330 train_time:75559ms step_avg:39.33ms
step:1922/2330 train_time:75615ms step_avg:39.34ms
step:1923/2330 train_time:75638ms step_avg:39.33ms
step:1924/2330 train_time:75694ms step_avg:39.34ms
step:1925/2330 train_time:75716ms step_avg:39.33ms
step:1926/2330 train_time:75772ms step_avg:39.34ms
step:1927/2330 train_time:75794ms step_avg:39.33ms
step:1928/2330 train_time:75850ms step_avg:39.34ms
step:1929/2330 train_time:75872ms step_avg:39.33ms
step:1930/2330 train_time:75928ms step_avg:39.34ms
step:1931/2330 train_time:75950ms step_avg:39.33ms
step:1932/2330 train_time:76006ms step_avg:39.34ms
step:1933/2330 train_time:76027ms step_avg:39.33ms
step:1934/2330 train_time:76083ms step_avg:39.34ms
step:1935/2330 train_time:76105ms step_avg:39.33ms
step:1936/2330 train_time:76160ms step_avg:39.34ms
step:1937/2330 train_time:76183ms step_avg:39.33ms
step:1938/2330 train_time:76239ms step_avg:39.34ms
step:1939/2330 train_time:76261ms step_avg:39.33ms
step:1940/2330 train_time:76317ms step_avg:39.34ms
step:1941/2330 train_time:76340ms step_avg:39.33ms
step:1942/2330 train_time:76396ms step_avg:39.34ms
step:1943/2330 train_time:76418ms step_avg:39.33ms
step:1944/2330 train_time:76473ms step_avg:39.34ms
step:1945/2330 train_time:76495ms step_avg:39.33ms
step:1946/2330 train_time:76551ms step_avg:39.34ms
step:1947/2330 train_time:76574ms step_avg:39.33ms
step:1948/2330 train_time:76631ms step_avg:39.34ms
step:1949/2330 train_time:76655ms step_avg:39.33ms
step:1950/2330 train_time:76712ms step_avg:39.34ms
step:1951/2330 train_time:76735ms step_avg:39.33ms
step:1952/2330 train_time:76790ms step_avg:39.34ms
step:1953/2330 train_time:76813ms step_avg:39.33ms
step:1954/2330 train_time:76869ms step_avg:39.34ms
step:1955/2330 train_time:76891ms step_avg:39.33ms
step:1956/2330 train_time:76947ms step_avg:39.34ms
step:1957/2330 train_time:76969ms step_avg:39.33ms
step:1958/2330 train_time:77025ms step_avg:39.34ms
step:1959/2330 train_time:77047ms step_avg:39.33ms
step:1960/2330 train_time:77104ms step_avg:39.34ms
step:1961/2330 train_time:77125ms step_avg:39.33ms
step:1962/2330 train_time:77181ms step_avg:39.34ms
step:1963/2330 train_time:77203ms step_avg:39.33ms
step:1964/2330 train_time:77258ms step_avg:39.34ms
step:1965/2330 train_time:77280ms step_avg:39.33ms
step:1966/2330 train_time:77337ms step_avg:39.34ms
step:1967/2330 train_time:77358ms step_avg:39.33ms
step:1968/2330 train_time:77414ms step_avg:39.34ms
step:1969/2330 train_time:77436ms step_avg:39.33ms
step:1970/2330 train_time:77492ms step_avg:39.34ms
step:1971/2330 train_time:77515ms step_avg:39.33ms
step:1972/2330 train_time:77571ms step_avg:39.34ms
step:1973/2330 train_time:77593ms step_avg:39.33ms
step:1974/2330 train_time:77649ms step_avg:39.34ms
step:1975/2330 train_time:77672ms step_avg:39.33ms
step:1976/2330 train_time:77728ms step_avg:39.34ms
step:1977/2330 train_time:77750ms step_avg:39.33ms
step:1978/2330 train_time:77806ms step_avg:39.34ms
step:1979/2330 train_time:77828ms step_avg:39.33ms
step:1980/2330 train_time:77884ms step_avg:39.34ms
step:1981/2330 train_time:77906ms step_avg:39.33ms
step:1982/2330 train_time:77961ms step_avg:39.33ms
step:1983/2330 train_time:77983ms step_avg:39.33ms
step:1984/2330 train_time:78039ms step_avg:39.33ms
step:1985/2330 train_time:78062ms step_avg:39.33ms
step:1986/2330 train_time:78117ms step_avg:39.33ms
step:1987/2330 train_time:78139ms step_avg:39.33ms
step:1988/2330 train_time:78196ms step_avg:39.33ms
step:1989/2330 train_time:78218ms step_avg:39.33ms
step:1990/2330 train_time:78274ms step_avg:39.33ms
step:1991/2330 train_time:78296ms step_avg:39.32ms
step:1992/2330 train_time:78352ms step_avg:39.33ms
step:1993/2330 train_time:78374ms step_avg:39.32ms
step:1994/2330 train_time:78430ms step_avg:39.33ms
step:1995/2330 train_time:78452ms step_avg:39.32ms
step:1996/2330 train_time:78508ms step_avg:39.33ms
step:1997/2330 train_time:78530ms step_avg:39.32ms
step:1998/2330 train_time:78586ms step_avg:39.33ms
step:1999/2330 train_time:78608ms step_avg:39.32ms
step:2000/2330 train_time:78664ms step_avg:39.33ms
step:2000/2330 val_loss:5.1491 train_time:78758ms step_avg:39.38ms
step:2001/2330 train_time:78772ms step_avg:39.37ms
step:2002/2330 train_time:78785ms step_avg:39.35ms
step:2003/2330 train_time:78795ms step_avg:39.34ms
step:2004/2330 train_time:78821ms step_avg:39.33ms
step:2005/2330 train_time:78843ms step_avg:39.32ms
step:2006/2330 train_time:78897ms step_avg:39.33ms
step:2007/2330 train_time:78919ms step_avg:39.32ms
step:2008/2330 train_time:78974ms step_avg:39.33ms
step:2009/2330 train_time:78996ms step_avg:39.32ms
step:2010/2330 train_time:79052ms step_avg:39.33ms
step:2011/2330 train_time:79078ms step_avg:39.32ms
step:2012/2330 train_time:79138ms step_avg:39.33ms
step:2013/2330 train_time:79161ms step_avg:39.33ms
step:2014/2330 train_time:79218ms step_avg:39.33ms
step:2015/2330 train_time:79241ms step_avg:39.33ms
step:2016/2330 train_time:79296ms step_avg:39.33ms
step:2017/2330 train_time:79319ms step_avg:39.33ms
step:2018/2330 train_time:79374ms step_avg:39.33ms
step:2019/2330 train_time:79396ms step_avg:39.32ms
step:2020/2330 train_time:79452ms step_avg:39.33ms
step:2021/2330 train_time:79474ms step_avg:39.32ms
step:2022/2330 train_time:79530ms step_avg:39.33ms
step:2023/2330 train_time:79553ms step_avg:39.32ms
step:2024/2330 train_time:79608ms step_avg:39.33ms
step:2025/2330 train_time:79630ms step_avg:39.32ms
step:2026/2330 train_time:79687ms step_avg:39.33ms
step:2027/2330 train_time:79710ms step_avg:39.32ms
step:2028/2330 train_time:79767ms step_avg:39.33ms
step:2029/2330 train_time:79789ms step_avg:39.32ms
step:2030/2330 train_time:79845ms step_avg:39.33ms
step:2031/2330 train_time:79867ms step_avg:39.32ms
step:2032/2330 train_time:79922ms step_avg:39.33ms
step:2033/2330 train_time:79944ms step_avg:39.32ms
step:2034/2330 train_time:80001ms step_avg:39.33ms
step:2035/2330 train_time:80024ms step_avg:39.32ms
step:2036/2330 train_time:80081ms step_avg:39.33ms
step:2037/2330 train_time:80105ms step_avg:39.32ms
step:2038/2330 train_time:80161ms step_avg:39.33ms
step:2039/2330 train_time:80184ms step_avg:39.33ms
step:2040/2330 train_time:80240ms step_avg:39.33ms
step:2041/2330 train_time:80263ms step_avg:39.33ms
step:2042/2330 train_time:80319ms step_avg:39.33ms
step:2043/2330 train_time:80341ms step_avg:39.33ms
step:2044/2330 train_time:80397ms step_avg:39.33ms
step:2045/2330 train_time:80420ms step_avg:39.33ms
step:2046/2330 train_time:80476ms step_avg:39.33ms
step:2047/2330 train_time:80498ms step_avg:39.32ms
step:2048/2330 train_time:80554ms step_avg:39.33ms
step:2049/2330 train_time:80577ms step_avg:39.32ms
step:2050/2330 train_time:80633ms step_avg:39.33ms
step:2051/2330 train_time:80655ms step_avg:39.32ms
step:2052/2330 train_time:80711ms step_avg:39.33ms
step:2053/2330 train_time:80734ms step_avg:39.32ms
step:2054/2330 train_time:80790ms step_avg:39.33ms
step:2055/2330 train_time:80812ms step_avg:39.32ms
step:2056/2330 train_time:80868ms step_avg:39.33ms
step:2057/2330 train_time:80890ms step_avg:39.32ms
step:2058/2330 train_time:80946ms step_avg:39.33ms
step:2059/2330 train_time:80968ms step_avg:39.32ms
step:2060/2330 train_time:81025ms step_avg:39.33ms
step:2061/2330 train_time:81047ms step_avg:39.32ms
step:2062/2330 train_time:81103ms step_avg:39.33ms
step:2063/2330 train_time:81126ms step_avg:39.32ms
step:2064/2330 train_time:81182ms step_avg:39.33ms
step:2065/2330 train_time:81205ms step_avg:39.32ms
step:2066/2330 train_time:81261ms step_avg:39.33ms
step:2067/2330 train_time:81284ms step_avg:39.32ms
step:2068/2330 train_time:81340ms step_avg:39.33ms
step:2069/2330 train_time:81363ms step_avg:39.32ms
step:2070/2330 train_time:81421ms step_avg:39.33ms
step:2071/2330 train_time:81444ms step_avg:39.33ms
step:2072/2330 train_time:81500ms step_avg:39.33ms
step:2073/2330 train_time:81523ms step_avg:39.33ms
step:2074/2330 train_time:81579ms step_avg:39.33ms
step:2075/2330 train_time:81601ms step_avg:39.33ms
step:2076/2330 train_time:81657ms step_avg:39.33ms
step:2077/2330 train_time:81679ms step_avg:39.33ms
step:2078/2330 train_time:81735ms step_avg:39.33ms
step:2079/2330 train_time:81757ms step_avg:39.33ms
step:2080/2330 train_time:81813ms step_avg:39.33ms
step:2081/2330 train_time:81836ms step_avg:39.33ms
step:2082/2330 train_time:81891ms step_avg:39.33ms
step:2083/2330 train_time:81914ms step_avg:39.32ms
step:2084/2330 train_time:81970ms step_avg:39.33ms
step:2085/2330 train_time:81992ms step_avg:39.32ms
step:2086/2330 train_time:82049ms step_avg:39.33ms
step:2087/2330 train_time:82070ms step_avg:39.32ms
step:2088/2330 train_time:82127ms step_avg:39.33ms
step:2089/2330 train_time:82149ms step_avg:39.32ms
step:2090/2330 train_time:82206ms step_avg:39.33ms
step:2091/2330 train_time:82227ms step_avg:39.32ms
step:2092/2330 train_time:82284ms step_avg:39.33ms
step:2093/2330 train_time:82306ms step_avg:39.32ms
step:2094/2330 train_time:82362ms step_avg:39.33ms
step:2095/2330 train_time:82385ms step_avg:39.32ms
step:2096/2330 train_time:82440ms step_avg:39.33ms
step:2097/2330 train_time:82463ms step_avg:39.32ms
step:2098/2330 train_time:82520ms step_avg:39.33ms
step:2099/2330 train_time:82543ms step_avg:39.32ms
step:2100/2330 train_time:82600ms step_avg:39.33ms
step:2101/2330 train_time:82623ms step_avg:39.33ms
step:2102/2330 train_time:82679ms step_avg:39.33ms
step:2103/2330 train_time:82702ms step_avg:39.33ms
step:2104/2330 train_time:82757ms step_avg:39.33ms
step:2105/2330 train_time:82780ms step_avg:39.33ms
step:2106/2330 train_time:82835ms step_avg:39.33ms
step:2107/2330 train_time:82858ms step_avg:39.32ms
step:2108/2330 train_time:82914ms step_avg:39.33ms
step:2109/2330 train_time:82936ms step_avg:39.32ms
step:2110/2330 train_time:82993ms step_avg:39.33ms
step:2111/2330 train_time:83015ms step_avg:39.33ms
step:2112/2330 train_time:83073ms step_avg:39.33ms
step:2113/2330 train_time:83095ms step_avg:39.33ms
step:2114/2330 train_time:83152ms step_avg:39.33ms
step:2115/2330 train_time:83174ms step_avg:39.33ms
step:2116/2330 train_time:83230ms step_avg:39.33ms
step:2117/2330 train_time:83252ms step_avg:39.33ms
step:2118/2330 train_time:83309ms step_avg:39.33ms
step:2119/2330 train_time:83331ms step_avg:39.33ms
step:2120/2330 train_time:83388ms step_avg:39.33ms
step:2121/2330 train_time:83410ms step_avg:39.33ms
step:2122/2330 train_time:83467ms step_avg:39.33ms
step:2123/2330 train_time:83489ms step_avg:39.33ms
step:2124/2330 train_time:83546ms step_avg:39.33ms
step:2125/2330 train_time:83568ms step_avg:39.33ms
step:2126/2330 train_time:83625ms step_avg:39.33ms
step:2127/2330 train_time:83647ms step_avg:39.33ms
step:2128/2330 train_time:83703ms step_avg:39.33ms
step:2129/2330 train_time:83725ms step_avg:39.33ms
step:2130/2330 train_time:83781ms step_avg:39.33ms
step:2131/2330 train_time:83804ms step_avg:39.33ms
step:2132/2330 train_time:83860ms step_avg:39.33ms
step:2133/2330 train_time:83884ms step_avg:39.33ms
step:2134/2330 train_time:83940ms step_avg:39.33ms
step:2135/2330 train_time:83963ms step_avg:39.33ms
step:2136/2330 train_time:84019ms step_avg:39.33ms
step:2137/2330 train_time:84041ms step_avg:39.33ms
step:2138/2330 train_time:84097ms step_avg:39.33ms
step:2139/2330 train_time:84119ms step_avg:39.33ms
step:2140/2330 train_time:84175ms step_avg:39.33ms
step:2141/2330 train_time:84198ms step_avg:39.33ms
step:2142/2330 train_time:84254ms step_avg:39.33ms
step:2143/2330 train_time:84276ms step_avg:39.33ms
step:2144/2330 train_time:84332ms step_avg:39.33ms
step:2145/2330 train_time:84355ms step_avg:39.33ms
step:2146/2330 train_time:84411ms step_avg:39.33ms
step:2147/2330 train_time:84434ms step_avg:39.33ms
step:2148/2330 train_time:84489ms step_avg:39.33ms
step:2149/2330 train_time:84512ms step_avg:39.33ms
step:2150/2330 train_time:84569ms step_avg:39.33ms
step:2151/2330 train_time:84592ms step_avg:39.33ms
step:2152/2330 train_time:84649ms step_avg:39.33ms
step:2153/2330 train_time:84671ms step_avg:39.33ms
step:2154/2330 train_time:84727ms step_avg:39.33ms
step:2155/2330 train_time:84749ms step_avg:39.33ms
step:2156/2330 train_time:84805ms step_avg:39.33ms
step:2157/2330 train_time:84828ms step_avg:39.33ms
step:2158/2330 train_time:84884ms step_avg:39.33ms
step:2159/2330 train_time:84907ms step_avg:39.33ms
step:2160/2330 train_time:84962ms step_avg:39.33ms
step:2161/2330 train_time:84985ms step_avg:39.33ms
step:2162/2330 train_time:85040ms step_avg:39.33ms
step:2163/2330 train_time:85063ms step_avg:39.33ms
step:2164/2330 train_time:85119ms step_avg:39.33ms
step:2165/2330 train_time:85141ms step_avg:39.33ms
step:2166/2330 train_time:85197ms step_avg:39.33ms
step:2167/2330 train_time:85219ms step_avg:39.33ms
step:2168/2330 train_time:85276ms step_avg:39.33ms
step:2169/2330 train_time:85298ms step_avg:39.33ms
step:2170/2330 train_time:85354ms step_avg:39.33ms
step:2171/2330 train_time:85377ms step_avg:39.33ms
step:2172/2330 train_time:85433ms step_avg:39.33ms
step:2173/2330 train_time:85457ms step_avg:39.33ms
step:2174/2330 train_time:85513ms step_avg:39.33ms
step:2175/2330 train_time:85536ms step_avg:39.33ms
step:2176/2330 train_time:85593ms step_avg:39.33ms
step:2177/2330 train_time:85616ms step_avg:39.33ms
step:2178/2330 train_time:85672ms step_avg:39.34ms
step:2179/2330 train_time:85694ms step_avg:39.33ms
step:2180/2330 train_time:85750ms step_avg:39.33ms
step:2181/2330 train_time:85772ms step_avg:39.33ms
step:2182/2330 train_time:85829ms step_avg:39.33ms
step:2183/2330 train_time:85850ms step_avg:39.33ms
step:2184/2330 train_time:85906ms step_avg:39.33ms
step:2185/2330 train_time:85929ms step_avg:39.33ms
step:2186/2330 train_time:85985ms step_avg:39.33ms
step:2187/2330 train_time:86007ms step_avg:39.33ms
step:2188/2330 train_time:86063ms step_avg:39.33ms
step:2189/2330 train_time:86085ms step_avg:39.33ms
step:2190/2330 train_time:86141ms step_avg:39.33ms
step:2191/2330 train_time:86163ms step_avg:39.33ms
step:2192/2330 train_time:86219ms step_avg:39.33ms
step:2193/2330 train_time:86241ms step_avg:39.33ms
step:2194/2330 train_time:86297ms step_avg:39.33ms
step:2195/2330 train_time:86320ms step_avg:39.33ms
step:2196/2330 train_time:86376ms step_avg:39.33ms
step:2197/2330 train_time:86398ms step_avg:39.33ms
step:2198/2330 train_time:86455ms step_avg:39.33ms
step:2199/2330 train_time:86477ms step_avg:39.33ms
step:2200/2330 train_time:86534ms step_avg:39.33ms
step:2201/2330 train_time:86558ms step_avg:39.33ms
step:2202/2330 train_time:86614ms step_avg:39.33ms
step:2203/2330 train_time:86636ms step_avg:39.33ms
step:2204/2330 train_time:86692ms step_avg:39.33ms
step:2205/2330 train_time:86714ms step_avg:39.33ms
step:2206/2330 train_time:86770ms step_avg:39.33ms
step:2207/2330 train_time:86793ms step_avg:39.33ms
step:2208/2330 train_time:86849ms step_avg:39.33ms
step:2209/2330 train_time:86871ms step_avg:39.33ms
step:2210/2330 train_time:86928ms step_avg:39.33ms
step:2211/2330 train_time:86950ms step_avg:39.33ms
step:2212/2330 train_time:87006ms step_avg:39.33ms
step:2213/2330 train_time:87029ms step_avg:39.33ms
step:2214/2330 train_time:87086ms step_avg:39.33ms
step:2215/2330 train_time:87107ms step_avg:39.33ms
step:2216/2330 train_time:87163ms step_avg:39.33ms
step:2217/2330 train_time:87186ms step_avg:39.33ms
step:2218/2330 train_time:87241ms step_avg:39.33ms
step:2219/2330 train_time:87265ms step_avg:39.33ms
step:2220/2330 train_time:87320ms step_avg:39.33ms
step:2221/2330 train_time:87343ms step_avg:39.33ms
step:2222/2330 train_time:87399ms step_avg:39.33ms
step:2223/2330 train_time:87423ms step_avg:39.33ms
step:2224/2330 train_time:87479ms step_avg:39.33ms
step:2225/2330 train_time:87503ms step_avg:39.33ms
step:2226/2330 train_time:87559ms step_avg:39.33ms
step:2227/2330 train_time:87581ms step_avg:39.33ms
step:2228/2330 train_time:87637ms step_avg:39.33ms
step:2229/2330 train_time:87660ms step_avg:39.33ms
step:2230/2330 train_time:87716ms step_avg:39.33ms
step:2231/2330 train_time:87738ms step_avg:39.33ms
step:2232/2330 train_time:87795ms step_avg:39.33ms
step:2233/2330 train_time:87818ms step_avg:39.33ms
step:2234/2330 train_time:87874ms step_avg:39.33ms
step:2235/2330 train_time:87897ms step_avg:39.33ms
step:2236/2330 train_time:87954ms step_avg:39.34ms
step:2237/2330 train_time:87978ms step_avg:39.33ms
step:2238/2330 train_time:88035ms step_avg:39.34ms
step:2239/2330 train_time:88057ms step_avg:39.33ms
step:2240/2330 train_time:88113ms step_avg:39.34ms
step:2241/2330 train_time:88135ms step_avg:39.33ms
step:2242/2330 train_time:88191ms step_avg:39.34ms
step:2243/2330 train_time:88214ms step_avg:39.33ms
step:2244/2330 train_time:88270ms step_avg:39.34ms
step:2245/2330 train_time:88292ms step_avg:39.33ms
step:2246/2330 train_time:88348ms step_avg:39.34ms
step:2247/2330 train_time:88371ms step_avg:39.33ms
step:2248/2330 train_time:88427ms step_avg:39.34ms
step:2249/2330 train_time:88450ms step_avg:39.33ms
step:2250/2330 train_time:88506ms step_avg:39.34ms
step:2250/2330 val_loss:5.1382 train_time:88601ms step_avg:39.38ms
step:2251/2330 train_time:88614ms step_avg:39.37ms
step:2252/2330 train_time:88627ms step_avg:39.35ms
step:2253/2330 train_time:88638ms step_avg:39.34ms
step:2254/2330 train_time:88664ms step_avg:39.34ms
step:2255/2330 train_time:88685ms step_avg:39.33ms
step:2256/2330 train_time:88740ms step_avg:39.33ms
step:2257/2330 train_time:88761ms step_avg:39.33ms
step:2258/2330 train_time:88816ms step_avg:39.33ms
step:2259/2330 train_time:88838ms step_avg:39.33ms
step:2260/2330 train_time:88893ms step_avg:39.33ms
step:2261/2330 train_time:88919ms step_avg:39.33ms
step:2262/2330 train_time:88979ms step_avg:39.34ms
step:2263/2330 train_time:89001ms step_avg:39.33ms
step:2264/2330 train_time:89059ms step_avg:39.34ms
step:2265/2330 train_time:89081ms step_avg:39.33ms
step:2266/2330 train_time:89136ms step_avg:39.34ms
step:2267/2330 train_time:89158ms step_avg:39.33ms
step:2268/2330 train_time:89214ms step_avg:39.34ms
step:2269/2330 train_time:89236ms step_avg:39.33ms
step:2270/2330 train_time:89291ms step_avg:39.34ms
step:2271/2330 train_time:89313ms step_avg:39.33ms
step:2272/2330 train_time:89368ms step_avg:39.33ms
step:2273/2330 train_time:89390ms step_avg:39.33ms
step:2274/2330 train_time:89445ms step_avg:39.33ms
step:2275/2330 train_time:89467ms step_avg:39.33ms
step:2276/2330 train_time:89522ms step_avg:39.33ms
step:2277/2330 train_time:89545ms step_avg:39.33ms
step:2278/2330 train_time:89601ms step_avg:39.33ms
step:2279/2330 train_time:89623ms step_avg:39.33ms
step:2280/2330 train_time:89678ms step_avg:39.33ms
step:2281/2330 train_time:89700ms step_avg:39.32ms
step:2282/2330 train_time:89756ms step_avg:39.33ms
step:2283/2330 train_time:89778ms step_avg:39.32ms
step:2284/2330 train_time:89834ms step_avg:39.33ms
step:2285/2330 train_time:89856ms step_avg:39.32ms
step:2286/2330 train_time:89913ms step_avg:39.33ms
step:2287/2330 train_time:89935ms step_avg:39.32ms
step:2288/2330 train_time:89992ms step_avg:39.33ms
step:2289/2330 train_time:90015ms step_avg:39.33ms
step:2290/2330 train_time:90072ms step_avg:39.33ms
step:2291/2330 train_time:90094ms step_avg:39.33ms
step:2292/2330 train_time:90150ms step_avg:39.33ms
step:2293/2330 train_time:90172ms step_avg:39.33ms
step:2294/2330 train_time:90229ms step_avg:39.33ms
step:2295/2330 train_time:90251ms step_avg:39.32ms
step:2296/2330 train_time:90306ms step_avg:39.33ms
step:2297/2330 train_time:90328ms step_avg:39.32ms
step:2298/2330 train_time:90383ms step_avg:39.33ms
step:2299/2330 train_time:90405ms step_avg:39.32ms
step:2300/2330 train_time:90461ms step_avg:39.33ms
step:2301/2330 train_time:90483ms step_avg:39.32ms
step:2302/2330 train_time:90538ms step_avg:39.33ms
step:2303/2330 train_time:90560ms step_avg:39.32ms
step:2304/2330 train_time:90616ms step_avg:39.33ms
step:2305/2330 train_time:90638ms step_avg:39.32ms
step:2306/2330 train_time:90694ms step_avg:39.33ms
step:2307/2330 train_time:90715ms step_avg:39.32ms
step:2308/2330 train_time:90772ms step_avg:39.33ms
step:2309/2330 train_time:90793ms step_avg:39.32ms
step:2310/2330 train_time:90849ms step_avg:39.33ms
step:2311/2330 train_time:90871ms step_avg:39.32ms
step:2312/2330 train_time:90928ms step_avg:39.33ms
step:2313/2330 train_time:90950ms step_avg:39.32ms
step:2314/2330 train_time:91007ms step_avg:39.33ms
step:2315/2330 train_time:91030ms step_avg:39.32ms
step:2316/2330 train_time:91087ms step_avg:39.33ms
step:2317/2330 train_time:91109ms step_avg:39.32ms
step:2318/2330 train_time:91164ms step_avg:39.33ms
step:2319/2330 train_time:91186ms step_avg:39.32ms
step:2320/2330 train_time:91241ms step_avg:39.33ms
step:2321/2330 train_time:91264ms step_avg:39.32ms
step:2322/2330 train_time:91319ms step_avg:39.33ms
step:2323/2330 train_time:91341ms step_avg:39.32ms
step:2324/2330 train_time:91397ms step_avg:39.33ms
step:2325/2330 train_time:91419ms step_avg:39.32ms
step:2326/2330 train_time:91475ms step_avg:39.33ms
step:2327/2330 train_time:91496ms step_avg:39.32ms
step:2328/2330 train_time:91552ms step_avg:39.33ms
step:2329/2330 train_time:91574ms step_avg:39.32ms
step:2330/2330 train_time:91630ms step_avg:39.33ms
step:2330/2330 val_loss:5.1296 train_time:91723ms step_avg:39.37ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
