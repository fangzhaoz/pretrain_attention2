import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-1"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:04:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:78ms step_avg:78.12ms
step:2/2330 train_time:172ms step_avg:86.04ms
step:3/2330 train_time:188ms step_avg:62.54ms
step:4/2330 train_time:202ms step_avg:50.41ms
step:5/2330 train_time:214ms step_avg:42.76ms
step:6/2330 train_time:243ms step_avg:40.44ms
step:7/2330 train_time:276ms step_avg:39.43ms
step:8/2330 train_time:320ms step_avg:39.99ms
step:9/2330 train_time:354ms step_avg:39.36ms
step:10/2330 train_time:398ms step_avg:39.83ms
step:11/2330 train_time:433ms step_avg:39.36ms
step:12/2330 train_time:477ms step_avg:39.78ms
step:13/2330 train_time:512ms step_avg:39.37ms
step:14/2330 train_time:556ms step_avg:39.68ms
step:15/2330 train_time:591ms step_avg:39.37ms
step:16/2330 train_time:635ms step_avg:39.67ms
step:17/2330 train_time:670ms step_avg:39.39ms
step:18/2330 train_time:713ms step_avg:39.61ms
step:19/2330 train_time:748ms step_avg:39.36ms
step:20/2330 train_time:791ms step_avg:39.56ms
step:21/2330 train_time:826ms step_avg:39.32ms
step:22/2330 train_time:869ms step_avg:39.51ms
step:23/2330 train_time:903ms step_avg:39.26ms
step:24/2330 train_time:946ms step_avg:39.44ms
step:25/2330 train_time:981ms step_avg:39.24ms
step:26/2330 train_time:1026ms step_avg:39.45ms
step:27/2330 train_time:1064ms step_avg:39.41ms
step:28/2330 train_time:1114ms step_avg:39.78ms
step:29/2330 train_time:1152ms step_avg:39.73ms
step:30/2330 train_time:1199ms step_avg:39.97ms
step:31/2330 train_time:1235ms step_avg:39.82ms
step:32/2330 train_time:1279ms step_avg:39.96ms
step:33/2330 train_time:1314ms step_avg:39.82ms
step:34/2330 train_time:1358ms step_avg:39.94ms
step:35/2330 train_time:1393ms step_avg:39.79ms
step:36/2330 train_time:1437ms step_avg:39.91ms
step:37/2330 train_time:1472ms step_avg:39.78ms
step:38/2330 train_time:1516ms step_avg:39.90ms
step:39/2330 train_time:1550ms step_avg:39.75ms
step:40/2330 train_time:1594ms step_avg:39.86ms
step:41/2330 train_time:1629ms step_avg:39.72ms
step:42/2330 train_time:1673ms step_avg:39.84ms
step:43/2330 train_time:1707ms step_avg:39.71ms
step:44/2330 train_time:1751ms step_avg:39.80ms
step:45/2330 train_time:1786ms step_avg:39.70ms
step:46/2330 train_time:1830ms step_avg:39.79ms
step:47/2330 train_time:1865ms step_avg:39.67ms
step:48/2330 train_time:1908ms step_avg:39.75ms
step:49/2330 train_time:1942ms step_avg:39.64ms
step:50/2330 train_time:1987ms step_avg:39.74ms
step:51/2330 train_time:2022ms step_avg:39.65ms
step:52/2330 train_time:2067ms step_avg:39.75ms
step:53/2330 train_time:2104ms step_avg:39.71ms
step:54/2330 train_time:2151ms step_avg:39.83ms
step:55/2330 train_time:2188ms step_avg:39.78ms
step:56/2330 train_time:2233ms step_avg:39.88ms
step:57/2330 train_time:2269ms step_avg:39.81ms
step:58/2330 train_time:2314ms step_avg:39.89ms
step:59/2330 train_time:2349ms step_avg:39.81ms
step:60/2330 train_time:2393ms step_avg:39.89ms
step:61/2330 train_time:2428ms step_avg:39.81ms
step:62/2330 train_time:2473ms step_avg:39.88ms
step:63/2330 train_time:2507ms step_avg:39.80ms
step:64/2330 train_time:2551ms step_avg:39.86ms
step:65/2330 train_time:2585ms step_avg:39.77ms
step:66/2330 train_time:2629ms step_avg:39.83ms
step:67/2330 train_time:2662ms step_avg:39.74ms
step:68/2330 train_time:2706ms step_avg:39.79ms
step:69/2330 train_time:2740ms step_avg:39.71ms
step:70/2330 train_time:2784ms step_avg:39.77ms
step:71/2330 train_time:2819ms step_avg:39.70ms
step:72/2330 train_time:2862ms step_avg:39.75ms
step:73/2330 train_time:2897ms step_avg:39.69ms
step:74/2330 train_time:2942ms step_avg:39.75ms
step:75/2330 train_time:2977ms step_avg:39.69ms
step:76/2330 train_time:3022ms step_avg:39.77ms
step:77/2330 train_time:3058ms step_avg:39.72ms
step:78/2330 train_time:3104ms step_avg:39.79ms
step:79/2330 train_time:3139ms step_avg:39.74ms
step:80/2330 train_time:3184ms step_avg:39.80ms
step:81/2330 train_time:3220ms step_avg:39.76ms
step:82/2330 train_time:3265ms step_avg:39.82ms
step:83/2330 train_time:3301ms step_avg:39.77ms
step:84/2330 train_time:3345ms step_avg:39.82ms
step:85/2330 train_time:3380ms step_avg:39.76ms
step:86/2330 train_time:3424ms step_avg:39.82ms
step:87/2330 train_time:3459ms step_avg:39.76ms
step:88/2330 train_time:3503ms step_avg:39.81ms
step:89/2330 train_time:3538ms step_avg:39.75ms
step:90/2330 train_time:3582ms step_avg:39.80ms
step:91/2330 train_time:3617ms step_avg:39.75ms
step:92/2330 train_time:3660ms step_avg:39.79ms
step:93/2330 train_time:3694ms step_avg:39.72ms
step:94/2330 train_time:3738ms step_avg:39.77ms
step:95/2330 train_time:3774ms step_avg:39.72ms
step:96/2330 train_time:3818ms step_avg:39.77ms
step:97/2330 train_time:3852ms step_avg:39.71ms
step:98/2330 train_time:3897ms step_avg:39.77ms
step:99/2330 train_time:3933ms step_avg:39.73ms
step:100/2330 train_time:3979ms step_avg:39.79ms
step:101/2330 train_time:4015ms step_avg:39.75ms
step:102/2330 train_time:4060ms step_avg:39.80ms
step:103/2330 train_time:4096ms step_avg:39.76ms
step:104/2330 train_time:4140ms step_avg:39.81ms
step:105/2330 train_time:4176ms step_avg:39.77ms
step:106/2330 train_time:4221ms step_avg:39.82ms
step:107/2330 train_time:4256ms step_avg:39.77ms
step:108/2330 train_time:4300ms step_avg:39.82ms
step:109/2330 train_time:4336ms step_avg:39.78ms
step:110/2330 train_time:4382ms step_avg:39.84ms
step:111/2330 train_time:4417ms step_avg:39.80ms
step:112/2330 train_time:4462ms step_avg:39.84ms
step:113/2330 train_time:4496ms step_avg:39.79ms
step:114/2330 train_time:4541ms step_avg:39.83ms
step:115/2330 train_time:4575ms step_avg:39.79ms
step:116/2330 train_time:4620ms step_avg:39.82ms
step:117/2330 train_time:4654ms step_avg:39.78ms
step:118/2330 train_time:4698ms step_avg:39.82ms
step:119/2330 train_time:4733ms step_avg:39.78ms
step:120/2330 train_time:4778ms step_avg:39.82ms
step:121/2330 train_time:4813ms step_avg:39.78ms
step:122/2330 train_time:4858ms step_avg:39.82ms
step:123/2330 train_time:4894ms step_avg:39.79ms
step:124/2330 train_time:4939ms step_avg:39.83ms
step:125/2330 train_time:4975ms step_avg:39.80ms
step:126/2330 train_time:5019ms step_avg:39.83ms
step:127/2330 train_time:5055ms step_avg:39.80ms
step:128/2330 train_time:5100ms step_avg:39.84ms
step:129/2330 train_time:5135ms step_avg:39.81ms
step:130/2330 train_time:5180ms step_avg:39.85ms
step:131/2330 train_time:5215ms step_avg:39.81ms
step:132/2330 train_time:5261ms step_avg:39.85ms
step:133/2330 train_time:5296ms step_avg:39.82ms
step:134/2330 train_time:5340ms step_avg:39.85ms
step:135/2330 train_time:5375ms step_avg:39.82ms
step:136/2330 train_time:5421ms step_avg:39.86ms
step:137/2330 train_time:5455ms step_avg:39.82ms
step:138/2330 train_time:5500ms step_avg:39.86ms
step:139/2330 train_time:5535ms step_avg:39.82ms
step:140/2330 train_time:5580ms step_avg:39.86ms
step:141/2330 train_time:5614ms step_avg:39.82ms
step:142/2330 train_time:5658ms step_avg:39.85ms
step:143/2330 train_time:5693ms step_avg:39.81ms
step:144/2330 train_time:5738ms step_avg:39.84ms
step:145/2330 train_time:5772ms step_avg:39.81ms
step:146/2330 train_time:5816ms step_avg:39.84ms
step:147/2330 train_time:5852ms step_avg:39.81ms
step:148/2330 train_time:5897ms step_avg:39.84ms
step:149/2330 train_time:5933ms step_avg:39.82ms
step:150/2330 train_time:5978ms step_avg:39.85ms
step:151/2330 train_time:6013ms step_avg:39.82ms
step:152/2330 train_time:6058ms step_avg:39.86ms
step:153/2330 train_time:6093ms step_avg:39.82ms
step:154/2330 train_time:6139ms step_avg:39.86ms
step:155/2330 train_time:6174ms step_avg:39.83ms
step:156/2330 train_time:6220ms step_avg:39.87ms
step:157/2330 train_time:6255ms step_avg:39.84ms
step:158/2330 train_time:6300ms step_avg:39.87ms
step:159/2330 train_time:6335ms step_avg:39.84ms
step:160/2330 train_time:6379ms step_avg:39.87ms
step:161/2330 train_time:6415ms step_avg:39.84ms
step:162/2330 train_time:6459ms step_avg:39.87ms
step:163/2330 train_time:6495ms step_avg:39.84ms
step:164/2330 train_time:6539ms step_avg:39.87ms
step:165/2330 train_time:6573ms step_avg:39.84ms
step:166/2330 train_time:6619ms step_avg:39.87ms
step:167/2330 train_time:6654ms step_avg:39.84ms
step:168/2330 train_time:6699ms step_avg:39.87ms
step:169/2330 train_time:6734ms step_avg:39.85ms
step:170/2330 train_time:6779ms step_avg:39.87ms
step:171/2330 train_time:6814ms step_avg:39.85ms
step:172/2330 train_time:6858ms step_avg:39.87ms
step:173/2330 train_time:6893ms step_avg:39.84ms
step:174/2330 train_time:6937ms step_avg:39.87ms
step:175/2330 train_time:6974ms step_avg:39.85ms
step:176/2330 train_time:7018ms step_avg:39.88ms
step:177/2330 train_time:7054ms step_avg:39.85ms
step:178/2330 train_time:7098ms step_avg:39.88ms
step:179/2330 train_time:7134ms step_avg:39.86ms
step:180/2330 train_time:7179ms step_avg:39.89ms
step:181/2330 train_time:7215ms step_avg:39.86ms
step:182/2330 train_time:7260ms step_avg:39.89ms
step:183/2330 train_time:7295ms step_avg:39.86ms
step:184/2330 train_time:7340ms step_avg:39.89ms
step:185/2330 train_time:7375ms step_avg:39.86ms
step:186/2330 train_time:7420ms step_avg:39.89ms
step:187/2330 train_time:7455ms step_avg:39.87ms
step:188/2330 train_time:7500ms step_avg:39.89ms
step:189/2330 train_time:7535ms step_avg:39.87ms
step:190/2330 train_time:7580ms step_avg:39.89ms
step:191/2330 train_time:7614ms step_avg:39.86ms
step:192/2330 train_time:7659ms step_avg:39.89ms
step:193/2330 train_time:7694ms step_avg:39.86ms
step:194/2330 train_time:7738ms step_avg:39.89ms
step:195/2330 train_time:7773ms step_avg:39.86ms
step:196/2330 train_time:7818ms step_avg:39.89ms
step:197/2330 train_time:7853ms step_avg:39.86ms
step:198/2330 train_time:7898ms step_avg:39.89ms
step:199/2330 train_time:7933ms step_avg:39.87ms
step:200/2330 train_time:7979ms step_avg:39.89ms
step:201/2330 train_time:8015ms step_avg:39.87ms
step:202/2330 train_time:8060ms step_avg:39.90ms
step:203/2330 train_time:8095ms step_avg:39.88ms
step:204/2330 train_time:8140ms step_avg:39.90ms
step:205/2330 train_time:8175ms step_avg:39.88ms
step:206/2330 train_time:8220ms step_avg:39.90ms
step:207/2330 train_time:8256ms step_avg:39.88ms
step:208/2330 train_time:8301ms step_avg:39.91ms
step:209/2330 train_time:8337ms step_avg:39.89ms
step:210/2330 train_time:8381ms step_avg:39.91ms
step:211/2330 train_time:8416ms step_avg:39.89ms
step:212/2330 train_time:8460ms step_avg:39.91ms
step:213/2330 train_time:8496ms step_avg:39.89ms
step:214/2330 train_time:8541ms step_avg:39.91ms
step:215/2330 train_time:8576ms step_avg:39.89ms
step:216/2330 train_time:8620ms step_avg:39.91ms
step:217/2330 train_time:8655ms step_avg:39.89ms
step:218/2330 train_time:8699ms step_avg:39.90ms
step:219/2330 train_time:8735ms step_avg:39.89ms
step:220/2330 train_time:8779ms step_avg:39.91ms
step:221/2330 train_time:8814ms step_avg:39.88ms
step:222/2330 train_time:8859ms step_avg:39.91ms
step:223/2330 train_time:8895ms step_avg:39.89ms
step:224/2330 train_time:8940ms step_avg:39.91ms
step:225/2330 train_time:8975ms step_avg:39.89ms
step:226/2330 train_time:9019ms step_avg:39.91ms
step:227/2330 train_time:9055ms step_avg:39.89ms
step:228/2330 train_time:9099ms step_avg:39.91ms
step:229/2330 train_time:9135ms step_avg:39.89ms
step:230/2330 train_time:9180ms step_avg:39.91ms
step:231/2330 train_time:9216ms step_avg:39.90ms
step:232/2330 train_time:9261ms step_avg:39.92ms
step:233/2330 train_time:9296ms step_avg:39.90ms
step:234/2330 train_time:9340ms step_avg:39.91ms
step:235/2330 train_time:9375ms step_avg:39.89ms
step:236/2330 train_time:9420ms step_avg:39.92ms
step:237/2330 train_time:9455ms step_avg:39.90ms
step:238/2330 train_time:9500ms step_avg:39.91ms
step:239/2330 train_time:9535ms step_avg:39.90ms
step:240/2330 train_time:9580ms step_avg:39.92ms
step:241/2330 train_time:9615ms step_avg:39.90ms
step:242/2330 train_time:9659ms step_avg:39.91ms
step:243/2330 train_time:9695ms step_avg:39.90ms
step:244/2330 train_time:9739ms step_avg:39.92ms
step:245/2330 train_time:9774ms step_avg:39.90ms
step:246/2330 train_time:9819ms step_avg:39.91ms
step:247/2330 train_time:9854ms step_avg:39.90ms
step:248/2330 train_time:9899ms step_avg:39.92ms
step:249/2330 train_time:9935ms step_avg:39.90ms
step:250/2330 train_time:9979ms step_avg:39.92ms
step:250/2330 val_loss:5.4326 train_time:10067ms step_avg:40.27ms
step:251/2330 train_time:10081ms step_avg:40.16ms
step:252/2330 train_time:10093ms step_avg:40.05ms
step:253/2330 train_time:10104ms step_avg:39.94ms
step:254/2330 train_time:10140ms step_avg:39.92ms
step:255/2330 train_time:10174ms step_avg:39.90ms
step:256/2330 train_time:10218ms step_avg:39.91ms
step:257/2330 train_time:10253ms step_avg:39.89ms
step:258/2330 train_time:10296ms step_avg:39.91ms
step:259/2330 train_time:10331ms step_avg:39.89ms
step:260/2330 train_time:10375ms step_avg:39.90ms
step:261/2330 train_time:10414ms step_avg:39.90ms
step:262/2330 train_time:10461ms step_avg:39.93ms
step:263/2330 train_time:10498ms step_avg:39.92ms
step:264/2330 train_time:10544ms step_avg:39.94ms
step:265/2330 train_time:10579ms step_avg:39.92ms
step:266/2330 train_time:10624ms step_avg:39.94ms
step:267/2330 train_time:10659ms step_avg:39.92ms
step:268/2330 train_time:10704ms step_avg:39.94ms
step:269/2330 train_time:10739ms step_avg:39.92ms
step:270/2330 train_time:10783ms step_avg:39.94ms
step:271/2330 train_time:10818ms step_avg:39.92ms
step:272/2330 train_time:10863ms step_avg:39.94ms
step:273/2330 train_time:10898ms step_avg:39.92ms
step:274/2330 train_time:10942ms step_avg:39.93ms
step:275/2330 train_time:10978ms step_avg:39.92ms
step:276/2330 train_time:11025ms step_avg:39.94ms
step:277/2330 train_time:11060ms step_avg:39.93ms
step:278/2330 train_time:11105ms step_avg:39.94ms
step:279/2330 train_time:11139ms step_avg:39.93ms
step:280/2330 train_time:11184ms step_avg:39.94ms
step:281/2330 train_time:11218ms step_avg:39.92ms
step:282/2330 train_time:11263ms step_avg:39.94ms
step:283/2330 train_time:11298ms step_avg:39.92ms
step:284/2330 train_time:11343ms step_avg:39.94ms
step:285/2330 train_time:11381ms step_avg:39.93ms
step:286/2330 train_time:11426ms step_avg:39.95ms
step:287/2330 train_time:11463ms step_avg:39.94ms
step:288/2330 train_time:11508ms step_avg:39.96ms
step:289/2330 train_time:11544ms step_avg:39.94ms
step:290/2330 train_time:11588ms step_avg:39.96ms
step:291/2330 train_time:11624ms step_avg:39.94ms
step:292/2330 train_time:11668ms step_avg:39.96ms
step:293/2330 train_time:11704ms step_avg:39.94ms
step:294/2330 train_time:11748ms step_avg:39.96ms
step:295/2330 train_time:11783ms step_avg:39.94ms
step:296/2330 train_time:11827ms step_avg:39.96ms
step:297/2330 train_time:11862ms step_avg:39.94ms
step:298/2330 train_time:11907ms step_avg:39.96ms
step:299/2330 train_time:11943ms step_avg:39.94ms
step:300/2330 train_time:11987ms step_avg:39.96ms
step:301/2330 train_time:12022ms step_avg:39.94ms
step:302/2330 train_time:12067ms step_avg:39.96ms
step:303/2330 train_time:12103ms step_avg:39.94ms
step:304/2330 train_time:12147ms step_avg:39.96ms
step:305/2330 train_time:12182ms step_avg:39.94ms
step:306/2330 train_time:12226ms step_avg:39.96ms
step:307/2330 train_time:12262ms step_avg:39.94ms
step:308/2330 train_time:12307ms step_avg:39.96ms
step:309/2330 train_time:12343ms step_avg:39.95ms
step:310/2330 train_time:12389ms step_avg:39.96ms
step:311/2330 train_time:12425ms step_avg:39.95ms
step:312/2330 train_time:12470ms step_avg:39.97ms
step:313/2330 train_time:12505ms step_avg:39.95ms
step:314/2330 train_time:12550ms step_avg:39.97ms
step:315/2330 train_time:12585ms step_avg:39.95ms
step:316/2330 train_time:12630ms step_avg:39.97ms
step:317/2330 train_time:12665ms step_avg:39.95ms
step:318/2330 train_time:12710ms step_avg:39.97ms
step:319/2330 train_time:12745ms step_avg:39.95ms
step:320/2330 train_time:12790ms step_avg:39.97ms
step:321/2330 train_time:12826ms step_avg:39.96ms
step:322/2330 train_time:12869ms step_avg:39.97ms
step:323/2330 train_time:12905ms step_avg:39.95ms
step:324/2330 train_time:12950ms step_avg:39.97ms
step:325/2330 train_time:12985ms step_avg:39.95ms
step:326/2330 train_time:13029ms step_avg:39.97ms
step:327/2330 train_time:13065ms step_avg:39.95ms
step:328/2330 train_time:13110ms step_avg:39.97ms
step:329/2330 train_time:13145ms step_avg:39.95ms
step:330/2330 train_time:13189ms step_avg:39.97ms
step:331/2330 train_time:13225ms step_avg:39.95ms
step:332/2330 train_time:13269ms step_avg:39.97ms
step:333/2330 train_time:13304ms step_avg:39.95ms
step:334/2330 train_time:13349ms step_avg:39.97ms
step:335/2330 train_time:13385ms step_avg:39.95ms
step:336/2330 train_time:13429ms step_avg:39.97ms
step:337/2330 train_time:13464ms step_avg:39.95ms
step:338/2330 train_time:13509ms step_avg:39.97ms
step:339/2330 train_time:13544ms step_avg:39.95ms
step:340/2330 train_time:13589ms step_avg:39.97ms
step:341/2330 train_time:13625ms step_avg:39.96ms
step:342/2330 train_time:13669ms step_avg:39.97ms
step:343/2330 train_time:13704ms step_avg:39.95ms
step:344/2330 train_time:13749ms step_avg:39.97ms
step:345/2330 train_time:13784ms step_avg:39.95ms
step:346/2330 train_time:13829ms step_avg:39.97ms
step:347/2330 train_time:13863ms step_avg:39.95ms
step:348/2330 train_time:13909ms step_avg:39.97ms
step:349/2330 train_time:13944ms step_avg:39.95ms
step:350/2330 train_time:13988ms step_avg:39.97ms
step:351/2330 train_time:14024ms step_avg:39.95ms
step:352/2330 train_time:14068ms step_avg:39.97ms
step:353/2330 train_time:14104ms step_avg:39.95ms
step:354/2330 train_time:14148ms step_avg:39.97ms
step:355/2330 train_time:14183ms step_avg:39.95ms
step:356/2330 train_time:14228ms step_avg:39.97ms
step:357/2330 train_time:14264ms step_avg:39.96ms
step:358/2330 train_time:14309ms step_avg:39.97ms
step:359/2330 train_time:14344ms step_avg:39.96ms
step:360/2330 train_time:14388ms step_avg:39.97ms
step:361/2330 train_time:14424ms step_avg:39.95ms
step:362/2330 train_time:14469ms step_avg:39.97ms
step:363/2330 train_time:14504ms step_avg:39.96ms
step:364/2330 train_time:14549ms step_avg:39.97ms
step:365/2330 train_time:14585ms step_avg:39.96ms
step:366/2330 train_time:14629ms step_avg:39.97ms
step:367/2330 train_time:14664ms step_avg:39.96ms
step:368/2330 train_time:14709ms step_avg:39.97ms
step:369/2330 train_time:14744ms step_avg:39.96ms
step:370/2330 train_time:14788ms step_avg:39.97ms
step:371/2330 train_time:14823ms step_avg:39.96ms
step:372/2330 train_time:14868ms step_avg:39.97ms
step:373/2330 train_time:14902ms step_avg:39.95ms
step:374/2330 train_time:14947ms step_avg:39.97ms
step:375/2330 train_time:14983ms step_avg:39.96ms
step:376/2330 train_time:15028ms step_avg:39.97ms
step:377/2330 train_time:15063ms step_avg:39.95ms
step:378/2330 train_time:15108ms step_avg:39.97ms
step:379/2330 train_time:15143ms step_avg:39.96ms
step:380/2330 train_time:15188ms step_avg:39.97ms
step:381/2330 train_time:15223ms step_avg:39.96ms
step:382/2330 train_time:15268ms step_avg:39.97ms
step:383/2330 train_time:15303ms step_avg:39.96ms
step:384/2330 train_time:15348ms step_avg:39.97ms
step:385/2330 train_time:15383ms step_avg:39.96ms
step:386/2330 train_time:15428ms step_avg:39.97ms
step:387/2330 train_time:15463ms step_avg:39.96ms
step:388/2330 train_time:15508ms step_avg:39.97ms
step:389/2330 train_time:15544ms step_avg:39.96ms
step:390/2330 train_time:15589ms step_avg:39.97ms
step:391/2330 train_time:15624ms step_avg:39.96ms
step:392/2330 train_time:15669ms step_avg:39.97ms
step:393/2330 train_time:15705ms step_avg:39.96ms
step:394/2330 train_time:15749ms step_avg:39.97ms
step:395/2330 train_time:15784ms step_avg:39.96ms
step:396/2330 train_time:15829ms step_avg:39.97ms
step:397/2330 train_time:15864ms step_avg:39.96ms
step:398/2330 train_time:15909ms step_avg:39.97ms
step:399/2330 train_time:15944ms step_avg:39.96ms
step:400/2330 train_time:15988ms step_avg:39.97ms
step:401/2330 train_time:16024ms step_avg:39.96ms
step:402/2330 train_time:16068ms step_avg:39.97ms
step:403/2330 train_time:16104ms step_avg:39.96ms
step:404/2330 train_time:16149ms step_avg:39.97ms
step:405/2330 train_time:16184ms step_avg:39.96ms
step:406/2330 train_time:16229ms step_avg:39.97ms
step:407/2330 train_time:16264ms step_avg:39.96ms
step:408/2330 train_time:16309ms step_avg:39.97ms
step:409/2330 train_time:16343ms step_avg:39.96ms
step:410/2330 train_time:16388ms step_avg:39.97ms
step:411/2330 train_time:16424ms step_avg:39.96ms
step:412/2330 train_time:16469ms step_avg:39.97ms
step:413/2330 train_time:16505ms step_avg:39.96ms
step:414/2330 train_time:16550ms step_avg:39.97ms
step:415/2330 train_time:16585ms step_avg:39.96ms
step:416/2330 train_time:16630ms step_avg:39.98ms
step:417/2330 train_time:16665ms step_avg:39.96ms
step:418/2330 train_time:16710ms step_avg:39.98ms
step:419/2330 train_time:16745ms step_avg:39.96ms
step:420/2330 train_time:16790ms step_avg:39.98ms
step:421/2330 train_time:16825ms step_avg:39.96ms
step:422/2330 train_time:16869ms step_avg:39.97ms
step:423/2330 train_time:16904ms step_avg:39.96ms
step:424/2330 train_time:16949ms step_avg:39.97ms
step:425/2330 train_time:16985ms step_avg:39.96ms
step:426/2330 train_time:17029ms step_avg:39.97ms
step:427/2330 train_time:17064ms step_avg:39.96ms
step:428/2330 train_time:17109ms step_avg:39.97ms
step:429/2330 train_time:17144ms step_avg:39.96ms
step:430/2330 train_time:17188ms step_avg:39.97ms
step:431/2330 train_time:17224ms step_avg:39.96ms
step:432/2330 train_time:17268ms step_avg:39.97ms
step:433/2330 train_time:17304ms step_avg:39.96ms
step:434/2330 train_time:17348ms step_avg:39.97ms
step:435/2330 train_time:17383ms step_avg:39.96ms
step:436/2330 train_time:17427ms step_avg:39.97ms
step:437/2330 train_time:17462ms step_avg:39.96ms
step:438/2330 train_time:17508ms step_avg:39.97ms
step:439/2330 train_time:17544ms step_avg:39.96ms
step:440/2330 train_time:17588ms step_avg:39.97ms
step:441/2330 train_time:17624ms step_avg:39.96ms
step:442/2330 train_time:17668ms step_avg:39.97ms
step:443/2330 train_time:17704ms step_avg:39.96ms
step:444/2330 train_time:17749ms step_avg:39.98ms
step:445/2330 train_time:17784ms step_avg:39.96ms
step:446/2330 train_time:17828ms step_avg:39.97ms
step:447/2330 train_time:17864ms step_avg:39.96ms
step:448/2330 train_time:17909ms step_avg:39.97ms
step:449/2330 train_time:17944ms step_avg:39.96ms
step:450/2330 train_time:17989ms step_avg:39.97ms
step:451/2330 train_time:18024ms step_avg:39.96ms
step:452/2330 train_time:18068ms step_avg:39.97ms
step:453/2330 train_time:18103ms step_avg:39.96ms
step:454/2330 train_time:18148ms step_avg:39.97ms
step:455/2330 train_time:18183ms step_avg:39.96ms
step:456/2330 train_time:18228ms step_avg:39.97ms
step:457/2330 train_time:18263ms step_avg:39.96ms
step:458/2330 train_time:18308ms step_avg:39.97ms
step:459/2330 train_time:18344ms step_avg:39.96ms
step:460/2330 train_time:18388ms step_avg:39.97ms
step:461/2330 train_time:18424ms step_avg:39.96ms
step:462/2330 train_time:18468ms step_avg:39.97ms
step:463/2330 train_time:18504ms step_avg:39.97ms
step:464/2330 train_time:18549ms step_avg:39.98ms
step:465/2330 train_time:18584ms step_avg:39.97ms
step:466/2330 train_time:18629ms step_avg:39.98ms
step:467/2330 train_time:18665ms step_avg:39.97ms
step:468/2330 train_time:18709ms step_avg:39.98ms
step:469/2330 train_time:18745ms step_avg:39.97ms
step:470/2330 train_time:18789ms step_avg:39.98ms
step:471/2330 train_time:18825ms step_avg:39.97ms
step:472/2330 train_time:18870ms step_avg:39.98ms
step:473/2330 train_time:18906ms step_avg:39.97ms
step:474/2330 train_time:18950ms step_avg:39.98ms
step:475/2330 train_time:18986ms step_avg:39.97ms
step:476/2330 train_time:19030ms step_avg:39.98ms
step:477/2330 train_time:19065ms step_avg:39.97ms
step:478/2330 train_time:19109ms step_avg:39.98ms
step:479/2330 train_time:19145ms step_avg:39.97ms
step:480/2330 train_time:19189ms step_avg:39.98ms
step:481/2330 train_time:19224ms step_avg:39.97ms
step:482/2330 train_time:19269ms step_avg:39.98ms
step:483/2330 train_time:19304ms step_avg:39.97ms
step:484/2330 train_time:19349ms step_avg:39.98ms
step:485/2330 train_time:19384ms step_avg:39.97ms
step:486/2330 train_time:19429ms step_avg:39.98ms
step:487/2330 train_time:19464ms step_avg:39.97ms
step:488/2330 train_time:19509ms step_avg:39.98ms
step:489/2330 train_time:19545ms step_avg:39.97ms
step:490/2330 train_time:19589ms step_avg:39.98ms
step:491/2330 train_time:19625ms step_avg:39.97ms
step:492/2330 train_time:19669ms step_avg:39.98ms
step:493/2330 train_time:19705ms step_avg:39.97ms
step:494/2330 train_time:19749ms step_avg:39.98ms
step:495/2330 train_time:19784ms step_avg:39.97ms
step:496/2330 train_time:19829ms step_avg:39.98ms
step:497/2330 train_time:19865ms step_avg:39.97ms
step:498/2330 train_time:19910ms step_avg:39.98ms
step:499/2330 train_time:19945ms step_avg:39.97ms
step:500/2330 train_time:19990ms step_avg:39.98ms
step:500/2330 val_loss:5.3106 train_time:20077ms step_avg:40.15ms
step:501/2330 train_time:20091ms step_avg:40.10ms
step:502/2330 train_time:20104ms step_avg:40.05ms
step:503/2330 train_time:20116ms step_avg:39.99ms
step:504/2330 train_time:20151ms step_avg:39.98ms
step:505/2330 train_time:20185ms step_avg:39.97ms
step:506/2330 train_time:20228ms step_avg:39.98ms
step:507/2330 train_time:20263ms step_avg:39.97ms
step:508/2330 train_time:20307ms step_avg:39.97ms
step:509/2330 train_time:20341ms step_avg:39.96ms
step:510/2330 train_time:20386ms step_avg:39.97ms
step:511/2330 train_time:20426ms step_avg:39.97ms
step:512/2330 train_time:20475ms step_avg:39.99ms
step:513/2330 train_time:20511ms step_avg:39.98ms
step:514/2330 train_time:20557ms step_avg:39.99ms
step:515/2330 train_time:20593ms step_avg:39.99ms
step:516/2330 train_time:20638ms step_avg:40.00ms
step:517/2330 train_time:20673ms step_avg:39.99ms
step:518/2330 train_time:20717ms step_avg:39.99ms
step:519/2330 train_time:20752ms step_avg:39.98ms
step:520/2330 train_time:20798ms step_avg:40.00ms
step:521/2330 train_time:20832ms step_avg:39.98ms
step:522/2330 train_time:20876ms step_avg:39.99ms
step:523/2330 train_time:20911ms step_avg:39.98ms
step:524/2330 train_time:20954ms step_avg:39.99ms
step:525/2330 train_time:20989ms step_avg:39.98ms
step:526/2330 train_time:21034ms step_avg:39.99ms
step:527/2330 train_time:21069ms step_avg:39.98ms
step:528/2330 train_time:21114ms step_avg:39.99ms
step:529/2330 train_time:21149ms step_avg:39.98ms
step:530/2330 train_time:21193ms step_avg:39.99ms
step:531/2330 train_time:21228ms step_avg:39.98ms
step:532/2330 train_time:21272ms step_avg:39.99ms
step:533/2330 train_time:21307ms step_avg:39.98ms
step:534/2330 train_time:21352ms step_avg:39.99ms
step:535/2330 train_time:21389ms step_avg:39.98ms
step:536/2330 train_time:21435ms step_avg:39.99ms
step:537/2330 train_time:21471ms step_avg:39.98ms
step:538/2330 train_time:21517ms step_avg:39.99ms
step:539/2330 train_time:21553ms step_avg:39.99ms
step:540/2330 train_time:21598ms step_avg:40.00ms
step:541/2330 train_time:21633ms step_avg:39.99ms
step:542/2330 train_time:21677ms step_avg:40.00ms
step:543/2330 train_time:21712ms step_avg:39.99ms
step:544/2330 train_time:21758ms step_avg:40.00ms
step:545/2330 train_time:21792ms step_avg:39.98ms
step:546/2330 train_time:21836ms step_avg:39.99ms
step:547/2330 train_time:21871ms step_avg:39.98ms
step:548/2330 train_time:21914ms step_avg:39.99ms
step:549/2330 train_time:21949ms step_avg:39.98ms
step:550/2330 train_time:21994ms step_avg:39.99ms
step:551/2330 train_time:22029ms step_avg:39.98ms
step:552/2330 train_time:22073ms step_avg:39.99ms
step:553/2330 train_time:22108ms step_avg:39.98ms
step:554/2330 train_time:22153ms step_avg:39.99ms
step:555/2330 train_time:22188ms step_avg:39.98ms
step:556/2330 train_time:22232ms step_avg:39.99ms
step:557/2330 train_time:22267ms step_avg:39.98ms
step:558/2330 train_time:22312ms step_avg:39.99ms
step:559/2330 train_time:22347ms step_avg:39.98ms
step:560/2330 train_time:22392ms step_avg:39.99ms
step:561/2330 train_time:22428ms step_avg:39.98ms
step:562/2330 train_time:22474ms step_avg:39.99ms
step:563/2330 train_time:22510ms step_avg:39.98ms
step:564/2330 train_time:22555ms step_avg:39.99ms
step:565/2330 train_time:22591ms step_avg:39.98ms
step:566/2330 train_time:22635ms step_avg:39.99ms
step:567/2330 train_time:22671ms step_avg:39.98ms
step:568/2330 train_time:22716ms step_avg:39.99ms
step:569/2330 train_time:22751ms step_avg:39.98ms
step:570/2330 train_time:22797ms step_avg:39.99ms
step:571/2330 train_time:22832ms step_avg:39.99ms
step:572/2330 train_time:22876ms step_avg:39.99ms
step:573/2330 train_time:22911ms step_avg:39.98ms
step:574/2330 train_time:22955ms step_avg:39.99ms
step:575/2330 train_time:22990ms step_avg:39.98ms
step:576/2330 train_time:23034ms step_avg:39.99ms
step:577/2330 train_time:23069ms step_avg:39.98ms
step:578/2330 train_time:23113ms step_avg:39.99ms
step:579/2330 train_time:23147ms step_avg:39.98ms
step:580/2330 train_time:23192ms step_avg:39.99ms
step:581/2330 train_time:23228ms step_avg:39.98ms
step:582/2330 train_time:23272ms step_avg:39.99ms
step:583/2330 train_time:23307ms step_avg:39.98ms
step:584/2330 train_time:23352ms step_avg:39.99ms
step:585/2330 train_time:23387ms step_avg:39.98ms
step:586/2330 train_time:23432ms step_avg:39.99ms
step:587/2330 train_time:23468ms step_avg:39.98ms
step:588/2330 train_time:23512ms step_avg:39.99ms
step:589/2330 train_time:23548ms step_avg:39.98ms
step:590/2330 train_time:23593ms step_avg:39.99ms
step:591/2330 train_time:23628ms step_avg:39.98ms
step:592/2330 train_time:23673ms step_avg:39.99ms
step:593/2330 train_time:23708ms step_avg:39.98ms
step:594/2330 train_time:23753ms step_avg:39.99ms
step:595/2330 train_time:23789ms step_avg:39.98ms
step:596/2330 train_time:23833ms step_avg:39.99ms
step:597/2330 train_time:23868ms step_avg:39.98ms
step:598/2330 train_time:23913ms step_avg:39.99ms
step:599/2330 train_time:23948ms step_avg:39.98ms
step:600/2330 train_time:23992ms step_avg:39.99ms
step:601/2330 train_time:24027ms step_avg:39.98ms
step:602/2330 train_time:24071ms step_avg:39.99ms
step:603/2330 train_time:24106ms step_avg:39.98ms
step:604/2330 train_time:24150ms step_avg:39.98ms
step:605/2330 train_time:24186ms step_avg:39.98ms
step:606/2330 train_time:24230ms step_avg:39.98ms
step:607/2330 train_time:24265ms step_avg:39.98ms
step:608/2330 train_time:24310ms step_avg:39.98ms
step:609/2330 train_time:24345ms step_avg:39.98ms
step:610/2330 train_time:24390ms step_avg:39.98ms
step:611/2330 train_time:24426ms step_avg:39.98ms
step:612/2330 train_time:24470ms step_avg:39.98ms
step:613/2330 train_time:24506ms step_avg:39.98ms
step:614/2330 train_time:24551ms step_avg:39.98ms
step:615/2330 train_time:24587ms step_avg:39.98ms
step:616/2330 train_time:24632ms step_avg:39.99ms
step:617/2330 train_time:24668ms step_avg:39.98ms
step:618/2330 train_time:24712ms step_avg:39.99ms
step:619/2330 train_time:24748ms step_avg:39.98ms
step:620/2330 train_time:24792ms step_avg:39.99ms
step:621/2330 train_time:24828ms step_avg:39.98ms
step:622/2330 train_time:24873ms step_avg:39.99ms
step:623/2330 train_time:24908ms step_avg:39.98ms
step:624/2330 train_time:24952ms step_avg:39.99ms
step:625/2330 train_time:24987ms step_avg:39.98ms
step:626/2330 train_time:25032ms step_avg:39.99ms
step:627/2330 train_time:25067ms step_avg:39.98ms
step:628/2330 train_time:25111ms step_avg:39.99ms
step:629/2330 train_time:25146ms step_avg:39.98ms
step:630/2330 train_time:25191ms step_avg:39.98ms
step:631/2330 train_time:25226ms step_avg:39.98ms
step:632/2330 train_time:25271ms step_avg:39.99ms
step:633/2330 train_time:25306ms step_avg:39.98ms
step:634/2330 train_time:25350ms step_avg:39.98ms
step:635/2330 train_time:25386ms step_avg:39.98ms
step:636/2330 train_time:25431ms step_avg:39.99ms
step:637/2330 train_time:25467ms step_avg:39.98ms
step:638/2330 train_time:25511ms step_avg:39.99ms
step:639/2330 train_time:25547ms step_avg:39.98ms
step:640/2330 train_time:25592ms step_avg:39.99ms
step:641/2330 train_time:25627ms step_avg:39.98ms
step:642/2330 train_time:25672ms step_avg:39.99ms
step:643/2330 train_time:25707ms step_avg:39.98ms
step:644/2330 train_time:25752ms step_avg:39.99ms
step:645/2330 train_time:25788ms step_avg:39.98ms
step:646/2330 train_time:25832ms step_avg:39.99ms
step:647/2330 train_time:25868ms step_avg:39.98ms
step:648/2330 train_time:25912ms step_avg:39.99ms
step:649/2330 train_time:25947ms step_avg:39.98ms
step:650/2330 train_time:25992ms step_avg:39.99ms
step:651/2330 train_time:26027ms step_avg:39.98ms
step:652/2330 train_time:26071ms step_avg:39.99ms
step:653/2330 train_time:26106ms step_avg:39.98ms
step:654/2330 train_time:26150ms step_avg:39.99ms
step:655/2330 train_time:26185ms step_avg:39.98ms
step:656/2330 train_time:26229ms step_avg:39.98ms
step:657/2330 train_time:26265ms step_avg:39.98ms
step:658/2330 train_time:26309ms step_avg:39.98ms
step:659/2330 train_time:26345ms step_avg:39.98ms
step:660/2330 train_time:26390ms step_avg:39.99ms
step:661/2330 train_time:26426ms step_avg:39.98ms
step:662/2330 train_time:26471ms step_avg:39.99ms
step:663/2330 train_time:26506ms step_avg:39.98ms
step:664/2330 train_time:26551ms step_avg:39.99ms
step:665/2330 train_time:26587ms step_avg:39.98ms
step:666/2330 train_time:26631ms step_avg:39.99ms
step:667/2330 train_time:26667ms step_avg:39.98ms
step:668/2330 train_time:26711ms step_avg:39.99ms
step:669/2330 train_time:26747ms step_avg:39.98ms
step:670/2330 train_time:26792ms step_avg:39.99ms
step:671/2330 train_time:26828ms step_avg:39.98ms
step:672/2330 train_time:26873ms step_avg:39.99ms
step:673/2330 train_time:26908ms step_avg:39.98ms
step:674/2330 train_time:26952ms step_avg:39.99ms
step:675/2330 train_time:26988ms step_avg:39.98ms
step:676/2330 train_time:27032ms step_avg:39.99ms
step:677/2330 train_time:27067ms step_avg:39.98ms
step:678/2330 train_time:27111ms step_avg:39.99ms
step:679/2330 train_time:27147ms step_avg:39.98ms
step:680/2330 train_time:27191ms step_avg:39.99ms
step:681/2330 train_time:27226ms step_avg:39.98ms
step:682/2330 train_time:27271ms step_avg:39.99ms
step:683/2330 train_time:27305ms step_avg:39.98ms
step:684/2330 train_time:27350ms step_avg:39.99ms
step:685/2330 train_time:27385ms step_avg:39.98ms
step:686/2330 train_time:27430ms step_avg:39.99ms
step:687/2330 train_time:27466ms step_avg:39.98ms
step:688/2330 train_time:27511ms step_avg:39.99ms
step:689/2330 train_time:27546ms step_avg:39.98ms
step:690/2330 train_time:27591ms step_avg:39.99ms
step:691/2330 train_time:27627ms step_avg:39.98ms
step:692/2330 train_time:27672ms step_avg:39.99ms
step:693/2330 train_time:27707ms step_avg:39.98ms
step:694/2330 train_time:27752ms step_avg:39.99ms
step:695/2330 train_time:27789ms step_avg:39.98ms
step:696/2330 train_time:27833ms step_avg:39.99ms
step:697/2330 train_time:27868ms step_avg:39.98ms
step:698/2330 train_time:27913ms step_avg:39.99ms
step:699/2330 train_time:27948ms step_avg:39.98ms
step:700/2330 train_time:27993ms step_avg:39.99ms
step:701/2330 train_time:28028ms step_avg:39.98ms
step:702/2330 train_time:28073ms step_avg:39.99ms
step:703/2330 train_time:28108ms step_avg:39.98ms
step:704/2330 train_time:28152ms step_avg:39.99ms
step:705/2330 train_time:28188ms step_avg:39.98ms
step:706/2330 train_time:28232ms step_avg:39.99ms
step:707/2330 train_time:28268ms step_avg:39.98ms
step:708/2330 train_time:28312ms step_avg:39.99ms
step:709/2330 train_time:28346ms step_avg:39.98ms
step:710/2330 train_time:28391ms step_avg:39.99ms
step:711/2330 train_time:28427ms step_avg:39.98ms
step:712/2330 train_time:28471ms step_avg:39.99ms
step:713/2330 train_time:28506ms step_avg:39.98ms
step:714/2330 train_time:28550ms step_avg:39.99ms
step:715/2330 train_time:28586ms step_avg:39.98ms
step:716/2330 train_time:28631ms step_avg:39.99ms
step:717/2330 train_time:28667ms step_avg:39.98ms
step:718/2330 train_time:28711ms step_avg:39.99ms
step:719/2330 train_time:28747ms step_avg:39.98ms
step:720/2330 train_time:28792ms step_avg:39.99ms
step:721/2330 train_time:28827ms step_avg:39.98ms
step:722/2330 train_time:28871ms step_avg:39.99ms
step:723/2330 train_time:28906ms step_avg:39.98ms
step:724/2330 train_time:28950ms step_avg:39.99ms
step:725/2330 train_time:28986ms step_avg:39.98ms
step:726/2330 train_time:29031ms step_avg:39.99ms
step:727/2330 train_time:29066ms step_avg:39.98ms
step:728/2330 train_time:29111ms step_avg:39.99ms
step:729/2330 train_time:29146ms step_avg:39.98ms
step:730/2330 train_time:29191ms step_avg:39.99ms
step:731/2330 train_time:29226ms step_avg:39.98ms
step:732/2330 train_time:29271ms step_avg:39.99ms
step:733/2330 train_time:29306ms step_avg:39.98ms
step:734/2330 train_time:29350ms step_avg:39.99ms
step:735/2330 train_time:29386ms step_avg:39.98ms
step:736/2330 train_time:29430ms step_avg:39.99ms
step:737/2330 train_time:29465ms step_avg:39.98ms
step:738/2330 train_time:29510ms step_avg:39.99ms
step:739/2330 train_time:29545ms step_avg:39.98ms
step:740/2330 train_time:29590ms step_avg:39.99ms
step:741/2330 train_time:29626ms step_avg:39.98ms
step:742/2330 train_time:29672ms step_avg:39.99ms
step:743/2330 train_time:29707ms step_avg:39.98ms
step:744/2330 train_time:29751ms step_avg:39.99ms
step:745/2330 train_time:29787ms step_avg:39.98ms
step:746/2330 train_time:29831ms step_avg:39.99ms
step:747/2330 train_time:29867ms step_avg:39.98ms
step:748/2330 train_time:29911ms step_avg:39.99ms
step:749/2330 train_time:29947ms step_avg:39.98ms
step:750/2330 train_time:29992ms step_avg:39.99ms
step:750/2330 val_loss:5.2480 train_time:30080ms step_avg:40.11ms
step:751/2330 train_time:30092ms step_avg:40.07ms
step:752/2330 train_time:30104ms step_avg:40.03ms
step:753/2330 train_time:30115ms step_avg:39.99ms
step:754/2330 train_time:30153ms step_avg:39.99ms
step:755/2330 train_time:30188ms step_avg:39.98ms
step:756/2330 train_time:30231ms step_avg:39.99ms
step:757/2330 train_time:30265ms step_avg:39.98ms
step:758/2330 train_time:30308ms step_avg:39.98ms
step:759/2330 train_time:30343ms step_avg:39.98ms
step:760/2330 train_time:30389ms step_avg:39.99ms
step:761/2330 train_time:30429ms step_avg:39.99ms
step:762/2330 train_time:30477ms step_avg:40.00ms
step:763/2330 train_time:30514ms step_avg:39.99ms
step:764/2330 train_time:30560ms step_avg:40.00ms
step:765/2330 train_time:30596ms step_avg:39.99ms
step:766/2330 train_time:30639ms step_avg:40.00ms
step:767/2330 train_time:30674ms step_avg:39.99ms
step:768/2330 train_time:30718ms step_avg:40.00ms
step:769/2330 train_time:30754ms step_avg:39.99ms
step:770/2330 train_time:30798ms step_avg:40.00ms
step:771/2330 train_time:30833ms step_avg:39.99ms
step:772/2330 train_time:30877ms step_avg:40.00ms
step:773/2330 train_time:30912ms step_avg:39.99ms
step:774/2330 train_time:30957ms step_avg:40.00ms
step:775/2330 train_time:30992ms step_avg:39.99ms
step:776/2330 train_time:31038ms step_avg:40.00ms
step:777/2330 train_time:31074ms step_avg:39.99ms
step:778/2330 train_time:31119ms step_avg:40.00ms
step:779/2330 train_time:31154ms step_avg:39.99ms
step:780/2330 train_time:31199ms step_avg:40.00ms
step:781/2330 train_time:31234ms step_avg:39.99ms
step:782/2330 train_time:31278ms step_avg:40.00ms
step:783/2330 train_time:31314ms step_avg:39.99ms
step:784/2330 train_time:31359ms step_avg:40.00ms
step:785/2330 train_time:31396ms step_avg:40.00ms
step:786/2330 train_time:31441ms step_avg:40.00ms
step:787/2330 train_time:31477ms step_avg:40.00ms
step:788/2330 train_time:31522ms step_avg:40.00ms
step:789/2330 train_time:31558ms step_avg:40.00ms
step:790/2330 train_time:31602ms step_avg:40.00ms
step:791/2330 train_time:31637ms step_avg:40.00ms
step:792/2330 train_time:31683ms step_avg:40.00ms
step:793/2330 train_time:31718ms step_avg:40.00ms
step:794/2330 train_time:31763ms step_avg:40.00ms
step:795/2330 train_time:31798ms step_avg:40.00ms
step:796/2330 train_time:31842ms step_avg:40.00ms
step:797/2330 train_time:31878ms step_avg:40.00ms
step:798/2330 train_time:31922ms step_avg:40.00ms
step:799/2330 train_time:31958ms step_avg:40.00ms
step:800/2330 train_time:32003ms step_avg:40.00ms
step:801/2330 train_time:32038ms step_avg:40.00ms
step:802/2330 train_time:32083ms step_avg:40.00ms
step:803/2330 train_time:32118ms step_avg:40.00ms
step:804/2330 train_time:32162ms step_avg:40.00ms
step:805/2330 train_time:32197ms step_avg:40.00ms
step:806/2330 train_time:32242ms step_avg:40.00ms
step:807/2330 train_time:32277ms step_avg:40.00ms
step:808/2330 train_time:32322ms step_avg:40.00ms
step:809/2330 train_time:32359ms step_avg:40.00ms
step:810/2330 train_time:32404ms step_avg:40.00ms
step:811/2330 train_time:32439ms step_avg:40.00ms
step:812/2330 train_time:32483ms step_avg:40.00ms
step:813/2330 train_time:32519ms step_avg:40.00ms
step:814/2330 train_time:32564ms step_avg:40.00ms
step:815/2330 train_time:32599ms step_avg:40.00ms
step:816/2330 train_time:32644ms step_avg:40.01ms
step:817/2330 train_time:32680ms step_avg:40.00ms
step:818/2330 train_time:32724ms step_avg:40.01ms
step:819/2330 train_time:32760ms step_avg:40.00ms
step:820/2330 train_time:32804ms step_avg:40.01ms
step:821/2330 train_time:32839ms step_avg:40.00ms
step:822/2330 train_time:32884ms step_avg:40.01ms
step:823/2330 train_time:32920ms step_avg:40.00ms
step:824/2330 train_time:32965ms step_avg:40.01ms
step:825/2330 train_time:33000ms step_avg:40.00ms
step:826/2330 train_time:33045ms step_avg:40.01ms
step:827/2330 train_time:33081ms step_avg:40.00ms
step:828/2330 train_time:33125ms step_avg:40.01ms
step:829/2330 train_time:33161ms step_avg:40.00ms
step:830/2330 train_time:33205ms step_avg:40.01ms
step:831/2330 train_time:33241ms step_avg:40.00ms
step:832/2330 train_time:33286ms step_avg:40.01ms
step:833/2330 train_time:33321ms step_avg:40.00ms
step:834/2330 train_time:33366ms step_avg:40.01ms
step:835/2330 train_time:33401ms step_avg:40.00ms
step:836/2330 train_time:33446ms step_avg:40.01ms
step:837/2330 train_time:33481ms step_avg:40.00ms
step:838/2330 train_time:33526ms step_avg:40.01ms
step:839/2330 train_time:33562ms step_avg:40.00ms
step:840/2330 train_time:33607ms step_avg:40.01ms
step:841/2330 train_time:33643ms step_avg:40.00ms
step:842/2330 train_time:33687ms step_avg:40.01ms
step:843/2330 train_time:33723ms step_avg:40.00ms
step:844/2330 train_time:33767ms step_avg:40.01ms
step:845/2330 train_time:33802ms step_avg:40.00ms
step:846/2330 train_time:33847ms step_avg:40.01ms
step:847/2330 train_time:33883ms step_avg:40.00ms
step:848/2330 train_time:33927ms step_avg:40.01ms
step:849/2330 train_time:33962ms step_avg:40.00ms
step:850/2330 train_time:34007ms step_avg:40.01ms
step:851/2330 train_time:34042ms step_avg:40.00ms
step:852/2330 train_time:34086ms step_avg:40.01ms
step:853/2330 train_time:34121ms step_avg:40.00ms
step:854/2330 train_time:34166ms step_avg:40.01ms
step:855/2330 train_time:34201ms step_avg:40.00ms
step:856/2330 train_time:34246ms step_avg:40.01ms
step:857/2330 train_time:34282ms step_avg:40.00ms
step:858/2330 train_time:34327ms step_avg:40.01ms
step:859/2330 train_time:34362ms step_avg:40.00ms
step:860/2330 train_time:34407ms step_avg:40.01ms
step:861/2330 train_time:34443ms step_avg:40.00ms
step:862/2330 train_time:34487ms step_avg:40.01ms
step:863/2330 train_time:34523ms step_avg:40.00ms
step:864/2330 train_time:34567ms step_avg:40.01ms
step:865/2330 train_time:34603ms step_avg:40.00ms
step:866/2330 train_time:34647ms step_avg:40.01ms
step:867/2330 train_time:34683ms step_avg:40.00ms
step:868/2330 train_time:34728ms step_avg:40.01ms
step:869/2330 train_time:34763ms step_avg:40.00ms
step:870/2330 train_time:34807ms step_avg:40.01ms
step:871/2330 train_time:34842ms step_avg:40.00ms
step:872/2330 train_time:34887ms step_avg:40.01ms
step:873/2330 train_time:34922ms step_avg:40.00ms
step:874/2330 train_time:34967ms step_avg:40.01ms
step:875/2330 train_time:35002ms step_avg:40.00ms
step:876/2330 train_time:35047ms step_avg:40.01ms
step:877/2330 train_time:35082ms step_avg:40.00ms
step:878/2330 train_time:35127ms step_avg:40.01ms
step:879/2330 train_time:35161ms step_avg:40.00ms
step:880/2330 train_time:35206ms step_avg:40.01ms
step:881/2330 train_time:35242ms step_avg:40.00ms
step:882/2330 train_time:35287ms step_avg:40.01ms
step:883/2330 train_time:35322ms step_avg:40.00ms
step:884/2330 train_time:35367ms step_avg:40.01ms
step:885/2330 train_time:35402ms step_avg:40.00ms
step:886/2330 train_time:35447ms step_avg:40.01ms
step:887/2330 train_time:35483ms step_avg:40.00ms
step:888/2330 train_time:35527ms step_avg:40.01ms
step:889/2330 train_time:35563ms step_avg:40.00ms
step:890/2330 train_time:35607ms step_avg:40.01ms
step:891/2330 train_time:35643ms step_avg:40.00ms
step:892/2330 train_time:35688ms step_avg:40.01ms
step:893/2330 train_time:35723ms step_avg:40.00ms
step:894/2330 train_time:35767ms step_avg:40.01ms
step:895/2330 train_time:35803ms step_avg:40.00ms
step:896/2330 train_time:35847ms step_avg:40.01ms
step:897/2330 train_time:35882ms step_avg:40.00ms
step:898/2330 train_time:35927ms step_avg:40.01ms
step:899/2330 train_time:35962ms step_avg:40.00ms
step:900/2330 train_time:36007ms step_avg:40.01ms
step:901/2330 train_time:36042ms step_avg:40.00ms
step:902/2330 train_time:36086ms step_avg:40.01ms
step:903/2330 train_time:36122ms step_avg:40.00ms
step:904/2330 train_time:36166ms step_avg:40.01ms
step:905/2330 train_time:36202ms step_avg:40.00ms
step:906/2330 train_time:36246ms step_avg:40.01ms
step:907/2330 train_time:36282ms step_avg:40.00ms
step:908/2330 train_time:36328ms step_avg:40.01ms
step:909/2330 train_time:36363ms step_avg:40.00ms
step:910/2330 train_time:36407ms step_avg:40.01ms
step:911/2330 train_time:36443ms step_avg:40.00ms
step:912/2330 train_time:36487ms step_avg:40.01ms
step:913/2330 train_time:36523ms step_avg:40.00ms
step:914/2330 train_time:36567ms step_avg:40.01ms
step:915/2330 train_time:36602ms step_avg:40.00ms
step:916/2330 train_time:36647ms step_avg:40.01ms
step:917/2330 train_time:36682ms step_avg:40.00ms
step:918/2330 train_time:36727ms step_avg:40.01ms
step:919/2330 train_time:36762ms step_avg:40.00ms
step:920/2330 train_time:36806ms step_avg:40.01ms
step:921/2330 train_time:36842ms step_avg:40.00ms
step:922/2330 train_time:36886ms step_avg:40.01ms
step:923/2330 train_time:36922ms step_avg:40.00ms
step:924/2330 train_time:36966ms step_avg:40.01ms
step:925/2330 train_time:37001ms step_avg:40.00ms
step:926/2330 train_time:37046ms step_avg:40.01ms
step:927/2330 train_time:37082ms step_avg:40.00ms
step:928/2330 train_time:37126ms step_avg:40.01ms
step:929/2330 train_time:37162ms step_avg:40.00ms
step:930/2330 train_time:37206ms step_avg:40.01ms
step:931/2330 train_time:37242ms step_avg:40.00ms
step:932/2330 train_time:37286ms step_avg:40.01ms
step:933/2330 train_time:37322ms step_avg:40.00ms
step:934/2330 train_time:37367ms step_avg:40.01ms
step:935/2330 train_time:37403ms step_avg:40.00ms
step:936/2330 train_time:37447ms step_avg:40.01ms
step:937/2330 train_time:37483ms step_avg:40.00ms
step:938/2330 train_time:37527ms step_avg:40.01ms
step:939/2330 train_time:37563ms step_avg:40.00ms
step:940/2330 train_time:37608ms step_avg:40.01ms
step:941/2330 train_time:37643ms step_avg:40.00ms
step:942/2330 train_time:37687ms step_avg:40.01ms
step:943/2330 train_time:37723ms step_avg:40.00ms
step:944/2330 train_time:37767ms step_avg:40.01ms
step:945/2330 train_time:37803ms step_avg:40.00ms
step:946/2330 train_time:37848ms step_avg:40.01ms
step:947/2330 train_time:37884ms step_avg:40.00ms
step:948/2330 train_time:37928ms step_avg:40.01ms
step:949/2330 train_time:37964ms step_avg:40.00ms
step:950/2330 train_time:38008ms step_avg:40.01ms
step:951/2330 train_time:38043ms step_avg:40.00ms
step:952/2330 train_time:38088ms step_avg:40.01ms
step:953/2330 train_time:38123ms step_avg:40.00ms
step:954/2330 train_time:38168ms step_avg:40.01ms
step:955/2330 train_time:38203ms step_avg:40.00ms
step:956/2330 train_time:38248ms step_avg:40.01ms
step:957/2330 train_time:38282ms step_avg:40.00ms
step:958/2330 train_time:38327ms step_avg:40.01ms
step:959/2330 train_time:38362ms step_avg:40.00ms
step:960/2330 train_time:38407ms step_avg:40.01ms
step:961/2330 train_time:38443ms step_avg:40.00ms
step:962/2330 train_time:38487ms step_avg:40.01ms
step:963/2330 train_time:38523ms step_avg:40.00ms
step:964/2330 train_time:38567ms step_avg:40.01ms
step:965/2330 train_time:38603ms step_avg:40.00ms
step:966/2330 train_time:38647ms step_avg:40.01ms
step:967/2330 train_time:38683ms step_avg:40.00ms
step:968/2330 train_time:38727ms step_avg:40.01ms
step:969/2330 train_time:38763ms step_avg:40.00ms
step:970/2330 train_time:38807ms step_avg:40.01ms
step:971/2330 train_time:38842ms step_avg:40.00ms
step:972/2330 train_time:38887ms step_avg:40.01ms
step:973/2330 train_time:38922ms step_avg:40.00ms
step:974/2330 train_time:38967ms step_avg:40.01ms
step:975/2330 train_time:39002ms step_avg:40.00ms
step:976/2330 train_time:39046ms step_avg:40.01ms
step:977/2330 train_time:39082ms step_avg:40.00ms
step:978/2330 train_time:39126ms step_avg:40.01ms
step:979/2330 train_time:39161ms step_avg:40.00ms
step:980/2330 train_time:39206ms step_avg:40.01ms
step:981/2330 train_time:39241ms step_avg:40.00ms
step:982/2330 train_time:39287ms step_avg:40.01ms
step:983/2330 train_time:39322ms step_avg:40.00ms
step:984/2330 train_time:39366ms step_avg:40.01ms
step:985/2330 train_time:39402ms step_avg:40.00ms
step:986/2330 train_time:39446ms step_avg:40.01ms
step:987/2330 train_time:39482ms step_avg:40.00ms
step:988/2330 train_time:39527ms step_avg:40.01ms
step:989/2330 train_time:39562ms step_avg:40.00ms
step:990/2330 train_time:39607ms step_avg:40.01ms
step:991/2330 train_time:39643ms step_avg:40.00ms
step:992/2330 train_time:39688ms step_avg:40.01ms
step:993/2330 train_time:39724ms step_avg:40.00ms
step:994/2330 train_time:39767ms step_avg:40.01ms
step:995/2330 train_time:39803ms step_avg:40.00ms
step:996/2330 train_time:39847ms step_avg:40.01ms
step:997/2330 train_time:39882ms step_avg:40.00ms
step:998/2330 train_time:39927ms step_avg:40.01ms
step:999/2330 train_time:39962ms step_avg:40.00ms
step:1000/2330 train_time:40006ms step_avg:40.01ms
step:1000/2330 val_loss:5.2121 train_time:40094ms step_avg:40.09ms
step:1001/2330 train_time:40107ms step_avg:40.07ms
step:1002/2330 train_time:40118ms step_avg:40.04ms
step:1003/2330 train_time:40128ms step_avg:40.01ms
step:1004/2330 train_time:40167ms step_avg:40.01ms
step:1005/2330 train_time:40201ms step_avg:40.00ms
step:1006/2330 train_time:40244ms step_avg:40.00ms
step:1007/2330 train_time:40280ms step_avg:40.00ms
step:1008/2330 train_time:40323ms step_avg:40.00ms
step:1009/2330 train_time:40358ms step_avg:40.00ms
step:1010/2330 train_time:40403ms step_avg:40.00ms
step:1011/2330 train_time:40443ms step_avg:40.00ms
step:1012/2330 train_time:40493ms step_avg:40.01ms
step:1013/2330 train_time:40530ms step_avg:40.01ms
step:1014/2330 train_time:40575ms step_avg:40.02ms
step:1015/2330 train_time:40611ms step_avg:40.01ms
step:1016/2330 train_time:40655ms step_avg:40.01ms
step:1017/2330 train_time:40690ms step_avg:40.01ms
step:1018/2330 train_time:40734ms step_avg:40.01ms
step:1019/2330 train_time:40769ms step_avg:40.01ms
step:1020/2330 train_time:40813ms step_avg:40.01ms
step:1021/2330 train_time:40847ms step_avg:40.01ms
step:1022/2330 train_time:40892ms step_avg:40.01ms
step:1023/2330 train_time:40926ms step_avg:40.01ms
step:1024/2330 train_time:40969ms step_avg:40.01ms
step:1025/2330 train_time:41005ms step_avg:40.00ms
step:1026/2330 train_time:41050ms step_avg:40.01ms
step:1027/2330 train_time:41085ms step_avg:40.01ms
step:1028/2330 train_time:41129ms step_avg:40.01ms
step:1029/2330 train_time:41163ms step_avg:40.00ms
step:1030/2330 train_time:41207ms step_avg:40.01ms
step:1031/2330 train_time:41241ms step_avg:40.00ms
step:1032/2330 train_time:41285ms step_avg:40.01ms
step:1033/2330 train_time:41320ms step_avg:40.00ms
step:1034/2330 train_time:41366ms step_avg:40.01ms
step:1035/2330 train_time:41401ms step_avg:40.00ms
step:1036/2330 train_time:41447ms step_avg:40.01ms
step:1037/2330 train_time:41483ms step_avg:40.00ms
step:1038/2330 train_time:41529ms step_avg:40.01ms
step:1039/2330 train_time:41565ms step_avg:40.00ms
step:1040/2330 train_time:41610ms step_avg:40.01ms
step:1041/2330 train_time:41646ms step_avg:40.01ms
step:1042/2330 train_time:41690ms step_avg:40.01ms
step:1043/2330 train_time:41726ms step_avg:40.01ms
step:1044/2330 train_time:41770ms step_avg:40.01ms
step:1045/2330 train_time:41804ms step_avg:40.00ms
step:1046/2330 train_time:41849ms step_avg:40.01ms
step:1047/2330 train_time:41884ms step_avg:40.00ms
step:1048/2330 train_time:41927ms step_avg:40.01ms
step:1049/2330 train_time:41962ms step_avg:40.00ms
step:1050/2330 train_time:42006ms step_avg:40.01ms
step:1051/2330 train_time:42041ms step_avg:40.00ms
step:1052/2330 train_time:42085ms step_avg:40.00ms
step:1053/2330 train_time:42120ms step_avg:40.00ms
step:1054/2330 train_time:42164ms step_avg:40.00ms
step:1055/2330 train_time:42199ms step_avg:40.00ms
step:1056/2330 train_time:42244ms step_avg:40.00ms
step:1057/2330 train_time:42279ms step_avg:40.00ms
step:1058/2330 train_time:42324ms step_avg:40.00ms
step:1059/2330 train_time:42360ms step_avg:40.00ms
step:1060/2330 train_time:42404ms step_avg:40.00ms
step:1061/2330 train_time:42441ms step_avg:40.00ms
step:1062/2330 train_time:42486ms step_avg:40.01ms
step:1063/2330 train_time:42523ms step_avg:40.00ms
step:1064/2330 train_time:42569ms step_avg:40.01ms
step:1065/2330 train_time:42605ms step_avg:40.00ms
step:1066/2330 train_time:42649ms step_avg:40.01ms
step:1067/2330 train_time:42685ms step_avg:40.00ms
step:1068/2330 train_time:42731ms step_avg:40.01ms
step:1069/2330 train_time:42765ms step_avg:40.01ms
step:1070/2330 train_time:42811ms step_avg:40.01ms
step:1071/2330 train_time:42845ms step_avg:40.00ms
step:1072/2330 train_time:42890ms step_avg:40.01ms
step:1073/2330 train_time:42925ms step_avg:40.00ms
step:1074/2330 train_time:42970ms step_avg:40.01ms
step:1075/2330 train_time:43006ms step_avg:40.01ms
step:1076/2330 train_time:43051ms step_avg:40.01ms
step:1077/2330 train_time:43086ms step_avg:40.01ms
step:1078/2330 train_time:43131ms step_avg:40.01ms
step:1079/2330 train_time:43166ms step_avg:40.01ms
step:1080/2330 train_time:43212ms step_avg:40.01ms
step:1081/2330 train_time:43247ms step_avg:40.01ms
step:1082/2330 train_time:43293ms step_avg:40.01ms
step:1083/2330 train_time:43329ms step_avg:40.01ms
step:1084/2330 train_time:43374ms step_avg:40.01ms
step:1085/2330 train_time:43410ms step_avg:40.01ms
step:1086/2330 train_time:43454ms step_avg:40.01ms
step:1087/2330 train_time:43490ms step_avg:40.01ms
step:1088/2330 train_time:43534ms step_avg:40.01ms
step:1089/2330 train_time:43570ms step_avg:40.01ms
step:1090/2330 train_time:43614ms step_avg:40.01ms
step:1091/2330 train_time:43650ms step_avg:40.01ms
step:1092/2330 train_time:43695ms step_avg:40.01ms
step:1093/2330 train_time:43730ms step_avg:40.01ms
step:1094/2330 train_time:43774ms step_avg:40.01ms
step:1095/2330 train_time:43809ms step_avg:40.01ms
step:1096/2330 train_time:43853ms step_avg:40.01ms
step:1097/2330 train_time:43888ms step_avg:40.01ms
step:1098/2330 train_time:43934ms step_avg:40.01ms
step:1099/2330 train_time:43968ms step_avg:40.01ms
step:1100/2330 train_time:44013ms step_avg:40.01ms
step:1101/2330 train_time:44047ms step_avg:40.01ms
step:1102/2330 train_time:44093ms step_avg:40.01ms
step:1103/2330 train_time:44128ms step_avg:40.01ms
step:1104/2330 train_time:44172ms step_avg:40.01ms
step:1105/2330 train_time:44207ms step_avg:40.01ms
step:1106/2330 train_time:44253ms step_avg:40.01ms
step:1107/2330 train_time:44288ms step_avg:40.01ms
step:1108/2330 train_time:44333ms step_avg:40.01ms
step:1109/2330 train_time:44369ms step_avg:40.01ms
step:1110/2330 train_time:44413ms step_avg:40.01ms
step:1111/2330 train_time:44449ms step_avg:40.01ms
step:1112/2330 train_time:44493ms step_avg:40.01ms
step:1113/2330 train_time:44530ms step_avg:40.01ms
step:1114/2330 train_time:44574ms step_avg:40.01ms
step:1115/2330 train_time:44610ms step_avg:40.01ms
step:1116/2330 train_time:44655ms step_avg:40.01ms
step:1117/2330 train_time:44690ms step_avg:40.01ms
step:1118/2330 train_time:44736ms step_avg:40.01ms
step:1119/2330 train_time:44771ms step_avg:40.01ms
step:1120/2330 train_time:44815ms step_avg:40.01ms
step:1121/2330 train_time:44850ms step_avg:40.01ms
step:1122/2330 train_time:44895ms step_avg:40.01ms
step:1123/2330 train_time:44930ms step_avg:40.01ms
step:1124/2330 train_time:44975ms step_avg:40.01ms
step:1125/2330 train_time:45010ms step_avg:40.01ms
step:1126/2330 train_time:45056ms step_avg:40.01ms
step:1127/2330 train_time:45091ms step_avg:40.01ms
step:1128/2330 train_time:45135ms step_avg:40.01ms
step:1129/2330 train_time:45170ms step_avg:40.01ms
step:1130/2330 train_time:45215ms step_avg:40.01ms
step:1131/2330 train_time:45249ms step_avg:40.01ms
step:1132/2330 train_time:45295ms step_avg:40.01ms
step:1133/2330 train_time:45331ms step_avg:40.01ms
step:1134/2330 train_time:45376ms step_avg:40.01ms
step:1135/2330 train_time:45411ms step_avg:40.01ms
step:1136/2330 train_time:45456ms step_avg:40.01ms
step:1137/2330 train_time:45491ms step_avg:40.01ms
step:1138/2330 train_time:45536ms step_avg:40.01ms
step:1139/2330 train_time:45571ms step_avg:40.01ms
step:1140/2330 train_time:45616ms step_avg:40.01ms
step:1141/2330 train_time:45651ms step_avg:40.01ms
step:1142/2330 train_time:45696ms step_avg:40.01ms
step:1143/2330 train_time:45731ms step_avg:40.01ms
step:1144/2330 train_time:45776ms step_avg:40.01ms
step:1145/2330 train_time:45812ms step_avg:40.01ms
step:1146/2330 train_time:45857ms step_avg:40.01ms
step:1147/2330 train_time:45892ms step_avg:40.01ms
step:1148/2330 train_time:45937ms step_avg:40.01ms
step:1149/2330 train_time:45972ms step_avg:40.01ms
step:1150/2330 train_time:46017ms step_avg:40.01ms
step:1151/2330 train_time:46052ms step_avg:40.01ms
step:1152/2330 train_time:46097ms step_avg:40.02ms
step:1153/2330 train_time:46133ms step_avg:40.01ms
step:1154/2330 train_time:46178ms step_avg:40.02ms
step:1155/2330 train_time:46213ms step_avg:40.01ms
step:1156/2330 train_time:46259ms step_avg:40.02ms
step:1157/2330 train_time:46294ms step_avg:40.01ms
step:1158/2330 train_time:46339ms step_avg:40.02ms
step:1159/2330 train_time:46374ms step_avg:40.01ms
step:1160/2330 train_time:46419ms step_avg:40.02ms
step:1161/2330 train_time:46454ms step_avg:40.01ms
step:1162/2330 train_time:46499ms step_avg:40.02ms
step:1163/2330 train_time:46534ms step_avg:40.01ms
step:1164/2330 train_time:46580ms step_avg:40.02ms
step:1165/2330 train_time:46615ms step_avg:40.01ms
step:1166/2330 train_time:46660ms step_avg:40.02ms
step:1167/2330 train_time:46696ms step_avg:40.01ms
step:1168/2330 train_time:46740ms step_avg:40.02ms
step:1169/2330 train_time:46776ms step_avg:40.01ms
step:1170/2330 train_time:46820ms step_avg:40.02ms
step:1171/2330 train_time:46856ms step_avg:40.01ms
step:1172/2330 train_time:46900ms step_avg:40.02ms
step:1173/2330 train_time:46935ms step_avg:40.01ms
step:1174/2330 train_time:46980ms step_avg:40.02ms
step:1175/2330 train_time:47016ms step_avg:40.01ms
step:1176/2330 train_time:47060ms step_avg:40.02ms
step:1177/2330 train_time:47096ms step_avg:40.01ms
step:1178/2330 train_time:47141ms step_avg:40.02ms
step:1179/2330 train_time:47176ms step_avg:40.01ms
step:1180/2330 train_time:47221ms step_avg:40.02ms
step:1181/2330 train_time:47256ms step_avg:40.01ms
step:1182/2330 train_time:47300ms step_avg:40.02ms
step:1183/2330 train_time:47336ms step_avg:40.01ms
step:1184/2330 train_time:47380ms step_avg:40.02ms
step:1185/2330 train_time:47416ms step_avg:40.01ms
step:1186/2330 train_time:47461ms step_avg:40.02ms
step:1187/2330 train_time:47496ms step_avg:40.01ms
step:1188/2330 train_time:47541ms step_avg:40.02ms
step:1189/2330 train_time:47577ms step_avg:40.01ms
step:1190/2330 train_time:47621ms step_avg:40.02ms
step:1191/2330 train_time:47657ms step_avg:40.01ms
step:1192/2330 train_time:47701ms step_avg:40.02ms
step:1193/2330 train_time:47736ms step_avg:40.01ms
step:1194/2330 train_time:47781ms step_avg:40.02ms
step:1195/2330 train_time:47816ms step_avg:40.01ms
step:1196/2330 train_time:47861ms step_avg:40.02ms
step:1197/2330 train_time:47897ms step_avg:40.01ms
step:1198/2330 train_time:47941ms step_avg:40.02ms
step:1199/2330 train_time:47977ms step_avg:40.01ms
step:1200/2330 train_time:48022ms step_avg:40.02ms
step:1201/2330 train_time:48057ms step_avg:40.01ms
step:1202/2330 train_time:48101ms step_avg:40.02ms
step:1203/2330 train_time:48137ms step_avg:40.01ms
step:1204/2330 train_time:48181ms step_avg:40.02ms
step:1205/2330 train_time:48217ms step_avg:40.01ms
step:1206/2330 train_time:48262ms step_avg:40.02ms
step:1207/2330 train_time:48297ms step_avg:40.01ms
step:1208/2330 train_time:48343ms step_avg:40.02ms
step:1209/2330 train_time:48379ms step_avg:40.02ms
step:1210/2330 train_time:48424ms step_avg:40.02ms
step:1211/2330 train_time:48459ms step_avg:40.02ms
step:1212/2330 train_time:48503ms step_avg:40.02ms
step:1213/2330 train_time:48539ms step_avg:40.02ms
step:1214/2330 train_time:48583ms step_avg:40.02ms
step:1215/2330 train_time:48619ms step_avg:40.02ms
step:1216/2330 train_time:48663ms step_avg:40.02ms
step:1217/2330 train_time:48699ms step_avg:40.02ms
step:1218/2330 train_time:48743ms step_avg:40.02ms
step:1219/2330 train_time:48779ms step_avg:40.02ms
step:1220/2330 train_time:48824ms step_avg:40.02ms
step:1221/2330 train_time:48859ms step_avg:40.02ms
step:1222/2330 train_time:48904ms step_avg:40.02ms
step:1223/2330 train_time:48939ms step_avg:40.02ms
step:1224/2330 train_time:48985ms step_avg:40.02ms
step:1225/2330 train_time:49020ms step_avg:40.02ms
step:1226/2330 train_time:49065ms step_avg:40.02ms
step:1227/2330 train_time:49100ms step_avg:40.02ms
step:1228/2330 train_time:49145ms step_avg:40.02ms
step:1229/2330 train_time:49180ms step_avg:40.02ms
step:1230/2330 train_time:49224ms step_avg:40.02ms
step:1231/2330 train_time:49260ms step_avg:40.02ms
step:1232/2330 train_time:49305ms step_avg:40.02ms
step:1233/2330 train_time:49341ms step_avg:40.02ms
step:1234/2330 train_time:49385ms step_avg:40.02ms
step:1235/2330 train_time:49421ms step_avg:40.02ms
step:1236/2330 train_time:49465ms step_avg:40.02ms
step:1237/2330 train_time:49500ms step_avg:40.02ms
step:1238/2330 train_time:49545ms step_avg:40.02ms
step:1239/2330 train_time:49580ms step_avg:40.02ms
step:1240/2330 train_time:49624ms step_avg:40.02ms
step:1241/2330 train_time:49660ms step_avg:40.02ms
step:1242/2330 train_time:49705ms step_avg:40.02ms
step:1243/2330 train_time:49740ms step_avg:40.02ms
step:1244/2330 train_time:49785ms step_avg:40.02ms
step:1245/2330 train_time:49820ms step_avg:40.02ms
step:1246/2330 train_time:49865ms step_avg:40.02ms
step:1247/2330 train_time:49900ms step_avg:40.02ms
step:1248/2330 train_time:49944ms step_avg:40.02ms
step:1249/2330 train_time:49979ms step_avg:40.02ms
step:1250/2330 train_time:50023ms step_avg:40.02ms
step:1250/2330 val_loss:5.2000 train_time:50111ms step_avg:40.09ms
step:1251/2330 train_time:50124ms step_avg:40.07ms
step:1252/2330 train_time:50136ms step_avg:40.04ms
step:1253/2330 train_time:50146ms step_avg:40.02ms
step:1254/2330 train_time:50185ms step_avg:40.02ms
step:1255/2330 train_time:50219ms step_avg:40.02ms
step:1256/2330 train_time:50263ms step_avg:40.02ms
step:1257/2330 train_time:50298ms step_avg:40.01ms
step:1258/2330 train_time:50342ms step_avg:40.02ms
step:1259/2330 train_time:50376ms step_avg:40.01ms
step:1260/2330 train_time:50423ms step_avg:40.02ms
step:1261/2330 train_time:50462ms step_avg:40.02ms
step:1262/2330 train_time:50510ms step_avg:40.02ms
step:1263/2330 train_time:50545ms step_avg:40.02ms
step:1264/2330 train_time:50590ms step_avg:40.02ms
step:1265/2330 train_time:50698ms step_avg:40.08ms
step:1266/2330 train_time:50741ms step_avg:40.08ms
step:1267/2330 train_time:50776ms step_avg:40.08ms
step:1268/2330 train_time:50820ms step_avg:40.08ms
step:1269/2330 train_time:50971ms step_avg:40.17ms
step:1270/2330 train_time:51064ms step_avg:40.21ms
step:1271/2330 train_time:51097ms step_avg:40.20ms
step:1272/2330 train_time:51203ms step_avg:40.25ms
step:1273/2330 train_time:51237ms step_avg:40.25ms
step:1274/2330 train_time:51429ms step_avg:40.37ms
step:1275/2330 train_time:51462ms step_avg:40.36ms
step:1276/2330 train_time:51505ms step_avg:40.36ms
step:1277/2330 train_time:51539ms step_avg:40.36ms
step:1278/2330 train_time:51583ms step_avg:40.36ms
step:1279/2330 train_time:51618ms step_avg:40.36ms
step:1280/2330 train_time:51662ms step_avg:40.36ms
step:1281/2330 train_time:51696ms step_avg:40.36ms
step:1282/2330 train_time:51740ms step_avg:40.36ms
step:1283/2330 train_time:51775ms step_avg:40.35ms
step:1284/2330 train_time:51818ms step_avg:40.36ms
step:1285/2330 train_time:51852ms step_avg:40.35ms
step:1286/2330 train_time:51896ms step_avg:40.35ms
step:1287/2330 train_time:51930ms step_avg:40.35ms
step:1288/2330 train_time:51974ms step_avg:40.35ms
step:1289/2330 train_time:52008ms step_avg:40.35ms
step:1290/2330 train_time:52051ms step_avg:40.35ms
step:1291/2330 train_time:52085ms step_avg:40.34ms
step:1292/2330 train_time:52129ms step_avg:40.35ms
step:1293/2330 train_time:52164ms step_avg:40.34ms
step:1294/2330 train_time:52207ms step_avg:40.35ms
step:1295/2330 train_time:52242ms step_avg:40.34ms
step:1296/2330 train_time:52290ms step_avg:40.35ms
step:1297/2330 train_time:52329ms step_avg:40.35ms
step:1298/2330 train_time:52378ms step_avg:40.35ms
step:1299/2330 train_time:52414ms step_avg:40.35ms
step:1300/2330 train_time:52461ms step_avg:40.35ms
step:1301/2330 train_time:52497ms step_avg:40.35ms
step:1302/2330 train_time:52541ms step_avg:40.35ms
step:1303/2330 train_time:52576ms step_avg:40.35ms
step:1304/2330 train_time:52620ms step_avg:40.35ms
step:1305/2330 train_time:52655ms step_avg:40.35ms
step:1306/2330 train_time:52700ms step_avg:40.35ms
step:1307/2330 train_time:52734ms step_avg:40.35ms
step:1308/2330 train_time:52778ms step_avg:40.35ms
step:1309/2330 train_time:52813ms step_avg:40.35ms
step:1310/2330 train_time:52857ms step_avg:40.35ms
step:1311/2330 train_time:52892ms step_avg:40.34ms
step:1312/2330 train_time:52936ms step_avg:40.35ms
step:1313/2330 train_time:52971ms step_avg:40.34ms
step:1314/2330 train_time:53015ms step_avg:40.35ms
step:1315/2330 train_time:53050ms step_avg:40.34ms
step:1316/2330 train_time:53093ms step_avg:40.34ms
step:1317/2330 train_time:53128ms step_avg:40.34ms
step:1318/2330 train_time:53171ms step_avg:40.34ms
step:1319/2330 train_time:53206ms step_avg:40.34ms
step:1320/2330 train_time:53250ms step_avg:40.34ms
step:1321/2330 train_time:53286ms step_avg:40.34ms
step:1322/2330 train_time:53331ms step_avg:40.34ms
step:1323/2330 train_time:53368ms step_avg:40.34ms
step:1324/2330 train_time:53416ms step_avg:40.34ms
step:1325/2330 train_time:53453ms step_avg:40.34ms
step:1326/2330 train_time:53498ms step_avg:40.35ms
step:1327/2330 train_time:53534ms step_avg:40.34ms
step:1328/2330 train_time:53577ms step_avg:40.34ms
step:1329/2330 train_time:53612ms step_avg:40.34ms
step:1330/2330 train_time:53657ms step_avg:40.34ms
step:1331/2330 train_time:53691ms step_avg:40.34ms
step:1332/2330 train_time:53735ms step_avg:40.34ms
step:1333/2330 train_time:53770ms step_avg:40.34ms
step:1334/2330 train_time:53814ms step_avg:40.34ms
step:1335/2330 train_time:53849ms step_avg:40.34ms
step:1336/2330 train_time:53892ms step_avg:40.34ms
step:1337/2330 train_time:53926ms step_avg:40.33ms
step:1338/2330 train_time:53970ms step_avg:40.34ms
step:1339/2330 train_time:54005ms step_avg:40.33ms
step:1340/2330 train_time:54049ms step_avg:40.34ms
step:1341/2330 train_time:54084ms step_avg:40.33ms
step:1342/2330 train_time:54128ms step_avg:40.33ms
step:1343/2330 train_time:54163ms step_avg:40.33ms
step:1344/2330 train_time:54208ms step_avg:40.33ms
step:1345/2330 train_time:54244ms step_avg:40.33ms
step:1346/2330 train_time:54289ms step_avg:40.33ms
step:1347/2330 train_time:54325ms step_avg:40.33ms
step:1348/2330 train_time:54370ms step_avg:40.33ms
step:1349/2330 train_time:54406ms step_avg:40.33ms
step:1350/2330 train_time:54451ms step_avg:40.33ms
step:1351/2330 train_time:54487ms step_avg:40.33ms
step:1352/2330 train_time:54532ms step_avg:40.33ms
step:1353/2330 train_time:54567ms step_avg:40.33ms
step:1354/2330 train_time:54611ms step_avg:40.33ms
step:1355/2330 train_time:54647ms step_avg:40.33ms
step:1356/2330 train_time:54692ms step_avg:40.33ms
step:1357/2330 train_time:54727ms step_avg:40.33ms
step:1358/2330 train_time:54771ms step_avg:40.33ms
step:1359/2330 train_time:54806ms step_avg:40.33ms
step:1360/2330 train_time:54850ms step_avg:40.33ms
step:1361/2330 train_time:54885ms step_avg:40.33ms
step:1362/2330 train_time:54928ms step_avg:40.33ms
step:1363/2330 train_time:54963ms step_avg:40.33ms
step:1364/2330 train_time:55007ms step_avg:40.33ms
step:1365/2330 train_time:55043ms step_avg:40.32ms
step:1366/2330 train_time:55086ms step_avg:40.33ms
step:1367/2330 train_time:55121ms step_avg:40.32ms
step:1368/2330 train_time:55166ms step_avg:40.33ms
step:1369/2330 train_time:55203ms step_avg:40.32ms
step:1370/2330 train_time:55248ms step_avg:40.33ms
step:1371/2330 train_time:55284ms step_avg:40.32ms
step:1372/2330 train_time:55329ms step_avg:40.33ms
step:1373/2330 train_time:55364ms step_avg:40.32ms
step:1374/2330 train_time:55409ms step_avg:40.33ms
step:1375/2330 train_time:55445ms step_avg:40.32ms
step:1376/2330 train_time:55489ms step_avg:40.33ms
step:1377/2330 train_time:55525ms step_avg:40.32ms
step:1378/2330 train_time:55569ms step_avg:40.33ms
step:1379/2330 train_time:55605ms step_avg:40.32ms
step:1380/2330 train_time:55650ms step_avg:40.33ms
step:1381/2330 train_time:55685ms step_avg:40.32ms
step:1382/2330 train_time:55730ms step_avg:40.33ms
step:1383/2330 train_time:55765ms step_avg:40.32ms
step:1384/2330 train_time:55809ms step_avg:40.32ms
step:1385/2330 train_time:55844ms step_avg:40.32ms
step:1386/2330 train_time:55888ms step_avg:40.32ms
step:1387/2330 train_time:55923ms step_avg:40.32ms
step:1388/2330 train_time:55967ms step_avg:40.32ms
step:1389/2330 train_time:56002ms step_avg:40.32ms
step:1390/2330 train_time:56047ms step_avg:40.32ms
step:1391/2330 train_time:56082ms step_avg:40.32ms
step:1392/2330 train_time:56126ms step_avg:40.32ms
step:1393/2330 train_time:56161ms step_avg:40.32ms
step:1394/2330 train_time:56206ms step_avg:40.32ms
step:1395/2330 train_time:56242ms step_avg:40.32ms
step:1396/2330 train_time:56287ms step_avg:40.32ms
step:1397/2330 train_time:56323ms step_avg:40.32ms
step:1398/2330 train_time:56368ms step_avg:40.32ms
step:1399/2330 train_time:56403ms step_avg:40.32ms
step:1400/2330 train_time:56448ms step_avg:40.32ms
step:1401/2330 train_time:56484ms step_avg:40.32ms
step:1402/2330 train_time:56529ms step_avg:40.32ms
step:1403/2330 train_time:56564ms step_avg:40.32ms
step:1404/2330 train_time:56609ms step_avg:40.32ms
step:1405/2330 train_time:56645ms step_avg:40.32ms
step:1406/2330 train_time:56689ms step_avg:40.32ms
step:1407/2330 train_time:56725ms step_avg:40.32ms
step:1408/2330 train_time:56769ms step_avg:40.32ms
step:1409/2330 train_time:56803ms step_avg:40.31ms
step:1410/2330 train_time:56848ms step_avg:40.32ms
step:1411/2330 train_time:56883ms step_avg:40.31ms
step:1412/2330 train_time:56928ms step_avg:40.32ms
step:1413/2330 train_time:56963ms step_avg:40.31ms
step:1414/2330 train_time:57006ms step_avg:40.32ms
step:1415/2330 train_time:57041ms step_avg:40.31ms
step:1416/2330 train_time:57086ms step_avg:40.32ms
step:1417/2330 train_time:57122ms step_avg:40.31ms
step:1418/2330 train_time:57166ms step_avg:40.31ms
step:1419/2330 train_time:57203ms step_avg:40.31ms
step:1420/2330 train_time:57248ms step_avg:40.32ms
step:1421/2330 train_time:57284ms step_avg:40.31ms
step:1422/2330 train_time:57328ms step_avg:40.32ms
step:1423/2330 train_time:57364ms step_avg:40.31ms
step:1424/2330 train_time:57409ms step_avg:40.32ms
step:1425/2330 train_time:57445ms step_avg:40.31ms
step:1426/2330 train_time:57490ms step_avg:40.32ms
step:1427/2330 train_time:57525ms step_avg:40.31ms
step:1428/2330 train_time:57570ms step_avg:40.31ms
step:1429/2330 train_time:57605ms step_avg:40.31ms
step:1430/2330 train_time:57650ms step_avg:40.31ms
step:1431/2330 train_time:57685ms step_avg:40.31ms
step:1432/2330 train_time:57729ms step_avg:40.31ms
step:1433/2330 train_time:57764ms step_avg:40.31ms
step:1434/2330 train_time:57808ms step_avg:40.31ms
step:1435/2330 train_time:57844ms step_avg:40.31ms
step:1436/2330 train_time:57888ms step_avg:40.31ms
step:1437/2330 train_time:57923ms step_avg:40.31ms
step:1438/2330 train_time:57967ms step_avg:40.31ms
step:1439/2330 train_time:58003ms step_avg:40.31ms
step:1440/2330 train_time:58047ms step_avg:40.31ms
step:1441/2330 train_time:58082ms step_avg:40.31ms
step:1442/2330 train_time:58127ms step_avg:40.31ms
step:1443/2330 train_time:58163ms step_avg:40.31ms
step:1444/2330 train_time:58208ms step_avg:40.31ms
step:1445/2330 train_time:58244ms step_avg:40.31ms
step:1446/2330 train_time:58289ms step_avg:40.31ms
step:1447/2330 train_time:58324ms step_avg:40.31ms
step:1448/2330 train_time:58369ms step_avg:40.31ms
step:1449/2330 train_time:58405ms step_avg:40.31ms
step:1450/2330 train_time:58449ms step_avg:40.31ms
step:1451/2330 train_time:58485ms step_avg:40.31ms
step:1452/2330 train_time:58530ms step_avg:40.31ms
step:1453/2330 train_time:58565ms step_avg:40.31ms
step:1454/2330 train_time:58610ms step_avg:40.31ms
step:1455/2330 train_time:58646ms step_avg:40.31ms
step:1456/2330 train_time:58690ms step_avg:40.31ms
step:1457/2330 train_time:58725ms step_avg:40.31ms
step:1458/2330 train_time:58769ms step_avg:40.31ms
step:1459/2330 train_time:58804ms step_avg:40.30ms
step:1460/2330 train_time:58849ms step_avg:40.31ms
step:1461/2330 train_time:58884ms step_avg:40.30ms
step:1462/2330 train_time:58928ms step_avg:40.31ms
step:1463/2330 train_time:58963ms step_avg:40.30ms
step:1464/2330 train_time:59007ms step_avg:40.31ms
step:1465/2330 train_time:59042ms step_avg:40.30ms
step:1466/2330 train_time:59086ms step_avg:40.30ms
step:1467/2330 train_time:59122ms step_avg:40.30ms
step:1468/2330 train_time:59167ms step_avg:40.30ms
step:1469/2330 train_time:59202ms step_avg:40.30ms
step:1470/2330 train_time:59248ms step_avg:40.30ms
step:1471/2330 train_time:59284ms step_avg:40.30ms
step:1472/2330 train_time:59328ms step_avg:40.30ms
step:1473/2330 train_time:59364ms step_avg:40.30ms
step:1474/2330 train_time:59408ms step_avg:40.30ms
step:1475/2330 train_time:59444ms step_avg:40.30ms
step:1476/2330 train_time:59489ms step_avg:40.30ms
step:1477/2330 train_time:59525ms step_avg:40.30ms
step:1478/2330 train_time:59570ms step_avg:40.30ms
step:1479/2330 train_time:59605ms step_avg:40.30ms
step:1480/2330 train_time:59650ms step_avg:40.30ms
step:1481/2330 train_time:59686ms step_avg:40.30ms
step:1482/2330 train_time:59730ms step_avg:40.30ms
step:1483/2330 train_time:59765ms step_avg:40.30ms
step:1484/2330 train_time:59810ms step_avg:40.30ms
step:1485/2330 train_time:59845ms step_avg:40.30ms
step:1486/2330 train_time:59889ms step_avg:40.30ms
step:1487/2330 train_time:59924ms step_avg:40.30ms
step:1488/2330 train_time:59969ms step_avg:40.30ms
step:1489/2330 train_time:60004ms step_avg:40.30ms
step:1490/2330 train_time:60048ms step_avg:40.30ms
step:1491/2330 train_time:60083ms step_avg:40.30ms
step:1492/2330 train_time:60128ms step_avg:40.30ms
step:1493/2330 train_time:60163ms step_avg:40.30ms
step:1494/2330 train_time:60207ms step_avg:40.30ms
step:1495/2330 train_time:60243ms step_avg:40.30ms
step:1496/2330 train_time:60287ms step_avg:40.30ms
step:1497/2330 train_time:60323ms step_avg:40.30ms
step:1498/2330 train_time:60367ms step_avg:40.30ms
step:1499/2330 train_time:60404ms step_avg:40.30ms
step:1500/2330 train_time:60449ms step_avg:40.30ms
step:1500/2330 val_loss:5.2015 train_time:60537ms step_avg:40.36ms
step:1501/2330 train_time:60550ms step_avg:40.34ms
step:1502/2330 train_time:60562ms step_avg:40.32ms
step:1503/2330 train_time:60572ms step_avg:40.30ms
step:1504/2330 train_time:60611ms step_avg:40.30ms
step:1505/2330 train_time:60645ms step_avg:40.30ms
step:1506/2330 train_time:60690ms step_avg:40.30ms
step:1507/2330 train_time:60725ms step_avg:40.30ms
step:1508/2330 train_time:60769ms step_avg:40.30ms
step:1509/2330 train_time:60805ms step_avg:40.29ms
step:1510/2330 train_time:60851ms step_avg:40.30ms
step:1511/2330 train_time:60891ms step_avg:40.30ms
step:1512/2330 train_time:60936ms step_avg:40.30ms
step:1513/2330 train_time:60971ms step_avg:40.30ms
step:1514/2330 train_time:61015ms step_avg:40.30ms
step:1515/2330 train_time:61051ms step_avg:40.30ms
step:1516/2330 train_time:61094ms step_avg:40.30ms
step:1517/2330 train_time:61129ms step_avg:40.30ms
step:1518/2330 train_time:61172ms step_avg:40.30ms
step:1519/2330 train_time:61299ms step_avg:40.35ms
step:1520/2330 train_time:61341ms step_avg:40.36ms
step:1521/2330 train_time:61375ms step_avg:40.35ms
step:1522/2330 train_time:61419ms step_avg:40.35ms
step:1523/2330 train_time:61454ms step_avg:40.35ms
step:1524/2330 train_time:61498ms step_avg:40.35ms
step:1525/2330 train_time:61532ms step_avg:40.35ms
step:1526/2330 train_time:61575ms step_avg:40.35ms
step:1527/2330 train_time:61609ms step_avg:40.35ms
step:1528/2330 train_time:61652ms step_avg:40.35ms
step:1529/2330 train_time:61689ms step_avg:40.35ms
step:1530/2330 train_time:61732ms step_avg:40.35ms
step:1531/2330 train_time:61766ms step_avg:40.34ms
step:1532/2330 train_time:61810ms step_avg:40.35ms
step:1533/2330 train_time:61844ms step_avg:40.34ms
step:1534/2330 train_time:61888ms step_avg:40.34ms
step:1535/2330 train_time:61922ms step_avg:40.34ms
step:1536/2330 train_time:61966ms step_avg:40.34ms
step:1537/2330 train_time:62000ms step_avg:40.34ms
step:1538/2330 train_time:62044ms step_avg:40.34ms
step:1539/2330 train_time:62078ms step_avg:40.34ms
step:1540/2330 train_time:62123ms step_avg:40.34ms
step:1541/2330 train_time:62161ms step_avg:40.34ms
step:1542/2330 train_time:62209ms step_avg:40.34ms
step:1543/2330 train_time:62247ms step_avg:40.34ms
step:1544/2330 train_time:62294ms step_avg:40.35ms
step:1545/2330 train_time:62330ms step_avg:40.34ms
step:1546/2330 train_time:62375ms step_avg:40.35ms
step:1547/2330 train_time:62411ms step_avg:40.34ms
step:1548/2330 train_time:62455ms step_avg:40.35ms
step:1549/2330 train_time:62490ms step_avg:40.34ms
step:1550/2330 train_time:62533ms step_avg:40.34ms
step:1551/2330 train_time:62568ms step_avg:40.34ms
step:1552/2330 train_time:62612ms step_avg:40.34ms
step:1553/2330 train_time:62646ms step_avg:40.34ms
step:1554/2330 train_time:62690ms step_avg:40.34ms
step:1555/2330 train_time:62725ms step_avg:40.34ms
step:1556/2330 train_time:62768ms step_avg:40.34ms
step:1557/2330 train_time:62803ms step_avg:40.34ms
step:1558/2330 train_time:62846ms step_avg:40.34ms
step:1559/2330 train_time:62881ms step_avg:40.33ms
step:1560/2330 train_time:62925ms step_avg:40.34ms
step:1561/2330 train_time:62959ms step_avg:40.33ms
step:1562/2330 train_time:63003ms step_avg:40.33ms
step:1563/2330 train_time:63038ms step_avg:40.33ms
step:1564/2330 train_time:63083ms step_avg:40.33ms
step:1565/2330 train_time:63119ms step_avg:40.33ms
step:1566/2330 train_time:63165ms step_avg:40.34ms
step:1567/2330 train_time:63202ms step_avg:40.33ms
step:1568/2330 train_time:63248ms step_avg:40.34ms
step:1569/2330 train_time:63284ms step_avg:40.33ms
step:1570/2330 train_time:63330ms step_avg:40.34ms
step:1571/2330 train_time:63366ms step_avg:40.33ms
step:1572/2330 train_time:63412ms step_avg:40.34ms
step:1573/2330 train_time:63448ms step_avg:40.34ms
step:1574/2330 train_time:63492ms step_avg:40.34ms
step:1575/2330 train_time:63527ms step_avg:40.33ms
step:1576/2330 train_time:63571ms step_avg:40.34ms
step:1577/2330 train_time:63606ms step_avg:40.33ms
step:1578/2330 train_time:63650ms step_avg:40.34ms
step:1579/2330 train_time:63685ms step_avg:40.33ms
step:1580/2330 train_time:63728ms step_avg:40.33ms
step:1581/2330 train_time:63763ms step_avg:40.33ms
step:1582/2330 train_time:63807ms step_avg:40.33ms
step:1583/2330 train_time:63842ms step_avg:40.33ms
step:1584/2330 train_time:63886ms step_avg:40.33ms
step:1585/2330 train_time:63920ms step_avg:40.33ms
step:1586/2330 train_time:63964ms step_avg:40.33ms
step:1587/2330 train_time:63999ms step_avg:40.33ms
step:1588/2330 train_time:64043ms step_avg:40.33ms
step:1589/2330 train_time:64079ms step_avg:40.33ms
step:1590/2330 train_time:64124ms step_avg:40.33ms
step:1591/2330 train_time:64160ms step_avg:40.33ms
step:1592/2330 train_time:64205ms step_avg:40.33ms
step:1593/2330 train_time:64242ms step_avg:40.33ms
step:1594/2330 train_time:64287ms step_avg:40.33ms
step:1595/2330 train_time:64323ms step_avg:40.33ms
step:1596/2330 train_time:64369ms step_avg:40.33ms
step:1597/2330 train_time:64405ms step_avg:40.33ms
step:1598/2330 train_time:64449ms step_avg:40.33ms
step:1599/2330 train_time:64486ms step_avg:40.33ms
step:1600/2330 train_time:64530ms step_avg:40.33ms
step:1601/2330 train_time:64564ms step_avg:40.33ms
step:1602/2330 train_time:64609ms step_avg:40.33ms
step:1603/2330 train_time:64645ms step_avg:40.33ms
step:1604/2330 train_time:64689ms step_avg:40.33ms
step:1605/2330 train_time:64723ms step_avg:40.33ms
step:1606/2330 train_time:64767ms step_avg:40.33ms
step:1607/2330 train_time:64803ms step_avg:40.33ms
step:1608/2330 train_time:64847ms step_avg:40.33ms
step:1609/2330 train_time:64882ms step_avg:40.32ms
step:1610/2330 train_time:64926ms step_avg:40.33ms
step:1611/2330 train_time:64961ms step_avg:40.32ms
step:1612/2330 train_time:65006ms step_avg:40.33ms
step:1613/2330 train_time:65042ms step_avg:40.32ms
step:1614/2330 train_time:65087ms step_avg:40.33ms
step:1615/2330 train_time:65122ms step_avg:40.32ms
step:1616/2330 train_time:65167ms step_avg:40.33ms
step:1617/2330 train_time:65202ms step_avg:40.32ms
step:1618/2330 train_time:65247ms step_avg:40.33ms
step:1619/2330 train_time:65283ms step_avg:40.32ms
step:1620/2330 train_time:65329ms step_avg:40.33ms
step:1621/2330 train_time:65365ms step_avg:40.32ms
step:1622/2330 train_time:65410ms step_avg:40.33ms
step:1623/2330 train_time:65446ms step_avg:40.32ms
step:1624/2330 train_time:65490ms step_avg:40.33ms
step:1625/2330 train_time:65525ms step_avg:40.32ms
step:1626/2330 train_time:65570ms step_avg:40.33ms
step:1627/2330 train_time:65605ms step_avg:40.32ms
step:1628/2330 train_time:65650ms step_avg:40.33ms
step:1629/2330 train_time:65685ms step_avg:40.32ms
step:1630/2330 train_time:65730ms step_avg:40.33ms
step:1631/2330 train_time:65765ms step_avg:40.32ms
step:1632/2330 train_time:65810ms step_avg:40.32ms
step:1633/2330 train_time:65845ms step_avg:40.32ms
step:1634/2330 train_time:65890ms step_avg:40.32ms
step:1635/2330 train_time:65926ms step_avg:40.32ms
step:1636/2330 train_time:65970ms step_avg:40.32ms
step:1637/2330 train_time:66006ms step_avg:40.32ms
step:1638/2330 train_time:66051ms step_avg:40.32ms
step:1639/2330 train_time:66086ms step_avg:40.32ms
step:1640/2330 train_time:66131ms step_avg:40.32ms
step:1641/2330 train_time:66167ms step_avg:40.32ms
step:1642/2330 train_time:66212ms step_avg:40.32ms
step:1643/2330 train_time:66247ms step_avg:40.32ms
step:1644/2330 train_time:66292ms step_avg:40.32ms
step:1645/2330 train_time:66327ms step_avg:40.32ms
step:1646/2330 train_time:66372ms step_avg:40.32ms
step:1647/2330 train_time:66408ms step_avg:40.32ms
step:1648/2330 train_time:66452ms step_avg:40.32ms
step:1649/2330 train_time:66488ms step_avg:40.32ms
step:1650/2330 train_time:66532ms step_avg:40.32ms
step:1651/2330 train_time:66567ms step_avg:40.32ms
step:1652/2330 train_time:66611ms step_avg:40.32ms
step:1653/2330 train_time:66647ms step_avg:40.32ms
step:1654/2330 train_time:66691ms step_avg:40.32ms
step:1655/2330 train_time:66726ms step_avg:40.32ms
step:1656/2330 train_time:66770ms step_avg:40.32ms
step:1657/2330 train_time:66806ms step_avg:40.32ms
step:1658/2330 train_time:66850ms step_avg:40.32ms
step:1659/2330 train_time:66886ms step_avg:40.32ms
step:1660/2330 train_time:66930ms step_avg:40.32ms
step:1661/2330 train_time:66966ms step_avg:40.32ms
step:1662/2330 train_time:67011ms step_avg:40.32ms
step:1663/2330 train_time:67046ms step_avg:40.32ms
step:1664/2330 train_time:67090ms step_avg:40.32ms
step:1665/2330 train_time:67126ms step_avg:40.32ms
step:1666/2330 train_time:67172ms step_avg:40.32ms
step:1667/2330 train_time:67207ms step_avg:40.32ms
step:1668/2330 train_time:67252ms step_avg:40.32ms
step:1669/2330 train_time:67287ms step_avg:40.32ms
step:1670/2330 train_time:67332ms step_avg:40.32ms
step:1671/2330 train_time:67367ms step_avg:40.32ms
step:1672/2330 train_time:67412ms step_avg:40.32ms
step:1673/2330 train_time:67447ms step_avg:40.32ms
step:1674/2330 train_time:67492ms step_avg:40.32ms
step:1675/2330 train_time:67527ms step_avg:40.31ms
step:1676/2330 train_time:67572ms step_avg:40.32ms
step:1677/2330 train_time:67607ms step_avg:40.31ms
step:1678/2330 train_time:67651ms step_avg:40.32ms
step:1679/2330 train_time:67686ms step_avg:40.31ms
step:1680/2330 train_time:67730ms step_avg:40.32ms
step:1681/2330 train_time:67765ms step_avg:40.31ms
step:1682/2330 train_time:67810ms step_avg:40.31ms
step:1683/2330 train_time:67845ms step_avg:40.31ms
step:1684/2330 train_time:67890ms step_avg:40.31ms
step:1685/2330 train_time:67925ms step_avg:40.31ms
step:1686/2330 train_time:67970ms step_avg:40.31ms
step:1687/2330 train_time:68005ms step_avg:40.31ms
step:1688/2330 train_time:68050ms step_avg:40.31ms
step:1689/2330 train_time:68086ms step_avg:40.31ms
step:1690/2330 train_time:68131ms step_avg:40.31ms
step:1691/2330 train_time:68167ms step_avg:40.31ms
step:1692/2330 train_time:68211ms step_avg:40.31ms
step:1693/2330 train_time:68247ms step_avg:40.31ms
step:1694/2330 train_time:68292ms step_avg:40.31ms
step:1695/2330 train_time:68327ms step_avg:40.31ms
step:1696/2330 train_time:68372ms step_avg:40.31ms
step:1697/2330 train_time:68408ms step_avg:40.31ms
step:1698/2330 train_time:68452ms step_avg:40.31ms
step:1699/2330 train_time:68487ms step_avg:40.31ms
step:1700/2330 train_time:68531ms step_avg:40.31ms
step:1701/2330 train_time:68567ms step_avg:40.31ms
step:1702/2330 train_time:68611ms step_avg:40.31ms
step:1703/2330 train_time:68646ms step_avg:40.31ms
step:1704/2330 train_time:68690ms step_avg:40.31ms
step:1705/2330 train_time:68725ms step_avg:40.31ms
step:1706/2330 train_time:68770ms step_avg:40.31ms
step:1707/2330 train_time:68805ms step_avg:40.31ms
step:1708/2330 train_time:68850ms step_avg:40.31ms
step:1709/2330 train_time:68886ms step_avg:40.31ms
step:1710/2330 train_time:68931ms step_avg:40.31ms
step:1711/2330 train_time:68966ms step_avg:40.31ms
step:1712/2330 train_time:69011ms step_avg:40.31ms
step:1713/2330 train_time:69047ms step_avg:40.31ms
step:1714/2330 train_time:69091ms step_avg:40.31ms
step:1715/2330 train_time:69126ms step_avg:40.31ms
step:1716/2330 train_time:69171ms step_avg:40.31ms
step:1717/2330 train_time:69207ms step_avg:40.31ms
step:1718/2330 train_time:69251ms step_avg:40.31ms
step:1719/2330 train_time:69287ms step_avg:40.31ms
step:1720/2330 train_time:69331ms step_avg:40.31ms
step:1721/2330 train_time:69367ms step_avg:40.31ms
step:1722/2330 train_time:69411ms step_avg:40.31ms
step:1723/2330 train_time:69446ms step_avg:40.31ms
step:1724/2330 train_time:69491ms step_avg:40.31ms
step:1725/2330 train_time:69526ms step_avg:40.30ms
step:1726/2330 train_time:69570ms step_avg:40.31ms
step:1727/2330 train_time:69605ms step_avg:40.30ms
step:1728/2330 train_time:69649ms step_avg:40.31ms
step:1729/2330 train_time:69685ms step_avg:40.30ms
step:1730/2330 train_time:69729ms step_avg:40.31ms
step:1731/2330 train_time:69764ms step_avg:40.30ms
step:1732/2330 train_time:69808ms step_avg:40.30ms
step:1733/2330 train_time:69843ms step_avg:40.30ms
step:1734/2330 train_time:69888ms step_avg:40.30ms
step:1735/2330 train_time:69923ms step_avg:40.30ms
step:1736/2330 train_time:69967ms step_avg:40.30ms
step:1737/2330 train_time:70003ms step_avg:40.30ms
step:1738/2330 train_time:70048ms step_avg:40.30ms
step:1739/2330 train_time:70084ms step_avg:40.30ms
step:1740/2330 train_time:70129ms step_avg:40.30ms
step:1741/2330 train_time:70163ms step_avg:40.30ms
step:1742/2330 train_time:70208ms step_avg:40.30ms
step:1743/2330 train_time:70244ms step_avg:40.30ms
step:1744/2330 train_time:70289ms step_avg:40.30ms
step:1745/2330 train_time:70323ms step_avg:40.30ms
step:1746/2330 train_time:70368ms step_avg:40.30ms
step:1747/2330 train_time:70403ms step_avg:40.30ms
step:1748/2330 train_time:70448ms step_avg:40.30ms
step:1749/2330 train_time:70483ms step_avg:40.30ms
step:1750/2330 train_time:70527ms step_avg:40.30ms
step:1750/2330 val_loss:5.1806 train_time:70614ms step_avg:40.35ms
step:1751/2330 train_time:70627ms step_avg:40.34ms
step:1752/2330 train_time:70640ms step_avg:40.32ms
step:1753/2330 train_time:70650ms step_avg:40.30ms
step:1754/2330 train_time:70687ms step_avg:40.30ms
step:1755/2330 train_time:70721ms step_avg:40.30ms
step:1756/2330 train_time:70765ms step_avg:40.30ms
step:1757/2330 train_time:70800ms step_avg:40.30ms
step:1758/2330 train_time:70843ms step_avg:40.30ms
step:1759/2330 train_time:70878ms step_avg:40.29ms
step:1760/2330 train_time:70923ms step_avg:40.30ms
step:1761/2330 train_time:70962ms step_avg:40.30ms
step:1762/2330 train_time:71009ms step_avg:40.30ms
step:1763/2330 train_time:71047ms step_avg:40.30ms
step:1764/2330 train_time:71092ms step_avg:40.30ms
step:1765/2330 train_time:71128ms step_avg:40.30ms
step:1766/2330 train_time:71172ms step_avg:40.30ms
step:1767/2330 train_time:71207ms step_avg:40.30ms
step:1768/2330 train_time:71252ms step_avg:40.30ms
step:1769/2330 train_time:71286ms step_avg:40.30ms
step:1770/2330 train_time:71330ms step_avg:40.30ms
step:1771/2330 train_time:71365ms step_avg:40.30ms
step:1772/2330 train_time:71409ms step_avg:40.30ms
step:1773/2330 train_time:71444ms step_avg:40.30ms
step:1774/2330 train_time:71489ms step_avg:40.30ms
step:1775/2330 train_time:71527ms step_avg:40.30ms
step:1776/2330 train_time:71576ms step_avg:40.30ms
step:1777/2330 train_time:71615ms step_avg:40.30ms
step:1778/2330 train_time:71661ms step_avg:40.30ms
step:1779/2330 train_time:71696ms step_avg:40.30ms
step:1780/2330 train_time:71740ms step_avg:40.30ms
step:1781/2330 train_time:71775ms step_avg:40.30ms
step:1782/2330 train_time:71819ms step_avg:40.30ms
step:1783/2330 train_time:71854ms step_avg:40.30ms
step:1784/2330 train_time:71898ms step_avg:40.30ms
step:1785/2330 train_time:71933ms step_avg:40.30ms
step:1786/2330 train_time:71977ms step_avg:40.30ms
step:1787/2330 train_time:72013ms step_avg:40.30ms
step:1788/2330 train_time:72058ms step_avg:40.30ms
step:1789/2330 train_time:72093ms step_avg:40.30ms
step:1790/2330 train_time:72138ms step_avg:40.30ms
step:1791/2330 train_time:72172ms step_avg:40.30ms
step:1792/2330 train_time:72216ms step_avg:40.30ms
step:1793/2330 train_time:72250ms step_avg:40.30ms
step:1794/2330 train_time:72294ms step_avg:40.30ms
step:1795/2330 train_time:72329ms step_avg:40.29ms
step:1796/2330 train_time:72373ms step_avg:40.30ms
step:1797/2330 train_time:72407ms step_avg:40.29ms
step:1798/2330 train_time:72452ms step_avg:40.30ms
step:1799/2330 train_time:72488ms step_avg:40.29ms
step:1800/2330 train_time:72533ms step_avg:40.30ms
step:1801/2330 train_time:72570ms step_avg:40.29ms
step:1802/2330 train_time:72616ms step_avg:40.30ms
step:1803/2330 train_time:72652ms step_avg:40.29ms
step:1804/2330 train_time:72696ms step_avg:40.30ms
step:1805/2330 train_time:72732ms step_avg:40.29ms
step:1806/2330 train_time:72776ms step_avg:40.30ms
step:1807/2330 train_time:72811ms step_avg:40.29ms
step:1808/2330 train_time:72856ms step_avg:40.30ms
step:1809/2330 train_time:72891ms step_avg:40.29ms
step:1810/2330 train_time:72934ms step_avg:40.30ms
step:1811/2330 train_time:72970ms step_avg:40.29ms
step:1812/2330 train_time:73014ms step_avg:40.29ms
step:1813/2330 train_time:73049ms step_avg:40.29ms
step:1814/2330 train_time:73093ms step_avg:40.29ms
step:1815/2330 train_time:73128ms step_avg:40.29ms
step:1816/2330 train_time:73173ms step_avg:40.29ms
step:1817/2330 train_time:73207ms step_avg:40.29ms
step:1818/2330 train_time:73252ms step_avg:40.29ms
step:1819/2330 train_time:73286ms step_avg:40.29ms
step:1820/2330 train_time:73331ms step_avg:40.29ms
step:1821/2330 train_time:73366ms step_avg:40.29ms
step:1822/2330 train_time:73411ms step_avg:40.29ms
step:1823/2330 train_time:73447ms step_avg:40.29ms
step:1824/2330 train_time:73492ms step_avg:40.29ms
step:1825/2330 train_time:73529ms step_avg:40.29ms
step:1826/2330 train_time:73574ms step_avg:40.29ms
step:1827/2330 train_time:73610ms step_avg:40.29ms
step:1828/2330 train_time:73655ms step_avg:40.29ms
step:1829/2330 train_time:73690ms step_avg:40.29ms
step:1830/2330 train_time:73734ms step_avg:40.29ms
step:1831/2330 train_time:73771ms step_avg:40.29ms
step:1832/2330 train_time:73815ms step_avg:40.29ms
step:1833/2330 train_time:73851ms step_avg:40.29ms
step:1834/2330 train_time:73895ms step_avg:40.29ms
step:1835/2330 train_time:73930ms step_avg:40.29ms
step:1836/2330 train_time:73975ms step_avg:40.29ms
step:1837/2330 train_time:74010ms step_avg:40.29ms
step:1838/2330 train_time:74054ms step_avg:40.29ms
step:1839/2330 train_time:74089ms step_avg:40.29ms
step:1840/2330 train_time:74134ms step_avg:40.29ms
step:1841/2330 train_time:74169ms step_avg:40.29ms
step:1842/2330 train_time:74213ms step_avg:40.29ms
step:1843/2330 train_time:74248ms step_avg:40.29ms
step:1844/2330 train_time:74292ms step_avg:40.29ms
step:1845/2330 train_time:74327ms step_avg:40.29ms
step:1846/2330 train_time:74371ms step_avg:40.29ms
step:1847/2330 train_time:74407ms step_avg:40.29ms
step:1848/2330 train_time:74452ms step_avg:40.29ms
step:1849/2330 train_time:74487ms step_avg:40.29ms
step:1850/2330 train_time:74533ms step_avg:40.29ms
step:1851/2330 train_time:74569ms step_avg:40.29ms
step:1852/2330 train_time:74614ms step_avg:40.29ms
step:1853/2330 train_time:74649ms step_avg:40.29ms
step:1854/2330 train_time:74694ms step_avg:40.29ms
step:1855/2330 train_time:74730ms step_avg:40.29ms
step:1856/2330 train_time:74775ms step_avg:40.29ms
step:1857/2330 train_time:74811ms step_avg:40.29ms
step:1858/2330 train_time:74856ms step_avg:40.29ms
step:1859/2330 train_time:74891ms step_avg:40.29ms
step:1860/2330 train_time:74935ms step_avg:40.29ms
step:1861/2330 train_time:74971ms step_avg:40.29ms
step:1862/2330 train_time:75015ms step_avg:40.29ms
step:1863/2330 train_time:75050ms step_avg:40.28ms
step:1864/2330 train_time:75094ms step_avg:40.29ms
step:1865/2330 train_time:75130ms step_avg:40.28ms
step:1866/2330 train_time:75174ms step_avg:40.29ms
step:1867/2330 train_time:75209ms step_avg:40.28ms
step:1868/2330 train_time:75253ms step_avg:40.29ms
step:1869/2330 train_time:75288ms step_avg:40.28ms
step:1870/2330 train_time:75332ms step_avg:40.28ms
step:1871/2330 train_time:75368ms step_avg:40.28ms
step:1872/2330 train_time:75412ms step_avg:40.28ms
step:1873/2330 train_time:75448ms step_avg:40.28ms
step:1874/2330 train_time:75492ms step_avg:40.28ms
step:1875/2330 train_time:75528ms step_avg:40.28ms
step:1876/2330 train_time:75573ms step_avg:40.28ms
step:1877/2330 train_time:75609ms step_avg:40.28ms
step:1878/2330 train_time:75654ms step_avg:40.28ms
step:1879/2330 train_time:75690ms step_avg:40.28ms
step:1880/2330 train_time:75734ms step_avg:40.28ms
step:1881/2330 train_time:75770ms step_avg:40.28ms
step:1882/2330 train_time:75815ms step_avg:40.28ms
step:1883/2330 train_time:75850ms step_avg:40.28ms
step:1884/2330 train_time:75894ms step_avg:40.28ms
step:1885/2330 train_time:75929ms step_avg:40.28ms
step:1886/2330 train_time:75974ms step_avg:40.28ms
step:1887/2330 train_time:76009ms step_avg:40.28ms
step:1888/2330 train_time:76054ms step_avg:40.28ms
step:1889/2330 train_time:76088ms step_avg:40.28ms
step:1890/2330 train_time:76133ms step_avg:40.28ms
step:1891/2330 train_time:76168ms step_avg:40.28ms
step:1892/2330 train_time:76212ms step_avg:40.28ms
step:1893/2330 train_time:76247ms step_avg:40.28ms
step:1894/2330 train_time:76292ms step_avg:40.28ms
step:1895/2330 train_time:76327ms step_avg:40.28ms
step:1896/2330 train_time:76371ms step_avg:40.28ms
step:1897/2330 train_time:76407ms step_avg:40.28ms
step:1898/2330 train_time:76451ms step_avg:40.28ms
step:1899/2330 train_time:76487ms step_avg:40.28ms
step:1900/2330 train_time:76531ms step_avg:40.28ms
step:1901/2330 train_time:76567ms step_avg:40.28ms
step:1902/2330 train_time:76612ms step_avg:40.28ms
step:1903/2330 train_time:76647ms step_avg:40.28ms
step:1904/2330 train_time:76692ms step_avg:40.28ms
step:1905/2330 train_time:76729ms step_avg:40.28ms
step:1906/2330 train_time:76774ms step_avg:40.28ms
step:1907/2330 train_time:76810ms step_avg:40.28ms
step:1908/2330 train_time:76854ms step_avg:40.28ms
step:1909/2330 train_time:76890ms step_avg:40.28ms
step:1910/2330 train_time:76934ms step_avg:40.28ms
step:1911/2330 train_time:76970ms step_avg:40.28ms
step:1912/2330 train_time:77014ms step_avg:40.28ms
step:1913/2330 train_time:77049ms step_avg:40.28ms
step:1914/2330 train_time:77093ms step_avg:40.28ms
step:1915/2330 train_time:77128ms step_avg:40.28ms
step:1916/2330 train_time:77173ms step_avg:40.28ms
step:1917/2330 train_time:77208ms step_avg:40.28ms
step:1918/2330 train_time:77253ms step_avg:40.28ms
step:1919/2330 train_time:77288ms step_avg:40.27ms
step:1920/2330 train_time:77332ms step_avg:40.28ms
step:1921/2330 train_time:77367ms step_avg:40.27ms
step:1922/2330 train_time:77412ms step_avg:40.28ms
step:1923/2330 train_time:77447ms step_avg:40.27ms
step:1924/2330 train_time:77492ms step_avg:40.28ms
step:1925/2330 train_time:77527ms step_avg:40.27ms
step:1926/2330 train_time:77572ms step_avg:40.28ms
step:1927/2330 train_time:77608ms step_avg:40.27ms
step:1928/2330 train_time:77653ms step_avg:40.28ms
step:1929/2330 train_time:77690ms step_avg:40.27ms
step:1930/2330 train_time:77734ms step_avg:40.28ms
step:1931/2330 train_time:77769ms step_avg:40.27ms
step:1932/2330 train_time:77814ms step_avg:40.28ms
step:1933/2330 train_time:77850ms step_avg:40.27ms
step:1934/2330 train_time:77894ms step_avg:40.28ms
step:1935/2330 train_time:77930ms step_avg:40.27ms
step:1936/2330 train_time:77974ms step_avg:40.28ms
step:1937/2330 train_time:78010ms step_avg:40.27ms
step:1938/2330 train_time:78054ms step_avg:40.28ms
step:1939/2330 train_time:78089ms step_avg:40.27ms
step:1940/2330 train_time:78133ms step_avg:40.27ms
step:1941/2330 train_time:78168ms step_avg:40.27ms
step:1942/2330 train_time:78212ms step_avg:40.27ms
step:1943/2330 train_time:78248ms step_avg:40.27ms
step:1944/2330 train_time:78292ms step_avg:40.27ms
step:1945/2330 train_time:78327ms step_avg:40.27ms
step:1946/2330 train_time:78371ms step_avg:40.27ms
step:1947/2330 train_time:78406ms step_avg:40.27ms
step:1948/2330 train_time:78451ms step_avg:40.27ms
step:1949/2330 train_time:78486ms step_avg:40.27ms
step:1950/2330 train_time:78531ms step_avg:40.27ms
step:1951/2330 train_time:78567ms step_avg:40.27ms
step:1952/2330 train_time:78613ms step_avg:40.27ms
step:1953/2330 train_time:78649ms step_avg:40.27ms
step:1954/2330 train_time:78693ms step_avg:40.27ms
step:1955/2330 train_time:78728ms step_avg:40.27ms
step:1956/2330 train_time:78773ms step_avg:40.27ms
step:1957/2330 train_time:78809ms step_avg:40.27ms
step:1958/2330 train_time:78854ms step_avg:40.27ms
step:1959/2330 train_time:78889ms step_avg:40.27ms
step:1960/2330 train_time:78933ms step_avg:40.27ms
step:1961/2330 train_time:78970ms step_avg:40.27ms
step:1962/2330 train_time:79015ms step_avg:40.27ms
step:1963/2330 train_time:79050ms step_avg:40.27ms
step:1964/2330 train_time:79094ms step_avg:40.27ms
step:1965/2330 train_time:79129ms step_avg:40.27ms
step:1966/2330 train_time:79174ms step_avg:40.27ms
step:1967/2330 train_time:79209ms step_avg:40.27ms
step:1968/2330 train_time:79253ms step_avg:40.27ms
step:1969/2330 train_time:79288ms step_avg:40.27ms
step:1970/2330 train_time:79333ms step_avg:40.27ms
step:1971/2330 train_time:79369ms step_avg:40.27ms
step:1972/2330 train_time:79413ms step_avg:40.27ms
step:1973/2330 train_time:79448ms step_avg:40.27ms
step:1974/2330 train_time:79492ms step_avg:40.27ms
step:1975/2330 train_time:79528ms step_avg:40.27ms
step:1976/2330 train_time:79573ms step_avg:40.27ms
step:1977/2330 train_time:79608ms step_avg:40.27ms
step:1978/2330 train_time:79653ms step_avg:40.27ms
step:1979/2330 train_time:79688ms step_avg:40.27ms
step:1980/2330 train_time:79733ms step_avg:40.27ms
step:1981/2330 train_time:79769ms step_avg:40.27ms
step:1982/2330 train_time:79814ms step_avg:40.27ms
step:1983/2330 train_time:79850ms step_avg:40.27ms
step:1984/2330 train_time:79894ms step_avg:40.27ms
step:1985/2330 train_time:79930ms step_avg:40.27ms
step:1986/2330 train_time:79975ms step_avg:40.27ms
step:1987/2330 train_time:80010ms step_avg:40.27ms
step:1988/2330 train_time:80054ms step_avg:40.27ms
step:1989/2330 train_time:80089ms step_avg:40.27ms
step:1990/2330 train_time:80134ms step_avg:40.27ms
step:1991/2330 train_time:80169ms step_avg:40.27ms
step:1992/2330 train_time:80213ms step_avg:40.27ms
step:1993/2330 train_time:80248ms step_avg:40.27ms
step:1994/2330 train_time:80293ms step_avg:40.27ms
step:1995/2330 train_time:80328ms step_avg:40.26ms
step:1996/2330 train_time:80373ms step_avg:40.27ms
step:1997/2330 train_time:80408ms step_avg:40.26ms
step:1998/2330 train_time:80452ms step_avg:40.27ms
step:1999/2330 train_time:80488ms step_avg:40.26ms
step:2000/2330 train_time:80532ms step_avg:40.27ms
step:2000/2330 val_loss:5.1463 train_time:80620ms step_avg:40.31ms
step:2001/2330 train_time:80633ms step_avg:40.30ms
step:2002/2330 train_time:80645ms step_avg:40.28ms
step:2003/2330 train_time:80655ms step_avg:40.27ms
step:2004/2330 train_time:80694ms step_avg:40.27ms
step:2005/2330 train_time:80729ms step_avg:40.26ms
step:2006/2330 train_time:80772ms step_avg:40.27ms
step:2007/2330 train_time:80808ms step_avg:40.26ms
step:2008/2330 train_time:80851ms step_avg:40.26ms
step:2009/2330 train_time:80886ms step_avg:40.26ms
step:2010/2330 train_time:80934ms step_avg:40.27ms
step:2011/2330 train_time:80972ms step_avg:40.26ms
step:2012/2330 train_time:81021ms step_avg:40.27ms
step:2013/2330 train_time:81057ms step_avg:40.27ms
step:2014/2330 train_time:81102ms step_avg:40.27ms
step:2015/2330 train_time:81137ms step_avg:40.27ms
step:2016/2330 train_time:81181ms step_avg:40.27ms
step:2017/2330 train_time:81216ms step_avg:40.27ms
step:2018/2330 train_time:81260ms step_avg:40.27ms
step:2019/2330 train_time:81295ms step_avg:40.27ms
step:2020/2330 train_time:81340ms step_avg:40.27ms
step:2021/2330 train_time:81374ms step_avg:40.26ms
step:2022/2330 train_time:81418ms step_avg:40.27ms
step:2023/2330 train_time:81453ms step_avg:40.26ms
step:2024/2330 train_time:81497ms step_avg:40.27ms
step:2025/2330 train_time:81532ms step_avg:40.26ms
step:2026/2330 train_time:81577ms step_avg:40.27ms
step:2027/2330 train_time:81613ms step_avg:40.26ms
step:2028/2330 train_time:81657ms step_avg:40.27ms
step:2029/2330 train_time:81692ms step_avg:40.26ms
step:2030/2330 train_time:81737ms step_avg:40.26ms
step:2031/2330 train_time:81772ms step_avg:40.26ms
step:2032/2330 train_time:81817ms step_avg:40.26ms
step:2033/2330 train_time:81852ms step_avg:40.26ms
step:2034/2330 train_time:81898ms step_avg:40.26ms
step:2035/2330 train_time:81934ms step_avg:40.26ms
step:2036/2330 train_time:81980ms step_avg:40.27ms
step:2037/2330 train_time:82017ms step_avg:40.26ms
step:2038/2330 train_time:82062ms step_avg:40.27ms
step:2039/2330 train_time:82099ms step_avg:40.26ms
step:2040/2330 train_time:82144ms step_avg:40.27ms
step:2041/2330 train_time:82179ms step_avg:40.26ms
step:2042/2330 train_time:82223ms step_avg:40.27ms
step:2043/2330 train_time:82258ms step_avg:40.26ms
step:2044/2330 train_time:82302ms step_avg:40.27ms
step:2045/2330 train_time:82337ms step_avg:40.26ms
step:2046/2330 train_time:82381ms step_avg:40.26ms
step:2047/2330 train_time:82416ms step_avg:40.26ms
step:2048/2330 train_time:82460ms step_avg:40.26ms
step:2049/2330 train_time:82495ms step_avg:40.26ms
step:2050/2330 train_time:82540ms step_avg:40.26ms
step:2051/2330 train_time:82575ms step_avg:40.26ms
step:2052/2330 train_time:82619ms step_avg:40.26ms
step:2053/2330 train_time:82654ms step_avg:40.26ms
step:2054/2330 train_time:82698ms step_avg:40.26ms
step:2055/2330 train_time:82734ms step_avg:40.26ms
step:2056/2330 train_time:82778ms step_avg:40.26ms
step:2057/2330 train_time:82814ms step_avg:40.26ms
step:2058/2330 train_time:82859ms step_avg:40.26ms
step:2059/2330 train_time:82894ms step_avg:40.26ms
step:2060/2330 train_time:82939ms step_avg:40.26ms
step:2061/2330 train_time:82975ms step_avg:40.26ms
step:2062/2330 train_time:83021ms step_avg:40.26ms
step:2063/2330 train_time:83057ms step_avg:40.26ms
step:2064/2330 train_time:83102ms step_avg:40.26ms
step:2065/2330 train_time:83138ms step_avg:40.26ms
step:2066/2330 train_time:83182ms step_avg:40.26ms
step:2067/2330 train_time:83217ms step_avg:40.26ms
step:2068/2330 train_time:83262ms step_avg:40.26ms
step:2069/2330 train_time:83297ms step_avg:40.26ms
step:2070/2330 train_time:83341ms step_avg:40.26ms
step:2071/2330 train_time:83376ms step_avg:40.26ms
step:2072/2330 train_time:83420ms step_avg:40.26ms
step:2073/2330 train_time:83455ms step_avg:40.26ms
step:2074/2330 train_time:83499ms step_avg:40.26ms
step:2075/2330 train_time:83535ms step_avg:40.26ms
step:2076/2330 train_time:83579ms step_avg:40.26ms
step:2077/2330 train_time:83614ms step_avg:40.26ms
step:2078/2330 train_time:83658ms step_avg:40.26ms
step:2079/2330 train_time:83693ms step_avg:40.26ms
step:2080/2330 train_time:83738ms step_avg:40.26ms
step:2081/2330 train_time:83773ms step_avg:40.26ms
step:2082/2330 train_time:83819ms step_avg:40.26ms
step:2083/2330 train_time:83854ms step_avg:40.26ms
step:2084/2330 train_time:83899ms step_avg:40.26ms
step:2085/2330 train_time:83935ms step_avg:40.26ms
step:2086/2330 train_time:83981ms step_avg:40.26ms
step:2087/2330 train_time:84017ms step_avg:40.26ms
step:2088/2330 train_time:84062ms step_avg:40.26ms
step:2089/2330 train_time:84097ms step_avg:40.26ms
step:2090/2330 train_time:84142ms step_avg:40.26ms
step:2091/2330 train_time:84178ms step_avg:40.26ms
step:2092/2330 train_time:84221ms step_avg:40.26ms
step:2093/2330 train_time:84256ms step_avg:40.26ms
step:2094/2330 train_time:84301ms step_avg:40.26ms
step:2095/2330 train_time:84336ms step_avg:40.26ms
step:2096/2330 train_time:84380ms step_avg:40.26ms
step:2097/2330 train_time:84416ms step_avg:40.26ms
step:2098/2330 train_time:84460ms step_avg:40.26ms
step:2099/2330 train_time:84495ms step_avg:40.25ms
step:2100/2330 train_time:84539ms step_avg:40.26ms
step:2101/2330 train_time:84574ms step_avg:40.25ms
step:2102/2330 train_time:84618ms step_avg:40.26ms
step:2103/2330 train_time:84652ms step_avg:40.25ms
step:2104/2330 train_time:84696ms step_avg:40.25ms
step:2105/2330 train_time:84731ms step_avg:40.25ms
step:2106/2330 train_time:84776ms step_avg:40.25ms
step:2107/2330 train_time:84812ms step_avg:40.25ms
step:2108/2330 train_time:84857ms step_avg:40.25ms
step:2109/2330 train_time:84893ms step_avg:40.25ms
step:2110/2330 train_time:84938ms step_avg:40.26ms
step:2111/2330 train_time:84974ms step_avg:40.25ms
step:2112/2330 train_time:85020ms step_avg:40.26ms
step:2113/2330 train_time:85056ms step_avg:40.25ms
step:2114/2330 train_time:85101ms step_avg:40.26ms
step:2115/2330 train_time:85137ms step_avg:40.25ms
step:2116/2330 train_time:85182ms step_avg:40.26ms
step:2117/2330 train_time:85217ms step_avg:40.25ms
step:2118/2330 train_time:85261ms step_avg:40.26ms
step:2119/2330 train_time:85296ms step_avg:40.25ms
step:2120/2330 train_time:85340ms step_avg:40.25ms
step:2121/2330 train_time:85376ms step_avg:40.25ms
step:2122/2330 train_time:85420ms step_avg:40.25ms
step:2123/2330 train_time:85455ms step_avg:40.25ms
step:2124/2330 train_time:85499ms step_avg:40.25ms
step:2125/2330 train_time:85535ms step_avg:40.25ms
step:2126/2330 train_time:85579ms step_avg:40.25ms
step:2127/2330 train_time:85614ms step_avg:40.25ms
step:2128/2330 train_time:85658ms step_avg:40.25ms
step:2129/2330 train_time:85692ms step_avg:40.25ms
step:2130/2330 train_time:85737ms step_avg:40.25ms
step:2131/2330 train_time:85773ms step_avg:40.25ms
step:2132/2330 train_time:85818ms step_avg:40.25ms
step:2133/2330 train_time:85855ms step_avg:40.25ms
step:2134/2330 train_time:85900ms step_avg:40.25ms
step:2135/2330 train_time:85936ms step_avg:40.25ms
step:2136/2330 train_time:85981ms step_avg:40.25ms
step:2137/2330 train_time:86017ms step_avg:40.25ms
step:2138/2330 train_time:86062ms step_avg:40.25ms
step:2139/2330 train_time:86098ms step_avg:40.25ms
step:2140/2330 train_time:86142ms step_avg:40.25ms
step:2141/2330 train_time:86178ms step_avg:40.25ms
step:2142/2330 train_time:86222ms step_avg:40.25ms
step:2143/2330 train_time:86257ms step_avg:40.25ms
step:2144/2330 train_time:86301ms step_avg:40.25ms
step:2145/2330 train_time:86337ms step_avg:40.25ms
step:2146/2330 train_time:86381ms step_avg:40.25ms
step:2147/2330 train_time:86415ms step_avg:40.25ms
step:2148/2330 train_time:86459ms step_avg:40.25ms
step:2149/2330 train_time:86495ms step_avg:40.25ms
step:2150/2330 train_time:86539ms step_avg:40.25ms
step:2151/2330 train_time:86575ms step_avg:40.25ms
step:2152/2330 train_time:86620ms step_avg:40.25ms
step:2153/2330 train_time:86655ms step_avg:40.25ms
step:2154/2330 train_time:86699ms step_avg:40.25ms
step:2155/2330 train_time:86734ms step_avg:40.25ms
step:2156/2330 train_time:86779ms step_avg:40.25ms
step:2157/2330 train_time:86815ms step_avg:40.25ms
step:2158/2330 train_time:86859ms step_avg:40.25ms
step:2159/2330 train_time:86895ms step_avg:40.25ms
step:2160/2330 train_time:86940ms step_avg:40.25ms
step:2161/2330 train_time:86977ms step_avg:40.25ms
step:2162/2330 train_time:87022ms step_avg:40.25ms
step:2163/2330 train_time:87057ms step_avg:40.25ms
step:2164/2330 train_time:87101ms step_avg:40.25ms
step:2165/2330 train_time:87136ms step_avg:40.25ms
step:2166/2330 train_time:87181ms step_avg:40.25ms
step:2167/2330 train_time:87217ms step_avg:40.25ms
step:2168/2330 train_time:87261ms step_avg:40.25ms
step:2169/2330 train_time:87296ms step_avg:40.25ms
step:2170/2330 train_time:87341ms step_avg:40.25ms
step:2171/2330 train_time:87376ms step_avg:40.25ms
step:2172/2330 train_time:87420ms step_avg:40.25ms
step:2173/2330 train_time:87455ms step_avg:40.25ms
step:2174/2330 train_time:87500ms step_avg:40.25ms
step:2175/2330 train_time:87536ms step_avg:40.25ms
step:2176/2330 train_time:87580ms step_avg:40.25ms
step:2177/2330 train_time:87616ms step_avg:40.25ms
step:2178/2330 train_time:87660ms step_avg:40.25ms
step:2179/2330 train_time:87696ms step_avg:40.25ms
step:2180/2330 train_time:87740ms step_avg:40.25ms
step:2181/2330 train_time:87776ms step_avg:40.25ms
step:2182/2330 train_time:87820ms step_avg:40.25ms
step:2183/2330 train_time:87856ms step_avg:40.25ms
step:2184/2330 train_time:87901ms step_avg:40.25ms
step:2185/2330 train_time:87936ms step_avg:40.25ms
step:2186/2330 train_time:87981ms step_avg:40.25ms
step:2187/2330 train_time:88017ms step_avg:40.25ms
step:2188/2330 train_time:88061ms step_avg:40.25ms
step:2189/2330 train_time:88097ms step_avg:40.25ms
step:2190/2330 train_time:88142ms step_avg:40.25ms
step:2191/2330 train_time:88177ms step_avg:40.25ms
step:2192/2330 train_time:88222ms step_avg:40.25ms
step:2193/2330 train_time:88257ms step_avg:40.24ms
step:2194/2330 train_time:88301ms step_avg:40.25ms
step:2195/2330 train_time:88337ms step_avg:40.24ms
step:2196/2330 train_time:88381ms step_avg:40.25ms
step:2197/2330 train_time:88416ms step_avg:40.24ms
step:2198/2330 train_time:88460ms step_avg:40.25ms
step:2199/2330 train_time:88495ms step_avg:40.24ms
step:2200/2330 train_time:88540ms step_avg:40.25ms
step:2201/2330 train_time:88575ms step_avg:40.24ms
step:2202/2330 train_time:88620ms step_avg:40.25ms
step:2203/2330 train_time:88655ms step_avg:40.24ms
step:2204/2330 train_time:88699ms step_avg:40.24ms
step:2205/2330 train_time:88735ms step_avg:40.24ms
step:2206/2330 train_time:88779ms step_avg:40.24ms
step:2207/2330 train_time:88815ms step_avg:40.24ms
step:2208/2330 train_time:88859ms step_avg:40.24ms
step:2209/2330 train_time:88895ms step_avg:40.24ms
step:2210/2330 train_time:88939ms step_avg:40.24ms
step:2211/2330 train_time:88975ms step_avg:40.24ms
step:2212/2330 train_time:89020ms step_avg:40.24ms
step:2213/2330 train_time:89056ms step_avg:40.24ms
step:2214/2330 train_time:89100ms step_avg:40.24ms
step:2215/2330 train_time:89136ms step_avg:40.24ms
step:2216/2330 train_time:89181ms step_avg:40.24ms
step:2217/2330 train_time:89216ms step_avg:40.24ms
step:2218/2330 train_time:89261ms step_avg:40.24ms
step:2219/2330 train_time:89296ms step_avg:40.24ms
step:2220/2330 train_time:89341ms step_avg:40.24ms
step:2221/2330 train_time:89376ms step_avg:40.24ms
step:2222/2330 train_time:89420ms step_avg:40.24ms
step:2223/2330 train_time:89455ms step_avg:40.24ms
step:2224/2330 train_time:89499ms step_avg:40.24ms
step:2225/2330 train_time:89535ms step_avg:40.24ms
step:2226/2330 train_time:89579ms step_avg:40.24ms
step:2227/2330 train_time:89614ms step_avg:40.24ms
step:2228/2330 train_time:89658ms step_avg:40.24ms
step:2229/2330 train_time:89694ms step_avg:40.24ms
step:2230/2330 train_time:89739ms step_avg:40.24ms
step:2231/2330 train_time:89774ms step_avg:40.24ms
step:2232/2330 train_time:89819ms step_avg:40.24ms
step:2233/2330 train_time:89854ms step_avg:40.24ms
step:2234/2330 train_time:89899ms step_avg:40.24ms
step:2235/2330 train_time:89935ms step_avg:40.24ms
step:2236/2330 train_time:89980ms step_avg:40.24ms
step:2237/2330 train_time:90016ms step_avg:40.24ms
step:2238/2330 train_time:90060ms step_avg:40.24ms
step:2239/2330 train_time:90096ms step_avg:40.24ms
step:2240/2330 train_time:90141ms step_avg:40.24ms
step:2241/2330 train_time:90177ms step_avg:40.24ms
step:2242/2330 train_time:90221ms step_avg:40.24ms
step:2243/2330 train_time:90257ms step_avg:40.24ms
step:2244/2330 train_time:90301ms step_avg:40.24ms
step:2245/2330 train_time:90337ms step_avg:40.24ms
step:2246/2330 train_time:90381ms step_avg:40.24ms
step:2247/2330 train_time:90416ms step_avg:40.24ms
step:2248/2330 train_time:90460ms step_avg:40.24ms
step:2249/2330 train_time:90495ms step_avg:40.24ms
step:2250/2330 train_time:90540ms step_avg:40.24ms
step:2250/2330 val_loss:5.1368 train_time:90627ms step_avg:40.28ms
step:2251/2330 train_time:90640ms step_avg:40.27ms
step:2252/2330 train_time:90652ms step_avg:40.25ms
step:2253/2330 train_time:90663ms step_avg:40.24ms
step:2254/2330 train_time:90701ms step_avg:40.24ms
step:2255/2330 train_time:90734ms step_avg:40.24ms
step:2256/2330 train_time:90778ms step_avg:40.24ms
step:2257/2330 train_time:90813ms step_avg:40.24ms
step:2258/2330 train_time:90856ms step_avg:40.24ms
step:2259/2330 train_time:90891ms step_avg:40.24ms
step:2260/2330 train_time:90935ms step_avg:40.24ms
step:2261/2330 train_time:90976ms step_avg:40.24ms
step:2262/2330 train_time:91024ms step_avg:40.24ms
step:2263/2330 train_time:91061ms step_avg:40.24ms
step:2264/2330 train_time:91107ms step_avg:40.24ms
step:2265/2330 train_time:91143ms step_avg:40.24ms
step:2266/2330 train_time:91187ms step_avg:40.24ms
step:2267/2330 train_time:91222ms step_avg:40.24ms
step:2268/2330 train_time:91268ms step_avg:40.24ms
step:2269/2330 train_time:91302ms step_avg:40.24ms
step:2270/2330 train_time:91346ms step_avg:40.24ms
step:2271/2330 train_time:91380ms step_avg:40.24ms
step:2272/2330 train_time:91424ms step_avg:40.24ms
step:2273/2330 train_time:91459ms step_avg:40.24ms
step:2274/2330 train_time:91503ms step_avg:40.24ms
step:2275/2330 train_time:91538ms step_avg:40.24ms
step:2276/2330 train_time:91584ms step_avg:40.24ms
step:2277/2330 train_time:91619ms step_avg:40.24ms
step:2278/2330 train_time:91664ms step_avg:40.24ms
step:2279/2330 train_time:91699ms step_avg:40.24ms
step:2280/2330 train_time:91743ms step_avg:40.24ms
step:2281/2330 train_time:91778ms step_avg:40.24ms
step:2282/2330 train_time:91822ms step_avg:40.24ms
step:2283/2330 train_time:91857ms step_avg:40.24ms
step:2284/2330 train_time:91903ms step_avg:40.24ms
step:2285/2330 train_time:91938ms step_avg:40.24ms
step:2286/2330 train_time:91984ms step_avg:40.24ms
step:2287/2330 train_time:92020ms step_avg:40.24ms
step:2288/2330 train_time:92066ms step_avg:40.24ms
step:2289/2330 train_time:92102ms step_avg:40.24ms
step:2290/2330 train_time:92147ms step_avg:40.24ms
step:2291/2330 train_time:92183ms step_avg:40.24ms
step:2292/2330 train_time:92227ms step_avg:40.24ms
step:2293/2330 train_time:92263ms step_avg:40.24ms
step:2294/2330 train_time:92307ms step_avg:40.24ms
step:2295/2330 train_time:92342ms step_avg:40.24ms
step:2296/2330 train_time:92386ms step_avg:40.24ms
step:2297/2330 train_time:92420ms step_avg:40.24ms
step:2298/2330 train_time:92464ms step_avg:40.24ms
step:2299/2330 train_time:92499ms step_avg:40.23ms
step:2300/2330 train_time:92543ms step_avg:40.24ms
step:2301/2330 train_time:92578ms step_avg:40.23ms
step:2302/2330 train_time:92622ms step_avg:40.24ms
step:2303/2330 train_time:92657ms step_avg:40.23ms
step:2304/2330 train_time:92701ms step_avg:40.23ms
step:2305/2330 train_time:92736ms step_avg:40.23ms
step:2306/2330 train_time:92780ms step_avg:40.23ms
step:2307/2330 train_time:92815ms step_avg:40.23ms
step:2308/2330 train_time:92860ms step_avg:40.23ms
step:2309/2330 train_time:92896ms step_avg:40.23ms
step:2310/2330 train_time:92941ms step_avg:40.23ms
step:2311/2330 train_time:92977ms step_avg:40.23ms
step:2312/2330 train_time:93022ms step_avg:40.23ms
step:2313/2330 train_time:93058ms step_avg:40.23ms
step:2314/2330 train_time:93103ms step_avg:40.23ms
step:2315/2330 train_time:93139ms step_avg:40.23ms
step:2316/2330 train_time:93183ms step_avg:40.23ms
step:2317/2330 train_time:93219ms step_avg:40.23ms
step:2318/2330 train_time:93264ms step_avg:40.23ms
step:2319/2330 train_time:93299ms step_avg:40.23ms
step:2320/2330 train_time:93342ms step_avg:40.23ms
step:2321/2330 train_time:93378ms step_avg:40.23ms
step:2322/2330 train_time:93422ms step_avg:40.23ms
step:2323/2330 train_time:93457ms step_avg:40.23ms
step:2324/2330 train_time:93502ms step_avg:40.23ms
step:2325/2330 train_time:93537ms step_avg:40.23ms
step:2326/2330 train_time:93581ms step_avg:40.23ms
step:2327/2330 train_time:93616ms step_avg:40.23ms
step:2328/2330 train_time:93660ms step_avg:40.23ms
step:2329/2330 train_time:93695ms step_avg:40.23ms
step:2330/2330 train_time:93740ms step_avg:40.23ms
step:2330/2330 val_loss:5.1240 train_time:93828ms step_avg:40.27ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
