import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:42:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:101ms step_avg:101.33ms
step:2/2330 train_time:192ms step_avg:96.08ms
step:3/2330 train_time:215ms step_avg:71.61ms
step:4/2330 train_time:249ms step_avg:62.28ms
step:5/2330 train_time:306ms step_avg:61.24ms
step:6/2330 train_time:368ms step_avg:61.26ms
step:7/2330 train_time:426ms step_avg:60.85ms
step:8/2330 train_time:488ms step_avg:60.96ms
step:9/2330 train_time:546ms step_avg:60.70ms
step:10/2330 train_time:608ms step_avg:60.79ms
step:11/2330 train_time:666ms step_avg:60.58ms
step:12/2330 train_time:728ms step_avg:60.67ms
step:13/2330 train_time:787ms step_avg:60.52ms
step:14/2330 train_time:848ms step_avg:60.61ms
step:15/2330 train_time:907ms step_avg:60.46ms
step:16/2330 train_time:968ms step_avg:60.53ms
step:17/2330 train_time:1029ms step_avg:60.54ms
step:18/2330 train_time:1094ms step_avg:60.79ms
step:19/2330 train_time:1156ms step_avg:60.85ms
step:20/2330 train_time:1219ms step_avg:60.95ms
step:21/2330 train_time:1280ms step_avg:60.93ms
step:22/2330 train_time:1342ms step_avg:61.02ms
step:23/2330 train_time:1402ms step_avg:60.95ms
step:24/2330 train_time:1464ms step_avg:61.01ms
step:25/2330 train_time:1523ms step_avg:60.93ms
step:26/2330 train_time:1585ms step_avg:60.98ms
step:27/2330 train_time:1645ms step_avg:60.91ms
step:28/2330 train_time:1707ms step_avg:60.95ms
step:29/2330 train_time:1766ms step_avg:60.88ms
step:30/2330 train_time:1827ms step_avg:60.91ms
step:31/2330 train_time:1886ms step_avg:60.83ms
step:32/2330 train_time:1948ms step_avg:60.87ms
step:33/2330 train_time:2007ms step_avg:60.82ms
step:34/2330 train_time:2071ms step_avg:60.90ms
step:35/2330 train_time:2132ms step_avg:60.91ms
step:36/2330 train_time:2195ms step_avg:60.98ms
step:37/2330 train_time:2255ms step_avg:60.94ms
step:38/2330 train_time:2317ms step_avg:60.97ms
step:39/2330 train_time:2376ms step_avg:60.93ms
step:40/2330 train_time:2439ms step_avg:60.97ms
step:41/2330 train_time:2498ms step_avg:60.94ms
step:42/2330 train_time:2562ms step_avg:61.00ms
step:43/2330 train_time:2620ms step_avg:60.94ms
step:44/2330 train_time:2683ms step_avg:60.98ms
step:45/2330 train_time:2743ms step_avg:60.94ms
step:46/2330 train_time:2805ms step_avg:60.97ms
step:47/2330 train_time:2864ms step_avg:60.94ms
step:48/2330 train_time:2926ms step_avg:60.96ms
step:49/2330 train_time:2986ms step_avg:60.94ms
step:50/2330 train_time:3048ms step_avg:60.96ms
step:51/2330 train_time:3107ms step_avg:60.93ms
step:52/2330 train_time:3170ms step_avg:60.96ms
step:53/2330 train_time:3232ms step_avg:60.98ms
step:54/2330 train_time:3294ms step_avg:61.00ms
step:55/2330 train_time:3353ms step_avg:60.97ms
step:56/2330 train_time:3415ms step_avg:60.97ms
step:57/2330 train_time:3474ms step_avg:60.95ms
step:58/2330 train_time:3536ms step_avg:60.97ms
step:59/2330 train_time:3595ms step_avg:60.93ms
step:60/2330 train_time:3658ms step_avg:60.96ms
step:61/2330 train_time:3717ms step_avg:60.94ms
step:62/2330 train_time:3780ms step_avg:60.96ms
step:63/2330 train_time:3839ms step_avg:60.93ms
step:64/2330 train_time:3901ms step_avg:60.96ms
step:65/2330 train_time:3960ms step_avg:60.93ms
step:66/2330 train_time:4024ms step_avg:60.97ms
step:67/2330 train_time:4083ms step_avg:60.94ms
step:68/2330 train_time:4146ms step_avg:60.97ms
step:69/2330 train_time:4205ms step_avg:60.95ms
step:70/2330 train_time:4268ms step_avg:60.97ms
step:71/2330 train_time:4326ms step_avg:60.93ms
step:72/2330 train_time:4388ms step_avg:60.94ms
step:73/2330 train_time:4447ms step_avg:60.92ms
step:74/2330 train_time:4509ms step_avg:60.93ms
step:75/2330 train_time:4569ms step_avg:60.92ms
step:76/2330 train_time:4631ms step_avg:60.94ms
step:77/2330 train_time:4691ms step_avg:60.92ms
step:78/2330 train_time:4753ms step_avg:60.93ms
step:79/2330 train_time:4812ms step_avg:60.91ms
step:80/2330 train_time:4873ms step_avg:60.92ms
step:81/2330 train_time:4932ms step_avg:60.89ms
step:82/2330 train_time:4995ms step_avg:60.91ms
step:83/2330 train_time:5054ms step_avg:60.89ms
step:84/2330 train_time:5116ms step_avg:60.91ms
step:85/2330 train_time:5175ms step_avg:60.88ms
step:86/2330 train_time:5238ms step_avg:60.91ms
step:87/2330 train_time:5297ms step_avg:60.88ms
step:88/2330 train_time:5359ms step_avg:60.90ms
step:89/2330 train_time:5419ms step_avg:60.89ms
step:90/2330 train_time:5482ms step_avg:60.91ms
step:91/2330 train_time:5541ms step_avg:60.89ms
step:92/2330 train_time:5604ms step_avg:60.91ms
step:93/2330 train_time:5662ms step_avg:60.88ms
step:94/2330 train_time:5724ms step_avg:60.90ms
step:95/2330 train_time:5784ms step_avg:60.88ms
step:96/2330 train_time:5846ms step_avg:60.90ms
step:97/2330 train_time:5905ms step_avg:60.88ms
step:98/2330 train_time:5967ms step_avg:60.89ms
step:99/2330 train_time:6025ms step_avg:60.86ms
step:100/2330 train_time:6087ms step_avg:60.87ms
step:101/2330 train_time:6146ms step_avg:60.85ms
step:102/2330 train_time:6207ms step_avg:60.86ms
step:103/2330 train_time:6267ms step_avg:60.84ms
step:104/2330 train_time:6329ms step_avg:60.85ms
step:105/2330 train_time:6388ms step_avg:60.84ms
step:106/2330 train_time:6450ms step_avg:60.85ms
step:107/2330 train_time:6509ms step_avg:60.83ms
step:108/2330 train_time:6571ms step_avg:60.85ms
step:109/2330 train_time:6631ms step_avg:60.83ms
step:110/2330 train_time:6693ms step_avg:60.84ms
step:111/2330 train_time:6752ms step_avg:60.83ms
step:112/2330 train_time:6813ms step_avg:60.83ms
step:113/2330 train_time:6873ms step_avg:60.82ms
step:114/2330 train_time:6935ms step_avg:60.83ms
step:115/2330 train_time:6993ms step_avg:60.81ms
step:116/2330 train_time:7056ms step_avg:60.83ms
step:117/2330 train_time:7115ms step_avg:60.81ms
step:118/2330 train_time:7177ms step_avg:60.82ms
step:119/2330 train_time:7236ms step_avg:60.80ms
step:120/2330 train_time:7298ms step_avg:60.82ms
step:121/2330 train_time:7358ms step_avg:60.81ms
step:122/2330 train_time:7420ms step_avg:60.82ms
step:123/2330 train_time:7479ms step_avg:60.81ms
step:124/2330 train_time:7542ms step_avg:60.82ms
step:125/2330 train_time:7600ms step_avg:60.80ms
step:126/2330 train_time:7663ms step_avg:60.81ms
step:127/2330 train_time:7722ms step_avg:60.80ms
step:128/2330 train_time:7785ms step_avg:60.82ms
step:129/2330 train_time:7844ms step_avg:60.81ms
step:130/2330 train_time:7906ms step_avg:60.82ms
step:131/2330 train_time:7964ms step_avg:60.80ms
step:132/2330 train_time:8026ms step_avg:60.81ms
step:133/2330 train_time:8085ms step_avg:60.79ms
step:134/2330 train_time:8147ms step_avg:60.80ms
step:135/2330 train_time:8206ms step_avg:60.78ms
step:136/2330 train_time:8267ms step_avg:60.79ms
step:137/2330 train_time:8326ms step_avg:60.78ms
step:138/2330 train_time:8388ms step_avg:60.78ms
step:139/2330 train_time:8448ms step_avg:60.77ms
step:140/2330 train_time:8509ms step_avg:60.78ms
step:141/2330 train_time:8568ms step_avg:60.77ms
step:142/2330 train_time:8630ms step_avg:60.78ms
step:143/2330 train_time:8689ms step_avg:60.76ms
step:144/2330 train_time:8751ms step_avg:60.77ms
step:145/2330 train_time:8810ms step_avg:60.76ms
step:146/2330 train_time:8871ms step_avg:60.76ms
step:147/2330 train_time:8929ms step_avg:60.74ms
step:148/2330 train_time:8991ms step_avg:60.75ms
step:149/2330 train_time:9050ms step_avg:60.74ms
step:150/2330 train_time:9111ms step_avg:60.74ms
step:151/2330 train_time:9169ms step_avg:60.72ms
step:152/2330 train_time:9231ms step_avg:60.73ms
step:153/2330 train_time:9290ms step_avg:60.72ms
step:154/2330 train_time:9352ms step_avg:60.73ms
step:155/2330 train_time:9411ms step_avg:60.72ms
step:156/2330 train_time:9473ms step_avg:60.72ms
step:157/2330 train_time:9532ms step_avg:60.71ms
step:158/2330 train_time:9594ms step_avg:60.72ms
step:159/2330 train_time:9653ms step_avg:60.71ms
step:160/2330 train_time:9714ms step_avg:60.71ms
step:161/2330 train_time:9773ms step_avg:60.70ms
step:162/2330 train_time:9835ms step_avg:60.71ms
step:163/2330 train_time:9894ms step_avg:60.70ms
step:164/2330 train_time:9956ms step_avg:60.71ms
step:165/2330 train_time:10015ms step_avg:60.70ms
step:166/2330 train_time:10077ms step_avg:60.70ms
step:167/2330 train_time:10135ms step_avg:60.69ms
step:168/2330 train_time:10197ms step_avg:60.70ms
step:169/2330 train_time:10256ms step_avg:60.69ms
step:170/2330 train_time:10318ms step_avg:60.70ms
step:171/2330 train_time:10378ms step_avg:60.69ms
step:172/2330 train_time:10440ms step_avg:60.70ms
step:173/2330 train_time:10500ms step_avg:60.69ms
step:174/2330 train_time:10562ms step_avg:60.70ms
step:175/2330 train_time:10621ms step_avg:60.69ms
step:176/2330 train_time:10683ms step_avg:60.70ms
step:177/2330 train_time:10743ms step_avg:60.70ms
step:178/2330 train_time:10805ms step_avg:60.70ms
step:179/2330 train_time:10864ms step_avg:60.69ms
step:180/2330 train_time:10925ms step_avg:60.70ms
step:181/2330 train_time:10984ms step_avg:60.68ms
step:182/2330 train_time:11046ms step_avg:60.69ms
step:183/2330 train_time:11104ms step_avg:60.68ms
step:184/2330 train_time:11166ms step_avg:60.68ms
step:185/2330 train_time:11224ms step_avg:60.67ms
step:186/2330 train_time:11286ms step_avg:60.68ms
step:187/2330 train_time:11345ms step_avg:60.67ms
step:188/2330 train_time:11407ms step_avg:60.67ms
step:189/2330 train_time:11465ms step_avg:60.66ms
step:190/2330 train_time:11527ms step_avg:60.67ms
step:191/2330 train_time:11586ms step_avg:60.66ms
step:192/2330 train_time:11648ms step_avg:60.66ms
step:193/2330 train_time:11708ms step_avg:60.66ms
step:194/2330 train_time:11769ms step_avg:60.67ms
step:195/2330 train_time:11828ms step_avg:60.65ms
step:196/2330 train_time:11889ms step_avg:60.66ms
step:197/2330 train_time:11948ms step_avg:60.65ms
step:198/2330 train_time:12009ms step_avg:60.65ms
step:199/2330 train_time:12067ms step_avg:60.64ms
step:200/2330 train_time:12129ms step_avg:60.64ms
step:201/2330 train_time:12187ms step_avg:60.63ms
step:202/2330 train_time:12249ms step_avg:60.64ms
step:203/2330 train_time:12307ms step_avg:60.63ms
step:204/2330 train_time:12368ms step_avg:60.63ms
step:205/2330 train_time:12428ms step_avg:60.63ms
step:206/2330 train_time:12491ms step_avg:60.64ms
step:207/2330 train_time:12550ms step_avg:60.63ms
step:208/2330 train_time:12611ms step_avg:60.63ms
step:209/2330 train_time:12670ms step_avg:60.62ms
step:210/2330 train_time:12731ms step_avg:60.63ms
step:211/2330 train_time:12790ms step_avg:60.62ms
step:212/2330 train_time:12852ms step_avg:60.62ms
step:213/2330 train_time:12911ms step_avg:60.61ms
step:214/2330 train_time:12972ms step_avg:60.62ms
step:215/2330 train_time:13031ms step_avg:60.61ms
step:216/2330 train_time:13093ms step_avg:60.62ms
step:217/2330 train_time:13153ms step_avg:60.61ms
step:218/2330 train_time:13214ms step_avg:60.62ms
step:219/2330 train_time:13273ms step_avg:60.61ms
step:220/2330 train_time:13335ms step_avg:60.61ms
step:221/2330 train_time:13393ms step_avg:60.60ms
step:222/2330 train_time:13455ms step_avg:60.61ms
step:223/2330 train_time:13514ms step_avg:60.60ms
step:224/2330 train_time:13576ms step_avg:60.61ms
step:225/2330 train_time:13635ms step_avg:60.60ms
step:226/2330 train_time:13697ms step_avg:60.61ms
step:227/2330 train_time:13756ms step_avg:60.60ms
step:228/2330 train_time:13818ms step_avg:60.60ms
step:229/2330 train_time:13877ms step_avg:60.60ms
step:230/2330 train_time:13940ms step_avg:60.61ms
step:231/2330 train_time:13999ms step_avg:60.60ms
step:232/2330 train_time:14061ms step_avg:60.61ms
step:233/2330 train_time:14120ms step_avg:60.60ms
step:234/2330 train_time:14183ms step_avg:60.61ms
step:235/2330 train_time:14242ms step_avg:60.60ms
step:236/2330 train_time:14304ms step_avg:60.61ms
step:237/2330 train_time:14362ms step_avg:60.60ms
step:238/2330 train_time:14424ms step_avg:60.60ms
step:239/2330 train_time:14483ms step_avg:60.60ms
step:240/2330 train_time:14545ms step_avg:60.60ms
step:241/2330 train_time:14604ms step_avg:60.60ms
step:242/2330 train_time:14665ms step_avg:60.60ms
step:243/2330 train_time:14724ms step_avg:60.59ms
step:244/2330 train_time:14786ms step_avg:60.60ms
step:245/2330 train_time:14845ms step_avg:60.59ms
step:246/2330 train_time:14906ms step_avg:60.59ms
step:247/2330 train_time:14965ms step_avg:60.59ms
step:248/2330 train_time:15026ms step_avg:60.59ms
step:249/2330 train_time:15085ms step_avg:60.58ms
step:250/2330 train_time:15147ms step_avg:60.59ms
step:250/2330 val_loss:4.1011 train_time:15210ms step_avg:60.84ms
step:251/2330 train_time:15235ms step_avg:60.70ms
step:252/2330 train_time:15270ms step_avg:60.60ms
step:253/2330 train_time:15333ms step_avg:60.60ms
step:254/2330 train_time:15400ms step_avg:60.63ms
step:255/2330 train_time:15461ms step_avg:60.63ms
step:256/2330 train_time:15523ms step_avg:60.64ms
step:257/2330 train_time:15582ms step_avg:60.63ms
step:258/2330 train_time:15643ms step_avg:60.63ms
step:259/2330 train_time:15702ms step_avg:60.63ms
step:260/2330 train_time:15764ms step_avg:60.63ms
step:261/2330 train_time:15821ms step_avg:60.62ms
step:262/2330 train_time:15883ms step_avg:60.62ms
step:263/2330 train_time:15940ms step_avg:60.61ms
step:264/2330 train_time:16001ms step_avg:60.61ms
step:265/2330 train_time:16059ms step_avg:60.60ms
step:266/2330 train_time:16121ms step_avg:60.60ms
step:267/2330 train_time:16179ms step_avg:60.60ms
step:268/2330 train_time:16242ms step_avg:60.60ms
step:269/2330 train_time:16302ms step_avg:60.60ms
step:270/2330 train_time:16365ms step_avg:60.61ms
step:271/2330 train_time:16424ms step_avg:60.60ms
step:272/2330 train_time:16486ms step_avg:60.61ms
step:273/2330 train_time:16545ms step_avg:60.60ms
step:274/2330 train_time:16607ms step_avg:60.61ms
step:275/2330 train_time:16666ms step_avg:60.60ms
step:276/2330 train_time:16727ms step_avg:60.61ms
step:277/2330 train_time:16786ms step_avg:60.60ms
step:278/2330 train_time:16848ms step_avg:60.60ms
step:279/2330 train_time:16907ms step_avg:60.60ms
step:280/2330 train_time:16968ms step_avg:60.60ms
step:281/2330 train_time:17027ms step_avg:60.60ms
step:282/2330 train_time:17089ms step_avg:60.60ms
step:283/2330 train_time:17148ms step_avg:60.59ms
step:284/2330 train_time:17211ms step_avg:60.60ms
step:285/2330 train_time:17270ms step_avg:60.60ms
step:286/2330 train_time:17332ms step_avg:60.60ms
step:287/2330 train_time:17392ms step_avg:60.60ms
step:288/2330 train_time:17455ms step_avg:60.61ms
step:289/2330 train_time:17514ms step_avg:60.60ms
step:290/2330 train_time:17577ms step_avg:60.61ms
step:291/2330 train_time:17636ms step_avg:60.60ms
step:292/2330 train_time:17698ms step_avg:60.61ms
step:293/2330 train_time:17757ms step_avg:60.60ms
step:294/2330 train_time:17819ms step_avg:60.61ms
step:295/2330 train_time:17878ms step_avg:60.60ms
step:296/2330 train_time:17940ms step_avg:60.61ms
step:297/2330 train_time:17999ms step_avg:60.60ms
step:298/2330 train_time:18060ms step_avg:60.60ms
step:299/2330 train_time:18119ms step_avg:60.60ms
step:300/2330 train_time:18181ms step_avg:60.60ms
step:301/2330 train_time:18240ms step_avg:60.60ms
step:302/2330 train_time:18302ms step_avg:60.60ms
step:303/2330 train_time:18361ms step_avg:60.60ms
step:304/2330 train_time:18423ms step_avg:60.60ms
step:305/2330 train_time:18482ms step_avg:60.60ms
step:306/2330 train_time:18544ms step_avg:60.60ms
step:307/2330 train_time:18603ms step_avg:60.60ms
step:308/2330 train_time:18666ms step_avg:60.60ms
step:309/2330 train_time:18724ms step_avg:60.60ms
step:310/2330 train_time:18786ms step_avg:60.60ms
step:311/2330 train_time:18844ms step_avg:60.59ms
step:312/2330 train_time:18905ms step_avg:60.59ms
step:313/2330 train_time:18964ms step_avg:60.59ms
step:314/2330 train_time:19026ms step_avg:60.59ms
step:315/2330 train_time:19084ms step_avg:60.59ms
step:316/2330 train_time:19146ms step_avg:60.59ms
step:317/2330 train_time:19205ms step_avg:60.58ms
step:318/2330 train_time:19266ms step_avg:60.59ms
step:319/2330 train_time:19325ms step_avg:60.58ms
step:320/2330 train_time:19387ms step_avg:60.58ms
step:321/2330 train_time:19446ms step_avg:60.58ms
step:322/2330 train_time:19508ms step_avg:60.58ms
step:323/2330 train_time:19567ms step_avg:60.58ms
step:324/2330 train_time:19630ms step_avg:60.59ms
step:325/2330 train_time:19689ms step_avg:60.58ms
step:326/2330 train_time:19751ms step_avg:60.58ms
step:327/2330 train_time:19809ms step_avg:60.58ms
step:328/2330 train_time:19871ms step_avg:60.58ms
step:329/2330 train_time:19930ms step_avg:60.58ms
step:330/2330 train_time:19992ms step_avg:60.58ms
step:331/2330 train_time:20052ms step_avg:60.58ms
step:332/2330 train_time:20114ms step_avg:60.59ms
step:333/2330 train_time:20173ms step_avg:60.58ms
step:334/2330 train_time:20236ms step_avg:60.59ms
step:335/2330 train_time:20295ms step_avg:60.58ms
step:336/2330 train_time:20357ms step_avg:60.59ms
step:337/2330 train_time:20417ms step_avg:60.59ms
step:338/2330 train_time:20479ms step_avg:60.59ms
step:339/2330 train_time:20538ms step_avg:60.58ms
step:340/2330 train_time:20599ms step_avg:60.59ms
step:341/2330 train_time:20659ms step_avg:60.58ms
step:342/2330 train_time:20720ms step_avg:60.59ms
step:343/2330 train_time:20779ms step_avg:60.58ms
step:344/2330 train_time:20840ms step_avg:60.58ms
step:345/2330 train_time:20899ms step_avg:60.58ms
step:346/2330 train_time:20961ms step_avg:60.58ms
step:347/2330 train_time:21021ms step_avg:60.58ms
step:348/2330 train_time:21083ms step_avg:60.58ms
step:349/2330 train_time:21142ms step_avg:60.58ms
step:350/2330 train_time:21205ms step_avg:60.59ms
step:351/2330 train_time:21264ms step_avg:60.58ms
step:352/2330 train_time:21325ms step_avg:60.58ms
step:353/2330 train_time:21384ms step_avg:60.58ms
step:354/2330 train_time:21445ms step_avg:60.58ms
step:355/2330 train_time:21503ms step_avg:60.57ms
step:356/2330 train_time:21565ms step_avg:60.58ms
step:357/2330 train_time:21624ms step_avg:60.57ms
step:358/2330 train_time:21686ms step_avg:60.57ms
step:359/2330 train_time:21744ms step_avg:60.57ms
step:360/2330 train_time:21806ms step_avg:60.57ms
step:361/2330 train_time:21864ms step_avg:60.57ms
step:362/2330 train_time:21926ms step_avg:60.57ms
step:363/2330 train_time:21984ms step_avg:60.56ms
step:364/2330 train_time:22046ms step_avg:60.57ms
step:365/2330 train_time:22105ms step_avg:60.56ms
step:366/2330 train_time:22167ms step_avg:60.56ms
step:367/2330 train_time:22226ms step_avg:60.56ms
step:368/2330 train_time:22288ms step_avg:60.57ms
step:369/2330 train_time:22348ms step_avg:60.56ms
step:370/2330 train_time:22409ms step_avg:60.57ms
step:371/2330 train_time:22468ms step_avg:60.56ms
step:372/2330 train_time:22530ms step_avg:60.57ms
step:373/2330 train_time:22589ms step_avg:60.56ms
step:374/2330 train_time:22651ms step_avg:60.56ms
step:375/2330 train_time:22710ms step_avg:60.56ms
step:376/2330 train_time:22772ms step_avg:60.56ms
step:377/2330 train_time:22831ms step_avg:60.56ms
step:378/2330 train_time:22893ms step_avg:60.56ms
step:379/2330 train_time:22952ms step_avg:60.56ms
step:380/2330 train_time:23014ms step_avg:60.56ms
step:381/2330 train_time:23074ms step_avg:60.56ms
step:382/2330 train_time:23136ms step_avg:60.57ms
step:383/2330 train_time:23196ms step_avg:60.56ms
step:384/2330 train_time:23258ms step_avg:60.57ms
step:385/2330 train_time:23318ms step_avg:60.57ms
step:386/2330 train_time:23380ms step_avg:60.57ms
step:387/2330 train_time:23438ms step_avg:60.56ms
step:388/2330 train_time:23500ms step_avg:60.57ms
step:389/2330 train_time:23559ms step_avg:60.56ms
step:390/2330 train_time:23621ms step_avg:60.57ms
step:391/2330 train_time:23679ms step_avg:60.56ms
step:392/2330 train_time:23741ms step_avg:60.56ms
step:393/2330 train_time:23799ms step_avg:60.56ms
step:394/2330 train_time:23862ms step_avg:60.56ms
step:395/2330 train_time:23921ms step_avg:60.56ms
step:396/2330 train_time:23982ms step_avg:60.56ms
step:397/2330 train_time:24041ms step_avg:60.56ms
step:398/2330 train_time:24103ms step_avg:60.56ms
step:399/2330 train_time:24162ms step_avg:60.56ms
step:400/2330 train_time:24224ms step_avg:60.56ms
step:401/2330 train_time:24282ms step_avg:60.55ms
step:402/2330 train_time:24344ms step_avg:60.56ms
step:403/2330 train_time:24402ms step_avg:60.55ms
step:404/2330 train_time:24464ms step_avg:60.55ms
step:405/2330 train_time:24522ms step_avg:60.55ms
step:406/2330 train_time:24584ms step_avg:60.55ms
step:407/2330 train_time:24642ms step_avg:60.55ms
step:408/2330 train_time:24704ms step_avg:60.55ms
step:409/2330 train_time:24763ms step_avg:60.55ms
step:410/2330 train_time:24825ms step_avg:60.55ms
step:411/2330 train_time:24883ms step_avg:60.54ms
step:412/2330 train_time:24945ms step_avg:60.55ms
step:413/2330 train_time:25005ms step_avg:60.54ms
step:414/2330 train_time:25066ms step_avg:60.55ms
step:415/2330 train_time:25126ms step_avg:60.54ms
step:416/2330 train_time:25187ms step_avg:60.55ms
step:417/2330 train_time:25246ms step_avg:60.54ms
step:418/2330 train_time:25308ms step_avg:60.54ms
step:419/2330 train_time:25367ms step_avg:60.54ms
step:420/2330 train_time:25429ms step_avg:60.55ms
step:421/2330 train_time:25488ms step_avg:60.54ms
step:422/2330 train_time:25550ms step_avg:60.54ms
step:423/2330 train_time:25609ms step_avg:60.54ms
step:424/2330 train_time:25671ms step_avg:60.55ms
step:425/2330 train_time:25731ms step_avg:60.54ms
step:426/2330 train_time:25793ms step_avg:60.55ms
step:427/2330 train_time:25852ms step_avg:60.54ms
step:428/2330 train_time:25915ms step_avg:60.55ms
step:429/2330 train_time:25974ms step_avg:60.55ms
step:430/2330 train_time:26036ms step_avg:60.55ms
step:431/2330 train_time:26095ms step_avg:60.55ms
step:432/2330 train_time:26158ms step_avg:60.55ms
step:433/2330 train_time:26217ms step_avg:60.55ms
step:434/2330 train_time:26279ms step_avg:60.55ms
step:435/2330 train_time:26338ms step_avg:60.55ms
step:436/2330 train_time:26400ms step_avg:60.55ms
step:437/2330 train_time:26459ms step_avg:60.55ms
step:438/2330 train_time:26521ms step_avg:60.55ms
step:439/2330 train_time:26580ms step_avg:60.55ms
step:440/2330 train_time:26642ms step_avg:60.55ms
step:441/2330 train_time:26701ms step_avg:60.55ms
step:442/2330 train_time:26763ms step_avg:60.55ms
step:443/2330 train_time:26822ms step_avg:60.55ms
step:444/2330 train_time:26884ms step_avg:60.55ms
step:445/2330 train_time:26943ms step_avg:60.55ms
step:446/2330 train_time:27005ms step_avg:60.55ms
step:447/2330 train_time:27064ms step_avg:60.55ms
step:448/2330 train_time:27126ms step_avg:60.55ms
step:449/2330 train_time:27184ms step_avg:60.54ms
step:450/2330 train_time:27247ms step_avg:60.55ms
step:451/2330 train_time:27306ms step_avg:60.54ms
step:452/2330 train_time:27367ms step_avg:60.55ms
step:453/2330 train_time:27426ms step_avg:60.54ms
step:454/2330 train_time:27487ms step_avg:60.54ms
step:455/2330 train_time:27547ms step_avg:60.54ms
step:456/2330 train_time:27609ms step_avg:60.55ms
step:457/2330 train_time:27669ms step_avg:60.54ms
step:458/2330 train_time:27732ms step_avg:60.55ms
step:459/2330 train_time:27791ms step_avg:60.55ms
step:460/2330 train_time:27853ms step_avg:60.55ms
step:461/2330 train_time:27912ms step_avg:60.55ms
step:462/2330 train_time:27974ms step_avg:60.55ms
step:463/2330 train_time:28033ms step_avg:60.55ms
step:464/2330 train_time:28096ms step_avg:60.55ms
step:465/2330 train_time:28155ms step_avg:60.55ms
step:466/2330 train_time:28218ms step_avg:60.55ms
step:467/2330 train_time:28278ms step_avg:60.55ms
step:468/2330 train_time:28340ms step_avg:60.55ms
step:469/2330 train_time:28399ms step_avg:60.55ms
step:470/2330 train_time:28461ms step_avg:60.56ms
step:471/2330 train_time:28520ms step_avg:60.55ms
step:472/2330 train_time:28581ms step_avg:60.55ms
step:473/2330 train_time:28640ms step_avg:60.55ms
step:474/2330 train_time:28702ms step_avg:60.55ms
step:475/2330 train_time:28761ms step_avg:60.55ms
step:476/2330 train_time:28822ms step_avg:60.55ms
step:477/2330 train_time:28882ms step_avg:60.55ms
step:478/2330 train_time:28944ms step_avg:60.55ms
step:479/2330 train_time:29004ms step_avg:60.55ms
step:480/2330 train_time:29066ms step_avg:60.55ms
step:481/2330 train_time:29124ms step_avg:60.55ms
step:482/2330 train_time:29186ms step_avg:60.55ms
step:483/2330 train_time:29245ms step_avg:60.55ms
step:484/2330 train_time:29307ms step_avg:60.55ms
step:485/2330 train_time:29366ms step_avg:60.55ms
step:486/2330 train_time:29427ms step_avg:60.55ms
step:487/2330 train_time:29485ms step_avg:60.54ms
step:488/2330 train_time:29547ms step_avg:60.55ms
step:489/2330 train_time:29606ms step_avg:60.54ms
step:490/2330 train_time:29667ms step_avg:60.55ms
step:491/2330 train_time:29726ms step_avg:60.54ms
step:492/2330 train_time:29788ms step_avg:60.54ms
step:493/2330 train_time:29847ms step_avg:60.54ms
step:494/2330 train_time:29909ms step_avg:60.54ms
step:495/2330 train_time:29968ms step_avg:60.54ms
step:496/2330 train_time:30030ms step_avg:60.54ms
step:497/2330 train_time:30089ms step_avg:60.54ms
step:498/2330 train_time:30151ms step_avg:60.54ms
step:499/2330 train_time:30211ms step_avg:60.54ms
step:500/2330 train_time:30273ms step_avg:60.55ms
step:500/2330 val_loss:3.8271 train_time:30337ms step_avg:60.67ms
step:501/2330 train_time:30359ms step_avg:60.60ms
step:502/2330 train_time:30396ms step_avg:60.55ms
step:503/2330 train_time:30460ms step_avg:60.56ms
step:504/2330 train_time:30524ms step_avg:60.56ms
step:505/2330 train_time:30583ms step_avg:60.56ms
step:506/2330 train_time:30646ms step_avg:60.56ms
step:507/2330 train_time:30705ms step_avg:60.56ms
step:508/2330 train_time:30766ms step_avg:60.56ms
step:509/2330 train_time:30824ms step_avg:60.56ms
step:510/2330 train_time:30885ms step_avg:60.56ms
step:511/2330 train_time:30944ms step_avg:60.56ms
step:512/2330 train_time:31006ms step_avg:60.56ms
step:513/2330 train_time:31064ms step_avg:60.55ms
step:514/2330 train_time:31125ms step_avg:60.55ms
step:515/2330 train_time:31183ms step_avg:60.55ms
step:516/2330 train_time:31245ms step_avg:60.55ms
step:517/2330 train_time:31304ms step_avg:60.55ms
step:518/2330 train_time:31368ms step_avg:60.56ms
step:519/2330 train_time:31429ms step_avg:60.56ms
step:520/2330 train_time:31492ms step_avg:60.56ms
step:521/2330 train_time:31552ms step_avg:60.56ms
step:522/2330 train_time:31614ms step_avg:60.56ms
step:523/2330 train_time:31673ms step_avg:60.56ms
step:524/2330 train_time:31735ms step_avg:60.56ms
step:525/2330 train_time:31794ms step_avg:60.56ms
step:526/2330 train_time:31855ms step_avg:60.56ms
step:527/2330 train_time:31914ms step_avg:60.56ms
step:528/2330 train_time:31976ms step_avg:60.56ms
step:529/2330 train_time:32034ms step_avg:60.56ms
step:530/2330 train_time:32096ms step_avg:60.56ms
step:531/2330 train_time:32155ms step_avg:60.55ms
step:532/2330 train_time:32216ms step_avg:60.56ms
step:533/2330 train_time:32275ms step_avg:60.55ms
step:534/2330 train_time:32337ms step_avg:60.56ms
step:535/2330 train_time:32396ms step_avg:60.55ms
step:536/2330 train_time:32458ms step_avg:60.56ms
step:537/2330 train_time:32517ms step_avg:60.55ms
step:538/2330 train_time:32580ms step_avg:60.56ms
step:539/2330 train_time:32639ms step_avg:60.55ms
step:540/2330 train_time:32700ms step_avg:60.56ms
step:541/2330 train_time:32759ms step_avg:60.55ms
step:542/2330 train_time:32821ms step_avg:60.55ms
step:543/2330 train_time:32879ms step_avg:60.55ms
step:544/2330 train_time:32941ms step_avg:60.55ms
step:545/2330 train_time:32999ms step_avg:60.55ms
step:546/2330 train_time:33061ms step_avg:60.55ms
step:547/2330 train_time:33119ms step_avg:60.55ms
step:548/2330 train_time:33181ms step_avg:60.55ms
step:549/2330 train_time:33240ms step_avg:60.55ms
step:550/2330 train_time:33301ms step_avg:60.55ms
step:551/2330 train_time:33360ms step_avg:60.54ms
step:552/2330 train_time:33422ms step_avg:60.55ms
step:553/2330 train_time:33481ms step_avg:60.54ms
step:554/2330 train_time:33543ms step_avg:60.55ms
step:555/2330 train_time:33602ms step_avg:60.54ms
step:556/2330 train_time:33664ms step_avg:60.55ms
step:557/2330 train_time:33724ms step_avg:60.55ms
step:558/2330 train_time:33786ms step_avg:60.55ms
step:559/2330 train_time:33845ms step_avg:60.54ms
step:560/2330 train_time:33907ms step_avg:60.55ms
step:561/2330 train_time:33965ms step_avg:60.54ms
step:562/2330 train_time:34027ms step_avg:60.55ms
step:563/2330 train_time:34086ms step_avg:60.54ms
step:564/2330 train_time:34148ms step_avg:60.55ms
step:565/2330 train_time:34207ms step_avg:60.54ms
step:566/2330 train_time:34269ms step_avg:60.55ms
step:567/2330 train_time:34328ms step_avg:60.54ms
step:568/2330 train_time:34391ms step_avg:60.55ms
step:569/2330 train_time:34450ms step_avg:60.54ms
step:570/2330 train_time:34513ms step_avg:60.55ms
step:571/2330 train_time:34573ms step_avg:60.55ms
step:572/2330 train_time:34635ms step_avg:60.55ms
step:573/2330 train_time:34695ms step_avg:60.55ms
step:574/2330 train_time:34757ms step_avg:60.55ms
step:575/2330 train_time:34815ms step_avg:60.55ms
step:576/2330 train_time:34877ms step_avg:60.55ms
step:577/2330 train_time:34936ms step_avg:60.55ms
step:578/2330 train_time:34998ms step_avg:60.55ms
step:579/2330 train_time:35057ms step_avg:60.55ms
step:580/2330 train_time:35118ms step_avg:60.55ms
step:581/2330 train_time:35177ms step_avg:60.55ms
step:582/2330 train_time:35239ms step_avg:60.55ms
step:583/2330 train_time:35299ms step_avg:60.55ms
step:584/2330 train_time:35361ms step_avg:60.55ms
step:585/2330 train_time:35420ms step_avg:60.55ms
step:586/2330 train_time:35482ms step_avg:60.55ms
step:587/2330 train_time:35540ms step_avg:60.55ms
step:588/2330 train_time:35602ms step_avg:60.55ms
step:589/2330 train_time:35661ms step_avg:60.55ms
step:590/2330 train_time:35723ms step_avg:60.55ms
step:591/2330 train_time:35782ms step_avg:60.55ms
step:592/2330 train_time:35844ms step_avg:60.55ms
step:593/2330 train_time:35903ms step_avg:60.55ms
step:594/2330 train_time:35966ms step_avg:60.55ms
step:595/2330 train_time:36024ms step_avg:60.55ms
step:596/2330 train_time:36086ms step_avg:60.55ms
step:597/2330 train_time:36145ms step_avg:60.55ms
step:598/2330 train_time:36207ms step_avg:60.55ms
step:599/2330 train_time:36267ms step_avg:60.55ms
step:600/2330 train_time:36329ms step_avg:60.55ms
step:601/2330 train_time:36389ms step_avg:60.55ms
step:602/2330 train_time:36451ms step_avg:60.55ms
step:603/2330 train_time:36511ms step_avg:60.55ms
step:604/2330 train_time:36573ms step_avg:60.55ms
step:605/2330 train_time:36632ms step_avg:60.55ms
step:606/2330 train_time:36694ms step_avg:60.55ms
step:607/2330 train_time:36753ms step_avg:60.55ms
step:608/2330 train_time:36815ms step_avg:60.55ms
step:609/2330 train_time:36874ms step_avg:60.55ms
step:610/2330 train_time:36936ms step_avg:60.55ms
step:611/2330 train_time:36995ms step_avg:60.55ms
step:612/2330 train_time:37057ms step_avg:60.55ms
step:613/2330 train_time:37116ms step_avg:60.55ms
step:614/2330 train_time:37178ms step_avg:60.55ms
step:615/2330 train_time:37237ms step_avg:60.55ms
step:616/2330 train_time:37299ms step_avg:60.55ms
step:617/2330 train_time:37359ms step_avg:60.55ms
step:618/2330 train_time:37420ms step_avg:60.55ms
step:619/2330 train_time:37479ms step_avg:60.55ms
step:620/2330 train_time:37541ms step_avg:60.55ms
step:621/2330 train_time:37600ms step_avg:60.55ms
step:622/2330 train_time:37662ms step_avg:60.55ms
step:623/2330 train_time:37720ms step_avg:60.55ms
step:624/2330 train_time:37781ms step_avg:60.55ms
step:625/2330 train_time:37840ms step_avg:60.54ms
step:626/2330 train_time:37902ms step_avg:60.55ms
step:627/2330 train_time:37961ms step_avg:60.54ms
step:628/2330 train_time:38022ms step_avg:60.55ms
step:629/2330 train_time:38081ms step_avg:60.54ms
step:630/2330 train_time:38143ms step_avg:60.54ms
step:631/2330 train_time:38202ms step_avg:60.54ms
step:632/2330 train_time:38265ms step_avg:60.55ms
step:633/2330 train_time:38324ms step_avg:60.54ms
step:634/2330 train_time:38391ms step_avg:60.55ms
step:635/2330 train_time:38444ms step_avg:60.54ms
step:636/2330 train_time:38506ms step_avg:60.54ms
step:637/2330 train_time:38565ms step_avg:60.54ms
step:638/2330 train_time:38627ms step_avg:60.54ms
step:639/2330 train_time:38686ms step_avg:60.54ms
step:640/2330 train_time:38749ms step_avg:60.55ms
step:641/2330 train_time:38809ms step_avg:60.54ms
step:642/2330 train_time:38871ms step_avg:60.55ms
step:643/2330 train_time:38930ms step_avg:60.54ms
step:644/2330 train_time:38993ms step_avg:60.55ms
step:645/2330 train_time:39053ms step_avg:60.55ms
step:646/2330 train_time:39115ms step_avg:60.55ms
step:647/2330 train_time:39174ms step_avg:60.55ms
step:648/2330 train_time:39236ms step_avg:60.55ms
step:649/2330 train_time:39295ms step_avg:60.55ms
step:650/2330 train_time:39357ms step_avg:60.55ms
step:651/2330 train_time:39416ms step_avg:60.55ms
step:652/2330 train_time:39478ms step_avg:60.55ms
step:653/2330 train_time:39536ms step_avg:60.55ms
step:654/2330 train_time:39599ms step_avg:60.55ms
step:655/2330 train_time:39657ms step_avg:60.55ms
step:656/2330 train_time:39719ms step_avg:60.55ms
step:657/2330 train_time:39778ms step_avg:60.55ms
step:658/2330 train_time:39841ms step_avg:60.55ms
step:659/2330 train_time:39900ms step_avg:60.55ms
step:660/2330 train_time:39962ms step_avg:60.55ms
step:661/2330 train_time:40021ms step_avg:60.55ms
step:662/2330 train_time:40083ms step_avg:60.55ms
step:663/2330 train_time:40142ms step_avg:60.55ms
step:664/2330 train_time:40204ms step_avg:60.55ms
step:665/2330 train_time:40263ms step_avg:60.55ms
step:666/2330 train_time:40325ms step_avg:60.55ms
step:667/2330 train_time:40384ms step_avg:60.55ms
step:668/2330 train_time:40447ms step_avg:60.55ms
step:669/2330 train_time:40506ms step_avg:60.55ms
step:670/2330 train_time:40568ms step_avg:60.55ms
step:671/2330 train_time:40628ms step_avg:60.55ms
step:672/2330 train_time:40691ms step_avg:60.55ms
step:673/2330 train_time:40750ms step_avg:60.55ms
step:674/2330 train_time:40813ms step_avg:60.55ms
step:675/2330 train_time:40872ms step_avg:60.55ms
step:676/2330 train_time:40934ms step_avg:60.55ms
step:677/2330 train_time:40994ms step_avg:60.55ms
step:678/2330 train_time:41056ms step_avg:60.55ms
step:679/2330 train_time:41115ms step_avg:60.55ms
step:680/2330 train_time:41176ms step_avg:60.55ms
step:681/2330 train_time:41235ms step_avg:60.55ms
step:682/2330 train_time:41297ms step_avg:60.55ms
step:683/2330 train_time:41356ms step_avg:60.55ms
step:684/2330 train_time:41418ms step_avg:60.55ms
step:685/2330 train_time:41477ms step_avg:60.55ms
step:686/2330 train_time:41539ms step_avg:60.55ms
step:687/2330 train_time:41599ms step_avg:60.55ms
step:688/2330 train_time:41661ms step_avg:60.55ms
step:689/2330 train_time:41720ms step_avg:60.55ms
step:690/2330 train_time:41782ms step_avg:60.55ms
step:691/2330 train_time:41841ms step_avg:60.55ms
step:692/2330 train_time:41903ms step_avg:60.55ms
step:693/2330 train_time:41962ms step_avg:60.55ms
step:694/2330 train_time:42023ms step_avg:60.55ms
step:695/2330 train_time:42082ms step_avg:60.55ms
step:696/2330 train_time:42145ms step_avg:60.55ms
step:697/2330 train_time:42204ms step_avg:60.55ms
step:698/2330 train_time:42266ms step_avg:60.55ms
step:699/2330 train_time:42325ms step_avg:60.55ms
step:700/2330 train_time:42388ms step_avg:60.55ms
step:701/2330 train_time:42448ms step_avg:60.55ms
step:702/2330 train_time:42510ms step_avg:60.56ms
step:703/2330 train_time:42569ms step_avg:60.55ms
step:704/2330 train_time:42631ms step_avg:60.56ms
step:705/2330 train_time:42691ms step_avg:60.56ms
step:706/2330 train_time:42754ms step_avg:60.56ms
step:707/2330 train_time:42814ms step_avg:60.56ms
step:708/2330 train_time:42876ms step_avg:60.56ms
step:709/2330 train_time:42935ms step_avg:60.56ms
step:710/2330 train_time:42996ms step_avg:60.56ms
step:711/2330 train_time:43055ms step_avg:60.56ms
step:712/2330 train_time:43117ms step_avg:60.56ms
step:713/2330 train_time:43175ms step_avg:60.55ms
step:714/2330 train_time:43237ms step_avg:60.56ms
step:715/2330 train_time:43296ms step_avg:60.55ms
step:716/2330 train_time:43358ms step_avg:60.56ms
step:717/2330 train_time:43418ms step_avg:60.56ms
step:718/2330 train_time:43480ms step_avg:60.56ms
step:719/2330 train_time:43539ms step_avg:60.56ms
step:720/2330 train_time:43602ms step_avg:60.56ms
step:721/2330 train_time:43661ms step_avg:60.56ms
step:722/2330 train_time:43722ms step_avg:60.56ms
step:723/2330 train_time:43781ms step_avg:60.55ms
step:724/2330 train_time:43842ms step_avg:60.56ms
step:725/2330 train_time:43901ms step_avg:60.55ms
step:726/2330 train_time:43963ms step_avg:60.55ms
step:727/2330 train_time:44021ms step_avg:60.55ms
step:728/2330 train_time:44083ms step_avg:60.55ms
step:729/2330 train_time:44142ms step_avg:60.55ms
step:730/2330 train_time:44204ms step_avg:60.55ms
step:731/2330 train_time:44264ms step_avg:60.55ms
step:732/2330 train_time:44326ms step_avg:60.56ms
step:733/2330 train_time:44385ms step_avg:60.55ms
step:734/2330 train_time:44448ms step_avg:60.56ms
step:735/2330 train_time:44507ms step_avg:60.55ms
step:736/2330 train_time:44569ms step_avg:60.56ms
step:737/2330 train_time:44629ms step_avg:60.55ms
step:738/2330 train_time:44692ms step_avg:60.56ms
step:739/2330 train_time:44751ms step_avg:60.56ms
step:740/2330 train_time:44814ms step_avg:60.56ms
step:741/2330 train_time:44873ms step_avg:60.56ms
step:742/2330 train_time:44936ms step_avg:60.56ms
step:743/2330 train_time:44995ms step_avg:60.56ms
step:744/2330 train_time:45056ms step_avg:60.56ms
step:745/2330 train_time:45115ms step_avg:60.56ms
step:746/2330 train_time:45177ms step_avg:60.56ms
step:747/2330 train_time:45236ms step_avg:60.56ms
step:748/2330 train_time:45298ms step_avg:60.56ms
step:749/2330 train_time:45358ms step_avg:60.56ms
step:750/2330 train_time:45420ms step_avg:60.56ms
step:750/2330 val_loss:3.6918 train_time:45484ms step_avg:60.65ms
step:751/2330 train_time:45508ms step_avg:60.60ms
step:752/2330 train_time:45545ms step_avg:60.56ms
step:753/2330 train_time:45607ms step_avg:60.57ms
step:754/2330 train_time:45676ms step_avg:60.58ms
step:755/2330 train_time:45735ms step_avg:60.58ms
step:756/2330 train_time:45797ms step_avg:60.58ms
step:757/2330 train_time:45855ms step_avg:60.57ms
step:758/2330 train_time:45917ms step_avg:60.58ms
step:759/2330 train_time:45975ms step_avg:60.57ms
step:760/2330 train_time:46036ms step_avg:60.57ms
step:761/2330 train_time:46095ms step_avg:60.57ms
step:762/2330 train_time:46156ms step_avg:60.57ms
step:763/2330 train_time:46214ms step_avg:60.57ms
step:764/2330 train_time:46276ms step_avg:60.57ms
step:765/2330 train_time:46334ms step_avg:60.57ms
step:766/2330 train_time:46397ms step_avg:60.57ms
step:767/2330 train_time:46458ms step_avg:60.57ms
step:768/2330 train_time:46522ms step_avg:60.58ms
step:769/2330 train_time:46585ms step_avg:60.58ms
step:770/2330 train_time:46648ms step_avg:60.58ms
step:771/2330 train_time:46709ms step_avg:60.58ms
step:772/2330 train_time:46772ms step_avg:60.59ms
step:773/2330 train_time:46832ms step_avg:60.58ms
step:774/2330 train_time:46895ms step_avg:60.59ms
step:775/2330 train_time:46954ms step_avg:60.59ms
step:776/2330 train_time:47016ms step_avg:60.59ms
step:777/2330 train_time:47076ms step_avg:60.59ms
step:778/2330 train_time:47138ms step_avg:60.59ms
step:779/2330 train_time:47198ms step_avg:60.59ms
step:780/2330 train_time:47260ms step_avg:60.59ms
step:781/2330 train_time:47319ms step_avg:60.59ms
step:782/2330 train_time:47381ms step_avg:60.59ms
step:783/2330 train_time:47441ms step_avg:60.59ms
step:784/2330 train_time:47504ms step_avg:60.59ms
step:785/2330 train_time:47566ms step_avg:60.59ms
step:786/2330 train_time:47629ms step_avg:60.60ms
step:787/2330 train_time:47689ms step_avg:60.60ms
step:788/2330 train_time:47752ms step_avg:60.60ms
step:789/2330 train_time:47811ms step_avg:60.60ms
step:790/2330 train_time:47875ms step_avg:60.60ms
step:791/2330 train_time:47935ms step_avg:60.60ms
step:792/2330 train_time:47997ms step_avg:60.60ms
step:793/2330 train_time:48057ms step_avg:60.60ms
step:794/2330 train_time:48119ms step_avg:60.60ms
step:795/2330 train_time:48178ms step_avg:60.60ms
step:796/2330 train_time:48240ms step_avg:60.60ms
step:797/2330 train_time:48300ms step_avg:60.60ms
step:798/2330 train_time:48362ms step_avg:60.60ms
step:799/2330 train_time:48422ms step_avg:60.60ms
step:800/2330 train_time:48484ms step_avg:60.61ms
step:801/2330 train_time:48545ms step_avg:60.60ms
step:802/2330 train_time:48607ms step_avg:60.61ms
step:803/2330 train_time:48667ms step_avg:60.61ms
step:804/2330 train_time:48731ms step_avg:60.61ms
step:805/2330 train_time:48792ms step_avg:60.61ms
step:806/2330 train_time:48854ms step_avg:60.61ms
step:807/2330 train_time:48915ms step_avg:60.61ms
step:808/2330 train_time:48978ms step_avg:60.62ms
step:809/2330 train_time:49038ms step_avg:60.62ms
step:810/2330 train_time:49100ms step_avg:60.62ms
step:811/2330 train_time:49160ms step_avg:60.62ms
step:812/2330 train_time:49222ms step_avg:60.62ms
step:813/2330 train_time:49281ms step_avg:60.62ms
step:814/2330 train_time:49344ms step_avg:60.62ms
step:815/2330 train_time:49404ms step_avg:60.62ms
step:816/2330 train_time:49466ms step_avg:60.62ms
step:817/2330 train_time:49525ms step_avg:60.62ms
step:818/2330 train_time:49588ms step_avg:60.62ms
step:819/2330 train_time:49648ms step_avg:60.62ms
step:820/2330 train_time:49711ms step_avg:60.62ms
step:821/2330 train_time:49771ms step_avg:60.62ms
step:822/2330 train_time:49835ms step_avg:60.63ms
step:823/2330 train_time:49895ms step_avg:60.63ms
step:824/2330 train_time:49957ms step_avg:60.63ms
step:825/2330 train_time:50016ms step_avg:60.63ms
step:826/2330 train_time:50079ms step_avg:60.63ms
step:827/2330 train_time:50139ms step_avg:60.63ms
step:828/2330 train_time:50202ms step_avg:60.63ms
step:829/2330 train_time:50261ms step_avg:60.63ms
step:830/2330 train_time:50324ms step_avg:60.63ms
step:831/2330 train_time:50384ms step_avg:60.63ms
step:832/2330 train_time:50447ms step_avg:60.63ms
step:833/2330 train_time:50507ms step_avg:60.63ms
step:834/2330 train_time:50569ms step_avg:60.63ms
step:835/2330 train_time:50629ms step_avg:60.63ms
step:836/2330 train_time:50693ms step_avg:60.64ms
step:837/2330 train_time:50753ms step_avg:60.64ms
step:838/2330 train_time:50816ms step_avg:60.64ms
step:839/2330 train_time:50875ms step_avg:60.64ms
step:840/2330 train_time:50939ms step_avg:60.64ms
step:841/2330 train_time:50998ms step_avg:60.64ms
step:842/2330 train_time:51061ms step_avg:60.64ms
step:843/2330 train_time:51121ms step_avg:60.64ms
step:844/2330 train_time:51184ms step_avg:60.64ms
step:845/2330 train_time:51244ms step_avg:60.64ms
step:846/2330 train_time:51306ms step_avg:60.65ms
step:847/2330 train_time:51366ms step_avg:60.64ms
step:848/2330 train_time:51428ms step_avg:60.65ms
step:849/2330 train_time:51488ms step_avg:60.64ms
step:850/2330 train_time:51550ms step_avg:60.65ms
step:851/2330 train_time:51610ms step_avg:60.65ms
step:852/2330 train_time:51674ms step_avg:60.65ms
step:853/2330 train_time:51734ms step_avg:60.65ms
step:854/2330 train_time:51796ms step_avg:60.65ms
step:855/2330 train_time:51856ms step_avg:60.65ms
step:856/2330 train_time:51919ms step_avg:60.65ms
step:857/2330 train_time:51979ms step_avg:60.65ms
step:858/2330 train_time:52042ms step_avg:60.65ms
step:859/2330 train_time:52101ms step_avg:60.65ms
step:860/2330 train_time:52164ms step_avg:60.66ms
step:861/2330 train_time:52223ms step_avg:60.65ms
step:862/2330 train_time:52286ms step_avg:60.66ms
step:863/2330 train_time:52346ms step_avg:60.66ms
step:864/2330 train_time:52408ms step_avg:60.66ms
step:865/2330 train_time:52468ms step_avg:60.66ms
step:866/2330 train_time:52531ms step_avg:60.66ms
step:867/2330 train_time:52591ms step_avg:60.66ms
step:868/2330 train_time:52654ms step_avg:60.66ms
step:869/2330 train_time:52714ms step_avg:60.66ms
step:870/2330 train_time:52777ms step_avg:60.66ms
step:871/2330 train_time:52837ms step_avg:60.66ms
step:872/2330 train_time:52900ms step_avg:60.66ms
step:873/2330 train_time:52960ms step_avg:60.66ms
step:874/2330 train_time:53023ms step_avg:60.67ms
step:875/2330 train_time:53082ms step_avg:60.67ms
step:876/2330 train_time:53145ms step_avg:60.67ms
step:877/2330 train_time:53204ms step_avg:60.67ms
step:878/2330 train_time:53267ms step_avg:60.67ms
step:879/2330 train_time:53327ms step_avg:60.67ms
step:880/2330 train_time:53390ms step_avg:60.67ms
step:881/2330 train_time:53450ms step_avg:60.67ms
step:882/2330 train_time:53513ms step_avg:60.67ms
step:883/2330 train_time:53573ms step_avg:60.67ms
step:884/2330 train_time:53636ms step_avg:60.67ms
step:885/2330 train_time:53696ms step_avg:60.67ms
step:886/2330 train_time:53759ms step_avg:60.68ms
step:887/2330 train_time:53819ms step_avg:60.68ms
step:888/2330 train_time:53882ms step_avg:60.68ms
step:889/2330 train_time:53942ms step_avg:60.68ms
step:890/2330 train_time:54004ms step_avg:60.68ms
step:891/2330 train_time:54064ms step_avg:60.68ms
step:892/2330 train_time:54127ms step_avg:60.68ms
step:893/2330 train_time:54187ms step_avg:60.68ms
step:894/2330 train_time:54249ms step_avg:60.68ms
step:895/2330 train_time:54309ms step_avg:60.68ms
step:896/2330 train_time:54372ms step_avg:60.68ms
step:897/2330 train_time:54432ms step_avg:60.68ms
step:898/2330 train_time:54495ms step_avg:60.68ms
step:899/2330 train_time:54555ms step_avg:60.68ms
step:900/2330 train_time:54618ms step_avg:60.69ms
step:901/2330 train_time:54678ms step_avg:60.69ms
step:902/2330 train_time:54741ms step_avg:60.69ms
step:903/2330 train_time:54800ms step_avg:60.69ms
step:904/2330 train_time:54863ms step_avg:60.69ms
step:905/2330 train_time:54923ms step_avg:60.69ms
step:906/2330 train_time:54985ms step_avg:60.69ms
step:907/2330 train_time:55046ms step_avg:60.69ms
step:908/2330 train_time:55109ms step_avg:60.69ms
step:909/2330 train_time:55168ms step_avg:60.69ms
step:910/2330 train_time:55231ms step_avg:60.69ms
step:911/2330 train_time:55291ms step_avg:60.69ms
step:912/2330 train_time:55354ms step_avg:60.70ms
step:913/2330 train_time:55414ms step_avg:60.69ms
step:914/2330 train_time:55477ms step_avg:60.70ms
step:915/2330 train_time:55537ms step_avg:60.70ms
step:916/2330 train_time:55600ms step_avg:60.70ms
step:917/2330 train_time:55659ms step_avg:60.70ms
step:918/2330 train_time:55722ms step_avg:60.70ms
step:919/2330 train_time:55782ms step_avg:60.70ms
step:920/2330 train_time:55846ms step_avg:60.70ms
step:921/2330 train_time:55906ms step_avg:60.70ms
step:922/2330 train_time:55969ms step_avg:60.70ms
step:923/2330 train_time:56030ms step_avg:60.70ms
step:924/2330 train_time:56093ms step_avg:60.71ms
step:925/2330 train_time:56152ms step_avg:60.71ms
step:926/2330 train_time:56215ms step_avg:60.71ms
step:927/2330 train_time:56275ms step_avg:60.71ms
step:928/2330 train_time:56338ms step_avg:60.71ms
step:929/2330 train_time:56398ms step_avg:60.71ms
step:930/2330 train_time:56460ms step_avg:60.71ms
step:931/2330 train_time:56520ms step_avg:60.71ms
step:932/2330 train_time:56583ms step_avg:60.71ms
step:933/2330 train_time:56643ms step_avg:60.71ms
step:934/2330 train_time:56706ms step_avg:60.71ms
step:935/2330 train_time:56765ms step_avg:60.71ms
step:936/2330 train_time:56828ms step_avg:60.71ms
step:937/2330 train_time:56888ms step_avg:60.71ms
step:938/2330 train_time:56951ms step_avg:60.72ms
step:939/2330 train_time:57012ms step_avg:60.72ms
step:940/2330 train_time:57074ms step_avg:60.72ms
step:941/2330 train_time:57135ms step_avg:60.72ms
step:942/2330 train_time:57197ms step_avg:60.72ms
step:943/2330 train_time:57257ms step_avg:60.72ms
step:944/2330 train_time:57320ms step_avg:60.72ms
step:945/2330 train_time:57378ms step_avg:60.72ms
step:946/2330 train_time:57442ms step_avg:60.72ms
step:947/2330 train_time:57501ms step_avg:60.72ms
step:948/2330 train_time:57564ms step_avg:60.72ms
step:949/2330 train_time:57624ms step_avg:60.72ms
step:950/2330 train_time:57686ms step_avg:60.72ms
step:951/2330 train_time:57746ms step_avg:60.72ms
step:952/2330 train_time:57809ms step_avg:60.72ms
step:953/2330 train_time:57869ms step_avg:60.72ms
step:954/2330 train_time:57933ms step_avg:60.73ms
step:955/2330 train_time:57993ms step_avg:60.73ms
step:956/2330 train_time:58056ms step_avg:60.73ms
step:957/2330 train_time:58116ms step_avg:60.73ms
step:958/2330 train_time:58179ms step_avg:60.73ms
step:959/2330 train_time:58238ms step_avg:60.73ms
step:960/2330 train_time:58301ms step_avg:60.73ms
step:961/2330 train_time:58360ms step_avg:60.73ms
step:962/2330 train_time:58423ms step_avg:60.73ms
step:963/2330 train_time:58482ms step_avg:60.73ms
step:964/2330 train_time:58545ms step_avg:60.73ms
step:965/2330 train_time:58604ms step_avg:60.73ms
step:966/2330 train_time:58667ms step_avg:60.73ms
step:967/2330 train_time:58727ms step_avg:60.73ms
step:968/2330 train_time:58791ms step_avg:60.73ms
step:969/2330 train_time:58851ms step_avg:60.73ms
step:970/2330 train_time:58914ms step_avg:60.74ms
step:971/2330 train_time:58975ms step_avg:60.74ms
step:972/2330 train_time:59037ms step_avg:60.74ms
step:973/2330 train_time:59096ms step_avg:60.74ms
step:974/2330 train_time:59159ms step_avg:60.74ms
step:975/2330 train_time:59218ms step_avg:60.74ms
step:976/2330 train_time:59281ms step_avg:60.74ms
step:977/2330 train_time:59340ms step_avg:60.74ms
step:978/2330 train_time:59403ms step_avg:60.74ms
step:979/2330 train_time:59463ms step_avg:60.74ms
step:980/2330 train_time:59525ms step_avg:60.74ms
step:981/2330 train_time:59585ms step_avg:60.74ms
step:982/2330 train_time:59648ms step_avg:60.74ms
step:983/2330 train_time:59708ms step_avg:60.74ms
step:984/2330 train_time:59771ms step_avg:60.74ms
step:985/2330 train_time:59831ms step_avg:60.74ms
step:986/2330 train_time:59894ms step_avg:60.74ms
step:987/2330 train_time:59954ms step_avg:60.74ms
step:988/2330 train_time:60017ms step_avg:60.75ms
step:989/2330 train_time:60077ms step_avg:60.75ms
step:990/2330 train_time:60140ms step_avg:60.75ms
step:991/2330 train_time:60199ms step_avg:60.75ms
step:992/2330 train_time:60261ms step_avg:60.75ms
step:993/2330 train_time:60321ms step_avg:60.75ms
step:994/2330 train_time:60384ms step_avg:60.75ms
step:995/2330 train_time:60444ms step_avg:60.75ms
step:996/2330 train_time:60506ms step_avg:60.75ms
step:997/2330 train_time:60566ms step_avg:60.75ms
step:998/2330 train_time:60628ms step_avg:60.75ms
step:999/2330 train_time:60688ms step_avg:60.75ms
step:1000/2330 train_time:60751ms step_avg:60.75ms
step:1000/2330 val_loss:3.5811 train_time:60816ms step_avg:60.82ms
step:1001/2330 train_time:60840ms step_avg:60.78ms
step:1002/2330 train_time:60875ms step_avg:60.75ms
step:1003/2330 train_time:60939ms step_avg:60.76ms
step:1004/2330 train_time:61002ms step_avg:60.76ms
step:1005/2330 train_time:61061ms step_avg:60.76ms
step:1006/2330 train_time:61123ms step_avg:60.76ms
step:1007/2330 train_time:61182ms step_avg:60.76ms
step:1008/2330 train_time:61245ms step_avg:60.76ms
step:1009/2330 train_time:61304ms step_avg:60.76ms
step:1010/2330 train_time:61366ms step_avg:60.76ms
step:1011/2330 train_time:61424ms step_avg:60.76ms
step:1012/2330 train_time:61486ms step_avg:60.76ms
step:1013/2330 train_time:61545ms step_avg:60.76ms
step:1014/2330 train_time:61607ms step_avg:60.76ms
step:1015/2330 train_time:61666ms step_avg:60.75ms
step:1016/2330 train_time:61734ms step_avg:60.76ms
step:1017/2330 train_time:61796ms step_avg:60.76ms
step:1018/2330 train_time:61860ms step_avg:60.77ms
step:1019/2330 train_time:61921ms step_avg:60.77ms
step:1020/2330 train_time:61984ms step_avg:60.77ms
step:1021/2330 train_time:62044ms step_avg:60.77ms
step:1022/2330 train_time:62107ms step_avg:60.77ms
step:1023/2330 train_time:62166ms step_avg:60.77ms
step:1024/2330 train_time:62228ms step_avg:60.77ms
step:1025/2330 train_time:62286ms step_avg:60.77ms
step:1026/2330 train_time:62348ms step_avg:60.77ms
step:1027/2330 train_time:62407ms step_avg:60.77ms
step:1028/2330 train_time:62469ms step_avg:60.77ms
step:1029/2330 train_time:62528ms step_avg:60.77ms
step:1030/2330 train_time:62590ms step_avg:60.77ms
step:1031/2330 train_time:62650ms step_avg:60.77ms
step:1032/2330 train_time:62714ms step_avg:60.77ms
step:1033/2330 train_time:62774ms step_avg:60.77ms
step:1034/2330 train_time:62837ms step_avg:60.77ms
step:1035/2330 train_time:62899ms step_avg:60.77ms
step:1036/2330 train_time:62962ms step_avg:60.77ms
step:1037/2330 train_time:63023ms step_avg:60.77ms
step:1038/2330 train_time:63086ms step_avg:60.78ms
step:1039/2330 train_time:63145ms step_avg:60.78ms
step:1040/2330 train_time:63208ms step_avg:60.78ms
step:1041/2330 train_time:63267ms step_avg:60.78ms
step:1042/2330 train_time:63329ms step_avg:60.78ms
step:1043/2330 train_time:63388ms step_avg:60.77ms
step:1044/2330 train_time:63451ms step_avg:60.78ms
step:1045/2330 train_time:63510ms step_avg:60.78ms
step:1046/2330 train_time:63572ms step_avg:60.78ms
step:1047/2330 train_time:63632ms step_avg:60.78ms
step:1048/2330 train_time:63694ms step_avg:60.78ms
step:1049/2330 train_time:63754ms step_avg:60.78ms
step:1050/2330 train_time:63817ms step_avg:60.78ms
step:1051/2330 train_time:63878ms step_avg:60.78ms
step:1052/2330 train_time:63941ms step_avg:60.78ms
step:1053/2330 train_time:64002ms step_avg:60.78ms
step:1054/2330 train_time:64065ms step_avg:60.78ms
step:1055/2330 train_time:64124ms step_avg:60.78ms
step:1056/2330 train_time:64187ms step_avg:60.78ms
step:1057/2330 train_time:64247ms step_avg:60.78ms
step:1058/2330 train_time:64309ms step_avg:60.78ms
step:1059/2330 train_time:64368ms step_avg:60.78ms
step:1060/2330 train_time:64430ms step_avg:60.78ms
step:1061/2330 train_time:64490ms step_avg:60.78ms
step:1062/2330 train_time:64552ms step_avg:60.78ms
step:1063/2330 train_time:64611ms step_avg:60.78ms
step:1064/2330 train_time:64674ms step_avg:60.78ms
step:1065/2330 train_time:64734ms step_avg:60.78ms
step:1066/2330 train_time:64796ms step_avg:60.78ms
step:1067/2330 train_time:64857ms step_avg:60.78ms
step:1068/2330 train_time:64921ms step_avg:60.79ms
step:1069/2330 train_time:64981ms step_avg:60.79ms
step:1070/2330 train_time:65044ms step_avg:60.79ms
step:1071/2330 train_time:65104ms step_avg:60.79ms
step:1072/2330 train_time:65167ms step_avg:60.79ms
step:1073/2330 train_time:65227ms step_avg:60.79ms
step:1074/2330 train_time:65289ms step_avg:60.79ms
step:1075/2330 train_time:65348ms step_avg:60.79ms
step:1076/2330 train_time:65411ms step_avg:60.79ms
step:1077/2330 train_time:65469ms step_avg:60.79ms
step:1078/2330 train_time:65532ms step_avg:60.79ms
step:1079/2330 train_time:65592ms step_avg:60.79ms
step:1080/2330 train_time:65654ms step_avg:60.79ms
step:1081/2330 train_time:65713ms step_avg:60.79ms
step:1082/2330 train_time:65776ms step_avg:60.79ms
step:1083/2330 train_time:65836ms step_avg:60.79ms
step:1084/2330 train_time:65900ms step_avg:60.79ms
step:1085/2330 train_time:65960ms step_avg:60.79ms
step:1086/2330 train_time:66023ms step_avg:60.79ms
step:1087/2330 train_time:66083ms step_avg:60.79ms
step:1088/2330 train_time:66146ms step_avg:60.80ms
step:1089/2330 train_time:66206ms step_avg:60.80ms
step:1090/2330 train_time:66269ms step_avg:60.80ms
step:1091/2330 train_time:66328ms step_avg:60.80ms
step:1092/2330 train_time:66391ms step_avg:60.80ms
step:1093/2330 train_time:66450ms step_avg:60.80ms
step:1094/2330 train_time:66513ms step_avg:60.80ms
step:1095/2330 train_time:66572ms step_avg:60.80ms
step:1096/2330 train_time:66634ms step_avg:60.80ms
step:1097/2330 train_time:66694ms step_avg:60.80ms
step:1098/2330 train_time:66757ms step_avg:60.80ms
step:1099/2330 train_time:66817ms step_avg:60.80ms
step:1100/2330 train_time:66880ms step_avg:60.80ms
step:1101/2330 train_time:66940ms step_avg:60.80ms
step:1102/2330 train_time:67004ms step_avg:60.80ms
step:1103/2330 train_time:67064ms step_avg:60.80ms
step:1104/2330 train_time:67127ms step_avg:60.80ms
step:1105/2330 train_time:67186ms step_avg:60.80ms
step:1106/2330 train_time:67250ms step_avg:60.80ms
step:1107/2330 train_time:67309ms step_avg:60.80ms
step:1108/2330 train_time:67372ms step_avg:60.80ms
step:1109/2330 train_time:67431ms step_avg:60.80ms
step:1110/2330 train_time:67494ms step_avg:60.81ms
step:1111/2330 train_time:67553ms step_avg:60.80ms
step:1112/2330 train_time:67616ms step_avg:60.81ms
step:1113/2330 train_time:67675ms step_avg:60.80ms
step:1114/2330 train_time:67738ms step_avg:60.81ms
step:1115/2330 train_time:67798ms step_avg:60.81ms
step:1116/2330 train_time:67862ms step_avg:60.81ms
step:1117/2330 train_time:67921ms step_avg:60.81ms
step:1118/2330 train_time:67984ms step_avg:60.81ms
step:1119/2330 train_time:68044ms step_avg:60.81ms
step:1120/2330 train_time:68107ms step_avg:60.81ms
step:1121/2330 train_time:68167ms step_avg:60.81ms
step:1122/2330 train_time:68229ms step_avg:60.81ms
step:1123/2330 train_time:68289ms step_avg:60.81ms
step:1124/2330 train_time:68352ms step_avg:60.81ms
step:1125/2330 train_time:68412ms step_avg:60.81ms
step:1126/2330 train_time:68474ms step_avg:60.81ms
step:1127/2330 train_time:68533ms step_avg:60.81ms
step:1128/2330 train_time:68596ms step_avg:60.81ms
step:1129/2330 train_time:68655ms step_avg:60.81ms
step:1130/2330 train_time:68717ms step_avg:60.81ms
step:1131/2330 train_time:68777ms step_avg:60.81ms
step:1132/2330 train_time:68840ms step_avg:60.81ms
step:1133/2330 train_time:68900ms step_avg:60.81ms
step:1134/2330 train_time:68963ms step_avg:60.81ms
step:1135/2330 train_time:69022ms step_avg:60.81ms
step:1136/2330 train_time:69085ms step_avg:60.81ms
step:1137/2330 train_time:69145ms step_avg:60.81ms
step:1138/2330 train_time:69207ms step_avg:60.81ms
step:1139/2330 train_time:69266ms step_avg:60.81ms
step:1140/2330 train_time:69329ms step_avg:60.81ms
step:1141/2330 train_time:69389ms step_avg:60.81ms
step:1142/2330 train_time:69452ms step_avg:60.82ms
step:1143/2330 train_time:69511ms step_avg:60.81ms
step:1144/2330 train_time:69573ms step_avg:60.82ms
step:1145/2330 train_time:69632ms step_avg:60.81ms
step:1146/2330 train_time:69695ms step_avg:60.82ms
step:1147/2330 train_time:69756ms step_avg:60.82ms
step:1148/2330 train_time:69819ms step_avg:60.82ms
step:1149/2330 train_time:69879ms step_avg:60.82ms
step:1150/2330 train_time:69942ms step_avg:60.82ms
step:1151/2330 train_time:70001ms step_avg:60.82ms
step:1152/2330 train_time:70064ms step_avg:60.82ms
step:1153/2330 train_time:70124ms step_avg:60.82ms
step:1154/2330 train_time:70187ms step_avg:60.82ms
step:1155/2330 train_time:70247ms step_avg:60.82ms
step:1156/2330 train_time:70309ms step_avg:60.82ms
step:1157/2330 train_time:70368ms step_avg:60.82ms
step:1158/2330 train_time:70431ms step_avg:60.82ms
step:1159/2330 train_time:70491ms step_avg:60.82ms
step:1160/2330 train_time:70553ms step_avg:60.82ms
step:1161/2330 train_time:70612ms step_avg:60.82ms
step:1162/2330 train_time:70674ms step_avg:60.82ms
step:1163/2330 train_time:70734ms step_avg:60.82ms
step:1164/2330 train_time:70798ms step_avg:60.82ms
step:1165/2330 train_time:70858ms step_avg:60.82ms
step:1166/2330 train_time:70921ms step_avg:60.82ms
step:1167/2330 train_time:70982ms step_avg:60.82ms
step:1168/2330 train_time:71044ms step_avg:60.83ms
step:1169/2330 train_time:71104ms step_avg:60.82ms
step:1170/2330 train_time:71167ms step_avg:60.83ms
step:1171/2330 train_time:71226ms step_avg:60.83ms
step:1172/2330 train_time:71289ms step_avg:60.83ms
step:1173/2330 train_time:71349ms step_avg:60.83ms
step:1174/2330 train_time:71411ms step_avg:60.83ms
step:1175/2330 train_time:71471ms step_avg:60.83ms
step:1176/2330 train_time:71533ms step_avg:60.83ms
step:1177/2330 train_time:71592ms step_avg:60.83ms
step:1178/2330 train_time:71654ms step_avg:60.83ms
step:1179/2330 train_time:71714ms step_avg:60.83ms
step:1180/2330 train_time:71777ms step_avg:60.83ms
step:1181/2330 train_time:71837ms step_avg:60.83ms
step:1182/2330 train_time:71901ms step_avg:60.83ms
step:1183/2330 train_time:71961ms step_avg:60.83ms
step:1184/2330 train_time:72024ms step_avg:60.83ms
step:1185/2330 train_time:72084ms step_avg:60.83ms
step:1186/2330 train_time:72147ms step_avg:60.83ms
step:1187/2330 train_time:72207ms step_avg:60.83ms
step:1188/2330 train_time:72270ms step_avg:60.83ms
step:1189/2330 train_time:72329ms step_avg:60.83ms
step:1190/2330 train_time:72392ms step_avg:60.83ms
step:1191/2330 train_time:72452ms step_avg:60.83ms
step:1192/2330 train_time:72514ms step_avg:60.83ms
step:1193/2330 train_time:72574ms step_avg:60.83ms
step:1194/2330 train_time:72635ms step_avg:60.83ms
step:1195/2330 train_time:72695ms step_avg:60.83ms
step:1196/2330 train_time:72758ms step_avg:60.83ms
step:1197/2330 train_time:72818ms step_avg:60.83ms
step:1198/2330 train_time:72881ms step_avg:60.84ms
step:1199/2330 train_time:72941ms step_avg:60.83ms
step:1200/2330 train_time:73004ms step_avg:60.84ms
step:1201/2330 train_time:73064ms step_avg:60.84ms
step:1202/2330 train_time:73127ms step_avg:60.84ms
step:1203/2330 train_time:73186ms step_avg:60.84ms
step:1204/2330 train_time:73250ms step_avg:60.84ms
step:1205/2330 train_time:73309ms step_avg:60.84ms
step:1206/2330 train_time:73372ms step_avg:60.84ms
step:1207/2330 train_time:73431ms step_avg:60.84ms
step:1208/2330 train_time:73494ms step_avg:60.84ms
step:1209/2330 train_time:73553ms step_avg:60.84ms
step:1210/2330 train_time:73616ms step_avg:60.84ms
step:1211/2330 train_time:73676ms step_avg:60.84ms
step:1212/2330 train_time:73739ms step_avg:60.84ms
step:1213/2330 train_time:73799ms step_avg:60.84ms
step:1214/2330 train_time:73861ms step_avg:60.84ms
step:1215/2330 train_time:73921ms step_avg:60.84ms
step:1216/2330 train_time:73984ms step_avg:60.84ms
step:1217/2330 train_time:74044ms step_avg:60.84ms
step:1218/2330 train_time:74108ms step_avg:60.84ms
step:1219/2330 train_time:74167ms step_avg:60.84ms
step:1220/2330 train_time:74230ms step_avg:60.84ms
step:1221/2330 train_time:74289ms step_avg:60.84ms
step:1222/2330 train_time:74353ms step_avg:60.84ms
step:1223/2330 train_time:74412ms step_avg:60.84ms
step:1224/2330 train_time:74474ms step_avg:60.84ms
step:1225/2330 train_time:74533ms step_avg:60.84ms
step:1226/2330 train_time:74596ms step_avg:60.85ms
step:1227/2330 train_time:74656ms step_avg:60.84ms
step:1228/2330 train_time:74718ms step_avg:60.85ms
step:1229/2330 train_time:74778ms step_avg:60.84ms
step:1230/2330 train_time:74841ms step_avg:60.85ms
step:1231/2330 train_time:74901ms step_avg:60.85ms
step:1232/2330 train_time:74964ms step_avg:60.85ms
step:1233/2330 train_time:75024ms step_avg:60.85ms
step:1234/2330 train_time:75086ms step_avg:60.85ms
step:1235/2330 train_time:75145ms step_avg:60.85ms
step:1236/2330 train_time:75208ms step_avg:60.85ms
step:1237/2330 train_time:75267ms step_avg:60.85ms
step:1238/2330 train_time:75330ms step_avg:60.85ms
step:1239/2330 train_time:75389ms step_avg:60.85ms
step:1240/2330 train_time:75452ms step_avg:60.85ms
step:1241/2330 train_time:75512ms step_avg:60.85ms
step:1242/2330 train_time:75574ms step_avg:60.85ms
step:1243/2330 train_time:75633ms step_avg:60.85ms
step:1244/2330 train_time:75697ms step_avg:60.85ms
step:1245/2330 train_time:75756ms step_avg:60.85ms
step:1246/2330 train_time:75819ms step_avg:60.85ms
step:1247/2330 train_time:75879ms step_avg:60.85ms
step:1248/2330 train_time:75942ms step_avg:60.85ms
step:1249/2330 train_time:76002ms step_avg:60.85ms
step:1250/2330 train_time:76065ms step_avg:60.85ms
step:1250/2330 val_loss:3.5184 train_time:76129ms step_avg:60.90ms
step:1251/2330 train_time:76154ms step_avg:60.87ms
step:1252/2330 train_time:76191ms step_avg:60.86ms
step:1253/2330 train_time:76256ms step_avg:60.86ms
step:1254/2330 train_time:76320ms step_avg:60.86ms
step:1255/2330 train_time:76381ms step_avg:60.86ms
step:1256/2330 train_time:76444ms step_avg:60.86ms
step:1257/2330 train_time:76502ms step_avg:60.86ms
step:1258/2330 train_time:76565ms step_avg:60.86ms
step:1259/2330 train_time:76624ms step_avg:60.86ms
step:1260/2330 train_time:76687ms step_avg:60.86ms
step:1261/2330 train_time:76746ms step_avg:60.86ms
step:1262/2330 train_time:76808ms step_avg:60.86ms
step:1263/2330 train_time:76867ms step_avg:60.86ms
step:1264/2330 train_time:76929ms step_avg:60.86ms
step:1265/2330 train_time:76987ms step_avg:60.86ms
step:1266/2330 train_time:77050ms step_avg:60.86ms
step:1267/2330 train_time:77111ms step_avg:60.86ms
step:1268/2330 train_time:77176ms step_avg:60.86ms
step:1269/2330 train_time:77238ms step_avg:60.87ms
step:1270/2330 train_time:77302ms step_avg:60.87ms
step:1271/2330 train_time:77362ms step_avg:60.87ms
step:1272/2330 train_time:77424ms step_avg:60.87ms
step:1273/2330 train_time:77484ms step_avg:60.87ms
step:1274/2330 train_time:77547ms step_avg:60.87ms
step:1275/2330 train_time:77606ms step_avg:60.87ms
step:1276/2330 train_time:77669ms step_avg:60.87ms
step:1277/2330 train_time:77729ms step_avg:60.87ms
step:1278/2330 train_time:77791ms step_avg:60.87ms
step:1279/2330 train_time:77850ms step_avg:60.87ms
step:1280/2330 train_time:77912ms step_avg:60.87ms
step:1281/2330 train_time:77972ms step_avg:60.87ms
step:1282/2330 train_time:78035ms step_avg:60.87ms
step:1283/2330 train_time:78095ms step_avg:60.87ms
step:1284/2330 train_time:78159ms step_avg:60.87ms
step:1285/2330 train_time:78220ms step_avg:60.87ms
step:1286/2330 train_time:78283ms step_avg:60.87ms
step:1287/2330 train_time:78343ms step_avg:60.87ms
step:1288/2330 train_time:78406ms step_avg:60.87ms
step:1289/2330 train_time:78466ms step_avg:60.87ms
step:1290/2330 train_time:78529ms step_avg:60.88ms
step:1291/2330 train_time:78589ms step_avg:60.87ms
step:1292/2330 train_time:78652ms step_avg:60.88ms
step:1293/2330 train_time:78712ms step_avg:60.88ms
step:1294/2330 train_time:78775ms step_avg:60.88ms
step:1295/2330 train_time:78834ms step_avg:60.88ms
step:1296/2330 train_time:78897ms step_avg:60.88ms
step:1297/2330 train_time:78955ms step_avg:60.88ms
step:1298/2330 train_time:79018ms step_avg:60.88ms
step:1299/2330 train_time:79077ms step_avg:60.88ms
step:1300/2330 train_time:79141ms step_avg:60.88ms
step:1301/2330 train_time:79202ms step_avg:60.88ms
step:1302/2330 train_time:79265ms step_avg:60.88ms
step:1303/2330 train_time:79324ms step_avg:60.88ms
step:1304/2330 train_time:79387ms step_avg:60.88ms
step:1305/2330 train_time:79447ms step_avg:60.88ms
step:1306/2330 train_time:79510ms step_avg:60.88ms
step:1307/2330 train_time:79570ms step_avg:60.88ms
step:1308/2330 train_time:79634ms step_avg:60.88ms
step:1309/2330 train_time:79694ms step_avg:60.88ms
step:1310/2330 train_time:79756ms step_avg:60.88ms
step:1311/2330 train_time:79816ms step_avg:60.88ms
step:1312/2330 train_time:79878ms step_avg:60.88ms
step:1313/2330 train_time:79938ms step_avg:60.88ms
step:1314/2330 train_time:80000ms step_avg:60.88ms
step:1315/2330 train_time:80059ms step_avg:60.88ms
step:1316/2330 train_time:80122ms step_avg:60.88ms
step:1317/2330 train_time:80181ms step_avg:60.88ms
step:1318/2330 train_time:80244ms step_avg:60.88ms
step:1319/2330 train_time:80303ms step_avg:60.88ms
step:1320/2330 train_time:80366ms step_avg:60.88ms
step:1321/2330 train_time:80426ms step_avg:60.88ms
step:1322/2330 train_time:80489ms step_avg:60.88ms
step:1323/2330 train_time:80548ms step_avg:60.88ms
step:1324/2330 train_time:80611ms step_avg:60.88ms
step:1325/2330 train_time:80671ms step_avg:60.88ms
step:1326/2330 train_time:80734ms step_avg:60.89ms
step:1327/2330 train_time:80794ms step_avg:60.88ms
step:1328/2330 train_time:80857ms step_avg:60.89ms
step:1329/2330 train_time:80916ms step_avg:60.88ms
step:1330/2330 train_time:80979ms step_avg:60.89ms
step:1331/2330 train_time:81038ms step_avg:60.89ms
step:1332/2330 train_time:81101ms step_avg:60.89ms
step:1333/2330 train_time:81160ms step_avg:60.89ms
step:1334/2330 train_time:81223ms step_avg:60.89ms
step:1335/2330 train_time:81283ms step_avg:60.89ms
step:1336/2330 train_time:81346ms step_avg:60.89ms
step:1337/2330 train_time:81405ms step_avg:60.89ms
step:1338/2330 train_time:81468ms step_avg:60.89ms
step:1339/2330 train_time:81528ms step_avg:60.89ms
step:1340/2330 train_time:81591ms step_avg:60.89ms
step:1341/2330 train_time:81650ms step_avg:60.89ms
step:1342/2330 train_time:81713ms step_avg:60.89ms
step:1343/2330 train_time:81774ms step_avg:60.89ms
step:1344/2330 train_time:81837ms step_avg:60.89ms
step:1345/2330 train_time:81897ms step_avg:60.89ms
step:1346/2330 train_time:81959ms step_avg:60.89ms
step:1347/2330 train_time:82019ms step_avg:60.89ms
step:1348/2330 train_time:82082ms step_avg:60.89ms
step:1349/2330 train_time:82141ms step_avg:60.89ms
step:1350/2330 train_time:82204ms step_avg:60.89ms
step:1351/2330 train_time:82264ms step_avg:60.89ms
step:1352/2330 train_time:82326ms step_avg:60.89ms
step:1353/2330 train_time:82386ms step_avg:60.89ms
step:1354/2330 train_time:82449ms step_avg:60.89ms
step:1355/2330 train_time:82508ms step_avg:60.89ms
step:1356/2330 train_time:82572ms step_avg:60.89ms
step:1357/2330 train_time:82632ms step_avg:60.89ms
step:1358/2330 train_time:82694ms step_avg:60.89ms
step:1359/2330 train_time:82754ms step_avg:60.89ms
step:1360/2330 train_time:82817ms step_avg:60.90ms
step:1361/2330 train_time:82877ms step_avg:60.89ms
step:1362/2330 train_time:82940ms step_avg:60.90ms
step:1363/2330 train_time:83000ms step_avg:60.89ms
step:1364/2330 train_time:83062ms step_avg:60.90ms
step:1365/2330 train_time:83122ms step_avg:60.90ms
step:1366/2330 train_time:83185ms step_avg:60.90ms
step:1367/2330 train_time:83245ms step_avg:60.90ms
step:1368/2330 train_time:83307ms step_avg:60.90ms
step:1369/2330 train_time:83367ms step_avg:60.90ms
step:1370/2330 train_time:83430ms step_avg:60.90ms
step:1371/2330 train_time:83489ms step_avg:60.90ms
step:1372/2330 train_time:83553ms step_avg:60.90ms
step:1373/2330 train_time:83613ms step_avg:60.90ms
step:1374/2330 train_time:83676ms step_avg:60.90ms
step:1375/2330 train_time:83736ms step_avg:60.90ms
step:1376/2330 train_time:83798ms step_avg:60.90ms
step:1377/2330 train_time:83858ms step_avg:60.90ms
step:1378/2330 train_time:83920ms step_avg:60.90ms
step:1379/2330 train_time:83980ms step_avg:60.90ms
step:1380/2330 train_time:84042ms step_avg:60.90ms
step:1381/2330 train_time:84102ms step_avg:60.90ms
step:1382/2330 train_time:84164ms step_avg:60.90ms
step:1383/2330 train_time:84224ms step_avg:60.90ms
step:1384/2330 train_time:84286ms step_avg:60.90ms
step:1385/2330 train_time:84346ms step_avg:60.90ms
step:1386/2330 train_time:84409ms step_avg:60.90ms
step:1387/2330 train_time:84469ms step_avg:60.90ms
step:1388/2330 train_time:84532ms step_avg:60.90ms
step:1389/2330 train_time:84592ms step_avg:60.90ms
step:1390/2330 train_time:84656ms step_avg:60.90ms
step:1391/2330 train_time:84716ms step_avg:60.90ms
step:1392/2330 train_time:84780ms step_avg:60.90ms
step:1393/2330 train_time:84839ms step_avg:60.90ms
step:1394/2330 train_time:84902ms step_avg:60.91ms
step:1395/2330 train_time:84962ms step_avg:60.90ms
step:1396/2330 train_time:85024ms step_avg:60.91ms
step:1397/2330 train_time:85084ms step_avg:60.90ms
step:1398/2330 train_time:85146ms step_avg:60.91ms
step:1399/2330 train_time:85206ms step_avg:60.90ms
step:1400/2330 train_time:85269ms step_avg:60.91ms
step:1401/2330 train_time:85328ms step_avg:60.91ms
step:1402/2330 train_time:85391ms step_avg:60.91ms
step:1403/2330 train_time:85451ms step_avg:60.91ms
step:1404/2330 train_time:85514ms step_avg:60.91ms
step:1405/2330 train_time:85575ms step_avg:60.91ms
step:1406/2330 train_time:85637ms step_avg:60.91ms
step:1407/2330 train_time:85697ms step_avg:60.91ms
step:1408/2330 train_time:85759ms step_avg:60.91ms
step:1409/2330 train_time:85819ms step_avg:60.91ms
step:1410/2330 train_time:85882ms step_avg:60.91ms
step:1411/2330 train_time:85941ms step_avg:60.91ms
step:1412/2330 train_time:86004ms step_avg:60.91ms
step:1413/2330 train_time:86063ms step_avg:60.91ms
step:1414/2330 train_time:86126ms step_avg:60.91ms
step:1415/2330 train_time:86185ms step_avg:60.91ms
step:1416/2330 train_time:86247ms step_avg:60.91ms
step:1417/2330 train_time:86307ms step_avg:60.91ms
step:1418/2330 train_time:86371ms step_avg:60.91ms
step:1419/2330 train_time:86430ms step_avg:60.91ms
step:1420/2330 train_time:86493ms step_avg:60.91ms
step:1421/2330 train_time:86553ms step_avg:60.91ms
step:1422/2330 train_time:86616ms step_avg:60.91ms
step:1423/2330 train_time:86676ms step_avg:60.91ms
step:1424/2330 train_time:86738ms step_avg:60.91ms
step:1425/2330 train_time:86798ms step_avg:60.91ms
step:1426/2330 train_time:86861ms step_avg:60.91ms
step:1427/2330 train_time:86921ms step_avg:60.91ms
step:1428/2330 train_time:86984ms step_avg:60.91ms
step:1429/2330 train_time:87043ms step_avg:60.91ms
step:1430/2330 train_time:87106ms step_avg:60.91ms
step:1431/2330 train_time:87166ms step_avg:60.91ms
step:1432/2330 train_time:87228ms step_avg:60.91ms
step:1433/2330 train_time:87288ms step_avg:60.91ms
step:1434/2330 train_time:87351ms step_avg:60.91ms
step:1435/2330 train_time:87411ms step_avg:60.91ms
step:1436/2330 train_time:87475ms step_avg:60.92ms
step:1437/2330 train_time:87536ms step_avg:60.92ms
step:1438/2330 train_time:87599ms step_avg:60.92ms
step:1439/2330 train_time:87659ms step_avg:60.92ms
step:1440/2330 train_time:87721ms step_avg:60.92ms
step:1441/2330 train_time:87780ms step_avg:60.92ms
step:1442/2330 train_time:87844ms step_avg:60.92ms
step:1443/2330 train_time:87904ms step_avg:60.92ms
step:1444/2330 train_time:87966ms step_avg:60.92ms
step:1445/2330 train_time:88026ms step_avg:60.92ms
step:1446/2330 train_time:88088ms step_avg:60.92ms
step:1447/2330 train_time:88148ms step_avg:60.92ms
step:1448/2330 train_time:88211ms step_avg:60.92ms
step:1449/2330 train_time:88271ms step_avg:60.92ms
step:1450/2330 train_time:88333ms step_avg:60.92ms
step:1451/2330 train_time:88393ms step_avg:60.92ms
step:1452/2330 train_time:88456ms step_avg:60.92ms
step:1453/2330 train_time:88516ms step_avg:60.92ms
step:1454/2330 train_time:88580ms step_avg:60.92ms
step:1455/2330 train_time:88639ms step_avg:60.92ms
step:1456/2330 train_time:88702ms step_avg:60.92ms
step:1457/2330 train_time:88761ms step_avg:60.92ms
step:1458/2330 train_time:88823ms step_avg:60.92ms
step:1459/2330 train_time:88883ms step_avg:60.92ms
step:1460/2330 train_time:88946ms step_avg:60.92ms
step:1461/2330 train_time:89005ms step_avg:60.92ms
step:1462/2330 train_time:89068ms step_avg:60.92ms
step:1463/2330 train_time:89128ms step_avg:60.92ms
step:1464/2330 train_time:89190ms step_avg:60.92ms
step:1465/2330 train_time:89250ms step_avg:60.92ms
step:1466/2330 train_time:89313ms step_avg:60.92ms
step:1467/2330 train_time:89373ms step_avg:60.92ms
step:1468/2330 train_time:89436ms step_avg:60.92ms
step:1469/2330 train_time:89496ms step_avg:60.92ms
step:1470/2330 train_time:89559ms step_avg:60.92ms
step:1471/2330 train_time:89618ms step_avg:60.92ms
step:1472/2330 train_time:89681ms step_avg:60.92ms
step:1473/2330 train_time:89740ms step_avg:60.92ms
step:1474/2330 train_time:89803ms step_avg:60.92ms
step:1475/2330 train_time:89862ms step_avg:60.92ms
step:1476/2330 train_time:89924ms step_avg:60.92ms
step:1477/2330 train_time:89984ms step_avg:60.92ms
step:1478/2330 train_time:90046ms step_avg:60.92ms
step:1479/2330 train_time:90105ms step_avg:60.92ms
step:1480/2330 train_time:90168ms step_avg:60.92ms
step:1481/2330 train_time:90228ms step_avg:60.92ms
step:1482/2330 train_time:90291ms step_avg:60.92ms
step:1483/2330 train_time:90350ms step_avg:60.92ms
step:1484/2330 train_time:90414ms step_avg:60.93ms
step:1485/2330 train_time:90475ms step_avg:60.93ms
step:1486/2330 train_time:90537ms step_avg:60.93ms
step:1487/2330 train_time:90597ms step_avg:60.93ms
step:1488/2330 train_time:90660ms step_avg:60.93ms
step:1489/2330 train_time:90720ms step_avg:60.93ms
step:1490/2330 train_time:90783ms step_avg:60.93ms
step:1491/2330 train_time:90843ms step_avg:60.93ms
step:1492/2330 train_time:90905ms step_avg:60.93ms
step:1493/2330 train_time:90964ms step_avg:60.93ms
step:1494/2330 train_time:91026ms step_avg:60.93ms
step:1495/2330 train_time:91086ms step_avg:60.93ms
step:1496/2330 train_time:91148ms step_avg:60.93ms
step:1497/2330 train_time:91208ms step_avg:60.93ms
step:1498/2330 train_time:91271ms step_avg:60.93ms
step:1499/2330 train_time:91331ms step_avg:60.93ms
step:1500/2330 train_time:91395ms step_avg:60.93ms
step:1500/2330 val_loss:3.4776 train_time:91460ms step_avg:60.97ms
step:1501/2330 train_time:91484ms step_avg:60.95ms
step:1502/2330 train_time:91522ms step_avg:60.93ms
step:1503/2330 train_time:91586ms step_avg:60.94ms
step:1504/2330 train_time:91652ms step_avg:60.94ms
step:1505/2330 train_time:91712ms step_avg:60.94ms
step:1506/2330 train_time:91775ms step_avg:60.94ms
step:1507/2330 train_time:91835ms step_avg:60.94ms
step:1508/2330 train_time:91898ms step_avg:60.94ms
step:1509/2330 train_time:91957ms step_avg:60.94ms
step:1510/2330 train_time:92019ms step_avg:60.94ms
step:1511/2330 train_time:92078ms step_avg:60.94ms
step:1512/2330 train_time:92141ms step_avg:60.94ms
step:1513/2330 train_time:92199ms step_avg:60.94ms
step:1514/2330 train_time:92261ms step_avg:60.94ms
step:1515/2330 train_time:92320ms step_avg:60.94ms
step:1516/2330 train_time:92382ms step_avg:60.94ms
step:1517/2330 train_time:92442ms step_avg:60.94ms
step:1518/2330 train_time:92506ms step_avg:60.94ms
step:1519/2330 train_time:92567ms step_avg:60.94ms
step:1520/2330 train_time:92630ms step_avg:60.94ms
step:1521/2330 train_time:92690ms step_avg:60.94ms
step:1522/2330 train_time:92753ms step_avg:60.94ms
step:1523/2330 train_time:92813ms step_avg:60.94ms
step:1524/2330 train_time:92876ms step_avg:60.94ms
step:1525/2330 train_time:92936ms step_avg:60.94ms
step:1526/2330 train_time:92999ms step_avg:60.94ms
step:1527/2330 train_time:93059ms step_avg:60.94ms
step:1528/2330 train_time:93121ms step_avg:60.94ms
step:1529/2330 train_time:93180ms step_avg:60.94ms
step:1530/2330 train_time:93243ms step_avg:60.94ms
step:1531/2330 train_time:93302ms step_avg:60.94ms
step:1532/2330 train_time:93365ms step_avg:60.94ms
step:1533/2330 train_time:93425ms step_avg:60.94ms
step:1534/2330 train_time:93489ms step_avg:60.94ms
step:1535/2330 train_time:93551ms step_avg:60.95ms
step:1536/2330 train_time:93615ms step_avg:60.95ms
step:1537/2330 train_time:93675ms step_avg:60.95ms
step:1538/2330 train_time:93739ms step_avg:60.95ms
step:1539/2330 train_time:93800ms step_avg:60.95ms
step:1540/2330 train_time:93863ms step_avg:60.95ms
step:1541/2330 train_time:93923ms step_avg:60.95ms
step:1542/2330 train_time:93987ms step_avg:60.95ms
step:1543/2330 train_time:94047ms step_avg:60.95ms
step:1544/2330 train_time:94110ms step_avg:60.95ms
step:1545/2330 train_time:94169ms step_avg:60.95ms
step:1546/2330 train_time:94232ms step_avg:60.95ms
step:1547/2330 train_time:94292ms step_avg:60.95ms
step:1548/2330 train_time:94357ms step_avg:60.95ms
step:1549/2330 train_time:94417ms step_avg:60.95ms
step:1550/2330 train_time:94481ms step_avg:60.96ms
step:1551/2330 train_time:94542ms step_avg:60.96ms
step:1552/2330 train_time:94606ms step_avg:60.96ms
step:1553/2330 train_time:94667ms step_avg:60.96ms
step:1554/2330 train_time:94730ms step_avg:60.96ms
step:1555/2330 train_time:94790ms step_avg:60.96ms
step:1556/2330 train_time:94853ms step_avg:60.96ms
step:1557/2330 train_time:94915ms step_avg:60.96ms
step:1558/2330 train_time:94978ms step_avg:60.96ms
step:1559/2330 train_time:95038ms step_avg:60.96ms
step:1560/2330 train_time:95101ms step_avg:60.96ms
step:1561/2330 train_time:95161ms step_avg:60.96ms
step:1562/2330 train_time:95224ms step_avg:60.96ms
step:1563/2330 train_time:95284ms step_avg:60.96ms
step:1564/2330 train_time:95347ms step_avg:60.96ms
step:1565/2330 train_time:95406ms step_avg:60.96ms
step:1566/2330 train_time:95469ms step_avg:60.96ms
step:1567/2330 train_time:95529ms step_avg:60.96ms
step:1568/2330 train_time:95593ms step_avg:60.97ms
step:1569/2330 train_time:95654ms step_avg:60.97ms
step:1570/2330 train_time:95718ms step_avg:60.97ms
step:1571/2330 train_time:95778ms step_avg:60.97ms
step:1572/2330 train_time:95842ms step_avg:60.97ms
step:1573/2330 train_time:95903ms step_avg:60.97ms
step:1574/2330 train_time:95966ms step_avg:60.97ms
step:1575/2330 train_time:96026ms step_avg:60.97ms
step:1576/2330 train_time:96089ms step_avg:60.97ms
step:1577/2330 train_time:96149ms step_avg:60.97ms
step:1578/2330 train_time:96213ms step_avg:60.97ms
step:1579/2330 train_time:96273ms step_avg:60.97ms
step:1580/2330 train_time:96337ms step_avg:60.97ms
step:1581/2330 train_time:96398ms step_avg:60.97ms
step:1582/2330 train_time:96461ms step_avg:60.97ms
step:1583/2330 train_time:96520ms step_avg:60.97ms
step:1584/2330 train_time:96583ms step_avg:60.97ms
step:1585/2330 train_time:96644ms step_avg:60.97ms
step:1586/2330 train_time:96707ms step_avg:60.98ms
step:1587/2330 train_time:96767ms step_avg:60.97ms
step:1588/2330 train_time:96830ms step_avg:60.98ms
step:1589/2330 train_time:96891ms step_avg:60.98ms
step:1590/2330 train_time:96955ms step_avg:60.98ms
step:1591/2330 train_time:97015ms step_avg:60.98ms
step:1592/2330 train_time:97079ms step_avg:60.98ms
step:1593/2330 train_time:97140ms step_avg:60.98ms
step:1594/2330 train_time:97203ms step_avg:60.98ms
step:1595/2330 train_time:97263ms step_avg:60.98ms
step:1596/2330 train_time:97326ms step_avg:60.98ms
step:1597/2330 train_time:97386ms step_avg:60.98ms
step:1598/2330 train_time:97449ms step_avg:60.98ms
step:1599/2330 train_time:97509ms step_avg:60.98ms
step:1600/2330 train_time:97572ms step_avg:60.98ms
step:1601/2330 train_time:97633ms step_avg:60.98ms
step:1602/2330 train_time:97696ms step_avg:60.98ms
step:1603/2330 train_time:97756ms step_avg:60.98ms
step:1604/2330 train_time:97820ms step_avg:60.99ms
step:1605/2330 train_time:97881ms step_avg:60.98ms
step:1606/2330 train_time:97944ms step_avg:60.99ms
step:1607/2330 train_time:98004ms step_avg:60.99ms
step:1608/2330 train_time:98067ms step_avg:60.99ms
step:1609/2330 train_time:98127ms step_avg:60.99ms
step:1610/2330 train_time:98191ms step_avg:60.99ms
step:1611/2330 train_time:98251ms step_avg:60.99ms
step:1612/2330 train_time:98314ms step_avg:60.99ms
step:1613/2330 train_time:98375ms step_avg:60.99ms
step:1614/2330 train_time:98439ms step_avg:60.99ms
step:1615/2330 train_time:98498ms step_avg:60.99ms
step:1616/2330 train_time:98562ms step_avg:60.99ms
step:1617/2330 train_time:98622ms step_avg:60.99ms
step:1618/2330 train_time:98685ms step_avg:60.99ms
step:1619/2330 train_time:98745ms step_avg:60.99ms
step:1620/2330 train_time:98808ms step_avg:60.99ms
step:1621/2330 train_time:98868ms step_avg:60.99ms
step:1622/2330 train_time:98931ms step_avg:60.99ms
step:1623/2330 train_time:98991ms step_avg:60.99ms
step:1624/2330 train_time:99055ms step_avg:60.99ms
step:1625/2330 train_time:99116ms step_avg:60.99ms
step:1626/2330 train_time:99180ms step_avg:61.00ms
step:1627/2330 train_time:99240ms step_avg:61.00ms
step:1628/2330 train_time:99304ms step_avg:61.00ms
step:1629/2330 train_time:99364ms step_avg:61.00ms
step:1630/2330 train_time:99427ms step_avg:61.00ms
step:1631/2330 train_time:99487ms step_avg:61.00ms
step:1632/2330 train_time:99551ms step_avg:61.00ms
step:1633/2330 train_time:99612ms step_avg:61.00ms
step:1634/2330 train_time:99676ms step_avg:61.00ms
step:1635/2330 train_time:99737ms step_avg:61.00ms
step:1636/2330 train_time:99802ms step_avg:61.00ms
step:1637/2330 train_time:99861ms step_avg:61.00ms
step:1638/2330 train_time:99924ms step_avg:61.00ms
step:1639/2330 train_time:99984ms step_avg:61.00ms
step:1640/2330 train_time:100047ms step_avg:61.00ms
step:1641/2330 train_time:100108ms step_avg:61.00ms
step:1642/2330 train_time:100171ms step_avg:61.01ms
step:1643/2330 train_time:100233ms step_avg:61.01ms
step:1644/2330 train_time:100296ms step_avg:61.01ms
step:1645/2330 train_time:100357ms step_avg:61.01ms
step:1646/2330 train_time:100421ms step_avg:61.01ms
step:1647/2330 train_time:100481ms step_avg:61.01ms
step:1648/2330 train_time:100545ms step_avg:61.01ms
step:1649/2330 train_time:100605ms step_avg:61.01ms
step:1650/2330 train_time:100669ms step_avg:61.01ms
step:1651/2330 train_time:100729ms step_avg:61.01ms
step:1652/2330 train_time:100792ms step_avg:61.01ms
step:1653/2330 train_time:100852ms step_avg:61.01ms
step:1654/2330 train_time:100915ms step_avg:61.01ms
step:1655/2330 train_time:100975ms step_avg:61.01ms
step:1656/2330 train_time:101039ms step_avg:61.01ms
step:1657/2330 train_time:101099ms step_avg:61.01ms
step:1658/2330 train_time:101163ms step_avg:61.01ms
step:1659/2330 train_time:101223ms step_avg:61.01ms
step:1660/2330 train_time:101286ms step_avg:61.02ms
step:1661/2330 train_time:101347ms step_avg:61.02ms
step:1662/2330 train_time:101409ms step_avg:61.02ms
step:1663/2330 train_time:101469ms step_avg:61.02ms
step:1664/2330 train_time:101532ms step_avg:61.02ms
step:1665/2330 train_time:101593ms step_avg:61.02ms
step:1666/2330 train_time:101657ms step_avg:61.02ms
step:1667/2330 train_time:101716ms step_avg:61.02ms
step:1668/2330 train_time:101779ms step_avg:61.02ms
step:1669/2330 train_time:101840ms step_avg:61.02ms
step:1670/2330 train_time:101903ms step_avg:61.02ms
step:1671/2330 train_time:101963ms step_avg:61.02ms
step:1672/2330 train_time:102026ms step_avg:61.02ms
step:1673/2330 train_time:102086ms step_avg:61.02ms
step:1674/2330 train_time:102149ms step_avg:61.02ms
step:1675/2330 train_time:102209ms step_avg:61.02ms
step:1676/2330 train_time:102272ms step_avg:61.02ms
step:1677/2330 train_time:102333ms step_avg:61.02ms
step:1678/2330 train_time:102397ms step_avg:61.02ms
step:1679/2330 train_time:102458ms step_avg:61.02ms
step:1680/2330 train_time:102521ms step_avg:61.02ms
step:1681/2330 train_time:102581ms step_avg:61.02ms
step:1682/2330 train_time:102644ms step_avg:61.03ms
step:1683/2330 train_time:102705ms step_avg:61.02ms
step:1684/2330 train_time:102768ms step_avg:61.03ms
step:1685/2330 train_time:102828ms step_avg:61.03ms
step:1686/2330 train_time:102892ms step_avg:61.03ms
step:1687/2330 train_time:102952ms step_avg:61.03ms
step:1688/2330 train_time:103016ms step_avg:61.03ms
step:1689/2330 train_time:103076ms step_avg:61.03ms
step:1690/2330 train_time:103140ms step_avg:61.03ms
step:1691/2330 train_time:103201ms step_avg:61.03ms
step:1692/2330 train_time:103264ms step_avg:61.03ms
step:1693/2330 train_time:103324ms step_avg:61.03ms
step:1694/2330 train_time:103387ms step_avg:61.03ms
step:1695/2330 train_time:103447ms step_avg:61.03ms
step:1696/2330 train_time:103510ms step_avg:61.03ms
step:1697/2330 train_time:103570ms step_avg:61.03ms
step:1698/2330 train_time:103634ms step_avg:61.03ms
step:1699/2330 train_time:103694ms step_avg:61.03ms
step:1700/2330 train_time:103758ms step_avg:61.03ms
step:1701/2330 train_time:103818ms step_avg:61.03ms
step:1702/2330 train_time:103882ms step_avg:61.04ms
step:1703/2330 train_time:103942ms step_avg:61.03ms
step:1704/2330 train_time:104005ms step_avg:61.04ms
step:1705/2330 train_time:104066ms step_avg:61.04ms
step:1706/2330 train_time:104129ms step_avg:61.04ms
step:1707/2330 train_time:104189ms step_avg:61.04ms
step:1708/2330 train_time:104253ms step_avg:61.04ms
step:1709/2330 train_time:104314ms step_avg:61.04ms
step:1710/2330 train_time:104378ms step_avg:61.04ms
step:1711/2330 train_time:104439ms step_avg:61.04ms
step:1712/2330 train_time:104503ms step_avg:61.04ms
step:1713/2330 train_time:104563ms step_avg:61.04ms
step:1714/2330 train_time:104626ms step_avg:61.04ms
step:1715/2330 train_time:104686ms step_avg:61.04ms
step:1716/2330 train_time:104749ms step_avg:61.04ms
step:1717/2330 train_time:104808ms step_avg:61.04ms
step:1718/2330 train_time:104871ms step_avg:61.04ms
step:1719/2330 train_time:104932ms step_avg:61.04ms
step:1720/2330 train_time:104995ms step_avg:61.04ms
step:1721/2330 train_time:105056ms step_avg:61.04ms
step:1722/2330 train_time:105120ms step_avg:61.05ms
step:1723/2330 train_time:105180ms step_avg:61.04ms
step:1724/2330 train_time:105243ms step_avg:61.05ms
step:1725/2330 train_time:105303ms step_avg:61.05ms
step:1726/2330 train_time:105367ms step_avg:61.05ms
step:1727/2330 train_time:105427ms step_avg:61.05ms
step:1728/2330 train_time:105490ms step_avg:61.05ms
step:1729/2330 train_time:105550ms step_avg:61.05ms
step:1730/2330 train_time:105614ms step_avg:61.05ms
step:1731/2330 train_time:105674ms step_avg:61.05ms
step:1732/2330 train_time:105739ms step_avg:61.05ms
step:1733/2330 train_time:105799ms step_avg:61.05ms
step:1734/2330 train_time:105863ms step_avg:61.05ms
step:1735/2330 train_time:105923ms step_avg:61.05ms
step:1736/2330 train_time:105986ms step_avg:61.05ms
step:1737/2330 train_time:106045ms step_avg:61.05ms
step:1738/2330 train_time:106109ms step_avg:61.05ms
step:1739/2330 train_time:106169ms step_avg:61.05ms
step:1740/2330 train_time:106232ms step_avg:61.05ms
step:1741/2330 train_time:106293ms step_avg:61.05ms
step:1742/2330 train_time:106357ms step_avg:61.05ms
step:1743/2330 train_time:106418ms step_avg:61.05ms
step:1744/2330 train_time:106481ms step_avg:61.06ms
step:1745/2330 train_time:106541ms step_avg:61.06ms
step:1746/2330 train_time:106604ms step_avg:61.06ms
step:1747/2330 train_time:106664ms step_avg:61.06ms
step:1748/2330 train_time:106727ms step_avg:61.06ms
step:1749/2330 train_time:106789ms step_avg:61.06ms
step:1750/2330 train_time:106850ms step_avg:61.06ms
step:1750/2330 val_loss:3.4344 train_time:106914ms step_avg:61.09ms
step:1751/2330 train_time:106938ms step_avg:61.07ms
step:1752/2330 train_time:106975ms step_avg:61.06ms
step:1753/2330 train_time:107040ms step_avg:61.06ms
step:1754/2330 train_time:107107ms step_avg:61.06ms
step:1755/2330 train_time:107167ms step_avg:61.06ms
step:1756/2330 train_time:107231ms step_avg:61.07ms
step:1757/2330 train_time:107290ms step_avg:61.06ms
step:1758/2330 train_time:107354ms step_avg:61.07ms
step:1759/2330 train_time:107414ms step_avg:61.07ms
step:1760/2330 train_time:107476ms step_avg:61.07ms
step:1761/2330 train_time:107535ms step_avg:61.06ms
step:1762/2330 train_time:107597ms step_avg:61.07ms
step:1763/2330 train_time:107657ms step_avg:61.06ms
step:1764/2330 train_time:107719ms step_avg:61.07ms
step:1765/2330 train_time:107778ms step_avg:61.06ms
step:1766/2330 train_time:107842ms step_avg:61.07ms
step:1767/2330 train_time:107904ms step_avg:61.07ms
step:1768/2330 train_time:107969ms step_avg:61.07ms
step:1769/2330 train_time:108030ms step_avg:61.07ms
step:1770/2330 train_time:108093ms step_avg:61.07ms
step:1771/2330 train_time:108154ms step_avg:61.07ms
step:1772/2330 train_time:108217ms step_avg:61.07ms
step:1773/2330 train_time:108277ms step_avg:61.07ms
step:1774/2330 train_time:108340ms step_avg:61.07ms
step:1775/2330 train_time:108399ms step_avg:61.07ms
step:1776/2330 train_time:108462ms step_avg:61.07ms
step:1777/2330 train_time:108522ms step_avg:61.07ms
step:1778/2330 train_time:108585ms step_avg:61.07ms
step:1779/2330 train_time:108645ms step_avg:61.07ms
step:1780/2330 train_time:108709ms step_avg:61.07ms
step:1781/2330 train_time:108769ms step_avg:61.07ms
step:1782/2330 train_time:108832ms step_avg:61.07ms
step:1783/2330 train_time:108892ms step_avg:61.07ms
step:1784/2330 train_time:108956ms step_avg:61.07ms
step:1785/2330 train_time:109017ms step_avg:61.07ms
step:1786/2330 train_time:109080ms step_avg:61.07ms
step:1787/2330 train_time:109140ms step_avg:61.07ms
step:1788/2330 train_time:109204ms step_avg:61.08ms
step:1789/2330 train_time:109265ms step_avg:61.08ms
step:1790/2330 train_time:109329ms step_avg:61.08ms
step:1791/2330 train_time:109390ms step_avg:61.08ms
step:1792/2330 train_time:109453ms step_avg:61.08ms
step:1793/2330 train_time:109513ms step_avg:61.08ms
step:1794/2330 train_time:109575ms step_avg:61.08ms
step:1795/2330 train_time:109635ms step_avg:61.08ms
step:1796/2330 train_time:109698ms step_avg:61.08ms
step:1797/2330 train_time:109758ms step_avg:61.08ms
step:1798/2330 train_time:109821ms step_avg:61.08ms
step:1799/2330 train_time:109881ms step_avg:61.08ms
step:1800/2330 train_time:109946ms step_avg:61.08ms
step:1801/2330 train_time:110006ms step_avg:61.08ms
step:1802/2330 train_time:110070ms step_avg:61.08ms
step:1803/2330 train_time:110130ms step_avg:61.08ms
step:1804/2330 train_time:110193ms step_avg:61.08ms
step:1805/2330 train_time:110253ms step_avg:61.08ms
step:1806/2330 train_time:110318ms step_avg:61.08ms
step:1807/2330 train_time:110377ms step_avg:61.08ms
step:1808/2330 train_time:110440ms step_avg:61.08ms
step:1809/2330 train_time:110499ms step_avg:61.08ms
step:1810/2330 train_time:110563ms step_avg:61.08ms
step:1811/2330 train_time:110623ms step_avg:61.08ms
step:1812/2330 train_time:110687ms step_avg:61.09ms
step:1813/2330 train_time:110747ms step_avg:61.08ms
step:1814/2330 train_time:110811ms step_avg:61.09ms
step:1815/2330 train_time:110871ms step_avg:61.09ms
step:1816/2330 train_time:110934ms step_avg:61.09ms
step:1817/2330 train_time:110994ms step_avg:61.09ms
step:1818/2330 train_time:111057ms step_avg:61.09ms
step:1819/2330 train_time:111118ms step_avg:61.09ms
step:1820/2330 train_time:111181ms step_avg:61.09ms
step:1821/2330 train_time:111241ms step_avg:61.09ms
step:1822/2330 train_time:111305ms step_avg:61.09ms
step:1823/2330 train_time:111366ms step_avg:61.09ms
step:1824/2330 train_time:111429ms step_avg:61.09ms
step:1825/2330 train_time:111489ms step_avg:61.09ms
step:1826/2330 train_time:111552ms step_avg:61.09ms
step:1827/2330 train_time:111612ms step_avg:61.09ms
step:1828/2330 train_time:111675ms step_avg:61.09ms
step:1829/2330 train_time:111735ms step_avg:61.09ms
step:1830/2330 train_time:111797ms step_avg:61.09ms
step:1831/2330 train_time:111857ms step_avg:61.09ms
step:1832/2330 train_time:111920ms step_avg:61.09ms
step:1833/2330 train_time:111981ms step_avg:61.09ms
step:1834/2330 train_time:112044ms step_avg:61.09ms
step:1835/2330 train_time:112104ms step_avg:61.09ms
step:1836/2330 train_time:112168ms step_avg:61.09ms
step:1837/2330 train_time:112229ms step_avg:61.09ms
step:1838/2330 train_time:112292ms step_avg:61.09ms
step:1839/2330 train_time:112352ms step_avg:61.09ms
step:1840/2330 train_time:112416ms step_avg:61.10ms
step:1841/2330 train_time:112475ms step_avg:61.09ms
step:1842/2330 train_time:112538ms step_avg:61.10ms
step:1843/2330 train_time:112598ms step_avg:61.09ms
step:1844/2330 train_time:112661ms step_avg:61.10ms
step:1845/2330 train_time:112722ms step_avg:61.10ms
step:1846/2330 train_time:112785ms step_avg:61.10ms
step:1847/2330 train_time:112846ms step_avg:61.10ms
step:1848/2330 train_time:112909ms step_avg:61.10ms
step:1849/2330 train_time:112970ms step_avg:61.10ms
step:1850/2330 train_time:113033ms step_avg:61.10ms
step:1851/2330 train_time:113094ms step_avg:61.10ms
step:1852/2330 train_time:113157ms step_avg:61.10ms
step:1853/2330 train_time:113217ms step_avg:61.10ms
step:1854/2330 train_time:113280ms step_avg:61.10ms
step:1855/2330 train_time:113340ms step_avg:61.10ms
step:1856/2330 train_time:113404ms step_avg:61.10ms
step:1857/2330 train_time:113465ms step_avg:61.10ms
step:1858/2330 train_time:113528ms step_avg:61.10ms
step:1859/2330 train_time:113589ms step_avg:61.10ms
step:1860/2330 train_time:113652ms step_avg:61.10ms
step:1861/2330 train_time:113712ms step_avg:61.10ms
step:1862/2330 train_time:113775ms step_avg:61.10ms
step:1863/2330 train_time:113835ms step_avg:61.10ms
step:1864/2330 train_time:113898ms step_avg:61.10ms
step:1865/2330 train_time:113958ms step_avg:61.10ms
step:1866/2330 train_time:114021ms step_avg:61.10ms
step:1867/2330 train_time:114081ms step_avg:61.10ms
step:1868/2330 train_time:114144ms step_avg:61.11ms
step:1869/2330 train_time:114205ms step_avg:61.10ms
step:1870/2330 train_time:114268ms step_avg:61.11ms
step:1871/2330 train_time:114329ms step_avg:61.11ms
step:1872/2330 train_time:114392ms step_avg:61.11ms
step:1873/2330 train_time:114452ms step_avg:61.11ms
step:1874/2330 train_time:114515ms step_avg:61.11ms
step:1875/2330 train_time:114575ms step_avg:61.11ms
step:1876/2330 train_time:114638ms step_avg:61.11ms
step:1877/2330 train_time:114698ms step_avg:61.11ms
step:1878/2330 train_time:114761ms step_avg:61.11ms
step:1879/2330 train_time:114821ms step_avg:61.11ms
step:1880/2330 train_time:114884ms step_avg:61.11ms
step:1881/2330 train_time:114945ms step_avg:61.11ms
step:1882/2330 train_time:115009ms step_avg:61.11ms
step:1883/2330 train_time:115070ms step_avg:61.11ms
step:1884/2330 train_time:115133ms step_avg:61.11ms
step:1885/2330 train_time:115193ms step_avg:61.11ms
step:1886/2330 train_time:115256ms step_avg:61.11ms
step:1887/2330 train_time:115317ms step_avg:61.11ms
step:1888/2330 train_time:115380ms step_avg:61.11ms
step:1889/2330 train_time:115440ms step_avg:61.11ms
step:1890/2330 train_time:115503ms step_avg:61.11ms
step:1891/2330 train_time:115564ms step_avg:61.11ms
step:1892/2330 train_time:115627ms step_avg:61.11ms
step:1893/2330 train_time:115687ms step_avg:61.11ms
step:1894/2330 train_time:115751ms step_avg:61.11ms
step:1895/2330 train_time:115812ms step_avg:61.11ms
step:1896/2330 train_time:115875ms step_avg:61.12ms
step:1897/2330 train_time:115934ms step_avg:61.11ms
step:1898/2330 train_time:115997ms step_avg:61.12ms
step:1899/2330 train_time:116057ms step_avg:61.11ms
step:1900/2330 train_time:116120ms step_avg:61.12ms
step:1901/2330 train_time:116180ms step_avg:61.12ms
step:1902/2330 train_time:116243ms step_avg:61.12ms
step:1903/2330 train_time:116304ms step_avg:61.12ms
step:1904/2330 train_time:116367ms step_avg:61.12ms
step:1905/2330 train_time:116428ms step_avg:61.12ms
step:1906/2330 train_time:116491ms step_avg:61.12ms
step:1907/2330 train_time:116551ms step_avg:61.12ms
step:1908/2330 train_time:116614ms step_avg:61.12ms
step:1909/2330 train_time:116674ms step_avg:61.12ms
step:1910/2330 train_time:116737ms step_avg:61.12ms
step:1911/2330 train_time:116797ms step_avg:61.12ms
step:1912/2330 train_time:116860ms step_avg:61.12ms
step:1913/2330 train_time:116921ms step_avg:61.12ms
step:1914/2330 train_time:116983ms step_avg:61.12ms
step:1915/2330 train_time:117043ms step_avg:61.12ms
step:1916/2330 train_time:117106ms step_avg:61.12ms
step:1917/2330 train_time:117166ms step_avg:61.12ms
step:1918/2330 train_time:117229ms step_avg:61.12ms
step:1919/2330 train_time:117290ms step_avg:61.12ms
step:1920/2330 train_time:117353ms step_avg:61.12ms
step:1921/2330 train_time:117414ms step_avg:61.12ms
step:1922/2330 train_time:117477ms step_avg:61.12ms
step:1923/2330 train_time:117537ms step_avg:61.12ms
step:1924/2330 train_time:117600ms step_avg:61.12ms
step:1925/2330 train_time:117660ms step_avg:61.12ms
step:1926/2330 train_time:117724ms step_avg:61.12ms
step:1927/2330 train_time:117784ms step_avg:61.12ms
step:1928/2330 train_time:117848ms step_avg:61.12ms
step:1929/2330 train_time:117909ms step_avg:61.12ms
step:1930/2330 train_time:117973ms step_avg:61.13ms
step:1931/2330 train_time:118033ms step_avg:61.13ms
step:1932/2330 train_time:118097ms step_avg:61.13ms
step:1933/2330 train_time:118157ms step_avg:61.13ms
step:1934/2330 train_time:118220ms step_avg:61.13ms
step:1935/2330 train_time:118280ms step_avg:61.13ms
step:1936/2330 train_time:118344ms step_avg:61.13ms
step:1937/2330 train_time:118404ms step_avg:61.13ms
step:1938/2330 train_time:118468ms step_avg:61.13ms
step:1939/2330 train_time:118528ms step_avg:61.13ms
step:1940/2330 train_time:118592ms step_avg:61.13ms
step:1941/2330 train_time:118653ms step_avg:61.13ms
step:1942/2330 train_time:118716ms step_avg:61.13ms
step:1943/2330 train_time:118776ms step_avg:61.13ms
step:1944/2330 train_time:118840ms step_avg:61.13ms
step:1945/2330 train_time:118900ms step_avg:61.13ms
step:1946/2330 train_time:118963ms step_avg:61.13ms
step:1947/2330 train_time:119023ms step_avg:61.13ms
step:1948/2330 train_time:119086ms step_avg:61.13ms
step:1949/2330 train_time:119147ms step_avg:61.13ms
step:1950/2330 train_time:119211ms step_avg:61.13ms
step:1951/2330 train_time:119271ms step_avg:61.13ms
step:1952/2330 train_time:119334ms step_avg:61.13ms
step:1953/2330 train_time:119394ms step_avg:61.13ms
step:1954/2330 train_time:119457ms step_avg:61.13ms
step:1955/2330 train_time:119517ms step_avg:61.13ms
step:1956/2330 train_time:119580ms step_avg:61.13ms
step:1957/2330 train_time:119640ms step_avg:61.13ms
step:1958/2330 train_time:119704ms step_avg:61.14ms
step:1959/2330 train_time:119764ms step_avg:61.14ms
step:1960/2330 train_time:119828ms step_avg:61.14ms
step:1961/2330 train_time:119889ms step_avg:61.14ms
step:1962/2330 train_time:119952ms step_avg:61.14ms
step:1963/2330 train_time:120012ms step_avg:61.14ms
step:1964/2330 train_time:120075ms step_avg:61.14ms
step:1965/2330 train_time:120136ms step_avg:61.14ms
step:1966/2330 train_time:120199ms step_avg:61.14ms
step:1967/2330 train_time:120259ms step_avg:61.14ms
step:1968/2330 train_time:120323ms step_avg:61.14ms
step:1969/2330 train_time:120383ms step_avg:61.14ms
step:1970/2330 train_time:120447ms step_avg:61.14ms
step:1971/2330 train_time:120508ms step_avg:61.14ms
step:1972/2330 train_time:120571ms step_avg:61.14ms
step:1973/2330 train_time:120631ms step_avg:61.14ms
step:1974/2330 train_time:120694ms step_avg:61.14ms
step:1975/2330 train_time:120754ms step_avg:61.14ms
step:1976/2330 train_time:120818ms step_avg:61.14ms
step:1977/2330 train_time:120878ms step_avg:61.14ms
step:1978/2330 train_time:120940ms step_avg:61.14ms
step:1979/2330 train_time:121000ms step_avg:61.14ms
step:1980/2330 train_time:121064ms step_avg:61.14ms
step:1981/2330 train_time:121124ms step_avg:61.14ms
step:1982/2330 train_time:121188ms step_avg:61.14ms
step:1983/2330 train_time:121249ms step_avg:61.14ms
step:1984/2330 train_time:121312ms step_avg:61.15ms
step:1985/2330 train_time:121372ms step_avg:61.14ms
step:1986/2330 train_time:121435ms step_avg:61.15ms
step:1987/2330 train_time:121495ms step_avg:61.14ms
step:1988/2330 train_time:121558ms step_avg:61.15ms
step:1989/2330 train_time:121618ms step_avg:61.15ms
step:1990/2330 train_time:121681ms step_avg:61.15ms
step:1991/2330 train_time:121741ms step_avg:61.15ms
step:1992/2330 train_time:121805ms step_avg:61.15ms
step:1993/2330 train_time:121865ms step_avg:61.15ms
step:1994/2330 train_time:121929ms step_avg:61.15ms
step:1995/2330 train_time:121989ms step_avg:61.15ms
step:1996/2330 train_time:122052ms step_avg:61.15ms
step:1997/2330 train_time:122112ms step_avg:61.15ms
step:1998/2330 train_time:122176ms step_avg:61.15ms
step:1999/2330 train_time:122236ms step_avg:61.15ms
step:2000/2330 train_time:122299ms step_avg:61.15ms
step:2000/2330 val_loss:3.4155 train_time:122364ms step_avg:61.18ms
step:2001/2330 train_time:122388ms step_avg:61.16ms
step:2002/2330 train_time:122428ms step_avg:61.15ms
step:2003/2330 train_time:122492ms step_avg:61.15ms
step:2004/2330 train_time:122556ms step_avg:61.16ms
step:2005/2330 train_time:122617ms step_avg:61.16ms
step:2006/2330 train_time:122680ms step_avg:61.16ms
step:2007/2330 train_time:122740ms step_avg:61.16ms
step:2008/2330 train_time:122803ms step_avg:61.16ms
step:2009/2330 train_time:122862ms step_avg:61.16ms
step:2010/2330 train_time:122925ms step_avg:61.16ms
step:2011/2330 train_time:122985ms step_avg:61.16ms
step:2012/2330 train_time:123048ms step_avg:61.16ms
step:2013/2330 train_time:123107ms step_avg:61.16ms
step:2014/2330 train_time:123170ms step_avg:61.16ms
step:2015/2330 train_time:123230ms step_avg:61.16ms
step:2016/2330 train_time:123293ms step_avg:61.16ms
step:2017/2330 train_time:123354ms step_avg:61.16ms
step:2018/2330 train_time:123418ms step_avg:61.16ms
step:2019/2330 train_time:123480ms step_avg:61.16ms
step:2020/2330 train_time:123544ms step_avg:61.16ms
step:2021/2330 train_time:123605ms step_avg:61.16ms
step:2022/2330 train_time:123668ms step_avg:61.16ms
step:2023/2330 train_time:123728ms step_avg:61.16ms
step:2024/2330 train_time:123791ms step_avg:61.16ms
step:2025/2330 train_time:123851ms step_avg:61.16ms
step:2026/2330 train_time:123913ms step_avg:61.16ms
step:2027/2330 train_time:123972ms step_avg:61.16ms
step:2028/2330 train_time:124035ms step_avg:61.16ms
step:2029/2330 train_time:124095ms step_avg:61.16ms
step:2030/2330 train_time:124158ms step_avg:61.16ms
step:2031/2330 train_time:124219ms step_avg:61.16ms
step:2032/2330 train_time:124282ms step_avg:61.16ms
step:2033/2330 train_time:124343ms step_avg:61.16ms
step:2034/2330 train_time:124407ms step_avg:61.16ms
step:2035/2330 train_time:124467ms step_avg:61.16ms
step:2036/2330 train_time:124531ms step_avg:61.16ms
step:2037/2330 train_time:124591ms step_avg:61.16ms
step:2038/2330 train_time:124655ms step_avg:61.17ms
step:2039/2330 train_time:124715ms step_avg:61.16ms
step:2040/2330 train_time:124779ms step_avg:61.17ms
step:2041/2330 train_time:124839ms step_avg:61.17ms
step:2042/2330 train_time:124903ms step_avg:61.17ms
step:2043/2330 train_time:124964ms step_avg:61.17ms
step:2044/2330 train_time:125027ms step_avg:61.17ms
step:2045/2330 train_time:125086ms step_avg:61.17ms
step:2046/2330 train_time:125149ms step_avg:61.17ms
step:2047/2330 train_time:125210ms step_avg:61.17ms
step:2048/2330 train_time:125272ms step_avg:61.17ms
step:2049/2330 train_time:125333ms step_avg:61.17ms
step:2050/2330 train_time:125396ms step_avg:61.17ms
step:2051/2330 train_time:125458ms step_avg:61.17ms
step:2052/2330 train_time:125522ms step_avg:61.17ms
step:2053/2330 train_time:125582ms step_avg:61.17ms
step:2054/2330 train_time:125645ms step_avg:61.17ms
step:2055/2330 train_time:125705ms step_avg:61.17ms
step:2056/2330 train_time:125769ms step_avg:61.17ms
step:2057/2330 train_time:125829ms step_avg:61.17ms
step:2058/2330 train_time:125892ms step_avg:61.17ms
step:2059/2330 train_time:125953ms step_avg:61.17ms
step:2060/2330 train_time:126016ms step_avg:61.17ms
step:2061/2330 train_time:126076ms step_avg:61.17ms
step:2062/2330 train_time:126140ms step_avg:61.17ms
step:2063/2330 train_time:126201ms step_avg:61.17ms
step:2064/2330 train_time:126264ms step_avg:61.17ms
step:2065/2330 train_time:126325ms step_avg:61.17ms
step:2066/2330 train_time:126388ms step_avg:61.18ms
step:2067/2330 train_time:126448ms step_avg:61.17ms
step:2068/2330 train_time:126511ms step_avg:61.18ms
step:2069/2330 train_time:126571ms step_avg:61.18ms
step:2070/2330 train_time:126636ms step_avg:61.18ms
step:2071/2330 train_time:126696ms step_avg:61.18ms
step:2072/2330 train_time:126761ms step_avg:61.18ms
step:2073/2330 train_time:126821ms step_avg:61.18ms
step:2074/2330 train_time:126885ms step_avg:61.18ms
step:2075/2330 train_time:126945ms step_avg:61.18ms
step:2076/2330 train_time:127008ms step_avg:61.18ms
step:2077/2330 train_time:127068ms step_avg:61.18ms
step:2078/2330 train_time:127131ms step_avg:61.18ms
step:2079/2330 train_time:127190ms step_avg:61.18ms
step:2080/2330 train_time:127253ms step_avg:61.18ms
step:2081/2330 train_time:127313ms step_avg:61.18ms
step:2082/2330 train_time:127377ms step_avg:61.18ms
step:2083/2330 train_time:127438ms step_avg:61.18ms
step:2084/2330 train_time:127502ms step_avg:61.18ms
step:2085/2330 train_time:127563ms step_avg:61.18ms
step:2086/2330 train_time:127627ms step_avg:61.18ms
step:2087/2330 train_time:127687ms step_avg:61.18ms
step:2088/2330 train_time:127750ms step_avg:61.18ms
step:2089/2330 train_time:127810ms step_avg:61.18ms
step:2090/2330 train_time:127873ms step_avg:61.18ms
step:2091/2330 train_time:127934ms step_avg:61.18ms
step:2092/2330 train_time:127997ms step_avg:61.18ms
step:2093/2330 train_time:128058ms step_avg:61.18ms
step:2094/2330 train_time:128122ms step_avg:61.19ms
step:2095/2330 train_time:128182ms step_avg:61.18ms
step:2096/2330 train_time:128246ms step_avg:61.19ms
step:2097/2330 train_time:128307ms step_avg:61.19ms
step:2098/2330 train_time:128370ms step_avg:61.19ms
step:2099/2330 train_time:128430ms step_avg:61.19ms
step:2100/2330 train_time:128494ms step_avg:61.19ms
step:2101/2330 train_time:128553ms step_avg:61.19ms
step:2102/2330 train_time:128617ms step_avg:61.19ms
step:2103/2330 train_time:128678ms step_avg:61.19ms
step:2104/2330 train_time:128742ms step_avg:61.19ms
step:2105/2330 train_time:128802ms step_avg:61.19ms
step:2106/2330 train_time:128866ms step_avg:61.19ms
step:2107/2330 train_time:128926ms step_avg:61.19ms
step:2108/2330 train_time:128989ms step_avg:61.19ms
step:2109/2330 train_time:129049ms step_avg:61.19ms
step:2110/2330 train_time:129112ms step_avg:61.19ms
step:2111/2330 train_time:129172ms step_avg:61.19ms
step:2112/2330 train_time:129235ms step_avg:61.19ms
step:2113/2330 train_time:129295ms step_avg:61.19ms
step:2114/2330 train_time:129359ms step_avg:61.19ms
step:2115/2330 train_time:129419ms step_avg:61.19ms
step:2116/2330 train_time:129483ms step_avg:61.19ms
step:2117/2330 train_time:129542ms step_avg:61.19ms
step:2118/2330 train_time:129606ms step_avg:61.19ms
step:2119/2330 train_time:129666ms step_avg:61.19ms
step:2120/2330 train_time:129729ms step_avg:61.19ms
step:2121/2330 train_time:129789ms step_avg:61.19ms
step:2122/2330 train_time:129852ms step_avg:61.19ms
step:2123/2330 train_time:129912ms step_avg:61.19ms
step:2124/2330 train_time:129976ms step_avg:61.19ms
step:2125/2330 train_time:130036ms step_avg:61.19ms
step:2126/2330 train_time:130100ms step_avg:61.19ms
step:2127/2330 train_time:130160ms step_avg:61.19ms
step:2128/2330 train_time:130225ms step_avg:61.20ms
step:2129/2330 train_time:130284ms step_avg:61.20ms
step:2130/2330 train_time:130347ms step_avg:61.20ms
step:2131/2330 train_time:130407ms step_avg:61.20ms
step:2132/2330 train_time:130470ms step_avg:61.20ms
step:2133/2330 train_time:130530ms step_avg:61.20ms
step:2134/2330 train_time:130593ms step_avg:61.20ms
step:2135/2330 train_time:130653ms step_avg:61.20ms
step:2136/2330 train_time:130716ms step_avg:61.20ms
step:2137/2330 train_time:130776ms step_avg:61.20ms
step:2138/2330 train_time:130840ms step_avg:61.20ms
step:2139/2330 train_time:130900ms step_avg:61.20ms
step:2140/2330 train_time:130963ms step_avg:61.20ms
step:2141/2330 train_time:131024ms step_avg:61.20ms
step:2142/2330 train_time:131087ms step_avg:61.20ms
step:2143/2330 train_time:131147ms step_avg:61.20ms
step:2144/2330 train_time:131210ms step_avg:61.20ms
step:2145/2330 train_time:131270ms step_avg:61.20ms
step:2146/2330 train_time:131333ms step_avg:61.20ms
step:2147/2330 train_time:131393ms step_avg:61.20ms
step:2148/2330 train_time:131456ms step_avg:61.20ms
step:2149/2330 train_time:131516ms step_avg:61.20ms
step:2150/2330 train_time:131580ms step_avg:61.20ms
step:2151/2330 train_time:131641ms step_avg:61.20ms
step:2152/2330 train_time:131704ms step_avg:61.20ms
step:2153/2330 train_time:131765ms step_avg:61.20ms
step:2154/2330 train_time:131828ms step_avg:61.20ms
step:2155/2330 train_time:131889ms step_avg:61.20ms
step:2156/2330 train_time:131951ms step_avg:61.20ms
step:2157/2330 train_time:132012ms step_avg:61.20ms
step:2158/2330 train_time:132076ms step_avg:61.20ms
step:2159/2330 train_time:132136ms step_avg:61.20ms
step:2160/2330 train_time:132200ms step_avg:61.20ms
step:2161/2330 train_time:132262ms step_avg:61.20ms
step:2162/2330 train_time:132326ms step_avg:61.21ms
step:2163/2330 train_time:132386ms step_avg:61.20ms
step:2164/2330 train_time:132448ms step_avg:61.21ms
step:2165/2330 train_time:132508ms step_avg:61.20ms
step:2166/2330 train_time:132571ms step_avg:61.21ms
step:2167/2330 train_time:132632ms step_avg:61.21ms
step:2168/2330 train_time:132695ms step_avg:61.21ms
step:2169/2330 train_time:132755ms step_avg:61.21ms
step:2170/2330 train_time:132819ms step_avg:61.21ms
step:2171/2330 train_time:132879ms step_avg:61.21ms
step:2172/2330 train_time:132942ms step_avg:61.21ms
step:2173/2330 train_time:133002ms step_avg:61.21ms
step:2174/2330 train_time:133066ms step_avg:61.21ms
step:2175/2330 train_time:133126ms step_avg:61.21ms
step:2176/2330 train_time:133189ms step_avg:61.21ms
step:2177/2330 train_time:133249ms step_avg:61.21ms
step:2178/2330 train_time:133312ms step_avg:61.21ms
step:2179/2330 train_time:133372ms step_avg:61.21ms
step:2180/2330 train_time:133435ms step_avg:61.21ms
step:2181/2330 train_time:133496ms step_avg:61.21ms
step:2182/2330 train_time:133559ms step_avg:61.21ms
step:2183/2330 train_time:133620ms step_avg:61.21ms
step:2184/2330 train_time:133683ms step_avg:61.21ms
step:2185/2330 train_time:133743ms step_avg:61.21ms
step:2186/2330 train_time:133806ms step_avg:61.21ms
step:2187/2330 train_time:133867ms step_avg:61.21ms
step:2188/2330 train_time:133930ms step_avg:61.21ms
step:2189/2330 train_time:133990ms step_avg:61.21ms
step:2190/2330 train_time:134053ms step_avg:61.21ms
step:2191/2330 train_time:134114ms step_avg:61.21ms
step:2192/2330 train_time:134178ms step_avg:61.21ms
step:2193/2330 train_time:134239ms step_avg:61.21ms
step:2194/2330 train_time:134303ms step_avg:61.21ms
step:2195/2330 train_time:134364ms step_avg:61.21ms
step:2196/2330 train_time:134427ms step_avg:61.21ms
step:2197/2330 train_time:134487ms step_avg:61.21ms
step:2198/2330 train_time:134550ms step_avg:61.21ms
step:2199/2330 train_time:134610ms step_avg:61.21ms
step:2200/2330 train_time:134673ms step_avg:61.22ms
step:2201/2330 train_time:134733ms step_avg:61.21ms
step:2202/2330 train_time:134797ms step_avg:61.22ms
step:2203/2330 train_time:134857ms step_avg:61.22ms
step:2204/2330 train_time:134922ms step_avg:61.22ms
step:2205/2330 train_time:134982ms step_avg:61.22ms
step:2206/2330 train_time:135045ms step_avg:61.22ms
step:2207/2330 train_time:135106ms step_avg:61.22ms
step:2208/2330 train_time:135169ms step_avg:61.22ms
step:2209/2330 train_time:135229ms step_avg:61.22ms
step:2210/2330 train_time:135292ms step_avg:61.22ms
step:2211/2330 train_time:135352ms step_avg:61.22ms
step:2212/2330 train_time:135416ms step_avg:61.22ms
step:2213/2330 train_time:135477ms step_avg:61.22ms
step:2214/2330 train_time:135540ms step_avg:61.22ms
step:2215/2330 train_time:135601ms step_avg:61.22ms
step:2216/2330 train_time:135664ms step_avg:61.22ms
step:2217/2330 train_time:135725ms step_avg:61.22ms
step:2218/2330 train_time:135787ms step_avg:61.22ms
step:2219/2330 train_time:135848ms step_avg:61.22ms
step:2220/2330 train_time:135911ms step_avg:61.22ms
step:2221/2330 train_time:135970ms step_avg:61.22ms
step:2222/2330 train_time:136034ms step_avg:61.22ms
step:2223/2330 train_time:136094ms step_avg:61.22ms
step:2224/2330 train_time:136157ms step_avg:61.22ms
step:2225/2330 train_time:136218ms step_avg:61.22ms
step:2226/2330 train_time:136282ms step_avg:61.22ms
step:2227/2330 train_time:136342ms step_avg:61.22ms
step:2228/2330 train_time:136405ms step_avg:61.22ms
step:2229/2330 train_time:136466ms step_avg:61.22ms
step:2230/2330 train_time:136529ms step_avg:61.22ms
step:2231/2330 train_time:136589ms step_avg:61.22ms
step:2232/2330 train_time:136652ms step_avg:61.22ms
step:2233/2330 train_time:136711ms step_avg:61.22ms
step:2234/2330 train_time:136775ms step_avg:61.22ms
step:2235/2330 train_time:136836ms step_avg:61.22ms
step:2236/2330 train_time:136899ms step_avg:61.23ms
step:2237/2330 train_time:136959ms step_avg:61.22ms
step:2238/2330 train_time:137023ms step_avg:61.23ms
step:2239/2330 train_time:137083ms step_avg:61.23ms
step:2240/2330 train_time:137146ms step_avg:61.23ms
step:2241/2330 train_time:137206ms step_avg:61.23ms
step:2242/2330 train_time:137269ms step_avg:61.23ms
step:2243/2330 train_time:137330ms step_avg:61.23ms
step:2244/2330 train_time:137393ms step_avg:61.23ms
step:2245/2330 train_time:137453ms step_avg:61.23ms
step:2246/2330 train_time:137516ms step_avg:61.23ms
step:2247/2330 train_time:137576ms step_avg:61.23ms
step:2248/2330 train_time:137640ms step_avg:61.23ms
step:2249/2330 train_time:137700ms step_avg:61.23ms
step:2250/2330 train_time:137764ms step_avg:61.23ms
step:2250/2330 val_loss:3.3855 train_time:137830ms step_avg:61.26ms
step:2251/2330 train_time:137854ms step_avg:61.24ms
step:2252/2330 train_time:137893ms step_avg:61.23ms
step:2253/2330 train_time:137959ms step_avg:61.23ms
step:2254/2330 train_time:138024ms step_avg:61.23ms
step:2255/2330 train_time:138084ms step_avg:61.23ms
step:2256/2330 train_time:138148ms step_avg:61.24ms
step:2257/2330 train_time:138207ms step_avg:61.23ms
step:2258/2330 train_time:138270ms step_avg:61.24ms
step:2259/2330 train_time:138329ms step_avg:61.23ms
step:2260/2330 train_time:138391ms step_avg:61.24ms
step:2261/2330 train_time:138451ms step_avg:61.23ms
step:2262/2330 train_time:138514ms step_avg:61.23ms
step:2263/2330 train_time:138573ms step_avg:61.23ms
step:2264/2330 train_time:138635ms step_avg:61.23ms
step:2265/2330 train_time:138695ms step_avg:61.23ms
step:2266/2330 train_time:138757ms step_avg:61.23ms
step:2267/2330 train_time:138818ms step_avg:61.23ms
step:2268/2330 train_time:138883ms step_avg:61.24ms
step:2269/2330 train_time:138944ms step_avg:61.24ms
step:2270/2330 train_time:139008ms step_avg:61.24ms
step:2271/2330 train_time:139069ms step_avg:61.24ms
step:2272/2330 train_time:139133ms step_avg:61.24ms
step:2273/2330 train_time:139192ms step_avg:61.24ms
step:2274/2330 train_time:139255ms step_avg:61.24ms
step:2275/2330 train_time:139315ms step_avg:61.24ms
step:2276/2330 train_time:139378ms step_avg:61.24ms
step:2277/2330 train_time:139438ms step_avg:61.24ms
step:2278/2330 train_time:139501ms step_avg:61.24ms
step:2279/2330 train_time:139561ms step_avg:61.24ms
step:2280/2330 train_time:139624ms step_avg:61.24ms
step:2281/2330 train_time:139684ms step_avg:61.24ms
step:2282/2330 train_time:139748ms step_avg:61.24ms
step:2283/2330 train_time:139807ms step_avg:61.24ms
step:2284/2330 train_time:139871ms step_avg:61.24ms
step:2285/2330 train_time:139932ms step_avg:61.24ms
step:2286/2330 train_time:139995ms step_avg:61.24ms
step:2287/2330 train_time:140055ms step_avg:61.24ms
step:2288/2330 train_time:140120ms step_avg:61.24ms
step:2289/2330 train_time:140181ms step_avg:61.24ms
step:2290/2330 train_time:140245ms step_avg:61.24ms
step:2291/2330 train_time:140305ms step_avg:61.24ms
step:2292/2330 train_time:140369ms step_avg:61.24ms
step:2293/2330 train_time:140428ms step_avg:61.24ms
step:2294/2330 train_time:140491ms step_avg:61.24ms
step:2295/2330 train_time:140551ms step_avg:61.24ms
step:2296/2330 train_time:140614ms step_avg:61.24ms
step:2297/2330 train_time:140674ms step_avg:61.24ms
step:2298/2330 train_time:140736ms step_avg:61.24ms
step:2299/2330 train_time:140795ms step_avg:61.24ms
step:2300/2330 train_time:140859ms step_avg:61.24ms
step:2301/2330 train_time:140919ms step_avg:61.24ms
step:2302/2330 train_time:140983ms step_avg:61.24ms
step:2303/2330 train_time:141043ms step_avg:61.24ms
step:2304/2330 train_time:141107ms step_avg:61.24ms
step:2305/2330 train_time:141167ms step_avg:61.24ms
step:2306/2330 train_time:141231ms step_avg:61.24ms
step:2307/2330 train_time:141290ms step_avg:61.24ms
step:2308/2330 train_time:141354ms step_avg:61.25ms
step:2309/2330 train_time:141414ms step_avg:61.24ms
step:2310/2330 train_time:141476ms step_avg:61.25ms
step:2311/2330 train_time:141536ms step_avg:61.24ms
step:2312/2330 train_time:141599ms step_avg:61.25ms
step:2313/2330 train_time:141658ms step_avg:61.24ms
step:2314/2330 train_time:141721ms step_avg:61.25ms
step:2315/2330 train_time:141781ms step_avg:61.24ms
step:2316/2330 train_time:141845ms step_avg:61.25ms
step:2317/2330 train_time:141905ms step_avg:61.25ms
step:2318/2330 train_time:141969ms step_avg:61.25ms
step:2319/2330 train_time:142029ms step_avg:61.25ms
step:2320/2330 train_time:142094ms step_avg:61.25ms
step:2321/2330 train_time:142153ms step_avg:61.25ms
step:2322/2330 train_time:142216ms step_avg:61.25ms
step:2323/2330 train_time:142276ms step_avg:61.25ms
step:2324/2330 train_time:142340ms step_avg:61.25ms
step:2325/2330 train_time:142400ms step_avg:61.25ms
step:2326/2330 train_time:142464ms step_avg:61.25ms
step:2327/2330 train_time:142523ms step_avg:61.25ms
step:2328/2330 train_time:142586ms step_avg:61.25ms
step:2329/2330 train_time:142646ms step_avg:61.25ms
step:2330/2330 train_time:142709ms step_avg:61.25ms
step:2330/2330 val_loss:3.3552 train_time:142774ms step_avg:61.28ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
