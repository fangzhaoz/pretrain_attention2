import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-3"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:14:24 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:104ms step_avg:104.49ms
step:2/2330 train_time:202ms step_avg:100.77ms
step:3/2330 train_time:224ms step_avg:74.62ms
step:4/2330 train_time:259ms step_avg:64.66ms
step:5/2330 train_time:316ms step_avg:63.18ms
step:6/2330 train_time:378ms step_avg:62.96ms
step:7/2330 train_time:436ms step_avg:62.30ms
step:8/2330 train_time:497ms step_avg:62.19ms
step:9/2330 train_time:556ms step_avg:61.76ms
step:10/2330 train_time:617ms step_avg:61.75ms
step:11/2330 train_time:676ms step_avg:61.47ms
step:12/2330 train_time:738ms step_avg:61.47ms
step:13/2330 train_time:796ms step_avg:61.23ms
step:14/2330 train_time:858ms step_avg:61.27ms
step:15/2330 train_time:917ms step_avg:61.12ms
step:16/2330 train_time:979ms step_avg:61.17ms
step:17/2330 train_time:1040ms step_avg:61.17ms
step:18/2330 train_time:1105ms step_avg:61.38ms
step:19/2330 train_time:1167ms step_avg:61.43ms
step:20/2330 train_time:1231ms step_avg:61.54ms
step:21/2330 train_time:1291ms step_avg:61.49ms
step:22/2330 train_time:1354ms step_avg:61.56ms
step:23/2330 train_time:1414ms step_avg:61.48ms
step:24/2330 train_time:1476ms step_avg:61.52ms
step:25/2330 train_time:1535ms step_avg:61.41ms
step:26/2330 train_time:1597ms step_avg:61.43ms
step:27/2330 train_time:1656ms step_avg:61.34ms
step:28/2330 train_time:1718ms step_avg:61.35ms
step:29/2330 train_time:1776ms step_avg:61.26ms
step:30/2330 train_time:1838ms step_avg:61.28ms
step:31/2330 train_time:1897ms step_avg:61.19ms
step:32/2330 train_time:1959ms step_avg:61.21ms
step:33/2330 train_time:2019ms step_avg:61.18ms
step:34/2330 train_time:2082ms step_avg:61.25ms
step:35/2330 train_time:2145ms step_avg:61.28ms
step:36/2330 train_time:2208ms step_avg:61.33ms
step:37/2330 train_time:2269ms step_avg:61.32ms
step:38/2330 train_time:2331ms step_avg:61.34ms
step:39/2330 train_time:2391ms step_avg:61.30ms
step:40/2330 train_time:2453ms step_avg:61.33ms
step:41/2330 train_time:2513ms step_avg:61.29ms
step:42/2330 train_time:2576ms step_avg:61.33ms
step:43/2330 train_time:2635ms step_avg:61.29ms
step:44/2330 train_time:2698ms step_avg:61.31ms
step:45/2330 train_time:2757ms step_avg:61.27ms
step:46/2330 train_time:2820ms step_avg:61.29ms
step:47/2330 train_time:2878ms step_avg:61.24ms
step:48/2330 train_time:2940ms step_avg:61.26ms
step:49/2330 train_time:3000ms step_avg:61.23ms
step:50/2330 train_time:3063ms step_avg:61.25ms
step:51/2330 train_time:3122ms step_avg:61.22ms
step:52/2330 train_time:3185ms step_avg:61.25ms
step:53/2330 train_time:3245ms step_avg:61.22ms
step:54/2330 train_time:3307ms step_avg:61.25ms
step:55/2330 train_time:3367ms step_avg:61.21ms
step:56/2330 train_time:3429ms step_avg:61.24ms
step:57/2330 train_time:3489ms step_avg:61.21ms
step:58/2330 train_time:3551ms step_avg:61.23ms
step:59/2330 train_time:3611ms step_avg:61.20ms
step:60/2330 train_time:3674ms step_avg:61.23ms
step:61/2330 train_time:3734ms step_avg:61.21ms
step:62/2330 train_time:3796ms step_avg:61.23ms
step:63/2330 train_time:3856ms step_avg:61.21ms
step:64/2330 train_time:3919ms step_avg:61.23ms
step:65/2330 train_time:3979ms step_avg:61.21ms
step:66/2330 train_time:4041ms step_avg:61.23ms
step:67/2330 train_time:4101ms step_avg:61.21ms
step:68/2330 train_time:4163ms step_avg:61.23ms
step:69/2330 train_time:4224ms step_avg:61.22ms
step:70/2330 train_time:4286ms step_avg:61.23ms
step:71/2330 train_time:4346ms step_avg:61.21ms
step:72/2330 train_time:4408ms step_avg:61.22ms
step:73/2330 train_time:4467ms step_avg:61.19ms
step:74/2330 train_time:4529ms step_avg:61.21ms
step:75/2330 train_time:4589ms step_avg:61.19ms
step:76/2330 train_time:4652ms step_avg:61.21ms
step:77/2330 train_time:4712ms step_avg:61.19ms
step:78/2330 train_time:4775ms step_avg:61.22ms
step:79/2330 train_time:4835ms step_avg:61.20ms
step:80/2330 train_time:4898ms step_avg:61.23ms
step:81/2330 train_time:4959ms step_avg:61.22ms
step:82/2330 train_time:5022ms step_avg:61.24ms
step:83/2330 train_time:5081ms step_avg:61.22ms
step:84/2330 train_time:5143ms step_avg:61.23ms
step:85/2330 train_time:5204ms step_avg:61.22ms
step:86/2330 train_time:5266ms step_avg:61.23ms
step:87/2330 train_time:5325ms step_avg:61.21ms
step:88/2330 train_time:5388ms step_avg:61.22ms
step:89/2330 train_time:5447ms step_avg:61.20ms
step:90/2330 train_time:5510ms step_avg:61.22ms
step:91/2330 train_time:5569ms step_avg:61.20ms
step:92/2330 train_time:5632ms step_avg:61.22ms
step:93/2330 train_time:5691ms step_avg:61.20ms
step:94/2330 train_time:5754ms step_avg:61.21ms
step:95/2330 train_time:5814ms step_avg:61.20ms
step:96/2330 train_time:5877ms step_avg:61.22ms
step:97/2330 train_time:5937ms step_avg:61.20ms
step:98/2330 train_time:5999ms step_avg:61.21ms
step:99/2330 train_time:6059ms step_avg:61.20ms
step:100/2330 train_time:6122ms step_avg:61.22ms
step:101/2330 train_time:6181ms step_avg:61.20ms
step:102/2330 train_time:6244ms step_avg:61.22ms
step:103/2330 train_time:6304ms step_avg:61.20ms
step:104/2330 train_time:6366ms step_avg:61.21ms
step:105/2330 train_time:6426ms step_avg:61.20ms
step:106/2330 train_time:6489ms step_avg:61.21ms
step:107/2330 train_time:6549ms step_avg:61.20ms
step:108/2330 train_time:6610ms step_avg:61.21ms
step:109/2330 train_time:6669ms step_avg:61.19ms
step:110/2330 train_time:6732ms step_avg:61.20ms
step:111/2330 train_time:6792ms step_avg:61.19ms
step:112/2330 train_time:6856ms step_avg:61.21ms
step:113/2330 train_time:6916ms step_avg:61.20ms
step:114/2330 train_time:6979ms step_avg:61.22ms
step:115/2330 train_time:7039ms step_avg:61.20ms
step:116/2330 train_time:7101ms step_avg:61.22ms
step:117/2330 train_time:7161ms step_avg:61.20ms
step:118/2330 train_time:7224ms step_avg:61.22ms
step:119/2330 train_time:7283ms step_avg:61.20ms
step:120/2330 train_time:7346ms step_avg:61.22ms
step:121/2330 train_time:7405ms step_avg:61.20ms
step:122/2330 train_time:7467ms step_avg:61.21ms
step:123/2330 train_time:7528ms step_avg:61.21ms
step:124/2330 train_time:7591ms step_avg:61.22ms
step:125/2330 train_time:7650ms step_avg:61.20ms
step:126/2330 train_time:7712ms step_avg:61.20ms
step:127/2330 train_time:7771ms step_avg:61.19ms
step:128/2330 train_time:7835ms step_avg:61.21ms
step:129/2330 train_time:7896ms step_avg:61.21ms
step:130/2330 train_time:7960ms step_avg:61.23ms
step:131/2330 train_time:8019ms step_avg:61.22ms
step:132/2330 train_time:8082ms step_avg:61.23ms
step:133/2330 train_time:8142ms step_avg:61.22ms
step:134/2330 train_time:8204ms step_avg:61.22ms
step:135/2330 train_time:8263ms step_avg:61.21ms
step:136/2330 train_time:8326ms step_avg:61.22ms
step:137/2330 train_time:8386ms step_avg:61.21ms
step:138/2330 train_time:8448ms step_avg:61.22ms
step:139/2330 train_time:8508ms step_avg:61.21ms
step:140/2330 train_time:8571ms step_avg:61.22ms
step:141/2330 train_time:8630ms step_avg:61.21ms
step:142/2330 train_time:8692ms step_avg:61.21ms
step:143/2330 train_time:8751ms step_avg:61.20ms
step:144/2330 train_time:8815ms step_avg:61.21ms
step:145/2330 train_time:8875ms step_avg:61.21ms
step:146/2330 train_time:8938ms step_avg:61.22ms
step:147/2330 train_time:8998ms step_avg:61.21ms
step:148/2330 train_time:9060ms step_avg:61.22ms
step:149/2330 train_time:9121ms step_avg:61.22ms
step:150/2330 train_time:9184ms step_avg:61.23ms
step:151/2330 train_time:9244ms step_avg:61.22ms
step:152/2330 train_time:9306ms step_avg:61.23ms
step:153/2330 train_time:9365ms step_avg:61.21ms
step:154/2330 train_time:9428ms step_avg:61.22ms
step:155/2330 train_time:9487ms step_avg:61.21ms
step:156/2330 train_time:9550ms step_avg:61.22ms
step:157/2330 train_time:9610ms step_avg:61.21ms
step:158/2330 train_time:9673ms step_avg:61.22ms
step:159/2330 train_time:9733ms step_avg:61.21ms
step:160/2330 train_time:9795ms step_avg:61.22ms
step:161/2330 train_time:9855ms step_avg:61.21ms
step:162/2330 train_time:9918ms step_avg:61.22ms
step:163/2330 train_time:9978ms step_avg:61.21ms
step:164/2330 train_time:10041ms step_avg:61.22ms
step:165/2330 train_time:10100ms step_avg:61.22ms
step:166/2330 train_time:10163ms step_avg:61.23ms
step:167/2330 train_time:10223ms step_avg:61.22ms
step:168/2330 train_time:10286ms step_avg:61.22ms
step:169/2330 train_time:10345ms step_avg:61.21ms
step:170/2330 train_time:10407ms step_avg:61.22ms
step:171/2330 train_time:10466ms step_avg:61.20ms
step:172/2330 train_time:10529ms step_avg:61.21ms
step:173/2330 train_time:10589ms step_avg:61.21ms
step:174/2330 train_time:10651ms step_avg:61.21ms
step:175/2330 train_time:10710ms step_avg:61.20ms
step:176/2330 train_time:10773ms step_avg:61.21ms
step:177/2330 train_time:10834ms step_avg:61.21ms
step:178/2330 train_time:10896ms step_avg:61.21ms
step:179/2330 train_time:10956ms step_avg:61.21ms
step:180/2330 train_time:11019ms step_avg:61.22ms
step:181/2330 train_time:11079ms step_avg:61.21ms
step:182/2330 train_time:11143ms step_avg:61.22ms
step:183/2330 train_time:11202ms step_avg:61.21ms
step:184/2330 train_time:11265ms step_avg:61.22ms
step:185/2330 train_time:11325ms step_avg:61.21ms
step:186/2330 train_time:11387ms step_avg:61.22ms
step:187/2330 train_time:11446ms step_avg:61.21ms
step:188/2330 train_time:11508ms step_avg:61.21ms
step:189/2330 train_time:11568ms step_avg:61.20ms
step:190/2330 train_time:11630ms step_avg:61.21ms
step:191/2330 train_time:11690ms step_avg:61.20ms
step:192/2330 train_time:11752ms step_avg:61.21ms
step:193/2330 train_time:11811ms step_avg:61.20ms
step:194/2330 train_time:11874ms step_avg:61.21ms
step:195/2330 train_time:11934ms step_avg:61.20ms
step:196/2330 train_time:11997ms step_avg:61.21ms
step:197/2330 train_time:12058ms step_avg:61.21ms
step:198/2330 train_time:12121ms step_avg:61.22ms
step:199/2330 train_time:12183ms step_avg:61.22ms
step:200/2330 train_time:12244ms step_avg:61.22ms
step:201/2330 train_time:12303ms step_avg:61.21ms
step:202/2330 train_time:12366ms step_avg:61.22ms
step:203/2330 train_time:12426ms step_avg:61.21ms
step:204/2330 train_time:12488ms step_avg:61.22ms
step:205/2330 train_time:12547ms step_avg:61.21ms
step:206/2330 train_time:12609ms step_avg:61.21ms
step:207/2330 train_time:12669ms step_avg:61.20ms
step:208/2330 train_time:12731ms step_avg:61.21ms
step:209/2330 train_time:12791ms step_avg:61.20ms
step:210/2330 train_time:12854ms step_avg:61.21ms
step:211/2330 train_time:12915ms step_avg:61.21ms
step:212/2330 train_time:12979ms step_avg:61.22ms
step:213/2330 train_time:13038ms step_avg:61.21ms
step:214/2330 train_time:13102ms step_avg:61.22ms
step:215/2330 train_time:13162ms step_avg:61.22ms
step:216/2330 train_time:13225ms step_avg:61.23ms
step:217/2330 train_time:13284ms step_avg:61.22ms
step:218/2330 train_time:13347ms step_avg:61.22ms
step:219/2330 train_time:13406ms step_avg:61.22ms
step:220/2330 train_time:13468ms step_avg:61.22ms
step:221/2330 train_time:13527ms step_avg:61.21ms
step:222/2330 train_time:13590ms step_avg:61.21ms
step:223/2330 train_time:13649ms step_avg:61.21ms
step:224/2330 train_time:13711ms step_avg:61.21ms
step:225/2330 train_time:13770ms step_avg:61.20ms
step:226/2330 train_time:13833ms step_avg:61.21ms
step:227/2330 train_time:13893ms step_avg:61.20ms
step:228/2330 train_time:13957ms step_avg:61.21ms
step:229/2330 train_time:14017ms step_avg:61.21ms
step:230/2330 train_time:14080ms step_avg:61.22ms
step:231/2330 train_time:14140ms step_avg:61.21ms
step:232/2330 train_time:14203ms step_avg:61.22ms
step:233/2330 train_time:14263ms step_avg:61.21ms
step:234/2330 train_time:14325ms step_avg:61.22ms
step:235/2330 train_time:14384ms step_avg:61.21ms
step:236/2330 train_time:14446ms step_avg:61.21ms
step:237/2330 train_time:14507ms step_avg:61.21ms
step:238/2330 train_time:14569ms step_avg:61.21ms
step:239/2330 train_time:14629ms step_avg:61.21ms
step:240/2330 train_time:14691ms step_avg:61.21ms
step:241/2330 train_time:14751ms step_avg:61.21ms
step:242/2330 train_time:14813ms step_avg:61.21ms
step:243/2330 train_time:14873ms step_avg:61.20ms
step:244/2330 train_time:14936ms step_avg:61.21ms
step:245/2330 train_time:14996ms step_avg:61.21ms
step:246/2330 train_time:15059ms step_avg:61.21ms
step:247/2330 train_time:15119ms step_avg:61.21ms
step:248/2330 train_time:15181ms step_avg:61.21ms
step:249/2330 train_time:15242ms step_avg:61.21ms
step:250/2330 train_time:15305ms step_avg:61.22ms
step:250/2330 val_loss:4.2140 train_time:15369ms step_avg:61.47ms
step:251/2330 train_time:15392ms step_avg:61.32ms
step:252/2330 train_time:15428ms step_avg:61.22ms
step:253/2330 train_time:15490ms step_avg:61.23ms
step:254/2330 train_time:15560ms step_avg:61.26ms
step:255/2330 train_time:15621ms step_avg:61.26ms
step:256/2330 train_time:15685ms step_avg:61.27ms
step:257/2330 train_time:15744ms step_avg:61.26ms
step:258/2330 train_time:15806ms step_avg:61.26ms
step:259/2330 train_time:15866ms step_avg:61.26ms
step:260/2330 train_time:15928ms step_avg:61.26ms
step:261/2330 train_time:15987ms step_avg:61.25ms
step:262/2330 train_time:16049ms step_avg:61.25ms
step:263/2330 train_time:16107ms step_avg:61.24ms
step:264/2330 train_time:16169ms step_avg:61.25ms
step:265/2330 train_time:16228ms step_avg:61.24ms
step:266/2330 train_time:16291ms step_avg:61.24ms
step:267/2330 train_time:16351ms step_avg:61.24ms
step:268/2330 train_time:16414ms step_avg:61.25ms
step:269/2330 train_time:16476ms step_avg:61.25ms
step:270/2330 train_time:16540ms step_avg:61.26ms
step:271/2330 train_time:16600ms step_avg:61.26ms
step:272/2330 train_time:16663ms step_avg:61.26ms
step:273/2330 train_time:16723ms step_avg:61.26ms
step:274/2330 train_time:16786ms step_avg:61.26ms
step:275/2330 train_time:16846ms step_avg:61.26ms
step:276/2330 train_time:16908ms step_avg:61.26ms
step:277/2330 train_time:16967ms step_avg:61.25ms
step:278/2330 train_time:17029ms step_avg:61.26ms
step:279/2330 train_time:17088ms step_avg:61.25ms
step:280/2330 train_time:17150ms step_avg:61.25ms
step:281/2330 train_time:17209ms step_avg:61.24ms
step:282/2330 train_time:17271ms step_avg:61.25ms
step:283/2330 train_time:17331ms step_avg:61.24ms
step:284/2330 train_time:17395ms step_avg:61.25ms
step:285/2330 train_time:17456ms step_avg:61.25ms
step:286/2330 train_time:17520ms step_avg:61.26ms
step:287/2330 train_time:17581ms step_avg:61.26ms
step:288/2330 train_time:17644ms step_avg:61.26ms
step:289/2330 train_time:17704ms step_avg:61.26ms
step:290/2330 train_time:17767ms step_avg:61.27ms
step:291/2330 train_time:17826ms step_avg:61.26ms
step:292/2330 train_time:17889ms step_avg:61.26ms
step:293/2330 train_time:17948ms step_avg:61.26ms
step:294/2330 train_time:18010ms step_avg:61.26ms
step:295/2330 train_time:18069ms step_avg:61.25ms
step:296/2330 train_time:18131ms step_avg:61.25ms
step:297/2330 train_time:18190ms step_avg:61.25ms
step:298/2330 train_time:18252ms step_avg:61.25ms
step:299/2330 train_time:18312ms step_avg:61.24ms
step:300/2330 train_time:18375ms step_avg:61.25ms
step:301/2330 train_time:18436ms step_avg:61.25ms
step:302/2330 train_time:18499ms step_avg:61.26ms
step:303/2330 train_time:18559ms step_avg:61.25ms
step:304/2330 train_time:18622ms step_avg:61.26ms
step:305/2330 train_time:18682ms step_avg:61.25ms
step:306/2330 train_time:18745ms step_avg:61.26ms
step:307/2330 train_time:18804ms step_avg:61.25ms
step:308/2330 train_time:18866ms step_avg:61.25ms
step:309/2330 train_time:18925ms step_avg:61.25ms
step:310/2330 train_time:18988ms step_avg:61.25ms
step:311/2330 train_time:19046ms step_avg:61.24ms
step:312/2330 train_time:19108ms step_avg:61.24ms
step:313/2330 train_time:19168ms step_avg:61.24ms
step:314/2330 train_time:19230ms step_avg:61.24ms
step:315/2330 train_time:19290ms step_avg:61.24ms
step:316/2330 train_time:19352ms step_avg:61.24ms
step:317/2330 train_time:19413ms step_avg:61.24ms
step:318/2330 train_time:19476ms step_avg:61.25ms
step:319/2330 train_time:19537ms step_avg:61.25ms
step:320/2330 train_time:19601ms step_avg:61.25ms
step:321/2330 train_time:19661ms step_avg:61.25ms
step:322/2330 train_time:19723ms step_avg:61.25ms
step:323/2330 train_time:19784ms step_avg:61.25ms
step:324/2330 train_time:19846ms step_avg:61.25ms
step:325/2330 train_time:19905ms step_avg:61.25ms
step:326/2330 train_time:19968ms step_avg:61.25ms
step:327/2330 train_time:20027ms step_avg:61.24ms
step:328/2330 train_time:20089ms step_avg:61.25ms
step:329/2330 train_time:20148ms step_avg:61.24ms
step:330/2330 train_time:20210ms step_avg:61.24ms
step:331/2330 train_time:20270ms step_avg:61.24ms
step:332/2330 train_time:20332ms step_avg:61.24ms
step:333/2330 train_time:20393ms step_avg:61.24ms
step:334/2330 train_time:20457ms step_avg:61.25ms
step:335/2330 train_time:20518ms step_avg:61.25ms
step:336/2330 train_time:20581ms step_avg:61.25ms
step:337/2330 train_time:20641ms step_avg:61.25ms
step:338/2330 train_time:20704ms step_avg:61.25ms
step:339/2330 train_time:20764ms step_avg:61.25ms
step:340/2330 train_time:20826ms step_avg:61.25ms
step:341/2330 train_time:20885ms step_avg:61.25ms
step:342/2330 train_time:20948ms step_avg:61.25ms
step:343/2330 train_time:21007ms step_avg:61.24ms
step:344/2330 train_time:21070ms step_avg:61.25ms
step:345/2330 train_time:21129ms step_avg:61.24ms
step:346/2330 train_time:21192ms step_avg:61.25ms
step:347/2330 train_time:21251ms step_avg:61.24ms
step:348/2330 train_time:21314ms step_avg:61.25ms
step:349/2330 train_time:21374ms step_avg:61.24ms
step:350/2330 train_time:21437ms step_avg:61.25ms
step:351/2330 train_time:21497ms step_avg:61.25ms
step:352/2330 train_time:21561ms step_avg:61.25ms
step:353/2330 train_time:21621ms step_avg:61.25ms
step:354/2330 train_time:21684ms step_avg:61.26ms
step:355/2330 train_time:21744ms step_avg:61.25ms
step:356/2330 train_time:21807ms step_avg:61.26ms
step:357/2330 train_time:21866ms step_avg:61.25ms
step:358/2330 train_time:21929ms step_avg:61.25ms
step:359/2330 train_time:21988ms step_avg:61.25ms
step:360/2330 train_time:22051ms step_avg:61.25ms
step:361/2330 train_time:22110ms step_avg:61.25ms
step:362/2330 train_time:22173ms step_avg:61.25ms
step:363/2330 train_time:22232ms step_avg:61.25ms
step:364/2330 train_time:22295ms step_avg:61.25ms
step:365/2330 train_time:22355ms step_avg:61.25ms
step:366/2330 train_time:22417ms step_avg:61.25ms
step:367/2330 train_time:22478ms step_avg:61.25ms
step:368/2330 train_time:22541ms step_avg:61.25ms
step:369/2330 train_time:22601ms step_avg:61.25ms
step:370/2330 train_time:22664ms step_avg:61.25ms
step:371/2330 train_time:22724ms step_avg:61.25ms
step:372/2330 train_time:22787ms step_avg:61.25ms
step:373/2330 train_time:22846ms step_avg:61.25ms
step:374/2330 train_time:22908ms step_avg:61.25ms
step:375/2330 train_time:22968ms step_avg:61.25ms
step:376/2330 train_time:23031ms step_avg:61.25ms
step:377/2330 train_time:23091ms step_avg:61.25ms
step:378/2330 train_time:23153ms step_avg:61.25ms
step:379/2330 train_time:23212ms step_avg:61.25ms
step:380/2330 train_time:23276ms step_avg:61.25ms
step:381/2330 train_time:23335ms step_avg:61.25ms
step:382/2330 train_time:23399ms step_avg:61.25ms
step:383/2330 train_time:23459ms step_avg:61.25ms
step:384/2330 train_time:23522ms step_avg:61.25ms
step:385/2330 train_time:23582ms step_avg:61.25ms
step:386/2330 train_time:23645ms step_avg:61.26ms
step:387/2330 train_time:23705ms step_avg:61.25ms
step:388/2330 train_time:23767ms step_avg:61.25ms
step:389/2330 train_time:23826ms step_avg:61.25ms
step:390/2330 train_time:23889ms step_avg:61.25ms
step:391/2330 train_time:23949ms step_avg:61.25ms
step:392/2330 train_time:24011ms step_avg:61.25ms
step:393/2330 train_time:24070ms step_avg:61.25ms
step:394/2330 train_time:24133ms step_avg:61.25ms
step:395/2330 train_time:24193ms step_avg:61.25ms
step:396/2330 train_time:24255ms step_avg:61.25ms
step:397/2330 train_time:24316ms step_avg:61.25ms
step:398/2330 train_time:24379ms step_avg:61.25ms
step:399/2330 train_time:24439ms step_avg:61.25ms
step:400/2330 train_time:24501ms step_avg:61.25ms
step:401/2330 train_time:24561ms step_avg:61.25ms
step:402/2330 train_time:24624ms step_avg:61.25ms
step:403/2330 train_time:24683ms step_avg:61.25ms
step:404/2330 train_time:24746ms step_avg:61.25ms
step:405/2330 train_time:24806ms step_avg:61.25ms
step:406/2330 train_time:24868ms step_avg:61.25ms
step:407/2330 train_time:24927ms step_avg:61.25ms
step:408/2330 train_time:24991ms step_avg:61.25ms
step:409/2330 train_time:25051ms step_avg:61.25ms
step:410/2330 train_time:25113ms step_avg:61.25ms
step:411/2330 train_time:25173ms step_avg:61.25ms
step:412/2330 train_time:25236ms step_avg:61.25ms
step:413/2330 train_time:25296ms step_avg:61.25ms
step:414/2330 train_time:25358ms step_avg:61.25ms
step:415/2330 train_time:25418ms step_avg:61.25ms
step:416/2330 train_time:25482ms step_avg:61.25ms
step:417/2330 train_time:25542ms step_avg:61.25ms
step:418/2330 train_time:25604ms step_avg:61.25ms
step:419/2330 train_time:25664ms step_avg:61.25ms
step:420/2330 train_time:25726ms step_avg:61.25ms
step:421/2330 train_time:25787ms step_avg:61.25ms
step:422/2330 train_time:25849ms step_avg:61.25ms
step:423/2330 train_time:25909ms step_avg:61.25ms
step:424/2330 train_time:25971ms step_avg:61.25ms
step:425/2330 train_time:26030ms step_avg:61.25ms
step:426/2330 train_time:26094ms step_avg:61.25ms
step:427/2330 train_time:26153ms step_avg:61.25ms
step:428/2330 train_time:26215ms step_avg:61.25ms
step:429/2330 train_time:26277ms step_avg:61.25ms
step:430/2330 train_time:26340ms step_avg:61.25ms
step:431/2330 train_time:26399ms step_avg:61.25ms
step:432/2330 train_time:26462ms step_avg:61.25ms
step:433/2330 train_time:26522ms step_avg:61.25ms
step:434/2330 train_time:26585ms step_avg:61.26ms
step:435/2330 train_time:26645ms step_avg:61.25ms
step:436/2330 train_time:26707ms step_avg:61.25ms
step:437/2330 train_time:26766ms step_avg:61.25ms
step:438/2330 train_time:26828ms step_avg:61.25ms
step:439/2330 train_time:26887ms step_avg:61.25ms
step:440/2330 train_time:26950ms step_avg:61.25ms
step:441/2330 train_time:27009ms step_avg:61.25ms
step:442/2330 train_time:27071ms step_avg:61.25ms
step:443/2330 train_time:27131ms step_avg:61.24ms
step:444/2330 train_time:27194ms step_avg:61.25ms
step:445/2330 train_time:27254ms step_avg:61.24ms
step:446/2330 train_time:27318ms step_avg:61.25ms
step:447/2330 train_time:27378ms step_avg:61.25ms
step:448/2330 train_time:27441ms step_avg:61.25ms
step:449/2330 train_time:27501ms step_avg:61.25ms
step:450/2330 train_time:27563ms step_avg:61.25ms
step:451/2330 train_time:27623ms step_avg:61.25ms
step:452/2330 train_time:27686ms step_avg:61.25ms
step:453/2330 train_time:27746ms step_avg:61.25ms
step:454/2330 train_time:27808ms step_avg:61.25ms
step:455/2330 train_time:27867ms step_avg:61.25ms
step:456/2330 train_time:27930ms step_avg:61.25ms
step:457/2330 train_time:27991ms step_avg:61.25ms
step:458/2330 train_time:28053ms step_avg:61.25ms
step:459/2330 train_time:28112ms step_avg:61.25ms
step:460/2330 train_time:28175ms step_avg:61.25ms
step:461/2330 train_time:28236ms step_avg:61.25ms
step:462/2330 train_time:28299ms step_avg:61.25ms
step:463/2330 train_time:28359ms step_avg:61.25ms
step:464/2330 train_time:28422ms step_avg:61.25ms
step:465/2330 train_time:28483ms step_avg:61.25ms
step:466/2330 train_time:28546ms step_avg:61.26ms
step:467/2330 train_time:28606ms step_avg:61.25ms
step:468/2330 train_time:28669ms step_avg:61.26ms
step:469/2330 train_time:28728ms step_avg:61.25ms
step:470/2330 train_time:28791ms step_avg:61.26ms
step:471/2330 train_time:28850ms step_avg:61.25ms
step:472/2330 train_time:28913ms step_avg:61.26ms
step:473/2330 train_time:28972ms step_avg:61.25ms
step:474/2330 train_time:29035ms step_avg:61.25ms
step:475/2330 train_time:29095ms step_avg:61.25ms
step:476/2330 train_time:29157ms step_avg:61.25ms
step:477/2330 train_time:29217ms step_avg:61.25ms
step:478/2330 train_time:29281ms step_avg:61.26ms
step:479/2330 train_time:29340ms step_avg:61.25ms
step:480/2330 train_time:29403ms step_avg:61.26ms
step:481/2330 train_time:29464ms step_avg:61.26ms
step:482/2330 train_time:29526ms step_avg:61.26ms
step:483/2330 train_time:29586ms step_avg:61.26ms
step:484/2330 train_time:29649ms step_avg:61.26ms
step:485/2330 train_time:29708ms step_avg:61.25ms
step:486/2330 train_time:29771ms step_avg:61.26ms
step:487/2330 train_time:29830ms step_avg:61.25ms
step:488/2330 train_time:29893ms step_avg:61.26ms
step:489/2330 train_time:29952ms step_avg:61.25ms
step:490/2330 train_time:30014ms step_avg:61.25ms
step:491/2330 train_time:30074ms step_avg:61.25ms
step:492/2330 train_time:30137ms step_avg:61.25ms
step:493/2330 train_time:30197ms step_avg:61.25ms
step:494/2330 train_time:30260ms step_avg:61.25ms
step:495/2330 train_time:30320ms step_avg:61.25ms
step:496/2330 train_time:30383ms step_avg:61.26ms
step:497/2330 train_time:30443ms step_avg:61.25ms
step:498/2330 train_time:30506ms step_avg:61.26ms
step:499/2330 train_time:30566ms step_avg:61.25ms
step:500/2330 train_time:30628ms step_avg:61.26ms
step:500/2330 val_loss:3.8837 train_time:30692ms step_avg:61.38ms
step:501/2330 train_time:30715ms step_avg:61.31ms
step:502/2330 train_time:30753ms step_avg:61.26ms
step:503/2330 train_time:30819ms step_avg:61.27ms
step:504/2330 train_time:30885ms step_avg:61.28ms
step:505/2330 train_time:30944ms step_avg:61.28ms
step:506/2330 train_time:31007ms step_avg:61.28ms
step:507/2330 train_time:31066ms step_avg:61.27ms
step:508/2330 train_time:31129ms step_avg:61.28ms
step:509/2330 train_time:31188ms step_avg:61.27ms
step:510/2330 train_time:31250ms step_avg:61.27ms
step:511/2330 train_time:31308ms step_avg:61.27ms
step:512/2330 train_time:31370ms step_avg:61.27ms
step:513/2330 train_time:31429ms step_avg:61.26ms
step:514/2330 train_time:31491ms step_avg:61.27ms
step:515/2330 train_time:31550ms step_avg:61.26ms
step:516/2330 train_time:31612ms step_avg:61.26ms
step:517/2330 train_time:31672ms step_avg:61.26ms
step:518/2330 train_time:31736ms step_avg:61.27ms
step:519/2330 train_time:31797ms step_avg:61.27ms
step:520/2330 train_time:31860ms step_avg:61.27ms
step:521/2330 train_time:31921ms step_avg:61.27ms
step:522/2330 train_time:31984ms step_avg:61.27ms
step:523/2330 train_time:32044ms step_avg:61.27ms
step:524/2330 train_time:32107ms step_avg:61.27ms
step:525/2330 train_time:32166ms step_avg:61.27ms
step:526/2330 train_time:32228ms step_avg:61.27ms
step:527/2330 train_time:32288ms step_avg:61.27ms
step:528/2330 train_time:32350ms step_avg:61.27ms
step:529/2330 train_time:32409ms step_avg:61.26ms
step:530/2330 train_time:32471ms step_avg:61.27ms
step:531/2330 train_time:32530ms step_avg:61.26ms
step:532/2330 train_time:32592ms step_avg:61.26ms
step:533/2330 train_time:32653ms step_avg:61.26ms
step:534/2330 train_time:32716ms step_avg:61.27ms
step:535/2330 train_time:32776ms step_avg:61.26ms
step:536/2330 train_time:32838ms step_avg:61.26ms
step:537/2330 train_time:32898ms step_avg:61.26ms
step:538/2330 train_time:32962ms step_avg:61.27ms
step:539/2330 train_time:33021ms step_avg:61.26ms
step:540/2330 train_time:33084ms step_avg:61.27ms
step:541/2330 train_time:33144ms step_avg:61.26ms
step:542/2330 train_time:33206ms step_avg:61.27ms
step:543/2330 train_time:33266ms step_avg:61.26ms
step:544/2330 train_time:33328ms step_avg:61.26ms
step:545/2330 train_time:33388ms step_avg:61.26ms
step:546/2330 train_time:33451ms step_avg:61.27ms
step:547/2330 train_time:33510ms step_avg:61.26ms
step:548/2330 train_time:33572ms step_avg:61.26ms
step:549/2330 train_time:33632ms step_avg:61.26ms
step:550/2330 train_time:33695ms step_avg:61.26ms
step:551/2330 train_time:33755ms step_avg:61.26ms
step:552/2330 train_time:33818ms step_avg:61.26ms
step:553/2330 train_time:33878ms step_avg:61.26ms
step:554/2330 train_time:33941ms step_avg:61.27ms
step:555/2330 train_time:34001ms step_avg:61.26ms
step:556/2330 train_time:34064ms step_avg:61.27ms
step:557/2330 train_time:34124ms step_avg:61.26ms
step:558/2330 train_time:34186ms step_avg:61.27ms
step:559/2330 train_time:34245ms step_avg:61.26ms
step:560/2330 train_time:34308ms step_avg:61.26ms
step:561/2330 train_time:34368ms step_avg:61.26ms
step:562/2330 train_time:34430ms step_avg:61.26ms
step:563/2330 train_time:34490ms step_avg:61.26ms
step:564/2330 train_time:34552ms step_avg:61.26ms
step:565/2330 train_time:34612ms step_avg:61.26ms
step:566/2330 train_time:34675ms step_avg:61.26ms
step:567/2330 train_time:34734ms step_avg:61.26ms
step:568/2330 train_time:34796ms step_avg:61.26ms
step:569/2330 train_time:34857ms step_avg:61.26ms
step:570/2330 train_time:34919ms step_avg:61.26ms
step:571/2330 train_time:34979ms step_avg:61.26ms
step:572/2330 train_time:35042ms step_avg:61.26ms
step:573/2330 train_time:35103ms step_avg:61.26ms
step:574/2330 train_time:35166ms step_avg:61.26ms
step:575/2330 train_time:35226ms step_avg:61.26ms
step:576/2330 train_time:35289ms step_avg:61.26ms
step:577/2330 train_time:35348ms step_avg:61.26ms
step:578/2330 train_time:35411ms step_avg:61.26ms
step:579/2330 train_time:35470ms step_avg:61.26ms
step:580/2330 train_time:35533ms step_avg:61.26ms
step:581/2330 train_time:35592ms step_avg:61.26ms
step:582/2330 train_time:35655ms step_avg:61.26ms
step:583/2330 train_time:35715ms step_avg:61.26ms
step:584/2330 train_time:35777ms step_avg:61.26ms
step:585/2330 train_time:35837ms step_avg:61.26ms
step:586/2330 train_time:35899ms step_avg:61.26ms
step:587/2330 train_time:35960ms step_avg:61.26ms
step:588/2330 train_time:36023ms step_avg:61.26ms
step:589/2330 train_time:36082ms step_avg:61.26ms
step:590/2330 train_time:36145ms step_avg:61.26ms
step:591/2330 train_time:36205ms step_avg:61.26ms
step:592/2330 train_time:36268ms step_avg:61.26ms
step:593/2330 train_time:36327ms step_avg:61.26ms
step:594/2330 train_time:36390ms step_avg:61.26ms
step:595/2330 train_time:36449ms step_avg:61.26ms
step:596/2330 train_time:36511ms step_avg:61.26ms
step:597/2330 train_time:36571ms step_avg:61.26ms
step:598/2330 train_time:36633ms step_avg:61.26ms
step:599/2330 train_time:36693ms step_avg:61.26ms
step:600/2330 train_time:36755ms step_avg:61.26ms
step:601/2330 train_time:36815ms step_avg:61.26ms
step:602/2330 train_time:36878ms step_avg:61.26ms
step:603/2330 train_time:36938ms step_avg:61.26ms
step:604/2330 train_time:37000ms step_avg:61.26ms
step:605/2330 train_time:37061ms step_avg:61.26ms
step:606/2330 train_time:37124ms step_avg:61.26ms
step:607/2330 train_time:37185ms step_avg:61.26ms
step:608/2330 train_time:37249ms step_avg:61.26ms
step:609/2330 train_time:37308ms step_avg:61.26ms
step:610/2330 train_time:37370ms step_avg:61.26ms
step:611/2330 train_time:37430ms step_avg:61.26ms
step:612/2330 train_time:37492ms step_avg:61.26ms
step:613/2330 train_time:37552ms step_avg:61.26ms
step:614/2330 train_time:37614ms step_avg:61.26ms
step:615/2330 train_time:37674ms step_avg:61.26ms
step:616/2330 train_time:37737ms step_avg:61.26ms
step:617/2330 train_time:37796ms step_avg:61.26ms
step:618/2330 train_time:37858ms step_avg:61.26ms
step:619/2330 train_time:37918ms step_avg:61.26ms
step:620/2330 train_time:37981ms step_avg:61.26ms
step:621/2330 train_time:38040ms step_avg:61.26ms
step:622/2330 train_time:38103ms step_avg:61.26ms
step:623/2330 train_time:38164ms step_avg:61.26ms
step:624/2330 train_time:38227ms step_avg:61.26ms
step:625/2330 train_time:38286ms step_avg:61.26ms
step:626/2330 train_time:38350ms step_avg:61.26ms
step:627/2330 train_time:38409ms step_avg:61.26ms
step:628/2330 train_time:38472ms step_avg:61.26ms
step:629/2330 train_time:38531ms step_avg:61.26ms
step:630/2330 train_time:38594ms step_avg:61.26ms
step:631/2330 train_time:38654ms step_avg:61.26ms
step:632/2330 train_time:38717ms step_avg:61.26ms
step:633/2330 train_time:38776ms step_avg:61.26ms
step:634/2330 train_time:38838ms step_avg:61.26ms
step:635/2330 train_time:38898ms step_avg:61.26ms
step:636/2330 train_time:38960ms step_avg:61.26ms
step:637/2330 train_time:39020ms step_avg:61.26ms
step:638/2330 train_time:39083ms step_avg:61.26ms
step:639/2330 train_time:39143ms step_avg:61.26ms
step:640/2330 train_time:39206ms step_avg:61.26ms
step:641/2330 train_time:39266ms step_avg:61.26ms
step:642/2330 train_time:39329ms step_avg:61.26ms
step:643/2330 train_time:39389ms step_avg:61.26ms
step:644/2330 train_time:39451ms step_avg:61.26ms
step:645/2330 train_time:39511ms step_avg:61.26ms
step:646/2330 train_time:39574ms step_avg:61.26ms
step:647/2330 train_time:39633ms step_avg:61.26ms
step:648/2330 train_time:39697ms step_avg:61.26ms
step:649/2330 train_time:39756ms step_avg:61.26ms
step:650/2330 train_time:39818ms step_avg:61.26ms
step:651/2330 train_time:39878ms step_avg:61.26ms
step:652/2330 train_time:39940ms step_avg:61.26ms
step:653/2330 train_time:40000ms step_avg:61.26ms
step:654/2330 train_time:40063ms step_avg:61.26ms
step:655/2330 train_time:40123ms step_avg:61.26ms
step:656/2330 train_time:40186ms step_avg:61.26ms
step:657/2330 train_time:40247ms step_avg:61.26ms
step:658/2330 train_time:40309ms step_avg:61.26ms
step:659/2330 train_time:40370ms step_avg:61.26ms
step:660/2330 train_time:40432ms step_avg:61.26ms
step:661/2330 train_time:40492ms step_avg:61.26ms
step:662/2330 train_time:40555ms step_avg:61.26ms
step:663/2330 train_time:40615ms step_avg:61.26ms
step:664/2330 train_time:40678ms step_avg:61.26ms
step:665/2330 train_time:40736ms step_avg:61.26ms
step:666/2330 train_time:40798ms step_avg:61.26ms
step:667/2330 train_time:40858ms step_avg:61.26ms
step:668/2330 train_time:40921ms step_avg:61.26ms
step:669/2330 train_time:40980ms step_avg:61.26ms
step:670/2330 train_time:41043ms step_avg:61.26ms
step:671/2330 train_time:41103ms step_avg:61.26ms
step:672/2330 train_time:41167ms step_avg:61.26ms
step:673/2330 train_time:41227ms step_avg:61.26ms
step:674/2330 train_time:41290ms step_avg:61.26ms
step:675/2330 train_time:41349ms step_avg:61.26ms
step:676/2330 train_time:41412ms step_avg:61.26ms
step:677/2330 train_time:41471ms step_avg:61.26ms
step:678/2330 train_time:41534ms step_avg:61.26ms
step:679/2330 train_time:41593ms step_avg:61.26ms
step:680/2330 train_time:41655ms step_avg:61.26ms
step:681/2330 train_time:41715ms step_avg:61.26ms
step:682/2330 train_time:41777ms step_avg:61.26ms
step:683/2330 train_time:41837ms step_avg:61.25ms
step:684/2330 train_time:41899ms step_avg:61.26ms
step:685/2330 train_time:41958ms step_avg:61.25ms
step:686/2330 train_time:42020ms step_avg:61.25ms
step:687/2330 train_time:42081ms step_avg:61.25ms
step:688/2330 train_time:42144ms step_avg:61.26ms
step:689/2330 train_time:42204ms step_avg:61.25ms
step:690/2330 train_time:42267ms step_avg:61.26ms
step:691/2330 train_time:42327ms step_avg:61.26ms
step:692/2330 train_time:42390ms step_avg:61.26ms
step:693/2330 train_time:42450ms step_avg:61.26ms
step:694/2330 train_time:42513ms step_avg:61.26ms
step:695/2330 train_time:42573ms step_avg:61.26ms
step:696/2330 train_time:42635ms step_avg:61.26ms
step:697/2330 train_time:42695ms step_avg:61.25ms
step:698/2330 train_time:42757ms step_avg:61.26ms
step:699/2330 train_time:42816ms step_avg:61.25ms
step:700/2330 train_time:42878ms step_avg:61.25ms
step:701/2330 train_time:42938ms step_avg:61.25ms
step:702/2330 train_time:43000ms step_avg:61.25ms
step:703/2330 train_time:43060ms step_avg:61.25ms
step:704/2330 train_time:43123ms step_avg:61.25ms
step:705/2330 train_time:43183ms step_avg:61.25ms
step:706/2330 train_time:43246ms step_avg:61.25ms
step:707/2330 train_time:43306ms step_avg:61.25ms
step:708/2330 train_time:43369ms step_avg:61.26ms
step:709/2330 train_time:43429ms step_avg:61.25ms
step:710/2330 train_time:43491ms step_avg:61.26ms
step:711/2330 train_time:43551ms step_avg:61.25ms
step:712/2330 train_time:43614ms step_avg:61.26ms
step:713/2330 train_time:43674ms step_avg:61.25ms
step:714/2330 train_time:43736ms step_avg:61.25ms
step:715/2330 train_time:43795ms step_avg:61.25ms
step:716/2330 train_time:43857ms step_avg:61.25ms
step:717/2330 train_time:43916ms step_avg:61.25ms
step:718/2330 train_time:43979ms step_avg:61.25ms
step:719/2330 train_time:44038ms step_avg:61.25ms
step:720/2330 train_time:44101ms step_avg:61.25ms
step:721/2330 train_time:44162ms step_avg:61.25ms
step:722/2330 train_time:44225ms step_avg:61.25ms
step:723/2330 train_time:44285ms step_avg:61.25ms
step:724/2330 train_time:44348ms step_avg:61.25ms
step:725/2330 train_time:44407ms step_avg:61.25ms
step:726/2330 train_time:44471ms step_avg:61.25ms
step:727/2330 train_time:44530ms step_avg:61.25ms
step:728/2330 train_time:44593ms step_avg:61.25ms
step:729/2330 train_time:44653ms step_avg:61.25ms
step:730/2330 train_time:44715ms step_avg:61.25ms
step:731/2330 train_time:44774ms step_avg:61.25ms
step:732/2330 train_time:44837ms step_avg:61.25ms
step:733/2330 train_time:44896ms step_avg:61.25ms
step:734/2330 train_time:44959ms step_avg:61.25ms
step:735/2330 train_time:45018ms step_avg:61.25ms
step:736/2330 train_time:45081ms step_avg:61.25ms
step:737/2330 train_time:45141ms step_avg:61.25ms
step:738/2330 train_time:45204ms step_avg:61.25ms
step:739/2330 train_time:45265ms step_avg:61.25ms
step:740/2330 train_time:45328ms step_avg:61.25ms
step:741/2330 train_time:45387ms step_avg:61.25ms
step:742/2330 train_time:45450ms step_avg:61.25ms
step:743/2330 train_time:45510ms step_avg:61.25ms
step:744/2330 train_time:45573ms step_avg:61.25ms
step:745/2330 train_time:45632ms step_avg:61.25ms
step:746/2330 train_time:45694ms step_avg:61.25ms
step:747/2330 train_time:45754ms step_avg:61.25ms
step:748/2330 train_time:45817ms step_avg:61.25ms
step:749/2330 train_time:45876ms step_avg:61.25ms
step:750/2330 train_time:45939ms step_avg:61.25ms
step:750/2330 val_loss:3.7390 train_time:46003ms step_avg:61.34ms
step:751/2330 train_time:46025ms step_avg:61.29ms
step:752/2330 train_time:46063ms step_avg:61.25ms
step:753/2330 train_time:46126ms step_avg:61.26ms
step:754/2330 train_time:46193ms step_avg:61.26ms
step:755/2330 train_time:46252ms step_avg:61.26ms
step:756/2330 train_time:46315ms step_avg:61.26ms
step:757/2330 train_time:46374ms step_avg:61.26ms
step:758/2330 train_time:46436ms step_avg:61.26ms
step:759/2330 train_time:46494ms step_avg:61.26ms
step:760/2330 train_time:46556ms step_avg:61.26ms
step:761/2330 train_time:46615ms step_avg:61.25ms
step:762/2330 train_time:46676ms step_avg:61.25ms
step:763/2330 train_time:46735ms step_avg:61.25ms
step:764/2330 train_time:46797ms step_avg:61.25ms
step:765/2330 train_time:46856ms step_avg:61.25ms
step:766/2330 train_time:46920ms step_avg:61.25ms
step:767/2330 train_time:46981ms step_avg:61.25ms
step:768/2330 train_time:47045ms step_avg:61.26ms
step:769/2330 train_time:47108ms step_avg:61.26ms
step:770/2330 train_time:47172ms step_avg:61.26ms
step:771/2330 train_time:47233ms step_avg:61.26ms
step:772/2330 train_time:47297ms step_avg:61.27ms
step:773/2330 train_time:47357ms step_avg:61.26ms
step:774/2330 train_time:47421ms step_avg:61.27ms
step:775/2330 train_time:47481ms step_avg:61.27ms
step:776/2330 train_time:47545ms step_avg:61.27ms
step:777/2330 train_time:47605ms step_avg:61.27ms
step:778/2330 train_time:47667ms step_avg:61.27ms
step:779/2330 train_time:47728ms step_avg:61.27ms
step:780/2330 train_time:47791ms step_avg:61.27ms
step:781/2330 train_time:47851ms step_avg:61.27ms
step:782/2330 train_time:47914ms step_avg:61.27ms
step:783/2330 train_time:47975ms step_avg:61.27ms
step:784/2330 train_time:48039ms step_avg:61.27ms
step:785/2330 train_time:48099ms step_avg:61.27ms
step:786/2330 train_time:48163ms step_avg:61.28ms
step:787/2330 train_time:48224ms step_avg:61.28ms
step:788/2330 train_time:48288ms step_avg:61.28ms
step:789/2330 train_time:48348ms step_avg:61.28ms
step:790/2330 train_time:48412ms step_avg:61.28ms
step:791/2330 train_time:48472ms step_avg:61.28ms
step:792/2330 train_time:48535ms step_avg:61.28ms
step:793/2330 train_time:48595ms step_avg:61.28ms
step:794/2330 train_time:48658ms step_avg:61.28ms
step:795/2330 train_time:48718ms step_avg:61.28ms
step:796/2330 train_time:48781ms step_avg:61.28ms
step:797/2330 train_time:48842ms step_avg:61.28ms
step:798/2330 train_time:48906ms step_avg:61.29ms
step:799/2330 train_time:48967ms step_avg:61.28ms
step:800/2330 train_time:49030ms step_avg:61.29ms
step:801/2330 train_time:49091ms step_avg:61.29ms
step:802/2330 train_time:49156ms step_avg:61.29ms
step:803/2330 train_time:49216ms step_avg:61.29ms
step:804/2330 train_time:49279ms step_avg:61.29ms
step:805/2330 train_time:49340ms step_avg:61.29ms
step:806/2330 train_time:49404ms step_avg:61.30ms
step:807/2330 train_time:49465ms step_avg:61.29ms
step:808/2330 train_time:49528ms step_avg:61.30ms
step:809/2330 train_time:49588ms step_avg:61.30ms
step:810/2330 train_time:49651ms step_avg:61.30ms
step:811/2330 train_time:49712ms step_avg:61.30ms
step:812/2330 train_time:49775ms step_avg:61.30ms
step:813/2330 train_time:49834ms step_avg:61.30ms
step:814/2330 train_time:49897ms step_avg:61.30ms
step:815/2330 train_time:49957ms step_avg:61.30ms
step:816/2330 train_time:50021ms step_avg:61.30ms
step:817/2330 train_time:50081ms step_avg:61.30ms
step:818/2330 train_time:50145ms step_avg:61.30ms
step:819/2330 train_time:50207ms step_avg:61.30ms
step:820/2330 train_time:50271ms step_avg:61.31ms
step:821/2330 train_time:50331ms step_avg:61.31ms
step:822/2330 train_time:50395ms step_avg:61.31ms
step:823/2330 train_time:50455ms step_avg:61.31ms
step:824/2330 train_time:50518ms step_avg:61.31ms
step:825/2330 train_time:50578ms step_avg:61.31ms
step:826/2330 train_time:50642ms step_avg:61.31ms
step:827/2330 train_time:50702ms step_avg:61.31ms
step:828/2330 train_time:50766ms step_avg:61.31ms
step:829/2330 train_time:50826ms step_avg:61.31ms
step:830/2330 train_time:50889ms step_avg:61.31ms
step:831/2330 train_time:50950ms step_avg:61.31ms
step:832/2330 train_time:51013ms step_avg:61.31ms
step:833/2330 train_time:51073ms step_avg:61.31ms
step:834/2330 train_time:51137ms step_avg:61.32ms
step:835/2330 train_time:51197ms step_avg:61.31ms
step:836/2330 train_time:51262ms step_avg:61.32ms
step:837/2330 train_time:51323ms step_avg:61.32ms
step:838/2330 train_time:51386ms step_avg:61.32ms
step:839/2330 train_time:51447ms step_avg:61.32ms
step:840/2330 train_time:51510ms step_avg:61.32ms
step:841/2330 train_time:51570ms step_avg:61.32ms
step:842/2330 train_time:51634ms step_avg:61.32ms
step:843/2330 train_time:51694ms step_avg:61.32ms
step:844/2330 train_time:51758ms step_avg:61.32ms
step:845/2330 train_time:51817ms step_avg:61.32ms
step:846/2330 train_time:51880ms step_avg:61.32ms
step:847/2330 train_time:51941ms step_avg:61.32ms
step:848/2330 train_time:52005ms step_avg:61.33ms
step:849/2330 train_time:52065ms step_avg:61.33ms
step:850/2330 train_time:52129ms step_avg:61.33ms
step:851/2330 train_time:52189ms step_avg:61.33ms
step:852/2330 train_time:52253ms step_avg:61.33ms
step:853/2330 train_time:52314ms step_avg:61.33ms
step:854/2330 train_time:52377ms step_avg:61.33ms
step:855/2330 train_time:52437ms step_avg:61.33ms
step:856/2330 train_time:52501ms step_avg:61.33ms
step:857/2330 train_time:52561ms step_avg:61.33ms
step:858/2330 train_time:52624ms step_avg:61.33ms
step:859/2330 train_time:52685ms step_avg:61.33ms
step:860/2330 train_time:52748ms step_avg:61.33ms
step:861/2330 train_time:52809ms step_avg:61.33ms
step:862/2330 train_time:52872ms step_avg:61.34ms
step:863/2330 train_time:52932ms step_avg:61.34ms
step:864/2330 train_time:52996ms step_avg:61.34ms
step:865/2330 train_time:53055ms step_avg:61.34ms
step:866/2330 train_time:53119ms step_avg:61.34ms
step:867/2330 train_time:53180ms step_avg:61.34ms
step:868/2330 train_time:53244ms step_avg:61.34ms
step:869/2330 train_time:53305ms step_avg:61.34ms
step:870/2330 train_time:53368ms step_avg:61.34ms
step:871/2330 train_time:53428ms step_avg:61.34ms
step:872/2330 train_time:53492ms step_avg:61.34ms
step:873/2330 train_time:53552ms step_avg:61.34ms
step:874/2330 train_time:53616ms step_avg:61.35ms
step:875/2330 train_time:53676ms step_avg:61.34ms
step:876/2330 train_time:53739ms step_avg:61.35ms
step:877/2330 train_time:53800ms step_avg:61.35ms
step:878/2330 train_time:53863ms step_avg:61.35ms
step:879/2330 train_time:53923ms step_avg:61.35ms
step:880/2330 train_time:53987ms step_avg:61.35ms
step:881/2330 train_time:54048ms step_avg:61.35ms
step:882/2330 train_time:54112ms step_avg:61.35ms
step:883/2330 train_time:54173ms step_avg:61.35ms
step:884/2330 train_time:54237ms step_avg:61.35ms
step:885/2330 train_time:54297ms step_avg:61.35ms
step:886/2330 train_time:54360ms step_avg:61.35ms
step:887/2330 train_time:54421ms step_avg:61.35ms
step:888/2330 train_time:54485ms step_avg:61.36ms
step:889/2330 train_time:54546ms step_avg:61.36ms
step:890/2330 train_time:54610ms step_avg:61.36ms
step:891/2330 train_time:54670ms step_avg:61.36ms
step:892/2330 train_time:54734ms step_avg:61.36ms
step:893/2330 train_time:54794ms step_avg:61.36ms
step:894/2330 train_time:54857ms step_avg:61.36ms
step:895/2330 train_time:54919ms step_avg:61.36ms
step:896/2330 train_time:54981ms step_avg:61.36ms
step:897/2330 train_time:55041ms step_avg:61.36ms
step:898/2330 train_time:55106ms step_avg:61.37ms
step:899/2330 train_time:55167ms step_avg:61.36ms
step:900/2330 train_time:55230ms step_avg:61.37ms
step:901/2330 train_time:55291ms step_avg:61.37ms
step:902/2330 train_time:55355ms step_avg:61.37ms
step:903/2330 train_time:55415ms step_avg:61.37ms
step:904/2330 train_time:55478ms step_avg:61.37ms
step:905/2330 train_time:55539ms step_avg:61.37ms
step:906/2330 train_time:55602ms step_avg:61.37ms
step:907/2330 train_time:55662ms step_avg:61.37ms
step:908/2330 train_time:55726ms step_avg:61.37ms
step:909/2330 train_time:55786ms step_avg:61.37ms
step:910/2330 train_time:55850ms step_avg:61.37ms
step:911/2330 train_time:55911ms step_avg:61.37ms
step:912/2330 train_time:55973ms step_avg:61.37ms
step:913/2330 train_time:56034ms step_avg:61.37ms
step:914/2330 train_time:56098ms step_avg:61.38ms
step:915/2330 train_time:56159ms step_avg:61.38ms
step:916/2330 train_time:56222ms step_avg:61.38ms
step:917/2330 train_time:56283ms step_avg:61.38ms
step:918/2330 train_time:56346ms step_avg:61.38ms
step:919/2330 train_time:56407ms step_avg:61.38ms
step:920/2330 train_time:56470ms step_avg:61.38ms
step:921/2330 train_time:56531ms step_avg:61.38ms
step:922/2330 train_time:56594ms step_avg:61.38ms
step:923/2330 train_time:56655ms step_avg:61.38ms
step:924/2330 train_time:56718ms step_avg:61.38ms
step:925/2330 train_time:56778ms step_avg:61.38ms
step:926/2330 train_time:56842ms step_avg:61.38ms
step:927/2330 train_time:56902ms step_avg:61.38ms
step:928/2330 train_time:56966ms step_avg:61.39ms
step:929/2330 train_time:57027ms step_avg:61.39ms
step:930/2330 train_time:57092ms step_avg:61.39ms
step:931/2330 train_time:57152ms step_avg:61.39ms
step:932/2330 train_time:57215ms step_avg:61.39ms
step:933/2330 train_time:57275ms step_avg:61.39ms
step:934/2330 train_time:57338ms step_avg:61.39ms
step:935/2330 train_time:57398ms step_avg:61.39ms
step:936/2330 train_time:57462ms step_avg:61.39ms
step:937/2330 train_time:57523ms step_avg:61.39ms
step:938/2330 train_time:57587ms step_avg:61.39ms
step:939/2330 train_time:57648ms step_avg:61.39ms
step:940/2330 train_time:57711ms step_avg:61.39ms
step:941/2330 train_time:57771ms step_avg:61.39ms
step:942/2330 train_time:57834ms step_avg:61.40ms
step:943/2330 train_time:57895ms step_avg:61.39ms
step:944/2330 train_time:57958ms step_avg:61.40ms
step:945/2330 train_time:58018ms step_avg:61.40ms
step:946/2330 train_time:58082ms step_avg:61.40ms
step:947/2330 train_time:58143ms step_avg:61.40ms
step:948/2330 train_time:58207ms step_avg:61.40ms
step:949/2330 train_time:58267ms step_avg:61.40ms
step:950/2330 train_time:58331ms step_avg:61.40ms
step:951/2330 train_time:58390ms step_avg:61.40ms
step:952/2330 train_time:58454ms step_avg:61.40ms
step:953/2330 train_time:58514ms step_avg:61.40ms
step:954/2330 train_time:58577ms step_avg:61.40ms
step:955/2330 train_time:58637ms step_avg:61.40ms
step:956/2330 train_time:58700ms step_avg:61.40ms
step:957/2330 train_time:58761ms step_avg:61.40ms
step:958/2330 train_time:58825ms step_avg:61.40ms
step:959/2330 train_time:58885ms step_avg:61.40ms
step:960/2330 train_time:58949ms step_avg:61.41ms
step:961/2330 train_time:59009ms step_avg:61.40ms
step:962/2330 train_time:59072ms step_avg:61.41ms
step:963/2330 train_time:59132ms step_avg:61.40ms
step:964/2330 train_time:59196ms step_avg:61.41ms
step:965/2330 train_time:59256ms step_avg:61.40ms
step:966/2330 train_time:59319ms step_avg:61.41ms
step:967/2330 train_time:59379ms step_avg:61.41ms
step:968/2330 train_time:59443ms step_avg:61.41ms
step:969/2330 train_time:59504ms step_avg:61.41ms
step:970/2330 train_time:59567ms step_avg:61.41ms
step:971/2330 train_time:59628ms step_avg:61.41ms
step:972/2330 train_time:59691ms step_avg:61.41ms
step:973/2330 train_time:59751ms step_avg:61.41ms
step:974/2330 train_time:59814ms step_avg:61.41ms
step:975/2330 train_time:59874ms step_avg:61.41ms
step:976/2330 train_time:59937ms step_avg:61.41ms
step:977/2330 train_time:59997ms step_avg:61.41ms
step:978/2330 train_time:60061ms step_avg:61.41ms
step:979/2330 train_time:60122ms step_avg:61.41ms
step:980/2330 train_time:60186ms step_avg:61.41ms
step:981/2330 train_time:60247ms step_avg:61.41ms
step:982/2330 train_time:60310ms step_avg:61.42ms
step:983/2330 train_time:60370ms step_avg:61.41ms
step:984/2330 train_time:60434ms step_avg:61.42ms
step:985/2330 train_time:60495ms step_avg:61.42ms
step:986/2330 train_time:60558ms step_avg:61.42ms
step:987/2330 train_time:60619ms step_avg:61.42ms
step:988/2330 train_time:60683ms step_avg:61.42ms
step:989/2330 train_time:60745ms step_avg:61.42ms
step:990/2330 train_time:60809ms step_avg:61.42ms
step:991/2330 train_time:60869ms step_avg:61.42ms
step:992/2330 train_time:60932ms step_avg:61.42ms
step:993/2330 train_time:60992ms step_avg:61.42ms
step:994/2330 train_time:61055ms step_avg:61.42ms
step:995/2330 train_time:61116ms step_avg:61.42ms
step:996/2330 train_time:61178ms step_avg:61.42ms
step:997/2330 train_time:61239ms step_avg:61.42ms
step:998/2330 train_time:61302ms step_avg:61.43ms
step:999/2330 train_time:61363ms step_avg:61.42ms
step:1000/2330 train_time:61426ms step_avg:61.43ms
step:1000/2330 val_loss:3.6267 train_time:61491ms step_avg:61.49ms
step:1001/2330 train_time:61514ms step_avg:61.45ms
step:1002/2330 train_time:61553ms step_avg:61.43ms
step:1003/2330 train_time:61619ms step_avg:61.43ms
step:1004/2330 train_time:61685ms step_avg:61.44ms
step:1005/2330 train_time:61746ms step_avg:61.44ms
step:1006/2330 train_time:61809ms step_avg:61.44ms
step:1007/2330 train_time:61869ms step_avg:61.44ms
step:1008/2330 train_time:61931ms step_avg:61.44ms
step:1009/2330 train_time:61991ms step_avg:61.44ms
step:1010/2330 train_time:62054ms step_avg:61.44ms
step:1011/2330 train_time:62114ms step_avg:61.44ms
step:1012/2330 train_time:62176ms step_avg:61.44ms
step:1013/2330 train_time:62235ms step_avg:61.44ms
step:1014/2330 train_time:62297ms step_avg:61.44ms
step:1015/2330 train_time:62357ms step_avg:61.44ms
step:1016/2330 train_time:62422ms step_avg:61.44ms
step:1017/2330 train_time:62484ms step_avg:61.44ms
step:1018/2330 train_time:62549ms step_avg:61.44ms
step:1019/2330 train_time:62611ms step_avg:61.44ms
step:1020/2330 train_time:62676ms step_avg:61.45ms
step:1021/2330 train_time:62737ms step_avg:61.45ms
step:1022/2330 train_time:62801ms step_avg:61.45ms
step:1023/2330 train_time:62862ms step_avg:61.45ms
step:1024/2330 train_time:62926ms step_avg:61.45ms
step:1025/2330 train_time:62986ms step_avg:61.45ms
step:1026/2330 train_time:63049ms step_avg:61.45ms
step:1027/2330 train_time:63109ms step_avg:61.45ms
step:1028/2330 train_time:63172ms step_avg:61.45ms
step:1029/2330 train_time:63231ms step_avg:61.45ms
step:1030/2330 train_time:63294ms step_avg:61.45ms
step:1031/2330 train_time:63354ms step_avg:61.45ms
step:1032/2330 train_time:63417ms step_avg:61.45ms
step:1033/2330 train_time:63478ms step_avg:61.45ms
step:1034/2330 train_time:63542ms step_avg:61.45ms
step:1035/2330 train_time:63604ms step_avg:61.45ms
step:1036/2330 train_time:63668ms step_avg:61.46ms
step:1037/2330 train_time:63729ms step_avg:61.45ms
step:1038/2330 train_time:63792ms step_avg:61.46ms
step:1039/2330 train_time:63853ms step_avg:61.46ms
step:1040/2330 train_time:63916ms step_avg:61.46ms
step:1041/2330 train_time:63976ms step_avg:61.46ms
step:1042/2330 train_time:64040ms step_avg:61.46ms
step:1043/2330 train_time:64100ms step_avg:61.46ms
step:1044/2330 train_time:64164ms step_avg:61.46ms
step:1045/2330 train_time:64225ms step_avg:61.46ms
step:1046/2330 train_time:64288ms step_avg:61.46ms
step:1047/2330 train_time:64348ms step_avg:61.46ms
step:1048/2330 train_time:64411ms step_avg:61.46ms
step:1049/2330 train_time:64471ms step_avg:61.46ms
step:1050/2330 train_time:64535ms step_avg:61.46ms
step:1051/2330 train_time:64595ms step_avg:61.46ms
step:1052/2330 train_time:64659ms step_avg:61.46ms
step:1053/2330 train_time:64720ms step_avg:61.46ms
step:1054/2330 train_time:64784ms step_avg:61.47ms
step:1055/2330 train_time:64845ms step_avg:61.46ms
step:1056/2330 train_time:64909ms step_avg:61.47ms
step:1057/2330 train_time:64969ms step_avg:61.47ms
step:1058/2330 train_time:65032ms step_avg:61.47ms
step:1059/2330 train_time:65092ms step_avg:61.47ms
step:1060/2330 train_time:65156ms step_avg:61.47ms
step:1061/2330 train_time:65216ms step_avg:61.47ms
step:1062/2330 train_time:65279ms step_avg:61.47ms
step:1063/2330 train_time:65340ms step_avg:61.47ms
step:1064/2330 train_time:65405ms step_avg:61.47ms
step:1065/2330 train_time:65466ms step_avg:61.47ms
step:1066/2330 train_time:65529ms step_avg:61.47ms
step:1067/2330 train_time:65589ms step_avg:61.47ms
step:1068/2330 train_time:65653ms step_avg:61.47ms
step:1069/2330 train_time:65713ms step_avg:61.47ms
step:1070/2330 train_time:65777ms step_avg:61.47ms
step:1071/2330 train_time:65837ms step_avg:61.47ms
step:1072/2330 train_time:65902ms step_avg:61.48ms
step:1073/2330 train_time:65963ms step_avg:61.47ms
step:1074/2330 train_time:66027ms step_avg:61.48ms
step:1075/2330 train_time:66087ms step_avg:61.48ms
step:1076/2330 train_time:66151ms step_avg:61.48ms
step:1077/2330 train_time:66211ms step_avg:61.48ms
step:1078/2330 train_time:66273ms step_avg:61.48ms
step:1079/2330 train_time:66333ms step_avg:61.48ms
step:1080/2330 train_time:66397ms step_avg:61.48ms
step:1081/2330 train_time:66458ms step_avg:61.48ms
step:1082/2330 train_time:66521ms step_avg:61.48ms
step:1083/2330 train_time:66582ms step_avg:61.48ms
step:1084/2330 train_time:66646ms step_avg:61.48ms
step:1085/2330 train_time:66707ms step_avg:61.48ms
step:1086/2330 train_time:66771ms step_avg:61.48ms
step:1087/2330 train_time:66832ms step_avg:61.48ms
step:1088/2330 train_time:66895ms step_avg:61.48ms
step:1089/2330 train_time:66955ms step_avg:61.48ms
step:1090/2330 train_time:67019ms step_avg:61.49ms
step:1091/2330 train_time:67080ms step_avg:61.48ms
step:1092/2330 train_time:67145ms step_avg:61.49ms
step:1093/2330 train_time:67205ms step_avg:61.49ms
step:1094/2330 train_time:67269ms step_avg:61.49ms
step:1095/2330 train_time:67329ms step_avg:61.49ms
step:1096/2330 train_time:67393ms step_avg:61.49ms
step:1097/2330 train_time:67453ms step_avg:61.49ms
step:1098/2330 train_time:67517ms step_avg:61.49ms
step:1099/2330 train_time:67577ms step_avg:61.49ms
step:1100/2330 train_time:67640ms step_avg:61.49ms
step:1101/2330 train_time:67700ms step_avg:61.49ms
step:1102/2330 train_time:67765ms step_avg:61.49ms
step:1103/2330 train_time:67825ms step_avg:61.49ms
step:1104/2330 train_time:67888ms step_avg:61.49ms
step:1105/2330 train_time:67949ms step_avg:61.49ms
step:1106/2330 train_time:68012ms step_avg:61.49ms
step:1107/2330 train_time:68073ms step_avg:61.49ms
step:1108/2330 train_time:68136ms step_avg:61.49ms
step:1109/2330 train_time:68196ms step_avg:61.49ms
step:1110/2330 train_time:68260ms step_avg:61.50ms
step:1111/2330 train_time:68320ms step_avg:61.49ms
step:1112/2330 train_time:68385ms step_avg:61.50ms
step:1113/2330 train_time:68447ms step_avg:61.50ms
step:1114/2330 train_time:68510ms step_avg:61.50ms
step:1115/2330 train_time:68570ms step_avg:61.50ms
step:1116/2330 train_time:68632ms step_avg:61.50ms
step:1117/2330 train_time:68693ms step_avg:61.50ms
step:1118/2330 train_time:68757ms step_avg:61.50ms
step:1119/2330 train_time:68817ms step_avg:61.50ms
step:1120/2330 train_time:68881ms step_avg:61.50ms
step:1121/2330 train_time:68943ms step_avg:61.50ms
step:1122/2330 train_time:69007ms step_avg:61.50ms
step:1123/2330 train_time:69067ms step_avg:61.50ms
step:1124/2330 train_time:69130ms step_avg:61.50ms
step:1125/2330 train_time:69190ms step_avg:61.50ms
step:1126/2330 train_time:69254ms step_avg:61.50ms
step:1127/2330 train_time:69315ms step_avg:61.50ms
step:1128/2330 train_time:69378ms step_avg:61.50ms
step:1129/2330 train_time:69438ms step_avg:61.50ms
step:1130/2330 train_time:69501ms step_avg:61.51ms
step:1131/2330 train_time:69562ms step_avg:61.50ms
step:1132/2330 train_time:69626ms step_avg:61.51ms
step:1133/2330 train_time:69686ms step_avg:61.51ms
step:1134/2330 train_time:69750ms step_avg:61.51ms
step:1135/2330 train_time:69810ms step_avg:61.51ms
step:1136/2330 train_time:69874ms step_avg:61.51ms
step:1137/2330 train_time:69933ms step_avg:61.51ms
step:1138/2330 train_time:69997ms step_avg:61.51ms
step:1139/2330 train_time:70057ms step_avg:61.51ms
step:1140/2330 train_time:70121ms step_avg:61.51ms
step:1141/2330 train_time:70182ms step_avg:61.51ms
step:1142/2330 train_time:70247ms step_avg:61.51ms
step:1143/2330 train_time:70307ms step_avg:61.51ms
step:1144/2330 train_time:70371ms step_avg:61.51ms
step:1145/2330 train_time:70431ms step_avg:61.51ms
step:1146/2330 train_time:70494ms step_avg:61.51ms
step:1147/2330 train_time:70554ms step_avg:61.51ms
step:1148/2330 train_time:70617ms step_avg:61.51ms
step:1149/2330 train_time:70677ms step_avg:61.51ms
step:1150/2330 train_time:70741ms step_avg:61.51ms
step:1151/2330 train_time:70802ms step_avg:61.51ms
step:1152/2330 train_time:70865ms step_avg:61.52ms
step:1153/2330 train_time:70927ms step_avg:61.51ms
step:1154/2330 train_time:70990ms step_avg:61.52ms
step:1155/2330 train_time:71052ms step_avg:61.52ms
step:1156/2330 train_time:71115ms step_avg:61.52ms
step:1157/2330 train_time:71175ms step_avg:61.52ms
step:1158/2330 train_time:71239ms step_avg:61.52ms
step:1159/2330 train_time:71299ms step_avg:61.52ms
step:1160/2330 train_time:71363ms step_avg:61.52ms
step:1161/2330 train_time:71425ms step_avg:61.52ms
step:1162/2330 train_time:71488ms step_avg:61.52ms
step:1163/2330 train_time:71548ms step_avg:61.52ms
step:1164/2330 train_time:71611ms step_avg:61.52ms
step:1165/2330 train_time:71671ms step_avg:61.52ms
step:1166/2330 train_time:71734ms step_avg:61.52ms
step:1167/2330 train_time:71794ms step_avg:61.52ms
step:1168/2330 train_time:71857ms step_avg:61.52ms
step:1169/2330 train_time:71917ms step_avg:61.52ms
step:1170/2330 train_time:71981ms step_avg:61.52ms
step:1171/2330 train_time:72042ms step_avg:61.52ms
step:1172/2330 train_time:72106ms step_avg:61.52ms
step:1173/2330 train_time:72166ms step_avg:61.52ms
step:1174/2330 train_time:72229ms step_avg:61.52ms
step:1175/2330 train_time:72289ms step_avg:61.52ms
step:1176/2330 train_time:72353ms step_avg:61.52ms
step:1177/2330 train_time:72413ms step_avg:61.52ms
step:1178/2330 train_time:72477ms step_avg:61.53ms
step:1179/2330 train_time:72536ms step_avg:61.52ms
step:1180/2330 train_time:72600ms step_avg:61.53ms
step:1181/2330 train_time:72661ms step_avg:61.53ms
step:1182/2330 train_time:72725ms step_avg:61.53ms
step:1183/2330 train_time:72785ms step_avg:61.53ms
step:1184/2330 train_time:72849ms step_avg:61.53ms
step:1185/2330 train_time:72908ms step_avg:61.53ms
step:1186/2330 train_time:72972ms step_avg:61.53ms
step:1187/2330 train_time:73032ms step_avg:61.53ms
step:1188/2330 train_time:73096ms step_avg:61.53ms
step:1189/2330 train_time:73156ms step_avg:61.53ms
step:1190/2330 train_time:73220ms step_avg:61.53ms
step:1191/2330 train_time:73280ms step_avg:61.53ms
step:1192/2330 train_time:73344ms step_avg:61.53ms
step:1193/2330 train_time:73405ms step_avg:61.53ms
step:1194/2330 train_time:73468ms step_avg:61.53ms
step:1195/2330 train_time:73528ms step_avg:61.53ms
step:1196/2330 train_time:73592ms step_avg:61.53ms
step:1197/2330 train_time:73651ms step_avg:61.53ms
step:1198/2330 train_time:73715ms step_avg:61.53ms
step:1199/2330 train_time:73775ms step_avg:61.53ms
step:1200/2330 train_time:73839ms step_avg:61.53ms
step:1201/2330 train_time:73899ms step_avg:61.53ms
step:1202/2330 train_time:73963ms step_avg:61.53ms
step:1203/2330 train_time:74025ms step_avg:61.53ms
step:1204/2330 train_time:74088ms step_avg:61.54ms
step:1205/2330 train_time:74148ms step_avg:61.53ms
step:1206/2330 train_time:74212ms step_avg:61.54ms
step:1207/2330 train_time:74272ms step_avg:61.53ms
step:1208/2330 train_time:74335ms step_avg:61.54ms
step:1209/2330 train_time:74396ms step_avg:61.54ms
step:1210/2330 train_time:74459ms step_avg:61.54ms
step:1211/2330 train_time:74520ms step_avg:61.54ms
step:1212/2330 train_time:74584ms step_avg:61.54ms
step:1213/2330 train_time:74645ms step_avg:61.54ms
step:1214/2330 train_time:74709ms step_avg:61.54ms
step:1215/2330 train_time:74768ms step_avg:61.54ms
step:1216/2330 train_time:74832ms step_avg:61.54ms
step:1217/2330 train_time:74892ms step_avg:61.54ms
step:1218/2330 train_time:74955ms step_avg:61.54ms
step:1219/2330 train_time:75016ms step_avg:61.54ms
step:1220/2330 train_time:75079ms step_avg:61.54ms
step:1221/2330 train_time:75140ms step_avg:61.54ms
step:1222/2330 train_time:75204ms step_avg:61.54ms
step:1223/2330 train_time:75265ms step_avg:61.54ms
step:1224/2330 train_time:75328ms step_avg:61.54ms
step:1225/2330 train_time:75388ms step_avg:61.54ms
step:1226/2330 train_time:75451ms step_avg:61.54ms
step:1227/2330 train_time:75512ms step_avg:61.54ms
step:1228/2330 train_time:75576ms step_avg:61.54ms
step:1229/2330 train_time:75636ms step_avg:61.54ms
step:1230/2330 train_time:75699ms step_avg:61.54ms
step:1231/2330 train_time:75760ms step_avg:61.54ms
step:1232/2330 train_time:75824ms step_avg:61.55ms
step:1233/2330 train_time:75885ms step_avg:61.55ms
step:1234/2330 train_time:75949ms step_avg:61.55ms
step:1235/2330 train_time:76009ms step_avg:61.55ms
step:1236/2330 train_time:76072ms step_avg:61.55ms
step:1237/2330 train_time:76133ms step_avg:61.55ms
step:1238/2330 train_time:76196ms step_avg:61.55ms
step:1239/2330 train_time:76256ms step_avg:61.55ms
step:1240/2330 train_time:76319ms step_avg:61.55ms
step:1241/2330 train_time:76380ms step_avg:61.55ms
step:1242/2330 train_time:76445ms step_avg:61.55ms
step:1243/2330 train_time:76506ms step_avg:61.55ms
step:1244/2330 train_time:76569ms step_avg:61.55ms
step:1245/2330 train_time:76629ms step_avg:61.55ms
step:1246/2330 train_time:76693ms step_avg:61.55ms
step:1247/2330 train_time:76753ms step_avg:61.55ms
step:1248/2330 train_time:76817ms step_avg:61.55ms
step:1249/2330 train_time:76877ms step_avg:61.55ms
step:1250/2330 train_time:76941ms step_avg:61.55ms
step:1250/2330 val_loss:3.5592 train_time:77007ms step_avg:61.61ms
step:1251/2330 train_time:77029ms step_avg:61.57ms
step:1252/2330 train_time:77069ms step_avg:61.56ms
step:1253/2330 train_time:77136ms step_avg:61.56ms
step:1254/2330 train_time:77200ms step_avg:61.56ms
step:1255/2330 train_time:77260ms step_avg:61.56ms
step:1256/2330 train_time:77324ms step_avg:61.56ms
step:1257/2330 train_time:77384ms step_avg:61.56ms
step:1258/2330 train_time:77447ms step_avg:61.56ms
step:1259/2330 train_time:77506ms step_avg:61.56ms
step:1260/2330 train_time:77568ms step_avg:61.56ms
step:1261/2330 train_time:77628ms step_avg:61.56ms
step:1262/2330 train_time:77691ms step_avg:61.56ms
step:1263/2330 train_time:77751ms step_avg:61.56ms
step:1264/2330 train_time:77814ms step_avg:61.56ms
step:1265/2330 train_time:77875ms step_avg:61.56ms
step:1266/2330 train_time:77939ms step_avg:61.56ms
step:1267/2330 train_time:77999ms step_avg:61.56ms
step:1268/2330 train_time:78064ms step_avg:61.56ms
step:1269/2330 train_time:78127ms step_avg:61.57ms
step:1270/2330 train_time:78191ms step_avg:61.57ms
step:1271/2330 train_time:78253ms step_avg:61.57ms
step:1272/2330 train_time:78317ms step_avg:61.57ms
step:1273/2330 train_time:78377ms step_avg:61.57ms
step:1274/2330 train_time:78440ms step_avg:61.57ms
step:1275/2330 train_time:78500ms step_avg:61.57ms
step:1276/2330 train_time:78563ms step_avg:61.57ms
step:1277/2330 train_time:78623ms step_avg:61.57ms
step:1278/2330 train_time:78686ms step_avg:61.57ms
step:1279/2330 train_time:78746ms step_avg:61.57ms
step:1280/2330 train_time:78809ms step_avg:61.57ms
step:1281/2330 train_time:78869ms step_avg:61.57ms
step:1282/2330 train_time:78933ms step_avg:61.57ms
step:1283/2330 train_time:78994ms step_avg:61.57ms
step:1284/2330 train_time:79058ms step_avg:61.57ms
step:1285/2330 train_time:79119ms step_avg:61.57ms
step:1286/2330 train_time:79183ms step_avg:61.57ms
step:1287/2330 train_time:79243ms step_avg:61.57ms
step:1288/2330 train_time:79307ms step_avg:61.57ms
step:1289/2330 train_time:79368ms step_avg:61.57ms
step:1290/2330 train_time:79431ms step_avg:61.57ms
step:1291/2330 train_time:79492ms step_avg:61.57ms
step:1292/2330 train_time:79556ms step_avg:61.58ms
step:1293/2330 train_time:79617ms step_avg:61.58ms
step:1294/2330 train_time:79681ms step_avg:61.58ms
step:1295/2330 train_time:79741ms step_avg:61.58ms
step:1296/2330 train_time:79804ms step_avg:61.58ms
step:1297/2330 train_time:79864ms step_avg:61.58ms
step:1298/2330 train_time:79927ms step_avg:61.58ms
step:1299/2330 train_time:79987ms step_avg:61.58ms
step:1300/2330 train_time:80052ms step_avg:61.58ms
step:1301/2330 train_time:80113ms step_avg:61.58ms
step:1302/2330 train_time:80178ms step_avg:61.58ms
step:1303/2330 train_time:80238ms step_avg:61.58ms
step:1304/2330 train_time:80302ms step_avg:61.58ms
step:1305/2330 train_time:80363ms step_avg:61.58ms
step:1306/2330 train_time:80425ms step_avg:61.58ms
step:1307/2330 train_time:80485ms step_avg:61.58ms
step:1308/2330 train_time:80548ms step_avg:61.58ms
step:1309/2330 train_time:80609ms step_avg:61.58ms
step:1310/2330 train_time:80673ms step_avg:61.58ms
step:1311/2330 train_time:80734ms step_avg:61.58ms
step:1312/2330 train_time:80798ms step_avg:61.58ms
step:1313/2330 train_time:80858ms step_avg:61.58ms
step:1314/2330 train_time:80921ms step_avg:61.58ms
step:1315/2330 train_time:80981ms step_avg:61.58ms
step:1316/2330 train_time:81044ms step_avg:61.58ms
step:1317/2330 train_time:81105ms step_avg:61.58ms
step:1318/2330 train_time:81168ms step_avg:61.58ms
step:1319/2330 train_time:81229ms step_avg:61.58ms
step:1320/2330 train_time:81294ms step_avg:61.59ms
step:1321/2330 train_time:81354ms step_avg:61.59ms
step:1322/2330 train_time:81418ms step_avg:61.59ms
step:1323/2330 train_time:81479ms step_avg:61.59ms
step:1324/2330 train_time:81542ms step_avg:61.59ms
step:1325/2330 train_time:81602ms step_avg:61.59ms
step:1326/2330 train_time:81665ms step_avg:61.59ms
step:1327/2330 train_time:81725ms step_avg:61.59ms
step:1328/2330 train_time:81789ms step_avg:61.59ms
step:1329/2330 train_time:81849ms step_avg:61.59ms
step:1330/2330 train_time:81912ms step_avg:61.59ms
step:1331/2330 train_time:81973ms step_avg:61.59ms
step:1332/2330 train_time:82036ms step_avg:61.59ms
step:1333/2330 train_time:82097ms step_avg:61.59ms
step:1334/2330 train_time:82160ms step_avg:61.59ms
step:1335/2330 train_time:82220ms step_avg:61.59ms
step:1336/2330 train_time:82284ms step_avg:61.59ms
step:1337/2330 train_time:82344ms step_avg:61.59ms
step:1338/2330 train_time:82407ms step_avg:61.59ms
step:1339/2330 train_time:82468ms step_avg:61.59ms
step:1340/2330 train_time:82532ms step_avg:61.59ms
step:1341/2330 train_time:82592ms step_avg:61.59ms
step:1342/2330 train_time:82657ms step_avg:61.59ms
step:1343/2330 train_time:82717ms step_avg:61.59ms
step:1344/2330 train_time:82781ms step_avg:61.59ms
step:1345/2330 train_time:82840ms step_avg:61.59ms
step:1346/2330 train_time:82904ms step_avg:61.59ms
step:1347/2330 train_time:82964ms step_avg:61.59ms
step:1348/2330 train_time:83027ms step_avg:61.59ms
step:1349/2330 train_time:83087ms step_avg:61.59ms
step:1350/2330 train_time:83151ms step_avg:61.59ms
step:1351/2330 train_time:83212ms step_avg:61.59ms
step:1352/2330 train_time:83277ms step_avg:61.60ms
step:1353/2330 train_time:83337ms step_avg:61.59ms
step:1354/2330 train_time:83401ms step_avg:61.60ms
step:1355/2330 train_time:83461ms step_avg:61.59ms
step:1356/2330 train_time:83524ms step_avg:61.60ms
step:1357/2330 train_time:83584ms step_avg:61.59ms
step:1358/2330 train_time:83648ms step_avg:61.60ms
step:1359/2330 train_time:83708ms step_avg:61.60ms
step:1360/2330 train_time:83772ms step_avg:61.60ms
step:1361/2330 train_time:83833ms step_avg:61.60ms
step:1362/2330 train_time:83897ms step_avg:61.60ms
step:1363/2330 train_time:83957ms step_avg:61.60ms
step:1364/2330 train_time:84020ms step_avg:61.60ms
step:1365/2330 train_time:84081ms step_avg:61.60ms
step:1366/2330 train_time:84144ms step_avg:61.60ms
step:1367/2330 train_time:84205ms step_avg:61.60ms
step:1368/2330 train_time:84268ms step_avg:61.60ms
step:1369/2330 train_time:84329ms step_avg:61.60ms
step:1370/2330 train_time:84393ms step_avg:61.60ms
step:1371/2330 train_time:84454ms step_avg:61.60ms
step:1372/2330 train_time:84517ms step_avg:61.60ms
step:1373/2330 train_time:84577ms step_avg:61.60ms
step:1374/2330 train_time:84641ms step_avg:61.60ms
step:1375/2330 train_time:84701ms step_avg:61.60ms
step:1376/2330 train_time:84764ms step_avg:61.60ms
step:1377/2330 train_time:84824ms step_avg:61.60ms
step:1378/2330 train_time:84887ms step_avg:61.60ms
step:1379/2330 train_time:84947ms step_avg:61.60ms
step:1380/2330 train_time:85011ms step_avg:61.60ms
step:1381/2330 train_time:85072ms step_avg:61.60ms
step:1382/2330 train_time:85136ms step_avg:61.60ms
step:1383/2330 train_time:85197ms step_avg:61.60ms
step:1384/2330 train_time:85260ms step_avg:61.60ms
step:1385/2330 train_time:85321ms step_avg:61.60ms
step:1386/2330 train_time:85385ms step_avg:61.61ms
step:1387/2330 train_time:85445ms step_avg:61.60ms
step:1388/2330 train_time:85508ms step_avg:61.61ms
step:1389/2330 train_time:85569ms step_avg:61.60ms
step:1390/2330 train_time:85634ms step_avg:61.61ms
step:1391/2330 train_time:85694ms step_avg:61.61ms
step:1392/2330 train_time:85758ms step_avg:61.61ms
step:1393/2330 train_time:85818ms step_avg:61.61ms
step:1394/2330 train_time:85881ms step_avg:61.61ms
step:1395/2330 train_time:85941ms step_avg:61.61ms
step:1396/2330 train_time:86005ms step_avg:61.61ms
step:1397/2330 train_time:86065ms step_avg:61.61ms
step:1398/2330 train_time:86128ms step_avg:61.61ms
step:1399/2330 train_time:86188ms step_avg:61.61ms
step:1400/2330 train_time:86253ms step_avg:61.61ms
step:1401/2330 train_time:86313ms step_avg:61.61ms
step:1402/2330 train_time:86377ms step_avg:61.61ms
step:1403/2330 train_time:86438ms step_avg:61.61ms
step:1404/2330 train_time:86502ms step_avg:61.61ms
step:1405/2330 train_time:86562ms step_avg:61.61ms
step:1406/2330 train_time:86625ms step_avg:61.61ms
step:1407/2330 train_time:86685ms step_avg:61.61ms
step:1408/2330 train_time:86748ms step_avg:61.61ms
step:1409/2330 train_time:86809ms step_avg:61.61ms
step:1410/2330 train_time:86874ms step_avg:61.61ms
step:1411/2330 train_time:86934ms step_avg:61.61ms
step:1412/2330 train_time:86998ms step_avg:61.61ms
step:1413/2330 train_time:87058ms step_avg:61.61ms
step:1414/2330 train_time:87121ms step_avg:61.61ms
step:1415/2330 train_time:87182ms step_avg:61.61ms
step:1416/2330 train_time:87246ms step_avg:61.61ms
step:1417/2330 train_time:87307ms step_avg:61.61ms
step:1418/2330 train_time:87370ms step_avg:61.61ms
step:1419/2330 train_time:87431ms step_avg:61.61ms
step:1420/2330 train_time:87494ms step_avg:61.62ms
step:1421/2330 train_time:87555ms step_avg:61.61ms
step:1422/2330 train_time:87619ms step_avg:61.62ms
step:1423/2330 train_time:87679ms step_avg:61.62ms
step:1424/2330 train_time:87742ms step_avg:61.62ms
step:1425/2330 train_time:87801ms step_avg:61.61ms
step:1426/2330 train_time:87864ms step_avg:61.62ms
step:1427/2330 train_time:87925ms step_avg:61.62ms
step:1428/2330 train_time:87988ms step_avg:61.62ms
step:1429/2330 train_time:88049ms step_avg:61.62ms
step:1430/2330 train_time:88113ms step_avg:61.62ms
step:1431/2330 train_time:88174ms step_avg:61.62ms
step:1432/2330 train_time:88237ms step_avg:61.62ms
step:1433/2330 train_time:88297ms step_avg:61.62ms
step:1434/2330 train_time:88360ms step_avg:61.62ms
step:1435/2330 train_time:88422ms step_avg:61.62ms
step:1436/2330 train_time:88485ms step_avg:61.62ms
step:1437/2330 train_time:88545ms step_avg:61.62ms
step:1438/2330 train_time:88608ms step_avg:61.62ms
step:1439/2330 train_time:88669ms step_avg:61.62ms
step:1440/2330 train_time:88733ms step_avg:61.62ms
step:1441/2330 train_time:88794ms step_avg:61.62ms
step:1442/2330 train_time:88858ms step_avg:61.62ms
step:1443/2330 train_time:88918ms step_avg:61.62ms
step:1444/2330 train_time:88981ms step_avg:61.62ms
step:1445/2330 train_time:89041ms step_avg:61.62ms
step:1446/2330 train_time:89104ms step_avg:61.62ms
step:1447/2330 train_time:89164ms step_avg:61.62ms
step:1448/2330 train_time:89228ms step_avg:61.62ms
step:1449/2330 train_time:89288ms step_avg:61.62ms
step:1450/2330 train_time:89352ms step_avg:61.62ms
step:1451/2330 train_time:89413ms step_avg:61.62ms
step:1452/2330 train_time:89477ms step_avg:61.62ms
step:1453/2330 train_time:89537ms step_avg:61.62ms
step:1454/2330 train_time:89601ms step_avg:61.62ms
step:1455/2330 train_time:89661ms step_avg:61.62ms
step:1456/2330 train_time:89724ms step_avg:61.62ms
step:1457/2330 train_time:89784ms step_avg:61.62ms
step:1458/2330 train_time:89847ms step_avg:61.62ms
step:1459/2330 train_time:89908ms step_avg:61.62ms
step:1460/2330 train_time:89972ms step_avg:61.62ms
step:1461/2330 train_time:90033ms step_avg:61.62ms
step:1462/2330 train_time:90097ms step_avg:61.63ms
step:1463/2330 train_time:90157ms step_avg:61.62ms
step:1464/2330 train_time:90220ms step_avg:61.63ms
step:1465/2330 train_time:90281ms step_avg:61.63ms
step:1466/2330 train_time:90344ms step_avg:61.63ms
step:1467/2330 train_time:90404ms step_avg:61.63ms
step:1468/2330 train_time:90467ms step_avg:61.63ms
step:1469/2330 train_time:90527ms step_avg:61.63ms
step:1470/2330 train_time:90591ms step_avg:61.63ms
step:1471/2330 train_time:90652ms step_avg:61.63ms
step:1472/2330 train_time:90715ms step_avg:61.63ms
step:1473/2330 train_time:90776ms step_avg:61.63ms
step:1474/2330 train_time:90839ms step_avg:61.63ms
step:1475/2330 train_time:90899ms step_avg:61.63ms
step:1476/2330 train_time:90963ms step_avg:61.63ms
step:1477/2330 train_time:91024ms step_avg:61.63ms
step:1478/2330 train_time:91087ms step_avg:61.63ms
step:1479/2330 train_time:91147ms step_avg:61.63ms
step:1480/2330 train_time:91211ms step_avg:61.63ms
step:1481/2330 train_time:91271ms step_avg:61.63ms
step:1482/2330 train_time:91336ms step_avg:61.63ms
step:1483/2330 train_time:91396ms step_avg:61.63ms
step:1484/2330 train_time:91459ms step_avg:61.63ms
step:1485/2330 train_time:91520ms step_avg:61.63ms
step:1486/2330 train_time:91583ms step_avg:61.63ms
step:1487/2330 train_time:91643ms step_avg:61.63ms
step:1488/2330 train_time:91707ms step_avg:61.63ms
step:1489/2330 train_time:91766ms step_avg:61.63ms
step:1490/2330 train_time:91830ms step_avg:61.63ms
step:1491/2330 train_time:91891ms step_avg:61.63ms
step:1492/2330 train_time:91955ms step_avg:61.63ms
step:1493/2330 train_time:92015ms step_avg:61.63ms
step:1494/2330 train_time:92079ms step_avg:61.63ms
step:1495/2330 train_time:92139ms step_avg:61.63ms
step:1496/2330 train_time:92202ms step_avg:61.63ms
step:1497/2330 train_time:92262ms step_avg:61.63ms
step:1498/2330 train_time:92326ms step_avg:61.63ms
step:1499/2330 train_time:92386ms step_avg:61.63ms
step:1500/2330 train_time:92450ms step_avg:61.63ms
step:1500/2330 val_loss:3.5235 train_time:92515ms step_avg:61.68ms
step:1501/2330 train_time:92538ms step_avg:61.65ms
step:1502/2330 train_time:92576ms step_avg:61.64ms
step:1503/2330 train_time:92640ms step_avg:61.64ms
step:1504/2330 train_time:92706ms step_avg:61.64ms
step:1505/2330 train_time:92767ms step_avg:61.64ms
step:1506/2330 train_time:92830ms step_avg:61.64ms
step:1507/2330 train_time:92890ms step_avg:61.64ms
step:1508/2330 train_time:92954ms step_avg:61.64ms
step:1509/2330 train_time:93013ms step_avg:61.64ms
step:1510/2330 train_time:93075ms step_avg:61.64ms
step:1511/2330 train_time:93134ms step_avg:61.64ms
step:1512/2330 train_time:93197ms step_avg:61.64ms
step:1513/2330 train_time:93256ms step_avg:61.64ms
step:1514/2330 train_time:93319ms step_avg:61.64ms
step:1515/2330 train_time:93378ms step_avg:61.64ms
step:1516/2330 train_time:93441ms step_avg:61.64ms
step:1517/2330 train_time:93503ms step_avg:61.64ms
step:1518/2330 train_time:93568ms step_avg:61.64ms
step:1519/2330 train_time:93630ms step_avg:61.64ms
step:1520/2330 train_time:93695ms step_avg:61.64ms
step:1521/2330 train_time:93756ms step_avg:61.64ms
step:1522/2330 train_time:93820ms step_avg:61.64ms
step:1523/2330 train_time:93880ms step_avg:61.64ms
step:1524/2330 train_time:93943ms step_avg:61.64ms
step:1525/2330 train_time:94003ms step_avg:61.64ms
step:1526/2330 train_time:94066ms step_avg:61.64ms
step:1527/2330 train_time:94126ms step_avg:61.64ms
step:1528/2330 train_time:94189ms step_avg:61.64ms
step:1529/2330 train_time:94249ms step_avg:61.64ms
step:1530/2330 train_time:94313ms step_avg:61.64ms
step:1531/2330 train_time:94373ms step_avg:61.64ms
step:1532/2330 train_time:94437ms step_avg:61.64ms
step:1533/2330 train_time:94498ms step_avg:61.64ms
step:1534/2330 train_time:94562ms step_avg:61.64ms
step:1535/2330 train_time:94623ms step_avg:61.64ms
step:1536/2330 train_time:94687ms step_avg:61.65ms
step:1537/2330 train_time:94749ms step_avg:61.65ms
step:1538/2330 train_time:94813ms step_avg:61.65ms
step:1539/2330 train_time:94874ms step_avg:61.65ms
step:1540/2330 train_time:94938ms step_avg:61.65ms
step:1541/2330 train_time:94998ms step_avg:61.65ms
step:1542/2330 train_time:95062ms step_avg:61.65ms
step:1543/2330 train_time:95122ms step_avg:61.65ms
step:1544/2330 train_time:95185ms step_avg:61.65ms
step:1545/2330 train_time:95245ms step_avg:61.65ms
step:1546/2330 train_time:95309ms step_avg:61.65ms
step:1547/2330 train_time:95370ms step_avg:61.65ms
step:1548/2330 train_time:95434ms step_avg:61.65ms
step:1549/2330 train_time:95495ms step_avg:61.65ms
step:1550/2330 train_time:95560ms step_avg:61.65ms
step:1551/2330 train_time:95620ms step_avg:61.65ms
step:1552/2330 train_time:95684ms step_avg:61.65ms
step:1553/2330 train_time:95746ms step_avg:61.65ms
step:1554/2330 train_time:95810ms step_avg:61.65ms
step:1555/2330 train_time:95871ms step_avg:61.65ms
step:1556/2330 train_time:95936ms step_avg:61.66ms
step:1557/2330 train_time:95996ms step_avg:61.65ms
step:1558/2330 train_time:96060ms step_avg:61.66ms
step:1559/2330 train_time:96121ms step_avg:61.66ms
step:1560/2330 train_time:96185ms step_avg:61.66ms
step:1561/2330 train_time:96245ms step_avg:61.66ms
step:1562/2330 train_time:96308ms step_avg:61.66ms
step:1563/2330 train_time:96369ms step_avg:61.66ms
step:1564/2330 train_time:96434ms step_avg:61.66ms
step:1565/2330 train_time:96495ms step_avg:61.66ms
step:1566/2330 train_time:96559ms step_avg:61.66ms
step:1567/2330 train_time:96620ms step_avg:61.66ms
step:1568/2330 train_time:96684ms step_avg:61.66ms
step:1569/2330 train_time:96745ms step_avg:61.66ms
step:1570/2330 train_time:96809ms step_avg:61.66ms
step:1571/2330 train_time:96871ms step_avg:61.66ms
step:1572/2330 train_time:96935ms step_avg:61.66ms
step:1573/2330 train_time:96996ms step_avg:61.66ms
step:1574/2330 train_time:97060ms step_avg:61.66ms
step:1575/2330 train_time:97121ms step_avg:61.66ms
step:1576/2330 train_time:97184ms step_avg:61.67ms
step:1577/2330 train_time:97244ms step_avg:61.66ms
step:1578/2330 train_time:97307ms step_avg:61.66ms
step:1579/2330 train_time:97368ms step_avg:61.66ms
step:1580/2330 train_time:97433ms step_avg:61.67ms
step:1581/2330 train_time:97494ms step_avg:61.67ms
step:1582/2330 train_time:97558ms step_avg:61.67ms
step:1583/2330 train_time:97618ms step_avg:61.67ms
step:1584/2330 train_time:97682ms step_avg:61.67ms
step:1585/2330 train_time:97743ms step_avg:61.67ms
step:1586/2330 train_time:97807ms step_avg:61.67ms
step:1587/2330 train_time:97868ms step_avg:61.67ms
step:1588/2330 train_time:97933ms step_avg:61.67ms
step:1589/2330 train_time:97995ms step_avg:61.67ms
step:1590/2330 train_time:98060ms step_avg:61.67ms
step:1591/2330 train_time:98121ms step_avg:61.67ms
step:1592/2330 train_time:98184ms step_avg:61.67ms
step:1593/2330 train_time:98245ms step_avg:61.67ms
step:1594/2330 train_time:98308ms step_avg:61.67ms
step:1595/2330 train_time:98369ms step_avg:61.67ms
step:1596/2330 train_time:98433ms step_avg:61.67ms
step:1597/2330 train_time:98494ms step_avg:61.67ms
step:1598/2330 train_time:98559ms step_avg:61.68ms
step:1599/2330 train_time:98619ms step_avg:61.68ms
step:1600/2330 train_time:98683ms step_avg:61.68ms
step:1601/2330 train_time:98743ms step_avg:61.68ms
step:1602/2330 train_time:98808ms step_avg:61.68ms
step:1603/2330 train_time:98869ms step_avg:61.68ms
step:1604/2330 train_time:98933ms step_avg:61.68ms
step:1605/2330 train_time:98994ms step_avg:61.68ms
step:1606/2330 train_time:99059ms step_avg:61.68ms
step:1607/2330 train_time:99119ms step_avg:61.68ms
step:1608/2330 train_time:99182ms step_avg:61.68ms
step:1609/2330 train_time:99243ms step_avg:61.68ms
step:1610/2330 train_time:99307ms step_avg:61.68ms
step:1611/2330 train_time:99367ms step_avg:61.68ms
step:1612/2330 train_time:99432ms step_avg:61.68ms
step:1613/2330 train_time:99493ms step_avg:61.68ms
step:1614/2330 train_time:99557ms step_avg:61.68ms
step:1615/2330 train_time:99618ms step_avg:61.68ms
step:1616/2330 train_time:99682ms step_avg:61.68ms
step:1617/2330 train_time:99742ms step_avg:61.68ms
step:1618/2330 train_time:99806ms step_avg:61.68ms
step:1619/2330 train_time:99867ms step_avg:61.68ms
step:1620/2330 train_time:99932ms step_avg:61.69ms
step:1621/2330 train_time:99994ms step_avg:61.69ms
step:1622/2330 train_time:100058ms step_avg:61.69ms
step:1623/2330 train_time:100118ms step_avg:61.69ms
step:1624/2330 train_time:100183ms step_avg:61.69ms
step:1625/2330 train_time:100244ms step_avg:61.69ms
step:1626/2330 train_time:100307ms step_avg:61.69ms
step:1627/2330 train_time:100367ms step_avg:61.69ms
step:1628/2330 train_time:100431ms step_avg:61.69ms
step:1629/2330 train_time:100492ms step_avg:61.69ms
step:1630/2330 train_time:100557ms step_avg:61.69ms
step:1631/2330 train_time:100617ms step_avg:61.69ms
step:1632/2330 train_time:100681ms step_avg:61.69ms
step:1633/2330 train_time:100741ms step_avg:61.69ms
step:1634/2330 train_time:100805ms step_avg:61.69ms
step:1635/2330 train_time:100867ms step_avg:61.69ms
step:1636/2330 train_time:100931ms step_avg:61.69ms
step:1637/2330 train_time:100993ms step_avg:61.69ms
step:1638/2330 train_time:101057ms step_avg:61.70ms
step:1639/2330 train_time:101117ms step_avg:61.69ms
step:1640/2330 train_time:101181ms step_avg:61.70ms
step:1641/2330 train_time:101242ms step_avg:61.70ms
step:1642/2330 train_time:101306ms step_avg:61.70ms
step:1643/2330 train_time:101366ms step_avg:61.70ms
step:1644/2330 train_time:101431ms step_avg:61.70ms
step:1645/2330 train_time:101492ms step_avg:61.70ms
step:1646/2330 train_time:101557ms step_avg:61.70ms
step:1647/2330 train_time:101617ms step_avg:61.70ms
step:1648/2330 train_time:101681ms step_avg:61.70ms
step:1649/2330 train_time:101742ms step_avg:61.70ms
step:1650/2330 train_time:101806ms step_avg:61.70ms
step:1651/2330 train_time:101867ms step_avg:61.70ms
step:1652/2330 train_time:101931ms step_avg:61.70ms
step:1653/2330 train_time:101993ms step_avg:61.70ms
step:1654/2330 train_time:102058ms step_avg:61.70ms
step:1655/2330 train_time:102118ms step_avg:61.70ms
step:1656/2330 train_time:102182ms step_avg:61.70ms
step:1657/2330 train_time:102242ms step_avg:61.70ms
step:1658/2330 train_time:102306ms step_avg:61.70ms
step:1659/2330 train_time:102366ms step_avg:61.70ms
step:1660/2330 train_time:102431ms step_avg:61.71ms
step:1661/2330 train_time:102493ms step_avg:61.71ms
step:1662/2330 train_time:102556ms step_avg:61.71ms
step:1663/2330 train_time:102617ms step_avg:61.71ms
step:1664/2330 train_time:102682ms step_avg:61.71ms
step:1665/2330 train_time:102742ms step_avg:61.71ms
step:1666/2330 train_time:102806ms step_avg:61.71ms
step:1667/2330 train_time:102867ms step_avg:61.71ms
step:1668/2330 train_time:102931ms step_avg:61.71ms
step:1669/2330 train_time:102994ms step_avg:61.71ms
step:1670/2330 train_time:103058ms step_avg:61.71ms
step:1671/2330 train_time:103118ms step_avg:61.71ms
step:1672/2330 train_time:103181ms step_avg:61.71ms
step:1673/2330 train_time:103242ms step_avg:61.71ms
step:1674/2330 train_time:103305ms step_avg:61.71ms
step:1675/2330 train_time:103366ms step_avg:61.71ms
step:1676/2330 train_time:103431ms step_avg:61.71ms
step:1677/2330 train_time:103492ms step_avg:61.71ms
step:1678/2330 train_time:103556ms step_avg:61.71ms
step:1679/2330 train_time:103617ms step_avg:61.71ms
step:1680/2330 train_time:103681ms step_avg:61.71ms
step:1681/2330 train_time:103742ms step_avg:61.71ms
step:1682/2330 train_time:103806ms step_avg:61.72ms
step:1683/2330 train_time:103866ms step_avg:61.71ms
step:1684/2330 train_time:103930ms step_avg:61.72ms
step:1685/2330 train_time:103992ms step_avg:61.72ms
step:1686/2330 train_time:104056ms step_avg:61.72ms
step:1687/2330 train_time:104117ms step_avg:61.72ms
step:1688/2330 train_time:104180ms step_avg:61.72ms
step:1689/2330 train_time:104241ms step_avg:61.72ms
step:1690/2330 train_time:104305ms step_avg:61.72ms
step:1691/2330 train_time:104365ms step_avg:61.72ms
step:1692/2330 train_time:104429ms step_avg:61.72ms
step:1693/2330 train_time:104491ms step_avg:61.72ms
step:1694/2330 train_time:104554ms step_avg:61.72ms
step:1695/2330 train_time:104615ms step_avg:61.72ms
step:1696/2330 train_time:104679ms step_avg:61.72ms
step:1697/2330 train_time:104740ms step_avg:61.72ms
step:1698/2330 train_time:104804ms step_avg:61.72ms
step:1699/2330 train_time:104865ms step_avg:61.72ms
step:1700/2330 train_time:104929ms step_avg:61.72ms
step:1701/2330 train_time:104990ms step_avg:61.72ms
step:1702/2330 train_time:105055ms step_avg:61.72ms
step:1703/2330 train_time:105115ms step_avg:61.72ms
step:1704/2330 train_time:105179ms step_avg:61.72ms
step:1705/2330 train_time:105239ms step_avg:61.72ms
step:1706/2330 train_time:105303ms step_avg:61.73ms
step:1707/2330 train_time:105364ms step_avg:61.72ms
step:1708/2330 train_time:105428ms step_avg:61.73ms
step:1709/2330 train_time:105489ms step_avg:61.73ms
step:1710/2330 train_time:105554ms step_avg:61.73ms
step:1711/2330 train_time:105615ms step_avg:61.73ms
step:1712/2330 train_time:105679ms step_avg:61.73ms
step:1713/2330 train_time:105740ms step_avg:61.73ms
step:1714/2330 train_time:105804ms step_avg:61.73ms
step:1715/2330 train_time:105865ms step_avg:61.73ms
step:1716/2330 train_time:105929ms step_avg:61.73ms
step:1717/2330 train_time:105990ms step_avg:61.73ms
step:1718/2330 train_time:106055ms step_avg:61.73ms
step:1719/2330 train_time:106116ms step_avg:61.73ms
step:1720/2330 train_time:106179ms step_avg:61.73ms
step:1721/2330 train_time:106241ms step_avg:61.73ms
step:1722/2330 train_time:106305ms step_avg:61.73ms
step:1723/2330 train_time:106366ms step_avg:61.73ms
step:1724/2330 train_time:106430ms step_avg:61.73ms
step:1725/2330 train_time:106491ms step_avg:61.73ms
step:1726/2330 train_time:106555ms step_avg:61.74ms
step:1727/2330 train_time:106616ms step_avg:61.73ms
step:1728/2330 train_time:106679ms step_avg:61.74ms
step:1729/2330 train_time:106740ms step_avg:61.74ms
step:1730/2330 train_time:106804ms step_avg:61.74ms
step:1731/2330 train_time:106864ms step_avg:61.74ms
step:1732/2330 train_time:106928ms step_avg:61.74ms
step:1733/2330 train_time:106989ms step_avg:61.74ms
step:1734/2330 train_time:107054ms step_avg:61.74ms
step:1735/2330 train_time:107115ms step_avg:61.74ms
step:1736/2330 train_time:107179ms step_avg:61.74ms
step:1737/2330 train_time:107240ms step_avg:61.74ms
step:1738/2330 train_time:107304ms step_avg:61.74ms
step:1739/2330 train_time:107365ms step_avg:61.74ms
step:1740/2330 train_time:107429ms step_avg:61.74ms
step:1741/2330 train_time:107490ms step_avg:61.74ms
step:1742/2330 train_time:107554ms step_avg:61.74ms
step:1743/2330 train_time:107615ms step_avg:61.74ms
step:1744/2330 train_time:107679ms step_avg:61.74ms
step:1745/2330 train_time:107740ms step_avg:61.74ms
step:1746/2330 train_time:107804ms step_avg:61.74ms
step:1747/2330 train_time:107865ms step_avg:61.74ms
step:1748/2330 train_time:107928ms step_avg:61.74ms
step:1749/2330 train_time:107988ms step_avg:61.74ms
step:1750/2330 train_time:108053ms step_avg:61.74ms
step:1750/2330 val_loss:3.4685 train_time:108119ms step_avg:61.78ms
step:1751/2330 train_time:108143ms step_avg:61.76ms
step:1752/2330 train_time:108181ms step_avg:61.75ms
step:1753/2330 train_time:108249ms step_avg:61.75ms
step:1754/2330 train_time:108314ms step_avg:61.75ms
step:1755/2330 train_time:108375ms step_avg:61.75ms
step:1756/2330 train_time:108439ms step_avg:61.75ms
step:1757/2330 train_time:108499ms step_avg:61.75ms
step:1758/2330 train_time:108563ms step_avg:61.75ms
step:1759/2330 train_time:108623ms step_avg:61.75ms
step:1760/2330 train_time:108686ms step_avg:61.75ms
step:1761/2330 train_time:108745ms step_avg:61.75ms
step:1762/2330 train_time:108808ms step_avg:61.75ms
step:1763/2330 train_time:108868ms step_avg:61.75ms
step:1764/2330 train_time:108930ms step_avg:61.75ms
step:1765/2330 train_time:108990ms step_avg:61.75ms
step:1766/2330 train_time:109053ms step_avg:61.75ms
step:1767/2330 train_time:109115ms step_avg:61.75ms
step:1768/2330 train_time:109181ms step_avg:61.75ms
step:1769/2330 train_time:109244ms step_avg:61.75ms
step:1770/2330 train_time:109308ms step_avg:61.76ms
step:1771/2330 train_time:109370ms step_avg:61.76ms
step:1772/2330 train_time:109433ms step_avg:61.76ms
step:1773/2330 train_time:109494ms step_avg:61.76ms
step:1774/2330 train_time:109558ms step_avg:61.76ms
step:1775/2330 train_time:109619ms step_avg:61.76ms
step:1776/2330 train_time:109683ms step_avg:61.76ms
step:1777/2330 train_time:109743ms step_avg:61.76ms
step:1778/2330 train_time:109806ms step_avg:61.76ms
step:1779/2330 train_time:109867ms step_avg:61.76ms
step:1780/2330 train_time:109930ms step_avg:61.76ms
step:1781/2330 train_time:109990ms step_avg:61.76ms
step:1782/2330 train_time:110053ms step_avg:61.76ms
step:1783/2330 train_time:110114ms step_avg:61.76ms
step:1784/2330 train_time:110178ms step_avg:61.76ms
step:1785/2330 train_time:110241ms step_avg:61.76ms
step:1786/2330 train_time:110305ms step_avg:61.76ms
step:1787/2330 train_time:110366ms step_avg:61.76ms
step:1788/2330 train_time:110430ms step_avg:61.76ms
step:1789/2330 train_time:110491ms step_avg:61.76ms
step:1790/2330 train_time:110554ms step_avg:61.76ms
step:1791/2330 train_time:110615ms step_avg:61.76ms
step:1792/2330 train_time:110679ms step_avg:61.76ms
step:1793/2330 train_time:110739ms step_avg:61.76ms
step:1794/2330 train_time:110804ms step_avg:61.76ms
step:1795/2330 train_time:110864ms step_avg:61.76ms
step:1796/2330 train_time:110927ms step_avg:61.76ms
step:1797/2330 train_time:110987ms step_avg:61.76ms
step:1798/2330 train_time:111051ms step_avg:61.76ms
step:1799/2330 train_time:111111ms step_avg:61.76ms
step:1800/2330 train_time:111175ms step_avg:61.76ms
step:1801/2330 train_time:111236ms step_avg:61.76ms
step:1802/2330 train_time:111302ms step_avg:61.77ms
step:1803/2330 train_time:111363ms step_avg:61.77ms
step:1804/2330 train_time:111427ms step_avg:61.77ms
step:1805/2330 train_time:111489ms step_avg:61.77ms
step:1806/2330 train_time:111552ms step_avg:61.77ms
step:1807/2330 train_time:111612ms step_avg:61.77ms
step:1808/2330 train_time:111676ms step_avg:61.77ms
step:1809/2330 train_time:111738ms step_avg:61.77ms
step:1810/2330 train_time:111802ms step_avg:61.77ms
step:1811/2330 train_time:111863ms step_avg:61.77ms
step:1812/2330 train_time:111926ms step_avg:61.77ms
step:1813/2330 train_time:111986ms step_avg:61.77ms
step:1814/2330 train_time:112050ms step_avg:61.77ms
step:1815/2330 train_time:112110ms step_avg:61.77ms
step:1816/2330 train_time:112173ms step_avg:61.77ms
step:1817/2330 train_time:112235ms step_avg:61.77ms
step:1818/2330 train_time:112299ms step_avg:61.77ms
step:1819/2330 train_time:112360ms step_avg:61.77ms
step:1820/2330 train_time:112425ms step_avg:61.77ms
step:1821/2330 train_time:112485ms step_avg:61.77ms
step:1822/2330 train_time:112549ms step_avg:61.77ms
step:1823/2330 train_time:112609ms step_avg:61.77ms
step:1824/2330 train_time:112673ms step_avg:61.77ms
step:1825/2330 train_time:112734ms step_avg:61.77ms
step:1826/2330 train_time:112797ms step_avg:61.77ms
step:1827/2330 train_time:112858ms step_avg:61.77ms
step:1828/2330 train_time:112922ms step_avg:61.77ms
step:1829/2330 train_time:112982ms step_avg:61.77ms
step:1830/2330 train_time:113046ms step_avg:61.77ms
step:1831/2330 train_time:113106ms step_avg:61.77ms
step:1832/2330 train_time:113170ms step_avg:61.77ms
step:1833/2330 train_time:113231ms step_avg:61.77ms
step:1834/2330 train_time:113294ms step_avg:61.77ms
step:1835/2330 train_time:113355ms step_avg:61.77ms
step:1836/2330 train_time:113421ms step_avg:61.78ms
step:1837/2330 train_time:113482ms step_avg:61.78ms
step:1838/2330 train_time:113546ms step_avg:61.78ms
step:1839/2330 train_time:113606ms step_avg:61.78ms
step:1840/2330 train_time:113670ms step_avg:61.78ms
step:1841/2330 train_time:113730ms step_avg:61.78ms
step:1842/2330 train_time:113794ms step_avg:61.78ms
step:1843/2330 train_time:113854ms step_avg:61.78ms
step:1844/2330 train_time:113919ms step_avg:61.78ms
step:1845/2330 train_time:113981ms step_avg:61.78ms
step:1846/2330 train_time:114045ms step_avg:61.78ms
step:1847/2330 train_time:114106ms step_avg:61.78ms
step:1848/2330 train_time:114170ms step_avg:61.78ms
step:1849/2330 train_time:114231ms step_avg:61.78ms
step:1850/2330 train_time:114295ms step_avg:61.78ms
step:1851/2330 train_time:114355ms step_avg:61.78ms
step:1852/2330 train_time:114420ms step_avg:61.78ms
step:1853/2330 train_time:114482ms step_avg:61.78ms
step:1854/2330 train_time:114546ms step_avg:61.78ms
step:1855/2330 train_time:114606ms step_avg:61.78ms
step:1856/2330 train_time:114670ms step_avg:61.78ms
step:1857/2330 train_time:114731ms step_avg:61.78ms
step:1858/2330 train_time:114794ms step_avg:61.78ms
step:1859/2330 train_time:114854ms step_avg:61.78ms
step:1860/2330 train_time:114918ms step_avg:61.78ms
step:1861/2330 train_time:114980ms step_avg:61.78ms
step:1862/2330 train_time:115043ms step_avg:61.78ms
step:1863/2330 train_time:115104ms step_avg:61.78ms
step:1864/2330 train_time:115168ms step_avg:61.79ms
step:1865/2330 train_time:115229ms step_avg:61.78ms
step:1866/2330 train_time:115292ms step_avg:61.79ms
step:1867/2330 train_time:115352ms step_avg:61.78ms
step:1868/2330 train_time:115416ms step_avg:61.79ms
step:1869/2330 train_time:115478ms step_avg:61.79ms
step:1870/2330 train_time:115541ms step_avg:61.79ms
step:1871/2330 train_time:115603ms step_avg:61.79ms
step:1872/2330 train_time:115666ms step_avg:61.79ms
step:1873/2330 train_time:115728ms step_avg:61.79ms
step:1874/2330 train_time:115791ms step_avg:61.79ms
step:1875/2330 train_time:115851ms step_avg:61.79ms
step:1876/2330 train_time:115915ms step_avg:61.79ms
step:1877/2330 train_time:115976ms step_avg:61.79ms
step:1878/2330 train_time:116041ms step_avg:61.79ms
step:1879/2330 train_time:116102ms step_avg:61.79ms
step:1880/2330 train_time:116166ms step_avg:61.79ms
step:1881/2330 train_time:116227ms step_avg:61.79ms
step:1882/2330 train_time:116291ms step_avg:61.79ms
step:1883/2330 train_time:116352ms step_avg:61.79ms
step:1884/2330 train_time:116416ms step_avg:61.79ms
step:1885/2330 train_time:116476ms step_avg:61.79ms
step:1886/2330 train_time:116540ms step_avg:61.79ms
step:1887/2330 train_time:116601ms step_avg:61.79ms
step:1888/2330 train_time:116665ms step_avg:61.79ms
step:1889/2330 train_time:116726ms step_avg:61.79ms
step:1890/2330 train_time:116789ms step_avg:61.79ms
step:1891/2330 train_time:116850ms step_avg:61.79ms
step:1892/2330 train_time:116913ms step_avg:61.79ms
step:1893/2330 train_time:116974ms step_avg:61.79ms
step:1894/2330 train_time:117038ms step_avg:61.79ms
step:1895/2330 train_time:117099ms step_avg:61.79ms
step:1896/2330 train_time:117163ms step_avg:61.79ms
step:1897/2330 train_time:117224ms step_avg:61.79ms
step:1898/2330 train_time:117287ms step_avg:61.80ms
step:1899/2330 train_time:117349ms step_avg:61.79ms
step:1900/2330 train_time:117412ms step_avg:61.80ms
step:1901/2330 train_time:117479ms step_avg:61.80ms
step:1902/2330 train_time:117536ms step_avg:61.80ms
step:1903/2330 train_time:117597ms step_avg:61.80ms
step:1904/2330 train_time:117661ms step_avg:61.80ms
step:1905/2330 train_time:117721ms step_avg:61.80ms
step:1906/2330 train_time:117785ms step_avg:61.80ms
step:1907/2330 train_time:117846ms step_avg:61.80ms
step:1908/2330 train_time:117910ms step_avg:61.80ms
step:1909/2330 train_time:117971ms step_avg:61.80ms
step:1910/2330 train_time:118034ms step_avg:61.80ms
step:1911/2330 train_time:118095ms step_avg:61.80ms
step:1912/2330 train_time:118159ms step_avg:61.80ms
step:1913/2330 train_time:118221ms step_avg:61.80ms
step:1914/2330 train_time:118284ms step_avg:61.80ms
step:1915/2330 train_time:118345ms step_avg:61.80ms
step:1916/2330 train_time:118408ms step_avg:61.80ms
step:1917/2330 train_time:118469ms step_avg:61.80ms
step:1918/2330 train_time:118532ms step_avg:61.80ms
step:1919/2330 train_time:118593ms step_avg:61.80ms
step:1920/2330 train_time:118656ms step_avg:61.80ms
step:1921/2330 train_time:118718ms step_avg:61.80ms
step:1922/2330 train_time:118782ms step_avg:61.80ms
step:1923/2330 train_time:118843ms step_avg:61.80ms
step:1924/2330 train_time:118907ms step_avg:61.80ms
step:1925/2330 train_time:118968ms step_avg:61.80ms
step:1926/2330 train_time:119033ms step_avg:61.80ms
step:1927/2330 train_time:119094ms step_avg:61.80ms
step:1928/2330 train_time:119158ms step_avg:61.80ms
step:1929/2330 train_time:119219ms step_avg:61.80ms
step:1930/2330 train_time:119283ms step_avg:61.80ms
step:1931/2330 train_time:119344ms step_avg:61.80ms
step:1932/2330 train_time:119407ms step_avg:61.81ms
step:1933/2330 train_time:119468ms step_avg:61.80ms
step:1934/2330 train_time:119532ms step_avg:61.81ms
step:1935/2330 train_time:119593ms step_avg:61.81ms
step:1936/2330 train_time:119656ms step_avg:61.81ms
step:1937/2330 train_time:119717ms step_avg:61.81ms
step:1938/2330 train_time:119781ms step_avg:61.81ms
step:1939/2330 train_time:119842ms step_avg:61.81ms
step:1940/2330 train_time:119906ms step_avg:61.81ms
step:1941/2330 train_time:119967ms step_avg:61.81ms
step:1942/2330 train_time:120031ms step_avg:61.81ms
step:1943/2330 train_time:120092ms step_avg:61.81ms
step:1944/2330 train_time:120155ms step_avg:61.81ms
step:1945/2330 train_time:120216ms step_avg:61.81ms
step:1946/2330 train_time:120280ms step_avg:61.81ms
step:1947/2330 train_time:120341ms step_avg:61.81ms
step:1948/2330 train_time:120405ms step_avg:61.81ms
step:1949/2330 train_time:120465ms step_avg:61.81ms
step:1950/2330 train_time:120529ms step_avg:61.81ms
step:1951/2330 train_time:120589ms step_avg:61.81ms
step:1952/2330 train_time:120653ms step_avg:61.81ms
step:1953/2330 train_time:120713ms step_avg:61.81ms
step:1954/2330 train_time:120778ms step_avg:61.81ms
step:1955/2330 train_time:120838ms step_avg:61.81ms
step:1956/2330 train_time:120902ms step_avg:61.81ms
step:1957/2330 train_time:120963ms step_avg:61.81ms
step:1958/2330 train_time:121027ms step_avg:61.81ms
step:1959/2330 train_time:121088ms step_avg:61.81ms
step:1960/2330 train_time:121152ms step_avg:61.81ms
step:1961/2330 train_time:121212ms step_avg:61.81ms
step:1962/2330 train_time:121276ms step_avg:61.81ms
step:1963/2330 train_time:121336ms step_avg:61.81ms
step:1964/2330 train_time:121401ms step_avg:61.81ms
step:1965/2330 train_time:121462ms step_avg:61.81ms
step:1966/2330 train_time:121527ms step_avg:61.81ms
step:1967/2330 train_time:121587ms step_avg:61.81ms
step:1968/2330 train_time:121650ms step_avg:61.81ms
step:1969/2330 train_time:121711ms step_avg:61.81ms
step:1970/2330 train_time:121774ms step_avg:61.81ms
step:1971/2330 train_time:121835ms step_avg:61.81ms
step:1972/2330 train_time:121900ms step_avg:61.82ms
step:1973/2330 train_time:121961ms step_avg:61.82ms
step:1974/2330 train_time:122026ms step_avg:61.82ms
step:1975/2330 train_time:122086ms step_avg:61.82ms
step:1976/2330 train_time:122150ms step_avg:61.82ms
step:1977/2330 train_time:122210ms step_avg:61.82ms
step:1978/2330 train_time:122274ms step_avg:61.82ms
step:1979/2330 train_time:122334ms step_avg:61.82ms
step:1980/2330 train_time:122398ms step_avg:61.82ms
step:1981/2330 train_time:122460ms step_avg:61.82ms
step:1982/2330 train_time:122524ms step_avg:61.82ms
step:1983/2330 train_time:122584ms step_avg:61.82ms
step:1984/2330 train_time:122648ms step_avg:61.82ms
step:1985/2330 train_time:122708ms step_avg:61.82ms
step:1986/2330 train_time:122772ms step_avg:61.82ms
step:1987/2330 train_time:122832ms step_avg:61.82ms
step:1988/2330 train_time:122896ms step_avg:61.82ms
step:1989/2330 train_time:122957ms step_avg:61.82ms
step:1990/2330 train_time:123021ms step_avg:61.82ms
step:1991/2330 train_time:123082ms step_avg:61.82ms
step:1992/2330 train_time:123146ms step_avg:61.82ms
step:1993/2330 train_time:123206ms step_avg:61.82ms
step:1994/2330 train_time:123270ms step_avg:61.82ms
step:1995/2330 train_time:123330ms step_avg:61.82ms
step:1996/2330 train_time:123394ms step_avg:61.82ms
step:1997/2330 train_time:123455ms step_avg:61.82ms
step:1998/2330 train_time:123519ms step_avg:61.82ms
step:1999/2330 train_time:123580ms step_avg:61.82ms
step:2000/2330 train_time:123645ms step_avg:61.82ms
step:2000/2330 val_loss:3.4406 train_time:123709ms step_avg:61.85ms
step:2001/2330 train_time:123734ms step_avg:61.84ms
step:2002/2330 train_time:123773ms step_avg:61.82ms
step:2003/2330 train_time:123840ms step_avg:61.83ms
step:2004/2330 train_time:123907ms step_avg:61.83ms
step:2005/2330 train_time:123968ms step_avg:61.83ms
step:2006/2330 train_time:124032ms step_avg:61.83ms
step:2007/2330 train_time:124093ms step_avg:61.83ms
step:2008/2330 train_time:124156ms step_avg:61.83ms
step:2009/2330 train_time:124216ms step_avg:61.83ms
step:2010/2330 train_time:124279ms step_avg:61.83ms
step:2011/2330 train_time:124338ms step_avg:61.83ms
step:2012/2330 train_time:124401ms step_avg:61.83ms
step:2013/2330 train_time:124461ms step_avg:61.83ms
step:2014/2330 train_time:124523ms step_avg:61.83ms
step:2015/2330 train_time:124583ms step_avg:61.83ms
step:2016/2330 train_time:124646ms step_avg:61.83ms
step:2017/2330 train_time:124708ms step_avg:61.83ms
step:2018/2330 train_time:124773ms step_avg:61.83ms
step:2019/2330 train_time:124837ms step_avg:61.83ms
step:2020/2330 train_time:124902ms step_avg:61.83ms
step:2021/2330 train_time:124963ms step_avg:61.83ms
step:2022/2330 train_time:125027ms step_avg:61.83ms
step:2023/2330 train_time:125087ms step_avg:61.83ms
step:2024/2330 train_time:125151ms step_avg:61.83ms
step:2025/2330 train_time:125211ms step_avg:61.83ms
step:2026/2330 train_time:125275ms step_avg:61.83ms
step:2027/2330 train_time:125336ms step_avg:61.83ms
step:2028/2330 train_time:125399ms step_avg:61.83ms
step:2029/2330 train_time:125459ms step_avg:61.83ms
step:2030/2330 train_time:125522ms step_avg:61.83ms
step:2031/2330 train_time:125581ms step_avg:61.83ms
step:2032/2330 train_time:125645ms step_avg:61.83ms
step:2033/2330 train_time:125705ms step_avg:61.83ms
step:2034/2330 train_time:125769ms step_avg:61.83ms
step:2035/2330 train_time:125831ms step_avg:61.83ms
step:2036/2330 train_time:125896ms step_avg:61.83ms
step:2037/2330 train_time:125958ms step_avg:61.83ms
step:2038/2330 train_time:126022ms step_avg:61.84ms
step:2039/2330 train_time:126083ms step_avg:61.84ms
step:2040/2330 train_time:126148ms step_avg:61.84ms
step:2041/2330 train_time:126208ms step_avg:61.84ms
step:2042/2330 train_time:126271ms step_avg:61.84ms
step:2043/2330 train_time:126332ms step_avg:61.84ms
step:2044/2330 train_time:126397ms step_avg:61.84ms
step:2045/2330 train_time:126457ms step_avg:61.84ms
step:2046/2330 train_time:126521ms step_avg:61.84ms
step:2047/2330 train_time:126581ms step_avg:61.84ms
step:2048/2330 train_time:126644ms step_avg:61.84ms
step:2049/2330 train_time:126705ms step_avg:61.84ms
step:2050/2330 train_time:126768ms step_avg:61.84ms
step:2051/2330 train_time:126829ms step_avg:61.84ms
step:2052/2330 train_time:126894ms step_avg:61.84ms
step:2053/2330 train_time:126955ms step_avg:61.84ms
step:2054/2330 train_time:127020ms step_avg:61.84ms
step:2055/2330 train_time:127080ms step_avg:61.84ms
step:2056/2330 train_time:127144ms step_avg:61.84ms
step:2057/2330 train_time:127205ms step_avg:61.84ms
step:2058/2330 train_time:127268ms step_avg:61.84ms
step:2059/2330 train_time:127329ms step_avg:61.84ms
step:2060/2330 train_time:127393ms step_avg:61.84ms
step:2061/2330 train_time:127455ms step_avg:61.84ms
step:2062/2330 train_time:127518ms step_avg:61.84ms
step:2063/2330 train_time:127579ms step_avg:61.84ms
step:2064/2330 train_time:127642ms step_avg:61.84ms
step:2065/2330 train_time:127702ms step_avg:61.84ms
step:2066/2330 train_time:127766ms step_avg:61.84ms
step:2067/2330 train_time:127827ms step_avg:61.84ms
step:2068/2330 train_time:127890ms step_avg:61.84ms
step:2069/2330 train_time:127951ms step_avg:61.84ms
step:2070/2330 train_time:128016ms step_avg:61.84ms
step:2071/2330 train_time:128078ms step_avg:61.84ms
step:2072/2330 train_time:128142ms step_avg:61.84ms
step:2073/2330 train_time:128203ms step_avg:61.84ms
step:2074/2330 train_time:128266ms step_avg:61.84ms
step:2075/2330 train_time:128327ms step_avg:61.84ms
step:2076/2330 train_time:128391ms step_avg:61.85ms
step:2077/2330 train_time:128452ms step_avg:61.84ms
step:2078/2330 train_time:128516ms step_avg:61.85ms
step:2079/2330 train_time:128577ms step_avg:61.85ms
step:2080/2330 train_time:128641ms step_avg:61.85ms
step:2081/2330 train_time:128701ms step_avg:61.85ms
step:2082/2330 train_time:128765ms step_avg:61.85ms
step:2083/2330 train_time:128825ms step_avg:61.85ms
step:2084/2330 train_time:128888ms step_avg:61.85ms
step:2085/2330 train_time:128950ms step_avg:61.85ms
step:2086/2330 train_time:129014ms step_avg:61.85ms
step:2087/2330 train_time:129075ms step_avg:61.85ms
step:2088/2330 train_time:129139ms step_avg:61.85ms
step:2089/2330 train_time:129200ms step_avg:61.85ms
step:2090/2330 train_time:129264ms step_avg:61.85ms
step:2091/2330 train_time:129324ms step_avg:61.85ms
step:2092/2330 train_time:129388ms step_avg:61.85ms
step:2093/2330 train_time:129449ms step_avg:61.85ms
step:2094/2330 train_time:129513ms step_avg:61.85ms
step:2095/2330 train_time:129574ms step_avg:61.85ms
step:2096/2330 train_time:129639ms step_avg:61.85ms
step:2097/2330 train_time:129699ms step_avg:61.85ms
step:2098/2330 train_time:129763ms step_avg:61.85ms
step:2099/2330 train_time:129824ms step_avg:61.85ms
step:2100/2330 train_time:129887ms step_avg:61.85ms
step:2101/2330 train_time:129947ms step_avg:61.85ms
step:2102/2330 train_time:130011ms step_avg:61.85ms
step:2103/2330 train_time:130072ms step_avg:61.85ms
step:2104/2330 train_time:130137ms step_avg:61.85ms
step:2105/2330 train_time:130198ms step_avg:61.85ms
step:2106/2330 train_time:130262ms step_avg:61.85ms
step:2107/2330 train_time:130323ms step_avg:61.85ms
step:2108/2330 train_time:130387ms step_avg:61.85ms
step:2109/2330 train_time:130448ms step_avg:61.85ms
step:2110/2330 train_time:130511ms step_avg:61.85ms
step:2111/2330 train_time:130572ms step_avg:61.85ms
step:2112/2330 train_time:130637ms step_avg:61.85ms
step:2113/2330 train_time:130697ms step_avg:61.85ms
step:2114/2330 train_time:130762ms step_avg:61.86ms
step:2115/2330 train_time:130822ms step_avg:61.85ms
step:2116/2330 train_time:130885ms step_avg:61.86ms
step:2117/2330 train_time:130946ms step_avg:61.85ms
step:2118/2330 train_time:131010ms step_avg:61.86ms
step:2119/2330 train_time:131071ms step_avg:61.86ms
step:2120/2330 train_time:131136ms step_avg:61.86ms
step:2121/2330 train_time:131196ms step_avg:61.86ms
step:2122/2330 train_time:131260ms step_avg:61.86ms
step:2123/2330 train_time:131321ms step_avg:61.86ms
step:2124/2330 train_time:131386ms step_avg:61.86ms
step:2125/2330 train_time:131447ms step_avg:61.86ms
step:2126/2330 train_time:131511ms step_avg:61.86ms
step:2127/2330 train_time:131572ms step_avg:61.86ms
step:2128/2330 train_time:131636ms step_avg:61.86ms
step:2129/2330 train_time:131698ms step_avg:61.86ms
step:2130/2330 train_time:131761ms step_avg:61.86ms
step:2131/2330 train_time:131821ms step_avg:61.86ms
step:2132/2330 train_time:131885ms step_avg:61.86ms
step:2133/2330 train_time:131945ms step_avg:61.86ms
step:2134/2330 train_time:132010ms step_avg:61.86ms
step:2135/2330 train_time:132070ms step_avg:61.86ms
step:2136/2330 train_time:132134ms step_avg:61.86ms
step:2137/2330 train_time:132196ms step_avg:61.86ms
step:2138/2330 train_time:132260ms step_avg:61.86ms
step:2139/2330 train_time:132321ms step_avg:61.86ms
step:2140/2330 train_time:132384ms step_avg:61.86ms
step:2141/2330 train_time:132446ms step_avg:61.86ms
step:2142/2330 train_time:132509ms step_avg:61.86ms
step:2143/2330 train_time:132570ms step_avg:61.86ms
step:2144/2330 train_time:132634ms step_avg:61.86ms
step:2145/2330 train_time:132695ms step_avg:61.86ms
step:2146/2330 train_time:132759ms step_avg:61.86ms
step:2147/2330 train_time:132820ms step_avg:61.86ms
step:2148/2330 train_time:132884ms step_avg:61.86ms
step:2149/2330 train_time:132944ms step_avg:61.86ms
step:2150/2330 train_time:133009ms step_avg:61.86ms
step:2151/2330 train_time:133069ms step_avg:61.86ms
step:2152/2330 train_time:133132ms step_avg:61.86ms
step:2153/2330 train_time:133194ms step_avg:61.86ms
step:2154/2330 train_time:133258ms step_avg:61.87ms
step:2155/2330 train_time:133319ms step_avg:61.87ms
step:2156/2330 train_time:133383ms step_avg:61.87ms
step:2157/2330 train_time:133444ms step_avg:61.87ms
step:2158/2330 train_time:133508ms step_avg:61.87ms
step:2159/2330 train_time:133569ms step_avg:61.87ms
step:2160/2330 train_time:133632ms step_avg:61.87ms
step:2161/2330 train_time:133693ms step_avg:61.87ms
step:2162/2330 train_time:133757ms step_avg:61.87ms
step:2163/2330 train_time:133819ms step_avg:61.87ms
step:2164/2330 train_time:133882ms step_avg:61.87ms
step:2165/2330 train_time:133943ms step_avg:61.87ms
step:2166/2330 train_time:134007ms step_avg:61.87ms
step:2167/2330 train_time:134067ms step_avg:61.87ms
step:2168/2330 train_time:134131ms step_avg:61.87ms
step:2169/2330 train_time:134192ms step_avg:61.87ms
step:2170/2330 train_time:134257ms step_avg:61.87ms
step:2171/2330 train_time:134318ms step_avg:61.87ms
step:2172/2330 train_time:134383ms step_avg:61.87ms
step:2173/2330 train_time:134444ms step_avg:61.87ms
step:2174/2330 train_time:134507ms step_avg:61.87ms
step:2175/2330 train_time:134568ms step_avg:61.87ms
step:2176/2330 train_time:134631ms step_avg:61.87ms
step:2177/2330 train_time:134692ms step_avg:61.87ms
step:2178/2330 train_time:134756ms step_avg:61.87ms
step:2179/2330 train_time:134817ms step_avg:61.87ms
step:2180/2330 train_time:134881ms step_avg:61.87ms
step:2181/2330 train_time:134942ms step_avg:61.87ms
step:2182/2330 train_time:135006ms step_avg:61.87ms
step:2183/2330 train_time:135067ms step_avg:61.87ms
step:2184/2330 train_time:135130ms step_avg:61.87ms
step:2185/2330 train_time:135191ms step_avg:61.87ms
step:2186/2330 train_time:135255ms step_avg:61.87ms
step:2187/2330 train_time:135317ms step_avg:61.87ms
step:2188/2330 train_time:135380ms step_avg:61.87ms
step:2189/2330 train_time:135441ms step_avg:61.87ms
step:2190/2330 train_time:135505ms step_avg:61.87ms
step:2191/2330 train_time:135566ms step_avg:61.87ms
step:2192/2330 train_time:135629ms step_avg:61.87ms
step:2193/2330 train_time:135690ms step_avg:61.87ms
step:2194/2330 train_time:135754ms step_avg:61.88ms
step:2195/2330 train_time:135815ms step_avg:61.87ms
step:2196/2330 train_time:135880ms step_avg:61.88ms
step:2197/2330 train_time:135941ms step_avg:61.88ms
step:2198/2330 train_time:136004ms step_avg:61.88ms
step:2199/2330 train_time:136065ms step_avg:61.88ms
step:2200/2330 train_time:136129ms step_avg:61.88ms
step:2201/2330 train_time:136190ms step_avg:61.88ms
step:2202/2330 train_time:136253ms step_avg:61.88ms
step:2203/2330 train_time:136314ms step_avg:61.88ms
step:2204/2330 train_time:136378ms step_avg:61.88ms
step:2205/2330 train_time:136440ms step_avg:61.88ms
step:2206/2330 train_time:136503ms step_avg:61.88ms
step:2207/2330 train_time:136564ms step_avg:61.88ms
step:2208/2330 train_time:136627ms step_avg:61.88ms
step:2209/2330 train_time:136687ms step_avg:61.88ms
step:2210/2330 train_time:136751ms step_avg:61.88ms
step:2211/2330 train_time:136812ms step_avg:61.88ms
step:2212/2330 train_time:136876ms step_avg:61.88ms
step:2213/2330 train_time:136937ms step_avg:61.88ms
step:2214/2330 train_time:137001ms step_avg:61.88ms
step:2215/2330 train_time:137062ms step_avg:61.88ms
step:2216/2330 train_time:137125ms step_avg:61.88ms
step:2217/2330 train_time:137186ms step_avg:61.88ms
step:2218/2330 train_time:137249ms step_avg:61.88ms
step:2219/2330 train_time:137310ms step_avg:61.88ms
step:2220/2330 train_time:137375ms step_avg:61.88ms
step:2221/2330 train_time:137437ms step_avg:61.88ms
step:2222/2330 train_time:137500ms step_avg:61.88ms
step:2223/2330 train_time:137561ms step_avg:61.88ms
step:2224/2330 train_time:137625ms step_avg:61.88ms
step:2225/2330 train_time:137686ms step_avg:61.88ms
step:2226/2330 train_time:137751ms step_avg:61.88ms
step:2227/2330 train_time:137811ms step_avg:61.88ms
step:2228/2330 train_time:137876ms step_avg:61.88ms
step:2229/2330 train_time:137938ms step_avg:61.88ms
step:2230/2330 train_time:138001ms step_avg:61.88ms
step:2231/2330 train_time:138061ms step_avg:61.88ms
step:2232/2330 train_time:138125ms step_avg:61.88ms
step:2233/2330 train_time:138187ms step_avg:61.88ms
step:2234/2330 train_time:138251ms step_avg:61.88ms
step:2235/2330 train_time:138311ms step_avg:61.88ms
step:2236/2330 train_time:138376ms step_avg:61.89ms
step:2237/2330 train_time:138437ms step_avg:61.89ms
step:2238/2330 train_time:138501ms step_avg:61.89ms
step:2239/2330 train_time:138561ms step_avg:61.89ms
step:2240/2330 train_time:138625ms step_avg:61.89ms
step:2241/2330 train_time:138686ms step_avg:61.89ms
step:2242/2330 train_time:138749ms step_avg:61.89ms
step:2243/2330 train_time:138810ms step_avg:61.89ms
step:2244/2330 train_time:138874ms step_avg:61.89ms
step:2245/2330 train_time:138935ms step_avg:61.89ms
step:2246/2330 train_time:138999ms step_avg:61.89ms
step:2247/2330 train_time:139059ms step_avg:61.89ms
step:2248/2330 train_time:139123ms step_avg:61.89ms
step:2249/2330 train_time:139184ms step_avg:61.89ms
step:2250/2330 train_time:139248ms step_avg:61.89ms
step:2250/2330 val_loss:3.4153 train_time:139313ms step_avg:61.92ms
step:2251/2330 train_time:139336ms step_avg:61.90ms
step:2252/2330 train_time:139374ms step_avg:61.89ms
step:2253/2330 train_time:139440ms step_avg:61.89ms
step:2254/2330 train_time:139504ms step_avg:61.89ms
step:2255/2330 train_time:139565ms step_avg:61.89ms
step:2256/2330 train_time:139629ms step_avg:61.89ms
step:2257/2330 train_time:139688ms step_avg:61.89ms
step:2258/2330 train_time:139752ms step_avg:61.89ms
step:2259/2330 train_time:139812ms step_avg:61.89ms
step:2260/2330 train_time:139875ms step_avg:61.89ms
step:2261/2330 train_time:139935ms step_avg:61.89ms
step:2262/2330 train_time:139998ms step_avg:61.89ms
step:2263/2330 train_time:140058ms step_avg:61.89ms
step:2264/2330 train_time:140121ms step_avg:61.89ms
step:2265/2330 train_time:140181ms step_avg:61.89ms
step:2266/2330 train_time:140245ms step_avg:61.89ms
step:2267/2330 train_time:140307ms step_avg:61.89ms
step:2268/2330 train_time:140373ms step_avg:61.89ms
step:2269/2330 train_time:140435ms step_avg:61.89ms
step:2270/2330 train_time:140500ms step_avg:61.89ms
step:2271/2330 train_time:140562ms step_avg:61.89ms
step:2272/2330 train_time:140626ms step_avg:61.90ms
step:2273/2330 train_time:140686ms step_avg:61.89ms
step:2274/2330 train_time:140750ms step_avg:61.90ms
step:2275/2330 train_time:140809ms step_avg:61.89ms
step:2276/2330 train_time:140873ms step_avg:61.89ms
step:2277/2330 train_time:140933ms step_avg:61.89ms
step:2278/2330 train_time:140996ms step_avg:61.89ms
step:2279/2330 train_time:141056ms step_avg:61.89ms
step:2280/2330 train_time:141119ms step_avg:61.89ms
step:2281/2330 train_time:141179ms step_avg:61.89ms
step:2282/2330 train_time:141243ms step_avg:61.89ms
step:2283/2330 train_time:141306ms step_avg:61.89ms
step:2284/2330 train_time:141370ms step_avg:61.90ms
step:2285/2330 train_time:141432ms step_avg:61.90ms
step:2286/2330 train_time:141496ms step_avg:61.90ms
step:2287/2330 train_time:141558ms step_avg:61.90ms
step:2288/2330 train_time:141623ms step_avg:61.90ms
step:2289/2330 train_time:141683ms step_avg:61.90ms
step:2290/2330 train_time:141747ms step_avg:61.90ms
step:2291/2330 train_time:141808ms step_avg:61.90ms
step:2292/2330 train_time:141871ms step_avg:61.90ms
step:2293/2330 train_time:141932ms step_avg:61.90ms
step:2294/2330 train_time:141996ms step_avg:61.90ms
step:2295/2330 train_time:142056ms step_avg:61.90ms
step:2296/2330 train_time:142119ms step_avg:61.90ms
step:2297/2330 train_time:142179ms step_avg:61.90ms
step:2298/2330 train_time:142243ms step_avg:61.90ms
step:2299/2330 train_time:142304ms step_avg:61.90ms
step:2300/2330 train_time:142368ms step_avg:61.90ms
step:2301/2330 train_time:142430ms step_avg:61.90ms
step:2302/2330 train_time:142494ms step_avg:61.90ms
step:2303/2330 train_time:142555ms step_avg:61.90ms
step:2304/2330 train_time:142619ms step_avg:61.90ms
step:2305/2330 train_time:142680ms step_avg:61.90ms
step:2306/2330 train_time:142743ms step_avg:61.90ms
step:2307/2330 train_time:142804ms step_avg:61.90ms
step:2308/2330 train_time:142867ms step_avg:61.90ms
step:2309/2330 train_time:142928ms step_avg:61.90ms
step:2310/2330 train_time:142992ms step_avg:61.90ms
step:2311/2330 train_time:143053ms step_avg:61.90ms
step:2312/2330 train_time:143116ms step_avg:61.90ms
step:2313/2330 train_time:143176ms step_avg:61.90ms
step:2314/2330 train_time:143240ms step_avg:61.90ms
step:2315/2330 train_time:143301ms step_avg:61.90ms
step:2316/2330 train_time:143365ms step_avg:61.90ms
step:2317/2330 train_time:143426ms step_avg:61.90ms
step:2318/2330 train_time:143490ms step_avg:61.90ms
step:2319/2330 train_time:143552ms step_avg:61.90ms
step:2320/2330 train_time:143616ms step_avg:61.90ms
step:2321/2330 train_time:143678ms step_avg:61.90ms
step:2322/2330 train_time:143742ms step_avg:61.90ms
step:2323/2330 train_time:143803ms step_avg:61.90ms
step:2324/2330 train_time:143866ms step_avg:61.90ms
step:2325/2330 train_time:143927ms step_avg:61.90ms
step:2326/2330 train_time:143991ms step_avg:61.90ms
step:2327/2330 train_time:144052ms step_avg:61.90ms
step:2328/2330 train_time:144115ms step_avg:61.91ms
step:2329/2330 train_time:144176ms step_avg:61.90ms
step:2330/2330 train_time:144240ms step_avg:61.91ms
step:2330/2330 val_loss:3.3904 train_time:144305ms step_avg:61.93ms
peak memory allocated: 29750 MiB reserved: 44076 MiB
