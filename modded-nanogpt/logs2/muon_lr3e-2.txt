import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:27:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:89ms step_avg:89.15ms
step:2/2330 train_time:186ms step_avg:92.83ms
step:3/2330 train_time:209ms step_avg:69.57ms
step:4/2330 train_time:243ms step_avg:60.80ms
step:5/2330 train_time:301ms step_avg:60.12ms
step:6/2330 train_time:362ms step_avg:60.41ms
step:7/2330 train_time:421ms step_avg:60.14ms
step:8/2330 train_time:482ms step_avg:60.31ms
step:9/2330 train_time:541ms step_avg:60.08ms
step:10/2330 train_time:603ms step_avg:60.28ms
step:11/2330 train_time:661ms step_avg:60.12ms
step:12/2330 train_time:724ms step_avg:60.30ms
step:13/2330 train_time:782ms step_avg:60.17ms
step:14/2330 train_time:844ms step_avg:60.30ms
step:15/2330 train_time:904ms step_avg:60.25ms
step:16/2330 train_time:966ms step_avg:60.40ms
step:17/2330 train_time:1029ms step_avg:60.52ms
step:18/2330 train_time:1094ms step_avg:60.80ms
step:19/2330 train_time:1158ms step_avg:60.95ms
step:20/2330 train_time:1222ms step_avg:61.08ms
step:21/2330 train_time:1281ms step_avg:61.02ms
step:22/2330 train_time:1344ms step_avg:61.08ms
step:23/2330 train_time:1402ms step_avg:60.98ms
step:24/2330 train_time:1465ms step_avg:61.04ms
step:25/2330 train_time:1524ms step_avg:60.95ms
step:26/2330 train_time:1586ms step_avg:61.01ms
step:27/2330 train_time:1645ms step_avg:60.92ms
step:28/2330 train_time:1707ms step_avg:60.98ms
step:29/2330 train_time:1767ms step_avg:60.91ms
step:30/2330 train_time:1828ms step_avg:60.95ms
step:31/2330 train_time:1887ms step_avg:60.88ms
step:32/2330 train_time:1950ms step_avg:60.94ms
step:33/2330 train_time:2011ms step_avg:60.93ms
step:34/2330 train_time:2075ms step_avg:61.02ms
step:35/2330 train_time:2136ms step_avg:61.02ms
step:36/2330 train_time:2199ms step_avg:61.08ms
step:37/2330 train_time:2259ms step_avg:61.04ms
step:38/2330 train_time:2321ms step_avg:61.08ms
step:39/2330 train_time:2381ms step_avg:61.05ms
step:40/2330 train_time:2444ms step_avg:61.09ms
step:41/2330 train_time:2503ms step_avg:61.04ms
step:42/2330 train_time:2565ms step_avg:61.06ms
step:43/2330 train_time:2625ms step_avg:61.04ms
step:44/2330 train_time:2687ms step_avg:61.07ms
step:45/2330 train_time:2745ms step_avg:61.01ms
step:46/2330 train_time:2807ms step_avg:61.03ms
step:47/2330 train_time:2866ms step_avg:60.98ms
step:48/2330 train_time:2929ms step_avg:61.02ms
step:49/2330 train_time:2989ms step_avg:60.99ms
step:50/2330 train_time:3052ms step_avg:61.04ms
step:51/2330 train_time:3112ms step_avg:61.03ms
step:52/2330 train_time:3176ms step_avg:61.08ms
step:53/2330 train_time:3235ms step_avg:61.04ms
step:54/2330 train_time:3298ms step_avg:61.07ms
step:55/2330 train_time:3357ms step_avg:61.04ms
step:56/2330 train_time:3419ms step_avg:61.06ms
step:57/2330 train_time:3479ms step_avg:61.03ms
step:58/2330 train_time:3541ms step_avg:61.05ms
step:59/2330 train_time:3600ms step_avg:61.02ms
step:60/2330 train_time:3663ms step_avg:61.04ms
step:61/2330 train_time:3722ms step_avg:61.02ms
step:62/2330 train_time:3784ms step_avg:61.03ms
step:63/2330 train_time:3843ms step_avg:61.00ms
step:64/2330 train_time:3906ms step_avg:61.03ms
step:65/2330 train_time:3967ms step_avg:61.03ms
step:66/2330 train_time:4030ms step_avg:61.07ms
step:67/2330 train_time:4090ms step_avg:61.05ms
step:68/2330 train_time:4153ms step_avg:61.08ms
step:69/2330 train_time:4212ms step_avg:61.05ms
step:70/2330 train_time:4275ms step_avg:61.07ms
step:71/2330 train_time:4334ms step_avg:61.04ms
step:72/2330 train_time:4396ms step_avg:61.05ms
step:73/2330 train_time:4455ms step_avg:61.03ms
step:74/2330 train_time:4518ms step_avg:61.05ms
step:75/2330 train_time:4578ms step_avg:61.04ms
step:76/2330 train_time:4640ms step_avg:61.05ms
step:77/2330 train_time:4699ms step_avg:61.03ms
step:78/2330 train_time:4761ms step_avg:61.04ms
step:79/2330 train_time:4820ms step_avg:61.02ms
step:80/2330 train_time:4883ms step_avg:61.03ms
step:81/2330 train_time:4943ms step_avg:61.02ms
step:82/2330 train_time:5006ms step_avg:61.04ms
step:83/2330 train_time:5067ms step_avg:61.04ms
step:84/2330 train_time:5130ms step_avg:61.07ms
step:85/2330 train_time:5190ms step_avg:61.05ms
step:86/2330 train_time:5252ms step_avg:61.07ms
step:87/2330 train_time:5312ms step_avg:61.05ms
step:88/2330 train_time:5374ms step_avg:61.07ms
step:89/2330 train_time:5433ms step_avg:61.05ms
step:90/2330 train_time:5496ms step_avg:61.07ms
step:91/2330 train_time:5555ms step_avg:61.05ms
step:92/2330 train_time:5617ms step_avg:61.06ms
step:93/2330 train_time:5677ms step_avg:61.04ms
step:94/2330 train_time:5739ms step_avg:61.05ms
step:95/2330 train_time:5798ms step_avg:61.03ms
step:96/2330 train_time:5860ms step_avg:61.04ms
step:97/2330 train_time:5920ms step_avg:61.03ms
step:98/2330 train_time:5982ms step_avg:61.04ms
step:99/2330 train_time:6042ms step_avg:61.03ms
step:100/2330 train_time:6105ms step_avg:61.05ms
step:101/2330 train_time:6165ms step_avg:61.04ms
step:102/2330 train_time:6228ms step_avg:61.06ms
step:103/2330 train_time:6288ms step_avg:61.05ms
step:104/2330 train_time:6351ms step_avg:61.07ms
step:105/2330 train_time:6411ms step_avg:61.06ms
step:106/2330 train_time:6474ms step_avg:61.07ms
step:107/2330 train_time:6533ms step_avg:61.06ms
step:108/2330 train_time:6595ms step_avg:61.06ms
step:109/2330 train_time:6654ms step_avg:61.04ms
step:110/2330 train_time:6716ms step_avg:61.05ms
step:111/2330 train_time:6775ms step_avg:61.04ms
step:112/2330 train_time:6837ms step_avg:61.05ms
step:113/2330 train_time:6897ms step_avg:61.03ms
step:114/2330 train_time:6959ms step_avg:61.04ms
step:115/2330 train_time:7019ms step_avg:61.03ms
step:116/2330 train_time:7081ms step_avg:61.04ms
step:117/2330 train_time:7140ms step_avg:61.03ms
step:118/2330 train_time:7202ms step_avg:61.04ms
step:119/2330 train_time:7263ms step_avg:61.03ms
step:120/2330 train_time:7326ms step_avg:61.05ms
step:121/2330 train_time:7387ms step_avg:61.05ms
step:122/2330 train_time:7450ms step_avg:61.06ms
step:123/2330 train_time:7509ms step_avg:61.05ms
step:124/2330 train_time:7572ms step_avg:61.07ms
step:125/2330 train_time:7631ms step_avg:61.05ms
step:126/2330 train_time:7693ms step_avg:61.06ms
step:127/2330 train_time:7752ms step_avg:61.04ms
step:128/2330 train_time:7814ms step_avg:61.04ms
step:129/2330 train_time:7873ms step_avg:61.03ms
step:130/2330 train_time:7935ms step_avg:61.04ms
step:131/2330 train_time:7994ms step_avg:61.02ms
step:132/2330 train_time:8057ms step_avg:61.03ms
step:133/2330 train_time:8117ms step_avg:61.03ms
step:134/2330 train_time:8180ms step_avg:61.04ms
step:135/2330 train_time:8239ms step_avg:61.03ms
step:136/2330 train_time:8301ms step_avg:61.04ms
step:137/2330 train_time:8361ms step_avg:61.03ms
step:138/2330 train_time:8423ms step_avg:61.04ms
step:139/2330 train_time:8484ms step_avg:61.03ms
step:140/2330 train_time:8546ms step_avg:61.04ms
step:141/2330 train_time:8606ms step_avg:61.03ms
step:142/2330 train_time:8668ms step_avg:61.04ms
step:143/2330 train_time:8728ms step_avg:61.03ms
step:144/2330 train_time:8791ms step_avg:61.05ms
step:145/2330 train_time:8850ms step_avg:61.04ms
step:146/2330 train_time:8913ms step_avg:61.05ms
step:147/2330 train_time:8972ms step_avg:61.03ms
step:148/2330 train_time:9034ms step_avg:61.04ms
step:149/2330 train_time:9093ms step_avg:61.03ms
step:150/2330 train_time:9156ms step_avg:61.04ms
step:151/2330 train_time:9215ms step_avg:61.03ms
step:152/2330 train_time:9278ms step_avg:61.04ms
step:153/2330 train_time:9338ms step_avg:61.03ms
step:154/2330 train_time:9401ms step_avg:61.04ms
step:155/2330 train_time:9460ms step_avg:61.03ms
step:156/2330 train_time:9522ms step_avg:61.04ms
step:157/2330 train_time:9581ms step_avg:61.03ms
step:158/2330 train_time:9644ms step_avg:61.03ms
step:159/2330 train_time:9703ms step_avg:61.03ms
step:160/2330 train_time:9766ms step_avg:61.04ms
step:161/2330 train_time:9826ms step_avg:61.03ms
step:162/2330 train_time:9889ms step_avg:61.04ms
step:163/2330 train_time:9949ms step_avg:61.04ms
step:164/2330 train_time:10011ms step_avg:61.05ms
step:165/2330 train_time:10071ms step_avg:61.04ms
step:166/2330 train_time:10133ms step_avg:61.04ms
step:167/2330 train_time:10193ms step_avg:61.03ms
step:168/2330 train_time:10255ms step_avg:61.04ms
step:169/2330 train_time:10314ms step_avg:61.03ms
step:170/2330 train_time:10377ms step_avg:61.04ms
step:171/2330 train_time:10436ms step_avg:61.03ms
step:172/2330 train_time:10498ms step_avg:61.04ms
step:173/2330 train_time:10558ms step_avg:61.03ms
step:174/2330 train_time:10620ms step_avg:61.03ms
step:175/2330 train_time:10679ms step_avg:61.02ms
step:176/2330 train_time:10741ms step_avg:61.03ms
step:177/2330 train_time:10800ms step_avg:61.02ms
step:178/2330 train_time:10863ms step_avg:61.03ms
step:179/2330 train_time:10922ms step_avg:61.02ms
step:180/2330 train_time:10985ms step_avg:61.03ms
step:181/2330 train_time:11045ms step_avg:61.02ms
step:182/2330 train_time:11108ms step_avg:61.03ms
step:183/2330 train_time:11168ms step_avg:61.03ms
step:184/2330 train_time:11230ms step_avg:61.03ms
step:185/2330 train_time:11291ms step_avg:61.03ms
step:186/2330 train_time:11354ms step_avg:61.04ms
step:187/2330 train_time:11413ms step_avg:61.03ms
step:188/2330 train_time:11476ms step_avg:61.04ms
step:189/2330 train_time:11535ms step_avg:61.03ms
step:190/2330 train_time:11597ms step_avg:61.04ms
step:191/2330 train_time:11656ms step_avg:61.03ms
step:192/2330 train_time:11718ms step_avg:61.03ms
step:193/2330 train_time:11778ms step_avg:61.03ms
step:194/2330 train_time:11840ms step_avg:61.03ms
step:195/2330 train_time:11899ms step_avg:61.02ms
step:196/2330 train_time:11961ms step_avg:61.03ms
step:197/2330 train_time:12021ms step_avg:61.02ms
step:198/2330 train_time:12084ms step_avg:61.03ms
step:199/2330 train_time:12144ms step_avg:61.03ms
step:200/2330 train_time:12207ms step_avg:61.04ms
step:201/2330 train_time:12267ms step_avg:61.03ms
step:202/2330 train_time:12330ms step_avg:61.04ms
step:203/2330 train_time:12390ms step_avg:61.03ms
step:204/2330 train_time:12452ms step_avg:61.04ms
step:205/2330 train_time:12512ms step_avg:61.03ms
step:206/2330 train_time:12574ms step_avg:61.04ms
step:207/2330 train_time:12634ms step_avg:61.03ms
step:208/2330 train_time:12696ms step_avg:61.04ms
step:209/2330 train_time:12755ms step_avg:61.03ms
step:210/2330 train_time:12817ms step_avg:61.03ms
step:211/2330 train_time:12877ms step_avg:61.03ms
step:212/2330 train_time:12939ms step_avg:61.03ms
step:213/2330 train_time:12998ms step_avg:61.03ms
step:214/2330 train_time:13060ms step_avg:61.03ms
step:215/2330 train_time:13119ms step_avg:61.02ms
step:216/2330 train_time:13182ms step_avg:61.03ms
step:217/2330 train_time:13242ms step_avg:61.02ms
step:218/2330 train_time:13305ms step_avg:61.03ms
step:219/2330 train_time:13365ms step_avg:61.03ms
step:220/2330 train_time:13428ms step_avg:61.04ms
step:221/2330 train_time:13488ms step_avg:61.03ms
step:222/2330 train_time:13551ms step_avg:61.04ms
step:223/2330 train_time:13610ms step_avg:61.03ms
step:224/2330 train_time:13673ms step_avg:61.04ms
step:225/2330 train_time:13732ms step_avg:61.03ms
step:226/2330 train_time:13794ms step_avg:61.04ms
step:227/2330 train_time:13853ms step_avg:61.03ms
step:228/2330 train_time:13915ms step_avg:61.03ms
step:229/2330 train_time:13975ms step_avg:61.02ms
step:230/2330 train_time:14037ms step_avg:61.03ms
step:231/2330 train_time:14097ms step_avg:61.03ms
step:232/2330 train_time:14159ms step_avg:61.03ms
step:233/2330 train_time:14219ms step_avg:61.03ms
step:234/2330 train_time:14281ms step_avg:61.03ms
step:235/2330 train_time:14341ms step_avg:61.02ms
step:236/2330 train_time:14403ms step_avg:61.03ms
step:237/2330 train_time:14462ms step_avg:61.02ms
step:238/2330 train_time:14525ms step_avg:61.03ms
step:239/2330 train_time:14585ms step_avg:61.02ms
step:240/2330 train_time:14647ms step_avg:61.03ms
step:241/2330 train_time:14707ms step_avg:61.02ms
step:242/2330 train_time:14770ms step_avg:61.03ms
step:243/2330 train_time:14829ms step_avg:61.03ms
step:244/2330 train_time:14892ms step_avg:61.03ms
step:245/2330 train_time:14951ms step_avg:61.02ms
step:246/2330 train_time:15013ms step_avg:61.03ms
step:247/2330 train_time:15071ms step_avg:61.02ms
step:248/2330 train_time:15133ms step_avg:61.02ms
step:249/2330 train_time:15193ms step_avg:61.01ms
step:250/2330 train_time:15255ms step_avg:61.02ms
step:250/2330 val_loss:4.0839 train_time:15319ms step_avg:61.28ms
step:251/2330 train_time:15343ms step_avg:61.13ms
step:252/2330 train_time:15380ms step_avg:61.03ms
step:253/2330 train_time:15443ms step_avg:61.04ms
step:254/2330 train_time:15510ms step_avg:61.06ms
step:255/2330 train_time:15570ms step_avg:61.06ms
step:256/2330 train_time:15634ms step_avg:61.07ms
step:257/2330 train_time:15693ms step_avg:61.06ms
step:258/2330 train_time:15755ms step_avg:61.07ms
step:259/2330 train_time:15815ms step_avg:61.06ms
step:260/2330 train_time:15877ms step_avg:61.06ms
step:261/2330 train_time:15935ms step_avg:61.05ms
step:262/2330 train_time:15997ms step_avg:61.06ms
step:263/2330 train_time:16056ms step_avg:61.05ms
step:264/2330 train_time:16118ms step_avg:61.05ms
step:265/2330 train_time:16176ms step_avg:61.04ms
step:266/2330 train_time:16240ms step_avg:61.05ms
step:267/2330 train_time:16300ms step_avg:61.05ms
step:268/2330 train_time:16363ms step_avg:61.06ms
step:269/2330 train_time:16426ms step_avg:61.06ms
step:270/2330 train_time:16489ms step_avg:61.07ms
step:271/2330 train_time:16548ms step_avg:61.06ms
step:272/2330 train_time:16611ms step_avg:61.07ms
step:273/2330 train_time:16670ms step_avg:61.06ms
step:274/2330 train_time:16733ms step_avg:61.07ms
step:275/2330 train_time:16793ms step_avg:61.06ms
step:276/2330 train_time:16855ms step_avg:61.07ms
step:277/2330 train_time:16914ms step_avg:61.06ms
step:278/2330 train_time:16976ms step_avg:61.06ms
step:279/2330 train_time:17035ms step_avg:61.06ms
step:280/2330 train_time:17097ms step_avg:61.06ms
step:281/2330 train_time:17155ms step_avg:61.05ms
step:282/2330 train_time:17217ms step_avg:61.05ms
step:283/2330 train_time:17277ms step_avg:61.05ms
step:284/2330 train_time:17341ms step_avg:61.06ms
step:285/2330 train_time:17402ms step_avg:61.06ms
step:286/2330 train_time:17466ms step_avg:61.07ms
step:287/2330 train_time:17526ms step_avg:61.06ms
step:288/2330 train_time:17588ms step_avg:61.07ms
step:289/2330 train_time:17647ms step_avg:61.06ms
step:290/2330 train_time:17710ms step_avg:61.07ms
step:291/2330 train_time:17768ms step_avg:61.06ms
step:292/2330 train_time:17831ms step_avg:61.06ms
step:293/2330 train_time:17890ms step_avg:61.06ms
step:294/2330 train_time:17952ms step_avg:61.06ms
step:295/2330 train_time:18010ms step_avg:61.05ms
step:296/2330 train_time:18073ms step_avg:61.06ms
step:297/2330 train_time:18131ms step_avg:61.05ms
step:298/2330 train_time:18193ms step_avg:61.05ms
step:299/2330 train_time:18252ms step_avg:61.04ms
step:300/2330 train_time:18315ms step_avg:61.05ms
step:301/2330 train_time:18375ms step_avg:61.05ms
step:302/2330 train_time:18439ms step_avg:61.05ms
step:303/2330 train_time:18499ms step_avg:61.05ms
step:304/2330 train_time:18563ms step_avg:61.06ms
step:305/2330 train_time:18622ms step_avg:61.06ms
step:306/2330 train_time:18684ms step_avg:61.06ms
step:307/2330 train_time:18743ms step_avg:61.05ms
step:308/2330 train_time:18805ms step_avg:61.06ms
step:309/2330 train_time:18865ms step_avg:61.05ms
step:310/2330 train_time:18927ms step_avg:61.05ms
step:311/2330 train_time:18986ms step_avg:61.05ms
step:312/2330 train_time:19048ms step_avg:61.05ms
step:313/2330 train_time:19107ms step_avg:61.04ms
step:314/2330 train_time:19169ms step_avg:61.05ms
step:315/2330 train_time:19228ms step_avg:61.04ms
step:316/2330 train_time:19290ms step_avg:61.04ms
step:317/2330 train_time:19350ms step_avg:61.04ms
step:318/2330 train_time:19413ms step_avg:61.05ms
step:319/2330 train_time:19473ms step_avg:61.04ms
step:320/2330 train_time:19536ms step_avg:61.05ms
step:321/2330 train_time:19596ms step_avg:61.05ms
step:322/2330 train_time:19659ms step_avg:61.05ms
step:323/2330 train_time:19719ms step_avg:61.05ms
step:324/2330 train_time:19781ms step_avg:61.05ms
step:325/2330 train_time:19840ms step_avg:61.05ms
step:326/2330 train_time:19902ms step_avg:61.05ms
step:327/2330 train_time:19961ms step_avg:61.04ms
step:328/2330 train_time:20024ms step_avg:61.05ms
step:329/2330 train_time:20083ms step_avg:61.04ms
step:330/2330 train_time:20144ms step_avg:61.04ms
step:331/2330 train_time:20205ms step_avg:61.04ms
step:332/2330 train_time:20267ms step_avg:61.04ms
step:333/2330 train_time:20327ms step_avg:61.04ms
step:334/2330 train_time:20388ms step_avg:61.04ms
step:335/2330 train_time:20448ms step_avg:61.04ms
step:336/2330 train_time:20511ms step_avg:61.04ms
step:337/2330 train_time:20571ms step_avg:61.04ms
step:338/2330 train_time:20633ms step_avg:61.04ms
step:339/2330 train_time:20692ms step_avg:61.04ms
step:340/2330 train_time:20755ms step_avg:61.04ms
step:341/2330 train_time:20814ms step_avg:61.04ms
step:342/2330 train_time:20877ms step_avg:61.04ms
step:343/2330 train_time:20937ms step_avg:61.04ms
step:344/2330 train_time:20999ms step_avg:61.04ms
step:345/2330 train_time:21058ms step_avg:61.04ms
step:346/2330 train_time:21121ms step_avg:61.04ms
step:347/2330 train_time:21180ms step_avg:61.04ms
step:348/2330 train_time:21242ms step_avg:61.04ms
step:349/2330 train_time:21301ms step_avg:61.03ms
step:350/2330 train_time:21364ms step_avg:61.04ms
step:351/2330 train_time:21423ms step_avg:61.03ms
step:352/2330 train_time:21486ms step_avg:61.04ms
step:353/2330 train_time:21545ms step_avg:61.03ms
step:354/2330 train_time:21607ms step_avg:61.04ms
step:355/2330 train_time:21667ms step_avg:61.03ms
step:356/2330 train_time:21729ms step_avg:61.04ms
step:357/2330 train_time:21788ms step_avg:61.03ms
step:358/2330 train_time:21850ms step_avg:61.03ms
step:359/2330 train_time:21910ms step_avg:61.03ms
step:360/2330 train_time:21973ms step_avg:61.04ms
step:361/2330 train_time:22032ms step_avg:61.03ms
step:362/2330 train_time:22094ms step_avg:61.03ms
step:363/2330 train_time:22155ms step_avg:61.03ms
step:364/2330 train_time:22218ms step_avg:61.04ms
step:365/2330 train_time:22277ms step_avg:61.03ms
step:366/2330 train_time:22340ms step_avg:61.04ms
step:367/2330 train_time:22400ms step_avg:61.04ms
step:368/2330 train_time:22462ms step_avg:61.04ms
step:369/2330 train_time:22521ms step_avg:61.03ms
step:370/2330 train_time:22583ms step_avg:61.04ms
step:371/2330 train_time:22642ms step_avg:61.03ms
step:372/2330 train_time:22704ms step_avg:61.03ms
step:373/2330 train_time:22764ms step_avg:61.03ms
step:374/2330 train_time:22826ms step_avg:61.03ms
step:375/2330 train_time:22886ms step_avg:61.03ms
step:376/2330 train_time:22948ms step_avg:61.03ms
step:377/2330 train_time:23007ms step_avg:61.03ms
step:378/2330 train_time:23069ms step_avg:61.03ms
step:379/2330 train_time:23128ms step_avg:61.02ms
step:380/2330 train_time:23189ms step_avg:61.02ms
step:381/2330 train_time:23249ms step_avg:61.02ms
step:382/2330 train_time:23312ms step_avg:61.03ms
step:383/2330 train_time:23371ms step_avg:61.02ms
step:384/2330 train_time:23434ms step_avg:61.03ms
step:385/2330 train_time:23494ms step_avg:61.02ms
step:386/2330 train_time:23557ms step_avg:61.03ms
step:387/2330 train_time:23617ms step_avg:61.03ms
step:388/2330 train_time:23680ms step_avg:61.03ms
step:389/2330 train_time:23738ms step_avg:61.02ms
step:390/2330 train_time:23801ms step_avg:61.03ms
step:391/2330 train_time:23861ms step_avg:61.03ms
step:392/2330 train_time:23923ms step_avg:61.03ms
step:393/2330 train_time:23983ms step_avg:61.02ms
step:394/2330 train_time:24044ms step_avg:61.03ms
step:395/2330 train_time:24104ms step_avg:61.02ms
step:396/2330 train_time:24167ms step_avg:61.03ms
step:397/2330 train_time:24226ms step_avg:61.02ms
step:398/2330 train_time:24288ms step_avg:61.03ms
step:399/2330 train_time:24347ms step_avg:61.02ms
step:400/2330 train_time:24409ms step_avg:61.02ms
step:401/2330 train_time:24469ms step_avg:61.02ms
step:402/2330 train_time:24531ms step_avg:61.02ms
step:403/2330 train_time:24590ms step_avg:61.02ms
step:404/2330 train_time:24653ms step_avg:61.02ms
step:405/2330 train_time:24712ms step_avg:61.02ms
step:406/2330 train_time:24775ms step_avg:61.02ms
step:407/2330 train_time:24834ms step_avg:61.02ms
step:408/2330 train_time:24898ms step_avg:61.02ms
step:409/2330 train_time:24957ms step_avg:61.02ms
step:410/2330 train_time:25020ms step_avg:61.02ms
step:411/2330 train_time:25080ms step_avg:61.02ms
step:412/2330 train_time:25142ms step_avg:61.02ms
step:413/2330 train_time:25201ms step_avg:61.02ms
step:414/2330 train_time:25263ms step_avg:61.02ms
step:415/2330 train_time:25322ms step_avg:61.02ms
step:416/2330 train_time:25385ms step_avg:61.02ms
step:417/2330 train_time:25445ms step_avg:61.02ms
step:418/2330 train_time:25507ms step_avg:61.02ms
step:419/2330 train_time:25567ms step_avg:61.02ms
step:420/2330 train_time:25628ms step_avg:61.02ms
step:421/2330 train_time:25687ms step_avg:61.01ms
step:422/2330 train_time:25749ms step_avg:61.02ms
step:423/2330 train_time:25809ms step_avg:61.01ms
step:424/2330 train_time:25871ms step_avg:61.02ms
step:425/2330 train_time:25930ms step_avg:61.01ms
step:426/2330 train_time:25993ms step_avg:61.02ms
step:427/2330 train_time:26053ms step_avg:61.01ms
step:428/2330 train_time:26116ms step_avg:61.02ms
step:429/2330 train_time:26176ms step_avg:61.02ms
step:430/2330 train_time:26238ms step_avg:61.02ms
step:431/2330 train_time:26298ms step_avg:61.02ms
step:432/2330 train_time:26361ms step_avg:61.02ms
step:433/2330 train_time:26421ms step_avg:61.02ms
step:434/2330 train_time:26483ms step_avg:61.02ms
step:435/2330 train_time:26542ms step_avg:61.02ms
step:436/2330 train_time:26604ms step_avg:61.02ms
step:437/2330 train_time:26664ms step_avg:61.02ms
step:438/2330 train_time:26726ms step_avg:61.02ms
step:439/2330 train_time:26786ms step_avg:61.02ms
step:440/2330 train_time:26847ms step_avg:61.02ms
step:441/2330 train_time:26906ms step_avg:61.01ms
step:442/2330 train_time:26969ms step_avg:61.02ms
step:443/2330 train_time:27028ms step_avg:61.01ms
step:444/2330 train_time:27090ms step_avg:61.01ms
step:445/2330 train_time:27150ms step_avg:61.01ms
step:446/2330 train_time:27213ms step_avg:61.02ms
step:447/2330 train_time:27273ms step_avg:61.01ms
step:448/2330 train_time:27336ms step_avg:61.02ms
step:449/2330 train_time:27396ms step_avg:61.02ms
step:450/2330 train_time:27458ms step_avg:61.02ms
step:451/2330 train_time:27518ms step_avg:61.02ms
step:452/2330 train_time:27580ms step_avg:61.02ms
step:453/2330 train_time:27640ms step_avg:61.02ms
step:454/2330 train_time:27702ms step_avg:61.02ms
step:455/2330 train_time:27761ms step_avg:61.01ms
step:456/2330 train_time:27823ms step_avg:61.02ms
step:457/2330 train_time:27882ms step_avg:61.01ms
step:458/2330 train_time:27944ms step_avg:61.01ms
step:459/2330 train_time:28003ms step_avg:61.01ms
step:460/2330 train_time:28066ms step_avg:61.01ms
step:461/2330 train_time:28126ms step_avg:61.01ms
step:462/2330 train_time:28188ms step_avg:61.01ms
step:463/2330 train_time:28247ms step_avg:61.01ms
step:464/2330 train_time:28309ms step_avg:61.01ms
step:465/2330 train_time:28369ms step_avg:61.01ms
step:466/2330 train_time:28431ms step_avg:61.01ms
step:467/2330 train_time:28491ms step_avg:61.01ms
step:468/2330 train_time:28554ms step_avg:61.01ms
step:469/2330 train_time:28614ms step_avg:61.01ms
step:470/2330 train_time:28677ms step_avg:61.01ms
step:471/2330 train_time:28737ms step_avg:61.01ms
step:472/2330 train_time:28799ms step_avg:61.02ms
step:473/2330 train_time:28859ms step_avg:61.01ms
step:474/2330 train_time:28921ms step_avg:61.02ms
step:475/2330 train_time:28981ms step_avg:61.01ms
step:476/2330 train_time:29042ms step_avg:61.01ms
step:477/2330 train_time:29101ms step_avg:61.01ms
step:478/2330 train_time:29164ms step_avg:61.01ms
step:479/2330 train_time:29224ms step_avg:61.01ms
step:480/2330 train_time:29286ms step_avg:61.01ms
step:481/2330 train_time:29346ms step_avg:61.01ms
step:482/2330 train_time:29407ms step_avg:61.01ms
step:483/2330 train_time:29466ms step_avg:61.01ms
step:484/2330 train_time:29528ms step_avg:61.01ms
step:485/2330 train_time:29588ms step_avg:61.01ms
step:486/2330 train_time:29650ms step_avg:61.01ms
step:487/2330 train_time:29709ms step_avg:61.00ms
step:488/2330 train_time:29772ms step_avg:61.01ms
step:489/2330 train_time:29832ms step_avg:61.01ms
step:490/2330 train_time:29894ms step_avg:61.01ms
step:491/2330 train_time:29954ms step_avg:61.01ms
step:492/2330 train_time:30017ms step_avg:61.01ms
step:493/2330 train_time:30077ms step_avg:61.01ms
step:494/2330 train_time:30139ms step_avg:61.01ms
step:495/2330 train_time:30198ms step_avg:61.01ms
step:496/2330 train_time:30261ms step_avg:61.01ms
step:497/2330 train_time:30321ms step_avg:61.01ms
step:498/2330 train_time:30383ms step_avg:61.01ms
step:499/2330 train_time:30442ms step_avg:61.01ms
step:500/2330 train_time:30504ms step_avg:61.01ms
step:500/2330 val_loss:3.8160 train_time:30567ms step_avg:61.13ms
step:501/2330 train_time:30591ms step_avg:61.06ms
step:502/2330 train_time:30629ms step_avg:61.01ms
step:503/2330 train_time:30692ms step_avg:61.02ms
step:504/2330 train_time:30757ms step_avg:61.02ms
step:505/2330 train_time:30815ms step_avg:61.02ms
step:506/2330 train_time:30878ms step_avg:61.02ms
step:507/2330 train_time:30937ms step_avg:61.02ms
step:508/2330 train_time:30998ms step_avg:61.02ms
step:509/2330 train_time:31057ms step_avg:61.02ms
step:510/2330 train_time:31119ms step_avg:61.02ms
step:511/2330 train_time:31178ms step_avg:61.01ms
step:512/2330 train_time:31240ms step_avg:61.02ms
step:513/2330 train_time:31298ms step_avg:61.01ms
step:514/2330 train_time:31360ms step_avg:61.01ms
step:515/2330 train_time:31419ms step_avg:61.01ms
step:516/2330 train_time:31480ms step_avg:61.01ms
step:517/2330 train_time:31539ms step_avg:61.00ms
step:518/2330 train_time:31604ms step_avg:61.01ms
step:519/2330 train_time:31664ms step_avg:61.01ms
step:520/2330 train_time:31727ms step_avg:61.01ms
step:521/2330 train_time:31786ms step_avg:61.01ms
step:522/2330 train_time:31849ms step_avg:61.01ms
step:523/2330 train_time:31908ms step_avg:61.01ms
step:524/2330 train_time:31970ms step_avg:61.01ms
step:525/2330 train_time:32030ms step_avg:61.01ms
step:526/2330 train_time:32092ms step_avg:61.01ms
step:527/2330 train_time:32151ms step_avg:61.01ms
step:528/2330 train_time:32214ms step_avg:61.01ms
step:529/2330 train_time:32273ms step_avg:61.01ms
step:530/2330 train_time:32336ms step_avg:61.01ms
step:531/2330 train_time:32394ms step_avg:61.01ms
step:532/2330 train_time:32456ms step_avg:61.01ms
step:533/2330 train_time:32516ms step_avg:61.01ms
step:534/2330 train_time:32579ms step_avg:61.01ms
step:535/2330 train_time:32639ms step_avg:61.01ms
step:536/2330 train_time:32701ms step_avg:61.01ms
step:537/2330 train_time:32761ms step_avg:61.01ms
step:538/2330 train_time:32823ms step_avg:61.01ms
step:539/2330 train_time:32883ms step_avg:61.01ms
step:540/2330 train_time:32945ms step_avg:61.01ms
step:541/2330 train_time:33004ms step_avg:61.01ms
step:542/2330 train_time:33066ms step_avg:61.01ms
step:543/2330 train_time:33125ms step_avg:61.00ms
step:544/2330 train_time:33187ms step_avg:61.01ms
step:545/2330 train_time:33248ms step_avg:61.01ms
step:546/2330 train_time:33310ms step_avg:61.01ms
step:547/2330 train_time:33369ms step_avg:61.00ms
step:548/2330 train_time:33431ms step_avg:61.01ms
step:549/2330 train_time:33492ms step_avg:61.01ms
step:550/2330 train_time:33555ms step_avg:61.01ms
step:551/2330 train_time:33615ms step_avg:61.01ms
step:552/2330 train_time:33678ms step_avg:61.01ms
step:553/2330 train_time:33738ms step_avg:61.01ms
step:554/2330 train_time:33801ms step_avg:61.01ms
step:555/2330 train_time:33860ms step_avg:61.01ms
step:556/2330 train_time:33922ms step_avg:61.01ms
step:557/2330 train_time:33982ms step_avg:61.01ms
step:558/2330 train_time:34044ms step_avg:61.01ms
step:559/2330 train_time:34103ms step_avg:61.01ms
step:560/2330 train_time:34165ms step_avg:61.01ms
step:561/2330 train_time:34224ms step_avg:61.01ms
step:562/2330 train_time:34286ms step_avg:61.01ms
step:563/2330 train_time:34345ms step_avg:61.00ms
step:564/2330 train_time:34407ms step_avg:61.00ms
step:565/2330 train_time:34466ms step_avg:61.00ms
step:566/2330 train_time:34528ms step_avg:61.00ms
step:567/2330 train_time:34588ms step_avg:61.00ms
step:568/2330 train_time:34651ms step_avg:61.01ms
step:569/2330 train_time:34711ms step_avg:61.00ms
step:570/2330 train_time:34774ms step_avg:61.01ms
step:571/2330 train_time:34834ms step_avg:61.00ms
step:572/2330 train_time:34897ms step_avg:61.01ms
step:573/2330 train_time:34956ms step_avg:61.01ms
step:574/2330 train_time:35019ms step_avg:61.01ms
step:575/2330 train_time:35079ms step_avg:61.01ms
step:576/2330 train_time:35141ms step_avg:61.01ms
step:577/2330 train_time:35200ms step_avg:61.01ms
step:578/2330 train_time:35263ms step_avg:61.01ms
step:579/2330 train_time:35322ms step_avg:61.00ms
step:580/2330 train_time:35383ms step_avg:61.01ms
step:581/2330 train_time:35443ms step_avg:61.00ms
step:582/2330 train_time:35506ms step_avg:61.01ms
step:583/2330 train_time:35566ms step_avg:61.00ms
step:584/2330 train_time:35627ms step_avg:61.01ms
step:585/2330 train_time:35687ms step_avg:61.00ms
step:586/2330 train_time:35749ms step_avg:61.00ms
step:587/2330 train_time:35809ms step_avg:61.00ms
step:588/2330 train_time:35872ms step_avg:61.01ms
step:589/2330 train_time:35931ms step_avg:61.00ms
step:590/2330 train_time:35995ms step_avg:61.01ms
step:591/2330 train_time:36055ms step_avg:61.01ms
step:592/2330 train_time:36118ms step_avg:61.01ms
step:593/2330 train_time:36177ms step_avg:61.01ms
step:594/2330 train_time:36240ms step_avg:61.01ms
step:595/2330 train_time:36299ms step_avg:61.01ms
step:596/2330 train_time:36362ms step_avg:61.01ms
step:597/2330 train_time:36422ms step_avg:61.01ms
step:598/2330 train_time:36483ms step_avg:61.01ms
step:599/2330 train_time:36543ms step_avg:61.01ms
step:600/2330 train_time:36605ms step_avg:61.01ms
step:601/2330 train_time:36665ms step_avg:61.01ms
step:602/2330 train_time:36727ms step_avg:61.01ms
step:603/2330 train_time:36786ms step_avg:61.00ms
step:604/2330 train_time:36848ms step_avg:61.01ms
step:605/2330 train_time:36908ms step_avg:61.00ms
step:606/2330 train_time:36970ms step_avg:61.01ms
step:607/2330 train_time:37030ms step_avg:61.00ms
step:608/2330 train_time:37092ms step_avg:61.01ms
step:609/2330 train_time:37152ms step_avg:61.00ms
step:610/2330 train_time:37215ms step_avg:61.01ms
step:611/2330 train_time:37275ms step_avg:61.01ms
step:612/2330 train_time:37338ms step_avg:61.01ms
step:613/2330 train_time:37398ms step_avg:61.01ms
step:614/2330 train_time:37460ms step_avg:61.01ms
step:615/2330 train_time:37519ms step_avg:61.01ms
step:616/2330 train_time:37582ms step_avg:61.01ms
step:617/2330 train_time:37641ms step_avg:61.01ms
step:618/2330 train_time:37704ms step_avg:61.01ms
step:619/2330 train_time:37763ms step_avg:61.01ms
step:620/2330 train_time:37825ms step_avg:61.01ms
step:621/2330 train_time:37884ms step_avg:61.01ms
step:622/2330 train_time:37947ms step_avg:61.01ms
step:623/2330 train_time:38007ms step_avg:61.01ms
step:624/2330 train_time:38069ms step_avg:61.01ms
step:625/2330 train_time:38128ms step_avg:61.00ms
step:626/2330 train_time:38190ms step_avg:61.01ms
step:627/2330 train_time:38250ms step_avg:61.01ms
step:628/2330 train_time:38313ms step_avg:61.01ms
step:629/2330 train_time:38373ms step_avg:61.01ms
step:630/2330 train_time:38435ms step_avg:61.01ms
step:631/2330 train_time:38495ms step_avg:61.01ms
step:632/2330 train_time:38558ms step_avg:61.01ms
step:633/2330 train_time:38619ms step_avg:61.01ms
step:634/2330 train_time:38681ms step_avg:61.01ms
step:635/2330 train_time:38740ms step_avg:61.01ms
step:636/2330 train_time:38802ms step_avg:61.01ms
step:637/2330 train_time:38861ms step_avg:61.01ms
step:638/2330 train_time:38923ms step_avg:61.01ms
step:639/2330 train_time:38983ms step_avg:61.01ms
step:640/2330 train_time:39045ms step_avg:61.01ms
step:641/2330 train_time:39105ms step_avg:61.01ms
step:642/2330 train_time:39166ms step_avg:61.01ms
step:643/2330 train_time:39226ms step_avg:61.00ms
step:644/2330 train_time:39288ms step_avg:61.01ms
step:645/2330 train_time:39347ms step_avg:61.00ms
step:646/2330 train_time:39410ms step_avg:61.01ms
step:647/2330 train_time:39470ms step_avg:61.00ms
step:648/2330 train_time:39533ms step_avg:61.01ms
step:649/2330 train_time:39592ms step_avg:61.00ms
step:650/2330 train_time:39655ms step_avg:61.01ms
step:651/2330 train_time:39715ms step_avg:61.01ms
step:652/2330 train_time:39777ms step_avg:61.01ms
step:653/2330 train_time:39837ms step_avg:61.01ms
step:654/2330 train_time:39900ms step_avg:61.01ms
step:655/2330 train_time:39959ms step_avg:61.01ms
step:656/2330 train_time:40021ms step_avg:61.01ms
step:657/2330 train_time:40080ms step_avg:61.00ms
step:658/2330 train_time:40142ms step_avg:61.01ms
step:659/2330 train_time:40202ms step_avg:61.00ms
step:660/2330 train_time:40264ms step_avg:61.01ms
step:661/2330 train_time:40323ms step_avg:61.00ms
step:662/2330 train_time:40386ms step_avg:61.01ms
step:663/2330 train_time:40446ms step_avg:61.00ms
step:664/2330 train_time:40508ms step_avg:61.01ms
step:665/2330 train_time:40567ms step_avg:61.00ms
step:666/2330 train_time:40629ms step_avg:61.00ms
step:667/2330 train_time:40688ms step_avg:61.00ms
step:668/2330 train_time:40751ms step_avg:61.00ms
step:669/2330 train_time:40812ms step_avg:61.00ms
step:670/2330 train_time:40874ms step_avg:61.01ms
step:671/2330 train_time:40934ms step_avg:61.00ms
step:672/2330 train_time:40997ms step_avg:61.01ms
step:673/2330 train_time:41058ms step_avg:61.01ms
step:674/2330 train_time:41120ms step_avg:61.01ms
step:675/2330 train_time:41179ms step_avg:61.01ms
step:676/2330 train_time:41241ms step_avg:61.01ms
step:677/2330 train_time:41300ms step_avg:61.00ms
step:678/2330 train_time:41362ms step_avg:61.01ms
step:679/2330 train_time:41422ms step_avg:61.00ms
step:680/2330 train_time:41484ms step_avg:61.01ms
step:681/2330 train_time:41545ms step_avg:61.01ms
step:682/2330 train_time:41607ms step_avg:61.01ms
step:683/2330 train_time:41667ms step_avg:61.01ms
step:684/2330 train_time:41728ms step_avg:61.01ms
step:685/2330 train_time:41788ms step_avg:61.00ms
step:686/2330 train_time:41851ms step_avg:61.01ms
step:687/2330 train_time:41911ms step_avg:61.01ms
step:688/2330 train_time:41974ms step_avg:61.01ms
step:689/2330 train_time:42033ms step_avg:61.01ms
step:690/2330 train_time:42097ms step_avg:61.01ms
step:691/2330 train_time:42157ms step_avg:61.01ms
step:692/2330 train_time:42219ms step_avg:61.01ms
step:693/2330 train_time:42278ms step_avg:61.01ms
step:694/2330 train_time:42340ms step_avg:61.01ms
step:695/2330 train_time:42399ms step_avg:61.01ms
step:696/2330 train_time:42461ms step_avg:61.01ms
step:697/2330 train_time:42521ms step_avg:61.01ms
step:698/2330 train_time:42583ms step_avg:61.01ms
step:699/2330 train_time:42643ms step_avg:61.01ms
step:700/2330 train_time:42706ms step_avg:61.01ms
step:701/2330 train_time:42765ms step_avg:61.01ms
step:702/2330 train_time:42827ms step_avg:61.01ms
step:703/2330 train_time:42886ms step_avg:61.00ms
step:704/2330 train_time:42949ms step_avg:61.01ms
step:705/2330 train_time:43009ms step_avg:61.01ms
step:706/2330 train_time:43072ms step_avg:61.01ms
step:707/2330 train_time:43132ms step_avg:61.01ms
step:708/2330 train_time:43194ms step_avg:61.01ms
step:709/2330 train_time:43254ms step_avg:61.01ms
step:710/2330 train_time:43317ms step_avg:61.01ms
step:711/2330 train_time:43376ms step_avg:61.01ms
step:712/2330 train_time:43440ms step_avg:61.01ms
step:713/2330 train_time:43499ms step_avg:61.01ms
step:714/2330 train_time:43562ms step_avg:61.01ms
step:715/2330 train_time:43621ms step_avg:61.01ms
step:716/2330 train_time:43683ms step_avg:61.01ms
step:717/2330 train_time:43743ms step_avg:61.01ms
step:718/2330 train_time:43805ms step_avg:61.01ms
step:719/2330 train_time:43865ms step_avg:61.01ms
step:720/2330 train_time:43927ms step_avg:61.01ms
step:721/2330 train_time:43986ms step_avg:61.01ms
step:722/2330 train_time:44048ms step_avg:61.01ms
step:723/2330 train_time:44107ms step_avg:61.01ms
step:724/2330 train_time:44169ms step_avg:61.01ms
step:725/2330 train_time:44229ms step_avg:61.01ms
step:726/2330 train_time:44291ms step_avg:61.01ms
step:727/2330 train_time:44351ms step_avg:61.01ms
step:728/2330 train_time:44414ms step_avg:61.01ms
step:729/2330 train_time:44473ms step_avg:61.01ms
step:730/2330 train_time:44536ms step_avg:61.01ms
step:731/2330 train_time:44596ms step_avg:61.01ms
step:732/2330 train_time:44659ms step_avg:61.01ms
step:733/2330 train_time:44719ms step_avg:61.01ms
step:734/2330 train_time:44781ms step_avg:61.01ms
step:735/2330 train_time:44841ms step_avg:61.01ms
step:736/2330 train_time:44904ms step_avg:61.01ms
step:737/2330 train_time:44963ms step_avg:61.01ms
step:738/2330 train_time:45025ms step_avg:61.01ms
step:739/2330 train_time:45084ms step_avg:61.01ms
step:740/2330 train_time:45146ms step_avg:61.01ms
step:741/2330 train_time:45206ms step_avg:61.01ms
step:742/2330 train_time:45267ms step_avg:61.01ms
step:743/2330 train_time:45326ms step_avg:61.00ms
step:744/2330 train_time:45388ms step_avg:61.01ms
step:745/2330 train_time:45448ms step_avg:61.00ms
step:746/2330 train_time:45511ms step_avg:61.01ms
step:747/2330 train_time:45570ms step_avg:61.00ms
step:748/2330 train_time:45633ms step_avg:61.01ms
step:749/2330 train_time:45693ms step_avg:61.01ms
step:750/2330 train_time:45756ms step_avg:61.01ms
step:750/2330 val_loss:3.6882 train_time:45820ms step_avg:61.09ms
step:751/2330 train_time:45845ms step_avg:61.04ms
step:752/2330 train_time:45881ms step_avg:61.01ms
step:753/2330 train_time:45945ms step_avg:61.02ms
step:754/2330 train_time:46010ms step_avg:61.02ms
step:755/2330 train_time:46070ms step_avg:61.02ms
step:756/2330 train_time:46133ms step_avg:61.02ms
step:757/2330 train_time:46192ms step_avg:61.02ms
step:758/2330 train_time:46254ms step_avg:61.02ms
step:759/2330 train_time:46312ms step_avg:61.02ms
step:760/2330 train_time:46374ms step_avg:61.02ms
step:761/2330 train_time:46435ms step_avg:61.02ms
step:762/2330 train_time:46496ms step_avg:61.02ms
step:763/2330 train_time:46554ms step_avg:61.02ms
step:764/2330 train_time:46616ms step_avg:61.02ms
step:765/2330 train_time:46675ms step_avg:61.01ms
step:766/2330 train_time:46738ms step_avg:61.02ms
step:767/2330 train_time:46798ms step_avg:61.01ms
step:768/2330 train_time:46862ms step_avg:61.02ms
step:769/2330 train_time:46924ms step_avg:61.02ms
step:770/2330 train_time:46987ms step_avg:61.02ms
step:771/2330 train_time:47048ms step_avg:61.02ms
step:772/2330 train_time:47111ms step_avg:61.03ms
step:773/2330 train_time:47171ms step_avg:61.02ms
step:774/2330 train_time:47234ms step_avg:61.03ms
step:775/2330 train_time:47294ms step_avg:61.02ms
step:776/2330 train_time:47356ms step_avg:61.03ms
step:777/2330 train_time:47416ms step_avg:61.02ms
step:778/2330 train_time:47478ms step_avg:61.03ms
step:779/2330 train_time:47538ms step_avg:61.02ms
step:780/2330 train_time:47600ms step_avg:61.03ms
step:781/2330 train_time:47660ms step_avg:61.02ms
step:782/2330 train_time:47722ms step_avg:61.03ms
step:783/2330 train_time:47783ms step_avg:61.03ms
step:784/2330 train_time:47846ms step_avg:61.03ms
step:785/2330 train_time:47906ms step_avg:61.03ms
step:786/2330 train_time:47970ms step_avg:61.03ms
step:787/2330 train_time:48030ms step_avg:61.03ms
step:788/2330 train_time:48093ms step_avg:61.03ms
step:789/2330 train_time:48154ms step_avg:61.03ms
step:790/2330 train_time:48217ms step_avg:61.03ms
step:791/2330 train_time:48277ms step_avg:61.03ms
step:792/2330 train_time:48340ms step_avg:61.04ms
step:793/2330 train_time:48399ms step_avg:61.03ms
step:794/2330 train_time:48461ms step_avg:61.03ms
step:795/2330 train_time:48521ms step_avg:61.03ms
step:796/2330 train_time:48583ms step_avg:61.03ms
step:797/2330 train_time:48643ms step_avg:61.03ms
step:798/2330 train_time:48706ms step_avg:61.03ms
step:799/2330 train_time:48765ms step_avg:61.03ms
step:800/2330 train_time:48828ms step_avg:61.03ms
step:801/2330 train_time:48888ms step_avg:61.03ms
step:802/2330 train_time:48951ms step_avg:61.04ms
step:803/2330 train_time:49012ms step_avg:61.04ms
step:804/2330 train_time:49075ms step_avg:61.04ms
step:805/2330 train_time:49135ms step_avg:61.04ms
step:806/2330 train_time:49199ms step_avg:61.04ms
step:807/2330 train_time:49259ms step_avg:61.04ms
step:808/2330 train_time:49322ms step_avg:61.04ms
step:809/2330 train_time:49381ms step_avg:61.04ms
step:810/2330 train_time:49444ms step_avg:61.04ms
step:811/2330 train_time:49504ms step_avg:61.04ms
step:812/2330 train_time:49566ms step_avg:61.04ms
step:813/2330 train_time:49626ms step_avg:61.04ms
step:814/2330 train_time:49688ms step_avg:61.04ms
step:815/2330 train_time:49748ms step_avg:61.04ms
step:816/2330 train_time:49811ms step_avg:61.04ms
step:817/2330 train_time:49871ms step_avg:61.04ms
step:818/2330 train_time:49934ms step_avg:61.04ms
step:819/2330 train_time:49995ms step_avg:61.04ms
step:820/2330 train_time:50058ms step_avg:61.05ms
step:821/2330 train_time:50118ms step_avg:61.04ms
step:822/2330 train_time:50181ms step_avg:61.05ms
step:823/2330 train_time:50241ms step_avg:61.05ms
step:824/2330 train_time:50304ms step_avg:61.05ms
step:825/2330 train_time:50363ms step_avg:61.05ms
step:826/2330 train_time:50426ms step_avg:61.05ms
step:827/2330 train_time:50486ms step_avg:61.05ms
step:828/2330 train_time:50548ms step_avg:61.05ms
step:829/2330 train_time:50608ms step_avg:61.05ms
step:830/2330 train_time:50671ms step_avg:61.05ms
step:831/2330 train_time:50731ms step_avg:61.05ms
step:832/2330 train_time:50794ms step_avg:61.05ms
step:833/2330 train_time:50854ms step_avg:61.05ms
step:834/2330 train_time:50916ms step_avg:61.05ms
step:835/2330 train_time:50976ms step_avg:61.05ms
step:836/2330 train_time:51039ms step_avg:61.05ms
step:837/2330 train_time:51099ms step_avg:61.05ms
step:838/2330 train_time:51162ms step_avg:61.05ms
step:839/2330 train_time:51222ms step_avg:61.05ms
step:840/2330 train_time:51286ms step_avg:61.05ms
step:841/2330 train_time:51346ms step_avg:61.05ms
step:842/2330 train_time:51408ms step_avg:61.05ms
step:843/2330 train_time:51468ms step_avg:61.05ms
step:844/2330 train_time:51531ms step_avg:61.06ms
step:845/2330 train_time:51590ms step_avg:61.05ms
step:846/2330 train_time:51653ms step_avg:61.06ms
step:847/2330 train_time:51713ms step_avg:61.05ms
step:848/2330 train_time:51776ms step_avg:61.06ms
step:849/2330 train_time:51837ms step_avg:61.06ms
step:850/2330 train_time:51899ms step_avg:61.06ms
step:851/2330 train_time:51959ms step_avg:61.06ms
step:852/2330 train_time:52022ms step_avg:61.06ms
step:853/2330 train_time:52082ms step_avg:61.06ms
step:854/2330 train_time:52146ms step_avg:61.06ms
step:855/2330 train_time:52205ms step_avg:61.06ms
step:856/2330 train_time:52268ms step_avg:61.06ms
step:857/2330 train_time:52328ms step_avg:61.06ms
step:858/2330 train_time:52391ms step_avg:61.06ms
step:859/2330 train_time:52451ms step_avg:61.06ms
step:860/2330 train_time:52514ms step_avg:61.06ms
step:861/2330 train_time:52573ms step_avg:61.06ms
step:862/2330 train_time:52636ms step_avg:61.06ms
step:863/2330 train_time:52696ms step_avg:61.06ms
step:864/2330 train_time:52759ms step_avg:61.06ms
step:865/2330 train_time:52818ms step_avg:61.06ms
step:866/2330 train_time:52881ms step_avg:61.06ms
step:867/2330 train_time:52941ms step_avg:61.06ms
step:868/2330 train_time:53005ms step_avg:61.07ms
step:869/2330 train_time:53065ms step_avg:61.06ms
step:870/2330 train_time:53127ms step_avg:61.07ms
step:871/2330 train_time:53187ms step_avg:61.06ms
step:872/2330 train_time:53250ms step_avg:61.07ms
step:873/2330 train_time:53310ms step_avg:61.06ms
step:874/2330 train_time:53373ms step_avg:61.07ms
step:875/2330 train_time:53432ms step_avg:61.07ms
step:876/2330 train_time:53495ms step_avg:61.07ms
step:877/2330 train_time:53555ms step_avg:61.07ms
step:878/2330 train_time:53618ms step_avg:61.07ms
step:879/2330 train_time:53678ms step_avg:61.07ms
step:880/2330 train_time:53741ms step_avg:61.07ms
step:881/2330 train_time:53801ms step_avg:61.07ms
step:882/2330 train_time:53864ms step_avg:61.07ms
step:883/2330 train_time:53923ms step_avg:61.07ms
step:884/2330 train_time:53986ms step_avg:61.07ms
step:885/2330 train_time:54046ms step_avg:61.07ms
step:886/2330 train_time:54108ms step_avg:61.07ms
step:887/2330 train_time:54168ms step_avg:61.07ms
step:888/2330 train_time:54231ms step_avg:61.07ms
step:889/2330 train_time:54292ms step_avg:61.07ms
step:890/2330 train_time:54355ms step_avg:61.07ms
step:891/2330 train_time:54414ms step_avg:61.07ms
step:892/2330 train_time:54477ms step_avg:61.07ms
step:893/2330 train_time:54538ms step_avg:61.07ms
step:894/2330 train_time:54600ms step_avg:61.07ms
step:895/2330 train_time:54660ms step_avg:61.07ms
step:896/2330 train_time:54723ms step_avg:61.07ms
step:897/2330 train_time:54783ms step_avg:61.07ms
step:898/2330 train_time:54846ms step_avg:61.08ms
step:899/2330 train_time:54906ms step_avg:61.07ms
step:900/2330 train_time:54969ms step_avg:61.08ms
step:901/2330 train_time:55029ms step_avg:61.07ms
step:902/2330 train_time:55092ms step_avg:61.08ms
step:903/2330 train_time:55152ms step_avg:61.08ms
step:904/2330 train_time:55215ms step_avg:61.08ms
step:905/2330 train_time:55274ms step_avg:61.08ms
step:906/2330 train_time:55337ms step_avg:61.08ms
step:907/2330 train_time:55396ms step_avg:61.08ms
step:908/2330 train_time:55459ms step_avg:61.08ms
step:909/2330 train_time:55518ms step_avg:61.08ms
step:910/2330 train_time:55581ms step_avg:61.08ms
step:911/2330 train_time:55641ms step_avg:61.08ms
step:912/2330 train_time:55704ms step_avg:61.08ms
step:913/2330 train_time:55764ms step_avg:61.08ms
step:914/2330 train_time:55826ms step_avg:61.08ms
step:915/2330 train_time:55886ms step_avg:61.08ms
step:916/2330 train_time:55949ms step_avg:61.08ms
step:917/2330 train_time:56009ms step_avg:61.08ms
step:918/2330 train_time:56072ms step_avg:61.08ms
step:919/2330 train_time:56132ms step_avg:61.08ms
step:920/2330 train_time:56195ms step_avg:61.08ms
step:921/2330 train_time:56255ms step_avg:61.08ms
step:922/2330 train_time:56318ms step_avg:61.08ms
step:923/2330 train_time:56378ms step_avg:61.08ms
step:924/2330 train_time:56441ms step_avg:61.08ms
step:925/2330 train_time:56500ms step_avg:61.08ms
step:926/2330 train_time:56564ms step_avg:61.08ms
step:927/2330 train_time:56623ms step_avg:61.08ms
step:928/2330 train_time:56686ms step_avg:61.08ms
step:929/2330 train_time:56745ms step_avg:61.08ms
step:930/2330 train_time:56808ms step_avg:61.08ms
step:931/2330 train_time:56868ms step_avg:61.08ms
step:932/2330 train_time:56931ms step_avg:61.08ms
step:933/2330 train_time:56991ms step_avg:61.08ms
step:934/2330 train_time:57055ms step_avg:61.09ms
step:935/2330 train_time:57115ms step_avg:61.09ms
step:936/2330 train_time:57177ms step_avg:61.09ms
step:937/2330 train_time:57237ms step_avg:61.09ms
step:938/2330 train_time:57300ms step_avg:61.09ms
step:939/2330 train_time:57360ms step_avg:61.09ms
step:940/2330 train_time:57423ms step_avg:61.09ms
step:941/2330 train_time:57483ms step_avg:61.09ms
step:942/2330 train_time:57546ms step_avg:61.09ms
step:943/2330 train_time:57606ms step_avg:61.09ms
step:944/2330 train_time:57668ms step_avg:61.09ms
step:945/2330 train_time:57728ms step_avg:61.09ms
step:946/2330 train_time:57791ms step_avg:61.09ms
step:947/2330 train_time:57851ms step_avg:61.09ms
step:948/2330 train_time:57914ms step_avg:61.09ms
step:949/2330 train_time:57975ms step_avg:61.09ms
step:950/2330 train_time:58038ms step_avg:61.09ms
step:951/2330 train_time:58097ms step_avg:61.09ms
step:952/2330 train_time:58160ms step_avg:61.09ms
step:953/2330 train_time:58220ms step_avg:61.09ms
step:954/2330 train_time:58282ms step_avg:61.09ms
step:955/2330 train_time:58342ms step_avg:61.09ms
step:956/2330 train_time:58405ms step_avg:61.09ms
step:957/2330 train_time:58465ms step_avg:61.09ms
step:958/2330 train_time:58528ms step_avg:61.09ms
step:959/2330 train_time:58588ms step_avg:61.09ms
step:960/2330 train_time:58651ms step_avg:61.09ms
step:961/2330 train_time:58711ms step_avg:61.09ms
step:962/2330 train_time:58774ms step_avg:61.10ms
step:963/2330 train_time:58834ms step_avg:61.09ms
step:964/2330 train_time:58896ms step_avg:61.10ms
step:965/2330 train_time:58956ms step_avg:61.09ms
step:966/2330 train_time:59019ms step_avg:61.10ms
step:967/2330 train_time:59078ms step_avg:61.09ms
step:968/2330 train_time:59141ms step_avg:61.10ms
step:969/2330 train_time:59201ms step_avg:61.09ms
step:970/2330 train_time:59264ms step_avg:61.10ms
step:971/2330 train_time:59324ms step_avg:61.10ms
step:972/2330 train_time:59386ms step_avg:61.10ms
step:973/2330 train_time:59446ms step_avg:61.10ms
step:974/2330 train_time:59509ms step_avg:61.10ms
step:975/2330 train_time:59569ms step_avg:61.10ms
step:976/2330 train_time:59632ms step_avg:61.10ms
step:977/2330 train_time:59692ms step_avg:61.10ms
step:978/2330 train_time:59755ms step_avg:61.10ms
step:979/2330 train_time:59814ms step_avg:61.10ms
step:980/2330 train_time:59877ms step_avg:61.10ms
step:981/2330 train_time:59937ms step_avg:61.10ms
step:982/2330 train_time:60000ms step_avg:61.10ms
step:983/2330 train_time:60060ms step_avg:61.10ms
step:984/2330 train_time:60123ms step_avg:61.10ms
step:985/2330 train_time:60182ms step_avg:61.10ms
step:986/2330 train_time:60245ms step_avg:61.10ms
step:987/2330 train_time:60305ms step_avg:61.10ms
step:988/2330 train_time:60368ms step_avg:61.10ms
step:989/2330 train_time:60428ms step_avg:61.10ms
step:990/2330 train_time:60491ms step_avg:61.10ms
step:991/2330 train_time:60551ms step_avg:61.10ms
step:992/2330 train_time:60614ms step_avg:61.10ms
step:993/2330 train_time:60673ms step_avg:61.10ms
step:994/2330 train_time:60737ms step_avg:61.10ms
step:995/2330 train_time:60796ms step_avg:61.10ms
step:996/2330 train_time:60860ms step_avg:61.10ms
step:997/2330 train_time:60919ms step_avg:61.10ms
step:998/2330 train_time:60982ms step_avg:61.10ms
step:999/2330 train_time:61042ms step_avg:61.10ms
step:1000/2330 train_time:61105ms step_avg:61.10ms
step:1000/2330 val_loss:3.5773 train_time:61169ms step_avg:61.17ms
step:1001/2330 train_time:61193ms step_avg:61.13ms
step:1002/2330 train_time:61230ms step_avg:61.11ms
step:1003/2330 train_time:61295ms step_avg:61.11ms
step:1004/2330 train_time:61364ms step_avg:61.12ms
step:1005/2330 train_time:61424ms step_avg:61.12ms
step:1006/2330 train_time:61487ms step_avg:61.12ms
step:1007/2330 train_time:61546ms step_avg:61.12ms
step:1008/2330 train_time:61608ms step_avg:61.12ms
step:1009/2330 train_time:61667ms step_avg:61.12ms
step:1010/2330 train_time:61729ms step_avg:61.12ms
step:1011/2330 train_time:61788ms step_avg:61.12ms
step:1012/2330 train_time:61850ms step_avg:61.12ms
step:1013/2330 train_time:61909ms step_avg:61.11ms
step:1014/2330 train_time:61972ms step_avg:61.12ms
step:1015/2330 train_time:62030ms step_avg:61.11ms
step:1016/2330 train_time:62095ms step_avg:61.12ms
step:1017/2330 train_time:62156ms step_avg:61.12ms
step:1018/2330 train_time:62221ms step_avg:61.12ms
step:1019/2330 train_time:62283ms step_avg:61.12ms
step:1020/2330 train_time:62347ms step_avg:61.12ms
step:1021/2330 train_time:62408ms step_avg:61.12ms
step:1022/2330 train_time:62472ms step_avg:61.13ms
step:1023/2330 train_time:62533ms step_avg:61.13ms
step:1024/2330 train_time:62596ms step_avg:61.13ms
step:1025/2330 train_time:62656ms step_avg:61.13ms
step:1026/2330 train_time:62719ms step_avg:61.13ms
step:1027/2330 train_time:62778ms step_avg:61.13ms
step:1028/2330 train_time:62841ms step_avg:61.13ms
step:1029/2330 train_time:62900ms step_avg:61.13ms
step:1030/2330 train_time:62963ms step_avg:61.13ms
step:1031/2330 train_time:63023ms step_avg:61.13ms
step:1032/2330 train_time:63086ms step_avg:61.13ms
step:1033/2330 train_time:63146ms step_avg:61.13ms
step:1034/2330 train_time:63209ms step_avg:61.13ms
step:1035/2330 train_time:63270ms step_avg:61.13ms
step:1036/2330 train_time:63334ms step_avg:61.13ms
step:1037/2330 train_time:63394ms step_avg:61.13ms
step:1038/2330 train_time:63459ms step_avg:61.14ms
step:1039/2330 train_time:63519ms step_avg:61.13ms
step:1040/2330 train_time:63582ms step_avg:61.14ms
step:1041/2330 train_time:63642ms step_avg:61.14ms
step:1042/2330 train_time:63704ms step_avg:61.14ms
step:1043/2330 train_time:63764ms step_avg:61.14ms
step:1044/2330 train_time:63827ms step_avg:61.14ms
step:1045/2330 train_time:63887ms step_avg:61.14ms
step:1046/2330 train_time:63949ms step_avg:61.14ms
step:1047/2330 train_time:64009ms step_avg:61.14ms
step:1048/2330 train_time:64073ms step_avg:61.14ms
step:1049/2330 train_time:64133ms step_avg:61.14ms
step:1050/2330 train_time:64196ms step_avg:61.14ms
step:1051/2330 train_time:64257ms step_avg:61.14ms
step:1052/2330 train_time:64320ms step_avg:61.14ms
step:1053/2330 train_time:64380ms step_avg:61.14ms
step:1054/2330 train_time:64443ms step_avg:61.14ms
step:1055/2330 train_time:64502ms step_avg:61.14ms
step:1056/2330 train_time:64565ms step_avg:61.14ms
step:1057/2330 train_time:64625ms step_avg:61.14ms
step:1058/2330 train_time:64688ms step_avg:61.14ms
step:1059/2330 train_time:64747ms step_avg:61.14ms
step:1060/2330 train_time:64809ms step_avg:61.14ms
step:1061/2330 train_time:64869ms step_avg:61.14ms
step:1062/2330 train_time:64931ms step_avg:61.14ms
step:1063/2330 train_time:64991ms step_avg:61.14ms
step:1064/2330 train_time:65054ms step_avg:61.14ms
step:1065/2330 train_time:65114ms step_avg:61.14ms
step:1066/2330 train_time:65178ms step_avg:61.14ms
step:1067/2330 train_time:65238ms step_avg:61.14ms
step:1068/2330 train_time:65302ms step_avg:61.14ms
step:1069/2330 train_time:65362ms step_avg:61.14ms
step:1070/2330 train_time:65426ms step_avg:61.15ms
step:1071/2330 train_time:65486ms step_avg:61.14ms
step:1072/2330 train_time:65549ms step_avg:61.15ms
step:1073/2330 train_time:65609ms step_avg:61.15ms
step:1074/2330 train_time:65672ms step_avg:61.15ms
step:1075/2330 train_time:65732ms step_avg:61.15ms
step:1076/2330 train_time:65795ms step_avg:61.15ms
step:1077/2330 train_time:65855ms step_avg:61.15ms
step:1078/2330 train_time:65917ms step_avg:61.15ms
step:1079/2330 train_time:65977ms step_avg:61.15ms
step:1080/2330 train_time:66040ms step_avg:61.15ms
step:1081/2330 train_time:66100ms step_avg:61.15ms
step:1082/2330 train_time:66163ms step_avg:61.15ms
step:1083/2330 train_time:66223ms step_avg:61.15ms
step:1084/2330 train_time:66287ms step_avg:61.15ms
step:1085/2330 train_time:66347ms step_avg:61.15ms
step:1086/2330 train_time:66410ms step_avg:61.15ms
step:1087/2330 train_time:66471ms step_avg:61.15ms
step:1088/2330 train_time:66534ms step_avg:61.15ms
step:1089/2330 train_time:66594ms step_avg:61.15ms
step:1090/2330 train_time:66657ms step_avg:61.15ms
step:1091/2330 train_time:66717ms step_avg:61.15ms
step:1092/2330 train_time:66780ms step_avg:61.15ms
step:1093/2330 train_time:66840ms step_avg:61.15ms
step:1094/2330 train_time:66902ms step_avg:61.15ms
step:1095/2330 train_time:66962ms step_avg:61.15ms
step:1096/2330 train_time:67025ms step_avg:61.15ms
step:1097/2330 train_time:67085ms step_avg:61.15ms
step:1098/2330 train_time:67148ms step_avg:61.15ms
step:1099/2330 train_time:67207ms step_avg:61.15ms
step:1100/2330 train_time:67271ms step_avg:61.16ms
step:1101/2330 train_time:67332ms step_avg:61.15ms
step:1102/2330 train_time:67396ms step_avg:61.16ms
step:1103/2330 train_time:67456ms step_avg:61.16ms
step:1104/2330 train_time:67520ms step_avg:61.16ms
step:1105/2330 train_time:67580ms step_avg:61.16ms
step:1106/2330 train_time:67642ms step_avg:61.16ms
step:1107/2330 train_time:67702ms step_avg:61.16ms
step:1108/2330 train_time:67765ms step_avg:61.16ms
step:1109/2330 train_time:67824ms step_avg:61.16ms
step:1110/2330 train_time:67887ms step_avg:61.16ms
step:1111/2330 train_time:67946ms step_avg:61.16ms
step:1112/2330 train_time:68009ms step_avg:61.16ms
step:1113/2330 train_time:68069ms step_avg:61.16ms
step:1114/2330 train_time:68132ms step_avg:61.16ms
step:1115/2330 train_time:68191ms step_avg:61.16ms
step:1116/2330 train_time:68254ms step_avg:61.16ms
step:1117/2330 train_time:68315ms step_avg:61.16ms
step:1118/2330 train_time:68378ms step_avg:61.16ms
step:1119/2330 train_time:68438ms step_avg:61.16ms
step:1120/2330 train_time:68502ms step_avg:61.16ms
step:1121/2330 train_time:68562ms step_avg:61.16ms
step:1122/2330 train_time:68625ms step_avg:61.16ms
step:1123/2330 train_time:68685ms step_avg:61.16ms
step:1124/2330 train_time:68748ms step_avg:61.16ms
step:1125/2330 train_time:68807ms step_avg:61.16ms
step:1126/2330 train_time:68870ms step_avg:61.16ms
step:1127/2330 train_time:68930ms step_avg:61.16ms
step:1128/2330 train_time:68993ms step_avg:61.16ms
step:1129/2330 train_time:69053ms step_avg:61.16ms
step:1130/2330 train_time:69117ms step_avg:61.17ms
step:1131/2330 train_time:69176ms step_avg:61.16ms
step:1132/2330 train_time:69239ms step_avg:61.17ms
step:1133/2330 train_time:69299ms step_avg:61.16ms
step:1134/2330 train_time:69362ms step_avg:61.17ms
step:1135/2330 train_time:69423ms step_avg:61.17ms
step:1136/2330 train_time:69485ms step_avg:61.17ms
step:1137/2330 train_time:69545ms step_avg:61.17ms
step:1138/2330 train_time:69608ms step_avg:61.17ms
step:1139/2330 train_time:69668ms step_avg:61.17ms
step:1140/2330 train_time:69731ms step_avg:61.17ms
step:1141/2330 train_time:69791ms step_avg:61.17ms
step:1142/2330 train_time:69854ms step_avg:61.17ms
step:1143/2330 train_time:69914ms step_avg:61.17ms
step:1144/2330 train_time:69977ms step_avg:61.17ms
step:1145/2330 train_time:70037ms step_avg:61.17ms
step:1146/2330 train_time:70101ms step_avg:61.17ms
step:1147/2330 train_time:70160ms step_avg:61.17ms
step:1148/2330 train_time:70224ms step_avg:61.17ms
step:1149/2330 train_time:70283ms step_avg:61.17ms
step:1150/2330 train_time:70346ms step_avg:61.17ms
step:1151/2330 train_time:70406ms step_avg:61.17ms
step:1152/2330 train_time:70469ms step_avg:61.17ms
step:1153/2330 train_time:70528ms step_avg:61.17ms
step:1154/2330 train_time:70592ms step_avg:61.17ms
step:1155/2330 train_time:70652ms step_avg:61.17ms
step:1156/2330 train_time:70715ms step_avg:61.17ms
step:1157/2330 train_time:70775ms step_avg:61.17ms
step:1158/2330 train_time:70839ms step_avg:61.17ms
step:1159/2330 train_time:70899ms step_avg:61.17ms
step:1160/2330 train_time:70963ms step_avg:61.17ms
step:1161/2330 train_time:71022ms step_avg:61.17ms
step:1162/2330 train_time:71085ms step_avg:61.17ms
step:1163/2330 train_time:71144ms step_avg:61.17ms
step:1164/2330 train_time:71207ms step_avg:61.17ms
step:1165/2330 train_time:71268ms step_avg:61.17ms
step:1166/2330 train_time:71331ms step_avg:61.18ms
step:1167/2330 train_time:71391ms step_avg:61.17ms
step:1168/2330 train_time:71454ms step_avg:61.18ms
step:1169/2330 train_time:71514ms step_avg:61.18ms
step:1170/2330 train_time:71578ms step_avg:61.18ms
step:1171/2330 train_time:71638ms step_avg:61.18ms
step:1172/2330 train_time:71701ms step_avg:61.18ms
step:1173/2330 train_time:71762ms step_avg:61.18ms
step:1174/2330 train_time:71825ms step_avg:61.18ms
step:1175/2330 train_time:71884ms step_avg:61.18ms
step:1176/2330 train_time:71947ms step_avg:61.18ms
step:1177/2330 train_time:72007ms step_avg:61.18ms
step:1178/2330 train_time:72070ms step_avg:61.18ms
step:1179/2330 train_time:72130ms step_avg:61.18ms
step:1180/2330 train_time:72193ms step_avg:61.18ms
step:1181/2330 train_time:72254ms step_avg:61.18ms
step:1182/2330 train_time:72318ms step_avg:61.18ms
step:1183/2330 train_time:72378ms step_avg:61.18ms
step:1184/2330 train_time:72441ms step_avg:61.18ms
step:1185/2330 train_time:72500ms step_avg:61.18ms
step:1186/2330 train_time:72563ms step_avg:61.18ms
step:1187/2330 train_time:72623ms step_avg:61.18ms
step:1188/2330 train_time:72686ms step_avg:61.18ms
step:1189/2330 train_time:72746ms step_avg:61.18ms
step:1190/2330 train_time:72809ms step_avg:61.18ms
step:1191/2330 train_time:72869ms step_avg:61.18ms
step:1192/2330 train_time:72932ms step_avg:61.18ms
step:1193/2330 train_time:72991ms step_avg:61.18ms
step:1194/2330 train_time:73055ms step_avg:61.18ms
step:1195/2330 train_time:73115ms step_avg:61.18ms
step:1196/2330 train_time:73178ms step_avg:61.19ms
step:1197/2330 train_time:73238ms step_avg:61.18ms
step:1198/2330 train_time:73301ms step_avg:61.19ms
step:1199/2330 train_time:73361ms step_avg:61.19ms
step:1200/2330 train_time:73424ms step_avg:61.19ms
step:1201/2330 train_time:73484ms step_avg:61.19ms
step:1202/2330 train_time:73547ms step_avg:61.19ms
step:1203/2330 train_time:73606ms step_avg:61.19ms
step:1204/2330 train_time:73669ms step_avg:61.19ms
step:1205/2330 train_time:73729ms step_avg:61.19ms
step:1206/2330 train_time:73793ms step_avg:61.19ms
step:1207/2330 train_time:73853ms step_avg:61.19ms
step:1208/2330 train_time:73917ms step_avg:61.19ms
step:1209/2330 train_time:73978ms step_avg:61.19ms
step:1210/2330 train_time:74042ms step_avg:61.19ms
step:1211/2330 train_time:74101ms step_avg:61.19ms
step:1212/2330 train_time:74165ms step_avg:61.19ms
step:1213/2330 train_time:74224ms step_avg:61.19ms
step:1214/2330 train_time:74287ms step_avg:61.19ms
step:1215/2330 train_time:74347ms step_avg:61.19ms
step:1216/2330 train_time:74410ms step_avg:61.19ms
step:1217/2330 train_time:74470ms step_avg:61.19ms
step:1218/2330 train_time:74533ms step_avg:61.19ms
step:1219/2330 train_time:74593ms step_avg:61.19ms
step:1220/2330 train_time:74657ms step_avg:61.19ms
step:1221/2330 train_time:74717ms step_avg:61.19ms
step:1222/2330 train_time:74780ms step_avg:61.19ms
step:1223/2330 train_time:74840ms step_avg:61.19ms
step:1224/2330 train_time:74903ms step_avg:61.19ms
step:1225/2330 train_time:74963ms step_avg:61.19ms
step:1226/2330 train_time:75026ms step_avg:61.20ms
step:1227/2330 train_time:75086ms step_avg:61.19ms
step:1228/2330 train_time:75149ms step_avg:61.20ms
step:1229/2330 train_time:75209ms step_avg:61.20ms
step:1230/2330 train_time:75273ms step_avg:61.20ms
step:1231/2330 train_time:75333ms step_avg:61.20ms
step:1232/2330 train_time:75396ms step_avg:61.20ms
step:1233/2330 train_time:75457ms step_avg:61.20ms
step:1234/2330 train_time:75520ms step_avg:61.20ms
step:1235/2330 train_time:75579ms step_avg:61.20ms
step:1236/2330 train_time:75642ms step_avg:61.20ms
step:1237/2330 train_time:75702ms step_avg:61.20ms
step:1238/2330 train_time:75765ms step_avg:61.20ms
step:1239/2330 train_time:75825ms step_avg:61.20ms
step:1240/2330 train_time:75888ms step_avg:61.20ms
step:1241/2330 train_time:75948ms step_avg:61.20ms
step:1242/2330 train_time:76012ms step_avg:61.20ms
step:1243/2330 train_time:76072ms step_avg:61.20ms
step:1244/2330 train_time:76136ms step_avg:61.20ms
step:1245/2330 train_time:76196ms step_avg:61.20ms
step:1246/2330 train_time:76259ms step_avg:61.20ms
step:1247/2330 train_time:76320ms step_avg:61.20ms
step:1248/2330 train_time:76383ms step_avg:61.20ms
step:1249/2330 train_time:76443ms step_avg:61.20ms
step:1250/2330 train_time:76506ms step_avg:61.20ms
step:1250/2330 val_loss:3.5196 train_time:76570ms step_avg:61.26ms
step:1251/2330 train_time:76594ms step_avg:61.23ms
step:1252/2330 train_time:76632ms step_avg:61.21ms
step:1253/2330 train_time:76698ms step_avg:61.21ms
step:1254/2330 train_time:76764ms step_avg:61.22ms
step:1255/2330 train_time:76824ms step_avg:61.21ms
step:1256/2330 train_time:76887ms step_avg:61.22ms
step:1257/2330 train_time:76946ms step_avg:61.21ms
step:1258/2330 train_time:77009ms step_avg:61.22ms
step:1259/2330 train_time:77068ms step_avg:61.21ms
step:1260/2330 train_time:77130ms step_avg:61.21ms
step:1261/2330 train_time:77190ms step_avg:61.21ms
step:1262/2330 train_time:77252ms step_avg:61.21ms
step:1263/2330 train_time:77311ms step_avg:61.21ms
step:1264/2330 train_time:77373ms step_avg:61.21ms
step:1265/2330 train_time:77432ms step_avg:61.21ms
step:1266/2330 train_time:77495ms step_avg:61.21ms
step:1267/2330 train_time:77555ms step_avg:61.21ms
step:1268/2330 train_time:77619ms step_avg:61.21ms
step:1269/2330 train_time:77681ms step_avg:61.21ms
step:1270/2330 train_time:77745ms step_avg:61.22ms
step:1271/2330 train_time:77805ms step_avg:61.22ms
step:1272/2330 train_time:77868ms step_avg:61.22ms
step:1273/2330 train_time:77928ms step_avg:61.22ms
step:1274/2330 train_time:77990ms step_avg:61.22ms
step:1275/2330 train_time:78050ms step_avg:61.22ms
step:1276/2330 train_time:78112ms step_avg:61.22ms
step:1277/2330 train_time:78172ms step_avg:61.21ms
step:1278/2330 train_time:78234ms step_avg:61.22ms
step:1279/2330 train_time:78293ms step_avg:61.21ms
step:1280/2330 train_time:78356ms step_avg:61.22ms
step:1281/2330 train_time:78415ms step_avg:61.21ms
step:1282/2330 train_time:78478ms step_avg:61.22ms
step:1283/2330 train_time:78538ms step_avg:61.21ms
step:1284/2330 train_time:78602ms step_avg:61.22ms
step:1285/2330 train_time:78663ms step_avg:61.22ms
step:1286/2330 train_time:78726ms step_avg:61.22ms
step:1287/2330 train_time:78787ms step_avg:61.22ms
step:1288/2330 train_time:78850ms step_avg:61.22ms
step:1289/2330 train_time:78910ms step_avg:61.22ms
step:1290/2330 train_time:78972ms step_avg:61.22ms
step:1291/2330 train_time:79032ms step_avg:61.22ms
step:1292/2330 train_time:79095ms step_avg:61.22ms
step:1293/2330 train_time:79155ms step_avg:61.22ms
step:1294/2330 train_time:79217ms step_avg:61.22ms
step:1295/2330 train_time:79277ms step_avg:61.22ms
step:1296/2330 train_time:79340ms step_avg:61.22ms
step:1297/2330 train_time:79399ms step_avg:61.22ms
step:1298/2330 train_time:79462ms step_avg:61.22ms
step:1299/2330 train_time:79522ms step_avg:61.22ms
step:1300/2330 train_time:79585ms step_avg:61.22ms
step:1301/2330 train_time:79646ms step_avg:61.22ms
step:1302/2330 train_time:79709ms step_avg:61.22ms
step:1303/2330 train_time:79770ms step_avg:61.22ms
step:1304/2330 train_time:79833ms step_avg:61.22ms
step:1305/2330 train_time:79893ms step_avg:61.22ms
step:1306/2330 train_time:79957ms step_avg:61.22ms
step:1307/2330 train_time:80016ms step_avg:61.22ms
step:1308/2330 train_time:80079ms step_avg:61.22ms
step:1309/2330 train_time:80139ms step_avg:61.22ms
step:1310/2330 train_time:80202ms step_avg:61.22ms
step:1311/2330 train_time:80261ms step_avg:61.22ms
step:1312/2330 train_time:80325ms step_avg:61.22ms
step:1313/2330 train_time:80384ms step_avg:61.22ms
step:1314/2330 train_time:80448ms step_avg:61.22ms
step:1315/2330 train_time:80507ms step_avg:61.22ms
step:1316/2330 train_time:80570ms step_avg:61.22ms
step:1317/2330 train_time:80630ms step_avg:61.22ms
step:1318/2330 train_time:80694ms step_avg:61.22ms
step:1319/2330 train_time:80753ms step_avg:61.22ms
step:1320/2330 train_time:80817ms step_avg:61.22ms
step:1321/2330 train_time:80877ms step_avg:61.22ms
step:1322/2330 train_time:80940ms step_avg:61.23ms
step:1323/2330 train_time:81001ms step_avg:61.23ms
step:1324/2330 train_time:81065ms step_avg:61.23ms
step:1325/2330 train_time:81125ms step_avg:61.23ms
step:1326/2330 train_time:81187ms step_avg:61.23ms
step:1327/2330 train_time:81247ms step_avg:61.23ms
step:1328/2330 train_time:81309ms step_avg:61.23ms
step:1329/2330 train_time:81369ms step_avg:61.23ms
step:1330/2330 train_time:81432ms step_avg:61.23ms
step:1331/2330 train_time:81491ms step_avg:61.23ms
step:1332/2330 train_time:81554ms step_avg:61.23ms
step:1333/2330 train_time:81614ms step_avg:61.23ms
step:1334/2330 train_time:81677ms step_avg:61.23ms
step:1335/2330 train_time:81737ms step_avg:61.23ms
step:1336/2330 train_time:81800ms step_avg:61.23ms
step:1337/2330 train_time:81860ms step_avg:61.23ms
step:1338/2330 train_time:81923ms step_avg:61.23ms
step:1339/2330 train_time:81985ms step_avg:61.23ms
step:1340/2330 train_time:82048ms step_avg:61.23ms
step:1341/2330 train_time:82108ms step_avg:61.23ms
step:1342/2330 train_time:82170ms step_avg:61.23ms
step:1343/2330 train_time:82230ms step_avg:61.23ms
step:1344/2330 train_time:82292ms step_avg:61.23ms
step:1345/2330 train_time:82352ms step_avg:61.23ms
step:1346/2330 train_time:82414ms step_avg:61.23ms
step:1347/2330 train_time:82474ms step_avg:61.23ms
step:1348/2330 train_time:82537ms step_avg:61.23ms
step:1349/2330 train_time:82598ms step_avg:61.23ms
step:1350/2330 train_time:82660ms step_avg:61.23ms
step:1351/2330 train_time:82721ms step_avg:61.23ms
step:1352/2330 train_time:82784ms step_avg:61.23ms
step:1353/2330 train_time:82844ms step_avg:61.23ms
step:1354/2330 train_time:82906ms step_avg:61.23ms
step:1355/2330 train_time:82966ms step_avg:61.23ms
step:1356/2330 train_time:83029ms step_avg:61.23ms
step:1357/2330 train_time:83089ms step_avg:61.23ms
step:1358/2330 train_time:83152ms step_avg:61.23ms
step:1359/2330 train_time:83211ms step_avg:61.23ms
step:1360/2330 train_time:83273ms step_avg:61.23ms
step:1361/2330 train_time:83333ms step_avg:61.23ms
step:1362/2330 train_time:83395ms step_avg:61.23ms
step:1363/2330 train_time:83455ms step_avg:61.23ms
step:1364/2330 train_time:83518ms step_avg:61.23ms
step:1365/2330 train_time:83577ms step_avg:61.23ms
step:1366/2330 train_time:83640ms step_avg:61.23ms
step:1367/2330 train_time:83700ms step_avg:61.23ms
step:1368/2330 train_time:83763ms step_avg:61.23ms
step:1369/2330 train_time:83823ms step_avg:61.23ms
step:1370/2330 train_time:83886ms step_avg:61.23ms
step:1371/2330 train_time:83946ms step_avg:61.23ms
step:1372/2330 train_time:84009ms step_avg:61.23ms
step:1373/2330 train_time:84069ms step_avg:61.23ms
step:1374/2330 train_time:84132ms step_avg:61.23ms
step:1375/2330 train_time:84191ms step_avg:61.23ms
step:1376/2330 train_time:84254ms step_avg:61.23ms
step:1377/2330 train_time:84314ms step_avg:61.23ms
step:1378/2330 train_time:84376ms step_avg:61.23ms
step:1379/2330 train_time:84436ms step_avg:61.23ms
step:1380/2330 train_time:84499ms step_avg:61.23ms
step:1381/2330 train_time:84559ms step_avg:61.23ms
step:1382/2330 train_time:84622ms step_avg:61.23ms
step:1383/2330 train_time:84681ms step_avg:61.23ms
step:1384/2330 train_time:84744ms step_avg:61.23ms
step:1385/2330 train_time:84805ms step_avg:61.23ms
step:1386/2330 train_time:84868ms step_avg:61.23ms
step:1387/2330 train_time:84928ms step_avg:61.23ms
step:1388/2330 train_time:84992ms step_avg:61.23ms
step:1389/2330 train_time:85051ms step_avg:61.23ms
step:1390/2330 train_time:85114ms step_avg:61.23ms
step:1391/2330 train_time:85174ms step_avg:61.23ms
step:1392/2330 train_time:85237ms step_avg:61.23ms
step:1393/2330 train_time:85297ms step_avg:61.23ms
step:1394/2330 train_time:85360ms step_avg:61.23ms
step:1395/2330 train_time:85420ms step_avg:61.23ms
step:1396/2330 train_time:85483ms step_avg:61.23ms
step:1397/2330 train_time:85542ms step_avg:61.23ms
step:1398/2330 train_time:85605ms step_avg:61.23ms
step:1399/2330 train_time:85664ms step_avg:61.23ms
step:1400/2330 train_time:85727ms step_avg:61.23ms
step:1401/2330 train_time:85787ms step_avg:61.23ms
step:1402/2330 train_time:85850ms step_avg:61.23ms
step:1403/2330 train_time:85909ms step_avg:61.23ms
step:1404/2330 train_time:85972ms step_avg:61.23ms
step:1405/2330 train_time:86032ms step_avg:61.23ms
step:1406/2330 train_time:86095ms step_avg:61.23ms
step:1407/2330 train_time:86154ms step_avg:61.23ms
step:1408/2330 train_time:86217ms step_avg:61.23ms
step:1409/2330 train_time:86276ms step_avg:61.23ms
step:1410/2330 train_time:86340ms step_avg:61.23ms
step:1411/2330 train_time:86401ms step_avg:61.23ms
step:1412/2330 train_time:86463ms step_avg:61.23ms
step:1413/2330 train_time:86523ms step_avg:61.23ms
step:1414/2330 train_time:86586ms step_avg:61.24ms
step:1415/2330 train_time:86646ms step_avg:61.23ms
step:1416/2330 train_time:86709ms step_avg:61.24ms
step:1417/2330 train_time:86769ms step_avg:61.23ms
step:1418/2330 train_time:86831ms step_avg:61.23ms
step:1419/2330 train_time:86891ms step_avg:61.23ms
step:1420/2330 train_time:86954ms step_avg:61.24ms
step:1421/2330 train_time:87014ms step_avg:61.23ms
step:1422/2330 train_time:87077ms step_avg:61.24ms
step:1423/2330 train_time:87136ms step_avg:61.23ms
step:1424/2330 train_time:87199ms step_avg:61.24ms
step:1425/2330 train_time:87259ms step_avg:61.23ms
step:1426/2330 train_time:87322ms step_avg:61.24ms
step:1427/2330 train_time:87382ms step_avg:61.23ms
step:1428/2330 train_time:87445ms step_avg:61.24ms
step:1429/2330 train_time:87504ms step_avg:61.23ms
step:1430/2330 train_time:87567ms step_avg:61.24ms
step:1431/2330 train_time:87627ms step_avg:61.23ms
step:1432/2330 train_time:87690ms step_avg:61.24ms
step:1433/2330 train_time:87750ms step_avg:61.24ms
step:1434/2330 train_time:87812ms step_avg:61.24ms
step:1435/2330 train_time:87872ms step_avg:61.23ms
step:1436/2330 train_time:87935ms step_avg:61.24ms
step:1437/2330 train_time:87994ms step_avg:61.23ms
step:1438/2330 train_time:88057ms step_avg:61.24ms
step:1439/2330 train_time:88117ms step_avg:61.23ms
step:1440/2330 train_time:88180ms step_avg:61.24ms
step:1441/2330 train_time:88240ms step_avg:61.24ms
step:1442/2330 train_time:88303ms step_avg:61.24ms
step:1443/2330 train_time:88363ms step_avg:61.24ms
step:1444/2330 train_time:88425ms step_avg:61.24ms
step:1445/2330 train_time:88485ms step_avg:61.24ms
step:1446/2330 train_time:88548ms step_avg:61.24ms
step:1447/2330 train_time:88607ms step_avg:61.24ms
step:1448/2330 train_time:88671ms step_avg:61.24ms
step:1449/2330 train_time:88731ms step_avg:61.24ms
step:1450/2330 train_time:88794ms step_avg:61.24ms
step:1451/2330 train_time:88854ms step_avg:61.24ms
step:1452/2330 train_time:88916ms step_avg:61.24ms
step:1453/2330 train_time:88976ms step_avg:61.24ms
step:1454/2330 train_time:89038ms step_avg:61.24ms
step:1455/2330 train_time:89098ms step_avg:61.24ms
step:1456/2330 train_time:89161ms step_avg:61.24ms
step:1457/2330 train_time:89221ms step_avg:61.24ms
step:1458/2330 train_time:89285ms step_avg:61.24ms
step:1459/2330 train_time:89345ms step_avg:61.24ms
step:1460/2330 train_time:89408ms step_avg:61.24ms
step:1461/2330 train_time:89468ms step_avg:61.24ms
step:1462/2330 train_time:89530ms step_avg:61.24ms
step:1463/2330 train_time:89590ms step_avg:61.24ms
step:1464/2330 train_time:89653ms step_avg:61.24ms
step:1465/2330 train_time:89713ms step_avg:61.24ms
step:1466/2330 train_time:89776ms step_avg:61.24ms
step:1467/2330 train_time:89835ms step_avg:61.24ms
step:1468/2330 train_time:89898ms step_avg:61.24ms
step:1469/2330 train_time:89957ms step_avg:61.24ms
step:1470/2330 train_time:90020ms step_avg:61.24ms
step:1471/2330 train_time:90080ms step_avg:61.24ms
step:1472/2330 train_time:90143ms step_avg:61.24ms
step:1473/2330 train_time:90203ms step_avg:61.24ms
step:1474/2330 train_time:90266ms step_avg:61.24ms
step:1475/2330 train_time:90326ms step_avg:61.24ms
step:1476/2330 train_time:90389ms step_avg:61.24ms
step:1477/2330 train_time:90448ms step_avg:61.24ms
step:1478/2330 train_time:90512ms step_avg:61.24ms
step:1479/2330 train_time:90571ms step_avg:61.24ms
step:1480/2330 train_time:90634ms step_avg:61.24ms
step:1481/2330 train_time:90694ms step_avg:61.24ms
step:1482/2330 train_time:90756ms step_avg:61.24ms
step:1483/2330 train_time:90816ms step_avg:61.24ms
step:1484/2330 train_time:90879ms step_avg:61.24ms
step:1485/2330 train_time:90940ms step_avg:61.24ms
step:1486/2330 train_time:91002ms step_avg:61.24ms
step:1487/2330 train_time:91062ms step_avg:61.24ms
step:1488/2330 train_time:91125ms step_avg:61.24ms
step:1489/2330 train_time:91185ms step_avg:61.24ms
step:1490/2330 train_time:91247ms step_avg:61.24ms
step:1491/2330 train_time:91307ms step_avg:61.24ms
step:1492/2330 train_time:91369ms step_avg:61.24ms
step:1493/2330 train_time:91429ms step_avg:61.24ms
step:1494/2330 train_time:91492ms step_avg:61.24ms
step:1495/2330 train_time:91552ms step_avg:61.24ms
step:1496/2330 train_time:91615ms step_avg:61.24ms
step:1497/2330 train_time:91675ms step_avg:61.24ms
step:1498/2330 train_time:91738ms step_avg:61.24ms
step:1499/2330 train_time:91798ms step_avg:61.24ms
step:1500/2330 train_time:91861ms step_avg:61.24ms
step:1500/2330 val_loss:3.4776 train_time:91926ms step_avg:61.28ms
step:1501/2330 train_time:91949ms step_avg:61.26ms
step:1502/2330 train_time:91987ms step_avg:61.24ms
step:1503/2330 train_time:92052ms step_avg:61.25ms
step:1504/2330 train_time:92118ms step_avg:61.25ms
step:1505/2330 train_time:92178ms step_avg:61.25ms
step:1506/2330 train_time:92241ms step_avg:61.25ms
step:1507/2330 train_time:92300ms step_avg:61.25ms
step:1508/2330 train_time:92362ms step_avg:61.25ms
step:1509/2330 train_time:92422ms step_avg:61.25ms
step:1510/2330 train_time:92484ms step_avg:61.25ms
step:1511/2330 train_time:92543ms step_avg:61.25ms
step:1512/2330 train_time:92604ms step_avg:61.25ms
step:1513/2330 train_time:92664ms step_avg:61.24ms
step:1514/2330 train_time:92726ms step_avg:61.25ms
step:1515/2330 train_time:92785ms step_avg:61.24ms
step:1516/2330 train_time:92849ms step_avg:61.25ms
step:1517/2330 train_time:92910ms step_avg:61.25ms
step:1518/2330 train_time:92975ms step_avg:61.25ms
step:1519/2330 train_time:93036ms step_avg:61.25ms
step:1520/2330 train_time:93100ms step_avg:61.25ms
step:1521/2330 train_time:93160ms step_avg:61.25ms
step:1522/2330 train_time:93223ms step_avg:61.25ms
step:1523/2330 train_time:93283ms step_avg:61.25ms
step:1524/2330 train_time:93345ms step_avg:61.25ms
step:1525/2330 train_time:93404ms step_avg:61.25ms
step:1526/2330 train_time:93467ms step_avg:61.25ms
step:1527/2330 train_time:93526ms step_avg:61.25ms
step:1528/2330 train_time:93589ms step_avg:61.25ms
step:1529/2330 train_time:93648ms step_avg:61.25ms
step:1530/2330 train_time:93711ms step_avg:61.25ms
step:1531/2330 train_time:93771ms step_avg:61.25ms
step:1532/2330 train_time:93834ms step_avg:61.25ms
step:1533/2330 train_time:93895ms step_avg:61.25ms
step:1534/2330 train_time:93959ms step_avg:61.25ms
step:1535/2330 train_time:94021ms step_avg:61.25ms
step:1536/2330 train_time:94085ms step_avg:61.25ms
step:1537/2330 train_time:94147ms step_avg:61.25ms
step:1538/2330 train_time:94211ms step_avg:61.26ms
step:1539/2330 train_time:94271ms step_avg:61.25ms
step:1540/2330 train_time:94335ms step_avg:61.26ms
step:1541/2330 train_time:94395ms step_avg:61.26ms
step:1542/2330 train_time:94459ms step_avg:61.26ms
step:1543/2330 train_time:94520ms step_avg:61.26ms
step:1544/2330 train_time:94583ms step_avg:61.26ms
step:1545/2330 train_time:94642ms step_avg:61.26ms
step:1546/2330 train_time:94705ms step_avg:61.26ms
step:1547/2330 train_time:94765ms step_avg:61.26ms
step:1548/2330 train_time:94828ms step_avg:61.26ms
step:1549/2330 train_time:94889ms step_avg:61.26ms
step:1550/2330 train_time:94953ms step_avg:61.26ms
step:1551/2330 train_time:95014ms step_avg:61.26ms
step:1552/2330 train_time:95077ms step_avg:61.26ms
step:1553/2330 train_time:95139ms step_avg:61.26ms
step:1554/2330 train_time:95202ms step_avg:61.26ms
step:1555/2330 train_time:95263ms step_avg:61.26ms
step:1556/2330 train_time:95326ms step_avg:61.26ms
step:1557/2330 train_time:95387ms step_avg:61.26ms
step:1558/2330 train_time:95450ms step_avg:61.26ms
step:1559/2330 train_time:95510ms step_avg:61.26ms
step:1560/2330 train_time:95574ms step_avg:61.27ms
step:1561/2330 train_time:95634ms step_avg:61.26ms
step:1562/2330 train_time:95697ms step_avg:61.27ms
step:1563/2330 train_time:95757ms step_avg:61.26ms
step:1564/2330 train_time:95820ms step_avg:61.27ms
step:1565/2330 train_time:95881ms step_avg:61.27ms
step:1566/2330 train_time:95944ms step_avg:61.27ms
step:1567/2330 train_time:96005ms step_avg:61.27ms
step:1568/2330 train_time:96069ms step_avg:61.27ms
step:1569/2330 train_time:96129ms step_avg:61.27ms
step:1570/2330 train_time:96192ms step_avg:61.27ms
step:1571/2330 train_time:96253ms step_avg:61.27ms
step:1572/2330 train_time:96316ms step_avg:61.27ms
step:1573/2330 train_time:96377ms step_avg:61.27ms
step:1574/2330 train_time:96440ms step_avg:61.27ms
step:1575/2330 train_time:96501ms step_avg:61.27ms
step:1576/2330 train_time:96564ms step_avg:61.27ms
step:1577/2330 train_time:96624ms step_avg:61.27ms
step:1578/2330 train_time:96687ms step_avg:61.27ms
step:1579/2330 train_time:96748ms step_avg:61.27ms
step:1580/2330 train_time:96811ms step_avg:61.27ms
step:1581/2330 train_time:96871ms step_avg:61.27ms
step:1582/2330 train_time:96934ms step_avg:61.27ms
step:1583/2330 train_time:96995ms step_avg:61.27ms
step:1584/2330 train_time:97059ms step_avg:61.27ms
step:1585/2330 train_time:97119ms step_avg:61.27ms
step:1586/2330 train_time:97182ms step_avg:61.27ms
step:1587/2330 train_time:97242ms step_avg:61.27ms
step:1588/2330 train_time:97307ms step_avg:61.28ms
step:1589/2330 train_time:97368ms step_avg:61.28ms
step:1590/2330 train_time:97432ms step_avg:61.28ms
step:1591/2330 train_time:97492ms step_avg:61.28ms
step:1592/2330 train_time:97555ms step_avg:61.28ms
step:1593/2330 train_time:97616ms step_avg:61.28ms
step:1594/2330 train_time:97680ms step_avg:61.28ms
step:1595/2330 train_time:97741ms step_avg:61.28ms
step:1596/2330 train_time:97803ms step_avg:61.28ms
step:1597/2330 train_time:97864ms step_avg:61.28ms
step:1598/2330 train_time:97927ms step_avg:61.28ms
step:1599/2330 train_time:97988ms step_avg:61.28ms
step:1600/2330 train_time:98052ms step_avg:61.28ms
step:1601/2330 train_time:98112ms step_avg:61.28ms
step:1602/2330 train_time:98175ms step_avg:61.28ms
step:1603/2330 train_time:98235ms step_avg:61.28ms
step:1604/2330 train_time:98299ms step_avg:61.28ms
step:1605/2330 train_time:98359ms step_avg:61.28ms
step:1606/2330 train_time:98423ms step_avg:61.28ms
step:1607/2330 train_time:98484ms step_avg:61.28ms
step:1608/2330 train_time:98547ms step_avg:61.29ms
step:1609/2330 train_time:98608ms step_avg:61.29ms
step:1610/2330 train_time:98671ms step_avg:61.29ms
step:1611/2330 train_time:98732ms step_avg:61.29ms
step:1612/2330 train_time:98794ms step_avg:61.29ms
step:1613/2330 train_time:98855ms step_avg:61.29ms
step:1614/2330 train_time:98920ms step_avg:61.29ms
step:1615/2330 train_time:98980ms step_avg:61.29ms
step:1616/2330 train_time:99043ms step_avg:61.29ms
step:1617/2330 train_time:99104ms step_avg:61.29ms
step:1618/2330 train_time:99167ms step_avg:61.29ms
step:1619/2330 train_time:99229ms step_avg:61.29ms
step:1620/2330 train_time:99292ms step_avg:61.29ms
step:1621/2330 train_time:99352ms step_avg:61.29ms
step:1622/2330 train_time:99415ms step_avg:61.29ms
step:1623/2330 train_time:99476ms step_avg:61.29ms
step:1624/2330 train_time:99540ms step_avg:61.29ms
step:1625/2330 train_time:99600ms step_avg:61.29ms
step:1626/2330 train_time:99663ms step_avg:61.29ms
step:1627/2330 train_time:99723ms step_avg:61.29ms
step:1628/2330 train_time:99787ms step_avg:61.29ms
step:1629/2330 train_time:99848ms step_avg:61.29ms
step:1630/2330 train_time:99912ms step_avg:61.30ms
step:1631/2330 train_time:99973ms step_avg:61.30ms
step:1632/2330 train_time:100036ms step_avg:61.30ms
step:1633/2330 train_time:100097ms step_avg:61.30ms
step:1634/2330 train_time:100161ms step_avg:61.30ms
step:1635/2330 train_time:100221ms step_avg:61.30ms
step:1636/2330 train_time:100284ms step_avg:61.30ms
step:1637/2330 train_time:100344ms step_avg:61.30ms
step:1638/2330 train_time:100407ms step_avg:61.30ms
step:1639/2330 train_time:100468ms step_avg:61.30ms
step:1640/2330 train_time:100532ms step_avg:61.30ms
step:1641/2330 train_time:100592ms step_avg:61.30ms
step:1642/2330 train_time:100655ms step_avg:61.30ms
step:1643/2330 train_time:100716ms step_avg:61.30ms
step:1644/2330 train_time:100779ms step_avg:61.30ms
step:1645/2330 train_time:100839ms step_avg:61.30ms
step:1646/2330 train_time:100902ms step_avg:61.30ms
step:1647/2330 train_time:100963ms step_avg:61.30ms
step:1648/2330 train_time:101026ms step_avg:61.30ms
step:1649/2330 train_time:101086ms step_avg:61.30ms
step:1650/2330 train_time:101150ms step_avg:61.30ms
step:1651/2330 train_time:101211ms step_avg:61.30ms
step:1652/2330 train_time:101274ms step_avg:61.30ms
step:1653/2330 train_time:101335ms step_avg:61.30ms
step:1654/2330 train_time:101398ms step_avg:61.30ms
step:1655/2330 train_time:101459ms step_avg:61.30ms
step:1656/2330 train_time:101522ms step_avg:61.31ms
step:1657/2330 train_time:101582ms step_avg:61.30ms
step:1658/2330 train_time:101645ms step_avg:61.31ms
step:1659/2330 train_time:101706ms step_avg:61.31ms
step:1660/2330 train_time:101769ms step_avg:61.31ms
step:1661/2330 train_time:101830ms step_avg:61.31ms
step:1662/2330 train_time:101893ms step_avg:61.31ms
step:1663/2330 train_time:101953ms step_avg:61.31ms
step:1664/2330 train_time:102016ms step_avg:61.31ms
step:1665/2330 train_time:102077ms step_avg:61.31ms
step:1666/2330 train_time:102141ms step_avg:61.31ms
step:1667/2330 train_time:102201ms step_avg:61.31ms
step:1668/2330 train_time:102264ms step_avg:61.31ms
step:1669/2330 train_time:102324ms step_avg:61.31ms
step:1670/2330 train_time:102388ms step_avg:61.31ms
step:1671/2330 train_time:102449ms step_avg:61.31ms
step:1672/2330 train_time:102513ms step_avg:61.31ms
step:1673/2330 train_time:102573ms step_avg:61.31ms
step:1674/2330 train_time:102636ms step_avg:61.31ms
step:1675/2330 train_time:102697ms step_avg:61.31ms
step:1676/2330 train_time:102760ms step_avg:61.31ms
step:1677/2330 train_time:102821ms step_avg:61.31ms
step:1678/2330 train_time:102884ms step_avg:61.31ms
step:1679/2330 train_time:102944ms step_avg:61.31ms
step:1680/2330 train_time:103007ms step_avg:61.31ms
step:1681/2330 train_time:103068ms step_avg:61.31ms
step:1682/2330 train_time:103131ms step_avg:61.31ms
step:1683/2330 train_time:103192ms step_avg:61.31ms
step:1684/2330 train_time:103255ms step_avg:61.32ms
step:1685/2330 train_time:103316ms step_avg:61.31ms
step:1686/2330 train_time:103379ms step_avg:61.32ms
step:1687/2330 train_time:103440ms step_avg:61.32ms
step:1688/2330 train_time:103503ms step_avg:61.32ms
step:1689/2330 train_time:103563ms step_avg:61.32ms
step:1690/2330 train_time:103626ms step_avg:61.32ms
step:1691/2330 train_time:103687ms step_avg:61.32ms
step:1692/2330 train_time:103751ms step_avg:61.32ms
step:1693/2330 train_time:103811ms step_avg:61.32ms
step:1694/2330 train_time:103874ms step_avg:61.32ms
step:1695/2330 train_time:103934ms step_avg:61.32ms
step:1696/2330 train_time:103997ms step_avg:61.32ms
step:1697/2330 train_time:104057ms step_avg:61.32ms
step:1698/2330 train_time:104121ms step_avg:61.32ms
step:1699/2330 train_time:104181ms step_avg:61.32ms
step:1700/2330 train_time:104245ms step_avg:61.32ms
step:1701/2330 train_time:104305ms step_avg:61.32ms
step:1702/2330 train_time:104369ms step_avg:61.32ms
step:1703/2330 train_time:104428ms step_avg:61.32ms
step:1704/2330 train_time:104492ms step_avg:61.32ms
step:1705/2330 train_time:104552ms step_avg:61.32ms
step:1706/2330 train_time:104616ms step_avg:61.32ms
step:1707/2330 train_time:104677ms step_avg:61.32ms
step:1708/2330 train_time:104740ms step_avg:61.32ms
step:1709/2330 train_time:104800ms step_avg:61.32ms
step:1710/2330 train_time:104863ms step_avg:61.32ms
step:1711/2330 train_time:104924ms step_avg:61.32ms
step:1712/2330 train_time:104987ms step_avg:61.32ms
step:1713/2330 train_time:105048ms step_avg:61.32ms
step:1714/2330 train_time:105113ms step_avg:61.33ms
step:1715/2330 train_time:105173ms step_avg:61.33ms
step:1716/2330 train_time:105236ms step_avg:61.33ms
step:1717/2330 train_time:105297ms step_avg:61.33ms
step:1718/2330 train_time:105360ms step_avg:61.33ms
step:1719/2330 train_time:105420ms step_avg:61.33ms
step:1720/2330 train_time:105483ms step_avg:61.33ms
step:1721/2330 train_time:105543ms step_avg:61.33ms
step:1722/2330 train_time:105607ms step_avg:61.33ms
step:1723/2330 train_time:105668ms step_avg:61.33ms
step:1724/2330 train_time:105731ms step_avg:61.33ms
step:1725/2330 train_time:105791ms step_avg:61.33ms
step:1726/2330 train_time:105855ms step_avg:61.33ms
step:1727/2330 train_time:105915ms step_avg:61.33ms
step:1728/2330 train_time:105979ms step_avg:61.33ms
step:1729/2330 train_time:106040ms step_avg:61.33ms
step:1730/2330 train_time:106103ms step_avg:61.33ms
step:1731/2330 train_time:106163ms step_avg:61.33ms
step:1732/2330 train_time:106227ms step_avg:61.33ms
step:1733/2330 train_time:106287ms step_avg:61.33ms
step:1734/2330 train_time:106352ms step_avg:61.33ms
step:1735/2330 train_time:106412ms step_avg:61.33ms
step:1736/2330 train_time:106475ms step_avg:61.33ms
step:1737/2330 train_time:106536ms step_avg:61.33ms
step:1738/2330 train_time:106599ms step_avg:61.33ms
step:1739/2330 train_time:106660ms step_avg:61.33ms
step:1740/2330 train_time:106724ms step_avg:61.34ms
step:1741/2330 train_time:106784ms step_avg:61.33ms
step:1742/2330 train_time:106847ms step_avg:61.34ms
step:1743/2330 train_time:106908ms step_avg:61.34ms
step:1744/2330 train_time:106971ms step_avg:61.34ms
step:1745/2330 train_time:107031ms step_avg:61.34ms
step:1746/2330 train_time:107095ms step_avg:61.34ms
step:1747/2330 train_time:107156ms step_avg:61.34ms
step:1748/2330 train_time:107220ms step_avg:61.34ms
step:1749/2330 train_time:107280ms step_avg:61.34ms
step:1750/2330 train_time:107343ms step_avg:61.34ms
step:1750/2330 val_loss:3.4363 train_time:107408ms step_avg:61.38ms
step:1751/2330 train_time:107431ms step_avg:61.35ms
step:1752/2330 train_time:107468ms step_avg:61.34ms
step:1753/2330 train_time:107534ms step_avg:61.34ms
step:1754/2330 train_time:107600ms step_avg:61.35ms
step:1755/2330 train_time:107661ms step_avg:61.35ms
step:1756/2330 train_time:107724ms step_avg:61.35ms
step:1757/2330 train_time:107785ms step_avg:61.35ms
step:1758/2330 train_time:107848ms step_avg:61.35ms
step:1759/2330 train_time:107908ms step_avg:61.35ms
step:1760/2330 train_time:107970ms step_avg:61.35ms
step:1761/2330 train_time:108030ms step_avg:61.35ms
step:1762/2330 train_time:108093ms step_avg:61.35ms
step:1763/2330 train_time:108152ms step_avg:61.35ms
step:1764/2330 train_time:108214ms step_avg:61.35ms
step:1765/2330 train_time:108274ms step_avg:61.34ms
step:1766/2330 train_time:108339ms step_avg:61.35ms
step:1767/2330 train_time:108400ms step_avg:61.35ms
step:1768/2330 train_time:108465ms step_avg:61.35ms
step:1769/2330 train_time:108527ms step_avg:61.35ms
step:1770/2330 train_time:108591ms step_avg:61.35ms
step:1771/2330 train_time:108651ms step_avg:61.35ms
step:1772/2330 train_time:108715ms step_avg:61.35ms
step:1773/2330 train_time:108776ms step_avg:61.35ms
step:1774/2330 train_time:108839ms step_avg:61.35ms
step:1775/2330 train_time:108900ms step_avg:61.35ms
step:1776/2330 train_time:108963ms step_avg:61.35ms
step:1777/2330 train_time:109023ms step_avg:61.35ms
step:1778/2330 train_time:109087ms step_avg:61.35ms
step:1779/2330 train_time:109147ms step_avg:61.35ms
step:1780/2330 train_time:109210ms step_avg:61.35ms
step:1781/2330 train_time:109271ms step_avg:61.35ms
step:1782/2330 train_time:109335ms step_avg:61.36ms
step:1783/2330 train_time:109396ms step_avg:61.35ms
step:1784/2330 train_time:109460ms step_avg:61.36ms
step:1785/2330 train_time:109520ms step_avg:61.36ms
step:1786/2330 train_time:109585ms step_avg:61.36ms
step:1787/2330 train_time:109646ms step_avg:61.36ms
step:1788/2330 train_time:109711ms step_avg:61.36ms
step:1789/2330 train_time:109771ms step_avg:61.36ms
step:1790/2330 train_time:109834ms step_avg:61.36ms
step:1791/2330 train_time:109894ms step_avg:61.36ms
step:1792/2330 train_time:109957ms step_avg:61.36ms
step:1793/2330 train_time:110017ms step_avg:61.36ms
step:1794/2330 train_time:110081ms step_avg:61.36ms
step:1795/2330 train_time:110141ms step_avg:61.36ms
step:1796/2330 train_time:110205ms step_avg:61.36ms
step:1797/2330 train_time:110266ms step_avg:61.36ms
step:1798/2330 train_time:110329ms step_avg:61.36ms
step:1799/2330 train_time:110390ms step_avg:61.36ms
step:1800/2330 train_time:110454ms step_avg:61.36ms
step:1801/2330 train_time:110514ms step_avg:61.36ms
step:1802/2330 train_time:110578ms step_avg:61.36ms
step:1803/2330 train_time:110638ms step_avg:61.36ms
step:1804/2330 train_time:110702ms step_avg:61.36ms
step:1805/2330 train_time:110763ms step_avg:61.36ms
step:1806/2330 train_time:110826ms step_avg:61.37ms
step:1807/2330 train_time:110887ms step_avg:61.37ms
step:1808/2330 train_time:110951ms step_avg:61.37ms
step:1809/2330 train_time:111011ms step_avg:61.37ms
step:1810/2330 train_time:111073ms step_avg:61.37ms
step:1811/2330 train_time:111134ms step_avg:61.37ms
step:1812/2330 train_time:111198ms step_avg:61.37ms
step:1813/2330 train_time:111258ms step_avg:61.37ms
step:1814/2330 train_time:111321ms step_avg:61.37ms
step:1815/2330 train_time:111382ms step_avg:61.37ms
step:1816/2330 train_time:111447ms step_avg:61.37ms
step:1817/2330 train_time:111507ms step_avg:61.37ms
step:1818/2330 train_time:111571ms step_avg:61.37ms
step:1819/2330 train_time:111631ms step_avg:61.37ms
step:1820/2330 train_time:111695ms step_avg:61.37ms
step:1821/2330 train_time:111755ms step_avg:61.37ms
step:1822/2330 train_time:111818ms step_avg:61.37ms
step:1823/2330 train_time:111878ms step_avg:61.37ms
step:1824/2330 train_time:111941ms step_avg:61.37ms
step:1825/2330 train_time:112001ms step_avg:61.37ms
step:1826/2330 train_time:112065ms step_avg:61.37ms
step:1827/2330 train_time:112126ms step_avg:61.37ms
step:1828/2330 train_time:112190ms step_avg:61.37ms
step:1829/2330 train_time:112250ms step_avg:61.37ms
step:1830/2330 train_time:112313ms step_avg:61.37ms
step:1831/2330 train_time:112374ms step_avg:61.37ms
step:1832/2330 train_time:112437ms step_avg:61.37ms
step:1833/2330 train_time:112498ms step_avg:61.37ms
step:1834/2330 train_time:112561ms step_avg:61.37ms
step:1835/2330 train_time:112622ms step_avg:61.37ms
step:1836/2330 train_time:112687ms step_avg:61.38ms
step:1837/2330 train_time:112747ms step_avg:61.38ms
step:1838/2330 train_time:112811ms step_avg:61.38ms
step:1839/2330 train_time:112871ms step_avg:61.38ms
step:1840/2330 train_time:112936ms step_avg:61.38ms
step:1841/2330 train_time:112995ms step_avg:61.38ms
step:1842/2330 train_time:113058ms step_avg:61.38ms
step:1843/2330 train_time:113119ms step_avg:61.38ms
step:1844/2330 train_time:113182ms step_avg:61.38ms
step:1845/2330 train_time:113242ms step_avg:61.38ms
step:1846/2330 train_time:113306ms step_avg:61.38ms
step:1847/2330 train_time:113366ms step_avg:61.38ms
step:1848/2330 train_time:113430ms step_avg:61.38ms
step:1849/2330 train_time:113490ms step_avg:61.38ms
step:1850/2330 train_time:113554ms step_avg:61.38ms
step:1851/2330 train_time:113614ms step_avg:61.38ms
step:1852/2330 train_time:113678ms step_avg:61.38ms
step:1853/2330 train_time:113738ms step_avg:61.38ms
step:1854/2330 train_time:113801ms step_avg:61.38ms
step:1855/2330 train_time:113862ms step_avg:61.38ms
step:1856/2330 train_time:113925ms step_avg:61.38ms
step:1857/2330 train_time:113986ms step_avg:61.38ms
step:1858/2330 train_time:114050ms step_avg:61.38ms
step:1859/2330 train_time:114110ms step_avg:61.38ms
step:1860/2330 train_time:114173ms step_avg:61.38ms
step:1861/2330 train_time:114234ms step_avg:61.38ms
step:1862/2330 train_time:114297ms step_avg:61.38ms
step:1863/2330 train_time:114357ms step_avg:61.38ms
step:1864/2330 train_time:114420ms step_avg:61.38ms
step:1865/2330 train_time:114481ms step_avg:61.38ms
step:1866/2330 train_time:114544ms step_avg:61.38ms
step:1867/2330 train_time:114605ms step_avg:61.38ms
step:1868/2330 train_time:114669ms step_avg:61.39ms
step:1869/2330 train_time:114729ms step_avg:61.39ms
step:1870/2330 train_time:114793ms step_avg:61.39ms
step:1871/2330 train_time:114853ms step_avg:61.39ms
step:1872/2330 train_time:114916ms step_avg:61.39ms
step:1873/2330 train_time:114977ms step_avg:61.39ms
step:1874/2330 train_time:115039ms step_avg:61.39ms
step:1875/2330 train_time:115100ms step_avg:61.39ms
step:1876/2330 train_time:115164ms step_avg:61.39ms
step:1877/2330 train_time:115224ms step_avg:61.39ms
step:1878/2330 train_time:115288ms step_avg:61.39ms
step:1879/2330 train_time:115348ms step_avg:61.39ms
step:1880/2330 train_time:115412ms step_avg:61.39ms
step:1881/2330 train_time:115472ms step_avg:61.39ms
step:1882/2330 train_time:115536ms step_avg:61.39ms
step:1883/2330 train_time:115597ms step_avg:61.39ms
step:1884/2330 train_time:115660ms step_avg:61.39ms
step:1885/2330 train_time:115720ms step_avg:61.39ms
step:1886/2330 train_time:115784ms step_avg:61.39ms
step:1887/2330 train_time:115845ms step_avg:61.39ms
step:1888/2330 train_time:115908ms step_avg:61.39ms
step:1889/2330 train_time:115969ms step_avg:61.39ms
step:1890/2330 train_time:116034ms step_avg:61.39ms
step:1891/2330 train_time:116093ms step_avg:61.39ms
step:1892/2330 train_time:116156ms step_avg:61.39ms
step:1893/2330 train_time:116217ms step_avg:61.39ms
step:1894/2330 train_time:116280ms step_avg:61.39ms
step:1895/2330 train_time:116341ms step_avg:61.39ms
step:1896/2330 train_time:116405ms step_avg:61.39ms
step:1897/2330 train_time:116466ms step_avg:61.39ms
step:1898/2330 train_time:116530ms step_avg:61.40ms
step:1899/2330 train_time:116590ms step_avg:61.40ms
step:1900/2330 train_time:116654ms step_avg:61.40ms
step:1901/2330 train_time:116714ms step_avg:61.40ms
step:1902/2330 train_time:116777ms step_avg:61.40ms
step:1903/2330 train_time:116839ms step_avg:61.40ms
step:1904/2330 train_time:116902ms step_avg:61.40ms
step:1905/2330 train_time:116962ms step_avg:61.40ms
step:1906/2330 train_time:117026ms step_avg:61.40ms
step:1907/2330 train_time:117086ms step_avg:61.40ms
step:1908/2330 train_time:117149ms step_avg:61.40ms
step:1909/2330 train_time:117210ms step_avg:61.40ms
step:1910/2330 train_time:117273ms step_avg:61.40ms
step:1911/2330 train_time:117333ms step_avg:61.40ms
step:1912/2330 train_time:117397ms step_avg:61.40ms
step:1913/2330 train_time:117457ms step_avg:61.40ms
step:1914/2330 train_time:117520ms step_avg:61.40ms
step:1915/2330 train_time:117581ms step_avg:61.40ms
step:1916/2330 train_time:117644ms step_avg:61.40ms
step:1917/2330 train_time:117704ms step_avg:61.40ms
step:1918/2330 train_time:117768ms step_avg:61.40ms
step:1919/2330 train_time:117828ms step_avg:61.40ms
step:1920/2330 train_time:117892ms step_avg:61.40ms
step:1921/2330 train_time:117952ms step_avg:61.40ms
step:1922/2330 train_time:118015ms step_avg:61.40ms
step:1923/2330 train_time:118076ms step_avg:61.40ms
step:1924/2330 train_time:118139ms step_avg:61.40ms
step:1925/2330 train_time:118200ms step_avg:61.40ms
step:1926/2330 train_time:118264ms step_avg:61.40ms
step:1927/2330 train_time:118325ms step_avg:61.40ms
step:1928/2330 train_time:118389ms step_avg:61.40ms
step:1929/2330 train_time:118449ms step_avg:61.40ms
step:1930/2330 train_time:118512ms step_avg:61.41ms
step:1931/2330 train_time:118572ms step_avg:61.40ms
step:1932/2330 train_time:118636ms step_avg:61.41ms
step:1933/2330 train_time:118696ms step_avg:61.41ms
step:1934/2330 train_time:118760ms step_avg:61.41ms
step:1935/2330 train_time:118820ms step_avg:61.41ms
step:1936/2330 train_time:118883ms step_avg:61.41ms
step:1937/2330 train_time:118943ms step_avg:61.41ms
step:1938/2330 train_time:119007ms step_avg:61.41ms
step:1939/2330 train_time:119068ms step_avg:61.41ms
step:1940/2330 train_time:119132ms step_avg:61.41ms
step:1941/2330 train_time:119193ms step_avg:61.41ms
step:1942/2330 train_time:119256ms step_avg:61.41ms
step:1943/2330 train_time:119317ms step_avg:61.41ms
step:1944/2330 train_time:119380ms step_avg:61.41ms
step:1945/2330 train_time:119441ms step_avg:61.41ms
step:1946/2330 train_time:119504ms step_avg:61.41ms
step:1947/2330 train_time:119565ms step_avg:61.41ms
step:1948/2330 train_time:119629ms step_avg:61.41ms
step:1949/2330 train_time:119690ms step_avg:61.41ms
step:1950/2330 train_time:119753ms step_avg:61.41ms
step:1951/2330 train_time:119813ms step_avg:61.41ms
step:1952/2330 train_time:119876ms step_avg:61.41ms
step:1953/2330 train_time:119936ms step_avg:61.41ms
step:1954/2330 train_time:120000ms step_avg:61.41ms
step:1955/2330 train_time:120060ms step_avg:61.41ms
step:1956/2330 train_time:120123ms step_avg:61.41ms
step:1957/2330 train_time:120184ms step_avg:61.41ms
step:1958/2330 train_time:120247ms step_avg:61.41ms
step:1959/2330 train_time:120308ms step_avg:61.41ms
step:1960/2330 train_time:120371ms step_avg:61.41ms
step:1961/2330 train_time:120432ms step_avg:61.41ms
step:1962/2330 train_time:120496ms step_avg:61.41ms
step:1963/2330 train_time:120556ms step_avg:61.41ms
step:1964/2330 train_time:120619ms step_avg:61.42ms
step:1965/2330 train_time:120679ms step_avg:61.41ms
step:1966/2330 train_time:120743ms step_avg:61.42ms
step:1967/2330 train_time:120803ms step_avg:61.41ms
step:1968/2330 train_time:120868ms step_avg:61.42ms
step:1969/2330 train_time:120929ms step_avg:61.42ms
step:1970/2330 train_time:120993ms step_avg:61.42ms
step:1971/2330 train_time:121053ms step_avg:61.42ms
step:1972/2330 train_time:121116ms step_avg:61.42ms
step:1973/2330 train_time:121177ms step_avg:61.42ms
step:1974/2330 train_time:121241ms step_avg:61.42ms
step:1975/2330 train_time:121301ms step_avg:61.42ms
step:1976/2330 train_time:121365ms step_avg:61.42ms
step:1977/2330 train_time:121426ms step_avg:61.42ms
step:1978/2330 train_time:121490ms step_avg:61.42ms
step:1979/2330 train_time:121550ms step_avg:61.42ms
step:1980/2330 train_time:121614ms step_avg:61.42ms
step:1981/2330 train_time:121675ms step_avg:61.42ms
step:1982/2330 train_time:121738ms step_avg:61.42ms
step:1983/2330 train_time:121799ms step_avg:61.42ms
step:1984/2330 train_time:121862ms step_avg:61.42ms
step:1985/2330 train_time:121922ms step_avg:61.42ms
step:1986/2330 train_time:121986ms step_avg:61.42ms
step:1987/2330 train_time:122046ms step_avg:61.42ms
step:1988/2330 train_time:122109ms step_avg:61.42ms
step:1989/2330 train_time:122170ms step_avg:61.42ms
step:1990/2330 train_time:122233ms step_avg:61.42ms
step:1991/2330 train_time:122294ms step_avg:61.42ms
step:1992/2330 train_time:122357ms step_avg:61.42ms
step:1993/2330 train_time:122417ms step_avg:61.42ms
step:1994/2330 train_time:122481ms step_avg:61.42ms
step:1995/2330 train_time:122541ms step_avg:61.42ms
step:1996/2330 train_time:122605ms step_avg:61.43ms
step:1997/2330 train_time:122666ms step_avg:61.43ms
step:1998/2330 train_time:122729ms step_avg:61.43ms
step:1999/2330 train_time:122790ms step_avg:61.43ms
step:2000/2330 train_time:122853ms step_avg:61.43ms
step:2000/2330 val_loss:3.4067 train_time:122918ms step_avg:61.46ms
step:2001/2330 train_time:122942ms step_avg:61.44ms
step:2002/2330 train_time:122981ms step_avg:61.43ms
step:2003/2330 train_time:123046ms step_avg:61.43ms
step:2004/2330 train_time:123112ms step_avg:61.43ms
step:2005/2330 train_time:123172ms step_avg:61.43ms
step:2006/2330 train_time:123235ms step_avg:61.43ms
step:2007/2330 train_time:123295ms step_avg:61.43ms
step:2008/2330 train_time:123358ms step_avg:61.43ms
step:2009/2330 train_time:123418ms step_avg:61.43ms
step:2010/2330 train_time:123480ms step_avg:61.43ms
step:2011/2330 train_time:123540ms step_avg:61.43ms
step:2012/2330 train_time:123603ms step_avg:61.43ms
step:2013/2330 train_time:123662ms step_avg:61.43ms
step:2014/2330 train_time:123725ms step_avg:61.43ms
step:2015/2330 train_time:123784ms step_avg:61.43ms
step:2016/2330 train_time:123848ms step_avg:61.43ms
step:2017/2330 train_time:123910ms step_avg:61.43ms
step:2018/2330 train_time:123975ms step_avg:61.43ms
step:2019/2330 train_time:124037ms step_avg:61.43ms
step:2020/2330 train_time:124102ms step_avg:61.44ms
step:2021/2330 train_time:124164ms step_avg:61.44ms
step:2022/2330 train_time:124227ms step_avg:61.44ms
step:2023/2330 train_time:124289ms step_avg:61.44ms
step:2024/2330 train_time:124352ms step_avg:61.44ms
step:2025/2330 train_time:124411ms step_avg:61.44ms
step:2026/2330 train_time:124475ms step_avg:61.44ms
step:2027/2330 train_time:124535ms step_avg:61.44ms
step:2028/2330 train_time:124598ms step_avg:61.44ms
step:2029/2330 train_time:124658ms step_avg:61.44ms
step:2030/2330 train_time:124721ms step_avg:61.44ms
step:2031/2330 train_time:124780ms step_avg:61.44ms
step:2032/2330 train_time:124844ms step_avg:61.44ms
step:2033/2330 train_time:124905ms step_avg:61.44ms
step:2034/2330 train_time:124969ms step_avg:61.44ms
step:2035/2330 train_time:125030ms step_avg:61.44ms
step:2036/2330 train_time:125094ms step_avg:61.44ms
step:2037/2330 train_time:125155ms step_avg:61.44ms
step:2038/2330 train_time:125219ms step_avg:61.44ms
step:2039/2330 train_time:125280ms step_avg:61.44ms
step:2040/2330 train_time:125344ms step_avg:61.44ms
step:2041/2330 train_time:125405ms step_avg:61.44ms
step:2042/2330 train_time:125468ms step_avg:61.44ms
step:2043/2330 train_time:125529ms step_avg:61.44ms
step:2044/2330 train_time:125592ms step_avg:61.44ms
step:2045/2330 train_time:125652ms step_avg:61.44ms
step:2046/2330 train_time:125715ms step_avg:61.44ms
step:2047/2330 train_time:125775ms step_avg:61.44ms
step:2048/2330 train_time:125840ms step_avg:61.45ms
step:2049/2330 train_time:125900ms step_avg:61.44ms
step:2050/2330 train_time:125963ms step_avg:61.45ms
step:2051/2330 train_time:126024ms step_avg:61.45ms
step:2052/2330 train_time:126089ms step_avg:61.45ms
step:2053/2330 train_time:126151ms step_avg:61.45ms
step:2054/2330 train_time:126214ms step_avg:61.45ms
step:2055/2330 train_time:126274ms step_avg:61.45ms
step:2056/2330 train_time:126338ms step_avg:61.45ms
step:2057/2330 train_time:126399ms step_avg:61.45ms
step:2058/2330 train_time:126462ms step_avg:61.45ms
step:2059/2330 train_time:126523ms step_avg:61.45ms
step:2060/2330 train_time:126586ms step_avg:61.45ms
step:2061/2330 train_time:126647ms step_avg:61.45ms
step:2062/2330 train_time:126711ms step_avg:61.45ms
step:2063/2330 train_time:126771ms step_avg:61.45ms
step:2064/2330 train_time:126834ms step_avg:61.45ms
step:2065/2330 train_time:126895ms step_avg:61.45ms
step:2066/2330 train_time:126958ms step_avg:61.45ms
step:2067/2330 train_time:127018ms step_avg:61.45ms
step:2068/2330 train_time:127082ms step_avg:61.45ms
step:2069/2330 train_time:127143ms step_avg:61.45ms
step:2070/2330 train_time:127208ms step_avg:61.45ms
step:2071/2330 train_time:127268ms step_avg:61.45ms
step:2072/2330 train_time:127332ms step_avg:61.45ms
step:2073/2330 train_time:127392ms step_avg:61.45ms
step:2074/2330 train_time:127455ms step_avg:61.45ms
step:2075/2330 train_time:127516ms step_avg:61.45ms
step:2076/2330 train_time:127579ms step_avg:61.45ms
step:2077/2330 train_time:127639ms step_avg:61.45ms
step:2078/2330 train_time:127702ms step_avg:61.45ms
step:2079/2330 train_time:127762ms step_avg:61.45ms
step:2080/2330 train_time:127826ms step_avg:61.45ms
step:2081/2330 train_time:127887ms step_avg:61.45ms
step:2082/2330 train_time:127951ms step_avg:61.46ms
step:2083/2330 train_time:128011ms step_avg:61.46ms
step:2084/2330 train_time:128074ms step_avg:61.46ms
step:2085/2330 train_time:128135ms step_avg:61.46ms
step:2086/2330 train_time:128198ms step_avg:61.46ms
step:2087/2330 train_time:128259ms step_avg:61.46ms
step:2088/2330 train_time:128324ms step_avg:61.46ms
step:2089/2330 train_time:128384ms step_avg:61.46ms
step:2090/2330 train_time:128447ms step_avg:61.46ms
step:2091/2330 train_time:128508ms step_avg:61.46ms
step:2092/2330 train_time:128571ms step_avg:61.46ms
step:2093/2330 train_time:128632ms step_avg:61.46ms
step:2094/2330 train_time:128695ms step_avg:61.46ms
step:2095/2330 train_time:128756ms step_avg:61.46ms
step:2096/2330 train_time:128819ms step_avg:61.46ms
step:2097/2330 train_time:128879ms step_avg:61.46ms
step:2098/2330 train_time:128943ms step_avg:61.46ms
step:2099/2330 train_time:129003ms step_avg:61.46ms
step:2100/2330 train_time:129068ms step_avg:61.46ms
step:2101/2330 train_time:129128ms step_avg:61.46ms
step:2102/2330 train_time:129192ms step_avg:61.46ms
step:2103/2330 train_time:129253ms step_avg:61.46ms
step:2104/2330 train_time:129316ms step_avg:61.46ms
step:2105/2330 train_time:129376ms step_avg:61.46ms
step:2106/2330 train_time:129439ms step_avg:61.46ms
step:2107/2330 train_time:129500ms step_avg:61.46ms
step:2108/2330 train_time:129564ms step_avg:61.46ms
step:2109/2330 train_time:129623ms step_avg:61.46ms
step:2110/2330 train_time:129687ms step_avg:61.46ms
step:2111/2330 train_time:129748ms step_avg:61.46ms
step:2112/2330 train_time:129811ms step_avg:61.46ms
step:2113/2330 train_time:129872ms step_avg:61.46ms
step:2114/2330 train_time:129936ms step_avg:61.46ms
step:2115/2330 train_time:129996ms step_avg:61.46ms
step:2116/2330 train_time:130060ms step_avg:61.46ms
step:2117/2330 train_time:130120ms step_avg:61.46ms
step:2118/2330 train_time:130183ms step_avg:61.47ms
step:2119/2330 train_time:130244ms step_avg:61.47ms
step:2120/2330 train_time:130309ms step_avg:61.47ms
step:2121/2330 train_time:130369ms step_avg:61.47ms
step:2122/2330 train_time:130433ms step_avg:61.47ms
step:2123/2330 train_time:130493ms step_avg:61.47ms
step:2124/2330 train_time:130556ms step_avg:61.47ms
step:2125/2330 train_time:130616ms step_avg:61.47ms
step:2126/2330 train_time:130679ms step_avg:61.47ms
step:2127/2330 train_time:130740ms step_avg:61.47ms
step:2128/2330 train_time:130804ms step_avg:61.47ms
step:2129/2330 train_time:130866ms step_avg:61.47ms
step:2130/2330 train_time:130929ms step_avg:61.47ms
step:2131/2330 train_time:130990ms step_avg:61.47ms
step:2132/2330 train_time:131053ms step_avg:61.47ms
step:2133/2330 train_time:131114ms step_avg:61.47ms
step:2134/2330 train_time:131177ms step_avg:61.47ms
step:2135/2330 train_time:131238ms step_avg:61.47ms
step:2136/2330 train_time:131301ms step_avg:61.47ms
step:2137/2330 train_time:131361ms step_avg:61.47ms
step:2138/2330 train_time:131425ms step_avg:61.47ms
step:2139/2330 train_time:131485ms step_avg:61.47ms
step:2140/2330 train_time:131550ms step_avg:61.47ms
step:2141/2330 train_time:131610ms step_avg:61.47ms
step:2142/2330 train_time:131673ms step_avg:61.47ms
step:2143/2330 train_time:131733ms step_avg:61.47ms
step:2144/2330 train_time:131797ms step_avg:61.47ms
step:2145/2330 train_time:131857ms step_avg:61.47ms
step:2146/2330 train_time:131920ms step_avg:61.47ms
step:2147/2330 train_time:131981ms step_avg:61.47ms
step:2148/2330 train_time:132045ms step_avg:61.47ms
step:2149/2330 train_time:132106ms step_avg:61.47ms
step:2150/2330 train_time:132169ms step_avg:61.47ms
step:2151/2330 train_time:132230ms step_avg:61.47ms
step:2152/2330 train_time:132293ms step_avg:61.47ms
step:2153/2330 train_time:132353ms step_avg:61.47ms
step:2154/2330 train_time:132417ms step_avg:61.48ms
step:2155/2330 train_time:132478ms step_avg:61.47ms
step:2156/2330 train_time:132541ms step_avg:61.48ms
step:2157/2330 train_time:132601ms step_avg:61.47ms
step:2158/2330 train_time:132665ms step_avg:61.48ms
step:2159/2330 train_time:132726ms step_avg:61.48ms
step:2160/2330 train_time:132789ms step_avg:61.48ms
step:2161/2330 train_time:132849ms step_avg:61.48ms
step:2162/2330 train_time:132913ms step_avg:61.48ms
step:2163/2330 train_time:132973ms step_avg:61.48ms
step:2164/2330 train_time:133036ms step_avg:61.48ms
step:2165/2330 train_time:133096ms step_avg:61.48ms
step:2166/2330 train_time:133160ms step_avg:61.48ms
step:2167/2330 train_time:133221ms step_avg:61.48ms
step:2168/2330 train_time:133284ms step_avg:61.48ms
step:2169/2330 train_time:133345ms step_avg:61.48ms
step:2170/2330 train_time:133409ms step_avg:61.48ms
step:2171/2330 train_time:133469ms step_avg:61.48ms
step:2172/2330 train_time:133533ms step_avg:61.48ms
step:2173/2330 train_time:133593ms step_avg:61.48ms
step:2174/2330 train_time:133657ms step_avg:61.48ms
step:2175/2330 train_time:133718ms step_avg:61.48ms
step:2176/2330 train_time:133781ms step_avg:61.48ms
step:2177/2330 train_time:133841ms step_avg:61.48ms
step:2178/2330 train_time:133905ms step_avg:61.48ms
step:2179/2330 train_time:133966ms step_avg:61.48ms
step:2180/2330 train_time:134030ms step_avg:61.48ms
step:2181/2330 train_time:134090ms step_avg:61.48ms
step:2182/2330 train_time:134154ms step_avg:61.48ms
step:2183/2330 train_time:134214ms step_avg:61.48ms
step:2184/2330 train_time:134278ms step_avg:61.48ms
step:2185/2330 train_time:134338ms step_avg:61.48ms
step:2186/2330 train_time:134402ms step_avg:61.48ms
step:2187/2330 train_time:134463ms step_avg:61.48ms
step:2188/2330 train_time:134526ms step_avg:61.48ms
step:2189/2330 train_time:134587ms step_avg:61.48ms
step:2190/2330 train_time:134651ms step_avg:61.48ms
step:2191/2330 train_time:134711ms step_avg:61.48ms
step:2192/2330 train_time:134775ms step_avg:61.48ms
step:2193/2330 train_time:134836ms step_avg:61.48ms
step:2194/2330 train_time:134900ms step_avg:61.49ms
step:2195/2330 train_time:134960ms step_avg:61.49ms
step:2196/2330 train_time:135023ms step_avg:61.49ms
step:2197/2330 train_time:135084ms step_avg:61.49ms
step:2198/2330 train_time:135148ms step_avg:61.49ms
step:2199/2330 train_time:135208ms step_avg:61.49ms
step:2200/2330 train_time:135271ms step_avg:61.49ms
step:2201/2330 train_time:135332ms step_avg:61.49ms
step:2202/2330 train_time:135395ms step_avg:61.49ms
step:2203/2330 train_time:135456ms step_avg:61.49ms
step:2204/2330 train_time:135519ms step_avg:61.49ms
step:2205/2330 train_time:135579ms step_avg:61.49ms
step:2206/2330 train_time:135644ms step_avg:61.49ms
step:2207/2330 train_time:135705ms step_avg:61.49ms
step:2208/2330 train_time:135769ms step_avg:61.49ms
step:2209/2330 train_time:135829ms step_avg:61.49ms
step:2210/2330 train_time:135893ms step_avg:61.49ms
step:2211/2330 train_time:135953ms step_avg:61.49ms
step:2212/2330 train_time:136017ms step_avg:61.49ms
step:2213/2330 train_time:136077ms step_avg:61.49ms
step:2214/2330 train_time:136140ms step_avg:61.49ms
step:2215/2330 train_time:136201ms step_avg:61.49ms
step:2216/2330 train_time:136264ms step_avg:61.49ms
step:2217/2330 train_time:136326ms step_avg:61.49ms
step:2218/2330 train_time:136389ms step_avg:61.49ms
step:2219/2330 train_time:136450ms step_avg:61.49ms
step:2220/2330 train_time:136513ms step_avg:61.49ms
step:2221/2330 train_time:136573ms step_avg:61.49ms
step:2222/2330 train_time:136637ms step_avg:61.49ms
step:2223/2330 train_time:136697ms step_avg:61.49ms
step:2224/2330 train_time:136761ms step_avg:61.49ms
step:2225/2330 train_time:136822ms step_avg:61.49ms
step:2226/2330 train_time:136886ms step_avg:61.49ms
step:2227/2330 train_time:136947ms step_avg:61.49ms
step:2228/2330 train_time:137011ms step_avg:61.49ms
step:2229/2330 train_time:137071ms step_avg:61.49ms
step:2230/2330 train_time:137134ms step_avg:61.50ms
step:2231/2330 train_time:137195ms step_avg:61.49ms
step:2232/2330 train_time:137258ms step_avg:61.50ms
step:2233/2330 train_time:137318ms step_avg:61.49ms
step:2234/2330 train_time:137381ms step_avg:61.50ms
step:2235/2330 train_time:137442ms step_avg:61.50ms
step:2236/2330 train_time:137507ms step_avg:61.50ms
step:2237/2330 train_time:137567ms step_avg:61.50ms
step:2238/2330 train_time:137631ms step_avg:61.50ms
step:2239/2330 train_time:137692ms step_avg:61.50ms
step:2240/2330 train_time:137755ms step_avg:61.50ms
step:2241/2330 train_time:137815ms step_avg:61.50ms
step:2242/2330 train_time:137878ms step_avg:61.50ms
step:2243/2330 train_time:137939ms step_avg:61.50ms
step:2244/2330 train_time:138003ms step_avg:61.50ms
step:2245/2330 train_time:138063ms step_avg:61.50ms
step:2246/2330 train_time:138127ms step_avg:61.50ms
step:2247/2330 train_time:138187ms step_avg:61.50ms
step:2248/2330 train_time:138251ms step_avg:61.50ms
step:2249/2330 train_time:138311ms step_avg:61.50ms
step:2250/2330 train_time:138375ms step_avg:61.50ms
step:2250/2330 val_loss:3.3850 train_time:138440ms step_avg:61.53ms
step:2251/2330 train_time:138464ms step_avg:61.51ms
step:2252/2330 train_time:138503ms step_avg:61.50ms
step:2253/2330 train_time:138567ms step_avg:61.50ms
step:2254/2330 train_time:138631ms step_avg:61.50ms
step:2255/2330 train_time:138691ms step_avg:61.50ms
step:2256/2330 train_time:138755ms step_avg:61.50ms
step:2257/2330 train_time:138814ms step_avg:61.50ms
step:2258/2330 train_time:138877ms step_avg:61.50ms
step:2259/2330 train_time:138936ms step_avg:61.50ms
step:2260/2330 train_time:138999ms step_avg:61.50ms
step:2261/2330 train_time:139059ms step_avg:61.50ms
step:2262/2330 train_time:139122ms step_avg:61.50ms
step:2263/2330 train_time:139181ms step_avg:61.50ms
step:2264/2330 train_time:139243ms step_avg:61.50ms
step:2265/2330 train_time:139303ms step_avg:61.50ms
step:2266/2330 train_time:139368ms step_avg:61.50ms
step:2267/2330 train_time:139430ms step_avg:61.50ms
step:2268/2330 train_time:139495ms step_avg:61.51ms
step:2269/2330 train_time:139556ms step_avg:61.51ms
step:2270/2330 train_time:139620ms step_avg:61.51ms
step:2271/2330 train_time:139681ms step_avg:61.51ms
step:2272/2330 train_time:139745ms step_avg:61.51ms
step:2273/2330 train_time:139805ms step_avg:61.51ms
step:2274/2330 train_time:139868ms step_avg:61.51ms
step:2275/2330 train_time:139929ms step_avg:61.51ms
step:2276/2330 train_time:139992ms step_avg:61.51ms
step:2277/2330 train_time:140052ms step_avg:61.51ms
step:2278/2330 train_time:140115ms step_avg:61.51ms
step:2279/2330 train_time:140175ms step_avg:61.51ms
step:2280/2330 train_time:140239ms step_avg:61.51ms
step:2281/2330 train_time:140299ms step_avg:61.51ms
step:2282/2330 train_time:140362ms step_avg:61.51ms
step:2283/2330 train_time:140423ms step_avg:61.51ms
step:2284/2330 train_time:140487ms step_avg:61.51ms
step:2285/2330 train_time:140548ms step_avg:61.51ms
step:2286/2330 train_time:140612ms step_avg:61.51ms
step:2287/2330 train_time:140673ms step_avg:61.51ms
step:2288/2330 train_time:140737ms step_avg:61.51ms
step:2289/2330 train_time:140797ms step_avg:61.51ms
step:2290/2330 train_time:140860ms step_avg:61.51ms
step:2291/2330 train_time:140921ms step_avg:61.51ms
step:2292/2330 train_time:140985ms step_avg:61.51ms
step:2293/2330 train_time:141044ms step_avg:61.51ms
step:2294/2330 train_time:141108ms step_avg:61.51ms
step:2295/2330 train_time:141169ms step_avg:61.51ms
step:2296/2330 train_time:141232ms step_avg:61.51ms
step:2297/2330 train_time:141292ms step_avg:61.51ms
step:2298/2330 train_time:141356ms step_avg:61.51ms
step:2299/2330 train_time:141416ms step_avg:61.51ms
step:2300/2330 train_time:141480ms step_avg:61.51ms
step:2301/2330 train_time:141541ms step_avg:61.51ms
step:2302/2330 train_time:141604ms step_avg:61.51ms
step:2303/2330 train_time:141665ms step_avg:61.51ms
step:2304/2330 train_time:141729ms step_avg:61.51ms
step:2305/2330 train_time:141789ms step_avg:61.51ms
step:2306/2330 train_time:141853ms step_avg:61.51ms
step:2307/2330 train_time:141914ms step_avg:61.51ms
step:2308/2330 train_time:141977ms step_avg:61.52ms
step:2309/2330 train_time:142038ms step_avg:61.51ms
step:2310/2330 train_time:142100ms step_avg:61.52ms
step:2311/2330 train_time:142161ms step_avg:61.51ms
step:2312/2330 train_time:142224ms step_avg:61.52ms
step:2313/2330 train_time:142284ms step_avg:61.51ms
step:2314/2330 train_time:142347ms step_avg:61.52ms
step:2315/2330 train_time:142408ms step_avg:61.52ms
step:2316/2330 train_time:142473ms step_avg:61.52ms
step:2317/2330 train_time:142534ms step_avg:61.52ms
step:2318/2330 train_time:142597ms step_avg:61.52ms
step:2319/2330 train_time:142658ms step_avg:61.52ms
step:2320/2330 train_time:142721ms step_avg:61.52ms
step:2321/2330 train_time:142782ms step_avg:61.52ms
step:2322/2330 train_time:142845ms step_avg:61.52ms
step:2323/2330 train_time:142905ms step_avg:61.52ms
step:2324/2330 train_time:142968ms step_avg:61.52ms
step:2325/2330 train_time:143028ms step_avg:61.52ms
step:2326/2330 train_time:143092ms step_avg:61.52ms
step:2327/2330 train_time:143152ms step_avg:61.52ms
step:2328/2330 train_time:143215ms step_avg:61.52ms
step:2329/2330 train_time:143276ms step_avg:61.52ms
step:2330/2330 train_time:143339ms step_avg:61.52ms
step:2330/2330 val_loss:3.3580 train_time:143404ms step_avg:61.55ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
