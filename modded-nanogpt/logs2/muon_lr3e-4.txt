import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-4"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:53:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:94ms step_avg:94.14ms
step:2/2330 train_time:198ms step_avg:98.90ms
step:3/2330 train_time:219ms step_avg:72.95ms
step:4/2330 train_time:255ms step_avg:63.78ms
step:5/2330 train_time:312ms step_avg:62.50ms
step:6/2330 train_time:374ms step_avg:62.32ms
step:7/2330 train_time:432ms step_avg:61.74ms
step:8/2330 train_time:494ms step_avg:61.74ms
step:9/2330 train_time:553ms step_avg:61.39ms
step:10/2330 train_time:614ms step_avg:61.45ms
step:11/2330 train_time:673ms step_avg:61.18ms
step:12/2330 train_time:735ms step_avg:61.24ms
step:13/2330 train_time:793ms step_avg:61.01ms
step:14/2330 train_time:855ms step_avg:61.04ms
step:15/2330 train_time:913ms step_avg:60.87ms
step:16/2330 train_time:975ms step_avg:60.93ms
step:17/2330 train_time:1034ms step_avg:60.81ms
step:18/2330 train_time:1098ms step_avg:60.97ms
step:19/2330 train_time:1159ms step_avg:61.02ms
step:20/2330 train_time:1222ms step_avg:61.12ms
step:21/2330 train_time:1282ms step_avg:61.04ms
step:22/2330 train_time:1345ms step_avg:61.12ms
step:23/2330 train_time:1404ms step_avg:61.06ms
step:24/2330 train_time:1467ms step_avg:61.12ms
step:25/2330 train_time:1527ms step_avg:61.07ms
step:26/2330 train_time:1589ms step_avg:61.12ms
step:27/2330 train_time:1649ms step_avg:61.06ms
step:28/2330 train_time:1711ms step_avg:61.11ms
step:29/2330 train_time:1770ms step_avg:61.03ms
step:30/2330 train_time:1832ms step_avg:61.07ms
step:31/2330 train_time:1891ms step_avg:61.00ms
step:32/2330 train_time:1954ms step_avg:61.05ms
step:33/2330 train_time:2013ms step_avg:61.00ms
step:34/2330 train_time:2076ms step_avg:61.05ms
step:35/2330 train_time:2135ms step_avg:61.01ms
step:36/2330 train_time:2198ms step_avg:61.06ms
step:37/2330 train_time:2258ms step_avg:61.02ms
step:38/2330 train_time:2320ms step_avg:61.05ms
step:39/2330 train_time:2379ms step_avg:61.01ms
step:40/2330 train_time:2442ms step_avg:61.05ms
step:41/2330 train_time:2502ms step_avg:61.02ms
step:42/2330 train_time:2565ms step_avg:61.07ms
step:43/2330 train_time:2624ms step_avg:61.03ms
step:44/2330 train_time:2687ms step_avg:61.06ms
step:45/2330 train_time:2748ms step_avg:61.06ms
step:46/2330 train_time:2811ms step_avg:61.10ms
step:47/2330 train_time:2870ms step_avg:61.06ms
step:48/2330 train_time:2932ms step_avg:61.09ms
step:49/2330 train_time:2992ms step_avg:61.06ms
step:50/2330 train_time:3055ms step_avg:61.09ms
step:51/2330 train_time:3114ms step_avg:61.06ms
step:52/2330 train_time:3176ms step_avg:61.09ms
step:53/2330 train_time:3236ms step_avg:61.06ms
step:54/2330 train_time:3299ms step_avg:61.10ms
step:55/2330 train_time:3360ms step_avg:61.10ms
step:56/2330 train_time:3422ms step_avg:61.12ms
step:57/2330 train_time:3481ms step_avg:61.08ms
step:58/2330 train_time:3544ms step_avg:61.10ms
step:59/2330 train_time:3603ms step_avg:61.07ms
step:60/2330 train_time:3666ms step_avg:61.10ms
step:61/2330 train_time:3726ms step_avg:61.08ms
step:62/2330 train_time:3789ms step_avg:61.11ms
step:63/2330 train_time:3849ms step_avg:61.09ms
step:64/2330 train_time:3912ms step_avg:61.12ms
step:65/2330 train_time:3971ms step_avg:61.10ms
step:66/2330 train_time:4034ms step_avg:61.12ms
step:67/2330 train_time:4094ms step_avg:61.10ms
step:68/2330 train_time:4156ms step_avg:61.12ms
step:69/2330 train_time:4216ms step_avg:61.10ms
step:70/2330 train_time:4279ms step_avg:61.13ms
step:71/2330 train_time:4339ms step_avg:61.12ms
step:72/2330 train_time:4401ms step_avg:61.13ms
step:73/2330 train_time:4461ms step_avg:61.11ms
step:74/2330 train_time:4524ms step_avg:61.13ms
step:75/2330 train_time:4583ms step_avg:61.10ms
step:76/2330 train_time:4645ms step_avg:61.12ms
step:77/2330 train_time:4705ms step_avg:61.11ms
step:78/2330 train_time:4769ms step_avg:61.14ms
step:79/2330 train_time:4829ms step_avg:61.13ms
step:80/2330 train_time:4892ms step_avg:61.15ms
step:81/2330 train_time:4952ms step_avg:61.13ms
step:82/2330 train_time:5015ms step_avg:61.16ms
step:83/2330 train_time:5075ms step_avg:61.14ms
step:84/2330 train_time:5137ms step_avg:61.16ms
step:85/2330 train_time:5197ms step_avg:61.14ms
step:86/2330 train_time:5259ms step_avg:61.15ms
step:87/2330 train_time:5319ms step_avg:61.14ms
step:88/2330 train_time:5381ms step_avg:61.15ms
step:89/2330 train_time:5442ms step_avg:61.14ms
step:90/2330 train_time:5504ms step_avg:61.16ms
step:91/2330 train_time:5564ms step_avg:61.14ms
step:92/2330 train_time:5626ms step_avg:61.15ms
step:93/2330 train_time:5687ms step_avg:61.15ms
step:94/2330 train_time:5749ms step_avg:61.16ms
step:95/2330 train_time:5810ms step_avg:61.16ms
step:96/2330 train_time:5872ms step_avg:61.17ms
step:97/2330 train_time:5932ms step_avg:61.16ms
step:98/2330 train_time:5995ms step_avg:61.17ms
step:99/2330 train_time:6055ms step_avg:61.16ms
step:100/2330 train_time:6117ms step_avg:61.17ms
step:101/2330 train_time:6176ms step_avg:61.15ms
step:102/2330 train_time:6239ms step_avg:61.16ms
step:103/2330 train_time:6298ms step_avg:61.14ms
step:104/2330 train_time:6360ms step_avg:61.16ms
step:105/2330 train_time:6420ms step_avg:61.14ms
step:106/2330 train_time:6482ms step_avg:61.15ms
step:107/2330 train_time:6541ms step_avg:61.13ms
step:108/2330 train_time:6604ms step_avg:61.15ms
step:109/2330 train_time:6663ms step_avg:61.13ms
step:110/2330 train_time:6726ms step_avg:61.15ms
step:111/2330 train_time:6786ms step_avg:61.14ms
step:112/2330 train_time:6850ms step_avg:61.16ms
step:113/2330 train_time:6910ms step_avg:61.15ms
step:114/2330 train_time:6973ms step_avg:61.17ms
step:115/2330 train_time:7032ms step_avg:61.15ms
step:116/2330 train_time:7095ms step_avg:61.17ms
step:117/2330 train_time:7156ms step_avg:61.16ms
step:118/2330 train_time:7218ms step_avg:61.17ms
step:119/2330 train_time:7278ms step_avg:61.16ms
step:120/2330 train_time:7340ms step_avg:61.16ms
step:121/2330 train_time:7399ms step_avg:61.15ms
step:122/2330 train_time:7461ms step_avg:61.16ms
step:123/2330 train_time:7520ms step_avg:61.14ms
step:124/2330 train_time:7582ms step_avg:61.15ms
step:125/2330 train_time:7642ms step_avg:61.13ms
step:126/2330 train_time:7706ms step_avg:61.16ms
step:127/2330 train_time:7765ms step_avg:61.15ms
step:128/2330 train_time:7829ms step_avg:61.16ms
step:129/2330 train_time:7889ms step_avg:61.16ms
step:130/2330 train_time:7953ms step_avg:61.18ms
step:131/2330 train_time:8013ms step_avg:61.17ms
step:132/2330 train_time:8076ms step_avg:61.18ms
step:133/2330 train_time:8136ms step_avg:61.17ms
step:134/2330 train_time:8198ms step_avg:61.18ms
step:135/2330 train_time:8257ms step_avg:61.16ms
step:136/2330 train_time:8319ms step_avg:61.17ms
step:137/2330 train_time:8379ms step_avg:61.16ms
step:138/2330 train_time:8441ms step_avg:61.17ms
step:139/2330 train_time:8500ms step_avg:61.15ms
step:140/2330 train_time:8562ms step_avg:61.16ms
step:141/2330 train_time:8622ms step_avg:61.15ms
step:142/2330 train_time:8684ms step_avg:61.16ms
step:143/2330 train_time:8744ms step_avg:61.15ms
step:144/2330 train_time:8808ms step_avg:61.17ms
step:145/2330 train_time:8868ms step_avg:61.16ms
step:146/2330 train_time:8932ms step_avg:61.18ms
step:147/2330 train_time:8993ms step_avg:61.18ms
step:148/2330 train_time:9056ms step_avg:61.19ms
step:149/2330 train_time:9116ms step_avg:61.18ms
step:150/2330 train_time:9178ms step_avg:61.19ms
step:151/2330 train_time:9237ms step_avg:61.17ms
step:152/2330 train_time:9300ms step_avg:61.18ms
step:153/2330 train_time:9359ms step_avg:61.17ms
step:154/2330 train_time:9421ms step_avg:61.18ms
step:155/2330 train_time:9480ms step_avg:61.16ms
step:156/2330 train_time:9543ms step_avg:61.17ms
step:157/2330 train_time:9602ms step_avg:61.16ms
step:158/2330 train_time:9665ms step_avg:61.17ms
step:159/2330 train_time:9725ms step_avg:61.16ms
step:160/2330 train_time:9789ms step_avg:61.18ms
step:161/2330 train_time:9849ms step_avg:61.17ms
step:162/2330 train_time:9913ms step_avg:61.19ms
step:163/2330 train_time:9974ms step_avg:61.19ms
step:164/2330 train_time:10036ms step_avg:61.20ms
step:165/2330 train_time:10096ms step_avg:61.19ms
step:166/2330 train_time:10159ms step_avg:61.20ms
step:167/2330 train_time:10218ms step_avg:61.19ms
step:168/2330 train_time:10280ms step_avg:61.19ms
step:169/2330 train_time:10340ms step_avg:61.18ms
step:170/2330 train_time:10402ms step_avg:61.19ms
step:171/2330 train_time:10461ms step_avg:61.17ms
step:172/2330 train_time:10523ms step_avg:61.18ms
step:173/2330 train_time:10583ms step_avg:61.17ms
step:174/2330 train_time:10646ms step_avg:61.18ms
step:175/2330 train_time:10706ms step_avg:61.18ms
step:176/2330 train_time:10769ms step_avg:61.19ms
step:177/2330 train_time:10830ms step_avg:61.18ms
step:178/2330 train_time:10893ms step_avg:61.20ms
step:179/2330 train_time:10953ms step_avg:61.19ms
step:180/2330 train_time:11016ms step_avg:61.20ms
step:181/2330 train_time:11076ms step_avg:61.19ms
step:182/2330 train_time:11138ms step_avg:61.20ms
step:183/2330 train_time:11198ms step_avg:61.19ms
step:184/2330 train_time:11260ms step_avg:61.20ms
step:185/2330 train_time:11319ms step_avg:61.19ms
step:186/2330 train_time:11382ms step_avg:61.19ms
step:187/2330 train_time:11442ms step_avg:61.18ms
step:188/2330 train_time:11504ms step_avg:61.19ms
step:189/2330 train_time:11564ms step_avg:61.18ms
step:190/2330 train_time:11627ms step_avg:61.20ms
step:191/2330 train_time:11687ms step_avg:61.19ms
step:192/2330 train_time:11750ms step_avg:61.20ms
step:193/2330 train_time:11810ms step_avg:61.19ms
step:194/2330 train_time:11873ms step_avg:61.20ms
step:195/2330 train_time:11933ms step_avg:61.19ms
step:196/2330 train_time:11995ms step_avg:61.20ms
step:197/2330 train_time:12055ms step_avg:61.19ms
step:198/2330 train_time:12118ms step_avg:61.20ms
step:199/2330 train_time:12178ms step_avg:61.20ms
step:200/2330 train_time:12240ms step_avg:61.20ms
step:201/2330 train_time:12300ms step_avg:61.19ms
step:202/2330 train_time:12362ms step_avg:61.20ms
step:203/2330 train_time:12421ms step_avg:61.19ms
step:204/2330 train_time:12484ms step_avg:61.19ms
step:205/2330 train_time:12543ms step_avg:61.19ms
step:206/2330 train_time:12606ms step_avg:61.20ms
step:207/2330 train_time:12666ms step_avg:61.19ms
step:208/2330 train_time:12729ms step_avg:61.20ms
step:209/2330 train_time:12789ms step_avg:61.19ms
step:210/2330 train_time:12852ms step_avg:61.20ms
step:211/2330 train_time:12913ms step_avg:61.20ms
step:212/2330 train_time:12976ms step_avg:61.21ms
step:213/2330 train_time:13035ms step_avg:61.20ms
step:214/2330 train_time:13098ms step_avg:61.21ms
step:215/2330 train_time:13158ms step_avg:61.20ms
step:216/2330 train_time:13220ms step_avg:61.21ms
step:217/2330 train_time:13280ms step_avg:61.20ms
step:218/2330 train_time:13342ms step_avg:61.20ms
step:219/2330 train_time:13401ms step_avg:61.19ms
step:220/2330 train_time:13464ms step_avg:61.20ms
step:221/2330 train_time:13523ms step_avg:61.19ms
step:222/2330 train_time:13586ms step_avg:61.20ms
step:223/2330 train_time:13646ms step_avg:61.19ms
step:224/2330 train_time:13709ms step_avg:61.20ms
step:225/2330 train_time:13769ms step_avg:61.20ms
step:226/2330 train_time:13832ms step_avg:61.20ms
step:227/2330 train_time:13892ms step_avg:61.20ms
step:228/2330 train_time:13955ms step_avg:61.21ms
step:229/2330 train_time:14016ms step_avg:61.20ms
step:230/2330 train_time:14078ms step_avg:61.21ms
step:231/2330 train_time:14138ms step_avg:61.20ms
step:232/2330 train_time:14200ms step_avg:61.21ms
step:233/2330 train_time:14260ms step_avg:61.20ms
step:234/2330 train_time:14322ms step_avg:61.21ms
step:235/2330 train_time:14382ms step_avg:61.20ms
step:236/2330 train_time:14444ms step_avg:61.20ms
step:237/2330 train_time:14504ms step_avg:61.20ms
step:238/2330 train_time:14566ms step_avg:61.20ms
step:239/2330 train_time:14626ms step_avg:61.20ms
step:240/2330 train_time:14689ms step_avg:61.20ms
step:241/2330 train_time:14750ms step_avg:61.20ms
step:242/2330 train_time:14813ms step_avg:61.21ms
step:243/2330 train_time:14873ms step_avg:61.21ms
step:244/2330 train_time:14936ms step_avg:61.21ms
step:245/2330 train_time:14996ms step_avg:61.21ms
step:246/2330 train_time:15059ms step_avg:61.21ms
step:247/2330 train_time:15118ms step_avg:61.21ms
step:248/2330 train_time:15180ms step_avg:61.21ms
step:249/2330 train_time:15240ms step_avg:61.20ms
step:250/2330 train_time:15303ms step_avg:61.21ms
step:250/2330 val_loss:4.9563 train_time:15367ms step_avg:61.47ms
step:251/2330 train_time:15390ms step_avg:61.31ms
step:252/2330 train_time:15428ms step_avg:61.22ms
step:253/2330 train_time:15493ms step_avg:61.24ms
step:254/2330 train_time:15561ms step_avg:61.26ms
step:255/2330 train_time:15623ms step_avg:61.27ms
step:256/2330 train_time:15686ms step_avg:61.27ms
step:257/2330 train_time:15745ms step_avg:61.27ms
step:258/2330 train_time:15808ms step_avg:61.27ms
step:259/2330 train_time:15867ms step_avg:61.26ms
step:260/2330 train_time:15929ms step_avg:61.27ms
step:261/2330 train_time:15988ms step_avg:61.26ms
step:262/2330 train_time:16051ms step_avg:61.26ms
step:263/2330 train_time:16110ms step_avg:61.25ms
step:264/2330 train_time:16172ms step_avg:61.26ms
step:265/2330 train_time:16231ms step_avg:61.25ms
step:266/2330 train_time:16293ms step_avg:61.25ms
step:267/2330 train_time:16353ms step_avg:61.25ms
step:268/2330 train_time:16417ms step_avg:61.26ms
step:269/2330 train_time:16479ms step_avg:61.26ms
step:270/2330 train_time:16543ms step_avg:61.27ms
step:271/2330 train_time:16603ms step_avg:61.27ms
step:272/2330 train_time:16666ms step_avg:61.27ms
step:273/2330 train_time:16726ms step_avg:61.27ms
step:274/2330 train_time:16788ms step_avg:61.27ms
step:275/2330 train_time:16847ms step_avg:61.26ms
step:276/2330 train_time:16910ms step_avg:61.27ms
step:277/2330 train_time:16969ms step_avg:61.26ms
step:278/2330 train_time:17031ms step_avg:61.26ms
step:279/2330 train_time:17090ms step_avg:61.26ms
step:280/2330 train_time:17153ms step_avg:61.26ms
step:281/2330 train_time:17212ms step_avg:61.25ms
step:282/2330 train_time:17275ms step_avg:61.26ms
step:283/2330 train_time:17335ms step_avg:61.25ms
step:284/2330 train_time:17398ms step_avg:61.26ms
step:285/2330 train_time:17459ms step_avg:61.26ms
step:286/2330 train_time:17522ms step_avg:61.27ms
step:287/2330 train_time:17583ms step_avg:61.27ms
step:288/2330 train_time:17646ms step_avg:61.27ms
step:289/2330 train_time:17706ms step_avg:61.27ms
step:290/2330 train_time:17768ms step_avg:61.27ms
step:291/2330 train_time:17828ms step_avg:61.26ms
step:292/2330 train_time:17890ms step_avg:61.27ms
step:293/2330 train_time:17950ms step_avg:61.26ms
step:294/2330 train_time:18013ms step_avg:61.27ms
step:295/2330 train_time:18073ms step_avg:61.27ms
step:296/2330 train_time:18136ms step_avg:61.27ms
step:297/2330 train_time:18196ms step_avg:61.27ms
step:298/2330 train_time:18259ms step_avg:61.27ms
step:299/2330 train_time:18318ms step_avg:61.26ms
step:300/2330 train_time:18380ms step_avg:61.27ms
step:301/2330 train_time:18442ms step_avg:61.27ms
step:302/2330 train_time:18505ms step_avg:61.27ms
step:303/2330 train_time:18564ms step_avg:61.27ms
step:304/2330 train_time:18627ms step_avg:61.27ms
step:305/2330 train_time:18687ms step_avg:61.27ms
step:306/2330 train_time:18749ms step_avg:61.27ms
step:307/2330 train_time:18810ms step_avg:61.27ms
step:308/2330 train_time:18872ms step_avg:61.27ms
step:309/2330 train_time:18932ms step_avg:61.27ms
step:310/2330 train_time:18995ms step_avg:61.27ms
step:311/2330 train_time:19056ms step_avg:61.27ms
step:312/2330 train_time:19118ms step_avg:61.28ms
step:313/2330 train_time:19178ms step_avg:61.27ms
step:314/2330 train_time:19240ms step_avg:61.27ms
step:315/2330 train_time:19299ms step_avg:61.27ms
step:316/2330 train_time:19362ms step_avg:61.27ms
step:317/2330 train_time:19423ms step_avg:61.27ms
step:318/2330 train_time:19485ms step_avg:61.27ms
step:319/2330 train_time:19545ms step_avg:61.27ms
step:320/2330 train_time:19607ms step_avg:61.27ms
step:321/2330 train_time:19667ms step_avg:61.27ms
step:322/2330 train_time:19729ms step_avg:61.27ms
step:323/2330 train_time:19789ms step_avg:61.27ms
step:324/2330 train_time:19852ms step_avg:61.27ms
step:325/2330 train_time:19912ms step_avg:61.27ms
step:326/2330 train_time:19975ms step_avg:61.27ms
step:327/2330 train_time:20036ms step_avg:61.27ms
step:328/2330 train_time:20099ms step_avg:61.28ms
step:329/2330 train_time:20159ms step_avg:61.27ms
step:330/2330 train_time:20221ms step_avg:61.28ms
step:331/2330 train_time:20281ms step_avg:61.27ms
step:332/2330 train_time:20344ms step_avg:61.28ms
step:333/2330 train_time:20404ms step_avg:61.27ms
step:334/2330 train_time:20467ms step_avg:61.28ms
step:335/2330 train_time:20527ms step_avg:61.28ms
step:336/2330 train_time:20590ms step_avg:61.28ms
step:337/2330 train_time:20650ms step_avg:61.28ms
step:338/2330 train_time:20712ms step_avg:61.28ms
step:339/2330 train_time:20772ms step_avg:61.27ms
step:340/2330 train_time:20835ms step_avg:61.28ms
step:341/2330 train_time:20894ms step_avg:61.27ms
step:342/2330 train_time:20957ms step_avg:61.28ms
step:343/2330 train_time:21017ms step_avg:61.27ms
step:344/2330 train_time:21079ms step_avg:61.28ms
step:345/2330 train_time:21140ms step_avg:61.27ms
step:346/2330 train_time:21202ms step_avg:61.28ms
step:347/2330 train_time:21262ms step_avg:61.27ms
step:348/2330 train_time:21325ms step_avg:61.28ms
step:349/2330 train_time:21384ms step_avg:61.27ms
step:350/2330 train_time:21447ms step_avg:61.28ms
step:351/2330 train_time:21507ms step_avg:61.27ms
step:352/2330 train_time:21570ms step_avg:61.28ms
step:353/2330 train_time:21629ms step_avg:61.27ms
step:354/2330 train_time:21692ms step_avg:61.28ms
step:355/2330 train_time:21752ms step_avg:61.27ms
step:356/2330 train_time:21815ms step_avg:61.28ms
step:357/2330 train_time:21874ms step_avg:61.27ms
step:358/2330 train_time:21937ms step_avg:61.28ms
step:359/2330 train_time:21998ms step_avg:61.27ms
step:360/2330 train_time:22061ms step_avg:61.28ms
step:361/2330 train_time:22120ms step_avg:61.28ms
step:362/2330 train_time:22183ms step_avg:61.28ms
step:363/2330 train_time:22243ms step_avg:61.28ms
step:364/2330 train_time:22306ms step_avg:61.28ms
step:365/2330 train_time:22366ms step_avg:61.28ms
step:366/2330 train_time:22429ms step_avg:61.28ms
step:367/2330 train_time:22488ms step_avg:61.28ms
step:368/2330 train_time:22552ms step_avg:61.28ms
step:369/2330 train_time:22611ms step_avg:61.28ms
step:370/2330 train_time:22674ms step_avg:61.28ms
step:371/2330 train_time:22734ms step_avg:61.28ms
step:372/2330 train_time:22797ms step_avg:61.28ms
step:373/2330 train_time:22857ms step_avg:61.28ms
step:374/2330 train_time:22919ms step_avg:61.28ms
step:375/2330 train_time:22979ms step_avg:61.28ms
step:376/2330 train_time:23041ms step_avg:61.28ms
step:377/2330 train_time:23102ms step_avg:61.28ms
step:378/2330 train_time:23164ms step_avg:61.28ms
step:379/2330 train_time:23224ms step_avg:61.28ms
step:380/2330 train_time:23286ms step_avg:61.28ms
step:381/2330 train_time:23346ms step_avg:61.28ms
step:382/2330 train_time:23409ms step_avg:61.28ms
step:383/2330 train_time:23469ms step_avg:61.28ms
step:384/2330 train_time:23532ms step_avg:61.28ms
step:385/2330 train_time:23592ms step_avg:61.28ms
step:386/2330 train_time:23654ms step_avg:61.28ms
step:387/2330 train_time:23714ms step_avg:61.28ms
step:388/2330 train_time:23777ms step_avg:61.28ms
step:389/2330 train_time:23837ms step_avg:61.28ms
step:390/2330 train_time:23900ms step_avg:61.28ms
step:391/2330 train_time:23959ms step_avg:61.28ms
step:392/2330 train_time:24022ms step_avg:61.28ms
step:393/2330 train_time:24083ms step_avg:61.28ms
step:394/2330 train_time:24146ms step_avg:61.28ms
step:395/2330 train_time:24206ms step_avg:61.28ms
step:396/2330 train_time:24269ms step_avg:61.29ms
step:397/2330 train_time:24329ms step_avg:61.28ms
step:398/2330 train_time:24391ms step_avg:61.28ms
step:399/2330 train_time:24451ms step_avg:61.28ms
step:400/2330 train_time:24514ms step_avg:61.28ms
step:401/2330 train_time:24573ms step_avg:61.28ms
step:402/2330 train_time:24636ms step_avg:61.28ms
step:403/2330 train_time:24696ms step_avg:61.28ms
step:404/2330 train_time:24759ms step_avg:61.28ms
step:405/2330 train_time:24818ms step_avg:61.28ms
step:406/2330 train_time:24881ms step_avg:61.28ms
step:407/2330 train_time:24941ms step_avg:61.28ms
step:408/2330 train_time:25004ms step_avg:61.28ms
step:409/2330 train_time:25064ms step_avg:61.28ms
step:410/2330 train_time:25126ms step_avg:61.28ms
step:411/2330 train_time:25186ms step_avg:61.28ms
step:412/2330 train_time:25249ms step_avg:61.28ms
step:413/2330 train_time:25309ms step_avg:61.28ms
step:414/2330 train_time:25372ms step_avg:61.28ms
step:415/2330 train_time:25432ms step_avg:61.28ms
step:416/2330 train_time:25495ms step_avg:61.29ms
step:417/2330 train_time:25555ms step_avg:61.28ms
step:418/2330 train_time:25617ms step_avg:61.29ms
step:419/2330 train_time:25677ms step_avg:61.28ms
step:420/2330 train_time:25740ms step_avg:61.28ms
step:421/2330 train_time:25800ms step_avg:61.28ms
step:422/2330 train_time:25863ms step_avg:61.29ms
step:423/2330 train_time:25923ms step_avg:61.28ms
step:424/2330 train_time:25986ms step_avg:61.29ms
step:425/2330 train_time:26045ms step_avg:61.28ms
step:426/2330 train_time:26108ms step_avg:61.29ms
step:427/2330 train_time:26167ms step_avg:61.28ms
step:428/2330 train_time:26230ms step_avg:61.29ms
step:429/2330 train_time:26290ms step_avg:61.28ms
step:430/2330 train_time:26354ms step_avg:61.29ms
step:431/2330 train_time:26414ms step_avg:61.28ms
step:432/2330 train_time:26476ms step_avg:61.29ms
step:433/2330 train_time:26536ms step_avg:61.28ms
step:434/2330 train_time:26599ms step_avg:61.29ms
step:435/2330 train_time:26658ms step_avg:61.28ms
step:436/2330 train_time:26721ms step_avg:61.29ms
step:437/2330 train_time:26781ms step_avg:61.28ms
step:438/2330 train_time:26843ms step_avg:61.29ms
step:439/2330 train_time:26903ms step_avg:61.28ms
step:440/2330 train_time:26966ms step_avg:61.29ms
step:441/2330 train_time:27025ms step_avg:61.28ms
step:442/2330 train_time:27087ms step_avg:61.28ms
step:443/2330 train_time:27147ms step_avg:61.28ms
step:444/2330 train_time:27210ms step_avg:61.28ms
step:445/2330 train_time:27269ms step_avg:61.28ms
step:446/2330 train_time:27332ms step_avg:61.28ms
step:447/2330 train_time:27393ms step_avg:61.28ms
step:448/2330 train_time:27456ms step_avg:61.29ms
step:449/2330 train_time:27516ms step_avg:61.28ms
step:450/2330 train_time:27579ms step_avg:61.29ms
step:451/2330 train_time:27639ms step_avg:61.28ms
step:452/2330 train_time:27701ms step_avg:61.29ms
step:453/2330 train_time:27761ms step_avg:61.28ms
step:454/2330 train_time:27823ms step_avg:61.28ms
step:455/2330 train_time:27883ms step_avg:61.28ms
step:456/2330 train_time:27945ms step_avg:61.28ms
step:457/2330 train_time:28006ms step_avg:61.28ms
step:458/2330 train_time:28068ms step_avg:61.28ms
step:459/2330 train_time:28128ms step_avg:61.28ms
step:460/2330 train_time:28191ms step_avg:61.28ms
step:461/2330 train_time:28251ms step_avg:61.28ms
step:462/2330 train_time:28314ms step_avg:61.29ms
step:463/2330 train_time:28374ms step_avg:61.28ms
step:464/2330 train_time:28437ms step_avg:61.29ms
step:465/2330 train_time:28497ms step_avg:61.28ms
step:466/2330 train_time:28560ms step_avg:61.29ms
step:467/2330 train_time:28620ms step_avg:61.28ms
step:468/2330 train_time:28683ms step_avg:61.29ms
step:469/2330 train_time:28742ms step_avg:61.28ms
step:470/2330 train_time:28806ms step_avg:61.29ms
step:471/2330 train_time:28865ms step_avg:61.28ms
step:472/2330 train_time:28928ms step_avg:61.29ms
step:473/2330 train_time:28987ms step_avg:61.28ms
step:474/2330 train_time:29050ms step_avg:61.29ms
step:475/2330 train_time:29110ms step_avg:61.28ms
step:476/2330 train_time:29173ms step_avg:61.29ms
step:477/2330 train_time:29233ms step_avg:61.29ms
step:478/2330 train_time:29297ms step_avg:61.29ms
step:479/2330 train_time:29357ms step_avg:61.29ms
step:480/2330 train_time:29419ms step_avg:61.29ms
step:481/2330 train_time:29479ms step_avg:61.29ms
step:482/2330 train_time:29542ms step_avg:61.29ms
step:483/2330 train_time:29602ms step_avg:61.29ms
step:484/2330 train_time:29664ms step_avg:61.29ms
step:485/2330 train_time:29724ms step_avg:61.29ms
step:486/2330 train_time:29787ms step_avg:61.29ms
step:487/2330 train_time:29847ms step_avg:61.29ms
step:488/2330 train_time:29909ms step_avg:61.29ms
step:489/2330 train_time:29969ms step_avg:61.29ms
step:490/2330 train_time:30032ms step_avg:61.29ms
step:491/2330 train_time:30092ms step_avg:61.29ms
step:492/2330 train_time:30155ms step_avg:61.29ms
step:493/2330 train_time:30215ms step_avg:61.29ms
step:494/2330 train_time:30278ms step_avg:61.29ms
step:495/2330 train_time:30338ms step_avg:61.29ms
step:496/2330 train_time:30400ms step_avg:61.29ms
step:497/2330 train_time:30461ms step_avg:61.29ms
step:498/2330 train_time:30524ms step_avg:61.29ms
step:499/2330 train_time:30584ms step_avg:61.29ms
step:500/2330 train_time:30646ms step_avg:61.29ms
step:500/2330 val_loss:4.4674 train_time:30710ms step_avg:61.42ms
step:501/2330 train_time:30733ms step_avg:61.34ms
step:502/2330 train_time:30772ms step_avg:61.30ms
step:503/2330 train_time:30834ms step_avg:61.30ms
step:504/2330 train_time:30901ms step_avg:61.31ms
step:505/2330 train_time:30961ms step_avg:61.31ms
step:506/2330 train_time:31024ms step_avg:61.31ms
step:507/2330 train_time:31083ms step_avg:61.31ms
step:508/2330 train_time:31145ms step_avg:61.31ms
step:509/2330 train_time:31204ms step_avg:61.30ms
step:510/2330 train_time:31266ms step_avg:61.31ms
step:511/2330 train_time:31325ms step_avg:61.30ms
step:512/2330 train_time:31387ms step_avg:61.30ms
step:513/2330 train_time:31446ms step_avg:61.30ms
step:514/2330 train_time:31508ms step_avg:61.30ms
step:515/2330 train_time:31567ms step_avg:61.29ms
step:516/2330 train_time:31629ms step_avg:61.30ms
step:517/2330 train_time:31690ms step_avg:61.30ms
step:518/2330 train_time:31755ms step_avg:61.30ms
step:519/2330 train_time:31817ms step_avg:61.30ms
step:520/2330 train_time:31881ms step_avg:61.31ms
step:521/2330 train_time:31941ms step_avg:61.31ms
step:522/2330 train_time:32004ms step_avg:61.31ms
step:523/2330 train_time:32064ms step_avg:61.31ms
step:524/2330 train_time:32126ms step_avg:61.31ms
step:525/2330 train_time:32186ms step_avg:61.31ms
step:526/2330 train_time:32248ms step_avg:61.31ms
step:527/2330 train_time:32307ms step_avg:61.30ms
step:528/2330 train_time:32370ms step_avg:61.31ms
step:529/2330 train_time:32429ms step_avg:61.30ms
step:530/2330 train_time:32492ms step_avg:61.31ms
step:531/2330 train_time:32552ms step_avg:61.30ms
step:532/2330 train_time:32614ms step_avg:61.31ms
step:533/2330 train_time:32676ms step_avg:61.31ms
step:534/2330 train_time:32740ms step_avg:61.31ms
step:535/2330 train_time:32802ms step_avg:61.31ms
step:536/2330 train_time:32864ms step_avg:61.31ms
step:537/2330 train_time:32925ms step_avg:61.31ms
step:538/2330 train_time:32987ms step_avg:61.31ms
step:539/2330 train_time:33047ms step_avg:61.31ms
step:540/2330 train_time:33110ms step_avg:61.32ms
step:541/2330 train_time:33170ms step_avg:61.31ms
step:542/2330 train_time:33234ms step_avg:61.32ms
step:543/2330 train_time:33293ms step_avg:61.31ms
step:544/2330 train_time:33356ms step_avg:61.32ms
step:545/2330 train_time:33416ms step_avg:61.31ms
step:546/2330 train_time:33478ms step_avg:61.32ms
step:547/2330 train_time:33538ms step_avg:61.31ms
step:548/2330 train_time:33600ms step_avg:61.31ms
step:549/2330 train_time:33660ms step_avg:61.31ms
step:550/2330 train_time:33723ms step_avg:61.31ms
step:551/2330 train_time:33783ms step_avg:61.31ms
step:552/2330 train_time:33846ms step_avg:61.32ms
step:553/2330 train_time:33906ms step_avg:61.31ms
step:554/2330 train_time:33969ms step_avg:61.31ms
step:555/2330 train_time:34029ms step_avg:61.31ms
step:556/2330 train_time:34092ms step_avg:61.32ms
step:557/2330 train_time:34152ms step_avg:61.31ms
step:558/2330 train_time:34215ms step_avg:61.32ms
step:559/2330 train_time:34275ms step_avg:61.31ms
step:560/2330 train_time:34337ms step_avg:61.32ms
step:561/2330 train_time:34397ms step_avg:61.31ms
step:562/2330 train_time:34459ms step_avg:61.32ms
step:563/2330 train_time:34518ms step_avg:61.31ms
step:564/2330 train_time:34581ms step_avg:61.31ms
step:565/2330 train_time:34641ms step_avg:61.31ms
step:566/2330 train_time:34704ms step_avg:61.32ms
step:567/2330 train_time:34764ms step_avg:61.31ms
step:568/2330 train_time:34827ms step_avg:61.31ms
step:569/2330 train_time:34887ms step_avg:61.31ms
step:570/2330 train_time:34949ms step_avg:61.31ms
step:571/2330 train_time:35010ms step_avg:61.31ms
step:572/2330 train_time:35073ms step_avg:61.32ms
step:573/2330 train_time:35133ms step_avg:61.31ms
step:574/2330 train_time:35196ms step_avg:61.32ms
step:575/2330 train_time:35256ms step_avg:61.32ms
step:576/2330 train_time:35319ms step_avg:61.32ms
step:577/2330 train_time:35378ms step_avg:61.31ms
step:578/2330 train_time:35441ms step_avg:61.32ms
step:579/2330 train_time:35500ms step_avg:61.31ms
step:580/2330 train_time:35562ms step_avg:61.31ms
step:581/2330 train_time:35622ms step_avg:61.31ms
step:582/2330 train_time:35685ms step_avg:61.31ms
step:583/2330 train_time:35745ms step_avg:61.31ms
step:584/2330 train_time:35807ms step_avg:61.31ms
step:585/2330 train_time:35866ms step_avg:61.31ms
step:586/2330 train_time:35929ms step_avg:61.31ms
step:587/2330 train_time:35989ms step_avg:61.31ms
step:588/2330 train_time:36052ms step_avg:61.31ms
step:589/2330 train_time:36113ms step_avg:61.31ms
step:590/2330 train_time:36176ms step_avg:61.32ms
step:591/2330 train_time:36237ms step_avg:61.31ms
step:592/2330 train_time:36300ms step_avg:61.32ms
step:593/2330 train_time:36360ms step_avg:61.32ms
step:594/2330 train_time:36422ms step_avg:61.32ms
step:595/2330 train_time:36482ms step_avg:61.31ms
step:596/2330 train_time:36546ms step_avg:61.32ms
step:597/2330 train_time:36606ms step_avg:61.32ms
step:598/2330 train_time:36668ms step_avg:61.32ms
step:599/2330 train_time:36728ms step_avg:61.32ms
step:600/2330 train_time:36792ms step_avg:61.32ms
step:601/2330 train_time:36851ms step_avg:61.32ms
step:602/2330 train_time:36914ms step_avg:61.32ms
step:603/2330 train_time:36975ms step_avg:61.32ms
step:604/2330 train_time:37038ms step_avg:61.32ms
step:605/2330 train_time:37098ms step_avg:61.32ms
step:606/2330 train_time:37161ms step_avg:61.32ms
step:607/2330 train_time:37220ms step_avg:61.32ms
step:608/2330 train_time:37282ms step_avg:61.32ms
step:609/2330 train_time:37342ms step_avg:61.32ms
step:610/2330 train_time:37405ms step_avg:61.32ms
step:611/2330 train_time:37465ms step_avg:61.32ms
step:612/2330 train_time:37527ms step_avg:61.32ms
step:613/2330 train_time:37587ms step_avg:61.32ms
step:614/2330 train_time:37650ms step_avg:61.32ms
step:615/2330 train_time:37709ms step_avg:61.32ms
step:616/2330 train_time:37772ms step_avg:61.32ms
step:617/2330 train_time:37832ms step_avg:61.32ms
step:618/2330 train_time:37896ms step_avg:61.32ms
step:619/2330 train_time:37956ms step_avg:61.32ms
step:620/2330 train_time:38019ms step_avg:61.32ms
step:621/2330 train_time:38079ms step_avg:61.32ms
step:622/2330 train_time:38142ms step_avg:61.32ms
step:623/2330 train_time:38202ms step_avg:61.32ms
step:624/2330 train_time:38264ms step_avg:61.32ms
step:625/2330 train_time:38324ms step_avg:61.32ms
step:626/2330 train_time:38387ms step_avg:61.32ms
step:627/2330 train_time:38447ms step_avg:61.32ms
step:628/2330 train_time:38510ms step_avg:61.32ms
step:629/2330 train_time:38570ms step_avg:61.32ms
step:630/2330 train_time:38633ms step_avg:61.32ms
step:631/2330 train_time:38692ms step_avg:61.32ms
step:632/2330 train_time:38755ms step_avg:61.32ms
step:633/2330 train_time:38815ms step_avg:61.32ms
step:634/2330 train_time:38879ms step_avg:61.32ms
step:635/2330 train_time:38938ms step_avg:61.32ms
step:636/2330 train_time:39001ms step_avg:61.32ms
step:637/2330 train_time:39060ms step_avg:61.32ms
step:638/2330 train_time:39124ms step_avg:61.32ms
step:639/2330 train_time:39184ms step_avg:61.32ms
step:640/2330 train_time:39247ms step_avg:61.32ms
step:641/2330 train_time:39307ms step_avg:61.32ms
step:642/2330 train_time:39371ms step_avg:61.32ms
step:643/2330 train_time:39431ms step_avg:61.32ms
step:644/2330 train_time:39494ms step_avg:61.33ms
step:645/2330 train_time:39554ms step_avg:61.32ms
step:646/2330 train_time:39617ms step_avg:61.33ms
step:647/2330 train_time:39677ms step_avg:61.32ms
step:648/2330 train_time:39739ms step_avg:61.33ms
step:649/2330 train_time:39799ms step_avg:61.32ms
step:650/2330 train_time:39862ms step_avg:61.33ms
step:651/2330 train_time:39921ms step_avg:61.32ms
step:652/2330 train_time:39984ms step_avg:61.33ms
step:653/2330 train_time:40043ms step_avg:61.32ms
step:654/2330 train_time:40106ms step_avg:61.32ms
step:655/2330 train_time:40166ms step_avg:61.32ms
step:656/2330 train_time:40228ms step_avg:61.32ms
step:657/2330 train_time:40288ms step_avg:61.32ms
step:658/2330 train_time:40351ms step_avg:61.32ms
step:659/2330 train_time:40411ms step_avg:61.32ms
step:660/2330 train_time:40475ms step_avg:61.33ms
step:661/2330 train_time:40535ms step_avg:61.32ms
step:662/2330 train_time:40598ms step_avg:61.33ms
step:663/2330 train_time:40658ms step_avg:61.32ms
step:664/2330 train_time:40721ms step_avg:61.33ms
step:665/2330 train_time:40781ms step_avg:61.33ms
step:666/2330 train_time:40844ms step_avg:61.33ms
step:667/2330 train_time:40904ms step_avg:61.33ms
step:668/2330 train_time:40967ms step_avg:61.33ms
step:669/2330 train_time:41026ms step_avg:61.32ms
step:670/2330 train_time:41089ms step_avg:61.33ms
step:671/2330 train_time:41149ms step_avg:61.32ms
step:672/2330 train_time:41212ms step_avg:61.33ms
step:673/2330 train_time:41272ms step_avg:61.33ms
step:674/2330 train_time:41336ms step_avg:61.33ms
step:675/2330 train_time:41396ms step_avg:61.33ms
step:676/2330 train_time:41459ms step_avg:61.33ms
step:677/2330 train_time:41520ms step_avg:61.33ms
step:678/2330 train_time:41583ms step_avg:61.33ms
step:679/2330 train_time:41642ms step_avg:61.33ms
step:680/2330 train_time:41704ms step_avg:61.33ms
step:681/2330 train_time:41763ms step_avg:61.33ms
step:682/2330 train_time:41826ms step_avg:61.33ms
step:683/2330 train_time:41886ms step_avg:61.33ms
step:684/2330 train_time:41949ms step_avg:61.33ms
step:685/2330 train_time:42008ms step_avg:61.33ms
step:686/2330 train_time:42071ms step_avg:61.33ms
step:687/2330 train_time:42131ms step_avg:61.33ms
step:688/2330 train_time:42194ms step_avg:61.33ms
step:689/2330 train_time:42254ms step_avg:61.33ms
step:690/2330 train_time:42318ms step_avg:61.33ms
step:691/2330 train_time:42378ms step_avg:61.33ms
step:692/2330 train_time:42441ms step_avg:61.33ms
step:693/2330 train_time:42500ms step_avg:61.33ms
step:694/2330 train_time:42563ms step_avg:61.33ms
step:695/2330 train_time:42623ms step_avg:61.33ms
step:696/2330 train_time:42686ms step_avg:61.33ms
step:697/2330 train_time:42746ms step_avg:61.33ms
step:698/2330 train_time:42808ms step_avg:61.33ms
step:699/2330 train_time:42868ms step_avg:61.33ms
step:700/2330 train_time:42931ms step_avg:61.33ms
step:701/2330 train_time:42991ms step_avg:61.33ms
step:702/2330 train_time:43054ms step_avg:61.33ms
step:703/2330 train_time:43115ms step_avg:61.33ms
step:704/2330 train_time:43177ms step_avg:61.33ms
step:705/2330 train_time:43238ms step_avg:61.33ms
step:706/2330 train_time:43300ms step_avg:61.33ms
step:707/2330 train_time:43360ms step_avg:61.33ms
step:708/2330 train_time:43424ms step_avg:61.33ms
step:709/2330 train_time:43484ms step_avg:61.33ms
step:710/2330 train_time:43546ms step_avg:61.33ms
step:711/2330 train_time:43606ms step_avg:61.33ms
step:712/2330 train_time:43669ms step_avg:61.33ms
step:713/2330 train_time:43729ms step_avg:61.33ms
step:714/2330 train_time:43792ms step_avg:61.33ms
step:715/2330 train_time:43852ms step_avg:61.33ms
step:716/2330 train_time:43915ms step_avg:61.33ms
step:717/2330 train_time:43975ms step_avg:61.33ms
step:718/2330 train_time:44038ms step_avg:61.33ms
step:719/2330 train_time:44098ms step_avg:61.33ms
step:720/2330 train_time:44161ms step_avg:61.33ms
step:721/2330 train_time:44221ms step_avg:61.33ms
step:722/2330 train_time:44284ms step_avg:61.34ms
step:723/2330 train_time:44344ms step_avg:61.33ms
step:724/2330 train_time:44407ms step_avg:61.34ms
step:725/2330 train_time:44467ms step_avg:61.33ms
step:726/2330 train_time:44529ms step_avg:61.34ms
step:727/2330 train_time:44589ms step_avg:61.33ms
step:728/2330 train_time:44653ms step_avg:61.34ms
step:729/2330 train_time:44713ms step_avg:61.33ms
step:730/2330 train_time:44776ms step_avg:61.34ms
step:731/2330 train_time:44836ms step_avg:61.34ms
step:732/2330 train_time:44899ms step_avg:61.34ms
step:733/2330 train_time:44958ms step_avg:61.33ms
step:734/2330 train_time:45021ms step_avg:61.34ms
step:735/2330 train_time:45081ms step_avg:61.33ms
step:736/2330 train_time:45144ms step_avg:61.34ms
step:737/2330 train_time:45203ms step_avg:61.33ms
step:738/2330 train_time:45266ms step_avg:61.34ms
step:739/2330 train_time:45325ms step_avg:61.33ms
step:740/2330 train_time:45388ms step_avg:61.34ms
step:741/2330 train_time:45448ms step_avg:61.33ms
step:742/2330 train_time:45511ms step_avg:61.34ms
step:743/2330 train_time:45571ms step_avg:61.33ms
step:744/2330 train_time:45634ms step_avg:61.34ms
step:745/2330 train_time:45694ms step_avg:61.33ms
step:746/2330 train_time:45757ms step_avg:61.34ms
step:747/2330 train_time:45817ms step_avg:61.33ms
step:748/2330 train_time:45880ms step_avg:61.34ms
step:749/2330 train_time:45940ms step_avg:61.34ms
step:750/2330 train_time:46003ms step_avg:61.34ms
step:750/2330 val_loss:4.2245 train_time:46067ms step_avg:61.42ms
step:751/2330 train_time:46089ms step_avg:61.37ms
step:752/2330 train_time:46128ms step_avg:61.34ms
step:753/2330 train_time:46193ms step_avg:61.35ms
step:754/2330 train_time:46258ms step_avg:61.35ms
step:755/2330 train_time:46318ms step_avg:61.35ms
step:756/2330 train_time:46381ms step_avg:61.35ms
step:757/2330 train_time:46440ms step_avg:61.35ms
step:758/2330 train_time:46503ms step_avg:61.35ms
step:759/2330 train_time:46562ms step_avg:61.35ms
step:760/2330 train_time:46624ms step_avg:61.35ms
step:761/2330 train_time:46683ms step_avg:61.34ms
step:762/2330 train_time:46745ms step_avg:61.34ms
step:763/2330 train_time:46803ms step_avg:61.34ms
step:764/2330 train_time:46865ms step_avg:61.34ms
step:765/2330 train_time:46924ms step_avg:61.34ms
step:766/2330 train_time:46987ms step_avg:61.34ms
step:767/2330 train_time:47049ms step_avg:61.34ms
step:768/2330 train_time:47114ms step_avg:61.35ms
step:769/2330 train_time:47177ms step_avg:61.35ms
step:770/2330 train_time:47242ms step_avg:61.35ms
step:771/2330 train_time:47303ms step_avg:61.35ms
step:772/2330 train_time:47367ms step_avg:61.36ms
step:773/2330 train_time:47429ms step_avg:61.36ms
step:774/2330 train_time:47493ms step_avg:61.36ms
step:775/2330 train_time:47554ms step_avg:61.36ms
step:776/2330 train_time:47617ms step_avg:61.36ms
step:777/2330 train_time:47678ms step_avg:61.36ms
step:778/2330 train_time:47742ms step_avg:61.36ms
step:779/2330 train_time:47801ms step_avg:61.36ms
step:780/2330 train_time:47864ms step_avg:61.36ms
step:781/2330 train_time:47925ms step_avg:61.36ms
step:782/2330 train_time:47988ms step_avg:61.37ms
step:783/2330 train_time:48048ms step_avg:61.36ms
step:784/2330 train_time:48113ms step_avg:61.37ms
step:785/2330 train_time:48175ms step_avg:61.37ms
step:786/2330 train_time:48239ms step_avg:61.37ms
step:787/2330 train_time:48300ms step_avg:61.37ms
step:788/2330 train_time:48364ms step_avg:61.38ms
step:789/2330 train_time:48425ms step_avg:61.37ms
step:790/2330 train_time:48488ms step_avg:61.38ms
step:791/2330 train_time:48550ms step_avg:61.38ms
step:792/2330 train_time:48614ms step_avg:61.38ms
step:793/2330 train_time:48676ms step_avg:61.38ms
step:794/2330 train_time:48739ms step_avg:61.38ms
step:795/2330 train_time:48800ms step_avg:61.38ms
step:796/2330 train_time:48863ms step_avg:61.39ms
step:797/2330 train_time:48923ms step_avg:61.38ms
step:798/2330 train_time:48986ms step_avg:61.39ms
step:799/2330 train_time:49047ms step_avg:61.39ms
step:800/2330 train_time:49110ms step_avg:61.39ms
step:801/2330 train_time:49171ms step_avg:61.39ms
step:802/2330 train_time:49236ms step_avg:61.39ms
step:803/2330 train_time:49297ms step_avg:61.39ms
step:804/2330 train_time:49361ms step_avg:61.39ms
step:805/2330 train_time:49422ms step_avg:61.39ms
step:806/2330 train_time:49487ms step_avg:61.40ms
step:807/2330 train_time:49547ms step_avg:61.40ms
step:808/2330 train_time:49611ms step_avg:61.40ms
step:809/2330 train_time:49672ms step_avg:61.40ms
step:810/2330 train_time:49736ms step_avg:61.40ms
step:811/2330 train_time:49797ms step_avg:61.40ms
step:812/2330 train_time:49861ms step_avg:61.40ms
step:813/2330 train_time:49921ms step_avg:61.40ms
step:814/2330 train_time:49985ms step_avg:61.41ms
step:815/2330 train_time:50045ms step_avg:61.40ms
step:816/2330 train_time:50108ms step_avg:61.41ms
step:817/2330 train_time:50168ms step_avg:61.40ms
step:818/2330 train_time:50232ms step_avg:61.41ms
step:819/2330 train_time:50294ms step_avg:61.41ms
step:820/2330 train_time:50358ms step_avg:61.41ms
step:821/2330 train_time:50420ms step_avg:61.41ms
step:822/2330 train_time:50484ms step_avg:61.42ms
step:823/2330 train_time:50544ms step_avg:61.41ms
step:824/2330 train_time:50607ms step_avg:61.42ms
step:825/2330 train_time:50669ms step_avg:61.42ms
step:826/2330 train_time:50734ms step_avg:61.42ms
step:827/2330 train_time:50795ms step_avg:61.42ms
step:828/2330 train_time:50859ms step_avg:61.42ms
step:829/2330 train_time:50919ms step_avg:61.42ms
step:830/2330 train_time:50983ms step_avg:61.43ms
step:831/2330 train_time:51043ms step_avg:61.42ms
step:832/2330 train_time:51106ms step_avg:61.43ms
step:833/2330 train_time:51166ms step_avg:61.42ms
step:834/2330 train_time:51229ms step_avg:61.43ms
step:835/2330 train_time:51290ms step_avg:61.42ms
step:836/2330 train_time:51354ms step_avg:61.43ms
step:837/2330 train_time:51416ms step_avg:61.43ms
step:838/2330 train_time:51480ms step_avg:61.43ms
step:839/2330 train_time:51541ms step_avg:61.43ms
step:840/2330 train_time:51605ms step_avg:61.43ms
step:841/2330 train_time:51665ms step_avg:61.43ms
step:842/2330 train_time:51729ms step_avg:61.44ms
step:843/2330 train_time:51790ms step_avg:61.44ms
step:844/2330 train_time:51855ms step_avg:61.44ms
step:845/2330 train_time:51916ms step_avg:61.44ms
step:846/2330 train_time:51981ms step_avg:61.44ms
step:847/2330 train_time:52041ms step_avg:61.44ms
step:848/2330 train_time:52104ms step_avg:61.44ms
step:849/2330 train_time:52164ms step_avg:61.44ms
step:850/2330 train_time:52227ms step_avg:61.44ms
step:851/2330 train_time:52288ms step_avg:61.44ms
step:852/2330 train_time:52352ms step_avg:61.45ms
step:853/2330 train_time:52413ms step_avg:61.44ms
step:854/2330 train_time:52478ms step_avg:61.45ms
step:855/2330 train_time:52538ms step_avg:61.45ms
step:856/2330 train_time:52602ms step_avg:61.45ms
step:857/2330 train_time:52663ms step_avg:61.45ms
step:858/2330 train_time:52726ms step_avg:61.45ms
step:859/2330 train_time:52786ms step_avg:61.45ms
step:860/2330 train_time:52851ms step_avg:61.45ms
step:861/2330 train_time:52913ms step_avg:61.45ms
step:862/2330 train_time:52978ms step_avg:61.46ms
step:863/2330 train_time:53038ms step_avg:61.46ms
step:864/2330 train_time:53101ms step_avg:61.46ms
step:865/2330 train_time:53161ms step_avg:61.46ms
step:866/2330 train_time:53225ms step_avg:61.46ms
step:867/2330 train_time:53285ms step_avg:61.46ms
step:868/2330 train_time:53348ms step_avg:61.46ms
step:869/2330 train_time:53408ms step_avg:61.46ms
step:870/2330 train_time:53473ms step_avg:61.46ms
step:871/2330 train_time:53535ms step_avg:61.46ms
step:872/2330 train_time:53598ms step_avg:61.47ms
step:873/2330 train_time:53659ms step_avg:61.47ms
step:874/2330 train_time:53723ms step_avg:61.47ms
step:875/2330 train_time:53784ms step_avg:61.47ms
step:876/2330 train_time:53848ms step_avg:61.47ms
step:877/2330 train_time:53909ms step_avg:61.47ms
step:878/2330 train_time:53975ms step_avg:61.47ms
step:879/2330 train_time:54036ms step_avg:61.47ms
step:880/2330 train_time:54099ms step_avg:61.48ms
step:881/2330 train_time:54160ms step_avg:61.48ms
step:882/2330 train_time:54223ms step_avg:61.48ms
step:883/2330 train_time:54284ms step_avg:61.48ms
step:884/2330 train_time:54347ms step_avg:61.48ms
step:885/2330 train_time:54407ms step_avg:61.48ms
step:886/2330 train_time:54472ms step_avg:61.48ms
step:887/2330 train_time:54533ms step_avg:61.48ms
step:888/2330 train_time:54597ms step_avg:61.48ms
step:889/2330 train_time:54659ms step_avg:61.48ms
step:890/2330 train_time:54722ms step_avg:61.49ms
step:891/2330 train_time:54783ms step_avg:61.49ms
step:892/2330 train_time:54847ms step_avg:61.49ms
step:893/2330 train_time:54907ms step_avg:61.49ms
step:894/2330 train_time:54971ms step_avg:61.49ms
step:895/2330 train_time:55033ms step_avg:61.49ms
step:896/2330 train_time:55097ms step_avg:61.49ms
step:897/2330 train_time:55159ms step_avg:61.49ms
step:898/2330 train_time:55222ms step_avg:61.49ms
step:899/2330 train_time:55283ms step_avg:61.49ms
step:900/2330 train_time:55347ms step_avg:61.50ms
step:901/2330 train_time:55407ms step_avg:61.49ms
step:902/2330 train_time:55471ms step_avg:61.50ms
step:903/2330 train_time:55532ms step_avg:61.50ms
step:904/2330 train_time:55596ms step_avg:61.50ms
step:905/2330 train_time:55657ms step_avg:61.50ms
step:906/2330 train_time:55721ms step_avg:61.50ms
step:907/2330 train_time:55782ms step_avg:61.50ms
step:908/2330 train_time:55845ms step_avg:61.50ms
step:909/2330 train_time:55906ms step_avg:61.50ms
step:910/2330 train_time:55970ms step_avg:61.51ms
step:911/2330 train_time:56031ms step_avg:61.51ms
step:912/2330 train_time:56096ms step_avg:61.51ms
step:913/2330 train_time:56158ms step_avg:61.51ms
step:914/2330 train_time:56222ms step_avg:61.51ms
step:915/2330 train_time:56283ms step_avg:61.51ms
step:916/2330 train_time:56346ms step_avg:61.51ms
step:917/2330 train_time:56407ms step_avg:61.51ms
step:918/2330 train_time:56470ms step_avg:61.51ms
step:919/2330 train_time:56531ms step_avg:61.51ms
step:920/2330 train_time:56596ms step_avg:61.52ms
step:921/2330 train_time:56657ms step_avg:61.52ms
step:922/2330 train_time:56720ms step_avg:61.52ms
step:923/2330 train_time:56781ms step_avg:61.52ms
step:924/2330 train_time:56845ms step_avg:61.52ms
step:925/2330 train_time:56904ms step_avg:61.52ms
step:926/2330 train_time:56968ms step_avg:61.52ms
step:927/2330 train_time:57029ms step_avg:61.52ms
step:928/2330 train_time:57093ms step_avg:61.52ms
step:929/2330 train_time:57156ms step_avg:61.52ms
step:930/2330 train_time:57220ms step_avg:61.53ms
step:931/2330 train_time:57281ms step_avg:61.53ms
step:932/2330 train_time:57344ms step_avg:61.53ms
step:933/2330 train_time:57405ms step_avg:61.53ms
step:934/2330 train_time:57468ms step_avg:61.53ms
step:935/2330 train_time:57528ms step_avg:61.53ms
step:936/2330 train_time:57593ms step_avg:61.53ms
step:937/2330 train_time:57654ms step_avg:61.53ms
step:938/2330 train_time:57719ms step_avg:61.53ms
step:939/2330 train_time:57780ms step_avg:61.53ms
step:940/2330 train_time:57844ms step_avg:61.54ms
step:941/2330 train_time:57903ms step_avg:61.53ms
step:942/2330 train_time:57967ms step_avg:61.54ms
step:943/2330 train_time:58027ms step_avg:61.53ms
step:944/2330 train_time:58091ms step_avg:61.54ms
step:945/2330 train_time:58152ms step_avg:61.54ms
step:946/2330 train_time:58217ms step_avg:61.54ms
step:947/2330 train_time:58277ms step_avg:61.54ms
step:948/2330 train_time:58341ms step_avg:61.54ms
step:949/2330 train_time:58402ms step_avg:61.54ms
step:950/2330 train_time:58465ms step_avg:61.54ms
step:951/2330 train_time:58525ms step_avg:61.54ms
step:952/2330 train_time:58589ms step_avg:61.54ms
step:953/2330 train_time:58650ms step_avg:61.54ms
step:954/2330 train_time:58715ms step_avg:61.55ms
step:955/2330 train_time:58777ms step_avg:61.55ms
step:956/2330 train_time:58840ms step_avg:61.55ms
step:957/2330 train_time:58900ms step_avg:61.55ms
step:958/2330 train_time:58964ms step_avg:61.55ms
step:959/2330 train_time:59025ms step_avg:61.55ms
step:960/2330 train_time:59088ms step_avg:61.55ms
step:961/2330 train_time:59150ms step_avg:61.55ms
step:962/2330 train_time:59214ms step_avg:61.55ms
step:963/2330 train_time:59276ms step_avg:61.55ms
step:964/2330 train_time:59340ms step_avg:61.56ms
step:965/2330 train_time:59400ms step_avg:61.55ms
step:966/2330 train_time:59464ms step_avg:61.56ms
step:967/2330 train_time:59524ms step_avg:61.56ms
step:968/2330 train_time:59588ms step_avg:61.56ms
step:969/2330 train_time:59648ms step_avg:61.56ms
step:970/2330 train_time:59713ms step_avg:61.56ms
step:971/2330 train_time:59774ms step_avg:61.56ms
step:972/2330 train_time:59839ms step_avg:61.56ms
step:973/2330 train_time:59899ms step_avg:61.56ms
step:974/2330 train_time:59962ms step_avg:61.56ms
step:975/2330 train_time:60023ms step_avg:61.56ms
step:976/2330 train_time:60086ms step_avg:61.56ms
step:977/2330 train_time:60146ms step_avg:61.56ms
step:978/2330 train_time:60210ms step_avg:61.56ms
step:979/2330 train_time:60271ms step_avg:61.56ms
step:980/2330 train_time:60335ms step_avg:61.57ms
step:981/2330 train_time:60397ms step_avg:61.57ms
step:982/2330 train_time:60461ms step_avg:61.57ms
step:983/2330 train_time:60521ms step_avg:61.57ms
step:984/2330 train_time:60586ms step_avg:61.57ms
step:985/2330 train_time:60646ms step_avg:61.57ms
step:986/2330 train_time:60709ms step_avg:61.57ms
step:987/2330 train_time:60771ms step_avg:61.57ms
step:988/2330 train_time:60836ms step_avg:61.57ms
step:989/2330 train_time:60897ms step_avg:61.57ms
step:990/2330 train_time:60961ms step_avg:61.58ms
step:991/2330 train_time:61021ms step_avg:61.58ms
step:992/2330 train_time:61085ms step_avg:61.58ms
step:993/2330 train_time:61146ms step_avg:61.58ms
step:994/2330 train_time:61209ms step_avg:61.58ms
step:995/2330 train_time:61270ms step_avg:61.58ms
step:996/2330 train_time:61335ms step_avg:61.58ms
step:997/2330 train_time:61396ms step_avg:61.58ms
step:998/2330 train_time:61460ms step_avg:61.58ms
step:999/2330 train_time:61521ms step_avg:61.58ms
step:1000/2330 train_time:61585ms step_avg:61.58ms
step:1000/2330 val_loss:4.0569 train_time:61650ms step_avg:61.65ms
step:1001/2330 train_time:61673ms step_avg:61.61ms
step:1002/2330 train_time:61711ms step_avg:61.59ms
step:1003/2330 train_time:61777ms step_avg:61.59ms
step:1004/2330 train_time:61843ms step_avg:61.60ms
step:1005/2330 train_time:61904ms step_avg:61.60ms
step:1006/2330 train_time:61968ms step_avg:61.60ms
step:1007/2330 train_time:62028ms step_avg:61.60ms
step:1008/2330 train_time:62090ms step_avg:61.60ms
step:1009/2330 train_time:62150ms step_avg:61.60ms
step:1010/2330 train_time:62213ms step_avg:61.60ms
step:1011/2330 train_time:62273ms step_avg:61.60ms
step:1012/2330 train_time:62336ms step_avg:61.60ms
step:1013/2330 train_time:62395ms step_avg:61.59ms
step:1014/2330 train_time:62458ms step_avg:61.60ms
step:1015/2330 train_time:62518ms step_avg:61.59ms
step:1016/2330 train_time:62586ms step_avg:61.60ms
step:1017/2330 train_time:62650ms step_avg:61.60ms
step:1018/2330 train_time:62715ms step_avg:61.61ms
step:1019/2330 train_time:62778ms step_avg:61.61ms
step:1020/2330 train_time:62844ms step_avg:61.61ms
step:1021/2330 train_time:62905ms step_avg:61.61ms
step:1022/2330 train_time:62969ms step_avg:61.61ms
step:1023/2330 train_time:63029ms step_avg:61.61ms
step:1024/2330 train_time:63092ms step_avg:61.61ms
step:1025/2330 train_time:63153ms step_avg:61.61ms
step:1026/2330 train_time:63216ms step_avg:61.61ms
step:1027/2330 train_time:63275ms step_avg:61.61ms
step:1028/2330 train_time:63338ms step_avg:61.61ms
step:1029/2330 train_time:63398ms step_avg:61.61ms
step:1030/2330 train_time:63461ms step_avg:61.61ms
step:1031/2330 train_time:63522ms step_avg:61.61ms
step:1032/2330 train_time:63586ms step_avg:61.61ms
step:1033/2330 train_time:63647ms step_avg:61.61ms
step:1034/2330 train_time:63712ms step_avg:61.62ms
step:1035/2330 train_time:63774ms step_avg:61.62ms
step:1036/2330 train_time:63839ms step_avg:61.62ms
step:1037/2330 train_time:63902ms step_avg:61.62ms
step:1038/2330 train_time:63966ms step_avg:61.62ms
step:1039/2330 train_time:64027ms step_avg:61.62ms
step:1040/2330 train_time:64091ms step_avg:61.63ms
step:1041/2330 train_time:64151ms step_avg:61.62ms
step:1042/2330 train_time:64214ms step_avg:61.63ms
step:1043/2330 train_time:64274ms step_avg:61.62ms
step:1044/2330 train_time:64337ms step_avg:61.63ms
step:1045/2330 train_time:64397ms step_avg:61.62ms
step:1046/2330 train_time:64461ms step_avg:61.63ms
step:1047/2330 train_time:64521ms step_avg:61.63ms
step:1048/2330 train_time:64585ms step_avg:61.63ms
step:1049/2330 train_time:64646ms step_avg:61.63ms
step:1050/2330 train_time:64711ms step_avg:61.63ms
step:1051/2330 train_time:64771ms step_avg:61.63ms
step:1052/2330 train_time:64836ms step_avg:61.63ms
step:1053/2330 train_time:64898ms step_avg:61.63ms
step:1054/2330 train_time:64962ms step_avg:61.63ms
step:1055/2330 train_time:65023ms step_avg:61.63ms
step:1056/2330 train_time:65086ms step_avg:61.63ms
step:1057/2330 train_time:65147ms step_avg:61.63ms
step:1058/2330 train_time:65211ms step_avg:61.64ms
step:1059/2330 train_time:65271ms step_avg:61.63ms
step:1060/2330 train_time:65334ms step_avg:61.64ms
step:1061/2330 train_time:65394ms step_avg:61.63ms
step:1062/2330 train_time:65457ms step_avg:61.64ms
step:1063/2330 train_time:65519ms step_avg:61.64ms
step:1064/2330 train_time:65583ms step_avg:61.64ms
step:1065/2330 train_time:65644ms step_avg:61.64ms
step:1066/2330 train_time:65707ms step_avg:61.64ms
step:1067/2330 train_time:65768ms step_avg:61.64ms
step:1068/2330 train_time:65832ms step_avg:61.64ms
step:1069/2330 train_time:65893ms step_avg:61.64ms
step:1070/2330 train_time:65957ms step_avg:61.64ms
step:1071/2330 train_time:66019ms step_avg:61.64ms
step:1072/2330 train_time:66083ms step_avg:61.64ms
step:1073/2330 train_time:66143ms step_avg:61.64ms
step:1074/2330 train_time:66206ms step_avg:61.64ms
step:1075/2330 train_time:66267ms step_avg:61.64ms
step:1076/2330 train_time:66330ms step_avg:61.64ms
step:1077/2330 train_time:66390ms step_avg:61.64ms
step:1078/2330 train_time:66453ms step_avg:61.64ms
step:1079/2330 train_time:66513ms step_avg:61.64ms
step:1080/2330 train_time:66577ms step_avg:61.65ms
step:1081/2330 train_time:66638ms step_avg:61.64ms
step:1082/2330 train_time:66702ms step_avg:61.65ms
step:1083/2330 train_time:66763ms step_avg:61.65ms
step:1084/2330 train_time:66827ms step_avg:61.65ms
step:1085/2330 train_time:66888ms step_avg:61.65ms
step:1086/2330 train_time:66952ms step_avg:61.65ms
step:1087/2330 train_time:67013ms step_avg:61.65ms
step:1088/2330 train_time:67076ms step_avg:61.65ms
step:1089/2330 train_time:67138ms step_avg:61.65ms
step:1090/2330 train_time:67202ms step_avg:61.65ms
step:1091/2330 train_time:67263ms step_avg:61.65ms
step:1092/2330 train_time:67326ms step_avg:61.65ms
step:1093/2330 train_time:67387ms step_avg:61.65ms
step:1094/2330 train_time:67450ms step_avg:61.65ms
step:1095/2330 train_time:67511ms step_avg:61.65ms
step:1096/2330 train_time:67574ms step_avg:61.66ms
step:1097/2330 train_time:67636ms step_avg:61.65ms
step:1098/2330 train_time:67701ms step_avg:61.66ms
step:1099/2330 train_time:67762ms step_avg:61.66ms
step:1100/2330 train_time:67826ms step_avg:61.66ms
step:1101/2330 train_time:67888ms step_avg:61.66ms
step:1102/2330 train_time:67951ms step_avg:61.66ms
step:1103/2330 train_time:68011ms step_avg:61.66ms
step:1104/2330 train_time:68075ms step_avg:61.66ms
step:1105/2330 train_time:68136ms step_avg:61.66ms
step:1106/2330 train_time:68200ms step_avg:61.66ms
step:1107/2330 train_time:68261ms step_avg:61.66ms
step:1108/2330 train_time:68325ms step_avg:61.67ms
step:1109/2330 train_time:68385ms step_avg:61.66ms
step:1110/2330 train_time:68449ms step_avg:61.67ms
step:1111/2330 train_time:68509ms step_avg:61.66ms
step:1112/2330 train_time:68573ms step_avg:61.67ms
step:1113/2330 train_time:68633ms step_avg:61.66ms
step:1114/2330 train_time:68697ms step_avg:61.67ms
step:1115/2330 train_time:68758ms step_avg:61.67ms
step:1116/2330 train_time:68823ms step_avg:61.67ms
step:1117/2330 train_time:68883ms step_avg:61.67ms
step:1118/2330 train_time:68947ms step_avg:61.67ms
step:1119/2330 train_time:69008ms step_avg:61.67ms
step:1120/2330 train_time:69071ms step_avg:61.67ms
step:1121/2330 train_time:69131ms step_avg:61.67ms
step:1122/2330 train_time:69195ms step_avg:61.67ms
step:1123/2330 train_time:69256ms step_avg:61.67ms
step:1124/2330 train_time:69321ms step_avg:61.67ms
step:1125/2330 train_time:69383ms step_avg:61.67ms
step:1126/2330 train_time:69446ms step_avg:61.68ms
step:1127/2330 train_time:69507ms step_avg:61.67ms
step:1128/2330 train_time:69571ms step_avg:61.68ms
step:1129/2330 train_time:69631ms step_avg:61.68ms
step:1130/2330 train_time:69695ms step_avg:61.68ms
step:1131/2330 train_time:69756ms step_avg:61.68ms
step:1132/2330 train_time:69820ms step_avg:61.68ms
step:1133/2330 train_time:69880ms step_avg:61.68ms
step:1134/2330 train_time:69944ms step_avg:61.68ms
step:1135/2330 train_time:70005ms step_avg:61.68ms
step:1136/2330 train_time:70069ms step_avg:61.68ms
step:1137/2330 train_time:70129ms step_avg:61.68ms
step:1138/2330 train_time:70192ms step_avg:61.68ms
step:1139/2330 train_time:70253ms step_avg:61.68ms
step:1140/2330 train_time:70317ms step_avg:61.68ms
step:1141/2330 train_time:70379ms step_avg:61.68ms
step:1142/2330 train_time:70443ms step_avg:61.68ms
step:1143/2330 train_time:70504ms step_avg:61.68ms
step:1144/2330 train_time:70567ms step_avg:61.68ms
step:1145/2330 train_time:70627ms step_avg:61.68ms
step:1146/2330 train_time:70690ms step_avg:61.68ms
step:1147/2330 train_time:70750ms step_avg:61.68ms
step:1148/2330 train_time:70814ms step_avg:61.68ms
step:1149/2330 train_time:70875ms step_avg:61.68ms
step:1150/2330 train_time:70939ms step_avg:61.69ms
step:1151/2330 train_time:71001ms step_avg:61.69ms
step:1152/2330 train_time:71065ms step_avg:61.69ms
step:1153/2330 train_time:71125ms step_avg:61.69ms
step:1154/2330 train_time:71189ms step_avg:61.69ms
step:1155/2330 train_time:71250ms step_avg:61.69ms
step:1156/2330 train_time:71313ms step_avg:61.69ms
step:1157/2330 train_time:71373ms step_avg:61.69ms
step:1158/2330 train_time:71437ms step_avg:61.69ms
step:1159/2330 train_time:71498ms step_avg:61.69ms
step:1160/2330 train_time:71563ms step_avg:61.69ms
step:1161/2330 train_time:71623ms step_avg:61.69ms
step:1162/2330 train_time:71686ms step_avg:61.69ms
step:1163/2330 train_time:71747ms step_avg:61.69ms
step:1164/2330 train_time:71811ms step_avg:61.69ms
step:1165/2330 train_time:71871ms step_avg:61.69ms
step:1166/2330 train_time:71934ms step_avg:61.69ms
step:1167/2330 train_time:71996ms step_avg:61.69ms
step:1168/2330 train_time:72060ms step_avg:61.70ms
step:1169/2330 train_time:72122ms step_avg:61.70ms
step:1170/2330 train_time:72185ms step_avg:61.70ms
step:1171/2330 train_time:72245ms step_avg:61.70ms
step:1172/2330 train_time:72310ms step_avg:61.70ms
step:1173/2330 train_time:72371ms step_avg:61.70ms
step:1174/2330 train_time:72434ms step_avg:61.70ms
step:1175/2330 train_time:72495ms step_avg:61.70ms
step:1176/2330 train_time:72559ms step_avg:61.70ms
step:1177/2330 train_time:72620ms step_avg:61.70ms
step:1178/2330 train_time:72683ms step_avg:61.70ms
step:1179/2330 train_time:72744ms step_avg:61.70ms
step:1180/2330 train_time:72808ms step_avg:61.70ms
step:1181/2330 train_time:72869ms step_avg:61.70ms
step:1182/2330 train_time:72932ms step_avg:61.70ms
step:1183/2330 train_time:72993ms step_avg:61.70ms
step:1184/2330 train_time:73058ms step_avg:61.70ms
step:1185/2330 train_time:73119ms step_avg:61.70ms
step:1186/2330 train_time:73184ms step_avg:61.71ms
step:1187/2330 train_time:73245ms step_avg:61.71ms
step:1188/2330 train_time:73309ms step_avg:61.71ms
step:1189/2330 train_time:73369ms step_avg:61.71ms
step:1190/2330 train_time:73433ms step_avg:61.71ms
step:1191/2330 train_time:73493ms step_avg:61.71ms
step:1192/2330 train_time:73558ms step_avg:61.71ms
step:1193/2330 train_time:73619ms step_avg:61.71ms
step:1194/2330 train_time:73683ms step_avg:61.71ms
step:1195/2330 train_time:73744ms step_avg:61.71ms
step:1196/2330 train_time:73808ms step_avg:61.71ms
step:1197/2330 train_time:73868ms step_avg:61.71ms
step:1198/2330 train_time:73931ms step_avg:61.71ms
step:1199/2330 train_time:73992ms step_avg:61.71ms
step:1200/2330 train_time:74055ms step_avg:61.71ms
step:1201/2330 train_time:74117ms step_avg:61.71ms
step:1202/2330 train_time:74181ms step_avg:61.71ms
step:1203/2330 train_time:74242ms step_avg:61.71ms
step:1204/2330 train_time:74307ms step_avg:61.72ms
step:1205/2330 train_time:74367ms step_avg:61.72ms
step:1206/2330 train_time:74431ms step_avg:61.72ms
step:1207/2330 train_time:74491ms step_avg:61.72ms
step:1208/2330 train_time:74555ms step_avg:61.72ms
step:1209/2330 train_time:74615ms step_avg:61.72ms
step:1210/2330 train_time:74681ms step_avg:61.72ms
step:1211/2330 train_time:74742ms step_avg:61.72ms
step:1212/2330 train_time:74807ms step_avg:61.72ms
step:1213/2330 train_time:74867ms step_avg:61.72ms
step:1214/2330 train_time:74931ms step_avg:61.72ms
step:1215/2330 train_time:74990ms step_avg:61.72ms
step:1216/2330 train_time:75054ms step_avg:61.72ms
step:1217/2330 train_time:75115ms step_avg:61.72ms
step:1218/2330 train_time:75180ms step_avg:61.72ms
step:1219/2330 train_time:75242ms step_avg:61.72ms
step:1220/2330 train_time:75306ms step_avg:61.73ms
step:1221/2330 train_time:75366ms step_avg:61.73ms
step:1222/2330 train_time:75430ms step_avg:61.73ms
step:1223/2330 train_time:75491ms step_avg:61.73ms
step:1224/2330 train_time:75555ms step_avg:61.73ms
step:1225/2330 train_time:75616ms step_avg:61.73ms
step:1226/2330 train_time:75680ms step_avg:61.73ms
step:1227/2330 train_time:75742ms step_avg:61.73ms
step:1228/2330 train_time:75807ms step_avg:61.73ms
step:1229/2330 train_time:75867ms step_avg:61.73ms
step:1230/2330 train_time:75930ms step_avg:61.73ms
step:1231/2330 train_time:75991ms step_avg:61.73ms
step:1232/2330 train_time:76054ms step_avg:61.73ms
step:1233/2330 train_time:76115ms step_avg:61.73ms
step:1234/2330 train_time:76179ms step_avg:61.73ms
step:1235/2330 train_time:76241ms step_avg:61.73ms
step:1236/2330 train_time:76305ms step_avg:61.74ms
step:1237/2330 train_time:76365ms step_avg:61.73ms
step:1238/2330 train_time:76430ms step_avg:61.74ms
step:1239/2330 train_time:76490ms step_avg:61.74ms
step:1240/2330 train_time:76553ms step_avg:61.74ms
step:1241/2330 train_time:76613ms step_avg:61.73ms
step:1242/2330 train_time:76677ms step_avg:61.74ms
step:1243/2330 train_time:76739ms step_avg:61.74ms
step:1244/2330 train_time:76804ms step_avg:61.74ms
step:1245/2330 train_time:76864ms step_avg:61.74ms
step:1246/2330 train_time:76927ms step_avg:61.74ms
step:1247/2330 train_time:76988ms step_avg:61.74ms
step:1248/2330 train_time:77051ms step_avg:61.74ms
step:1249/2330 train_time:77111ms step_avg:61.74ms
step:1250/2330 train_time:77175ms step_avg:61.74ms
step:1250/2330 val_loss:3.9976 train_time:77242ms step_avg:61.79ms
step:1251/2330 train_time:77265ms step_avg:61.76ms
step:1252/2330 train_time:77303ms step_avg:61.74ms
step:1253/2330 train_time:77369ms step_avg:61.75ms
step:1254/2330 train_time:77435ms step_avg:61.75ms
step:1255/2330 train_time:77496ms step_avg:61.75ms
step:1256/2330 train_time:77559ms step_avg:61.75ms
step:1257/2330 train_time:77619ms step_avg:61.75ms
step:1258/2330 train_time:77682ms step_avg:61.75ms
step:1259/2330 train_time:77742ms step_avg:61.75ms
step:1260/2330 train_time:77805ms step_avg:61.75ms
step:1261/2330 train_time:77864ms step_avg:61.75ms
step:1262/2330 train_time:77927ms step_avg:61.75ms
step:1263/2330 train_time:77987ms step_avg:61.75ms
step:1264/2330 train_time:78051ms step_avg:61.75ms
step:1265/2330 train_time:78110ms step_avg:61.75ms
step:1266/2330 train_time:78174ms step_avg:61.75ms
step:1267/2330 train_time:78235ms step_avg:61.75ms
step:1268/2330 train_time:78300ms step_avg:61.75ms
step:1269/2330 train_time:78362ms step_avg:61.75ms
step:1270/2330 train_time:78427ms step_avg:61.75ms
step:1271/2330 train_time:78489ms step_avg:61.75ms
step:1272/2330 train_time:78552ms step_avg:61.76ms
step:1273/2330 train_time:78613ms step_avg:61.75ms
step:1274/2330 train_time:78677ms step_avg:61.76ms
step:1275/2330 train_time:78737ms step_avg:61.75ms
step:1276/2330 train_time:78800ms step_avg:61.76ms
step:1277/2330 train_time:78860ms step_avg:61.75ms
step:1278/2330 train_time:78923ms step_avg:61.75ms
step:1279/2330 train_time:78982ms step_avg:61.75ms
step:1280/2330 train_time:79046ms step_avg:61.75ms
step:1281/2330 train_time:79106ms step_avg:61.75ms
step:1282/2330 train_time:79170ms step_avg:61.76ms
step:1283/2330 train_time:79232ms step_avg:61.76ms
step:1284/2330 train_time:79296ms step_avg:61.76ms
step:1285/2330 train_time:79357ms step_avg:61.76ms
step:1286/2330 train_time:79421ms step_avg:61.76ms
step:1287/2330 train_time:79483ms step_avg:61.76ms
step:1288/2330 train_time:79547ms step_avg:61.76ms
step:1289/2330 train_time:79608ms step_avg:61.76ms
step:1290/2330 train_time:79672ms step_avg:61.76ms
step:1291/2330 train_time:79733ms step_avg:61.76ms
step:1292/2330 train_time:79796ms step_avg:61.76ms
step:1293/2330 train_time:79857ms step_avg:61.76ms
step:1294/2330 train_time:79920ms step_avg:61.76ms
step:1295/2330 train_time:79980ms step_avg:61.76ms
step:1296/2330 train_time:80043ms step_avg:61.76ms
step:1297/2330 train_time:80103ms step_avg:61.76ms
step:1298/2330 train_time:80167ms step_avg:61.76ms
step:1299/2330 train_time:80227ms step_avg:61.76ms
step:1300/2330 train_time:80291ms step_avg:61.76ms
step:1301/2330 train_time:80352ms step_avg:61.76ms
step:1302/2330 train_time:80416ms step_avg:61.76ms
step:1303/2330 train_time:80477ms step_avg:61.76ms
step:1304/2330 train_time:80542ms step_avg:61.77ms
step:1305/2330 train_time:80603ms step_avg:61.77ms
step:1306/2330 train_time:80667ms step_avg:61.77ms
step:1307/2330 train_time:80729ms step_avg:61.77ms
step:1308/2330 train_time:80793ms step_avg:61.77ms
step:1309/2330 train_time:80853ms step_avg:61.77ms
step:1310/2330 train_time:80917ms step_avg:61.77ms
step:1311/2330 train_time:80977ms step_avg:61.77ms
step:1312/2330 train_time:81041ms step_avg:61.77ms
step:1313/2330 train_time:81101ms step_avg:61.77ms
step:1314/2330 train_time:81165ms step_avg:61.77ms
step:1315/2330 train_time:81225ms step_avg:61.77ms
step:1316/2330 train_time:81289ms step_avg:61.77ms
step:1317/2330 train_time:81350ms step_avg:61.77ms
step:1318/2330 train_time:81413ms step_avg:61.77ms
step:1319/2330 train_time:81474ms step_avg:61.77ms
step:1320/2330 train_time:81539ms step_avg:61.77ms
step:1321/2330 train_time:81600ms step_avg:61.77ms
step:1322/2330 train_time:81663ms step_avg:61.77ms
step:1323/2330 train_time:81725ms step_avg:61.77ms
step:1324/2330 train_time:81790ms step_avg:61.77ms
step:1325/2330 train_time:81851ms step_avg:61.77ms
step:1326/2330 train_time:81914ms step_avg:61.78ms
step:1327/2330 train_time:81974ms step_avg:61.77ms
step:1328/2330 train_time:82038ms step_avg:61.78ms
step:1329/2330 train_time:82099ms step_avg:61.77ms
step:1330/2330 train_time:82162ms step_avg:61.78ms
step:1331/2330 train_time:82222ms step_avg:61.77ms
step:1332/2330 train_time:82286ms step_avg:61.78ms
step:1333/2330 train_time:82347ms step_avg:61.78ms
step:1334/2330 train_time:82412ms step_avg:61.78ms
step:1335/2330 train_time:82473ms step_avg:61.78ms
step:1336/2330 train_time:82537ms step_avg:61.78ms
step:1337/2330 train_time:82597ms step_avg:61.78ms
step:1338/2330 train_time:82661ms step_avg:61.78ms
step:1339/2330 train_time:82721ms step_avg:61.78ms
step:1340/2330 train_time:82785ms step_avg:61.78ms
step:1341/2330 train_time:82846ms step_avg:61.78ms
step:1342/2330 train_time:82910ms step_avg:61.78ms
step:1343/2330 train_time:82972ms step_avg:61.78ms
step:1344/2330 train_time:83035ms step_avg:61.78ms
step:1345/2330 train_time:83096ms step_avg:61.78ms
step:1346/2330 train_time:83160ms step_avg:61.78ms
step:1347/2330 train_time:83219ms step_avg:61.78ms
step:1348/2330 train_time:83282ms step_avg:61.78ms
step:1349/2330 train_time:83343ms step_avg:61.78ms
step:1350/2330 train_time:83406ms step_avg:61.78ms
step:1351/2330 train_time:83467ms step_avg:61.78ms
step:1352/2330 train_time:83532ms step_avg:61.78ms
step:1353/2330 train_time:83592ms step_avg:61.78ms
step:1354/2330 train_time:83656ms step_avg:61.78ms
step:1355/2330 train_time:83718ms step_avg:61.78ms
step:1356/2330 train_time:83781ms step_avg:61.79ms
step:1357/2330 train_time:83842ms step_avg:61.78ms
step:1358/2330 train_time:83906ms step_avg:61.79ms
step:1359/2330 train_time:83967ms step_avg:61.79ms
step:1360/2330 train_time:84031ms step_avg:61.79ms
step:1361/2330 train_time:84092ms step_avg:61.79ms
step:1362/2330 train_time:84155ms step_avg:61.79ms
step:1363/2330 train_time:84216ms step_avg:61.79ms
step:1364/2330 train_time:84279ms step_avg:61.79ms
step:1365/2330 train_time:84340ms step_avg:61.79ms
step:1366/2330 train_time:84403ms step_avg:61.79ms
step:1367/2330 train_time:84463ms step_avg:61.79ms
step:1368/2330 train_time:84528ms step_avg:61.79ms
step:1369/2330 train_time:84590ms step_avg:61.79ms
step:1370/2330 train_time:84653ms step_avg:61.79ms
step:1371/2330 train_time:84713ms step_avg:61.79ms
step:1372/2330 train_time:84776ms step_avg:61.79ms
step:1373/2330 train_time:84838ms step_avg:61.79ms
step:1374/2330 train_time:84901ms step_avg:61.79ms
step:1375/2330 train_time:84961ms step_avg:61.79ms
step:1376/2330 train_time:85025ms step_avg:61.79ms
step:1377/2330 train_time:85086ms step_avg:61.79ms
step:1378/2330 train_time:85150ms step_avg:61.79ms
step:1379/2330 train_time:85210ms step_avg:61.79ms
step:1380/2330 train_time:85274ms step_avg:61.79ms
step:1381/2330 train_time:85335ms step_avg:61.79ms
step:1382/2330 train_time:85399ms step_avg:61.79ms
step:1383/2330 train_time:85460ms step_avg:61.79ms
step:1384/2330 train_time:85523ms step_avg:61.79ms
step:1385/2330 train_time:85584ms step_avg:61.79ms
step:1386/2330 train_time:85648ms step_avg:61.79ms
step:1387/2330 train_time:85709ms step_avg:61.79ms
step:1388/2330 train_time:85773ms step_avg:61.80ms
step:1389/2330 train_time:85834ms step_avg:61.80ms
step:1390/2330 train_time:85898ms step_avg:61.80ms
step:1391/2330 train_time:85958ms step_avg:61.80ms
step:1392/2330 train_time:86021ms step_avg:61.80ms
step:1393/2330 train_time:86081ms step_avg:61.80ms
step:1394/2330 train_time:86146ms step_avg:61.80ms
step:1395/2330 train_time:86207ms step_avg:61.80ms
step:1396/2330 train_time:86271ms step_avg:61.80ms
step:1397/2330 train_time:86332ms step_avg:61.80ms
step:1398/2330 train_time:86395ms step_avg:61.80ms
step:1399/2330 train_time:86455ms step_avg:61.80ms
step:1400/2330 train_time:86519ms step_avg:61.80ms
step:1401/2330 train_time:86579ms step_avg:61.80ms
step:1402/2330 train_time:86642ms step_avg:61.80ms
step:1403/2330 train_time:86703ms step_avg:61.80ms
step:1404/2330 train_time:86768ms step_avg:61.80ms
step:1405/2330 train_time:86830ms step_avg:61.80ms
step:1406/2330 train_time:86894ms step_avg:61.80ms
step:1407/2330 train_time:86954ms step_avg:61.80ms
step:1408/2330 train_time:87018ms step_avg:61.80ms
step:1409/2330 train_time:87078ms step_avg:61.80ms
step:1410/2330 train_time:87141ms step_avg:61.80ms
step:1411/2330 train_time:87202ms step_avg:61.80ms
step:1412/2330 train_time:87266ms step_avg:61.80ms
step:1413/2330 train_time:87327ms step_avg:61.80ms
step:1414/2330 train_time:87391ms step_avg:61.80ms
step:1415/2330 train_time:87452ms step_avg:61.80ms
step:1416/2330 train_time:87515ms step_avg:61.80ms
step:1417/2330 train_time:87576ms step_avg:61.80ms
step:1418/2330 train_time:87640ms step_avg:61.81ms
step:1419/2330 train_time:87700ms step_avg:61.80ms
step:1420/2330 train_time:87763ms step_avg:61.80ms
step:1421/2330 train_time:87824ms step_avg:61.80ms
step:1422/2330 train_time:87888ms step_avg:61.81ms
step:1423/2330 train_time:87949ms step_avg:61.81ms
step:1424/2330 train_time:88013ms step_avg:61.81ms
step:1425/2330 train_time:88073ms step_avg:61.81ms
step:1426/2330 train_time:88137ms step_avg:61.81ms
step:1427/2330 train_time:88198ms step_avg:61.81ms
step:1428/2330 train_time:88261ms step_avg:61.81ms
step:1429/2330 train_time:88321ms step_avg:61.81ms
step:1430/2330 train_time:88384ms step_avg:61.81ms
step:1431/2330 train_time:88445ms step_avg:61.81ms
step:1432/2330 train_time:88509ms step_avg:61.81ms
step:1433/2330 train_time:88570ms step_avg:61.81ms
step:1434/2330 train_time:88635ms step_avg:61.81ms
step:1435/2330 train_time:88695ms step_avg:61.81ms
step:1436/2330 train_time:88759ms step_avg:61.81ms
step:1437/2330 train_time:88819ms step_avg:61.81ms
step:1438/2330 train_time:88882ms step_avg:61.81ms
step:1439/2330 train_time:88943ms step_avg:61.81ms
step:1440/2330 train_time:89007ms step_avg:61.81ms
step:1441/2330 train_time:89068ms step_avg:61.81ms
step:1442/2330 train_time:89132ms step_avg:61.81ms
step:1443/2330 train_time:89192ms step_avg:61.81ms
step:1444/2330 train_time:89256ms step_avg:61.81ms
step:1445/2330 train_time:89316ms step_avg:61.81ms
step:1446/2330 train_time:89380ms step_avg:61.81ms
step:1447/2330 train_time:89440ms step_avg:61.81ms
step:1448/2330 train_time:89504ms step_avg:61.81ms
step:1449/2330 train_time:89565ms step_avg:61.81ms
step:1450/2330 train_time:89630ms step_avg:61.81ms
step:1451/2330 train_time:89691ms step_avg:61.81ms
step:1452/2330 train_time:89754ms step_avg:61.81ms
step:1453/2330 train_time:89814ms step_avg:61.81ms
step:1454/2330 train_time:89879ms step_avg:61.81ms
step:1455/2330 train_time:89940ms step_avg:61.81ms
step:1456/2330 train_time:90002ms step_avg:61.81ms
step:1457/2330 train_time:90063ms step_avg:61.81ms
step:1458/2330 train_time:90127ms step_avg:61.82ms
step:1459/2330 train_time:90189ms step_avg:61.82ms
step:1460/2330 train_time:90253ms step_avg:61.82ms
step:1461/2330 train_time:90313ms step_avg:61.82ms
step:1462/2330 train_time:90377ms step_avg:61.82ms
step:1463/2330 train_time:90437ms step_avg:61.82ms
step:1464/2330 train_time:90501ms step_avg:61.82ms
step:1465/2330 train_time:90561ms step_avg:61.82ms
step:1466/2330 train_time:90624ms step_avg:61.82ms
step:1467/2330 train_time:90685ms step_avg:61.82ms
step:1468/2330 train_time:90750ms step_avg:61.82ms
step:1469/2330 train_time:90810ms step_avg:61.82ms
step:1470/2330 train_time:90873ms step_avg:61.82ms
step:1471/2330 train_time:90934ms step_avg:61.82ms
step:1472/2330 train_time:90998ms step_avg:61.82ms
step:1473/2330 train_time:91059ms step_avg:61.82ms
step:1474/2330 train_time:91122ms step_avg:61.82ms
step:1475/2330 train_time:91182ms step_avg:61.82ms
step:1476/2330 train_time:91246ms step_avg:61.82ms
step:1477/2330 train_time:91307ms step_avg:61.82ms
step:1478/2330 train_time:91372ms step_avg:61.82ms
step:1479/2330 train_time:91433ms step_avg:61.82ms
step:1480/2330 train_time:91497ms step_avg:61.82ms
step:1481/2330 train_time:91557ms step_avg:61.82ms
step:1482/2330 train_time:91620ms step_avg:61.82ms
step:1483/2330 train_time:91681ms step_avg:61.82ms
step:1484/2330 train_time:91744ms step_avg:61.82ms
step:1485/2330 train_time:91805ms step_avg:61.82ms
step:1486/2330 train_time:91870ms step_avg:61.82ms
step:1487/2330 train_time:91931ms step_avg:61.82ms
step:1488/2330 train_time:91995ms step_avg:61.82ms
step:1489/2330 train_time:92055ms step_avg:61.82ms
step:1490/2330 train_time:92119ms step_avg:61.82ms
step:1491/2330 train_time:92179ms step_avg:61.82ms
step:1492/2330 train_time:92242ms step_avg:61.82ms
step:1493/2330 train_time:92302ms step_avg:61.82ms
step:1494/2330 train_time:92366ms step_avg:61.82ms
step:1495/2330 train_time:92428ms step_avg:61.82ms
step:1496/2330 train_time:92492ms step_avg:61.83ms
step:1497/2330 train_time:92553ms step_avg:61.83ms
step:1498/2330 train_time:92616ms step_avg:61.83ms
step:1499/2330 train_time:92677ms step_avg:61.83ms
step:1500/2330 train_time:92741ms step_avg:61.83ms
step:1500/2330 val_loss:3.8944 train_time:92805ms step_avg:61.87ms
step:1501/2330 train_time:92829ms step_avg:61.84ms
step:1502/2330 train_time:92867ms step_avg:61.83ms
step:1503/2330 train_time:92932ms step_avg:61.83ms
step:1504/2330 train_time:92997ms step_avg:61.83ms
step:1505/2330 train_time:93057ms step_avg:61.83ms
step:1506/2330 train_time:93122ms step_avg:61.83ms
step:1507/2330 train_time:93181ms step_avg:61.83ms
step:1508/2330 train_time:93245ms step_avg:61.83ms
step:1509/2330 train_time:93305ms step_avg:61.83ms
step:1510/2330 train_time:93367ms step_avg:61.83ms
step:1511/2330 train_time:93427ms step_avg:61.83ms
step:1512/2330 train_time:93490ms step_avg:61.83ms
step:1513/2330 train_time:93550ms step_avg:61.83ms
step:1514/2330 train_time:93613ms step_avg:61.83ms
step:1515/2330 train_time:93672ms step_avg:61.83ms
step:1516/2330 train_time:93737ms step_avg:61.83ms
step:1517/2330 train_time:93800ms step_avg:61.83ms
step:1518/2330 train_time:93866ms step_avg:61.84ms
step:1519/2330 train_time:93927ms step_avg:61.83ms
step:1520/2330 train_time:93992ms step_avg:61.84ms
step:1521/2330 train_time:94053ms step_avg:61.84ms
step:1522/2330 train_time:94117ms step_avg:61.84ms
step:1523/2330 train_time:94178ms step_avg:61.84ms
step:1524/2330 train_time:94241ms step_avg:61.84ms
step:1525/2330 train_time:94301ms step_avg:61.84ms
step:1526/2330 train_time:94365ms step_avg:61.84ms
step:1527/2330 train_time:94424ms step_avg:61.84ms
step:1528/2330 train_time:94487ms step_avg:61.84ms
step:1529/2330 train_time:94547ms step_avg:61.84ms
step:1530/2330 train_time:94611ms step_avg:61.84ms
step:1531/2330 train_time:94671ms step_avg:61.84ms
step:1532/2330 train_time:94735ms step_avg:61.84ms
step:1533/2330 train_time:94797ms step_avg:61.84ms
step:1534/2330 train_time:94863ms step_avg:61.84ms
step:1535/2330 train_time:94924ms step_avg:61.84ms
step:1536/2330 train_time:94990ms step_avg:61.84ms
step:1537/2330 train_time:95052ms step_avg:61.84ms
step:1538/2330 train_time:95116ms step_avg:61.84ms
step:1539/2330 train_time:95177ms step_avg:61.84ms
step:1540/2330 train_time:95241ms step_avg:61.85ms
step:1541/2330 train_time:95302ms step_avg:61.84ms
step:1542/2330 train_time:95366ms step_avg:61.85ms
step:1543/2330 train_time:95427ms step_avg:61.85ms
step:1544/2330 train_time:95491ms step_avg:61.85ms
step:1545/2330 train_time:95551ms step_avg:61.85ms
step:1546/2330 train_time:95615ms step_avg:61.85ms
step:1547/2330 train_time:95675ms step_avg:61.85ms
step:1548/2330 train_time:95740ms step_avg:61.85ms
step:1549/2330 train_time:95802ms step_avg:61.85ms
step:1550/2330 train_time:95868ms step_avg:61.85ms
step:1551/2330 train_time:95929ms step_avg:61.85ms
step:1552/2330 train_time:95995ms step_avg:61.85ms
step:1553/2330 train_time:96056ms step_avg:61.85ms
step:1554/2330 train_time:96120ms step_avg:61.85ms
step:1555/2330 train_time:96181ms step_avg:61.85ms
step:1556/2330 train_time:96246ms step_avg:61.85ms
step:1557/2330 train_time:96306ms step_avg:61.85ms
step:1558/2330 train_time:96370ms step_avg:61.86ms
step:1559/2330 train_time:96431ms step_avg:61.85ms
step:1560/2330 train_time:96494ms step_avg:61.86ms
step:1561/2330 train_time:96555ms step_avg:61.85ms
step:1562/2330 train_time:96619ms step_avg:61.86ms
step:1563/2330 train_time:96680ms step_avg:61.86ms
step:1564/2330 train_time:96744ms step_avg:61.86ms
step:1565/2330 train_time:96806ms step_avg:61.86ms
step:1566/2330 train_time:96870ms step_avg:61.86ms
step:1567/2330 train_time:96932ms step_avg:61.86ms
step:1568/2330 train_time:96996ms step_avg:61.86ms
step:1569/2330 train_time:97057ms step_avg:61.86ms
step:1570/2330 train_time:97122ms step_avg:61.86ms
step:1571/2330 train_time:97184ms step_avg:61.86ms
step:1572/2330 train_time:97248ms step_avg:61.86ms
step:1573/2330 train_time:97309ms step_avg:61.86ms
step:1574/2330 train_time:97373ms step_avg:61.86ms
step:1575/2330 train_time:97433ms step_avg:61.86ms
step:1576/2330 train_time:97497ms step_avg:61.86ms
step:1577/2330 train_time:97558ms step_avg:61.86ms
step:1578/2330 train_time:97623ms step_avg:61.87ms
step:1579/2330 train_time:97684ms step_avg:61.86ms
step:1580/2330 train_time:97748ms step_avg:61.87ms
step:1581/2330 train_time:97809ms step_avg:61.87ms
step:1582/2330 train_time:97874ms step_avg:61.87ms
step:1583/2330 train_time:97935ms step_avg:61.87ms
step:1584/2330 train_time:98000ms step_avg:61.87ms
step:1585/2330 train_time:98061ms step_avg:61.87ms
step:1586/2330 train_time:98126ms step_avg:61.87ms
step:1587/2330 train_time:98187ms step_avg:61.87ms
step:1588/2330 train_time:98251ms step_avg:61.87ms
step:1589/2330 train_time:98311ms step_avg:61.87ms
step:1590/2330 train_time:98376ms step_avg:61.87ms
step:1591/2330 train_time:98436ms step_avg:61.87ms
step:1592/2330 train_time:98500ms step_avg:61.87ms
step:1593/2330 train_time:98561ms step_avg:61.87ms
step:1594/2330 train_time:98626ms step_avg:61.87ms
step:1595/2330 train_time:98686ms step_avg:61.87ms
step:1596/2330 train_time:98750ms step_avg:61.87ms
step:1597/2330 train_time:98812ms step_avg:61.87ms
step:1598/2330 train_time:98876ms step_avg:61.87ms
step:1599/2330 train_time:98937ms step_avg:61.87ms
step:1600/2330 train_time:99001ms step_avg:61.88ms
step:1601/2330 train_time:99062ms step_avg:61.88ms
step:1602/2330 train_time:99127ms step_avg:61.88ms
step:1603/2330 train_time:99188ms step_avg:61.88ms
step:1604/2330 train_time:99252ms step_avg:61.88ms
step:1605/2330 train_time:99312ms step_avg:61.88ms
step:1606/2330 train_time:99377ms step_avg:61.88ms
step:1607/2330 train_time:99437ms step_avg:61.88ms
step:1608/2330 train_time:99501ms step_avg:61.88ms
step:1609/2330 train_time:99563ms step_avg:61.88ms
step:1610/2330 train_time:99628ms step_avg:61.88ms
step:1611/2330 train_time:99688ms step_avg:61.88ms
step:1612/2330 train_time:99753ms step_avg:61.88ms
step:1613/2330 train_time:99815ms step_avg:61.88ms
step:1614/2330 train_time:99878ms step_avg:61.88ms
step:1615/2330 train_time:99940ms step_avg:61.88ms
step:1616/2330 train_time:100004ms step_avg:61.88ms
step:1617/2330 train_time:100066ms step_avg:61.88ms
step:1618/2330 train_time:100130ms step_avg:61.88ms
step:1619/2330 train_time:100190ms step_avg:61.88ms
step:1620/2330 train_time:100254ms step_avg:61.89ms
step:1621/2330 train_time:100314ms step_avg:61.88ms
step:1622/2330 train_time:100378ms step_avg:61.89ms
step:1623/2330 train_time:100439ms step_avg:61.88ms
step:1624/2330 train_time:100503ms step_avg:61.89ms
step:1625/2330 train_time:100565ms step_avg:61.89ms
step:1626/2330 train_time:100629ms step_avg:61.89ms
step:1627/2330 train_time:100690ms step_avg:61.89ms
step:1628/2330 train_time:100754ms step_avg:61.89ms
step:1629/2330 train_time:100815ms step_avg:61.89ms
step:1630/2330 train_time:100879ms step_avg:61.89ms
step:1631/2330 train_time:100941ms step_avg:61.89ms
step:1632/2330 train_time:101006ms step_avg:61.89ms
step:1633/2330 train_time:101068ms step_avg:61.89ms
step:1634/2330 train_time:101132ms step_avg:61.89ms
step:1635/2330 train_time:101193ms step_avg:61.89ms
step:1636/2330 train_time:101256ms step_avg:61.89ms
step:1637/2330 train_time:101317ms step_avg:61.89ms
step:1638/2330 train_time:101381ms step_avg:61.89ms
step:1639/2330 train_time:101443ms step_avg:61.89ms
step:1640/2330 train_time:101507ms step_avg:61.89ms
step:1641/2330 train_time:101568ms step_avg:61.89ms
step:1642/2330 train_time:101633ms step_avg:61.90ms
step:1643/2330 train_time:101694ms step_avg:61.90ms
step:1644/2330 train_time:101758ms step_avg:61.90ms
step:1645/2330 train_time:101819ms step_avg:61.90ms
step:1646/2330 train_time:101884ms step_avg:61.90ms
step:1647/2330 train_time:101945ms step_avg:61.90ms
step:1648/2330 train_time:102010ms step_avg:61.90ms
step:1649/2330 train_time:102071ms step_avg:61.90ms
step:1650/2330 train_time:102135ms step_avg:61.90ms
step:1651/2330 train_time:102196ms step_avg:61.90ms
step:1652/2330 train_time:102260ms step_avg:61.90ms
step:1653/2330 train_time:102321ms step_avg:61.90ms
step:1654/2330 train_time:102385ms step_avg:61.90ms
step:1655/2330 train_time:102446ms step_avg:61.90ms
step:1656/2330 train_time:102510ms step_avg:61.90ms
step:1657/2330 train_time:102571ms step_avg:61.90ms
step:1658/2330 train_time:102635ms step_avg:61.90ms
step:1659/2330 train_time:102696ms step_avg:61.90ms
step:1660/2330 train_time:102760ms step_avg:61.90ms
step:1661/2330 train_time:102821ms step_avg:61.90ms
step:1662/2330 train_time:102884ms step_avg:61.90ms
step:1663/2330 train_time:102946ms step_avg:61.90ms
step:1664/2330 train_time:103010ms step_avg:61.90ms
step:1665/2330 train_time:103070ms step_avg:61.90ms
step:1666/2330 train_time:103134ms step_avg:61.91ms
step:1667/2330 train_time:103195ms step_avg:61.90ms
step:1668/2330 train_time:103260ms step_avg:61.91ms
step:1669/2330 train_time:103321ms step_avg:61.91ms
step:1670/2330 train_time:103385ms step_avg:61.91ms
step:1671/2330 train_time:103447ms step_avg:61.91ms
step:1672/2330 train_time:103510ms step_avg:61.91ms
step:1673/2330 train_time:103571ms step_avg:61.91ms
step:1674/2330 train_time:103635ms step_avg:61.91ms
step:1675/2330 train_time:103696ms step_avg:61.91ms
step:1676/2330 train_time:103760ms step_avg:61.91ms
step:1677/2330 train_time:103821ms step_avg:61.91ms
step:1678/2330 train_time:103886ms step_avg:61.91ms
step:1679/2330 train_time:103947ms step_avg:61.91ms
step:1680/2330 train_time:104011ms step_avg:61.91ms
step:1681/2330 train_time:104072ms step_avg:61.91ms
step:1682/2330 train_time:104136ms step_avg:61.91ms
step:1683/2330 train_time:104197ms step_avg:61.91ms
step:1684/2330 train_time:104263ms step_avg:61.91ms
step:1685/2330 train_time:104324ms step_avg:61.91ms
step:1686/2330 train_time:104388ms step_avg:61.91ms
step:1687/2330 train_time:104450ms step_avg:61.91ms
step:1688/2330 train_time:104514ms step_avg:61.92ms
step:1689/2330 train_time:104574ms step_avg:61.91ms
step:1690/2330 train_time:104638ms step_avg:61.92ms
step:1691/2330 train_time:104699ms step_avg:61.92ms
step:1692/2330 train_time:104764ms step_avg:61.92ms
step:1693/2330 train_time:104826ms step_avg:61.92ms
step:1694/2330 train_time:104890ms step_avg:61.92ms
step:1695/2330 train_time:104951ms step_avg:61.92ms
step:1696/2330 train_time:105014ms step_avg:61.92ms
step:1697/2330 train_time:105075ms step_avg:61.92ms
step:1698/2330 train_time:105139ms step_avg:61.92ms
step:1699/2330 train_time:105201ms step_avg:61.92ms
step:1700/2330 train_time:105266ms step_avg:61.92ms
step:1701/2330 train_time:105327ms step_avg:61.92ms
step:1702/2330 train_time:105391ms step_avg:61.92ms
step:1703/2330 train_time:105452ms step_avg:61.92ms
step:1704/2330 train_time:105516ms step_avg:61.92ms
step:1705/2330 train_time:105577ms step_avg:61.92ms
step:1706/2330 train_time:105641ms step_avg:61.92ms
step:1707/2330 train_time:105703ms step_avg:61.92ms
step:1708/2330 train_time:105767ms step_avg:61.92ms
step:1709/2330 train_time:105828ms step_avg:61.92ms
step:1710/2330 train_time:105893ms step_avg:61.93ms
step:1711/2330 train_time:105954ms step_avg:61.93ms
step:1712/2330 train_time:106018ms step_avg:61.93ms
step:1713/2330 train_time:106079ms step_avg:61.93ms
step:1714/2330 train_time:106143ms step_avg:61.93ms
step:1715/2330 train_time:106204ms step_avg:61.93ms
step:1716/2330 train_time:106269ms step_avg:61.93ms
step:1717/2330 train_time:106329ms step_avg:61.93ms
step:1718/2330 train_time:106393ms step_avg:61.93ms
step:1719/2330 train_time:106454ms step_avg:61.93ms
step:1720/2330 train_time:106519ms step_avg:61.93ms
step:1721/2330 train_time:106580ms step_avg:61.93ms
step:1722/2330 train_time:106644ms step_avg:61.93ms
step:1723/2330 train_time:106705ms step_avg:61.93ms
step:1724/2330 train_time:106770ms step_avg:61.93ms
step:1725/2330 train_time:106830ms step_avg:61.93ms
step:1726/2330 train_time:106894ms step_avg:61.93ms
step:1727/2330 train_time:106956ms step_avg:61.93ms
step:1728/2330 train_time:107020ms step_avg:61.93ms
step:1729/2330 train_time:107082ms step_avg:61.93ms
step:1730/2330 train_time:107146ms step_avg:61.93ms
step:1731/2330 train_time:107208ms step_avg:61.93ms
step:1732/2330 train_time:107272ms step_avg:61.94ms
step:1733/2330 train_time:107332ms step_avg:61.93ms
step:1734/2330 train_time:107396ms step_avg:61.94ms
step:1735/2330 train_time:107457ms step_avg:61.93ms
step:1736/2330 train_time:107521ms step_avg:61.94ms
step:1737/2330 train_time:107582ms step_avg:61.94ms
step:1738/2330 train_time:107647ms step_avg:61.94ms
step:1739/2330 train_time:107708ms step_avg:61.94ms
step:1740/2330 train_time:107772ms step_avg:61.94ms
step:1741/2330 train_time:107833ms step_avg:61.94ms
step:1742/2330 train_time:107897ms step_avg:61.94ms
step:1743/2330 train_time:107958ms step_avg:61.94ms
step:1744/2330 train_time:108023ms step_avg:61.94ms
step:1745/2330 train_time:108084ms step_avg:61.94ms
step:1746/2330 train_time:108149ms step_avg:61.94ms
step:1747/2330 train_time:108210ms step_avg:61.94ms
step:1748/2330 train_time:108274ms step_avg:61.94ms
step:1749/2330 train_time:108335ms step_avg:61.94ms
step:1750/2330 train_time:108399ms step_avg:61.94ms
step:1750/2330 val_loss:3.8049 train_time:108466ms step_avg:61.98ms
step:1751/2330 train_time:108488ms step_avg:61.96ms
step:1752/2330 train_time:108528ms step_avg:61.95ms
step:1753/2330 train_time:108594ms step_avg:61.95ms
step:1754/2330 train_time:108663ms step_avg:61.95ms
step:1755/2330 train_time:108724ms step_avg:61.95ms
step:1756/2330 train_time:108789ms step_avg:61.95ms
step:1757/2330 train_time:108848ms step_avg:61.95ms
step:1758/2330 train_time:108912ms step_avg:61.95ms
step:1759/2330 train_time:108973ms step_avg:61.95ms
step:1760/2330 train_time:109037ms step_avg:61.95ms
step:1761/2330 train_time:109097ms step_avg:61.95ms
step:1762/2330 train_time:109160ms step_avg:61.95ms
step:1763/2330 train_time:109220ms step_avg:61.95ms
step:1764/2330 train_time:109283ms step_avg:61.95ms
step:1765/2330 train_time:109343ms step_avg:61.95ms
step:1766/2330 train_time:109408ms step_avg:61.95ms
step:1767/2330 train_time:109470ms step_avg:61.95ms
step:1768/2330 train_time:109536ms step_avg:61.95ms
step:1769/2330 train_time:109599ms step_avg:61.96ms
step:1770/2330 train_time:109665ms step_avg:61.96ms
step:1771/2330 train_time:109726ms step_avg:61.96ms
step:1772/2330 train_time:109789ms step_avg:61.96ms
step:1773/2330 train_time:109850ms step_avg:61.96ms
step:1774/2330 train_time:109914ms step_avg:61.96ms
step:1775/2330 train_time:109974ms step_avg:61.96ms
step:1776/2330 train_time:110039ms step_avg:61.96ms
step:1777/2330 train_time:110099ms step_avg:61.96ms
step:1778/2330 train_time:110163ms step_avg:61.96ms
step:1779/2330 train_time:110223ms step_avg:61.96ms
step:1780/2330 train_time:110287ms step_avg:61.96ms
step:1781/2330 train_time:110347ms step_avg:61.96ms
step:1782/2330 train_time:110410ms step_avg:61.96ms
step:1783/2330 train_time:110472ms step_avg:61.96ms
step:1784/2330 train_time:110538ms step_avg:61.96ms
step:1785/2330 train_time:110600ms step_avg:61.96ms
step:1786/2330 train_time:110664ms step_avg:61.96ms
step:1787/2330 train_time:110726ms step_avg:61.96ms
step:1788/2330 train_time:110791ms step_avg:61.96ms
step:1789/2330 train_time:110851ms step_avg:61.96ms
step:1790/2330 train_time:110915ms step_avg:61.96ms
step:1791/2330 train_time:110976ms step_avg:61.96ms
step:1792/2330 train_time:111040ms step_avg:61.96ms
step:1793/2330 train_time:111100ms step_avg:61.96ms
step:1794/2330 train_time:111164ms step_avg:61.96ms
step:1795/2330 train_time:111226ms step_avg:61.96ms
step:1796/2330 train_time:111290ms step_avg:61.97ms
step:1797/2330 train_time:111350ms step_avg:61.96ms
step:1798/2330 train_time:111414ms step_avg:61.97ms
step:1799/2330 train_time:111475ms step_avg:61.97ms
step:1800/2330 train_time:111541ms step_avg:61.97ms
step:1801/2330 train_time:111603ms step_avg:61.97ms
step:1802/2330 train_time:111667ms step_avg:61.97ms
step:1803/2330 train_time:111728ms step_avg:61.97ms
step:1804/2330 train_time:111792ms step_avg:61.97ms
step:1805/2330 train_time:111852ms step_avg:61.97ms
step:1806/2330 train_time:111917ms step_avg:61.97ms
step:1807/2330 train_time:111979ms step_avg:61.97ms
step:1808/2330 train_time:112043ms step_avg:61.97ms
step:1809/2330 train_time:112104ms step_avg:61.97ms
step:1810/2330 train_time:112167ms step_avg:61.97ms
step:1811/2330 train_time:112228ms step_avg:61.97ms
step:1812/2330 train_time:112291ms step_avg:61.97ms
step:1813/2330 train_time:112352ms step_avg:61.97ms
step:1814/2330 train_time:112416ms step_avg:61.97ms
step:1815/2330 train_time:112478ms step_avg:61.97ms
step:1816/2330 train_time:112543ms step_avg:61.97ms
step:1817/2330 train_time:112604ms step_avg:61.97ms
step:1818/2330 train_time:112668ms step_avg:61.97ms
step:1819/2330 train_time:112729ms step_avg:61.97ms
step:1820/2330 train_time:112794ms step_avg:61.97ms
step:1821/2330 train_time:112854ms step_avg:61.97ms
step:1822/2330 train_time:112918ms step_avg:61.98ms
step:1823/2330 train_time:112980ms step_avg:61.97ms
step:1824/2330 train_time:113043ms step_avg:61.98ms
step:1825/2330 train_time:113104ms step_avg:61.97ms
step:1826/2330 train_time:113168ms step_avg:61.98ms
step:1827/2330 train_time:113229ms step_avg:61.98ms
step:1828/2330 train_time:113292ms step_avg:61.98ms
step:1829/2330 train_time:113353ms step_avg:61.98ms
step:1830/2330 train_time:113418ms step_avg:61.98ms
step:1831/2330 train_time:113480ms step_avg:61.98ms
step:1832/2330 train_time:113545ms step_avg:61.98ms
step:1833/2330 train_time:113605ms step_avg:61.98ms
step:1834/2330 train_time:113669ms step_avg:61.98ms
step:1835/2330 train_time:113730ms step_avg:61.98ms
step:1836/2330 train_time:113794ms step_avg:61.98ms
step:1837/2330 train_time:113856ms step_avg:61.98ms
step:1838/2330 train_time:113920ms step_avg:61.98ms
step:1839/2330 train_time:113981ms step_avg:61.98ms
step:1840/2330 train_time:114045ms step_avg:61.98ms
step:1841/2330 train_time:114106ms step_avg:61.98ms
step:1842/2330 train_time:114170ms step_avg:61.98ms
step:1843/2330 train_time:114231ms step_avg:61.98ms
step:1844/2330 train_time:114295ms step_avg:61.98ms
step:1845/2330 train_time:114356ms step_avg:61.98ms
step:1846/2330 train_time:114420ms step_avg:61.98ms
step:1847/2330 train_time:114482ms step_avg:61.98ms
step:1848/2330 train_time:114546ms step_avg:61.98ms
step:1849/2330 train_time:114607ms step_avg:61.98ms
step:1850/2330 train_time:114671ms step_avg:61.98ms
step:1851/2330 train_time:114732ms step_avg:61.98ms
step:1852/2330 train_time:114797ms step_avg:61.99ms
step:1853/2330 train_time:114859ms step_avg:61.99ms
step:1854/2330 train_time:114923ms step_avg:61.99ms
step:1855/2330 train_time:114985ms step_avg:61.99ms
step:1856/2330 train_time:115049ms step_avg:61.99ms
step:1857/2330 train_time:115110ms step_avg:61.99ms
step:1858/2330 train_time:115174ms step_avg:61.99ms
step:1859/2330 train_time:115236ms step_avg:61.99ms
step:1860/2330 train_time:115299ms step_avg:61.99ms
step:1861/2330 train_time:115360ms step_avg:61.99ms
step:1862/2330 train_time:115424ms step_avg:61.99ms
step:1863/2330 train_time:115486ms step_avg:61.99ms
step:1864/2330 train_time:115550ms step_avg:61.99ms
step:1865/2330 train_time:115611ms step_avg:61.99ms
step:1866/2330 train_time:115675ms step_avg:61.99ms
step:1867/2330 train_time:115736ms step_avg:61.99ms
step:1868/2330 train_time:115801ms step_avg:61.99ms
step:1869/2330 train_time:115862ms step_avg:61.99ms
step:1870/2330 train_time:115926ms step_avg:61.99ms
step:1871/2330 train_time:115987ms step_avg:61.99ms
step:1872/2330 train_time:116051ms step_avg:61.99ms
step:1873/2330 train_time:116112ms step_avg:61.99ms
step:1874/2330 train_time:116176ms step_avg:61.99ms
step:1875/2330 train_time:116238ms step_avg:61.99ms
step:1876/2330 train_time:116302ms step_avg:61.99ms
step:1877/2330 train_time:116363ms step_avg:61.99ms
step:1878/2330 train_time:116427ms step_avg:62.00ms
step:1879/2330 train_time:116488ms step_avg:61.99ms
step:1880/2330 train_time:116552ms step_avg:62.00ms
step:1881/2330 train_time:116612ms step_avg:61.99ms
step:1882/2330 train_time:116676ms step_avg:62.00ms
step:1883/2330 train_time:116738ms step_avg:62.00ms
step:1884/2330 train_time:116802ms step_avg:62.00ms
step:1885/2330 train_time:116863ms step_avg:62.00ms
step:1886/2330 train_time:116927ms step_avg:62.00ms
step:1887/2330 train_time:116988ms step_avg:62.00ms
step:1888/2330 train_time:117051ms step_avg:62.00ms
step:1889/2330 train_time:117112ms step_avg:62.00ms
step:1890/2330 train_time:117176ms step_avg:62.00ms
step:1891/2330 train_time:117237ms step_avg:62.00ms
step:1892/2330 train_time:117302ms step_avg:62.00ms
step:1893/2330 train_time:117363ms step_avg:62.00ms
step:1894/2330 train_time:117427ms step_avg:62.00ms
step:1895/2330 train_time:117488ms step_avg:62.00ms
step:1896/2330 train_time:117552ms step_avg:62.00ms
step:1897/2330 train_time:117613ms step_avg:62.00ms
step:1898/2330 train_time:117677ms step_avg:62.00ms
step:1899/2330 train_time:117739ms step_avg:62.00ms
step:1900/2330 train_time:117803ms step_avg:62.00ms
step:1901/2330 train_time:117865ms step_avg:62.00ms
step:1902/2330 train_time:117929ms step_avg:62.00ms
step:1903/2330 train_time:117990ms step_avg:62.00ms
step:1904/2330 train_time:118053ms step_avg:62.00ms
step:1905/2330 train_time:118114ms step_avg:62.00ms
step:1906/2330 train_time:118178ms step_avg:62.00ms
step:1907/2330 train_time:118241ms step_avg:62.00ms
step:1908/2330 train_time:118304ms step_avg:62.00ms
step:1909/2330 train_time:118366ms step_avg:62.00ms
step:1910/2330 train_time:118430ms step_avg:62.01ms
step:1911/2330 train_time:118490ms step_avg:62.00ms
step:1912/2330 train_time:118554ms step_avg:62.01ms
step:1913/2330 train_time:118615ms step_avg:62.00ms
step:1914/2330 train_time:118680ms step_avg:62.01ms
step:1915/2330 train_time:118741ms step_avg:62.01ms
step:1916/2330 train_time:118805ms step_avg:62.01ms
step:1917/2330 train_time:118867ms step_avg:62.01ms
step:1918/2330 train_time:118931ms step_avg:62.01ms
step:1919/2330 train_time:118991ms step_avg:62.01ms
step:1920/2330 train_time:119055ms step_avg:62.01ms
step:1921/2330 train_time:119116ms step_avg:62.01ms
step:1922/2330 train_time:119182ms step_avg:62.01ms
step:1923/2330 train_time:119243ms step_avg:62.01ms
step:1924/2330 train_time:119307ms step_avg:62.01ms
step:1925/2330 train_time:119368ms step_avg:62.01ms
step:1926/2330 train_time:119431ms step_avg:62.01ms
step:1927/2330 train_time:119493ms step_avg:62.01ms
step:1928/2330 train_time:119557ms step_avg:62.01ms
step:1929/2330 train_time:119618ms step_avg:62.01ms
step:1930/2330 train_time:119683ms step_avg:62.01ms
step:1931/2330 train_time:119744ms step_avg:62.01ms
step:1932/2330 train_time:119809ms step_avg:62.01ms
step:1933/2330 train_time:119869ms step_avg:62.01ms
step:1934/2330 train_time:119933ms step_avg:62.01ms
step:1935/2330 train_time:119995ms step_avg:62.01ms
step:1936/2330 train_time:120059ms step_avg:62.01ms
step:1937/2330 train_time:120121ms step_avg:62.01ms
step:1938/2330 train_time:120186ms step_avg:62.02ms
step:1939/2330 train_time:120246ms step_avg:62.01ms
step:1940/2330 train_time:120311ms step_avg:62.02ms
step:1941/2330 train_time:120372ms step_avg:62.02ms
step:1942/2330 train_time:120435ms step_avg:62.02ms
step:1943/2330 train_time:120497ms step_avg:62.02ms
step:1944/2330 train_time:120562ms step_avg:62.02ms
step:1945/2330 train_time:120623ms step_avg:62.02ms
step:1946/2330 train_time:120687ms step_avg:62.02ms
step:1947/2330 train_time:120748ms step_avg:62.02ms
step:1948/2330 train_time:120812ms step_avg:62.02ms
step:1949/2330 train_time:120872ms step_avg:62.02ms
step:1950/2330 train_time:120936ms step_avg:62.02ms
step:1951/2330 train_time:120997ms step_avg:62.02ms
step:1952/2330 train_time:121062ms step_avg:62.02ms
step:1953/2330 train_time:121123ms step_avg:62.02ms
step:1954/2330 train_time:121187ms step_avg:62.02ms
step:1955/2330 train_time:121249ms step_avg:62.02ms
step:1956/2330 train_time:121312ms step_avg:62.02ms
step:1957/2330 train_time:121374ms step_avg:62.02ms
step:1958/2330 train_time:121438ms step_avg:62.02ms
step:1959/2330 train_time:121500ms step_avg:62.02ms
step:1960/2330 train_time:121564ms step_avg:62.02ms
step:1961/2330 train_time:121625ms step_avg:62.02ms
step:1962/2330 train_time:121690ms step_avg:62.02ms
step:1963/2330 train_time:121751ms step_avg:62.02ms
step:1964/2330 train_time:121815ms step_avg:62.02ms
step:1965/2330 train_time:121876ms step_avg:62.02ms
step:1966/2330 train_time:121941ms step_avg:62.02ms
step:1967/2330 train_time:122002ms step_avg:62.02ms
step:1968/2330 train_time:122066ms step_avg:62.03ms
step:1969/2330 train_time:122127ms step_avg:62.02ms
step:1970/2330 train_time:122191ms step_avg:62.03ms
step:1971/2330 train_time:122252ms step_avg:62.03ms
step:1972/2330 train_time:122316ms step_avg:62.03ms
step:1973/2330 train_time:122377ms step_avg:62.03ms
step:1974/2330 train_time:122441ms step_avg:62.03ms
step:1975/2330 train_time:122502ms step_avg:62.03ms
step:1976/2330 train_time:122567ms step_avg:62.03ms
step:1977/2330 train_time:122628ms step_avg:62.03ms
step:1978/2330 train_time:122692ms step_avg:62.03ms
step:1979/2330 train_time:122753ms step_avg:62.03ms
step:1980/2330 train_time:122817ms step_avg:62.03ms
step:1981/2330 train_time:122878ms step_avg:62.03ms
step:1982/2330 train_time:122943ms step_avg:62.03ms
step:1983/2330 train_time:123003ms step_avg:62.03ms
step:1984/2330 train_time:123068ms step_avg:62.03ms
step:1985/2330 train_time:123129ms step_avg:62.03ms
step:1986/2330 train_time:123193ms step_avg:62.03ms
step:1987/2330 train_time:123254ms step_avg:62.03ms
step:1988/2330 train_time:123318ms step_avg:62.03ms
step:1989/2330 train_time:123379ms step_avg:62.03ms
step:1990/2330 train_time:123443ms step_avg:62.03ms
step:1991/2330 train_time:123505ms step_avg:62.03ms
step:1992/2330 train_time:123569ms step_avg:62.03ms
step:1993/2330 train_time:123630ms step_avg:62.03ms
step:1994/2330 train_time:123694ms step_avg:62.03ms
step:1995/2330 train_time:123755ms step_avg:62.03ms
step:1996/2330 train_time:123820ms step_avg:62.03ms
step:1997/2330 train_time:123882ms step_avg:62.03ms
step:1998/2330 train_time:123947ms step_avg:62.04ms
step:1999/2330 train_time:124007ms step_avg:62.03ms
step:2000/2330 train_time:124071ms step_avg:62.04ms
step:2000/2330 val_loss:3.7623 train_time:124137ms step_avg:62.07ms
step:2001/2330 train_time:124161ms step_avg:62.05ms
step:2002/2330 train_time:124203ms step_avg:62.04ms
step:2003/2330 train_time:124270ms step_avg:62.04ms
step:2004/2330 train_time:124335ms step_avg:62.04ms
step:2005/2330 train_time:124396ms step_avg:62.04ms
step:2006/2330 train_time:124460ms step_avg:62.04ms
step:2007/2330 train_time:124521ms step_avg:62.04ms
step:2008/2330 train_time:124584ms step_avg:62.04ms
step:2009/2330 train_time:124644ms step_avg:62.04ms
step:2010/2330 train_time:124707ms step_avg:62.04ms
step:2011/2330 train_time:124767ms step_avg:62.04ms
step:2012/2330 train_time:124831ms step_avg:62.04ms
step:2013/2330 train_time:124891ms step_avg:62.04ms
step:2014/2330 train_time:124956ms step_avg:62.04ms
step:2015/2330 train_time:125015ms step_avg:62.04ms
step:2016/2330 train_time:125079ms step_avg:62.04ms
step:2017/2330 train_time:125141ms step_avg:62.04ms
step:2018/2330 train_time:125208ms step_avg:62.05ms
step:2019/2330 train_time:125271ms step_avg:62.05ms
step:2020/2330 train_time:125336ms step_avg:62.05ms
step:2021/2330 train_time:125398ms step_avg:62.05ms
step:2022/2330 train_time:125463ms step_avg:62.05ms
step:2023/2330 train_time:125523ms step_avg:62.05ms
step:2024/2330 train_time:125587ms step_avg:62.05ms
step:2025/2330 train_time:125648ms step_avg:62.05ms
step:2026/2330 train_time:125711ms step_avg:62.05ms
step:2027/2330 train_time:125771ms step_avg:62.05ms
step:2028/2330 train_time:125834ms step_avg:62.05ms
step:2029/2330 train_time:125894ms step_avg:62.05ms
step:2030/2330 train_time:125958ms step_avg:62.05ms
step:2031/2330 train_time:126019ms step_avg:62.05ms
step:2032/2330 train_time:126082ms step_avg:62.05ms
step:2033/2330 train_time:126143ms step_avg:62.05ms
step:2034/2330 train_time:126209ms step_avg:62.05ms
step:2035/2330 train_time:126271ms step_avg:62.05ms
step:2036/2330 train_time:126336ms step_avg:62.05ms
step:2037/2330 train_time:126398ms step_avg:62.05ms
step:2038/2330 train_time:126463ms step_avg:62.05ms
step:2039/2330 train_time:126523ms step_avg:62.05ms
step:2040/2330 train_time:126587ms step_avg:62.05ms
step:2041/2330 train_time:126648ms step_avg:62.05ms
step:2042/2330 train_time:126712ms step_avg:62.05ms
step:2043/2330 train_time:126773ms step_avg:62.05ms
step:2044/2330 train_time:126837ms step_avg:62.05ms
step:2045/2330 train_time:126898ms step_avg:62.05ms
step:2046/2330 train_time:126961ms step_avg:62.05ms
step:2047/2330 train_time:127022ms step_avg:62.05ms
step:2048/2330 train_time:127086ms step_avg:62.05ms
step:2049/2330 train_time:127147ms step_avg:62.05ms
step:2050/2330 train_time:127213ms step_avg:62.05ms
step:2051/2330 train_time:127275ms step_avg:62.06ms
step:2052/2330 train_time:127340ms step_avg:62.06ms
step:2053/2330 train_time:127402ms step_avg:62.06ms
step:2054/2330 train_time:127466ms step_avg:62.06ms
step:2055/2330 train_time:127527ms step_avg:62.06ms
step:2056/2330 train_time:127591ms step_avg:62.06ms
step:2057/2330 train_time:127653ms step_avg:62.06ms
step:2058/2330 train_time:127717ms step_avg:62.06ms
step:2059/2330 train_time:127778ms step_avg:62.06ms
step:2060/2330 train_time:127841ms step_avg:62.06ms
step:2061/2330 train_time:127902ms step_avg:62.06ms
step:2062/2330 train_time:127965ms step_avg:62.06ms
step:2063/2330 train_time:128026ms step_avg:62.06ms
step:2064/2330 train_time:128090ms step_avg:62.06ms
step:2065/2330 train_time:128151ms step_avg:62.06ms
step:2066/2330 train_time:128216ms step_avg:62.06ms
step:2067/2330 train_time:128278ms step_avg:62.06ms
step:2068/2330 train_time:128342ms step_avg:62.06ms
step:2069/2330 train_time:128403ms step_avg:62.06ms
step:2070/2330 train_time:128467ms step_avg:62.06ms
step:2071/2330 train_time:128528ms step_avg:62.06ms
step:2072/2330 train_time:128592ms step_avg:62.06ms
step:2073/2330 train_time:128654ms step_avg:62.06ms
step:2074/2330 train_time:128718ms step_avg:62.06ms
step:2075/2330 train_time:128779ms step_avg:62.06ms
step:2076/2330 train_time:128843ms step_avg:62.06ms
step:2077/2330 train_time:128903ms step_avg:62.06ms
step:2078/2330 train_time:128967ms step_avg:62.06ms
step:2079/2330 train_time:129027ms step_avg:62.06ms
step:2080/2330 train_time:129091ms step_avg:62.06ms
step:2081/2330 train_time:129153ms step_avg:62.06ms
step:2082/2330 train_time:129218ms step_avg:62.06ms
step:2083/2330 train_time:129279ms step_avg:62.06ms
step:2084/2330 train_time:129344ms step_avg:62.07ms
step:2085/2330 train_time:129405ms step_avg:62.06ms
step:2086/2330 train_time:129469ms step_avg:62.07ms
step:2087/2330 train_time:129530ms step_avg:62.07ms
step:2088/2330 train_time:129594ms step_avg:62.07ms
step:2089/2330 train_time:129655ms step_avg:62.07ms
step:2090/2330 train_time:129719ms step_avg:62.07ms
step:2091/2330 train_time:129780ms step_avg:62.07ms
step:2092/2330 train_time:129843ms step_avg:62.07ms
step:2093/2330 train_time:129904ms step_avg:62.07ms
step:2094/2330 train_time:129968ms step_avg:62.07ms
step:2095/2330 train_time:130029ms step_avg:62.07ms
step:2096/2330 train_time:130094ms step_avg:62.07ms
step:2097/2330 train_time:130155ms step_avg:62.07ms
step:2098/2330 train_time:130219ms step_avg:62.07ms
step:2099/2330 train_time:130281ms step_avg:62.07ms
step:2100/2330 train_time:130344ms step_avg:62.07ms
step:2101/2330 train_time:130406ms step_avg:62.07ms
step:2102/2330 train_time:130469ms step_avg:62.07ms
step:2103/2330 train_time:130531ms step_avg:62.07ms
step:2104/2330 train_time:130595ms step_avg:62.07ms
step:2105/2330 train_time:130657ms step_avg:62.07ms
step:2106/2330 train_time:130721ms step_avg:62.07ms
step:2107/2330 train_time:130782ms step_avg:62.07ms
step:2108/2330 train_time:130846ms step_avg:62.07ms
step:2109/2330 train_time:130906ms step_avg:62.07ms
step:2110/2330 train_time:130970ms step_avg:62.07ms
step:2111/2330 train_time:131032ms step_avg:62.07ms
step:2112/2330 train_time:131096ms step_avg:62.07ms
step:2113/2330 train_time:131157ms step_avg:62.07ms
step:2114/2330 train_time:131221ms step_avg:62.07ms
step:2115/2330 train_time:131282ms step_avg:62.07ms
step:2116/2330 train_time:131345ms step_avg:62.07ms
step:2117/2330 train_time:131406ms step_avg:62.07ms
step:2118/2330 train_time:131471ms step_avg:62.07ms
step:2119/2330 train_time:131532ms step_avg:62.07ms
step:2120/2330 train_time:131597ms step_avg:62.07ms
step:2121/2330 train_time:131658ms step_avg:62.07ms
step:2122/2330 train_time:131723ms step_avg:62.08ms
step:2123/2330 train_time:131784ms step_avg:62.07ms
step:2124/2330 train_time:131847ms step_avg:62.08ms
step:2125/2330 train_time:131908ms step_avg:62.07ms
step:2126/2330 train_time:131972ms step_avg:62.08ms
step:2127/2330 train_time:132034ms step_avg:62.08ms
step:2128/2330 train_time:132099ms step_avg:62.08ms
step:2129/2330 train_time:132161ms step_avg:62.08ms
step:2130/2330 train_time:132224ms step_avg:62.08ms
step:2131/2330 train_time:132285ms step_avg:62.08ms
step:2132/2330 train_time:132349ms step_avg:62.08ms
step:2133/2330 train_time:132410ms step_avg:62.08ms
step:2134/2330 train_time:132474ms step_avg:62.08ms
step:2135/2330 train_time:132535ms step_avg:62.08ms
step:2136/2330 train_time:132599ms step_avg:62.08ms
step:2137/2330 train_time:132661ms step_avg:62.08ms
step:2138/2330 train_time:132725ms step_avg:62.08ms
step:2139/2330 train_time:132786ms step_avg:62.08ms
step:2140/2330 train_time:132850ms step_avg:62.08ms
step:2141/2330 train_time:132911ms step_avg:62.08ms
step:2142/2330 train_time:132975ms step_avg:62.08ms
step:2143/2330 train_time:133037ms step_avg:62.08ms
step:2144/2330 train_time:133101ms step_avg:62.08ms
step:2145/2330 train_time:133162ms step_avg:62.08ms
step:2146/2330 train_time:133226ms step_avg:62.08ms
step:2147/2330 train_time:133287ms step_avg:62.08ms
step:2148/2330 train_time:133351ms step_avg:62.08ms
step:2149/2330 train_time:133412ms step_avg:62.08ms
step:2150/2330 train_time:133477ms step_avg:62.08ms
step:2151/2330 train_time:133539ms step_avg:62.08ms
step:2152/2330 train_time:133603ms step_avg:62.08ms
step:2153/2330 train_time:133663ms step_avg:62.08ms
step:2154/2330 train_time:133727ms step_avg:62.08ms
step:2155/2330 train_time:133788ms step_avg:62.08ms
step:2156/2330 train_time:133853ms step_avg:62.08ms
step:2157/2330 train_time:133914ms step_avg:62.08ms
step:2158/2330 train_time:133978ms step_avg:62.08ms
step:2159/2330 train_time:134039ms step_avg:62.08ms
step:2160/2330 train_time:134103ms step_avg:62.08ms
step:2161/2330 train_time:134164ms step_avg:62.08ms
step:2162/2330 train_time:134228ms step_avg:62.08ms
step:2163/2330 train_time:134289ms step_avg:62.08ms
step:2164/2330 train_time:134353ms step_avg:62.09ms
step:2165/2330 train_time:134415ms step_avg:62.09ms
step:2166/2330 train_time:134479ms step_avg:62.09ms
step:2167/2330 train_time:134540ms step_avg:62.09ms
step:2168/2330 train_time:134604ms step_avg:62.09ms
step:2169/2330 train_time:134666ms step_avg:62.09ms
step:2170/2330 train_time:134730ms step_avg:62.09ms
step:2171/2330 train_time:134791ms step_avg:62.09ms
step:2172/2330 train_time:134857ms step_avg:62.09ms
step:2173/2330 train_time:134917ms step_avg:62.09ms
step:2174/2330 train_time:134981ms step_avg:62.09ms
step:2175/2330 train_time:135042ms step_avg:62.09ms
step:2176/2330 train_time:135106ms step_avg:62.09ms
step:2177/2330 train_time:135167ms step_avg:62.09ms
step:2178/2330 train_time:135232ms step_avg:62.09ms
step:2179/2330 train_time:135293ms step_avg:62.09ms
step:2180/2330 train_time:135358ms step_avg:62.09ms
step:2181/2330 train_time:135419ms step_avg:62.09ms
step:2182/2330 train_time:135484ms step_avg:62.09ms
step:2183/2330 train_time:135545ms step_avg:62.09ms
step:2184/2330 train_time:135609ms step_avg:62.09ms
step:2185/2330 train_time:135670ms step_avg:62.09ms
step:2186/2330 train_time:135734ms step_avg:62.09ms
step:2187/2330 train_time:135796ms step_avg:62.09ms
step:2188/2330 train_time:135861ms step_avg:62.09ms
step:2189/2330 train_time:135921ms step_avg:62.09ms
step:2190/2330 train_time:135985ms step_avg:62.09ms
step:2191/2330 train_time:136047ms step_avg:62.09ms
step:2192/2330 train_time:136111ms step_avg:62.09ms
step:2193/2330 train_time:136172ms step_avg:62.09ms
step:2194/2330 train_time:136236ms step_avg:62.09ms
step:2195/2330 train_time:136297ms step_avg:62.09ms
step:2196/2330 train_time:136361ms step_avg:62.09ms
step:2197/2330 train_time:136421ms step_avg:62.09ms
step:2198/2330 train_time:136485ms step_avg:62.10ms
step:2199/2330 train_time:136547ms step_avg:62.09ms
step:2200/2330 train_time:136611ms step_avg:62.10ms
step:2201/2330 train_time:136673ms step_avg:62.10ms
step:2202/2330 train_time:136737ms step_avg:62.10ms
step:2203/2330 train_time:136798ms step_avg:62.10ms
step:2204/2330 train_time:136862ms step_avg:62.10ms
step:2205/2330 train_time:136923ms step_avg:62.10ms
step:2206/2330 train_time:136987ms step_avg:62.10ms
step:2207/2330 train_time:137048ms step_avg:62.10ms
step:2208/2330 train_time:137111ms step_avg:62.10ms
step:2209/2330 train_time:137173ms step_avg:62.10ms
step:2210/2330 train_time:137237ms step_avg:62.10ms
step:2211/2330 train_time:137298ms step_avg:62.10ms
step:2212/2330 train_time:137362ms step_avg:62.10ms
step:2213/2330 train_time:137422ms step_avg:62.10ms
step:2214/2330 train_time:137486ms step_avg:62.10ms
step:2215/2330 train_time:137548ms step_avg:62.10ms
step:2216/2330 train_time:137613ms step_avg:62.10ms
step:2217/2330 train_time:137675ms step_avg:62.10ms
step:2218/2330 train_time:137739ms step_avg:62.10ms
step:2219/2330 train_time:137801ms step_avg:62.10ms
step:2220/2330 train_time:137864ms step_avg:62.10ms
step:2221/2330 train_time:137925ms step_avg:62.10ms
step:2222/2330 train_time:137989ms step_avg:62.10ms
step:2223/2330 train_time:138050ms step_avg:62.10ms
step:2224/2330 train_time:138115ms step_avg:62.10ms
step:2225/2330 train_time:138176ms step_avg:62.10ms
step:2226/2330 train_time:138240ms step_avg:62.10ms
step:2227/2330 train_time:138302ms step_avg:62.10ms
step:2228/2330 train_time:138366ms step_avg:62.10ms
step:2229/2330 train_time:138426ms step_avg:62.10ms
step:2230/2330 train_time:138491ms step_avg:62.10ms
step:2231/2330 train_time:138552ms step_avg:62.10ms
step:2232/2330 train_time:138617ms step_avg:62.10ms
step:2233/2330 train_time:138679ms step_avg:62.10ms
step:2234/2330 train_time:138743ms step_avg:62.11ms
step:2235/2330 train_time:138805ms step_avg:62.11ms
step:2236/2330 train_time:138868ms step_avg:62.11ms
step:2237/2330 train_time:138929ms step_avg:62.11ms
step:2238/2330 train_time:138994ms step_avg:62.11ms
step:2239/2330 train_time:139055ms step_avg:62.11ms
step:2240/2330 train_time:139119ms step_avg:62.11ms
step:2241/2330 train_time:139179ms step_avg:62.11ms
step:2242/2330 train_time:139243ms step_avg:62.11ms
step:2243/2330 train_time:139304ms step_avg:62.11ms
step:2244/2330 train_time:139368ms step_avg:62.11ms
step:2245/2330 train_time:139429ms step_avg:62.11ms
step:2246/2330 train_time:139493ms step_avg:62.11ms
step:2247/2330 train_time:139555ms step_avg:62.11ms
step:2248/2330 train_time:139619ms step_avg:62.11ms
step:2249/2330 train_time:139680ms step_avg:62.11ms
step:2250/2330 train_time:139744ms step_avg:62.11ms
step:2250/2330 val_loss:3.7286 train_time:139810ms step_avg:62.14ms
step:2251/2330 train_time:139833ms step_avg:62.12ms
step:2252/2330 train_time:139873ms step_avg:62.11ms
step:2253/2330 train_time:139939ms step_avg:62.11ms
step:2254/2330 train_time:140003ms step_avg:62.11ms
step:2255/2330 train_time:140063ms step_avg:62.11ms
step:2256/2330 train_time:140128ms step_avg:62.11ms
step:2257/2330 train_time:140188ms step_avg:62.11ms
step:2258/2330 train_time:140252ms step_avg:62.11ms
step:2259/2330 train_time:140313ms step_avg:62.11ms
step:2260/2330 train_time:140376ms step_avg:62.11ms
step:2261/2330 train_time:140437ms step_avg:62.11ms
step:2262/2330 train_time:140500ms step_avg:62.11ms
step:2263/2330 train_time:140561ms step_avg:62.11ms
step:2264/2330 train_time:140624ms step_avg:62.11ms
step:2265/2330 train_time:140685ms step_avg:62.11ms
step:2266/2330 train_time:140750ms step_avg:62.11ms
step:2267/2330 train_time:140813ms step_avg:62.11ms
step:2268/2330 train_time:140879ms step_avg:62.12ms
step:2269/2330 train_time:140941ms step_avg:62.12ms
step:2270/2330 train_time:141006ms step_avg:62.12ms
step:2271/2330 train_time:141068ms step_avg:62.12ms
step:2272/2330 train_time:141132ms step_avg:62.12ms
step:2273/2330 train_time:141193ms step_avg:62.12ms
step:2274/2330 train_time:141257ms step_avg:62.12ms
step:2275/2330 train_time:141318ms step_avg:62.12ms
step:2276/2330 train_time:141381ms step_avg:62.12ms
step:2277/2330 train_time:141441ms step_avg:62.12ms
step:2278/2330 train_time:141505ms step_avg:62.12ms
step:2279/2330 train_time:141565ms step_avg:62.12ms
step:2280/2330 train_time:141629ms step_avg:62.12ms
step:2281/2330 train_time:141690ms step_avg:62.12ms
step:2282/2330 train_time:141755ms step_avg:62.12ms
step:2283/2330 train_time:141817ms step_avg:62.12ms
step:2284/2330 train_time:141881ms step_avg:62.12ms
step:2285/2330 train_time:141943ms step_avg:62.12ms
step:2286/2330 train_time:142007ms step_avg:62.12ms
step:2287/2330 train_time:142068ms step_avg:62.12ms
step:2288/2330 train_time:142133ms step_avg:62.12ms
step:2289/2330 train_time:142195ms step_avg:62.12ms
step:2290/2330 train_time:142259ms step_avg:62.12ms
step:2291/2330 train_time:142320ms step_avg:62.12ms
step:2292/2330 train_time:142383ms step_avg:62.12ms
step:2293/2330 train_time:142444ms step_avg:62.12ms
step:2294/2330 train_time:142507ms step_avg:62.12ms
step:2295/2330 train_time:142567ms step_avg:62.12ms
step:2296/2330 train_time:142631ms step_avg:62.12ms
step:2297/2330 train_time:142693ms step_avg:62.12ms
step:2298/2330 train_time:142757ms step_avg:62.12ms
step:2299/2330 train_time:142819ms step_avg:62.12ms
step:2300/2330 train_time:142883ms step_avg:62.12ms
step:2301/2330 train_time:142944ms step_avg:62.12ms
step:2302/2330 train_time:143008ms step_avg:62.12ms
step:2303/2330 train_time:143070ms step_avg:62.12ms
step:2304/2330 train_time:143135ms step_avg:62.12ms
step:2305/2330 train_time:143197ms step_avg:62.12ms
step:2306/2330 train_time:143261ms step_avg:62.13ms
step:2307/2330 train_time:143322ms step_avg:62.12ms
step:2308/2330 train_time:143386ms step_avg:62.13ms
step:2309/2330 train_time:143446ms step_avg:62.12ms
step:2310/2330 train_time:143510ms step_avg:62.13ms
step:2311/2330 train_time:143572ms step_avg:62.13ms
step:2312/2330 train_time:143635ms step_avg:62.13ms
step:2313/2330 train_time:143696ms step_avg:62.13ms
step:2314/2330 train_time:143760ms step_avg:62.13ms
step:2315/2330 train_time:143821ms step_avg:62.13ms
step:2316/2330 train_time:143885ms step_avg:62.13ms
step:2317/2330 train_time:143946ms step_avg:62.13ms
step:2318/2330 train_time:144012ms step_avg:62.13ms
step:2319/2330 train_time:144073ms step_avg:62.13ms
step:2320/2330 train_time:144137ms step_avg:62.13ms
step:2321/2330 train_time:144198ms step_avg:62.13ms
step:2322/2330 train_time:144262ms step_avg:62.13ms
step:2323/2330 train_time:144322ms step_avg:62.13ms
step:2324/2330 train_time:144386ms step_avg:62.13ms
step:2325/2330 train_time:144447ms step_avg:62.13ms
step:2326/2330 train_time:144511ms step_avg:62.13ms
step:2327/2330 train_time:144571ms step_avg:62.13ms
step:2328/2330 train_time:144635ms step_avg:62.13ms
step:2329/2330 train_time:144697ms step_avg:62.13ms
step:2330/2330 train_time:144761ms step_avg:62.13ms
step:2330/2330 val_loss:3.6962 train_time:144828ms step_avg:62.16ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
