import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr5e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:22:41 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:85ms step_avg:84.66ms
step:2/2330 train_time:149ms step_avg:74.37ms
step:3/2330 train_time:160ms step_avg:53.47ms
step:4/2330 train_time:172ms step_avg:43.11ms
step:5/2330 train_time:183ms step_avg:36.66ms
step:6/2330 train_time:277ms step_avg:46.11ms
step:7/2330 train_time:298ms step_avg:42.56ms
step:8/2330 train_time:352ms step_avg:44.00ms
step:9/2330 train_time:374ms step_avg:41.54ms
step:10/2330 train_time:429ms step_avg:42.87ms
step:11/2330 train_time:451ms step_avg:41.00ms
step:12/2330 train_time:506ms step_avg:42.20ms
step:13/2330 train_time:529ms step_avg:40.67ms
step:14/2330 train_time:585ms step_avg:41.77ms
step:15/2330 train_time:607ms step_avg:40.44ms
step:16/2330 train_time:662ms step_avg:41.40ms
step:17/2330 train_time:685ms step_avg:40.27ms
step:18/2330 train_time:740ms step_avg:41.14ms
step:19/2330 train_time:763ms step_avg:40.14ms
step:20/2330 train_time:818ms step_avg:40.92ms
step:21/2330 train_time:840ms step_avg:40.01ms
step:22/2330 train_time:896ms step_avg:40.71ms
step:23/2330 train_time:918ms step_avg:39.90ms
step:24/2330 train_time:973ms step_avg:40.55ms
step:25/2330 train_time:995ms step_avg:39.81ms
step:26/2330 train_time:1051ms step_avg:40.41ms
step:27/2330 train_time:1073ms step_avg:39.74ms
step:28/2330 train_time:1131ms step_avg:40.41ms
step:29/2330 train_time:1157ms step_avg:39.91ms
step:30/2330 train_time:1219ms step_avg:40.64ms
step:31/2330 train_time:1242ms step_avg:40.08ms
step:32/2330 train_time:1301ms step_avg:40.65ms
step:33/2330 train_time:1323ms step_avg:40.09ms
step:34/2330 train_time:1381ms step_avg:40.61ms
step:35/2330 train_time:1403ms step_avg:40.10ms
step:36/2330 train_time:1460ms step_avg:40.56ms
step:37/2330 train_time:1483ms step_avg:40.07ms
step:38/2330 train_time:1539ms step_avg:40.50ms
step:39/2330 train_time:1561ms step_avg:40.02ms
step:40/2330 train_time:1617ms step_avg:40.43ms
step:41/2330 train_time:1639ms step_avg:39.97ms
step:42/2330 train_time:1695ms step_avg:40.35ms
step:43/2330 train_time:1716ms step_avg:39.91ms
step:44/2330 train_time:1771ms step_avg:40.26ms
step:45/2330 train_time:1794ms step_avg:39.86ms
step:46/2330 train_time:1850ms step_avg:40.21ms
step:47/2330 train_time:1872ms step_avg:39.82ms
step:48/2330 train_time:1928ms step_avg:40.16ms
step:49/2330 train_time:1950ms step_avg:39.79ms
step:50/2330 train_time:2005ms step_avg:40.10ms
step:51/2330 train_time:2027ms step_avg:39.75ms
step:52/2330 train_time:2085ms step_avg:40.09ms
step:53/2330 train_time:2109ms step_avg:39.79ms
step:54/2330 train_time:2168ms step_avg:40.15ms
step:55/2330 train_time:2192ms step_avg:39.85ms
step:56/2330 train_time:2250ms step_avg:40.19ms
step:57/2330 train_time:2274ms step_avg:39.89ms
step:58/2330 train_time:2331ms step_avg:40.20ms
step:59/2330 train_time:2355ms step_avg:39.92ms
step:60/2330 train_time:2411ms step_avg:40.18ms
step:61/2330 train_time:2434ms step_avg:39.90ms
step:62/2330 train_time:2490ms step_avg:40.17ms
step:63/2330 train_time:2513ms step_avg:39.89ms
step:64/2330 train_time:2569ms step_avg:40.15ms
step:65/2330 train_time:2592ms step_avg:39.88ms
step:66/2330 train_time:2648ms step_avg:40.12ms
step:67/2330 train_time:2671ms step_avg:39.86ms
step:68/2330 train_time:2727ms step_avg:40.10ms
step:69/2330 train_time:2749ms step_avg:39.85ms
step:70/2330 train_time:2805ms step_avg:40.07ms
step:71/2330 train_time:2827ms step_avg:39.82ms
step:72/2330 train_time:2884ms step_avg:40.05ms
step:73/2330 train_time:2906ms step_avg:39.81ms
step:74/2330 train_time:2962ms step_avg:40.03ms
step:75/2330 train_time:2984ms step_avg:39.79ms
step:76/2330 train_time:3041ms step_avg:40.02ms
step:77/2330 train_time:3064ms step_avg:39.79ms
step:78/2330 train_time:3122ms step_avg:40.03ms
step:79/2330 train_time:3145ms step_avg:39.81ms
step:80/2330 train_time:3204ms step_avg:40.05ms
step:81/2330 train_time:3227ms step_avg:39.83ms
step:82/2330 train_time:3284ms step_avg:40.05ms
step:83/2330 train_time:3308ms step_avg:39.85ms
step:84/2330 train_time:3366ms step_avg:40.07ms
step:85/2330 train_time:3388ms step_avg:39.86ms
step:86/2330 train_time:3445ms step_avg:40.06ms
step:87/2330 train_time:3468ms step_avg:39.86ms
step:88/2330 train_time:3525ms step_avg:40.05ms
step:89/2330 train_time:3547ms step_avg:39.86ms
step:90/2330 train_time:3604ms step_avg:40.04ms
step:91/2330 train_time:3626ms step_avg:39.84ms
step:92/2330 train_time:3682ms step_avg:40.03ms
step:93/2330 train_time:3704ms step_avg:39.83ms
step:94/2330 train_time:3760ms step_avg:40.00ms
step:95/2330 train_time:3782ms step_avg:39.81ms
step:96/2330 train_time:3839ms step_avg:39.99ms
step:97/2330 train_time:3862ms step_avg:39.81ms
step:98/2330 train_time:3918ms step_avg:39.98ms
step:99/2330 train_time:3940ms step_avg:39.79ms
step:100/2330 train_time:3996ms step_avg:39.96ms
step:101/2330 train_time:4018ms step_avg:39.78ms
step:102/2330 train_time:4076ms step_avg:39.96ms
step:103/2330 train_time:4098ms step_avg:39.79ms
step:104/2330 train_time:4155ms step_avg:39.95ms
step:105/2330 train_time:4178ms step_avg:39.79ms
step:106/2330 train_time:4235ms step_avg:39.96ms
step:107/2330 train_time:4258ms step_avg:39.79ms
step:108/2330 train_time:4316ms step_avg:39.96ms
step:109/2330 train_time:4338ms step_avg:39.80ms
step:110/2330 train_time:4395ms step_avg:39.95ms
step:111/2330 train_time:4418ms step_avg:39.80ms
step:112/2330 train_time:4475ms step_avg:39.95ms
step:113/2330 train_time:4497ms step_avg:39.80ms
step:114/2330 train_time:4554ms step_avg:39.95ms
step:115/2330 train_time:4577ms step_avg:39.80ms
step:116/2330 train_time:4633ms step_avg:39.94ms
step:117/2330 train_time:4656ms step_avg:39.80ms
step:118/2330 train_time:4712ms step_avg:39.93ms
step:119/2330 train_time:4735ms step_avg:39.79ms
step:120/2330 train_time:4791ms step_avg:39.92ms
step:121/2330 train_time:4813ms step_avg:39.78ms
step:122/2330 train_time:4869ms step_avg:39.91ms
step:123/2330 train_time:4892ms step_avg:39.77ms
step:124/2330 train_time:4948ms step_avg:39.90ms
step:125/2330 train_time:4971ms step_avg:39.77ms
step:126/2330 train_time:5027ms step_avg:39.90ms
step:127/2330 train_time:5051ms step_avg:39.77ms
step:128/2330 train_time:5107ms step_avg:39.90ms
step:129/2330 train_time:5131ms step_avg:39.77ms
step:130/2330 train_time:5188ms step_avg:39.90ms
step:131/2330 train_time:5211ms step_avg:39.78ms
step:132/2330 train_time:5267ms step_avg:39.90ms
step:133/2330 train_time:5291ms step_avg:39.78ms
step:134/2330 train_time:5347ms step_avg:39.91ms
step:135/2330 train_time:5370ms step_avg:39.78ms
step:136/2330 train_time:5426ms step_avg:39.90ms
step:137/2330 train_time:5449ms step_avg:39.78ms
step:138/2330 train_time:5506ms step_avg:39.90ms
step:139/2330 train_time:5529ms step_avg:39.77ms
step:140/2330 train_time:5586ms step_avg:39.90ms
step:141/2330 train_time:5609ms step_avg:39.78ms
step:142/2330 train_time:5667ms step_avg:39.91ms
step:143/2330 train_time:5689ms step_avg:39.79ms
step:144/2330 train_time:5746ms step_avg:39.91ms
step:145/2330 train_time:5768ms step_avg:39.78ms
step:146/2330 train_time:5825ms step_avg:39.90ms
step:147/2330 train_time:5848ms step_avg:39.78ms
step:148/2330 train_time:5905ms step_avg:39.90ms
step:149/2330 train_time:5927ms step_avg:39.78ms
step:150/2330 train_time:5984ms step_avg:39.89ms
step:151/2330 train_time:6006ms step_avg:39.77ms
step:152/2330 train_time:6063ms step_avg:39.89ms
step:153/2330 train_time:6086ms step_avg:39.78ms
step:154/2330 train_time:6143ms step_avg:39.89ms
step:155/2330 train_time:6167ms step_avg:39.79ms
step:156/2330 train_time:6224ms step_avg:39.90ms
step:157/2330 train_time:6247ms step_avg:39.79ms
step:158/2330 train_time:6303ms step_avg:39.89ms
step:159/2330 train_time:6326ms step_avg:39.78ms
step:160/2330 train_time:6383ms step_avg:39.90ms
step:161/2330 train_time:6406ms step_avg:39.79ms
step:162/2330 train_time:6462ms step_avg:39.89ms
step:163/2330 train_time:6485ms step_avg:39.78ms
step:164/2330 train_time:6542ms step_avg:39.89ms
step:165/2330 train_time:6564ms step_avg:39.78ms
step:166/2330 train_time:6622ms step_avg:39.89ms
step:167/2330 train_time:6645ms step_avg:39.79ms
step:168/2330 train_time:6701ms step_avg:39.89ms
step:169/2330 train_time:6724ms step_avg:39.79ms
step:170/2330 train_time:6781ms step_avg:39.89ms
step:171/2330 train_time:6804ms step_avg:39.79ms
step:172/2330 train_time:6860ms step_avg:39.89ms
step:173/2330 train_time:6883ms step_avg:39.79ms
step:174/2330 train_time:6940ms step_avg:39.88ms
step:175/2330 train_time:6962ms step_avg:39.78ms
step:176/2330 train_time:7019ms step_avg:39.88ms
step:177/2330 train_time:7041ms step_avg:39.78ms
step:178/2330 train_time:7098ms step_avg:39.88ms
step:179/2330 train_time:7120ms step_avg:39.78ms
step:180/2330 train_time:7178ms step_avg:39.88ms
step:181/2330 train_time:7200ms step_avg:39.78ms
step:182/2330 train_time:7257ms step_avg:39.87ms
step:183/2330 train_time:7279ms step_avg:39.78ms
step:184/2330 train_time:7337ms step_avg:39.87ms
step:185/2330 train_time:7359ms step_avg:39.78ms
step:186/2330 train_time:7416ms step_avg:39.87ms
step:187/2330 train_time:7438ms step_avg:39.77ms
step:188/2330 train_time:7494ms step_avg:39.86ms
step:189/2330 train_time:7517ms step_avg:39.77ms
step:190/2330 train_time:7574ms step_avg:39.86ms
step:191/2330 train_time:7597ms step_avg:39.77ms
step:192/2330 train_time:7653ms step_avg:39.86ms
step:193/2330 train_time:7676ms step_avg:39.77ms
step:194/2330 train_time:7732ms step_avg:39.85ms
step:195/2330 train_time:7755ms step_avg:39.77ms
step:196/2330 train_time:7811ms step_avg:39.85ms
step:197/2330 train_time:7834ms step_avg:39.77ms
step:198/2330 train_time:7890ms step_avg:39.85ms
step:199/2330 train_time:7913ms step_avg:39.76ms
step:200/2330 train_time:7969ms step_avg:39.85ms
step:201/2330 train_time:7992ms step_avg:39.76ms
step:202/2330 train_time:8049ms step_avg:39.85ms
step:203/2330 train_time:8072ms step_avg:39.76ms
step:204/2330 train_time:8128ms step_avg:39.84ms
step:205/2330 train_time:8151ms step_avg:39.76ms
step:206/2330 train_time:8208ms step_avg:39.84ms
step:207/2330 train_time:8231ms step_avg:39.76ms
step:208/2330 train_time:8287ms step_avg:39.84ms
step:209/2330 train_time:8310ms step_avg:39.76ms
step:210/2330 train_time:8367ms step_avg:39.84ms
step:211/2330 train_time:8390ms step_avg:39.76ms
step:212/2330 train_time:8447ms step_avg:39.84ms
step:213/2330 train_time:8470ms step_avg:39.77ms
step:214/2330 train_time:8527ms step_avg:39.85ms
step:215/2330 train_time:8550ms step_avg:39.77ms
step:216/2330 train_time:8606ms step_avg:39.84ms
step:217/2330 train_time:8629ms step_avg:39.76ms
step:218/2330 train_time:8686ms step_avg:39.84ms
step:219/2330 train_time:8708ms step_avg:39.76ms
step:220/2330 train_time:8765ms step_avg:39.84ms
step:221/2330 train_time:8788ms step_avg:39.76ms
step:222/2330 train_time:8845ms step_avg:39.84ms
step:223/2330 train_time:8868ms step_avg:39.77ms
step:224/2330 train_time:8925ms step_avg:39.84ms
step:225/2330 train_time:8948ms step_avg:39.77ms
step:226/2330 train_time:9004ms step_avg:39.84ms
step:227/2330 train_time:9027ms step_avg:39.77ms
step:228/2330 train_time:9084ms step_avg:39.84ms
step:229/2330 train_time:9108ms step_avg:39.77ms
step:230/2330 train_time:9166ms step_avg:39.85ms
step:231/2330 train_time:9188ms step_avg:39.78ms
step:232/2330 train_time:9245ms step_avg:39.85ms
step:233/2330 train_time:9268ms step_avg:39.78ms
step:234/2330 train_time:9325ms step_avg:39.85ms
step:235/2330 train_time:9348ms step_avg:39.78ms
step:236/2330 train_time:9405ms step_avg:39.85ms
step:237/2330 train_time:9428ms step_avg:39.78ms
step:238/2330 train_time:9486ms step_avg:39.86ms
step:239/2330 train_time:9508ms step_avg:39.78ms
step:240/2330 train_time:9565ms step_avg:39.85ms
step:241/2330 train_time:9588ms step_avg:39.78ms
step:242/2330 train_time:9644ms step_avg:39.85ms
step:243/2330 train_time:9667ms step_avg:39.78ms
step:244/2330 train_time:9724ms step_avg:39.85ms
step:245/2330 train_time:9747ms step_avg:39.78ms
step:246/2330 train_time:9804ms step_avg:39.85ms
step:247/2330 train_time:9827ms step_avg:39.78ms
step:248/2330 train_time:9884ms step_avg:39.85ms
step:249/2330 train_time:9907ms step_avg:39.79ms
step:250/2330 train_time:9964ms step_avg:39.85ms
step:250/2330 val_loss:5.5416 train_time:10059ms step_avg:40.24ms
step:251/2330 train_time:10071ms step_avg:40.12ms
step:252/2330 train_time:10081ms step_avg:40.01ms
step:253/2330 train_time:10091ms step_avg:39.88ms
step:254/2330 train_time:10122ms step_avg:39.85ms
step:255/2330 train_time:10143ms step_avg:39.78ms
step:256/2330 train_time:10199ms step_avg:39.84ms
step:257/2330 train_time:10221ms step_avg:39.77ms
step:258/2330 train_time:10277ms step_avg:39.83ms
step:259/2330 train_time:10298ms step_avg:39.76ms
step:260/2330 train_time:10354ms step_avg:39.82ms
step:261/2330 train_time:10379ms step_avg:39.77ms
step:262/2330 train_time:10438ms step_avg:39.84ms
step:263/2330 train_time:10462ms step_avg:39.78ms
step:264/2330 train_time:10519ms step_avg:39.85ms
step:265/2330 train_time:10544ms step_avg:39.79ms
step:266/2330 train_time:10600ms step_avg:39.85ms
step:267/2330 train_time:10623ms step_avg:39.79ms
step:268/2330 train_time:10679ms step_avg:39.85ms
step:269/2330 train_time:10701ms step_avg:39.78ms
step:270/2330 train_time:10757ms step_avg:39.84ms
step:271/2330 train_time:10780ms step_avg:39.78ms
step:272/2330 train_time:10835ms step_avg:39.84ms
step:273/2330 train_time:10857ms step_avg:39.77ms
step:274/2330 train_time:10913ms step_avg:39.83ms
step:275/2330 train_time:10936ms step_avg:39.77ms
step:276/2330 train_time:10996ms step_avg:39.84ms
step:277/2330 train_time:11020ms step_avg:39.78ms
step:278/2330 train_time:11078ms step_avg:39.85ms
step:279/2330 train_time:11100ms step_avg:39.79ms
step:280/2330 train_time:11157ms step_avg:39.85ms
step:281/2330 train_time:11179ms step_avg:39.78ms
step:282/2330 train_time:11236ms step_avg:39.84ms
step:283/2330 train_time:11258ms step_avg:39.78ms
step:284/2330 train_time:11314ms step_avg:39.84ms
step:285/2330 train_time:11337ms step_avg:39.78ms
step:286/2330 train_time:11394ms step_avg:39.84ms
step:287/2330 train_time:11417ms step_avg:39.78ms
step:288/2330 train_time:11474ms step_avg:39.84ms
step:289/2330 train_time:11496ms step_avg:39.78ms
step:290/2330 train_time:11554ms step_avg:39.84ms
step:291/2330 train_time:11577ms step_avg:39.78ms
step:292/2330 train_time:11634ms step_avg:39.84ms
step:293/2330 train_time:11656ms step_avg:39.78ms
step:294/2330 train_time:11713ms step_avg:39.84ms
step:295/2330 train_time:11735ms step_avg:39.78ms
step:296/2330 train_time:11791ms step_avg:39.84ms
step:297/2330 train_time:11813ms step_avg:39.78ms
step:298/2330 train_time:11870ms step_avg:39.83ms
step:299/2330 train_time:11892ms step_avg:39.77ms
step:300/2330 train_time:11950ms step_avg:39.83ms
step:301/2330 train_time:11972ms step_avg:39.77ms
step:302/2330 train_time:12030ms step_avg:39.83ms
step:303/2330 train_time:12052ms step_avg:39.78ms
step:304/2330 train_time:12108ms step_avg:39.83ms
step:305/2330 train_time:12131ms step_avg:39.77ms
step:306/2330 train_time:12187ms step_avg:39.83ms
step:307/2330 train_time:12210ms step_avg:39.77ms
step:308/2330 train_time:12266ms step_avg:39.82ms
step:309/2330 train_time:12289ms step_avg:39.77ms
step:310/2330 train_time:12344ms step_avg:39.82ms
step:311/2330 train_time:12368ms step_avg:39.77ms
step:312/2330 train_time:12424ms step_avg:39.82ms
step:313/2330 train_time:12447ms step_avg:39.77ms
step:314/2330 train_time:12504ms step_avg:39.82ms
step:315/2330 train_time:12527ms step_avg:39.77ms
step:316/2330 train_time:12584ms step_avg:39.82ms
step:317/2330 train_time:12607ms step_avg:39.77ms
step:318/2330 train_time:12663ms step_avg:39.82ms
step:319/2330 train_time:12687ms step_avg:39.77ms
step:320/2330 train_time:12743ms step_avg:39.82ms
step:321/2330 train_time:12766ms step_avg:39.77ms
step:322/2330 train_time:12822ms step_avg:39.82ms
step:323/2330 train_time:12846ms step_avg:39.77ms
step:324/2330 train_time:12901ms step_avg:39.82ms
step:325/2330 train_time:12924ms step_avg:39.77ms
step:326/2330 train_time:12981ms step_avg:39.82ms
step:327/2330 train_time:13004ms step_avg:39.77ms
step:328/2330 train_time:13060ms step_avg:39.82ms
step:329/2330 train_time:13083ms step_avg:39.77ms
step:330/2330 train_time:13139ms step_avg:39.82ms
step:331/2330 train_time:13162ms step_avg:39.76ms
step:332/2330 train_time:13219ms step_avg:39.82ms
step:333/2330 train_time:13242ms step_avg:39.77ms
step:334/2330 train_time:13298ms step_avg:39.81ms
step:335/2330 train_time:13321ms step_avg:39.77ms
step:336/2330 train_time:13378ms step_avg:39.82ms
step:337/2330 train_time:13401ms step_avg:39.77ms
step:338/2330 train_time:13458ms step_avg:39.82ms
step:339/2330 train_time:13481ms step_avg:39.77ms
step:340/2330 train_time:13538ms step_avg:39.82ms
step:341/2330 train_time:13561ms step_avg:39.77ms
step:342/2330 train_time:13617ms step_avg:39.82ms
step:343/2330 train_time:13639ms step_avg:39.76ms
step:344/2330 train_time:13696ms step_avg:39.82ms
step:345/2330 train_time:13719ms step_avg:39.76ms
step:346/2330 train_time:13776ms step_avg:39.81ms
step:347/2330 train_time:13798ms step_avg:39.76ms
step:348/2330 train_time:13854ms step_avg:39.81ms
step:349/2330 train_time:13876ms step_avg:39.76ms
step:350/2330 train_time:13933ms step_avg:39.81ms
step:351/2330 train_time:13955ms step_avg:39.76ms
step:352/2330 train_time:14012ms step_avg:39.81ms
step:353/2330 train_time:14035ms step_avg:39.76ms
step:354/2330 train_time:14091ms step_avg:39.81ms
step:355/2330 train_time:14114ms step_avg:39.76ms
step:356/2330 train_time:14170ms step_avg:39.80ms
step:357/2330 train_time:14192ms step_avg:39.75ms
step:358/2330 train_time:14249ms step_avg:39.80ms
step:359/2330 train_time:14271ms step_avg:39.75ms
step:360/2330 train_time:14327ms step_avg:39.80ms
step:361/2330 train_time:14350ms step_avg:39.75ms
step:362/2330 train_time:14407ms step_avg:39.80ms
step:363/2330 train_time:14429ms step_avg:39.75ms
step:364/2330 train_time:14485ms step_avg:39.79ms
step:365/2330 train_time:14509ms step_avg:39.75ms
step:366/2330 train_time:14564ms step_avg:39.79ms
step:367/2330 train_time:14587ms step_avg:39.75ms
step:368/2330 train_time:14644ms step_avg:39.79ms
step:369/2330 train_time:14667ms step_avg:39.75ms
step:370/2330 train_time:14725ms step_avg:39.80ms
step:371/2330 train_time:14748ms step_avg:39.75ms
step:372/2330 train_time:14804ms step_avg:39.80ms
step:373/2330 train_time:14827ms step_avg:39.75ms
step:374/2330 train_time:14884ms step_avg:39.80ms
step:375/2330 train_time:14908ms step_avg:39.75ms
step:376/2330 train_time:14964ms step_avg:39.80ms
step:377/2330 train_time:14987ms step_avg:39.75ms
step:378/2330 train_time:15043ms step_avg:39.80ms
step:379/2330 train_time:15065ms step_avg:39.75ms
step:380/2330 train_time:15121ms step_avg:39.79ms
step:381/2330 train_time:15144ms step_avg:39.75ms
step:382/2330 train_time:15201ms step_avg:39.79ms
step:383/2330 train_time:15224ms step_avg:39.75ms
step:384/2330 train_time:15280ms step_avg:39.79ms
step:385/2330 train_time:15303ms step_avg:39.75ms
step:386/2330 train_time:15359ms step_avg:39.79ms
step:387/2330 train_time:15382ms step_avg:39.75ms
step:388/2330 train_time:15438ms step_avg:39.79ms
step:389/2330 train_time:15461ms step_avg:39.75ms
step:390/2330 train_time:15518ms step_avg:39.79ms
step:391/2330 train_time:15541ms step_avg:39.75ms
step:392/2330 train_time:15598ms step_avg:39.79ms
step:393/2330 train_time:15620ms step_avg:39.75ms
step:394/2330 train_time:15677ms step_avg:39.79ms
step:395/2330 train_time:15700ms step_avg:39.75ms
step:396/2330 train_time:15757ms step_avg:39.79ms
step:397/2330 train_time:15780ms step_avg:39.75ms
step:398/2330 train_time:15836ms step_avg:39.79ms
step:399/2330 train_time:15858ms step_avg:39.74ms
step:400/2330 train_time:15916ms step_avg:39.79ms
step:401/2330 train_time:15939ms step_avg:39.75ms
step:402/2330 train_time:15995ms step_avg:39.79ms
step:403/2330 train_time:16017ms step_avg:39.75ms
step:404/2330 train_time:16074ms step_avg:39.79ms
step:405/2330 train_time:16096ms step_avg:39.74ms
step:406/2330 train_time:16153ms step_avg:39.78ms
step:407/2330 train_time:16175ms step_avg:39.74ms
step:408/2330 train_time:16232ms step_avg:39.78ms
step:409/2330 train_time:16255ms step_avg:39.74ms
step:410/2330 train_time:16312ms step_avg:39.79ms
step:411/2330 train_time:16334ms step_avg:39.74ms
step:412/2330 train_time:16391ms step_avg:39.78ms
step:413/2330 train_time:16413ms step_avg:39.74ms
step:414/2330 train_time:16469ms step_avg:39.78ms
step:415/2330 train_time:16491ms step_avg:39.74ms
step:416/2330 train_time:16548ms step_avg:39.78ms
step:417/2330 train_time:16571ms step_avg:39.74ms
step:418/2330 train_time:16628ms step_avg:39.78ms
step:419/2330 train_time:16650ms step_avg:39.74ms
step:420/2330 train_time:16707ms step_avg:39.78ms
step:421/2330 train_time:16729ms step_avg:39.74ms
step:422/2330 train_time:16786ms step_avg:39.78ms
step:423/2330 train_time:16809ms step_avg:39.74ms
step:424/2330 train_time:16866ms step_avg:39.78ms
step:425/2330 train_time:16889ms step_avg:39.74ms
step:426/2330 train_time:16945ms step_avg:39.78ms
step:427/2330 train_time:16968ms step_avg:39.74ms
step:428/2330 train_time:17024ms step_avg:39.78ms
step:429/2330 train_time:17047ms step_avg:39.74ms
step:430/2330 train_time:17104ms step_avg:39.78ms
step:431/2330 train_time:17128ms step_avg:39.74ms
step:432/2330 train_time:17184ms step_avg:39.78ms
step:433/2330 train_time:17208ms step_avg:39.74ms
step:434/2330 train_time:17264ms step_avg:39.78ms
step:435/2330 train_time:17287ms step_avg:39.74ms
step:436/2330 train_time:17344ms step_avg:39.78ms
step:437/2330 train_time:17367ms step_avg:39.74ms
step:438/2330 train_time:17423ms step_avg:39.78ms
step:439/2330 train_time:17446ms step_avg:39.74ms
step:440/2330 train_time:17502ms step_avg:39.78ms
step:441/2330 train_time:17525ms step_avg:39.74ms
step:442/2330 train_time:17581ms step_avg:39.78ms
step:443/2330 train_time:17603ms step_avg:39.74ms
step:444/2330 train_time:17659ms step_avg:39.77ms
step:445/2330 train_time:17682ms step_avg:39.73ms
step:446/2330 train_time:17738ms step_avg:39.77ms
step:447/2330 train_time:17761ms step_avg:39.73ms
step:448/2330 train_time:17818ms step_avg:39.77ms
step:449/2330 train_time:17841ms step_avg:39.74ms
step:450/2330 train_time:17898ms step_avg:39.77ms
step:451/2330 train_time:17920ms step_avg:39.73ms
step:452/2330 train_time:17977ms step_avg:39.77ms
step:453/2330 train_time:18000ms step_avg:39.74ms
step:454/2330 train_time:18057ms step_avg:39.77ms
step:455/2330 train_time:18080ms step_avg:39.74ms
step:456/2330 train_time:18137ms step_avg:39.77ms
step:457/2330 train_time:18160ms step_avg:39.74ms
step:458/2330 train_time:18217ms step_avg:39.78ms
step:459/2330 train_time:18240ms step_avg:39.74ms
step:460/2330 train_time:18297ms step_avg:39.78ms
step:461/2330 train_time:18320ms step_avg:39.74ms
step:462/2330 train_time:18376ms step_avg:39.78ms
step:463/2330 train_time:18399ms step_avg:39.74ms
step:464/2330 train_time:18456ms step_avg:39.77ms
step:465/2330 train_time:18478ms step_avg:39.74ms
step:466/2330 train_time:18534ms step_avg:39.77ms
step:467/2330 train_time:18556ms step_avg:39.74ms
step:468/2330 train_time:18613ms step_avg:39.77ms
step:469/2330 train_time:18636ms step_avg:39.74ms
step:470/2330 train_time:18692ms step_avg:39.77ms
step:471/2330 train_time:18715ms step_avg:39.73ms
step:472/2330 train_time:18772ms step_avg:39.77ms
step:473/2330 train_time:18795ms step_avg:39.73ms
step:474/2330 train_time:18851ms step_avg:39.77ms
step:475/2330 train_time:18874ms step_avg:39.74ms
step:476/2330 train_time:18931ms step_avg:39.77ms
step:477/2330 train_time:18954ms step_avg:39.74ms
step:478/2330 train_time:19011ms step_avg:39.77ms
step:479/2330 train_time:19034ms step_avg:39.74ms
step:480/2330 train_time:19091ms step_avg:39.77ms
step:481/2330 train_time:19114ms step_avg:39.74ms
step:482/2330 train_time:19171ms step_avg:39.77ms
step:483/2330 train_time:19193ms step_avg:39.74ms
step:484/2330 train_time:19249ms step_avg:39.77ms
step:485/2330 train_time:19272ms step_avg:39.74ms
step:486/2330 train_time:19329ms step_avg:39.77ms
step:487/2330 train_time:19351ms step_avg:39.74ms
step:488/2330 train_time:19408ms step_avg:39.77ms
step:489/2330 train_time:19430ms step_avg:39.73ms
step:490/2330 train_time:19487ms step_avg:39.77ms
step:491/2330 train_time:19509ms step_avg:39.73ms
step:492/2330 train_time:19565ms step_avg:39.77ms
step:493/2330 train_time:19588ms step_avg:39.73ms
step:494/2330 train_time:19643ms step_avg:39.76ms
step:495/2330 train_time:19666ms step_avg:39.73ms
step:496/2330 train_time:19723ms step_avg:39.76ms
step:497/2330 train_time:19747ms step_avg:39.73ms
step:498/2330 train_time:19803ms step_avg:39.77ms
step:499/2330 train_time:19826ms step_avg:39.73ms
step:500/2330 train_time:19882ms step_avg:39.76ms
step:500/2330 val_loss:5.3599 train_time:19980ms step_avg:39.96ms
step:501/2330 train_time:19992ms step_avg:39.90ms
step:502/2330 train_time:20004ms step_avg:39.85ms
step:503/2330 train_time:20013ms step_avg:39.79ms
step:504/2330 train_time:20043ms step_avg:39.77ms
step:505/2330 train_time:20065ms step_avg:39.73ms
step:506/2330 train_time:20120ms step_avg:39.76ms
step:507/2330 train_time:20142ms step_avg:39.73ms
step:508/2330 train_time:20197ms step_avg:39.76ms
step:509/2330 train_time:20220ms step_avg:39.72ms
step:510/2330 train_time:20275ms step_avg:39.76ms
step:511/2330 train_time:20301ms step_avg:39.73ms
step:512/2330 train_time:20361ms step_avg:39.77ms
step:513/2330 train_time:20386ms step_avg:39.74ms
step:514/2330 train_time:20443ms step_avg:39.77ms
step:515/2330 train_time:20467ms step_avg:39.74ms
step:516/2330 train_time:20523ms step_avg:39.77ms
step:517/2330 train_time:20545ms step_avg:39.74ms
step:518/2330 train_time:20601ms step_avg:39.77ms
step:519/2330 train_time:20624ms step_avg:39.74ms
step:520/2330 train_time:20680ms step_avg:39.77ms
step:521/2330 train_time:20702ms step_avg:39.74ms
step:522/2330 train_time:20758ms step_avg:39.77ms
step:523/2330 train_time:20780ms step_avg:39.73ms
step:524/2330 train_time:20836ms step_avg:39.76ms
step:525/2330 train_time:20858ms step_avg:39.73ms
step:526/2330 train_time:20914ms step_avg:39.76ms
step:527/2330 train_time:20937ms step_avg:39.73ms
step:528/2330 train_time:20994ms step_avg:39.76ms
step:529/2330 train_time:21017ms step_avg:39.73ms
step:530/2330 train_time:21073ms step_avg:39.76ms
step:531/2330 train_time:21095ms step_avg:39.73ms
step:532/2330 train_time:21151ms step_avg:39.76ms
step:533/2330 train_time:21173ms step_avg:39.72ms
step:534/2330 train_time:21231ms step_avg:39.76ms
step:535/2330 train_time:21255ms step_avg:39.73ms
step:536/2330 train_time:21312ms step_avg:39.76ms
step:537/2330 train_time:21336ms step_avg:39.73ms
step:538/2330 train_time:21394ms step_avg:39.77ms
step:539/2330 train_time:21417ms step_avg:39.74ms
step:540/2330 train_time:21474ms step_avg:39.77ms
step:541/2330 train_time:21498ms step_avg:39.74ms
step:542/2330 train_time:21555ms step_avg:39.77ms
step:543/2330 train_time:21578ms step_avg:39.74ms
step:544/2330 train_time:21635ms step_avg:39.77ms
step:545/2330 train_time:21657ms step_avg:39.74ms
step:546/2330 train_time:21713ms step_avg:39.77ms
step:547/2330 train_time:21736ms step_avg:39.74ms
step:548/2330 train_time:21791ms step_avg:39.77ms
step:549/2330 train_time:21814ms step_avg:39.73ms
step:550/2330 train_time:21870ms step_avg:39.76ms
step:551/2330 train_time:21892ms step_avg:39.73ms
step:552/2330 train_time:21948ms step_avg:39.76ms
step:553/2330 train_time:21971ms step_avg:39.73ms
step:554/2330 train_time:22027ms step_avg:39.76ms
step:555/2330 train_time:22049ms step_avg:39.73ms
step:556/2330 train_time:22106ms step_avg:39.76ms
step:557/2330 train_time:22128ms step_avg:39.73ms
step:558/2330 train_time:22184ms step_avg:39.76ms
step:559/2330 train_time:22206ms step_avg:39.73ms
step:560/2330 train_time:22265ms step_avg:39.76ms
step:561/2330 train_time:22288ms step_avg:39.73ms
step:562/2330 train_time:22345ms step_avg:39.76ms
step:563/2330 train_time:22368ms step_avg:39.73ms
step:564/2330 train_time:22426ms step_avg:39.76ms
step:565/2330 train_time:22449ms step_avg:39.73ms
step:566/2330 train_time:22506ms step_avg:39.76ms
step:567/2330 train_time:22528ms step_avg:39.73ms
step:568/2330 train_time:22586ms step_avg:39.76ms
step:569/2330 train_time:22608ms step_avg:39.73ms
step:570/2330 train_time:22665ms step_avg:39.76ms
step:571/2330 train_time:22688ms step_avg:39.73ms
step:572/2330 train_time:22745ms step_avg:39.76ms
step:573/2330 train_time:22767ms step_avg:39.73ms
step:574/2330 train_time:22823ms step_avg:39.76ms
step:575/2330 train_time:22845ms step_avg:39.73ms
step:576/2330 train_time:22902ms step_avg:39.76ms
step:577/2330 train_time:22924ms step_avg:39.73ms
step:578/2330 train_time:22980ms step_avg:39.76ms
step:579/2330 train_time:23002ms step_avg:39.73ms
step:580/2330 train_time:23057ms step_avg:39.75ms
step:581/2330 train_time:23079ms step_avg:39.72ms
step:582/2330 train_time:23136ms step_avg:39.75ms
step:583/2330 train_time:23159ms step_avg:39.72ms
step:584/2330 train_time:23217ms step_avg:39.75ms
step:585/2330 train_time:23241ms step_avg:39.73ms
step:586/2330 train_time:23297ms step_avg:39.76ms
step:587/2330 train_time:23321ms step_avg:39.73ms
step:588/2330 train_time:23377ms step_avg:39.76ms
step:589/2330 train_time:23401ms step_avg:39.73ms
step:590/2330 train_time:23457ms step_avg:39.76ms
step:591/2330 train_time:23480ms step_avg:39.73ms
step:592/2330 train_time:23537ms step_avg:39.76ms
step:593/2330 train_time:23561ms step_avg:39.73ms
step:594/2330 train_time:23617ms step_avg:39.76ms
step:595/2330 train_time:23640ms step_avg:39.73ms
step:596/2330 train_time:23696ms step_avg:39.76ms
step:597/2330 train_time:23719ms step_avg:39.73ms
step:598/2330 train_time:23776ms step_avg:39.76ms
step:599/2330 train_time:23799ms step_avg:39.73ms
step:600/2330 train_time:23856ms step_avg:39.76ms
step:601/2330 train_time:23878ms step_avg:39.73ms
step:602/2330 train_time:23935ms step_avg:39.76ms
step:603/2330 train_time:23958ms step_avg:39.73ms
step:604/2330 train_time:24013ms step_avg:39.76ms
step:605/2330 train_time:24037ms step_avg:39.73ms
step:606/2330 train_time:24093ms step_avg:39.76ms
step:607/2330 train_time:24115ms step_avg:39.73ms
step:608/2330 train_time:24172ms step_avg:39.76ms
step:609/2330 train_time:24195ms step_avg:39.73ms
step:610/2330 train_time:24251ms step_avg:39.76ms
step:611/2330 train_time:24274ms step_avg:39.73ms
step:612/2330 train_time:24330ms step_avg:39.76ms
step:613/2330 train_time:24353ms step_avg:39.73ms
step:614/2330 train_time:24410ms step_avg:39.76ms
step:615/2330 train_time:24433ms step_avg:39.73ms
step:616/2330 train_time:24490ms step_avg:39.76ms
step:617/2330 train_time:24514ms step_avg:39.73ms
step:618/2330 train_time:24571ms step_avg:39.76ms
step:619/2330 train_time:24595ms step_avg:39.73ms
step:620/2330 train_time:24651ms step_avg:39.76ms
step:621/2330 train_time:24674ms step_avg:39.73ms
step:622/2330 train_time:24731ms step_avg:39.76ms
step:623/2330 train_time:24754ms step_avg:39.73ms
step:624/2330 train_time:24811ms step_avg:39.76ms
step:625/2330 train_time:24833ms step_avg:39.73ms
step:626/2330 train_time:24890ms step_avg:39.76ms
step:627/2330 train_time:24912ms step_avg:39.73ms
step:628/2330 train_time:24968ms step_avg:39.76ms
step:629/2330 train_time:24991ms step_avg:39.73ms
step:630/2330 train_time:25047ms step_avg:39.76ms
step:631/2330 train_time:25069ms step_avg:39.73ms
step:632/2330 train_time:25127ms step_avg:39.76ms
step:633/2330 train_time:25149ms step_avg:39.73ms
step:634/2330 train_time:25206ms step_avg:39.76ms
step:635/2330 train_time:25228ms step_avg:39.73ms
step:636/2330 train_time:25285ms step_avg:39.76ms
step:637/2330 train_time:25307ms step_avg:39.73ms
step:638/2330 train_time:25363ms step_avg:39.75ms
step:639/2330 train_time:25385ms step_avg:39.73ms
step:640/2330 train_time:25443ms step_avg:39.75ms
step:641/2330 train_time:25465ms step_avg:39.73ms
step:642/2330 train_time:25522ms step_avg:39.75ms
step:643/2330 train_time:25544ms step_avg:39.73ms
step:644/2330 train_time:25601ms step_avg:39.75ms
step:645/2330 train_time:25623ms step_avg:39.73ms
step:646/2330 train_time:25681ms step_avg:39.75ms
step:647/2330 train_time:25703ms step_avg:39.73ms
step:648/2330 train_time:25760ms step_avg:39.75ms
step:649/2330 train_time:25782ms step_avg:39.73ms
step:650/2330 train_time:25838ms step_avg:39.75ms
step:651/2330 train_time:25861ms step_avg:39.73ms
step:652/2330 train_time:25917ms step_avg:39.75ms
step:653/2330 train_time:25940ms step_avg:39.72ms
step:654/2330 train_time:25996ms step_avg:39.75ms
step:655/2330 train_time:26019ms step_avg:39.72ms
step:656/2330 train_time:26076ms step_avg:39.75ms
step:657/2330 train_time:26099ms step_avg:39.72ms
step:658/2330 train_time:26156ms step_avg:39.75ms
step:659/2330 train_time:26179ms step_avg:39.73ms
step:660/2330 train_time:26236ms step_avg:39.75ms
step:661/2330 train_time:26260ms step_avg:39.73ms
step:662/2330 train_time:26316ms step_avg:39.75ms
step:663/2330 train_time:26339ms step_avg:39.73ms
step:664/2330 train_time:26395ms step_avg:39.75ms
step:665/2330 train_time:26419ms step_avg:39.73ms
step:666/2330 train_time:26475ms step_avg:39.75ms
step:667/2330 train_time:26498ms step_avg:39.73ms
step:668/2330 train_time:26554ms step_avg:39.75ms
step:669/2330 train_time:26577ms step_avg:39.73ms
step:670/2330 train_time:26634ms step_avg:39.75ms
step:671/2330 train_time:26657ms step_avg:39.73ms
step:672/2330 train_time:26714ms step_avg:39.75ms
step:673/2330 train_time:26736ms step_avg:39.73ms
step:674/2330 train_time:26793ms step_avg:39.75ms
step:675/2330 train_time:26816ms step_avg:39.73ms
step:676/2330 train_time:26872ms step_avg:39.75ms
step:677/2330 train_time:26894ms step_avg:39.73ms
step:678/2330 train_time:26951ms step_avg:39.75ms
step:679/2330 train_time:26974ms step_avg:39.73ms
step:680/2330 train_time:27030ms step_avg:39.75ms
step:681/2330 train_time:27052ms step_avg:39.72ms
step:682/2330 train_time:27109ms step_avg:39.75ms
step:683/2330 train_time:27131ms step_avg:39.72ms
step:684/2330 train_time:27188ms step_avg:39.75ms
step:685/2330 train_time:27211ms step_avg:39.72ms
step:686/2330 train_time:27267ms step_avg:39.75ms
step:687/2330 train_time:27289ms step_avg:39.72ms
step:688/2330 train_time:27346ms step_avg:39.75ms
step:689/2330 train_time:27368ms step_avg:39.72ms
step:690/2330 train_time:27425ms step_avg:39.75ms
step:691/2330 train_time:27447ms step_avg:39.72ms
step:692/2330 train_time:27505ms step_avg:39.75ms
step:693/2330 train_time:27528ms step_avg:39.72ms
step:694/2330 train_time:27585ms step_avg:39.75ms
step:695/2330 train_time:27607ms step_avg:39.72ms
step:696/2330 train_time:27664ms step_avg:39.75ms
step:697/2330 train_time:27686ms step_avg:39.72ms
step:698/2330 train_time:27743ms step_avg:39.75ms
step:699/2330 train_time:27765ms step_avg:39.72ms
step:700/2330 train_time:27821ms step_avg:39.74ms
step:701/2330 train_time:27844ms step_avg:39.72ms
step:702/2330 train_time:27900ms step_avg:39.74ms
step:703/2330 train_time:27922ms step_avg:39.72ms
step:704/2330 train_time:27979ms step_avg:39.74ms
step:705/2330 train_time:28001ms step_avg:39.72ms
step:706/2330 train_time:28057ms step_avg:39.74ms
step:707/2330 train_time:28081ms step_avg:39.72ms
step:708/2330 train_time:28137ms step_avg:39.74ms
step:709/2330 train_time:28161ms step_avg:39.72ms
step:710/2330 train_time:28218ms step_avg:39.74ms
step:711/2330 train_time:28241ms step_avg:39.72ms
step:712/2330 train_time:28298ms step_avg:39.74ms
step:713/2330 train_time:28321ms step_avg:39.72ms
step:714/2330 train_time:28377ms step_avg:39.74ms
step:715/2330 train_time:28401ms step_avg:39.72ms
step:716/2330 train_time:28457ms step_avg:39.74ms
step:717/2330 train_time:28480ms step_avg:39.72ms
step:718/2330 train_time:28536ms step_avg:39.74ms
step:719/2330 train_time:28560ms step_avg:39.72ms
step:720/2330 train_time:28616ms step_avg:39.74ms
step:721/2330 train_time:28639ms step_avg:39.72ms
step:722/2330 train_time:28696ms step_avg:39.74ms
step:723/2330 train_time:28719ms step_avg:39.72ms
step:724/2330 train_time:28775ms step_avg:39.74ms
step:725/2330 train_time:28798ms step_avg:39.72ms
step:726/2330 train_time:28855ms step_avg:39.74ms
step:727/2330 train_time:28877ms step_avg:39.72ms
step:728/2330 train_time:28933ms step_avg:39.74ms
step:729/2330 train_time:28956ms step_avg:39.72ms
step:730/2330 train_time:29013ms step_avg:39.74ms
step:731/2330 train_time:29035ms step_avg:39.72ms
step:732/2330 train_time:29092ms step_avg:39.74ms
step:733/2330 train_time:29115ms step_avg:39.72ms
step:734/2330 train_time:29171ms step_avg:39.74ms
step:735/2330 train_time:29195ms step_avg:39.72ms
step:736/2330 train_time:29252ms step_avg:39.74ms
step:737/2330 train_time:29274ms step_avg:39.72ms
step:738/2330 train_time:29331ms step_avg:39.74ms
step:739/2330 train_time:29354ms step_avg:39.72ms
step:740/2330 train_time:29411ms step_avg:39.74ms
step:741/2330 train_time:29433ms step_avg:39.72ms
step:742/2330 train_time:29490ms step_avg:39.74ms
step:743/2330 train_time:29513ms step_avg:39.72ms
step:744/2330 train_time:29570ms step_avg:39.74ms
step:745/2330 train_time:29593ms step_avg:39.72ms
step:746/2330 train_time:29650ms step_avg:39.75ms
step:747/2330 train_time:29673ms step_avg:39.72ms
step:748/2330 train_time:29729ms step_avg:39.74ms
step:749/2330 train_time:29751ms step_avg:39.72ms
step:750/2330 train_time:29807ms step_avg:39.74ms
step:750/2330 val_loss:5.2729 train_time:29903ms step_avg:39.87ms
step:751/2330 train_time:29914ms step_avg:39.83ms
step:752/2330 train_time:29925ms step_avg:39.79ms
step:753/2330 train_time:29934ms step_avg:39.75ms
step:754/2330 train_time:29965ms step_avg:39.74ms
step:755/2330 train_time:29987ms step_avg:39.72ms
step:756/2330 train_time:30042ms step_avg:39.74ms
step:757/2330 train_time:30064ms step_avg:39.71ms
step:758/2330 train_time:30120ms step_avg:39.74ms
step:759/2330 train_time:30141ms step_avg:39.71ms
step:760/2330 train_time:30198ms step_avg:39.73ms
step:761/2330 train_time:30225ms step_avg:39.72ms
step:762/2330 train_time:30285ms step_avg:39.74ms
step:763/2330 train_time:30309ms step_avg:39.72ms
step:764/2330 train_time:30367ms step_avg:39.75ms
step:765/2330 train_time:30390ms step_avg:39.73ms
step:766/2330 train_time:30448ms step_avg:39.75ms
step:767/2330 train_time:30469ms step_avg:39.73ms
step:768/2330 train_time:30525ms step_avg:39.75ms
step:769/2330 train_time:30547ms step_avg:39.72ms
step:770/2330 train_time:30603ms step_avg:39.74ms
step:771/2330 train_time:30626ms step_avg:39.72ms
step:772/2330 train_time:30681ms step_avg:39.74ms
step:773/2330 train_time:30703ms step_avg:39.72ms
step:774/2330 train_time:30759ms step_avg:39.74ms
step:775/2330 train_time:30781ms step_avg:39.72ms
step:776/2330 train_time:30838ms step_avg:39.74ms
step:777/2330 train_time:30862ms step_avg:39.72ms
step:778/2330 train_time:30918ms step_avg:39.74ms
step:779/2330 train_time:30941ms step_avg:39.72ms
step:780/2330 train_time:30997ms step_avg:39.74ms
step:781/2330 train_time:31020ms step_avg:39.72ms
step:782/2330 train_time:31076ms step_avg:39.74ms
step:783/2330 train_time:31099ms step_avg:39.72ms
step:784/2330 train_time:31156ms step_avg:39.74ms
step:785/2330 train_time:31179ms step_avg:39.72ms
step:786/2330 train_time:31237ms step_avg:39.74ms
step:787/2330 train_time:31261ms step_avg:39.72ms
step:788/2330 train_time:31319ms step_avg:39.74ms
step:789/2330 train_time:31343ms step_avg:39.72ms
step:790/2330 train_time:31399ms step_avg:39.75ms
step:791/2330 train_time:31423ms step_avg:39.73ms
step:792/2330 train_time:31478ms step_avg:39.75ms
step:793/2330 train_time:31501ms step_avg:39.72ms
step:794/2330 train_time:31556ms step_avg:39.74ms
step:795/2330 train_time:31579ms step_avg:39.72ms
step:796/2330 train_time:31635ms step_avg:39.74ms
step:797/2330 train_time:31658ms step_avg:39.72ms
step:798/2330 train_time:31714ms step_avg:39.74ms
step:799/2330 train_time:31736ms step_avg:39.72ms
step:800/2330 train_time:31793ms step_avg:39.74ms
step:801/2330 train_time:31815ms step_avg:39.72ms
step:802/2330 train_time:31871ms step_avg:39.74ms
step:803/2330 train_time:31894ms step_avg:39.72ms
step:804/2330 train_time:31951ms step_avg:39.74ms
step:805/2330 train_time:31974ms step_avg:39.72ms
step:806/2330 train_time:32031ms step_avg:39.74ms
step:807/2330 train_time:32054ms step_avg:39.72ms
step:808/2330 train_time:32110ms step_avg:39.74ms
step:809/2330 train_time:32133ms step_avg:39.72ms
step:810/2330 train_time:32190ms step_avg:39.74ms
step:811/2330 train_time:32214ms step_avg:39.72ms
step:812/2330 train_time:32272ms step_avg:39.74ms
step:813/2330 train_time:32295ms step_avg:39.72ms
step:814/2330 train_time:32351ms step_avg:39.74ms
step:815/2330 train_time:32375ms step_avg:39.72ms
step:816/2330 train_time:32431ms step_avg:39.74ms
step:817/2330 train_time:32454ms step_avg:39.72ms
step:818/2330 train_time:32511ms step_avg:39.74ms
step:819/2330 train_time:32533ms step_avg:39.72ms
step:820/2330 train_time:32589ms step_avg:39.74ms
step:821/2330 train_time:32612ms step_avg:39.72ms
step:822/2330 train_time:32669ms step_avg:39.74ms
step:823/2330 train_time:32691ms step_avg:39.72ms
step:824/2330 train_time:32748ms step_avg:39.74ms
step:825/2330 train_time:32770ms step_avg:39.72ms
step:826/2330 train_time:32827ms step_avg:39.74ms
step:827/2330 train_time:32849ms step_avg:39.72ms
step:828/2330 train_time:32905ms step_avg:39.74ms
step:829/2330 train_time:32927ms step_avg:39.72ms
step:830/2330 train_time:32984ms step_avg:39.74ms
step:831/2330 train_time:33006ms step_avg:39.72ms
step:832/2330 train_time:33063ms step_avg:39.74ms
step:833/2330 train_time:33085ms step_avg:39.72ms
step:834/2330 train_time:33142ms step_avg:39.74ms
step:835/2330 train_time:33164ms step_avg:39.72ms
step:836/2330 train_time:33220ms step_avg:39.74ms
step:837/2330 train_time:33243ms step_avg:39.72ms
step:838/2330 train_time:33300ms step_avg:39.74ms
step:839/2330 train_time:33323ms step_avg:39.72ms
step:840/2330 train_time:33380ms step_avg:39.74ms
step:841/2330 train_time:33403ms step_avg:39.72ms
step:842/2330 train_time:33459ms step_avg:39.74ms
step:843/2330 train_time:33482ms step_avg:39.72ms
step:844/2330 train_time:33538ms step_avg:39.74ms
step:845/2330 train_time:33561ms step_avg:39.72ms
step:846/2330 train_time:33617ms step_avg:39.74ms
step:847/2330 train_time:33641ms step_avg:39.72ms
step:848/2330 train_time:33697ms step_avg:39.74ms
step:849/2330 train_time:33720ms step_avg:39.72ms
step:850/2330 train_time:33776ms step_avg:39.74ms
step:851/2330 train_time:33799ms step_avg:39.72ms
step:852/2330 train_time:33855ms step_avg:39.74ms
step:853/2330 train_time:33878ms step_avg:39.72ms
step:854/2330 train_time:33934ms step_avg:39.74ms
step:855/2330 train_time:33957ms step_avg:39.72ms
step:856/2330 train_time:34013ms step_avg:39.73ms
step:857/2330 train_time:34035ms step_avg:39.71ms
step:858/2330 train_time:34091ms step_avg:39.73ms
step:859/2330 train_time:34115ms step_avg:39.71ms
step:860/2330 train_time:34171ms step_avg:39.73ms
step:861/2330 train_time:34194ms step_avg:39.71ms
step:862/2330 train_time:34251ms step_avg:39.73ms
step:863/2330 train_time:34274ms step_avg:39.71ms
step:864/2330 train_time:34331ms step_avg:39.73ms
step:865/2330 train_time:34354ms step_avg:39.72ms
step:866/2330 train_time:34411ms step_avg:39.74ms
step:867/2330 train_time:34433ms step_avg:39.71ms
step:868/2330 train_time:34489ms step_avg:39.73ms
step:869/2330 train_time:34512ms step_avg:39.72ms
step:870/2330 train_time:34570ms step_avg:39.74ms
step:871/2330 train_time:34592ms step_avg:39.72ms
step:872/2330 train_time:34649ms step_avg:39.74ms
step:873/2330 train_time:34672ms step_avg:39.72ms
step:874/2330 train_time:34729ms step_avg:39.74ms
step:875/2330 train_time:34752ms step_avg:39.72ms
step:876/2330 train_time:34808ms step_avg:39.74ms
step:877/2330 train_time:34830ms step_avg:39.72ms
step:878/2330 train_time:34887ms step_avg:39.73ms
step:879/2330 train_time:34909ms step_avg:39.71ms
step:880/2330 train_time:34966ms step_avg:39.73ms
step:881/2330 train_time:34988ms step_avg:39.71ms
step:882/2330 train_time:35045ms step_avg:39.73ms
step:883/2330 train_time:35069ms step_avg:39.72ms
step:884/2330 train_time:35125ms step_avg:39.73ms
step:885/2330 train_time:35147ms step_avg:39.71ms
step:886/2330 train_time:35204ms step_avg:39.73ms
step:887/2330 train_time:35226ms step_avg:39.71ms
step:888/2330 train_time:35283ms step_avg:39.73ms
step:889/2330 train_time:35306ms step_avg:39.71ms
step:890/2330 train_time:35362ms step_avg:39.73ms
step:891/2330 train_time:35385ms step_avg:39.71ms
step:892/2330 train_time:35442ms step_avg:39.73ms
step:893/2330 train_time:35464ms step_avg:39.71ms
step:894/2330 train_time:35521ms step_avg:39.73ms
step:895/2330 train_time:35543ms step_avg:39.71ms
step:896/2330 train_time:35600ms step_avg:39.73ms
step:897/2330 train_time:35622ms step_avg:39.71ms
step:898/2330 train_time:35679ms step_avg:39.73ms
step:899/2330 train_time:35702ms step_avg:39.71ms
step:900/2330 train_time:35758ms step_avg:39.73ms
step:901/2330 train_time:35781ms step_avg:39.71ms
step:902/2330 train_time:35837ms step_avg:39.73ms
step:903/2330 train_time:35860ms step_avg:39.71ms
step:904/2330 train_time:35917ms step_avg:39.73ms
step:905/2330 train_time:35940ms step_avg:39.71ms
step:906/2330 train_time:35997ms step_avg:39.73ms
step:907/2330 train_time:36020ms step_avg:39.71ms
step:908/2330 train_time:36076ms step_avg:39.73ms
step:909/2330 train_time:36099ms step_avg:39.71ms
step:910/2330 train_time:36156ms step_avg:39.73ms
step:911/2330 train_time:36179ms step_avg:39.71ms
step:912/2330 train_time:36235ms step_avg:39.73ms
step:913/2330 train_time:36258ms step_avg:39.71ms
step:914/2330 train_time:36315ms step_avg:39.73ms
step:915/2330 train_time:36338ms step_avg:39.71ms
step:916/2330 train_time:36395ms step_avg:39.73ms
step:917/2330 train_time:36418ms step_avg:39.71ms
step:918/2330 train_time:36474ms step_avg:39.73ms
step:919/2330 train_time:36497ms step_avg:39.71ms
step:920/2330 train_time:36553ms step_avg:39.73ms
step:921/2330 train_time:36576ms step_avg:39.71ms
step:922/2330 train_time:36633ms step_avg:39.73ms
step:923/2330 train_time:36655ms step_avg:39.71ms
step:924/2330 train_time:36712ms step_avg:39.73ms
step:925/2330 train_time:36735ms step_avg:39.71ms
step:926/2330 train_time:36791ms step_avg:39.73ms
step:927/2330 train_time:36814ms step_avg:39.71ms
step:928/2330 train_time:36871ms step_avg:39.73ms
step:929/2330 train_time:36893ms step_avg:39.71ms
step:930/2330 train_time:36950ms step_avg:39.73ms
step:931/2330 train_time:36973ms step_avg:39.71ms
step:932/2330 train_time:37030ms step_avg:39.73ms
step:933/2330 train_time:37053ms step_avg:39.71ms
step:934/2330 train_time:37110ms step_avg:39.73ms
step:935/2330 train_time:37133ms step_avg:39.71ms
step:936/2330 train_time:37189ms step_avg:39.73ms
step:937/2330 train_time:37212ms step_avg:39.71ms
step:938/2330 train_time:37268ms step_avg:39.73ms
step:939/2330 train_time:37292ms step_avg:39.71ms
step:940/2330 train_time:37349ms step_avg:39.73ms
step:941/2330 train_time:37372ms step_avg:39.71ms
step:942/2330 train_time:37428ms step_avg:39.73ms
step:943/2330 train_time:37450ms step_avg:39.71ms
step:944/2330 train_time:37507ms step_avg:39.73ms
step:945/2330 train_time:37529ms step_avg:39.71ms
step:946/2330 train_time:37586ms step_avg:39.73ms
step:947/2330 train_time:37608ms step_avg:39.71ms
step:948/2330 train_time:37664ms step_avg:39.73ms
step:949/2330 train_time:37687ms step_avg:39.71ms
step:950/2330 train_time:37743ms step_avg:39.73ms
step:951/2330 train_time:37765ms step_avg:39.71ms
step:952/2330 train_time:37822ms step_avg:39.73ms
step:953/2330 train_time:37844ms step_avg:39.71ms
step:954/2330 train_time:37901ms step_avg:39.73ms
step:955/2330 train_time:37923ms step_avg:39.71ms
step:956/2330 train_time:37980ms step_avg:39.73ms
step:957/2330 train_time:38002ms step_avg:39.71ms
step:958/2330 train_time:38058ms step_avg:39.73ms
step:959/2330 train_time:38081ms step_avg:39.71ms
step:960/2330 train_time:38137ms step_avg:39.73ms
step:961/2330 train_time:38160ms step_avg:39.71ms
step:962/2330 train_time:38217ms step_avg:39.73ms
step:963/2330 train_time:38240ms step_avg:39.71ms
step:964/2330 train_time:38297ms step_avg:39.73ms
step:965/2330 train_time:38320ms step_avg:39.71ms
step:966/2330 train_time:38378ms step_avg:39.73ms
step:967/2330 train_time:38401ms step_avg:39.71ms
step:968/2330 train_time:38457ms step_avg:39.73ms
step:969/2330 train_time:38481ms step_avg:39.71ms
step:970/2330 train_time:38537ms step_avg:39.73ms
step:971/2330 train_time:38560ms step_avg:39.71ms
step:972/2330 train_time:38615ms step_avg:39.73ms
step:973/2330 train_time:38639ms step_avg:39.71ms
step:974/2330 train_time:38695ms step_avg:39.73ms
step:975/2330 train_time:38718ms step_avg:39.71ms
step:976/2330 train_time:38775ms step_avg:39.73ms
step:977/2330 train_time:38798ms step_avg:39.71ms
step:978/2330 train_time:38854ms step_avg:39.73ms
step:979/2330 train_time:38877ms step_avg:39.71ms
step:980/2330 train_time:38933ms step_avg:39.73ms
step:981/2330 train_time:38956ms step_avg:39.71ms
step:982/2330 train_time:39012ms step_avg:39.73ms
step:983/2330 train_time:39035ms step_avg:39.71ms
step:984/2330 train_time:39091ms step_avg:39.73ms
step:985/2330 train_time:39114ms step_avg:39.71ms
step:986/2330 train_time:39170ms step_avg:39.73ms
step:987/2330 train_time:39194ms step_avg:39.71ms
step:988/2330 train_time:39251ms step_avg:39.73ms
step:989/2330 train_time:39274ms step_avg:39.71ms
step:990/2330 train_time:39330ms step_avg:39.73ms
step:991/2330 train_time:39354ms step_avg:39.71ms
step:992/2330 train_time:39411ms step_avg:39.73ms
step:993/2330 train_time:39434ms step_avg:39.71ms
step:994/2330 train_time:39490ms step_avg:39.73ms
step:995/2330 train_time:39513ms step_avg:39.71ms
step:996/2330 train_time:39569ms step_avg:39.73ms
step:997/2330 train_time:39592ms step_avg:39.71ms
step:998/2330 train_time:39649ms step_avg:39.73ms
step:999/2330 train_time:39671ms step_avg:39.71ms
step:1000/2330 train_time:39728ms step_avg:39.73ms
step:1000/2330 val_loss:5.2314 train_time:39824ms step_avg:39.82ms
step:1001/2330 train_time:39836ms step_avg:39.80ms
step:1002/2330 train_time:39847ms step_avg:39.77ms
step:1003/2330 train_time:39857ms step_avg:39.74ms
step:1004/2330 train_time:39886ms step_avg:39.73ms
step:1005/2330 train_time:39907ms step_avg:39.71ms
step:1006/2330 train_time:39962ms step_avg:39.72ms
step:1007/2330 train_time:39984ms step_avg:39.71ms
step:1008/2330 train_time:40039ms step_avg:39.72ms
step:1009/2330 train_time:40061ms step_avg:39.70ms
step:1010/2330 train_time:40118ms step_avg:39.72ms
step:1011/2330 train_time:40145ms step_avg:39.71ms
step:1012/2330 train_time:40205ms step_avg:39.73ms
step:1013/2330 train_time:40229ms step_avg:39.71ms
step:1014/2330 train_time:40287ms step_avg:39.73ms
step:1015/2330 train_time:40310ms step_avg:39.71ms
step:1016/2330 train_time:40366ms step_avg:39.73ms
step:1017/2330 train_time:40388ms step_avg:39.71ms
step:1018/2330 train_time:40444ms step_avg:39.73ms
step:1019/2330 train_time:40466ms step_avg:39.71ms
step:1020/2330 train_time:40522ms step_avg:39.73ms
step:1021/2330 train_time:40544ms step_avg:39.71ms
step:1022/2330 train_time:40598ms step_avg:39.72ms
step:1023/2330 train_time:40621ms step_avg:39.71ms
step:1024/2330 train_time:40676ms step_avg:39.72ms
step:1025/2330 train_time:40698ms step_avg:39.71ms
step:1026/2330 train_time:40756ms step_avg:39.72ms
step:1027/2330 train_time:40780ms step_avg:39.71ms
step:1028/2330 train_time:40837ms step_avg:39.72ms
step:1029/2330 train_time:40859ms step_avg:39.71ms
step:1030/2330 train_time:40915ms step_avg:39.72ms
step:1031/2330 train_time:40937ms step_avg:39.71ms
step:1032/2330 train_time:40993ms step_avg:39.72ms
step:1033/2330 train_time:41015ms step_avg:39.71ms
step:1034/2330 train_time:41073ms step_avg:39.72ms
step:1035/2330 train_time:41097ms step_avg:39.71ms
step:1036/2330 train_time:41155ms step_avg:39.72ms
step:1037/2330 train_time:41179ms step_avg:39.71ms
step:1038/2330 train_time:41237ms step_avg:39.73ms
step:1039/2330 train_time:41260ms step_avg:39.71ms
step:1040/2330 train_time:41316ms step_avg:39.73ms
step:1041/2330 train_time:41339ms step_avg:39.71ms
step:1042/2330 train_time:41395ms step_avg:39.73ms
step:1043/2330 train_time:41419ms step_avg:39.71ms
step:1044/2330 train_time:41475ms step_avg:39.73ms
step:1045/2330 train_time:41498ms step_avg:39.71ms
step:1046/2330 train_time:41554ms step_avg:39.73ms
step:1047/2330 train_time:41577ms step_avg:39.71ms
step:1048/2330 train_time:41633ms step_avg:39.73ms
step:1049/2330 train_time:41655ms step_avg:39.71ms
step:1050/2330 train_time:41712ms step_avg:39.73ms
step:1051/2330 train_time:41734ms step_avg:39.71ms
step:1052/2330 train_time:41791ms step_avg:39.73ms
step:1053/2330 train_time:41814ms step_avg:39.71ms
step:1054/2330 train_time:41870ms step_avg:39.72ms
step:1055/2330 train_time:41892ms step_avg:39.71ms
step:1056/2330 train_time:41948ms step_avg:39.72ms
step:1057/2330 train_time:41970ms step_avg:39.71ms
step:1058/2330 train_time:42028ms step_avg:39.72ms
step:1059/2330 train_time:42050ms step_avg:39.71ms
step:1060/2330 train_time:42107ms step_avg:39.72ms
step:1061/2330 train_time:42130ms step_avg:39.71ms
step:1062/2330 train_time:42187ms step_avg:39.72ms
step:1063/2330 train_time:42209ms step_avg:39.71ms
step:1064/2330 train_time:42267ms step_avg:39.72ms
step:1065/2330 train_time:42289ms step_avg:39.71ms
step:1066/2330 train_time:42346ms step_avg:39.72ms
step:1067/2330 train_time:42369ms step_avg:39.71ms
step:1068/2330 train_time:42426ms step_avg:39.72ms
step:1069/2330 train_time:42448ms step_avg:39.71ms
step:1070/2330 train_time:42504ms step_avg:39.72ms
step:1071/2330 train_time:42526ms step_avg:39.71ms
step:1072/2330 train_time:42582ms step_avg:39.72ms
step:1073/2330 train_time:42605ms step_avg:39.71ms
step:1074/2330 train_time:42660ms step_avg:39.72ms
step:1075/2330 train_time:42684ms step_avg:39.71ms
step:1076/2330 train_time:42740ms step_avg:39.72ms
step:1077/2330 train_time:42763ms step_avg:39.71ms
step:1078/2330 train_time:42819ms step_avg:39.72ms
step:1079/2330 train_time:42842ms step_avg:39.71ms
step:1080/2330 train_time:42897ms step_avg:39.72ms
step:1081/2330 train_time:42920ms step_avg:39.70ms
step:1082/2330 train_time:42976ms step_avg:39.72ms
step:1083/2330 train_time:42999ms step_avg:39.70ms
step:1084/2330 train_time:43055ms step_avg:39.72ms
step:1085/2330 train_time:43078ms step_avg:39.70ms
step:1086/2330 train_time:43135ms step_avg:39.72ms
step:1087/2330 train_time:43159ms step_avg:39.70ms
step:1088/2330 train_time:43215ms step_avg:39.72ms
step:1089/2330 train_time:43238ms step_avg:39.70ms
step:1090/2330 train_time:43295ms step_avg:39.72ms
step:1091/2330 train_time:43318ms step_avg:39.71ms
step:1092/2330 train_time:43375ms step_avg:39.72ms
step:1093/2330 train_time:43398ms step_avg:39.71ms
step:1094/2330 train_time:43455ms step_avg:39.72ms
step:1095/2330 train_time:43478ms step_avg:39.71ms
step:1096/2330 train_time:43534ms step_avg:39.72ms
step:1097/2330 train_time:43556ms step_avg:39.71ms
step:1098/2330 train_time:43613ms step_avg:39.72ms
step:1099/2330 train_time:43636ms step_avg:39.70ms
step:1100/2330 train_time:43692ms step_avg:39.72ms
step:1101/2330 train_time:43715ms step_avg:39.70ms
step:1102/2330 train_time:43771ms step_avg:39.72ms
step:1103/2330 train_time:43794ms step_avg:39.70ms
step:1104/2330 train_time:43850ms step_avg:39.72ms
step:1105/2330 train_time:43872ms step_avg:39.70ms
step:1106/2330 train_time:43929ms step_avg:39.72ms
step:1107/2330 train_time:43951ms step_avg:39.70ms
step:1108/2330 train_time:44007ms step_avg:39.72ms
step:1109/2330 train_time:44030ms step_avg:39.70ms
step:1110/2330 train_time:44087ms step_avg:39.72ms
step:1111/2330 train_time:44109ms step_avg:39.70ms
step:1112/2330 train_time:44165ms step_avg:39.72ms
step:1113/2330 train_time:44187ms step_avg:39.70ms
step:1114/2330 train_time:44244ms step_avg:39.72ms
step:1115/2330 train_time:44266ms step_avg:39.70ms
step:1116/2330 train_time:44322ms step_avg:39.72ms
step:1117/2330 train_time:44345ms step_avg:39.70ms
step:1118/2330 train_time:44401ms step_avg:39.71ms
step:1119/2330 train_time:44423ms step_avg:39.70ms
step:1120/2330 train_time:44479ms step_avg:39.71ms
step:1121/2330 train_time:44502ms step_avg:39.70ms
step:1122/2330 train_time:44558ms step_avg:39.71ms
step:1123/2330 train_time:44581ms step_avg:39.70ms
step:1124/2330 train_time:44637ms step_avg:39.71ms
step:1125/2330 train_time:44660ms step_avg:39.70ms
step:1126/2330 train_time:44716ms step_avg:39.71ms
step:1127/2330 train_time:44739ms step_avg:39.70ms
step:1128/2330 train_time:44795ms step_avg:39.71ms
step:1129/2330 train_time:44817ms step_avg:39.70ms
step:1130/2330 train_time:44874ms step_avg:39.71ms
step:1131/2330 train_time:44896ms step_avg:39.70ms
step:1132/2330 train_time:44953ms step_avg:39.71ms
step:1133/2330 train_time:44976ms step_avg:39.70ms
step:1134/2330 train_time:45033ms step_avg:39.71ms
step:1135/2330 train_time:45056ms step_avg:39.70ms
step:1136/2330 train_time:45113ms step_avg:39.71ms
step:1137/2330 train_time:45136ms step_avg:39.70ms
step:1138/2330 train_time:45192ms step_avg:39.71ms
step:1139/2330 train_time:45214ms step_avg:39.70ms
step:1140/2330 train_time:45271ms step_avg:39.71ms
step:1141/2330 train_time:45293ms step_avg:39.70ms
step:1142/2330 train_time:45350ms step_avg:39.71ms
step:1143/2330 train_time:45373ms step_avg:39.70ms
step:1144/2330 train_time:45430ms step_avg:39.71ms
step:1145/2330 train_time:45452ms step_avg:39.70ms
step:1146/2330 train_time:45509ms step_avg:39.71ms
step:1147/2330 train_time:45531ms step_avg:39.70ms
step:1148/2330 train_time:45588ms step_avg:39.71ms
step:1149/2330 train_time:45611ms step_avg:39.70ms
step:1150/2330 train_time:45668ms step_avg:39.71ms
step:1151/2330 train_time:45690ms step_avg:39.70ms
step:1152/2330 train_time:45746ms step_avg:39.71ms
step:1153/2330 train_time:45769ms step_avg:39.70ms
step:1154/2330 train_time:45825ms step_avg:39.71ms
step:1155/2330 train_time:45847ms step_avg:39.69ms
step:1156/2330 train_time:45904ms step_avg:39.71ms
step:1157/2330 train_time:45925ms step_avg:39.69ms
step:1158/2330 train_time:45982ms step_avg:39.71ms
step:1159/2330 train_time:46004ms step_avg:39.69ms
step:1160/2330 train_time:46060ms step_avg:39.71ms
step:1161/2330 train_time:46083ms step_avg:39.69ms
step:1162/2330 train_time:46140ms step_avg:39.71ms
step:1163/2330 train_time:46163ms step_avg:39.69ms
step:1164/2330 train_time:46220ms step_avg:39.71ms
step:1165/2330 train_time:46243ms step_avg:39.69ms
step:1166/2330 train_time:46298ms step_avg:39.71ms
step:1167/2330 train_time:46322ms step_avg:39.69ms
step:1168/2330 train_time:46378ms step_avg:39.71ms
step:1169/2330 train_time:46401ms step_avg:39.69ms
step:1170/2330 train_time:46457ms step_avg:39.71ms
step:1171/2330 train_time:46480ms step_avg:39.69ms
step:1172/2330 train_time:46536ms step_avg:39.71ms
step:1173/2330 train_time:46559ms step_avg:39.69ms
step:1174/2330 train_time:46615ms step_avg:39.71ms
step:1175/2330 train_time:46638ms step_avg:39.69ms
step:1176/2330 train_time:46694ms step_avg:39.71ms
step:1177/2330 train_time:46717ms step_avg:39.69ms
step:1178/2330 train_time:46774ms step_avg:39.71ms
step:1179/2330 train_time:46797ms step_avg:39.69ms
step:1180/2330 train_time:46854ms step_avg:39.71ms
step:1181/2330 train_time:46877ms step_avg:39.69ms
step:1182/2330 train_time:46934ms step_avg:39.71ms
step:1183/2330 train_time:46957ms step_avg:39.69ms
step:1184/2330 train_time:47014ms step_avg:39.71ms
step:1185/2330 train_time:47037ms step_avg:39.69ms
step:1186/2330 train_time:47093ms step_avg:39.71ms
step:1187/2330 train_time:47116ms step_avg:39.69ms
step:1188/2330 train_time:47173ms step_avg:39.71ms
step:1189/2330 train_time:47196ms step_avg:39.69ms
step:1190/2330 train_time:47252ms step_avg:39.71ms
step:1191/2330 train_time:47274ms step_avg:39.69ms
step:1192/2330 train_time:47332ms step_avg:39.71ms
step:1193/2330 train_time:47355ms step_avg:39.69ms
step:1194/2330 train_time:47412ms step_avg:39.71ms
step:1195/2330 train_time:47435ms step_avg:39.69ms
step:1196/2330 train_time:47492ms step_avg:39.71ms
step:1197/2330 train_time:47515ms step_avg:39.69ms
step:1198/2330 train_time:47571ms step_avg:39.71ms
step:1199/2330 train_time:47593ms step_avg:39.69ms
step:1200/2330 train_time:47649ms step_avg:39.71ms
step:1201/2330 train_time:47671ms step_avg:39.69ms
step:1202/2330 train_time:47728ms step_avg:39.71ms
step:1203/2330 train_time:47750ms step_avg:39.69ms
step:1204/2330 train_time:47806ms step_avg:39.71ms
step:1205/2330 train_time:47828ms step_avg:39.69ms
step:1206/2330 train_time:47885ms step_avg:39.71ms
step:1207/2330 train_time:47907ms step_avg:39.69ms
step:1208/2330 train_time:47964ms step_avg:39.71ms
step:1209/2330 train_time:47986ms step_avg:39.69ms
step:1210/2330 train_time:48043ms step_avg:39.70ms
step:1211/2330 train_time:48065ms step_avg:39.69ms
step:1212/2330 train_time:48121ms step_avg:39.70ms
step:1213/2330 train_time:48144ms step_avg:39.69ms
step:1214/2330 train_time:48199ms step_avg:39.70ms
step:1215/2330 train_time:48222ms step_avg:39.69ms
step:1216/2330 train_time:48277ms step_avg:39.70ms
step:1217/2330 train_time:48301ms step_avg:39.69ms
step:1218/2330 train_time:48357ms step_avg:39.70ms
step:1219/2330 train_time:48380ms step_avg:39.69ms
step:1220/2330 train_time:48436ms step_avg:39.70ms
step:1221/2330 train_time:48459ms step_avg:39.69ms
step:1222/2330 train_time:48515ms step_avg:39.70ms
step:1223/2330 train_time:48538ms step_avg:39.69ms
step:1224/2330 train_time:48594ms step_avg:39.70ms
step:1225/2330 train_time:48618ms step_avg:39.69ms
step:1226/2330 train_time:48674ms step_avg:39.70ms
step:1227/2330 train_time:48697ms step_avg:39.69ms
step:1228/2330 train_time:48754ms step_avg:39.70ms
step:1229/2330 train_time:48776ms step_avg:39.69ms
step:1230/2330 train_time:48833ms step_avg:39.70ms
step:1231/2330 train_time:48855ms step_avg:39.69ms
step:1232/2330 train_time:48912ms step_avg:39.70ms
step:1233/2330 train_time:48935ms step_avg:39.69ms
step:1234/2330 train_time:48992ms step_avg:39.70ms
step:1235/2330 train_time:49013ms step_avg:39.69ms
step:1236/2330 train_time:49070ms step_avg:39.70ms
step:1237/2330 train_time:49093ms step_avg:39.69ms
step:1238/2330 train_time:49150ms step_avg:39.70ms
step:1239/2330 train_time:49172ms step_avg:39.69ms
step:1240/2330 train_time:49229ms step_avg:39.70ms
step:1241/2330 train_time:49252ms step_avg:39.69ms
step:1242/2330 train_time:49309ms step_avg:39.70ms
step:1243/2330 train_time:49331ms step_avg:39.69ms
step:1244/2330 train_time:49387ms step_avg:39.70ms
step:1245/2330 train_time:49410ms step_avg:39.69ms
step:1246/2330 train_time:49466ms step_avg:39.70ms
step:1247/2330 train_time:49489ms step_avg:39.69ms
step:1248/2330 train_time:49546ms step_avg:39.70ms
step:1249/2330 train_time:49568ms step_avg:39.69ms
step:1250/2330 train_time:49624ms step_avg:39.70ms
step:1250/2330 val_loss:5.2003 train_time:49719ms step_avg:39.78ms
step:1251/2330 train_time:49731ms step_avg:39.75ms
step:1252/2330 train_time:49743ms step_avg:39.73ms
step:1253/2330 train_time:49753ms step_avg:39.71ms
step:1254/2330 train_time:49782ms step_avg:39.70ms
step:1255/2330 train_time:49803ms step_avg:39.68ms
step:1256/2330 train_time:49859ms step_avg:39.70ms
step:1257/2330 train_time:49881ms step_avg:39.68ms
step:1258/2330 train_time:49937ms step_avg:39.70ms
step:1259/2330 train_time:49959ms step_avg:39.68ms
step:1260/2330 train_time:50015ms step_avg:39.69ms
step:1261/2330 train_time:50041ms step_avg:39.68ms
step:1262/2330 train_time:50101ms step_avg:39.70ms
step:1263/2330 train_time:50124ms step_avg:39.69ms
step:1264/2330 train_time:50182ms step_avg:39.70ms
step:1265/2330 train_time:50205ms step_avg:39.69ms
step:1266/2330 train_time:50262ms step_avg:39.70ms
step:1267/2330 train_time:50285ms step_avg:39.69ms
step:1268/2330 train_time:50342ms step_avg:39.70ms
step:1269/2330 train_time:50364ms step_avg:39.69ms
step:1270/2330 train_time:50420ms step_avg:39.70ms
step:1271/2330 train_time:50442ms step_avg:39.69ms
step:1272/2330 train_time:50498ms step_avg:39.70ms
step:1273/2330 train_time:50521ms step_avg:39.69ms
step:1274/2330 train_time:50577ms step_avg:39.70ms
step:1275/2330 train_time:50599ms step_avg:39.69ms
step:1276/2330 train_time:50657ms step_avg:39.70ms
step:1277/2330 train_time:50680ms step_avg:39.69ms
step:1278/2330 train_time:50737ms step_avg:39.70ms
step:1279/2330 train_time:50759ms step_avg:39.69ms
step:1280/2330 train_time:50815ms step_avg:39.70ms
step:1281/2330 train_time:50837ms step_avg:39.69ms
step:1282/2330 train_time:50893ms step_avg:39.70ms
step:1283/2330 train_time:50915ms step_avg:39.68ms
step:1284/2330 train_time:50971ms step_avg:39.70ms
step:1285/2330 train_time:50994ms step_avg:39.68ms
step:1286/2330 train_time:51053ms step_avg:39.70ms
step:1287/2330 train_time:51076ms step_avg:39.69ms
step:1288/2330 train_time:51134ms step_avg:39.70ms
step:1289/2330 train_time:51156ms step_avg:39.69ms
step:1290/2330 train_time:51213ms step_avg:39.70ms
step:1291/2330 train_time:51235ms step_avg:39.69ms
step:1292/2330 train_time:51292ms step_avg:39.70ms
step:1293/2330 train_time:51314ms step_avg:39.69ms
step:1294/2330 train_time:51370ms step_avg:39.70ms
step:1295/2330 train_time:51392ms step_avg:39.69ms
step:1296/2330 train_time:51448ms step_avg:39.70ms
step:1297/2330 train_time:51470ms step_avg:39.68ms
step:1298/2330 train_time:51526ms step_avg:39.70ms
step:1299/2330 train_time:51549ms step_avg:39.68ms
step:1300/2330 train_time:51605ms step_avg:39.70ms
step:1301/2330 train_time:51628ms step_avg:39.68ms
step:1302/2330 train_time:51684ms step_avg:39.70ms
step:1303/2330 train_time:51707ms step_avg:39.68ms
step:1304/2330 train_time:51764ms step_avg:39.70ms
step:1305/2330 train_time:51786ms step_avg:39.68ms
step:1306/2330 train_time:51842ms step_avg:39.70ms
step:1307/2330 train_time:51865ms step_avg:39.68ms
step:1308/2330 train_time:51921ms step_avg:39.70ms
step:1309/2330 train_time:51945ms step_avg:39.68ms
step:1310/2330 train_time:52002ms step_avg:39.70ms
step:1311/2330 train_time:52026ms step_avg:39.68ms
step:1312/2330 train_time:52083ms step_avg:39.70ms
step:1313/2330 train_time:52106ms step_avg:39.68ms
step:1314/2330 train_time:52163ms step_avg:39.70ms
step:1315/2330 train_time:52187ms step_avg:39.69ms
step:1316/2330 train_time:52243ms step_avg:39.70ms
step:1317/2330 train_time:52266ms step_avg:39.69ms
step:1318/2330 train_time:52323ms step_avg:39.70ms
step:1319/2330 train_time:52345ms step_avg:39.69ms
step:1320/2330 train_time:52402ms step_avg:39.70ms
step:1321/2330 train_time:52424ms step_avg:39.68ms
step:1322/2330 train_time:52480ms step_avg:39.70ms
step:1323/2330 train_time:52503ms step_avg:39.68ms
step:1324/2330 train_time:52560ms step_avg:39.70ms
step:1325/2330 train_time:52582ms step_avg:39.68ms
step:1326/2330 train_time:52638ms step_avg:39.70ms
step:1327/2330 train_time:52660ms step_avg:39.68ms
step:1328/2330 train_time:52717ms step_avg:39.70ms
step:1329/2330 train_time:52739ms step_avg:39.68ms
step:1330/2330 train_time:52796ms step_avg:39.70ms
step:1331/2330 train_time:52818ms step_avg:39.68ms
step:1332/2330 train_time:52875ms step_avg:39.70ms
step:1333/2330 train_time:52897ms step_avg:39.68ms
step:1334/2330 train_time:52954ms step_avg:39.70ms
step:1335/2330 train_time:52976ms step_avg:39.68ms
step:1336/2330 train_time:53032ms step_avg:39.69ms
step:1337/2330 train_time:53055ms step_avg:39.68ms
step:1338/2330 train_time:53112ms step_avg:39.69ms
step:1339/2330 train_time:53134ms step_avg:39.68ms
step:1340/2330 train_time:53192ms step_avg:39.70ms
step:1341/2330 train_time:53214ms step_avg:39.68ms
step:1342/2330 train_time:53270ms step_avg:39.69ms
step:1343/2330 train_time:53292ms step_avg:39.68ms
step:1344/2330 train_time:53348ms step_avg:39.69ms
step:1345/2330 train_time:53371ms step_avg:39.68ms
step:1346/2330 train_time:53427ms step_avg:39.69ms
step:1347/2330 train_time:53449ms step_avg:39.68ms
step:1348/2330 train_time:53505ms step_avg:39.69ms
step:1349/2330 train_time:53528ms step_avg:39.68ms
step:1350/2330 train_time:53584ms step_avg:39.69ms
step:1351/2330 train_time:53607ms step_avg:39.68ms
step:1352/2330 train_time:53663ms step_avg:39.69ms
step:1353/2330 train_time:53686ms step_avg:39.68ms
step:1354/2330 train_time:53742ms step_avg:39.69ms
step:1355/2330 train_time:53765ms step_avg:39.68ms
step:1356/2330 train_time:53821ms step_avg:39.69ms
step:1357/2330 train_time:53844ms step_avg:39.68ms
step:1358/2330 train_time:53901ms step_avg:39.69ms
step:1359/2330 train_time:53923ms step_avg:39.68ms
step:1360/2330 train_time:53981ms step_avg:39.69ms
step:1361/2330 train_time:54004ms step_avg:39.68ms
step:1362/2330 train_time:54062ms step_avg:39.69ms
step:1363/2330 train_time:54085ms step_avg:39.68ms
step:1364/2330 train_time:54142ms step_avg:39.69ms
step:1365/2330 train_time:54165ms step_avg:39.68ms
step:1366/2330 train_time:54221ms step_avg:39.69ms
step:1367/2330 train_time:54244ms step_avg:39.68ms
step:1368/2330 train_time:54302ms step_avg:39.69ms
step:1369/2330 train_time:54324ms step_avg:39.68ms
step:1370/2330 train_time:54382ms step_avg:39.69ms
step:1371/2330 train_time:54404ms step_avg:39.68ms
step:1372/2330 train_time:54461ms step_avg:39.69ms
step:1373/2330 train_time:54484ms step_avg:39.68ms
step:1374/2330 train_time:54540ms step_avg:39.69ms
step:1375/2330 train_time:54563ms step_avg:39.68ms
step:1376/2330 train_time:54620ms step_avg:39.69ms
step:1377/2330 train_time:54642ms step_avg:39.68ms
step:1378/2330 train_time:54698ms step_avg:39.69ms
step:1379/2330 train_time:54720ms step_avg:39.68ms
step:1380/2330 train_time:54777ms step_avg:39.69ms
step:1381/2330 train_time:54799ms step_avg:39.68ms
step:1382/2330 train_time:54856ms step_avg:39.69ms
step:1383/2330 train_time:54878ms step_avg:39.68ms
step:1384/2330 train_time:54935ms step_avg:39.69ms
step:1385/2330 train_time:54958ms step_avg:39.68ms
step:1386/2330 train_time:55014ms step_avg:39.69ms
step:1387/2330 train_time:55037ms step_avg:39.68ms
step:1388/2330 train_time:55094ms step_avg:39.69ms
step:1389/2330 train_time:55116ms step_avg:39.68ms
step:1390/2330 train_time:55172ms step_avg:39.69ms
step:1391/2330 train_time:55195ms step_avg:39.68ms
step:1392/2330 train_time:55252ms step_avg:39.69ms
step:1393/2330 train_time:55274ms step_avg:39.68ms
step:1394/2330 train_time:55331ms step_avg:39.69ms
step:1395/2330 train_time:55353ms step_avg:39.68ms
step:1396/2330 train_time:55409ms step_avg:39.69ms
step:1397/2330 train_time:55433ms step_avg:39.68ms
step:1398/2330 train_time:55489ms step_avg:39.69ms
step:1399/2330 train_time:55512ms step_avg:39.68ms
step:1400/2330 train_time:55568ms step_avg:39.69ms
step:1401/2330 train_time:55591ms step_avg:39.68ms
step:1402/2330 train_time:55646ms step_avg:39.69ms
step:1403/2330 train_time:55670ms step_avg:39.68ms
step:1404/2330 train_time:55726ms step_avg:39.69ms
step:1405/2330 train_time:55749ms step_avg:39.68ms
step:1406/2330 train_time:55805ms step_avg:39.69ms
step:1407/2330 train_time:55828ms step_avg:39.68ms
step:1408/2330 train_time:55885ms step_avg:39.69ms
step:1409/2330 train_time:55908ms step_avg:39.68ms
step:1410/2330 train_time:55964ms step_avg:39.69ms
step:1411/2330 train_time:55987ms step_avg:39.68ms
step:1412/2330 train_time:56044ms step_avg:39.69ms
step:1413/2330 train_time:56067ms step_avg:39.68ms
step:1414/2330 train_time:56124ms step_avg:39.69ms
step:1415/2330 train_time:56147ms step_avg:39.68ms
step:1416/2330 train_time:56204ms step_avg:39.69ms
step:1417/2330 train_time:56226ms step_avg:39.68ms
step:1418/2330 train_time:56282ms step_avg:39.69ms
step:1419/2330 train_time:56305ms step_avg:39.68ms
step:1420/2330 train_time:56362ms step_avg:39.69ms
step:1421/2330 train_time:56385ms step_avg:39.68ms
step:1422/2330 train_time:56441ms step_avg:39.69ms
step:1423/2330 train_time:56464ms step_avg:39.68ms
step:1424/2330 train_time:56521ms step_avg:39.69ms
step:1425/2330 train_time:56544ms step_avg:39.68ms
step:1426/2330 train_time:56601ms step_avg:39.69ms
step:1427/2330 train_time:56623ms step_avg:39.68ms
step:1428/2330 train_time:56680ms step_avg:39.69ms
step:1429/2330 train_time:56702ms step_avg:39.68ms
step:1430/2330 train_time:56758ms step_avg:39.69ms
step:1431/2330 train_time:56781ms step_avg:39.68ms
step:1432/2330 train_time:56838ms step_avg:39.69ms
step:1433/2330 train_time:56860ms step_avg:39.68ms
step:1434/2330 train_time:56917ms step_avg:39.69ms
step:1435/2330 train_time:56940ms step_avg:39.68ms
step:1436/2330 train_time:56996ms step_avg:39.69ms
step:1437/2330 train_time:57018ms step_avg:39.68ms
step:1438/2330 train_time:57075ms step_avg:39.69ms
step:1439/2330 train_time:57097ms step_avg:39.68ms
step:1440/2330 train_time:57154ms step_avg:39.69ms
step:1441/2330 train_time:57176ms step_avg:39.68ms
step:1442/2330 train_time:57232ms step_avg:39.69ms
step:1443/2330 train_time:57255ms step_avg:39.68ms
step:1444/2330 train_time:57311ms step_avg:39.69ms
step:1445/2330 train_time:57334ms step_avg:39.68ms
step:1446/2330 train_time:57390ms step_avg:39.69ms
step:1447/2330 train_time:57412ms step_avg:39.68ms
step:1448/2330 train_time:57468ms step_avg:39.69ms
step:1449/2330 train_time:57492ms step_avg:39.68ms
step:1450/2330 train_time:57548ms step_avg:39.69ms
step:1451/2330 train_time:57572ms step_avg:39.68ms
step:1452/2330 train_time:57628ms step_avg:39.69ms
step:1453/2330 train_time:57652ms step_avg:39.68ms
step:1454/2330 train_time:57708ms step_avg:39.69ms
step:1455/2330 train_time:57731ms step_avg:39.68ms
step:1456/2330 train_time:57787ms step_avg:39.69ms
step:1457/2330 train_time:57810ms step_avg:39.68ms
step:1458/2330 train_time:57866ms step_avg:39.69ms
step:1459/2330 train_time:57890ms step_avg:39.68ms
step:1460/2330 train_time:57946ms step_avg:39.69ms
step:1461/2330 train_time:57969ms step_avg:39.68ms
step:1462/2330 train_time:58025ms step_avg:39.69ms
step:1463/2330 train_time:58048ms step_avg:39.68ms
step:1464/2330 train_time:58104ms step_avg:39.69ms
step:1465/2330 train_time:58128ms step_avg:39.68ms
step:1466/2330 train_time:58184ms step_avg:39.69ms
step:1467/2330 train_time:58206ms step_avg:39.68ms
step:1468/2330 train_time:58263ms step_avg:39.69ms
step:1469/2330 train_time:58286ms step_avg:39.68ms
step:1470/2330 train_time:58342ms step_avg:39.69ms
step:1471/2330 train_time:58365ms step_avg:39.68ms
step:1472/2330 train_time:58422ms step_avg:39.69ms
step:1473/2330 train_time:58445ms step_avg:39.68ms
step:1474/2330 train_time:58502ms step_avg:39.69ms
step:1475/2330 train_time:58526ms step_avg:39.68ms
step:1476/2330 train_time:58583ms step_avg:39.69ms
step:1477/2330 train_time:58605ms step_avg:39.68ms
step:1478/2330 train_time:58662ms step_avg:39.69ms
step:1479/2330 train_time:58685ms step_avg:39.68ms
step:1480/2330 train_time:58742ms step_avg:39.69ms
step:1481/2330 train_time:58765ms step_avg:39.68ms
step:1482/2330 train_time:58821ms step_avg:39.69ms
step:1483/2330 train_time:58844ms step_avg:39.68ms
step:1484/2330 train_time:58901ms step_avg:39.69ms
step:1485/2330 train_time:58924ms step_avg:39.68ms
step:1486/2330 train_time:58981ms step_avg:39.69ms
step:1487/2330 train_time:59003ms step_avg:39.68ms
step:1488/2330 train_time:59059ms step_avg:39.69ms
step:1489/2330 train_time:59082ms step_avg:39.68ms
step:1490/2330 train_time:59138ms step_avg:39.69ms
step:1491/2330 train_time:59161ms step_avg:39.68ms
step:1492/2330 train_time:59218ms step_avg:39.69ms
step:1493/2330 train_time:59240ms step_avg:39.68ms
step:1494/2330 train_time:59297ms step_avg:39.69ms
step:1495/2330 train_time:59319ms step_avg:39.68ms
step:1496/2330 train_time:59375ms step_avg:39.69ms
step:1497/2330 train_time:59397ms step_avg:39.68ms
step:1498/2330 train_time:59455ms step_avg:39.69ms
step:1499/2330 train_time:59477ms step_avg:39.68ms
step:1500/2330 train_time:59534ms step_avg:39.69ms
step:1500/2330 val_loss:5.1732 train_time:59630ms step_avg:39.75ms
step:1501/2330 train_time:59641ms step_avg:39.73ms
step:1502/2330 train_time:59652ms step_avg:39.72ms
step:1503/2330 train_time:59663ms step_avg:39.70ms
step:1504/2330 train_time:59692ms step_avg:39.69ms
step:1505/2330 train_time:59714ms step_avg:39.68ms
step:1506/2330 train_time:59769ms step_avg:39.69ms
step:1507/2330 train_time:59791ms step_avg:39.68ms
step:1508/2330 train_time:59846ms step_avg:39.69ms
step:1509/2330 train_time:59868ms step_avg:39.67ms
step:1510/2330 train_time:59925ms step_avg:39.69ms
step:1511/2330 train_time:59952ms step_avg:39.68ms
step:1512/2330 train_time:60012ms step_avg:39.69ms
step:1513/2330 train_time:60038ms step_avg:39.68ms
step:1514/2330 train_time:60096ms step_avg:39.69ms
step:1515/2330 train_time:60119ms step_avg:39.68ms
step:1516/2330 train_time:60176ms step_avg:39.69ms
step:1517/2330 train_time:60198ms step_avg:39.68ms
step:1518/2330 train_time:60254ms step_avg:39.69ms
step:1519/2330 train_time:60276ms step_avg:39.68ms
step:1520/2330 train_time:60331ms step_avg:39.69ms
step:1521/2330 train_time:60353ms step_avg:39.68ms
step:1522/2330 train_time:60409ms step_avg:39.69ms
step:1523/2330 train_time:60431ms step_avg:39.68ms
step:1524/2330 train_time:60487ms step_avg:39.69ms
step:1525/2330 train_time:60509ms step_avg:39.68ms
step:1526/2330 train_time:60566ms step_avg:39.69ms
step:1527/2330 train_time:60590ms step_avg:39.68ms
step:1528/2330 train_time:60647ms step_avg:39.69ms
step:1529/2330 train_time:60670ms step_avg:39.68ms
step:1530/2330 train_time:60726ms step_avg:39.69ms
step:1531/2330 train_time:60748ms step_avg:39.68ms
step:1532/2330 train_time:60803ms step_avg:39.69ms
step:1533/2330 train_time:60826ms step_avg:39.68ms
step:1534/2330 train_time:60883ms step_avg:39.69ms
step:1535/2330 train_time:60906ms step_avg:39.68ms
step:1536/2330 train_time:60965ms step_avg:39.69ms
step:1537/2330 train_time:60989ms step_avg:39.68ms
step:1538/2330 train_time:61046ms step_avg:39.69ms
step:1539/2330 train_time:61071ms step_avg:39.68ms
step:1540/2330 train_time:61127ms step_avg:39.69ms
step:1541/2330 train_time:61151ms step_avg:39.68ms
step:1542/2330 train_time:61207ms step_avg:39.69ms
step:1543/2330 train_time:61230ms step_avg:39.68ms
step:1544/2330 train_time:61287ms step_avg:39.69ms
step:1545/2330 train_time:61309ms step_avg:39.68ms
step:1546/2330 train_time:61364ms step_avg:39.69ms
step:1547/2330 train_time:61386ms step_avg:39.68ms
step:1548/2330 train_time:61441ms step_avg:39.69ms
step:1549/2330 train_time:61463ms step_avg:39.68ms
step:1550/2330 train_time:61520ms step_avg:39.69ms
step:1551/2330 train_time:61542ms step_avg:39.68ms
step:1552/2330 train_time:61598ms step_avg:39.69ms
step:1553/2330 train_time:61620ms step_avg:39.68ms
step:1554/2330 train_time:61676ms step_avg:39.69ms
step:1555/2330 train_time:61698ms step_avg:39.68ms
step:1556/2330 train_time:61755ms step_avg:39.69ms
step:1557/2330 train_time:61776ms step_avg:39.68ms
step:1558/2330 train_time:61833ms step_avg:39.69ms
step:1559/2330 train_time:61855ms step_avg:39.68ms
step:1560/2330 train_time:61912ms step_avg:39.69ms
step:1561/2330 train_time:61935ms step_avg:39.68ms
step:1562/2330 train_time:61992ms step_avg:39.69ms
step:1563/2330 train_time:62015ms step_avg:39.68ms
step:1564/2330 train_time:62072ms step_avg:39.69ms
step:1565/2330 train_time:62096ms step_avg:39.68ms
step:1566/2330 train_time:62152ms step_avg:39.69ms
step:1567/2330 train_time:62175ms step_avg:39.68ms
step:1568/2330 train_time:62232ms step_avg:39.69ms
step:1569/2330 train_time:62255ms step_avg:39.68ms
step:1570/2330 train_time:62311ms step_avg:39.69ms
step:1571/2330 train_time:62335ms step_avg:39.68ms
step:1572/2330 train_time:62391ms step_avg:39.69ms
step:1573/2330 train_time:62414ms step_avg:39.68ms
step:1574/2330 train_time:62470ms step_avg:39.69ms
step:1575/2330 train_time:62492ms step_avg:39.68ms
step:1576/2330 train_time:62548ms step_avg:39.69ms
step:1577/2330 train_time:62571ms step_avg:39.68ms
step:1578/2330 train_time:62626ms step_avg:39.69ms
step:1579/2330 train_time:62649ms step_avg:39.68ms
step:1580/2330 train_time:62705ms step_avg:39.69ms
step:1581/2330 train_time:62727ms step_avg:39.68ms
step:1582/2330 train_time:62783ms step_avg:39.69ms
step:1583/2330 train_time:62806ms step_avg:39.68ms
step:1584/2330 train_time:62863ms step_avg:39.69ms
step:1585/2330 train_time:62885ms step_avg:39.68ms
step:1586/2330 train_time:62942ms step_avg:39.69ms
step:1587/2330 train_time:62965ms step_avg:39.68ms
step:1588/2330 train_time:63022ms step_avg:39.69ms
step:1589/2330 train_time:63044ms step_avg:39.68ms
step:1590/2330 train_time:63101ms step_avg:39.69ms
step:1591/2330 train_time:63124ms step_avg:39.68ms
step:1592/2330 train_time:63181ms step_avg:39.69ms
step:1593/2330 train_time:63203ms step_avg:39.68ms
step:1594/2330 train_time:63260ms step_avg:39.69ms
step:1595/2330 train_time:63283ms step_avg:39.68ms
step:1596/2330 train_time:63339ms step_avg:39.69ms
step:1597/2330 train_time:63362ms step_avg:39.68ms
step:1598/2330 train_time:63419ms step_avg:39.69ms
step:1599/2330 train_time:63442ms step_avg:39.68ms
step:1600/2330 train_time:63499ms step_avg:39.69ms
step:1601/2330 train_time:63521ms step_avg:39.68ms
step:1602/2330 train_time:63577ms step_avg:39.69ms
step:1603/2330 train_time:63599ms step_avg:39.68ms
step:1604/2330 train_time:63655ms step_avg:39.69ms
step:1605/2330 train_time:63677ms step_avg:39.67ms
step:1606/2330 train_time:63733ms step_avg:39.68ms
step:1607/2330 train_time:63756ms step_avg:39.67ms
step:1608/2330 train_time:63812ms step_avg:39.68ms
step:1609/2330 train_time:63835ms step_avg:39.67ms
step:1610/2330 train_time:63891ms step_avg:39.68ms
step:1611/2330 train_time:63915ms step_avg:39.67ms
step:1612/2330 train_time:63971ms step_avg:39.68ms
step:1613/2330 train_time:63994ms step_avg:39.67ms
step:1614/2330 train_time:64050ms step_avg:39.68ms
step:1615/2330 train_time:64074ms step_avg:39.67ms
step:1616/2330 train_time:64131ms step_avg:39.68ms
step:1617/2330 train_time:64154ms step_avg:39.67ms
step:1618/2330 train_time:64210ms step_avg:39.68ms
step:1619/2330 train_time:64233ms step_avg:39.67ms
step:1620/2330 train_time:64290ms step_avg:39.68ms
step:1621/2330 train_time:64313ms step_avg:39.67ms
step:1622/2330 train_time:64369ms step_avg:39.68ms
step:1623/2330 train_time:64392ms step_avg:39.67ms
step:1624/2330 train_time:64448ms step_avg:39.68ms
step:1625/2330 train_time:64470ms step_avg:39.67ms
step:1626/2330 train_time:64526ms step_avg:39.68ms
step:1627/2330 train_time:64549ms step_avg:39.67ms
step:1628/2330 train_time:64605ms step_avg:39.68ms
step:1629/2330 train_time:64627ms step_avg:39.67ms
step:1630/2330 train_time:64683ms step_avg:39.68ms
step:1631/2330 train_time:64706ms step_avg:39.67ms
step:1632/2330 train_time:64763ms step_avg:39.68ms
step:1633/2330 train_time:64785ms step_avg:39.67ms
step:1634/2330 train_time:64842ms step_avg:39.68ms
step:1635/2330 train_time:64865ms step_avg:39.67ms
step:1636/2330 train_time:64922ms step_avg:39.68ms
step:1637/2330 train_time:64944ms step_avg:39.67ms
step:1638/2330 train_time:65000ms step_avg:39.68ms
step:1639/2330 train_time:65023ms step_avg:39.67ms
step:1640/2330 train_time:65080ms step_avg:39.68ms
step:1641/2330 train_time:65102ms step_avg:39.67ms
step:1642/2330 train_time:65160ms step_avg:39.68ms
step:1643/2330 train_time:65182ms step_avg:39.67ms
step:1644/2330 train_time:65239ms step_avg:39.68ms
step:1645/2330 train_time:65262ms step_avg:39.67ms
step:1646/2330 train_time:65319ms step_avg:39.68ms
step:1647/2330 train_time:65342ms step_avg:39.67ms
step:1648/2330 train_time:65399ms step_avg:39.68ms
step:1649/2330 train_time:65421ms step_avg:39.67ms
step:1650/2330 train_time:65478ms step_avg:39.68ms
step:1651/2330 train_time:65500ms step_avg:39.67ms
step:1652/2330 train_time:65556ms step_avg:39.68ms
step:1653/2330 train_time:65579ms step_avg:39.67ms
step:1654/2330 train_time:65635ms step_avg:39.68ms
step:1655/2330 train_time:65657ms step_avg:39.67ms
step:1656/2330 train_time:65713ms step_avg:39.68ms
step:1657/2330 train_time:65736ms step_avg:39.67ms
step:1658/2330 train_time:65792ms step_avg:39.68ms
step:1659/2330 train_time:65815ms step_avg:39.67ms
step:1660/2330 train_time:65870ms step_avg:39.68ms
step:1661/2330 train_time:65894ms step_avg:39.67ms
step:1662/2330 train_time:65950ms step_avg:39.68ms
step:1663/2330 train_time:65974ms step_avg:39.67ms
step:1664/2330 train_time:66029ms step_avg:39.68ms
step:1665/2330 train_time:66052ms step_avg:39.67ms
step:1666/2330 train_time:66109ms step_avg:39.68ms
step:1667/2330 train_time:66132ms step_avg:39.67ms
step:1668/2330 train_time:66188ms step_avg:39.68ms
step:1669/2330 train_time:66211ms step_avg:39.67ms
step:1670/2330 train_time:66267ms step_avg:39.68ms
step:1671/2330 train_time:66291ms step_avg:39.67ms
step:1672/2330 train_time:66347ms step_avg:39.68ms
step:1673/2330 train_time:66371ms step_avg:39.67ms
step:1674/2330 train_time:66427ms step_avg:39.68ms
step:1675/2330 train_time:66449ms step_avg:39.67ms
step:1676/2330 train_time:66506ms step_avg:39.68ms
step:1677/2330 train_time:66529ms step_avg:39.67ms
step:1678/2330 train_time:66585ms step_avg:39.68ms
step:1679/2330 train_time:66607ms step_avg:39.67ms
step:1680/2330 train_time:66663ms step_avg:39.68ms
step:1681/2330 train_time:66686ms step_avg:39.67ms
step:1682/2330 train_time:66742ms step_avg:39.68ms
step:1683/2330 train_time:66765ms step_avg:39.67ms
step:1684/2330 train_time:66822ms step_avg:39.68ms
step:1685/2330 train_time:66845ms step_avg:39.67ms
step:1686/2330 train_time:66901ms step_avg:39.68ms
step:1687/2330 train_time:66924ms step_avg:39.67ms
step:1688/2330 train_time:66981ms step_avg:39.68ms
step:1689/2330 train_time:67004ms step_avg:39.67ms
step:1690/2330 train_time:67061ms step_avg:39.68ms
step:1691/2330 train_time:67084ms step_avg:39.67ms
step:1692/2330 train_time:67140ms step_avg:39.68ms
step:1693/2330 train_time:67162ms step_avg:39.67ms
step:1694/2330 train_time:67220ms step_avg:39.68ms
step:1695/2330 train_time:67242ms step_avg:39.67ms
step:1696/2330 train_time:67299ms step_avg:39.68ms
step:1697/2330 train_time:67322ms step_avg:39.67ms
step:1698/2330 train_time:67379ms step_avg:39.68ms
step:1699/2330 train_time:67401ms step_avg:39.67ms
step:1700/2330 train_time:67457ms step_avg:39.68ms
step:1701/2330 train_time:67479ms step_avg:39.67ms
step:1702/2330 train_time:67536ms step_avg:39.68ms
step:1703/2330 train_time:67558ms step_avg:39.67ms
step:1704/2330 train_time:67615ms step_avg:39.68ms
step:1705/2330 train_time:67637ms step_avg:39.67ms
step:1706/2330 train_time:67693ms step_avg:39.68ms
step:1707/2330 train_time:67716ms step_avg:39.67ms
step:1708/2330 train_time:67772ms step_avg:39.68ms
step:1709/2330 train_time:67796ms step_avg:39.67ms
step:1710/2330 train_time:67852ms step_avg:39.68ms
step:1711/2330 train_time:67875ms step_avg:39.67ms
step:1712/2330 train_time:67931ms step_avg:39.68ms
step:1713/2330 train_time:67954ms step_avg:39.67ms
step:1714/2330 train_time:68011ms step_avg:39.68ms
step:1715/2330 train_time:68035ms step_avg:39.67ms
step:1716/2330 train_time:68091ms step_avg:39.68ms
step:1717/2330 train_time:68114ms step_avg:39.67ms
step:1718/2330 train_time:68170ms step_avg:39.68ms
step:1719/2330 train_time:68193ms step_avg:39.67ms
step:1720/2330 train_time:68249ms step_avg:39.68ms
step:1721/2330 train_time:68272ms step_avg:39.67ms
step:1722/2330 train_time:68328ms step_avg:39.68ms
step:1723/2330 train_time:68351ms step_avg:39.67ms
step:1724/2330 train_time:68407ms step_avg:39.68ms
step:1725/2330 train_time:68429ms step_avg:39.67ms
step:1726/2330 train_time:68485ms step_avg:39.68ms
step:1727/2330 train_time:68508ms step_avg:39.67ms
step:1728/2330 train_time:68564ms step_avg:39.68ms
step:1729/2330 train_time:68587ms step_avg:39.67ms
step:1730/2330 train_time:68644ms step_avg:39.68ms
step:1731/2330 train_time:68666ms step_avg:39.67ms
step:1732/2330 train_time:68723ms step_avg:39.68ms
step:1733/2330 train_time:68746ms step_avg:39.67ms
step:1734/2330 train_time:68802ms step_avg:39.68ms
step:1735/2330 train_time:68824ms step_avg:39.67ms
step:1736/2330 train_time:68880ms step_avg:39.68ms
step:1737/2330 train_time:68903ms step_avg:39.67ms
step:1738/2330 train_time:68960ms step_avg:39.68ms
step:1739/2330 train_time:68982ms step_avg:39.67ms
step:1740/2330 train_time:69039ms step_avg:39.68ms
step:1741/2330 train_time:69062ms step_avg:39.67ms
step:1742/2330 train_time:69118ms step_avg:39.68ms
step:1743/2330 train_time:69141ms step_avg:39.67ms
step:1744/2330 train_time:69197ms step_avg:39.68ms
step:1745/2330 train_time:69219ms step_avg:39.67ms
step:1746/2330 train_time:69277ms step_avg:39.68ms
step:1747/2330 train_time:69299ms step_avg:39.67ms
step:1748/2330 train_time:69355ms step_avg:39.68ms
step:1749/2330 train_time:69377ms step_avg:39.67ms
step:1750/2330 train_time:69434ms step_avg:39.68ms
step:1750/2330 val_loss:5.1571 train_time:69530ms step_avg:39.73ms
step:1751/2330 train_time:69542ms step_avg:39.72ms
step:1752/2330 train_time:69553ms step_avg:39.70ms
step:1753/2330 train_time:69562ms step_avg:39.68ms
step:1754/2330 train_time:69592ms step_avg:39.68ms
step:1755/2330 train_time:69614ms step_avg:39.67ms
step:1756/2330 train_time:69669ms step_avg:39.67ms
step:1757/2330 train_time:69691ms step_avg:39.66ms
step:1758/2330 train_time:69746ms step_avg:39.67ms
step:1759/2330 train_time:69767ms step_avg:39.66ms
step:1760/2330 train_time:69823ms step_avg:39.67ms
step:1761/2330 train_time:69849ms step_avg:39.66ms
step:1762/2330 train_time:69909ms step_avg:39.68ms
step:1763/2330 train_time:69933ms step_avg:39.67ms
step:1764/2330 train_time:69991ms step_avg:39.68ms
step:1765/2330 train_time:70014ms step_avg:39.67ms
step:1766/2330 train_time:70071ms step_avg:39.68ms
step:1767/2330 train_time:70093ms step_avg:39.67ms
step:1768/2330 train_time:70151ms step_avg:39.68ms
step:1769/2330 train_time:70173ms step_avg:39.67ms
step:1770/2330 train_time:70229ms step_avg:39.68ms
step:1771/2330 train_time:70251ms step_avg:39.67ms
step:1772/2330 train_time:70307ms step_avg:39.68ms
step:1773/2330 train_time:70329ms step_avg:39.67ms
step:1774/2330 train_time:70385ms step_avg:39.68ms
step:1775/2330 train_time:70407ms step_avg:39.67ms
step:1776/2330 train_time:70464ms step_avg:39.68ms
step:1777/2330 train_time:70489ms step_avg:39.67ms
step:1778/2330 train_time:70545ms step_avg:39.68ms
step:1779/2330 train_time:70569ms step_avg:39.67ms
step:1780/2330 train_time:70626ms step_avg:39.68ms
step:1781/2330 train_time:70647ms step_avg:39.67ms
step:1782/2330 train_time:70703ms step_avg:39.68ms
step:1783/2330 train_time:70724ms step_avg:39.67ms
step:1784/2330 train_time:70780ms step_avg:39.68ms
step:1785/2330 train_time:70804ms step_avg:39.67ms
step:1786/2330 train_time:70861ms step_avg:39.68ms
step:1787/2330 train_time:70885ms step_avg:39.67ms
step:1788/2330 train_time:70942ms step_avg:39.68ms
step:1789/2330 train_time:70966ms step_avg:39.67ms
step:1790/2330 train_time:71023ms step_avg:39.68ms
step:1791/2330 train_time:71046ms step_avg:39.67ms
step:1792/2330 train_time:71102ms step_avg:39.68ms
step:1793/2330 train_time:71125ms step_avg:39.67ms
step:1794/2330 train_time:71181ms step_avg:39.68ms
step:1795/2330 train_time:71204ms step_avg:39.67ms
step:1796/2330 train_time:71260ms step_avg:39.68ms
step:1797/2330 train_time:71282ms step_avg:39.67ms
step:1798/2330 train_time:71338ms step_avg:39.68ms
step:1799/2330 train_time:71361ms step_avg:39.67ms
step:1800/2330 train_time:71417ms step_avg:39.68ms
step:1801/2330 train_time:71440ms step_avg:39.67ms
step:1802/2330 train_time:71496ms step_avg:39.68ms
step:1803/2330 train_time:71519ms step_avg:39.67ms
step:1804/2330 train_time:71575ms step_avg:39.68ms
step:1805/2330 train_time:71597ms step_avg:39.67ms
step:1806/2330 train_time:71654ms step_avg:39.68ms
step:1807/2330 train_time:71676ms step_avg:39.67ms
step:1808/2330 train_time:71733ms step_avg:39.68ms
step:1809/2330 train_time:71755ms step_avg:39.67ms
step:1810/2330 train_time:71812ms step_avg:39.68ms
step:1811/2330 train_time:71834ms step_avg:39.67ms
step:1812/2330 train_time:71891ms step_avg:39.67ms
step:1813/2330 train_time:71913ms step_avg:39.67ms
step:1814/2330 train_time:71971ms step_avg:39.68ms
step:1815/2330 train_time:71993ms step_avg:39.67ms
step:1816/2330 train_time:72049ms step_avg:39.67ms
step:1817/2330 train_time:72072ms step_avg:39.67ms
step:1818/2330 train_time:72129ms step_avg:39.67ms
step:1819/2330 train_time:72151ms step_avg:39.67ms
step:1820/2330 train_time:72208ms step_avg:39.67ms
step:1821/2330 train_time:72230ms step_avg:39.66ms
step:1822/2330 train_time:72286ms step_avg:39.67ms
step:1823/2330 train_time:72308ms step_avg:39.66ms
step:1824/2330 train_time:72365ms step_avg:39.67ms
step:1825/2330 train_time:72387ms step_avg:39.66ms
step:1826/2330 train_time:72443ms step_avg:39.67ms
step:1827/2330 train_time:72466ms step_avg:39.66ms
step:1828/2330 train_time:72521ms step_avg:39.67ms
step:1829/2330 train_time:72545ms step_avg:39.66ms
step:1830/2330 train_time:72601ms step_avg:39.67ms
step:1831/2330 train_time:72623ms step_avg:39.66ms
step:1832/2330 train_time:72680ms step_avg:39.67ms
step:1833/2330 train_time:72703ms step_avg:39.66ms
step:1834/2330 train_time:72759ms step_avg:39.67ms
step:1835/2330 train_time:72783ms step_avg:39.66ms
step:1836/2330 train_time:72839ms step_avg:39.67ms
step:1837/2330 train_time:72862ms step_avg:39.66ms
step:1838/2330 train_time:72918ms step_avg:39.67ms
step:1839/2330 train_time:72941ms step_avg:39.66ms
step:1840/2330 train_time:72997ms step_avg:39.67ms
step:1841/2330 train_time:73020ms step_avg:39.66ms
step:1842/2330 train_time:73076ms step_avg:39.67ms
step:1843/2330 train_time:73099ms step_avg:39.66ms
step:1844/2330 train_time:73155ms step_avg:39.67ms
step:1845/2330 train_time:73179ms step_avg:39.66ms
step:1846/2330 train_time:73236ms step_avg:39.67ms
step:1847/2330 train_time:73259ms step_avg:39.66ms
step:1848/2330 train_time:73316ms step_avg:39.67ms
step:1849/2330 train_time:73339ms step_avg:39.66ms
step:1850/2330 train_time:73395ms step_avg:39.67ms
step:1851/2330 train_time:73418ms step_avg:39.66ms
step:1852/2330 train_time:73475ms step_avg:39.67ms
step:1853/2330 train_time:73498ms step_avg:39.66ms
step:1854/2330 train_time:73554ms step_avg:39.67ms
step:1855/2330 train_time:73577ms step_avg:39.66ms
step:1856/2330 train_time:73634ms step_avg:39.67ms
step:1857/2330 train_time:73656ms step_avg:39.66ms
step:1858/2330 train_time:73713ms step_avg:39.67ms
step:1859/2330 train_time:73735ms step_avg:39.66ms
step:1860/2330 train_time:73792ms step_avg:39.67ms
step:1861/2330 train_time:73814ms step_avg:39.66ms
step:1862/2330 train_time:73871ms step_avg:39.67ms
step:1863/2330 train_time:73893ms step_avg:39.66ms
step:1864/2330 train_time:73950ms step_avg:39.67ms
step:1865/2330 train_time:73972ms step_avg:39.66ms
step:1866/2330 train_time:74029ms step_avg:39.67ms
step:1867/2330 train_time:74051ms step_avg:39.66ms
step:1868/2330 train_time:74107ms step_avg:39.67ms
step:1869/2330 train_time:74130ms step_avg:39.66ms
step:1870/2330 train_time:74186ms step_avg:39.67ms
step:1871/2330 train_time:74208ms step_avg:39.66ms
step:1872/2330 train_time:74265ms step_avg:39.67ms
step:1873/2330 train_time:74287ms step_avg:39.66ms
step:1874/2330 train_time:74342ms step_avg:39.67ms
step:1875/2330 train_time:74365ms step_avg:39.66ms
step:1876/2330 train_time:74421ms step_avg:39.67ms
step:1877/2330 train_time:74444ms step_avg:39.66ms
step:1878/2330 train_time:74500ms step_avg:39.67ms
step:1879/2330 train_time:74523ms step_avg:39.66ms
step:1880/2330 train_time:74579ms step_avg:39.67ms
step:1881/2330 train_time:74602ms step_avg:39.66ms
step:1882/2330 train_time:74658ms step_avg:39.67ms
step:1883/2330 train_time:74681ms step_avg:39.66ms
step:1884/2330 train_time:74737ms step_avg:39.67ms
step:1885/2330 train_time:74760ms step_avg:39.66ms
step:1886/2330 train_time:74816ms step_avg:39.67ms
step:1887/2330 train_time:74839ms step_avg:39.66ms
step:1888/2330 train_time:74895ms step_avg:39.67ms
step:1889/2330 train_time:74917ms step_avg:39.66ms
step:1890/2330 train_time:74974ms step_avg:39.67ms
step:1891/2330 train_time:74997ms step_avg:39.66ms
step:1892/2330 train_time:75053ms step_avg:39.67ms
step:1893/2330 train_time:75076ms step_avg:39.66ms
step:1894/2330 train_time:75133ms step_avg:39.67ms
step:1895/2330 train_time:75156ms step_avg:39.66ms
step:1896/2330 train_time:75213ms step_avg:39.67ms
step:1897/2330 train_time:75235ms step_avg:39.66ms
step:1898/2330 train_time:75291ms step_avg:39.67ms
step:1899/2330 train_time:75313ms step_avg:39.66ms
step:1900/2330 train_time:75370ms step_avg:39.67ms
step:1901/2330 train_time:75392ms step_avg:39.66ms
step:1902/2330 train_time:75449ms step_avg:39.67ms
step:1903/2330 train_time:75471ms step_avg:39.66ms
step:1904/2330 train_time:75528ms step_avg:39.67ms
step:1905/2330 train_time:75551ms step_avg:39.66ms
step:1906/2330 train_time:75609ms step_avg:39.67ms
step:1907/2330 train_time:75631ms step_avg:39.66ms
step:1908/2330 train_time:75688ms step_avg:39.67ms
step:1909/2330 train_time:75709ms step_avg:39.66ms
step:1910/2330 train_time:75766ms step_avg:39.67ms
step:1911/2330 train_time:75789ms step_avg:39.66ms
step:1912/2330 train_time:75845ms step_avg:39.67ms
step:1913/2330 train_time:75868ms step_avg:39.66ms
step:1914/2330 train_time:75924ms step_avg:39.67ms
step:1915/2330 train_time:75947ms step_avg:39.66ms
step:1916/2330 train_time:76002ms step_avg:39.67ms
step:1917/2330 train_time:76026ms step_avg:39.66ms
step:1918/2330 train_time:76082ms step_avg:39.67ms
step:1919/2330 train_time:76105ms step_avg:39.66ms
step:1920/2330 train_time:76162ms step_avg:39.67ms
step:1921/2330 train_time:76185ms step_avg:39.66ms
step:1922/2330 train_time:76241ms step_avg:39.67ms
step:1923/2330 train_time:76264ms step_avg:39.66ms
step:1924/2330 train_time:76320ms step_avg:39.67ms
step:1925/2330 train_time:76343ms step_avg:39.66ms
step:1926/2330 train_time:76398ms step_avg:39.67ms
step:1927/2330 train_time:76421ms step_avg:39.66ms
step:1928/2330 train_time:76478ms step_avg:39.67ms
step:1929/2330 train_time:76502ms step_avg:39.66ms
step:1930/2330 train_time:76558ms step_avg:39.67ms
step:1931/2330 train_time:76581ms step_avg:39.66ms
step:1932/2330 train_time:76637ms step_avg:39.67ms
step:1933/2330 train_time:76660ms step_avg:39.66ms
step:1934/2330 train_time:76715ms step_avg:39.67ms
step:1935/2330 train_time:76739ms step_avg:39.66ms
step:1936/2330 train_time:76795ms step_avg:39.67ms
step:1937/2330 train_time:76818ms step_avg:39.66ms
step:1938/2330 train_time:76874ms step_avg:39.67ms
step:1939/2330 train_time:76897ms step_avg:39.66ms
step:1940/2330 train_time:76954ms step_avg:39.67ms
step:1941/2330 train_time:76977ms step_avg:39.66ms
step:1942/2330 train_time:77033ms step_avg:39.67ms
step:1943/2330 train_time:77055ms step_avg:39.66ms
step:1944/2330 train_time:77113ms step_avg:39.67ms
step:1945/2330 train_time:77135ms step_avg:39.66ms
step:1946/2330 train_time:77192ms step_avg:39.67ms
step:1947/2330 train_time:77214ms step_avg:39.66ms
step:1948/2330 train_time:77270ms step_avg:39.67ms
step:1949/2330 train_time:77293ms step_avg:39.66ms
step:1950/2330 train_time:77349ms step_avg:39.67ms
step:1951/2330 train_time:77372ms step_avg:39.66ms
step:1952/2330 train_time:77429ms step_avg:39.67ms
step:1953/2330 train_time:77451ms step_avg:39.66ms
step:1954/2330 train_time:77507ms step_avg:39.67ms
step:1955/2330 train_time:77530ms step_avg:39.66ms
step:1956/2330 train_time:77587ms step_avg:39.67ms
step:1957/2330 train_time:77609ms step_avg:39.66ms
step:1958/2330 train_time:77666ms step_avg:39.67ms
step:1959/2330 train_time:77688ms step_avg:39.66ms
step:1960/2330 train_time:77744ms step_avg:39.67ms
step:1961/2330 train_time:77767ms step_avg:39.66ms
step:1962/2330 train_time:77823ms step_avg:39.67ms
step:1963/2330 train_time:77846ms step_avg:39.66ms
step:1964/2330 train_time:77902ms step_avg:39.67ms
step:1965/2330 train_time:77925ms step_avg:39.66ms
step:1966/2330 train_time:77982ms step_avg:39.67ms
step:1967/2330 train_time:78005ms step_avg:39.66ms
step:1968/2330 train_time:78061ms step_avg:39.67ms
step:1969/2330 train_time:78085ms step_avg:39.66ms
step:1970/2330 train_time:78142ms step_avg:39.67ms
step:1971/2330 train_time:78165ms step_avg:39.66ms
step:1972/2330 train_time:78221ms step_avg:39.67ms
step:1973/2330 train_time:78244ms step_avg:39.66ms
step:1974/2330 train_time:78300ms step_avg:39.67ms
step:1975/2330 train_time:78323ms step_avg:39.66ms
step:1976/2330 train_time:78378ms step_avg:39.67ms
step:1977/2330 train_time:78401ms step_avg:39.66ms
step:1978/2330 train_time:78458ms step_avg:39.67ms
step:1979/2330 train_time:78480ms step_avg:39.66ms
step:1980/2330 train_time:78537ms step_avg:39.66ms
step:1981/2330 train_time:78559ms step_avg:39.66ms
step:1982/2330 train_time:78615ms step_avg:39.66ms
step:1983/2330 train_time:78638ms step_avg:39.66ms
step:1984/2330 train_time:78694ms step_avg:39.66ms
step:1985/2330 train_time:78716ms step_avg:39.66ms
step:1986/2330 train_time:78773ms step_avg:39.66ms
step:1987/2330 train_time:78795ms step_avg:39.66ms
step:1988/2330 train_time:78852ms step_avg:39.66ms
step:1989/2330 train_time:78874ms step_avg:39.66ms
step:1990/2330 train_time:78931ms step_avg:39.66ms
step:1991/2330 train_time:78953ms step_avg:39.65ms
step:1992/2330 train_time:79010ms step_avg:39.66ms
step:1993/2330 train_time:79033ms step_avg:39.66ms
step:1994/2330 train_time:79090ms step_avg:39.66ms
step:1995/2330 train_time:79112ms step_avg:39.65ms
step:1996/2330 train_time:79168ms step_avg:39.66ms
step:1997/2330 train_time:79191ms step_avg:39.65ms
step:1998/2330 train_time:79247ms step_avg:39.66ms
step:1999/2330 train_time:79269ms step_avg:39.65ms
step:2000/2330 train_time:79326ms step_avg:39.66ms
step:2000/2330 val_loss:5.1440 train_time:79422ms step_avg:39.71ms
step:2001/2330 train_time:79434ms step_avg:39.70ms
step:2002/2330 train_time:79446ms step_avg:39.68ms
step:2003/2330 train_time:79456ms step_avg:39.67ms
step:2004/2330 train_time:79485ms step_avg:39.66ms
step:2005/2330 train_time:79507ms step_avg:39.65ms
step:2006/2330 train_time:79562ms step_avg:39.66ms
step:2007/2330 train_time:79584ms step_avg:39.65ms
step:2008/2330 train_time:79639ms step_avg:39.66ms
step:2009/2330 train_time:79661ms step_avg:39.65ms
step:2010/2330 train_time:79719ms step_avg:39.66ms
step:2011/2330 train_time:79743ms step_avg:39.65ms
step:2012/2330 train_time:79803ms step_avg:39.66ms
step:2013/2330 train_time:79827ms step_avg:39.66ms
step:2014/2330 train_time:79884ms step_avg:39.66ms
step:2015/2330 train_time:79907ms step_avg:39.66ms
step:2016/2330 train_time:79963ms step_avg:39.66ms
step:2017/2330 train_time:79985ms step_avg:39.66ms
step:2018/2330 train_time:80041ms step_avg:39.66ms
step:2019/2330 train_time:80063ms step_avg:39.65ms
step:2020/2330 train_time:80119ms step_avg:39.66ms
step:2021/2330 train_time:80142ms step_avg:39.65ms
step:2022/2330 train_time:80198ms step_avg:39.66ms
step:2023/2330 train_time:80219ms step_avg:39.65ms
step:2024/2330 train_time:80275ms step_avg:39.66ms
step:2025/2330 train_time:80296ms step_avg:39.65ms
step:2026/2330 train_time:80354ms step_avg:39.66ms
step:2027/2330 train_time:80376ms step_avg:39.65ms
step:2028/2330 train_time:80434ms step_avg:39.66ms
step:2029/2330 train_time:80456ms step_avg:39.65ms
step:2030/2330 train_time:80512ms step_avg:39.66ms
step:2031/2330 train_time:80533ms step_avg:39.65ms
step:2032/2330 train_time:80589ms step_avg:39.66ms
step:2033/2330 train_time:80612ms step_avg:39.65ms
step:2034/2330 train_time:80669ms step_avg:39.66ms
step:2035/2330 train_time:80694ms step_avg:39.65ms
step:2036/2330 train_time:80751ms step_avg:39.66ms
step:2037/2330 train_time:80775ms step_avg:39.65ms
step:2038/2330 train_time:80831ms step_avg:39.66ms
step:2039/2330 train_time:80854ms step_avg:39.65ms
step:2040/2330 train_time:80911ms step_avg:39.66ms
step:2041/2330 train_time:80933ms step_avg:39.65ms
step:2042/2330 train_time:80989ms step_avg:39.66ms
step:2043/2330 train_time:81012ms step_avg:39.65ms
step:2044/2330 train_time:81068ms step_avg:39.66ms
step:2045/2330 train_time:81091ms step_avg:39.65ms
step:2046/2330 train_time:81146ms step_avg:39.66ms
step:2047/2330 train_time:81169ms step_avg:39.65ms
step:2048/2330 train_time:81225ms step_avg:39.66ms
step:2049/2330 train_time:81248ms step_avg:39.65ms
step:2050/2330 train_time:81304ms step_avg:39.66ms
step:2051/2330 train_time:81327ms step_avg:39.65ms
step:2052/2330 train_time:81383ms step_avg:39.66ms
step:2053/2330 train_time:81406ms step_avg:39.65ms
step:2054/2330 train_time:81461ms step_avg:39.66ms
step:2055/2330 train_time:81484ms step_avg:39.65ms
step:2056/2330 train_time:81541ms step_avg:39.66ms
step:2057/2330 train_time:81563ms step_avg:39.65ms
step:2058/2330 train_time:81620ms step_avg:39.66ms
step:2059/2330 train_time:81642ms step_avg:39.65ms
step:2060/2330 train_time:81699ms step_avg:39.66ms
step:2061/2330 train_time:81721ms step_avg:39.65ms
step:2062/2330 train_time:81778ms step_avg:39.66ms
step:2063/2330 train_time:81801ms step_avg:39.65ms
step:2064/2330 train_time:81858ms step_avg:39.66ms
step:2065/2330 train_time:81880ms step_avg:39.65ms
step:2066/2330 train_time:81937ms step_avg:39.66ms
step:2067/2330 train_time:81959ms step_avg:39.65ms
step:2068/2330 train_time:82017ms step_avg:39.66ms
step:2069/2330 train_time:82039ms step_avg:39.65ms
step:2070/2330 train_time:82096ms step_avg:39.66ms
step:2071/2330 train_time:82118ms step_avg:39.65ms
step:2072/2330 train_time:82174ms step_avg:39.66ms
step:2073/2330 train_time:82196ms step_avg:39.65ms
step:2074/2330 train_time:82252ms step_avg:39.66ms
step:2075/2330 train_time:82274ms step_avg:39.65ms
step:2076/2330 train_time:82331ms step_avg:39.66ms
step:2077/2330 train_time:82353ms step_avg:39.65ms
step:2078/2330 train_time:82409ms step_avg:39.66ms
step:2079/2330 train_time:82432ms step_avg:39.65ms
step:2080/2330 train_time:82488ms step_avg:39.66ms
step:2081/2330 train_time:82511ms step_avg:39.65ms
step:2082/2330 train_time:82566ms step_avg:39.66ms
step:2083/2330 train_time:82589ms step_avg:39.65ms
step:2084/2330 train_time:82645ms step_avg:39.66ms
step:2085/2330 train_time:82668ms step_avg:39.65ms
step:2086/2330 train_time:82725ms step_avg:39.66ms
step:2087/2330 train_time:82748ms step_avg:39.65ms
step:2088/2330 train_time:82805ms step_avg:39.66ms
step:2089/2330 train_time:82828ms step_avg:39.65ms
step:2090/2330 train_time:82884ms step_avg:39.66ms
step:2091/2330 train_time:82907ms step_avg:39.65ms
step:2092/2330 train_time:82963ms step_avg:39.66ms
step:2093/2330 train_time:82986ms step_avg:39.65ms
step:2094/2330 train_time:83042ms step_avg:39.66ms
step:2095/2330 train_time:83065ms step_avg:39.65ms
step:2096/2330 train_time:83122ms step_avg:39.66ms
step:2097/2330 train_time:83144ms step_avg:39.65ms
step:2098/2330 train_time:83200ms step_avg:39.66ms
step:2099/2330 train_time:83223ms step_avg:39.65ms
step:2100/2330 train_time:83280ms step_avg:39.66ms
step:2101/2330 train_time:83302ms step_avg:39.65ms
step:2102/2330 train_time:83358ms step_avg:39.66ms
step:2103/2330 train_time:83380ms step_avg:39.65ms
step:2104/2330 train_time:83437ms step_avg:39.66ms
step:2105/2330 train_time:83459ms step_avg:39.65ms
step:2106/2330 train_time:83516ms step_avg:39.66ms
step:2107/2330 train_time:83539ms step_avg:39.65ms
step:2108/2330 train_time:83596ms step_avg:39.66ms
step:2109/2330 train_time:83617ms step_avg:39.65ms
step:2110/2330 train_time:83674ms step_avg:39.66ms
step:2111/2330 train_time:83696ms step_avg:39.65ms
step:2112/2330 train_time:83753ms step_avg:39.66ms
step:2113/2330 train_time:83775ms step_avg:39.65ms
step:2114/2330 train_time:83832ms step_avg:39.66ms
step:2115/2330 train_time:83854ms step_avg:39.65ms
step:2116/2330 train_time:83910ms step_avg:39.66ms
step:2117/2330 train_time:83933ms step_avg:39.65ms
step:2118/2330 train_time:83989ms step_avg:39.66ms
step:2119/2330 train_time:84013ms step_avg:39.65ms
step:2120/2330 train_time:84069ms step_avg:39.66ms
step:2121/2330 train_time:84092ms step_avg:39.65ms
step:2122/2330 train_time:84148ms step_avg:39.66ms
step:2123/2330 train_time:84171ms step_avg:39.65ms
step:2124/2330 train_time:84228ms step_avg:39.66ms
step:2125/2330 train_time:84251ms step_avg:39.65ms
step:2126/2330 train_time:84307ms step_avg:39.66ms
step:2127/2330 train_time:84330ms step_avg:39.65ms
step:2128/2330 train_time:84386ms step_avg:39.66ms
step:2129/2330 train_time:84408ms step_avg:39.65ms
step:2130/2330 train_time:84464ms step_avg:39.65ms
step:2131/2330 train_time:84487ms step_avg:39.65ms
step:2132/2330 train_time:84543ms step_avg:39.65ms
step:2133/2330 train_time:84565ms step_avg:39.65ms
step:2134/2330 train_time:84622ms step_avg:39.65ms
step:2135/2330 train_time:84645ms step_avg:39.65ms
step:2136/2330 train_time:84701ms step_avg:39.65ms
step:2137/2330 train_time:84724ms step_avg:39.65ms
step:2138/2330 train_time:84781ms step_avg:39.65ms
step:2139/2330 train_time:84804ms step_avg:39.65ms
step:2140/2330 train_time:84860ms step_avg:39.65ms
step:2141/2330 train_time:84883ms step_avg:39.65ms
step:2142/2330 train_time:84940ms step_avg:39.65ms
step:2143/2330 train_time:84963ms step_avg:39.65ms
step:2144/2330 train_time:85019ms step_avg:39.65ms
step:2145/2330 train_time:85041ms step_avg:39.65ms
step:2146/2330 train_time:85098ms step_avg:39.65ms
step:2147/2330 train_time:85120ms step_avg:39.65ms
step:2148/2330 train_time:85178ms step_avg:39.65ms
step:2149/2330 train_time:85200ms step_avg:39.65ms
step:2150/2330 train_time:85257ms step_avg:39.65ms
step:2151/2330 train_time:85279ms step_avg:39.65ms
step:2152/2330 train_time:85337ms step_avg:39.65ms
step:2153/2330 train_time:85359ms step_avg:39.65ms
step:2154/2330 train_time:85416ms step_avg:39.65ms
step:2155/2330 train_time:85438ms step_avg:39.65ms
step:2156/2330 train_time:85495ms step_avg:39.65ms
step:2157/2330 train_time:85517ms step_avg:39.65ms
step:2158/2330 train_time:85573ms step_avg:39.65ms
step:2159/2330 train_time:85595ms step_avg:39.65ms
step:2160/2330 train_time:85651ms step_avg:39.65ms
step:2161/2330 train_time:85674ms step_avg:39.65ms
step:2162/2330 train_time:85731ms step_avg:39.65ms
step:2163/2330 train_time:85754ms step_avg:39.65ms
step:2164/2330 train_time:85810ms step_avg:39.65ms
step:2165/2330 train_time:85833ms step_avg:39.65ms
step:2166/2330 train_time:85890ms step_avg:39.65ms
step:2167/2330 train_time:85914ms step_avg:39.65ms
step:2168/2330 train_time:85970ms step_avg:39.65ms
step:2169/2330 train_time:85992ms step_avg:39.65ms
step:2170/2330 train_time:86049ms step_avg:39.65ms
step:2171/2330 train_time:86071ms step_avg:39.65ms
step:2172/2330 train_time:86127ms step_avg:39.65ms
step:2173/2330 train_time:86149ms step_avg:39.65ms
step:2174/2330 train_time:86205ms step_avg:39.65ms
step:2175/2330 train_time:86228ms step_avg:39.64ms
step:2176/2330 train_time:86284ms step_avg:39.65ms
step:2177/2330 train_time:86306ms step_avg:39.64ms
step:2178/2330 train_time:86363ms step_avg:39.65ms
step:2179/2330 train_time:86385ms step_avg:39.64ms
step:2180/2330 train_time:86441ms step_avg:39.65ms
step:2181/2330 train_time:86463ms step_avg:39.64ms
step:2182/2330 train_time:86521ms step_avg:39.65ms
step:2183/2330 train_time:86543ms step_avg:39.64ms
step:2184/2330 train_time:86599ms step_avg:39.65ms
step:2185/2330 train_time:86622ms step_avg:39.64ms
step:2186/2330 train_time:86679ms step_avg:39.65ms
step:2187/2330 train_time:86701ms step_avg:39.64ms
step:2188/2330 train_time:86757ms step_avg:39.65ms
step:2189/2330 train_time:86780ms step_avg:39.64ms
step:2190/2330 train_time:86837ms step_avg:39.65ms
step:2191/2330 train_time:86859ms step_avg:39.64ms
step:2192/2330 train_time:86916ms step_avg:39.65ms
step:2193/2330 train_time:86938ms step_avg:39.64ms
step:2194/2330 train_time:86995ms step_avg:39.65ms
step:2195/2330 train_time:87017ms step_avg:39.64ms
step:2196/2330 train_time:87074ms step_avg:39.65ms
step:2197/2330 train_time:87095ms step_avg:39.64ms
step:2198/2330 train_time:87152ms step_avg:39.65ms
step:2199/2330 train_time:87174ms step_avg:39.64ms
step:2200/2330 train_time:87231ms step_avg:39.65ms
step:2201/2330 train_time:87254ms step_avg:39.64ms
step:2202/2330 train_time:87310ms step_avg:39.65ms
step:2203/2330 train_time:87333ms step_avg:39.64ms
step:2204/2330 train_time:87389ms step_avg:39.65ms
step:2205/2330 train_time:87412ms step_avg:39.64ms
step:2206/2330 train_time:87468ms step_avg:39.65ms
step:2207/2330 train_time:87491ms step_avg:39.64ms
step:2208/2330 train_time:87547ms step_avg:39.65ms
step:2209/2330 train_time:87570ms step_avg:39.64ms
step:2210/2330 train_time:87626ms step_avg:39.65ms
step:2211/2330 train_time:87649ms step_avg:39.64ms
step:2212/2330 train_time:87706ms step_avg:39.65ms
step:2213/2330 train_time:87728ms step_avg:39.64ms
step:2214/2330 train_time:87784ms step_avg:39.65ms
step:2215/2330 train_time:87807ms step_avg:39.64ms
step:2216/2330 train_time:87863ms step_avg:39.65ms
step:2217/2330 train_time:87886ms step_avg:39.64ms
step:2218/2330 train_time:87943ms step_avg:39.65ms
step:2219/2330 train_time:87966ms step_avg:39.64ms
step:2220/2330 train_time:88022ms step_avg:39.65ms
step:2221/2330 train_time:88045ms step_avg:39.64ms
step:2222/2330 train_time:88102ms step_avg:39.65ms
step:2223/2330 train_time:88125ms step_avg:39.64ms
step:2224/2330 train_time:88181ms step_avg:39.65ms
step:2225/2330 train_time:88204ms step_avg:39.64ms
step:2226/2330 train_time:88260ms step_avg:39.65ms
step:2227/2330 train_time:88282ms step_avg:39.64ms
step:2228/2330 train_time:88339ms step_avg:39.65ms
step:2229/2330 train_time:88361ms step_avg:39.64ms
step:2230/2330 train_time:88418ms step_avg:39.65ms
step:2231/2330 train_time:88440ms step_avg:39.64ms
step:2232/2330 train_time:88497ms step_avg:39.65ms
step:2233/2330 train_time:88519ms step_avg:39.64ms
step:2234/2330 train_time:88576ms step_avg:39.65ms
step:2235/2330 train_time:88598ms step_avg:39.64ms
step:2236/2330 train_time:88655ms step_avg:39.65ms
step:2237/2330 train_time:88677ms step_avg:39.64ms
step:2238/2330 train_time:88734ms step_avg:39.65ms
step:2239/2330 train_time:88756ms step_avg:39.64ms
step:2240/2330 train_time:88812ms step_avg:39.65ms
step:2241/2330 train_time:88834ms step_avg:39.64ms
step:2242/2330 train_time:88890ms step_avg:39.65ms
step:2243/2330 train_time:88913ms step_avg:39.64ms
step:2244/2330 train_time:88968ms step_avg:39.65ms
step:2245/2330 train_time:88991ms step_avg:39.64ms
step:2246/2330 train_time:89048ms step_avg:39.65ms
step:2247/2330 train_time:89070ms step_avg:39.64ms
step:2248/2330 train_time:89126ms step_avg:39.65ms
step:2249/2330 train_time:89149ms step_avg:39.64ms
step:2250/2330 train_time:89205ms step_avg:39.65ms
step:2250/2330 val_loss:5.1310 train_time:89301ms step_avg:39.69ms
step:2251/2330 train_time:89313ms step_avg:39.68ms
step:2252/2330 train_time:89325ms step_avg:39.66ms
step:2253/2330 train_time:89334ms step_avg:39.65ms
step:2254/2330 train_time:89364ms step_avg:39.65ms
step:2255/2330 train_time:89385ms step_avg:39.64ms
step:2256/2330 train_time:89440ms step_avg:39.65ms
step:2257/2330 train_time:89462ms step_avg:39.64ms
step:2258/2330 train_time:89517ms step_avg:39.64ms
step:2259/2330 train_time:89538ms step_avg:39.64ms
step:2260/2330 train_time:89594ms step_avg:39.64ms
step:2261/2330 train_time:89622ms step_avg:39.64ms
step:2262/2330 train_time:89683ms step_avg:39.65ms
step:2263/2330 train_time:89707ms step_avg:39.64ms
step:2264/2330 train_time:89764ms step_avg:39.65ms
step:2265/2330 train_time:89787ms step_avg:39.64ms
step:2266/2330 train_time:89843ms step_avg:39.65ms
step:2267/2330 train_time:89866ms step_avg:39.64ms
step:2268/2330 train_time:89922ms step_avg:39.65ms
step:2269/2330 train_time:89944ms step_avg:39.64ms
step:2270/2330 train_time:90001ms step_avg:39.65ms
step:2271/2330 train_time:90023ms step_avg:39.64ms
step:2272/2330 train_time:90078ms step_avg:39.65ms
step:2273/2330 train_time:90101ms step_avg:39.64ms
step:2274/2330 train_time:90156ms step_avg:39.65ms
step:2275/2330 train_time:90179ms step_avg:39.64ms
step:2276/2330 train_time:90234ms step_avg:39.65ms
step:2277/2330 train_time:90258ms step_avg:39.64ms
step:2278/2330 train_time:90314ms step_avg:39.65ms
step:2279/2330 train_time:90336ms step_avg:39.64ms
step:2280/2330 train_time:90392ms step_avg:39.65ms
step:2281/2330 train_time:90414ms step_avg:39.64ms
step:2282/2330 train_time:90470ms step_avg:39.64ms
step:2283/2330 train_time:90492ms step_avg:39.64ms
step:2284/2330 train_time:90549ms step_avg:39.65ms
step:2285/2330 train_time:90573ms step_avg:39.64ms
step:2286/2330 train_time:90631ms step_avg:39.65ms
step:2287/2330 train_time:90654ms step_avg:39.64ms
step:2288/2330 train_time:90711ms step_avg:39.65ms
step:2289/2330 train_time:90734ms step_avg:39.64ms
step:2290/2330 train_time:90791ms step_avg:39.65ms
step:2291/2330 train_time:90815ms step_avg:39.64ms
step:2292/2330 train_time:90872ms step_avg:39.65ms
step:2293/2330 train_time:90895ms step_avg:39.64ms
step:2294/2330 train_time:90951ms step_avg:39.65ms
step:2295/2330 train_time:90974ms step_avg:39.64ms
step:2296/2330 train_time:91030ms step_avg:39.65ms
step:2297/2330 train_time:91052ms step_avg:39.64ms
step:2298/2330 train_time:91108ms step_avg:39.65ms
step:2299/2330 train_time:91131ms step_avg:39.64ms
step:2300/2330 train_time:91188ms step_avg:39.65ms
step:2301/2330 train_time:91210ms step_avg:39.64ms
step:2302/2330 train_time:91266ms step_avg:39.65ms
step:2303/2330 train_time:91288ms step_avg:39.64ms
step:2304/2330 train_time:91345ms step_avg:39.65ms
step:2305/2330 train_time:91367ms step_avg:39.64ms
step:2306/2330 train_time:91423ms step_avg:39.65ms
step:2307/2330 train_time:91445ms step_avg:39.64ms
step:2308/2330 train_time:91502ms step_avg:39.65ms
step:2309/2330 train_time:91524ms step_avg:39.64ms
step:2310/2330 train_time:91581ms step_avg:39.65ms
step:2311/2330 train_time:91603ms step_avg:39.64ms
step:2312/2330 train_time:91660ms step_avg:39.65ms
step:2313/2330 train_time:91682ms step_avg:39.64ms
step:2314/2330 train_time:91739ms step_avg:39.65ms
step:2315/2330 train_time:91762ms step_avg:39.64ms
step:2316/2330 train_time:91818ms step_avg:39.65ms
step:2317/2330 train_time:91841ms step_avg:39.64ms
step:2318/2330 train_time:91898ms step_avg:39.65ms
step:2319/2330 train_time:91921ms step_avg:39.64ms
step:2320/2330 train_time:91977ms step_avg:39.65ms
step:2321/2330 train_time:92000ms step_avg:39.64ms
step:2322/2330 train_time:92056ms step_avg:39.65ms
step:2323/2330 train_time:92079ms step_avg:39.64ms
step:2324/2330 train_time:92135ms step_avg:39.65ms
step:2325/2330 train_time:92158ms step_avg:39.64ms
step:2326/2330 train_time:92214ms step_avg:39.65ms
step:2327/2330 train_time:92236ms step_avg:39.64ms
step:2328/2330 train_time:92292ms step_avg:39.64ms
step:2329/2330 train_time:92314ms step_avg:39.64ms
step:2330/2330 train_time:92370ms step_avg:39.64ms
step:2330/2330 val_loss:5.1243 train_time:92467ms step_avg:39.69ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
