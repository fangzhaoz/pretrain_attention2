import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_lr2e-1"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=2e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:59:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:72ms step_avg:72.06ms
step:2/2330 train_time:177ms step_avg:88.47ms
step:3/2330 train_time:200ms step_avg:66.53ms
step:4/2330 train_time:227ms step_avg:56.87ms
step:5/2330 train_time:284ms step_avg:56.75ms
step:6/2330 train_time:344ms step_avg:57.33ms
step:7/2330 train_time:401ms step_avg:57.34ms
step:8/2330 train_time:462ms step_avg:57.81ms
step:9/2330 train_time:520ms step_avg:57.82ms
step:10/2330 train_time:582ms step_avg:58.15ms
step:11/2330 train_time:639ms step_avg:58.07ms
step:12/2330 train_time:700ms step_avg:58.34ms
step:13/2330 train_time:758ms step_avg:58.32ms
step:14/2330 train_time:820ms step_avg:58.55ms
step:15/2330 train_time:877ms step_avg:58.49ms
step:16/2330 train_time:938ms step_avg:58.65ms
step:17/2330 train_time:996ms step_avg:58.56ms
step:18/2330 train_time:1058ms step_avg:58.80ms
step:19/2330 train_time:1120ms step_avg:58.95ms
step:20/2330 train_time:1187ms step_avg:59.33ms
step:21/2330 train_time:1246ms step_avg:59.35ms
step:22/2330 train_time:1309ms step_avg:59.51ms
step:23/2330 train_time:1368ms step_avg:59.46ms
step:24/2330 train_time:1429ms step_avg:59.56ms
step:25/2330 train_time:1487ms step_avg:59.50ms
step:26/2330 train_time:1548ms step_avg:59.55ms
step:27/2330 train_time:1606ms step_avg:59.48ms
step:28/2330 train_time:1668ms step_avg:59.59ms
step:29/2330 train_time:1726ms step_avg:59.53ms
step:30/2330 train_time:1789ms step_avg:59.64ms
step:31/2330 train_time:1846ms step_avg:59.56ms
step:32/2330 train_time:1909ms step_avg:59.65ms
step:33/2330 train_time:1967ms step_avg:59.61ms
step:34/2330 train_time:2031ms step_avg:59.73ms
step:35/2330 train_time:2090ms step_avg:59.71ms
step:36/2330 train_time:2153ms step_avg:59.81ms
step:37/2330 train_time:2212ms step_avg:59.79ms
step:38/2330 train_time:2275ms step_avg:59.86ms
step:39/2330 train_time:2333ms step_avg:59.82ms
step:40/2330 train_time:2394ms step_avg:59.84ms
step:41/2330 train_time:2452ms step_avg:59.80ms
step:42/2330 train_time:2514ms step_avg:59.86ms
step:43/2330 train_time:2572ms step_avg:59.83ms
step:44/2330 train_time:2635ms step_avg:59.88ms
step:45/2330 train_time:2694ms step_avg:59.86ms
step:46/2330 train_time:2756ms step_avg:59.91ms
step:47/2330 train_time:2815ms step_avg:59.89ms
step:48/2330 train_time:2877ms step_avg:59.93ms
step:49/2330 train_time:2935ms step_avg:59.90ms
step:50/2330 train_time:2997ms step_avg:59.93ms
step:51/2330 train_time:3055ms step_avg:59.90ms
step:52/2330 train_time:3117ms step_avg:59.95ms
step:53/2330 train_time:3175ms step_avg:59.91ms
step:54/2330 train_time:3237ms step_avg:59.95ms
step:55/2330 train_time:3296ms step_avg:59.92ms
step:56/2330 train_time:3358ms step_avg:59.96ms
step:57/2330 train_time:3416ms step_avg:59.94ms
step:58/2330 train_time:3478ms step_avg:59.97ms
step:59/2330 train_time:3538ms step_avg:59.96ms
step:60/2330 train_time:3599ms step_avg:59.99ms
step:61/2330 train_time:3658ms step_avg:59.97ms
step:62/2330 train_time:3721ms step_avg:60.01ms
step:63/2330 train_time:3778ms step_avg:59.96ms
step:64/2330 train_time:3840ms step_avg:60.00ms
step:65/2330 train_time:3898ms step_avg:59.97ms
step:66/2330 train_time:3960ms step_avg:60.00ms
step:67/2330 train_time:4019ms step_avg:59.98ms
step:68/2330 train_time:4080ms step_avg:60.01ms
step:69/2330 train_time:4139ms step_avg:59.98ms
step:70/2330 train_time:4201ms step_avg:60.01ms
step:71/2330 train_time:4259ms step_avg:59.99ms
step:72/2330 train_time:4322ms step_avg:60.03ms
step:73/2330 train_time:4380ms step_avg:60.00ms
step:74/2330 train_time:4442ms step_avg:60.03ms
step:75/2330 train_time:4500ms step_avg:60.01ms
step:76/2330 train_time:4562ms step_avg:60.03ms
step:77/2330 train_time:4621ms step_avg:60.01ms
step:78/2330 train_time:4683ms step_avg:60.04ms
step:79/2330 train_time:4741ms step_avg:60.01ms
step:80/2330 train_time:4803ms step_avg:60.03ms
step:81/2330 train_time:4861ms step_avg:60.01ms
step:82/2330 train_time:4922ms step_avg:60.03ms
step:83/2330 train_time:4981ms step_avg:60.01ms
step:84/2330 train_time:5042ms step_avg:60.02ms
step:85/2330 train_time:5100ms step_avg:60.00ms
step:86/2330 train_time:5162ms step_avg:60.03ms
step:87/2330 train_time:5221ms step_avg:60.01ms
step:88/2330 train_time:5283ms step_avg:60.03ms
step:89/2330 train_time:5341ms step_avg:60.01ms
step:90/2330 train_time:5403ms step_avg:60.04ms
step:91/2330 train_time:5462ms step_avg:60.03ms
step:92/2330 train_time:5525ms step_avg:60.06ms
step:93/2330 train_time:5583ms step_avg:60.03ms
step:94/2330 train_time:5646ms step_avg:60.06ms
step:95/2330 train_time:5704ms step_avg:60.04ms
step:96/2330 train_time:5766ms step_avg:60.07ms
step:97/2330 train_time:5824ms step_avg:60.04ms
step:98/2330 train_time:5886ms step_avg:60.06ms
step:99/2330 train_time:5943ms step_avg:60.03ms
step:100/2330 train_time:6005ms step_avg:60.05ms
step:101/2330 train_time:6064ms step_avg:60.04ms
step:102/2330 train_time:6126ms step_avg:60.06ms
step:103/2330 train_time:6184ms step_avg:60.04ms
step:104/2330 train_time:6246ms step_avg:60.05ms
step:105/2330 train_time:6303ms step_avg:60.03ms
step:106/2330 train_time:6366ms step_avg:60.06ms
step:107/2330 train_time:6424ms step_avg:60.04ms
step:108/2330 train_time:6485ms step_avg:60.05ms
step:109/2330 train_time:6543ms step_avg:60.03ms
step:110/2330 train_time:6606ms step_avg:60.05ms
step:111/2330 train_time:6664ms step_avg:60.03ms
step:112/2330 train_time:6726ms step_avg:60.06ms
step:113/2330 train_time:6784ms step_avg:60.03ms
step:114/2330 train_time:6846ms step_avg:60.05ms
step:115/2330 train_time:6904ms step_avg:60.04ms
step:116/2330 train_time:6966ms step_avg:60.05ms
step:117/2330 train_time:7024ms step_avg:60.04ms
step:118/2330 train_time:7086ms step_avg:60.05ms
step:119/2330 train_time:7143ms step_avg:60.03ms
step:120/2330 train_time:7206ms step_avg:60.05ms
step:121/2330 train_time:7264ms step_avg:60.03ms
step:122/2330 train_time:7326ms step_avg:60.05ms
step:123/2330 train_time:7384ms step_avg:60.03ms
step:124/2330 train_time:7446ms step_avg:60.05ms
step:125/2330 train_time:7504ms step_avg:60.03ms
step:126/2330 train_time:7566ms step_avg:60.05ms
step:127/2330 train_time:7624ms step_avg:60.03ms
step:128/2330 train_time:7685ms step_avg:60.04ms
step:129/2330 train_time:7743ms step_avg:60.02ms
step:130/2330 train_time:7805ms step_avg:60.04ms
step:131/2330 train_time:7863ms step_avg:60.03ms
step:132/2330 train_time:7926ms step_avg:60.04ms
step:133/2330 train_time:7984ms step_avg:60.03ms
step:134/2330 train_time:8045ms step_avg:60.04ms
step:135/2330 train_time:8103ms step_avg:60.02ms
step:136/2330 train_time:8165ms step_avg:60.04ms
step:137/2330 train_time:8223ms step_avg:60.03ms
step:138/2330 train_time:8285ms step_avg:60.04ms
step:139/2330 train_time:8343ms step_avg:60.02ms
step:140/2330 train_time:8405ms step_avg:60.04ms
step:141/2330 train_time:8463ms step_avg:60.02ms
step:142/2330 train_time:8525ms step_avg:60.04ms
step:143/2330 train_time:8583ms step_avg:60.02ms
step:144/2330 train_time:8646ms step_avg:60.04ms
step:145/2330 train_time:8704ms step_avg:60.03ms
step:146/2330 train_time:8766ms step_avg:60.04ms
step:147/2330 train_time:8825ms step_avg:60.03ms
step:148/2330 train_time:8886ms step_avg:60.04ms
step:149/2330 train_time:8944ms step_avg:60.02ms
step:150/2330 train_time:9006ms step_avg:60.04ms
step:151/2330 train_time:9064ms step_avg:60.03ms
step:152/2330 train_time:9127ms step_avg:60.04ms
step:153/2330 train_time:9185ms step_avg:60.03ms
step:154/2330 train_time:9246ms step_avg:60.04ms
step:155/2330 train_time:9305ms step_avg:60.03ms
step:156/2330 train_time:9367ms step_avg:60.04ms
step:157/2330 train_time:9425ms step_avg:60.03ms
step:158/2330 train_time:9487ms step_avg:60.04ms
step:159/2330 train_time:9544ms step_avg:60.02ms
step:160/2330 train_time:9606ms step_avg:60.04ms
step:161/2330 train_time:9664ms step_avg:60.02ms
step:162/2330 train_time:9726ms step_avg:60.04ms
step:163/2330 train_time:9784ms step_avg:60.03ms
step:164/2330 train_time:9846ms step_avg:60.04ms
step:165/2330 train_time:9905ms step_avg:60.03ms
step:166/2330 train_time:9967ms step_avg:60.04ms
step:167/2330 train_time:10025ms step_avg:60.03ms
step:168/2330 train_time:10087ms step_avg:60.04ms
step:169/2330 train_time:10145ms step_avg:60.03ms
step:170/2330 train_time:10206ms step_avg:60.04ms
step:171/2330 train_time:10265ms step_avg:60.03ms
step:172/2330 train_time:10327ms step_avg:60.04ms
step:173/2330 train_time:10384ms step_avg:60.02ms
step:174/2330 train_time:10446ms step_avg:60.03ms
step:175/2330 train_time:10504ms step_avg:60.02ms
step:176/2330 train_time:10566ms step_avg:60.03ms
step:177/2330 train_time:10624ms step_avg:60.02ms
step:178/2330 train_time:10685ms step_avg:60.03ms
step:179/2330 train_time:10743ms step_avg:60.02ms
step:180/2330 train_time:10805ms step_avg:60.03ms
step:181/2330 train_time:10865ms step_avg:60.03ms
step:182/2330 train_time:10927ms step_avg:60.04ms
step:183/2330 train_time:10984ms step_avg:60.02ms
step:184/2330 train_time:11046ms step_avg:60.03ms
step:185/2330 train_time:11104ms step_avg:60.02ms
step:186/2330 train_time:11166ms step_avg:60.03ms
step:187/2330 train_time:11224ms step_avg:60.02ms
step:188/2330 train_time:11286ms step_avg:60.03ms
step:189/2330 train_time:11343ms step_avg:60.02ms
step:190/2330 train_time:11406ms step_avg:60.03ms
step:191/2330 train_time:11463ms step_avg:60.02ms
step:192/2330 train_time:11526ms step_avg:60.03ms
step:193/2330 train_time:11584ms step_avg:60.02ms
step:194/2330 train_time:11646ms step_avg:60.03ms
step:195/2330 train_time:11704ms step_avg:60.02ms
step:196/2330 train_time:11766ms step_avg:60.03ms
step:197/2330 train_time:11824ms step_avg:60.02ms
step:198/2330 train_time:11886ms step_avg:60.03ms
step:199/2330 train_time:11944ms step_avg:60.02ms
step:200/2330 train_time:12006ms step_avg:60.03ms
step:201/2330 train_time:12064ms step_avg:60.02ms
step:202/2330 train_time:12127ms step_avg:60.03ms
step:203/2330 train_time:12185ms step_avg:60.02ms
step:204/2330 train_time:12247ms step_avg:60.03ms
step:205/2330 train_time:12305ms step_avg:60.02ms
step:206/2330 train_time:12366ms step_avg:60.03ms
step:207/2330 train_time:12425ms step_avg:60.02ms
step:208/2330 train_time:12487ms step_avg:60.03ms
step:209/2330 train_time:12545ms step_avg:60.02ms
step:210/2330 train_time:12607ms step_avg:60.03ms
step:211/2330 train_time:12665ms step_avg:60.02ms
step:212/2330 train_time:12727ms step_avg:60.03ms
step:213/2330 train_time:12785ms step_avg:60.02ms
step:214/2330 train_time:12848ms step_avg:60.04ms
step:215/2330 train_time:12906ms step_avg:60.03ms
step:216/2330 train_time:12968ms step_avg:60.04ms
step:217/2330 train_time:13026ms step_avg:60.03ms
step:218/2330 train_time:13088ms step_avg:60.03ms
step:219/2330 train_time:13146ms step_avg:60.03ms
step:220/2330 train_time:13208ms step_avg:60.04ms
step:221/2330 train_time:13267ms step_avg:60.03ms
step:222/2330 train_time:13328ms step_avg:60.04ms
step:223/2330 train_time:13386ms step_avg:60.03ms
step:224/2330 train_time:13449ms step_avg:60.04ms
step:225/2330 train_time:13507ms step_avg:60.03ms
step:226/2330 train_time:13569ms step_avg:60.04ms
step:227/2330 train_time:13627ms step_avg:60.03ms
step:228/2330 train_time:13688ms step_avg:60.04ms
step:229/2330 train_time:13746ms step_avg:60.03ms
step:230/2330 train_time:13809ms step_avg:60.04ms
step:231/2330 train_time:13867ms step_avg:60.03ms
step:232/2330 train_time:13930ms step_avg:60.04ms
step:233/2330 train_time:13988ms step_avg:60.03ms
step:234/2330 train_time:14049ms step_avg:60.04ms
step:235/2330 train_time:14107ms step_avg:60.03ms
step:236/2330 train_time:14170ms step_avg:60.04ms
step:237/2330 train_time:14228ms step_avg:60.03ms
step:238/2330 train_time:14290ms step_avg:60.04ms
step:239/2330 train_time:14348ms step_avg:60.03ms
step:240/2330 train_time:14409ms step_avg:60.04ms
step:241/2330 train_time:14468ms step_avg:60.03ms
step:242/2330 train_time:14529ms step_avg:60.04ms
step:243/2330 train_time:14587ms step_avg:60.03ms
step:244/2330 train_time:14649ms step_avg:60.04ms
step:245/2330 train_time:14707ms step_avg:60.03ms
step:246/2330 train_time:14769ms step_avg:60.04ms
step:247/2330 train_time:14827ms step_avg:60.03ms
step:248/2330 train_time:14889ms step_avg:60.04ms
step:249/2330 train_time:14947ms step_avg:60.03ms
step:250/2330 train_time:15010ms step_avg:60.04ms
step:250/2330 val_loss:4.7916 train_time:15082ms step_avg:60.33ms
step:251/2330 train_time:15103ms step_avg:60.17ms
step:252/2330 train_time:15134ms step_avg:60.05ms
step:253/2330 train_time:15192ms step_avg:60.05ms
step:254/2330 train_time:15258ms step_avg:60.07ms
step:255/2330 train_time:15321ms step_avg:60.08ms
step:256/2330 train_time:15383ms step_avg:60.09ms
step:257/2330 train_time:15443ms step_avg:60.09ms
step:258/2330 train_time:15505ms step_avg:60.10ms
step:259/2330 train_time:15562ms step_avg:60.09ms
step:260/2330 train_time:15624ms step_avg:60.09ms
step:261/2330 train_time:15682ms step_avg:60.09ms
step:262/2330 train_time:15744ms step_avg:60.09ms
step:263/2330 train_time:15802ms step_avg:60.08ms
step:264/2330 train_time:15863ms step_avg:60.09ms
step:265/2330 train_time:15921ms step_avg:60.08ms
step:266/2330 train_time:15984ms step_avg:60.09ms
step:267/2330 train_time:16043ms step_avg:60.09ms
step:268/2330 train_time:16107ms step_avg:60.10ms
step:269/2330 train_time:16167ms step_avg:60.10ms
step:270/2330 train_time:16230ms step_avg:60.11ms
step:271/2330 train_time:16290ms step_avg:60.11ms
step:272/2330 train_time:16352ms step_avg:60.12ms
step:273/2330 train_time:16411ms step_avg:60.11ms
step:274/2330 train_time:16474ms step_avg:60.12ms
step:275/2330 train_time:16532ms step_avg:60.12ms
step:276/2330 train_time:16593ms step_avg:60.12ms
step:277/2330 train_time:16651ms step_avg:60.11ms
step:278/2330 train_time:16712ms step_avg:60.12ms
step:279/2330 train_time:16771ms step_avg:60.11ms
step:280/2330 train_time:16833ms step_avg:60.12ms
step:281/2330 train_time:16890ms step_avg:60.11ms
step:282/2330 train_time:16952ms step_avg:60.11ms
step:283/2330 train_time:17010ms step_avg:60.11ms
step:284/2330 train_time:17073ms step_avg:60.12ms
step:285/2330 train_time:17132ms step_avg:60.11ms
step:286/2330 train_time:17195ms step_avg:60.12ms
step:287/2330 train_time:17253ms step_avg:60.12ms
step:288/2330 train_time:17316ms step_avg:60.13ms
step:289/2330 train_time:17374ms step_avg:60.12ms
step:290/2330 train_time:17436ms step_avg:60.13ms
step:291/2330 train_time:17494ms step_avg:60.12ms
step:292/2330 train_time:17556ms step_avg:60.12ms
step:293/2330 train_time:17614ms step_avg:60.12ms
step:294/2330 train_time:17676ms step_avg:60.12ms
step:295/2330 train_time:17734ms step_avg:60.11ms
step:296/2330 train_time:17796ms step_avg:60.12ms
step:297/2330 train_time:17853ms step_avg:60.11ms
step:298/2330 train_time:17915ms step_avg:60.12ms
step:299/2330 train_time:17973ms step_avg:60.11ms
step:300/2330 train_time:18035ms step_avg:60.12ms
step:301/2330 train_time:18094ms step_avg:60.11ms
step:302/2330 train_time:18156ms step_avg:60.12ms
step:303/2330 train_time:18214ms step_avg:60.11ms
step:304/2330 train_time:18276ms step_avg:60.12ms
step:305/2330 train_time:18335ms step_avg:60.11ms
step:306/2330 train_time:18397ms step_avg:60.12ms
step:307/2330 train_time:18455ms step_avg:60.11ms
step:308/2330 train_time:18517ms step_avg:60.12ms
step:309/2330 train_time:18574ms step_avg:60.11ms
step:310/2330 train_time:18636ms step_avg:60.12ms
step:311/2330 train_time:18694ms step_avg:60.11ms
step:312/2330 train_time:18755ms step_avg:60.11ms
step:313/2330 train_time:18814ms step_avg:60.11ms
step:314/2330 train_time:18876ms step_avg:60.11ms
step:315/2330 train_time:18933ms step_avg:60.11ms
step:316/2330 train_time:18995ms step_avg:60.11ms
step:317/2330 train_time:19054ms step_avg:60.11ms
step:318/2330 train_time:19117ms step_avg:60.12ms
step:319/2330 train_time:19175ms step_avg:60.11ms
step:320/2330 train_time:19237ms step_avg:60.12ms
step:321/2330 train_time:19295ms step_avg:60.11ms
step:322/2330 train_time:19357ms step_avg:60.11ms
step:323/2330 train_time:19415ms step_avg:60.11ms
step:324/2330 train_time:19477ms step_avg:60.12ms
step:325/2330 train_time:19536ms step_avg:60.11ms
step:326/2330 train_time:19597ms step_avg:60.11ms
step:327/2330 train_time:19655ms step_avg:60.11ms
step:328/2330 train_time:19717ms step_avg:60.11ms
step:329/2330 train_time:19775ms step_avg:60.11ms
step:330/2330 train_time:19838ms step_avg:60.11ms
step:331/2330 train_time:19896ms step_avg:60.11ms
step:332/2330 train_time:19957ms step_avg:60.11ms
step:333/2330 train_time:20016ms step_avg:60.11ms
step:334/2330 train_time:20077ms step_avg:60.11ms
step:335/2330 train_time:20136ms step_avg:60.11ms
step:336/2330 train_time:20199ms step_avg:60.12ms
step:337/2330 train_time:20258ms step_avg:60.11ms
step:338/2330 train_time:20319ms step_avg:60.11ms
step:339/2330 train_time:20378ms step_avg:60.11ms
step:340/2330 train_time:20440ms step_avg:60.12ms
step:341/2330 train_time:20498ms step_avg:60.11ms
step:342/2330 train_time:20560ms step_avg:60.12ms
step:343/2330 train_time:20618ms step_avg:60.11ms
step:344/2330 train_time:20679ms step_avg:60.11ms
step:345/2330 train_time:20737ms step_avg:60.11ms
step:346/2330 train_time:20800ms step_avg:60.11ms
step:347/2330 train_time:20858ms step_avg:60.11ms
step:348/2330 train_time:20920ms step_avg:60.11ms
step:349/2330 train_time:20979ms step_avg:60.11ms
step:350/2330 train_time:21041ms step_avg:60.12ms
step:351/2330 train_time:21100ms step_avg:60.11ms
step:352/2330 train_time:21162ms step_avg:60.12ms
step:353/2330 train_time:21221ms step_avg:60.12ms
step:354/2330 train_time:21283ms step_avg:60.12ms
step:355/2330 train_time:21341ms step_avg:60.12ms
step:356/2330 train_time:21403ms step_avg:60.12ms
step:357/2330 train_time:21461ms step_avg:60.12ms
step:358/2330 train_time:21524ms step_avg:60.12ms
step:359/2330 train_time:21583ms step_avg:60.12ms
step:360/2330 train_time:21645ms step_avg:60.12ms
step:361/2330 train_time:21703ms step_avg:60.12ms
step:362/2330 train_time:21765ms step_avg:60.12ms
step:363/2330 train_time:21824ms step_avg:60.12ms
step:364/2330 train_time:21887ms step_avg:60.13ms
step:365/2330 train_time:21945ms step_avg:60.12ms
step:366/2330 train_time:22007ms step_avg:60.13ms
step:367/2330 train_time:22066ms step_avg:60.12ms
step:368/2330 train_time:22129ms step_avg:60.13ms
step:369/2330 train_time:22187ms step_avg:60.13ms
step:370/2330 train_time:22250ms step_avg:60.13ms
step:371/2330 train_time:22307ms step_avg:60.13ms
step:372/2330 train_time:22369ms step_avg:60.13ms
step:373/2330 train_time:22427ms step_avg:60.13ms
step:374/2330 train_time:22489ms step_avg:60.13ms
step:375/2330 train_time:22547ms step_avg:60.13ms
step:376/2330 train_time:22611ms step_avg:60.13ms
step:377/2330 train_time:22669ms step_avg:60.13ms
step:378/2330 train_time:22731ms step_avg:60.13ms
step:379/2330 train_time:22790ms step_avg:60.13ms
step:380/2330 train_time:22851ms step_avg:60.13ms
step:381/2330 train_time:22910ms step_avg:60.13ms
step:382/2330 train_time:22972ms step_avg:60.14ms
step:383/2330 train_time:23031ms step_avg:60.13ms
step:384/2330 train_time:23093ms step_avg:60.14ms
step:385/2330 train_time:23151ms step_avg:60.13ms
step:386/2330 train_time:23214ms step_avg:60.14ms
step:387/2330 train_time:23272ms step_avg:60.13ms
step:388/2330 train_time:23335ms step_avg:60.14ms
step:389/2330 train_time:23393ms step_avg:60.14ms
step:390/2330 train_time:23455ms step_avg:60.14ms
step:391/2330 train_time:23512ms step_avg:60.13ms
step:392/2330 train_time:23574ms step_avg:60.14ms
step:393/2330 train_time:23633ms step_avg:60.13ms
step:394/2330 train_time:23695ms step_avg:60.14ms
step:395/2330 train_time:23752ms step_avg:60.13ms
step:396/2330 train_time:23814ms step_avg:60.14ms
step:397/2330 train_time:23872ms step_avg:60.13ms
step:398/2330 train_time:23934ms step_avg:60.14ms
step:399/2330 train_time:23992ms step_avg:60.13ms
step:400/2330 train_time:24054ms step_avg:60.14ms
step:401/2330 train_time:24113ms step_avg:60.13ms
step:402/2330 train_time:24175ms step_avg:60.14ms
step:403/2330 train_time:24233ms step_avg:60.13ms
step:404/2330 train_time:24295ms step_avg:60.14ms
step:405/2330 train_time:24353ms step_avg:60.13ms
step:406/2330 train_time:24415ms step_avg:60.14ms
step:407/2330 train_time:24473ms step_avg:60.13ms
step:408/2330 train_time:24536ms step_avg:60.14ms
step:409/2330 train_time:24594ms step_avg:60.13ms
step:410/2330 train_time:24656ms step_avg:60.14ms
step:411/2330 train_time:24714ms step_avg:60.13ms
step:412/2330 train_time:24777ms step_avg:60.14ms
step:413/2330 train_time:24835ms step_avg:60.13ms
step:414/2330 train_time:24898ms step_avg:60.14ms
step:415/2330 train_time:24956ms step_avg:60.13ms
step:416/2330 train_time:25017ms step_avg:60.14ms
step:417/2330 train_time:25076ms step_avg:60.13ms
step:418/2330 train_time:25137ms step_avg:60.14ms
step:419/2330 train_time:25196ms step_avg:60.13ms
step:420/2330 train_time:25257ms step_avg:60.14ms
step:421/2330 train_time:25315ms step_avg:60.13ms
step:422/2330 train_time:25377ms step_avg:60.13ms
step:423/2330 train_time:25435ms step_avg:60.13ms
step:424/2330 train_time:25497ms step_avg:60.13ms
step:425/2330 train_time:25555ms step_avg:60.13ms
step:426/2330 train_time:25617ms step_avg:60.13ms
step:427/2330 train_time:25675ms step_avg:60.13ms
step:428/2330 train_time:25737ms step_avg:60.13ms
step:429/2330 train_time:25796ms step_avg:60.13ms
step:430/2330 train_time:25857ms step_avg:60.13ms
step:431/2330 train_time:25915ms step_avg:60.13ms
step:432/2330 train_time:25978ms step_avg:60.13ms
step:433/2330 train_time:26036ms step_avg:60.13ms
step:434/2330 train_time:26097ms step_avg:60.13ms
step:435/2330 train_time:26155ms step_avg:60.13ms
step:436/2330 train_time:26217ms step_avg:60.13ms
step:437/2330 train_time:26275ms step_avg:60.12ms
step:438/2330 train_time:26337ms step_avg:60.13ms
step:439/2330 train_time:26396ms step_avg:60.13ms
step:440/2330 train_time:26457ms step_avg:60.13ms
step:441/2330 train_time:26515ms step_avg:60.12ms
step:442/2330 train_time:26577ms step_avg:60.13ms
step:443/2330 train_time:26635ms step_avg:60.12ms
step:444/2330 train_time:26697ms step_avg:60.13ms
step:445/2330 train_time:26755ms step_avg:60.12ms
step:446/2330 train_time:26816ms step_avg:60.13ms
step:447/2330 train_time:26875ms step_avg:60.12ms
step:448/2330 train_time:26938ms step_avg:60.13ms
step:449/2330 train_time:26997ms step_avg:60.13ms
step:450/2330 train_time:27058ms step_avg:60.13ms
step:451/2330 train_time:27117ms step_avg:60.13ms
step:452/2330 train_time:27179ms step_avg:60.13ms
step:453/2330 train_time:27237ms step_avg:60.13ms
step:454/2330 train_time:27298ms step_avg:60.13ms
step:455/2330 train_time:27357ms step_avg:60.12ms
step:456/2330 train_time:27419ms step_avg:60.13ms
step:457/2330 train_time:27477ms step_avg:60.12ms
step:458/2330 train_time:27540ms step_avg:60.13ms
step:459/2330 train_time:27598ms step_avg:60.13ms
step:460/2330 train_time:27661ms step_avg:60.13ms
step:461/2330 train_time:27720ms step_avg:60.13ms
step:462/2330 train_time:27782ms step_avg:60.13ms
step:463/2330 train_time:27840ms step_avg:60.13ms
step:464/2330 train_time:27902ms step_avg:60.13ms
step:465/2330 train_time:27960ms step_avg:60.13ms
step:466/2330 train_time:28023ms step_avg:60.13ms
step:467/2330 train_time:28081ms step_avg:60.13ms
step:468/2330 train_time:28143ms step_avg:60.13ms
step:469/2330 train_time:28202ms step_avg:60.13ms
step:470/2330 train_time:28264ms step_avg:60.14ms
step:471/2330 train_time:28323ms step_avg:60.13ms
step:472/2330 train_time:28385ms step_avg:60.14ms
step:473/2330 train_time:28443ms step_avg:60.13ms
step:474/2330 train_time:28505ms step_avg:60.14ms
step:475/2330 train_time:28564ms step_avg:60.13ms
step:476/2330 train_time:28627ms step_avg:60.14ms
step:477/2330 train_time:28685ms step_avg:60.14ms
step:478/2330 train_time:28748ms step_avg:60.14ms
step:479/2330 train_time:28806ms step_avg:60.14ms
step:480/2330 train_time:28868ms step_avg:60.14ms
step:481/2330 train_time:28927ms step_avg:60.14ms
step:482/2330 train_time:28990ms step_avg:60.14ms
step:483/2330 train_time:29048ms step_avg:60.14ms
step:484/2330 train_time:29110ms step_avg:60.15ms
step:485/2330 train_time:29169ms step_avg:60.14ms
step:486/2330 train_time:29232ms step_avg:60.15ms
step:487/2330 train_time:29290ms step_avg:60.14ms
step:488/2330 train_time:29352ms step_avg:60.15ms
step:489/2330 train_time:29410ms step_avg:60.14ms
step:490/2330 train_time:29473ms step_avg:60.15ms
step:491/2330 train_time:29532ms step_avg:60.15ms
step:492/2330 train_time:29594ms step_avg:60.15ms
step:493/2330 train_time:29652ms step_avg:60.15ms
step:494/2330 train_time:29714ms step_avg:60.15ms
step:495/2330 train_time:29773ms step_avg:60.15ms
step:496/2330 train_time:29834ms step_avg:60.15ms
step:497/2330 train_time:29892ms step_avg:60.14ms
step:498/2330 train_time:29954ms step_avg:60.15ms
step:499/2330 train_time:30012ms step_avg:60.14ms
step:500/2330 train_time:30074ms step_avg:60.15ms
step:500/2330 val_loss:4.1764 train_time:30145ms step_avg:60.29ms
step:501/2330 train_time:30165ms step_avg:60.21ms
step:502/2330 train_time:30196ms step_avg:60.15ms
step:503/2330 train_time:30258ms step_avg:60.15ms
step:504/2330 train_time:30325ms step_avg:60.17ms
step:505/2330 train_time:30385ms step_avg:60.17ms
step:506/2330 train_time:30447ms step_avg:60.17ms
step:507/2330 train_time:30505ms step_avg:60.17ms
step:508/2330 train_time:30567ms step_avg:60.17ms
step:509/2330 train_time:30624ms step_avg:60.17ms
step:510/2330 train_time:30687ms step_avg:60.17ms
step:511/2330 train_time:30744ms step_avg:60.17ms
step:512/2330 train_time:30806ms step_avg:60.17ms
step:513/2330 train_time:30863ms step_avg:60.16ms
step:514/2330 train_time:30925ms step_avg:60.17ms
step:515/2330 train_time:30983ms step_avg:60.16ms
step:516/2330 train_time:31044ms step_avg:60.16ms
step:517/2330 train_time:31102ms step_avg:60.16ms
step:518/2330 train_time:31166ms step_avg:60.17ms
step:519/2330 train_time:31227ms step_avg:60.17ms
step:520/2330 train_time:31291ms step_avg:60.18ms
step:521/2330 train_time:31350ms step_avg:60.17ms
step:522/2330 train_time:31413ms step_avg:60.18ms
step:523/2330 train_time:31472ms step_avg:60.18ms
step:524/2330 train_time:31533ms step_avg:60.18ms
step:525/2330 train_time:31590ms step_avg:60.17ms
step:526/2330 train_time:31652ms step_avg:60.18ms
step:527/2330 train_time:31710ms step_avg:60.17ms
step:528/2330 train_time:31772ms step_avg:60.17ms
step:529/2330 train_time:31830ms step_avg:60.17ms
step:530/2330 train_time:31892ms step_avg:60.17ms
step:531/2330 train_time:31950ms step_avg:60.17ms
step:532/2330 train_time:32012ms step_avg:60.17ms
step:533/2330 train_time:32071ms step_avg:60.17ms
step:534/2330 train_time:32134ms step_avg:60.18ms
step:535/2330 train_time:32193ms step_avg:60.17ms
step:536/2330 train_time:32255ms step_avg:60.18ms
step:537/2330 train_time:32314ms step_avg:60.17ms
step:538/2330 train_time:32377ms step_avg:60.18ms
step:539/2330 train_time:32436ms step_avg:60.18ms
step:540/2330 train_time:32497ms step_avg:60.18ms
step:541/2330 train_time:32556ms step_avg:60.18ms
step:542/2330 train_time:32618ms step_avg:60.18ms
step:543/2330 train_time:32677ms step_avg:60.18ms
step:544/2330 train_time:32739ms step_avg:60.18ms
step:545/2330 train_time:32797ms step_avg:60.18ms
step:546/2330 train_time:32859ms step_avg:60.18ms
step:547/2330 train_time:32917ms step_avg:60.18ms
step:548/2330 train_time:32980ms step_avg:60.18ms
step:549/2330 train_time:33039ms step_avg:60.18ms
step:550/2330 train_time:33101ms step_avg:60.18ms
step:551/2330 train_time:33159ms step_avg:60.18ms
step:552/2330 train_time:33222ms step_avg:60.18ms
step:553/2330 train_time:33281ms step_avg:60.18ms
step:554/2330 train_time:33343ms step_avg:60.19ms
step:555/2330 train_time:33401ms step_avg:60.18ms
step:556/2330 train_time:33464ms step_avg:60.19ms
step:557/2330 train_time:33523ms step_avg:60.19ms
step:558/2330 train_time:33586ms step_avg:60.19ms
step:559/2330 train_time:33645ms step_avg:60.19ms
step:560/2330 train_time:33709ms step_avg:60.19ms
step:561/2330 train_time:33768ms step_avg:60.19ms
step:562/2330 train_time:33830ms step_avg:60.20ms
step:563/2330 train_time:33888ms step_avg:60.19ms
step:564/2330 train_time:33950ms step_avg:60.20ms
step:565/2330 train_time:34009ms step_avg:60.19ms
step:566/2330 train_time:34071ms step_avg:60.20ms
step:567/2330 train_time:34130ms step_avg:60.19ms
step:568/2330 train_time:34193ms step_avg:60.20ms
step:569/2330 train_time:34251ms step_avg:60.20ms
step:570/2330 train_time:34313ms step_avg:60.20ms
step:571/2330 train_time:34372ms step_avg:60.20ms
step:572/2330 train_time:34435ms step_avg:60.20ms
step:573/2330 train_time:34494ms step_avg:60.20ms
step:574/2330 train_time:34556ms step_avg:60.20ms
step:575/2330 train_time:34615ms step_avg:60.20ms
step:576/2330 train_time:34677ms step_avg:60.20ms
step:577/2330 train_time:34735ms step_avg:60.20ms
step:578/2330 train_time:34797ms step_avg:60.20ms
step:579/2330 train_time:34856ms step_avg:60.20ms
step:580/2330 train_time:34918ms step_avg:60.20ms
step:581/2330 train_time:34977ms step_avg:60.20ms
step:582/2330 train_time:35038ms step_avg:60.20ms
step:583/2330 train_time:35096ms step_avg:60.20ms
step:584/2330 train_time:35158ms step_avg:60.20ms
step:585/2330 train_time:35217ms step_avg:60.20ms
step:586/2330 train_time:35279ms step_avg:60.20ms
step:587/2330 train_time:35337ms step_avg:60.20ms
step:588/2330 train_time:35399ms step_avg:60.20ms
step:589/2330 train_time:35457ms step_avg:60.20ms
step:590/2330 train_time:35520ms step_avg:60.20ms
step:591/2330 train_time:35578ms step_avg:60.20ms
step:592/2330 train_time:35640ms step_avg:60.20ms
step:593/2330 train_time:35698ms step_avg:60.20ms
step:594/2330 train_time:35760ms step_avg:60.20ms
step:595/2330 train_time:35819ms step_avg:60.20ms
step:596/2330 train_time:35880ms step_avg:60.20ms
step:597/2330 train_time:35939ms step_avg:60.20ms
step:598/2330 train_time:36002ms step_avg:60.20ms
step:599/2330 train_time:36061ms step_avg:60.20ms
step:600/2330 train_time:36123ms step_avg:60.21ms
step:601/2330 train_time:36181ms step_avg:60.20ms
step:602/2330 train_time:36243ms step_avg:60.20ms
step:603/2330 train_time:36302ms step_avg:60.20ms
step:604/2330 train_time:36366ms step_avg:60.21ms
step:605/2330 train_time:36424ms step_avg:60.21ms
step:606/2330 train_time:36486ms step_avg:60.21ms
step:607/2330 train_time:36545ms step_avg:60.21ms
step:608/2330 train_time:36607ms step_avg:60.21ms
step:609/2330 train_time:36665ms step_avg:60.21ms
step:610/2330 train_time:36728ms step_avg:60.21ms
step:611/2330 train_time:36786ms step_avg:60.21ms
step:612/2330 train_time:36848ms step_avg:60.21ms
step:613/2330 train_time:36907ms step_avg:60.21ms
step:614/2330 train_time:36969ms step_avg:60.21ms
step:615/2330 train_time:37028ms step_avg:60.21ms
step:616/2330 train_time:37090ms step_avg:60.21ms
step:617/2330 train_time:37149ms step_avg:60.21ms
step:618/2330 train_time:37212ms step_avg:60.21ms
step:619/2330 train_time:37270ms step_avg:60.21ms
step:620/2330 train_time:37333ms step_avg:60.21ms
step:621/2330 train_time:37390ms step_avg:60.21ms
step:622/2330 train_time:37452ms step_avg:60.21ms
step:623/2330 train_time:37511ms step_avg:60.21ms
step:624/2330 train_time:37573ms step_avg:60.21ms
step:625/2330 train_time:37632ms step_avg:60.21ms
step:626/2330 train_time:37694ms step_avg:60.21ms
step:627/2330 train_time:37751ms step_avg:60.21ms
step:628/2330 train_time:37813ms step_avg:60.21ms
step:629/2330 train_time:37872ms step_avg:60.21ms
step:630/2330 train_time:37934ms step_avg:60.21ms
step:631/2330 train_time:37991ms step_avg:60.21ms
step:632/2330 train_time:38054ms step_avg:60.21ms
step:633/2330 train_time:38112ms step_avg:60.21ms
step:634/2330 train_time:38174ms step_avg:60.21ms
step:635/2330 train_time:38232ms step_avg:60.21ms
step:636/2330 train_time:38294ms step_avg:60.21ms
step:637/2330 train_time:38353ms step_avg:60.21ms
step:638/2330 train_time:38414ms step_avg:60.21ms
step:639/2330 train_time:38473ms step_avg:60.21ms
step:640/2330 train_time:38535ms step_avg:60.21ms
step:641/2330 train_time:38594ms step_avg:60.21ms
step:642/2330 train_time:38657ms step_avg:60.21ms
step:643/2330 train_time:38715ms step_avg:60.21ms
step:644/2330 train_time:38778ms step_avg:60.21ms
step:645/2330 train_time:38836ms step_avg:60.21ms
step:646/2330 train_time:38898ms step_avg:60.21ms
step:647/2330 train_time:38956ms step_avg:60.21ms
step:648/2330 train_time:39019ms step_avg:60.21ms
step:649/2330 train_time:39077ms step_avg:60.21ms
step:650/2330 train_time:39138ms step_avg:60.21ms
step:651/2330 train_time:39196ms step_avg:60.21ms
step:652/2330 train_time:39258ms step_avg:60.21ms
step:653/2330 train_time:39316ms step_avg:60.21ms
step:654/2330 train_time:39378ms step_avg:60.21ms
step:655/2330 train_time:39437ms step_avg:60.21ms
step:656/2330 train_time:39499ms step_avg:60.21ms
step:657/2330 train_time:39559ms step_avg:60.21ms
step:658/2330 train_time:39621ms step_avg:60.21ms
step:659/2330 train_time:39679ms step_avg:60.21ms
step:660/2330 train_time:39741ms step_avg:60.21ms
step:661/2330 train_time:39799ms step_avg:60.21ms
step:662/2330 train_time:39861ms step_avg:60.21ms
step:663/2330 train_time:39920ms step_avg:60.21ms
step:664/2330 train_time:39982ms step_avg:60.21ms
step:665/2330 train_time:40041ms step_avg:60.21ms
step:666/2330 train_time:40103ms step_avg:60.21ms
step:667/2330 train_time:40161ms step_avg:60.21ms
step:668/2330 train_time:40223ms step_avg:60.21ms
step:669/2330 train_time:40282ms step_avg:60.21ms
step:670/2330 train_time:40344ms step_avg:60.22ms
step:671/2330 train_time:40403ms step_avg:60.21ms
step:672/2330 train_time:40465ms step_avg:60.22ms
step:673/2330 train_time:40525ms step_avg:60.22ms
step:674/2330 train_time:40588ms step_avg:60.22ms
step:675/2330 train_time:40646ms step_avg:60.22ms
step:676/2330 train_time:40709ms step_avg:60.22ms
step:677/2330 train_time:40767ms step_avg:60.22ms
step:678/2330 train_time:40830ms step_avg:60.22ms
step:679/2330 train_time:40888ms step_avg:60.22ms
step:680/2330 train_time:40950ms step_avg:60.22ms
step:681/2330 train_time:41009ms step_avg:60.22ms
step:682/2330 train_time:41071ms step_avg:60.22ms
step:683/2330 train_time:41130ms step_avg:60.22ms
step:684/2330 train_time:41192ms step_avg:60.22ms
step:685/2330 train_time:41250ms step_avg:60.22ms
step:686/2330 train_time:41312ms step_avg:60.22ms
step:687/2330 train_time:41371ms step_avg:60.22ms
step:688/2330 train_time:41433ms step_avg:60.22ms
step:689/2330 train_time:41492ms step_avg:60.22ms
step:690/2330 train_time:41554ms step_avg:60.22ms
step:691/2330 train_time:41612ms step_avg:60.22ms
step:692/2330 train_time:41674ms step_avg:60.22ms
step:693/2330 train_time:41733ms step_avg:60.22ms
step:694/2330 train_time:41795ms step_avg:60.22ms
step:695/2330 train_time:41853ms step_avg:60.22ms
step:696/2330 train_time:41915ms step_avg:60.22ms
step:697/2330 train_time:41973ms step_avg:60.22ms
step:698/2330 train_time:42035ms step_avg:60.22ms
step:699/2330 train_time:42092ms step_avg:60.22ms
step:700/2330 train_time:42154ms step_avg:60.22ms
step:701/2330 train_time:42212ms step_avg:60.22ms
step:702/2330 train_time:42275ms step_avg:60.22ms
step:703/2330 train_time:42334ms step_avg:60.22ms
step:704/2330 train_time:42396ms step_avg:60.22ms
step:705/2330 train_time:42456ms step_avg:60.22ms
step:706/2330 train_time:42518ms step_avg:60.22ms
step:707/2330 train_time:42577ms step_avg:60.22ms
step:708/2330 train_time:42638ms step_avg:60.22ms
step:709/2330 train_time:42696ms step_avg:60.22ms
step:710/2330 train_time:42758ms step_avg:60.22ms
step:711/2330 train_time:42817ms step_avg:60.22ms
step:712/2330 train_time:42878ms step_avg:60.22ms
step:713/2330 train_time:42937ms step_avg:60.22ms
step:714/2330 train_time:42999ms step_avg:60.22ms
step:715/2330 train_time:43058ms step_avg:60.22ms
step:716/2330 train_time:43121ms step_avg:60.22ms
step:717/2330 train_time:43180ms step_avg:60.22ms
step:718/2330 train_time:43242ms step_avg:60.23ms
step:719/2330 train_time:43301ms step_avg:60.22ms
step:720/2330 train_time:43362ms step_avg:60.23ms
step:721/2330 train_time:43422ms step_avg:60.22ms
step:722/2330 train_time:43484ms step_avg:60.23ms
step:723/2330 train_time:43542ms step_avg:60.22ms
step:724/2330 train_time:43604ms step_avg:60.23ms
step:725/2330 train_time:43663ms step_avg:60.22ms
step:726/2330 train_time:43726ms step_avg:60.23ms
step:727/2330 train_time:43784ms step_avg:60.23ms
step:728/2330 train_time:43847ms step_avg:60.23ms
step:729/2330 train_time:43906ms step_avg:60.23ms
step:730/2330 train_time:43968ms step_avg:60.23ms
step:731/2330 train_time:44027ms step_avg:60.23ms
step:732/2330 train_time:44088ms step_avg:60.23ms
step:733/2330 train_time:44147ms step_avg:60.23ms
step:734/2330 train_time:44209ms step_avg:60.23ms
step:735/2330 train_time:44268ms step_avg:60.23ms
step:736/2330 train_time:44330ms step_avg:60.23ms
step:737/2330 train_time:44389ms step_avg:60.23ms
step:738/2330 train_time:44451ms step_avg:60.23ms
step:739/2330 train_time:44509ms step_avg:60.23ms
step:740/2330 train_time:44571ms step_avg:60.23ms
step:741/2330 train_time:44629ms step_avg:60.23ms
step:742/2330 train_time:44692ms step_avg:60.23ms
step:743/2330 train_time:44750ms step_avg:60.23ms
step:744/2330 train_time:44814ms step_avg:60.23ms
step:745/2330 train_time:44872ms step_avg:60.23ms
step:746/2330 train_time:44935ms step_avg:60.23ms
step:747/2330 train_time:44993ms step_avg:60.23ms
step:748/2330 train_time:45055ms step_avg:60.23ms
step:749/2330 train_time:45113ms step_avg:60.23ms
step:750/2330 train_time:45175ms step_avg:60.23ms
step:750/2330 val_loss:3.9613 train_time:45246ms step_avg:60.33ms
step:751/2330 train_time:45267ms step_avg:60.28ms
step:752/2330 train_time:45298ms step_avg:60.24ms
step:753/2330 train_time:45359ms step_avg:60.24ms
step:754/2330 train_time:45426ms step_avg:60.25ms
step:755/2330 train_time:45486ms step_avg:60.25ms
step:756/2330 train_time:45548ms step_avg:60.25ms
step:757/2330 train_time:45607ms step_avg:60.25ms
step:758/2330 train_time:45669ms step_avg:60.25ms
step:759/2330 train_time:45727ms step_avg:60.25ms
step:760/2330 train_time:45788ms step_avg:60.25ms
step:761/2330 train_time:45846ms step_avg:60.24ms
step:762/2330 train_time:45908ms step_avg:60.25ms
step:763/2330 train_time:45966ms step_avg:60.24ms
step:764/2330 train_time:46026ms step_avg:60.24ms
step:765/2330 train_time:46085ms step_avg:60.24ms
step:766/2330 train_time:46146ms step_avg:60.24ms
step:767/2330 train_time:46205ms step_avg:60.24ms
step:768/2330 train_time:46268ms step_avg:60.24ms
step:769/2330 train_time:46328ms step_avg:60.24ms
step:770/2330 train_time:46394ms step_avg:60.25ms
step:771/2330 train_time:46452ms step_avg:60.25ms
step:772/2330 train_time:46517ms step_avg:60.25ms
step:773/2330 train_time:46578ms step_avg:60.26ms
step:774/2330 train_time:46640ms step_avg:60.26ms
step:775/2330 train_time:46699ms step_avg:60.26ms
step:776/2330 train_time:46763ms step_avg:60.26ms
step:777/2330 train_time:46822ms step_avg:60.26ms
step:778/2330 train_time:46884ms step_avg:60.26ms
step:779/2330 train_time:46943ms step_avg:60.26ms
step:780/2330 train_time:47005ms step_avg:60.26ms
step:781/2330 train_time:47064ms step_avg:60.26ms
step:782/2330 train_time:47126ms step_avg:60.26ms
step:783/2330 train_time:47184ms step_avg:60.26ms
step:784/2330 train_time:47246ms step_avg:60.26ms
step:785/2330 train_time:47307ms step_avg:60.26ms
step:786/2330 train_time:47373ms step_avg:60.27ms
step:787/2330 train_time:47431ms step_avg:60.27ms
step:788/2330 train_time:47495ms step_avg:60.27ms
step:789/2330 train_time:47554ms step_avg:60.27ms
step:790/2330 train_time:47617ms step_avg:60.27ms
step:791/2330 train_time:47676ms step_avg:60.27ms
step:792/2330 train_time:47739ms step_avg:60.28ms
step:793/2330 train_time:47798ms step_avg:60.28ms
step:794/2330 train_time:47862ms step_avg:60.28ms
step:795/2330 train_time:47921ms step_avg:60.28ms
step:796/2330 train_time:47984ms step_avg:60.28ms
step:797/2330 train_time:48042ms step_avg:60.28ms
step:798/2330 train_time:48105ms step_avg:60.28ms
step:799/2330 train_time:48164ms step_avg:60.28ms
step:800/2330 train_time:48226ms step_avg:60.28ms
step:801/2330 train_time:48286ms step_avg:60.28ms
step:802/2330 train_time:48349ms step_avg:60.29ms
step:803/2330 train_time:48409ms step_avg:60.28ms
step:804/2330 train_time:48472ms step_avg:60.29ms
step:805/2330 train_time:48531ms step_avg:60.29ms
step:806/2330 train_time:48594ms step_avg:60.29ms
step:807/2330 train_time:48655ms step_avg:60.29ms
step:808/2330 train_time:48718ms step_avg:60.29ms
step:809/2330 train_time:48777ms step_avg:60.29ms
step:810/2330 train_time:48839ms step_avg:60.30ms
step:811/2330 train_time:48899ms step_avg:60.29ms
step:812/2330 train_time:48961ms step_avg:60.30ms
step:813/2330 train_time:49022ms step_avg:60.30ms
step:814/2330 train_time:49084ms step_avg:60.30ms
step:815/2330 train_time:49144ms step_avg:60.30ms
step:816/2330 train_time:49206ms step_avg:60.30ms
step:817/2330 train_time:49265ms step_avg:60.30ms
step:818/2330 train_time:49328ms step_avg:60.30ms
step:819/2330 train_time:49387ms step_avg:60.30ms
step:820/2330 train_time:49451ms step_avg:60.31ms
step:821/2330 train_time:49510ms step_avg:60.30ms
step:822/2330 train_time:49574ms step_avg:60.31ms
step:823/2330 train_time:49632ms step_avg:60.31ms
step:824/2330 train_time:49695ms step_avg:60.31ms
step:825/2330 train_time:49754ms step_avg:60.31ms
step:826/2330 train_time:49816ms step_avg:60.31ms
step:827/2330 train_time:49875ms step_avg:60.31ms
step:828/2330 train_time:49938ms step_avg:60.31ms
step:829/2330 train_time:49998ms step_avg:60.31ms
step:830/2330 train_time:50062ms step_avg:60.32ms
step:831/2330 train_time:50120ms step_avg:60.31ms
step:832/2330 train_time:50183ms step_avg:60.32ms
step:833/2330 train_time:50243ms step_avg:60.32ms
step:834/2330 train_time:50306ms step_avg:60.32ms
step:835/2330 train_time:50366ms step_avg:60.32ms
step:836/2330 train_time:50428ms step_avg:60.32ms
step:837/2330 train_time:50487ms step_avg:60.32ms
step:838/2330 train_time:50551ms step_avg:60.32ms
step:839/2330 train_time:50610ms step_avg:60.32ms
step:840/2330 train_time:50673ms step_avg:60.33ms
step:841/2330 train_time:50731ms step_avg:60.32ms
step:842/2330 train_time:50794ms step_avg:60.33ms
step:843/2330 train_time:50853ms step_avg:60.32ms
step:844/2330 train_time:50916ms step_avg:60.33ms
step:845/2330 train_time:50975ms step_avg:60.33ms
step:846/2330 train_time:51038ms step_avg:60.33ms
step:847/2330 train_time:51098ms step_avg:60.33ms
step:848/2330 train_time:51161ms step_avg:60.33ms
step:849/2330 train_time:51220ms step_avg:60.33ms
step:850/2330 train_time:51283ms step_avg:60.33ms
step:851/2330 train_time:51342ms step_avg:60.33ms
step:852/2330 train_time:51405ms step_avg:60.33ms
step:853/2330 train_time:51465ms step_avg:60.33ms
step:854/2330 train_time:51528ms step_avg:60.34ms
step:855/2330 train_time:51586ms step_avg:60.34ms
step:856/2330 train_time:51649ms step_avg:60.34ms
step:857/2330 train_time:51709ms step_avg:60.34ms
step:858/2330 train_time:51773ms step_avg:60.34ms
step:859/2330 train_time:51831ms step_avg:60.34ms
step:860/2330 train_time:51894ms step_avg:60.34ms
step:861/2330 train_time:51952ms step_avg:60.34ms
step:862/2330 train_time:52015ms step_avg:60.34ms
step:863/2330 train_time:52074ms step_avg:60.34ms
step:864/2330 train_time:52137ms step_avg:60.34ms
step:865/2330 train_time:52197ms step_avg:60.34ms
step:866/2330 train_time:52261ms step_avg:60.35ms
step:867/2330 train_time:52320ms step_avg:60.35ms
step:868/2330 train_time:52383ms step_avg:60.35ms
step:869/2330 train_time:52442ms step_avg:60.35ms
step:870/2330 train_time:52505ms step_avg:60.35ms
step:871/2330 train_time:52565ms step_avg:60.35ms
step:872/2330 train_time:52627ms step_avg:60.35ms
step:873/2330 train_time:52686ms step_avg:60.35ms
step:874/2330 train_time:52750ms step_avg:60.35ms
step:875/2330 train_time:52808ms step_avg:60.35ms
step:876/2330 train_time:52872ms step_avg:60.36ms
step:877/2330 train_time:52930ms step_avg:60.35ms
step:878/2330 train_time:52993ms step_avg:60.36ms
step:879/2330 train_time:53051ms step_avg:60.35ms
step:880/2330 train_time:53114ms step_avg:60.36ms
step:881/2330 train_time:53174ms step_avg:60.36ms
step:882/2330 train_time:53238ms step_avg:60.36ms
step:883/2330 train_time:53297ms step_avg:60.36ms
step:884/2330 train_time:53361ms step_avg:60.36ms
step:885/2330 train_time:53420ms step_avg:60.36ms
step:886/2330 train_time:53483ms step_avg:60.36ms
step:887/2330 train_time:53543ms step_avg:60.36ms
step:888/2330 train_time:53605ms step_avg:60.37ms
step:889/2330 train_time:53665ms step_avg:60.37ms
step:890/2330 train_time:53727ms step_avg:60.37ms
step:891/2330 train_time:53786ms step_avg:60.37ms
step:892/2330 train_time:53850ms step_avg:60.37ms
step:893/2330 train_time:53909ms step_avg:60.37ms
step:894/2330 train_time:53972ms step_avg:60.37ms
step:895/2330 train_time:54030ms step_avg:60.37ms
step:896/2330 train_time:54093ms step_avg:60.37ms
step:897/2330 train_time:54152ms step_avg:60.37ms
step:898/2330 train_time:54216ms step_avg:60.37ms
step:899/2330 train_time:54275ms step_avg:60.37ms
step:900/2330 train_time:54338ms step_avg:60.38ms
step:901/2330 train_time:54398ms step_avg:60.37ms
step:902/2330 train_time:54460ms step_avg:60.38ms
step:903/2330 train_time:54519ms step_avg:60.38ms
step:904/2330 train_time:54582ms step_avg:60.38ms
step:905/2330 train_time:54642ms step_avg:60.38ms
step:906/2330 train_time:54705ms step_avg:60.38ms
step:907/2330 train_time:54765ms step_avg:60.38ms
step:908/2330 train_time:54828ms step_avg:60.38ms
step:909/2330 train_time:54887ms step_avg:60.38ms
step:910/2330 train_time:54950ms step_avg:60.38ms
step:911/2330 train_time:55009ms step_avg:60.38ms
step:912/2330 train_time:55072ms step_avg:60.39ms
step:913/2330 train_time:55130ms step_avg:60.38ms
step:914/2330 train_time:55194ms step_avg:60.39ms
step:915/2330 train_time:55253ms step_avg:60.39ms
step:916/2330 train_time:55315ms step_avg:60.39ms
step:917/2330 train_time:55375ms step_avg:60.39ms
step:918/2330 train_time:55438ms step_avg:60.39ms
step:919/2330 train_time:55497ms step_avg:60.39ms
step:920/2330 train_time:55560ms step_avg:60.39ms
step:921/2330 train_time:55619ms step_avg:60.39ms
step:922/2330 train_time:55683ms step_avg:60.39ms
step:923/2330 train_time:55742ms step_avg:60.39ms
step:924/2330 train_time:55805ms step_avg:60.40ms
step:925/2330 train_time:55866ms step_avg:60.40ms
step:926/2330 train_time:55927ms step_avg:60.40ms
step:927/2330 train_time:55987ms step_avg:60.40ms
step:928/2330 train_time:56050ms step_avg:60.40ms
step:929/2330 train_time:56110ms step_avg:60.40ms
step:930/2330 train_time:56173ms step_avg:60.40ms
step:931/2330 train_time:56231ms step_avg:60.40ms
step:932/2330 train_time:56294ms step_avg:60.40ms
step:933/2330 train_time:56353ms step_avg:60.40ms
step:934/2330 train_time:56416ms step_avg:60.40ms
step:935/2330 train_time:56475ms step_avg:60.40ms
step:936/2330 train_time:56538ms step_avg:60.40ms
step:937/2330 train_time:56597ms step_avg:60.40ms
step:938/2330 train_time:56659ms step_avg:60.40ms
step:939/2330 train_time:56719ms step_avg:60.40ms
step:940/2330 train_time:56783ms step_avg:60.41ms
step:941/2330 train_time:56842ms step_avg:60.41ms
step:942/2330 train_time:56905ms step_avg:60.41ms
step:943/2330 train_time:56965ms step_avg:60.41ms
step:944/2330 train_time:57028ms step_avg:60.41ms
step:945/2330 train_time:57087ms step_avg:60.41ms
step:946/2330 train_time:57151ms step_avg:60.41ms
step:947/2330 train_time:57210ms step_avg:60.41ms
step:948/2330 train_time:57273ms step_avg:60.41ms
step:949/2330 train_time:57332ms step_avg:60.41ms
step:950/2330 train_time:57394ms step_avg:60.41ms
step:951/2330 train_time:57452ms step_avg:60.41ms
step:952/2330 train_time:57515ms step_avg:60.42ms
step:953/2330 train_time:57576ms step_avg:60.42ms
step:954/2330 train_time:57638ms step_avg:60.42ms
step:955/2330 train_time:57698ms step_avg:60.42ms
step:956/2330 train_time:57761ms step_avg:60.42ms
step:957/2330 train_time:57820ms step_avg:60.42ms
step:958/2330 train_time:57883ms step_avg:60.42ms
step:959/2330 train_time:57943ms step_avg:60.42ms
step:960/2330 train_time:58006ms step_avg:60.42ms
step:961/2330 train_time:58066ms step_avg:60.42ms
step:962/2330 train_time:58129ms step_avg:60.42ms
step:963/2330 train_time:58188ms step_avg:60.42ms
step:964/2330 train_time:58252ms step_avg:60.43ms
step:965/2330 train_time:58311ms step_avg:60.43ms
step:966/2330 train_time:58373ms step_avg:60.43ms
step:967/2330 train_time:58432ms step_avg:60.43ms
step:968/2330 train_time:58495ms step_avg:60.43ms
step:969/2330 train_time:58554ms step_avg:60.43ms
step:970/2330 train_time:58617ms step_avg:60.43ms
step:971/2330 train_time:58676ms step_avg:60.43ms
step:972/2330 train_time:58739ms step_avg:60.43ms
step:973/2330 train_time:58799ms step_avg:60.43ms
step:974/2330 train_time:58861ms step_avg:60.43ms
step:975/2330 train_time:58921ms step_avg:60.43ms
step:976/2330 train_time:58984ms step_avg:60.43ms
step:977/2330 train_time:59044ms step_avg:60.43ms
step:978/2330 train_time:59106ms step_avg:60.44ms
step:979/2330 train_time:59167ms step_avg:60.44ms
step:980/2330 train_time:59229ms step_avg:60.44ms
step:981/2330 train_time:59288ms step_avg:60.44ms
step:982/2330 train_time:59351ms step_avg:60.44ms
step:983/2330 train_time:59410ms step_avg:60.44ms
step:984/2330 train_time:59473ms step_avg:60.44ms
step:985/2330 train_time:59531ms step_avg:60.44ms
step:986/2330 train_time:59594ms step_avg:60.44ms
step:987/2330 train_time:59653ms step_avg:60.44ms
step:988/2330 train_time:59715ms step_avg:60.44ms
step:989/2330 train_time:59775ms step_avg:60.44ms
step:990/2330 train_time:59839ms step_avg:60.44ms
step:991/2330 train_time:59898ms step_avg:60.44ms
step:992/2330 train_time:59961ms step_avg:60.44ms
step:993/2330 train_time:60021ms step_avg:60.44ms
step:994/2330 train_time:60084ms step_avg:60.45ms
step:995/2330 train_time:60144ms step_avg:60.45ms
step:996/2330 train_time:60207ms step_avg:60.45ms
step:997/2330 train_time:60267ms step_avg:60.45ms
step:998/2330 train_time:60329ms step_avg:60.45ms
step:999/2330 train_time:60389ms step_avg:60.45ms
step:1000/2330 train_time:60452ms step_avg:60.45ms
step:1000/2330 val_loss:3.8161 train_time:60525ms step_avg:60.52ms
step:1001/2330 train_time:60546ms step_avg:60.49ms
step:1002/2330 train_time:60576ms step_avg:60.46ms
step:1003/2330 train_time:60636ms step_avg:60.45ms
step:1004/2330 train_time:60704ms step_avg:60.46ms
step:1005/2330 train_time:60766ms step_avg:60.46ms
step:1006/2330 train_time:60828ms step_avg:60.47ms
step:1007/2330 train_time:60887ms step_avg:60.46ms
step:1008/2330 train_time:60949ms step_avg:60.46ms
step:1009/2330 train_time:61007ms step_avg:60.46ms
step:1010/2330 train_time:61070ms step_avg:60.46ms
step:1011/2330 train_time:61129ms step_avg:60.46ms
step:1012/2330 train_time:61191ms step_avg:60.47ms
step:1013/2330 train_time:61249ms step_avg:60.46ms
step:1014/2330 train_time:61312ms step_avg:60.47ms
step:1015/2330 train_time:61370ms step_avg:60.46ms
step:1016/2330 train_time:61433ms step_avg:60.47ms
step:1017/2330 train_time:61494ms step_avg:60.47ms
step:1018/2330 train_time:61558ms step_avg:60.47ms
step:1019/2330 train_time:61619ms step_avg:60.47ms
step:1020/2330 train_time:61684ms step_avg:60.47ms
step:1021/2330 train_time:61744ms step_avg:60.47ms
step:1022/2330 train_time:61806ms step_avg:60.48ms
step:1023/2330 train_time:61866ms step_avg:60.47ms
step:1024/2330 train_time:61928ms step_avg:60.48ms
step:1025/2330 train_time:61987ms step_avg:60.48ms
step:1026/2330 train_time:62049ms step_avg:60.48ms
step:1027/2330 train_time:62108ms step_avg:60.47ms
step:1028/2330 train_time:62170ms step_avg:60.48ms
step:1029/2330 train_time:62230ms step_avg:60.48ms
step:1030/2330 train_time:62292ms step_avg:60.48ms
step:1031/2330 train_time:62351ms step_avg:60.48ms
step:1032/2330 train_time:62414ms step_avg:60.48ms
step:1033/2330 train_time:62474ms step_avg:60.48ms
step:1034/2330 train_time:62538ms step_avg:60.48ms
step:1035/2330 train_time:62597ms step_avg:60.48ms
step:1036/2330 train_time:62660ms step_avg:60.48ms
step:1037/2330 train_time:62721ms step_avg:60.48ms
step:1038/2330 train_time:62784ms step_avg:60.49ms
step:1039/2330 train_time:62844ms step_avg:60.48ms
step:1040/2330 train_time:62906ms step_avg:60.49ms
step:1041/2330 train_time:62965ms step_avg:60.49ms
step:1042/2330 train_time:63028ms step_avg:60.49ms
step:1043/2330 train_time:63087ms step_avg:60.49ms
step:1044/2330 train_time:63149ms step_avg:60.49ms
step:1045/2330 train_time:63208ms step_avg:60.49ms
step:1046/2330 train_time:63271ms step_avg:60.49ms
step:1047/2330 train_time:63330ms step_avg:60.49ms
step:1048/2330 train_time:63394ms step_avg:60.49ms
step:1049/2330 train_time:63452ms step_avg:60.49ms
step:1050/2330 train_time:63515ms step_avg:60.49ms
step:1051/2330 train_time:63575ms step_avg:60.49ms
step:1052/2330 train_time:63638ms step_avg:60.49ms
step:1053/2330 train_time:63698ms step_avg:60.49ms
step:1054/2330 train_time:63762ms step_avg:60.50ms
step:1055/2330 train_time:63822ms step_avg:60.50ms
step:1056/2330 train_time:63885ms step_avg:60.50ms
step:1057/2330 train_time:63944ms step_avg:60.50ms
step:1058/2330 train_time:64006ms step_avg:60.50ms
step:1059/2330 train_time:64066ms step_avg:60.50ms
step:1060/2330 train_time:64129ms step_avg:60.50ms
step:1061/2330 train_time:64188ms step_avg:60.50ms
step:1062/2330 train_time:64252ms step_avg:60.50ms
step:1063/2330 train_time:64311ms step_avg:60.50ms
step:1064/2330 train_time:64374ms step_avg:60.50ms
step:1065/2330 train_time:64434ms step_avg:60.50ms
step:1066/2330 train_time:64496ms step_avg:60.50ms
step:1067/2330 train_time:64555ms step_avg:60.50ms
step:1068/2330 train_time:64618ms step_avg:60.50ms
step:1069/2330 train_time:64678ms step_avg:60.50ms
step:1070/2330 train_time:64741ms step_avg:60.51ms
step:1071/2330 train_time:64800ms step_avg:60.50ms
step:1072/2330 train_time:64862ms step_avg:60.51ms
step:1073/2330 train_time:64922ms step_avg:60.51ms
step:1074/2330 train_time:64985ms step_avg:60.51ms
step:1075/2330 train_time:65045ms step_avg:60.51ms
step:1076/2330 train_time:65108ms step_avg:60.51ms
step:1077/2330 train_time:65167ms step_avg:60.51ms
step:1078/2330 train_time:65229ms step_avg:60.51ms
step:1079/2330 train_time:65290ms step_avg:60.51ms
step:1080/2330 train_time:65351ms step_avg:60.51ms
step:1081/2330 train_time:65411ms step_avg:60.51ms
step:1082/2330 train_time:65474ms step_avg:60.51ms
step:1083/2330 train_time:65534ms step_avg:60.51ms
step:1084/2330 train_time:65596ms step_avg:60.51ms
step:1085/2330 train_time:65655ms step_avg:60.51ms
step:1086/2330 train_time:65718ms step_avg:60.51ms
step:1087/2330 train_time:65777ms step_avg:60.51ms
step:1088/2330 train_time:65840ms step_avg:60.52ms
step:1089/2330 train_time:65900ms step_avg:60.51ms
step:1090/2330 train_time:65963ms step_avg:60.52ms
step:1091/2330 train_time:66023ms step_avg:60.52ms
step:1092/2330 train_time:66086ms step_avg:60.52ms
step:1093/2330 train_time:66145ms step_avg:60.52ms
step:1094/2330 train_time:66209ms step_avg:60.52ms
step:1095/2330 train_time:66269ms step_avg:60.52ms
step:1096/2330 train_time:66332ms step_avg:60.52ms
step:1097/2330 train_time:66391ms step_avg:60.52ms
step:1098/2330 train_time:66454ms step_avg:60.52ms
step:1099/2330 train_time:66513ms step_avg:60.52ms
step:1100/2330 train_time:66576ms step_avg:60.52ms
step:1101/2330 train_time:66635ms step_avg:60.52ms
step:1102/2330 train_time:66698ms step_avg:60.52ms
step:1103/2330 train_time:66758ms step_avg:60.52ms
step:1104/2330 train_time:66820ms step_avg:60.53ms
step:1105/2330 train_time:66879ms step_avg:60.52ms
step:1106/2330 train_time:66943ms step_avg:60.53ms
step:1107/2330 train_time:67002ms step_avg:60.53ms
step:1108/2330 train_time:67065ms step_avg:60.53ms
step:1109/2330 train_time:67124ms step_avg:60.53ms
step:1110/2330 train_time:67188ms step_avg:60.53ms
step:1111/2330 train_time:67247ms step_avg:60.53ms
step:1112/2330 train_time:67310ms step_avg:60.53ms
step:1113/2330 train_time:67370ms step_avg:60.53ms
step:1114/2330 train_time:67434ms step_avg:60.53ms
step:1115/2330 train_time:67493ms step_avg:60.53ms
step:1116/2330 train_time:67556ms step_avg:60.53ms
step:1117/2330 train_time:67615ms step_avg:60.53ms
step:1118/2330 train_time:67677ms step_avg:60.53ms
step:1119/2330 train_time:67737ms step_avg:60.53ms
step:1120/2330 train_time:67800ms step_avg:60.54ms
step:1121/2330 train_time:67859ms step_avg:60.53ms
step:1122/2330 train_time:67922ms step_avg:60.54ms
step:1123/2330 train_time:67981ms step_avg:60.53ms
step:1124/2330 train_time:68044ms step_avg:60.54ms
step:1125/2330 train_time:68104ms step_avg:60.54ms
step:1126/2330 train_time:68167ms step_avg:60.54ms
step:1127/2330 train_time:68227ms step_avg:60.54ms
step:1128/2330 train_time:68291ms step_avg:60.54ms
step:1129/2330 train_time:68349ms step_avg:60.54ms
step:1130/2330 train_time:68413ms step_avg:60.54ms
step:1131/2330 train_time:68472ms step_avg:60.54ms
step:1132/2330 train_time:68535ms step_avg:60.54ms
step:1133/2330 train_time:68595ms step_avg:60.54ms
step:1134/2330 train_time:68656ms step_avg:60.54ms
step:1135/2330 train_time:68717ms step_avg:60.54ms
step:1136/2330 train_time:68780ms step_avg:60.55ms
step:1137/2330 train_time:68839ms step_avg:60.54ms
step:1138/2330 train_time:68902ms step_avg:60.55ms
step:1139/2330 train_time:68961ms step_avg:60.54ms
step:1140/2330 train_time:69023ms step_avg:60.55ms
step:1141/2330 train_time:69083ms step_avg:60.55ms
step:1142/2330 train_time:69146ms step_avg:60.55ms
step:1143/2330 train_time:69206ms step_avg:60.55ms
step:1144/2330 train_time:69268ms step_avg:60.55ms
step:1145/2330 train_time:69328ms step_avg:60.55ms
step:1146/2330 train_time:69391ms step_avg:60.55ms
step:1147/2330 train_time:69451ms step_avg:60.55ms
step:1148/2330 train_time:69514ms step_avg:60.55ms
step:1149/2330 train_time:69573ms step_avg:60.55ms
step:1150/2330 train_time:69636ms step_avg:60.55ms
step:1151/2330 train_time:69695ms step_avg:60.55ms
step:1152/2330 train_time:69758ms step_avg:60.55ms
step:1153/2330 train_time:69818ms step_avg:60.55ms
step:1154/2330 train_time:69882ms step_avg:60.56ms
step:1155/2330 train_time:69941ms step_avg:60.56ms
step:1156/2330 train_time:70003ms step_avg:60.56ms
step:1157/2330 train_time:70063ms step_avg:60.56ms
step:1158/2330 train_time:70126ms step_avg:60.56ms
step:1159/2330 train_time:70187ms step_avg:60.56ms
step:1160/2330 train_time:70249ms step_avg:60.56ms
step:1161/2330 train_time:70309ms step_avg:60.56ms
step:1162/2330 train_time:70373ms step_avg:60.56ms
step:1163/2330 train_time:70432ms step_avg:60.56ms
step:1164/2330 train_time:70496ms step_avg:60.56ms
step:1165/2330 train_time:70554ms step_avg:60.56ms
step:1166/2330 train_time:70616ms step_avg:60.56ms
step:1167/2330 train_time:70676ms step_avg:60.56ms
step:1168/2330 train_time:70739ms step_avg:60.56ms
step:1169/2330 train_time:70798ms step_avg:60.56ms
step:1170/2330 train_time:70861ms step_avg:60.56ms
step:1171/2330 train_time:70920ms step_avg:60.56ms
step:1172/2330 train_time:70983ms step_avg:60.57ms
step:1173/2330 train_time:71042ms step_avg:60.56ms
step:1174/2330 train_time:71105ms step_avg:60.57ms
step:1175/2330 train_time:71165ms step_avg:60.57ms
step:1176/2330 train_time:71227ms step_avg:60.57ms
step:1177/2330 train_time:71288ms step_avg:60.57ms
step:1178/2330 train_time:71351ms step_avg:60.57ms
step:1179/2330 train_time:71410ms step_avg:60.57ms
step:1180/2330 train_time:71472ms step_avg:60.57ms
step:1181/2330 train_time:71532ms step_avg:60.57ms
step:1182/2330 train_time:71595ms step_avg:60.57ms
step:1183/2330 train_time:71653ms step_avg:60.57ms
step:1184/2330 train_time:71716ms step_avg:60.57ms
step:1185/2330 train_time:71776ms step_avg:60.57ms
step:1186/2330 train_time:71839ms step_avg:60.57ms
step:1187/2330 train_time:71898ms step_avg:60.57ms
step:1188/2330 train_time:71962ms step_avg:60.57ms
step:1189/2330 train_time:72021ms step_avg:60.57ms
step:1190/2330 train_time:72084ms step_avg:60.57ms
step:1191/2330 train_time:72143ms step_avg:60.57ms
step:1192/2330 train_time:72206ms step_avg:60.58ms
step:1193/2330 train_time:72266ms step_avg:60.58ms
step:1194/2330 train_time:72330ms step_avg:60.58ms
step:1195/2330 train_time:72390ms step_avg:60.58ms
step:1196/2330 train_time:72452ms step_avg:60.58ms
step:1197/2330 train_time:72511ms step_avg:60.58ms
step:1198/2330 train_time:72574ms step_avg:60.58ms
step:1199/2330 train_time:72634ms step_avg:60.58ms
step:1200/2330 train_time:72697ms step_avg:60.58ms
step:1201/2330 train_time:72756ms step_avg:60.58ms
step:1202/2330 train_time:72820ms step_avg:60.58ms
step:1203/2330 train_time:72880ms step_avg:60.58ms
step:1204/2330 train_time:72942ms step_avg:60.58ms
step:1205/2330 train_time:73001ms step_avg:60.58ms
step:1206/2330 train_time:73063ms step_avg:60.58ms
step:1207/2330 train_time:73123ms step_avg:60.58ms
step:1208/2330 train_time:73186ms step_avg:60.58ms
step:1209/2330 train_time:73245ms step_avg:60.58ms
step:1210/2330 train_time:73308ms step_avg:60.59ms
step:1211/2330 train_time:73368ms step_avg:60.58ms
step:1212/2330 train_time:73431ms step_avg:60.59ms
step:1213/2330 train_time:73491ms step_avg:60.59ms
step:1214/2330 train_time:73554ms step_avg:60.59ms
step:1215/2330 train_time:73613ms step_avg:60.59ms
step:1216/2330 train_time:73676ms step_avg:60.59ms
step:1217/2330 train_time:73736ms step_avg:60.59ms
step:1218/2330 train_time:73799ms step_avg:60.59ms
step:1219/2330 train_time:73857ms step_avg:60.59ms
step:1220/2330 train_time:73920ms step_avg:60.59ms
step:1221/2330 train_time:73978ms step_avg:60.59ms
step:1222/2330 train_time:74042ms step_avg:60.59ms
step:1223/2330 train_time:74101ms step_avg:60.59ms
step:1224/2330 train_time:74164ms step_avg:60.59ms
step:1225/2330 train_time:74224ms step_avg:60.59ms
step:1226/2330 train_time:74288ms step_avg:60.59ms
step:1227/2330 train_time:74347ms step_avg:60.59ms
step:1228/2330 train_time:74409ms step_avg:60.59ms
step:1229/2330 train_time:74468ms step_avg:60.59ms
step:1230/2330 train_time:74532ms step_avg:60.59ms
step:1231/2330 train_time:74592ms step_avg:60.59ms
step:1232/2330 train_time:74655ms step_avg:60.60ms
step:1233/2330 train_time:74714ms step_avg:60.60ms
step:1234/2330 train_time:74776ms step_avg:60.60ms
step:1235/2330 train_time:74835ms step_avg:60.59ms
step:1236/2330 train_time:74897ms step_avg:60.60ms
step:1237/2330 train_time:74956ms step_avg:60.60ms
step:1238/2330 train_time:75019ms step_avg:60.60ms
step:1239/2330 train_time:75079ms step_avg:60.60ms
step:1240/2330 train_time:75142ms step_avg:60.60ms
step:1241/2330 train_time:75201ms step_avg:60.60ms
step:1242/2330 train_time:75264ms step_avg:60.60ms
step:1243/2330 train_time:75323ms step_avg:60.60ms
step:1244/2330 train_time:75387ms step_avg:60.60ms
step:1245/2330 train_time:75447ms step_avg:60.60ms
step:1246/2330 train_time:75511ms step_avg:60.60ms
step:1247/2330 train_time:75570ms step_avg:60.60ms
step:1248/2330 train_time:75634ms step_avg:60.60ms
step:1249/2330 train_time:75693ms step_avg:60.60ms
step:1250/2330 train_time:75755ms step_avg:60.60ms
step:1250/2330 val_loss:3.7330 train_time:75827ms step_avg:60.66ms
step:1251/2330 train_time:75848ms step_avg:60.63ms
step:1252/2330 train_time:75880ms step_avg:60.61ms
step:1253/2330 train_time:75945ms step_avg:60.61ms
step:1254/2330 train_time:76012ms step_avg:60.62ms
step:1255/2330 train_time:76072ms step_avg:60.62ms
step:1256/2330 train_time:76135ms step_avg:60.62ms
step:1257/2330 train_time:76195ms step_avg:60.62ms
step:1258/2330 train_time:76257ms step_avg:60.62ms
step:1259/2330 train_time:76316ms step_avg:60.62ms
step:1260/2330 train_time:76378ms step_avg:60.62ms
step:1261/2330 train_time:76437ms step_avg:60.62ms
step:1262/2330 train_time:76499ms step_avg:60.62ms
step:1263/2330 train_time:76558ms step_avg:60.62ms
step:1264/2330 train_time:76620ms step_avg:60.62ms
step:1265/2330 train_time:76678ms step_avg:60.61ms
step:1266/2330 train_time:76741ms step_avg:60.62ms
step:1267/2330 train_time:76802ms step_avg:60.62ms
step:1268/2330 train_time:76866ms step_avg:60.62ms
step:1269/2330 train_time:76927ms step_avg:60.62ms
step:1270/2330 train_time:76992ms step_avg:60.62ms
step:1271/2330 train_time:77051ms step_avg:60.62ms
step:1272/2330 train_time:77115ms step_avg:60.62ms
step:1273/2330 train_time:77174ms step_avg:60.62ms
step:1274/2330 train_time:77236ms step_avg:60.63ms
step:1275/2330 train_time:77296ms step_avg:60.62ms
step:1276/2330 train_time:77358ms step_avg:60.63ms
step:1277/2330 train_time:77417ms step_avg:60.62ms
step:1278/2330 train_time:77479ms step_avg:60.63ms
step:1279/2330 train_time:77538ms step_avg:60.62ms
step:1280/2330 train_time:77600ms step_avg:60.63ms
step:1281/2330 train_time:77660ms step_avg:60.62ms
step:1282/2330 train_time:77723ms step_avg:60.63ms
step:1283/2330 train_time:77782ms step_avg:60.63ms
step:1284/2330 train_time:77847ms step_avg:60.63ms
step:1285/2330 train_time:77908ms step_avg:60.63ms
step:1286/2330 train_time:77970ms step_avg:60.63ms
step:1287/2330 train_time:78030ms step_avg:60.63ms
step:1288/2330 train_time:78094ms step_avg:60.63ms
step:1289/2330 train_time:78153ms step_avg:60.63ms
step:1290/2330 train_time:78216ms step_avg:60.63ms
step:1291/2330 train_time:78275ms step_avg:60.63ms
step:1292/2330 train_time:78338ms step_avg:60.63ms
step:1293/2330 train_time:78396ms step_avg:60.63ms
step:1294/2330 train_time:78458ms step_avg:60.63ms
step:1295/2330 train_time:78517ms step_avg:60.63ms
step:1296/2330 train_time:78579ms step_avg:60.63ms
step:1297/2330 train_time:78638ms step_avg:60.63ms
step:1298/2330 train_time:78701ms step_avg:60.63ms
step:1299/2330 train_time:78761ms step_avg:60.63ms
step:1300/2330 train_time:78825ms step_avg:60.63ms
step:1301/2330 train_time:78885ms step_avg:60.63ms
step:1302/2330 train_time:78948ms step_avg:60.64ms
step:1303/2330 train_time:79009ms step_avg:60.64ms
step:1304/2330 train_time:79071ms step_avg:60.64ms
step:1305/2330 train_time:79130ms step_avg:60.64ms
step:1306/2330 train_time:79193ms step_avg:60.64ms
step:1307/2330 train_time:79253ms step_avg:60.64ms
step:1308/2330 train_time:79315ms step_avg:60.64ms
step:1309/2330 train_time:79374ms step_avg:60.64ms
step:1310/2330 train_time:79437ms step_avg:60.64ms
step:1311/2330 train_time:79496ms step_avg:60.64ms
step:1312/2330 train_time:79558ms step_avg:60.64ms
step:1313/2330 train_time:79616ms step_avg:60.64ms
step:1314/2330 train_time:79679ms step_avg:60.64ms
step:1315/2330 train_time:79738ms step_avg:60.64ms
step:1316/2330 train_time:79802ms step_avg:60.64ms
step:1317/2330 train_time:79862ms step_avg:60.64ms
step:1318/2330 train_time:79925ms step_avg:60.64ms
step:1319/2330 train_time:79985ms step_avg:60.64ms
step:1320/2330 train_time:80049ms step_avg:60.64ms
step:1321/2330 train_time:80110ms step_avg:60.64ms
step:1322/2330 train_time:80173ms step_avg:60.64ms
step:1323/2330 train_time:80232ms step_avg:60.64ms
step:1324/2330 train_time:80295ms step_avg:60.65ms
step:1325/2330 train_time:80354ms step_avg:60.64ms
step:1326/2330 train_time:80416ms step_avg:60.65ms
step:1327/2330 train_time:80474ms step_avg:60.64ms
step:1328/2330 train_time:80537ms step_avg:60.65ms
step:1329/2330 train_time:80596ms step_avg:60.64ms
step:1330/2330 train_time:80658ms step_avg:60.65ms
step:1331/2330 train_time:80718ms step_avg:60.64ms
step:1332/2330 train_time:80781ms step_avg:60.65ms
step:1333/2330 train_time:80840ms step_avg:60.65ms
step:1334/2330 train_time:80903ms step_avg:60.65ms
step:1335/2330 train_time:80964ms step_avg:60.65ms
step:1336/2330 train_time:81027ms step_avg:60.65ms
step:1337/2330 train_time:81086ms step_avg:60.65ms
step:1338/2330 train_time:81149ms step_avg:60.65ms
step:1339/2330 train_time:81209ms step_avg:60.65ms
step:1340/2330 train_time:81273ms step_avg:60.65ms
step:1341/2330 train_time:81332ms step_avg:60.65ms
step:1342/2330 train_time:81394ms step_avg:60.65ms
step:1343/2330 train_time:81454ms step_avg:60.65ms
step:1344/2330 train_time:81516ms step_avg:60.65ms
step:1345/2330 train_time:81575ms step_avg:60.65ms
step:1346/2330 train_time:81637ms step_avg:60.65ms
step:1347/2330 train_time:81697ms step_avg:60.65ms
step:1348/2330 train_time:81760ms step_avg:60.65ms
step:1349/2330 train_time:81819ms step_avg:60.65ms
step:1350/2330 train_time:81882ms step_avg:60.65ms
step:1351/2330 train_time:81943ms step_avg:60.65ms
step:1352/2330 train_time:82006ms step_avg:60.66ms
step:1353/2330 train_time:82065ms step_avg:60.65ms
step:1354/2330 train_time:82129ms step_avg:60.66ms
step:1355/2330 train_time:82188ms step_avg:60.66ms
step:1356/2330 train_time:82252ms step_avg:60.66ms
step:1357/2330 train_time:82311ms step_avg:60.66ms
step:1358/2330 train_time:82373ms step_avg:60.66ms
step:1359/2330 train_time:82433ms step_avg:60.66ms
step:1360/2330 train_time:82496ms step_avg:60.66ms
step:1361/2330 train_time:82555ms step_avg:60.66ms
step:1362/2330 train_time:82617ms step_avg:60.66ms
step:1363/2330 train_time:82676ms step_avg:60.66ms
step:1364/2330 train_time:82740ms step_avg:60.66ms
step:1365/2330 train_time:82799ms step_avg:60.66ms
step:1366/2330 train_time:82863ms step_avg:60.66ms
step:1367/2330 train_time:82922ms step_avg:60.66ms
step:1368/2330 train_time:82985ms step_avg:60.66ms
step:1369/2330 train_time:83045ms step_avg:60.66ms
step:1370/2330 train_time:83109ms step_avg:60.66ms
step:1371/2330 train_time:83168ms step_avg:60.66ms
step:1372/2330 train_time:83231ms step_avg:60.66ms
step:1373/2330 train_time:83290ms step_avg:60.66ms
step:1374/2330 train_time:83353ms step_avg:60.66ms
step:1375/2330 train_time:83412ms step_avg:60.66ms
step:1376/2330 train_time:83474ms step_avg:60.66ms
step:1377/2330 train_time:83533ms step_avg:60.66ms
step:1378/2330 train_time:83596ms step_avg:60.66ms
step:1379/2330 train_time:83655ms step_avg:60.66ms
step:1380/2330 train_time:83718ms step_avg:60.67ms
step:1381/2330 train_time:83777ms step_avg:60.66ms
step:1382/2330 train_time:83841ms step_avg:60.67ms
step:1383/2330 train_time:83900ms step_avg:60.67ms
step:1384/2330 train_time:83963ms step_avg:60.67ms
step:1385/2330 train_time:84023ms step_avg:60.67ms
step:1386/2330 train_time:84086ms step_avg:60.67ms
step:1387/2330 train_time:84145ms step_avg:60.67ms
step:1388/2330 train_time:84209ms step_avg:60.67ms
step:1389/2330 train_time:84268ms step_avg:60.67ms
step:1390/2330 train_time:84331ms step_avg:60.67ms
step:1391/2330 train_time:84390ms step_avg:60.67ms
step:1392/2330 train_time:84453ms step_avg:60.67ms
step:1393/2330 train_time:84512ms step_avg:60.67ms
step:1394/2330 train_time:84575ms step_avg:60.67ms
step:1395/2330 train_time:84634ms step_avg:60.67ms
step:1396/2330 train_time:84697ms step_avg:60.67ms
step:1397/2330 train_time:84757ms step_avg:60.67ms
step:1398/2330 train_time:84820ms step_avg:60.67ms
step:1399/2330 train_time:84879ms step_avg:60.67ms
step:1400/2330 train_time:84944ms step_avg:60.67ms
step:1401/2330 train_time:85003ms step_avg:60.67ms
step:1402/2330 train_time:85067ms step_avg:60.68ms
step:1403/2330 train_time:85126ms step_avg:60.67ms
step:1404/2330 train_time:85189ms step_avg:60.68ms
step:1405/2330 train_time:85250ms step_avg:60.68ms
step:1406/2330 train_time:85313ms step_avg:60.68ms
step:1407/2330 train_time:85372ms step_avg:60.68ms
step:1408/2330 train_time:85434ms step_avg:60.68ms
step:1409/2330 train_time:85493ms step_avg:60.68ms
step:1410/2330 train_time:85556ms step_avg:60.68ms
step:1411/2330 train_time:85615ms step_avg:60.68ms
step:1412/2330 train_time:85678ms step_avg:60.68ms
step:1413/2330 train_time:85737ms step_avg:60.68ms
step:1414/2330 train_time:85801ms step_avg:60.68ms
step:1415/2330 train_time:85860ms step_avg:60.68ms
step:1416/2330 train_time:85922ms step_avg:60.68ms
step:1417/2330 train_time:85981ms step_avg:60.68ms
step:1418/2330 train_time:86044ms step_avg:60.68ms
step:1419/2330 train_time:86106ms step_avg:60.68ms
step:1420/2330 train_time:86169ms step_avg:60.68ms
step:1421/2330 train_time:86229ms step_avg:60.68ms
step:1422/2330 train_time:86292ms step_avg:60.68ms
step:1423/2330 train_time:86351ms step_avg:60.68ms
step:1424/2330 train_time:86414ms step_avg:60.68ms
step:1425/2330 train_time:86473ms step_avg:60.68ms
step:1426/2330 train_time:86535ms step_avg:60.68ms
step:1427/2330 train_time:86595ms step_avg:60.68ms
step:1428/2330 train_time:86658ms step_avg:60.68ms
step:1429/2330 train_time:86717ms step_avg:60.68ms
step:1430/2330 train_time:86780ms step_avg:60.69ms
step:1431/2330 train_time:86839ms step_avg:60.68ms
step:1432/2330 train_time:86902ms step_avg:60.69ms
step:1433/2330 train_time:86961ms step_avg:60.68ms
step:1434/2330 train_time:87024ms step_avg:60.69ms
step:1435/2330 train_time:87084ms step_avg:60.69ms
step:1436/2330 train_time:87147ms step_avg:60.69ms
step:1437/2330 train_time:87208ms step_avg:60.69ms
step:1438/2330 train_time:87271ms step_avg:60.69ms
step:1439/2330 train_time:87330ms step_avg:60.69ms
step:1440/2330 train_time:87394ms step_avg:60.69ms
step:1441/2330 train_time:87453ms step_avg:60.69ms
step:1442/2330 train_time:87515ms step_avg:60.69ms
step:1443/2330 train_time:87575ms step_avg:60.69ms
step:1444/2330 train_time:87638ms step_avg:60.69ms
step:1445/2330 train_time:87697ms step_avg:60.69ms
step:1446/2330 train_time:87759ms step_avg:60.69ms
step:1447/2330 train_time:87819ms step_avg:60.69ms
step:1448/2330 train_time:87881ms step_avg:60.69ms
step:1449/2330 train_time:87940ms step_avg:60.69ms
step:1450/2330 train_time:88003ms step_avg:60.69ms
step:1451/2330 train_time:88063ms step_avg:60.69ms
step:1452/2330 train_time:88126ms step_avg:60.69ms
step:1453/2330 train_time:88187ms step_avg:60.69ms
step:1454/2330 train_time:88249ms step_avg:60.69ms
step:1455/2330 train_time:88309ms step_avg:60.69ms
step:1456/2330 train_time:88371ms step_avg:60.69ms
step:1457/2330 train_time:88429ms step_avg:60.69ms
step:1458/2330 train_time:88493ms step_avg:60.69ms
step:1459/2330 train_time:88552ms step_avg:60.69ms
step:1460/2330 train_time:88615ms step_avg:60.69ms
step:1461/2330 train_time:88674ms step_avg:60.69ms
step:1462/2330 train_time:88737ms step_avg:60.70ms
step:1463/2330 train_time:88796ms step_avg:60.69ms
step:1464/2330 train_time:88859ms step_avg:60.70ms
step:1465/2330 train_time:88919ms step_avg:60.70ms
step:1466/2330 train_time:88982ms step_avg:60.70ms
step:1467/2330 train_time:89041ms step_avg:60.70ms
step:1468/2330 train_time:89105ms step_avg:60.70ms
step:1469/2330 train_time:89164ms step_avg:60.70ms
step:1470/2330 train_time:89227ms step_avg:60.70ms
step:1471/2330 train_time:89286ms step_avg:60.70ms
step:1472/2330 train_time:89350ms step_avg:60.70ms
step:1473/2330 train_time:89410ms step_avg:60.70ms
step:1474/2330 train_time:89472ms step_avg:60.70ms
step:1475/2330 train_time:89532ms step_avg:60.70ms
step:1476/2330 train_time:89595ms step_avg:60.70ms
step:1477/2330 train_time:89654ms step_avg:60.70ms
step:1478/2330 train_time:89717ms step_avg:60.70ms
step:1479/2330 train_time:89776ms step_avg:60.70ms
step:1480/2330 train_time:89839ms step_avg:60.70ms
step:1481/2330 train_time:89899ms step_avg:60.70ms
step:1482/2330 train_time:89962ms step_avg:60.70ms
step:1483/2330 train_time:90021ms step_avg:60.70ms
step:1484/2330 train_time:90084ms step_avg:60.70ms
step:1485/2330 train_time:90143ms step_avg:60.70ms
step:1486/2330 train_time:90207ms step_avg:60.70ms
step:1487/2330 train_time:90266ms step_avg:60.70ms
step:1488/2330 train_time:90329ms step_avg:60.71ms
step:1489/2330 train_time:90389ms step_avg:60.70ms
step:1490/2330 train_time:90453ms step_avg:60.71ms
step:1491/2330 train_time:90512ms step_avg:60.71ms
step:1492/2330 train_time:90575ms step_avg:60.71ms
step:1493/2330 train_time:90634ms step_avg:60.71ms
step:1494/2330 train_time:90696ms step_avg:60.71ms
step:1495/2330 train_time:90756ms step_avg:60.71ms
step:1496/2330 train_time:90818ms step_avg:60.71ms
step:1497/2330 train_time:90877ms step_avg:60.71ms
step:1498/2330 train_time:90940ms step_avg:60.71ms
step:1499/2330 train_time:90999ms step_avg:60.71ms
step:1500/2330 train_time:91063ms step_avg:60.71ms
step:1500/2330 val_loss:3.6620 train_time:91134ms step_avg:60.76ms
step:1501/2330 train_time:91156ms step_avg:60.73ms
step:1502/2330 train_time:91187ms step_avg:60.71ms
step:1503/2330 train_time:91249ms step_avg:60.71ms
step:1504/2330 train_time:91314ms step_avg:60.71ms
step:1505/2330 train_time:91375ms step_avg:60.71ms
step:1506/2330 train_time:91438ms step_avg:60.72ms
step:1507/2330 train_time:91497ms step_avg:60.71ms
step:1508/2330 train_time:91560ms step_avg:60.72ms
step:1509/2330 train_time:91619ms step_avg:60.72ms
step:1510/2330 train_time:91681ms step_avg:60.72ms
step:1511/2330 train_time:91739ms step_avg:60.71ms
step:1512/2330 train_time:91802ms step_avg:60.72ms
step:1513/2330 train_time:91861ms step_avg:60.71ms
step:1514/2330 train_time:91925ms step_avg:60.72ms
step:1515/2330 train_time:91985ms step_avg:60.72ms
step:1516/2330 train_time:92048ms step_avg:60.72ms
step:1517/2330 train_time:92106ms step_avg:60.72ms
step:1518/2330 train_time:92171ms step_avg:60.72ms
step:1519/2330 train_time:92232ms step_avg:60.72ms
step:1520/2330 train_time:92295ms step_avg:60.72ms
step:1521/2330 train_time:92356ms step_avg:60.72ms
step:1522/2330 train_time:92419ms step_avg:60.72ms
step:1523/2330 train_time:92478ms step_avg:60.72ms
step:1524/2330 train_time:92540ms step_avg:60.72ms
step:1525/2330 train_time:92599ms step_avg:60.72ms
step:1526/2330 train_time:92661ms step_avg:60.72ms
step:1527/2330 train_time:92721ms step_avg:60.72ms
step:1528/2330 train_time:92783ms step_avg:60.72ms
step:1529/2330 train_time:92843ms step_avg:60.72ms
step:1530/2330 train_time:92906ms step_avg:60.72ms
step:1531/2330 train_time:92965ms step_avg:60.72ms
step:1532/2330 train_time:93029ms step_avg:60.72ms
step:1533/2330 train_time:93089ms step_avg:60.72ms
step:1534/2330 train_time:93154ms step_avg:60.73ms
step:1535/2330 train_time:93213ms step_avg:60.73ms
step:1536/2330 train_time:93278ms step_avg:60.73ms
step:1537/2330 train_time:93339ms step_avg:60.73ms
step:1538/2330 train_time:93403ms step_avg:60.73ms
step:1539/2330 train_time:93463ms step_avg:60.73ms
step:1540/2330 train_time:93527ms step_avg:60.73ms
step:1541/2330 train_time:93586ms step_avg:60.73ms
step:1542/2330 train_time:93650ms step_avg:60.73ms
step:1543/2330 train_time:93709ms step_avg:60.73ms
step:1544/2330 train_time:93772ms step_avg:60.73ms
step:1545/2330 train_time:93833ms step_avg:60.73ms
step:1546/2330 train_time:93897ms step_avg:60.74ms
step:1547/2330 train_time:93957ms step_avg:60.74ms
step:1548/2330 train_time:94021ms step_avg:60.74ms
step:1549/2330 train_time:94080ms step_avg:60.74ms
step:1550/2330 train_time:94143ms step_avg:60.74ms
step:1551/2330 train_time:94203ms step_avg:60.74ms
step:1552/2330 train_time:94267ms step_avg:60.74ms
step:1553/2330 train_time:94327ms step_avg:60.74ms
step:1554/2330 train_time:94391ms step_avg:60.74ms
step:1555/2330 train_time:94451ms step_avg:60.74ms
step:1556/2330 train_time:94514ms step_avg:60.74ms
step:1557/2330 train_time:94574ms step_avg:60.74ms
step:1558/2330 train_time:94638ms step_avg:60.74ms
step:1559/2330 train_time:94697ms step_avg:60.74ms
step:1560/2330 train_time:94760ms step_avg:60.74ms
step:1561/2330 train_time:94821ms step_avg:60.74ms
step:1562/2330 train_time:94884ms step_avg:60.75ms
step:1563/2330 train_time:94943ms step_avg:60.74ms
step:1564/2330 train_time:95007ms step_avg:60.75ms
step:1565/2330 train_time:95067ms step_avg:60.75ms
step:1566/2330 train_time:95131ms step_avg:60.75ms
step:1567/2330 train_time:95190ms step_avg:60.75ms
step:1568/2330 train_time:95253ms step_avg:60.75ms
step:1569/2330 train_time:95312ms step_avg:60.75ms
step:1570/2330 train_time:95376ms step_avg:60.75ms
step:1571/2330 train_time:95436ms step_avg:60.75ms
step:1572/2330 train_time:95499ms step_avg:60.75ms
step:1573/2330 train_time:95559ms step_avg:60.75ms
step:1574/2330 train_time:95621ms step_avg:60.75ms
step:1575/2330 train_time:95681ms step_avg:60.75ms
step:1576/2330 train_time:95744ms step_avg:60.75ms
step:1577/2330 train_time:95804ms step_avg:60.75ms
step:1578/2330 train_time:95869ms step_avg:60.75ms
step:1579/2330 train_time:95928ms step_avg:60.75ms
step:1580/2330 train_time:95992ms step_avg:60.75ms
step:1581/2330 train_time:96052ms step_avg:60.75ms
step:1582/2330 train_time:96116ms step_avg:60.76ms
step:1583/2330 train_time:96176ms step_avg:60.76ms
step:1584/2330 train_time:96240ms step_avg:60.76ms
step:1585/2330 train_time:96298ms step_avg:60.76ms
step:1586/2330 train_time:96362ms step_avg:60.76ms
step:1587/2330 train_time:96423ms step_avg:60.76ms
step:1588/2330 train_time:96486ms step_avg:60.76ms
step:1589/2330 train_time:96546ms step_avg:60.76ms
step:1590/2330 train_time:96609ms step_avg:60.76ms
step:1591/2330 train_time:96669ms step_avg:60.76ms
step:1592/2330 train_time:96732ms step_avg:60.76ms
step:1593/2330 train_time:96792ms step_avg:60.76ms
step:1594/2330 train_time:96855ms step_avg:60.76ms
step:1595/2330 train_time:96914ms step_avg:60.76ms
step:1596/2330 train_time:96977ms step_avg:60.76ms
step:1597/2330 train_time:97037ms step_avg:60.76ms
step:1598/2330 train_time:97099ms step_avg:60.76ms
step:1599/2330 train_time:97159ms step_avg:60.76ms
step:1600/2330 train_time:97223ms step_avg:60.76ms
step:1601/2330 train_time:97283ms step_avg:60.76ms
step:1602/2330 train_time:97347ms step_avg:60.77ms
step:1603/2330 train_time:97406ms step_avg:60.77ms
step:1604/2330 train_time:97470ms step_avg:60.77ms
step:1605/2330 train_time:97530ms step_avg:60.77ms
step:1606/2330 train_time:97592ms step_avg:60.77ms
step:1607/2330 train_time:97652ms step_avg:60.77ms
step:1608/2330 train_time:97715ms step_avg:60.77ms
step:1609/2330 train_time:97775ms step_avg:60.77ms
step:1610/2330 train_time:97838ms step_avg:60.77ms
step:1611/2330 train_time:97897ms step_avg:60.77ms
step:1612/2330 train_time:97961ms step_avg:60.77ms
step:1613/2330 train_time:98021ms step_avg:60.77ms
step:1614/2330 train_time:98084ms step_avg:60.77ms
step:1615/2330 train_time:98144ms step_avg:60.77ms
step:1616/2330 train_time:98207ms step_avg:60.77ms
step:1617/2330 train_time:98267ms step_avg:60.77ms
step:1618/2330 train_time:98331ms step_avg:60.77ms
step:1619/2330 train_time:98390ms step_avg:60.77ms
step:1620/2330 train_time:98453ms step_avg:60.77ms
step:1621/2330 train_time:98513ms step_avg:60.77ms
step:1622/2330 train_time:98576ms step_avg:60.77ms
step:1623/2330 train_time:98635ms step_avg:60.77ms
step:1624/2330 train_time:98698ms step_avg:60.77ms
step:1625/2330 train_time:98757ms step_avg:60.77ms
step:1626/2330 train_time:98821ms step_avg:60.78ms
step:1627/2330 train_time:98880ms step_avg:60.77ms
step:1628/2330 train_time:98944ms step_avg:60.78ms
step:1629/2330 train_time:99003ms step_avg:60.78ms
step:1630/2330 train_time:99068ms step_avg:60.78ms
step:1631/2330 train_time:99128ms step_avg:60.78ms
step:1632/2330 train_time:99191ms step_avg:60.78ms
step:1633/2330 train_time:99251ms step_avg:60.78ms
step:1634/2330 train_time:99314ms step_avg:60.78ms
step:1635/2330 train_time:99374ms step_avg:60.78ms
step:1636/2330 train_time:99437ms step_avg:60.78ms
step:1637/2330 train_time:99498ms step_avg:60.78ms
step:1638/2330 train_time:99560ms step_avg:60.78ms
step:1639/2330 train_time:99621ms step_avg:60.78ms
step:1640/2330 train_time:99684ms step_avg:60.78ms
step:1641/2330 train_time:99743ms step_avg:60.78ms
step:1642/2330 train_time:99807ms step_avg:60.78ms
step:1643/2330 train_time:99867ms step_avg:60.78ms
step:1644/2330 train_time:99931ms step_avg:60.79ms
step:1645/2330 train_time:99990ms step_avg:60.78ms
step:1646/2330 train_time:100054ms step_avg:60.79ms
step:1647/2330 train_time:100114ms step_avg:60.79ms
step:1648/2330 train_time:100177ms step_avg:60.79ms
step:1649/2330 train_time:100237ms step_avg:60.79ms
step:1650/2330 train_time:100300ms step_avg:60.79ms
step:1651/2330 train_time:100359ms step_avg:60.79ms
step:1652/2330 train_time:100424ms step_avg:60.79ms
step:1653/2330 train_time:100483ms step_avg:60.79ms
step:1654/2330 train_time:100546ms step_avg:60.79ms
step:1655/2330 train_time:100606ms step_avg:60.79ms
step:1656/2330 train_time:100669ms step_avg:60.79ms
step:1657/2330 train_time:100729ms step_avg:60.79ms
step:1658/2330 train_time:100792ms step_avg:60.79ms
step:1659/2330 train_time:100852ms step_avg:60.79ms
step:1660/2330 train_time:100914ms step_avg:60.79ms
step:1661/2330 train_time:100974ms step_avg:60.79ms
step:1662/2330 train_time:101038ms step_avg:60.79ms
step:1663/2330 train_time:101099ms step_avg:60.79ms
step:1664/2330 train_time:101161ms step_avg:60.79ms
step:1665/2330 train_time:101222ms step_avg:60.79ms
step:1666/2330 train_time:101285ms step_avg:60.80ms
step:1667/2330 train_time:101345ms step_avg:60.79ms
step:1668/2330 train_time:101408ms step_avg:60.80ms
step:1669/2330 train_time:101467ms step_avg:60.80ms
step:1670/2330 train_time:101531ms step_avg:60.80ms
step:1671/2330 train_time:101591ms step_avg:60.80ms
step:1672/2330 train_time:101654ms step_avg:60.80ms
step:1673/2330 train_time:101715ms step_avg:60.80ms
step:1674/2330 train_time:101778ms step_avg:60.80ms
step:1675/2330 train_time:101837ms step_avg:60.80ms
step:1676/2330 train_time:101900ms step_avg:60.80ms
step:1677/2330 train_time:101960ms step_avg:60.80ms
step:1678/2330 train_time:102024ms step_avg:60.80ms
step:1679/2330 train_time:102084ms step_avg:60.80ms
step:1680/2330 train_time:102149ms step_avg:60.80ms
step:1681/2330 train_time:102208ms step_avg:60.80ms
step:1682/2330 train_time:102272ms step_avg:60.80ms
step:1683/2330 train_time:102332ms step_avg:60.80ms
step:1684/2330 train_time:102395ms step_avg:60.80ms
step:1685/2330 train_time:102455ms step_avg:60.80ms
step:1686/2330 train_time:102519ms step_avg:60.81ms
step:1687/2330 train_time:102579ms step_avg:60.81ms
step:1688/2330 train_time:102642ms step_avg:60.81ms
step:1689/2330 train_time:102702ms step_avg:60.81ms
step:1690/2330 train_time:102766ms step_avg:60.81ms
step:1691/2330 train_time:102825ms step_avg:60.81ms
step:1692/2330 train_time:102890ms step_avg:60.81ms
step:1693/2330 train_time:102950ms step_avg:60.81ms
step:1694/2330 train_time:103013ms step_avg:60.81ms
step:1695/2330 train_time:103073ms step_avg:60.81ms
step:1696/2330 train_time:103137ms step_avg:60.81ms
step:1697/2330 train_time:103196ms step_avg:60.81ms
step:1698/2330 train_time:103260ms step_avg:60.81ms
step:1699/2330 train_time:103321ms step_avg:60.81ms
step:1700/2330 train_time:103383ms step_avg:60.81ms
step:1701/2330 train_time:103442ms step_avg:60.81ms
step:1702/2330 train_time:103506ms step_avg:60.81ms
step:1703/2330 train_time:103566ms step_avg:60.81ms
step:1704/2330 train_time:103629ms step_avg:60.82ms
step:1705/2330 train_time:103689ms step_avg:60.81ms
step:1706/2330 train_time:103752ms step_avg:60.82ms
step:1707/2330 train_time:103812ms step_avg:60.82ms
step:1708/2330 train_time:103876ms step_avg:60.82ms
step:1709/2330 train_time:103935ms step_avg:60.82ms
step:1710/2330 train_time:103998ms step_avg:60.82ms
step:1711/2330 train_time:104059ms step_avg:60.82ms
step:1712/2330 train_time:104122ms step_avg:60.82ms
step:1713/2330 train_time:104182ms step_avg:60.82ms
step:1714/2330 train_time:104245ms step_avg:60.82ms
step:1715/2330 train_time:104306ms step_avg:60.82ms
step:1716/2330 train_time:104370ms step_avg:60.82ms
step:1717/2330 train_time:104429ms step_avg:60.82ms
step:1718/2330 train_time:104491ms step_avg:60.82ms
step:1719/2330 train_time:104551ms step_avg:60.82ms
step:1720/2330 train_time:104615ms step_avg:60.82ms
step:1721/2330 train_time:104675ms step_avg:60.82ms
step:1722/2330 train_time:104739ms step_avg:60.82ms
step:1723/2330 train_time:104798ms step_avg:60.82ms
step:1724/2330 train_time:104861ms step_avg:60.82ms
step:1725/2330 train_time:104921ms step_avg:60.82ms
step:1726/2330 train_time:104984ms step_avg:60.83ms
step:1727/2330 train_time:105045ms step_avg:60.83ms
step:1728/2330 train_time:105109ms step_avg:60.83ms
step:1729/2330 train_time:105169ms step_avg:60.83ms
step:1730/2330 train_time:105232ms step_avg:60.83ms
step:1731/2330 train_time:105292ms step_avg:60.83ms
step:1732/2330 train_time:105356ms step_avg:60.83ms
step:1733/2330 train_time:105415ms step_avg:60.83ms
step:1734/2330 train_time:105478ms step_avg:60.83ms
step:1735/2330 train_time:105538ms step_avg:60.83ms
step:1736/2330 train_time:105602ms step_avg:60.83ms
step:1737/2330 train_time:105662ms step_avg:60.83ms
step:1738/2330 train_time:105726ms step_avg:60.83ms
step:1739/2330 train_time:105785ms step_avg:60.83ms
step:1740/2330 train_time:105848ms step_avg:60.83ms
step:1741/2330 train_time:105908ms step_avg:60.83ms
step:1742/2330 train_time:105971ms step_avg:60.83ms
step:1743/2330 train_time:106032ms step_avg:60.83ms
step:1744/2330 train_time:106096ms step_avg:60.83ms
step:1745/2330 train_time:106156ms step_avg:60.83ms
step:1746/2330 train_time:106220ms step_avg:60.84ms
step:1747/2330 train_time:106279ms step_avg:60.83ms
step:1748/2330 train_time:106343ms step_avg:60.84ms
step:1749/2330 train_time:106403ms step_avg:60.84ms
step:1750/2330 train_time:106466ms step_avg:60.84ms
step:1750/2330 val_loss:3.6075 train_time:106538ms step_avg:60.88ms
step:1751/2330 train_time:106560ms step_avg:60.86ms
step:1752/2330 train_time:106590ms step_avg:60.84ms
step:1753/2330 train_time:106651ms step_avg:60.84ms
step:1754/2330 train_time:106720ms step_avg:60.84ms
step:1755/2330 train_time:106781ms step_avg:60.84ms
step:1756/2330 train_time:106844ms step_avg:60.85ms
step:1757/2330 train_time:106903ms step_avg:60.84ms
step:1758/2330 train_time:106965ms step_avg:60.84ms
step:1759/2330 train_time:107025ms step_avg:60.84ms
step:1760/2330 train_time:107087ms step_avg:60.84ms
step:1761/2330 train_time:107146ms step_avg:60.84ms
step:1762/2330 train_time:107209ms step_avg:60.85ms
step:1763/2330 train_time:107268ms step_avg:60.84ms
step:1764/2330 train_time:107330ms step_avg:60.84ms
step:1765/2330 train_time:107390ms step_avg:60.84ms
step:1766/2330 train_time:107456ms step_avg:60.85ms
step:1767/2330 train_time:107518ms step_avg:60.85ms
step:1768/2330 train_time:107582ms step_avg:60.85ms
step:1769/2330 train_time:107642ms step_avg:60.85ms
step:1770/2330 train_time:107706ms step_avg:60.85ms
step:1771/2330 train_time:107767ms step_avg:60.85ms
step:1772/2330 train_time:107831ms step_avg:60.85ms
step:1773/2330 train_time:107891ms step_avg:60.85ms
step:1774/2330 train_time:107953ms step_avg:60.85ms
step:1775/2330 train_time:108013ms step_avg:60.85ms
step:1776/2330 train_time:108077ms step_avg:60.85ms
step:1777/2330 train_time:108137ms step_avg:60.85ms
step:1778/2330 train_time:108202ms step_avg:60.86ms
step:1779/2330 train_time:108261ms step_avg:60.86ms
step:1780/2330 train_time:108323ms step_avg:60.86ms
step:1781/2330 train_time:108381ms step_avg:60.85ms
step:1782/2330 train_time:108445ms step_avg:60.86ms
step:1783/2330 train_time:108506ms step_avg:60.86ms
step:1784/2330 train_time:108571ms step_avg:60.86ms
step:1785/2330 train_time:108630ms step_avg:60.86ms
step:1786/2330 train_time:108693ms step_avg:60.86ms
step:1787/2330 train_time:108754ms step_avg:60.86ms
step:1788/2330 train_time:108817ms step_avg:60.86ms
step:1789/2330 train_time:108877ms step_avg:60.86ms
step:1790/2330 train_time:108941ms step_avg:60.86ms
step:1791/2330 train_time:109000ms step_avg:60.86ms
step:1792/2330 train_time:109063ms step_avg:60.86ms
step:1793/2330 train_time:109123ms step_avg:60.86ms
step:1794/2330 train_time:109185ms step_avg:60.86ms
step:1795/2330 train_time:109244ms step_avg:60.86ms
step:1796/2330 train_time:109307ms step_avg:60.86ms
step:1797/2330 train_time:109366ms step_avg:60.86ms
step:1798/2330 train_time:109430ms step_avg:60.86ms
step:1799/2330 train_time:109490ms step_avg:60.86ms
step:1800/2330 train_time:109553ms step_avg:60.86ms
step:1801/2330 train_time:109613ms step_avg:60.86ms
step:1802/2330 train_time:109676ms step_avg:60.86ms
step:1803/2330 train_time:109736ms step_avg:60.86ms
step:1804/2330 train_time:109800ms step_avg:60.86ms
step:1805/2330 train_time:109860ms step_avg:60.86ms
step:1806/2330 train_time:109924ms step_avg:60.87ms
step:1807/2330 train_time:109983ms step_avg:60.86ms
step:1808/2330 train_time:110046ms step_avg:60.87ms
step:1809/2330 train_time:110105ms step_avg:60.87ms
step:1810/2330 train_time:110168ms step_avg:60.87ms
step:1811/2330 train_time:110228ms step_avg:60.87ms
step:1812/2330 train_time:110291ms step_avg:60.87ms
step:1813/2330 train_time:110351ms step_avg:60.87ms
step:1814/2330 train_time:110414ms step_avg:60.87ms
step:1815/2330 train_time:110474ms step_avg:60.87ms
step:1816/2330 train_time:110537ms step_avg:60.87ms
step:1817/2330 train_time:110597ms step_avg:60.87ms
step:1818/2330 train_time:110660ms step_avg:60.87ms
step:1819/2330 train_time:110720ms step_avg:60.87ms
step:1820/2330 train_time:110783ms step_avg:60.87ms
step:1821/2330 train_time:110842ms step_avg:60.87ms
step:1822/2330 train_time:110905ms step_avg:60.87ms
step:1823/2330 train_time:110965ms step_avg:60.87ms
step:1824/2330 train_time:111028ms step_avg:60.87ms
step:1825/2330 train_time:111087ms step_avg:60.87ms
step:1826/2330 train_time:111150ms step_avg:60.87ms
step:1827/2330 train_time:111209ms step_avg:60.87ms
step:1828/2330 train_time:111272ms step_avg:60.87ms
step:1829/2330 train_time:111332ms step_avg:60.87ms
step:1830/2330 train_time:111395ms step_avg:60.87ms
step:1831/2330 train_time:111455ms step_avg:60.87ms
step:1832/2330 train_time:111519ms step_avg:60.87ms
step:1833/2330 train_time:111579ms step_avg:60.87ms
step:1834/2330 train_time:111642ms step_avg:60.87ms
step:1835/2330 train_time:111702ms step_avg:60.87ms
step:1836/2330 train_time:111765ms step_avg:60.87ms
step:1837/2330 train_time:111825ms step_avg:60.87ms
step:1838/2330 train_time:111888ms step_avg:60.87ms
step:1839/2330 train_time:111948ms step_avg:60.87ms
step:1840/2330 train_time:112011ms step_avg:60.88ms
step:1841/2330 train_time:112071ms step_avg:60.87ms
step:1842/2330 train_time:112134ms step_avg:60.88ms
step:1843/2330 train_time:112193ms step_avg:60.88ms
step:1844/2330 train_time:112257ms step_avg:60.88ms
step:1845/2330 train_time:112316ms step_avg:60.88ms
step:1846/2330 train_time:112379ms step_avg:60.88ms
step:1847/2330 train_time:112439ms step_avg:60.88ms
step:1848/2330 train_time:112502ms step_avg:60.88ms
step:1849/2330 train_time:112563ms step_avg:60.88ms
step:1850/2330 train_time:112626ms step_avg:60.88ms
step:1851/2330 train_time:112685ms step_avg:60.88ms
step:1852/2330 train_time:112748ms step_avg:60.88ms
step:1853/2330 train_time:112808ms step_avg:60.88ms
step:1854/2330 train_time:112871ms step_avg:60.88ms
step:1855/2330 train_time:112930ms step_avg:60.88ms
step:1856/2330 train_time:112993ms step_avg:60.88ms
step:1857/2330 train_time:113053ms step_avg:60.88ms
step:1858/2330 train_time:113117ms step_avg:60.88ms
step:1859/2330 train_time:113177ms step_avg:60.88ms
step:1860/2330 train_time:113240ms step_avg:60.88ms
step:1861/2330 train_time:113300ms step_avg:60.88ms
step:1862/2330 train_time:113363ms step_avg:60.88ms
step:1863/2330 train_time:113423ms step_avg:60.88ms
step:1864/2330 train_time:113486ms step_avg:60.88ms
step:1865/2330 train_time:113546ms step_avg:60.88ms
step:1866/2330 train_time:113609ms step_avg:60.88ms
step:1867/2330 train_time:113668ms step_avg:60.88ms
step:1868/2330 train_time:113733ms step_avg:60.88ms
step:1869/2330 train_time:113792ms step_avg:60.88ms
step:1870/2330 train_time:113855ms step_avg:60.88ms
step:1871/2330 train_time:113914ms step_avg:60.88ms
step:1872/2330 train_time:113978ms step_avg:60.89ms
step:1873/2330 train_time:114039ms step_avg:60.89ms
step:1874/2330 train_time:114102ms step_avg:60.89ms
step:1875/2330 train_time:114162ms step_avg:60.89ms
step:1876/2330 train_time:114225ms step_avg:60.89ms
step:1877/2330 train_time:114284ms step_avg:60.89ms
step:1878/2330 train_time:114348ms step_avg:60.89ms
step:1879/2330 train_time:114407ms step_avg:60.89ms
step:1880/2330 train_time:114470ms step_avg:60.89ms
step:1881/2330 train_time:114529ms step_avg:60.89ms
step:1882/2330 train_time:114592ms step_avg:60.89ms
step:1883/2330 train_time:114652ms step_avg:60.89ms
step:1884/2330 train_time:114716ms step_avg:60.89ms
step:1885/2330 train_time:114776ms step_avg:60.89ms
step:1886/2330 train_time:114840ms step_avg:60.89ms
step:1887/2330 train_time:114899ms step_avg:60.89ms
step:1888/2330 train_time:114963ms step_avg:60.89ms
step:1889/2330 train_time:115023ms step_avg:60.89ms
step:1890/2330 train_time:115086ms step_avg:60.89ms
step:1891/2330 train_time:115145ms step_avg:60.89ms
step:1892/2330 train_time:115209ms step_avg:60.89ms
step:1893/2330 train_time:115268ms step_avg:60.89ms
step:1894/2330 train_time:115331ms step_avg:60.89ms
step:1895/2330 train_time:115391ms step_avg:60.89ms
step:1896/2330 train_time:115453ms step_avg:60.89ms
step:1897/2330 train_time:115513ms step_avg:60.89ms
step:1898/2330 train_time:115576ms step_avg:60.89ms
step:1899/2330 train_time:115636ms step_avg:60.89ms
step:1900/2330 train_time:115700ms step_avg:60.89ms
step:1901/2330 train_time:115761ms step_avg:60.89ms
step:1902/2330 train_time:115824ms step_avg:60.90ms
step:1903/2330 train_time:115882ms step_avg:60.89ms
step:1904/2330 train_time:115946ms step_avg:60.90ms
step:1905/2330 train_time:116005ms step_avg:60.90ms
step:1906/2330 train_time:116069ms step_avg:60.90ms
step:1907/2330 train_time:116129ms step_avg:60.90ms
step:1908/2330 train_time:116192ms step_avg:60.90ms
step:1909/2330 train_time:116251ms step_avg:60.90ms
step:1910/2330 train_time:116315ms step_avg:60.90ms
step:1911/2330 train_time:116375ms step_avg:60.90ms
step:1912/2330 train_time:116438ms step_avg:60.90ms
step:1913/2330 train_time:116498ms step_avg:60.90ms
step:1914/2330 train_time:116562ms step_avg:60.90ms
step:1915/2330 train_time:116621ms step_avg:60.90ms
step:1916/2330 train_time:116685ms step_avg:60.90ms
step:1917/2330 train_time:116745ms step_avg:60.90ms
step:1918/2330 train_time:116808ms step_avg:60.90ms
step:1919/2330 train_time:116867ms step_avg:60.90ms
step:1920/2330 train_time:116930ms step_avg:60.90ms
step:1921/2330 train_time:116989ms step_avg:60.90ms
step:1922/2330 train_time:117053ms step_avg:60.90ms
step:1923/2330 train_time:117113ms step_avg:60.90ms
step:1924/2330 train_time:117176ms step_avg:60.90ms
step:1925/2330 train_time:117235ms step_avg:60.90ms
step:1926/2330 train_time:117299ms step_avg:60.90ms
step:1927/2330 train_time:117359ms step_avg:60.90ms
step:1928/2330 train_time:117423ms step_avg:60.90ms
step:1929/2330 train_time:117482ms step_avg:60.90ms
step:1930/2330 train_time:117545ms step_avg:60.90ms
step:1931/2330 train_time:117606ms step_avg:60.90ms
step:1932/2330 train_time:117670ms step_avg:60.91ms
step:1933/2330 train_time:117730ms step_avg:60.91ms
step:1934/2330 train_time:117794ms step_avg:60.91ms
step:1935/2330 train_time:117853ms step_avg:60.91ms
step:1936/2330 train_time:117917ms step_avg:60.91ms
step:1937/2330 train_time:117975ms step_avg:60.91ms
step:1938/2330 train_time:118040ms step_avg:60.91ms
step:1939/2330 train_time:118100ms step_avg:60.91ms
step:1940/2330 train_time:118163ms step_avg:60.91ms
step:1941/2330 train_time:118223ms step_avg:60.91ms
step:1942/2330 train_time:118286ms step_avg:60.91ms
step:1943/2330 train_time:118346ms step_avg:60.91ms
step:1944/2330 train_time:118410ms step_avg:60.91ms
step:1945/2330 train_time:118469ms step_avg:60.91ms
step:1946/2330 train_time:118532ms step_avg:60.91ms
step:1947/2330 train_time:118592ms step_avg:60.91ms
step:1948/2330 train_time:118656ms step_avg:60.91ms
step:1949/2330 train_time:118716ms step_avg:60.91ms
step:1950/2330 train_time:118779ms step_avg:60.91ms
step:1951/2330 train_time:118839ms step_avg:60.91ms
step:1952/2330 train_time:118902ms step_avg:60.91ms
step:1953/2330 train_time:118962ms step_avg:60.91ms
step:1954/2330 train_time:119025ms step_avg:60.91ms
step:1955/2330 train_time:119084ms step_avg:60.91ms
step:1956/2330 train_time:119147ms step_avg:60.91ms
step:1957/2330 train_time:119206ms step_avg:60.91ms
step:1958/2330 train_time:119269ms step_avg:60.91ms
step:1959/2330 train_time:119329ms step_avg:60.91ms
step:1960/2330 train_time:119392ms step_avg:60.91ms
step:1961/2330 train_time:119452ms step_avg:60.91ms
step:1962/2330 train_time:119515ms step_avg:60.92ms
step:1963/2330 train_time:119575ms step_avg:60.91ms
step:1964/2330 train_time:119639ms step_avg:60.92ms
step:1965/2330 train_time:119699ms step_avg:60.92ms
step:1966/2330 train_time:119763ms step_avg:60.92ms
step:1967/2330 train_time:119823ms step_avg:60.92ms
step:1968/2330 train_time:119886ms step_avg:60.92ms
step:1969/2330 train_time:119945ms step_avg:60.92ms
step:1970/2330 train_time:120009ms step_avg:60.92ms
step:1971/2330 train_time:120068ms step_avg:60.92ms
step:1972/2330 train_time:120131ms step_avg:60.92ms
step:1973/2330 train_time:120191ms step_avg:60.92ms
step:1974/2330 train_time:120255ms step_avg:60.92ms
step:1975/2330 train_time:120314ms step_avg:60.92ms
step:1976/2330 train_time:120376ms step_avg:60.92ms
step:1977/2330 train_time:120436ms step_avg:60.92ms
step:1978/2330 train_time:120500ms step_avg:60.92ms
step:1979/2330 train_time:120560ms step_avg:60.92ms
step:1980/2330 train_time:120623ms step_avg:60.92ms
step:1981/2330 train_time:120681ms step_avg:60.92ms
step:1982/2330 train_time:120746ms step_avg:60.92ms
step:1983/2330 train_time:120805ms step_avg:60.92ms
step:1984/2330 train_time:120868ms step_avg:60.92ms
step:1985/2330 train_time:120927ms step_avg:60.92ms
step:1986/2330 train_time:120991ms step_avg:60.92ms
step:1987/2330 train_time:121050ms step_avg:60.92ms
step:1988/2330 train_time:121114ms step_avg:60.92ms
step:1989/2330 train_time:121173ms step_avg:60.92ms
step:1990/2330 train_time:121236ms step_avg:60.92ms
step:1991/2330 train_time:121297ms step_avg:60.92ms
step:1992/2330 train_time:121360ms step_avg:60.92ms
step:1993/2330 train_time:121420ms step_avg:60.92ms
step:1994/2330 train_time:121482ms step_avg:60.92ms
step:1995/2330 train_time:121542ms step_avg:60.92ms
step:1996/2330 train_time:121606ms step_avg:60.92ms
step:1997/2330 train_time:121666ms step_avg:60.92ms
step:1998/2330 train_time:121729ms step_avg:60.93ms
step:1999/2330 train_time:121789ms step_avg:60.93ms
step:2000/2330 train_time:121852ms step_avg:60.93ms
step:2000/2330 val_loss:3.5658 train_time:121924ms step_avg:60.96ms
step:2001/2330 train_time:121946ms step_avg:60.94ms
step:2002/2330 train_time:121980ms step_avg:60.93ms
step:2003/2330 train_time:122042ms step_avg:60.93ms
step:2004/2330 train_time:122108ms step_avg:60.93ms
step:2005/2330 train_time:122170ms step_avg:60.93ms
step:2006/2330 train_time:122234ms step_avg:60.93ms
step:2007/2330 train_time:122293ms step_avg:60.93ms
step:2008/2330 train_time:122355ms step_avg:60.93ms
step:2009/2330 train_time:122414ms step_avg:60.93ms
step:2010/2330 train_time:122477ms step_avg:60.93ms
step:2011/2330 train_time:122536ms step_avg:60.93ms
step:2012/2330 train_time:122597ms step_avg:60.93ms
step:2013/2330 train_time:122656ms step_avg:60.93ms
step:2014/2330 train_time:122718ms step_avg:60.93ms
step:2015/2330 train_time:122777ms step_avg:60.93ms
step:2016/2330 train_time:122839ms step_avg:60.93ms
step:2017/2330 train_time:122899ms step_avg:60.93ms
step:2018/2330 train_time:122964ms step_avg:60.93ms
step:2019/2330 train_time:123026ms step_avg:60.93ms
step:2020/2330 train_time:123089ms step_avg:60.94ms
step:2021/2330 train_time:123150ms step_avg:60.94ms
step:2022/2330 train_time:123213ms step_avg:60.94ms
step:2023/2330 train_time:123273ms step_avg:60.94ms
step:2024/2330 train_time:123336ms step_avg:60.94ms
step:2025/2330 train_time:123395ms step_avg:60.94ms
step:2026/2330 train_time:123458ms step_avg:60.94ms
step:2027/2330 train_time:123516ms step_avg:60.94ms
step:2028/2330 train_time:123579ms step_avg:60.94ms
step:2029/2330 train_time:123638ms step_avg:60.94ms
step:2030/2330 train_time:123699ms step_avg:60.94ms
step:2031/2330 train_time:123758ms step_avg:60.93ms
step:2032/2330 train_time:123821ms step_avg:60.94ms
step:2033/2330 train_time:123880ms step_avg:60.93ms
step:2034/2330 train_time:123944ms step_avg:60.94ms
step:2035/2330 train_time:124004ms step_avg:60.94ms
step:2036/2330 train_time:124068ms step_avg:60.94ms
step:2037/2330 train_time:124128ms step_avg:60.94ms
step:2038/2330 train_time:124192ms step_avg:60.94ms
step:2039/2330 train_time:124251ms step_avg:60.94ms
step:2040/2330 train_time:124314ms step_avg:60.94ms
step:2041/2330 train_time:124373ms step_avg:60.94ms
step:2042/2330 train_time:124437ms step_avg:60.94ms
step:2043/2330 train_time:124495ms step_avg:60.94ms
step:2044/2330 train_time:124558ms step_avg:60.94ms
step:2045/2330 train_time:124617ms step_avg:60.94ms
step:2046/2330 train_time:124680ms step_avg:60.94ms
step:2047/2330 train_time:124739ms step_avg:60.94ms
step:2048/2330 train_time:124802ms step_avg:60.94ms
step:2049/2330 train_time:124861ms step_avg:60.94ms
step:2050/2330 train_time:124925ms step_avg:60.94ms
step:2051/2330 train_time:124985ms step_avg:60.94ms
step:2052/2330 train_time:125048ms step_avg:60.94ms
step:2053/2330 train_time:125108ms step_avg:60.94ms
step:2054/2330 train_time:125172ms step_avg:60.94ms
step:2055/2330 train_time:125232ms step_avg:60.94ms
step:2056/2330 train_time:125296ms step_avg:60.94ms
step:2057/2330 train_time:125355ms step_avg:60.94ms
step:2058/2330 train_time:125418ms step_avg:60.94ms
step:2059/2330 train_time:125479ms step_avg:60.94ms
step:2060/2330 train_time:125541ms step_avg:60.94ms
step:2061/2330 train_time:125600ms step_avg:60.94ms
step:2062/2330 train_time:125663ms step_avg:60.94ms
step:2063/2330 train_time:125722ms step_avg:60.94ms
step:2064/2330 train_time:125785ms step_avg:60.94ms
step:2065/2330 train_time:125844ms step_avg:60.94ms
step:2066/2330 train_time:125907ms step_avg:60.94ms
step:2067/2330 train_time:125967ms step_avg:60.94ms
step:2068/2330 train_time:126031ms step_avg:60.94ms
step:2069/2330 train_time:126091ms step_avg:60.94ms
step:2070/2330 train_time:126154ms step_avg:60.94ms
step:2071/2330 train_time:126213ms step_avg:60.94ms
step:2072/2330 train_time:126277ms step_avg:60.94ms
step:2073/2330 train_time:126337ms step_avg:60.94ms
step:2074/2330 train_time:126399ms step_avg:60.94ms
step:2075/2330 train_time:126460ms step_avg:60.94ms
step:2076/2330 train_time:126523ms step_avg:60.95ms
step:2077/2330 train_time:126582ms step_avg:60.94ms
step:2078/2330 train_time:126644ms step_avg:60.95ms
step:2079/2330 train_time:126703ms step_avg:60.94ms
step:2080/2330 train_time:126767ms step_avg:60.95ms
step:2081/2330 train_time:126826ms step_avg:60.94ms
step:2082/2330 train_time:126889ms step_avg:60.95ms
step:2083/2330 train_time:126949ms step_avg:60.95ms
step:2084/2330 train_time:127013ms step_avg:60.95ms
step:2085/2330 train_time:127073ms step_avg:60.95ms
step:2086/2330 train_time:127137ms step_avg:60.95ms
step:2087/2330 train_time:127196ms step_avg:60.95ms
step:2088/2330 train_time:127259ms step_avg:60.95ms
step:2089/2330 train_time:127320ms step_avg:60.95ms
step:2090/2330 train_time:127382ms step_avg:60.95ms
step:2091/2330 train_time:127442ms step_avg:60.95ms
step:2092/2330 train_time:127505ms step_avg:60.95ms
step:2093/2330 train_time:127565ms step_avg:60.95ms
step:2094/2330 train_time:127628ms step_avg:60.95ms
step:2095/2330 train_time:127687ms step_avg:60.95ms
step:2096/2330 train_time:127750ms step_avg:60.95ms
step:2097/2330 train_time:127809ms step_avg:60.95ms
step:2098/2330 train_time:127873ms step_avg:60.95ms
step:2099/2330 train_time:127934ms step_avg:60.95ms
step:2100/2330 train_time:127996ms step_avg:60.95ms
step:2101/2330 train_time:128056ms step_avg:60.95ms
step:2102/2330 train_time:128119ms step_avg:60.95ms
step:2103/2330 train_time:128179ms step_avg:60.95ms
step:2104/2330 train_time:128242ms step_avg:60.95ms
step:2105/2330 train_time:128302ms step_avg:60.95ms
step:2106/2330 train_time:128365ms step_avg:60.95ms
step:2107/2330 train_time:128426ms step_avg:60.95ms
step:2108/2330 train_time:128489ms step_avg:60.95ms
step:2109/2330 train_time:128548ms step_avg:60.95ms
step:2110/2330 train_time:128611ms step_avg:60.95ms
step:2111/2330 train_time:128670ms step_avg:60.95ms
step:2112/2330 train_time:128733ms step_avg:60.95ms
step:2113/2330 train_time:128791ms step_avg:60.95ms
step:2114/2330 train_time:128855ms step_avg:60.95ms
step:2115/2330 train_time:128914ms step_avg:60.95ms
step:2116/2330 train_time:128977ms step_avg:60.95ms
step:2117/2330 train_time:129037ms step_avg:60.95ms
step:2118/2330 train_time:129099ms step_avg:60.95ms
step:2119/2330 train_time:129159ms step_avg:60.95ms
step:2120/2330 train_time:129223ms step_avg:60.95ms
step:2121/2330 train_time:129282ms step_avg:60.95ms
step:2122/2330 train_time:129346ms step_avg:60.95ms
step:2123/2330 train_time:129406ms step_avg:60.95ms
step:2124/2330 train_time:129469ms step_avg:60.96ms
step:2125/2330 train_time:129529ms step_avg:60.96ms
step:2126/2330 train_time:129592ms step_avg:60.96ms
step:2127/2330 train_time:129651ms step_avg:60.95ms
step:2128/2330 train_time:129714ms step_avg:60.96ms
step:2129/2330 train_time:129773ms step_avg:60.96ms
step:2130/2330 train_time:129837ms step_avg:60.96ms
step:2131/2330 train_time:129897ms step_avg:60.96ms
step:2132/2330 train_time:129960ms step_avg:60.96ms
step:2133/2330 train_time:130020ms step_avg:60.96ms
step:2134/2330 train_time:130083ms step_avg:60.96ms
step:2135/2330 train_time:130142ms step_avg:60.96ms
step:2136/2330 train_time:130207ms step_avg:60.96ms
step:2137/2330 train_time:130265ms step_avg:60.96ms
step:2138/2330 train_time:130329ms step_avg:60.96ms
step:2139/2330 train_time:130389ms step_avg:60.96ms
step:2140/2330 train_time:130452ms step_avg:60.96ms
step:2141/2330 train_time:130511ms step_avg:60.96ms
step:2142/2330 train_time:130575ms step_avg:60.96ms
step:2143/2330 train_time:130634ms step_avg:60.96ms
step:2144/2330 train_time:130697ms step_avg:60.96ms
step:2145/2330 train_time:130756ms step_avg:60.96ms
step:2146/2330 train_time:130818ms step_avg:60.96ms
step:2147/2330 train_time:130878ms step_avg:60.96ms
step:2148/2330 train_time:130941ms step_avg:60.96ms
step:2149/2330 train_time:131001ms step_avg:60.96ms
step:2150/2330 train_time:131064ms step_avg:60.96ms
step:2151/2330 train_time:131124ms step_avg:60.96ms
step:2152/2330 train_time:131188ms step_avg:60.96ms
step:2153/2330 train_time:131247ms step_avg:60.96ms
step:2154/2330 train_time:131310ms step_avg:60.96ms
step:2155/2330 train_time:131370ms step_avg:60.96ms
step:2156/2330 train_time:131434ms step_avg:60.96ms
step:2157/2330 train_time:131494ms step_avg:60.96ms
step:2158/2330 train_time:131557ms step_avg:60.96ms
step:2159/2330 train_time:131617ms step_avg:60.96ms
step:2160/2330 train_time:131679ms step_avg:60.96ms
step:2161/2330 train_time:131739ms step_avg:60.96ms
step:2162/2330 train_time:131802ms step_avg:60.96ms
step:2163/2330 train_time:131863ms step_avg:60.96ms
step:2164/2330 train_time:131925ms step_avg:60.96ms
step:2165/2330 train_time:131984ms step_avg:60.96ms
step:2166/2330 train_time:132047ms step_avg:60.96ms
step:2167/2330 train_time:132107ms step_avg:60.96ms
step:2168/2330 train_time:132170ms step_avg:60.96ms
step:2169/2330 train_time:132230ms step_avg:60.96ms
step:2170/2330 train_time:132294ms step_avg:60.96ms
step:2171/2330 train_time:132354ms step_avg:60.96ms
step:2172/2330 train_time:132416ms step_avg:60.97ms
step:2173/2330 train_time:132475ms step_avg:60.96ms
step:2174/2330 train_time:132539ms step_avg:60.97ms
step:2175/2330 train_time:132599ms step_avg:60.96ms
step:2176/2330 train_time:132662ms step_avg:60.97ms
step:2177/2330 train_time:132721ms step_avg:60.97ms
step:2178/2330 train_time:132784ms step_avg:60.97ms
step:2179/2330 train_time:132844ms step_avg:60.97ms
step:2180/2330 train_time:132907ms step_avg:60.97ms
step:2181/2330 train_time:132968ms step_avg:60.97ms
step:2182/2330 train_time:133032ms step_avg:60.97ms
step:2183/2330 train_time:133091ms step_avg:60.97ms
step:2184/2330 train_time:133155ms step_avg:60.97ms
step:2185/2330 train_time:133214ms step_avg:60.97ms
step:2186/2330 train_time:133278ms step_avg:60.97ms
step:2187/2330 train_time:133338ms step_avg:60.97ms
step:2188/2330 train_time:133401ms step_avg:60.97ms
step:2189/2330 train_time:133460ms step_avg:60.97ms
step:2190/2330 train_time:133523ms step_avg:60.97ms
step:2191/2330 train_time:133583ms step_avg:60.97ms
step:2192/2330 train_time:133646ms step_avg:60.97ms
step:2193/2330 train_time:133705ms step_avg:60.97ms
step:2194/2330 train_time:133768ms step_avg:60.97ms
step:2195/2330 train_time:133828ms step_avg:60.97ms
step:2196/2330 train_time:133891ms step_avg:60.97ms
step:2197/2330 train_time:133951ms step_avg:60.97ms
step:2198/2330 train_time:134014ms step_avg:60.97ms
step:2199/2330 train_time:134074ms step_avg:60.97ms
step:2200/2330 train_time:134137ms step_avg:60.97ms
step:2201/2330 train_time:134196ms step_avg:60.97ms
step:2202/2330 train_time:134259ms step_avg:60.97ms
step:2203/2330 train_time:134318ms step_avg:60.97ms
step:2204/2330 train_time:134381ms step_avg:60.97ms
step:2205/2330 train_time:134441ms step_avg:60.97ms
step:2206/2330 train_time:134503ms step_avg:60.97ms
step:2207/2330 train_time:134562ms step_avg:60.97ms
step:2208/2330 train_time:134627ms step_avg:60.97ms
step:2209/2330 train_time:134686ms step_avg:60.97ms
step:2210/2330 train_time:134749ms step_avg:60.97ms
step:2211/2330 train_time:134808ms step_avg:60.97ms
step:2212/2330 train_time:134872ms step_avg:60.97ms
step:2213/2330 train_time:134932ms step_avg:60.97ms
step:2214/2330 train_time:134995ms step_avg:60.97ms
step:2215/2330 train_time:135054ms step_avg:60.97ms
step:2216/2330 train_time:135117ms step_avg:60.97ms
step:2217/2330 train_time:135176ms step_avg:60.97ms
step:2218/2330 train_time:135239ms step_avg:60.97ms
step:2219/2330 train_time:135298ms step_avg:60.97ms
step:2220/2330 train_time:135361ms step_avg:60.97ms
step:2221/2330 train_time:135421ms step_avg:60.97ms
step:2222/2330 train_time:135485ms step_avg:60.97ms
step:2223/2330 train_time:135544ms step_avg:60.97ms
step:2224/2330 train_time:135607ms step_avg:60.97ms
step:2225/2330 train_time:135667ms step_avg:60.97ms
step:2226/2330 train_time:135730ms step_avg:60.97ms
step:2227/2330 train_time:135790ms step_avg:60.97ms
step:2228/2330 train_time:135853ms step_avg:60.98ms
step:2229/2330 train_time:135912ms step_avg:60.97ms
step:2230/2330 train_time:135975ms step_avg:60.98ms
step:2231/2330 train_time:136035ms step_avg:60.97ms
step:2232/2330 train_time:136098ms step_avg:60.98ms
step:2233/2330 train_time:136158ms step_avg:60.98ms
step:2234/2330 train_time:136220ms step_avg:60.98ms
step:2235/2330 train_time:136280ms step_avg:60.98ms
step:2236/2330 train_time:136343ms step_avg:60.98ms
step:2237/2330 train_time:136402ms step_avg:60.98ms
step:2238/2330 train_time:136466ms step_avg:60.98ms
step:2239/2330 train_time:136525ms step_avg:60.98ms
step:2240/2330 train_time:136588ms step_avg:60.98ms
step:2241/2330 train_time:136647ms step_avg:60.98ms
step:2242/2330 train_time:136710ms step_avg:60.98ms
step:2243/2330 train_time:136771ms step_avg:60.98ms
step:2244/2330 train_time:136835ms step_avg:60.98ms
step:2245/2330 train_time:136894ms step_avg:60.98ms
step:2246/2330 train_time:136957ms step_avg:60.98ms
step:2247/2330 train_time:137016ms step_avg:60.98ms
step:2248/2330 train_time:137079ms step_avg:60.98ms
step:2249/2330 train_time:137140ms step_avg:60.98ms
step:2250/2330 train_time:137203ms step_avg:60.98ms
step:2250/2330 val_loss:3.5392 train_time:137274ms step_avg:61.01ms
step:2251/2330 train_time:137296ms step_avg:60.99ms
step:2252/2330 train_time:137328ms step_avg:60.98ms
step:2253/2330 train_time:137392ms step_avg:60.98ms
step:2254/2330 train_time:137461ms step_avg:60.99ms
step:2255/2330 train_time:137520ms step_avg:60.98ms
step:2256/2330 train_time:137583ms step_avg:60.99ms
step:2257/2330 train_time:137643ms step_avg:60.98ms
step:2258/2330 train_time:137705ms step_avg:60.99ms
step:2259/2330 train_time:137764ms step_avg:60.98ms
step:2260/2330 train_time:137826ms step_avg:60.99ms
step:2261/2330 train_time:137885ms step_avg:60.98ms
step:2262/2330 train_time:137948ms step_avg:60.98ms
step:2263/2330 train_time:138007ms step_avg:60.98ms
step:2264/2330 train_time:138069ms step_avg:60.98ms
step:2265/2330 train_time:138128ms step_avg:60.98ms
step:2266/2330 train_time:138190ms step_avg:60.98ms
step:2267/2330 train_time:138252ms step_avg:60.98ms
step:2268/2330 train_time:138315ms step_avg:60.99ms
step:2269/2330 train_time:138377ms step_avg:60.99ms
step:2270/2330 train_time:138442ms step_avg:60.99ms
step:2271/2330 train_time:138503ms step_avg:60.99ms
step:2272/2330 train_time:138567ms step_avg:60.99ms
step:2273/2330 train_time:138626ms step_avg:60.99ms
step:2274/2330 train_time:138690ms step_avg:60.99ms
step:2275/2330 train_time:138748ms step_avg:60.99ms
step:2276/2330 train_time:138811ms step_avg:60.99ms
step:2277/2330 train_time:138870ms step_avg:60.99ms
step:2278/2330 train_time:138933ms step_avg:60.99ms
step:2279/2330 train_time:138992ms step_avg:60.99ms
step:2280/2330 train_time:139055ms step_avg:60.99ms
step:2281/2330 train_time:139113ms step_avg:60.99ms
step:2282/2330 train_time:139176ms step_avg:60.99ms
step:2283/2330 train_time:139236ms step_avg:60.99ms
step:2284/2330 train_time:139299ms step_avg:60.99ms
step:2285/2330 train_time:139359ms step_avg:60.99ms
step:2286/2330 train_time:139423ms step_avg:60.99ms
step:2287/2330 train_time:139484ms step_avg:60.99ms
step:2288/2330 train_time:139549ms step_avg:60.99ms
step:2289/2330 train_time:139609ms step_avg:60.99ms
step:2290/2330 train_time:139671ms step_avg:60.99ms
step:2291/2330 train_time:139730ms step_avg:60.99ms
step:2292/2330 train_time:139794ms step_avg:60.99ms
step:2293/2330 train_time:139853ms step_avg:60.99ms
step:2294/2330 train_time:139916ms step_avg:60.99ms
step:2295/2330 train_time:139975ms step_avg:60.99ms
step:2296/2330 train_time:140038ms step_avg:60.99ms
step:2297/2330 train_time:140096ms step_avg:60.99ms
step:2298/2330 train_time:140160ms step_avg:60.99ms
step:2299/2330 train_time:140219ms step_avg:60.99ms
step:2300/2330 train_time:140282ms step_avg:60.99ms
step:2301/2330 train_time:140342ms step_avg:60.99ms
step:2302/2330 train_time:140406ms step_avg:60.99ms
step:2303/2330 train_time:140465ms step_avg:60.99ms
step:2304/2330 train_time:140529ms step_avg:60.99ms
step:2305/2330 train_time:140589ms step_avg:60.99ms
step:2306/2330 train_time:140653ms step_avg:60.99ms
step:2307/2330 train_time:140712ms step_avg:60.99ms
step:2308/2330 train_time:140776ms step_avg:60.99ms
step:2309/2330 train_time:140835ms step_avg:60.99ms
step:2310/2330 train_time:140898ms step_avg:60.99ms
step:2311/2330 train_time:140957ms step_avg:60.99ms
step:2312/2330 train_time:141020ms step_avg:60.99ms
step:2313/2330 train_time:141079ms step_avg:60.99ms
step:2314/2330 train_time:141142ms step_avg:60.99ms
step:2315/2330 train_time:141201ms step_avg:60.99ms
step:2316/2330 train_time:141264ms step_avg:60.99ms
step:2317/2330 train_time:141324ms step_avg:60.99ms
step:2318/2330 train_time:141387ms step_avg:61.00ms
step:2319/2330 train_time:141447ms step_avg:60.99ms
step:2320/2330 train_time:141510ms step_avg:61.00ms
step:2321/2330 train_time:141570ms step_avg:61.00ms
step:2322/2330 train_time:141634ms step_avg:61.00ms
step:2323/2330 train_time:141694ms step_avg:61.00ms
step:2324/2330 train_time:141756ms step_avg:61.00ms
step:2325/2330 train_time:141815ms step_avg:61.00ms
step:2326/2330 train_time:141879ms step_avg:61.00ms
step:2327/2330 train_time:141938ms step_avg:61.00ms
step:2328/2330 train_time:142001ms step_avg:61.00ms
step:2329/2330 train_time:142060ms step_avg:61.00ms
step:2330/2330 train_time:142122ms step_avg:61.00ms
step:2330/2330 val_loss:3.5155 train_time:142194ms step_avg:61.03ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
