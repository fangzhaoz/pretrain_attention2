import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-4"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:57:59 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:92ms step_avg:92.45ms
step:2/2330 train_time:179ms step_avg:89.74ms
step:3/2330 train_time:201ms step_avg:67.03ms
step:4/2330 train_time:237ms step_avg:59.22ms
step:5/2330 train_time:294ms step_avg:58.85ms
step:6/2330 train_time:356ms step_avg:59.31ms
step:7/2330 train_time:414ms step_avg:59.18ms
step:8/2330 train_time:476ms step_avg:59.53ms
step:9/2330 train_time:535ms step_avg:59.40ms
step:10/2330 train_time:596ms step_avg:59.64ms
step:11/2330 train_time:655ms step_avg:59.53ms
step:12/2330 train_time:716ms step_avg:59.71ms
step:13/2330 train_time:775ms step_avg:59.65ms
step:14/2330 train_time:837ms step_avg:59.80ms
step:15/2330 train_time:896ms step_avg:59.73ms
step:16/2330 train_time:958ms step_avg:59.87ms
step:17/2330 train_time:1021ms step_avg:60.04ms
step:18/2330 train_time:1085ms step_avg:60.30ms
step:19/2330 train_time:1147ms step_avg:60.38ms
step:20/2330 train_time:1210ms step_avg:60.51ms
step:21/2330 train_time:1271ms step_avg:60.54ms
step:22/2330 train_time:1335ms step_avg:60.66ms
step:23/2330 train_time:1394ms step_avg:60.61ms
step:24/2330 train_time:1456ms step_avg:60.66ms
step:25/2330 train_time:1515ms step_avg:60.60ms
step:26/2330 train_time:1577ms step_avg:60.65ms
step:27/2330 train_time:1636ms step_avg:60.59ms
step:28/2330 train_time:1698ms step_avg:60.63ms
step:29/2330 train_time:1757ms step_avg:60.57ms
step:30/2330 train_time:1819ms step_avg:60.64ms
step:31/2330 train_time:1878ms step_avg:60.57ms
step:32/2330 train_time:1940ms step_avg:60.63ms
step:33/2330 train_time:2001ms step_avg:60.65ms
step:34/2330 train_time:2065ms step_avg:60.73ms
step:35/2330 train_time:2125ms step_avg:60.73ms
step:36/2330 train_time:2189ms step_avg:60.79ms
step:37/2330 train_time:2249ms step_avg:60.77ms
step:38/2330 train_time:2312ms step_avg:60.83ms
step:39/2330 train_time:2371ms step_avg:60.80ms
step:40/2330 train_time:2434ms step_avg:60.86ms
step:41/2330 train_time:2494ms step_avg:60.82ms
step:42/2330 train_time:2556ms step_avg:60.86ms
step:43/2330 train_time:2615ms step_avg:60.82ms
step:44/2330 train_time:2678ms step_avg:60.85ms
step:45/2330 train_time:2737ms step_avg:60.81ms
step:46/2330 train_time:2799ms step_avg:60.85ms
step:47/2330 train_time:2858ms step_avg:60.81ms
step:48/2330 train_time:2920ms step_avg:60.84ms
step:49/2330 train_time:2981ms step_avg:60.83ms
step:50/2330 train_time:3043ms step_avg:60.87ms
step:51/2330 train_time:3103ms step_avg:60.84ms
step:52/2330 train_time:3165ms step_avg:60.87ms
step:53/2330 train_time:3225ms step_avg:60.86ms
step:54/2330 train_time:3288ms step_avg:60.89ms
step:55/2330 train_time:3348ms step_avg:60.87ms
step:56/2330 train_time:3411ms step_avg:60.90ms
step:57/2330 train_time:3470ms step_avg:60.88ms
step:58/2330 train_time:3534ms step_avg:60.93ms
step:59/2330 train_time:3594ms step_avg:60.91ms
step:60/2330 train_time:3656ms step_avg:60.94ms
step:61/2330 train_time:3716ms step_avg:60.92ms
step:62/2330 train_time:3778ms step_avg:60.94ms
step:63/2330 train_time:3837ms step_avg:60.91ms
step:64/2330 train_time:3900ms step_avg:60.94ms
step:65/2330 train_time:3960ms step_avg:60.92ms
step:66/2330 train_time:4022ms step_avg:60.94ms
step:67/2330 train_time:4082ms step_avg:60.93ms
step:68/2330 train_time:4145ms step_avg:60.95ms
step:69/2330 train_time:4204ms step_avg:60.93ms
step:70/2330 train_time:4267ms step_avg:60.95ms
step:71/2330 train_time:4326ms step_avg:60.94ms
step:72/2330 train_time:4389ms step_avg:60.95ms
step:73/2330 train_time:4449ms step_avg:60.94ms
step:74/2330 train_time:4512ms step_avg:60.97ms
step:75/2330 train_time:4571ms step_avg:60.95ms
step:76/2330 train_time:4634ms step_avg:60.98ms
step:77/2330 train_time:4695ms step_avg:60.97ms
step:78/2330 train_time:4758ms step_avg:61.00ms
step:79/2330 train_time:4817ms step_avg:60.98ms
step:80/2330 train_time:4880ms step_avg:61.00ms
step:81/2330 train_time:4939ms step_avg:60.98ms
step:82/2330 train_time:5002ms step_avg:60.99ms
step:83/2330 train_time:5061ms step_avg:60.98ms
step:84/2330 train_time:5124ms step_avg:61.00ms
step:85/2330 train_time:5183ms step_avg:60.97ms
step:86/2330 train_time:5245ms step_avg:60.99ms
step:87/2330 train_time:5305ms step_avg:60.98ms
step:88/2330 train_time:5367ms step_avg:60.99ms
step:89/2330 train_time:5427ms step_avg:60.98ms
step:90/2330 train_time:5490ms step_avg:61.00ms
step:91/2330 train_time:5550ms step_avg:60.98ms
step:92/2330 train_time:5613ms step_avg:61.01ms
step:93/2330 train_time:5673ms step_avg:61.00ms
step:94/2330 train_time:5736ms step_avg:61.02ms
step:95/2330 train_time:5796ms step_avg:61.01ms
step:96/2330 train_time:5858ms step_avg:61.03ms
step:97/2330 train_time:5918ms step_avg:61.01ms
step:98/2330 train_time:5981ms step_avg:61.03ms
step:99/2330 train_time:6040ms step_avg:61.01ms
step:100/2330 train_time:6103ms step_avg:61.03ms
step:101/2330 train_time:6163ms step_avg:61.02ms
step:102/2330 train_time:6225ms step_avg:61.03ms
step:103/2330 train_time:6284ms step_avg:61.01ms
step:104/2330 train_time:6347ms step_avg:61.03ms
step:105/2330 train_time:6407ms step_avg:61.02ms
step:106/2330 train_time:6469ms step_avg:61.03ms
step:107/2330 train_time:6529ms step_avg:61.02ms
step:108/2330 train_time:6592ms step_avg:61.03ms
step:109/2330 train_time:6651ms step_avg:61.02ms
step:110/2330 train_time:6715ms step_avg:61.04ms
step:111/2330 train_time:6775ms step_avg:61.04ms
step:112/2330 train_time:6838ms step_avg:61.05ms
step:113/2330 train_time:6898ms step_avg:61.04ms
step:114/2330 train_time:6961ms step_avg:61.06ms
step:115/2330 train_time:7021ms step_avg:61.05ms
step:116/2330 train_time:7084ms step_avg:61.07ms
step:117/2330 train_time:7143ms step_avg:61.05ms
step:118/2330 train_time:7206ms step_avg:61.07ms
step:119/2330 train_time:7265ms step_avg:61.05ms
step:120/2330 train_time:7328ms step_avg:61.06ms
step:121/2330 train_time:7389ms step_avg:61.07ms
step:122/2330 train_time:7450ms step_avg:61.07ms
step:123/2330 train_time:7509ms step_avg:61.05ms
step:124/2330 train_time:7573ms step_avg:61.07ms
step:125/2330 train_time:7632ms step_avg:61.06ms
step:126/2330 train_time:7695ms step_avg:61.07ms
step:127/2330 train_time:7755ms step_avg:61.06ms
step:128/2330 train_time:7818ms step_avg:61.08ms
step:129/2330 train_time:7877ms step_avg:61.06ms
step:130/2330 train_time:7940ms step_avg:61.07ms
step:131/2330 train_time:7999ms step_avg:61.06ms
step:132/2330 train_time:8062ms step_avg:61.08ms
step:133/2330 train_time:8123ms step_avg:61.07ms
step:134/2330 train_time:8185ms step_avg:61.08ms
step:135/2330 train_time:8243ms step_avg:61.06ms
step:136/2330 train_time:8306ms step_avg:61.07ms
step:137/2330 train_time:8365ms step_avg:61.06ms
step:138/2330 train_time:8427ms step_avg:61.07ms
step:139/2330 train_time:8487ms step_avg:61.06ms
step:140/2330 train_time:8550ms step_avg:61.07ms
step:141/2330 train_time:8610ms step_avg:61.06ms
step:142/2330 train_time:8674ms step_avg:61.08ms
step:143/2330 train_time:8734ms step_avg:61.07ms
step:144/2330 train_time:8796ms step_avg:61.09ms
step:145/2330 train_time:8856ms step_avg:61.08ms
step:146/2330 train_time:8919ms step_avg:61.09ms
step:147/2330 train_time:8979ms step_avg:61.08ms
step:148/2330 train_time:9041ms step_avg:61.09ms
step:149/2330 train_time:9101ms step_avg:61.08ms
step:150/2330 train_time:9164ms step_avg:61.09ms
step:151/2330 train_time:9223ms step_avg:61.08ms
step:152/2330 train_time:9286ms step_avg:61.09ms
step:153/2330 train_time:9345ms step_avg:61.08ms
step:154/2330 train_time:9407ms step_avg:61.08ms
step:155/2330 train_time:9467ms step_avg:61.08ms
step:156/2330 train_time:9530ms step_avg:61.09ms
step:157/2330 train_time:9590ms step_avg:61.09ms
step:158/2330 train_time:9653ms step_avg:61.09ms
step:159/2330 train_time:9713ms step_avg:61.09ms
step:160/2330 train_time:9777ms step_avg:61.10ms
step:161/2330 train_time:9836ms step_avg:61.09ms
step:162/2330 train_time:9899ms step_avg:61.10ms
step:163/2330 train_time:9959ms step_avg:61.10ms
step:164/2330 train_time:10021ms step_avg:61.11ms
step:165/2330 train_time:10081ms step_avg:61.10ms
step:166/2330 train_time:10144ms step_avg:61.11ms
step:167/2330 train_time:10203ms step_avg:61.10ms
step:168/2330 train_time:10266ms step_avg:61.10ms
step:169/2330 train_time:10325ms step_avg:61.09ms
step:170/2330 train_time:10387ms step_avg:61.10ms
step:171/2330 train_time:10447ms step_avg:61.09ms
step:172/2330 train_time:10509ms step_avg:61.10ms
step:173/2330 train_time:10569ms step_avg:61.09ms
step:174/2330 train_time:10631ms step_avg:61.10ms
step:175/2330 train_time:10692ms step_avg:61.10ms
step:176/2330 train_time:10755ms step_avg:61.11ms
step:177/2330 train_time:10816ms step_avg:61.11ms
step:178/2330 train_time:10879ms step_avg:61.12ms
step:179/2330 train_time:10938ms step_avg:61.11ms
step:180/2330 train_time:11001ms step_avg:61.12ms
step:181/2330 train_time:11060ms step_avg:61.11ms
step:182/2330 train_time:11123ms step_avg:61.12ms
step:183/2330 train_time:11182ms step_avg:61.11ms
step:184/2330 train_time:11245ms step_avg:61.11ms
step:185/2330 train_time:11304ms step_avg:61.10ms
step:186/2330 train_time:11366ms step_avg:61.11ms
step:187/2330 train_time:11426ms step_avg:61.10ms
step:188/2330 train_time:11489ms step_avg:61.11ms
step:189/2330 train_time:11548ms step_avg:61.10ms
step:190/2330 train_time:11611ms step_avg:61.11ms
step:191/2330 train_time:11671ms step_avg:61.10ms
step:192/2330 train_time:11734ms step_avg:61.12ms
step:193/2330 train_time:11795ms step_avg:61.11ms
step:194/2330 train_time:11857ms step_avg:61.12ms
step:195/2330 train_time:11917ms step_avg:61.12ms
step:196/2330 train_time:11980ms step_avg:61.12ms
step:197/2330 train_time:12040ms step_avg:61.11ms
step:198/2330 train_time:12102ms step_avg:61.12ms
step:199/2330 train_time:12161ms step_avg:61.11ms
step:200/2330 train_time:12224ms step_avg:61.12ms
step:201/2330 train_time:12283ms step_avg:61.11ms
step:202/2330 train_time:12346ms step_avg:61.12ms
step:203/2330 train_time:12406ms step_avg:61.11ms
step:204/2330 train_time:12468ms step_avg:61.12ms
step:205/2330 train_time:12528ms step_avg:61.11ms
step:206/2330 train_time:12591ms step_avg:61.12ms
step:207/2330 train_time:12651ms step_avg:61.12ms
step:208/2330 train_time:12715ms step_avg:61.13ms
step:209/2330 train_time:12775ms step_avg:61.13ms
step:210/2330 train_time:12838ms step_avg:61.13ms
step:211/2330 train_time:12898ms step_avg:61.13ms
step:212/2330 train_time:12961ms step_avg:61.14ms
step:213/2330 train_time:13021ms step_avg:61.13ms
step:214/2330 train_time:13084ms step_avg:61.14ms
step:215/2330 train_time:13143ms step_avg:61.13ms
step:216/2330 train_time:13205ms step_avg:61.14ms
step:217/2330 train_time:13265ms step_avg:61.13ms
step:218/2330 train_time:13328ms step_avg:61.14ms
step:219/2330 train_time:13387ms step_avg:61.13ms
step:220/2330 train_time:13450ms step_avg:61.13ms
step:221/2330 train_time:13510ms step_avg:61.13ms
step:222/2330 train_time:13572ms step_avg:61.14ms
step:223/2330 train_time:13632ms step_avg:61.13ms
step:224/2330 train_time:13695ms step_avg:61.14ms
step:225/2330 train_time:13756ms step_avg:61.14ms
step:226/2330 train_time:13819ms step_avg:61.15ms
step:227/2330 train_time:13880ms step_avg:61.14ms
step:228/2330 train_time:13942ms step_avg:61.15ms
step:229/2330 train_time:14002ms step_avg:61.14ms
step:230/2330 train_time:14065ms step_avg:61.15ms
step:231/2330 train_time:14124ms step_avg:61.14ms
step:232/2330 train_time:14186ms step_avg:61.15ms
step:233/2330 train_time:14246ms step_avg:61.14ms
step:234/2330 train_time:14308ms step_avg:61.15ms
step:235/2330 train_time:14368ms step_avg:61.14ms
step:236/2330 train_time:14430ms step_avg:61.14ms
step:237/2330 train_time:14490ms step_avg:61.14ms
step:238/2330 train_time:14552ms step_avg:61.14ms
step:239/2330 train_time:14612ms step_avg:61.14ms
step:240/2330 train_time:14675ms step_avg:61.15ms
step:241/2330 train_time:14735ms step_avg:61.14ms
step:242/2330 train_time:14799ms step_avg:61.15ms
step:243/2330 train_time:14859ms step_avg:61.15ms
step:244/2330 train_time:14922ms step_avg:61.16ms
step:245/2330 train_time:14981ms step_avg:61.15ms
step:246/2330 train_time:15044ms step_avg:61.15ms
step:247/2330 train_time:15103ms step_avg:61.15ms
step:248/2330 train_time:15165ms step_avg:61.15ms
step:249/2330 train_time:15225ms step_avg:61.14ms
step:250/2330 train_time:15287ms step_avg:61.15ms
step:250/2330 val_loss:5.0221 train_time:15351ms step_avg:61.40ms
step:251/2330 train_time:15374ms step_avg:61.25ms
step:252/2330 train_time:15411ms step_avg:61.16ms
step:253/2330 train_time:15476ms step_avg:61.17ms
step:254/2330 train_time:15542ms step_avg:61.19ms
step:255/2330 train_time:15602ms step_avg:61.18ms
step:256/2330 train_time:15665ms step_avg:61.19ms
step:257/2330 train_time:15724ms step_avg:61.18ms
step:258/2330 train_time:15787ms step_avg:61.19ms
step:259/2330 train_time:15847ms step_avg:61.18ms
step:260/2330 train_time:15909ms step_avg:61.19ms
step:261/2330 train_time:15967ms step_avg:61.18ms
step:262/2330 train_time:16029ms step_avg:61.18ms
step:263/2330 train_time:16088ms step_avg:61.17ms
step:264/2330 train_time:16151ms step_avg:61.18ms
step:265/2330 train_time:16209ms step_avg:61.17ms
step:266/2330 train_time:16274ms step_avg:61.18ms
step:267/2330 train_time:16334ms step_avg:61.18ms
step:268/2330 train_time:16397ms step_avg:61.18ms
step:269/2330 train_time:16458ms step_avg:61.18ms
step:270/2330 train_time:16521ms step_avg:61.19ms
step:271/2330 train_time:16581ms step_avg:61.19ms
step:272/2330 train_time:16645ms step_avg:61.19ms
step:273/2330 train_time:16705ms step_avg:61.19ms
step:274/2330 train_time:16768ms step_avg:61.20ms
step:275/2330 train_time:16828ms step_avg:61.19ms
step:276/2330 train_time:16890ms step_avg:61.20ms
step:277/2330 train_time:16949ms step_avg:61.19ms
step:278/2330 train_time:17011ms step_avg:61.19ms
step:279/2330 train_time:17070ms step_avg:61.18ms
step:280/2330 train_time:17133ms step_avg:61.19ms
step:281/2330 train_time:17191ms step_avg:61.18ms
step:282/2330 train_time:17254ms step_avg:61.19ms
step:283/2330 train_time:17314ms step_avg:61.18ms
step:284/2330 train_time:17376ms step_avg:61.18ms
step:285/2330 train_time:17436ms step_avg:61.18ms
step:286/2330 train_time:17499ms step_avg:61.19ms
step:287/2330 train_time:17560ms step_avg:61.18ms
step:288/2330 train_time:17623ms step_avg:61.19ms
step:289/2330 train_time:17683ms step_avg:61.19ms
step:290/2330 train_time:17747ms step_avg:61.20ms
step:291/2330 train_time:17806ms step_avg:61.19ms
step:292/2330 train_time:17869ms step_avg:61.19ms
step:293/2330 train_time:17929ms step_avg:61.19ms
step:294/2330 train_time:17991ms step_avg:61.19ms
step:295/2330 train_time:18052ms step_avg:61.19ms
step:296/2330 train_time:18115ms step_avg:61.20ms
step:297/2330 train_time:18174ms step_avg:61.19ms
step:298/2330 train_time:18236ms step_avg:61.19ms
step:299/2330 train_time:18295ms step_avg:61.19ms
step:300/2330 train_time:18358ms step_avg:61.19ms
step:301/2330 train_time:18418ms step_avg:61.19ms
step:302/2330 train_time:18480ms step_avg:61.19ms
step:303/2330 train_time:18540ms step_avg:61.19ms
step:304/2330 train_time:18604ms step_avg:61.20ms
step:305/2330 train_time:18664ms step_avg:61.19ms
step:306/2330 train_time:18728ms step_avg:61.20ms
step:307/2330 train_time:18787ms step_avg:61.20ms
step:308/2330 train_time:18851ms step_avg:61.20ms
step:309/2330 train_time:18910ms step_avg:61.20ms
step:310/2330 train_time:18973ms step_avg:61.20ms
step:311/2330 train_time:19033ms step_avg:61.20ms
step:312/2330 train_time:19095ms step_avg:61.20ms
step:313/2330 train_time:19155ms step_avg:61.20ms
step:314/2330 train_time:19217ms step_avg:61.20ms
step:315/2330 train_time:19277ms step_avg:61.20ms
step:316/2330 train_time:19339ms step_avg:61.20ms
step:317/2330 train_time:19400ms step_avg:61.20ms
step:318/2330 train_time:19462ms step_avg:61.20ms
step:319/2330 train_time:19522ms step_avg:61.20ms
step:320/2330 train_time:19585ms step_avg:61.20ms
step:321/2330 train_time:19646ms step_avg:61.20ms
step:322/2330 train_time:19709ms step_avg:61.21ms
step:323/2330 train_time:19769ms step_avg:61.20ms
step:324/2330 train_time:19832ms step_avg:61.21ms
step:325/2330 train_time:19892ms step_avg:61.20ms
step:326/2330 train_time:19954ms step_avg:61.21ms
step:327/2330 train_time:20014ms step_avg:61.21ms
step:328/2330 train_time:20077ms step_avg:61.21ms
step:329/2330 train_time:20136ms step_avg:61.20ms
step:330/2330 train_time:20199ms step_avg:61.21ms
step:331/2330 train_time:20259ms step_avg:61.20ms
step:332/2330 train_time:20321ms step_avg:61.21ms
step:333/2330 train_time:20381ms step_avg:61.20ms
step:334/2330 train_time:20444ms step_avg:61.21ms
step:335/2330 train_time:20504ms step_avg:61.21ms
step:336/2330 train_time:20567ms step_avg:61.21ms
step:337/2330 train_time:20626ms step_avg:61.21ms
step:338/2330 train_time:20689ms step_avg:61.21ms
step:339/2330 train_time:20751ms step_avg:61.21ms
step:340/2330 train_time:20814ms step_avg:61.22ms
step:341/2330 train_time:20873ms step_avg:61.21ms
step:342/2330 train_time:20936ms step_avg:61.22ms
step:343/2330 train_time:20996ms step_avg:61.21ms
step:344/2330 train_time:21059ms step_avg:61.22ms
step:345/2330 train_time:21119ms step_avg:61.21ms
step:346/2330 train_time:21182ms step_avg:61.22ms
step:347/2330 train_time:21241ms step_avg:61.21ms
step:348/2330 train_time:21305ms step_avg:61.22ms
step:349/2330 train_time:21364ms step_avg:61.22ms
step:350/2330 train_time:21427ms step_avg:61.22ms
step:351/2330 train_time:21487ms step_avg:61.22ms
step:352/2330 train_time:21550ms step_avg:61.22ms
step:353/2330 train_time:21610ms step_avg:61.22ms
step:354/2330 train_time:21673ms step_avg:61.22ms
step:355/2330 train_time:21732ms step_avg:61.22ms
step:356/2330 train_time:21795ms step_avg:61.22ms
step:357/2330 train_time:21855ms step_avg:61.22ms
step:358/2330 train_time:21918ms step_avg:61.22ms
step:359/2330 train_time:21978ms step_avg:61.22ms
step:360/2330 train_time:22042ms step_avg:61.23ms
step:361/2330 train_time:22101ms step_avg:61.22ms
step:362/2330 train_time:22164ms step_avg:61.23ms
step:363/2330 train_time:22224ms step_avg:61.22ms
step:364/2330 train_time:22286ms step_avg:61.23ms
step:365/2330 train_time:22346ms step_avg:61.22ms
step:366/2330 train_time:22409ms step_avg:61.23ms
step:367/2330 train_time:22468ms step_avg:61.22ms
step:368/2330 train_time:22531ms step_avg:61.22ms
step:369/2330 train_time:22590ms step_avg:61.22ms
step:370/2330 train_time:22653ms step_avg:61.22ms
step:371/2330 train_time:22712ms step_avg:61.22ms
step:372/2330 train_time:22775ms step_avg:61.22ms
step:373/2330 train_time:22834ms step_avg:61.22ms
step:374/2330 train_time:22897ms step_avg:61.22ms
step:375/2330 train_time:22957ms step_avg:61.22ms
step:376/2330 train_time:23020ms step_avg:61.22ms
step:377/2330 train_time:23080ms step_avg:61.22ms
step:378/2330 train_time:23144ms step_avg:61.23ms
step:379/2330 train_time:23203ms step_avg:61.22ms
step:380/2330 train_time:23266ms step_avg:61.23ms
step:381/2330 train_time:23326ms step_avg:61.22ms
step:382/2330 train_time:23389ms step_avg:61.23ms
step:383/2330 train_time:23449ms step_avg:61.22ms
step:384/2330 train_time:23512ms step_avg:61.23ms
step:385/2330 train_time:23572ms step_avg:61.22ms
step:386/2330 train_time:23634ms step_avg:61.23ms
step:387/2330 train_time:23694ms step_avg:61.22ms
step:388/2330 train_time:23757ms step_avg:61.23ms
step:389/2330 train_time:23816ms step_avg:61.22ms
step:390/2330 train_time:23879ms step_avg:61.23ms
step:391/2330 train_time:23939ms step_avg:61.23ms
step:392/2330 train_time:24002ms step_avg:61.23ms
step:393/2330 train_time:24062ms step_avg:61.23ms
step:394/2330 train_time:24125ms step_avg:61.23ms
step:395/2330 train_time:24185ms step_avg:61.23ms
step:396/2330 train_time:24248ms step_avg:61.23ms
step:397/2330 train_time:24308ms step_avg:61.23ms
step:398/2330 train_time:24371ms step_avg:61.23ms
step:399/2330 train_time:24430ms step_avg:61.23ms
step:400/2330 train_time:24493ms step_avg:61.23ms
step:401/2330 train_time:24553ms step_avg:61.23ms
step:402/2330 train_time:24615ms step_avg:61.23ms
step:403/2330 train_time:24675ms step_avg:61.23ms
step:404/2330 train_time:24738ms step_avg:61.23ms
step:405/2330 train_time:24797ms step_avg:61.23ms
step:406/2330 train_time:24860ms step_avg:61.23ms
step:407/2330 train_time:24920ms step_avg:61.23ms
step:408/2330 train_time:24983ms step_avg:61.23ms
step:409/2330 train_time:25043ms step_avg:61.23ms
step:410/2330 train_time:25106ms step_avg:61.23ms
step:411/2330 train_time:25166ms step_avg:61.23ms
step:412/2330 train_time:25228ms step_avg:61.23ms
step:413/2330 train_time:25288ms step_avg:61.23ms
step:414/2330 train_time:25351ms step_avg:61.23ms
step:415/2330 train_time:25410ms step_avg:61.23ms
step:416/2330 train_time:25473ms step_avg:61.23ms
step:417/2330 train_time:25532ms step_avg:61.23ms
step:418/2330 train_time:25595ms step_avg:61.23ms
step:419/2330 train_time:25654ms step_avg:61.23ms
step:420/2330 train_time:25716ms step_avg:61.23ms
step:421/2330 train_time:25776ms step_avg:61.23ms
step:422/2330 train_time:25839ms step_avg:61.23ms
step:423/2330 train_time:25899ms step_avg:61.23ms
step:424/2330 train_time:25962ms step_avg:61.23ms
step:425/2330 train_time:26022ms step_avg:61.23ms
step:426/2330 train_time:26085ms step_avg:61.23ms
step:427/2330 train_time:26145ms step_avg:61.23ms
step:428/2330 train_time:26208ms step_avg:61.23ms
step:429/2330 train_time:26268ms step_avg:61.23ms
step:430/2330 train_time:26332ms step_avg:61.24ms
step:431/2330 train_time:26392ms step_avg:61.23ms
step:432/2330 train_time:26455ms step_avg:61.24ms
step:433/2330 train_time:26514ms step_avg:61.23ms
step:434/2330 train_time:26577ms step_avg:61.24ms
step:435/2330 train_time:26637ms step_avg:61.23ms
step:436/2330 train_time:26700ms step_avg:61.24ms
step:437/2330 train_time:26759ms step_avg:61.23ms
step:438/2330 train_time:26822ms step_avg:61.24ms
step:439/2330 train_time:26882ms step_avg:61.23ms
step:440/2330 train_time:26945ms step_avg:61.24ms
step:441/2330 train_time:27005ms step_avg:61.24ms
step:442/2330 train_time:27068ms step_avg:61.24ms
step:443/2330 train_time:27128ms step_avg:61.24ms
step:444/2330 train_time:27190ms step_avg:61.24ms
step:445/2330 train_time:27251ms step_avg:61.24ms
step:446/2330 train_time:27314ms step_avg:61.24ms
step:447/2330 train_time:27373ms step_avg:61.24ms
step:448/2330 train_time:27436ms step_avg:61.24ms
step:449/2330 train_time:27496ms step_avg:61.24ms
step:450/2330 train_time:27558ms step_avg:61.24ms
step:451/2330 train_time:27618ms step_avg:61.24ms
step:452/2330 train_time:27681ms step_avg:61.24ms
step:453/2330 train_time:27741ms step_avg:61.24ms
step:454/2330 train_time:27805ms step_avg:61.24ms
step:455/2330 train_time:27864ms step_avg:61.24ms
step:456/2330 train_time:27927ms step_avg:61.24ms
step:457/2330 train_time:27987ms step_avg:61.24ms
step:458/2330 train_time:28050ms step_avg:61.25ms
step:459/2330 train_time:28110ms step_avg:61.24ms
step:460/2330 train_time:28173ms step_avg:61.24ms
step:461/2330 train_time:28232ms step_avg:61.24ms
step:462/2330 train_time:28295ms step_avg:61.24ms
step:463/2330 train_time:28354ms step_avg:61.24ms
step:464/2330 train_time:28417ms step_avg:61.24ms
step:465/2330 train_time:28477ms step_avg:61.24ms
step:466/2330 train_time:28540ms step_avg:61.24ms
step:467/2330 train_time:28600ms step_avg:61.24ms
step:468/2330 train_time:28663ms step_avg:61.25ms
step:469/2330 train_time:28722ms step_avg:61.24ms
step:470/2330 train_time:28785ms step_avg:61.24ms
step:471/2330 train_time:28845ms step_avg:61.24ms
step:472/2330 train_time:28908ms step_avg:61.25ms
step:473/2330 train_time:28967ms step_avg:61.24ms
step:474/2330 train_time:29030ms step_avg:61.25ms
step:475/2330 train_time:29091ms step_avg:61.24ms
step:476/2330 train_time:29154ms step_avg:61.25ms
step:477/2330 train_time:29214ms step_avg:61.24ms
step:478/2330 train_time:29277ms step_avg:61.25ms
step:479/2330 train_time:29336ms step_avg:61.24ms
step:480/2330 train_time:29399ms step_avg:61.25ms
step:481/2330 train_time:29459ms step_avg:61.25ms
step:482/2330 train_time:29522ms step_avg:61.25ms
step:483/2330 train_time:29582ms step_avg:61.25ms
step:484/2330 train_time:29645ms step_avg:61.25ms
step:485/2330 train_time:29706ms step_avg:61.25ms
step:486/2330 train_time:29768ms step_avg:61.25ms
step:487/2330 train_time:29829ms step_avg:61.25ms
step:488/2330 train_time:29891ms step_avg:61.25ms
step:489/2330 train_time:29951ms step_avg:61.25ms
step:490/2330 train_time:30014ms step_avg:61.25ms
step:491/2330 train_time:30073ms step_avg:61.25ms
step:492/2330 train_time:30136ms step_avg:61.25ms
step:493/2330 train_time:30195ms step_avg:61.25ms
step:494/2330 train_time:30258ms step_avg:61.25ms
step:495/2330 train_time:30319ms step_avg:61.25ms
step:496/2330 train_time:30382ms step_avg:61.25ms
step:497/2330 train_time:30442ms step_avg:61.25ms
step:498/2330 train_time:30505ms step_avg:61.25ms
step:499/2330 train_time:30565ms step_avg:61.25ms
step:500/2330 train_time:30627ms step_avg:61.25ms
step:500/2330 val_loss:4.4797 train_time:30691ms step_avg:61.38ms
step:501/2330 train_time:30713ms step_avg:61.30ms
step:502/2330 train_time:30754ms step_avg:61.26ms
step:503/2330 train_time:30821ms step_avg:61.27ms
step:504/2330 train_time:30889ms step_avg:61.29ms
step:505/2330 train_time:30948ms step_avg:61.28ms
step:506/2330 train_time:31011ms step_avg:61.29ms
step:507/2330 train_time:31070ms step_avg:61.28ms
step:508/2330 train_time:31132ms step_avg:61.28ms
step:509/2330 train_time:31192ms step_avg:61.28ms
step:510/2330 train_time:31255ms step_avg:61.28ms
step:511/2330 train_time:31314ms step_avg:61.28ms
step:512/2330 train_time:31377ms step_avg:61.28ms
step:513/2330 train_time:31436ms step_avg:61.28ms
step:514/2330 train_time:31499ms step_avg:61.28ms
step:515/2330 train_time:31558ms step_avg:61.28ms
step:516/2330 train_time:31620ms step_avg:61.28ms
step:517/2330 train_time:31680ms step_avg:61.28ms
step:518/2330 train_time:31744ms step_avg:61.28ms
step:519/2330 train_time:31806ms step_avg:61.28ms
step:520/2330 train_time:31870ms step_avg:61.29ms
step:521/2330 train_time:31930ms step_avg:61.29ms
step:522/2330 train_time:31994ms step_avg:61.29ms
step:523/2330 train_time:32054ms step_avg:61.29ms
step:524/2330 train_time:32117ms step_avg:61.29ms
step:525/2330 train_time:32176ms step_avg:61.29ms
step:526/2330 train_time:32238ms step_avg:61.29ms
step:527/2330 train_time:32298ms step_avg:61.29ms
step:528/2330 train_time:32361ms step_avg:61.29ms
step:529/2330 train_time:32420ms step_avg:61.28ms
step:530/2330 train_time:32482ms step_avg:61.29ms
step:531/2330 train_time:32541ms step_avg:61.28ms
step:532/2330 train_time:32603ms step_avg:61.28ms
step:533/2330 train_time:32663ms step_avg:61.28ms
step:534/2330 train_time:32726ms step_avg:61.29ms
step:535/2330 train_time:32787ms step_avg:61.28ms
step:536/2330 train_time:32850ms step_avg:61.29ms
step:537/2330 train_time:32910ms step_avg:61.29ms
step:538/2330 train_time:32974ms step_avg:61.29ms
step:539/2330 train_time:33035ms step_avg:61.29ms
step:540/2330 train_time:33098ms step_avg:61.29ms
step:541/2330 train_time:33158ms step_avg:61.29ms
step:542/2330 train_time:33220ms step_avg:61.29ms
step:543/2330 train_time:33280ms step_avg:61.29ms
step:544/2330 train_time:33343ms step_avg:61.29ms
step:545/2330 train_time:33402ms step_avg:61.29ms
step:546/2330 train_time:33464ms step_avg:61.29ms
step:547/2330 train_time:33525ms step_avg:61.29ms
step:548/2330 train_time:33588ms step_avg:61.29ms
step:549/2330 train_time:33647ms step_avg:61.29ms
step:550/2330 train_time:33710ms step_avg:61.29ms
step:551/2330 train_time:33770ms step_avg:61.29ms
step:552/2330 train_time:33833ms step_avg:61.29ms
step:553/2330 train_time:33893ms step_avg:61.29ms
step:554/2330 train_time:33957ms step_avg:61.29ms
step:555/2330 train_time:34017ms step_avg:61.29ms
step:556/2330 train_time:34081ms step_avg:61.30ms
step:557/2330 train_time:34141ms step_avg:61.29ms
step:558/2330 train_time:34203ms step_avg:61.30ms
step:559/2330 train_time:34262ms step_avg:61.29ms
step:560/2330 train_time:34325ms step_avg:61.29ms
step:561/2330 train_time:34385ms step_avg:61.29ms
step:562/2330 train_time:34448ms step_avg:61.30ms
step:563/2330 train_time:34507ms step_avg:61.29ms
step:564/2330 train_time:34569ms step_avg:61.29ms
step:565/2330 train_time:34629ms step_avg:61.29ms
step:566/2330 train_time:34692ms step_avg:61.29ms
step:567/2330 train_time:34752ms step_avg:61.29ms
step:568/2330 train_time:34815ms step_avg:61.29ms
step:569/2330 train_time:34875ms step_avg:61.29ms
step:570/2330 train_time:34939ms step_avg:61.30ms
step:571/2330 train_time:34999ms step_avg:61.30ms
step:572/2330 train_time:35063ms step_avg:61.30ms
step:573/2330 train_time:35122ms step_avg:61.30ms
step:574/2330 train_time:35185ms step_avg:61.30ms
step:575/2330 train_time:35245ms step_avg:61.30ms
step:576/2330 train_time:35307ms step_avg:61.30ms
step:577/2330 train_time:35367ms step_avg:61.30ms
step:578/2330 train_time:35429ms step_avg:61.30ms
step:579/2330 train_time:35489ms step_avg:61.29ms
step:580/2330 train_time:35551ms step_avg:61.30ms
step:581/2330 train_time:35612ms step_avg:61.29ms
step:582/2330 train_time:35675ms step_avg:61.30ms
step:583/2330 train_time:35735ms step_avg:61.30ms
step:584/2330 train_time:35798ms step_avg:61.30ms
step:585/2330 train_time:35858ms step_avg:61.30ms
step:586/2330 train_time:35921ms step_avg:61.30ms
step:587/2330 train_time:35981ms step_avg:61.30ms
step:588/2330 train_time:36045ms step_avg:61.30ms
step:589/2330 train_time:36105ms step_avg:61.30ms
step:590/2330 train_time:36168ms step_avg:61.30ms
step:591/2330 train_time:36227ms step_avg:61.30ms
step:592/2330 train_time:36290ms step_avg:61.30ms
step:593/2330 train_time:36350ms step_avg:61.30ms
step:594/2330 train_time:36413ms step_avg:61.30ms
step:595/2330 train_time:36473ms step_avg:61.30ms
step:596/2330 train_time:36536ms step_avg:61.30ms
step:597/2330 train_time:36595ms step_avg:61.30ms
step:598/2330 train_time:36658ms step_avg:61.30ms
step:599/2330 train_time:36718ms step_avg:61.30ms
step:600/2330 train_time:36781ms step_avg:61.30ms
step:601/2330 train_time:36841ms step_avg:61.30ms
step:602/2330 train_time:36904ms step_avg:61.30ms
step:603/2330 train_time:36963ms step_avg:61.30ms
step:604/2330 train_time:37026ms step_avg:61.30ms
step:605/2330 train_time:37086ms step_avg:61.30ms
step:606/2330 train_time:37149ms step_avg:61.30ms
step:607/2330 train_time:37209ms step_avg:61.30ms
step:608/2330 train_time:37272ms step_avg:61.30ms
step:609/2330 train_time:37331ms step_avg:61.30ms
step:610/2330 train_time:37394ms step_avg:61.30ms
step:611/2330 train_time:37454ms step_avg:61.30ms
step:612/2330 train_time:37517ms step_avg:61.30ms
step:613/2330 train_time:37577ms step_avg:61.30ms
step:614/2330 train_time:37640ms step_avg:61.30ms
step:615/2330 train_time:37700ms step_avg:61.30ms
step:616/2330 train_time:37763ms step_avg:61.30ms
step:617/2330 train_time:37823ms step_avg:61.30ms
step:618/2330 train_time:37886ms step_avg:61.30ms
step:619/2330 train_time:37945ms step_avg:61.30ms
step:620/2330 train_time:38008ms step_avg:61.30ms
step:621/2330 train_time:38068ms step_avg:61.30ms
step:622/2330 train_time:38130ms step_avg:61.30ms
step:623/2330 train_time:38191ms step_avg:61.30ms
step:624/2330 train_time:38254ms step_avg:61.30ms
step:625/2330 train_time:38314ms step_avg:61.30ms
step:626/2330 train_time:38378ms step_avg:61.31ms
step:627/2330 train_time:38439ms step_avg:61.31ms
step:628/2330 train_time:38501ms step_avg:61.31ms
step:629/2330 train_time:38560ms step_avg:61.30ms
step:630/2330 train_time:38622ms step_avg:61.30ms
step:631/2330 train_time:38681ms step_avg:61.30ms
step:632/2330 train_time:38744ms step_avg:61.30ms
step:633/2330 train_time:38803ms step_avg:61.30ms
step:634/2330 train_time:38866ms step_avg:61.30ms
step:635/2330 train_time:38926ms step_avg:61.30ms
step:636/2330 train_time:38989ms step_avg:61.30ms
step:637/2330 train_time:39048ms step_avg:61.30ms
step:638/2330 train_time:39110ms step_avg:61.30ms
step:639/2330 train_time:39170ms step_avg:61.30ms
step:640/2330 train_time:39233ms step_avg:61.30ms
step:641/2330 train_time:39293ms step_avg:61.30ms
step:642/2330 train_time:39357ms step_avg:61.30ms
step:643/2330 train_time:39417ms step_avg:61.30ms
step:644/2330 train_time:39480ms step_avg:61.31ms
step:645/2330 train_time:39540ms step_avg:61.30ms
step:646/2330 train_time:39603ms step_avg:61.30ms
step:647/2330 train_time:39663ms step_avg:61.30ms
step:648/2330 train_time:39725ms step_avg:61.30ms
step:649/2330 train_time:39785ms step_avg:61.30ms
step:650/2330 train_time:39848ms step_avg:61.30ms
step:651/2330 train_time:39907ms step_avg:61.30ms
step:652/2330 train_time:39970ms step_avg:61.30ms
step:653/2330 train_time:40030ms step_avg:61.30ms
step:654/2330 train_time:40093ms step_avg:61.30ms
step:655/2330 train_time:40153ms step_avg:61.30ms
step:656/2330 train_time:40216ms step_avg:61.30ms
step:657/2330 train_time:40276ms step_avg:61.30ms
step:658/2330 train_time:40339ms step_avg:61.31ms
step:659/2330 train_time:40399ms step_avg:61.30ms
step:660/2330 train_time:40462ms step_avg:61.31ms
step:661/2330 train_time:40522ms step_avg:61.30ms
step:662/2330 train_time:40585ms step_avg:61.31ms
step:663/2330 train_time:40645ms step_avg:61.30ms
step:664/2330 train_time:40708ms step_avg:61.31ms
step:665/2330 train_time:40768ms step_avg:61.30ms
step:666/2330 train_time:40831ms step_avg:61.31ms
step:667/2330 train_time:40890ms step_avg:61.30ms
step:668/2330 train_time:40953ms step_avg:61.31ms
step:669/2330 train_time:41013ms step_avg:61.31ms
step:670/2330 train_time:41076ms step_avg:61.31ms
step:671/2330 train_time:41136ms step_avg:61.31ms
step:672/2330 train_time:41199ms step_avg:61.31ms
step:673/2330 train_time:41259ms step_avg:61.31ms
step:674/2330 train_time:41322ms step_avg:61.31ms
step:675/2330 train_time:41381ms step_avg:61.31ms
step:676/2330 train_time:41444ms step_avg:61.31ms
step:677/2330 train_time:41504ms step_avg:61.31ms
step:678/2330 train_time:41566ms step_avg:61.31ms
step:679/2330 train_time:41626ms step_avg:61.30ms
step:680/2330 train_time:41689ms step_avg:61.31ms
step:681/2330 train_time:41748ms step_avg:61.30ms
step:682/2330 train_time:41811ms step_avg:61.31ms
step:683/2330 train_time:41871ms step_avg:61.30ms
step:684/2330 train_time:41935ms step_avg:61.31ms
step:685/2330 train_time:41994ms step_avg:61.31ms
step:686/2330 train_time:42057ms step_avg:61.31ms
step:687/2330 train_time:42117ms step_avg:61.31ms
step:688/2330 train_time:42180ms step_avg:61.31ms
step:689/2330 train_time:42240ms step_avg:61.31ms
step:690/2330 train_time:42303ms step_avg:61.31ms
step:691/2330 train_time:42363ms step_avg:61.31ms
step:692/2330 train_time:42426ms step_avg:61.31ms
step:693/2330 train_time:42486ms step_avg:61.31ms
step:694/2330 train_time:42548ms step_avg:61.31ms
step:695/2330 train_time:42608ms step_avg:61.31ms
step:696/2330 train_time:42670ms step_avg:61.31ms
step:697/2330 train_time:42730ms step_avg:61.31ms
step:698/2330 train_time:42793ms step_avg:61.31ms
step:699/2330 train_time:42853ms step_avg:61.31ms
step:700/2330 train_time:42916ms step_avg:61.31ms
step:701/2330 train_time:42977ms step_avg:61.31ms
step:702/2330 train_time:43040ms step_avg:61.31ms
step:703/2330 train_time:43099ms step_avg:61.31ms
step:704/2330 train_time:43162ms step_avg:61.31ms
step:705/2330 train_time:43222ms step_avg:61.31ms
step:706/2330 train_time:43285ms step_avg:61.31ms
step:707/2330 train_time:43345ms step_avg:61.31ms
step:708/2330 train_time:43408ms step_avg:61.31ms
step:709/2330 train_time:43468ms step_avg:61.31ms
step:710/2330 train_time:43531ms step_avg:61.31ms
step:711/2330 train_time:43590ms step_avg:61.31ms
step:712/2330 train_time:43653ms step_avg:61.31ms
step:713/2330 train_time:43714ms step_avg:61.31ms
step:714/2330 train_time:43778ms step_avg:61.31ms
step:715/2330 train_time:43837ms step_avg:61.31ms
step:716/2330 train_time:43900ms step_avg:61.31ms
step:717/2330 train_time:43960ms step_avg:61.31ms
step:718/2330 train_time:44023ms step_avg:61.31ms
step:719/2330 train_time:44083ms step_avg:61.31ms
step:720/2330 train_time:44145ms step_avg:61.31ms
step:721/2330 train_time:44205ms step_avg:61.31ms
step:722/2330 train_time:44268ms step_avg:61.31ms
step:723/2330 train_time:44328ms step_avg:61.31ms
step:724/2330 train_time:44391ms step_avg:61.31ms
step:725/2330 train_time:44451ms step_avg:61.31ms
step:726/2330 train_time:44514ms step_avg:61.31ms
step:727/2330 train_time:44575ms step_avg:61.31ms
step:728/2330 train_time:44638ms step_avg:61.32ms
step:729/2330 train_time:44698ms step_avg:61.31ms
step:730/2330 train_time:44761ms step_avg:61.32ms
step:731/2330 train_time:44822ms step_avg:61.32ms
step:732/2330 train_time:44884ms step_avg:61.32ms
step:733/2330 train_time:44944ms step_avg:61.31ms
step:734/2330 train_time:45006ms step_avg:61.32ms
step:735/2330 train_time:45066ms step_avg:61.31ms
step:736/2330 train_time:45129ms step_avg:61.32ms
step:737/2330 train_time:45189ms step_avg:61.31ms
step:738/2330 train_time:45252ms step_avg:61.32ms
step:739/2330 train_time:45312ms step_avg:61.31ms
step:740/2330 train_time:45375ms step_avg:61.32ms
step:741/2330 train_time:45435ms step_avg:61.32ms
step:742/2330 train_time:45498ms step_avg:61.32ms
step:743/2330 train_time:45558ms step_avg:61.32ms
step:744/2330 train_time:45621ms step_avg:61.32ms
step:745/2330 train_time:45681ms step_avg:61.32ms
step:746/2330 train_time:45744ms step_avg:61.32ms
step:747/2330 train_time:45803ms step_avg:61.32ms
step:748/2330 train_time:45866ms step_avg:61.32ms
step:749/2330 train_time:45925ms step_avg:61.32ms
step:750/2330 train_time:45988ms step_avg:61.32ms
step:750/2330 val_loss:4.1446 train_time:46053ms step_avg:61.40ms
step:751/2330 train_time:46074ms step_avg:61.35ms
step:752/2330 train_time:46113ms step_avg:61.32ms
step:753/2330 train_time:46178ms step_avg:61.32ms
step:754/2330 train_time:46244ms step_avg:61.33ms
step:755/2330 train_time:46304ms step_avg:61.33ms
step:756/2330 train_time:46367ms step_avg:61.33ms
step:757/2330 train_time:46426ms step_avg:61.33ms
step:758/2330 train_time:46489ms step_avg:61.33ms
step:759/2330 train_time:46548ms step_avg:61.33ms
step:760/2330 train_time:46610ms step_avg:61.33ms
step:761/2330 train_time:46669ms step_avg:61.33ms
step:762/2330 train_time:46732ms step_avg:61.33ms
step:763/2330 train_time:46791ms step_avg:61.32ms
step:764/2330 train_time:46853ms step_avg:61.33ms
step:765/2330 train_time:46912ms step_avg:61.32ms
step:766/2330 train_time:46976ms step_avg:61.33ms
step:767/2330 train_time:47038ms step_avg:61.33ms
step:768/2330 train_time:47102ms step_avg:61.33ms
step:769/2330 train_time:47165ms step_avg:61.33ms
step:770/2330 train_time:47230ms step_avg:61.34ms
step:771/2330 train_time:47292ms step_avg:61.34ms
step:772/2330 train_time:47355ms step_avg:61.34ms
step:773/2330 train_time:47417ms step_avg:61.34ms
step:774/2330 train_time:47480ms step_avg:61.34ms
step:775/2330 train_time:47540ms step_avg:61.34ms
step:776/2330 train_time:47602ms step_avg:61.34ms
step:777/2330 train_time:47662ms step_avg:61.34ms
step:778/2330 train_time:47725ms step_avg:61.34ms
step:779/2330 train_time:47786ms step_avg:61.34ms
step:780/2330 train_time:47849ms step_avg:61.34ms
step:781/2330 train_time:47909ms step_avg:61.34ms
step:782/2330 train_time:47973ms step_avg:61.35ms
step:783/2330 train_time:48034ms step_avg:61.35ms
step:784/2330 train_time:48097ms step_avg:61.35ms
step:785/2330 train_time:48158ms step_avg:61.35ms
step:786/2330 train_time:48222ms step_avg:61.35ms
step:787/2330 train_time:48282ms step_avg:61.35ms
step:788/2330 train_time:48346ms step_avg:61.35ms
step:789/2330 train_time:48408ms step_avg:61.35ms
step:790/2330 train_time:48472ms step_avg:61.36ms
step:791/2330 train_time:48533ms step_avg:61.36ms
step:792/2330 train_time:48597ms step_avg:61.36ms
step:793/2330 train_time:48657ms step_avg:61.36ms
step:794/2330 train_time:48721ms step_avg:61.36ms
step:795/2330 train_time:48781ms step_avg:61.36ms
step:796/2330 train_time:48844ms step_avg:61.36ms
step:797/2330 train_time:48904ms step_avg:61.36ms
step:798/2330 train_time:48967ms step_avg:61.36ms
step:799/2330 train_time:49028ms step_avg:61.36ms
step:800/2330 train_time:49093ms step_avg:61.37ms
step:801/2330 train_time:49154ms step_avg:61.37ms
step:802/2330 train_time:49217ms step_avg:61.37ms
step:803/2330 train_time:49278ms step_avg:61.37ms
step:804/2330 train_time:49342ms step_avg:61.37ms
step:805/2330 train_time:49402ms step_avg:61.37ms
step:806/2330 train_time:49466ms step_avg:61.37ms
step:807/2330 train_time:49526ms step_avg:61.37ms
step:808/2330 train_time:49591ms step_avg:61.37ms
step:809/2330 train_time:49652ms step_avg:61.37ms
step:810/2330 train_time:49716ms step_avg:61.38ms
step:811/2330 train_time:49777ms step_avg:61.38ms
step:812/2330 train_time:49840ms step_avg:61.38ms
step:813/2330 train_time:49901ms step_avg:61.38ms
step:814/2330 train_time:49964ms step_avg:61.38ms
step:815/2330 train_time:50024ms step_avg:61.38ms
step:816/2330 train_time:50088ms step_avg:61.38ms
step:817/2330 train_time:50149ms step_avg:61.38ms
step:818/2330 train_time:50214ms step_avg:61.39ms
step:819/2330 train_time:50275ms step_avg:61.39ms
step:820/2330 train_time:50338ms step_avg:61.39ms
step:821/2330 train_time:50399ms step_avg:61.39ms
step:822/2330 train_time:50463ms step_avg:61.39ms
step:823/2330 train_time:50523ms step_avg:61.39ms
step:824/2330 train_time:50587ms step_avg:61.39ms
step:825/2330 train_time:50648ms step_avg:61.39ms
step:826/2330 train_time:50712ms step_avg:61.39ms
step:827/2330 train_time:50773ms step_avg:61.39ms
step:828/2330 train_time:50836ms step_avg:61.40ms
step:829/2330 train_time:50897ms step_avg:61.40ms
step:830/2330 train_time:50961ms step_avg:61.40ms
step:831/2330 train_time:51021ms step_avg:61.40ms
step:832/2330 train_time:51084ms step_avg:61.40ms
step:833/2330 train_time:51144ms step_avg:61.40ms
step:834/2330 train_time:51208ms step_avg:61.40ms
step:835/2330 train_time:51269ms step_avg:61.40ms
step:836/2330 train_time:51333ms step_avg:61.40ms
step:837/2330 train_time:51394ms step_avg:61.40ms
step:838/2330 train_time:51458ms step_avg:61.41ms
step:839/2330 train_time:51518ms step_avg:61.40ms
step:840/2330 train_time:51582ms step_avg:61.41ms
step:841/2330 train_time:51642ms step_avg:61.41ms
step:842/2330 train_time:51705ms step_avg:61.41ms
step:843/2330 train_time:51766ms step_avg:61.41ms
step:844/2330 train_time:51831ms step_avg:61.41ms
step:845/2330 train_time:51891ms step_avg:61.41ms
step:846/2330 train_time:51955ms step_avg:61.41ms
step:847/2330 train_time:52015ms step_avg:61.41ms
step:848/2330 train_time:52079ms step_avg:61.41ms
step:849/2330 train_time:52140ms step_avg:61.41ms
step:850/2330 train_time:52203ms step_avg:61.42ms
step:851/2330 train_time:52263ms step_avg:61.41ms
step:852/2330 train_time:52326ms step_avg:61.42ms
step:853/2330 train_time:52387ms step_avg:61.42ms
step:854/2330 train_time:52452ms step_avg:61.42ms
step:855/2330 train_time:52513ms step_avg:61.42ms
step:856/2330 train_time:52577ms step_avg:61.42ms
step:857/2330 train_time:52637ms step_avg:61.42ms
step:858/2330 train_time:52701ms step_avg:61.42ms
step:859/2330 train_time:52761ms step_avg:61.42ms
step:860/2330 train_time:52824ms step_avg:61.42ms
step:861/2330 train_time:52884ms step_avg:61.42ms
step:862/2330 train_time:52948ms step_avg:61.42ms
step:863/2330 train_time:53009ms step_avg:61.42ms
step:864/2330 train_time:53073ms step_avg:61.43ms
step:865/2330 train_time:53133ms step_avg:61.43ms
step:866/2330 train_time:53197ms step_avg:61.43ms
step:867/2330 train_time:53258ms step_avg:61.43ms
step:868/2330 train_time:53321ms step_avg:61.43ms
step:869/2330 train_time:53381ms step_avg:61.43ms
step:870/2330 train_time:53445ms step_avg:61.43ms
step:871/2330 train_time:53505ms step_avg:61.43ms
step:872/2330 train_time:53570ms step_avg:61.43ms
step:873/2330 train_time:53631ms step_avg:61.43ms
step:874/2330 train_time:53695ms step_avg:61.44ms
step:875/2330 train_time:53755ms step_avg:61.43ms
step:876/2330 train_time:53820ms step_avg:61.44ms
step:877/2330 train_time:53880ms step_avg:61.44ms
step:878/2330 train_time:53943ms step_avg:61.44ms
step:879/2330 train_time:54003ms step_avg:61.44ms
step:880/2330 train_time:54067ms step_avg:61.44ms
step:881/2330 train_time:54128ms step_avg:61.44ms
step:882/2330 train_time:54192ms step_avg:61.44ms
step:883/2330 train_time:54252ms step_avg:61.44ms
step:884/2330 train_time:54316ms step_avg:61.44ms
step:885/2330 train_time:54377ms step_avg:61.44ms
step:886/2330 train_time:54440ms step_avg:61.45ms
step:887/2330 train_time:54501ms step_avg:61.44ms
step:888/2330 train_time:54563ms step_avg:61.45ms
step:889/2330 train_time:54624ms step_avg:61.44ms
step:890/2330 train_time:54688ms step_avg:61.45ms
step:891/2330 train_time:54750ms step_avg:61.45ms
step:892/2330 train_time:54814ms step_avg:61.45ms
step:893/2330 train_time:54874ms step_avg:61.45ms
step:894/2330 train_time:54938ms step_avg:61.45ms
step:895/2330 train_time:54998ms step_avg:61.45ms
step:896/2330 train_time:55062ms step_avg:61.45ms
step:897/2330 train_time:55122ms step_avg:61.45ms
step:898/2330 train_time:55185ms step_avg:61.45ms
step:899/2330 train_time:55246ms step_avg:61.45ms
step:900/2330 train_time:55310ms step_avg:61.46ms
step:901/2330 train_time:55372ms step_avg:61.46ms
step:902/2330 train_time:55436ms step_avg:61.46ms
step:903/2330 train_time:55496ms step_avg:61.46ms
step:904/2330 train_time:55559ms step_avg:61.46ms
step:905/2330 train_time:55620ms step_avg:61.46ms
step:906/2330 train_time:55683ms step_avg:61.46ms
step:907/2330 train_time:55744ms step_avg:61.46ms
step:908/2330 train_time:55808ms step_avg:61.46ms
step:909/2330 train_time:55868ms step_avg:61.46ms
step:910/2330 train_time:55931ms step_avg:61.46ms
step:911/2330 train_time:55992ms step_avg:61.46ms
step:912/2330 train_time:56056ms step_avg:61.46ms
step:913/2330 train_time:56116ms step_avg:61.46ms
step:914/2330 train_time:56180ms step_avg:61.47ms
step:915/2330 train_time:56240ms step_avg:61.46ms
step:916/2330 train_time:56304ms step_avg:61.47ms
step:917/2330 train_time:56365ms step_avg:61.47ms
step:918/2330 train_time:56429ms step_avg:61.47ms
step:919/2330 train_time:56490ms step_avg:61.47ms
step:920/2330 train_time:56554ms step_avg:61.47ms
step:921/2330 train_time:56614ms step_avg:61.47ms
step:922/2330 train_time:56678ms step_avg:61.47ms
step:923/2330 train_time:56738ms step_avg:61.47ms
step:924/2330 train_time:56802ms step_avg:61.47ms
step:925/2330 train_time:56861ms step_avg:61.47ms
step:926/2330 train_time:56925ms step_avg:61.47ms
step:927/2330 train_time:56985ms step_avg:61.47ms
step:928/2330 train_time:57049ms step_avg:61.48ms
step:929/2330 train_time:57110ms step_avg:61.47ms
step:930/2330 train_time:57174ms step_avg:61.48ms
step:931/2330 train_time:57234ms step_avg:61.48ms
step:932/2330 train_time:57298ms step_avg:61.48ms
step:933/2330 train_time:57359ms step_avg:61.48ms
step:934/2330 train_time:57422ms step_avg:61.48ms
step:935/2330 train_time:57482ms step_avg:61.48ms
step:936/2330 train_time:57545ms step_avg:61.48ms
step:937/2330 train_time:57606ms step_avg:61.48ms
step:938/2330 train_time:57670ms step_avg:61.48ms
step:939/2330 train_time:57731ms step_avg:61.48ms
step:940/2330 train_time:57795ms step_avg:61.48ms
step:941/2330 train_time:57855ms step_avg:61.48ms
step:942/2330 train_time:57919ms step_avg:61.48ms
step:943/2330 train_time:57980ms step_avg:61.48ms
step:944/2330 train_time:58044ms step_avg:61.49ms
step:945/2330 train_time:58104ms step_avg:61.49ms
step:946/2330 train_time:58168ms step_avg:61.49ms
step:947/2330 train_time:58229ms step_avg:61.49ms
step:948/2330 train_time:58293ms step_avg:61.49ms
step:949/2330 train_time:58354ms step_avg:61.49ms
step:950/2330 train_time:58418ms step_avg:61.49ms
step:951/2330 train_time:58478ms step_avg:61.49ms
step:952/2330 train_time:58542ms step_avg:61.49ms
step:953/2330 train_time:58602ms step_avg:61.49ms
step:954/2330 train_time:58666ms step_avg:61.49ms
step:955/2330 train_time:58727ms step_avg:61.49ms
step:956/2330 train_time:58790ms step_avg:61.50ms
step:957/2330 train_time:58851ms step_avg:61.50ms
step:958/2330 train_time:58916ms step_avg:61.50ms
step:959/2330 train_time:58976ms step_avg:61.50ms
step:960/2330 train_time:59040ms step_avg:61.50ms
step:961/2330 train_time:59101ms step_avg:61.50ms
step:962/2330 train_time:59163ms step_avg:61.50ms
step:963/2330 train_time:59224ms step_avg:61.50ms
step:964/2330 train_time:59287ms step_avg:61.50ms
step:965/2330 train_time:59349ms step_avg:61.50ms
step:966/2330 train_time:59413ms step_avg:61.50ms
step:967/2330 train_time:59474ms step_avg:61.50ms
step:968/2330 train_time:59537ms step_avg:61.51ms
step:969/2330 train_time:59597ms step_avg:61.50ms
step:970/2330 train_time:59661ms step_avg:61.51ms
step:971/2330 train_time:59721ms step_avg:61.50ms
step:972/2330 train_time:59785ms step_avg:61.51ms
step:973/2330 train_time:59846ms step_avg:61.51ms
step:974/2330 train_time:59909ms step_avg:61.51ms
step:975/2330 train_time:59971ms step_avg:61.51ms
step:976/2330 train_time:60035ms step_avg:61.51ms
step:977/2330 train_time:60096ms step_avg:61.51ms
step:978/2330 train_time:60160ms step_avg:61.51ms
step:979/2330 train_time:60221ms step_avg:61.51ms
step:980/2330 train_time:60284ms step_avg:61.51ms
step:981/2330 train_time:60344ms step_avg:61.51ms
step:982/2330 train_time:60408ms step_avg:61.52ms
step:983/2330 train_time:60469ms step_avg:61.51ms
step:984/2330 train_time:60533ms step_avg:61.52ms
step:985/2330 train_time:60593ms step_avg:61.52ms
step:986/2330 train_time:60657ms step_avg:61.52ms
step:987/2330 train_time:60718ms step_avg:61.52ms
step:988/2330 train_time:60781ms step_avg:61.52ms
step:989/2330 train_time:60841ms step_avg:61.52ms
step:990/2330 train_time:60905ms step_avg:61.52ms
step:991/2330 train_time:60966ms step_avg:61.52ms
step:992/2330 train_time:61031ms step_avg:61.52ms
step:993/2330 train_time:61092ms step_avg:61.52ms
step:994/2330 train_time:61156ms step_avg:61.53ms
step:995/2330 train_time:61216ms step_avg:61.52ms
step:996/2330 train_time:61280ms step_avg:61.53ms
step:997/2330 train_time:61340ms step_avg:61.53ms
step:998/2330 train_time:61404ms step_avg:61.53ms
step:999/2330 train_time:61464ms step_avg:61.53ms
step:1000/2330 train_time:61528ms step_avg:61.53ms
step:1000/2330 val_loss:3.9621 train_time:61593ms step_avg:61.59ms
step:1001/2330 train_time:61616ms step_avg:61.55ms
step:1002/2330 train_time:61654ms step_avg:61.53ms
step:1003/2330 train_time:61721ms step_avg:61.54ms
step:1004/2330 train_time:61789ms step_avg:61.54ms
step:1005/2330 train_time:61850ms step_avg:61.54ms
step:1006/2330 train_time:61913ms step_avg:61.54ms
step:1007/2330 train_time:61973ms step_avg:61.54ms
step:1008/2330 train_time:62036ms step_avg:61.54ms
step:1009/2330 train_time:62095ms step_avg:61.54ms
step:1010/2330 train_time:62158ms step_avg:61.54ms
step:1011/2330 train_time:62219ms step_avg:61.54ms
step:1012/2330 train_time:62282ms step_avg:61.54ms
step:1013/2330 train_time:62341ms step_avg:61.54ms
step:1014/2330 train_time:62404ms step_avg:61.54ms
step:1015/2330 train_time:62463ms step_avg:61.54ms
step:1016/2330 train_time:62529ms step_avg:61.54ms
step:1017/2330 train_time:62592ms step_avg:61.55ms
step:1018/2330 train_time:62657ms step_avg:61.55ms
step:1019/2330 train_time:62721ms step_avg:61.55ms
step:1020/2330 train_time:62786ms step_avg:61.55ms
step:1021/2330 train_time:62847ms step_avg:61.55ms
step:1022/2330 train_time:62911ms step_avg:61.56ms
step:1023/2330 train_time:62971ms step_avg:61.56ms
step:1024/2330 train_time:63034ms step_avg:61.56ms
step:1025/2330 train_time:63094ms step_avg:61.56ms
step:1026/2330 train_time:63157ms step_avg:61.56ms
step:1027/2330 train_time:63217ms step_avg:61.55ms
step:1028/2330 train_time:63280ms step_avg:61.56ms
step:1029/2330 train_time:63340ms step_avg:61.55ms
step:1030/2330 train_time:63404ms step_avg:61.56ms
step:1031/2330 train_time:63465ms step_avg:61.56ms
step:1032/2330 train_time:63529ms step_avg:61.56ms
step:1033/2330 train_time:63590ms step_avg:61.56ms
step:1034/2330 train_time:63655ms step_avg:61.56ms
step:1035/2330 train_time:63718ms step_avg:61.56ms
step:1036/2330 train_time:63782ms step_avg:61.57ms
step:1037/2330 train_time:63843ms step_avg:61.57ms
step:1038/2330 train_time:63907ms step_avg:61.57ms
step:1039/2330 train_time:63967ms step_avg:61.57ms
step:1040/2330 train_time:64031ms step_avg:61.57ms
step:1041/2330 train_time:64091ms step_avg:61.57ms
step:1042/2330 train_time:64154ms step_avg:61.57ms
step:1043/2330 train_time:64215ms step_avg:61.57ms
step:1044/2330 train_time:64279ms step_avg:61.57ms
step:1045/2330 train_time:64339ms step_avg:61.57ms
step:1046/2330 train_time:64403ms step_avg:61.57ms
step:1047/2330 train_time:64463ms step_avg:61.57ms
step:1048/2330 train_time:64527ms step_avg:61.57ms
step:1049/2330 train_time:64587ms step_avg:61.57ms
step:1050/2330 train_time:64651ms step_avg:61.57ms
step:1051/2330 train_time:64712ms step_avg:61.57ms
step:1052/2330 train_time:64777ms step_avg:61.58ms
step:1053/2330 train_time:64839ms step_avg:61.58ms
step:1054/2330 train_time:64903ms step_avg:61.58ms
step:1055/2330 train_time:64963ms step_avg:61.58ms
step:1056/2330 train_time:65028ms step_avg:61.58ms
step:1057/2330 train_time:65089ms step_avg:61.58ms
step:1058/2330 train_time:65151ms step_avg:61.58ms
step:1059/2330 train_time:65211ms step_avg:61.58ms
step:1060/2330 train_time:65275ms step_avg:61.58ms
step:1061/2330 train_time:65335ms step_avg:61.58ms
step:1062/2330 train_time:65400ms step_avg:61.58ms
step:1063/2330 train_time:65462ms step_avg:61.58ms
step:1064/2330 train_time:65526ms step_avg:61.58ms
step:1065/2330 train_time:65586ms step_avg:61.58ms
step:1066/2330 train_time:65650ms step_avg:61.59ms
step:1067/2330 train_time:65710ms step_avg:61.58ms
step:1068/2330 train_time:65774ms step_avg:61.59ms
step:1069/2330 train_time:65835ms step_avg:61.59ms
step:1070/2330 train_time:65900ms step_avg:61.59ms
step:1071/2330 train_time:65961ms step_avg:61.59ms
step:1072/2330 train_time:66025ms step_avg:61.59ms
step:1073/2330 train_time:66086ms step_avg:61.59ms
step:1074/2330 train_time:66150ms step_avg:61.59ms
step:1075/2330 train_time:66211ms step_avg:61.59ms
step:1076/2330 train_time:66274ms step_avg:61.59ms
step:1077/2330 train_time:66334ms step_avg:61.59ms
step:1078/2330 train_time:66398ms step_avg:61.59ms
step:1079/2330 train_time:66459ms step_avg:61.59ms
step:1080/2330 train_time:66523ms step_avg:61.60ms
step:1081/2330 train_time:66583ms step_avg:61.59ms
step:1082/2330 train_time:66647ms step_avg:61.60ms
step:1083/2330 train_time:66708ms step_avg:61.60ms
step:1084/2330 train_time:66772ms step_avg:61.60ms
step:1085/2330 train_time:66832ms step_avg:61.60ms
step:1086/2330 train_time:66896ms step_avg:61.60ms
step:1087/2330 train_time:66957ms step_avg:61.60ms
step:1088/2330 train_time:67022ms step_avg:61.60ms
step:1089/2330 train_time:67082ms step_avg:61.60ms
step:1090/2330 train_time:67146ms step_avg:61.60ms
step:1091/2330 train_time:67207ms step_avg:61.60ms
step:1092/2330 train_time:67270ms step_avg:61.60ms
step:1093/2330 train_time:67330ms step_avg:61.60ms
step:1094/2330 train_time:67394ms step_avg:61.60ms
step:1095/2330 train_time:67454ms step_avg:61.60ms
step:1096/2330 train_time:67519ms step_avg:61.60ms
step:1097/2330 train_time:67580ms step_avg:61.60ms
step:1098/2330 train_time:67644ms step_avg:61.61ms
step:1099/2330 train_time:67705ms step_avg:61.61ms
step:1100/2330 train_time:67769ms step_avg:61.61ms
step:1101/2330 train_time:67829ms step_avg:61.61ms
step:1102/2330 train_time:67893ms step_avg:61.61ms
step:1103/2330 train_time:67954ms step_avg:61.61ms
step:1104/2330 train_time:68018ms step_avg:61.61ms
step:1105/2330 train_time:68080ms step_avg:61.61ms
step:1106/2330 train_time:68143ms step_avg:61.61ms
step:1107/2330 train_time:68203ms step_avg:61.61ms
step:1108/2330 train_time:68267ms step_avg:61.61ms
step:1109/2330 train_time:68328ms step_avg:61.61ms
step:1110/2330 train_time:68392ms step_avg:61.61ms
step:1111/2330 train_time:68452ms step_avg:61.61ms
step:1112/2330 train_time:68516ms step_avg:61.61ms
step:1113/2330 train_time:68577ms step_avg:61.61ms
step:1114/2330 train_time:68641ms step_avg:61.62ms
step:1115/2330 train_time:68703ms step_avg:61.62ms
step:1116/2330 train_time:68766ms step_avg:61.62ms
step:1117/2330 train_time:68827ms step_avg:61.62ms
step:1118/2330 train_time:68890ms step_avg:61.62ms
step:1119/2330 train_time:68950ms step_avg:61.62ms
step:1120/2330 train_time:69014ms step_avg:61.62ms
step:1121/2330 train_time:69075ms step_avg:61.62ms
step:1122/2330 train_time:69139ms step_avg:61.62ms
step:1123/2330 train_time:69200ms step_avg:61.62ms
step:1124/2330 train_time:69264ms step_avg:61.62ms
step:1125/2330 train_time:69325ms step_avg:61.62ms
step:1126/2330 train_time:69389ms step_avg:61.62ms
step:1127/2330 train_time:69450ms step_avg:61.62ms
step:1128/2330 train_time:69512ms step_avg:61.62ms
step:1129/2330 train_time:69573ms step_avg:61.62ms
step:1130/2330 train_time:69638ms step_avg:61.63ms
step:1131/2330 train_time:69699ms step_avg:61.63ms
step:1132/2330 train_time:69764ms step_avg:61.63ms
step:1133/2330 train_time:69824ms step_avg:61.63ms
step:1134/2330 train_time:69887ms step_avg:61.63ms
step:1135/2330 train_time:69948ms step_avg:61.63ms
step:1136/2330 train_time:70011ms step_avg:61.63ms
step:1137/2330 train_time:70072ms step_avg:61.63ms
step:1138/2330 train_time:70135ms step_avg:61.63ms
step:1139/2330 train_time:70197ms step_avg:61.63ms
step:1140/2330 train_time:70261ms step_avg:61.63ms
step:1141/2330 train_time:70321ms step_avg:61.63ms
step:1142/2330 train_time:70385ms step_avg:61.63ms
step:1143/2330 train_time:70446ms step_avg:61.63ms
step:1144/2330 train_time:70509ms step_avg:61.63ms
step:1145/2330 train_time:70569ms step_avg:61.63ms
step:1146/2330 train_time:70633ms step_avg:61.63ms
step:1147/2330 train_time:70695ms step_avg:61.63ms
step:1148/2330 train_time:70759ms step_avg:61.64ms
step:1149/2330 train_time:70821ms step_avg:61.64ms
step:1150/2330 train_time:70885ms step_avg:61.64ms
step:1151/2330 train_time:70945ms step_avg:61.64ms
step:1152/2330 train_time:71009ms step_avg:61.64ms
step:1153/2330 train_time:71070ms step_avg:61.64ms
step:1154/2330 train_time:71133ms step_avg:61.64ms
step:1155/2330 train_time:71194ms step_avg:61.64ms
step:1156/2330 train_time:71258ms step_avg:61.64ms
step:1157/2330 train_time:71319ms step_avg:61.64ms
step:1158/2330 train_time:71382ms step_avg:61.64ms
step:1159/2330 train_time:71443ms step_avg:61.64ms
step:1160/2330 train_time:71506ms step_avg:61.64ms
step:1161/2330 train_time:71566ms step_avg:61.64ms
step:1162/2330 train_time:71630ms step_avg:61.64ms
step:1163/2330 train_time:71690ms step_avg:61.64ms
step:1164/2330 train_time:71753ms step_avg:61.64ms
step:1165/2330 train_time:71815ms step_avg:61.64ms
step:1166/2330 train_time:71879ms step_avg:61.65ms
step:1167/2330 train_time:71940ms step_avg:61.65ms
step:1168/2330 train_time:72004ms step_avg:61.65ms
step:1169/2330 train_time:72065ms step_avg:61.65ms
step:1170/2330 train_time:72129ms step_avg:61.65ms
step:1171/2330 train_time:72189ms step_avg:61.65ms
step:1172/2330 train_time:72252ms step_avg:61.65ms
step:1173/2330 train_time:72313ms step_avg:61.65ms
step:1174/2330 train_time:72377ms step_avg:61.65ms
step:1175/2330 train_time:72438ms step_avg:61.65ms
step:1176/2330 train_time:72502ms step_avg:61.65ms
step:1177/2330 train_time:72563ms step_avg:61.65ms
step:1178/2330 train_time:72627ms step_avg:61.65ms
step:1179/2330 train_time:72688ms step_avg:61.65ms
step:1180/2330 train_time:72752ms step_avg:61.65ms
step:1181/2330 train_time:72811ms step_avg:61.65ms
step:1182/2330 train_time:72875ms step_avg:61.65ms
step:1183/2330 train_time:72936ms step_avg:61.65ms
step:1184/2330 train_time:73001ms step_avg:61.66ms
step:1185/2330 train_time:73062ms step_avg:61.66ms
step:1186/2330 train_time:73126ms step_avg:61.66ms
step:1187/2330 train_time:73187ms step_avg:61.66ms
step:1188/2330 train_time:73251ms step_avg:61.66ms
step:1189/2330 train_time:73311ms step_avg:61.66ms
step:1190/2330 train_time:73374ms step_avg:61.66ms
step:1191/2330 train_time:73435ms step_avg:61.66ms
step:1192/2330 train_time:73500ms step_avg:61.66ms
step:1193/2330 train_time:73562ms step_avg:61.66ms
step:1194/2330 train_time:73626ms step_avg:61.66ms
step:1195/2330 train_time:73686ms step_avg:61.66ms
step:1196/2330 train_time:73750ms step_avg:61.66ms
step:1197/2330 train_time:73810ms step_avg:61.66ms
step:1198/2330 train_time:73874ms step_avg:61.66ms
step:1199/2330 train_time:73934ms step_avg:61.66ms
step:1200/2330 train_time:73998ms step_avg:61.67ms
step:1201/2330 train_time:74059ms step_avg:61.66ms
step:1202/2330 train_time:74124ms step_avg:61.67ms
step:1203/2330 train_time:74185ms step_avg:61.67ms
step:1204/2330 train_time:74248ms step_avg:61.67ms
step:1205/2330 train_time:74309ms step_avg:61.67ms
step:1206/2330 train_time:74372ms step_avg:61.67ms
step:1207/2330 train_time:74432ms step_avg:61.67ms
step:1208/2330 train_time:74496ms step_avg:61.67ms
step:1209/2330 train_time:74559ms step_avg:61.67ms
step:1210/2330 train_time:74624ms step_avg:61.67ms
step:1211/2330 train_time:74684ms step_avg:61.67ms
step:1212/2330 train_time:74748ms step_avg:61.67ms
step:1213/2330 train_time:74808ms step_avg:61.67ms
step:1214/2330 train_time:74871ms step_avg:61.67ms
step:1215/2330 train_time:74931ms step_avg:61.67ms
step:1216/2330 train_time:74995ms step_avg:61.67ms
step:1217/2330 train_time:75057ms step_avg:61.67ms
step:1218/2330 train_time:75121ms step_avg:61.68ms
step:1219/2330 train_time:75181ms step_avg:61.67ms
step:1220/2330 train_time:75245ms step_avg:61.68ms
step:1221/2330 train_time:75306ms step_avg:61.68ms
step:1222/2330 train_time:75369ms step_avg:61.68ms
step:1223/2330 train_time:75429ms step_avg:61.68ms
step:1224/2330 train_time:75493ms step_avg:61.68ms
step:1225/2330 train_time:75555ms step_avg:61.68ms
step:1226/2330 train_time:75619ms step_avg:61.68ms
step:1227/2330 train_time:75681ms step_avg:61.68ms
step:1228/2330 train_time:75744ms step_avg:61.68ms
step:1229/2330 train_time:75805ms step_avg:61.68ms
step:1230/2330 train_time:75868ms step_avg:61.68ms
step:1231/2330 train_time:75929ms step_avg:61.68ms
step:1232/2330 train_time:75992ms step_avg:61.68ms
step:1233/2330 train_time:76053ms step_avg:61.68ms
step:1234/2330 train_time:76117ms step_avg:61.68ms
step:1235/2330 train_time:76178ms step_avg:61.68ms
step:1236/2330 train_time:76242ms step_avg:61.68ms
step:1237/2330 train_time:76302ms step_avg:61.68ms
step:1238/2330 train_time:76366ms step_avg:61.68ms
step:1239/2330 train_time:76427ms step_avg:61.68ms
step:1240/2330 train_time:76490ms step_avg:61.69ms
step:1241/2330 train_time:76550ms step_avg:61.68ms
step:1242/2330 train_time:76614ms step_avg:61.69ms
step:1243/2330 train_time:76674ms step_avg:61.68ms
step:1244/2330 train_time:76739ms step_avg:61.69ms
step:1245/2330 train_time:76801ms step_avg:61.69ms
step:1246/2330 train_time:76864ms step_avg:61.69ms
step:1247/2330 train_time:76925ms step_avg:61.69ms
step:1248/2330 train_time:76989ms step_avg:61.69ms
step:1249/2330 train_time:77049ms step_avg:61.69ms
step:1250/2330 train_time:77113ms step_avg:61.69ms
step:1250/2330 val_loss:3.8827 train_time:77178ms step_avg:61.74ms
step:1251/2330 train_time:77200ms step_avg:61.71ms
step:1252/2330 train_time:77240ms step_avg:61.69ms
step:1253/2330 train_time:77308ms step_avg:61.70ms
step:1254/2330 train_time:77373ms step_avg:61.70ms
step:1255/2330 train_time:77434ms step_avg:61.70ms
step:1256/2330 train_time:77498ms step_avg:61.70ms
step:1257/2330 train_time:77558ms step_avg:61.70ms
step:1258/2330 train_time:77621ms step_avg:61.70ms
step:1259/2330 train_time:77682ms step_avg:61.70ms
step:1260/2330 train_time:77745ms step_avg:61.70ms
step:1261/2330 train_time:77805ms step_avg:61.70ms
step:1262/2330 train_time:77868ms step_avg:61.70ms
step:1263/2330 train_time:77927ms step_avg:61.70ms
step:1264/2330 train_time:77990ms step_avg:61.70ms
step:1265/2330 train_time:78049ms step_avg:61.70ms
step:1266/2330 train_time:78113ms step_avg:61.70ms
step:1267/2330 train_time:78176ms step_avg:61.70ms
step:1268/2330 train_time:78240ms step_avg:61.70ms
step:1269/2330 train_time:78310ms step_avg:61.71ms
step:1270/2330 train_time:78367ms step_avg:61.71ms
step:1271/2330 train_time:78428ms step_avg:61.71ms
step:1272/2330 train_time:78492ms step_avg:61.71ms
step:1273/2330 train_time:78553ms step_avg:61.71ms
step:1274/2330 train_time:78617ms step_avg:61.71ms
step:1275/2330 train_time:78677ms step_avg:61.71ms
step:1276/2330 train_time:78741ms step_avg:61.71ms
step:1277/2330 train_time:78801ms step_avg:61.71ms
step:1278/2330 train_time:78865ms step_avg:61.71ms
step:1279/2330 train_time:78926ms step_avg:61.71ms
step:1280/2330 train_time:78990ms step_avg:61.71ms
step:1281/2330 train_time:79050ms step_avg:61.71ms
step:1282/2330 train_time:79113ms step_avg:61.71ms
step:1283/2330 train_time:79174ms step_avg:61.71ms
step:1284/2330 train_time:79238ms step_avg:61.71ms
step:1285/2330 train_time:79300ms step_avg:61.71ms
step:1286/2330 train_time:79364ms step_avg:61.71ms
step:1287/2330 train_time:79425ms step_avg:61.71ms
step:1288/2330 train_time:79490ms step_avg:61.72ms
step:1289/2330 train_time:79551ms step_avg:61.72ms
step:1290/2330 train_time:79615ms step_avg:61.72ms
step:1291/2330 train_time:79675ms step_avg:61.72ms
step:1292/2330 train_time:79738ms step_avg:61.72ms
step:1293/2330 train_time:79798ms step_avg:61.72ms
step:1294/2330 train_time:79862ms step_avg:61.72ms
step:1295/2330 train_time:79922ms step_avg:61.72ms
step:1296/2330 train_time:79986ms step_avg:61.72ms
step:1297/2330 train_time:80047ms step_avg:61.72ms
step:1298/2330 train_time:80110ms step_avg:61.72ms
step:1299/2330 train_time:80171ms step_avg:61.72ms
step:1300/2330 train_time:80234ms step_avg:61.72ms
step:1301/2330 train_time:80295ms step_avg:61.72ms
step:1302/2330 train_time:80359ms step_avg:61.72ms
step:1303/2330 train_time:80420ms step_avg:61.72ms
step:1304/2330 train_time:80485ms step_avg:61.72ms
step:1305/2330 train_time:80546ms step_avg:61.72ms
step:1306/2330 train_time:80610ms step_avg:61.72ms
step:1307/2330 train_time:80671ms step_avg:61.72ms
step:1308/2330 train_time:80736ms step_avg:61.72ms
step:1309/2330 train_time:80796ms step_avg:61.72ms
step:1310/2330 train_time:80859ms step_avg:61.72ms
step:1311/2330 train_time:80919ms step_avg:61.72ms
step:1312/2330 train_time:80983ms step_avg:61.72ms
step:1313/2330 train_time:81044ms step_avg:61.72ms
step:1314/2330 train_time:81109ms step_avg:61.73ms
step:1315/2330 train_time:81169ms step_avg:61.73ms
step:1316/2330 train_time:81232ms step_avg:61.73ms
step:1317/2330 train_time:81293ms step_avg:61.73ms
step:1318/2330 train_time:81357ms step_avg:61.73ms
step:1319/2330 train_time:81417ms step_avg:61.73ms
step:1320/2330 train_time:81481ms step_avg:61.73ms
step:1321/2330 train_time:81543ms step_avg:61.73ms
step:1322/2330 train_time:81607ms step_avg:61.73ms
step:1323/2330 train_time:81668ms step_avg:61.73ms
step:1324/2330 train_time:81732ms step_avg:61.73ms
step:1325/2330 train_time:81792ms step_avg:61.73ms
step:1326/2330 train_time:81856ms step_avg:61.73ms
step:1327/2330 train_time:81916ms step_avg:61.73ms
step:1328/2330 train_time:81980ms step_avg:61.73ms
step:1329/2330 train_time:82041ms step_avg:61.73ms
step:1330/2330 train_time:82105ms step_avg:61.73ms
step:1331/2330 train_time:82166ms step_avg:61.73ms
step:1332/2330 train_time:82230ms step_avg:61.73ms
step:1333/2330 train_time:82291ms step_avg:61.73ms
step:1334/2330 train_time:82355ms step_avg:61.74ms
step:1335/2330 train_time:82415ms step_avg:61.73ms
step:1336/2330 train_time:82479ms step_avg:61.74ms
step:1337/2330 train_time:82540ms step_avg:61.73ms
step:1338/2330 train_time:82604ms step_avg:61.74ms
step:1339/2330 train_time:82665ms step_avg:61.74ms
step:1340/2330 train_time:82729ms step_avg:61.74ms
step:1341/2330 train_time:82790ms step_avg:61.74ms
step:1342/2330 train_time:82854ms step_avg:61.74ms
step:1343/2330 train_time:82915ms step_avg:61.74ms
step:1344/2330 train_time:82978ms step_avg:61.74ms
step:1345/2330 train_time:83038ms step_avg:61.74ms
step:1346/2330 train_time:83102ms step_avg:61.74ms
step:1347/2330 train_time:83163ms step_avg:61.74ms
step:1348/2330 train_time:83228ms step_avg:61.74ms
step:1349/2330 train_time:83288ms step_avg:61.74ms
step:1350/2330 train_time:83352ms step_avg:61.74ms
step:1351/2330 train_time:83413ms step_avg:61.74ms
step:1352/2330 train_time:83477ms step_avg:61.74ms
step:1353/2330 train_time:83537ms step_avg:61.74ms
step:1354/2330 train_time:83601ms step_avg:61.74ms
step:1355/2330 train_time:83663ms step_avg:61.74ms
step:1356/2330 train_time:83727ms step_avg:61.75ms
step:1357/2330 train_time:83787ms step_avg:61.74ms
step:1358/2330 train_time:83851ms step_avg:61.75ms
step:1359/2330 train_time:83912ms step_avg:61.75ms
step:1360/2330 train_time:83976ms step_avg:61.75ms
step:1361/2330 train_time:84036ms step_avg:61.75ms
step:1362/2330 train_time:84099ms step_avg:61.75ms
step:1363/2330 train_time:84160ms step_avg:61.75ms
step:1364/2330 train_time:84224ms step_avg:61.75ms
step:1365/2330 train_time:84286ms step_avg:61.75ms
step:1366/2330 train_time:84351ms step_avg:61.75ms
step:1367/2330 train_time:84411ms step_avg:61.75ms
step:1368/2330 train_time:84476ms step_avg:61.75ms
step:1369/2330 train_time:84536ms step_avg:61.75ms
step:1370/2330 train_time:84599ms step_avg:61.75ms
step:1371/2330 train_time:84660ms step_avg:61.75ms
step:1372/2330 train_time:84724ms step_avg:61.75ms
step:1373/2330 train_time:84785ms step_avg:61.75ms
step:1374/2330 train_time:84849ms step_avg:61.75ms
step:1375/2330 train_time:84910ms step_avg:61.75ms
step:1376/2330 train_time:84974ms step_avg:61.75ms
step:1377/2330 train_time:85035ms step_avg:61.75ms
step:1378/2330 train_time:85098ms step_avg:61.75ms
step:1379/2330 train_time:85158ms step_avg:61.75ms
step:1380/2330 train_time:85223ms step_avg:61.76ms
step:1381/2330 train_time:85283ms step_avg:61.75ms
step:1382/2330 train_time:85348ms step_avg:61.76ms
step:1383/2330 train_time:85408ms step_avg:61.76ms
step:1384/2330 train_time:85472ms step_avg:61.76ms
step:1385/2330 train_time:85533ms step_avg:61.76ms
step:1386/2330 train_time:85596ms step_avg:61.76ms
step:1387/2330 train_time:85656ms step_avg:61.76ms
step:1388/2330 train_time:85720ms step_avg:61.76ms
step:1389/2330 train_time:85781ms step_avg:61.76ms
step:1390/2330 train_time:85845ms step_avg:61.76ms
step:1391/2330 train_time:85906ms step_avg:61.76ms
step:1392/2330 train_time:85970ms step_avg:61.76ms
step:1393/2330 train_time:86031ms step_avg:61.76ms
step:1394/2330 train_time:86095ms step_avg:61.76ms
step:1395/2330 train_time:86155ms step_avg:61.76ms
step:1396/2330 train_time:86218ms step_avg:61.76ms
step:1397/2330 train_time:86279ms step_avg:61.76ms
step:1398/2330 train_time:86343ms step_avg:61.76ms
step:1399/2330 train_time:86405ms step_avg:61.76ms
step:1400/2330 train_time:86469ms step_avg:61.76ms
step:1401/2330 train_time:86529ms step_avg:61.76ms
step:1402/2330 train_time:86593ms step_avg:61.76ms
step:1403/2330 train_time:86654ms step_avg:61.76ms
step:1404/2330 train_time:86717ms step_avg:61.76ms
step:1405/2330 train_time:86777ms step_avg:61.76ms
step:1406/2330 train_time:86842ms step_avg:61.77ms
step:1407/2330 train_time:86903ms step_avg:61.77ms
step:1408/2330 train_time:86967ms step_avg:61.77ms
step:1409/2330 train_time:87028ms step_avg:61.77ms
step:1410/2330 train_time:87091ms step_avg:61.77ms
step:1411/2330 train_time:87152ms step_avg:61.77ms
step:1412/2330 train_time:87215ms step_avg:61.77ms
step:1413/2330 train_time:87276ms step_avg:61.77ms
step:1414/2330 train_time:87339ms step_avg:61.77ms
step:1415/2330 train_time:87401ms step_avg:61.77ms
step:1416/2330 train_time:87465ms step_avg:61.77ms
step:1417/2330 train_time:87526ms step_avg:61.77ms
step:1418/2330 train_time:87590ms step_avg:61.77ms
step:1419/2330 train_time:87651ms step_avg:61.77ms
step:1420/2330 train_time:87714ms step_avg:61.77ms
step:1421/2330 train_time:87774ms step_avg:61.77ms
step:1422/2330 train_time:87838ms step_avg:61.77ms
step:1423/2330 train_time:87898ms step_avg:61.77ms
step:1424/2330 train_time:87963ms step_avg:61.77ms
step:1425/2330 train_time:88024ms step_avg:61.77ms
step:1426/2330 train_time:88088ms step_avg:61.77ms
step:1427/2330 train_time:88149ms step_avg:61.77ms
step:1428/2330 train_time:88213ms step_avg:61.77ms
step:1429/2330 train_time:88273ms step_avg:61.77ms
step:1430/2330 train_time:88337ms step_avg:61.77ms
step:1431/2330 train_time:88397ms step_avg:61.77ms
step:1432/2330 train_time:88462ms step_avg:61.78ms
step:1433/2330 train_time:88524ms step_avg:61.78ms
step:1434/2330 train_time:88588ms step_avg:61.78ms
step:1435/2330 train_time:88649ms step_avg:61.78ms
step:1436/2330 train_time:88713ms step_avg:61.78ms
step:1437/2330 train_time:88773ms step_avg:61.78ms
step:1438/2330 train_time:88836ms step_avg:61.78ms
step:1439/2330 train_time:88897ms step_avg:61.78ms
step:1440/2330 train_time:88961ms step_avg:61.78ms
step:1441/2330 train_time:89022ms step_avg:61.78ms
step:1442/2330 train_time:89087ms step_avg:61.78ms
step:1443/2330 train_time:89148ms step_avg:61.78ms
step:1444/2330 train_time:89211ms step_avg:61.78ms
step:1445/2330 train_time:89271ms step_avg:61.78ms
step:1446/2330 train_time:89335ms step_avg:61.78ms
step:1447/2330 train_time:89396ms step_avg:61.78ms
step:1448/2330 train_time:89459ms step_avg:61.78ms
step:1449/2330 train_time:89520ms step_avg:61.78ms
step:1450/2330 train_time:89585ms step_avg:61.78ms
step:1451/2330 train_time:89645ms step_avg:61.78ms
step:1452/2330 train_time:89710ms step_avg:61.78ms
step:1453/2330 train_time:89770ms step_avg:61.78ms
step:1454/2330 train_time:89834ms step_avg:61.78ms
step:1455/2330 train_time:89895ms step_avg:61.78ms
step:1456/2330 train_time:89958ms step_avg:61.78ms
step:1457/2330 train_time:90019ms step_avg:61.78ms
step:1458/2330 train_time:90083ms step_avg:61.79ms
step:1459/2330 train_time:90144ms step_avg:61.79ms
step:1460/2330 train_time:90208ms step_avg:61.79ms
step:1461/2330 train_time:90268ms step_avg:61.79ms
step:1462/2330 train_time:90333ms step_avg:61.79ms
step:1463/2330 train_time:90393ms step_avg:61.79ms
step:1464/2330 train_time:90457ms step_avg:61.79ms
step:1465/2330 train_time:90518ms step_avg:61.79ms
step:1466/2330 train_time:90582ms step_avg:61.79ms
step:1467/2330 train_time:90643ms step_avg:61.79ms
step:1468/2330 train_time:90708ms step_avg:61.79ms
step:1469/2330 train_time:90768ms step_avg:61.79ms
step:1470/2330 train_time:90832ms step_avg:61.79ms
step:1471/2330 train_time:90893ms step_avg:61.79ms
step:1472/2330 train_time:90956ms step_avg:61.79ms
step:1473/2330 train_time:91017ms step_avg:61.79ms
step:1474/2330 train_time:91080ms step_avg:61.79ms
step:1475/2330 train_time:91142ms step_avg:61.79ms
step:1476/2330 train_time:91206ms step_avg:61.79ms
step:1477/2330 train_time:91267ms step_avg:61.79ms
step:1478/2330 train_time:91331ms step_avg:61.79ms
step:1479/2330 train_time:91392ms step_avg:61.79ms
step:1480/2330 train_time:91455ms step_avg:61.79ms
step:1481/2330 train_time:91515ms step_avg:61.79ms
step:1482/2330 train_time:91579ms step_avg:61.79ms
step:1483/2330 train_time:91640ms step_avg:61.79ms
step:1484/2330 train_time:91705ms step_avg:61.80ms
step:1485/2330 train_time:91766ms step_avg:61.80ms
step:1486/2330 train_time:91830ms step_avg:61.80ms
step:1487/2330 train_time:91891ms step_avg:61.80ms
step:1488/2330 train_time:91954ms step_avg:61.80ms
step:1489/2330 train_time:92014ms step_avg:61.80ms
step:1490/2330 train_time:92078ms step_avg:61.80ms
step:1491/2330 train_time:92138ms step_avg:61.80ms
step:1492/2330 train_time:92202ms step_avg:61.80ms
step:1493/2330 train_time:92263ms step_avg:61.80ms
step:1494/2330 train_time:92327ms step_avg:61.80ms
step:1495/2330 train_time:92388ms step_avg:61.80ms
step:1496/2330 train_time:92452ms step_avg:61.80ms
step:1497/2330 train_time:92513ms step_avg:61.80ms
step:1498/2330 train_time:92576ms step_avg:61.80ms
step:1499/2330 train_time:92636ms step_avg:61.80ms
step:1500/2330 train_time:92700ms step_avg:61.80ms
step:1500/2330 val_loss:3.7915 train_time:92766ms step_avg:61.84ms
step:1501/2330 train_time:92791ms step_avg:61.82ms
step:1502/2330 train_time:92829ms step_avg:61.80ms
step:1503/2330 train_time:92895ms step_avg:61.81ms
step:1504/2330 train_time:92962ms step_avg:61.81ms
step:1505/2330 train_time:93023ms step_avg:61.81ms
step:1506/2330 train_time:93088ms step_avg:61.81ms
step:1507/2330 train_time:93148ms step_avg:61.81ms
step:1508/2330 train_time:93211ms step_avg:61.81ms
step:1509/2330 train_time:93271ms step_avg:61.81ms
step:1510/2330 train_time:93334ms step_avg:61.81ms
step:1511/2330 train_time:93393ms step_avg:61.81ms
step:1512/2330 train_time:93457ms step_avg:61.81ms
step:1513/2330 train_time:93516ms step_avg:61.81ms
step:1514/2330 train_time:93579ms step_avg:61.81ms
step:1515/2330 train_time:93639ms step_avg:61.81ms
step:1516/2330 train_time:93702ms step_avg:61.81ms
step:1517/2330 train_time:93763ms step_avg:61.81ms
step:1518/2330 train_time:93828ms step_avg:61.81ms
step:1519/2330 train_time:93891ms step_avg:61.81ms
step:1520/2330 train_time:93956ms step_avg:61.81ms
step:1521/2330 train_time:94019ms step_avg:61.81ms
step:1522/2330 train_time:94084ms step_avg:61.82ms
step:1523/2330 train_time:94145ms step_avg:61.82ms
step:1524/2330 train_time:94208ms step_avg:61.82ms
step:1525/2330 train_time:94269ms step_avg:61.82ms
step:1526/2330 train_time:94333ms step_avg:61.82ms
step:1527/2330 train_time:94392ms step_avg:61.82ms
step:1528/2330 train_time:94455ms step_avg:61.82ms
step:1529/2330 train_time:94515ms step_avg:61.81ms
step:1530/2330 train_time:94578ms step_avg:61.82ms
step:1531/2330 train_time:94639ms step_avg:61.82ms
step:1532/2330 train_time:94704ms step_avg:61.82ms
step:1533/2330 train_time:94766ms step_avg:61.82ms
step:1534/2330 train_time:94831ms step_avg:61.82ms
step:1535/2330 train_time:94893ms step_avg:61.82ms
step:1536/2330 train_time:94958ms step_avg:61.82ms
step:1537/2330 train_time:95021ms step_avg:61.82ms
step:1538/2330 train_time:95085ms step_avg:61.82ms
step:1539/2330 train_time:95146ms step_avg:61.82ms
step:1540/2330 train_time:95210ms step_avg:61.82ms
step:1541/2330 train_time:95271ms step_avg:61.82ms
step:1542/2330 train_time:95335ms step_avg:61.83ms
step:1543/2330 train_time:95395ms step_avg:61.82ms
step:1544/2330 train_time:95459ms step_avg:61.83ms
step:1545/2330 train_time:95519ms step_avg:61.82ms
step:1546/2330 train_time:95583ms step_avg:61.83ms
step:1547/2330 train_time:95644ms step_avg:61.83ms
step:1548/2330 train_time:95709ms step_avg:61.83ms
step:1549/2330 train_time:95770ms step_avg:61.83ms
step:1550/2330 train_time:95835ms step_avg:61.83ms
step:1551/2330 train_time:95897ms step_avg:61.83ms
step:1552/2330 train_time:95962ms step_avg:61.83ms
step:1553/2330 train_time:96026ms step_avg:61.83ms
step:1554/2330 train_time:96089ms step_avg:61.83ms
step:1555/2330 train_time:96151ms step_avg:61.83ms
step:1556/2330 train_time:96215ms step_avg:61.84ms
step:1557/2330 train_time:96277ms step_avg:61.83ms
step:1558/2330 train_time:96341ms step_avg:61.84ms
step:1559/2330 train_time:96401ms step_avg:61.84ms
step:1560/2330 train_time:96465ms step_avg:61.84ms
step:1561/2330 train_time:96525ms step_avg:61.84ms
step:1562/2330 train_time:96590ms step_avg:61.84ms
step:1563/2330 train_time:96650ms step_avg:61.84ms
step:1564/2330 train_time:96714ms step_avg:61.84ms
step:1565/2330 train_time:96775ms step_avg:61.84ms
step:1566/2330 train_time:96839ms step_avg:61.84ms
step:1567/2330 train_time:96901ms step_avg:61.84ms
step:1568/2330 train_time:96966ms step_avg:61.84ms
step:1569/2330 train_time:97028ms step_avg:61.84ms
step:1570/2330 train_time:97092ms step_avg:61.84ms
step:1571/2330 train_time:97153ms step_avg:61.84ms
step:1572/2330 train_time:97218ms step_avg:61.84ms
step:1573/2330 train_time:97279ms step_avg:61.84ms
step:1574/2330 train_time:97343ms step_avg:61.84ms
step:1575/2330 train_time:97405ms step_avg:61.84ms
step:1576/2330 train_time:97469ms step_avg:61.85ms
step:1577/2330 train_time:97530ms step_avg:61.85ms
step:1578/2330 train_time:97594ms step_avg:61.85ms
step:1579/2330 train_time:97654ms step_avg:61.85ms
step:1580/2330 train_time:97718ms step_avg:61.85ms
step:1581/2330 train_time:97779ms step_avg:61.85ms
step:1582/2330 train_time:97843ms step_avg:61.85ms
step:1583/2330 train_time:97907ms step_avg:61.85ms
step:1584/2330 train_time:97971ms step_avg:61.85ms
step:1585/2330 train_time:98032ms step_avg:61.85ms
step:1586/2330 train_time:98096ms step_avg:61.85ms
step:1587/2330 train_time:98158ms step_avg:61.85ms
step:1588/2330 train_time:98223ms step_avg:61.85ms
step:1589/2330 train_time:98284ms step_avg:61.85ms
step:1590/2330 train_time:98349ms step_avg:61.85ms
step:1591/2330 train_time:98410ms step_avg:61.85ms
step:1592/2330 train_time:98474ms step_avg:61.86ms
step:1593/2330 train_time:98535ms step_avg:61.85ms
step:1594/2330 train_time:98599ms step_avg:61.86ms
step:1595/2330 train_time:98660ms step_avg:61.86ms
step:1596/2330 train_time:98724ms step_avg:61.86ms
step:1597/2330 train_time:98784ms step_avg:61.86ms
step:1598/2330 train_time:98849ms step_avg:61.86ms
step:1599/2330 train_time:98911ms step_avg:61.86ms
step:1600/2330 train_time:98975ms step_avg:61.86ms
step:1601/2330 train_time:99036ms step_avg:61.86ms
step:1602/2330 train_time:99101ms step_avg:61.86ms
step:1603/2330 train_time:99163ms step_avg:61.86ms
step:1604/2330 train_time:99228ms step_avg:61.86ms
step:1605/2330 train_time:99288ms step_avg:61.86ms
step:1606/2330 train_time:99353ms step_avg:61.86ms
step:1607/2330 train_time:99414ms step_avg:61.86ms
step:1608/2330 train_time:99478ms step_avg:61.86ms
step:1609/2330 train_time:99539ms step_avg:61.86ms
step:1610/2330 train_time:99604ms step_avg:61.87ms
step:1611/2330 train_time:99665ms step_avg:61.87ms
step:1612/2330 train_time:99729ms step_avg:61.87ms
step:1613/2330 train_time:99790ms step_avg:61.87ms
step:1614/2330 train_time:99854ms step_avg:61.87ms
step:1615/2330 train_time:99915ms step_avg:61.87ms
step:1616/2330 train_time:99979ms step_avg:61.87ms
step:1617/2330 train_time:100041ms step_avg:61.87ms
step:1618/2330 train_time:100105ms step_avg:61.87ms
step:1619/2330 train_time:100167ms step_avg:61.87ms
step:1620/2330 train_time:100231ms step_avg:61.87ms
step:1621/2330 train_time:100292ms step_avg:61.87ms
step:1622/2330 train_time:100356ms step_avg:61.87ms
step:1623/2330 train_time:100417ms step_avg:61.87ms
step:1624/2330 train_time:100481ms step_avg:61.87ms
step:1625/2330 train_time:100542ms step_avg:61.87ms
step:1626/2330 train_time:100606ms step_avg:61.87ms
step:1627/2330 train_time:100667ms step_avg:61.87ms
step:1628/2330 train_time:100732ms step_avg:61.87ms
step:1629/2330 train_time:100792ms step_avg:61.87ms
step:1630/2330 train_time:100856ms step_avg:61.88ms
step:1631/2330 train_time:100918ms step_avg:61.87ms
step:1632/2330 train_time:100982ms step_avg:61.88ms
step:1633/2330 train_time:101044ms step_avg:61.88ms
step:1634/2330 train_time:101108ms step_avg:61.88ms
step:1635/2330 train_time:101170ms step_avg:61.88ms
step:1636/2330 train_time:101235ms step_avg:61.88ms
step:1637/2330 train_time:101296ms step_avg:61.88ms
step:1638/2330 train_time:101360ms step_avg:61.88ms
step:1639/2330 train_time:101422ms step_avg:61.88ms
step:1640/2330 train_time:101486ms step_avg:61.88ms
step:1641/2330 train_time:101547ms step_avg:61.88ms
step:1642/2330 train_time:101611ms step_avg:61.88ms
step:1643/2330 train_time:101672ms step_avg:61.88ms
step:1644/2330 train_time:101736ms step_avg:61.88ms
step:1645/2330 train_time:101796ms step_avg:61.88ms
step:1646/2330 train_time:101861ms step_avg:61.88ms
step:1647/2330 train_time:101922ms step_avg:61.88ms
step:1648/2330 train_time:101986ms step_avg:61.88ms
step:1649/2330 train_time:102047ms step_avg:61.88ms
step:1650/2330 train_time:102111ms step_avg:61.89ms
step:1651/2330 train_time:102172ms step_avg:61.88ms
step:1652/2330 train_time:102236ms step_avg:61.89ms
step:1653/2330 train_time:102297ms step_avg:61.89ms
step:1654/2330 train_time:102362ms step_avg:61.89ms
step:1655/2330 train_time:102423ms step_avg:61.89ms
step:1656/2330 train_time:102487ms step_avg:61.89ms
step:1657/2330 train_time:102548ms step_avg:61.89ms
step:1658/2330 train_time:102612ms step_avg:61.89ms
step:1659/2330 train_time:102673ms step_avg:61.89ms
step:1660/2330 train_time:102737ms step_avg:61.89ms
step:1661/2330 train_time:102798ms step_avg:61.89ms
step:1662/2330 train_time:102862ms step_avg:61.89ms
step:1663/2330 train_time:102924ms step_avg:61.89ms
step:1664/2330 train_time:102988ms step_avg:61.89ms
step:1665/2330 train_time:103050ms step_avg:61.89ms
step:1666/2330 train_time:103113ms step_avg:61.89ms
step:1667/2330 train_time:103175ms step_avg:61.89ms
step:1668/2330 train_time:103240ms step_avg:61.89ms
step:1669/2330 train_time:103301ms step_avg:61.89ms
step:1670/2330 train_time:103365ms step_avg:61.90ms
step:1671/2330 train_time:103426ms step_avg:61.89ms
step:1672/2330 train_time:103490ms step_avg:61.90ms
step:1673/2330 train_time:103552ms step_avg:61.90ms
step:1674/2330 train_time:103616ms step_avg:61.90ms
step:1675/2330 train_time:103677ms step_avg:61.90ms
step:1676/2330 train_time:103741ms step_avg:61.90ms
step:1677/2330 train_time:103803ms step_avg:61.90ms
step:1678/2330 train_time:103866ms step_avg:61.90ms
step:1679/2330 train_time:103927ms step_avg:61.90ms
step:1680/2330 train_time:103992ms step_avg:61.90ms
step:1681/2330 train_time:104053ms step_avg:61.90ms
step:1682/2330 train_time:104117ms step_avg:61.90ms
step:1683/2330 train_time:104178ms step_avg:61.90ms
step:1684/2330 train_time:104242ms step_avg:61.90ms
step:1685/2330 train_time:104304ms step_avg:61.90ms
step:1686/2330 train_time:104369ms step_avg:61.90ms
step:1687/2330 train_time:104430ms step_avg:61.90ms
step:1688/2330 train_time:104494ms step_avg:61.90ms
step:1689/2330 train_time:104556ms step_avg:61.90ms
step:1690/2330 train_time:104621ms step_avg:61.91ms
step:1691/2330 train_time:104681ms step_avg:61.90ms
step:1692/2330 train_time:104746ms step_avg:61.91ms
step:1693/2330 train_time:104808ms step_avg:61.91ms
step:1694/2330 train_time:104872ms step_avg:61.91ms
step:1695/2330 train_time:104934ms step_avg:61.91ms
step:1696/2330 train_time:104998ms step_avg:61.91ms
step:1697/2330 train_time:105058ms step_avg:61.91ms
step:1698/2330 train_time:105123ms step_avg:61.91ms
step:1699/2330 train_time:105184ms step_avg:61.91ms
step:1700/2330 train_time:105248ms step_avg:61.91ms
step:1701/2330 train_time:105309ms step_avg:61.91ms
step:1702/2330 train_time:105374ms step_avg:61.91ms
step:1703/2330 train_time:105434ms step_avg:61.91ms
step:1704/2330 train_time:105498ms step_avg:61.91ms
step:1705/2330 train_time:105560ms step_avg:61.91ms
step:1706/2330 train_time:105624ms step_avg:61.91ms
step:1707/2330 train_time:105686ms step_avg:61.91ms
step:1708/2330 train_time:105750ms step_avg:61.91ms
step:1709/2330 train_time:105812ms step_avg:61.91ms
step:1710/2330 train_time:105876ms step_avg:61.92ms
step:1711/2330 train_time:105937ms step_avg:61.92ms
step:1712/2330 train_time:106002ms step_avg:61.92ms
step:1713/2330 train_time:106063ms step_avg:61.92ms
step:1714/2330 train_time:106127ms step_avg:61.92ms
step:1715/2330 train_time:106188ms step_avg:61.92ms
step:1716/2330 train_time:106252ms step_avg:61.92ms
step:1717/2330 train_time:106313ms step_avg:61.92ms
step:1718/2330 train_time:106377ms step_avg:61.92ms
step:1719/2330 train_time:106438ms step_avg:61.92ms
step:1720/2330 train_time:106502ms step_avg:61.92ms
step:1721/2330 train_time:106564ms step_avg:61.92ms
step:1722/2330 train_time:106628ms step_avg:61.92ms
step:1723/2330 train_time:106690ms step_avg:61.92ms
step:1724/2330 train_time:106754ms step_avg:61.92ms
step:1725/2330 train_time:106815ms step_avg:61.92ms
step:1726/2330 train_time:106880ms step_avg:61.92ms
step:1727/2330 train_time:106941ms step_avg:61.92ms
step:1728/2330 train_time:107005ms step_avg:61.92ms
step:1729/2330 train_time:107067ms step_avg:61.92ms
step:1730/2330 train_time:107131ms step_avg:61.93ms
step:1731/2330 train_time:107192ms step_avg:61.93ms
step:1732/2330 train_time:107257ms step_avg:61.93ms
step:1733/2330 train_time:107318ms step_avg:61.93ms
step:1734/2330 train_time:107383ms step_avg:61.93ms
step:1735/2330 train_time:107444ms step_avg:61.93ms
step:1736/2330 train_time:107508ms step_avg:61.93ms
step:1737/2330 train_time:107569ms step_avg:61.93ms
step:1738/2330 train_time:107633ms step_avg:61.93ms
step:1739/2330 train_time:107694ms step_avg:61.93ms
step:1740/2330 train_time:107758ms step_avg:61.93ms
step:1741/2330 train_time:107820ms step_avg:61.93ms
step:1742/2330 train_time:107884ms step_avg:61.93ms
step:1743/2330 train_time:107945ms step_avg:61.93ms
step:1744/2330 train_time:108009ms step_avg:61.93ms
step:1745/2330 train_time:108070ms step_avg:61.93ms
step:1746/2330 train_time:108134ms step_avg:61.93ms
step:1747/2330 train_time:108196ms step_avg:61.93ms
step:1748/2330 train_time:108260ms step_avg:61.93ms
step:1749/2330 train_time:108322ms step_avg:61.93ms
step:1750/2330 train_time:108386ms step_avg:61.93ms
step:1750/2330 val_loss:3.7313 train_time:108452ms step_avg:61.97ms
step:1751/2330 train_time:108475ms step_avg:61.95ms
step:1752/2330 train_time:108516ms step_avg:61.94ms
step:1753/2330 train_time:108583ms step_avg:61.94ms
step:1754/2330 train_time:108648ms step_avg:61.94ms
step:1755/2330 train_time:108710ms step_avg:61.94ms
step:1756/2330 train_time:108775ms step_avg:61.94ms
step:1757/2330 train_time:108835ms step_avg:61.94ms
step:1758/2330 train_time:108899ms step_avg:61.94ms
step:1759/2330 train_time:108958ms step_avg:61.94ms
step:1760/2330 train_time:109022ms step_avg:61.94ms
step:1761/2330 train_time:109082ms step_avg:61.94ms
step:1762/2330 train_time:109146ms step_avg:61.94ms
step:1763/2330 train_time:109206ms step_avg:61.94ms
step:1764/2330 train_time:109269ms step_avg:61.94ms
step:1765/2330 train_time:109330ms step_avg:61.94ms
step:1766/2330 train_time:109398ms step_avg:61.95ms
step:1767/2330 train_time:109462ms step_avg:61.95ms
step:1768/2330 train_time:109527ms step_avg:61.95ms
step:1769/2330 train_time:109589ms step_avg:61.95ms
step:1770/2330 train_time:109654ms step_avg:61.95ms
step:1771/2330 train_time:109716ms step_avg:61.95ms
step:1772/2330 train_time:109780ms step_avg:61.95ms
step:1773/2330 train_time:109841ms step_avg:61.95ms
step:1774/2330 train_time:109905ms step_avg:61.95ms
step:1775/2330 train_time:109966ms step_avg:61.95ms
step:1776/2330 train_time:110029ms step_avg:61.95ms
step:1777/2330 train_time:110090ms step_avg:61.95ms
step:1778/2330 train_time:110154ms step_avg:61.95ms
step:1779/2330 train_time:110214ms step_avg:61.95ms
step:1780/2330 train_time:110278ms step_avg:61.95ms
step:1781/2330 train_time:110340ms step_avg:61.95ms
step:1782/2330 train_time:110404ms step_avg:61.95ms
step:1783/2330 train_time:110466ms step_avg:61.95ms
step:1784/2330 train_time:110531ms step_avg:61.96ms
step:1785/2330 train_time:110593ms step_avg:61.96ms
step:1786/2330 train_time:110658ms step_avg:61.96ms
step:1787/2330 train_time:110719ms step_avg:61.96ms
step:1788/2330 train_time:110784ms step_avg:61.96ms
step:1789/2330 train_time:110845ms step_avg:61.96ms
step:1790/2330 train_time:110909ms step_avg:61.96ms
step:1791/2330 train_time:110970ms step_avg:61.96ms
step:1792/2330 train_time:111034ms step_avg:61.96ms
step:1793/2330 train_time:111095ms step_avg:61.96ms
step:1794/2330 train_time:111159ms step_avg:61.96ms
step:1795/2330 train_time:111219ms step_avg:61.96ms
step:1796/2330 train_time:111284ms step_avg:61.96ms
step:1797/2330 train_time:111345ms step_avg:61.96ms
step:1798/2330 train_time:111409ms step_avg:61.96ms
step:1799/2330 train_time:111470ms step_avg:61.96ms
step:1800/2330 train_time:111536ms step_avg:61.96ms
step:1801/2330 train_time:111598ms step_avg:61.96ms
step:1802/2330 train_time:111663ms step_avg:61.97ms
step:1803/2330 train_time:111724ms step_avg:61.97ms
step:1804/2330 train_time:111788ms step_avg:61.97ms
step:1805/2330 train_time:111848ms step_avg:61.97ms
step:1806/2330 train_time:111913ms step_avg:61.97ms
step:1807/2330 train_time:111974ms step_avg:61.97ms
step:1808/2330 train_time:112038ms step_avg:61.97ms
step:1809/2330 train_time:112099ms step_avg:61.97ms
step:1810/2330 train_time:112163ms step_avg:61.97ms
step:1811/2330 train_time:112223ms step_avg:61.97ms
step:1812/2330 train_time:112287ms step_avg:61.97ms
step:1813/2330 train_time:112348ms step_avg:61.97ms
step:1814/2330 train_time:112412ms step_avg:61.97ms
step:1815/2330 train_time:112475ms step_avg:61.97ms
step:1816/2330 train_time:112540ms step_avg:61.97ms
step:1817/2330 train_time:112602ms step_avg:61.97ms
step:1818/2330 train_time:112668ms step_avg:61.97ms
step:1819/2330 train_time:112728ms step_avg:61.97ms
step:1820/2330 train_time:112793ms step_avg:61.97ms
step:1821/2330 train_time:112855ms step_avg:61.97ms
step:1822/2330 train_time:112919ms step_avg:61.98ms
step:1823/2330 train_time:112979ms step_avg:61.97ms
step:1824/2330 train_time:113043ms step_avg:61.98ms
step:1825/2330 train_time:113104ms step_avg:61.97ms
step:1826/2330 train_time:113168ms step_avg:61.98ms
step:1827/2330 train_time:113228ms step_avg:61.97ms
step:1828/2330 train_time:113292ms step_avg:61.98ms
step:1829/2330 train_time:113353ms step_avg:61.98ms
step:1830/2330 train_time:113418ms step_avg:61.98ms
step:1831/2330 train_time:113479ms step_avg:61.98ms
step:1832/2330 train_time:113543ms step_avg:61.98ms
step:1833/2330 train_time:113604ms step_avg:61.98ms
step:1834/2330 train_time:113669ms step_avg:61.98ms
step:1835/2330 train_time:113731ms step_avg:61.98ms
step:1836/2330 train_time:113797ms step_avg:61.98ms
step:1837/2330 train_time:113858ms step_avg:61.98ms
step:1838/2330 train_time:113922ms step_avg:61.98ms
step:1839/2330 train_time:113983ms step_avg:61.98ms
step:1840/2330 train_time:114047ms step_avg:61.98ms
step:1841/2330 train_time:114108ms step_avg:61.98ms
step:1842/2330 train_time:114172ms step_avg:61.98ms
step:1843/2330 train_time:114233ms step_avg:61.98ms
step:1844/2330 train_time:114297ms step_avg:61.98ms
step:1845/2330 train_time:114358ms step_avg:61.98ms
step:1846/2330 train_time:114423ms step_avg:61.98ms
step:1847/2330 train_time:114484ms step_avg:61.98ms
step:1848/2330 train_time:114548ms step_avg:61.98ms
step:1849/2330 train_time:114609ms step_avg:61.98ms
step:1850/2330 train_time:114674ms step_avg:61.99ms
step:1851/2330 train_time:114736ms step_avg:61.99ms
step:1852/2330 train_time:114800ms step_avg:61.99ms
step:1853/2330 train_time:114863ms step_avg:61.99ms
step:1854/2330 train_time:114927ms step_avg:61.99ms
step:1855/2330 train_time:114988ms step_avg:61.99ms
step:1856/2330 train_time:115052ms step_avg:61.99ms
step:1857/2330 train_time:115114ms step_avg:61.99ms
step:1858/2330 train_time:115178ms step_avg:61.99ms
step:1859/2330 train_time:115239ms step_avg:61.99ms
step:1860/2330 train_time:115303ms step_avg:61.99ms
step:1861/2330 train_time:115364ms step_avg:61.99ms
step:1862/2330 train_time:115428ms step_avg:61.99ms
step:1863/2330 train_time:115489ms step_avg:61.99ms
step:1864/2330 train_time:115553ms step_avg:61.99ms
step:1865/2330 train_time:115614ms step_avg:61.99ms
step:1866/2330 train_time:115679ms step_avg:61.99ms
step:1867/2330 train_time:115740ms step_avg:61.99ms
step:1868/2330 train_time:115804ms step_avg:61.99ms
step:1869/2330 train_time:115866ms step_avg:61.99ms
step:1870/2330 train_time:115930ms step_avg:61.99ms
step:1871/2330 train_time:115992ms step_avg:61.99ms
step:1872/2330 train_time:116056ms step_avg:62.00ms
step:1873/2330 train_time:116117ms step_avg:62.00ms
step:1874/2330 train_time:116181ms step_avg:62.00ms
step:1875/2330 train_time:116242ms step_avg:62.00ms
step:1876/2330 train_time:116306ms step_avg:62.00ms
step:1877/2330 train_time:116367ms step_avg:62.00ms
step:1878/2330 train_time:116432ms step_avg:62.00ms
step:1879/2330 train_time:116493ms step_avg:62.00ms
step:1880/2330 train_time:116557ms step_avg:62.00ms
step:1881/2330 train_time:116618ms step_avg:62.00ms
step:1882/2330 train_time:116682ms step_avg:62.00ms
step:1883/2330 train_time:116743ms step_avg:62.00ms
step:1884/2330 train_time:116807ms step_avg:62.00ms
step:1885/2330 train_time:116869ms step_avg:62.00ms
step:1886/2330 train_time:116933ms step_avg:62.00ms
step:1887/2330 train_time:116995ms step_avg:62.00ms
step:1888/2330 train_time:117061ms step_avg:62.00ms
step:1889/2330 train_time:117122ms step_avg:62.00ms
step:1890/2330 train_time:117186ms step_avg:62.00ms
step:1891/2330 train_time:117247ms step_avg:62.00ms
step:1892/2330 train_time:117311ms step_avg:62.00ms
step:1893/2330 train_time:117373ms step_avg:62.00ms
step:1894/2330 train_time:117437ms step_avg:62.00ms
step:1895/2330 train_time:117497ms step_avg:62.00ms
step:1896/2330 train_time:117562ms step_avg:62.01ms
step:1897/2330 train_time:117623ms step_avg:62.00ms
step:1898/2330 train_time:117687ms step_avg:62.01ms
step:1899/2330 train_time:117748ms step_avg:62.01ms
step:1900/2330 train_time:117813ms step_avg:62.01ms
step:1901/2330 train_time:117875ms step_avg:62.01ms
step:1902/2330 train_time:117939ms step_avg:62.01ms
step:1903/2330 train_time:118000ms step_avg:62.01ms
step:1904/2330 train_time:118064ms step_avg:62.01ms
step:1905/2330 train_time:118125ms step_avg:62.01ms
step:1906/2330 train_time:118189ms step_avg:62.01ms
step:1907/2330 train_time:118250ms step_avg:62.01ms
step:1908/2330 train_time:118315ms step_avg:62.01ms
step:1909/2330 train_time:118377ms step_avg:62.01ms
step:1910/2330 train_time:118441ms step_avg:62.01ms
step:1911/2330 train_time:118503ms step_avg:62.01ms
step:1912/2330 train_time:118567ms step_avg:62.01ms
step:1913/2330 train_time:118629ms step_avg:62.01ms
step:1914/2330 train_time:118693ms step_avg:62.01ms
step:1915/2330 train_time:118754ms step_avg:62.01ms
step:1916/2330 train_time:118819ms step_avg:62.01ms
step:1917/2330 train_time:118879ms step_avg:62.01ms
step:1918/2330 train_time:118943ms step_avg:62.01ms
step:1919/2330 train_time:119004ms step_avg:62.01ms
step:1920/2330 train_time:119067ms step_avg:62.01ms
step:1921/2330 train_time:119129ms step_avg:62.01ms
step:1922/2330 train_time:119194ms step_avg:62.02ms
step:1923/2330 train_time:119255ms step_avg:62.01ms
step:1924/2330 train_time:119319ms step_avg:62.02ms
step:1925/2330 train_time:119380ms step_avg:62.02ms
step:1926/2330 train_time:119445ms step_avg:62.02ms
step:1927/2330 train_time:119506ms step_avg:62.02ms
step:1928/2330 train_time:119571ms step_avg:62.02ms
step:1929/2330 train_time:119632ms step_avg:62.02ms
step:1930/2330 train_time:119697ms step_avg:62.02ms
step:1931/2330 train_time:119758ms step_avg:62.02ms
step:1932/2330 train_time:119822ms step_avg:62.02ms
step:1933/2330 train_time:119884ms step_avg:62.02ms
step:1934/2330 train_time:119948ms step_avg:62.02ms
step:1935/2330 train_time:120010ms step_avg:62.02ms
step:1936/2330 train_time:120074ms step_avg:62.02ms
step:1937/2330 train_time:120135ms step_avg:62.02ms
step:1938/2330 train_time:120200ms step_avg:62.02ms
step:1939/2330 train_time:120262ms step_avg:62.02ms
step:1940/2330 train_time:120326ms step_avg:62.02ms
step:1941/2330 train_time:120388ms step_avg:62.02ms
step:1942/2330 train_time:120453ms step_avg:62.03ms
step:1943/2330 train_time:120514ms step_avg:62.02ms
step:1944/2330 train_time:120578ms step_avg:62.03ms
step:1945/2330 train_time:120640ms step_avg:62.03ms
step:1946/2330 train_time:120704ms step_avg:62.03ms
step:1947/2330 train_time:120765ms step_avg:62.03ms
step:1948/2330 train_time:120829ms step_avg:62.03ms
step:1949/2330 train_time:120890ms step_avg:62.03ms
step:1950/2330 train_time:120955ms step_avg:62.03ms
step:1951/2330 train_time:121016ms step_avg:62.03ms
step:1952/2330 train_time:121080ms step_avg:62.03ms
step:1953/2330 train_time:121141ms step_avg:62.03ms
step:1954/2330 train_time:121204ms step_avg:62.03ms
step:1955/2330 train_time:121266ms step_avg:62.03ms
step:1956/2330 train_time:121329ms step_avg:62.03ms
step:1957/2330 train_time:121392ms step_avg:62.03ms
step:1958/2330 train_time:121457ms step_avg:62.03ms
step:1959/2330 train_time:121518ms step_avg:62.03ms
step:1960/2330 train_time:121582ms step_avg:62.03ms
step:1961/2330 train_time:121644ms step_avg:62.03ms
step:1962/2330 train_time:121708ms step_avg:62.03ms
step:1963/2330 train_time:121769ms step_avg:62.03ms
step:1964/2330 train_time:121833ms step_avg:62.03ms
step:1965/2330 train_time:121895ms step_avg:62.03ms
step:1966/2330 train_time:121959ms step_avg:62.03ms
step:1967/2330 train_time:122020ms step_avg:62.03ms
step:1968/2330 train_time:122085ms step_avg:62.03ms
step:1969/2330 train_time:122146ms step_avg:62.03ms
step:1970/2330 train_time:122210ms step_avg:62.04ms
step:1971/2330 train_time:122271ms step_avg:62.04ms
step:1972/2330 train_time:122335ms step_avg:62.04ms
step:1973/2330 train_time:122397ms step_avg:62.04ms
step:1974/2330 train_time:122462ms step_avg:62.04ms
step:1975/2330 train_time:122522ms step_avg:62.04ms
step:1976/2330 train_time:122586ms step_avg:62.04ms
step:1977/2330 train_time:122647ms step_avg:62.04ms
step:1978/2330 train_time:122712ms step_avg:62.04ms
step:1979/2330 train_time:122773ms step_avg:62.04ms
step:1980/2330 train_time:122837ms step_avg:62.04ms
step:1981/2330 train_time:122899ms step_avg:62.04ms
step:1982/2330 train_time:122963ms step_avg:62.04ms
step:1983/2330 train_time:123024ms step_avg:62.04ms
step:1984/2330 train_time:123088ms step_avg:62.04ms
step:1985/2330 train_time:123149ms step_avg:62.04ms
step:1986/2330 train_time:123214ms step_avg:62.04ms
step:1987/2330 train_time:123274ms step_avg:62.04ms
step:1988/2330 train_time:123339ms step_avg:62.04ms
step:1989/2330 train_time:123400ms step_avg:62.04ms
step:1990/2330 train_time:123465ms step_avg:62.04ms
step:1991/2330 train_time:123525ms step_avg:62.04ms
step:1992/2330 train_time:123589ms step_avg:62.04ms
step:1993/2330 train_time:123651ms step_avg:62.04ms
step:1994/2330 train_time:123715ms step_avg:62.04ms
step:1995/2330 train_time:123776ms step_avg:62.04ms
step:1996/2330 train_time:123841ms step_avg:62.04ms
step:1997/2330 train_time:123902ms step_avg:62.04ms
step:1998/2330 train_time:123966ms step_avg:62.05ms
step:1999/2330 train_time:124027ms step_avg:62.04ms
step:2000/2330 train_time:124092ms step_avg:62.05ms
step:2000/2330 val_loss:3.6494 train_time:124158ms step_avg:62.08ms
step:2001/2330 train_time:124181ms step_avg:62.06ms
step:2002/2330 train_time:124220ms step_avg:62.05ms
step:2003/2330 train_time:124287ms step_avg:62.05ms
step:2004/2330 train_time:124355ms step_avg:62.05ms
step:2005/2330 train_time:124416ms step_avg:62.05ms
step:2006/2330 train_time:124481ms step_avg:62.05ms
step:2007/2330 train_time:124541ms step_avg:62.05ms
step:2008/2330 train_time:124604ms step_avg:62.05ms
step:2009/2330 train_time:124664ms step_avg:62.05ms
step:2010/2330 train_time:124727ms step_avg:62.05ms
step:2011/2330 train_time:124787ms step_avg:62.05ms
step:2012/2330 train_time:124850ms step_avg:62.05ms
step:2013/2330 train_time:124911ms step_avg:62.05ms
step:2014/2330 train_time:124974ms step_avg:62.05ms
step:2015/2330 train_time:125035ms step_avg:62.05ms
step:2016/2330 train_time:125099ms step_avg:62.05ms
step:2017/2330 train_time:125161ms step_avg:62.05ms
step:2018/2330 train_time:125227ms step_avg:62.06ms
step:2019/2330 train_time:125289ms step_avg:62.06ms
step:2020/2330 train_time:125355ms step_avg:62.06ms
step:2021/2330 train_time:125417ms step_avg:62.06ms
step:2022/2330 train_time:125482ms step_avg:62.06ms
step:2023/2330 train_time:125543ms step_avg:62.06ms
step:2024/2330 train_time:125607ms step_avg:62.06ms
step:2025/2330 train_time:125668ms step_avg:62.06ms
step:2026/2330 train_time:125732ms step_avg:62.06ms
step:2027/2330 train_time:125793ms step_avg:62.06ms
step:2028/2330 train_time:125856ms step_avg:62.06ms
step:2029/2330 train_time:125916ms step_avg:62.06ms
step:2030/2330 train_time:125980ms step_avg:62.06ms
step:2031/2330 train_time:126040ms step_avg:62.06ms
step:2032/2330 train_time:126105ms step_avg:62.06ms
step:2033/2330 train_time:126165ms step_avg:62.06ms
step:2034/2330 train_time:126230ms step_avg:62.06ms
step:2035/2330 train_time:126292ms step_avg:62.06ms
step:2036/2330 train_time:126358ms step_avg:62.06ms
step:2037/2330 train_time:126419ms step_avg:62.06ms
step:2038/2330 train_time:126484ms step_avg:62.06ms
step:2039/2330 train_time:126544ms step_avg:62.06ms
step:2040/2330 train_time:126609ms step_avg:62.06ms
step:2041/2330 train_time:126669ms step_avg:62.06ms
step:2042/2330 train_time:126734ms step_avg:62.06ms
step:2043/2330 train_time:126795ms step_avg:62.06ms
step:2044/2330 train_time:126859ms step_avg:62.06ms
step:2045/2330 train_time:126920ms step_avg:62.06ms
step:2046/2330 train_time:126984ms step_avg:62.06ms
step:2047/2330 train_time:127045ms step_avg:62.06ms
step:2048/2330 train_time:127109ms step_avg:62.06ms
step:2049/2330 train_time:127170ms step_avg:62.06ms
step:2050/2330 train_time:127235ms step_avg:62.07ms
step:2051/2330 train_time:127297ms step_avg:62.07ms
step:2052/2330 train_time:127363ms step_avg:62.07ms
step:2053/2330 train_time:127424ms step_avg:62.07ms
step:2054/2330 train_time:127488ms step_avg:62.07ms
step:2055/2330 train_time:127550ms step_avg:62.07ms
step:2056/2330 train_time:127614ms step_avg:62.07ms
step:2057/2330 train_time:127675ms step_avg:62.07ms
step:2058/2330 train_time:127739ms step_avg:62.07ms
step:2059/2330 train_time:127799ms step_avg:62.07ms
step:2060/2330 train_time:127863ms step_avg:62.07ms
step:2061/2330 train_time:127923ms step_avg:62.07ms
step:2062/2330 train_time:127987ms step_avg:62.07ms
step:2063/2330 train_time:128048ms step_avg:62.07ms
step:2064/2330 train_time:128112ms step_avg:62.07ms
step:2065/2330 train_time:128174ms step_avg:62.07ms
step:2066/2330 train_time:128239ms step_avg:62.07ms
step:2067/2330 train_time:128300ms step_avg:62.07ms
step:2068/2330 train_time:128364ms step_avg:62.07ms
step:2069/2330 train_time:128425ms step_avg:62.07ms
step:2070/2330 train_time:128490ms step_avg:62.07ms
step:2071/2330 train_time:128551ms step_avg:62.07ms
step:2072/2330 train_time:128615ms step_avg:62.07ms
step:2073/2330 train_time:128676ms step_avg:62.07ms
step:2074/2330 train_time:128741ms step_avg:62.07ms
step:2075/2330 train_time:128801ms step_avg:62.07ms
step:2076/2330 train_time:128865ms step_avg:62.07ms
step:2077/2330 train_time:128926ms step_avg:62.07ms
step:2078/2330 train_time:128989ms step_avg:62.07ms
step:2079/2330 train_time:129050ms step_avg:62.07ms
step:2080/2330 train_time:129114ms step_avg:62.07ms
step:2081/2330 train_time:129176ms step_avg:62.07ms
step:2082/2330 train_time:129240ms step_avg:62.08ms
step:2083/2330 train_time:129302ms step_avg:62.07ms
step:2084/2330 train_time:129365ms step_avg:62.08ms
step:2085/2330 train_time:129426ms step_avg:62.07ms
step:2086/2330 train_time:129490ms step_avg:62.08ms
step:2087/2330 train_time:129551ms step_avg:62.08ms
step:2088/2330 train_time:129616ms step_avg:62.08ms
step:2089/2330 train_time:129678ms step_avg:62.08ms
step:2090/2330 train_time:129742ms step_avg:62.08ms
step:2091/2330 train_time:129803ms step_avg:62.08ms
step:2092/2330 train_time:129867ms step_avg:62.08ms
step:2093/2330 train_time:129927ms step_avg:62.08ms
step:2094/2330 train_time:129992ms step_avg:62.08ms
step:2095/2330 train_time:130053ms step_avg:62.08ms
step:2096/2330 train_time:130118ms step_avg:62.08ms
step:2097/2330 train_time:130180ms step_avg:62.08ms
step:2098/2330 train_time:130244ms step_avg:62.08ms
step:2099/2330 train_time:130305ms step_avg:62.08ms
step:2100/2330 train_time:130368ms step_avg:62.08ms
step:2101/2330 train_time:130429ms step_avg:62.08ms
step:2102/2330 train_time:130494ms step_avg:62.08ms
step:2103/2330 train_time:130555ms step_avg:62.08ms
step:2104/2330 train_time:130619ms step_avg:62.08ms
step:2105/2330 train_time:130681ms step_avg:62.08ms
step:2106/2330 train_time:130745ms step_avg:62.08ms
step:2107/2330 train_time:130806ms step_avg:62.08ms
step:2108/2330 train_time:130870ms step_avg:62.08ms
step:2109/2330 train_time:130931ms step_avg:62.08ms
step:2110/2330 train_time:130995ms step_avg:62.08ms
step:2111/2330 train_time:131056ms step_avg:62.08ms
step:2112/2330 train_time:131121ms step_avg:62.08ms
step:2113/2330 train_time:131182ms step_avg:62.08ms
step:2114/2330 train_time:131246ms step_avg:62.08ms
step:2115/2330 train_time:131307ms step_avg:62.08ms
step:2116/2330 train_time:131371ms step_avg:62.08ms
step:2117/2330 train_time:131432ms step_avg:62.08ms
step:2118/2330 train_time:131496ms step_avg:62.09ms
step:2119/2330 train_time:131558ms step_avg:62.08ms
step:2120/2330 train_time:131623ms step_avg:62.09ms
step:2121/2330 train_time:131684ms step_avg:62.09ms
step:2122/2330 train_time:131748ms step_avg:62.09ms
step:2123/2330 train_time:131809ms step_avg:62.09ms
step:2124/2330 train_time:131873ms step_avg:62.09ms
step:2125/2330 train_time:131934ms step_avg:62.09ms
step:2126/2330 train_time:131998ms step_avg:62.09ms
step:2127/2330 train_time:132059ms step_avg:62.09ms
step:2128/2330 train_time:132123ms step_avg:62.09ms
step:2129/2330 train_time:132184ms step_avg:62.09ms
step:2130/2330 train_time:132248ms step_avg:62.09ms
step:2131/2330 train_time:132309ms step_avg:62.09ms
step:2132/2330 train_time:132374ms step_avg:62.09ms
step:2133/2330 train_time:132436ms step_avg:62.09ms
step:2134/2330 train_time:132500ms step_avg:62.09ms
step:2135/2330 train_time:132561ms step_avg:62.09ms
step:2136/2330 train_time:132625ms step_avg:62.09ms
step:2137/2330 train_time:132686ms step_avg:62.09ms
step:2138/2330 train_time:132750ms step_avg:62.09ms
step:2139/2330 train_time:132811ms step_avg:62.09ms
step:2140/2330 train_time:132875ms step_avg:62.09ms
step:2141/2330 train_time:132936ms step_avg:62.09ms
step:2142/2330 train_time:133000ms step_avg:62.09ms
step:2143/2330 train_time:133060ms step_avg:62.09ms
step:2144/2330 train_time:133125ms step_avg:62.09ms
step:2145/2330 train_time:133186ms step_avg:62.09ms
step:2146/2330 train_time:133250ms step_avg:62.09ms
step:2147/2330 train_time:133311ms step_avg:62.09ms
step:2148/2330 train_time:133375ms step_avg:62.09ms
step:2149/2330 train_time:133437ms step_avg:62.09ms
step:2150/2330 train_time:133502ms step_avg:62.09ms
step:2151/2330 train_time:133563ms step_avg:62.09ms
step:2152/2330 train_time:133627ms step_avg:62.09ms
step:2153/2330 train_time:133688ms step_avg:62.09ms
step:2154/2330 train_time:133752ms step_avg:62.09ms
step:2155/2330 train_time:133813ms step_avg:62.09ms
step:2156/2330 train_time:133879ms step_avg:62.10ms
step:2157/2330 train_time:133939ms step_avg:62.10ms
step:2158/2330 train_time:134004ms step_avg:62.10ms
step:2159/2330 train_time:134065ms step_avg:62.10ms
step:2160/2330 train_time:134129ms step_avg:62.10ms
step:2161/2330 train_time:134191ms step_avg:62.10ms
step:2162/2330 train_time:134255ms step_avg:62.10ms
step:2163/2330 train_time:134316ms step_avg:62.10ms
step:2164/2330 train_time:134380ms step_avg:62.10ms
step:2165/2330 train_time:134441ms step_avg:62.10ms
step:2166/2330 train_time:134506ms step_avg:62.10ms
step:2167/2330 train_time:134566ms step_avg:62.10ms
step:2168/2330 train_time:134630ms step_avg:62.10ms
step:2169/2330 train_time:134691ms step_avg:62.10ms
step:2170/2330 train_time:134756ms step_avg:62.10ms
step:2171/2330 train_time:134818ms step_avg:62.10ms
step:2172/2330 train_time:134882ms step_avg:62.10ms
step:2173/2330 train_time:134943ms step_avg:62.10ms
step:2174/2330 train_time:135007ms step_avg:62.10ms
step:2175/2330 train_time:135068ms step_avg:62.10ms
step:2176/2330 train_time:135133ms step_avg:62.10ms
step:2177/2330 train_time:135194ms step_avg:62.10ms
step:2178/2330 train_time:135259ms step_avg:62.10ms
step:2179/2330 train_time:135320ms step_avg:62.10ms
step:2180/2330 train_time:135385ms step_avg:62.10ms
step:2181/2330 train_time:135446ms step_avg:62.10ms
step:2182/2330 train_time:135510ms step_avg:62.10ms
step:2183/2330 train_time:135571ms step_avg:62.10ms
step:2184/2330 train_time:135636ms step_avg:62.10ms
step:2185/2330 train_time:135697ms step_avg:62.10ms
step:2186/2330 train_time:135761ms step_avg:62.10ms
step:2187/2330 train_time:135822ms step_avg:62.10ms
step:2188/2330 train_time:135886ms step_avg:62.11ms
step:2189/2330 train_time:135946ms step_avg:62.10ms
step:2190/2330 train_time:136011ms step_avg:62.11ms
step:2191/2330 train_time:136072ms step_avg:62.10ms
step:2192/2330 train_time:136136ms step_avg:62.11ms
step:2193/2330 train_time:136197ms step_avg:62.11ms
step:2194/2330 train_time:136261ms step_avg:62.11ms
step:2195/2330 train_time:136323ms step_avg:62.11ms
step:2196/2330 train_time:136388ms step_avg:62.11ms
step:2197/2330 train_time:136448ms step_avg:62.11ms
step:2198/2330 train_time:136513ms step_avg:62.11ms
step:2199/2330 train_time:136574ms step_avg:62.11ms
step:2200/2330 train_time:136638ms step_avg:62.11ms
step:2201/2330 train_time:136699ms step_avg:62.11ms
step:2202/2330 train_time:136762ms step_avg:62.11ms
step:2203/2330 train_time:136824ms step_avg:62.11ms
step:2204/2330 train_time:136888ms step_avg:62.11ms
step:2205/2330 train_time:136949ms step_avg:62.11ms
step:2206/2330 train_time:137014ms step_avg:62.11ms
step:2207/2330 train_time:137076ms step_avg:62.11ms
step:2208/2330 train_time:137141ms step_avg:62.11ms
step:2209/2330 train_time:137201ms step_avg:62.11ms
step:2210/2330 train_time:137265ms step_avg:62.11ms
step:2211/2330 train_time:137327ms step_avg:62.11ms
step:2212/2330 train_time:137390ms step_avg:62.11ms
step:2213/2330 train_time:137452ms step_avg:62.11ms
step:2214/2330 train_time:137517ms step_avg:62.11ms
step:2215/2330 train_time:137578ms step_avg:62.11ms
step:2216/2330 train_time:137642ms step_avg:62.11ms
step:2217/2330 train_time:137702ms step_avg:62.11ms
step:2218/2330 train_time:137766ms step_avg:62.11ms
step:2219/2330 train_time:137828ms step_avg:62.11ms
step:2220/2330 train_time:137892ms step_avg:62.11ms
step:2221/2330 train_time:137952ms step_avg:62.11ms
step:2222/2330 train_time:138017ms step_avg:62.11ms
step:2223/2330 train_time:138080ms step_avg:62.11ms
step:2224/2330 train_time:138144ms step_avg:62.11ms
step:2225/2330 train_time:138204ms step_avg:62.11ms
step:2226/2330 train_time:138269ms step_avg:62.12ms
step:2227/2330 train_time:138330ms step_avg:62.11ms
step:2228/2330 train_time:138394ms step_avg:62.12ms
step:2229/2330 train_time:138455ms step_avg:62.12ms
step:2230/2330 train_time:138519ms step_avg:62.12ms
step:2231/2330 train_time:138580ms step_avg:62.12ms
step:2232/2330 train_time:138644ms step_avg:62.12ms
step:2233/2330 train_time:138705ms step_avg:62.12ms
step:2234/2330 train_time:138769ms step_avg:62.12ms
step:2235/2330 train_time:138830ms step_avg:62.12ms
step:2236/2330 train_time:138894ms step_avg:62.12ms
step:2237/2330 train_time:138956ms step_avg:62.12ms
step:2238/2330 train_time:139020ms step_avg:62.12ms
step:2239/2330 train_time:139081ms step_avg:62.12ms
step:2240/2330 train_time:139145ms step_avg:62.12ms
step:2241/2330 train_time:139206ms step_avg:62.12ms
step:2242/2330 train_time:139269ms step_avg:62.12ms
step:2243/2330 train_time:139330ms step_avg:62.12ms
step:2244/2330 train_time:139395ms step_avg:62.12ms
step:2245/2330 train_time:139457ms step_avg:62.12ms
step:2246/2330 train_time:139521ms step_avg:62.12ms
step:2247/2330 train_time:139582ms step_avg:62.12ms
step:2248/2330 train_time:139646ms step_avg:62.12ms
step:2249/2330 train_time:139707ms step_avg:62.12ms
step:2250/2330 train_time:139771ms step_avg:62.12ms
step:2250/2330 val_loss:3.6129 train_time:139837ms step_avg:62.15ms
step:2251/2330 train_time:139860ms step_avg:62.13ms
step:2252/2330 train_time:139901ms step_avg:62.12ms
step:2253/2330 train_time:139967ms step_avg:62.12ms
step:2254/2330 train_time:140031ms step_avg:62.13ms
step:2255/2330 train_time:140093ms step_avg:62.13ms
step:2256/2330 train_time:140157ms step_avg:62.13ms
step:2257/2330 train_time:140217ms step_avg:62.13ms
step:2258/2330 train_time:140280ms step_avg:62.13ms
step:2259/2330 train_time:140340ms step_avg:62.12ms
step:2260/2330 train_time:140404ms step_avg:62.13ms
step:2261/2330 train_time:140465ms step_avg:62.13ms
step:2262/2330 train_time:140529ms step_avg:62.13ms
step:2263/2330 train_time:140589ms step_avg:62.13ms
step:2264/2330 train_time:140654ms step_avg:62.13ms
step:2265/2330 train_time:140714ms step_avg:62.13ms
step:2266/2330 train_time:140779ms step_avg:62.13ms
step:2267/2330 train_time:140840ms step_avg:62.13ms
step:2268/2330 train_time:140907ms step_avg:62.13ms
step:2269/2330 train_time:140970ms step_avg:62.13ms
step:2270/2330 train_time:141036ms step_avg:62.13ms
step:2271/2330 train_time:141097ms step_avg:62.13ms
step:2272/2330 train_time:141161ms step_avg:62.13ms
step:2273/2330 train_time:141221ms step_avg:62.13ms
step:2274/2330 train_time:141285ms step_avg:62.13ms
step:2275/2330 train_time:141345ms step_avg:62.13ms
step:2276/2330 train_time:141409ms step_avg:62.13ms
step:2277/2330 train_time:141469ms step_avg:62.13ms
step:2278/2330 train_time:141533ms step_avg:62.13ms
step:2279/2330 train_time:141593ms step_avg:62.13ms
step:2280/2330 train_time:141657ms step_avg:62.13ms
step:2281/2330 train_time:141718ms step_avg:62.13ms
step:2282/2330 train_time:141781ms step_avg:62.13ms
step:2283/2330 train_time:141843ms step_avg:62.13ms
step:2284/2330 train_time:141909ms step_avg:62.13ms
step:2285/2330 train_time:141971ms step_avg:62.13ms
step:2286/2330 train_time:142036ms step_avg:62.13ms
step:2287/2330 train_time:142097ms step_avg:62.13ms
step:2288/2330 train_time:142161ms step_avg:62.13ms
step:2289/2330 train_time:142222ms step_avg:62.13ms
step:2290/2330 train_time:142286ms step_avg:62.13ms
step:2291/2330 train_time:142347ms step_avg:62.13ms
step:2292/2330 train_time:142410ms step_avg:62.13ms
step:2293/2330 train_time:142470ms step_avg:62.13ms
step:2294/2330 train_time:142534ms step_avg:62.13ms
step:2295/2330 train_time:142595ms step_avg:62.13ms
step:2296/2330 train_time:142659ms step_avg:62.13ms
step:2297/2330 train_time:142720ms step_avg:62.13ms
step:2298/2330 train_time:142784ms step_avg:62.13ms
step:2299/2330 train_time:142846ms step_avg:62.13ms
step:2300/2330 train_time:142912ms step_avg:62.14ms
step:2301/2330 train_time:142974ms step_avg:62.14ms
step:2302/2330 train_time:143038ms step_avg:62.14ms
step:2303/2330 train_time:143099ms step_avg:62.14ms
step:2304/2330 train_time:143163ms step_avg:62.14ms
step:2305/2330 train_time:143224ms step_avg:62.14ms
step:2306/2330 train_time:143288ms step_avg:62.14ms
step:2307/2330 train_time:143349ms step_avg:62.14ms
step:2308/2330 train_time:143413ms step_avg:62.14ms
step:2309/2330 train_time:143474ms step_avg:62.14ms
step:2310/2330 train_time:143538ms step_avg:62.14ms
step:2311/2330 train_time:143599ms step_avg:62.14ms
step:2312/2330 train_time:143663ms step_avg:62.14ms
step:2313/2330 train_time:143723ms step_avg:62.14ms
step:2314/2330 train_time:143788ms step_avg:62.14ms
step:2315/2330 train_time:143850ms step_avg:62.14ms
step:2316/2330 train_time:143915ms step_avg:62.14ms
step:2317/2330 train_time:143976ms step_avg:62.14ms
step:2318/2330 train_time:144040ms step_avg:62.14ms
step:2319/2330 train_time:144102ms step_avg:62.14ms
step:2320/2330 train_time:144166ms step_avg:62.14ms
step:2321/2330 train_time:144228ms step_avg:62.14ms
step:2322/2330 train_time:144292ms step_avg:62.14ms
step:2323/2330 train_time:144353ms step_avg:62.14ms
step:2324/2330 train_time:144416ms step_avg:62.14ms
step:2325/2330 train_time:144477ms step_avg:62.14ms
step:2326/2330 train_time:144541ms step_avg:62.14ms
step:2327/2330 train_time:144602ms step_avg:62.14ms
step:2328/2330 train_time:144665ms step_avg:62.14ms
step:2329/2330 train_time:144728ms step_avg:62.14ms
step:2330/2330 train_time:144792ms step_avg:62.14ms
step:2330/2330 val_loss:3.5900 train_time:144859ms step_avg:62.17ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
