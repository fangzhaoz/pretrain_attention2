import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:23:54 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/2330 train_time:88ms step_avg:87.69ms
step:2/2330 train_time:236ms step_avg:117.80ms
step:3/2330 train_time:258ms step_avg:85.89ms
step:4/2330 train_time:293ms step_avg:73.19ms
step:5/2330 train_time:350ms step_avg:70.00ms
step:6/2330 train_time:411ms step_avg:68.55ms
step:7/2330 train_time:470ms step_avg:67.14ms
step:8/2330 train_time:532ms step_avg:66.47ms
step:9/2330 train_time:590ms step_avg:65.54ms
step:10/2330 train_time:652ms step_avg:65.17ms
step:11/2330 train_time:710ms step_avg:64.57ms
step:12/2330 train_time:772ms step_avg:64.37ms
step:13/2330 train_time:831ms step_avg:63.96ms
step:14/2330 train_time:893ms step_avg:63.81ms
step:15/2330 train_time:952ms step_avg:63.45ms
step:16/2330 train_time:1014ms step_avg:63.36ms
step:17/2330 train_time:1072ms step_avg:63.08ms
step:18/2330 train_time:1136ms step_avg:63.14ms
step:19/2330 train_time:1200ms step_avg:63.14ms
step:20/2330 train_time:1264ms step_avg:63.18ms
step:21/2330 train_time:1324ms step_avg:63.04ms
step:22/2330 train_time:1388ms step_avg:63.07ms
step:23/2330 train_time:1447ms step_avg:62.89ms
step:24/2330 train_time:1509ms step_avg:62.87ms
step:25/2330 train_time:1567ms step_avg:62.69ms
step:26/2330 train_time:1630ms step_avg:62.69ms
step:27/2330 train_time:1689ms step_avg:62.56ms
step:28/2330 train_time:1751ms step_avg:62.54ms
step:29/2330 train_time:1810ms step_avg:62.41ms
step:30/2330 train_time:1872ms step_avg:62.41ms
step:31/2330 train_time:1931ms step_avg:62.29ms
step:32/2330 train_time:1993ms step_avg:62.27ms
step:33/2330 train_time:2052ms step_avg:62.18ms
step:34/2330 train_time:2115ms step_avg:62.21ms
step:35/2330 train_time:2175ms step_avg:62.16ms
step:36/2330 train_time:2238ms step_avg:62.18ms
step:37/2330 train_time:2298ms step_avg:62.11ms
step:38/2330 train_time:2361ms step_avg:62.13ms
step:39/2330 train_time:2421ms step_avg:62.07ms
step:40/2330 train_time:2484ms step_avg:62.09ms
step:41/2330 train_time:2543ms step_avg:62.03ms
step:42/2330 train_time:2606ms step_avg:62.04ms
step:43/2330 train_time:2665ms step_avg:61.97ms
step:44/2330 train_time:2727ms step_avg:61.98ms
step:45/2330 train_time:2786ms step_avg:61.91ms
step:46/2330 train_time:2848ms step_avg:61.92ms
step:47/2330 train_time:2907ms step_avg:61.86ms
step:48/2330 train_time:2970ms step_avg:61.87ms
step:49/2330 train_time:3030ms step_avg:61.84ms
step:50/2330 train_time:3094ms step_avg:61.87ms
step:51/2330 train_time:3153ms step_avg:61.83ms
step:52/2330 train_time:3216ms step_avg:61.85ms
step:53/2330 train_time:3276ms step_avg:61.81ms
step:54/2330 train_time:3339ms step_avg:61.83ms
step:55/2330 train_time:3399ms step_avg:61.80ms
step:56/2330 train_time:3462ms step_avg:61.82ms
step:57/2330 train_time:3521ms step_avg:61.78ms
step:58/2330 train_time:3584ms step_avg:61.79ms
step:59/2330 train_time:3643ms step_avg:61.75ms
step:60/2330 train_time:3705ms step_avg:61.75ms
step:61/2330 train_time:3764ms step_avg:61.71ms
step:62/2330 train_time:3827ms step_avg:61.73ms
step:63/2330 train_time:3887ms step_avg:61.69ms
step:64/2330 train_time:3950ms step_avg:61.71ms
step:65/2330 train_time:4009ms step_avg:61.68ms
step:66/2330 train_time:4073ms step_avg:61.71ms
step:67/2330 train_time:4133ms step_avg:61.69ms
step:68/2330 train_time:4196ms step_avg:61.70ms
step:69/2330 train_time:4255ms step_avg:61.67ms
step:70/2330 train_time:4318ms step_avg:61.68ms
step:71/2330 train_time:4378ms step_avg:61.66ms
step:72/2330 train_time:4440ms step_avg:61.66ms
step:73/2330 train_time:4500ms step_avg:61.65ms
step:74/2330 train_time:4562ms step_avg:61.65ms
step:75/2330 train_time:4621ms step_avg:61.62ms
step:76/2330 train_time:4683ms step_avg:61.62ms
step:77/2330 train_time:4743ms step_avg:61.60ms
step:78/2330 train_time:4806ms step_avg:61.61ms
step:79/2330 train_time:4865ms step_avg:61.58ms
step:80/2330 train_time:4927ms step_avg:61.59ms
step:81/2330 train_time:4987ms step_avg:61.56ms
step:82/2330 train_time:5050ms step_avg:61.58ms
step:83/2330 train_time:5110ms step_avg:61.56ms
step:84/2330 train_time:5173ms step_avg:61.59ms
step:85/2330 train_time:5234ms step_avg:61.57ms
step:86/2330 train_time:5296ms step_avg:61.58ms
step:87/2330 train_time:5356ms step_avg:61.56ms
step:88/2330 train_time:5419ms step_avg:61.58ms
step:89/2330 train_time:5478ms step_avg:61.55ms
step:90/2330 train_time:5541ms step_avg:61.56ms
step:91/2330 train_time:5601ms step_avg:61.55ms
step:92/2330 train_time:5663ms step_avg:61.55ms
step:93/2330 train_time:5722ms step_avg:61.53ms
step:94/2330 train_time:5785ms step_avg:61.54ms
step:95/2330 train_time:5843ms step_avg:61.51ms
step:96/2330 train_time:5906ms step_avg:61.52ms
step:97/2330 train_time:5966ms step_avg:61.51ms
step:98/2330 train_time:6029ms step_avg:61.52ms
step:99/2330 train_time:6090ms step_avg:61.51ms
step:100/2330 train_time:6153ms step_avg:61.53ms
step:101/2330 train_time:6213ms step_avg:61.51ms
step:102/2330 train_time:6275ms step_avg:61.52ms
step:103/2330 train_time:6335ms step_avg:61.50ms
step:104/2330 train_time:6397ms step_avg:61.51ms
step:105/2330 train_time:6457ms step_avg:61.49ms
step:106/2330 train_time:6519ms step_avg:61.50ms
step:107/2330 train_time:6578ms step_avg:61.48ms
step:108/2330 train_time:6640ms step_avg:61.48ms
step:109/2330 train_time:6700ms step_avg:61.47ms
step:110/2330 train_time:6763ms step_avg:61.48ms
step:111/2330 train_time:6823ms step_avg:61.47ms
step:112/2330 train_time:6885ms step_avg:61.47ms
step:113/2330 train_time:6944ms step_avg:61.45ms
step:114/2330 train_time:7006ms step_avg:61.46ms
step:115/2330 train_time:7066ms step_avg:61.45ms
step:116/2330 train_time:7130ms step_avg:61.47ms
step:117/2330 train_time:7190ms step_avg:61.45ms
step:118/2330 train_time:7253ms step_avg:61.46ms
step:119/2330 train_time:7313ms step_avg:61.45ms
step:120/2330 train_time:7376ms step_avg:61.47ms
step:121/2330 train_time:7437ms step_avg:61.46ms
step:122/2330 train_time:7499ms step_avg:61.47ms
step:123/2330 train_time:7558ms step_avg:61.45ms
step:124/2330 train_time:7620ms step_avg:61.45ms
step:125/2330 train_time:7680ms step_avg:61.44ms
step:126/2330 train_time:7742ms step_avg:61.45ms
step:127/2330 train_time:7802ms step_avg:61.43ms
step:128/2330 train_time:7864ms step_avg:61.44ms
step:129/2330 train_time:7923ms step_avg:61.42ms
step:130/2330 train_time:7987ms step_avg:61.44ms
step:131/2330 train_time:8045ms step_avg:61.41ms
step:132/2330 train_time:8108ms step_avg:61.43ms
step:133/2330 train_time:8169ms step_avg:61.42ms
step:134/2330 train_time:8232ms step_avg:61.43ms
step:135/2330 train_time:8292ms step_avg:61.42ms
step:136/2330 train_time:8355ms step_avg:61.43ms
step:137/2330 train_time:8415ms step_avg:61.43ms
step:138/2330 train_time:8478ms step_avg:61.44ms
step:139/2330 train_time:8537ms step_avg:61.42ms
step:140/2330 train_time:8600ms step_avg:61.43ms
step:141/2330 train_time:8659ms step_avg:61.41ms
step:142/2330 train_time:8722ms step_avg:61.42ms
step:143/2330 train_time:8781ms step_avg:61.40ms
step:144/2330 train_time:8843ms step_avg:61.41ms
step:145/2330 train_time:8902ms step_avg:61.39ms
step:146/2330 train_time:8964ms step_avg:61.40ms
step:147/2330 train_time:9024ms step_avg:61.39ms
step:148/2330 train_time:9086ms step_avg:61.39ms
step:149/2330 train_time:9146ms step_avg:61.38ms
step:150/2330 train_time:9209ms step_avg:61.39ms
step:151/2330 train_time:9270ms step_avg:61.39ms
step:152/2330 train_time:9334ms step_avg:61.41ms
step:153/2330 train_time:9393ms step_avg:61.39ms
step:154/2330 train_time:9456ms step_avg:61.40ms
step:155/2330 train_time:9516ms step_avg:61.39ms
step:156/2330 train_time:9579ms step_avg:61.40ms
step:157/2330 train_time:9638ms step_avg:61.39ms
step:158/2330 train_time:9701ms step_avg:61.40ms
step:159/2330 train_time:9760ms step_avg:61.38ms
step:160/2330 train_time:9821ms step_avg:61.38ms
step:161/2330 train_time:9881ms step_avg:61.37ms
step:162/2330 train_time:9943ms step_avg:61.38ms
step:163/2330 train_time:10002ms step_avg:61.36ms
step:164/2330 train_time:10064ms step_avg:61.37ms
step:165/2330 train_time:10123ms step_avg:61.35ms
step:166/2330 train_time:10186ms step_avg:61.36ms
step:167/2330 train_time:10246ms step_avg:61.36ms
step:168/2330 train_time:10309ms step_avg:61.36ms
step:169/2330 train_time:10369ms step_avg:61.35ms
step:170/2330 train_time:10432ms step_avg:61.37ms
step:171/2330 train_time:10492ms step_avg:61.36ms
step:172/2330 train_time:10555ms step_avg:61.37ms
step:173/2330 train_time:10615ms step_avg:61.36ms
step:174/2330 train_time:10677ms step_avg:61.36ms
step:175/2330 train_time:10737ms step_avg:61.35ms
step:176/2330 train_time:10799ms step_avg:61.36ms
step:177/2330 train_time:10858ms step_avg:61.34ms
step:178/2330 train_time:10920ms step_avg:61.35ms
step:179/2330 train_time:10981ms step_avg:61.34ms
step:180/2330 train_time:11043ms step_avg:61.35ms
step:181/2330 train_time:11102ms step_avg:61.34ms
step:182/2330 train_time:11165ms step_avg:61.34ms
step:183/2330 train_time:11225ms step_avg:61.34ms
step:184/2330 train_time:11287ms step_avg:61.34ms
step:185/2330 train_time:11348ms step_avg:61.34ms
step:186/2330 train_time:11411ms step_avg:61.35ms
step:187/2330 train_time:11472ms step_avg:61.35ms
step:188/2330 train_time:11535ms step_avg:61.36ms
step:189/2330 train_time:11595ms step_avg:61.35ms
step:190/2330 train_time:11658ms step_avg:61.36ms
step:191/2330 train_time:11717ms step_avg:61.35ms
step:192/2330 train_time:11779ms step_avg:61.35ms
step:193/2330 train_time:11839ms step_avg:61.34ms
step:194/2330 train_time:11901ms step_avg:61.35ms
step:195/2330 train_time:11960ms step_avg:61.33ms
step:196/2330 train_time:12021ms step_avg:61.33ms
step:197/2330 train_time:12082ms step_avg:61.33ms
step:198/2330 train_time:12145ms step_avg:61.34ms
step:199/2330 train_time:12203ms step_avg:61.32ms
step:200/2330 train_time:12266ms step_avg:61.33ms
step:201/2330 train_time:12326ms step_avg:61.32ms
step:202/2330 train_time:12389ms step_avg:61.33ms
step:203/2330 train_time:12449ms step_avg:61.32ms
step:204/2330 train_time:12512ms step_avg:61.33ms
step:205/2330 train_time:12572ms step_avg:61.33ms
step:206/2330 train_time:12636ms step_avg:61.34ms
step:207/2330 train_time:12695ms step_avg:61.33ms
step:208/2330 train_time:12758ms step_avg:61.33ms
step:209/2330 train_time:12817ms step_avg:61.32ms
step:210/2330 train_time:12879ms step_avg:61.33ms
step:211/2330 train_time:12939ms step_avg:61.32ms
step:212/2330 train_time:13001ms step_avg:61.33ms
step:213/2330 train_time:13061ms step_avg:61.32ms
step:214/2330 train_time:13123ms step_avg:61.32ms
step:215/2330 train_time:13183ms step_avg:61.32ms
step:216/2330 train_time:13246ms step_avg:61.32ms
step:217/2330 train_time:13305ms step_avg:61.31ms
step:218/2330 train_time:13368ms step_avg:61.32ms
step:219/2330 train_time:13428ms step_avg:61.31ms
step:220/2330 train_time:13491ms step_avg:61.32ms
step:221/2330 train_time:13551ms step_avg:61.32ms
step:222/2330 train_time:13615ms step_avg:61.33ms
step:223/2330 train_time:13675ms step_avg:61.32ms
step:224/2330 train_time:13738ms step_avg:61.33ms
step:225/2330 train_time:13797ms step_avg:61.32ms
step:226/2330 train_time:13859ms step_avg:61.32ms
step:227/2330 train_time:13918ms step_avg:61.31ms
step:228/2330 train_time:13981ms step_avg:61.32ms
step:229/2330 train_time:14040ms step_avg:61.31ms
step:230/2330 train_time:14102ms step_avg:61.31ms
step:231/2330 train_time:14161ms step_avg:61.30ms
step:232/2330 train_time:14223ms step_avg:61.31ms
step:233/2330 train_time:14283ms step_avg:61.30ms
step:234/2330 train_time:14346ms step_avg:61.31ms
step:235/2330 train_time:14406ms step_avg:61.30ms
step:236/2330 train_time:14469ms step_avg:61.31ms
step:237/2330 train_time:14529ms step_avg:61.31ms
step:238/2330 train_time:14593ms step_avg:61.31ms
step:239/2330 train_time:14652ms step_avg:61.31ms
step:240/2330 train_time:14715ms step_avg:61.31ms
step:241/2330 train_time:14775ms step_avg:61.31ms
step:242/2330 train_time:14838ms step_avg:61.31ms
step:243/2330 train_time:14897ms step_avg:61.30ms
step:244/2330 train_time:14959ms step_avg:61.31ms
step:245/2330 train_time:15018ms step_avg:61.30ms
step:246/2330 train_time:15082ms step_avg:61.31ms
step:247/2330 train_time:15141ms step_avg:61.30ms
step:248/2330 train_time:15203ms step_avg:61.30ms
step:249/2330 train_time:15262ms step_avg:61.29ms
step:250/2330 train_time:15325ms step_avg:61.30ms
step:250/2330 val_loss:4.1299 train_time:15390ms step_avg:61.56ms
step:251/2330 train_time:15414ms step_avg:61.41ms
step:252/2330 train_time:15450ms step_avg:61.31ms
step:253/2330 train_time:15516ms step_avg:61.33ms
step:254/2330 train_time:15582ms step_avg:61.35ms
step:255/2330 train_time:15641ms step_avg:61.34ms
step:256/2330 train_time:15704ms step_avg:61.34ms
step:257/2330 train_time:15763ms step_avg:61.34ms
step:258/2330 train_time:15825ms step_avg:61.34ms
step:259/2330 train_time:15884ms step_avg:61.33ms
step:260/2330 train_time:15946ms step_avg:61.33ms
step:261/2330 train_time:16006ms step_avg:61.33ms
step:262/2330 train_time:16067ms step_avg:61.33ms
step:263/2330 train_time:16126ms step_avg:61.32ms
step:264/2330 train_time:16188ms step_avg:61.32ms
step:265/2330 train_time:16247ms step_avg:61.31ms
step:266/2330 train_time:16310ms step_avg:61.32ms
step:267/2330 train_time:16370ms step_avg:61.31ms
step:268/2330 train_time:16433ms step_avg:61.32ms
step:269/2330 train_time:16495ms step_avg:61.32ms
step:270/2330 train_time:16558ms step_avg:61.33ms
step:271/2330 train_time:16619ms step_avg:61.32ms
step:272/2330 train_time:16681ms step_avg:61.33ms
step:273/2330 train_time:16741ms step_avg:61.32ms
step:274/2330 train_time:16803ms step_avg:61.33ms
step:275/2330 train_time:16863ms step_avg:61.32ms
step:276/2330 train_time:16925ms step_avg:61.32ms
step:277/2330 train_time:16984ms step_avg:61.31ms
step:278/2330 train_time:17046ms step_avg:61.32ms
step:279/2330 train_time:17105ms step_avg:61.31ms
step:280/2330 train_time:17167ms step_avg:61.31ms
step:281/2330 train_time:17226ms step_avg:61.30ms
step:282/2330 train_time:17288ms step_avg:61.31ms
step:283/2330 train_time:17348ms step_avg:61.30ms
step:284/2330 train_time:17411ms step_avg:61.31ms
step:285/2330 train_time:17472ms step_avg:61.31ms
step:286/2330 train_time:17535ms step_avg:61.31ms
step:287/2330 train_time:17595ms step_avg:61.31ms
step:288/2330 train_time:17657ms step_avg:61.31ms
step:289/2330 train_time:17718ms step_avg:61.31ms
step:290/2330 train_time:17781ms step_avg:61.31ms
step:291/2330 train_time:17840ms step_avg:61.31ms
step:292/2330 train_time:17903ms step_avg:61.31ms
step:293/2330 train_time:17963ms step_avg:61.31ms
step:294/2330 train_time:18025ms step_avg:61.31ms
step:295/2330 train_time:18084ms step_avg:61.30ms
step:296/2330 train_time:18146ms step_avg:61.30ms
step:297/2330 train_time:18205ms step_avg:61.30ms
step:298/2330 train_time:18267ms step_avg:61.30ms
step:299/2330 train_time:18327ms step_avg:61.29ms
step:300/2330 train_time:18390ms step_avg:61.30ms
step:301/2330 train_time:18450ms step_avg:61.30ms
step:302/2330 train_time:18512ms step_avg:61.30ms
step:303/2330 train_time:18572ms step_avg:61.29ms
step:304/2330 train_time:18635ms step_avg:61.30ms
step:305/2330 train_time:18695ms step_avg:61.29ms
step:306/2330 train_time:18758ms step_avg:61.30ms
step:307/2330 train_time:18818ms step_avg:61.30ms
step:308/2330 train_time:18881ms step_avg:61.30ms
step:309/2330 train_time:18940ms step_avg:61.29ms
step:310/2330 train_time:19003ms step_avg:61.30ms
step:311/2330 train_time:19063ms step_avg:61.30ms
step:312/2330 train_time:19126ms step_avg:61.30ms
step:313/2330 train_time:19185ms step_avg:61.30ms
step:314/2330 train_time:19248ms step_avg:61.30ms
step:315/2330 train_time:19307ms step_avg:61.29ms
step:316/2330 train_time:19370ms step_avg:61.30ms
step:317/2330 train_time:19430ms step_avg:61.29ms
step:318/2330 train_time:19492ms step_avg:61.29ms
step:319/2330 train_time:19551ms step_avg:61.29ms
step:320/2330 train_time:19613ms step_avg:61.29ms
step:321/2330 train_time:19674ms step_avg:61.29ms
step:322/2330 train_time:19736ms step_avg:61.29ms
step:323/2330 train_time:19797ms step_avg:61.29ms
step:324/2330 train_time:19859ms step_avg:61.29ms
step:325/2330 train_time:19920ms step_avg:61.29ms
step:326/2330 train_time:19982ms step_avg:61.30ms
step:327/2330 train_time:20042ms step_avg:61.29ms
step:328/2330 train_time:20106ms step_avg:61.30ms
step:329/2330 train_time:20165ms step_avg:61.29ms
step:330/2330 train_time:20228ms step_avg:61.30ms
step:331/2330 train_time:20288ms step_avg:61.29ms
step:332/2330 train_time:20351ms step_avg:61.30ms
step:333/2330 train_time:20410ms step_avg:61.29ms
step:334/2330 train_time:20473ms step_avg:61.30ms
step:335/2330 train_time:20532ms step_avg:61.29ms
step:336/2330 train_time:20594ms step_avg:61.29ms
step:337/2330 train_time:20653ms step_avg:61.29ms
step:338/2330 train_time:20716ms step_avg:61.29ms
step:339/2330 train_time:20776ms step_avg:61.29ms
step:340/2330 train_time:20839ms step_avg:61.29ms
step:341/2330 train_time:20899ms step_avg:61.29ms
step:342/2330 train_time:20962ms step_avg:61.29ms
step:343/2330 train_time:21022ms step_avg:61.29ms
step:344/2330 train_time:21084ms step_avg:61.29ms
step:345/2330 train_time:21144ms step_avg:61.29ms
step:346/2330 train_time:21207ms step_avg:61.29ms
step:347/2330 train_time:21267ms step_avg:61.29ms
step:348/2330 train_time:21329ms step_avg:61.29ms
step:349/2330 train_time:21389ms step_avg:61.29ms
step:350/2330 train_time:21451ms step_avg:61.29ms
step:351/2330 train_time:21510ms step_avg:61.28ms
step:352/2330 train_time:21573ms step_avg:61.29ms
step:353/2330 train_time:21633ms step_avg:61.28ms
step:354/2330 train_time:21695ms step_avg:61.28ms
step:355/2330 train_time:21755ms step_avg:61.28ms
step:356/2330 train_time:21818ms step_avg:61.29ms
step:357/2330 train_time:21878ms step_avg:61.28ms
step:358/2330 train_time:21941ms step_avg:61.29ms
step:359/2330 train_time:22001ms step_avg:61.28ms
step:360/2330 train_time:22064ms step_avg:61.29ms
step:361/2330 train_time:22124ms step_avg:61.29ms
step:362/2330 train_time:22186ms step_avg:61.29ms
step:363/2330 train_time:22246ms step_avg:61.28ms
step:364/2330 train_time:22308ms step_avg:61.29ms
step:365/2330 train_time:22367ms step_avg:61.28ms
step:366/2330 train_time:22429ms step_avg:61.28ms
step:367/2330 train_time:22488ms step_avg:61.28ms
step:368/2330 train_time:22550ms step_avg:61.28ms
step:369/2330 train_time:22609ms step_avg:61.27ms
step:370/2330 train_time:22672ms step_avg:61.27ms
step:371/2330 train_time:22731ms step_avg:61.27ms
step:372/2330 train_time:22794ms step_avg:61.27ms
step:373/2330 train_time:22853ms step_avg:61.27ms
step:374/2330 train_time:22917ms step_avg:61.28ms
step:375/2330 train_time:22977ms step_avg:61.27ms
step:376/2330 train_time:23040ms step_avg:61.28ms
step:377/2330 train_time:23100ms step_avg:61.27ms
step:378/2330 train_time:23163ms step_avg:61.28ms
step:379/2330 train_time:23224ms step_avg:61.28ms
step:380/2330 train_time:23286ms step_avg:61.28ms
step:381/2330 train_time:23345ms step_avg:61.27ms
step:382/2330 train_time:23408ms step_avg:61.28ms
step:383/2330 train_time:23467ms step_avg:61.27ms
step:384/2330 train_time:23530ms step_avg:61.28ms
step:385/2330 train_time:23589ms step_avg:61.27ms
step:386/2330 train_time:23652ms step_avg:61.27ms
step:387/2330 train_time:23711ms step_avg:61.27ms
step:388/2330 train_time:23773ms step_avg:61.27ms
step:389/2330 train_time:23832ms step_avg:61.27ms
step:390/2330 train_time:23894ms step_avg:61.27ms
step:391/2330 train_time:23956ms step_avg:61.27ms
step:392/2330 train_time:24019ms step_avg:61.27ms
step:393/2330 train_time:24079ms step_avg:61.27ms
step:394/2330 train_time:24142ms step_avg:61.28ms
step:395/2330 train_time:24202ms step_avg:61.27ms
step:396/2330 train_time:24265ms step_avg:61.28ms
step:397/2330 train_time:24325ms step_avg:61.27ms
step:398/2330 train_time:24388ms step_avg:61.28ms
step:399/2330 train_time:24447ms step_avg:61.27ms
step:400/2330 train_time:24509ms step_avg:61.27ms
step:401/2330 train_time:24569ms step_avg:61.27ms
step:402/2330 train_time:24631ms step_avg:61.27ms
step:403/2330 train_time:24690ms step_avg:61.27ms
step:404/2330 train_time:24752ms step_avg:61.27ms
step:405/2330 train_time:24812ms step_avg:61.26ms
step:406/2330 train_time:24874ms step_avg:61.27ms
step:407/2330 train_time:24934ms step_avg:61.26ms
step:408/2330 train_time:24997ms step_avg:61.27ms
step:409/2330 train_time:25058ms step_avg:61.27ms
step:410/2330 train_time:25121ms step_avg:61.27ms
step:411/2330 train_time:25180ms step_avg:61.27ms
step:412/2330 train_time:25244ms step_avg:61.27ms
step:413/2330 train_time:25303ms step_avg:61.27ms
step:414/2330 train_time:25366ms step_avg:61.27ms
step:415/2330 train_time:25425ms step_avg:61.27ms
step:416/2330 train_time:25488ms step_avg:61.27ms
step:417/2330 train_time:25547ms step_avg:61.26ms
step:418/2330 train_time:25610ms step_avg:61.27ms
step:419/2330 train_time:25671ms step_avg:61.27ms
step:420/2330 train_time:25732ms step_avg:61.27ms
step:421/2330 train_time:25792ms step_avg:61.26ms
step:422/2330 train_time:25854ms step_avg:61.27ms
step:423/2330 train_time:25913ms step_avg:61.26ms
step:424/2330 train_time:25975ms step_avg:61.26ms
step:425/2330 train_time:26035ms step_avg:61.26ms
step:426/2330 train_time:26097ms step_avg:61.26ms
step:427/2330 train_time:26158ms step_avg:61.26ms
step:428/2330 train_time:26222ms step_avg:61.27ms
step:429/2330 train_time:26282ms step_avg:61.26ms
step:430/2330 train_time:26344ms step_avg:61.27ms
step:431/2330 train_time:26405ms step_avg:61.27ms
step:432/2330 train_time:26468ms step_avg:61.27ms
step:433/2330 train_time:26528ms step_avg:61.27ms
step:434/2330 train_time:26590ms step_avg:61.27ms
step:435/2330 train_time:26649ms step_avg:61.26ms
step:436/2330 train_time:26712ms step_avg:61.26ms
step:437/2330 train_time:26771ms step_avg:61.26ms
step:438/2330 train_time:26833ms step_avg:61.26ms
step:439/2330 train_time:26893ms step_avg:61.26ms
step:440/2330 train_time:26955ms step_avg:61.26ms
step:441/2330 train_time:27014ms step_avg:61.26ms
step:442/2330 train_time:27077ms step_avg:61.26ms
step:443/2330 train_time:27137ms step_avg:61.26ms
step:444/2330 train_time:27200ms step_avg:61.26ms
step:445/2330 train_time:27261ms step_avg:61.26ms
step:446/2330 train_time:27323ms step_avg:61.26ms
step:447/2330 train_time:27384ms step_avg:61.26ms
step:448/2330 train_time:27446ms step_avg:61.26ms
step:449/2330 train_time:27506ms step_avg:61.26ms
step:450/2330 train_time:27568ms step_avg:61.26ms
step:451/2330 train_time:27628ms step_avg:61.26ms
step:452/2330 train_time:27690ms step_avg:61.26ms
step:453/2330 train_time:27749ms step_avg:61.26ms
step:454/2330 train_time:27810ms step_avg:61.26ms
step:455/2330 train_time:27871ms step_avg:61.25ms
step:456/2330 train_time:27932ms step_avg:61.26ms
step:457/2330 train_time:27992ms step_avg:61.25ms
step:458/2330 train_time:28054ms step_avg:61.25ms
step:459/2330 train_time:28115ms step_avg:61.25ms
step:460/2330 train_time:28177ms step_avg:61.26ms
step:461/2330 train_time:28238ms step_avg:61.25ms
step:462/2330 train_time:28300ms step_avg:61.26ms
step:463/2330 train_time:28360ms step_avg:61.25ms
step:464/2330 train_time:28423ms step_avg:61.26ms
step:465/2330 train_time:28483ms step_avg:61.25ms
step:466/2330 train_time:28546ms step_avg:61.26ms
step:467/2330 train_time:28606ms step_avg:61.25ms
step:468/2330 train_time:28669ms step_avg:61.26ms
step:469/2330 train_time:28728ms step_avg:61.25ms
step:470/2330 train_time:28790ms step_avg:61.26ms
step:471/2330 train_time:28849ms step_avg:61.25ms
step:472/2330 train_time:28912ms step_avg:61.25ms
step:473/2330 train_time:28970ms step_avg:61.25ms
step:474/2330 train_time:29032ms step_avg:61.25ms
step:475/2330 train_time:29093ms step_avg:61.25ms
step:476/2330 train_time:29156ms step_avg:61.25ms
step:477/2330 train_time:29216ms step_avg:61.25ms
step:478/2330 train_time:29279ms step_avg:61.25ms
step:479/2330 train_time:29340ms step_avg:61.25ms
step:480/2330 train_time:29403ms step_avg:61.26ms
step:481/2330 train_time:29463ms step_avg:61.25ms
step:482/2330 train_time:29526ms step_avg:61.26ms
step:483/2330 train_time:29585ms step_avg:61.25ms
step:484/2330 train_time:29648ms step_avg:61.26ms
step:485/2330 train_time:29707ms step_avg:61.25ms
step:486/2330 train_time:29769ms step_avg:61.25ms
step:487/2330 train_time:29829ms step_avg:61.25ms
step:488/2330 train_time:29891ms step_avg:61.25ms
step:489/2330 train_time:29950ms step_avg:61.25ms
step:490/2330 train_time:30012ms step_avg:61.25ms
step:491/2330 train_time:30072ms step_avg:61.25ms
step:492/2330 train_time:30135ms step_avg:61.25ms
step:493/2330 train_time:30194ms step_avg:61.25ms
step:494/2330 train_time:30257ms step_avg:61.25ms
step:495/2330 train_time:30317ms step_avg:61.25ms
step:496/2330 train_time:30380ms step_avg:61.25ms
step:497/2330 train_time:30440ms step_avg:61.25ms
step:498/2330 train_time:30503ms step_avg:61.25ms
step:499/2330 train_time:30563ms step_avg:61.25ms
step:500/2330 train_time:30626ms step_avg:61.25ms
step:500/2330 val_loss:3.8697 train_time:30690ms step_avg:61.38ms
step:501/2330 train_time:30712ms step_avg:61.30ms
step:502/2330 train_time:30750ms step_avg:61.26ms
step:503/2330 train_time:30816ms step_avg:61.26ms
step:504/2330 train_time:30883ms step_avg:61.28ms
step:505/2330 train_time:30944ms step_avg:61.27ms
step:506/2330 train_time:31007ms step_avg:61.28ms
step:507/2330 train_time:31066ms step_avg:61.27ms
step:508/2330 train_time:31128ms step_avg:61.28ms
step:509/2330 train_time:31186ms step_avg:61.27ms
step:510/2330 train_time:31248ms step_avg:61.27ms
step:511/2330 train_time:31307ms step_avg:61.27ms
step:512/2330 train_time:31369ms step_avg:61.27ms
step:513/2330 train_time:31428ms step_avg:61.26ms
step:514/2330 train_time:31489ms step_avg:61.26ms
step:515/2330 train_time:31547ms step_avg:61.26ms
step:516/2330 train_time:31609ms step_avg:61.26ms
step:517/2330 train_time:31668ms step_avg:61.25ms
step:518/2330 train_time:31731ms step_avg:61.26ms
step:519/2330 train_time:31794ms step_avg:61.26ms
step:520/2330 train_time:31858ms step_avg:61.27ms
step:521/2330 train_time:31918ms step_avg:61.26ms
step:522/2330 train_time:31981ms step_avg:61.27ms
step:523/2330 train_time:32041ms step_avg:61.26ms
step:524/2330 train_time:32104ms step_avg:61.27ms
step:525/2330 train_time:32163ms step_avg:61.26ms
step:526/2330 train_time:32225ms step_avg:61.26ms
step:527/2330 train_time:32284ms step_avg:61.26ms
step:528/2330 train_time:32346ms step_avg:61.26ms
step:529/2330 train_time:32405ms step_avg:61.26ms
step:530/2330 train_time:32467ms step_avg:61.26ms
step:531/2330 train_time:32526ms step_avg:61.25ms
step:532/2330 train_time:32587ms step_avg:61.25ms
step:533/2330 train_time:32646ms step_avg:61.25ms
step:534/2330 train_time:32709ms step_avg:61.25ms
step:535/2330 train_time:32769ms step_avg:61.25ms
step:536/2330 train_time:32831ms step_avg:61.25ms
step:537/2330 train_time:32891ms step_avg:61.25ms
step:538/2330 train_time:32955ms step_avg:61.25ms
step:539/2330 train_time:33016ms step_avg:61.25ms
step:540/2330 train_time:33079ms step_avg:61.26ms
step:541/2330 train_time:33139ms step_avg:61.25ms
step:542/2330 train_time:33202ms step_avg:61.26ms
step:543/2330 train_time:33261ms step_avg:61.25ms
step:544/2330 train_time:33323ms step_avg:61.26ms
step:545/2330 train_time:33383ms step_avg:61.25ms
step:546/2330 train_time:33445ms step_avg:61.25ms
step:547/2330 train_time:33503ms step_avg:61.25ms
step:548/2330 train_time:33566ms step_avg:61.25ms
step:549/2330 train_time:33626ms step_avg:61.25ms
step:550/2330 train_time:33688ms step_avg:61.25ms
step:551/2330 train_time:33747ms step_avg:61.25ms
step:552/2330 train_time:33810ms step_avg:61.25ms
step:553/2330 train_time:33869ms step_avg:61.25ms
step:554/2330 train_time:33932ms step_avg:61.25ms
step:555/2330 train_time:33992ms step_avg:61.25ms
step:556/2330 train_time:34056ms step_avg:61.25ms
step:557/2330 train_time:34116ms step_avg:61.25ms
step:558/2330 train_time:34178ms step_avg:61.25ms
step:559/2330 train_time:34239ms step_avg:61.25ms
step:560/2330 train_time:34302ms step_avg:61.25ms
step:561/2330 train_time:34361ms step_avg:61.25ms
step:562/2330 train_time:34424ms step_avg:61.25ms
step:563/2330 train_time:34483ms step_avg:61.25ms
step:564/2330 train_time:34545ms step_avg:61.25ms
step:565/2330 train_time:34604ms step_avg:61.25ms
step:566/2330 train_time:34666ms step_avg:61.25ms
step:567/2330 train_time:34726ms step_avg:61.25ms
step:568/2330 train_time:34789ms step_avg:61.25ms
step:569/2330 train_time:34849ms step_avg:61.25ms
step:570/2330 train_time:34912ms step_avg:61.25ms
step:571/2330 train_time:34971ms step_avg:61.24ms
step:572/2330 train_time:35034ms step_avg:61.25ms
step:573/2330 train_time:35094ms step_avg:61.25ms
step:574/2330 train_time:35158ms step_avg:61.25ms
step:575/2330 train_time:35218ms step_avg:61.25ms
step:576/2330 train_time:35281ms step_avg:61.25ms
step:577/2330 train_time:35341ms step_avg:61.25ms
step:578/2330 train_time:35404ms step_avg:61.25ms
step:579/2330 train_time:35462ms step_avg:61.25ms
step:580/2330 train_time:35525ms step_avg:61.25ms
step:581/2330 train_time:35584ms step_avg:61.25ms
step:582/2330 train_time:35647ms step_avg:61.25ms
step:583/2330 train_time:35707ms step_avg:61.25ms
step:584/2330 train_time:35770ms step_avg:61.25ms
step:585/2330 train_time:35829ms step_avg:61.25ms
step:586/2330 train_time:35891ms step_avg:61.25ms
step:587/2330 train_time:35951ms step_avg:61.24ms
step:588/2330 train_time:36013ms step_avg:61.25ms
step:589/2330 train_time:36072ms step_avg:61.24ms
step:590/2330 train_time:36135ms step_avg:61.25ms
step:591/2330 train_time:36195ms step_avg:61.24ms
step:592/2330 train_time:36259ms step_avg:61.25ms
step:593/2330 train_time:36319ms step_avg:61.25ms
step:594/2330 train_time:36382ms step_avg:61.25ms
step:595/2330 train_time:36441ms step_avg:61.25ms
step:596/2330 train_time:36504ms step_avg:61.25ms
step:597/2330 train_time:36563ms step_avg:61.24ms
step:598/2330 train_time:36625ms step_avg:61.25ms
step:599/2330 train_time:36685ms step_avg:61.24ms
step:600/2330 train_time:36748ms step_avg:61.25ms
step:601/2330 train_time:36808ms step_avg:61.24ms
step:602/2330 train_time:36870ms step_avg:61.25ms
step:603/2330 train_time:36929ms step_avg:61.24ms
step:604/2330 train_time:36990ms step_avg:61.24ms
step:605/2330 train_time:37050ms step_avg:61.24ms
step:606/2330 train_time:37112ms step_avg:61.24ms
step:607/2330 train_time:37172ms step_avg:61.24ms
step:608/2330 train_time:37235ms step_avg:61.24ms
step:609/2330 train_time:37295ms step_avg:61.24ms
step:610/2330 train_time:37358ms step_avg:61.24ms
step:611/2330 train_time:37419ms step_avg:61.24ms
step:612/2330 train_time:37482ms step_avg:61.24ms
step:613/2330 train_time:37541ms step_avg:61.24ms
step:614/2330 train_time:37604ms step_avg:61.24ms
step:615/2330 train_time:37663ms step_avg:61.24ms
step:616/2330 train_time:37726ms step_avg:61.24ms
step:617/2330 train_time:37785ms step_avg:61.24ms
step:618/2330 train_time:37847ms step_avg:61.24ms
step:619/2330 train_time:37907ms step_avg:61.24ms
step:620/2330 train_time:37969ms step_avg:61.24ms
step:621/2330 train_time:38028ms step_avg:61.24ms
step:622/2330 train_time:38090ms step_avg:61.24ms
step:623/2330 train_time:38150ms step_avg:61.24ms
step:624/2330 train_time:38213ms step_avg:61.24ms
step:625/2330 train_time:38273ms step_avg:61.24ms
step:626/2330 train_time:38336ms step_avg:61.24ms
step:627/2330 train_time:38397ms step_avg:61.24ms
step:628/2330 train_time:38460ms step_avg:61.24ms
step:629/2330 train_time:38520ms step_avg:61.24ms
step:630/2330 train_time:38582ms step_avg:61.24ms
step:631/2330 train_time:38642ms step_avg:61.24ms
step:632/2330 train_time:38704ms step_avg:61.24ms
step:633/2330 train_time:38763ms step_avg:61.24ms
step:634/2330 train_time:38826ms step_avg:61.24ms
step:635/2330 train_time:38886ms step_avg:61.24ms
step:636/2330 train_time:38947ms step_avg:61.24ms
step:637/2330 train_time:39008ms step_avg:61.24ms
step:638/2330 train_time:39070ms step_avg:61.24ms
step:639/2330 train_time:39129ms step_avg:61.24ms
step:640/2330 train_time:39191ms step_avg:61.24ms
step:641/2330 train_time:39251ms step_avg:61.23ms
step:642/2330 train_time:39313ms step_avg:61.24ms
step:643/2330 train_time:39373ms step_avg:61.23ms
step:644/2330 train_time:39436ms step_avg:61.24ms
step:645/2330 train_time:39497ms step_avg:61.24ms
step:646/2330 train_time:39560ms step_avg:61.24ms
step:647/2330 train_time:39620ms step_avg:61.24ms
step:648/2330 train_time:39683ms step_avg:61.24ms
step:649/2330 train_time:39742ms step_avg:61.24ms
step:650/2330 train_time:39805ms step_avg:61.24ms
step:651/2330 train_time:39865ms step_avg:61.24ms
step:652/2330 train_time:39927ms step_avg:61.24ms
step:653/2330 train_time:39986ms step_avg:61.23ms
step:654/2330 train_time:40048ms step_avg:61.24ms
step:655/2330 train_time:40108ms step_avg:61.23ms
step:656/2330 train_time:40170ms step_avg:61.24ms
step:657/2330 train_time:40229ms step_avg:61.23ms
step:658/2330 train_time:40291ms step_avg:61.23ms
step:659/2330 train_time:40351ms step_avg:61.23ms
step:660/2330 train_time:40415ms step_avg:61.23ms
step:661/2330 train_time:40474ms step_avg:61.23ms
step:662/2330 train_time:40537ms step_avg:61.23ms
step:663/2330 train_time:40597ms step_avg:61.23ms
step:664/2330 train_time:40660ms step_avg:61.23ms
step:665/2330 train_time:40721ms step_avg:61.23ms
step:666/2330 train_time:40783ms step_avg:61.24ms
step:667/2330 train_time:40844ms step_avg:61.23ms
step:668/2330 train_time:40906ms step_avg:61.24ms
step:669/2330 train_time:40965ms step_avg:61.23ms
step:670/2330 train_time:41027ms step_avg:61.23ms
step:671/2330 train_time:41087ms step_avg:61.23ms
step:672/2330 train_time:41149ms step_avg:61.23ms
step:673/2330 train_time:41208ms step_avg:61.23ms
step:674/2330 train_time:41270ms step_avg:61.23ms
step:675/2330 train_time:41329ms step_avg:61.23ms
step:676/2330 train_time:41392ms step_avg:61.23ms
step:677/2330 train_time:41451ms step_avg:61.23ms
step:678/2330 train_time:41514ms step_avg:61.23ms
step:679/2330 train_time:41574ms step_avg:61.23ms
step:680/2330 train_time:41638ms step_avg:61.23ms
step:681/2330 train_time:41698ms step_avg:61.23ms
step:682/2330 train_time:41761ms step_avg:61.23ms
step:683/2330 train_time:41820ms step_avg:61.23ms
step:684/2330 train_time:41883ms step_avg:61.23ms
step:685/2330 train_time:41943ms step_avg:61.23ms
step:686/2330 train_time:42005ms step_avg:61.23ms
step:687/2330 train_time:42064ms step_avg:61.23ms
step:688/2330 train_time:42127ms step_avg:61.23ms
step:689/2330 train_time:42186ms step_avg:61.23ms
step:690/2330 train_time:42248ms step_avg:61.23ms
step:691/2330 train_time:42308ms step_avg:61.23ms
step:692/2330 train_time:42370ms step_avg:61.23ms
step:693/2330 train_time:42430ms step_avg:61.23ms
step:694/2330 train_time:42492ms step_avg:61.23ms
step:695/2330 train_time:42552ms step_avg:61.23ms
step:696/2330 train_time:42615ms step_avg:61.23ms
step:697/2330 train_time:42675ms step_avg:61.23ms
step:698/2330 train_time:42739ms step_avg:61.23ms
step:699/2330 train_time:42799ms step_avg:61.23ms
step:700/2330 train_time:42861ms step_avg:61.23ms
step:701/2330 train_time:42922ms step_avg:61.23ms
step:702/2330 train_time:42985ms step_avg:61.23ms
step:703/2330 train_time:43044ms step_avg:61.23ms
step:704/2330 train_time:43107ms step_avg:61.23ms
step:705/2330 train_time:43167ms step_avg:61.23ms
step:706/2330 train_time:43229ms step_avg:61.23ms
step:707/2330 train_time:43288ms step_avg:61.23ms
step:708/2330 train_time:43350ms step_avg:61.23ms
step:709/2330 train_time:43409ms step_avg:61.23ms
step:710/2330 train_time:43472ms step_avg:61.23ms
step:711/2330 train_time:43531ms step_avg:61.22ms
step:712/2330 train_time:43594ms step_avg:61.23ms
step:713/2330 train_time:43654ms step_avg:61.23ms
step:714/2330 train_time:43717ms step_avg:61.23ms
step:715/2330 train_time:43777ms step_avg:61.23ms
step:716/2330 train_time:43840ms step_avg:61.23ms
step:717/2330 train_time:43900ms step_avg:61.23ms
step:718/2330 train_time:43963ms step_avg:61.23ms
step:719/2330 train_time:44022ms step_avg:61.23ms
step:720/2330 train_time:44085ms step_avg:61.23ms
step:721/2330 train_time:44144ms step_avg:61.23ms
step:722/2330 train_time:44207ms step_avg:61.23ms
step:723/2330 train_time:44266ms step_avg:61.23ms
step:724/2330 train_time:44328ms step_avg:61.23ms
step:725/2330 train_time:44388ms step_avg:61.22ms
step:726/2330 train_time:44449ms step_avg:61.23ms
step:727/2330 train_time:44509ms step_avg:61.22ms
step:728/2330 train_time:44571ms step_avg:61.22ms
step:729/2330 train_time:44630ms step_avg:61.22ms
step:730/2330 train_time:44693ms step_avg:61.22ms
step:731/2330 train_time:44754ms step_avg:61.22ms
step:732/2330 train_time:44817ms step_avg:61.23ms
step:733/2330 train_time:44878ms step_avg:61.22ms
step:734/2330 train_time:44941ms step_avg:61.23ms
step:735/2330 train_time:45001ms step_avg:61.23ms
step:736/2330 train_time:45064ms step_avg:61.23ms
step:737/2330 train_time:45123ms step_avg:61.22ms
step:738/2330 train_time:45185ms step_avg:61.23ms
step:739/2330 train_time:45244ms step_avg:61.22ms
step:740/2330 train_time:45306ms step_avg:61.22ms
step:741/2330 train_time:45366ms step_avg:61.22ms
step:742/2330 train_time:45428ms step_avg:61.22ms
step:743/2330 train_time:45487ms step_avg:61.22ms
step:744/2330 train_time:45550ms step_avg:61.22ms
step:745/2330 train_time:45610ms step_avg:61.22ms
step:746/2330 train_time:45672ms step_avg:61.22ms
step:747/2330 train_time:45732ms step_avg:61.22ms
step:748/2330 train_time:45795ms step_avg:61.22ms
step:749/2330 train_time:45855ms step_avg:61.22ms
step:750/2330 train_time:45918ms step_avg:61.22ms
step:750/2330 val_loss:3.7079 train_time:45983ms step_avg:61.31ms
step:751/2330 train_time:46005ms step_avg:61.26ms
step:752/2330 train_time:46044ms step_avg:61.23ms
step:753/2330 train_time:46110ms step_avg:61.23ms
step:754/2330 train_time:46176ms step_avg:61.24ms
step:755/2330 train_time:46237ms step_avg:61.24ms
step:756/2330 train_time:46299ms step_avg:61.24ms
step:757/2330 train_time:46358ms step_avg:61.24ms
step:758/2330 train_time:46420ms step_avg:61.24ms
step:759/2330 train_time:46479ms step_avg:61.24ms
step:760/2330 train_time:46541ms step_avg:61.24ms
step:761/2330 train_time:46600ms step_avg:61.23ms
step:762/2330 train_time:46661ms step_avg:61.24ms
step:763/2330 train_time:46720ms step_avg:61.23ms
step:764/2330 train_time:46782ms step_avg:61.23ms
step:765/2330 train_time:46841ms step_avg:61.23ms
step:766/2330 train_time:46906ms step_avg:61.23ms
step:767/2330 train_time:46967ms step_avg:61.23ms
step:768/2330 train_time:47033ms step_avg:61.24ms
step:769/2330 train_time:47095ms step_avg:61.24ms
step:770/2330 train_time:47158ms step_avg:61.24ms
step:771/2330 train_time:47219ms step_avg:61.24ms
step:772/2330 train_time:47282ms step_avg:61.25ms
step:773/2330 train_time:47342ms step_avg:61.24ms
step:774/2330 train_time:47405ms step_avg:61.25ms
step:775/2330 train_time:47465ms step_avg:61.25ms
step:776/2330 train_time:47528ms step_avg:61.25ms
step:777/2330 train_time:47588ms step_avg:61.25ms
step:778/2330 train_time:47651ms step_avg:61.25ms
step:779/2330 train_time:47710ms step_avg:61.24ms
step:780/2330 train_time:47772ms step_avg:61.25ms
step:781/2330 train_time:47833ms step_avg:61.25ms
step:782/2330 train_time:47896ms step_avg:61.25ms
step:783/2330 train_time:47956ms step_avg:61.25ms
step:784/2330 train_time:48019ms step_avg:61.25ms
step:785/2330 train_time:48080ms step_avg:61.25ms
step:786/2330 train_time:48144ms step_avg:61.25ms
step:787/2330 train_time:48204ms step_avg:61.25ms
step:788/2330 train_time:48268ms step_avg:61.25ms
step:789/2330 train_time:48329ms step_avg:61.25ms
step:790/2330 train_time:48392ms step_avg:61.26ms
step:791/2330 train_time:48452ms step_avg:61.25ms
step:792/2330 train_time:48515ms step_avg:61.26ms
step:793/2330 train_time:48575ms step_avg:61.25ms
step:794/2330 train_time:48637ms step_avg:61.26ms
step:795/2330 train_time:48697ms step_avg:61.25ms
step:796/2330 train_time:48759ms step_avg:61.26ms
step:797/2330 train_time:48819ms step_avg:61.25ms
step:798/2330 train_time:48882ms step_avg:61.26ms
step:799/2330 train_time:48942ms step_avg:61.25ms
step:800/2330 train_time:49006ms step_avg:61.26ms
step:801/2330 train_time:49067ms step_avg:61.26ms
step:802/2330 train_time:49131ms step_avg:61.26ms
step:803/2330 train_time:49192ms step_avg:61.26ms
step:804/2330 train_time:49255ms step_avg:61.26ms
step:805/2330 train_time:49315ms step_avg:61.26ms
step:806/2330 train_time:49378ms step_avg:61.26ms
step:807/2330 train_time:49438ms step_avg:61.26ms
step:808/2330 train_time:49501ms step_avg:61.26ms
step:809/2330 train_time:49562ms step_avg:61.26ms
step:810/2330 train_time:49626ms step_avg:61.27ms
step:811/2330 train_time:49685ms step_avg:61.26ms
step:812/2330 train_time:49748ms step_avg:61.27ms
step:813/2330 train_time:49808ms step_avg:61.26ms
step:814/2330 train_time:49871ms step_avg:61.27ms
step:815/2330 train_time:49931ms step_avg:61.27ms
step:816/2330 train_time:49994ms step_avg:61.27ms
step:817/2330 train_time:50054ms step_avg:61.27ms
step:818/2330 train_time:50117ms step_avg:61.27ms
step:819/2330 train_time:50177ms step_avg:61.27ms
step:820/2330 train_time:50241ms step_avg:61.27ms
step:821/2330 train_time:50301ms step_avg:61.27ms
step:822/2330 train_time:50364ms step_avg:61.27ms
step:823/2330 train_time:50425ms step_avg:61.27ms
step:824/2330 train_time:50488ms step_avg:61.27ms
step:825/2330 train_time:50548ms step_avg:61.27ms
step:826/2330 train_time:50611ms step_avg:61.27ms
step:827/2330 train_time:50671ms step_avg:61.27ms
step:828/2330 train_time:50734ms step_avg:61.27ms
step:829/2330 train_time:50794ms step_avg:61.27ms
step:830/2330 train_time:50856ms step_avg:61.27ms
step:831/2330 train_time:50916ms step_avg:61.27ms
step:832/2330 train_time:50979ms step_avg:61.27ms
step:833/2330 train_time:51039ms step_avg:61.27ms
step:834/2330 train_time:51102ms step_avg:61.27ms
step:835/2330 train_time:51163ms step_avg:61.27ms
step:836/2330 train_time:51227ms step_avg:61.28ms
step:837/2330 train_time:51287ms step_avg:61.28ms
step:838/2330 train_time:51350ms step_avg:61.28ms
step:839/2330 train_time:51410ms step_avg:61.28ms
step:840/2330 train_time:51474ms step_avg:61.28ms
step:841/2330 train_time:51534ms step_avg:61.28ms
step:842/2330 train_time:51597ms step_avg:61.28ms
step:843/2330 train_time:51658ms step_avg:61.28ms
step:844/2330 train_time:51721ms step_avg:61.28ms
step:845/2330 train_time:51781ms step_avg:61.28ms
step:846/2330 train_time:51844ms step_avg:61.28ms
step:847/2330 train_time:51905ms step_avg:61.28ms
step:848/2330 train_time:51968ms step_avg:61.28ms
step:849/2330 train_time:52029ms step_avg:61.28ms
step:850/2330 train_time:52092ms step_avg:61.28ms
step:851/2330 train_time:52152ms step_avg:61.28ms
step:852/2330 train_time:52215ms step_avg:61.28ms
step:853/2330 train_time:52276ms step_avg:61.28ms
step:854/2330 train_time:52338ms step_avg:61.29ms
step:855/2330 train_time:52398ms step_avg:61.28ms
step:856/2330 train_time:52462ms step_avg:61.29ms
step:857/2330 train_time:52522ms step_avg:61.29ms
step:858/2330 train_time:52585ms step_avg:61.29ms
step:859/2330 train_time:52645ms step_avg:61.29ms
step:860/2330 train_time:52709ms step_avg:61.29ms
step:861/2330 train_time:52769ms step_avg:61.29ms
step:862/2330 train_time:52833ms step_avg:61.29ms
step:863/2330 train_time:52893ms step_avg:61.29ms
step:864/2330 train_time:52956ms step_avg:61.29ms
step:865/2330 train_time:53015ms step_avg:61.29ms
step:866/2330 train_time:53078ms step_avg:61.29ms
step:867/2330 train_time:53139ms step_avg:61.29ms
step:868/2330 train_time:53202ms step_avg:61.29ms
step:869/2330 train_time:53263ms step_avg:61.29ms
step:870/2330 train_time:53326ms step_avg:61.29ms
step:871/2330 train_time:53387ms step_avg:61.29ms
step:872/2330 train_time:53450ms step_avg:61.30ms
step:873/2330 train_time:53510ms step_avg:61.29ms
step:874/2330 train_time:53573ms step_avg:61.30ms
step:875/2330 train_time:53634ms step_avg:61.30ms
step:876/2330 train_time:53697ms step_avg:61.30ms
step:877/2330 train_time:53757ms step_avg:61.30ms
step:878/2330 train_time:53820ms step_avg:61.30ms
step:879/2330 train_time:53881ms step_avg:61.30ms
step:880/2330 train_time:53944ms step_avg:61.30ms
step:881/2330 train_time:54004ms step_avg:61.30ms
step:882/2330 train_time:54068ms step_avg:61.30ms
step:883/2330 train_time:54128ms step_avg:61.30ms
step:884/2330 train_time:54192ms step_avg:61.30ms
step:885/2330 train_time:54252ms step_avg:61.30ms
step:886/2330 train_time:54315ms step_avg:61.30ms
step:887/2330 train_time:54375ms step_avg:61.30ms
step:888/2330 train_time:54439ms step_avg:61.31ms
step:889/2330 train_time:54498ms step_avg:61.30ms
step:890/2330 train_time:54562ms step_avg:61.31ms
step:891/2330 train_time:54622ms step_avg:61.30ms
step:892/2330 train_time:54686ms step_avg:61.31ms
step:893/2330 train_time:54746ms step_avg:61.31ms
step:894/2330 train_time:54810ms step_avg:61.31ms
step:895/2330 train_time:54870ms step_avg:61.31ms
step:896/2330 train_time:54933ms step_avg:61.31ms
step:897/2330 train_time:54993ms step_avg:61.31ms
step:898/2330 train_time:55056ms step_avg:61.31ms
step:899/2330 train_time:55116ms step_avg:61.31ms
step:900/2330 train_time:55179ms step_avg:61.31ms
step:901/2330 train_time:55239ms step_avg:61.31ms
step:902/2330 train_time:55302ms step_avg:61.31ms
step:903/2330 train_time:55362ms step_avg:61.31ms
step:904/2330 train_time:55426ms step_avg:61.31ms
step:905/2330 train_time:55486ms step_avg:61.31ms
step:906/2330 train_time:55550ms step_avg:61.31ms
step:907/2330 train_time:55610ms step_avg:61.31ms
step:908/2330 train_time:55673ms step_avg:61.31ms
step:909/2330 train_time:55734ms step_avg:61.31ms
step:910/2330 train_time:55796ms step_avg:61.31ms
step:911/2330 train_time:55857ms step_avg:61.31ms
step:912/2330 train_time:55919ms step_avg:61.32ms
step:913/2330 train_time:55980ms step_avg:61.31ms
step:914/2330 train_time:56043ms step_avg:61.32ms
step:915/2330 train_time:56103ms step_avg:61.31ms
step:916/2330 train_time:56167ms step_avg:61.32ms
step:917/2330 train_time:56227ms step_avg:61.32ms
step:918/2330 train_time:56291ms step_avg:61.32ms
step:919/2330 train_time:56351ms step_avg:61.32ms
step:920/2330 train_time:56414ms step_avg:61.32ms
step:921/2330 train_time:56474ms step_avg:61.32ms
step:922/2330 train_time:56537ms step_avg:61.32ms
step:923/2330 train_time:56597ms step_avg:61.32ms
step:924/2330 train_time:56660ms step_avg:61.32ms
step:925/2330 train_time:56720ms step_avg:61.32ms
step:926/2330 train_time:56783ms step_avg:61.32ms
step:927/2330 train_time:56843ms step_avg:61.32ms
step:928/2330 train_time:56907ms step_avg:61.32ms
step:929/2330 train_time:56968ms step_avg:61.32ms
step:930/2330 train_time:57031ms step_avg:61.32ms
step:931/2330 train_time:57091ms step_avg:61.32ms
step:932/2330 train_time:57154ms step_avg:61.32ms
step:933/2330 train_time:57214ms step_avg:61.32ms
step:934/2330 train_time:57276ms step_avg:61.32ms
step:935/2330 train_time:57336ms step_avg:61.32ms
step:936/2330 train_time:57399ms step_avg:61.32ms
step:937/2330 train_time:57459ms step_avg:61.32ms
step:938/2330 train_time:57522ms step_avg:61.32ms
step:939/2330 train_time:57583ms step_avg:61.32ms
step:940/2330 train_time:57646ms step_avg:61.33ms
step:941/2330 train_time:57707ms step_avg:61.32ms
step:942/2330 train_time:57770ms step_avg:61.33ms
step:943/2330 train_time:57830ms step_avg:61.33ms
step:944/2330 train_time:57893ms step_avg:61.33ms
step:945/2330 train_time:57953ms step_avg:61.33ms
step:946/2330 train_time:58016ms step_avg:61.33ms
step:947/2330 train_time:58077ms step_avg:61.33ms
step:948/2330 train_time:58140ms step_avg:61.33ms
step:949/2330 train_time:58200ms step_avg:61.33ms
step:950/2330 train_time:58263ms step_avg:61.33ms
step:951/2330 train_time:58324ms step_avg:61.33ms
step:952/2330 train_time:58387ms step_avg:61.33ms
step:953/2330 train_time:58448ms step_avg:61.33ms
step:954/2330 train_time:58511ms step_avg:61.33ms
step:955/2330 train_time:58571ms step_avg:61.33ms
step:956/2330 train_time:58635ms step_avg:61.33ms
step:957/2330 train_time:58695ms step_avg:61.33ms
step:958/2330 train_time:58758ms step_avg:61.33ms
step:959/2330 train_time:58818ms step_avg:61.33ms
step:960/2330 train_time:58881ms step_avg:61.33ms
step:961/2330 train_time:58941ms step_avg:61.33ms
step:962/2330 train_time:59005ms step_avg:61.34ms
step:963/2330 train_time:59065ms step_avg:61.33ms
step:964/2330 train_time:59129ms step_avg:61.34ms
step:965/2330 train_time:59190ms step_avg:61.34ms
step:966/2330 train_time:59252ms step_avg:61.34ms
step:967/2330 train_time:59313ms step_avg:61.34ms
step:968/2330 train_time:59376ms step_avg:61.34ms
step:969/2330 train_time:59437ms step_avg:61.34ms
step:970/2330 train_time:59500ms step_avg:61.34ms
step:971/2330 train_time:59560ms step_avg:61.34ms
step:972/2330 train_time:59623ms step_avg:61.34ms
step:973/2330 train_time:59684ms step_avg:61.34ms
step:974/2330 train_time:59747ms step_avg:61.34ms
step:975/2330 train_time:59807ms step_avg:61.34ms
step:976/2330 train_time:59871ms step_avg:61.34ms
step:977/2330 train_time:59931ms step_avg:61.34ms
step:978/2330 train_time:59994ms step_avg:61.34ms
step:979/2330 train_time:60054ms step_avg:61.34ms
step:980/2330 train_time:60117ms step_avg:61.34ms
step:981/2330 train_time:60177ms step_avg:61.34ms
step:982/2330 train_time:60240ms step_avg:61.34ms
step:983/2330 train_time:60299ms step_avg:61.34ms
step:984/2330 train_time:60364ms step_avg:61.35ms
step:985/2330 train_time:60424ms step_avg:61.34ms
step:986/2330 train_time:60488ms step_avg:61.35ms
step:987/2330 train_time:60548ms step_avg:61.35ms
step:988/2330 train_time:60612ms step_avg:61.35ms
step:989/2330 train_time:60672ms step_avg:61.35ms
step:990/2330 train_time:60735ms step_avg:61.35ms
step:991/2330 train_time:60795ms step_avg:61.35ms
step:992/2330 train_time:60857ms step_avg:61.35ms
step:993/2330 train_time:60917ms step_avg:61.35ms
step:994/2330 train_time:60981ms step_avg:61.35ms
step:995/2330 train_time:61041ms step_avg:61.35ms
step:996/2330 train_time:61104ms step_avg:61.35ms
step:997/2330 train_time:61165ms step_avg:61.35ms
step:998/2330 train_time:61229ms step_avg:61.35ms
step:999/2330 train_time:61289ms step_avg:61.35ms
step:1000/2330 train_time:61353ms step_avg:61.35ms
step:1000/2330 val_loss:3.5936 train_time:61417ms step_avg:61.42ms
step:1001/2330 train_time:61441ms step_avg:61.38ms
step:1002/2330 train_time:61477ms step_avg:61.35ms
step:1003/2330 train_time:61540ms step_avg:61.36ms
step:1004/2330 train_time:61608ms step_avg:61.36ms
step:1005/2330 train_time:61669ms step_avg:61.36ms
step:1006/2330 train_time:61731ms step_avg:61.36ms
step:1007/2330 train_time:61791ms step_avg:61.36ms
step:1008/2330 train_time:61853ms step_avg:61.36ms
step:1009/2330 train_time:61913ms step_avg:61.36ms
step:1010/2330 train_time:61975ms step_avg:61.36ms
step:1011/2330 train_time:62034ms step_avg:61.36ms
step:1012/2330 train_time:62097ms step_avg:61.36ms
step:1013/2330 train_time:62156ms step_avg:61.36ms
step:1014/2330 train_time:62218ms step_avg:61.36ms
step:1015/2330 train_time:62278ms step_avg:61.36ms
step:1016/2330 train_time:62345ms step_avg:61.36ms
step:1017/2330 train_time:62409ms step_avg:61.37ms
step:1018/2330 train_time:62473ms step_avg:61.37ms
step:1019/2330 train_time:62535ms step_avg:61.37ms
step:1020/2330 train_time:62599ms step_avg:61.37ms
step:1021/2330 train_time:62660ms step_avg:61.37ms
step:1022/2330 train_time:62723ms step_avg:61.37ms
step:1023/2330 train_time:62783ms step_avg:61.37ms
step:1024/2330 train_time:62846ms step_avg:61.37ms
step:1025/2330 train_time:62905ms step_avg:61.37ms
step:1026/2330 train_time:62968ms step_avg:61.37ms
step:1027/2330 train_time:63028ms step_avg:61.37ms
step:1028/2330 train_time:63090ms step_avg:61.37ms
step:1029/2330 train_time:63149ms step_avg:61.37ms
step:1030/2330 train_time:63211ms step_avg:61.37ms
step:1031/2330 train_time:63272ms step_avg:61.37ms
step:1032/2330 train_time:63336ms step_avg:61.37ms
step:1033/2330 train_time:63398ms step_avg:61.37ms
step:1034/2330 train_time:63462ms step_avg:61.38ms
step:1035/2330 train_time:63524ms step_avg:61.38ms
step:1036/2330 train_time:63587ms step_avg:61.38ms
step:1037/2330 train_time:63647ms step_avg:61.38ms
step:1038/2330 train_time:63710ms step_avg:61.38ms
step:1039/2330 train_time:63770ms step_avg:61.38ms
step:1040/2330 train_time:63832ms step_avg:61.38ms
step:1041/2330 train_time:63893ms step_avg:61.38ms
step:1042/2330 train_time:63955ms step_avg:61.38ms
step:1043/2330 train_time:64016ms step_avg:61.38ms
step:1044/2330 train_time:64079ms step_avg:61.38ms
step:1045/2330 train_time:64139ms step_avg:61.38ms
step:1046/2330 train_time:64202ms step_avg:61.38ms
step:1047/2330 train_time:64262ms step_avg:61.38ms
step:1048/2330 train_time:64325ms step_avg:61.38ms
step:1049/2330 train_time:64385ms step_avg:61.38ms
step:1050/2330 train_time:64448ms step_avg:61.38ms
step:1051/2330 train_time:64509ms step_avg:61.38ms
step:1052/2330 train_time:64573ms step_avg:61.38ms
step:1053/2330 train_time:64635ms step_avg:61.38ms
step:1054/2330 train_time:64699ms step_avg:61.38ms
step:1055/2330 train_time:64759ms step_avg:61.38ms
step:1056/2330 train_time:64822ms step_avg:61.38ms
step:1057/2330 train_time:64882ms step_avg:61.38ms
step:1058/2330 train_time:64945ms step_avg:61.38ms
step:1059/2330 train_time:65005ms step_avg:61.38ms
step:1060/2330 train_time:65068ms step_avg:61.38ms
step:1061/2330 train_time:65128ms step_avg:61.38ms
step:1062/2330 train_time:65190ms step_avg:61.38ms
step:1063/2330 train_time:65251ms step_avg:61.38ms
step:1064/2330 train_time:65314ms step_avg:61.39ms
step:1065/2330 train_time:65375ms step_avg:61.38ms
step:1066/2330 train_time:65439ms step_avg:61.39ms
step:1067/2330 train_time:65500ms step_avg:61.39ms
step:1068/2330 train_time:65564ms step_avg:61.39ms
step:1069/2330 train_time:65624ms step_avg:61.39ms
step:1070/2330 train_time:65687ms step_avg:61.39ms
step:1071/2330 train_time:65747ms step_avg:61.39ms
step:1072/2330 train_time:65810ms step_avg:61.39ms
step:1073/2330 train_time:65870ms step_avg:61.39ms
step:1074/2330 train_time:65934ms step_avg:61.39ms
step:1075/2330 train_time:65994ms step_avg:61.39ms
step:1076/2330 train_time:66057ms step_avg:61.39ms
step:1077/2330 train_time:66117ms step_avg:61.39ms
step:1078/2330 train_time:66180ms step_avg:61.39ms
step:1079/2330 train_time:66240ms step_avg:61.39ms
step:1080/2330 train_time:66303ms step_avg:61.39ms
step:1081/2330 train_time:66363ms step_avg:61.39ms
step:1082/2330 train_time:66426ms step_avg:61.39ms
step:1083/2330 train_time:66487ms step_avg:61.39ms
step:1084/2330 train_time:66551ms step_avg:61.39ms
step:1085/2330 train_time:66611ms step_avg:61.39ms
step:1086/2330 train_time:66675ms step_avg:61.39ms
step:1087/2330 train_time:66736ms step_avg:61.39ms
step:1088/2330 train_time:66799ms step_avg:61.40ms
step:1089/2330 train_time:66859ms step_avg:61.40ms
step:1090/2330 train_time:66923ms step_avg:61.40ms
step:1091/2330 train_time:66983ms step_avg:61.40ms
step:1092/2330 train_time:67047ms step_avg:61.40ms
step:1093/2330 train_time:67107ms step_avg:61.40ms
step:1094/2330 train_time:67170ms step_avg:61.40ms
step:1095/2330 train_time:67229ms step_avg:61.40ms
step:1096/2330 train_time:67292ms step_avg:61.40ms
step:1097/2330 train_time:67352ms step_avg:61.40ms
step:1098/2330 train_time:67416ms step_avg:61.40ms
step:1099/2330 train_time:67477ms step_avg:61.40ms
step:1100/2330 train_time:67542ms step_avg:61.40ms
step:1101/2330 train_time:67602ms step_avg:61.40ms
step:1102/2330 train_time:67665ms step_avg:61.40ms
step:1103/2330 train_time:67725ms step_avg:61.40ms
step:1104/2330 train_time:67788ms step_avg:61.40ms
step:1105/2330 train_time:67848ms step_avg:61.40ms
step:1106/2330 train_time:67911ms step_avg:61.40ms
step:1107/2330 train_time:67971ms step_avg:61.40ms
step:1108/2330 train_time:68034ms step_avg:61.40ms
step:1109/2330 train_time:68094ms step_avg:61.40ms
step:1110/2330 train_time:68157ms step_avg:61.40ms
step:1111/2330 train_time:68218ms step_avg:61.40ms
step:1112/2330 train_time:68281ms step_avg:61.40ms
step:1113/2330 train_time:68342ms step_avg:61.40ms
step:1114/2330 train_time:68405ms step_avg:61.40ms
step:1115/2330 train_time:68465ms step_avg:61.40ms
step:1116/2330 train_time:68528ms step_avg:61.40ms
step:1117/2330 train_time:68588ms step_avg:61.40ms
step:1118/2330 train_time:68652ms step_avg:61.41ms
step:1119/2330 train_time:68712ms step_avg:61.41ms
step:1120/2330 train_time:68776ms step_avg:61.41ms
step:1121/2330 train_time:68837ms step_avg:61.41ms
step:1122/2330 train_time:68901ms step_avg:61.41ms
step:1123/2330 train_time:68961ms step_avg:61.41ms
step:1124/2330 train_time:69024ms step_avg:61.41ms
step:1125/2330 train_time:69084ms step_avg:61.41ms
step:1126/2330 train_time:69147ms step_avg:61.41ms
step:1127/2330 train_time:69207ms step_avg:61.41ms
step:1128/2330 train_time:69270ms step_avg:61.41ms
step:1129/2330 train_time:69330ms step_avg:61.41ms
step:1130/2330 train_time:69394ms step_avg:61.41ms
step:1131/2330 train_time:69455ms step_avg:61.41ms
step:1132/2330 train_time:69518ms step_avg:61.41ms
step:1133/2330 train_time:69579ms step_avg:61.41ms
step:1134/2330 train_time:69642ms step_avg:61.41ms
step:1135/2330 train_time:69703ms step_avg:61.41ms
step:1136/2330 train_time:69766ms step_avg:61.41ms
step:1137/2330 train_time:69825ms step_avg:61.41ms
step:1138/2330 train_time:69888ms step_avg:61.41ms
step:1139/2330 train_time:69948ms step_avg:61.41ms
step:1140/2330 train_time:70011ms step_avg:61.41ms
step:1141/2330 train_time:70072ms step_avg:61.41ms
step:1142/2330 train_time:70135ms step_avg:61.41ms
step:1143/2330 train_time:70195ms step_avg:61.41ms
step:1144/2330 train_time:70258ms step_avg:61.41ms
step:1145/2330 train_time:70318ms step_avg:61.41ms
step:1146/2330 train_time:70382ms step_avg:61.42ms
step:1147/2330 train_time:70442ms step_avg:61.41ms
step:1148/2330 train_time:70506ms step_avg:61.42ms
step:1149/2330 train_time:70566ms step_avg:61.42ms
step:1150/2330 train_time:70629ms step_avg:61.42ms
step:1151/2330 train_time:70689ms step_avg:61.42ms
step:1152/2330 train_time:70753ms step_avg:61.42ms
step:1153/2330 train_time:70813ms step_avg:61.42ms
step:1154/2330 train_time:70877ms step_avg:61.42ms
step:1155/2330 train_time:70937ms step_avg:61.42ms
step:1156/2330 train_time:71001ms step_avg:61.42ms
step:1157/2330 train_time:71061ms step_avg:61.42ms
step:1158/2330 train_time:71124ms step_avg:61.42ms
step:1159/2330 train_time:71184ms step_avg:61.42ms
step:1160/2330 train_time:71247ms step_avg:61.42ms
step:1161/2330 train_time:71307ms step_avg:61.42ms
step:1162/2330 train_time:71370ms step_avg:61.42ms
step:1163/2330 train_time:71430ms step_avg:61.42ms
step:1164/2330 train_time:71494ms step_avg:61.42ms
step:1165/2330 train_time:71554ms step_avg:61.42ms
step:1166/2330 train_time:71618ms step_avg:61.42ms
step:1167/2330 train_time:71679ms step_avg:61.42ms
step:1168/2330 train_time:71742ms step_avg:61.42ms
step:1169/2330 train_time:71802ms step_avg:61.42ms
step:1170/2330 train_time:71865ms step_avg:61.42ms
step:1171/2330 train_time:71925ms step_avg:61.42ms
step:1172/2330 train_time:71988ms step_avg:61.42ms
step:1173/2330 train_time:72048ms step_avg:61.42ms
step:1174/2330 train_time:72111ms step_avg:61.42ms
step:1175/2330 train_time:72171ms step_avg:61.42ms
step:1176/2330 train_time:72234ms step_avg:61.42ms
step:1177/2330 train_time:72294ms step_avg:61.42ms
step:1178/2330 train_time:72358ms step_avg:61.42ms
step:1179/2330 train_time:72418ms step_avg:61.42ms
step:1180/2330 train_time:72482ms step_avg:61.43ms
step:1181/2330 train_time:72542ms step_avg:61.42ms
step:1182/2330 train_time:72605ms step_avg:61.43ms
step:1183/2330 train_time:72665ms step_avg:61.42ms
step:1184/2330 train_time:72728ms step_avg:61.43ms
step:1185/2330 train_time:72788ms step_avg:61.42ms
step:1186/2330 train_time:72851ms step_avg:61.43ms
step:1187/2330 train_time:72911ms step_avg:61.42ms
step:1188/2330 train_time:72975ms step_avg:61.43ms
step:1189/2330 train_time:73035ms step_avg:61.43ms
step:1190/2330 train_time:73099ms step_avg:61.43ms
step:1191/2330 train_time:73159ms step_avg:61.43ms
step:1192/2330 train_time:73222ms step_avg:61.43ms
step:1193/2330 train_time:73282ms step_avg:61.43ms
step:1194/2330 train_time:73345ms step_avg:61.43ms
step:1195/2330 train_time:73405ms step_avg:61.43ms
step:1196/2330 train_time:73467ms step_avg:61.43ms
step:1197/2330 train_time:73527ms step_avg:61.43ms
step:1198/2330 train_time:73591ms step_avg:61.43ms
step:1199/2330 train_time:73651ms step_avg:61.43ms
step:1200/2330 train_time:73714ms step_avg:61.43ms
step:1201/2330 train_time:73775ms step_avg:61.43ms
step:1202/2330 train_time:73838ms step_avg:61.43ms
step:1203/2330 train_time:73899ms step_avg:61.43ms
step:1204/2330 train_time:73962ms step_avg:61.43ms
step:1205/2330 train_time:74022ms step_avg:61.43ms
step:1206/2330 train_time:74085ms step_avg:61.43ms
step:1207/2330 train_time:74145ms step_avg:61.43ms
step:1208/2330 train_time:74208ms step_avg:61.43ms
step:1209/2330 train_time:74268ms step_avg:61.43ms
step:1210/2330 train_time:74331ms step_avg:61.43ms
step:1211/2330 train_time:74391ms step_avg:61.43ms
step:1212/2330 train_time:74454ms step_avg:61.43ms
step:1213/2330 train_time:74515ms step_avg:61.43ms
step:1214/2330 train_time:74579ms step_avg:61.43ms
step:1215/2330 train_time:74639ms step_avg:61.43ms
step:1216/2330 train_time:74702ms step_avg:61.43ms
step:1217/2330 train_time:74762ms step_avg:61.43ms
step:1218/2330 train_time:74825ms step_avg:61.43ms
step:1219/2330 train_time:74885ms step_avg:61.43ms
step:1220/2330 train_time:74949ms step_avg:61.43ms
step:1221/2330 train_time:75009ms step_avg:61.43ms
step:1222/2330 train_time:75071ms step_avg:61.43ms
step:1223/2330 train_time:75131ms step_avg:61.43ms
step:1224/2330 train_time:75195ms step_avg:61.43ms
step:1225/2330 train_time:75255ms step_avg:61.43ms
step:1226/2330 train_time:75319ms step_avg:61.43ms
step:1227/2330 train_time:75379ms step_avg:61.43ms
step:1228/2330 train_time:75442ms step_avg:61.44ms
step:1229/2330 train_time:75502ms step_avg:61.43ms
step:1230/2330 train_time:75565ms step_avg:61.44ms
step:1231/2330 train_time:75625ms step_avg:61.43ms
step:1232/2330 train_time:75688ms step_avg:61.44ms
step:1233/2330 train_time:75749ms step_avg:61.43ms
step:1234/2330 train_time:75812ms step_avg:61.44ms
step:1235/2330 train_time:75872ms step_avg:61.43ms
step:1236/2330 train_time:75936ms step_avg:61.44ms
step:1237/2330 train_time:75996ms step_avg:61.44ms
step:1238/2330 train_time:76060ms step_avg:61.44ms
step:1239/2330 train_time:76120ms step_avg:61.44ms
step:1240/2330 train_time:76183ms step_avg:61.44ms
step:1241/2330 train_time:76243ms step_avg:61.44ms
step:1242/2330 train_time:76307ms step_avg:61.44ms
step:1243/2330 train_time:76367ms step_avg:61.44ms
step:1244/2330 train_time:76430ms step_avg:61.44ms
step:1245/2330 train_time:76490ms step_avg:61.44ms
step:1246/2330 train_time:76553ms step_avg:61.44ms
step:1247/2330 train_time:76613ms step_avg:61.44ms
step:1248/2330 train_time:76677ms step_avg:61.44ms
step:1249/2330 train_time:76737ms step_avg:61.44ms
step:1250/2330 train_time:76802ms step_avg:61.44ms
step:1250/2330 val_loss:3.5348 train_time:76866ms step_avg:61.49ms
step:1251/2330 train_time:76889ms step_avg:61.46ms
step:1252/2330 train_time:76927ms step_avg:61.44ms
step:1253/2330 train_time:76992ms step_avg:61.45ms
step:1254/2330 train_time:77059ms step_avg:61.45ms
step:1255/2330 train_time:77120ms step_avg:61.45ms
step:1256/2330 train_time:77184ms step_avg:61.45ms
step:1257/2330 train_time:77243ms step_avg:61.45ms
step:1258/2330 train_time:77306ms step_avg:61.45ms
step:1259/2330 train_time:77365ms step_avg:61.45ms
step:1260/2330 train_time:77427ms step_avg:61.45ms
step:1261/2330 train_time:77486ms step_avg:61.45ms
step:1262/2330 train_time:77549ms step_avg:61.45ms
step:1263/2330 train_time:77608ms step_avg:61.45ms
step:1264/2330 train_time:77670ms step_avg:61.45ms
step:1265/2330 train_time:77730ms step_avg:61.45ms
step:1266/2330 train_time:77795ms step_avg:61.45ms
step:1267/2330 train_time:77856ms step_avg:61.45ms
step:1268/2330 train_time:77920ms step_avg:61.45ms
step:1269/2330 train_time:77982ms step_avg:61.45ms
step:1270/2330 train_time:78046ms step_avg:61.45ms
step:1271/2330 train_time:78107ms step_avg:61.45ms
step:1272/2330 train_time:78170ms step_avg:61.45ms
step:1273/2330 train_time:78230ms step_avg:61.45ms
step:1274/2330 train_time:78293ms step_avg:61.45ms
step:1275/2330 train_time:78352ms step_avg:61.45ms
step:1276/2330 train_time:78415ms step_avg:61.45ms
step:1277/2330 train_time:78475ms step_avg:61.45ms
step:1278/2330 train_time:78537ms step_avg:61.45ms
step:1279/2330 train_time:78598ms step_avg:61.45ms
step:1280/2330 train_time:78661ms step_avg:61.45ms
step:1281/2330 train_time:78722ms step_avg:61.45ms
step:1282/2330 train_time:78785ms step_avg:61.46ms
step:1283/2330 train_time:78846ms step_avg:61.45ms
step:1284/2330 train_time:78910ms step_avg:61.46ms
step:1285/2330 train_time:78971ms step_avg:61.46ms
step:1286/2330 train_time:79034ms step_avg:61.46ms
step:1287/2330 train_time:79094ms step_avg:61.46ms
step:1288/2330 train_time:79159ms step_avg:61.46ms
step:1289/2330 train_time:79219ms step_avg:61.46ms
step:1290/2330 train_time:79283ms step_avg:61.46ms
step:1291/2330 train_time:79343ms step_avg:61.46ms
step:1292/2330 train_time:79406ms step_avg:61.46ms
step:1293/2330 train_time:79465ms step_avg:61.46ms
step:1294/2330 train_time:79528ms step_avg:61.46ms
step:1295/2330 train_time:79587ms step_avg:61.46ms
step:1296/2330 train_time:79651ms step_avg:61.46ms
step:1297/2330 train_time:79711ms step_avg:61.46ms
step:1298/2330 train_time:79773ms step_avg:61.46ms
step:1299/2330 train_time:79834ms step_avg:61.46ms
step:1300/2330 train_time:79897ms step_avg:61.46ms
step:1301/2330 train_time:79958ms step_avg:61.46ms
step:1302/2330 train_time:80022ms step_avg:61.46ms
step:1303/2330 train_time:80082ms step_avg:61.46ms
step:1304/2330 train_time:80146ms step_avg:61.46ms
step:1305/2330 train_time:80207ms step_avg:61.46ms
step:1306/2330 train_time:80270ms step_avg:61.46ms
step:1307/2330 train_time:80329ms step_avg:61.46ms
step:1308/2330 train_time:80392ms step_avg:61.46ms
step:1309/2330 train_time:80452ms step_avg:61.46ms
step:1310/2330 train_time:80515ms step_avg:61.46ms
step:1311/2330 train_time:80574ms step_avg:61.46ms
step:1312/2330 train_time:80638ms step_avg:61.46ms
step:1313/2330 train_time:80698ms step_avg:61.46ms
step:1314/2330 train_time:80761ms step_avg:61.46ms
step:1315/2330 train_time:80820ms step_avg:61.46ms
step:1316/2330 train_time:80884ms step_avg:61.46ms
step:1317/2330 train_time:80943ms step_avg:61.46ms
step:1318/2330 train_time:81006ms step_avg:61.46ms
step:1319/2330 train_time:81066ms step_avg:61.46ms
step:1320/2330 train_time:81129ms step_avg:61.46ms
step:1321/2330 train_time:81190ms step_avg:61.46ms
step:1322/2330 train_time:81253ms step_avg:61.46ms
step:1323/2330 train_time:81313ms step_avg:61.46ms
step:1324/2330 train_time:81375ms step_avg:61.46ms
step:1325/2330 train_time:81436ms step_avg:61.46ms
step:1326/2330 train_time:81499ms step_avg:61.46ms
step:1327/2330 train_time:81559ms step_avg:61.46ms
step:1328/2330 train_time:81623ms step_avg:61.46ms
step:1329/2330 train_time:81683ms step_avg:61.46ms
step:1330/2330 train_time:81746ms step_avg:61.46ms
step:1331/2330 train_time:81806ms step_avg:61.46ms
step:1332/2330 train_time:81868ms step_avg:61.46ms
step:1333/2330 train_time:81928ms step_avg:61.46ms
step:1334/2330 train_time:81991ms step_avg:61.46ms
step:1335/2330 train_time:82051ms step_avg:61.46ms
step:1336/2330 train_time:82115ms step_avg:61.46ms
step:1337/2330 train_time:82176ms step_avg:61.46ms
step:1338/2330 train_time:82240ms step_avg:61.46ms
step:1339/2330 train_time:82300ms step_avg:61.46ms
step:1340/2330 train_time:82363ms step_avg:61.47ms
step:1341/2330 train_time:82423ms step_avg:61.46ms
step:1342/2330 train_time:82486ms step_avg:61.46ms
step:1343/2330 train_time:82546ms step_avg:61.46ms
step:1344/2330 train_time:82609ms step_avg:61.46ms
step:1345/2330 train_time:82668ms step_avg:61.46ms
step:1346/2330 train_time:82731ms step_avg:61.46ms
step:1347/2330 train_time:82791ms step_avg:61.46ms
step:1348/2330 train_time:82853ms step_avg:61.46ms
step:1349/2330 train_time:82913ms step_avg:61.46ms
step:1350/2330 train_time:82976ms step_avg:61.46ms
step:1351/2330 train_time:83036ms step_avg:61.46ms
step:1352/2330 train_time:83100ms step_avg:61.46ms
step:1353/2330 train_time:83160ms step_avg:61.46ms
step:1354/2330 train_time:83223ms step_avg:61.46ms
step:1355/2330 train_time:83283ms step_avg:61.46ms
step:1356/2330 train_time:83347ms step_avg:61.47ms
step:1357/2330 train_time:83407ms step_avg:61.46ms
step:1358/2330 train_time:83470ms step_avg:61.47ms
step:1359/2330 train_time:83530ms step_avg:61.46ms
step:1360/2330 train_time:83593ms step_avg:61.47ms
step:1361/2330 train_time:83653ms step_avg:61.46ms
step:1362/2330 train_time:83716ms step_avg:61.47ms
step:1363/2330 train_time:83776ms step_avg:61.46ms
step:1364/2330 train_time:83839ms step_avg:61.47ms
step:1365/2330 train_time:83900ms step_avg:61.47ms
step:1366/2330 train_time:83964ms step_avg:61.47ms
step:1367/2330 train_time:84024ms step_avg:61.47ms
step:1368/2330 train_time:84087ms step_avg:61.47ms
step:1369/2330 train_time:84147ms step_avg:61.47ms
step:1370/2330 train_time:84210ms step_avg:61.47ms
step:1371/2330 train_time:84270ms step_avg:61.47ms
step:1372/2330 train_time:84333ms step_avg:61.47ms
step:1373/2330 train_time:84393ms step_avg:61.47ms
step:1374/2330 train_time:84456ms step_avg:61.47ms
step:1375/2330 train_time:84516ms step_avg:61.47ms
step:1376/2330 train_time:84580ms step_avg:61.47ms
step:1377/2330 train_time:84640ms step_avg:61.47ms
step:1378/2330 train_time:84703ms step_avg:61.47ms
step:1379/2330 train_time:84763ms step_avg:61.47ms
step:1380/2330 train_time:84825ms step_avg:61.47ms
step:1381/2330 train_time:84886ms step_avg:61.47ms
step:1382/2330 train_time:84949ms step_avg:61.47ms
step:1383/2330 train_time:85009ms step_avg:61.47ms
step:1384/2330 train_time:85072ms step_avg:61.47ms
step:1385/2330 train_time:85132ms step_avg:61.47ms
step:1386/2330 train_time:85195ms step_avg:61.47ms
step:1387/2330 train_time:85256ms step_avg:61.47ms
step:1388/2330 train_time:85320ms step_avg:61.47ms
step:1389/2330 train_time:85381ms step_avg:61.47ms
step:1390/2330 train_time:85444ms step_avg:61.47ms
step:1391/2330 train_time:85505ms step_avg:61.47ms
step:1392/2330 train_time:85567ms step_avg:61.47ms
step:1393/2330 train_time:85628ms step_avg:61.47ms
step:1394/2330 train_time:85690ms step_avg:61.47ms
step:1395/2330 train_time:85750ms step_avg:61.47ms
step:1396/2330 train_time:85812ms step_avg:61.47ms
step:1397/2330 train_time:85872ms step_avg:61.47ms
step:1398/2330 train_time:85935ms step_avg:61.47ms
step:1399/2330 train_time:85996ms step_avg:61.47ms
step:1400/2330 train_time:86059ms step_avg:61.47ms
step:1401/2330 train_time:86120ms step_avg:61.47ms
step:1402/2330 train_time:86184ms step_avg:61.47ms
step:1403/2330 train_time:86244ms step_avg:61.47ms
step:1404/2330 train_time:86307ms step_avg:61.47ms
step:1405/2330 train_time:86367ms step_avg:61.47ms
step:1406/2330 train_time:86430ms step_avg:61.47ms
step:1407/2330 train_time:86490ms step_avg:61.47ms
step:1408/2330 train_time:86553ms step_avg:61.47ms
step:1409/2330 train_time:86613ms step_avg:61.47ms
step:1410/2330 train_time:86676ms step_avg:61.47ms
step:1411/2330 train_time:86735ms step_avg:61.47ms
step:1412/2330 train_time:86799ms step_avg:61.47ms
step:1413/2330 train_time:86859ms step_avg:61.47ms
step:1414/2330 train_time:86922ms step_avg:61.47ms
step:1415/2330 train_time:86982ms step_avg:61.47ms
step:1416/2330 train_time:87046ms step_avg:61.47ms
step:1417/2330 train_time:87106ms step_avg:61.47ms
step:1418/2330 train_time:87169ms step_avg:61.47ms
step:1419/2330 train_time:87228ms step_avg:61.47ms
step:1420/2330 train_time:87292ms step_avg:61.47ms
step:1421/2330 train_time:87352ms step_avg:61.47ms
step:1422/2330 train_time:87415ms step_avg:61.47ms
step:1423/2330 train_time:87475ms step_avg:61.47ms
step:1424/2330 train_time:87538ms step_avg:61.47ms
step:1425/2330 train_time:87598ms step_avg:61.47ms
step:1426/2330 train_time:87662ms step_avg:61.47ms
step:1427/2330 train_time:87722ms step_avg:61.47ms
step:1428/2330 train_time:87785ms step_avg:61.47ms
step:1429/2330 train_time:87845ms step_avg:61.47ms
step:1430/2330 train_time:87908ms step_avg:61.47ms
step:1431/2330 train_time:87968ms step_avg:61.47ms
step:1432/2330 train_time:88031ms step_avg:61.47ms
step:1433/2330 train_time:88090ms step_avg:61.47ms
step:1434/2330 train_time:88154ms step_avg:61.47ms
step:1435/2330 train_time:88214ms step_avg:61.47ms
step:1436/2330 train_time:88277ms step_avg:61.47ms
step:1437/2330 train_time:88337ms step_avg:61.47ms
step:1438/2330 train_time:88401ms step_avg:61.47ms
step:1439/2330 train_time:88461ms step_avg:61.47ms
step:1440/2330 train_time:88524ms step_avg:61.48ms
step:1441/2330 train_time:88584ms step_avg:61.47ms
step:1442/2330 train_time:88648ms step_avg:61.48ms
step:1443/2330 train_time:88707ms step_avg:61.47ms
step:1444/2330 train_time:88770ms step_avg:61.47ms
step:1445/2330 train_time:88829ms step_avg:61.47ms
step:1446/2330 train_time:88892ms step_avg:61.47ms
step:1447/2330 train_time:88952ms step_avg:61.47ms
step:1448/2330 train_time:89016ms step_avg:61.47ms
step:1449/2330 train_time:89075ms step_avg:61.47ms
step:1450/2330 train_time:89139ms step_avg:61.47ms
step:1451/2330 train_time:89199ms step_avg:61.47ms
step:1452/2330 train_time:89263ms step_avg:61.48ms
step:1453/2330 train_time:89323ms step_avg:61.47ms
step:1454/2330 train_time:89386ms step_avg:61.48ms
step:1455/2330 train_time:89446ms step_avg:61.48ms
step:1456/2330 train_time:89509ms step_avg:61.48ms
step:1457/2330 train_time:89570ms step_avg:61.48ms
step:1458/2330 train_time:89632ms step_avg:61.48ms
step:1459/2330 train_time:89693ms step_avg:61.48ms
step:1460/2330 train_time:89756ms step_avg:61.48ms
step:1461/2330 train_time:89817ms step_avg:61.48ms
step:1462/2330 train_time:89881ms step_avg:61.48ms
step:1463/2330 train_time:89942ms step_avg:61.48ms
step:1464/2330 train_time:90005ms step_avg:61.48ms
step:1465/2330 train_time:90065ms step_avg:61.48ms
step:1466/2330 train_time:90128ms step_avg:61.48ms
step:1467/2330 train_time:90187ms step_avg:61.48ms
step:1468/2330 train_time:90251ms step_avg:61.48ms
step:1469/2330 train_time:90311ms step_avg:61.48ms
step:1470/2330 train_time:90374ms step_avg:61.48ms
step:1471/2330 train_time:90435ms step_avg:61.48ms
step:1472/2330 train_time:90498ms step_avg:61.48ms
step:1473/2330 train_time:90558ms step_avg:61.48ms
step:1474/2330 train_time:90621ms step_avg:61.48ms
step:1475/2330 train_time:90681ms step_avg:61.48ms
step:1476/2330 train_time:90745ms step_avg:61.48ms
step:1477/2330 train_time:90805ms step_avg:61.48ms
step:1478/2330 train_time:90868ms step_avg:61.48ms
step:1479/2330 train_time:90928ms step_avg:61.48ms
step:1480/2330 train_time:90990ms step_avg:61.48ms
step:1481/2330 train_time:91051ms step_avg:61.48ms
step:1482/2330 train_time:91113ms step_avg:61.48ms
step:1483/2330 train_time:91173ms step_avg:61.48ms
step:1484/2330 train_time:91236ms step_avg:61.48ms
step:1485/2330 train_time:91297ms step_avg:61.48ms
step:1486/2330 train_time:91360ms step_avg:61.48ms
step:1487/2330 train_time:91420ms step_avg:61.48ms
step:1488/2330 train_time:91483ms step_avg:61.48ms
step:1489/2330 train_time:91543ms step_avg:61.48ms
step:1490/2330 train_time:91606ms step_avg:61.48ms
step:1491/2330 train_time:91667ms step_avg:61.48ms
step:1492/2330 train_time:91730ms step_avg:61.48ms
step:1493/2330 train_time:91790ms step_avg:61.48ms
step:1494/2330 train_time:91853ms step_avg:61.48ms
step:1495/2330 train_time:91913ms step_avg:61.48ms
step:1496/2330 train_time:91976ms step_avg:61.48ms
step:1497/2330 train_time:92037ms step_avg:61.48ms
step:1498/2330 train_time:92100ms step_avg:61.48ms
step:1499/2330 train_time:92161ms step_avg:61.48ms
step:1500/2330 train_time:92224ms step_avg:61.48ms
step:1500/2330 val_loss:3.4918 train_time:92288ms step_avg:61.53ms
step:1501/2330 train_time:92311ms step_avg:61.50ms
step:1502/2330 train_time:92349ms step_avg:61.48ms
step:1503/2330 train_time:92414ms step_avg:61.49ms
step:1504/2330 train_time:92480ms step_avg:61.49ms
step:1505/2330 train_time:92541ms step_avg:61.49ms
step:1506/2330 train_time:92604ms step_avg:61.49ms
step:1507/2330 train_time:92663ms step_avg:61.49ms
step:1508/2330 train_time:92726ms step_avg:61.49ms
step:1509/2330 train_time:92786ms step_avg:61.49ms
step:1510/2330 train_time:92848ms step_avg:61.49ms
step:1511/2330 train_time:92907ms step_avg:61.49ms
step:1512/2330 train_time:92970ms step_avg:61.49ms
step:1513/2330 train_time:93029ms step_avg:61.49ms
step:1514/2330 train_time:93091ms step_avg:61.49ms
step:1515/2330 train_time:93150ms step_avg:61.48ms
step:1516/2330 train_time:93212ms step_avg:61.49ms
step:1517/2330 train_time:93273ms step_avg:61.49ms
step:1518/2330 train_time:93338ms step_avg:61.49ms
step:1519/2330 train_time:93399ms step_avg:61.49ms
step:1520/2330 train_time:93464ms step_avg:61.49ms
step:1521/2330 train_time:93525ms step_avg:61.49ms
step:1522/2330 train_time:93588ms step_avg:61.49ms
step:1523/2330 train_time:93648ms step_avg:61.49ms
step:1524/2330 train_time:93710ms step_avg:61.49ms
step:1525/2330 train_time:93770ms step_avg:61.49ms
step:1526/2330 train_time:93833ms step_avg:61.49ms
step:1527/2330 train_time:93892ms step_avg:61.49ms
step:1528/2330 train_time:93954ms step_avg:61.49ms
step:1529/2330 train_time:94014ms step_avg:61.49ms
step:1530/2330 train_time:94077ms step_avg:61.49ms
step:1531/2330 train_time:94137ms step_avg:61.49ms
step:1532/2330 train_time:94200ms step_avg:61.49ms
step:1533/2330 train_time:94261ms step_avg:61.49ms
step:1534/2330 train_time:94325ms step_avg:61.49ms
step:1535/2330 train_time:94387ms step_avg:61.49ms
step:1536/2330 train_time:94451ms step_avg:61.49ms
step:1537/2330 train_time:94513ms step_avg:61.49ms
step:1538/2330 train_time:94577ms step_avg:61.49ms
step:1539/2330 train_time:94638ms step_avg:61.49ms
step:1540/2330 train_time:94701ms step_avg:61.49ms
step:1541/2330 train_time:94761ms step_avg:61.49ms
step:1542/2330 train_time:94826ms step_avg:61.50ms
step:1543/2330 train_time:94886ms step_avg:61.49ms
step:1544/2330 train_time:94949ms step_avg:61.50ms
step:1545/2330 train_time:95008ms step_avg:61.49ms
step:1546/2330 train_time:95071ms step_avg:61.50ms
step:1547/2330 train_time:95132ms step_avg:61.49ms
step:1548/2330 train_time:95195ms step_avg:61.50ms
step:1549/2330 train_time:95256ms step_avg:61.50ms
step:1550/2330 train_time:95319ms step_avg:61.50ms
step:1551/2330 train_time:95380ms step_avg:61.50ms
step:1552/2330 train_time:95445ms step_avg:61.50ms
step:1553/2330 train_time:95506ms step_avg:61.50ms
step:1554/2330 train_time:95570ms step_avg:61.50ms
step:1555/2330 train_time:95630ms step_avg:61.50ms
step:1556/2330 train_time:95694ms step_avg:61.50ms
step:1557/2330 train_time:95755ms step_avg:61.50ms
step:1558/2330 train_time:95818ms step_avg:61.50ms
step:1559/2330 train_time:95878ms step_avg:61.50ms
step:1560/2330 train_time:95941ms step_avg:61.50ms
step:1561/2330 train_time:96002ms step_avg:61.50ms
step:1562/2330 train_time:96066ms step_avg:61.50ms
step:1563/2330 train_time:96127ms step_avg:61.50ms
step:1564/2330 train_time:96190ms step_avg:61.50ms
step:1565/2330 train_time:96250ms step_avg:61.50ms
step:1566/2330 train_time:96313ms step_avg:61.50ms
step:1567/2330 train_time:96374ms step_avg:61.50ms
step:1568/2330 train_time:96438ms step_avg:61.50ms
step:1569/2330 train_time:96499ms step_avg:61.50ms
step:1570/2330 train_time:96563ms step_avg:61.50ms
step:1571/2330 train_time:96623ms step_avg:61.50ms
step:1572/2330 train_time:96687ms step_avg:61.51ms
step:1573/2330 train_time:96748ms step_avg:61.51ms
step:1574/2330 train_time:96812ms step_avg:61.51ms
step:1575/2330 train_time:96872ms step_avg:61.51ms
step:1576/2330 train_time:96936ms step_avg:61.51ms
step:1577/2330 train_time:96996ms step_avg:61.51ms
step:1578/2330 train_time:97060ms step_avg:61.51ms
step:1579/2330 train_time:97120ms step_avg:61.51ms
step:1580/2330 train_time:97184ms step_avg:61.51ms
step:1581/2330 train_time:97244ms step_avg:61.51ms
step:1582/2330 train_time:97309ms step_avg:61.51ms
step:1583/2330 train_time:97369ms step_avg:61.51ms
step:1584/2330 train_time:97433ms step_avg:61.51ms
step:1585/2330 train_time:97494ms step_avg:61.51ms
step:1586/2330 train_time:97557ms step_avg:61.51ms
step:1587/2330 train_time:97618ms step_avg:61.51ms
step:1588/2330 train_time:97682ms step_avg:61.51ms
step:1589/2330 train_time:97742ms step_avg:61.51ms
step:1590/2330 train_time:97807ms step_avg:61.51ms
step:1591/2330 train_time:97868ms step_avg:61.51ms
step:1592/2330 train_time:97932ms step_avg:61.51ms
step:1593/2330 train_time:97992ms step_avg:61.51ms
step:1594/2330 train_time:98055ms step_avg:61.52ms
step:1595/2330 train_time:98116ms step_avg:61.51ms
step:1596/2330 train_time:98179ms step_avg:61.52ms
step:1597/2330 train_time:98239ms step_avg:61.51ms
step:1598/2330 train_time:98303ms step_avg:61.52ms
step:1599/2330 train_time:98364ms step_avg:61.52ms
step:1600/2330 train_time:98428ms step_avg:61.52ms
step:1601/2330 train_time:98488ms step_avg:61.52ms
step:1602/2330 train_time:98553ms step_avg:61.52ms
step:1603/2330 train_time:98614ms step_avg:61.52ms
step:1604/2330 train_time:98678ms step_avg:61.52ms
step:1605/2330 train_time:98738ms step_avg:61.52ms
step:1606/2330 train_time:98802ms step_avg:61.52ms
step:1607/2330 train_time:98863ms step_avg:61.52ms
step:1608/2330 train_time:98927ms step_avg:61.52ms
step:1609/2330 train_time:98988ms step_avg:61.52ms
step:1610/2330 train_time:99051ms step_avg:61.52ms
step:1611/2330 train_time:99112ms step_avg:61.52ms
step:1612/2330 train_time:99175ms step_avg:61.52ms
step:1613/2330 train_time:99235ms step_avg:61.52ms
step:1614/2330 train_time:99299ms step_avg:61.52ms
step:1615/2330 train_time:99359ms step_avg:61.52ms
step:1616/2330 train_time:99424ms step_avg:61.52ms
step:1617/2330 train_time:99485ms step_avg:61.52ms
step:1618/2330 train_time:99548ms step_avg:61.53ms
step:1619/2330 train_time:99608ms step_avg:61.52ms
step:1620/2330 train_time:99672ms step_avg:61.53ms
step:1621/2330 train_time:99733ms step_avg:61.53ms
step:1622/2330 train_time:99797ms step_avg:61.53ms
step:1623/2330 train_time:99857ms step_avg:61.53ms
step:1624/2330 train_time:99921ms step_avg:61.53ms
step:1625/2330 train_time:99982ms step_avg:61.53ms
step:1626/2330 train_time:100046ms step_avg:61.53ms
step:1627/2330 train_time:100107ms step_avg:61.53ms
step:1628/2330 train_time:100171ms step_avg:61.53ms
step:1629/2330 train_time:100231ms step_avg:61.53ms
step:1630/2330 train_time:100295ms step_avg:61.53ms
step:1631/2330 train_time:100356ms step_avg:61.53ms
step:1632/2330 train_time:100419ms step_avg:61.53ms
step:1633/2330 train_time:100480ms step_avg:61.53ms
step:1634/2330 train_time:100544ms step_avg:61.53ms
step:1635/2330 train_time:100605ms step_avg:61.53ms
step:1636/2330 train_time:100670ms step_avg:61.53ms
step:1637/2330 train_time:100730ms step_avg:61.53ms
step:1638/2330 train_time:100794ms step_avg:61.53ms
step:1639/2330 train_time:100854ms step_avg:61.53ms
step:1640/2330 train_time:100917ms step_avg:61.53ms
step:1641/2330 train_time:100977ms step_avg:61.53ms
step:1642/2330 train_time:101041ms step_avg:61.54ms
step:1643/2330 train_time:101102ms step_avg:61.53ms
step:1644/2330 train_time:101166ms step_avg:61.54ms
step:1645/2330 train_time:101226ms step_avg:61.54ms
step:1646/2330 train_time:101290ms step_avg:61.54ms
step:1647/2330 train_time:101350ms step_avg:61.54ms
step:1648/2330 train_time:101413ms step_avg:61.54ms
step:1649/2330 train_time:101475ms step_avg:61.54ms
step:1650/2330 train_time:101538ms step_avg:61.54ms
step:1651/2330 train_time:101598ms step_avg:61.54ms
step:1652/2330 train_time:101662ms step_avg:61.54ms
step:1653/2330 train_time:101723ms step_avg:61.54ms
step:1654/2330 train_time:101786ms step_avg:61.54ms
step:1655/2330 train_time:101847ms step_avg:61.54ms
step:1656/2330 train_time:101911ms step_avg:61.54ms
step:1657/2330 train_time:101971ms step_avg:61.54ms
step:1658/2330 train_time:102034ms step_avg:61.54ms
step:1659/2330 train_time:102094ms step_avg:61.54ms
step:1660/2330 train_time:102158ms step_avg:61.54ms
step:1661/2330 train_time:102219ms step_avg:61.54ms
step:1662/2330 train_time:102282ms step_avg:61.54ms
step:1663/2330 train_time:102343ms step_avg:61.54ms
step:1664/2330 train_time:102407ms step_avg:61.54ms
step:1665/2330 train_time:102467ms step_avg:61.54ms
step:1666/2330 train_time:102530ms step_avg:61.54ms
step:1667/2330 train_time:102591ms step_avg:61.54ms
step:1668/2330 train_time:102655ms step_avg:61.54ms
step:1669/2330 train_time:102716ms step_avg:61.54ms
step:1670/2330 train_time:102779ms step_avg:61.54ms
step:1671/2330 train_time:102840ms step_avg:61.54ms
step:1672/2330 train_time:102904ms step_avg:61.55ms
step:1673/2330 train_time:102965ms step_avg:61.54ms
step:1674/2330 train_time:103029ms step_avg:61.55ms
step:1675/2330 train_time:103089ms step_avg:61.55ms
step:1676/2330 train_time:103153ms step_avg:61.55ms
step:1677/2330 train_time:103215ms step_avg:61.55ms
step:1678/2330 train_time:103278ms step_avg:61.55ms
step:1679/2330 train_time:103338ms step_avg:61.55ms
step:1680/2330 train_time:103402ms step_avg:61.55ms
step:1681/2330 train_time:103463ms step_avg:61.55ms
step:1682/2330 train_time:103527ms step_avg:61.55ms
step:1683/2330 train_time:103588ms step_avg:61.55ms
step:1684/2330 train_time:103651ms step_avg:61.55ms
step:1685/2330 train_time:103712ms step_avg:61.55ms
step:1686/2330 train_time:103776ms step_avg:61.55ms
step:1687/2330 train_time:103836ms step_avg:61.55ms
step:1688/2330 train_time:103900ms step_avg:61.55ms
step:1689/2330 train_time:103960ms step_avg:61.55ms
step:1690/2330 train_time:104025ms step_avg:61.55ms
step:1691/2330 train_time:104086ms step_avg:61.55ms
step:1692/2330 train_time:104150ms step_avg:61.55ms
step:1693/2330 train_time:104210ms step_avg:61.55ms
step:1694/2330 train_time:104274ms step_avg:61.55ms
step:1695/2330 train_time:104335ms step_avg:61.55ms
step:1696/2330 train_time:104398ms step_avg:61.56ms
step:1697/2330 train_time:104458ms step_avg:61.55ms
step:1698/2330 train_time:104522ms step_avg:61.56ms
step:1699/2330 train_time:104583ms step_avg:61.56ms
step:1700/2330 train_time:104647ms step_avg:61.56ms
step:1701/2330 train_time:104709ms step_avg:61.56ms
step:1702/2330 train_time:104773ms step_avg:61.56ms
step:1703/2330 train_time:104833ms step_avg:61.56ms
step:1704/2330 train_time:104896ms step_avg:61.56ms
step:1705/2330 train_time:104956ms step_avg:61.56ms
step:1706/2330 train_time:105020ms step_avg:61.56ms
step:1707/2330 train_time:105082ms step_avg:61.56ms
step:1708/2330 train_time:105145ms step_avg:61.56ms
step:1709/2330 train_time:105206ms step_avg:61.56ms
step:1710/2330 train_time:105271ms step_avg:61.56ms
step:1711/2330 train_time:105331ms step_avg:61.56ms
step:1712/2330 train_time:105394ms step_avg:61.56ms
step:1713/2330 train_time:105455ms step_avg:61.56ms
step:1714/2330 train_time:105518ms step_avg:61.56ms
step:1715/2330 train_time:105579ms step_avg:61.56ms
step:1716/2330 train_time:105642ms step_avg:61.56ms
step:1717/2330 train_time:105704ms step_avg:61.56ms
step:1718/2330 train_time:105768ms step_avg:61.56ms
step:1719/2330 train_time:105828ms step_avg:61.56ms
step:1720/2330 train_time:105891ms step_avg:61.56ms
step:1721/2330 train_time:105952ms step_avg:61.56ms
step:1722/2330 train_time:106015ms step_avg:61.57ms
step:1723/2330 train_time:106075ms step_avg:61.56ms
step:1724/2330 train_time:106139ms step_avg:61.57ms
step:1725/2330 train_time:106200ms step_avg:61.57ms
step:1726/2330 train_time:106265ms step_avg:61.57ms
step:1727/2330 train_time:106326ms step_avg:61.57ms
step:1728/2330 train_time:106389ms step_avg:61.57ms
step:1729/2330 train_time:106450ms step_avg:61.57ms
step:1730/2330 train_time:106514ms step_avg:61.57ms
step:1731/2330 train_time:106575ms step_avg:61.57ms
step:1732/2330 train_time:106638ms step_avg:61.57ms
step:1733/2330 train_time:106700ms step_avg:61.57ms
step:1734/2330 train_time:106763ms step_avg:61.57ms
step:1735/2330 train_time:106824ms step_avg:61.57ms
step:1736/2330 train_time:106887ms step_avg:61.57ms
step:1737/2330 train_time:106948ms step_avg:61.57ms
step:1738/2330 train_time:107011ms step_avg:61.57ms
step:1739/2330 train_time:107072ms step_avg:61.57ms
step:1740/2330 train_time:107136ms step_avg:61.57ms
step:1741/2330 train_time:107196ms step_avg:61.57ms
step:1742/2330 train_time:107259ms step_avg:61.57ms
step:1743/2330 train_time:107320ms step_avg:61.57ms
step:1744/2330 train_time:107384ms step_avg:61.57ms
step:1745/2330 train_time:107444ms step_avg:61.57ms
step:1746/2330 train_time:107508ms step_avg:61.57ms
step:1747/2330 train_time:107569ms step_avg:61.57ms
step:1748/2330 train_time:107633ms step_avg:61.58ms
step:1749/2330 train_time:107694ms step_avg:61.57ms
step:1750/2330 train_time:107757ms step_avg:61.58ms
step:1750/2330 val_loss:3.4476 train_time:107821ms step_avg:61.61ms
step:1751/2330 train_time:107846ms step_avg:61.59ms
step:1752/2330 train_time:107887ms step_avg:61.58ms
step:1753/2330 train_time:107955ms step_avg:61.58ms
step:1754/2330 train_time:108020ms step_avg:61.58ms
step:1755/2330 train_time:108080ms step_avg:61.58ms
step:1756/2330 train_time:108143ms step_avg:61.58ms
step:1757/2330 train_time:108203ms step_avg:61.58ms
step:1758/2330 train_time:108266ms step_avg:61.58ms
step:1759/2330 train_time:108325ms step_avg:61.58ms
step:1760/2330 train_time:108388ms step_avg:61.58ms
step:1761/2330 train_time:108448ms step_avg:61.58ms
step:1762/2330 train_time:108511ms step_avg:61.58ms
step:1763/2330 train_time:108571ms step_avg:61.58ms
step:1764/2330 train_time:108633ms step_avg:61.58ms
step:1765/2330 train_time:108693ms step_avg:61.58ms
step:1766/2330 train_time:108757ms step_avg:61.58ms
step:1767/2330 train_time:108819ms step_avg:61.58ms
step:1768/2330 train_time:108885ms step_avg:61.59ms
step:1769/2330 train_time:108949ms step_avg:61.59ms
step:1770/2330 train_time:109014ms step_avg:61.59ms
step:1771/2330 train_time:109076ms step_avg:61.59ms
step:1772/2330 train_time:109140ms step_avg:61.59ms
step:1773/2330 train_time:109200ms step_avg:61.59ms
step:1774/2330 train_time:109263ms step_avg:61.59ms
step:1775/2330 train_time:109324ms step_avg:61.59ms
step:1776/2330 train_time:109387ms step_avg:61.59ms
step:1777/2330 train_time:109447ms step_avg:61.59ms
step:1778/2330 train_time:109510ms step_avg:61.59ms
step:1779/2330 train_time:109570ms step_avg:61.59ms
step:1780/2330 train_time:109633ms step_avg:61.59ms
step:1781/2330 train_time:109693ms step_avg:61.59ms
step:1782/2330 train_time:109757ms step_avg:61.59ms
step:1783/2330 train_time:109819ms step_avg:61.59ms
step:1784/2330 train_time:109884ms step_avg:61.59ms
step:1785/2330 train_time:109946ms step_avg:61.59ms
step:1786/2330 train_time:110011ms step_avg:61.60ms
step:1787/2330 train_time:110073ms step_avg:61.60ms
step:1788/2330 train_time:110137ms step_avg:61.60ms
step:1789/2330 train_time:110198ms step_avg:61.60ms
step:1790/2330 train_time:110261ms step_avg:61.60ms
step:1791/2330 train_time:110322ms step_avg:61.60ms
step:1792/2330 train_time:110385ms step_avg:61.60ms
step:1793/2330 train_time:110445ms step_avg:61.60ms
step:1794/2330 train_time:110508ms step_avg:61.60ms
step:1795/2330 train_time:110568ms step_avg:61.60ms
step:1796/2330 train_time:110632ms step_avg:61.60ms
step:1797/2330 train_time:110692ms step_avg:61.60ms
step:1798/2330 train_time:110756ms step_avg:61.60ms
step:1799/2330 train_time:110817ms step_avg:61.60ms
step:1800/2330 train_time:110882ms step_avg:61.60ms
step:1801/2330 train_time:110943ms step_avg:61.60ms
step:1802/2330 train_time:111008ms step_avg:61.60ms
step:1803/2330 train_time:111070ms step_avg:61.60ms
step:1804/2330 train_time:111135ms step_avg:61.60ms
step:1805/2330 train_time:111195ms step_avg:61.60ms
step:1806/2330 train_time:111259ms step_avg:61.61ms
step:1807/2330 train_time:111320ms step_avg:61.60ms
step:1808/2330 train_time:111384ms step_avg:61.61ms
step:1809/2330 train_time:111443ms step_avg:61.61ms
step:1810/2330 train_time:111506ms step_avg:61.61ms
step:1811/2330 train_time:111567ms step_avg:61.61ms
step:1812/2330 train_time:111631ms step_avg:61.61ms
step:1813/2330 train_time:111691ms step_avg:61.61ms
step:1814/2330 train_time:111755ms step_avg:61.61ms
step:1815/2330 train_time:111816ms step_avg:61.61ms
step:1816/2330 train_time:111881ms step_avg:61.61ms
step:1817/2330 train_time:111942ms step_avg:61.61ms
step:1818/2330 train_time:112005ms step_avg:61.61ms
step:1819/2330 train_time:112066ms step_avg:61.61ms
step:1820/2330 train_time:112131ms step_avg:61.61ms
step:1821/2330 train_time:112192ms step_avg:61.61ms
step:1822/2330 train_time:112257ms step_avg:61.61ms
step:1823/2330 train_time:112317ms step_avg:61.61ms
step:1824/2330 train_time:112380ms step_avg:61.61ms
step:1825/2330 train_time:112441ms step_avg:61.61ms
step:1826/2330 train_time:112504ms step_avg:61.61ms
step:1827/2330 train_time:112564ms step_avg:61.61ms
step:1828/2330 train_time:112628ms step_avg:61.61ms
step:1829/2330 train_time:112688ms step_avg:61.61ms
step:1830/2330 train_time:112752ms step_avg:61.61ms
step:1831/2330 train_time:112813ms step_avg:61.61ms
step:1832/2330 train_time:112878ms step_avg:61.61ms
step:1833/2330 train_time:112938ms step_avg:61.61ms
step:1834/2330 train_time:113001ms step_avg:61.61ms
step:1835/2330 train_time:113062ms step_avg:61.61ms
step:1836/2330 train_time:113126ms step_avg:61.62ms
step:1837/2330 train_time:113188ms step_avg:61.62ms
step:1838/2330 train_time:113251ms step_avg:61.62ms
step:1839/2330 train_time:113313ms step_avg:61.62ms
step:1840/2330 train_time:113377ms step_avg:61.62ms
step:1841/2330 train_time:113437ms step_avg:61.62ms
step:1842/2330 train_time:113501ms step_avg:61.62ms
step:1843/2330 train_time:113561ms step_avg:61.62ms
step:1844/2330 train_time:113624ms step_avg:61.62ms
step:1845/2330 train_time:113684ms step_avg:61.62ms
step:1846/2330 train_time:113748ms step_avg:61.62ms
step:1847/2330 train_time:113809ms step_avg:61.62ms
step:1848/2330 train_time:113874ms step_avg:61.62ms
step:1849/2330 train_time:113935ms step_avg:61.62ms
step:1850/2330 train_time:113999ms step_avg:61.62ms
step:1851/2330 train_time:114060ms step_avg:61.62ms
step:1852/2330 train_time:114123ms step_avg:61.62ms
step:1853/2330 train_time:114184ms step_avg:61.62ms
step:1854/2330 train_time:114247ms step_avg:61.62ms
step:1855/2330 train_time:114309ms step_avg:61.62ms
step:1856/2330 train_time:114374ms step_avg:61.62ms
step:1857/2330 train_time:114434ms step_avg:61.62ms
step:1858/2330 train_time:114498ms step_avg:61.62ms
step:1859/2330 train_time:114558ms step_avg:61.62ms
step:1860/2330 train_time:114621ms step_avg:61.62ms
step:1861/2330 train_time:114682ms step_avg:61.62ms
step:1862/2330 train_time:114745ms step_avg:61.62ms
step:1863/2330 train_time:114806ms step_avg:61.62ms
step:1864/2330 train_time:114870ms step_avg:61.63ms
step:1865/2330 train_time:114932ms step_avg:61.63ms
step:1866/2330 train_time:114996ms step_avg:61.63ms
step:1867/2330 train_time:115057ms step_avg:61.63ms
step:1868/2330 train_time:115121ms step_avg:61.63ms
step:1869/2330 train_time:115181ms step_avg:61.63ms
step:1870/2330 train_time:115245ms step_avg:61.63ms
step:1871/2330 train_time:115306ms step_avg:61.63ms
step:1872/2330 train_time:115370ms step_avg:61.63ms
step:1873/2330 train_time:115431ms step_avg:61.63ms
step:1874/2330 train_time:115495ms step_avg:61.63ms
step:1875/2330 train_time:115555ms step_avg:61.63ms
step:1876/2330 train_time:115618ms step_avg:61.63ms
step:1877/2330 train_time:115679ms step_avg:61.63ms
step:1878/2330 train_time:115743ms step_avg:61.63ms
step:1879/2330 train_time:115803ms step_avg:61.63ms
step:1880/2330 train_time:115866ms step_avg:61.63ms
step:1881/2330 train_time:115927ms step_avg:61.63ms
step:1882/2330 train_time:115991ms step_avg:61.63ms
step:1883/2330 train_time:116052ms step_avg:61.63ms
step:1884/2330 train_time:116116ms step_avg:61.63ms
step:1885/2330 train_time:116177ms step_avg:61.63ms
step:1886/2330 train_time:116241ms step_avg:61.63ms
step:1887/2330 train_time:116301ms step_avg:61.63ms
step:1888/2330 train_time:116364ms step_avg:61.63ms
step:1889/2330 train_time:116425ms step_avg:61.63ms
step:1890/2330 train_time:116489ms step_avg:61.63ms
step:1891/2330 train_time:116550ms step_avg:61.63ms
step:1892/2330 train_time:116614ms step_avg:61.64ms
step:1893/2330 train_time:116675ms step_avg:61.63ms
step:1894/2330 train_time:116738ms step_avg:61.64ms
step:1895/2330 train_time:116798ms step_avg:61.63ms
step:1896/2330 train_time:116861ms step_avg:61.64ms
step:1897/2330 train_time:116921ms step_avg:61.63ms
step:1898/2330 train_time:116985ms step_avg:61.64ms
step:1899/2330 train_time:117046ms step_avg:61.64ms
step:1900/2330 train_time:117111ms step_avg:61.64ms
step:1901/2330 train_time:117174ms step_avg:61.64ms
step:1902/2330 train_time:117237ms step_avg:61.64ms
step:1903/2330 train_time:117297ms step_avg:61.64ms
step:1904/2330 train_time:117361ms step_avg:61.64ms
step:1905/2330 train_time:117421ms step_avg:61.64ms
step:1906/2330 train_time:117484ms step_avg:61.64ms
step:1907/2330 train_time:117545ms step_avg:61.64ms
step:1908/2330 train_time:117609ms step_avg:61.64ms
step:1909/2330 train_time:117669ms step_avg:61.64ms
step:1910/2330 train_time:117733ms step_avg:61.64ms
step:1911/2330 train_time:117794ms step_avg:61.64ms
step:1912/2330 train_time:117857ms step_avg:61.64ms
step:1913/2330 train_time:117917ms step_avg:61.64ms
step:1914/2330 train_time:117981ms step_avg:61.64ms
step:1915/2330 train_time:118041ms step_avg:61.64ms
step:1916/2330 train_time:118105ms step_avg:61.64ms
step:1917/2330 train_time:118166ms step_avg:61.64ms
step:1918/2330 train_time:118230ms step_avg:61.64ms
step:1919/2330 train_time:118291ms step_avg:61.64ms
step:1920/2330 train_time:118356ms step_avg:61.64ms
step:1921/2330 train_time:118416ms step_avg:61.64ms
step:1922/2330 train_time:118481ms step_avg:61.64ms
step:1923/2330 train_time:118541ms step_avg:61.64ms
step:1924/2330 train_time:118605ms step_avg:61.64ms
step:1925/2330 train_time:118665ms step_avg:61.64ms
step:1926/2330 train_time:118729ms step_avg:61.65ms
step:1927/2330 train_time:118790ms step_avg:61.65ms
step:1928/2330 train_time:118854ms step_avg:61.65ms
step:1929/2330 train_time:118915ms step_avg:61.65ms
step:1930/2330 train_time:118979ms step_avg:61.65ms
step:1931/2330 train_time:119040ms step_avg:61.65ms
step:1932/2330 train_time:119104ms step_avg:61.65ms
step:1933/2330 train_time:119164ms step_avg:61.65ms
step:1934/2330 train_time:119228ms step_avg:61.65ms
step:1935/2330 train_time:119290ms step_avg:61.65ms
step:1936/2330 train_time:119354ms step_avg:61.65ms
step:1937/2330 train_time:119415ms step_avg:61.65ms
step:1938/2330 train_time:119479ms step_avg:61.65ms
step:1939/2330 train_time:119540ms step_avg:61.65ms
step:1940/2330 train_time:119603ms step_avg:61.65ms
step:1941/2330 train_time:119663ms step_avg:61.65ms
step:1942/2330 train_time:119727ms step_avg:61.65ms
step:1943/2330 train_time:119788ms step_avg:61.65ms
step:1944/2330 train_time:119852ms step_avg:61.65ms
step:1945/2330 train_time:119912ms step_avg:61.65ms
step:1946/2330 train_time:119976ms step_avg:61.65ms
step:1947/2330 train_time:120036ms step_avg:61.65ms
step:1948/2330 train_time:120099ms step_avg:61.65ms
step:1949/2330 train_time:120160ms step_avg:61.65ms
step:1950/2330 train_time:120223ms step_avg:61.65ms
step:1951/2330 train_time:120284ms step_avg:61.65ms
step:1952/2330 train_time:120348ms step_avg:61.65ms
step:1953/2330 train_time:120408ms step_avg:61.65ms
step:1954/2330 train_time:120473ms step_avg:61.65ms
step:1955/2330 train_time:120534ms step_avg:61.65ms
step:1956/2330 train_time:120598ms step_avg:61.66ms
step:1957/2330 train_time:120658ms step_avg:61.65ms
step:1958/2330 train_time:120721ms step_avg:61.66ms
step:1959/2330 train_time:120782ms step_avg:61.65ms
step:1960/2330 train_time:120845ms step_avg:61.66ms
step:1961/2330 train_time:120906ms step_avg:61.66ms
step:1962/2330 train_time:120971ms step_avg:61.66ms
step:1963/2330 train_time:121032ms step_avg:61.66ms
step:1964/2330 train_time:121096ms step_avg:61.66ms
step:1965/2330 train_time:121156ms step_avg:61.66ms
step:1966/2330 train_time:121219ms step_avg:61.66ms
step:1967/2330 train_time:121280ms step_avg:61.66ms
step:1968/2330 train_time:121343ms step_avg:61.66ms
step:1969/2330 train_time:121404ms step_avg:61.66ms
step:1970/2330 train_time:121467ms step_avg:61.66ms
step:1971/2330 train_time:121528ms step_avg:61.66ms
step:1972/2330 train_time:121592ms step_avg:61.66ms
step:1973/2330 train_time:121653ms step_avg:61.66ms
step:1974/2330 train_time:121717ms step_avg:61.66ms
step:1975/2330 train_time:121777ms step_avg:61.66ms
step:1976/2330 train_time:121841ms step_avg:61.66ms
step:1977/2330 train_time:121901ms step_avg:61.66ms
step:1978/2330 train_time:121964ms step_avg:61.66ms
step:1979/2330 train_time:122025ms step_avg:61.66ms
step:1980/2330 train_time:122089ms step_avg:61.66ms
step:1981/2330 train_time:122150ms step_avg:61.66ms
step:1982/2330 train_time:122213ms step_avg:61.66ms
step:1983/2330 train_time:122274ms step_avg:61.66ms
step:1984/2330 train_time:122338ms step_avg:61.66ms
step:1985/2330 train_time:122399ms step_avg:61.66ms
step:1986/2330 train_time:122462ms step_avg:61.66ms
step:1987/2330 train_time:122522ms step_avg:61.66ms
step:1988/2330 train_time:122586ms step_avg:61.66ms
step:1989/2330 train_time:122646ms step_avg:61.66ms
step:1990/2330 train_time:122711ms step_avg:61.66ms
step:1991/2330 train_time:122773ms step_avg:61.66ms
step:1992/2330 train_time:122836ms step_avg:61.66ms
step:1993/2330 train_time:122897ms step_avg:61.66ms
step:1994/2330 train_time:122960ms step_avg:61.67ms
step:1995/2330 train_time:123022ms step_avg:61.66ms
step:1996/2330 train_time:123085ms step_avg:61.67ms
step:1997/2330 train_time:123146ms step_avg:61.67ms
step:1998/2330 train_time:123210ms step_avg:61.67ms
step:1999/2330 train_time:123272ms step_avg:61.67ms
step:2000/2330 train_time:123336ms step_avg:61.67ms
step:2000/2330 val_loss:3.4211 train_time:123401ms step_avg:61.70ms
step:2001/2330 train_time:123425ms step_avg:61.68ms
step:2002/2330 train_time:123464ms step_avg:61.67ms
step:2003/2330 train_time:123530ms step_avg:61.67ms
step:2004/2330 train_time:123596ms step_avg:61.67ms
step:2005/2330 train_time:123656ms step_avg:61.67ms
step:2006/2330 train_time:123721ms step_avg:61.68ms
step:2007/2330 train_time:123781ms step_avg:61.67ms
step:2008/2330 train_time:123845ms step_avg:61.68ms
step:2009/2330 train_time:123904ms step_avg:61.67ms
step:2010/2330 train_time:123967ms step_avg:61.67ms
step:2011/2330 train_time:124027ms step_avg:61.67ms
step:2012/2330 train_time:124089ms step_avg:61.67ms
step:2013/2330 train_time:124149ms step_avg:61.67ms
step:2014/2330 train_time:124212ms step_avg:61.67ms
step:2015/2330 train_time:124271ms step_avg:61.67ms
step:2016/2330 train_time:124335ms step_avg:61.67ms
step:2017/2330 train_time:124397ms step_avg:61.67ms
step:2018/2330 train_time:124462ms step_avg:61.68ms
step:2019/2330 train_time:124524ms step_avg:61.68ms
step:2020/2330 train_time:124589ms step_avg:61.68ms
step:2021/2330 train_time:124650ms step_avg:61.68ms
step:2022/2330 train_time:124713ms step_avg:61.68ms
step:2023/2330 train_time:124774ms step_avg:61.68ms
step:2024/2330 train_time:124838ms step_avg:61.68ms
step:2025/2330 train_time:124898ms step_avg:61.68ms
step:2026/2330 train_time:124962ms step_avg:61.68ms
step:2027/2330 train_time:125022ms step_avg:61.68ms
step:2028/2330 train_time:125086ms step_avg:61.68ms
step:2029/2330 train_time:125145ms step_avg:61.68ms
step:2030/2330 train_time:125208ms step_avg:61.68ms
step:2031/2330 train_time:125268ms step_avg:61.68ms
step:2032/2330 train_time:125331ms step_avg:61.68ms
step:2033/2330 train_time:125392ms step_avg:61.68ms
step:2034/2330 train_time:125455ms step_avg:61.68ms
step:2035/2330 train_time:125517ms step_avg:61.68ms
step:2036/2330 train_time:125582ms step_avg:61.68ms
step:2037/2330 train_time:125643ms step_avg:61.68ms
step:2038/2330 train_time:125707ms step_avg:61.68ms
step:2039/2330 train_time:125768ms step_avg:61.68ms
step:2040/2330 train_time:125831ms step_avg:61.68ms
step:2041/2330 train_time:125892ms step_avg:61.68ms
step:2042/2330 train_time:125955ms step_avg:61.68ms
step:2043/2330 train_time:126016ms step_avg:61.68ms
step:2044/2330 train_time:126081ms step_avg:61.68ms
step:2045/2330 train_time:126142ms step_avg:61.68ms
step:2046/2330 train_time:126205ms step_avg:61.68ms
step:2047/2330 train_time:126266ms step_avg:61.68ms
step:2048/2330 train_time:126329ms step_avg:61.68ms
step:2049/2330 train_time:126390ms step_avg:61.68ms
step:2050/2330 train_time:126454ms step_avg:61.68ms
step:2051/2330 train_time:126514ms step_avg:61.68ms
step:2052/2330 train_time:126579ms step_avg:61.69ms
step:2053/2330 train_time:126639ms step_avg:61.69ms
step:2054/2330 train_time:126703ms step_avg:61.69ms
step:2055/2330 train_time:126764ms step_avg:61.69ms
step:2056/2330 train_time:126828ms step_avg:61.69ms
step:2057/2330 train_time:126888ms step_avg:61.69ms
step:2058/2330 train_time:126952ms step_avg:61.69ms
step:2059/2330 train_time:127012ms step_avg:61.69ms
step:2060/2330 train_time:127075ms step_avg:61.69ms
step:2061/2330 train_time:127136ms step_avg:61.69ms
step:2062/2330 train_time:127200ms step_avg:61.69ms
step:2063/2330 train_time:127261ms step_avg:61.69ms
step:2064/2330 train_time:127324ms step_avg:61.69ms
step:2065/2330 train_time:127384ms step_avg:61.69ms
step:2066/2330 train_time:127449ms step_avg:61.69ms
step:2067/2330 train_time:127509ms step_avg:61.69ms
step:2068/2330 train_time:127572ms step_avg:61.69ms
step:2069/2330 train_time:127633ms step_avg:61.69ms
step:2070/2330 train_time:127697ms step_avg:61.69ms
step:2071/2330 train_time:127758ms step_avg:61.69ms
step:2072/2330 train_time:127822ms step_avg:61.69ms
step:2073/2330 train_time:127883ms step_avg:61.69ms
step:2074/2330 train_time:127947ms step_avg:61.69ms
step:2075/2330 train_time:128008ms step_avg:61.69ms
step:2076/2330 train_time:128071ms step_avg:61.69ms
step:2077/2330 train_time:128132ms step_avg:61.69ms
step:2078/2330 train_time:128195ms step_avg:61.69ms
step:2079/2330 train_time:128255ms step_avg:61.69ms
step:2080/2330 train_time:128319ms step_avg:61.69ms
step:2081/2330 train_time:128381ms step_avg:61.69ms
step:2082/2330 train_time:128444ms step_avg:61.69ms
step:2083/2330 train_time:128504ms step_avg:61.69ms
step:2084/2330 train_time:128568ms step_avg:61.69ms
step:2085/2330 train_time:128629ms step_avg:61.69ms
step:2086/2330 train_time:128692ms step_avg:61.69ms
step:2087/2330 train_time:128753ms step_avg:61.69ms
step:2088/2330 train_time:128817ms step_avg:61.69ms
step:2089/2330 train_time:128878ms step_avg:61.69ms
step:2090/2330 train_time:128942ms step_avg:61.69ms
step:2091/2330 train_time:129003ms step_avg:61.69ms
step:2092/2330 train_time:129066ms step_avg:61.69ms
step:2093/2330 train_time:129126ms step_avg:61.69ms
step:2094/2330 train_time:129191ms step_avg:61.70ms
step:2095/2330 train_time:129251ms step_avg:61.70ms
step:2096/2330 train_time:129315ms step_avg:61.70ms
step:2097/2330 train_time:129376ms step_avg:61.70ms
step:2098/2330 train_time:129441ms step_avg:61.70ms
step:2099/2330 train_time:129501ms step_avg:61.70ms
step:2100/2330 train_time:129564ms step_avg:61.70ms
step:2101/2330 train_time:129625ms step_avg:61.70ms
step:2102/2330 train_time:129689ms step_avg:61.70ms
step:2103/2330 train_time:129750ms step_avg:61.70ms
step:2104/2330 train_time:129813ms step_avg:61.70ms
step:2105/2330 train_time:129873ms step_avg:61.70ms
step:2106/2330 train_time:129937ms step_avg:61.70ms
step:2107/2330 train_time:129999ms step_avg:61.70ms
step:2108/2330 train_time:130062ms step_avg:61.70ms
step:2109/2330 train_time:130123ms step_avg:61.70ms
step:2110/2330 train_time:130187ms step_avg:61.70ms
step:2111/2330 train_time:130248ms step_avg:61.70ms
step:2112/2330 train_time:130311ms step_avg:61.70ms
step:2113/2330 train_time:130372ms step_avg:61.70ms
step:2114/2330 train_time:130436ms step_avg:61.70ms
step:2115/2330 train_time:130496ms step_avg:61.70ms
step:2116/2330 train_time:130560ms step_avg:61.70ms
step:2117/2330 train_time:130621ms step_avg:61.70ms
step:2118/2330 train_time:130685ms step_avg:61.70ms
step:2119/2330 train_time:130746ms step_avg:61.70ms
step:2120/2330 train_time:130809ms step_avg:61.70ms
step:2121/2330 train_time:130870ms step_avg:61.70ms
step:2122/2330 train_time:130933ms step_avg:61.70ms
step:2123/2330 train_time:130994ms step_avg:61.70ms
step:2124/2330 train_time:131058ms step_avg:61.70ms
step:2125/2330 train_time:131118ms step_avg:61.70ms
step:2126/2330 train_time:131183ms step_avg:61.70ms
step:2127/2330 train_time:131244ms step_avg:61.70ms
step:2128/2330 train_time:131307ms step_avg:61.70ms
step:2129/2330 train_time:131368ms step_avg:61.70ms
step:2130/2330 train_time:131431ms step_avg:61.70ms
step:2131/2330 train_time:131492ms step_avg:61.70ms
step:2132/2330 train_time:131556ms step_avg:61.71ms
step:2133/2330 train_time:131617ms step_avg:61.70ms
step:2134/2330 train_time:131680ms step_avg:61.71ms
step:2135/2330 train_time:131741ms step_avg:61.71ms
step:2136/2330 train_time:131805ms step_avg:61.71ms
step:2137/2330 train_time:131865ms step_avg:61.71ms
step:2138/2330 train_time:131929ms step_avg:61.71ms
step:2139/2330 train_time:131989ms step_avg:61.71ms
step:2140/2330 train_time:132053ms step_avg:61.71ms
step:2141/2330 train_time:132113ms step_avg:61.71ms
step:2142/2330 train_time:132177ms step_avg:61.71ms
step:2143/2330 train_time:132237ms step_avg:61.71ms
step:2144/2330 train_time:132302ms step_avg:61.71ms
step:2145/2330 train_time:132363ms step_avg:61.71ms
step:2146/2330 train_time:132426ms step_avg:61.71ms
step:2147/2330 train_time:132486ms step_avg:61.71ms
step:2148/2330 train_time:132551ms step_avg:61.71ms
step:2149/2330 train_time:132611ms step_avg:61.71ms
step:2150/2330 train_time:132675ms step_avg:61.71ms
step:2151/2330 train_time:132736ms step_avg:61.71ms
step:2152/2330 train_time:132799ms step_avg:61.71ms
step:2153/2330 train_time:132860ms step_avg:61.71ms
step:2154/2330 train_time:132923ms step_avg:61.71ms
step:2155/2330 train_time:132984ms step_avg:61.71ms
step:2156/2330 train_time:133049ms step_avg:61.71ms
step:2157/2330 train_time:133109ms step_avg:61.71ms
step:2158/2330 train_time:133172ms step_avg:61.71ms
step:2159/2330 train_time:133233ms step_avg:61.71ms
step:2160/2330 train_time:133297ms step_avg:61.71ms
step:2161/2330 train_time:133358ms step_avg:61.71ms
step:2162/2330 train_time:133422ms step_avg:61.71ms
step:2163/2330 train_time:133484ms step_avg:61.71ms
step:2164/2330 train_time:133547ms step_avg:61.71ms
step:2165/2330 train_time:133608ms step_avg:61.71ms
step:2166/2330 train_time:133672ms step_avg:61.71ms
step:2167/2330 train_time:133732ms step_avg:61.71ms
step:2168/2330 train_time:133796ms step_avg:61.71ms
step:2169/2330 train_time:133857ms step_avg:61.71ms
step:2170/2330 train_time:133920ms step_avg:61.71ms
step:2171/2330 train_time:133981ms step_avg:61.71ms
step:2172/2330 train_time:134044ms step_avg:61.71ms
step:2173/2330 train_time:134104ms step_avg:61.71ms
step:2174/2330 train_time:134168ms step_avg:61.71ms
step:2175/2330 train_time:134229ms step_avg:61.71ms
step:2176/2330 train_time:134293ms step_avg:61.72ms
step:2177/2330 train_time:134353ms step_avg:61.71ms
step:2178/2330 train_time:134418ms step_avg:61.72ms
step:2179/2330 train_time:134479ms step_avg:61.72ms
step:2180/2330 train_time:134543ms step_avg:61.72ms
step:2181/2330 train_time:134604ms step_avg:61.72ms
step:2182/2330 train_time:134667ms step_avg:61.72ms
step:2183/2330 train_time:134728ms step_avg:61.72ms
step:2184/2330 train_time:134792ms step_avg:61.72ms
step:2185/2330 train_time:134853ms step_avg:61.72ms
step:2186/2330 train_time:134917ms step_avg:61.72ms
step:2187/2330 train_time:134978ms step_avg:61.72ms
step:2188/2330 train_time:135043ms step_avg:61.72ms
step:2189/2330 train_time:135103ms step_avg:61.72ms
step:2190/2330 train_time:135167ms step_avg:61.72ms
step:2191/2330 train_time:135228ms step_avg:61.72ms
step:2192/2330 train_time:135291ms step_avg:61.72ms
step:2193/2330 train_time:135352ms step_avg:61.72ms
step:2194/2330 train_time:135415ms step_avg:61.72ms
step:2195/2330 train_time:135475ms step_avg:61.72ms
step:2196/2330 train_time:135540ms step_avg:61.72ms
step:2197/2330 train_time:135601ms step_avg:61.72ms
step:2198/2330 train_time:135664ms step_avg:61.72ms
step:2199/2330 train_time:135725ms step_avg:61.72ms
step:2200/2330 train_time:135789ms step_avg:61.72ms
step:2201/2330 train_time:135850ms step_avg:61.72ms
step:2202/2330 train_time:135913ms step_avg:61.72ms
step:2203/2330 train_time:135975ms step_avg:61.72ms
step:2204/2330 train_time:136038ms step_avg:61.72ms
step:2205/2330 train_time:136099ms step_avg:61.72ms
step:2206/2330 train_time:136164ms step_avg:61.72ms
step:2207/2330 train_time:136224ms step_avg:61.72ms
step:2208/2330 train_time:136289ms step_avg:61.73ms
step:2209/2330 train_time:136349ms step_avg:61.72ms
step:2210/2330 train_time:136413ms step_avg:61.73ms
step:2211/2330 train_time:136473ms step_avg:61.72ms
step:2212/2330 train_time:136537ms step_avg:61.73ms
step:2213/2330 train_time:136598ms step_avg:61.73ms
step:2214/2330 train_time:136662ms step_avg:61.73ms
step:2215/2330 train_time:136724ms step_avg:61.73ms
step:2216/2330 train_time:136787ms step_avg:61.73ms
step:2217/2330 train_time:136848ms step_avg:61.73ms
step:2218/2330 train_time:136911ms step_avg:61.73ms
step:2219/2330 train_time:136971ms step_avg:61.73ms
step:2220/2330 train_time:137035ms step_avg:61.73ms
step:2221/2330 train_time:137096ms step_avg:61.73ms
step:2222/2330 train_time:137159ms step_avg:61.73ms
step:2223/2330 train_time:137220ms step_avg:61.73ms
step:2224/2330 train_time:137285ms step_avg:61.73ms
step:2225/2330 train_time:137345ms step_avg:61.73ms
step:2226/2330 train_time:137409ms step_avg:61.73ms
step:2227/2330 train_time:137470ms step_avg:61.73ms
step:2228/2330 train_time:137533ms step_avg:61.73ms
step:2229/2330 train_time:137593ms step_avg:61.73ms
step:2230/2330 train_time:137657ms step_avg:61.73ms
step:2231/2330 train_time:137718ms step_avg:61.73ms
step:2232/2330 train_time:137783ms step_avg:61.73ms
step:2233/2330 train_time:137844ms step_avg:61.73ms
step:2234/2330 train_time:137907ms step_avg:61.73ms
step:2235/2330 train_time:137968ms step_avg:61.73ms
step:2236/2330 train_time:138032ms step_avg:61.73ms
step:2237/2330 train_time:138092ms step_avg:61.73ms
step:2238/2330 train_time:138156ms step_avg:61.73ms
step:2239/2330 train_time:138218ms step_avg:61.73ms
step:2240/2330 train_time:138282ms step_avg:61.73ms
step:2241/2330 train_time:138342ms step_avg:61.73ms
step:2242/2330 train_time:138406ms step_avg:61.73ms
step:2243/2330 train_time:138466ms step_avg:61.73ms
step:2244/2330 train_time:138530ms step_avg:61.73ms
step:2245/2330 train_time:138591ms step_avg:61.73ms
step:2246/2330 train_time:138654ms step_avg:61.73ms
step:2247/2330 train_time:138714ms step_avg:61.73ms
step:2248/2330 train_time:138778ms step_avg:61.73ms
step:2249/2330 train_time:138839ms step_avg:61.73ms
step:2250/2330 train_time:138904ms step_avg:61.73ms
step:2250/2330 val_loss:3.3987 train_time:138969ms step_avg:61.76ms
step:2251/2330 train_time:138993ms step_avg:61.75ms
step:2252/2330 train_time:139032ms step_avg:61.74ms
step:2253/2330 train_time:139099ms step_avg:61.74ms
step:2254/2330 train_time:139165ms step_avg:61.74ms
step:2255/2330 train_time:139226ms step_avg:61.74ms
step:2256/2330 train_time:139289ms step_avg:61.74ms
step:2257/2330 train_time:139349ms step_avg:61.74ms
step:2258/2330 train_time:139412ms step_avg:61.74ms
step:2259/2330 train_time:139473ms step_avg:61.74ms
step:2260/2330 train_time:139535ms step_avg:61.74ms
step:2261/2330 train_time:139595ms step_avg:61.74ms
step:2262/2330 train_time:139658ms step_avg:61.74ms
step:2263/2330 train_time:139717ms step_avg:61.74ms
step:2264/2330 train_time:139780ms step_avg:61.74ms
step:2265/2330 train_time:139840ms step_avg:61.74ms
step:2266/2330 train_time:139904ms step_avg:61.74ms
step:2267/2330 train_time:139965ms step_avg:61.74ms
step:2268/2330 train_time:140029ms step_avg:61.74ms
step:2269/2330 train_time:140092ms step_avg:61.74ms
step:2270/2330 train_time:140158ms step_avg:61.74ms
step:2271/2330 train_time:140219ms step_avg:61.74ms
step:2272/2330 train_time:140283ms step_avg:61.74ms
step:2273/2330 train_time:140343ms step_avg:61.74ms
step:2274/2330 train_time:140407ms step_avg:61.74ms
step:2275/2330 train_time:140467ms step_avg:61.74ms
step:2276/2330 train_time:140530ms step_avg:61.74ms
step:2277/2330 train_time:140590ms step_avg:61.74ms
step:2278/2330 train_time:140653ms step_avg:61.74ms
step:2279/2330 train_time:140713ms step_avg:61.74ms
step:2280/2330 train_time:140776ms step_avg:61.74ms
step:2281/2330 train_time:140836ms step_avg:61.74ms
step:2282/2330 train_time:140901ms step_avg:61.74ms
step:2283/2330 train_time:140962ms step_avg:61.74ms
step:2284/2330 train_time:141026ms step_avg:61.75ms
step:2285/2330 train_time:141087ms step_avg:61.74ms
step:2286/2330 train_time:141151ms step_avg:61.75ms
step:2287/2330 train_time:141212ms step_avg:61.75ms
step:2288/2330 train_time:141277ms step_avg:61.75ms
step:2289/2330 train_time:141338ms step_avg:61.75ms
step:2290/2330 train_time:141401ms step_avg:61.75ms
step:2291/2330 train_time:141461ms step_avg:61.75ms
step:2292/2330 train_time:141525ms step_avg:61.75ms
step:2293/2330 train_time:141586ms step_avg:61.75ms
step:2294/2330 train_time:141649ms step_avg:61.75ms
step:2295/2330 train_time:141709ms step_avg:61.75ms
step:2296/2330 train_time:141772ms step_avg:61.75ms
step:2297/2330 train_time:141832ms step_avg:61.75ms
step:2298/2330 train_time:141895ms step_avg:61.75ms
step:2299/2330 train_time:141956ms step_avg:61.75ms
step:2300/2330 train_time:142021ms step_avg:61.75ms
step:2301/2330 train_time:142082ms step_avg:61.75ms
step:2302/2330 train_time:142145ms step_avg:61.75ms
step:2303/2330 train_time:142207ms step_avg:61.75ms
step:2304/2330 train_time:142271ms step_avg:61.75ms
step:2305/2330 train_time:142331ms step_avg:61.75ms
step:2306/2330 train_time:142395ms step_avg:61.75ms
step:2307/2330 train_time:142456ms step_avg:61.75ms
step:2308/2330 train_time:142520ms step_avg:61.75ms
step:2309/2330 train_time:142581ms step_avg:61.75ms
step:2310/2330 train_time:142644ms step_avg:61.75ms
step:2311/2330 train_time:142705ms step_avg:61.75ms
step:2312/2330 train_time:142768ms step_avg:61.75ms
step:2313/2330 train_time:142828ms step_avg:61.75ms
step:2314/2330 train_time:142891ms step_avg:61.75ms
step:2315/2330 train_time:142953ms step_avg:61.75ms
step:2316/2330 train_time:143016ms step_avg:61.75ms
step:2317/2330 train_time:143077ms step_avg:61.75ms
step:2318/2330 train_time:143141ms step_avg:61.75ms
step:2319/2330 train_time:143202ms step_avg:61.75ms
step:2320/2330 train_time:143265ms step_avg:61.75ms
step:2321/2330 train_time:143326ms step_avg:61.75ms
step:2322/2330 train_time:143389ms step_avg:61.75ms
step:2323/2330 train_time:143449ms step_avg:61.75ms
step:2324/2330 train_time:143513ms step_avg:61.75ms
step:2325/2330 train_time:143573ms step_avg:61.75ms
step:2326/2330 train_time:143637ms step_avg:61.75ms
step:2327/2330 train_time:143697ms step_avg:61.75ms
step:2328/2330 train_time:143761ms step_avg:61.75ms
step:2329/2330 train_time:143822ms step_avg:61.75ms
step:2330/2330 train_time:143885ms step_avg:61.75ms
step:2330/2330 val_loss:3.3723 train_time:143951ms step_avg:61.78ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
