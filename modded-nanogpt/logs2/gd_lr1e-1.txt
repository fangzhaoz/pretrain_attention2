import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_lr1e-1"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:51:37 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:77ms step_avg:77.44ms
step:2/2330 train_time:191ms step_avg:95.31ms
step:3/2330 train_time:211ms step_avg:70.49ms
step:4/2330 train_time:241ms step_avg:60.27ms
step:5/2330 train_time:298ms step_avg:59.52ms
step:6/2330 train_time:358ms step_avg:59.70ms
step:7/2330 train_time:415ms step_avg:59.36ms
step:8/2330 train_time:476ms step_avg:59.56ms
step:9/2330 train_time:534ms step_avg:59.34ms
step:10/2330 train_time:596ms step_avg:59.65ms
step:11/2330 train_time:654ms step_avg:59.47ms
step:12/2330 train_time:716ms step_avg:59.63ms
step:13/2330 train_time:773ms step_avg:59.48ms
step:14/2330 train_time:834ms step_avg:59.60ms
step:15/2330 train_time:893ms step_avg:59.54ms
step:16/2330 train_time:955ms step_avg:59.71ms
step:17/2330 train_time:1013ms step_avg:59.59ms
step:18/2330 train_time:1075ms step_avg:59.74ms
step:19/2330 train_time:1137ms step_avg:59.83ms
step:20/2330 train_time:1203ms step_avg:60.14ms
step:21/2330 train_time:1262ms step_avg:60.11ms
step:22/2330 train_time:1324ms step_avg:60.19ms
step:23/2330 train_time:1383ms step_avg:60.13ms
step:24/2330 train_time:1445ms step_avg:60.21ms
step:25/2330 train_time:1503ms step_avg:60.12ms
step:26/2330 train_time:1565ms step_avg:60.18ms
step:27/2330 train_time:1622ms step_avg:60.08ms
step:28/2330 train_time:1684ms step_avg:60.13ms
step:29/2330 train_time:1741ms step_avg:60.04ms
step:30/2330 train_time:1803ms step_avg:60.11ms
step:31/2330 train_time:1861ms step_avg:60.04ms
step:32/2330 train_time:1923ms step_avg:60.10ms
step:33/2330 train_time:1981ms step_avg:60.02ms
step:34/2330 train_time:2043ms step_avg:60.10ms
step:35/2330 train_time:2103ms step_avg:60.08ms
step:36/2330 train_time:2166ms step_avg:60.16ms
step:37/2330 train_time:2224ms step_avg:60.11ms
step:38/2330 train_time:2287ms step_avg:60.17ms
step:39/2330 train_time:2345ms step_avg:60.13ms
step:40/2330 train_time:2407ms step_avg:60.17ms
step:41/2330 train_time:2465ms step_avg:60.13ms
step:42/2330 train_time:2527ms step_avg:60.15ms
step:43/2330 train_time:2585ms step_avg:60.11ms
step:44/2330 train_time:2647ms step_avg:60.15ms
step:45/2330 train_time:2706ms step_avg:60.12ms
step:46/2330 train_time:2768ms step_avg:60.17ms
step:47/2330 train_time:2826ms step_avg:60.13ms
step:48/2330 train_time:2889ms step_avg:60.18ms
step:49/2330 train_time:2948ms step_avg:60.17ms
step:50/2330 train_time:3010ms step_avg:60.21ms
step:51/2330 train_time:3069ms step_avg:60.17ms
step:52/2330 train_time:3130ms step_avg:60.20ms
step:53/2330 train_time:3190ms step_avg:60.19ms
step:54/2330 train_time:3252ms step_avg:60.23ms
step:55/2330 train_time:3310ms step_avg:60.18ms
step:56/2330 train_time:3372ms step_avg:60.22ms
step:57/2330 train_time:3430ms step_avg:60.18ms
step:58/2330 train_time:3493ms step_avg:60.22ms
step:59/2330 train_time:3553ms step_avg:60.22ms
step:60/2330 train_time:3614ms step_avg:60.24ms
step:61/2330 train_time:3673ms step_avg:60.21ms
step:62/2330 train_time:3736ms step_avg:60.26ms
step:63/2330 train_time:3795ms step_avg:60.24ms
step:64/2330 train_time:3857ms step_avg:60.26ms
step:65/2330 train_time:3915ms step_avg:60.24ms
step:66/2330 train_time:3978ms step_avg:60.27ms
step:67/2330 train_time:4036ms step_avg:60.23ms
step:68/2330 train_time:4098ms step_avg:60.26ms
step:69/2330 train_time:4156ms step_avg:60.23ms
step:70/2330 train_time:4218ms step_avg:60.26ms
step:71/2330 train_time:4276ms step_avg:60.23ms
step:72/2330 train_time:4339ms step_avg:60.26ms
step:73/2330 train_time:4397ms step_avg:60.24ms
step:74/2330 train_time:4460ms step_avg:60.27ms
step:75/2330 train_time:4518ms step_avg:60.24ms
step:76/2330 train_time:4581ms step_avg:60.27ms
step:77/2330 train_time:4639ms step_avg:60.25ms
step:78/2330 train_time:4701ms step_avg:60.28ms
step:79/2330 train_time:4760ms step_avg:60.26ms
step:80/2330 train_time:4822ms step_avg:60.28ms
step:81/2330 train_time:4881ms step_avg:60.25ms
step:82/2330 train_time:4942ms step_avg:60.27ms
step:83/2330 train_time:5000ms step_avg:60.24ms
step:84/2330 train_time:5062ms step_avg:60.26ms
step:85/2330 train_time:5120ms step_avg:60.24ms
step:86/2330 train_time:5182ms step_avg:60.25ms
step:87/2330 train_time:5240ms step_avg:60.23ms
step:88/2330 train_time:5302ms step_avg:60.25ms
step:89/2330 train_time:5361ms step_avg:60.23ms
step:90/2330 train_time:5422ms step_avg:60.24ms
step:91/2330 train_time:5480ms step_avg:60.22ms
step:92/2330 train_time:5542ms step_avg:60.24ms
step:93/2330 train_time:5601ms step_avg:60.23ms
step:94/2330 train_time:5664ms step_avg:60.25ms
step:95/2330 train_time:5721ms step_avg:60.22ms
step:96/2330 train_time:5784ms step_avg:60.25ms
step:97/2330 train_time:5841ms step_avg:60.22ms
step:98/2330 train_time:5904ms step_avg:60.24ms
step:99/2330 train_time:5962ms step_avg:60.22ms
step:100/2330 train_time:6023ms step_avg:60.23ms
step:101/2330 train_time:6081ms step_avg:60.21ms
step:102/2330 train_time:6143ms step_avg:60.23ms
step:103/2330 train_time:6201ms step_avg:60.21ms
step:104/2330 train_time:6263ms step_avg:60.22ms
step:105/2330 train_time:6321ms step_avg:60.20ms
step:106/2330 train_time:6384ms step_avg:60.23ms
step:107/2330 train_time:6443ms step_avg:60.21ms
step:108/2330 train_time:6504ms step_avg:60.23ms
step:109/2330 train_time:6563ms step_avg:60.21ms
step:110/2330 train_time:6625ms step_avg:60.23ms
step:111/2330 train_time:6683ms step_avg:60.21ms
step:112/2330 train_time:6746ms step_avg:60.23ms
step:113/2330 train_time:6804ms step_avg:60.21ms
step:114/2330 train_time:6867ms step_avg:60.24ms
step:115/2330 train_time:6925ms step_avg:60.22ms
step:116/2330 train_time:6988ms step_avg:60.24ms
step:117/2330 train_time:7047ms step_avg:60.23ms
step:118/2330 train_time:7109ms step_avg:60.24ms
step:119/2330 train_time:7168ms step_avg:60.23ms
step:120/2330 train_time:7230ms step_avg:60.25ms
step:121/2330 train_time:7288ms step_avg:60.23ms
step:122/2330 train_time:7351ms step_avg:60.25ms
step:123/2330 train_time:7409ms step_avg:60.24ms
step:124/2330 train_time:7471ms step_avg:60.25ms
step:125/2330 train_time:7530ms step_avg:60.24ms
step:126/2330 train_time:7592ms step_avg:60.26ms
step:127/2330 train_time:7651ms step_avg:60.25ms
step:128/2330 train_time:7713ms step_avg:60.26ms
step:129/2330 train_time:7772ms step_avg:60.25ms
step:130/2330 train_time:7834ms step_avg:60.26ms
step:131/2330 train_time:7893ms step_avg:60.25ms
step:132/2330 train_time:7956ms step_avg:60.27ms
step:133/2330 train_time:8015ms step_avg:60.26ms
step:134/2330 train_time:8077ms step_avg:60.28ms
step:135/2330 train_time:8135ms step_avg:60.26ms
step:136/2330 train_time:8197ms step_avg:60.27ms
step:137/2330 train_time:8255ms step_avg:60.26ms
step:138/2330 train_time:8318ms step_avg:60.27ms
step:139/2330 train_time:8376ms step_avg:60.26ms
step:140/2330 train_time:8439ms step_avg:60.28ms
step:141/2330 train_time:8497ms step_avg:60.26ms
step:142/2330 train_time:8560ms step_avg:60.28ms
step:143/2330 train_time:8618ms step_avg:60.27ms
step:144/2330 train_time:8681ms step_avg:60.28ms
step:145/2330 train_time:8739ms step_avg:60.27ms
step:146/2330 train_time:8802ms step_avg:60.28ms
step:147/2330 train_time:8861ms step_avg:60.28ms
step:148/2330 train_time:8922ms step_avg:60.28ms
step:149/2330 train_time:8980ms step_avg:60.27ms
step:150/2330 train_time:9043ms step_avg:60.29ms
step:151/2330 train_time:9101ms step_avg:60.27ms
step:152/2330 train_time:9163ms step_avg:60.28ms
step:153/2330 train_time:9221ms step_avg:60.27ms
step:154/2330 train_time:9283ms step_avg:60.28ms
step:155/2330 train_time:9340ms step_avg:60.26ms
step:156/2330 train_time:9403ms step_avg:60.27ms
step:157/2330 train_time:9461ms step_avg:60.26ms
step:158/2330 train_time:9522ms step_avg:60.27ms
step:159/2330 train_time:9581ms step_avg:60.26ms
step:160/2330 train_time:9644ms step_avg:60.27ms
step:161/2330 train_time:9702ms step_avg:60.26ms
step:162/2330 train_time:9765ms step_avg:60.28ms
step:163/2330 train_time:9823ms step_avg:60.26ms
step:164/2330 train_time:9885ms step_avg:60.27ms
step:165/2330 train_time:9943ms step_avg:60.26ms
step:166/2330 train_time:10007ms step_avg:60.28ms
step:167/2330 train_time:10065ms step_avg:60.27ms
step:168/2330 train_time:10126ms step_avg:60.27ms
step:169/2330 train_time:10184ms step_avg:60.26ms
step:170/2330 train_time:10246ms step_avg:60.27ms
step:171/2330 train_time:10305ms step_avg:60.26ms
step:172/2330 train_time:10367ms step_avg:60.28ms
step:173/2330 train_time:10426ms step_avg:60.26ms
step:174/2330 train_time:10488ms step_avg:60.28ms
step:175/2330 train_time:10547ms step_avg:60.27ms
step:176/2330 train_time:10610ms step_avg:60.28ms
step:177/2330 train_time:10669ms step_avg:60.28ms
step:178/2330 train_time:10731ms step_avg:60.28ms
step:179/2330 train_time:10789ms step_avg:60.27ms
step:180/2330 train_time:10852ms step_avg:60.29ms
step:181/2330 train_time:10911ms step_avg:60.28ms
step:182/2330 train_time:10973ms step_avg:60.29ms
step:183/2330 train_time:11032ms step_avg:60.28ms
step:184/2330 train_time:11094ms step_avg:60.29ms
step:185/2330 train_time:11154ms step_avg:60.29ms
step:186/2330 train_time:11216ms step_avg:60.30ms
step:187/2330 train_time:11274ms step_avg:60.29ms
step:188/2330 train_time:11336ms step_avg:60.30ms
step:189/2330 train_time:11396ms step_avg:60.29ms
step:190/2330 train_time:11458ms step_avg:60.30ms
step:191/2330 train_time:11516ms step_avg:60.29ms
step:192/2330 train_time:11578ms step_avg:60.30ms
step:193/2330 train_time:11636ms step_avg:60.29ms
step:194/2330 train_time:11698ms step_avg:60.30ms
step:195/2330 train_time:11756ms step_avg:60.29ms
step:196/2330 train_time:11818ms step_avg:60.30ms
step:197/2330 train_time:11877ms step_avg:60.29ms
step:198/2330 train_time:11940ms step_avg:60.30ms
step:199/2330 train_time:11998ms step_avg:60.29ms
step:200/2330 train_time:12061ms step_avg:60.31ms
step:201/2330 train_time:12120ms step_avg:60.30ms
step:202/2330 train_time:12182ms step_avg:60.31ms
step:203/2330 train_time:12241ms step_avg:60.30ms
step:204/2330 train_time:12303ms step_avg:60.31ms
step:205/2330 train_time:12362ms step_avg:60.30ms
step:206/2330 train_time:12424ms step_avg:60.31ms
step:207/2330 train_time:12482ms step_avg:60.30ms
step:208/2330 train_time:12544ms step_avg:60.31ms
step:209/2330 train_time:12602ms step_avg:60.30ms
step:210/2330 train_time:12664ms step_avg:60.31ms
step:211/2330 train_time:12722ms step_avg:60.29ms
step:212/2330 train_time:12784ms step_avg:60.30ms
step:213/2330 train_time:12842ms step_avg:60.29ms
step:214/2330 train_time:12905ms step_avg:60.30ms
step:215/2330 train_time:12963ms step_avg:60.29ms
step:216/2330 train_time:13025ms step_avg:60.30ms
step:217/2330 train_time:13084ms step_avg:60.29ms
step:218/2330 train_time:13145ms step_avg:60.30ms
step:219/2330 train_time:13204ms step_avg:60.29ms
step:220/2330 train_time:13267ms step_avg:60.30ms
step:221/2330 train_time:13325ms step_avg:60.30ms
step:222/2330 train_time:13387ms step_avg:60.30ms
step:223/2330 train_time:13446ms step_avg:60.30ms
step:224/2330 train_time:13508ms step_avg:60.30ms
step:225/2330 train_time:13567ms step_avg:60.30ms
step:226/2330 train_time:13628ms step_avg:60.30ms
step:227/2330 train_time:13687ms step_avg:60.30ms
step:228/2330 train_time:13750ms step_avg:60.31ms
step:229/2330 train_time:13809ms step_avg:60.30ms
step:230/2330 train_time:13871ms step_avg:60.31ms
step:231/2330 train_time:13929ms step_avg:60.30ms
step:232/2330 train_time:13991ms step_avg:60.31ms
step:233/2330 train_time:14050ms step_avg:60.30ms
step:234/2330 train_time:14112ms step_avg:60.31ms
step:235/2330 train_time:14171ms step_avg:60.30ms
step:236/2330 train_time:14234ms step_avg:60.31ms
step:237/2330 train_time:14294ms step_avg:60.31ms
step:238/2330 train_time:14356ms step_avg:60.32ms
step:239/2330 train_time:14415ms step_avg:60.31ms
step:240/2330 train_time:14477ms step_avg:60.32ms
step:241/2330 train_time:14536ms step_avg:60.32ms
step:242/2330 train_time:14599ms step_avg:60.33ms
step:243/2330 train_time:14658ms step_avg:60.32ms
step:244/2330 train_time:14720ms step_avg:60.33ms
step:245/2330 train_time:14778ms step_avg:60.32ms
step:246/2330 train_time:14840ms step_avg:60.33ms
step:247/2330 train_time:14898ms step_avg:60.32ms
step:248/2330 train_time:14960ms step_avg:60.32ms
step:249/2330 train_time:15018ms step_avg:60.31ms
step:250/2330 train_time:15080ms step_avg:60.32ms
step:250/2330 val_loss:4.6337 train_time:15152ms step_avg:60.61ms
step:251/2330 train_time:15174ms step_avg:60.45ms
step:252/2330 train_time:15204ms step_avg:60.33ms
step:253/2330 train_time:15263ms step_avg:60.33ms
step:254/2330 train_time:15333ms step_avg:60.37ms
step:255/2330 train_time:15392ms step_avg:60.36ms
step:256/2330 train_time:15454ms step_avg:60.37ms
step:257/2330 train_time:15512ms step_avg:60.36ms
step:258/2330 train_time:15574ms step_avg:60.37ms
step:259/2330 train_time:15633ms step_avg:60.36ms
step:260/2330 train_time:15693ms step_avg:60.36ms
step:261/2330 train_time:15751ms step_avg:60.35ms
step:262/2330 train_time:15812ms step_avg:60.35ms
step:263/2330 train_time:15870ms step_avg:60.34ms
step:264/2330 train_time:15931ms step_avg:60.35ms
step:265/2330 train_time:15989ms step_avg:60.33ms
step:266/2330 train_time:16051ms step_avg:60.34ms
step:267/2330 train_time:16111ms step_avg:60.34ms
step:268/2330 train_time:16176ms step_avg:60.36ms
step:269/2330 train_time:16237ms step_avg:60.36ms
step:270/2330 train_time:16299ms step_avg:60.37ms
step:271/2330 train_time:16359ms step_avg:60.36ms
step:272/2330 train_time:16422ms step_avg:60.37ms
step:273/2330 train_time:16480ms step_avg:60.37ms
step:274/2330 train_time:16542ms step_avg:60.37ms
step:275/2330 train_time:16600ms step_avg:60.36ms
step:276/2330 train_time:16663ms step_avg:60.37ms
step:277/2330 train_time:16722ms step_avg:60.37ms
step:278/2330 train_time:16785ms step_avg:60.38ms
step:279/2330 train_time:16842ms step_avg:60.37ms
step:280/2330 train_time:16905ms step_avg:60.38ms
step:281/2330 train_time:16963ms step_avg:60.37ms
step:282/2330 train_time:17025ms step_avg:60.37ms
step:283/2330 train_time:17083ms step_avg:60.36ms
step:284/2330 train_time:17146ms step_avg:60.37ms
step:285/2330 train_time:17204ms step_avg:60.37ms
step:286/2330 train_time:17267ms step_avg:60.37ms
step:287/2330 train_time:17327ms step_avg:60.37ms
step:288/2330 train_time:17390ms step_avg:60.38ms
step:289/2330 train_time:17449ms step_avg:60.38ms
step:290/2330 train_time:17511ms step_avg:60.38ms
step:291/2330 train_time:17569ms step_avg:60.37ms
step:292/2330 train_time:17631ms step_avg:60.38ms
step:293/2330 train_time:17689ms step_avg:60.37ms
step:294/2330 train_time:17752ms step_avg:60.38ms
step:295/2330 train_time:17810ms step_avg:60.37ms
step:296/2330 train_time:17871ms step_avg:60.38ms
step:297/2330 train_time:17930ms step_avg:60.37ms
step:298/2330 train_time:17991ms step_avg:60.37ms
step:299/2330 train_time:18050ms step_avg:60.37ms
step:300/2330 train_time:18112ms step_avg:60.37ms
step:301/2330 train_time:18171ms step_avg:60.37ms
step:302/2330 train_time:18234ms step_avg:60.38ms
step:303/2330 train_time:18293ms step_avg:60.37ms
step:304/2330 train_time:18355ms step_avg:60.38ms
step:305/2330 train_time:18414ms step_avg:60.37ms
step:306/2330 train_time:18476ms step_avg:60.38ms
step:307/2330 train_time:18535ms step_avg:60.37ms
step:308/2330 train_time:18596ms step_avg:60.38ms
step:309/2330 train_time:18655ms step_avg:60.37ms
step:310/2330 train_time:18717ms step_avg:60.38ms
step:311/2330 train_time:18776ms step_avg:60.37ms
step:312/2330 train_time:18838ms step_avg:60.38ms
step:313/2330 train_time:18898ms step_avg:60.38ms
step:314/2330 train_time:18960ms step_avg:60.38ms
step:315/2330 train_time:19020ms step_avg:60.38ms
step:316/2330 train_time:19081ms step_avg:60.38ms
step:317/2330 train_time:19140ms step_avg:60.38ms
step:318/2330 train_time:19202ms step_avg:60.38ms
step:319/2330 train_time:19261ms step_avg:60.38ms
step:320/2330 train_time:19324ms step_avg:60.39ms
step:321/2330 train_time:19382ms step_avg:60.38ms
step:322/2330 train_time:19443ms step_avg:60.38ms
step:323/2330 train_time:19502ms step_avg:60.38ms
step:324/2330 train_time:19566ms step_avg:60.39ms
step:325/2330 train_time:19624ms step_avg:60.38ms
step:326/2330 train_time:19686ms step_avg:60.39ms
step:327/2330 train_time:19745ms step_avg:60.38ms
step:328/2330 train_time:19807ms step_avg:60.39ms
step:329/2330 train_time:19865ms step_avg:60.38ms
step:330/2330 train_time:19928ms step_avg:60.39ms
step:331/2330 train_time:19987ms step_avg:60.38ms
step:332/2330 train_time:20050ms step_avg:60.39ms
step:333/2330 train_time:20107ms step_avg:60.38ms
step:334/2330 train_time:20169ms step_avg:60.39ms
step:335/2330 train_time:20227ms step_avg:60.38ms
step:336/2330 train_time:20289ms step_avg:60.38ms
step:337/2330 train_time:20347ms step_avg:60.38ms
step:338/2330 train_time:20409ms step_avg:60.38ms
step:339/2330 train_time:20468ms step_avg:60.38ms
step:340/2330 train_time:20530ms step_avg:60.38ms
step:341/2330 train_time:20587ms step_avg:60.37ms
step:342/2330 train_time:20649ms step_avg:60.38ms
step:343/2330 train_time:20707ms step_avg:60.37ms
step:344/2330 train_time:20769ms step_avg:60.38ms
step:345/2330 train_time:20828ms step_avg:60.37ms
step:346/2330 train_time:20890ms step_avg:60.38ms
step:347/2330 train_time:20948ms step_avg:60.37ms
step:348/2330 train_time:21010ms step_avg:60.37ms
step:349/2330 train_time:21070ms step_avg:60.37ms
step:350/2330 train_time:21131ms step_avg:60.38ms
step:351/2330 train_time:21189ms step_avg:60.37ms
step:352/2330 train_time:21251ms step_avg:60.37ms
step:353/2330 train_time:21309ms step_avg:60.37ms
step:354/2330 train_time:21371ms step_avg:60.37ms
step:355/2330 train_time:21430ms step_avg:60.37ms
step:356/2330 train_time:21492ms step_avg:60.37ms
step:357/2330 train_time:21551ms step_avg:60.37ms
step:358/2330 train_time:21613ms step_avg:60.37ms
step:359/2330 train_time:21671ms step_avg:60.37ms
step:360/2330 train_time:21733ms step_avg:60.37ms
step:361/2330 train_time:21791ms step_avg:60.36ms
step:362/2330 train_time:21854ms step_avg:60.37ms
step:363/2330 train_time:21914ms step_avg:60.37ms
step:364/2330 train_time:21976ms step_avg:60.37ms
step:365/2330 train_time:22035ms step_avg:60.37ms
step:366/2330 train_time:22097ms step_avg:60.37ms
step:367/2330 train_time:22155ms step_avg:60.37ms
step:368/2330 train_time:22218ms step_avg:60.38ms
step:369/2330 train_time:22277ms step_avg:60.37ms
step:370/2330 train_time:22339ms step_avg:60.38ms
step:371/2330 train_time:22398ms step_avg:60.37ms
step:372/2330 train_time:22460ms step_avg:60.38ms
step:373/2330 train_time:22520ms step_avg:60.37ms
step:374/2330 train_time:22582ms step_avg:60.38ms
step:375/2330 train_time:22640ms step_avg:60.37ms
step:376/2330 train_time:22702ms step_avg:60.38ms
step:377/2330 train_time:22761ms step_avg:60.37ms
step:378/2330 train_time:22823ms step_avg:60.38ms
step:379/2330 train_time:22882ms step_avg:60.38ms
step:380/2330 train_time:22944ms step_avg:60.38ms
step:381/2330 train_time:23003ms step_avg:60.38ms
step:382/2330 train_time:23065ms step_avg:60.38ms
step:383/2330 train_time:23124ms step_avg:60.38ms
step:384/2330 train_time:23187ms step_avg:60.38ms
step:385/2330 train_time:23246ms step_avg:60.38ms
step:386/2330 train_time:23309ms step_avg:60.39ms
step:387/2330 train_time:23367ms step_avg:60.38ms
step:388/2330 train_time:23430ms step_avg:60.39ms
step:389/2330 train_time:23488ms step_avg:60.38ms
step:390/2330 train_time:23550ms step_avg:60.39ms
step:391/2330 train_time:23608ms step_avg:60.38ms
step:392/2330 train_time:23670ms step_avg:60.38ms
step:393/2330 train_time:23729ms step_avg:60.38ms
step:394/2330 train_time:23791ms step_avg:60.38ms
step:395/2330 train_time:23849ms step_avg:60.38ms
step:396/2330 train_time:23911ms step_avg:60.38ms
step:397/2330 train_time:23969ms step_avg:60.38ms
step:398/2330 train_time:24032ms step_avg:60.38ms
step:399/2330 train_time:24090ms step_avg:60.38ms
step:400/2330 train_time:24153ms step_avg:60.38ms
step:401/2330 train_time:24212ms step_avg:60.38ms
step:402/2330 train_time:24274ms step_avg:60.38ms
step:403/2330 train_time:24333ms step_avg:60.38ms
step:404/2330 train_time:24395ms step_avg:60.38ms
step:405/2330 train_time:24454ms step_avg:60.38ms
step:406/2330 train_time:24516ms step_avg:60.38ms
step:407/2330 train_time:24574ms step_avg:60.38ms
step:408/2330 train_time:24636ms step_avg:60.38ms
step:409/2330 train_time:24694ms step_avg:60.38ms
step:410/2330 train_time:24756ms step_avg:60.38ms
step:411/2330 train_time:24815ms step_avg:60.38ms
step:412/2330 train_time:24878ms step_avg:60.38ms
step:413/2330 train_time:24936ms step_avg:60.38ms
step:414/2330 train_time:24999ms step_avg:60.38ms
step:415/2330 train_time:25058ms step_avg:60.38ms
step:416/2330 train_time:25121ms step_avg:60.39ms
step:417/2330 train_time:25179ms step_avg:60.38ms
step:418/2330 train_time:25241ms step_avg:60.38ms
step:419/2330 train_time:25300ms step_avg:60.38ms
step:420/2330 train_time:25362ms step_avg:60.39ms
step:421/2330 train_time:25422ms step_avg:60.38ms
step:422/2330 train_time:25483ms step_avg:60.39ms
step:423/2330 train_time:25542ms step_avg:60.38ms
step:424/2330 train_time:25603ms step_avg:60.38ms
step:425/2330 train_time:25662ms step_avg:60.38ms
step:426/2330 train_time:25724ms step_avg:60.39ms
step:427/2330 train_time:25782ms step_avg:60.38ms
step:428/2330 train_time:25844ms step_avg:60.38ms
step:429/2330 train_time:25903ms step_avg:60.38ms
step:430/2330 train_time:25966ms step_avg:60.39ms
step:431/2330 train_time:26024ms step_avg:60.38ms
step:432/2330 train_time:26086ms step_avg:60.39ms
step:433/2330 train_time:26144ms step_avg:60.38ms
step:434/2330 train_time:26207ms step_avg:60.38ms
step:435/2330 train_time:26265ms step_avg:60.38ms
step:436/2330 train_time:26327ms step_avg:60.38ms
step:437/2330 train_time:26385ms step_avg:60.38ms
step:438/2330 train_time:26447ms step_avg:60.38ms
step:439/2330 train_time:26505ms step_avg:60.38ms
step:440/2330 train_time:26568ms step_avg:60.38ms
step:441/2330 train_time:26626ms step_avg:60.38ms
step:442/2330 train_time:26688ms step_avg:60.38ms
step:443/2330 train_time:26746ms step_avg:60.37ms
step:444/2330 train_time:26808ms step_avg:60.38ms
step:445/2330 train_time:26866ms step_avg:60.37ms
step:446/2330 train_time:26929ms step_avg:60.38ms
step:447/2330 train_time:26987ms step_avg:60.37ms
step:448/2330 train_time:27049ms step_avg:60.38ms
step:449/2330 train_time:27108ms step_avg:60.37ms
step:450/2330 train_time:27170ms step_avg:60.38ms
step:451/2330 train_time:27228ms step_avg:60.37ms
step:452/2330 train_time:27290ms step_avg:60.38ms
step:453/2330 train_time:27349ms step_avg:60.37ms
step:454/2330 train_time:27411ms step_avg:60.38ms
step:455/2330 train_time:27470ms step_avg:60.37ms
step:456/2330 train_time:27532ms step_avg:60.38ms
step:457/2330 train_time:27590ms step_avg:60.37ms
step:458/2330 train_time:27652ms step_avg:60.37ms
step:459/2330 train_time:27710ms step_avg:60.37ms
step:460/2330 train_time:27772ms step_avg:60.37ms
step:461/2330 train_time:27831ms step_avg:60.37ms
step:462/2330 train_time:27892ms step_avg:60.37ms
step:463/2330 train_time:27950ms step_avg:60.37ms
step:464/2330 train_time:28013ms step_avg:60.37ms
step:465/2330 train_time:28071ms step_avg:60.37ms
step:466/2330 train_time:28133ms step_avg:60.37ms
step:467/2330 train_time:28192ms step_avg:60.37ms
step:468/2330 train_time:28253ms step_avg:60.37ms
step:469/2330 train_time:28312ms step_avg:60.37ms
step:470/2330 train_time:28374ms step_avg:60.37ms
step:471/2330 train_time:28432ms step_avg:60.37ms
step:472/2330 train_time:28493ms step_avg:60.37ms
step:473/2330 train_time:28553ms step_avg:60.37ms
step:474/2330 train_time:28616ms step_avg:60.37ms
step:475/2330 train_time:28675ms step_avg:60.37ms
step:476/2330 train_time:28736ms step_avg:60.37ms
step:477/2330 train_time:28795ms step_avg:60.37ms
step:478/2330 train_time:28857ms step_avg:60.37ms
step:479/2330 train_time:28916ms step_avg:60.37ms
step:480/2330 train_time:28978ms step_avg:60.37ms
step:481/2330 train_time:29037ms step_avg:60.37ms
step:482/2330 train_time:29099ms step_avg:60.37ms
step:483/2330 train_time:29158ms step_avg:60.37ms
step:484/2330 train_time:29221ms step_avg:60.37ms
step:485/2330 train_time:29279ms step_avg:60.37ms
step:486/2330 train_time:29341ms step_avg:60.37ms
step:487/2330 train_time:29400ms step_avg:60.37ms
step:488/2330 train_time:29462ms step_avg:60.37ms
step:489/2330 train_time:29521ms step_avg:60.37ms
step:490/2330 train_time:29584ms step_avg:60.38ms
step:491/2330 train_time:29643ms step_avg:60.37ms
step:492/2330 train_time:29704ms step_avg:60.37ms
step:493/2330 train_time:29763ms step_avg:60.37ms
step:494/2330 train_time:29825ms step_avg:60.37ms
step:495/2330 train_time:29883ms step_avg:60.37ms
step:496/2330 train_time:29945ms step_avg:60.37ms
step:497/2330 train_time:30004ms step_avg:60.37ms
step:498/2330 train_time:30066ms step_avg:60.37ms
step:499/2330 train_time:30124ms step_avg:60.37ms
step:500/2330 train_time:30187ms step_avg:60.37ms
step:500/2330 val_loss:4.1196 train_time:30257ms step_avg:60.51ms
step:501/2330 train_time:30277ms step_avg:60.43ms
step:502/2330 train_time:30310ms step_avg:60.38ms
step:503/2330 train_time:30369ms step_avg:60.38ms
step:504/2330 train_time:30436ms step_avg:60.39ms
step:505/2330 train_time:30497ms step_avg:60.39ms
step:506/2330 train_time:30560ms step_avg:60.40ms
step:507/2330 train_time:30619ms step_avg:60.39ms
step:508/2330 train_time:30681ms step_avg:60.39ms
step:509/2330 train_time:30738ms step_avg:60.39ms
step:510/2330 train_time:30801ms step_avg:60.39ms
step:511/2330 train_time:30859ms step_avg:60.39ms
step:512/2330 train_time:30921ms step_avg:60.39ms
step:513/2330 train_time:30979ms step_avg:60.39ms
step:514/2330 train_time:31041ms step_avg:60.39ms
step:515/2330 train_time:31099ms step_avg:60.39ms
step:516/2330 train_time:31161ms step_avg:60.39ms
step:517/2330 train_time:31219ms step_avg:60.38ms
step:518/2330 train_time:31283ms step_avg:60.39ms
step:519/2330 train_time:31343ms step_avg:60.39ms
step:520/2330 train_time:31407ms step_avg:60.40ms
step:521/2330 train_time:31466ms step_avg:60.40ms
step:522/2330 train_time:31529ms step_avg:60.40ms
step:523/2330 train_time:31587ms step_avg:60.40ms
step:524/2330 train_time:31649ms step_avg:60.40ms
step:525/2330 train_time:31707ms step_avg:60.40ms
step:526/2330 train_time:31770ms step_avg:60.40ms
step:527/2330 train_time:31827ms step_avg:60.39ms
step:528/2330 train_time:31889ms step_avg:60.40ms
step:529/2330 train_time:31947ms step_avg:60.39ms
step:530/2330 train_time:32009ms step_avg:60.39ms
step:531/2330 train_time:32067ms step_avg:60.39ms
step:532/2330 train_time:32129ms step_avg:60.39ms
step:533/2330 train_time:32187ms step_avg:60.39ms
step:534/2330 train_time:32250ms step_avg:60.39ms
step:535/2330 train_time:32308ms step_avg:60.39ms
step:536/2330 train_time:32370ms step_avg:60.39ms
step:537/2330 train_time:32428ms step_avg:60.39ms
step:538/2330 train_time:32490ms step_avg:60.39ms
step:539/2330 train_time:32549ms step_avg:60.39ms
step:540/2330 train_time:32611ms step_avg:60.39ms
step:541/2330 train_time:32670ms step_avg:60.39ms
step:542/2330 train_time:32732ms step_avg:60.39ms
step:543/2330 train_time:32791ms step_avg:60.39ms
step:544/2330 train_time:32852ms step_avg:60.39ms
step:545/2330 train_time:32912ms step_avg:60.39ms
step:546/2330 train_time:32973ms step_avg:60.39ms
step:547/2330 train_time:33032ms step_avg:60.39ms
step:548/2330 train_time:33094ms step_avg:60.39ms
step:549/2330 train_time:33152ms step_avg:60.39ms
step:550/2330 train_time:33214ms step_avg:60.39ms
step:551/2330 train_time:33272ms step_avg:60.39ms
step:552/2330 train_time:33334ms step_avg:60.39ms
step:553/2330 train_time:33395ms step_avg:60.39ms
step:554/2330 train_time:33457ms step_avg:60.39ms
step:555/2330 train_time:33516ms step_avg:60.39ms
step:556/2330 train_time:33578ms step_avg:60.39ms
step:557/2330 train_time:33637ms step_avg:60.39ms
step:558/2330 train_time:33699ms step_avg:60.39ms
step:559/2330 train_time:33758ms step_avg:60.39ms
step:560/2330 train_time:33821ms step_avg:60.39ms
step:561/2330 train_time:33879ms step_avg:60.39ms
step:562/2330 train_time:33942ms step_avg:60.40ms
step:563/2330 train_time:34001ms step_avg:60.39ms
step:564/2330 train_time:34064ms step_avg:60.40ms
step:565/2330 train_time:34122ms step_avg:60.39ms
step:566/2330 train_time:34184ms step_avg:60.40ms
step:567/2330 train_time:34243ms step_avg:60.39ms
step:568/2330 train_time:34305ms step_avg:60.40ms
step:569/2330 train_time:34364ms step_avg:60.39ms
step:570/2330 train_time:34426ms step_avg:60.40ms
step:571/2330 train_time:34484ms step_avg:60.39ms
step:572/2330 train_time:34547ms step_avg:60.40ms
step:573/2330 train_time:34605ms step_avg:60.39ms
step:574/2330 train_time:34668ms step_avg:60.40ms
step:575/2330 train_time:34727ms step_avg:60.39ms
step:576/2330 train_time:34789ms step_avg:60.40ms
step:577/2330 train_time:34847ms step_avg:60.39ms
step:578/2330 train_time:34910ms step_avg:60.40ms
step:579/2330 train_time:34968ms step_avg:60.39ms
step:580/2330 train_time:35030ms step_avg:60.40ms
step:581/2330 train_time:35088ms step_avg:60.39ms
step:582/2330 train_time:35150ms step_avg:60.39ms
step:583/2330 train_time:35208ms step_avg:60.39ms
step:584/2330 train_time:35270ms step_avg:60.39ms
step:585/2330 train_time:35327ms step_avg:60.39ms
step:586/2330 train_time:35390ms step_avg:60.39ms
step:587/2330 train_time:35448ms step_avg:60.39ms
step:588/2330 train_time:35510ms step_avg:60.39ms
step:589/2330 train_time:35569ms step_avg:60.39ms
step:590/2330 train_time:35631ms step_avg:60.39ms
step:591/2330 train_time:35690ms step_avg:60.39ms
step:592/2330 train_time:35752ms step_avg:60.39ms
step:593/2330 train_time:35811ms step_avg:60.39ms
step:594/2330 train_time:35873ms step_avg:60.39ms
step:595/2330 train_time:35932ms step_avg:60.39ms
step:596/2330 train_time:35994ms step_avg:60.39ms
step:597/2330 train_time:36053ms step_avg:60.39ms
step:598/2330 train_time:36115ms step_avg:60.39ms
step:599/2330 train_time:36174ms step_avg:60.39ms
step:600/2330 train_time:36235ms step_avg:60.39ms
step:601/2330 train_time:36293ms step_avg:60.39ms
step:602/2330 train_time:36355ms step_avg:60.39ms
step:603/2330 train_time:36414ms step_avg:60.39ms
step:604/2330 train_time:36477ms step_avg:60.39ms
step:605/2330 train_time:36535ms step_avg:60.39ms
step:606/2330 train_time:36598ms step_avg:60.39ms
step:607/2330 train_time:36657ms step_avg:60.39ms
step:608/2330 train_time:36719ms step_avg:60.39ms
step:609/2330 train_time:36778ms step_avg:60.39ms
step:610/2330 train_time:36840ms step_avg:60.39ms
step:611/2330 train_time:36900ms step_avg:60.39ms
step:612/2330 train_time:36962ms step_avg:60.40ms
step:613/2330 train_time:37021ms step_avg:60.39ms
step:614/2330 train_time:37084ms step_avg:60.40ms
step:615/2330 train_time:37142ms step_avg:60.39ms
step:616/2330 train_time:37204ms step_avg:60.40ms
step:617/2330 train_time:37263ms step_avg:60.39ms
step:618/2330 train_time:37325ms step_avg:60.40ms
step:619/2330 train_time:37383ms step_avg:60.39ms
step:620/2330 train_time:37446ms step_avg:60.40ms
step:621/2330 train_time:37505ms step_avg:60.39ms
step:622/2330 train_time:37567ms step_avg:60.40ms
step:623/2330 train_time:37625ms step_avg:60.39ms
step:624/2330 train_time:37688ms step_avg:60.40ms
step:625/2330 train_time:37746ms step_avg:60.39ms
step:626/2330 train_time:37808ms step_avg:60.40ms
step:627/2330 train_time:37867ms step_avg:60.39ms
step:628/2330 train_time:37929ms step_avg:60.40ms
step:629/2330 train_time:37987ms step_avg:60.39ms
step:630/2330 train_time:38049ms step_avg:60.40ms
step:631/2330 train_time:38107ms step_avg:60.39ms
step:632/2330 train_time:38169ms step_avg:60.39ms
step:633/2330 train_time:38227ms step_avg:60.39ms
step:634/2330 train_time:38289ms step_avg:60.39ms
step:635/2330 train_time:38347ms step_avg:60.39ms
step:636/2330 train_time:38409ms step_avg:60.39ms
step:637/2330 train_time:38467ms step_avg:60.39ms
step:638/2330 train_time:38529ms step_avg:60.39ms
step:639/2330 train_time:38587ms step_avg:60.39ms
step:640/2330 train_time:38650ms step_avg:60.39ms
step:641/2330 train_time:38708ms step_avg:60.39ms
step:642/2330 train_time:38771ms step_avg:60.39ms
step:643/2330 train_time:38829ms step_avg:60.39ms
step:644/2330 train_time:38891ms step_avg:60.39ms
step:645/2330 train_time:38950ms step_avg:60.39ms
step:646/2330 train_time:39012ms step_avg:60.39ms
step:647/2330 train_time:39071ms step_avg:60.39ms
step:648/2330 train_time:39132ms step_avg:60.39ms
step:649/2330 train_time:39190ms step_avg:60.39ms
step:650/2330 train_time:39252ms step_avg:60.39ms
step:651/2330 train_time:39311ms step_avg:60.39ms
step:652/2330 train_time:39373ms step_avg:60.39ms
step:653/2330 train_time:39432ms step_avg:60.39ms
step:654/2330 train_time:39494ms step_avg:60.39ms
step:655/2330 train_time:39553ms step_avg:60.39ms
step:656/2330 train_time:39615ms step_avg:60.39ms
step:657/2330 train_time:39674ms step_avg:60.39ms
step:658/2330 train_time:39737ms step_avg:60.39ms
step:659/2330 train_time:39795ms step_avg:60.39ms
step:660/2330 train_time:39858ms step_avg:60.39ms
step:661/2330 train_time:39916ms step_avg:60.39ms
step:662/2330 train_time:39978ms step_avg:60.39ms
step:663/2330 train_time:40037ms step_avg:60.39ms
step:664/2330 train_time:40099ms step_avg:60.39ms
step:665/2330 train_time:40157ms step_avg:60.39ms
step:666/2330 train_time:40221ms step_avg:60.39ms
step:667/2330 train_time:40280ms step_avg:60.39ms
step:668/2330 train_time:40343ms step_avg:60.39ms
step:669/2330 train_time:40402ms step_avg:60.39ms
step:670/2330 train_time:40463ms step_avg:60.39ms
step:671/2330 train_time:40522ms step_avg:60.39ms
step:672/2330 train_time:40585ms step_avg:60.39ms
step:673/2330 train_time:40643ms step_avg:60.39ms
step:674/2330 train_time:40705ms step_avg:60.39ms
step:675/2330 train_time:40764ms step_avg:60.39ms
step:676/2330 train_time:40826ms step_avg:60.39ms
step:677/2330 train_time:40884ms step_avg:60.39ms
step:678/2330 train_time:40946ms step_avg:60.39ms
step:679/2330 train_time:41005ms step_avg:60.39ms
step:680/2330 train_time:41068ms step_avg:60.39ms
step:681/2330 train_time:41126ms step_avg:60.39ms
step:682/2330 train_time:41188ms step_avg:60.39ms
step:683/2330 train_time:41247ms step_avg:60.39ms
step:684/2330 train_time:41310ms step_avg:60.39ms
step:685/2330 train_time:41368ms step_avg:60.39ms
step:686/2330 train_time:41430ms step_avg:60.39ms
step:687/2330 train_time:41489ms step_avg:60.39ms
step:688/2330 train_time:41550ms step_avg:60.39ms
step:689/2330 train_time:41608ms step_avg:60.39ms
step:690/2330 train_time:41671ms step_avg:60.39ms
step:691/2330 train_time:41728ms step_avg:60.39ms
step:692/2330 train_time:41791ms step_avg:60.39ms
step:693/2330 train_time:41849ms step_avg:60.39ms
step:694/2330 train_time:41912ms step_avg:60.39ms
step:695/2330 train_time:41971ms step_avg:60.39ms
step:696/2330 train_time:42032ms step_avg:60.39ms
step:697/2330 train_time:42092ms step_avg:60.39ms
step:698/2330 train_time:42154ms step_avg:60.39ms
step:699/2330 train_time:42213ms step_avg:60.39ms
step:700/2330 train_time:42275ms step_avg:60.39ms
step:701/2330 train_time:42333ms step_avg:60.39ms
step:702/2330 train_time:42395ms step_avg:60.39ms
step:703/2330 train_time:42453ms step_avg:60.39ms
step:704/2330 train_time:42516ms step_avg:60.39ms
step:705/2330 train_time:42574ms step_avg:60.39ms
step:706/2330 train_time:42636ms step_avg:60.39ms
step:707/2330 train_time:42695ms step_avg:60.39ms
step:708/2330 train_time:42757ms step_avg:60.39ms
step:709/2330 train_time:42815ms step_avg:60.39ms
step:710/2330 train_time:42878ms step_avg:60.39ms
step:711/2330 train_time:42937ms step_avg:60.39ms
step:712/2330 train_time:42999ms step_avg:60.39ms
step:713/2330 train_time:43058ms step_avg:60.39ms
step:714/2330 train_time:43121ms step_avg:60.39ms
step:715/2330 train_time:43180ms step_avg:60.39ms
step:716/2330 train_time:43243ms step_avg:60.40ms
step:717/2330 train_time:43301ms step_avg:60.39ms
step:718/2330 train_time:43363ms step_avg:60.39ms
step:719/2330 train_time:43422ms step_avg:60.39ms
step:720/2330 train_time:43485ms step_avg:60.40ms
step:721/2330 train_time:43543ms step_avg:60.39ms
step:722/2330 train_time:43606ms step_avg:60.40ms
step:723/2330 train_time:43664ms step_avg:60.39ms
step:724/2330 train_time:43727ms step_avg:60.40ms
step:725/2330 train_time:43785ms step_avg:60.39ms
step:726/2330 train_time:43848ms step_avg:60.40ms
step:727/2330 train_time:43906ms step_avg:60.39ms
step:728/2330 train_time:43969ms step_avg:60.40ms
step:729/2330 train_time:44027ms step_avg:60.39ms
step:730/2330 train_time:44090ms step_avg:60.40ms
step:731/2330 train_time:44148ms step_avg:60.39ms
step:732/2330 train_time:44210ms step_avg:60.40ms
step:733/2330 train_time:44269ms step_avg:60.39ms
step:734/2330 train_time:44330ms step_avg:60.40ms
step:735/2330 train_time:44388ms step_avg:60.39ms
step:736/2330 train_time:44450ms step_avg:60.39ms
step:737/2330 train_time:44508ms step_avg:60.39ms
step:738/2330 train_time:44571ms step_avg:60.39ms
step:739/2330 train_time:44629ms step_avg:60.39ms
step:740/2330 train_time:44691ms step_avg:60.39ms
step:741/2330 train_time:44749ms step_avg:60.39ms
step:742/2330 train_time:44811ms step_avg:60.39ms
step:743/2330 train_time:44870ms step_avg:60.39ms
step:744/2330 train_time:44931ms step_avg:60.39ms
step:745/2330 train_time:44990ms step_avg:60.39ms
step:746/2330 train_time:45052ms step_avg:60.39ms
step:747/2330 train_time:45110ms step_avg:60.39ms
step:748/2330 train_time:45172ms step_avg:60.39ms
step:749/2330 train_time:45231ms step_avg:60.39ms
step:750/2330 train_time:45293ms step_avg:60.39ms
step:750/2330 val_loss:3.8987 train_time:45365ms step_avg:60.49ms
step:751/2330 train_time:45386ms step_avg:60.43ms
step:752/2330 train_time:45418ms step_avg:60.40ms
step:753/2330 train_time:45479ms step_avg:60.40ms
step:754/2330 train_time:45545ms step_avg:60.40ms
step:755/2330 train_time:45605ms step_avg:60.40ms
step:756/2330 train_time:45667ms step_avg:60.41ms
step:757/2330 train_time:45726ms step_avg:60.40ms
step:758/2330 train_time:45788ms step_avg:60.41ms
step:759/2330 train_time:45846ms step_avg:60.40ms
step:760/2330 train_time:45908ms step_avg:60.41ms
step:761/2330 train_time:45966ms step_avg:60.40ms
step:762/2330 train_time:46027ms step_avg:60.40ms
step:763/2330 train_time:46085ms step_avg:60.40ms
step:764/2330 train_time:46147ms step_avg:60.40ms
step:765/2330 train_time:46205ms step_avg:60.40ms
step:766/2330 train_time:46267ms step_avg:60.40ms
step:767/2330 train_time:46326ms step_avg:60.40ms
step:768/2330 train_time:46390ms step_avg:60.40ms
step:769/2330 train_time:46451ms step_avg:60.40ms
step:770/2330 train_time:46516ms step_avg:60.41ms
step:771/2330 train_time:46577ms step_avg:60.41ms
step:772/2330 train_time:46641ms step_avg:60.42ms
step:773/2330 train_time:46701ms step_avg:60.41ms
step:774/2330 train_time:46763ms step_avg:60.42ms
step:775/2330 train_time:46823ms step_avg:60.42ms
step:776/2330 train_time:46886ms step_avg:60.42ms
step:777/2330 train_time:46946ms step_avg:60.42ms
step:778/2330 train_time:47009ms step_avg:60.42ms
step:779/2330 train_time:47067ms step_avg:60.42ms
step:780/2330 train_time:47129ms step_avg:60.42ms
step:781/2330 train_time:47188ms step_avg:60.42ms
step:782/2330 train_time:47251ms step_avg:60.42ms
step:783/2330 train_time:47309ms step_avg:60.42ms
step:784/2330 train_time:47373ms step_avg:60.42ms
step:785/2330 train_time:47432ms step_avg:60.42ms
step:786/2330 train_time:47495ms step_avg:60.43ms
step:787/2330 train_time:47555ms step_avg:60.43ms
step:788/2330 train_time:47619ms step_avg:60.43ms
step:789/2330 train_time:47678ms step_avg:60.43ms
step:790/2330 train_time:47742ms step_avg:60.43ms
step:791/2330 train_time:47802ms step_avg:60.43ms
step:792/2330 train_time:47865ms step_avg:60.44ms
step:793/2330 train_time:47925ms step_avg:60.43ms
step:794/2330 train_time:47988ms step_avg:60.44ms
step:795/2330 train_time:48047ms step_avg:60.44ms
step:796/2330 train_time:48110ms step_avg:60.44ms
step:797/2330 train_time:48169ms step_avg:60.44ms
step:798/2330 train_time:48230ms step_avg:60.44ms
step:799/2330 train_time:48289ms step_avg:60.44ms
step:800/2330 train_time:48353ms step_avg:60.44ms
step:801/2330 train_time:48411ms step_avg:60.44ms
step:802/2330 train_time:48474ms step_avg:60.44ms
step:803/2330 train_time:48534ms step_avg:60.44ms
step:804/2330 train_time:48597ms step_avg:60.44ms
step:805/2330 train_time:48657ms step_avg:60.44ms
step:806/2330 train_time:48721ms step_avg:60.45ms
step:807/2330 train_time:48780ms step_avg:60.45ms
step:808/2330 train_time:48843ms step_avg:60.45ms
step:809/2330 train_time:48903ms step_avg:60.45ms
step:810/2330 train_time:48966ms step_avg:60.45ms
step:811/2330 train_time:49026ms step_avg:60.45ms
step:812/2330 train_time:49088ms step_avg:60.45ms
step:813/2330 train_time:49148ms step_avg:60.45ms
step:814/2330 train_time:49210ms step_avg:60.46ms
step:815/2330 train_time:49268ms step_avg:60.45ms
step:816/2330 train_time:49331ms step_avg:60.46ms
step:817/2330 train_time:49391ms step_avg:60.45ms
step:818/2330 train_time:49454ms step_avg:60.46ms
step:819/2330 train_time:49513ms step_avg:60.46ms
step:820/2330 train_time:49576ms step_avg:60.46ms
step:821/2330 train_time:49636ms step_avg:60.46ms
step:822/2330 train_time:49700ms step_avg:60.46ms
step:823/2330 train_time:49759ms step_avg:60.46ms
step:824/2330 train_time:49822ms step_avg:60.46ms
step:825/2330 train_time:49881ms step_avg:60.46ms
step:826/2330 train_time:49945ms step_avg:60.47ms
step:827/2330 train_time:50005ms step_avg:60.47ms
step:828/2330 train_time:50067ms step_avg:60.47ms
step:829/2330 train_time:50126ms step_avg:60.47ms
step:830/2330 train_time:50189ms step_avg:60.47ms
step:831/2330 train_time:50248ms step_avg:60.47ms
step:832/2330 train_time:50311ms step_avg:60.47ms
step:833/2330 train_time:50370ms step_avg:60.47ms
step:834/2330 train_time:50433ms step_avg:60.47ms
step:835/2330 train_time:50492ms step_avg:60.47ms
step:836/2330 train_time:50556ms step_avg:60.47ms
step:837/2330 train_time:50615ms step_avg:60.47ms
step:838/2330 train_time:50678ms step_avg:60.48ms
step:839/2330 train_time:50738ms step_avg:60.47ms
step:840/2330 train_time:50801ms step_avg:60.48ms
step:841/2330 train_time:50861ms step_avg:60.48ms
step:842/2330 train_time:50924ms step_avg:60.48ms
step:843/2330 train_time:50983ms step_avg:60.48ms
step:844/2330 train_time:51047ms step_avg:60.48ms
step:845/2330 train_time:51107ms step_avg:60.48ms
step:846/2330 train_time:51169ms step_avg:60.48ms
step:847/2330 train_time:51228ms step_avg:60.48ms
step:848/2330 train_time:51292ms step_avg:60.49ms
step:849/2330 train_time:51351ms step_avg:60.48ms
step:850/2330 train_time:51414ms step_avg:60.49ms
step:851/2330 train_time:51472ms step_avg:60.48ms
step:852/2330 train_time:51534ms step_avg:60.49ms
step:853/2330 train_time:51594ms step_avg:60.49ms
step:854/2330 train_time:51658ms step_avg:60.49ms
step:855/2330 train_time:51717ms step_avg:60.49ms
step:856/2330 train_time:51779ms step_avg:60.49ms
step:857/2330 train_time:51839ms step_avg:60.49ms
step:858/2330 train_time:51902ms step_avg:60.49ms
step:859/2330 train_time:51962ms step_avg:60.49ms
step:860/2330 train_time:52025ms step_avg:60.49ms
step:861/2330 train_time:52084ms step_avg:60.49ms
step:862/2330 train_time:52147ms step_avg:60.50ms
step:863/2330 train_time:52207ms step_avg:60.50ms
step:864/2330 train_time:52270ms step_avg:60.50ms
step:865/2330 train_time:52330ms step_avg:60.50ms
step:866/2330 train_time:52393ms step_avg:60.50ms
step:867/2330 train_time:52453ms step_avg:60.50ms
step:868/2330 train_time:52515ms step_avg:60.50ms
step:869/2330 train_time:52574ms step_avg:60.50ms
step:870/2330 train_time:52637ms step_avg:60.50ms
step:871/2330 train_time:52697ms step_avg:60.50ms
step:872/2330 train_time:52759ms step_avg:60.50ms
step:873/2330 train_time:52819ms step_avg:60.50ms
step:874/2330 train_time:52882ms step_avg:60.51ms
step:875/2330 train_time:52941ms step_avg:60.50ms
step:876/2330 train_time:53005ms step_avg:60.51ms
step:877/2330 train_time:53065ms step_avg:60.51ms
step:878/2330 train_time:53128ms step_avg:60.51ms
step:879/2330 train_time:53188ms step_avg:60.51ms
step:880/2330 train_time:53251ms step_avg:60.51ms
step:881/2330 train_time:53311ms step_avg:60.51ms
step:882/2330 train_time:53373ms step_avg:60.51ms
step:883/2330 train_time:53432ms step_avg:60.51ms
step:884/2330 train_time:53495ms step_avg:60.51ms
step:885/2330 train_time:53555ms step_avg:60.51ms
step:886/2330 train_time:53617ms step_avg:60.52ms
step:887/2330 train_time:53677ms step_avg:60.51ms
step:888/2330 train_time:53739ms step_avg:60.52ms
step:889/2330 train_time:53798ms step_avg:60.52ms
step:890/2330 train_time:53861ms step_avg:60.52ms
step:891/2330 train_time:53921ms step_avg:60.52ms
step:892/2330 train_time:53984ms step_avg:60.52ms
step:893/2330 train_time:54044ms step_avg:60.52ms
step:894/2330 train_time:54107ms step_avg:60.52ms
step:895/2330 train_time:54167ms step_avg:60.52ms
step:896/2330 train_time:54230ms step_avg:60.52ms
step:897/2330 train_time:54289ms step_avg:60.52ms
step:898/2330 train_time:54351ms step_avg:60.52ms
step:899/2330 train_time:54410ms step_avg:60.52ms
step:900/2330 train_time:54473ms step_avg:60.53ms
step:901/2330 train_time:54532ms step_avg:60.52ms
step:902/2330 train_time:54595ms step_avg:60.53ms
step:903/2330 train_time:54654ms step_avg:60.53ms
step:904/2330 train_time:54718ms step_avg:60.53ms
step:905/2330 train_time:54777ms step_avg:60.53ms
step:906/2330 train_time:54839ms step_avg:60.53ms
step:907/2330 train_time:54898ms step_avg:60.53ms
step:908/2330 train_time:54961ms step_avg:60.53ms
step:909/2330 train_time:55022ms step_avg:60.53ms
step:910/2330 train_time:55085ms step_avg:60.53ms
step:911/2330 train_time:55145ms step_avg:60.53ms
step:912/2330 train_time:55209ms step_avg:60.54ms
step:913/2330 train_time:55268ms step_avg:60.53ms
step:914/2330 train_time:55332ms step_avg:60.54ms
step:915/2330 train_time:55390ms step_avg:60.54ms
step:916/2330 train_time:55453ms step_avg:60.54ms
step:917/2330 train_time:55513ms step_avg:60.54ms
step:918/2330 train_time:55575ms step_avg:60.54ms
step:919/2330 train_time:55635ms step_avg:60.54ms
step:920/2330 train_time:55698ms step_avg:60.54ms
step:921/2330 train_time:55757ms step_avg:60.54ms
step:922/2330 train_time:55819ms step_avg:60.54ms
step:923/2330 train_time:55879ms step_avg:60.54ms
step:924/2330 train_time:55941ms step_avg:60.54ms
step:925/2330 train_time:56001ms step_avg:60.54ms
step:926/2330 train_time:56064ms step_avg:60.54ms
step:927/2330 train_time:56124ms step_avg:60.54ms
step:928/2330 train_time:56186ms step_avg:60.55ms
step:929/2330 train_time:56247ms step_avg:60.55ms
step:930/2330 train_time:56310ms step_avg:60.55ms
step:931/2330 train_time:56369ms step_avg:60.55ms
step:932/2330 train_time:56432ms step_avg:60.55ms
step:933/2330 train_time:56491ms step_avg:60.55ms
step:934/2330 train_time:56554ms step_avg:60.55ms
step:935/2330 train_time:56613ms step_avg:60.55ms
step:936/2330 train_time:56675ms step_avg:60.55ms
step:937/2330 train_time:56734ms step_avg:60.55ms
step:938/2330 train_time:56796ms step_avg:60.55ms
step:939/2330 train_time:56855ms step_avg:60.55ms
step:940/2330 train_time:56919ms step_avg:60.55ms
step:941/2330 train_time:56979ms step_avg:60.55ms
step:942/2330 train_time:57041ms step_avg:60.55ms
step:943/2330 train_time:57102ms step_avg:60.55ms
step:944/2330 train_time:57165ms step_avg:60.56ms
step:945/2330 train_time:57224ms step_avg:60.55ms
step:946/2330 train_time:57287ms step_avg:60.56ms
step:947/2330 train_time:57347ms step_avg:60.56ms
step:948/2330 train_time:57410ms step_avg:60.56ms
step:949/2330 train_time:57469ms step_avg:60.56ms
step:950/2330 train_time:57532ms step_avg:60.56ms
step:951/2330 train_time:57591ms step_avg:60.56ms
step:952/2330 train_time:57654ms step_avg:60.56ms
step:953/2330 train_time:57712ms step_avg:60.56ms
step:954/2330 train_time:57775ms step_avg:60.56ms
step:955/2330 train_time:57835ms step_avg:60.56ms
step:956/2330 train_time:57897ms step_avg:60.56ms
step:957/2330 train_time:57957ms step_avg:60.56ms
step:958/2330 train_time:58020ms step_avg:60.56ms
step:959/2330 train_time:58080ms step_avg:60.56ms
step:960/2330 train_time:58143ms step_avg:60.57ms
step:961/2330 train_time:58205ms step_avg:60.57ms
step:962/2330 train_time:58268ms step_avg:60.57ms
step:963/2330 train_time:58327ms step_avg:60.57ms
step:964/2330 train_time:58390ms step_avg:60.57ms
step:965/2330 train_time:58451ms step_avg:60.57ms
step:966/2330 train_time:58514ms step_avg:60.57ms
step:967/2330 train_time:58573ms step_avg:60.57ms
step:968/2330 train_time:58635ms step_avg:60.57ms
step:969/2330 train_time:58694ms step_avg:60.57ms
step:970/2330 train_time:58757ms step_avg:60.57ms
step:971/2330 train_time:58816ms step_avg:60.57ms
step:972/2330 train_time:58878ms step_avg:60.57ms
step:973/2330 train_time:58938ms step_avg:60.57ms
step:974/2330 train_time:59001ms step_avg:60.58ms
step:975/2330 train_time:59060ms step_avg:60.57ms
step:976/2330 train_time:59123ms step_avg:60.58ms
step:977/2330 train_time:59184ms step_avg:60.58ms
step:978/2330 train_time:59247ms step_avg:60.58ms
step:979/2330 train_time:59307ms step_avg:60.58ms
step:980/2330 train_time:59371ms step_avg:60.58ms
step:981/2330 train_time:59430ms step_avg:60.58ms
step:982/2330 train_time:59494ms step_avg:60.58ms
step:983/2330 train_time:59553ms step_avg:60.58ms
step:984/2330 train_time:59616ms step_avg:60.59ms
step:985/2330 train_time:59675ms step_avg:60.58ms
step:986/2330 train_time:59737ms step_avg:60.59ms
step:987/2330 train_time:59797ms step_avg:60.58ms
step:988/2330 train_time:59860ms step_avg:60.59ms
step:989/2330 train_time:59919ms step_avg:60.59ms
step:990/2330 train_time:59982ms step_avg:60.59ms
step:991/2330 train_time:60041ms step_avg:60.59ms
step:992/2330 train_time:60105ms step_avg:60.59ms
step:993/2330 train_time:60164ms step_avg:60.59ms
step:994/2330 train_time:60227ms step_avg:60.59ms
step:995/2330 train_time:60287ms step_avg:60.59ms
step:996/2330 train_time:60351ms step_avg:60.59ms
step:997/2330 train_time:60410ms step_avg:60.59ms
step:998/2330 train_time:60473ms step_avg:60.59ms
step:999/2330 train_time:60532ms step_avg:60.59ms
step:1000/2330 train_time:60595ms step_avg:60.60ms
step:1000/2330 val_loss:3.7550 train_time:60666ms step_avg:60.67ms
step:1001/2330 train_time:60688ms step_avg:60.63ms
step:1002/2330 train_time:60718ms step_avg:60.60ms
step:1003/2330 train_time:60780ms step_avg:60.60ms
step:1004/2330 train_time:60849ms step_avg:60.61ms
step:1005/2330 train_time:60910ms step_avg:60.61ms
step:1006/2330 train_time:60973ms step_avg:60.61ms
step:1007/2330 train_time:61032ms step_avg:60.61ms
step:1008/2330 train_time:61094ms step_avg:60.61ms
step:1009/2330 train_time:61152ms step_avg:60.61ms
step:1010/2330 train_time:61214ms step_avg:60.61ms
step:1011/2330 train_time:61273ms step_avg:60.61ms
step:1012/2330 train_time:61335ms step_avg:60.61ms
step:1013/2330 train_time:61394ms step_avg:60.61ms
step:1014/2330 train_time:61455ms step_avg:60.61ms
step:1015/2330 train_time:61513ms step_avg:60.60ms
step:1016/2330 train_time:61576ms step_avg:60.61ms
step:1017/2330 train_time:61639ms step_avg:60.61ms
step:1018/2330 train_time:61704ms step_avg:60.61ms
step:1019/2330 train_time:61765ms step_avg:60.61ms
step:1020/2330 train_time:61830ms step_avg:60.62ms
step:1021/2330 train_time:61891ms step_avg:60.62ms
step:1022/2330 train_time:61953ms step_avg:60.62ms
step:1023/2330 train_time:62013ms step_avg:60.62ms
step:1024/2330 train_time:62074ms step_avg:60.62ms
step:1025/2330 train_time:62133ms step_avg:60.62ms
step:1026/2330 train_time:62195ms step_avg:60.62ms
step:1027/2330 train_time:62254ms step_avg:60.62ms
step:1028/2330 train_time:62316ms step_avg:60.62ms
step:1029/2330 train_time:62374ms step_avg:60.62ms
step:1030/2330 train_time:62437ms step_avg:60.62ms
step:1031/2330 train_time:62495ms step_avg:60.62ms
step:1032/2330 train_time:62559ms step_avg:60.62ms
step:1033/2330 train_time:62619ms step_avg:60.62ms
step:1034/2330 train_time:62683ms step_avg:60.62ms
step:1035/2330 train_time:62743ms step_avg:60.62ms
step:1036/2330 train_time:62807ms step_avg:60.62ms
step:1037/2330 train_time:62867ms step_avg:60.62ms
step:1038/2330 train_time:62931ms step_avg:60.63ms
step:1039/2330 train_time:62991ms step_avg:60.63ms
step:1040/2330 train_time:63052ms step_avg:60.63ms
step:1041/2330 train_time:63112ms step_avg:60.63ms
step:1042/2330 train_time:63175ms step_avg:60.63ms
step:1043/2330 train_time:63233ms step_avg:60.63ms
step:1044/2330 train_time:63296ms step_avg:60.63ms
step:1045/2330 train_time:63355ms step_avg:60.63ms
step:1046/2330 train_time:63417ms step_avg:60.63ms
step:1047/2330 train_time:63476ms step_avg:60.63ms
step:1048/2330 train_time:63539ms step_avg:60.63ms
step:1049/2330 train_time:63597ms step_avg:60.63ms
step:1050/2330 train_time:63661ms step_avg:60.63ms
step:1051/2330 train_time:63720ms step_avg:60.63ms
step:1052/2330 train_time:63784ms step_avg:60.63ms
step:1053/2330 train_time:63845ms step_avg:60.63ms
step:1054/2330 train_time:63908ms step_avg:60.63ms
step:1055/2330 train_time:63967ms step_avg:60.63ms
step:1056/2330 train_time:64030ms step_avg:60.63ms
step:1057/2330 train_time:64090ms step_avg:60.63ms
step:1058/2330 train_time:64152ms step_avg:60.64ms
step:1059/2330 train_time:64212ms step_avg:60.63ms
step:1060/2330 train_time:64275ms step_avg:60.64ms
step:1061/2330 train_time:64333ms step_avg:60.63ms
step:1062/2330 train_time:64397ms step_avg:60.64ms
step:1063/2330 train_time:64456ms step_avg:60.64ms
step:1064/2330 train_time:64518ms step_avg:60.64ms
step:1065/2330 train_time:64577ms step_avg:60.64ms
step:1066/2330 train_time:64640ms step_avg:60.64ms
step:1067/2330 train_time:64699ms step_avg:60.64ms
step:1068/2330 train_time:64762ms step_avg:60.64ms
step:1069/2330 train_time:64822ms step_avg:60.64ms
step:1070/2330 train_time:64885ms step_avg:60.64ms
step:1071/2330 train_time:64945ms step_avg:60.64ms
step:1072/2330 train_time:65008ms step_avg:60.64ms
step:1073/2330 train_time:65068ms step_avg:60.64ms
step:1074/2330 train_time:65131ms step_avg:60.64ms
step:1075/2330 train_time:65191ms step_avg:60.64ms
step:1076/2330 train_time:65253ms step_avg:60.64ms
step:1077/2330 train_time:65312ms step_avg:60.64ms
step:1078/2330 train_time:65375ms step_avg:60.65ms
step:1079/2330 train_time:65434ms step_avg:60.64ms
step:1080/2330 train_time:65497ms step_avg:60.65ms
step:1081/2330 train_time:65556ms step_avg:60.64ms
step:1082/2330 train_time:65619ms step_avg:60.65ms
step:1083/2330 train_time:65679ms step_avg:60.65ms
step:1084/2330 train_time:65742ms step_avg:60.65ms
step:1085/2330 train_time:65801ms step_avg:60.65ms
step:1086/2330 train_time:65863ms step_avg:60.65ms
step:1087/2330 train_time:65923ms step_avg:60.65ms
step:1088/2330 train_time:65986ms step_avg:60.65ms
step:1089/2330 train_time:66046ms step_avg:60.65ms
step:1090/2330 train_time:66109ms step_avg:60.65ms
step:1091/2330 train_time:66168ms step_avg:60.65ms
step:1092/2330 train_time:66232ms step_avg:60.65ms
step:1093/2330 train_time:66293ms step_avg:60.65ms
step:1094/2330 train_time:66355ms step_avg:60.65ms
step:1095/2330 train_time:66414ms step_avg:60.65ms
step:1096/2330 train_time:66478ms step_avg:60.66ms
step:1097/2330 train_time:66538ms step_avg:60.65ms
step:1098/2330 train_time:66600ms step_avg:60.66ms
step:1099/2330 train_time:66660ms step_avg:60.66ms
step:1100/2330 train_time:66723ms step_avg:60.66ms
step:1101/2330 train_time:66782ms step_avg:60.66ms
step:1102/2330 train_time:66846ms step_avg:60.66ms
step:1103/2330 train_time:66905ms step_avg:60.66ms
step:1104/2330 train_time:66968ms step_avg:60.66ms
step:1105/2330 train_time:67027ms step_avg:60.66ms
step:1106/2330 train_time:67090ms step_avg:60.66ms
step:1107/2330 train_time:67149ms step_avg:60.66ms
step:1108/2330 train_time:67212ms step_avg:60.66ms
step:1109/2330 train_time:67272ms step_avg:60.66ms
step:1110/2330 train_time:67336ms step_avg:60.66ms
step:1111/2330 train_time:67395ms step_avg:60.66ms
step:1112/2330 train_time:67458ms step_avg:60.66ms
step:1113/2330 train_time:67517ms step_avg:60.66ms
step:1114/2330 train_time:67579ms step_avg:60.66ms
step:1115/2330 train_time:67638ms step_avg:60.66ms
step:1116/2330 train_time:67701ms step_avg:60.66ms
step:1117/2330 train_time:67760ms step_avg:60.66ms
step:1118/2330 train_time:67823ms step_avg:60.66ms
step:1119/2330 train_time:67882ms step_avg:60.66ms
step:1120/2330 train_time:67945ms step_avg:60.66ms
step:1121/2330 train_time:68004ms step_avg:60.66ms
step:1122/2330 train_time:68067ms step_avg:60.67ms
step:1123/2330 train_time:68126ms step_avg:60.66ms
step:1124/2330 train_time:68191ms step_avg:60.67ms
step:1125/2330 train_time:68250ms step_avg:60.67ms
step:1126/2330 train_time:68314ms step_avg:60.67ms
step:1127/2330 train_time:68373ms step_avg:60.67ms
step:1128/2330 train_time:68437ms step_avg:60.67ms
step:1129/2330 train_time:68496ms step_avg:60.67ms
step:1130/2330 train_time:68559ms step_avg:60.67ms
step:1131/2330 train_time:68617ms step_avg:60.67ms
step:1132/2330 train_time:68680ms step_avg:60.67ms
step:1133/2330 train_time:68740ms step_avg:60.67ms
step:1134/2330 train_time:68802ms step_avg:60.67ms
step:1135/2330 train_time:68861ms step_avg:60.67ms
step:1136/2330 train_time:68924ms step_avg:60.67ms
step:1137/2330 train_time:68983ms step_avg:60.67ms
step:1138/2330 train_time:69046ms step_avg:60.67ms
step:1139/2330 train_time:69105ms step_avg:60.67ms
step:1140/2330 train_time:69168ms step_avg:60.67ms
step:1141/2330 train_time:69228ms step_avg:60.67ms
step:1142/2330 train_time:69292ms step_avg:60.68ms
step:1143/2330 train_time:69351ms step_avg:60.67ms
step:1144/2330 train_time:69415ms step_avg:60.68ms
step:1145/2330 train_time:69475ms step_avg:60.68ms
step:1146/2330 train_time:69538ms step_avg:60.68ms
step:1147/2330 train_time:69597ms step_avg:60.68ms
step:1148/2330 train_time:69659ms step_avg:60.68ms
step:1149/2330 train_time:69718ms step_avg:60.68ms
step:1150/2330 train_time:69781ms step_avg:60.68ms
step:1151/2330 train_time:69841ms step_avg:60.68ms
step:1152/2330 train_time:69904ms step_avg:60.68ms
step:1153/2330 train_time:69963ms step_avg:60.68ms
step:1154/2330 train_time:70026ms step_avg:60.68ms
step:1155/2330 train_time:70086ms step_avg:60.68ms
step:1156/2330 train_time:70149ms step_avg:60.68ms
step:1157/2330 train_time:70209ms step_avg:60.68ms
step:1158/2330 train_time:70271ms step_avg:60.68ms
step:1159/2330 train_time:70331ms step_avg:60.68ms
step:1160/2330 train_time:70394ms step_avg:60.68ms
step:1161/2330 train_time:70454ms step_avg:60.68ms
step:1162/2330 train_time:70517ms step_avg:60.69ms
step:1163/2330 train_time:70576ms step_avg:60.68ms
step:1164/2330 train_time:70640ms step_avg:60.69ms
step:1165/2330 train_time:70699ms step_avg:60.69ms
step:1166/2330 train_time:70761ms step_avg:60.69ms
step:1167/2330 train_time:70820ms step_avg:60.69ms
step:1168/2330 train_time:70883ms step_avg:60.69ms
step:1169/2330 train_time:70942ms step_avg:60.69ms
step:1170/2330 train_time:71005ms step_avg:60.69ms
step:1171/2330 train_time:71064ms step_avg:60.69ms
step:1172/2330 train_time:71128ms step_avg:60.69ms
step:1173/2330 train_time:71187ms step_avg:60.69ms
step:1174/2330 train_time:71251ms step_avg:60.69ms
step:1175/2330 train_time:71311ms step_avg:60.69ms
step:1176/2330 train_time:71374ms step_avg:60.69ms
step:1177/2330 train_time:71434ms step_avg:60.69ms
step:1178/2330 train_time:71497ms step_avg:60.69ms
step:1179/2330 train_time:71555ms step_avg:60.69ms
step:1180/2330 train_time:71619ms step_avg:60.69ms
step:1181/2330 train_time:71678ms step_avg:60.69ms
step:1182/2330 train_time:71741ms step_avg:60.69ms
step:1183/2330 train_time:71800ms step_avg:60.69ms
step:1184/2330 train_time:71862ms step_avg:60.69ms
step:1185/2330 train_time:71922ms step_avg:60.69ms
step:1186/2330 train_time:71984ms step_avg:60.70ms
step:1187/2330 train_time:72043ms step_avg:60.69ms
step:1188/2330 train_time:72106ms step_avg:60.70ms
step:1189/2330 train_time:72166ms step_avg:60.69ms
step:1190/2330 train_time:72229ms step_avg:60.70ms
step:1191/2330 train_time:72289ms step_avg:60.70ms
step:1192/2330 train_time:72353ms step_avg:60.70ms
step:1193/2330 train_time:72412ms step_avg:60.70ms
step:1194/2330 train_time:72476ms step_avg:60.70ms
step:1195/2330 train_time:72536ms step_avg:60.70ms
step:1196/2330 train_time:72599ms step_avg:60.70ms
step:1197/2330 train_time:72658ms step_avg:60.70ms
step:1198/2330 train_time:72721ms step_avg:60.70ms
step:1199/2330 train_time:72779ms step_avg:60.70ms
step:1200/2330 train_time:72843ms step_avg:60.70ms
step:1201/2330 train_time:72903ms step_avg:60.70ms
step:1202/2330 train_time:72965ms step_avg:60.70ms
step:1203/2330 train_time:73024ms step_avg:60.70ms
step:1204/2330 train_time:73087ms step_avg:60.70ms
step:1205/2330 train_time:73146ms step_avg:60.70ms
step:1206/2330 train_time:73209ms step_avg:60.70ms
step:1207/2330 train_time:73268ms step_avg:60.70ms
step:1208/2330 train_time:73332ms step_avg:60.71ms
step:1209/2330 train_time:73393ms step_avg:60.71ms
step:1210/2330 train_time:73455ms step_avg:60.71ms
step:1211/2330 train_time:73515ms step_avg:60.71ms
step:1212/2330 train_time:73578ms step_avg:60.71ms
step:1213/2330 train_time:73637ms step_avg:60.71ms
step:1214/2330 train_time:73701ms step_avg:60.71ms
step:1215/2330 train_time:73759ms step_avg:60.71ms
step:1216/2330 train_time:73821ms step_avg:60.71ms
step:1217/2330 train_time:73881ms step_avg:60.71ms
step:1218/2330 train_time:73944ms step_avg:60.71ms
step:1219/2330 train_time:74003ms step_avg:60.71ms
step:1220/2330 train_time:74066ms step_avg:60.71ms
step:1221/2330 train_time:74125ms step_avg:60.71ms
step:1222/2330 train_time:74188ms step_avg:60.71ms
step:1223/2330 train_time:74248ms step_avg:60.71ms
step:1224/2330 train_time:74311ms step_avg:60.71ms
step:1225/2330 train_time:74371ms step_avg:60.71ms
step:1226/2330 train_time:74434ms step_avg:60.71ms
step:1227/2330 train_time:74495ms step_avg:60.71ms
step:1228/2330 train_time:74558ms step_avg:60.71ms
step:1229/2330 train_time:74618ms step_avg:60.71ms
step:1230/2330 train_time:74680ms step_avg:60.72ms
step:1231/2330 train_time:74739ms step_avg:60.71ms
step:1232/2330 train_time:74802ms step_avg:60.72ms
step:1233/2330 train_time:74862ms step_avg:60.72ms
step:1234/2330 train_time:74925ms step_avg:60.72ms
step:1235/2330 train_time:74984ms step_avg:60.72ms
step:1236/2330 train_time:75047ms step_avg:60.72ms
step:1237/2330 train_time:75106ms step_avg:60.72ms
step:1238/2330 train_time:75169ms step_avg:60.72ms
step:1239/2330 train_time:75228ms step_avg:60.72ms
step:1240/2330 train_time:75292ms step_avg:60.72ms
step:1241/2330 train_time:75350ms step_avg:60.72ms
step:1242/2330 train_time:75413ms step_avg:60.72ms
step:1243/2330 train_time:75473ms step_avg:60.72ms
step:1244/2330 train_time:75537ms step_avg:60.72ms
step:1245/2330 train_time:75597ms step_avg:60.72ms
step:1246/2330 train_time:75660ms step_avg:60.72ms
step:1247/2330 train_time:75719ms step_avg:60.72ms
step:1248/2330 train_time:75782ms step_avg:60.72ms
step:1249/2330 train_time:75842ms step_avg:60.72ms
step:1250/2330 train_time:75904ms step_avg:60.72ms
step:1250/2330 val_loss:3.6746 train_time:75975ms step_avg:60.78ms
step:1251/2330 train_time:75995ms step_avg:60.75ms
step:1252/2330 train_time:76027ms step_avg:60.72ms
step:1253/2330 train_time:76090ms step_avg:60.73ms
step:1254/2330 train_time:76156ms step_avg:60.73ms
step:1255/2330 train_time:76216ms step_avg:60.73ms
step:1256/2330 train_time:76277ms step_avg:60.73ms
step:1257/2330 train_time:76337ms step_avg:60.73ms
step:1258/2330 train_time:76399ms step_avg:60.73ms
step:1259/2330 train_time:76458ms step_avg:60.73ms
step:1260/2330 train_time:76521ms step_avg:60.73ms
step:1261/2330 train_time:76578ms step_avg:60.73ms
step:1262/2330 train_time:76641ms step_avg:60.73ms
step:1263/2330 train_time:76700ms step_avg:60.73ms
step:1264/2330 train_time:76762ms step_avg:60.73ms
step:1265/2330 train_time:76820ms step_avg:60.73ms
step:1266/2330 train_time:76882ms step_avg:60.73ms
step:1267/2330 train_time:76943ms step_avg:60.73ms
step:1268/2330 train_time:77009ms step_avg:60.73ms
step:1269/2330 train_time:77070ms step_avg:60.73ms
step:1270/2330 train_time:77135ms step_avg:60.74ms
step:1271/2330 train_time:77194ms step_avg:60.73ms
step:1272/2330 train_time:77256ms step_avg:60.74ms
step:1273/2330 train_time:77315ms step_avg:60.73ms
step:1274/2330 train_time:77378ms step_avg:60.74ms
step:1275/2330 train_time:77437ms step_avg:60.74ms
step:1276/2330 train_time:77500ms step_avg:60.74ms
step:1277/2330 train_time:77559ms step_avg:60.74ms
step:1278/2330 train_time:77621ms step_avg:60.74ms
step:1279/2330 train_time:77679ms step_avg:60.73ms
step:1280/2330 train_time:77741ms step_avg:60.74ms
step:1281/2330 train_time:77800ms step_avg:60.73ms
step:1282/2330 train_time:77863ms step_avg:60.74ms
step:1283/2330 train_time:77923ms step_avg:60.73ms
step:1284/2330 train_time:77987ms step_avg:60.74ms
step:1285/2330 train_time:78047ms step_avg:60.74ms
step:1286/2330 train_time:78110ms step_avg:60.74ms
step:1287/2330 train_time:78171ms step_avg:60.74ms
step:1288/2330 train_time:78234ms step_avg:60.74ms
step:1289/2330 train_time:78294ms step_avg:60.74ms
step:1290/2330 train_time:78356ms step_avg:60.74ms
step:1291/2330 train_time:78416ms step_avg:60.74ms
step:1292/2330 train_time:78478ms step_avg:60.74ms
step:1293/2330 train_time:78537ms step_avg:60.74ms
step:1294/2330 train_time:78600ms step_avg:60.74ms
step:1295/2330 train_time:78659ms step_avg:60.74ms
step:1296/2330 train_time:78722ms step_avg:60.74ms
step:1297/2330 train_time:78780ms step_avg:60.74ms
step:1298/2330 train_time:78843ms step_avg:60.74ms
step:1299/2330 train_time:78902ms step_avg:60.74ms
step:1300/2330 train_time:78966ms step_avg:60.74ms
step:1301/2330 train_time:79025ms step_avg:60.74ms
step:1302/2330 train_time:79088ms step_avg:60.74ms
step:1303/2330 train_time:79148ms step_avg:60.74ms
step:1304/2330 train_time:79212ms step_avg:60.75ms
step:1305/2330 train_time:79271ms step_avg:60.74ms
step:1306/2330 train_time:79334ms step_avg:60.75ms
step:1307/2330 train_time:79394ms step_avg:60.75ms
step:1308/2330 train_time:79457ms step_avg:60.75ms
step:1309/2330 train_time:79517ms step_avg:60.75ms
step:1310/2330 train_time:79579ms step_avg:60.75ms
step:1311/2330 train_time:79638ms step_avg:60.75ms
step:1312/2330 train_time:79701ms step_avg:60.75ms
step:1313/2330 train_time:79759ms step_avg:60.75ms
step:1314/2330 train_time:79822ms step_avg:60.75ms
step:1315/2330 train_time:79880ms step_avg:60.75ms
step:1316/2330 train_time:79943ms step_avg:60.75ms
step:1317/2330 train_time:80003ms step_avg:60.75ms
step:1318/2330 train_time:80066ms step_avg:60.75ms
step:1319/2330 train_time:80125ms step_avg:60.75ms
step:1320/2330 train_time:80188ms step_avg:60.75ms
step:1321/2330 train_time:80248ms step_avg:60.75ms
step:1322/2330 train_time:80312ms step_avg:60.75ms
step:1323/2330 train_time:80371ms step_avg:60.75ms
step:1324/2330 train_time:80434ms step_avg:60.75ms
step:1325/2330 train_time:80493ms step_avg:60.75ms
step:1326/2330 train_time:80556ms step_avg:60.75ms
step:1327/2330 train_time:80615ms step_avg:60.75ms
step:1328/2330 train_time:80678ms step_avg:60.75ms
step:1329/2330 train_time:80737ms step_avg:60.75ms
step:1330/2330 train_time:80800ms step_avg:60.75ms
step:1331/2330 train_time:80859ms step_avg:60.75ms
step:1332/2330 train_time:80922ms step_avg:60.75ms
step:1333/2330 train_time:80981ms step_avg:60.75ms
step:1334/2330 train_time:81044ms step_avg:60.75ms
step:1335/2330 train_time:81103ms step_avg:60.75ms
step:1336/2330 train_time:81167ms step_avg:60.75ms
step:1337/2330 train_time:81227ms step_avg:60.75ms
step:1338/2330 train_time:81290ms step_avg:60.75ms
step:1339/2330 train_time:81350ms step_avg:60.75ms
step:1340/2330 train_time:81413ms step_avg:60.76ms
step:1341/2330 train_time:81472ms step_avg:60.75ms
step:1342/2330 train_time:81536ms step_avg:60.76ms
step:1343/2330 train_time:81595ms step_avg:60.76ms
step:1344/2330 train_time:81658ms step_avg:60.76ms
step:1345/2330 train_time:81718ms step_avg:60.76ms
step:1346/2330 train_time:81780ms step_avg:60.76ms
step:1347/2330 train_time:81839ms step_avg:60.76ms
step:1348/2330 train_time:81901ms step_avg:60.76ms
step:1349/2330 train_time:81960ms step_avg:60.76ms
step:1350/2330 train_time:82023ms step_avg:60.76ms
step:1351/2330 train_time:82082ms step_avg:60.76ms
step:1352/2330 train_time:82145ms step_avg:60.76ms
step:1353/2330 train_time:82204ms step_avg:60.76ms
step:1354/2330 train_time:82268ms step_avg:60.76ms
step:1355/2330 train_time:82327ms step_avg:60.76ms
step:1356/2330 train_time:82390ms step_avg:60.76ms
step:1357/2330 train_time:82450ms step_avg:60.76ms
step:1358/2330 train_time:82514ms step_avg:60.76ms
step:1359/2330 train_time:82574ms step_avg:60.76ms
step:1360/2330 train_time:82637ms step_avg:60.76ms
step:1361/2330 train_time:82696ms step_avg:60.76ms
step:1362/2330 train_time:82759ms step_avg:60.76ms
step:1363/2330 train_time:82819ms step_avg:60.76ms
step:1364/2330 train_time:82881ms step_avg:60.76ms
step:1365/2330 train_time:82940ms step_avg:60.76ms
step:1366/2330 train_time:83003ms step_avg:60.76ms
step:1367/2330 train_time:83062ms step_avg:60.76ms
step:1368/2330 train_time:83125ms step_avg:60.76ms
step:1369/2330 train_time:83184ms step_avg:60.76ms
step:1370/2330 train_time:83247ms step_avg:60.76ms
step:1371/2330 train_time:83307ms step_avg:60.76ms
step:1372/2330 train_time:83369ms step_avg:60.76ms
step:1373/2330 train_time:83429ms step_avg:60.76ms
step:1374/2330 train_time:83492ms step_avg:60.77ms
step:1375/2330 train_time:83551ms step_avg:60.76ms
step:1376/2330 train_time:83615ms step_avg:60.77ms
step:1377/2330 train_time:83674ms step_avg:60.77ms
step:1378/2330 train_time:83737ms step_avg:60.77ms
step:1379/2330 train_time:83797ms step_avg:60.77ms
step:1380/2330 train_time:83860ms step_avg:60.77ms
step:1381/2330 train_time:83920ms step_avg:60.77ms
step:1382/2330 train_time:83982ms step_avg:60.77ms
step:1383/2330 train_time:84041ms step_avg:60.77ms
step:1384/2330 train_time:84104ms step_avg:60.77ms
step:1385/2330 train_time:84164ms step_avg:60.77ms
step:1386/2330 train_time:84227ms step_avg:60.77ms
step:1387/2330 train_time:84286ms step_avg:60.77ms
step:1388/2330 train_time:84349ms step_avg:60.77ms
step:1389/2330 train_time:84409ms step_avg:60.77ms
step:1390/2330 train_time:84472ms step_avg:60.77ms
step:1391/2330 train_time:84533ms step_avg:60.77ms
step:1392/2330 train_time:84595ms step_avg:60.77ms
step:1393/2330 train_time:84655ms step_avg:60.77ms
step:1394/2330 train_time:84718ms step_avg:60.77ms
step:1395/2330 train_time:84776ms step_avg:60.77ms
step:1396/2330 train_time:84840ms step_avg:60.77ms
step:1397/2330 train_time:84899ms step_avg:60.77ms
step:1398/2330 train_time:84961ms step_avg:60.77ms
step:1399/2330 train_time:85020ms step_avg:60.77ms
step:1400/2330 train_time:85084ms step_avg:60.77ms
step:1401/2330 train_time:85143ms step_avg:60.77ms
step:1402/2330 train_time:85205ms step_avg:60.77ms
step:1403/2330 train_time:85265ms step_avg:60.77ms
step:1404/2330 train_time:85328ms step_avg:60.77ms
step:1405/2330 train_time:85387ms step_avg:60.77ms
step:1406/2330 train_time:85450ms step_avg:60.77ms
step:1407/2330 train_time:85510ms step_avg:60.77ms
step:1408/2330 train_time:85573ms step_avg:60.78ms
step:1409/2330 train_time:85632ms step_avg:60.77ms
step:1410/2330 train_time:85695ms step_avg:60.78ms
step:1411/2330 train_time:85755ms step_avg:60.78ms
step:1412/2330 train_time:85818ms step_avg:60.78ms
step:1413/2330 train_time:85877ms step_avg:60.78ms
step:1414/2330 train_time:85940ms step_avg:60.78ms
step:1415/2330 train_time:85999ms step_avg:60.78ms
step:1416/2330 train_time:86062ms step_avg:60.78ms
step:1417/2330 train_time:86121ms step_avg:60.78ms
step:1418/2330 train_time:86184ms step_avg:60.78ms
step:1419/2330 train_time:86243ms step_avg:60.78ms
step:1420/2330 train_time:86306ms step_avg:60.78ms
step:1421/2330 train_time:86366ms step_avg:60.78ms
step:1422/2330 train_time:86429ms step_avg:60.78ms
step:1423/2330 train_time:86488ms step_avg:60.78ms
step:1424/2330 train_time:86551ms step_avg:60.78ms
step:1425/2330 train_time:86611ms step_avg:60.78ms
step:1426/2330 train_time:86674ms step_avg:60.78ms
step:1427/2330 train_time:86734ms step_avg:60.78ms
step:1428/2330 train_time:86797ms step_avg:60.78ms
step:1429/2330 train_time:86857ms step_avg:60.78ms
step:1430/2330 train_time:86921ms step_avg:60.78ms
step:1431/2330 train_time:86979ms step_avg:60.78ms
step:1432/2330 train_time:87042ms step_avg:60.78ms
step:1433/2330 train_time:87101ms step_avg:60.78ms
step:1434/2330 train_time:87164ms step_avg:60.78ms
step:1435/2330 train_time:87222ms step_avg:60.78ms
step:1436/2330 train_time:87285ms step_avg:60.78ms
step:1437/2330 train_time:87344ms step_avg:60.78ms
step:1438/2330 train_time:87407ms step_avg:60.78ms
step:1439/2330 train_time:87466ms step_avg:60.78ms
step:1440/2330 train_time:87529ms step_avg:60.78ms
step:1441/2330 train_time:87589ms step_avg:60.78ms
step:1442/2330 train_time:87652ms step_avg:60.79ms
step:1443/2330 train_time:87713ms step_avg:60.79ms
step:1444/2330 train_time:87776ms step_avg:60.79ms
step:1445/2330 train_time:87835ms step_avg:60.79ms
step:1446/2330 train_time:87899ms step_avg:60.79ms
step:1447/2330 train_time:87959ms step_avg:60.79ms
step:1448/2330 train_time:88022ms step_avg:60.79ms
step:1449/2330 train_time:88080ms step_avg:60.79ms
step:1450/2330 train_time:88143ms step_avg:60.79ms
step:1451/2330 train_time:88202ms step_avg:60.79ms
step:1452/2330 train_time:88265ms step_avg:60.79ms
step:1453/2330 train_time:88324ms step_avg:60.79ms
step:1454/2330 train_time:88386ms step_avg:60.79ms
step:1455/2330 train_time:88445ms step_avg:60.79ms
step:1456/2330 train_time:88508ms step_avg:60.79ms
step:1457/2330 train_time:88568ms step_avg:60.79ms
step:1458/2330 train_time:88631ms step_avg:60.79ms
step:1459/2330 train_time:88690ms step_avg:60.79ms
step:1460/2330 train_time:88754ms step_avg:60.79ms
step:1461/2330 train_time:88814ms step_avg:60.79ms
step:1462/2330 train_time:88877ms step_avg:60.79ms
step:1463/2330 train_time:88936ms step_avg:60.79ms
step:1464/2330 train_time:89000ms step_avg:60.79ms
step:1465/2330 train_time:89060ms step_avg:60.79ms
step:1466/2330 train_time:89123ms step_avg:60.79ms
step:1467/2330 train_time:89181ms step_avg:60.79ms
step:1468/2330 train_time:89244ms step_avg:60.79ms
step:1469/2330 train_time:89303ms step_avg:60.79ms
step:1470/2330 train_time:89365ms step_avg:60.79ms
step:1471/2330 train_time:89424ms step_avg:60.79ms
step:1472/2330 train_time:89487ms step_avg:60.79ms
step:1473/2330 train_time:89546ms step_avg:60.79ms
step:1474/2330 train_time:89610ms step_avg:60.79ms
step:1475/2330 train_time:89670ms step_avg:60.79ms
step:1476/2330 train_time:89733ms step_avg:60.79ms
step:1477/2330 train_time:89793ms step_avg:60.79ms
step:1478/2330 train_time:89856ms step_avg:60.80ms
step:1479/2330 train_time:89916ms step_avg:60.79ms
step:1480/2330 train_time:89978ms step_avg:60.80ms
step:1481/2330 train_time:90038ms step_avg:60.80ms
step:1482/2330 train_time:90100ms step_avg:60.80ms
step:1483/2330 train_time:90160ms step_avg:60.80ms
step:1484/2330 train_time:90223ms step_avg:60.80ms
step:1485/2330 train_time:90282ms step_avg:60.80ms
step:1486/2330 train_time:90344ms step_avg:60.80ms
step:1487/2330 train_time:90403ms step_avg:60.80ms
step:1488/2330 train_time:90465ms step_avg:60.80ms
step:1489/2330 train_time:90525ms step_avg:60.80ms
step:1490/2330 train_time:90588ms step_avg:60.80ms
step:1491/2330 train_time:90648ms step_avg:60.80ms
step:1492/2330 train_time:90711ms step_avg:60.80ms
step:1493/2330 train_time:90770ms step_avg:60.80ms
step:1494/2330 train_time:90834ms step_avg:60.80ms
step:1495/2330 train_time:90894ms step_avg:60.80ms
step:1496/2330 train_time:90957ms step_avg:60.80ms
step:1497/2330 train_time:91017ms step_avg:60.80ms
step:1498/2330 train_time:91079ms step_avg:60.80ms
step:1499/2330 train_time:91138ms step_avg:60.80ms
step:1500/2330 train_time:91201ms step_avg:60.80ms
step:1500/2330 val_loss:3.6233 train_time:91272ms step_avg:60.85ms
step:1501/2330 train_time:91294ms step_avg:60.82ms
step:1502/2330 train_time:91325ms step_avg:60.80ms
step:1503/2330 train_time:91386ms step_avg:60.80ms
step:1504/2330 train_time:91455ms step_avg:60.81ms
step:1505/2330 train_time:91515ms step_avg:60.81ms
step:1506/2330 train_time:91579ms step_avg:60.81ms
step:1507/2330 train_time:91639ms step_avg:60.81ms
step:1508/2330 train_time:91701ms step_avg:60.81ms
step:1509/2330 train_time:91759ms step_avg:60.81ms
step:1510/2330 train_time:91821ms step_avg:60.81ms
step:1511/2330 train_time:91879ms step_avg:60.81ms
step:1512/2330 train_time:91943ms step_avg:60.81ms
step:1513/2330 train_time:92001ms step_avg:60.81ms
step:1514/2330 train_time:92063ms step_avg:60.81ms
step:1515/2330 train_time:92122ms step_avg:60.81ms
step:1516/2330 train_time:92184ms step_avg:60.81ms
step:1517/2330 train_time:92245ms step_avg:60.81ms
step:1518/2330 train_time:92310ms step_avg:60.81ms
step:1519/2330 train_time:92371ms step_avg:60.81ms
step:1520/2330 train_time:92435ms step_avg:60.81ms
step:1521/2330 train_time:92495ms step_avg:60.81ms
step:1522/2330 train_time:92559ms step_avg:60.81ms
step:1523/2330 train_time:92618ms step_avg:60.81ms
step:1524/2330 train_time:92682ms step_avg:60.81ms
step:1525/2330 train_time:92741ms step_avg:60.81ms
step:1526/2330 train_time:92803ms step_avg:60.81ms
step:1527/2330 train_time:92862ms step_avg:60.81ms
step:1528/2330 train_time:92924ms step_avg:60.81ms
step:1529/2330 train_time:92984ms step_avg:60.81ms
step:1530/2330 train_time:93046ms step_avg:60.81ms
step:1531/2330 train_time:93105ms step_avg:60.81ms
step:1532/2330 train_time:93168ms step_avg:60.81ms
step:1533/2330 train_time:93228ms step_avg:60.81ms
step:1534/2330 train_time:93292ms step_avg:60.82ms
step:1535/2330 train_time:93352ms step_avg:60.82ms
step:1536/2330 train_time:93415ms step_avg:60.82ms
step:1537/2330 train_time:93476ms step_avg:60.82ms
step:1538/2330 train_time:93540ms step_avg:60.82ms
step:1539/2330 train_time:93600ms step_avg:60.82ms
step:1540/2330 train_time:93664ms step_avg:60.82ms
step:1541/2330 train_time:93723ms step_avg:60.82ms
step:1542/2330 train_time:93785ms step_avg:60.82ms
step:1543/2330 train_time:93845ms step_avg:60.82ms
step:1544/2330 train_time:93907ms step_avg:60.82ms
step:1545/2330 train_time:93967ms step_avg:60.82ms
step:1546/2330 train_time:94030ms step_avg:60.82ms
step:1547/2330 train_time:94089ms step_avg:60.82ms
step:1548/2330 train_time:94152ms step_avg:60.82ms
step:1549/2330 train_time:94211ms step_avg:60.82ms
step:1550/2330 train_time:94275ms step_avg:60.82ms
step:1551/2330 train_time:94334ms step_avg:60.82ms
step:1552/2330 train_time:94398ms step_avg:60.82ms
step:1553/2330 train_time:94459ms step_avg:60.82ms
step:1554/2330 train_time:94522ms step_avg:60.82ms
step:1555/2330 train_time:94581ms step_avg:60.82ms
step:1556/2330 train_time:94645ms step_avg:60.83ms
step:1557/2330 train_time:94705ms step_avg:60.82ms
step:1558/2330 train_time:94768ms step_avg:60.83ms
step:1559/2330 train_time:94827ms step_avg:60.83ms
step:1560/2330 train_time:94890ms step_avg:60.83ms
step:1561/2330 train_time:94949ms step_avg:60.83ms
step:1562/2330 train_time:95012ms step_avg:60.83ms
step:1563/2330 train_time:95071ms step_avg:60.83ms
step:1564/2330 train_time:95134ms step_avg:60.83ms
step:1565/2330 train_time:95194ms step_avg:60.83ms
step:1566/2330 train_time:95257ms step_avg:60.83ms
step:1567/2330 train_time:95317ms step_avg:60.83ms
step:1568/2330 train_time:95381ms step_avg:60.83ms
step:1569/2330 train_time:95441ms step_avg:60.83ms
step:1570/2330 train_time:95504ms step_avg:60.83ms
step:1571/2330 train_time:95564ms step_avg:60.83ms
step:1572/2330 train_time:95628ms step_avg:60.83ms
step:1573/2330 train_time:95689ms step_avg:60.83ms
step:1574/2330 train_time:95752ms step_avg:60.83ms
step:1575/2330 train_time:95810ms step_avg:60.83ms
step:1576/2330 train_time:95874ms step_avg:60.83ms
step:1577/2330 train_time:95933ms step_avg:60.83ms
step:1578/2330 train_time:95996ms step_avg:60.83ms
step:1579/2330 train_time:96056ms step_avg:60.83ms
step:1580/2330 train_time:96119ms step_avg:60.83ms
step:1581/2330 train_time:96179ms step_avg:60.83ms
step:1582/2330 train_time:96242ms step_avg:60.84ms
step:1583/2330 train_time:96303ms step_avg:60.84ms
step:1584/2330 train_time:96366ms step_avg:60.84ms
step:1585/2330 train_time:96426ms step_avg:60.84ms
step:1586/2330 train_time:96489ms step_avg:60.84ms
step:1587/2330 train_time:96550ms step_avg:60.84ms
step:1588/2330 train_time:96612ms step_avg:60.84ms
step:1589/2330 train_time:96671ms step_avg:60.84ms
step:1590/2330 train_time:96735ms step_avg:60.84ms
step:1591/2330 train_time:96795ms step_avg:60.84ms
step:1592/2330 train_time:96858ms step_avg:60.84ms
step:1593/2330 train_time:96919ms step_avg:60.84ms
step:1594/2330 train_time:96982ms step_avg:60.84ms
step:1595/2330 train_time:97042ms step_avg:60.84ms
step:1596/2330 train_time:97105ms step_avg:60.84ms
step:1597/2330 train_time:97163ms step_avg:60.84ms
step:1598/2330 train_time:97227ms step_avg:60.84ms
step:1599/2330 train_time:97287ms step_avg:60.84ms
step:1600/2330 train_time:97351ms step_avg:60.84ms
step:1601/2330 train_time:97410ms step_avg:60.84ms
step:1602/2330 train_time:97473ms step_avg:60.84ms
step:1603/2330 train_time:97532ms step_avg:60.84ms
step:1604/2330 train_time:97596ms step_avg:60.85ms
step:1605/2330 train_time:97656ms step_avg:60.84ms
step:1606/2330 train_time:97719ms step_avg:60.85ms
step:1607/2330 train_time:97779ms step_avg:60.85ms
step:1608/2330 train_time:97843ms step_avg:60.85ms
step:1609/2330 train_time:97903ms step_avg:60.85ms
step:1610/2330 train_time:97966ms step_avg:60.85ms
step:1611/2330 train_time:98026ms step_avg:60.85ms
step:1612/2330 train_time:98089ms step_avg:60.85ms
step:1613/2330 train_time:98148ms step_avg:60.85ms
step:1614/2330 train_time:98212ms step_avg:60.85ms
step:1615/2330 train_time:98271ms step_avg:60.85ms
step:1616/2330 train_time:98334ms step_avg:60.85ms
step:1617/2330 train_time:98394ms step_avg:60.85ms
step:1618/2330 train_time:98457ms step_avg:60.85ms
step:1619/2330 train_time:98516ms step_avg:60.85ms
step:1620/2330 train_time:98580ms step_avg:60.85ms
step:1621/2330 train_time:98641ms step_avg:60.85ms
step:1622/2330 train_time:98704ms step_avg:60.85ms
step:1623/2330 train_time:98764ms step_avg:60.85ms
step:1624/2330 train_time:98827ms step_avg:60.85ms
step:1625/2330 train_time:98887ms step_avg:60.85ms
step:1626/2330 train_time:98950ms step_avg:60.85ms
step:1627/2330 train_time:99009ms step_avg:60.85ms
step:1628/2330 train_time:99072ms step_avg:60.85ms
step:1629/2330 train_time:99131ms step_avg:60.85ms
step:1630/2330 train_time:99194ms step_avg:60.86ms
step:1631/2330 train_time:99254ms step_avg:60.85ms
step:1632/2330 train_time:99316ms step_avg:60.86ms
step:1633/2330 train_time:99376ms step_avg:60.86ms
step:1634/2330 train_time:99439ms step_avg:60.86ms
step:1635/2330 train_time:99500ms step_avg:60.86ms
step:1636/2330 train_time:99564ms step_avg:60.86ms
step:1637/2330 train_time:99624ms step_avg:60.86ms
step:1638/2330 train_time:99687ms step_avg:60.86ms
step:1639/2330 train_time:99747ms step_avg:60.86ms
step:1640/2330 train_time:99809ms step_avg:60.86ms
step:1641/2330 train_time:99869ms step_avg:60.86ms
step:1642/2330 train_time:99932ms step_avg:60.86ms
step:1643/2330 train_time:99992ms step_avg:60.86ms
step:1644/2330 train_time:100055ms step_avg:60.86ms
step:1645/2330 train_time:100115ms step_avg:60.86ms
step:1646/2330 train_time:100179ms step_avg:60.86ms
step:1647/2330 train_time:100240ms step_avg:60.86ms
step:1648/2330 train_time:100303ms step_avg:60.86ms
step:1649/2330 train_time:100362ms step_avg:60.86ms
step:1650/2330 train_time:100425ms step_avg:60.86ms
step:1651/2330 train_time:100484ms step_avg:60.86ms
step:1652/2330 train_time:100548ms step_avg:60.86ms
step:1653/2330 train_time:100607ms step_avg:60.86ms
step:1654/2330 train_time:100670ms step_avg:60.86ms
step:1655/2330 train_time:100729ms step_avg:60.86ms
step:1656/2330 train_time:100793ms step_avg:60.87ms
step:1657/2330 train_time:100853ms step_avg:60.86ms
step:1658/2330 train_time:100916ms step_avg:60.87ms
step:1659/2330 train_time:100975ms step_avg:60.87ms
step:1660/2330 train_time:101038ms step_avg:60.87ms
step:1661/2330 train_time:101098ms step_avg:60.87ms
step:1662/2330 train_time:101161ms step_avg:60.87ms
step:1663/2330 train_time:101221ms step_avg:60.87ms
step:1664/2330 train_time:101283ms step_avg:60.87ms
step:1665/2330 train_time:101343ms step_avg:60.87ms
step:1666/2330 train_time:101406ms step_avg:60.87ms
step:1667/2330 train_time:101467ms step_avg:60.87ms
step:1668/2330 train_time:101529ms step_avg:60.87ms
step:1669/2330 train_time:101588ms step_avg:60.87ms
step:1670/2330 train_time:101652ms step_avg:60.87ms
step:1671/2330 train_time:101711ms step_avg:60.87ms
step:1672/2330 train_time:101774ms step_avg:60.87ms
step:1673/2330 train_time:101834ms step_avg:60.87ms
step:1674/2330 train_time:101896ms step_avg:60.87ms
step:1675/2330 train_time:101956ms step_avg:60.87ms
step:1676/2330 train_time:102019ms step_avg:60.87ms
step:1677/2330 train_time:102079ms step_avg:60.87ms
step:1678/2330 train_time:102143ms step_avg:60.87ms
step:1679/2330 train_time:102202ms step_avg:60.87ms
step:1680/2330 train_time:102265ms step_avg:60.87ms
step:1681/2330 train_time:102325ms step_avg:60.87ms
step:1682/2330 train_time:102388ms step_avg:60.87ms
step:1683/2330 train_time:102448ms step_avg:60.87ms
step:1684/2330 train_time:102511ms step_avg:60.87ms
step:1685/2330 train_time:102572ms step_avg:60.87ms
step:1686/2330 train_time:102635ms step_avg:60.87ms
step:1687/2330 train_time:102695ms step_avg:60.87ms
step:1688/2330 train_time:102759ms step_avg:60.88ms
step:1689/2330 train_time:102820ms step_avg:60.88ms
step:1690/2330 train_time:102883ms step_avg:60.88ms
step:1691/2330 train_time:102943ms step_avg:60.88ms
step:1692/2330 train_time:103005ms step_avg:60.88ms
step:1693/2330 train_time:103065ms step_avg:60.88ms
step:1694/2330 train_time:103128ms step_avg:60.88ms
step:1695/2330 train_time:103188ms step_avg:60.88ms
step:1696/2330 train_time:103251ms step_avg:60.88ms
step:1697/2330 train_time:103310ms step_avg:60.88ms
step:1698/2330 train_time:103373ms step_avg:60.88ms
step:1699/2330 train_time:103433ms step_avg:60.88ms
step:1700/2330 train_time:103496ms step_avg:60.88ms
step:1701/2330 train_time:103556ms step_avg:60.88ms
step:1702/2330 train_time:103619ms step_avg:60.88ms
step:1703/2330 train_time:103678ms step_avg:60.88ms
step:1704/2330 train_time:103741ms step_avg:60.88ms
step:1705/2330 train_time:103801ms step_avg:60.88ms
step:1706/2330 train_time:103864ms step_avg:60.88ms
step:1707/2330 train_time:103924ms step_avg:60.88ms
step:1708/2330 train_time:103988ms step_avg:60.88ms
step:1709/2330 train_time:104048ms step_avg:60.88ms
step:1710/2330 train_time:104110ms step_avg:60.88ms
step:1711/2330 train_time:104169ms step_avg:60.88ms
step:1712/2330 train_time:104232ms step_avg:60.88ms
step:1713/2330 train_time:104292ms step_avg:60.88ms
step:1714/2330 train_time:104355ms step_avg:60.88ms
step:1715/2330 train_time:104415ms step_avg:60.88ms
step:1716/2330 train_time:104478ms step_avg:60.88ms
step:1717/2330 train_time:104538ms step_avg:60.88ms
step:1718/2330 train_time:104601ms step_avg:60.89ms
step:1719/2330 train_time:104661ms step_avg:60.88ms
step:1720/2330 train_time:104724ms step_avg:60.89ms
step:1721/2330 train_time:104783ms step_avg:60.89ms
step:1722/2330 train_time:104846ms step_avg:60.89ms
step:1723/2330 train_time:104906ms step_avg:60.89ms
step:1724/2330 train_time:104969ms step_avg:60.89ms
step:1725/2330 train_time:105028ms step_avg:60.89ms
step:1726/2330 train_time:105092ms step_avg:60.89ms
step:1727/2330 train_time:105152ms step_avg:60.89ms
step:1728/2330 train_time:105214ms step_avg:60.89ms
step:1729/2330 train_time:105274ms step_avg:60.89ms
step:1730/2330 train_time:105337ms step_avg:60.89ms
step:1731/2330 train_time:105397ms step_avg:60.89ms
step:1732/2330 train_time:105460ms step_avg:60.89ms
step:1733/2330 train_time:105520ms step_avg:60.89ms
step:1734/2330 train_time:105584ms step_avg:60.89ms
step:1735/2330 train_time:105644ms step_avg:60.89ms
step:1736/2330 train_time:105707ms step_avg:60.89ms
step:1737/2330 train_time:105766ms step_avg:60.89ms
step:1738/2330 train_time:105830ms step_avg:60.89ms
step:1739/2330 train_time:105890ms step_avg:60.89ms
step:1740/2330 train_time:105953ms step_avg:60.89ms
step:1741/2330 train_time:106012ms step_avg:60.89ms
step:1742/2330 train_time:106075ms step_avg:60.89ms
step:1743/2330 train_time:106135ms step_avg:60.89ms
step:1744/2330 train_time:106199ms step_avg:60.89ms
step:1745/2330 train_time:106259ms step_avg:60.89ms
step:1746/2330 train_time:106321ms step_avg:60.89ms
step:1747/2330 train_time:106381ms step_avg:60.89ms
step:1748/2330 train_time:106445ms step_avg:60.90ms
step:1749/2330 train_time:106504ms step_avg:60.89ms
step:1750/2330 train_time:106568ms step_avg:60.90ms
step:1750/2330 val_loss:3.5664 train_time:106640ms step_avg:60.94ms
step:1751/2330 train_time:106662ms step_avg:60.92ms
step:1752/2330 train_time:106692ms step_avg:60.90ms
step:1753/2330 train_time:106755ms step_avg:60.90ms
step:1754/2330 train_time:106825ms step_avg:60.90ms
step:1755/2330 train_time:106887ms step_avg:60.90ms
step:1756/2330 train_time:106951ms step_avg:60.91ms
step:1757/2330 train_time:107011ms step_avg:60.91ms
step:1758/2330 train_time:107074ms step_avg:60.91ms
step:1759/2330 train_time:107133ms step_avg:60.91ms
step:1760/2330 train_time:107196ms step_avg:60.91ms
step:1761/2330 train_time:107254ms step_avg:60.91ms
step:1762/2330 train_time:107317ms step_avg:60.91ms
step:1763/2330 train_time:107375ms step_avg:60.90ms
step:1764/2330 train_time:107438ms step_avg:60.91ms
step:1765/2330 train_time:107496ms step_avg:60.90ms
step:1766/2330 train_time:107561ms step_avg:60.91ms
step:1767/2330 train_time:107623ms step_avg:60.91ms
step:1768/2330 train_time:107689ms step_avg:60.91ms
step:1769/2330 train_time:107750ms step_avg:60.91ms
step:1770/2330 train_time:107815ms step_avg:60.91ms
step:1771/2330 train_time:107875ms step_avg:60.91ms
step:1772/2330 train_time:107939ms step_avg:60.91ms
step:1773/2330 train_time:107997ms step_avg:60.91ms
step:1774/2330 train_time:108060ms step_avg:60.91ms
step:1775/2330 train_time:108119ms step_avg:60.91ms
step:1776/2330 train_time:108183ms step_avg:60.91ms
step:1777/2330 train_time:108242ms step_avg:60.91ms
step:1778/2330 train_time:108305ms step_avg:60.91ms
step:1779/2330 train_time:108364ms step_avg:60.91ms
step:1780/2330 train_time:108427ms step_avg:60.91ms
step:1781/2330 train_time:108487ms step_avg:60.91ms
step:1782/2330 train_time:108549ms step_avg:60.91ms
step:1783/2330 train_time:108609ms step_avg:60.91ms
step:1784/2330 train_time:108673ms step_avg:60.92ms
step:1785/2330 train_time:108734ms step_avg:60.92ms
step:1786/2330 train_time:108798ms step_avg:60.92ms
step:1787/2330 train_time:108858ms step_avg:60.92ms
step:1788/2330 train_time:108922ms step_avg:60.92ms
step:1789/2330 train_time:108981ms step_avg:60.92ms
step:1790/2330 train_time:109044ms step_avg:60.92ms
step:1791/2330 train_time:109103ms step_avg:60.92ms
step:1792/2330 train_time:109166ms step_avg:60.92ms
step:1793/2330 train_time:109226ms step_avg:60.92ms
step:1794/2330 train_time:109289ms step_avg:60.92ms
step:1795/2330 train_time:109348ms step_avg:60.92ms
step:1796/2330 train_time:109410ms step_avg:60.92ms
step:1797/2330 train_time:109469ms step_avg:60.92ms
step:1798/2330 train_time:109533ms step_avg:60.92ms
step:1799/2330 train_time:109591ms step_avg:60.92ms
step:1800/2330 train_time:109655ms step_avg:60.92ms
step:1801/2330 train_time:109716ms step_avg:60.92ms
step:1802/2330 train_time:109779ms step_avg:60.92ms
step:1803/2330 train_time:109840ms step_avg:60.92ms
step:1804/2330 train_time:109904ms step_avg:60.92ms
step:1805/2330 train_time:109963ms step_avg:60.92ms
step:1806/2330 train_time:110026ms step_avg:60.92ms
step:1807/2330 train_time:110085ms step_avg:60.92ms
step:1808/2330 train_time:110148ms step_avg:60.92ms
step:1809/2330 train_time:110208ms step_avg:60.92ms
step:1810/2330 train_time:110272ms step_avg:60.92ms
step:1811/2330 train_time:110331ms step_avg:60.92ms
step:1812/2330 train_time:110393ms step_avg:60.92ms
step:1813/2330 train_time:110452ms step_avg:60.92ms
step:1814/2330 train_time:110516ms step_avg:60.92ms
step:1815/2330 train_time:110576ms step_avg:60.92ms
step:1816/2330 train_time:110640ms step_avg:60.93ms
step:1817/2330 train_time:110699ms step_avg:60.92ms
step:1818/2330 train_time:110762ms step_avg:60.93ms
step:1819/2330 train_time:110822ms step_avg:60.92ms
step:1820/2330 train_time:110885ms step_avg:60.93ms
step:1821/2330 train_time:110945ms step_avg:60.93ms
step:1822/2330 train_time:111007ms step_avg:60.93ms
step:1823/2330 train_time:111067ms step_avg:60.93ms
step:1824/2330 train_time:111129ms step_avg:60.93ms
step:1825/2330 train_time:111189ms step_avg:60.93ms
step:1826/2330 train_time:111251ms step_avg:60.93ms
step:1827/2330 train_time:111310ms step_avg:60.92ms
step:1828/2330 train_time:111373ms step_avg:60.93ms
step:1829/2330 train_time:111433ms step_avg:60.93ms
step:1830/2330 train_time:111495ms step_avg:60.93ms
step:1831/2330 train_time:111556ms step_avg:60.93ms
step:1832/2330 train_time:111619ms step_avg:60.93ms
step:1833/2330 train_time:111679ms step_avg:60.93ms
step:1834/2330 train_time:111742ms step_avg:60.93ms
step:1835/2330 train_time:111801ms step_avg:60.93ms
step:1836/2330 train_time:111865ms step_avg:60.93ms
step:1837/2330 train_time:111924ms step_avg:60.93ms
step:1838/2330 train_time:111988ms step_avg:60.93ms
step:1839/2330 train_time:112047ms step_avg:60.93ms
step:1840/2330 train_time:112111ms step_avg:60.93ms
step:1841/2330 train_time:112170ms step_avg:60.93ms
step:1842/2330 train_time:112233ms step_avg:60.93ms
step:1843/2330 train_time:112292ms step_avg:60.93ms
step:1844/2330 train_time:112355ms step_avg:60.93ms
step:1845/2330 train_time:112414ms step_avg:60.93ms
step:1846/2330 train_time:112478ms step_avg:60.93ms
step:1847/2330 train_time:112538ms step_avg:60.93ms
step:1848/2330 train_time:112601ms step_avg:60.93ms
step:1849/2330 train_time:112660ms step_avg:60.93ms
step:1850/2330 train_time:112724ms step_avg:60.93ms
step:1851/2330 train_time:112783ms step_avg:60.93ms
step:1852/2330 train_time:112846ms step_avg:60.93ms
step:1853/2330 train_time:112906ms step_avg:60.93ms
step:1854/2330 train_time:112969ms step_avg:60.93ms
step:1855/2330 train_time:113029ms step_avg:60.93ms
step:1856/2330 train_time:113093ms step_avg:60.93ms
step:1857/2330 train_time:113153ms step_avg:60.93ms
step:1858/2330 train_time:113216ms step_avg:60.93ms
step:1859/2330 train_time:113275ms step_avg:60.93ms
step:1860/2330 train_time:113339ms step_avg:60.93ms
step:1861/2330 train_time:113398ms step_avg:60.93ms
step:1862/2330 train_time:113462ms step_avg:60.94ms
step:1863/2330 train_time:113521ms step_avg:60.93ms
step:1864/2330 train_time:113584ms step_avg:60.94ms
step:1865/2330 train_time:113644ms step_avg:60.93ms
step:1866/2330 train_time:113707ms step_avg:60.94ms
step:1867/2330 train_time:113767ms step_avg:60.94ms
step:1868/2330 train_time:113831ms step_avg:60.94ms
step:1869/2330 train_time:113891ms step_avg:60.94ms
step:1870/2330 train_time:113953ms step_avg:60.94ms
step:1871/2330 train_time:114012ms step_avg:60.94ms
step:1872/2330 train_time:114076ms step_avg:60.94ms
step:1873/2330 train_time:114137ms step_avg:60.94ms
step:1874/2330 train_time:114199ms step_avg:60.94ms
step:1875/2330 train_time:114259ms step_avg:60.94ms
step:1876/2330 train_time:114322ms step_avg:60.94ms
step:1877/2330 train_time:114381ms step_avg:60.94ms
step:1878/2330 train_time:114444ms step_avg:60.94ms
step:1879/2330 train_time:114503ms step_avg:60.94ms
step:1880/2330 train_time:114566ms step_avg:60.94ms
step:1881/2330 train_time:114625ms step_avg:60.94ms
step:1882/2330 train_time:114688ms step_avg:60.94ms
step:1883/2330 train_time:114748ms step_avg:60.94ms
step:1884/2330 train_time:114811ms step_avg:60.94ms
step:1885/2330 train_time:114871ms step_avg:60.94ms
step:1886/2330 train_time:114934ms step_avg:60.94ms
step:1887/2330 train_time:114993ms step_avg:60.94ms
step:1888/2330 train_time:115056ms step_avg:60.94ms
step:1889/2330 train_time:115116ms step_avg:60.94ms
step:1890/2330 train_time:115180ms step_avg:60.94ms
step:1891/2330 train_time:115240ms step_avg:60.94ms
step:1892/2330 train_time:115304ms step_avg:60.94ms
step:1893/2330 train_time:115363ms step_avg:60.94ms
step:1894/2330 train_time:115426ms step_avg:60.94ms
step:1895/2330 train_time:115485ms step_avg:60.94ms
step:1896/2330 train_time:115549ms step_avg:60.94ms
step:1897/2330 train_time:115609ms step_avg:60.94ms
step:1898/2330 train_time:115671ms step_avg:60.94ms
step:1899/2330 train_time:115731ms step_avg:60.94ms
step:1900/2330 train_time:115794ms step_avg:60.94ms
step:1901/2330 train_time:115854ms step_avg:60.94ms
step:1902/2330 train_time:115918ms step_avg:60.95ms
step:1903/2330 train_time:115978ms step_avg:60.94ms
step:1904/2330 train_time:116041ms step_avg:60.95ms
step:1905/2330 train_time:116100ms step_avg:60.95ms
step:1906/2330 train_time:116164ms step_avg:60.95ms
step:1907/2330 train_time:116223ms step_avg:60.95ms
step:1908/2330 train_time:116287ms step_avg:60.95ms
step:1909/2330 train_time:116346ms step_avg:60.95ms
step:1910/2330 train_time:116409ms step_avg:60.95ms
step:1911/2330 train_time:116470ms step_avg:60.95ms
step:1912/2330 train_time:116533ms step_avg:60.95ms
step:1913/2330 train_time:116592ms step_avg:60.95ms
step:1914/2330 train_time:116655ms step_avg:60.95ms
step:1915/2330 train_time:116714ms step_avg:60.95ms
step:1916/2330 train_time:116777ms step_avg:60.95ms
step:1917/2330 train_time:116837ms step_avg:60.95ms
step:1918/2330 train_time:116900ms step_avg:60.95ms
step:1919/2330 train_time:116959ms step_avg:60.95ms
step:1920/2330 train_time:117024ms step_avg:60.95ms
step:1921/2330 train_time:117083ms step_avg:60.95ms
step:1922/2330 train_time:117147ms step_avg:60.95ms
step:1923/2330 train_time:117206ms step_avg:60.95ms
step:1924/2330 train_time:117269ms step_avg:60.95ms
step:1925/2330 train_time:117329ms step_avg:60.95ms
step:1926/2330 train_time:117391ms step_avg:60.95ms
step:1927/2330 train_time:117451ms step_avg:60.95ms
step:1928/2330 train_time:117514ms step_avg:60.95ms
step:1929/2330 train_time:117574ms step_avg:60.95ms
step:1930/2330 train_time:117638ms step_avg:60.95ms
step:1931/2330 train_time:117697ms step_avg:60.95ms
step:1932/2330 train_time:117760ms step_avg:60.95ms
step:1933/2330 train_time:117820ms step_avg:60.95ms
step:1934/2330 train_time:117884ms step_avg:60.95ms
step:1935/2330 train_time:117943ms step_avg:60.95ms
step:1936/2330 train_time:118006ms step_avg:60.95ms
step:1937/2330 train_time:118066ms step_avg:60.95ms
step:1938/2330 train_time:118129ms step_avg:60.95ms
step:1939/2330 train_time:118189ms step_avg:60.95ms
step:1940/2330 train_time:118253ms step_avg:60.96ms
step:1941/2330 train_time:118312ms step_avg:60.95ms
step:1942/2330 train_time:118375ms step_avg:60.96ms
step:1943/2330 train_time:118436ms step_avg:60.96ms
step:1944/2330 train_time:118498ms step_avg:60.96ms
step:1945/2330 train_time:118558ms step_avg:60.96ms
step:1946/2330 train_time:118622ms step_avg:60.96ms
step:1947/2330 train_time:118681ms step_avg:60.96ms
step:1948/2330 train_time:118746ms step_avg:60.96ms
step:1949/2330 train_time:118805ms step_avg:60.96ms
step:1950/2330 train_time:118868ms step_avg:60.96ms
step:1951/2330 train_time:118927ms step_avg:60.96ms
step:1952/2330 train_time:118990ms step_avg:60.96ms
step:1953/2330 train_time:119049ms step_avg:60.96ms
step:1954/2330 train_time:119112ms step_avg:60.96ms
step:1955/2330 train_time:119172ms step_avg:60.96ms
step:1956/2330 train_time:119236ms step_avg:60.96ms
step:1957/2330 train_time:119296ms step_avg:60.96ms
step:1958/2330 train_time:119359ms step_avg:60.96ms
step:1959/2330 train_time:119419ms step_avg:60.96ms
step:1960/2330 train_time:119482ms step_avg:60.96ms
step:1961/2330 train_time:119541ms step_avg:60.96ms
step:1962/2330 train_time:119604ms step_avg:60.96ms
step:1963/2330 train_time:119664ms step_avg:60.96ms
step:1964/2330 train_time:119727ms step_avg:60.96ms
step:1965/2330 train_time:119787ms step_avg:60.96ms
step:1966/2330 train_time:119851ms step_avg:60.96ms
step:1967/2330 train_time:119910ms step_avg:60.96ms
step:1968/2330 train_time:119973ms step_avg:60.96ms
step:1969/2330 train_time:120032ms step_avg:60.96ms
step:1970/2330 train_time:120096ms step_avg:60.96ms
step:1971/2330 train_time:120156ms step_avg:60.96ms
step:1972/2330 train_time:120219ms step_avg:60.96ms
step:1973/2330 train_time:120279ms step_avg:60.96ms
step:1974/2330 train_time:120342ms step_avg:60.96ms
step:1975/2330 train_time:120401ms step_avg:60.96ms
step:1976/2330 train_time:120464ms step_avg:60.96ms
step:1977/2330 train_time:120524ms step_avg:60.96ms
step:1978/2330 train_time:120587ms step_avg:60.96ms
step:1979/2330 train_time:120647ms step_avg:60.96ms
step:1980/2330 train_time:120710ms step_avg:60.96ms
step:1981/2330 train_time:120769ms step_avg:60.96ms
step:1982/2330 train_time:120833ms step_avg:60.97ms
step:1983/2330 train_time:120891ms step_avg:60.96ms
step:1984/2330 train_time:120954ms step_avg:60.96ms
step:1985/2330 train_time:121013ms step_avg:60.96ms
step:1986/2330 train_time:121077ms step_avg:60.97ms
step:1987/2330 train_time:121136ms step_avg:60.96ms
step:1988/2330 train_time:121199ms step_avg:60.97ms
step:1989/2330 train_time:121258ms step_avg:60.96ms
step:1990/2330 train_time:121322ms step_avg:60.97ms
step:1991/2330 train_time:121383ms step_avg:60.97ms
step:1992/2330 train_time:121446ms step_avg:60.97ms
step:1993/2330 train_time:121505ms step_avg:60.97ms
step:1994/2330 train_time:121568ms step_avg:60.97ms
step:1995/2330 train_time:121628ms step_avg:60.97ms
step:1996/2330 train_time:121691ms step_avg:60.97ms
step:1997/2330 train_time:121751ms step_avg:60.97ms
step:1998/2330 train_time:121814ms step_avg:60.97ms
step:1999/2330 train_time:121874ms step_avg:60.97ms
step:2000/2330 train_time:121938ms step_avg:60.97ms
step:2000/2330 val_loss:3.5329 train_time:122010ms step_avg:61.00ms
step:2001/2330 train_time:122031ms step_avg:60.99ms
step:2002/2330 train_time:122063ms step_avg:60.97ms
step:2003/2330 train_time:122126ms step_avg:60.97ms
step:2004/2330 train_time:122192ms step_avg:60.97ms
step:2005/2330 train_time:122253ms step_avg:60.97ms
step:2006/2330 train_time:122317ms step_avg:60.98ms
step:2007/2330 train_time:122376ms step_avg:60.97ms
step:2008/2330 train_time:122438ms step_avg:60.97ms
step:2009/2330 train_time:122497ms step_avg:60.97ms
step:2010/2330 train_time:122561ms step_avg:60.98ms
step:2011/2330 train_time:122620ms step_avg:60.97ms
step:2012/2330 train_time:122682ms step_avg:60.98ms
step:2013/2330 train_time:122741ms step_avg:60.97ms
step:2014/2330 train_time:122804ms step_avg:60.98ms
step:2015/2330 train_time:122863ms step_avg:60.97ms
step:2016/2330 train_time:122926ms step_avg:60.98ms
step:2017/2330 train_time:122986ms step_avg:60.97ms
step:2018/2330 train_time:123051ms step_avg:60.98ms
step:2019/2330 train_time:123113ms step_avg:60.98ms
step:2020/2330 train_time:123176ms step_avg:60.98ms
step:2021/2330 train_time:123237ms step_avg:60.98ms
step:2022/2330 train_time:123300ms step_avg:60.98ms
step:2023/2330 train_time:123360ms step_avg:60.98ms
step:2024/2330 train_time:123423ms step_avg:60.98ms
step:2025/2330 train_time:123482ms step_avg:60.98ms
step:2026/2330 train_time:123546ms step_avg:60.98ms
step:2027/2330 train_time:123605ms step_avg:60.98ms
step:2028/2330 train_time:123669ms step_avg:60.98ms
step:2029/2330 train_time:123727ms step_avg:60.98ms
step:2030/2330 train_time:123790ms step_avg:60.98ms
step:2031/2330 train_time:123849ms step_avg:60.98ms
step:2032/2330 train_time:123912ms step_avg:60.98ms
step:2033/2330 train_time:123971ms step_avg:60.98ms
step:2034/2330 train_time:124035ms step_avg:60.98ms
step:2035/2330 train_time:124096ms step_avg:60.98ms
step:2036/2330 train_time:124160ms step_avg:60.98ms
step:2037/2330 train_time:124220ms step_avg:60.98ms
step:2038/2330 train_time:124283ms step_avg:60.98ms
step:2039/2330 train_time:124343ms step_avg:60.98ms
step:2040/2330 train_time:124406ms step_avg:60.98ms
step:2041/2330 train_time:124466ms step_avg:60.98ms
step:2042/2330 train_time:124529ms step_avg:60.98ms
step:2043/2330 train_time:124589ms step_avg:60.98ms
step:2044/2330 train_time:124652ms step_avg:60.98ms
step:2045/2330 train_time:124712ms step_avg:60.98ms
step:2046/2330 train_time:124776ms step_avg:60.99ms
step:2047/2330 train_time:124834ms step_avg:60.98ms
step:2048/2330 train_time:124897ms step_avg:60.98ms
step:2049/2330 train_time:124956ms step_avg:60.98ms
step:2050/2330 train_time:125020ms step_avg:60.99ms
step:2051/2330 train_time:125080ms step_avg:60.98ms
step:2052/2330 train_time:125144ms step_avg:60.99ms
step:2053/2330 train_time:125204ms step_avg:60.99ms
step:2054/2330 train_time:125268ms step_avg:60.99ms
step:2055/2330 train_time:125327ms step_avg:60.99ms
step:2056/2330 train_time:125390ms step_avg:60.99ms
step:2057/2330 train_time:125449ms step_avg:60.99ms
step:2058/2330 train_time:125513ms step_avg:60.99ms
step:2059/2330 train_time:125573ms step_avg:60.99ms
step:2060/2330 train_time:125635ms step_avg:60.99ms
step:2061/2330 train_time:125695ms step_avg:60.99ms
step:2062/2330 train_time:125759ms step_avg:60.99ms
step:2063/2330 train_time:125818ms step_avg:60.99ms
step:2064/2330 train_time:125881ms step_avg:60.99ms
step:2065/2330 train_time:125940ms step_avg:60.99ms
step:2066/2330 train_time:126004ms step_avg:60.99ms
step:2067/2330 train_time:126065ms step_avg:60.99ms
step:2068/2330 train_time:126128ms step_avg:60.99ms
step:2069/2330 train_time:126188ms step_avg:60.99ms
step:2070/2330 train_time:126251ms step_avg:60.99ms
step:2071/2330 train_time:126312ms step_avg:60.99ms
step:2072/2330 train_time:126376ms step_avg:60.99ms
step:2073/2330 train_time:126435ms step_avg:60.99ms
step:2074/2330 train_time:126498ms step_avg:60.99ms
step:2075/2330 train_time:126559ms step_avg:60.99ms
step:2076/2330 train_time:126621ms step_avg:60.99ms
step:2077/2330 train_time:126680ms step_avg:60.99ms
step:2078/2330 train_time:126744ms step_avg:60.99ms
step:2079/2330 train_time:126803ms step_avg:60.99ms
step:2080/2330 train_time:126867ms step_avg:60.99ms
step:2081/2330 train_time:126926ms step_avg:60.99ms
step:2082/2330 train_time:126989ms step_avg:60.99ms
step:2083/2330 train_time:127049ms step_avg:60.99ms
step:2084/2330 train_time:127113ms step_avg:60.99ms
step:2085/2330 train_time:127173ms step_avg:60.99ms
step:2086/2330 train_time:127236ms step_avg:61.00ms
step:2087/2330 train_time:127296ms step_avg:60.99ms
step:2088/2330 train_time:127359ms step_avg:61.00ms
step:2089/2330 train_time:127419ms step_avg:61.00ms
step:2090/2330 train_time:127482ms step_avg:61.00ms
step:2091/2330 train_time:127541ms step_avg:61.00ms
step:2092/2330 train_time:127605ms step_avg:61.00ms
step:2093/2330 train_time:127664ms step_avg:61.00ms
step:2094/2330 train_time:127727ms step_avg:61.00ms
step:2095/2330 train_time:127786ms step_avg:61.00ms
step:2096/2330 train_time:127850ms step_avg:61.00ms
step:2097/2330 train_time:127909ms step_avg:61.00ms
step:2098/2330 train_time:127972ms step_avg:61.00ms
step:2099/2330 train_time:128033ms step_avg:61.00ms
step:2100/2330 train_time:128096ms step_avg:61.00ms
step:2101/2330 train_time:128155ms step_avg:61.00ms
step:2102/2330 train_time:128219ms step_avg:61.00ms
step:2103/2330 train_time:128278ms step_avg:61.00ms
step:2104/2330 train_time:128342ms step_avg:61.00ms
step:2105/2330 train_time:128402ms step_avg:61.00ms
step:2106/2330 train_time:128465ms step_avg:61.00ms
step:2107/2330 train_time:128524ms step_avg:61.00ms
step:2108/2330 train_time:128588ms step_avg:61.00ms
step:2109/2330 train_time:128647ms step_avg:61.00ms
step:2110/2330 train_time:128710ms step_avg:61.00ms
step:2111/2330 train_time:128770ms step_avg:61.00ms
step:2112/2330 train_time:128833ms step_avg:61.00ms
step:2113/2330 train_time:128892ms step_avg:61.00ms
step:2114/2330 train_time:128956ms step_avg:61.00ms
step:2115/2330 train_time:129015ms step_avg:61.00ms
step:2116/2330 train_time:129078ms step_avg:61.00ms
step:2117/2330 train_time:129137ms step_avg:61.00ms
step:2118/2330 train_time:129200ms step_avg:61.00ms
step:2119/2330 train_time:129260ms step_avg:61.00ms
step:2120/2330 train_time:129323ms step_avg:61.00ms
step:2121/2330 train_time:129383ms step_avg:61.00ms
step:2122/2330 train_time:129448ms step_avg:61.00ms
step:2123/2330 train_time:129507ms step_avg:61.00ms
step:2124/2330 train_time:129570ms step_avg:61.00ms
step:2125/2330 train_time:129629ms step_avg:61.00ms
step:2126/2330 train_time:129692ms step_avg:61.00ms
step:2127/2330 train_time:129751ms step_avg:61.00ms
step:2128/2330 train_time:129815ms step_avg:61.00ms
step:2129/2330 train_time:129875ms step_avg:61.00ms
step:2130/2330 train_time:129938ms step_avg:61.00ms
step:2131/2330 train_time:129997ms step_avg:61.00ms
step:2132/2330 train_time:130061ms step_avg:61.00ms
step:2133/2330 train_time:130120ms step_avg:61.00ms
step:2134/2330 train_time:130184ms step_avg:61.00ms
step:2135/2330 train_time:130243ms step_avg:61.00ms
step:2136/2330 train_time:130307ms step_avg:61.01ms
step:2137/2330 train_time:130367ms step_avg:61.00ms
step:2138/2330 train_time:130429ms step_avg:61.01ms
step:2139/2330 train_time:130489ms step_avg:61.00ms
step:2140/2330 train_time:130551ms step_avg:61.01ms
step:2141/2330 train_time:130611ms step_avg:61.00ms
step:2142/2330 train_time:130675ms step_avg:61.01ms
step:2143/2330 train_time:130734ms step_avg:61.01ms
step:2144/2330 train_time:130796ms step_avg:61.01ms
step:2145/2330 train_time:130857ms step_avg:61.01ms
step:2146/2330 train_time:130919ms step_avg:61.01ms
step:2147/2330 train_time:130978ms step_avg:61.01ms
step:2148/2330 train_time:131042ms step_avg:61.01ms
step:2149/2330 train_time:131102ms step_avg:61.01ms
step:2150/2330 train_time:131166ms step_avg:61.01ms
step:2151/2330 train_time:131225ms step_avg:61.01ms
step:2152/2330 train_time:131289ms step_avg:61.01ms
step:2153/2330 train_time:131348ms step_avg:61.01ms
step:2154/2330 train_time:131411ms step_avg:61.01ms
step:2155/2330 train_time:131471ms step_avg:61.01ms
step:2156/2330 train_time:131534ms step_avg:61.01ms
step:2157/2330 train_time:131594ms step_avg:61.01ms
step:2158/2330 train_time:131657ms step_avg:61.01ms
step:2159/2330 train_time:131717ms step_avg:61.01ms
step:2160/2330 train_time:131780ms step_avg:61.01ms
step:2161/2330 train_time:131839ms step_avg:61.01ms
step:2162/2330 train_time:131902ms step_avg:61.01ms
step:2163/2330 train_time:131962ms step_avg:61.01ms
step:2164/2330 train_time:132025ms step_avg:61.01ms
step:2165/2330 train_time:132085ms step_avg:61.01ms
step:2166/2330 train_time:132149ms step_avg:61.01ms
step:2167/2330 train_time:132208ms step_avg:61.01ms
step:2168/2330 train_time:132271ms step_avg:61.01ms
step:2169/2330 train_time:132331ms step_avg:61.01ms
step:2170/2330 train_time:132394ms step_avg:61.01ms
step:2171/2330 train_time:132454ms step_avg:61.01ms
step:2172/2330 train_time:132517ms step_avg:61.01ms
step:2173/2330 train_time:132576ms step_avg:61.01ms
step:2174/2330 train_time:132640ms step_avg:61.01ms
step:2175/2330 train_time:132700ms step_avg:61.01ms
step:2176/2330 train_time:132763ms step_avg:61.01ms
step:2177/2330 train_time:132822ms step_avg:61.01ms
step:2178/2330 train_time:132884ms step_avg:61.01ms
step:2179/2330 train_time:132944ms step_avg:61.01ms
step:2180/2330 train_time:133008ms step_avg:61.01ms
step:2181/2330 train_time:133067ms step_avg:61.01ms
step:2182/2330 train_time:133131ms step_avg:61.01ms
step:2183/2330 train_time:133190ms step_avg:61.01ms
step:2184/2330 train_time:133253ms step_avg:61.01ms
step:2185/2330 train_time:133313ms step_avg:61.01ms
step:2186/2330 train_time:133377ms step_avg:61.01ms
step:2187/2330 train_time:133437ms step_avg:61.01ms
step:2188/2330 train_time:133500ms step_avg:61.01ms
step:2189/2330 train_time:133560ms step_avg:61.01ms
step:2190/2330 train_time:133622ms step_avg:61.01ms
step:2191/2330 train_time:133681ms step_avg:61.01ms
step:2192/2330 train_time:133746ms step_avg:61.02ms
step:2193/2330 train_time:133804ms step_avg:61.01ms
step:2194/2330 train_time:133868ms step_avg:61.02ms
step:2195/2330 train_time:133926ms step_avg:61.01ms
step:2196/2330 train_time:133989ms step_avg:61.02ms
step:2197/2330 train_time:134049ms step_avg:61.01ms
step:2198/2330 train_time:134113ms step_avg:61.02ms
step:2199/2330 train_time:134173ms step_avg:61.02ms
step:2200/2330 train_time:134236ms step_avg:61.02ms
step:2201/2330 train_time:134295ms step_avg:61.02ms
step:2202/2330 train_time:134359ms step_avg:61.02ms
step:2203/2330 train_time:134418ms step_avg:61.02ms
step:2204/2330 train_time:134482ms step_avg:61.02ms
step:2205/2330 train_time:134542ms step_avg:61.02ms
step:2206/2330 train_time:134604ms step_avg:61.02ms
step:2207/2330 train_time:134664ms step_avg:61.02ms
step:2208/2330 train_time:134727ms step_avg:61.02ms
step:2209/2330 train_time:134786ms step_avg:61.02ms
step:2210/2330 train_time:134850ms step_avg:61.02ms
step:2211/2330 train_time:134909ms step_avg:61.02ms
step:2212/2330 train_time:134973ms step_avg:61.02ms
step:2213/2330 train_time:135032ms step_avg:61.02ms
step:2214/2330 train_time:135096ms step_avg:61.02ms
step:2215/2330 train_time:135156ms step_avg:61.02ms
step:2216/2330 train_time:135220ms step_avg:61.02ms
step:2217/2330 train_time:135280ms step_avg:61.02ms
step:2218/2330 train_time:135343ms step_avg:61.02ms
step:2219/2330 train_time:135402ms step_avg:61.02ms
step:2220/2330 train_time:135467ms step_avg:61.02ms
step:2221/2330 train_time:135525ms step_avg:61.02ms
step:2222/2330 train_time:135589ms step_avg:61.02ms
step:2223/2330 train_time:135649ms step_avg:61.02ms
step:2224/2330 train_time:135712ms step_avg:61.02ms
step:2225/2330 train_time:135772ms step_avg:61.02ms
step:2226/2330 train_time:135835ms step_avg:61.02ms
step:2227/2330 train_time:135894ms step_avg:61.02ms
step:2228/2330 train_time:135958ms step_avg:61.02ms
step:2229/2330 train_time:136017ms step_avg:61.02ms
step:2230/2330 train_time:136079ms step_avg:61.02ms
step:2231/2330 train_time:136139ms step_avg:61.02ms
step:2232/2330 train_time:136202ms step_avg:61.02ms
step:2233/2330 train_time:136262ms step_avg:61.02ms
step:2234/2330 train_time:136325ms step_avg:61.02ms
step:2235/2330 train_time:136384ms step_avg:61.02ms
step:2236/2330 train_time:136448ms step_avg:61.02ms
step:2237/2330 train_time:136507ms step_avg:61.02ms
step:2238/2330 train_time:136571ms step_avg:61.02ms
step:2239/2330 train_time:136629ms step_avg:61.02ms
step:2240/2330 train_time:136693ms step_avg:61.02ms
step:2241/2330 train_time:136753ms step_avg:61.02ms
step:2242/2330 train_time:136816ms step_avg:61.02ms
step:2243/2330 train_time:136875ms step_avg:61.02ms
step:2244/2330 train_time:136938ms step_avg:61.02ms
step:2245/2330 train_time:136998ms step_avg:61.02ms
step:2246/2330 train_time:137061ms step_avg:61.02ms
step:2247/2330 train_time:137121ms step_avg:61.02ms
step:2248/2330 train_time:137183ms step_avg:61.02ms
step:2249/2330 train_time:137242ms step_avg:61.02ms
step:2250/2330 train_time:137306ms step_avg:61.03ms
step:2250/2330 val_loss:3.5037 train_time:137378ms step_avg:61.06ms
step:2251/2330 train_time:137401ms step_avg:61.04ms
step:2252/2330 train_time:137433ms step_avg:61.03ms
step:2253/2330 train_time:137495ms step_avg:61.03ms
step:2254/2330 train_time:137562ms step_avg:61.03ms
step:2255/2330 train_time:137623ms step_avg:61.03ms
step:2256/2330 train_time:137686ms step_avg:61.03ms
step:2257/2330 train_time:137746ms step_avg:61.03ms
step:2258/2330 train_time:137810ms step_avg:61.03ms
step:2259/2330 train_time:137868ms step_avg:61.03ms
step:2260/2330 train_time:137932ms step_avg:61.03ms
step:2261/2330 train_time:137991ms step_avg:61.03ms
step:2262/2330 train_time:138053ms step_avg:61.03ms
step:2263/2330 train_time:138112ms step_avg:61.03ms
step:2264/2330 train_time:138174ms step_avg:61.03ms
step:2265/2330 train_time:138233ms step_avg:61.03ms
step:2266/2330 train_time:138296ms step_avg:61.03ms
step:2267/2330 train_time:138356ms step_avg:61.03ms
step:2268/2330 train_time:138421ms step_avg:61.03ms
step:2269/2330 train_time:138482ms step_avg:61.03ms
step:2270/2330 train_time:138546ms step_avg:61.03ms
step:2271/2330 train_time:138605ms step_avg:61.03ms
step:2272/2330 train_time:138669ms step_avg:61.03ms
step:2273/2330 train_time:138728ms step_avg:61.03ms
step:2274/2330 train_time:138792ms step_avg:61.03ms
step:2275/2330 train_time:138850ms step_avg:61.03ms
step:2276/2330 train_time:138913ms step_avg:61.03ms
step:2277/2330 train_time:138972ms step_avg:61.03ms
step:2278/2330 train_time:139035ms step_avg:61.03ms
step:2279/2330 train_time:139094ms step_avg:61.03ms
step:2280/2330 train_time:139156ms step_avg:61.03ms
step:2281/2330 train_time:139215ms step_avg:61.03ms
step:2282/2330 train_time:139279ms step_avg:61.03ms
step:2283/2330 train_time:139339ms step_avg:61.03ms
step:2284/2330 train_time:139403ms step_avg:61.03ms
step:2285/2330 train_time:139463ms step_avg:61.03ms
step:2286/2330 train_time:139526ms step_avg:61.04ms
step:2287/2330 train_time:139586ms step_avg:61.03ms
step:2288/2330 train_time:139649ms step_avg:61.04ms
step:2289/2330 train_time:139709ms step_avg:61.03ms
step:2290/2330 train_time:139773ms step_avg:61.04ms
step:2291/2330 train_time:139833ms step_avg:61.04ms
step:2292/2330 train_time:139896ms step_avg:61.04ms
step:2293/2330 train_time:139954ms step_avg:61.04ms
step:2294/2330 train_time:140018ms step_avg:61.04ms
step:2295/2330 train_time:140076ms step_avg:61.04ms
step:2296/2330 train_time:140139ms step_avg:61.04ms
step:2297/2330 train_time:140197ms step_avg:61.03ms
step:2298/2330 train_time:140261ms step_avg:61.04ms
step:2299/2330 train_time:140321ms step_avg:61.04ms
step:2300/2330 train_time:140385ms step_avg:61.04ms
step:2301/2330 train_time:140445ms step_avg:61.04ms
step:2302/2330 train_time:140508ms step_avg:61.04ms
step:2303/2330 train_time:140568ms step_avg:61.04ms
step:2304/2330 train_time:140631ms step_avg:61.04ms
step:2305/2330 train_time:140691ms step_avg:61.04ms
step:2306/2330 train_time:140754ms step_avg:61.04ms
step:2307/2330 train_time:140814ms step_avg:61.04ms
step:2308/2330 train_time:140878ms step_avg:61.04ms
step:2309/2330 train_time:140938ms step_avg:61.04ms
step:2310/2330 train_time:141000ms step_avg:61.04ms
step:2311/2330 train_time:141060ms step_avg:61.04ms
step:2312/2330 train_time:141122ms step_avg:61.04ms
step:2313/2330 train_time:141182ms step_avg:61.04ms
step:2314/2330 train_time:141245ms step_avg:61.04ms
step:2315/2330 train_time:141304ms step_avg:61.04ms
step:2316/2330 train_time:141367ms step_avg:61.04ms
step:2317/2330 train_time:141428ms step_avg:61.04ms
step:2318/2330 train_time:141492ms step_avg:61.04ms
step:2319/2330 train_time:141551ms step_avg:61.04ms
step:2320/2330 train_time:141615ms step_avg:61.04ms
step:2321/2330 train_time:141675ms step_avg:61.04ms
step:2322/2330 train_time:141739ms step_avg:61.04ms
step:2323/2330 train_time:141798ms step_avg:61.04ms
step:2324/2330 train_time:141862ms step_avg:61.04ms
step:2325/2330 train_time:141922ms step_avg:61.04ms
step:2326/2330 train_time:141985ms step_avg:61.04ms
step:2327/2330 train_time:142044ms step_avg:61.04ms
step:2328/2330 train_time:142106ms step_avg:61.04ms
step:2329/2330 train_time:142166ms step_avg:61.04ms
step:2330/2330 train_time:142228ms step_avg:61.04ms
step:2330/2330 val_loss:3.4790 train_time:142301ms step_avg:61.07ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
