import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-1"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:46:45 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:94ms step_avg:93.95ms
step:2/2330 train_time:210ms step_avg:105.05ms
step:3/2330 train_time:232ms step_avg:77.39ms
step:4/2330 train_time:267ms step_avg:66.77ms
step:5/2330 train_time:324ms step_avg:64.90ms
step:6/2330 train_time:386ms step_avg:64.32ms
step:7/2330 train_time:444ms step_avg:63.42ms
step:8/2330 train_time:505ms step_avg:63.18ms
step:9/2330 train_time:564ms step_avg:62.63ms
step:10/2330 train_time:626ms step_avg:62.57ms
step:11/2330 train_time:684ms step_avg:62.18ms
step:12/2330 train_time:746ms step_avg:62.13ms
step:13/2330 train_time:804ms step_avg:61.84ms
step:14/2330 train_time:866ms step_avg:61.83ms
step:15/2330 train_time:924ms step_avg:61.61ms
step:16/2330 train_time:987ms step_avg:61.70ms
step:17/2330 train_time:1050ms step_avg:61.78ms
step:18/2330 train_time:1116ms step_avg:61.99ms
step:19/2330 train_time:1178ms step_avg:62.01ms
step:20/2330 train_time:1241ms step_avg:62.07ms
step:21/2330 train_time:1302ms step_avg:62.00ms
step:22/2330 train_time:1364ms step_avg:62.02ms
step:23/2330 train_time:1424ms step_avg:61.89ms
step:24/2330 train_time:1485ms step_avg:61.89ms
step:25/2330 train_time:1544ms step_avg:61.75ms
step:26/2330 train_time:1605ms step_avg:61.73ms
step:27/2330 train_time:1664ms step_avg:61.62ms
step:28/2330 train_time:1725ms step_avg:61.62ms
step:29/2330 train_time:1783ms step_avg:61.50ms
step:30/2330 train_time:1845ms step_avg:61.50ms
step:31/2330 train_time:1904ms step_avg:61.41ms
step:32/2330 train_time:1967ms step_avg:61.45ms
step:33/2330 train_time:2028ms step_avg:61.45ms
step:34/2330 train_time:2093ms step_avg:61.55ms
step:35/2330 train_time:2153ms step_avg:61.52ms
step:36/2330 train_time:2217ms step_avg:61.57ms
step:37/2330 train_time:2277ms step_avg:61.53ms
step:38/2330 train_time:2339ms step_avg:61.54ms
step:39/2330 train_time:2398ms step_avg:61.47ms
step:40/2330 train_time:2459ms step_avg:61.49ms
step:41/2330 train_time:2520ms step_avg:61.46ms
step:42/2330 train_time:2582ms step_avg:61.47ms
step:43/2330 train_time:2641ms step_avg:61.42ms
step:44/2330 train_time:2702ms step_avg:61.42ms
step:45/2330 train_time:2761ms step_avg:61.36ms
step:46/2330 train_time:2823ms step_avg:61.36ms
step:47/2330 train_time:2881ms step_avg:61.30ms
step:48/2330 train_time:2943ms step_avg:61.31ms
step:49/2330 train_time:3002ms step_avg:61.26ms
step:50/2330 train_time:3064ms step_avg:61.28ms
step:51/2330 train_time:3124ms step_avg:61.26ms
step:52/2330 train_time:3188ms step_avg:61.30ms
step:53/2330 train_time:3247ms step_avg:61.27ms
step:54/2330 train_time:3310ms step_avg:61.29ms
step:55/2330 train_time:3370ms step_avg:61.27ms
step:56/2330 train_time:3432ms step_avg:61.29ms
step:57/2330 train_time:3492ms step_avg:61.26ms
step:58/2330 train_time:3555ms step_avg:61.29ms
step:59/2330 train_time:3614ms step_avg:61.26ms
step:60/2330 train_time:3676ms step_avg:61.27ms
step:61/2330 train_time:3735ms step_avg:61.24ms
step:62/2330 train_time:3797ms step_avg:61.24ms
step:63/2330 train_time:3856ms step_avg:61.20ms
step:64/2330 train_time:3917ms step_avg:61.21ms
step:65/2330 train_time:3977ms step_avg:61.18ms
step:66/2330 train_time:4039ms step_avg:61.20ms
step:67/2330 train_time:4099ms step_avg:61.18ms
step:68/2330 train_time:4161ms step_avg:61.20ms
step:69/2330 train_time:4221ms step_avg:61.18ms
step:70/2330 train_time:4283ms step_avg:61.19ms
step:71/2330 train_time:4343ms step_avg:61.16ms
step:72/2330 train_time:4405ms step_avg:61.18ms
step:73/2330 train_time:4465ms step_avg:61.17ms
step:74/2330 train_time:4528ms step_avg:61.20ms
step:75/2330 train_time:4588ms step_avg:61.18ms
step:76/2330 train_time:4650ms step_avg:61.19ms
step:77/2330 train_time:4710ms step_avg:61.17ms
step:78/2330 train_time:4772ms step_avg:61.18ms
step:79/2330 train_time:4832ms step_avg:61.16ms
step:80/2330 train_time:4894ms step_avg:61.17ms
step:81/2330 train_time:4953ms step_avg:61.15ms
step:82/2330 train_time:5016ms step_avg:61.17ms
step:83/2330 train_time:5075ms step_avg:61.15ms
step:84/2330 train_time:5137ms step_avg:61.15ms
step:85/2330 train_time:5196ms step_avg:61.13ms
step:86/2330 train_time:5258ms step_avg:61.14ms
step:87/2330 train_time:5317ms step_avg:61.11ms
step:88/2330 train_time:5379ms step_avg:61.12ms
step:89/2330 train_time:5438ms step_avg:61.10ms
step:90/2330 train_time:5500ms step_avg:61.11ms
step:91/2330 train_time:5561ms step_avg:61.10ms
step:92/2330 train_time:5623ms step_avg:61.12ms
step:93/2330 train_time:5682ms step_avg:61.09ms
step:94/2330 train_time:5743ms step_avg:61.09ms
step:95/2330 train_time:5802ms step_avg:61.07ms
step:96/2330 train_time:5863ms step_avg:61.08ms
step:97/2330 train_time:5923ms step_avg:61.06ms
step:98/2330 train_time:5985ms step_avg:61.07ms
step:99/2330 train_time:6044ms step_avg:61.05ms
step:100/2330 train_time:6106ms step_avg:61.06ms
step:101/2330 train_time:6166ms step_avg:61.05ms
step:102/2330 train_time:6229ms step_avg:61.07ms
step:103/2330 train_time:6288ms step_avg:61.05ms
step:104/2330 train_time:6350ms step_avg:61.06ms
step:105/2330 train_time:6409ms step_avg:61.04ms
step:106/2330 train_time:6472ms step_avg:61.06ms
step:107/2330 train_time:6531ms step_avg:61.04ms
step:108/2330 train_time:6593ms step_avg:61.05ms
step:109/2330 train_time:6652ms step_avg:61.03ms
step:110/2330 train_time:6714ms step_avg:61.04ms
step:111/2330 train_time:6774ms step_avg:61.02ms
step:112/2330 train_time:6835ms step_avg:61.03ms
step:113/2330 train_time:6894ms step_avg:61.01ms
step:114/2330 train_time:6956ms step_avg:61.02ms
step:115/2330 train_time:7016ms step_avg:61.00ms
step:116/2330 train_time:7077ms step_avg:61.01ms
step:117/2330 train_time:7136ms step_avg:61.00ms
step:118/2330 train_time:7199ms step_avg:61.01ms
step:119/2330 train_time:7258ms step_avg:60.99ms
step:120/2330 train_time:7320ms step_avg:61.00ms
step:121/2330 train_time:7379ms step_avg:60.98ms
step:122/2330 train_time:7440ms step_avg:60.99ms
step:123/2330 train_time:7500ms step_avg:60.97ms
step:124/2330 train_time:7561ms step_avg:60.98ms
step:125/2330 train_time:7620ms step_avg:60.96ms
step:126/2330 train_time:7682ms step_avg:60.97ms
step:127/2330 train_time:7741ms step_avg:60.95ms
step:128/2330 train_time:7803ms step_avg:60.96ms
step:129/2330 train_time:7862ms step_avg:60.94ms
step:130/2330 train_time:7924ms step_avg:60.95ms
step:131/2330 train_time:7983ms step_avg:60.94ms
step:132/2330 train_time:8046ms step_avg:60.95ms
step:133/2330 train_time:8105ms step_avg:60.94ms
step:134/2330 train_time:8168ms step_avg:60.96ms
step:135/2330 train_time:8228ms step_avg:60.95ms
step:136/2330 train_time:8290ms step_avg:60.96ms
step:137/2330 train_time:8349ms step_avg:60.94ms
step:138/2330 train_time:8411ms step_avg:60.95ms
step:139/2330 train_time:8471ms step_avg:60.94ms
step:140/2330 train_time:8533ms step_avg:60.95ms
step:141/2330 train_time:8592ms step_avg:60.93ms
step:142/2330 train_time:8653ms step_avg:60.94ms
step:143/2330 train_time:8712ms step_avg:60.93ms
step:144/2330 train_time:8775ms step_avg:60.94ms
step:145/2330 train_time:8834ms step_avg:60.93ms
step:146/2330 train_time:8896ms step_avg:60.93ms
step:147/2330 train_time:8955ms step_avg:60.92ms
step:148/2330 train_time:9017ms step_avg:60.92ms
step:149/2330 train_time:9077ms step_avg:60.92ms
step:150/2330 train_time:9139ms step_avg:60.93ms
step:151/2330 train_time:9199ms step_avg:60.92ms
step:152/2330 train_time:9262ms step_avg:60.93ms
step:153/2330 train_time:9322ms step_avg:60.93ms
step:154/2330 train_time:9383ms step_avg:60.93ms
step:155/2330 train_time:9442ms step_avg:60.92ms
step:156/2330 train_time:9504ms step_avg:60.92ms
step:157/2330 train_time:9563ms step_avg:60.91ms
step:158/2330 train_time:9625ms step_avg:60.92ms
step:159/2330 train_time:9684ms step_avg:60.91ms
step:160/2330 train_time:9746ms step_avg:60.91ms
step:161/2330 train_time:9806ms step_avg:60.91ms
step:162/2330 train_time:9868ms step_avg:60.92ms
step:163/2330 train_time:9928ms step_avg:60.91ms
step:164/2330 train_time:9991ms step_avg:60.92ms
step:165/2330 train_time:10050ms step_avg:60.91ms
step:166/2330 train_time:10112ms step_avg:60.92ms
step:167/2330 train_time:10172ms step_avg:60.91ms
step:168/2330 train_time:10234ms step_avg:60.92ms
step:169/2330 train_time:10293ms step_avg:60.90ms
step:170/2330 train_time:10355ms step_avg:60.91ms
step:171/2330 train_time:10415ms step_avg:60.91ms
step:172/2330 train_time:10476ms step_avg:60.91ms
step:173/2330 train_time:10535ms step_avg:60.90ms
step:174/2330 train_time:10598ms step_avg:60.91ms
step:175/2330 train_time:10657ms step_avg:60.89ms
step:176/2330 train_time:10719ms step_avg:60.90ms
step:177/2330 train_time:10779ms step_avg:60.90ms
step:178/2330 train_time:10841ms step_avg:60.91ms
step:179/2330 train_time:10900ms step_avg:60.89ms
step:180/2330 train_time:10962ms step_avg:60.90ms
step:181/2330 train_time:11022ms step_avg:60.89ms
step:182/2330 train_time:11083ms step_avg:60.90ms
step:183/2330 train_time:11142ms step_avg:60.88ms
step:184/2330 train_time:11204ms step_avg:60.89ms
step:185/2330 train_time:11263ms step_avg:60.88ms
step:186/2330 train_time:11325ms step_avg:60.89ms
step:187/2330 train_time:11384ms step_avg:60.88ms
step:188/2330 train_time:11446ms step_avg:60.88ms
step:189/2330 train_time:11506ms step_avg:60.88ms
step:190/2330 train_time:11568ms step_avg:60.88ms
step:191/2330 train_time:11628ms step_avg:60.88ms
step:192/2330 train_time:11690ms step_avg:60.89ms
step:193/2330 train_time:11749ms step_avg:60.88ms
step:194/2330 train_time:11811ms step_avg:60.88ms
step:195/2330 train_time:11871ms step_avg:60.88ms
step:196/2330 train_time:11933ms step_avg:60.88ms
step:197/2330 train_time:11992ms step_avg:60.87ms
step:198/2330 train_time:12056ms step_avg:60.89ms
step:199/2330 train_time:12115ms step_avg:60.88ms
step:200/2330 train_time:12177ms step_avg:60.88ms
step:201/2330 train_time:12235ms step_avg:60.87ms
step:202/2330 train_time:12297ms step_avg:60.88ms
step:203/2330 train_time:12356ms step_avg:60.87ms
step:204/2330 train_time:12418ms step_avg:60.87ms
step:205/2330 train_time:12477ms step_avg:60.86ms
step:206/2330 train_time:12539ms step_avg:60.87ms
step:207/2330 train_time:12599ms step_avg:60.86ms
step:208/2330 train_time:12660ms step_avg:60.87ms
step:209/2330 train_time:12719ms step_avg:60.86ms
step:210/2330 train_time:12782ms step_avg:60.87ms
step:211/2330 train_time:12842ms step_avg:60.86ms
step:212/2330 train_time:12903ms step_avg:60.86ms
step:213/2330 train_time:12962ms step_avg:60.86ms
step:214/2330 train_time:13025ms step_avg:60.86ms
step:215/2330 train_time:13084ms step_avg:60.86ms
step:216/2330 train_time:13146ms step_avg:60.86ms
step:217/2330 train_time:13205ms step_avg:60.85ms
step:218/2330 train_time:13268ms step_avg:60.86ms
step:219/2330 train_time:13327ms step_avg:60.85ms
step:220/2330 train_time:13389ms step_avg:60.86ms
step:221/2330 train_time:13448ms step_avg:60.85ms
step:222/2330 train_time:13510ms step_avg:60.86ms
step:223/2330 train_time:13570ms step_avg:60.85ms
step:224/2330 train_time:13632ms step_avg:60.86ms
step:225/2330 train_time:13691ms step_avg:60.85ms
step:226/2330 train_time:13755ms step_avg:60.86ms
step:227/2330 train_time:13814ms step_avg:60.86ms
step:228/2330 train_time:13876ms step_avg:60.86ms
step:229/2330 train_time:13935ms step_avg:60.85ms
step:230/2330 train_time:13996ms step_avg:60.85ms
step:231/2330 train_time:14055ms step_avg:60.84ms
step:232/2330 train_time:14117ms step_avg:60.85ms
step:233/2330 train_time:14175ms step_avg:60.84ms
step:234/2330 train_time:14237ms step_avg:60.84ms
step:235/2330 train_time:14296ms step_avg:60.83ms
step:236/2330 train_time:14358ms step_avg:60.84ms
step:237/2330 train_time:14417ms step_avg:60.83ms
step:238/2330 train_time:14480ms step_avg:60.84ms
step:239/2330 train_time:14539ms step_avg:60.83ms
step:240/2330 train_time:14602ms step_avg:60.84ms
step:241/2330 train_time:14662ms step_avg:60.84ms
step:242/2330 train_time:14723ms step_avg:60.84ms
step:243/2330 train_time:14782ms step_avg:60.83ms
step:244/2330 train_time:14843ms step_avg:60.83ms
step:245/2330 train_time:14902ms step_avg:60.82ms
step:246/2330 train_time:14964ms step_avg:60.83ms
step:247/2330 train_time:15023ms step_avg:60.82ms
step:248/2330 train_time:15085ms step_avg:60.83ms
step:249/2330 train_time:15144ms step_avg:60.82ms
step:250/2330 train_time:15206ms step_avg:60.82ms
step:250/2330 val_loss:4.1265 train_time:15269ms step_avg:61.08ms
step:251/2330 train_time:15293ms step_avg:60.93ms
step:252/2330 train_time:15330ms step_avg:60.83ms
step:253/2330 train_time:15393ms step_avg:60.84ms
step:254/2330 train_time:15460ms step_avg:60.87ms
step:255/2330 train_time:15520ms step_avg:60.86ms
step:256/2330 train_time:15581ms step_avg:60.86ms
step:257/2330 train_time:15640ms step_avg:60.85ms
step:258/2330 train_time:15701ms step_avg:60.86ms
step:259/2330 train_time:15760ms step_avg:60.85ms
step:260/2330 train_time:15821ms step_avg:60.85ms
step:261/2330 train_time:15879ms step_avg:60.84ms
step:262/2330 train_time:15941ms step_avg:60.84ms
step:263/2330 train_time:15999ms step_avg:60.83ms
step:264/2330 train_time:16060ms step_avg:60.83ms
step:265/2330 train_time:16118ms step_avg:60.82ms
step:266/2330 train_time:16180ms step_avg:60.83ms
step:267/2330 train_time:16239ms step_avg:60.82ms
step:268/2330 train_time:16303ms step_avg:60.83ms
step:269/2330 train_time:16365ms step_avg:60.84ms
step:270/2330 train_time:16428ms step_avg:60.85ms
step:271/2330 train_time:16488ms step_avg:60.84ms
step:272/2330 train_time:16550ms step_avg:60.85ms
step:273/2330 train_time:16609ms step_avg:60.84ms
step:274/2330 train_time:16671ms step_avg:60.84ms
step:275/2330 train_time:16730ms step_avg:60.84ms
step:276/2330 train_time:16792ms step_avg:60.84ms
step:277/2330 train_time:16852ms step_avg:60.84ms
step:278/2330 train_time:16913ms step_avg:60.84ms
step:279/2330 train_time:16972ms step_avg:60.83ms
step:280/2330 train_time:17034ms step_avg:60.84ms
step:281/2330 train_time:17093ms step_avg:60.83ms
step:282/2330 train_time:17155ms step_avg:60.83ms
step:283/2330 train_time:17215ms step_avg:60.83ms
step:284/2330 train_time:17277ms step_avg:60.84ms
step:285/2330 train_time:17338ms step_avg:60.83ms
step:286/2330 train_time:17400ms step_avg:60.84ms
step:287/2330 train_time:17460ms step_avg:60.84ms
step:288/2330 train_time:17522ms step_avg:60.84ms
step:289/2330 train_time:17581ms step_avg:60.83ms
step:290/2330 train_time:17643ms step_avg:60.84ms
step:291/2330 train_time:17702ms step_avg:60.83ms
step:292/2330 train_time:17764ms step_avg:60.83ms
step:293/2330 train_time:17823ms step_avg:60.83ms
step:294/2330 train_time:17885ms step_avg:60.83ms
step:295/2330 train_time:17946ms step_avg:60.83ms
step:296/2330 train_time:18007ms step_avg:60.84ms
step:297/2330 train_time:18066ms step_avg:60.83ms
step:298/2330 train_time:18128ms step_avg:60.83ms
step:299/2330 train_time:18187ms step_avg:60.83ms
step:300/2330 train_time:18248ms step_avg:60.83ms
step:301/2330 train_time:18308ms step_avg:60.82ms
step:302/2330 train_time:18370ms step_avg:60.83ms
step:303/2330 train_time:18430ms step_avg:60.83ms
step:304/2330 train_time:18493ms step_avg:60.83ms
step:305/2330 train_time:18553ms step_avg:60.83ms
step:306/2330 train_time:18615ms step_avg:60.83ms
step:307/2330 train_time:18675ms step_avg:60.83ms
step:308/2330 train_time:18737ms step_avg:60.84ms
step:309/2330 train_time:18796ms step_avg:60.83ms
step:310/2330 train_time:18859ms step_avg:60.83ms
step:311/2330 train_time:18918ms step_avg:60.83ms
step:312/2330 train_time:18981ms step_avg:60.84ms
step:313/2330 train_time:19041ms step_avg:60.83ms
step:314/2330 train_time:19102ms step_avg:60.84ms
step:315/2330 train_time:19162ms step_avg:60.83ms
step:316/2330 train_time:19224ms step_avg:60.83ms
step:317/2330 train_time:19283ms step_avg:60.83ms
step:318/2330 train_time:19345ms step_avg:60.83ms
step:319/2330 train_time:19404ms step_avg:60.83ms
step:320/2330 train_time:19466ms step_avg:60.83ms
step:321/2330 train_time:19525ms step_avg:60.83ms
step:322/2330 train_time:19587ms step_avg:60.83ms
step:323/2330 train_time:19647ms step_avg:60.83ms
step:324/2330 train_time:19708ms step_avg:60.83ms
step:325/2330 train_time:19768ms step_avg:60.83ms
step:326/2330 train_time:19830ms step_avg:60.83ms
step:327/2330 train_time:19889ms step_avg:60.82ms
step:328/2330 train_time:19951ms step_avg:60.83ms
step:329/2330 train_time:20010ms step_avg:60.82ms
step:330/2330 train_time:20073ms step_avg:60.83ms
step:331/2330 train_time:20133ms step_avg:60.82ms
step:332/2330 train_time:20195ms step_avg:60.83ms
step:333/2330 train_time:20255ms step_avg:60.83ms
step:334/2330 train_time:20317ms step_avg:60.83ms
step:335/2330 train_time:20377ms step_avg:60.83ms
step:336/2330 train_time:20439ms step_avg:60.83ms
step:337/2330 train_time:20499ms step_avg:60.83ms
step:338/2330 train_time:20561ms step_avg:60.83ms
step:339/2330 train_time:20621ms step_avg:60.83ms
step:340/2330 train_time:20683ms step_avg:60.83ms
step:341/2330 train_time:20742ms step_avg:60.83ms
step:342/2330 train_time:20804ms step_avg:60.83ms
step:343/2330 train_time:20863ms step_avg:60.82ms
step:344/2330 train_time:20924ms step_avg:60.83ms
step:345/2330 train_time:20983ms step_avg:60.82ms
step:346/2330 train_time:21045ms step_avg:60.82ms
step:347/2330 train_time:21104ms step_avg:60.82ms
step:348/2330 train_time:21167ms step_avg:60.82ms
step:349/2330 train_time:21227ms step_avg:60.82ms
step:350/2330 train_time:21288ms step_avg:60.82ms
step:351/2330 train_time:21348ms step_avg:60.82ms
step:352/2330 train_time:21410ms step_avg:60.82ms
step:353/2330 train_time:21469ms step_avg:60.82ms
step:354/2330 train_time:21531ms step_avg:60.82ms
step:355/2330 train_time:21591ms step_avg:60.82ms
step:356/2330 train_time:21654ms step_avg:60.83ms
step:357/2330 train_time:21713ms step_avg:60.82ms
step:358/2330 train_time:21775ms step_avg:60.82ms
step:359/2330 train_time:21834ms step_avg:60.82ms
step:360/2330 train_time:21897ms step_avg:60.82ms
step:361/2330 train_time:21956ms step_avg:60.82ms
step:362/2330 train_time:22018ms step_avg:60.82ms
step:363/2330 train_time:22078ms step_avg:60.82ms
step:364/2330 train_time:22140ms step_avg:60.82ms
step:365/2330 train_time:22200ms step_avg:60.82ms
step:366/2330 train_time:22262ms step_avg:60.82ms
step:367/2330 train_time:22320ms step_avg:60.82ms
step:368/2330 train_time:22383ms step_avg:60.82ms
step:369/2330 train_time:22441ms step_avg:60.82ms
step:370/2330 train_time:22504ms step_avg:60.82ms
step:371/2330 train_time:22563ms step_avg:60.82ms
step:372/2330 train_time:22625ms step_avg:60.82ms
step:373/2330 train_time:22685ms step_avg:60.82ms
step:374/2330 train_time:22747ms step_avg:60.82ms
step:375/2330 train_time:22806ms step_avg:60.82ms
step:376/2330 train_time:22868ms step_avg:60.82ms
step:377/2330 train_time:22926ms step_avg:60.81ms
step:378/2330 train_time:22988ms step_avg:60.81ms
step:379/2330 train_time:23048ms step_avg:60.81ms
step:380/2330 train_time:23111ms step_avg:60.82ms
step:381/2330 train_time:23170ms step_avg:60.81ms
step:382/2330 train_time:23232ms step_avg:60.82ms
step:383/2330 train_time:23291ms step_avg:60.81ms
step:384/2330 train_time:23355ms step_avg:60.82ms
step:385/2330 train_time:23414ms step_avg:60.82ms
step:386/2330 train_time:23477ms step_avg:60.82ms
step:387/2330 train_time:23536ms step_avg:60.82ms
step:388/2330 train_time:23599ms step_avg:60.82ms
step:389/2330 train_time:23659ms step_avg:60.82ms
step:390/2330 train_time:23721ms step_avg:60.82ms
step:391/2330 train_time:23779ms step_avg:60.82ms
step:392/2330 train_time:23841ms step_avg:60.82ms
step:393/2330 train_time:23900ms step_avg:60.81ms
step:394/2330 train_time:23962ms step_avg:60.82ms
step:395/2330 train_time:24021ms step_avg:60.81ms
step:396/2330 train_time:24083ms step_avg:60.82ms
step:397/2330 train_time:24143ms step_avg:60.81ms
step:398/2330 train_time:24205ms step_avg:60.82ms
step:399/2330 train_time:24265ms step_avg:60.81ms
step:400/2330 train_time:24327ms step_avg:60.82ms
step:401/2330 train_time:24386ms step_avg:60.81ms
step:402/2330 train_time:24449ms step_avg:60.82ms
step:403/2330 train_time:24508ms step_avg:60.81ms
step:404/2330 train_time:24571ms step_avg:60.82ms
step:405/2330 train_time:24630ms step_avg:60.81ms
step:406/2330 train_time:24693ms step_avg:60.82ms
step:407/2330 train_time:24752ms step_avg:60.82ms
step:408/2330 train_time:24814ms step_avg:60.82ms
step:409/2330 train_time:24874ms step_avg:60.82ms
step:410/2330 train_time:24937ms step_avg:60.82ms
step:411/2330 train_time:24995ms step_avg:60.82ms
step:412/2330 train_time:25058ms step_avg:60.82ms
step:413/2330 train_time:25117ms step_avg:60.82ms
step:414/2330 train_time:25180ms step_avg:60.82ms
step:415/2330 train_time:25240ms step_avg:60.82ms
step:416/2330 train_time:25302ms step_avg:60.82ms
step:417/2330 train_time:25361ms step_avg:60.82ms
step:418/2330 train_time:25423ms step_avg:60.82ms
step:419/2330 train_time:25482ms step_avg:60.82ms
step:420/2330 train_time:25544ms step_avg:60.82ms
step:421/2330 train_time:25604ms step_avg:60.82ms
step:422/2330 train_time:25665ms step_avg:60.82ms
step:423/2330 train_time:25725ms step_avg:60.81ms
step:424/2330 train_time:25786ms step_avg:60.82ms
step:425/2330 train_time:25845ms step_avg:60.81ms
step:426/2330 train_time:25907ms step_avg:60.82ms
step:427/2330 train_time:25966ms step_avg:60.81ms
step:428/2330 train_time:26028ms step_avg:60.81ms
step:429/2330 train_time:26088ms step_avg:60.81ms
step:430/2330 train_time:26150ms step_avg:60.81ms
step:431/2330 train_time:26209ms step_avg:60.81ms
step:432/2330 train_time:26271ms step_avg:60.81ms
step:433/2330 train_time:26330ms step_avg:60.81ms
step:434/2330 train_time:26393ms step_avg:60.81ms
step:435/2330 train_time:26452ms step_avg:60.81ms
step:436/2330 train_time:26514ms step_avg:60.81ms
step:437/2330 train_time:26574ms step_avg:60.81ms
step:438/2330 train_time:26636ms step_avg:60.81ms
step:439/2330 train_time:26696ms step_avg:60.81ms
step:440/2330 train_time:26759ms step_avg:60.82ms
step:441/2330 train_time:26818ms step_avg:60.81ms
step:442/2330 train_time:26881ms step_avg:60.82ms
step:443/2330 train_time:26940ms step_avg:60.81ms
step:444/2330 train_time:27002ms step_avg:60.82ms
step:445/2330 train_time:27061ms step_avg:60.81ms
step:446/2330 train_time:27124ms step_avg:60.82ms
step:447/2330 train_time:27183ms step_avg:60.81ms
step:448/2330 train_time:27245ms step_avg:60.82ms
step:449/2330 train_time:27304ms step_avg:60.81ms
step:450/2330 train_time:27366ms step_avg:60.81ms
step:451/2330 train_time:27425ms step_avg:60.81ms
step:452/2330 train_time:27487ms step_avg:60.81ms
step:453/2330 train_time:27546ms step_avg:60.81ms
step:454/2330 train_time:27608ms step_avg:60.81ms
step:455/2330 train_time:27667ms step_avg:60.81ms
step:456/2330 train_time:27730ms step_avg:60.81ms
step:457/2330 train_time:27789ms step_avg:60.81ms
step:458/2330 train_time:27852ms step_avg:60.81ms
step:459/2330 train_time:27911ms step_avg:60.81ms
step:460/2330 train_time:27974ms step_avg:60.81ms
step:461/2330 train_time:28033ms step_avg:60.81ms
step:462/2330 train_time:28096ms step_avg:60.81ms
step:463/2330 train_time:28156ms step_avg:60.81ms
step:464/2330 train_time:28218ms step_avg:60.82ms
step:465/2330 train_time:28278ms step_avg:60.81ms
step:466/2330 train_time:28341ms step_avg:60.82ms
step:467/2330 train_time:28400ms step_avg:60.81ms
step:468/2330 train_time:28462ms step_avg:60.82ms
step:469/2330 train_time:28521ms step_avg:60.81ms
step:470/2330 train_time:28582ms step_avg:60.81ms
step:471/2330 train_time:28641ms step_avg:60.81ms
step:472/2330 train_time:28703ms step_avg:60.81ms
step:473/2330 train_time:28763ms step_avg:60.81ms
step:474/2330 train_time:28825ms step_avg:60.81ms
step:475/2330 train_time:28885ms step_avg:60.81ms
step:476/2330 train_time:28947ms step_avg:60.81ms
step:477/2330 train_time:29006ms step_avg:60.81ms
step:478/2330 train_time:29068ms step_avg:60.81ms
step:479/2330 train_time:29127ms step_avg:60.81ms
step:480/2330 train_time:29189ms step_avg:60.81ms
step:481/2330 train_time:29248ms step_avg:60.81ms
step:482/2330 train_time:29310ms step_avg:60.81ms
step:483/2330 train_time:29369ms step_avg:60.81ms
step:484/2330 train_time:29431ms step_avg:60.81ms
step:485/2330 train_time:29491ms step_avg:60.81ms
step:486/2330 train_time:29553ms step_avg:60.81ms
step:487/2330 train_time:29612ms step_avg:60.81ms
step:488/2330 train_time:29675ms step_avg:60.81ms
step:489/2330 train_time:29735ms step_avg:60.81ms
step:490/2330 train_time:29798ms step_avg:60.81ms
step:491/2330 train_time:29858ms step_avg:60.81ms
step:492/2330 train_time:29920ms step_avg:60.81ms
step:493/2330 train_time:29980ms step_avg:60.81ms
step:494/2330 train_time:30042ms step_avg:60.81ms
step:495/2330 train_time:30101ms step_avg:60.81ms
step:496/2330 train_time:30164ms step_avg:60.81ms
step:497/2330 train_time:30223ms step_avg:60.81ms
step:498/2330 train_time:30285ms step_avg:60.81ms
step:499/2330 train_time:30344ms step_avg:60.81ms
step:500/2330 train_time:30406ms step_avg:60.81ms
step:500/2330 val_loss:3.8353 train_time:30469ms step_avg:60.94ms
step:501/2330 train_time:30491ms step_avg:60.86ms
step:502/2330 train_time:30530ms step_avg:60.82ms
step:503/2330 train_time:30592ms step_avg:60.82ms
step:504/2330 train_time:30656ms step_avg:60.82ms
step:505/2330 train_time:30714ms step_avg:60.82ms
step:506/2330 train_time:30777ms step_avg:60.82ms
step:507/2330 train_time:30835ms step_avg:60.82ms
step:508/2330 train_time:30897ms step_avg:60.82ms
step:509/2330 train_time:30955ms step_avg:60.82ms
step:510/2330 train_time:31017ms step_avg:60.82ms
step:511/2330 train_time:31076ms step_avg:60.81ms
step:512/2330 train_time:31138ms step_avg:60.82ms
step:513/2330 train_time:31197ms step_avg:60.81ms
step:514/2330 train_time:31258ms step_avg:60.81ms
step:515/2330 train_time:31316ms step_avg:60.81ms
step:516/2330 train_time:31379ms step_avg:60.81ms
step:517/2330 train_time:31439ms step_avg:60.81ms
step:518/2330 train_time:31501ms step_avg:60.81ms
step:519/2330 train_time:31562ms step_avg:60.81ms
step:520/2330 train_time:31625ms step_avg:60.82ms
step:521/2330 train_time:31686ms step_avg:60.82ms
step:522/2330 train_time:31748ms step_avg:60.82ms
step:523/2330 train_time:31807ms step_avg:60.82ms
step:524/2330 train_time:31869ms step_avg:60.82ms
step:525/2330 train_time:31929ms step_avg:60.82ms
step:526/2330 train_time:31991ms step_avg:60.82ms
step:527/2330 train_time:32050ms step_avg:60.82ms
step:528/2330 train_time:32112ms step_avg:60.82ms
step:529/2330 train_time:32171ms step_avg:60.81ms
step:530/2330 train_time:32232ms step_avg:60.82ms
step:531/2330 train_time:32291ms step_avg:60.81ms
step:532/2330 train_time:32353ms step_avg:60.81ms
step:533/2330 train_time:32412ms step_avg:60.81ms
step:534/2330 train_time:32476ms step_avg:60.82ms
step:535/2330 train_time:32536ms step_avg:60.82ms
step:536/2330 train_time:32598ms step_avg:60.82ms
step:537/2330 train_time:32658ms step_avg:60.82ms
step:538/2330 train_time:32720ms step_avg:60.82ms
step:539/2330 train_time:32780ms step_avg:60.82ms
step:540/2330 train_time:32843ms step_avg:60.82ms
step:541/2330 train_time:32902ms step_avg:60.82ms
step:542/2330 train_time:32965ms step_avg:60.82ms
step:543/2330 train_time:33025ms step_avg:60.82ms
step:544/2330 train_time:33087ms step_avg:60.82ms
step:545/2330 train_time:33146ms step_avg:60.82ms
step:546/2330 train_time:33208ms step_avg:60.82ms
step:547/2330 train_time:33268ms step_avg:60.82ms
step:548/2330 train_time:33331ms step_avg:60.82ms
step:549/2330 train_time:33390ms step_avg:60.82ms
step:550/2330 train_time:33452ms step_avg:60.82ms
step:551/2330 train_time:33511ms step_avg:60.82ms
step:552/2330 train_time:33574ms step_avg:60.82ms
step:553/2330 train_time:33633ms step_avg:60.82ms
step:554/2330 train_time:33695ms step_avg:60.82ms
step:555/2330 train_time:33754ms step_avg:60.82ms
step:556/2330 train_time:33816ms step_avg:60.82ms
step:557/2330 train_time:33876ms step_avg:60.82ms
step:558/2330 train_time:33938ms step_avg:60.82ms
step:559/2330 train_time:33998ms step_avg:60.82ms
step:560/2330 train_time:34060ms step_avg:60.82ms
step:561/2330 train_time:34119ms step_avg:60.82ms
step:562/2330 train_time:34180ms step_avg:60.82ms
step:563/2330 train_time:34239ms step_avg:60.82ms
step:564/2330 train_time:34302ms step_avg:60.82ms
step:565/2330 train_time:34361ms step_avg:60.82ms
step:566/2330 train_time:34424ms step_avg:60.82ms
step:567/2330 train_time:34484ms step_avg:60.82ms
step:568/2330 train_time:34546ms step_avg:60.82ms
step:569/2330 train_time:34606ms step_avg:60.82ms
step:570/2330 train_time:34669ms step_avg:60.82ms
step:571/2330 train_time:34729ms step_avg:60.82ms
step:572/2330 train_time:34791ms step_avg:60.82ms
step:573/2330 train_time:34851ms step_avg:60.82ms
step:574/2330 train_time:34913ms step_avg:60.82ms
step:575/2330 train_time:34972ms step_avg:60.82ms
step:576/2330 train_time:35033ms step_avg:60.82ms
step:577/2330 train_time:35093ms step_avg:60.82ms
step:578/2330 train_time:35155ms step_avg:60.82ms
step:579/2330 train_time:35215ms step_avg:60.82ms
step:580/2330 train_time:35277ms step_avg:60.82ms
step:581/2330 train_time:35336ms step_avg:60.82ms
step:582/2330 train_time:35398ms step_avg:60.82ms
step:583/2330 train_time:35457ms step_avg:60.82ms
step:584/2330 train_time:35518ms step_avg:60.82ms
step:585/2330 train_time:35579ms step_avg:60.82ms
step:586/2330 train_time:35641ms step_avg:60.82ms
step:587/2330 train_time:35699ms step_avg:60.82ms
step:588/2330 train_time:35762ms step_avg:60.82ms
step:589/2330 train_time:35821ms step_avg:60.82ms
step:590/2330 train_time:35883ms step_avg:60.82ms
step:591/2330 train_time:35943ms step_avg:60.82ms
step:592/2330 train_time:36005ms step_avg:60.82ms
step:593/2330 train_time:36065ms step_avg:60.82ms
step:594/2330 train_time:36127ms step_avg:60.82ms
step:595/2330 train_time:36187ms step_avg:60.82ms
step:596/2330 train_time:36249ms step_avg:60.82ms
step:597/2330 train_time:36308ms step_avg:60.82ms
step:598/2330 train_time:36371ms step_avg:60.82ms
step:599/2330 train_time:36430ms step_avg:60.82ms
step:600/2330 train_time:36492ms step_avg:60.82ms
step:601/2330 train_time:36551ms step_avg:60.82ms
step:602/2330 train_time:36613ms step_avg:60.82ms
step:603/2330 train_time:36672ms step_avg:60.82ms
step:604/2330 train_time:36734ms step_avg:60.82ms
step:605/2330 train_time:36793ms step_avg:60.81ms
step:606/2330 train_time:36855ms step_avg:60.82ms
step:607/2330 train_time:36915ms step_avg:60.82ms
step:608/2330 train_time:36978ms step_avg:60.82ms
step:609/2330 train_time:37037ms step_avg:60.82ms
step:610/2330 train_time:37099ms step_avg:60.82ms
step:611/2330 train_time:37159ms step_avg:60.82ms
step:612/2330 train_time:37221ms step_avg:60.82ms
step:613/2330 train_time:37279ms step_avg:60.81ms
step:614/2330 train_time:37341ms step_avg:60.82ms
step:615/2330 train_time:37400ms step_avg:60.81ms
step:616/2330 train_time:37463ms step_avg:60.82ms
step:617/2330 train_time:37522ms step_avg:60.81ms
step:618/2330 train_time:37585ms step_avg:60.82ms
step:619/2330 train_time:37644ms step_avg:60.81ms
step:620/2330 train_time:37707ms step_avg:60.82ms
step:621/2330 train_time:37767ms step_avg:60.82ms
step:622/2330 train_time:37830ms step_avg:60.82ms
step:623/2330 train_time:37890ms step_avg:60.82ms
step:624/2330 train_time:37952ms step_avg:60.82ms
step:625/2330 train_time:38011ms step_avg:60.82ms
step:626/2330 train_time:38074ms step_avg:60.82ms
step:627/2330 train_time:38133ms step_avg:60.82ms
step:628/2330 train_time:38195ms step_avg:60.82ms
step:629/2330 train_time:38253ms step_avg:60.82ms
step:630/2330 train_time:38316ms step_avg:60.82ms
step:631/2330 train_time:38375ms step_avg:60.82ms
step:632/2330 train_time:38438ms step_avg:60.82ms
step:633/2330 train_time:38497ms step_avg:60.82ms
step:634/2330 train_time:38559ms step_avg:60.82ms
step:635/2330 train_time:38618ms step_avg:60.82ms
step:636/2330 train_time:38680ms step_avg:60.82ms
step:637/2330 train_time:38740ms step_avg:60.82ms
step:638/2330 train_time:38802ms step_avg:60.82ms
step:639/2330 train_time:38862ms step_avg:60.82ms
step:640/2330 train_time:38925ms step_avg:60.82ms
step:641/2330 train_time:38985ms step_avg:60.82ms
step:642/2330 train_time:39047ms step_avg:60.82ms
step:643/2330 train_time:39107ms step_avg:60.82ms
step:644/2330 train_time:39171ms step_avg:60.82ms
step:645/2330 train_time:39230ms step_avg:60.82ms
step:646/2330 train_time:39292ms step_avg:60.82ms
step:647/2330 train_time:39351ms step_avg:60.82ms
step:648/2330 train_time:39413ms step_avg:60.82ms
step:649/2330 train_time:39472ms step_avg:60.82ms
step:650/2330 train_time:39535ms step_avg:60.82ms
step:651/2330 train_time:39594ms step_avg:60.82ms
step:652/2330 train_time:39656ms step_avg:60.82ms
step:653/2330 train_time:39716ms step_avg:60.82ms
step:654/2330 train_time:39778ms step_avg:60.82ms
step:655/2330 train_time:39838ms step_avg:60.82ms
step:656/2330 train_time:39900ms step_avg:60.82ms
step:657/2330 train_time:39959ms step_avg:60.82ms
step:658/2330 train_time:40021ms step_avg:60.82ms
step:659/2330 train_time:40081ms step_avg:60.82ms
step:660/2330 train_time:40144ms step_avg:60.82ms
step:661/2330 train_time:40203ms step_avg:60.82ms
step:662/2330 train_time:40266ms step_avg:60.83ms
step:663/2330 train_time:40326ms step_avg:60.82ms
step:664/2330 train_time:40388ms step_avg:60.83ms
step:665/2330 train_time:40448ms step_avg:60.82ms
step:666/2330 train_time:40510ms step_avg:60.83ms
step:667/2330 train_time:40570ms step_avg:60.82ms
step:668/2330 train_time:40632ms step_avg:60.83ms
step:669/2330 train_time:40691ms step_avg:60.82ms
step:670/2330 train_time:40753ms step_avg:60.83ms
step:671/2330 train_time:40813ms step_avg:60.82ms
step:672/2330 train_time:40875ms step_avg:60.83ms
step:673/2330 train_time:40934ms step_avg:60.82ms
step:674/2330 train_time:40997ms step_avg:60.83ms
step:675/2330 train_time:41057ms step_avg:60.83ms
step:676/2330 train_time:41119ms step_avg:60.83ms
step:677/2330 train_time:41178ms step_avg:60.82ms
step:678/2330 train_time:41240ms step_avg:60.83ms
step:679/2330 train_time:41299ms step_avg:60.82ms
step:680/2330 train_time:41361ms step_avg:60.82ms
step:681/2330 train_time:41420ms step_avg:60.82ms
step:682/2330 train_time:41483ms step_avg:60.83ms
step:683/2330 train_time:41543ms step_avg:60.82ms
step:684/2330 train_time:41605ms step_avg:60.83ms
step:685/2330 train_time:41665ms step_avg:60.82ms
step:686/2330 train_time:41728ms step_avg:60.83ms
step:687/2330 train_time:41787ms step_avg:60.83ms
step:688/2330 train_time:41850ms step_avg:60.83ms
step:689/2330 train_time:41909ms step_avg:60.83ms
step:690/2330 train_time:41972ms step_avg:60.83ms
step:691/2330 train_time:42031ms step_avg:60.83ms
step:692/2330 train_time:42092ms step_avg:60.83ms
step:693/2330 train_time:42151ms step_avg:60.82ms
step:694/2330 train_time:42213ms step_avg:60.83ms
step:695/2330 train_time:42273ms step_avg:60.82ms
step:696/2330 train_time:42335ms step_avg:60.83ms
step:697/2330 train_time:42395ms step_avg:60.82ms
step:698/2330 train_time:42457ms step_avg:60.83ms
step:699/2330 train_time:42516ms step_avg:60.82ms
step:700/2330 train_time:42578ms step_avg:60.83ms
step:701/2330 train_time:42637ms step_avg:60.82ms
step:702/2330 train_time:42699ms step_avg:60.82ms
step:703/2330 train_time:42759ms step_avg:60.82ms
step:704/2330 train_time:42821ms step_avg:60.83ms
step:705/2330 train_time:42881ms step_avg:60.82ms
step:706/2330 train_time:42943ms step_avg:60.83ms
step:707/2330 train_time:43003ms step_avg:60.82ms
step:708/2330 train_time:43065ms step_avg:60.83ms
step:709/2330 train_time:43125ms step_avg:60.82ms
step:710/2330 train_time:43188ms step_avg:60.83ms
step:711/2330 train_time:43247ms step_avg:60.83ms
step:712/2330 train_time:43310ms step_avg:60.83ms
step:713/2330 train_time:43369ms step_avg:60.83ms
step:714/2330 train_time:43431ms step_avg:60.83ms
step:715/2330 train_time:43491ms step_avg:60.83ms
step:716/2330 train_time:43553ms step_avg:60.83ms
step:717/2330 train_time:43613ms step_avg:60.83ms
step:718/2330 train_time:43675ms step_avg:60.83ms
step:719/2330 train_time:43734ms step_avg:60.83ms
step:720/2330 train_time:43796ms step_avg:60.83ms
step:721/2330 train_time:43855ms step_avg:60.83ms
step:722/2330 train_time:43917ms step_avg:60.83ms
step:723/2330 train_time:43976ms step_avg:60.82ms
step:724/2330 train_time:44038ms step_avg:60.83ms
step:725/2330 train_time:44097ms step_avg:60.82ms
step:726/2330 train_time:44159ms step_avg:60.82ms
step:727/2330 train_time:44218ms step_avg:60.82ms
step:728/2330 train_time:44280ms step_avg:60.82ms
step:729/2330 train_time:44339ms step_avg:60.82ms
step:730/2330 train_time:44401ms step_avg:60.82ms
step:731/2330 train_time:44461ms step_avg:60.82ms
step:732/2330 train_time:44524ms step_avg:60.82ms
step:733/2330 train_time:44583ms step_avg:60.82ms
step:734/2330 train_time:44646ms step_avg:60.83ms
step:735/2330 train_time:44705ms step_avg:60.82ms
step:736/2330 train_time:44769ms step_avg:60.83ms
step:737/2330 train_time:44828ms step_avg:60.82ms
step:738/2330 train_time:44891ms step_avg:60.83ms
step:739/2330 train_time:44950ms step_avg:60.82ms
step:740/2330 train_time:45012ms step_avg:60.83ms
step:741/2330 train_time:45071ms step_avg:60.82ms
step:742/2330 train_time:45133ms step_avg:60.83ms
step:743/2330 train_time:45192ms step_avg:60.82ms
step:744/2330 train_time:45253ms step_avg:60.82ms
step:745/2330 train_time:45312ms step_avg:60.82ms
step:746/2330 train_time:45375ms step_avg:60.82ms
step:747/2330 train_time:45434ms step_avg:60.82ms
step:748/2330 train_time:45497ms step_avg:60.82ms
step:749/2330 train_time:45556ms step_avg:60.82ms
step:750/2330 train_time:45618ms step_avg:60.82ms
step:750/2330 val_loss:3.6993 train_time:45683ms step_avg:60.91ms
step:751/2330 train_time:45705ms step_avg:60.86ms
step:752/2330 train_time:45743ms step_avg:60.83ms
step:753/2330 train_time:45808ms step_avg:60.83ms
step:754/2330 train_time:45874ms step_avg:60.84ms
step:755/2330 train_time:45935ms step_avg:60.84ms
step:756/2330 train_time:45998ms step_avg:60.84ms
step:757/2330 train_time:46056ms step_avg:60.84ms
step:758/2330 train_time:46118ms step_avg:60.84ms
step:759/2330 train_time:46176ms step_avg:60.84ms
step:760/2330 train_time:46238ms step_avg:60.84ms
step:761/2330 train_time:46296ms step_avg:60.84ms
step:762/2330 train_time:46358ms step_avg:60.84ms
step:763/2330 train_time:46416ms step_avg:60.83ms
step:764/2330 train_time:46477ms step_avg:60.83ms
step:765/2330 train_time:46535ms step_avg:60.83ms
step:766/2330 train_time:46598ms step_avg:60.83ms
step:767/2330 train_time:46659ms step_avg:60.83ms
step:768/2330 train_time:46723ms step_avg:60.84ms
step:769/2330 train_time:46785ms step_avg:60.84ms
step:770/2330 train_time:46850ms step_avg:60.84ms
step:771/2330 train_time:46911ms step_avg:60.84ms
step:772/2330 train_time:46975ms step_avg:60.85ms
step:773/2330 train_time:47035ms step_avg:60.85ms
step:774/2330 train_time:47098ms step_avg:60.85ms
step:775/2330 train_time:47158ms step_avg:60.85ms
step:776/2330 train_time:47220ms step_avg:60.85ms
step:777/2330 train_time:47279ms step_avg:60.85ms
step:778/2330 train_time:47342ms step_avg:60.85ms
step:779/2330 train_time:47402ms step_avg:60.85ms
step:780/2330 train_time:47465ms step_avg:60.85ms
step:781/2330 train_time:47524ms step_avg:60.85ms
step:782/2330 train_time:47586ms step_avg:60.85ms
step:783/2330 train_time:47647ms step_avg:60.85ms
step:784/2330 train_time:47711ms step_avg:60.86ms
step:785/2330 train_time:47772ms step_avg:60.86ms
step:786/2330 train_time:47835ms step_avg:60.86ms
step:787/2330 train_time:47896ms step_avg:60.86ms
step:788/2330 train_time:47960ms step_avg:60.86ms
step:789/2330 train_time:48020ms step_avg:60.86ms
step:790/2330 train_time:48083ms step_avg:60.86ms
step:791/2330 train_time:48143ms step_avg:60.86ms
step:792/2330 train_time:48206ms step_avg:60.87ms
step:793/2330 train_time:48266ms step_avg:60.86ms
step:794/2330 train_time:48328ms step_avg:60.87ms
step:795/2330 train_time:48387ms step_avg:60.86ms
step:796/2330 train_time:48450ms step_avg:60.87ms
step:797/2330 train_time:48510ms step_avg:60.87ms
step:798/2330 train_time:48572ms step_avg:60.87ms
step:799/2330 train_time:48632ms step_avg:60.87ms
step:800/2330 train_time:48695ms step_avg:60.87ms
step:801/2330 train_time:48754ms step_avg:60.87ms
step:802/2330 train_time:48818ms step_avg:60.87ms
step:803/2330 train_time:48878ms step_avg:60.87ms
step:804/2330 train_time:48942ms step_avg:60.87ms
step:805/2330 train_time:49002ms step_avg:60.87ms
step:806/2330 train_time:49065ms step_avg:60.88ms
step:807/2330 train_time:49125ms step_avg:60.87ms
step:808/2330 train_time:49188ms step_avg:60.88ms
step:809/2330 train_time:49248ms step_avg:60.87ms
step:810/2330 train_time:49311ms step_avg:60.88ms
step:811/2330 train_time:49370ms step_avg:60.88ms
step:812/2330 train_time:49432ms step_avg:60.88ms
step:813/2330 train_time:49492ms step_avg:60.88ms
step:814/2330 train_time:49555ms step_avg:60.88ms
step:815/2330 train_time:49615ms step_avg:60.88ms
step:816/2330 train_time:49677ms step_avg:60.88ms
step:817/2330 train_time:49737ms step_avg:60.88ms
step:818/2330 train_time:49800ms step_avg:60.88ms
step:819/2330 train_time:49860ms step_avg:60.88ms
step:820/2330 train_time:49924ms step_avg:60.88ms
step:821/2330 train_time:49985ms step_avg:60.88ms
step:822/2330 train_time:50048ms step_avg:60.89ms
step:823/2330 train_time:50108ms step_avg:60.88ms
step:824/2330 train_time:50171ms step_avg:60.89ms
step:825/2330 train_time:50231ms step_avg:60.89ms
step:826/2330 train_time:50293ms step_avg:60.89ms
step:827/2330 train_time:50353ms step_avg:60.89ms
step:828/2330 train_time:50415ms step_avg:60.89ms
step:829/2330 train_time:50475ms step_avg:60.89ms
step:830/2330 train_time:50538ms step_avg:60.89ms
step:831/2330 train_time:50597ms step_avg:60.89ms
step:832/2330 train_time:50660ms step_avg:60.89ms
step:833/2330 train_time:50720ms step_avg:60.89ms
step:834/2330 train_time:50783ms step_avg:60.89ms
step:835/2330 train_time:50844ms step_avg:60.89ms
step:836/2330 train_time:50907ms step_avg:60.89ms
step:837/2330 train_time:50967ms step_avg:60.89ms
step:838/2330 train_time:51030ms step_avg:60.89ms
step:839/2330 train_time:51090ms step_avg:60.89ms
step:840/2330 train_time:51152ms step_avg:60.90ms
step:841/2330 train_time:51212ms step_avg:60.89ms
step:842/2330 train_time:51274ms step_avg:60.90ms
step:843/2330 train_time:51334ms step_avg:60.89ms
step:844/2330 train_time:51396ms step_avg:60.90ms
step:845/2330 train_time:51456ms step_avg:60.89ms
step:846/2330 train_time:51518ms step_avg:60.90ms
step:847/2330 train_time:51578ms step_avg:60.89ms
step:848/2330 train_time:51641ms step_avg:60.90ms
step:849/2330 train_time:51701ms step_avg:60.90ms
step:850/2330 train_time:51764ms step_avg:60.90ms
step:851/2330 train_time:51824ms step_avg:60.90ms
step:852/2330 train_time:51887ms step_avg:60.90ms
step:853/2330 train_time:51947ms step_avg:60.90ms
step:854/2330 train_time:52011ms step_avg:60.90ms
step:855/2330 train_time:52070ms step_avg:60.90ms
step:856/2330 train_time:52133ms step_avg:60.90ms
step:857/2330 train_time:52193ms step_avg:60.90ms
step:858/2330 train_time:52256ms step_avg:60.90ms
step:859/2330 train_time:52316ms step_avg:60.90ms
step:860/2330 train_time:52379ms step_avg:60.91ms
step:861/2330 train_time:52438ms step_avg:60.90ms
step:862/2330 train_time:52501ms step_avg:60.91ms
step:863/2330 train_time:52562ms step_avg:60.91ms
step:864/2330 train_time:52625ms step_avg:60.91ms
step:865/2330 train_time:52684ms step_avg:60.91ms
step:866/2330 train_time:52748ms step_avg:60.91ms
step:867/2330 train_time:52809ms step_avg:60.91ms
step:868/2330 train_time:52871ms step_avg:60.91ms
step:869/2330 train_time:52931ms step_avg:60.91ms
step:870/2330 train_time:52994ms step_avg:60.91ms
step:871/2330 train_time:53054ms step_avg:60.91ms
step:872/2330 train_time:53117ms step_avg:60.91ms
step:873/2330 train_time:53177ms step_avg:60.91ms
step:874/2330 train_time:53240ms step_avg:60.92ms
step:875/2330 train_time:53301ms step_avg:60.92ms
step:876/2330 train_time:53364ms step_avg:60.92ms
step:877/2330 train_time:53424ms step_avg:60.92ms
step:878/2330 train_time:53486ms step_avg:60.92ms
step:879/2330 train_time:53546ms step_avg:60.92ms
step:880/2330 train_time:53609ms step_avg:60.92ms
step:881/2330 train_time:53669ms step_avg:60.92ms
step:882/2330 train_time:53731ms step_avg:60.92ms
step:883/2330 train_time:53791ms step_avg:60.92ms
step:884/2330 train_time:53853ms step_avg:60.92ms
step:885/2330 train_time:53913ms step_avg:60.92ms
step:886/2330 train_time:53976ms step_avg:60.92ms
step:887/2330 train_time:54036ms step_avg:60.92ms
step:888/2330 train_time:54098ms step_avg:60.92ms
step:889/2330 train_time:54158ms step_avg:60.92ms
step:890/2330 train_time:54221ms step_avg:60.92ms
step:891/2330 train_time:54281ms step_avg:60.92ms
step:892/2330 train_time:54345ms step_avg:60.92ms
step:893/2330 train_time:54405ms step_avg:60.92ms
step:894/2330 train_time:54467ms step_avg:60.93ms
step:895/2330 train_time:54527ms step_avg:60.92ms
step:896/2330 train_time:54590ms step_avg:60.93ms
step:897/2330 train_time:54649ms step_avg:60.92ms
step:898/2330 train_time:54713ms step_avg:60.93ms
step:899/2330 train_time:54772ms step_avg:60.93ms
step:900/2330 train_time:54835ms step_avg:60.93ms
step:901/2330 train_time:54894ms step_avg:60.93ms
step:902/2330 train_time:54957ms step_avg:60.93ms
step:903/2330 train_time:55017ms step_avg:60.93ms
step:904/2330 train_time:55080ms step_avg:60.93ms
step:905/2330 train_time:55140ms step_avg:60.93ms
step:906/2330 train_time:55203ms step_avg:60.93ms
step:907/2330 train_time:55263ms step_avg:60.93ms
step:908/2330 train_time:55326ms step_avg:60.93ms
step:909/2330 train_time:55386ms step_avg:60.93ms
step:910/2330 train_time:55449ms step_avg:60.93ms
step:911/2330 train_time:55509ms step_avg:60.93ms
step:912/2330 train_time:55572ms step_avg:60.93ms
step:913/2330 train_time:55632ms step_avg:60.93ms
step:914/2330 train_time:55694ms step_avg:60.93ms
step:915/2330 train_time:55754ms step_avg:60.93ms
step:916/2330 train_time:55817ms step_avg:60.94ms
step:917/2330 train_time:55877ms step_avg:60.93ms
step:918/2330 train_time:55939ms step_avg:60.94ms
step:919/2330 train_time:55998ms step_avg:60.93ms
step:920/2330 train_time:56062ms step_avg:60.94ms
step:921/2330 train_time:56122ms step_avg:60.94ms
step:922/2330 train_time:56185ms step_avg:60.94ms
step:923/2330 train_time:56244ms step_avg:60.94ms
step:924/2330 train_time:56308ms step_avg:60.94ms
step:925/2330 train_time:56368ms step_avg:60.94ms
step:926/2330 train_time:56431ms step_avg:60.94ms
step:927/2330 train_time:56490ms step_avg:60.94ms
step:928/2330 train_time:56553ms step_avg:60.94ms
step:929/2330 train_time:56613ms step_avg:60.94ms
step:930/2330 train_time:56676ms step_avg:60.94ms
step:931/2330 train_time:56736ms step_avg:60.94ms
step:932/2330 train_time:56799ms step_avg:60.94ms
step:933/2330 train_time:56859ms step_avg:60.94ms
step:934/2330 train_time:56923ms step_avg:60.95ms
step:935/2330 train_time:56982ms step_avg:60.94ms
step:936/2330 train_time:57045ms step_avg:60.95ms
step:937/2330 train_time:57105ms step_avg:60.94ms
step:938/2330 train_time:57169ms step_avg:60.95ms
step:939/2330 train_time:57229ms step_avg:60.95ms
step:940/2330 train_time:57292ms step_avg:60.95ms
step:941/2330 train_time:57352ms step_avg:60.95ms
step:942/2330 train_time:57415ms step_avg:60.95ms
step:943/2330 train_time:57475ms step_avg:60.95ms
step:944/2330 train_time:57538ms step_avg:60.95ms
step:945/2330 train_time:57598ms step_avg:60.95ms
step:946/2330 train_time:57661ms step_avg:60.95ms
step:947/2330 train_time:57721ms step_avg:60.95ms
step:948/2330 train_time:57784ms step_avg:60.95ms
step:949/2330 train_time:57844ms step_avg:60.95ms
step:950/2330 train_time:57908ms step_avg:60.96ms
step:951/2330 train_time:57967ms step_avg:60.95ms
step:952/2330 train_time:58030ms step_avg:60.96ms
step:953/2330 train_time:58090ms step_avg:60.95ms
step:954/2330 train_time:58153ms step_avg:60.96ms
step:955/2330 train_time:58213ms step_avg:60.96ms
step:956/2330 train_time:58276ms step_avg:60.96ms
step:957/2330 train_time:58336ms step_avg:60.96ms
step:958/2330 train_time:58399ms step_avg:60.96ms
step:959/2330 train_time:58460ms step_avg:60.96ms
step:960/2330 train_time:58523ms step_avg:60.96ms
step:961/2330 train_time:58584ms step_avg:60.96ms
step:962/2330 train_time:58646ms step_avg:60.96ms
step:963/2330 train_time:58707ms step_avg:60.96ms
step:964/2330 train_time:58769ms step_avg:60.96ms
step:965/2330 train_time:58829ms step_avg:60.96ms
step:966/2330 train_time:58891ms step_avg:60.96ms
step:967/2330 train_time:58951ms step_avg:60.96ms
step:968/2330 train_time:59014ms step_avg:60.96ms
step:969/2330 train_time:59073ms step_avg:60.96ms
step:970/2330 train_time:59136ms step_avg:60.97ms
step:971/2330 train_time:59196ms step_avg:60.96ms
step:972/2330 train_time:59260ms step_avg:60.97ms
step:973/2330 train_time:59320ms step_avg:60.97ms
step:974/2330 train_time:59383ms step_avg:60.97ms
step:975/2330 train_time:59443ms step_avg:60.97ms
step:976/2330 train_time:59507ms step_avg:60.97ms
step:977/2330 train_time:59567ms step_avg:60.97ms
step:978/2330 train_time:59630ms step_avg:60.97ms
step:979/2330 train_time:59689ms step_avg:60.97ms
step:980/2330 train_time:59752ms step_avg:60.97ms
step:981/2330 train_time:59812ms step_avg:60.97ms
step:982/2330 train_time:59874ms step_avg:60.97ms
step:983/2330 train_time:59935ms step_avg:60.97ms
step:984/2330 train_time:59997ms step_avg:60.97ms
step:985/2330 train_time:60056ms step_avg:60.97ms
step:986/2330 train_time:60120ms step_avg:60.97ms
step:987/2330 train_time:60180ms step_avg:60.97ms
step:988/2330 train_time:60244ms step_avg:60.98ms
step:989/2330 train_time:60304ms step_avg:60.97ms
step:990/2330 train_time:60368ms step_avg:60.98ms
step:991/2330 train_time:60427ms step_avg:60.98ms
step:992/2330 train_time:60491ms step_avg:60.98ms
step:993/2330 train_time:60550ms step_avg:60.98ms
step:994/2330 train_time:60613ms step_avg:60.98ms
step:995/2330 train_time:60673ms step_avg:60.98ms
step:996/2330 train_time:60735ms step_avg:60.98ms
step:997/2330 train_time:60795ms step_avg:60.98ms
step:998/2330 train_time:60857ms step_avg:60.98ms
step:999/2330 train_time:60917ms step_avg:60.98ms
step:1000/2330 train_time:60980ms step_avg:60.98ms
step:1000/2330 val_loss:3.5835 train_time:61044ms step_avg:61.04ms
step:1001/2330 train_time:61067ms step_avg:61.01ms
step:1002/2330 train_time:61105ms step_avg:60.98ms
step:1003/2330 train_time:61169ms step_avg:60.99ms
step:1004/2330 train_time:61237ms step_avg:60.99ms
step:1005/2330 train_time:61297ms step_avg:60.99ms
step:1006/2330 train_time:61360ms step_avg:60.99ms
step:1007/2330 train_time:61419ms step_avg:60.99ms
step:1008/2330 train_time:61481ms step_avg:60.99ms
step:1009/2330 train_time:61540ms step_avg:60.99ms
step:1010/2330 train_time:61602ms step_avg:60.99ms
step:1011/2330 train_time:61661ms step_avg:60.99ms
step:1012/2330 train_time:61723ms step_avg:60.99ms
step:1013/2330 train_time:61782ms step_avg:60.99ms
step:1014/2330 train_time:61844ms step_avg:60.99ms
step:1015/2330 train_time:61902ms step_avg:60.99ms
step:1016/2330 train_time:61967ms step_avg:60.99ms
step:1017/2330 train_time:62029ms step_avg:60.99ms
step:1018/2330 train_time:62092ms step_avg:60.99ms
step:1019/2330 train_time:62153ms step_avg:60.99ms
step:1020/2330 train_time:62216ms step_avg:61.00ms
step:1021/2330 train_time:62278ms step_avg:61.00ms
step:1022/2330 train_time:62340ms step_avg:61.00ms
step:1023/2330 train_time:62400ms step_avg:61.00ms
step:1024/2330 train_time:62462ms step_avg:61.00ms
step:1025/2330 train_time:62521ms step_avg:61.00ms
step:1026/2330 train_time:62583ms step_avg:61.00ms
step:1027/2330 train_time:62642ms step_avg:61.00ms
step:1028/2330 train_time:62704ms step_avg:61.00ms
step:1029/2330 train_time:62763ms step_avg:60.99ms
step:1030/2330 train_time:62826ms step_avg:61.00ms
step:1031/2330 train_time:62885ms step_avg:60.99ms
step:1032/2330 train_time:62948ms step_avg:61.00ms
step:1033/2330 train_time:63009ms step_avg:61.00ms
step:1034/2330 train_time:63072ms step_avg:61.00ms
step:1035/2330 train_time:63132ms step_avg:61.00ms
step:1036/2330 train_time:63196ms step_avg:61.00ms
step:1037/2330 train_time:63256ms step_avg:61.00ms
step:1038/2330 train_time:63319ms step_avg:61.00ms
step:1039/2330 train_time:63380ms step_avg:61.00ms
step:1040/2330 train_time:63442ms step_avg:61.00ms
step:1041/2330 train_time:63502ms step_avg:61.00ms
step:1042/2330 train_time:63564ms step_avg:61.00ms
step:1043/2330 train_time:63624ms step_avg:61.00ms
step:1044/2330 train_time:63687ms step_avg:61.00ms
step:1045/2330 train_time:63746ms step_avg:61.00ms
step:1046/2330 train_time:63808ms step_avg:61.00ms
step:1047/2330 train_time:63867ms step_avg:61.00ms
step:1048/2330 train_time:63930ms step_avg:61.00ms
step:1049/2330 train_time:63990ms step_avg:61.00ms
step:1050/2330 train_time:64052ms step_avg:61.00ms
step:1051/2330 train_time:64112ms step_avg:61.00ms
step:1052/2330 train_time:64176ms step_avg:61.00ms
step:1053/2330 train_time:64237ms step_avg:61.00ms
step:1054/2330 train_time:64300ms step_avg:61.01ms
step:1055/2330 train_time:64360ms step_avg:61.01ms
step:1056/2330 train_time:64423ms step_avg:61.01ms
step:1057/2330 train_time:64483ms step_avg:61.01ms
step:1058/2330 train_time:64545ms step_avg:61.01ms
step:1059/2330 train_time:64605ms step_avg:61.01ms
step:1060/2330 train_time:64668ms step_avg:61.01ms
step:1061/2330 train_time:64728ms step_avg:61.01ms
step:1062/2330 train_time:64791ms step_avg:61.01ms
step:1063/2330 train_time:64850ms step_avg:61.01ms
step:1064/2330 train_time:64912ms step_avg:61.01ms
step:1065/2330 train_time:64971ms step_avg:61.01ms
step:1066/2330 train_time:65034ms step_avg:61.01ms
step:1067/2330 train_time:65095ms step_avg:61.01ms
step:1068/2330 train_time:65158ms step_avg:61.01ms
step:1069/2330 train_time:65219ms step_avg:61.01ms
step:1070/2330 train_time:65282ms step_avg:61.01ms
step:1071/2330 train_time:65342ms step_avg:61.01ms
step:1072/2330 train_time:65404ms step_avg:61.01ms
step:1073/2330 train_time:65465ms step_avg:61.01ms
step:1074/2330 train_time:65527ms step_avg:61.01ms
step:1075/2330 train_time:65587ms step_avg:61.01ms
step:1076/2330 train_time:65649ms step_avg:61.01ms
step:1077/2330 train_time:65709ms step_avg:61.01ms
step:1078/2330 train_time:65772ms step_avg:61.01ms
step:1079/2330 train_time:65832ms step_avg:61.01ms
step:1080/2330 train_time:65894ms step_avg:61.01ms
step:1081/2330 train_time:65954ms step_avg:61.01ms
step:1082/2330 train_time:66016ms step_avg:61.01ms
step:1083/2330 train_time:66076ms step_avg:61.01ms
step:1084/2330 train_time:66139ms step_avg:61.01ms
step:1085/2330 train_time:66199ms step_avg:61.01ms
step:1086/2330 train_time:66262ms step_avg:61.01ms
step:1087/2330 train_time:66323ms step_avg:61.01ms
step:1088/2330 train_time:66386ms step_avg:61.02ms
step:1089/2330 train_time:66445ms step_avg:61.01ms
step:1090/2330 train_time:66508ms step_avg:61.02ms
step:1091/2330 train_time:66568ms step_avg:61.02ms
step:1092/2330 train_time:66630ms step_avg:61.02ms
step:1093/2330 train_time:66690ms step_avg:61.02ms
step:1094/2330 train_time:66753ms step_avg:61.02ms
step:1095/2330 train_time:66813ms step_avg:61.02ms
step:1096/2330 train_time:66876ms step_avg:61.02ms
step:1097/2330 train_time:66936ms step_avg:61.02ms
step:1098/2330 train_time:66998ms step_avg:61.02ms
step:1099/2330 train_time:67058ms step_avg:61.02ms
step:1100/2330 train_time:67121ms step_avg:61.02ms
step:1101/2330 train_time:67181ms step_avg:61.02ms
step:1102/2330 train_time:67244ms step_avg:61.02ms
step:1103/2330 train_time:67303ms step_avg:61.02ms
step:1104/2330 train_time:67366ms step_avg:61.02ms
step:1105/2330 train_time:67427ms step_avg:61.02ms
step:1106/2330 train_time:67489ms step_avg:61.02ms
step:1107/2330 train_time:67549ms step_avg:61.02ms
step:1108/2330 train_time:67611ms step_avg:61.02ms
step:1109/2330 train_time:67671ms step_avg:61.02ms
step:1110/2330 train_time:67734ms step_avg:61.02ms
step:1111/2330 train_time:67794ms step_avg:61.02ms
step:1112/2330 train_time:67856ms step_avg:61.02ms
step:1113/2330 train_time:67916ms step_avg:61.02ms
step:1114/2330 train_time:67979ms step_avg:61.02ms
step:1115/2330 train_time:68038ms step_avg:61.02ms
step:1116/2330 train_time:68101ms step_avg:61.02ms
step:1117/2330 train_time:68161ms step_avg:61.02ms
step:1118/2330 train_time:68224ms step_avg:61.02ms
step:1119/2330 train_time:68284ms step_avg:61.02ms
step:1120/2330 train_time:68347ms step_avg:61.02ms
step:1121/2330 train_time:68407ms step_avg:61.02ms
step:1122/2330 train_time:68469ms step_avg:61.02ms
step:1123/2330 train_time:68529ms step_avg:61.02ms
step:1124/2330 train_time:68592ms step_avg:61.02ms
step:1125/2330 train_time:68652ms step_avg:61.02ms
step:1126/2330 train_time:68714ms step_avg:61.03ms
step:1127/2330 train_time:68775ms step_avg:61.02ms
step:1128/2330 train_time:68837ms step_avg:61.03ms
step:1129/2330 train_time:68897ms step_avg:61.02ms
step:1130/2330 train_time:68960ms step_avg:61.03ms
step:1131/2330 train_time:69020ms step_avg:61.03ms
step:1132/2330 train_time:69083ms step_avg:61.03ms
step:1133/2330 train_time:69142ms step_avg:61.03ms
step:1134/2330 train_time:69204ms step_avg:61.03ms
step:1135/2330 train_time:69264ms step_avg:61.03ms
step:1136/2330 train_time:69327ms step_avg:61.03ms
step:1137/2330 train_time:69386ms step_avg:61.03ms
step:1138/2330 train_time:69449ms step_avg:61.03ms
step:1139/2330 train_time:69508ms step_avg:61.03ms
step:1140/2330 train_time:69571ms step_avg:61.03ms
step:1141/2330 train_time:69631ms step_avg:61.03ms
step:1142/2330 train_time:69694ms step_avg:61.03ms
step:1143/2330 train_time:69754ms step_avg:61.03ms
step:1144/2330 train_time:69817ms step_avg:61.03ms
step:1145/2330 train_time:69877ms step_avg:61.03ms
step:1146/2330 train_time:69940ms step_avg:61.03ms
step:1147/2330 train_time:69999ms step_avg:61.03ms
step:1148/2330 train_time:70063ms step_avg:61.03ms
step:1149/2330 train_time:70122ms step_avg:61.03ms
step:1150/2330 train_time:70186ms step_avg:61.03ms
step:1151/2330 train_time:70245ms step_avg:61.03ms
step:1152/2330 train_time:70308ms step_avg:61.03ms
step:1153/2330 train_time:70368ms step_avg:61.03ms
step:1154/2330 train_time:70430ms step_avg:61.03ms
step:1155/2330 train_time:70490ms step_avg:61.03ms
step:1156/2330 train_time:70553ms step_avg:61.03ms
step:1157/2330 train_time:70612ms step_avg:61.03ms
step:1158/2330 train_time:70676ms step_avg:61.03ms
step:1159/2330 train_time:70735ms step_avg:61.03ms
step:1160/2330 train_time:70798ms step_avg:61.03ms
step:1161/2330 train_time:70858ms step_avg:61.03ms
step:1162/2330 train_time:70921ms step_avg:61.03ms
step:1163/2330 train_time:70981ms step_avg:61.03ms
step:1164/2330 train_time:71043ms step_avg:61.03ms
step:1165/2330 train_time:71103ms step_avg:61.03ms
step:1166/2330 train_time:71166ms step_avg:61.03ms
step:1167/2330 train_time:71226ms step_avg:61.03ms
step:1168/2330 train_time:71289ms step_avg:61.04ms
step:1169/2330 train_time:71349ms step_avg:61.03ms
step:1170/2330 train_time:71412ms step_avg:61.04ms
step:1171/2330 train_time:71472ms step_avg:61.03ms
step:1172/2330 train_time:71534ms step_avg:61.04ms
step:1173/2330 train_time:71593ms step_avg:61.03ms
step:1174/2330 train_time:71656ms step_avg:61.04ms
step:1175/2330 train_time:71716ms step_avg:61.03ms
step:1176/2330 train_time:71779ms step_avg:61.04ms
step:1177/2330 train_time:71839ms step_avg:61.04ms
step:1178/2330 train_time:71902ms step_avg:61.04ms
step:1179/2330 train_time:71962ms step_avg:61.04ms
step:1180/2330 train_time:72025ms step_avg:61.04ms
step:1181/2330 train_time:72085ms step_avg:61.04ms
step:1182/2330 train_time:72147ms step_avg:61.04ms
step:1183/2330 train_time:72207ms step_avg:61.04ms
step:1184/2330 train_time:72270ms step_avg:61.04ms
step:1185/2330 train_time:72330ms step_avg:61.04ms
step:1186/2330 train_time:72392ms step_avg:61.04ms
step:1187/2330 train_time:72452ms step_avg:61.04ms
step:1188/2330 train_time:72514ms step_avg:61.04ms
step:1189/2330 train_time:72574ms step_avg:61.04ms
step:1190/2330 train_time:72637ms step_avg:61.04ms
step:1191/2330 train_time:72697ms step_avg:61.04ms
step:1192/2330 train_time:72760ms step_avg:61.04ms
step:1193/2330 train_time:72820ms step_avg:61.04ms
step:1194/2330 train_time:72883ms step_avg:61.04ms
step:1195/2330 train_time:72943ms step_avg:61.04ms
step:1196/2330 train_time:73005ms step_avg:61.04ms
step:1197/2330 train_time:73065ms step_avg:61.04ms
step:1198/2330 train_time:73129ms step_avg:61.04ms
step:1199/2330 train_time:73189ms step_avg:61.04ms
step:1200/2330 train_time:73251ms step_avg:61.04ms
step:1201/2330 train_time:73312ms step_avg:61.04ms
step:1202/2330 train_time:73375ms step_avg:61.04ms
step:1203/2330 train_time:73435ms step_avg:61.04ms
step:1204/2330 train_time:73497ms step_avg:61.04ms
step:1205/2330 train_time:73557ms step_avg:61.04ms
step:1206/2330 train_time:73620ms step_avg:61.04ms
step:1207/2330 train_time:73680ms step_avg:61.04ms
step:1208/2330 train_time:73742ms step_avg:61.04ms
step:1209/2330 train_time:73802ms step_avg:61.04ms
step:1210/2330 train_time:73864ms step_avg:61.04ms
step:1211/2330 train_time:73924ms step_avg:61.04ms
step:1212/2330 train_time:73987ms step_avg:61.05ms
step:1213/2330 train_time:74047ms step_avg:61.04ms
step:1214/2330 train_time:74109ms step_avg:61.05ms
step:1215/2330 train_time:74169ms step_avg:61.04ms
step:1216/2330 train_time:74232ms step_avg:61.05ms
step:1217/2330 train_time:74292ms step_avg:61.05ms
step:1218/2330 train_time:74355ms step_avg:61.05ms
step:1219/2330 train_time:74415ms step_avg:61.05ms
step:1220/2330 train_time:74478ms step_avg:61.05ms
step:1221/2330 train_time:74537ms step_avg:61.05ms
step:1222/2330 train_time:74600ms step_avg:61.05ms
step:1223/2330 train_time:74660ms step_avg:61.05ms
step:1224/2330 train_time:74723ms step_avg:61.05ms
step:1225/2330 train_time:74783ms step_avg:61.05ms
step:1226/2330 train_time:74845ms step_avg:61.05ms
step:1227/2330 train_time:74905ms step_avg:61.05ms
step:1228/2330 train_time:74968ms step_avg:61.05ms
step:1229/2330 train_time:75027ms step_avg:61.05ms
step:1230/2330 train_time:75090ms step_avg:61.05ms
step:1231/2330 train_time:75149ms step_avg:61.05ms
step:1232/2330 train_time:75212ms step_avg:61.05ms
step:1233/2330 train_time:75272ms step_avg:61.05ms
step:1234/2330 train_time:75334ms step_avg:61.05ms
step:1235/2330 train_time:75394ms step_avg:61.05ms
step:1236/2330 train_time:75457ms step_avg:61.05ms
step:1237/2330 train_time:75517ms step_avg:61.05ms
step:1238/2330 train_time:75580ms step_avg:61.05ms
step:1239/2330 train_time:75640ms step_avg:61.05ms
step:1240/2330 train_time:75702ms step_avg:61.05ms
step:1241/2330 train_time:75762ms step_avg:61.05ms
step:1242/2330 train_time:75825ms step_avg:61.05ms
step:1243/2330 train_time:75885ms step_avg:61.05ms
step:1244/2330 train_time:75947ms step_avg:61.05ms
step:1245/2330 train_time:76007ms step_avg:61.05ms
step:1246/2330 train_time:76069ms step_avg:61.05ms
step:1247/2330 train_time:76128ms step_avg:61.05ms
step:1248/2330 train_time:76191ms step_avg:61.05ms
step:1249/2330 train_time:76250ms step_avg:61.05ms
step:1250/2330 train_time:76313ms step_avg:61.05ms
step:1250/2330 val_loss:3.5229 train_time:76377ms step_avg:61.10ms
step:1251/2330 train_time:76401ms step_avg:61.07ms
step:1252/2330 train_time:76438ms step_avg:61.05ms
step:1253/2330 train_time:76501ms step_avg:61.05ms
step:1254/2330 train_time:76567ms step_avg:61.06ms
step:1255/2330 train_time:76628ms step_avg:61.06ms
step:1256/2330 train_time:76691ms step_avg:61.06ms
step:1257/2330 train_time:76750ms step_avg:61.06ms
step:1258/2330 train_time:76812ms step_avg:61.06ms
step:1259/2330 train_time:76871ms step_avg:61.06ms
step:1260/2330 train_time:76933ms step_avg:61.06ms
step:1261/2330 train_time:76993ms step_avg:61.06ms
step:1262/2330 train_time:77055ms step_avg:61.06ms
step:1263/2330 train_time:77113ms step_avg:61.06ms
step:1264/2330 train_time:77176ms step_avg:61.06ms
step:1265/2330 train_time:77235ms step_avg:61.06ms
step:1266/2330 train_time:77298ms step_avg:61.06ms
step:1267/2330 train_time:77360ms step_avg:61.06ms
step:1268/2330 train_time:77423ms step_avg:61.06ms
step:1269/2330 train_time:77484ms step_avg:61.06ms
step:1270/2330 train_time:77548ms step_avg:61.06ms
step:1271/2330 train_time:77609ms step_avg:61.06ms
step:1272/2330 train_time:77671ms step_avg:61.06ms
step:1273/2330 train_time:77731ms step_avg:61.06ms
step:1274/2330 train_time:77793ms step_avg:61.06ms
step:1275/2330 train_time:77853ms step_avg:61.06ms
step:1276/2330 train_time:77915ms step_avg:61.06ms
step:1277/2330 train_time:77974ms step_avg:61.06ms
step:1278/2330 train_time:78036ms step_avg:61.06ms
step:1279/2330 train_time:78095ms step_avg:61.06ms
step:1280/2330 train_time:78157ms step_avg:61.06ms
step:1281/2330 train_time:78217ms step_avg:61.06ms
step:1282/2330 train_time:78279ms step_avg:61.06ms
step:1283/2330 train_time:78339ms step_avg:61.06ms
step:1284/2330 train_time:78403ms step_avg:61.06ms
step:1285/2330 train_time:78463ms step_avg:61.06ms
step:1286/2330 train_time:78526ms step_avg:61.06ms
step:1287/2330 train_time:78586ms step_avg:61.06ms
step:1288/2330 train_time:78650ms step_avg:61.06ms
step:1289/2330 train_time:78710ms step_avg:61.06ms
step:1290/2330 train_time:78772ms step_avg:61.06ms
step:1291/2330 train_time:78832ms step_avg:61.06ms
step:1292/2330 train_time:78895ms step_avg:61.06ms
step:1293/2330 train_time:78954ms step_avg:61.06ms
step:1294/2330 train_time:79017ms step_avg:61.06ms
step:1295/2330 train_time:79076ms step_avg:61.06ms
step:1296/2330 train_time:79138ms step_avg:61.06ms
step:1297/2330 train_time:79197ms step_avg:61.06ms
step:1298/2330 train_time:79259ms step_avg:61.06ms
step:1299/2330 train_time:79319ms step_avg:61.06ms
step:1300/2330 train_time:79382ms step_avg:61.06ms
step:1301/2330 train_time:79442ms step_avg:61.06ms
step:1302/2330 train_time:79506ms step_avg:61.06ms
step:1303/2330 train_time:79566ms step_avg:61.06ms
step:1304/2330 train_time:79629ms step_avg:61.07ms
step:1305/2330 train_time:79689ms step_avg:61.06ms
step:1306/2330 train_time:79752ms step_avg:61.07ms
step:1307/2330 train_time:79812ms step_avg:61.07ms
step:1308/2330 train_time:79875ms step_avg:61.07ms
step:1309/2330 train_time:79935ms step_avg:61.07ms
step:1310/2330 train_time:79998ms step_avg:61.07ms
step:1311/2330 train_time:80057ms step_avg:61.07ms
step:1312/2330 train_time:80119ms step_avg:61.07ms
step:1313/2330 train_time:80179ms step_avg:61.07ms
step:1314/2330 train_time:80241ms step_avg:61.07ms
step:1315/2330 train_time:80300ms step_avg:61.06ms
step:1316/2330 train_time:80363ms step_avg:61.07ms
step:1317/2330 train_time:80423ms step_avg:61.07ms
step:1318/2330 train_time:80486ms step_avg:61.07ms
step:1319/2330 train_time:80546ms step_avg:61.07ms
step:1320/2330 train_time:80609ms step_avg:61.07ms
step:1321/2330 train_time:80669ms step_avg:61.07ms
step:1322/2330 train_time:80732ms step_avg:61.07ms
step:1323/2330 train_time:80792ms step_avg:61.07ms
step:1324/2330 train_time:80855ms step_avg:61.07ms
step:1325/2330 train_time:80915ms step_avg:61.07ms
step:1326/2330 train_time:80978ms step_avg:61.07ms
step:1327/2330 train_time:81037ms step_avg:61.07ms
step:1328/2330 train_time:81099ms step_avg:61.07ms
step:1329/2330 train_time:81159ms step_avg:61.07ms
step:1330/2330 train_time:81221ms step_avg:61.07ms
step:1331/2330 train_time:81281ms step_avg:61.07ms
step:1332/2330 train_time:81344ms step_avg:61.07ms
step:1333/2330 train_time:81404ms step_avg:61.07ms
step:1334/2330 train_time:81467ms step_avg:61.07ms
step:1335/2330 train_time:81527ms step_avg:61.07ms
step:1336/2330 train_time:81590ms step_avg:61.07ms
step:1337/2330 train_time:81650ms step_avg:61.07ms
step:1338/2330 train_time:81713ms step_avg:61.07ms
step:1339/2330 train_time:81772ms step_avg:61.07ms
step:1340/2330 train_time:81835ms step_avg:61.07ms
step:1341/2330 train_time:81895ms step_avg:61.07ms
step:1342/2330 train_time:81958ms step_avg:61.07ms
step:1343/2330 train_time:82017ms step_avg:61.07ms
step:1344/2330 train_time:82079ms step_avg:61.07ms
step:1345/2330 train_time:82138ms step_avg:61.07ms
step:1346/2330 train_time:82201ms step_avg:61.07ms
step:1347/2330 train_time:82261ms step_avg:61.07ms
step:1348/2330 train_time:82324ms step_avg:61.07ms
step:1349/2330 train_time:82384ms step_avg:61.07ms
step:1350/2330 train_time:82447ms step_avg:61.07ms
step:1351/2330 train_time:82507ms step_avg:61.07ms
step:1352/2330 train_time:82570ms step_avg:61.07ms
step:1353/2330 train_time:82630ms step_avg:61.07ms
step:1354/2330 train_time:82693ms step_avg:61.07ms
step:1355/2330 train_time:82753ms step_avg:61.07ms
step:1356/2330 train_time:82815ms step_avg:61.07ms
step:1357/2330 train_time:82875ms step_avg:61.07ms
step:1358/2330 train_time:82938ms step_avg:61.07ms
step:1359/2330 train_time:82997ms step_avg:61.07ms
step:1360/2330 train_time:83059ms step_avg:61.07ms
step:1361/2330 train_time:83118ms step_avg:61.07ms
step:1362/2330 train_time:83181ms step_avg:61.07ms
step:1363/2330 train_time:83241ms step_avg:61.07ms
step:1364/2330 train_time:83304ms step_avg:61.07ms
step:1365/2330 train_time:83363ms step_avg:61.07ms
step:1366/2330 train_time:83426ms step_avg:61.07ms
step:1367/2330 train_time:83486ms step_avg:61.07ms
step:1368/2330 train_time:83549ms step_avg:61.07ms
step:1369/2330 train_time:83609ms step_avg:61.07ms
step:1370/2330 train_time:83673ms step_avg:61.07ms
step:1371/2330 train_time:83732ms step_avg:61.07ms
step:1372/2330 train_time:83795ms step_avg:61.08ms
step:1373/2330 train_time:83855ms step_avg:61.07ms
step:1374/2330 train_time:83917ms step_avg:61.08ms
step:1375/2330 train_time:83977ms step_avg:61.07ms
step:1376/2330 train_time:84039ms step_avg:61.08ms
step:1377/2330 train_time:84099ms step_avg:61.07ms
step:1378/2330 train_time:84161ms step_avg:61.07ms
step:1379/2330 train_time:84221ms step_avg:61.07ms
step:1380/2330 train_time:84283ms step_avg:61.07ms
step:1381/2330 train_time:84343ms step_avg:61.07ms
step:1382/2330 train_time:84406ms step_avg:61.07ms
step:1383/2330 train_time:84466ms step_avg:61.07ms
step:1384/2330 train_time:84529ms step_avg:61.08ms
step:1385/2330 train_time:84588ms step_avg:61.07ms
step:1386/2330 train_time:84652ms step_avg:61.08ms
step:1387/2330 train_time:84711ms step_avg:61.08ms
step:1388/2330 train_time:84775ms step_avg:61.08ms
step:1389/2330 train_time:84834ms step_avg:61.08ms
step:1390/2330 train_time:84898ms step_avg:61.08ms
step:1391/2330 train_time:84958ms step_avg:61.08ms
step:1392/2330 train_time:85020ms step_avg:61.08ms
step:1393/2330 train_time:85080ms step_avg:61.08ms
step:1394/2330 train_time:85143ms step_avg:61.08ms
step:1395/2330 train_time:85203ms step_avg:61.08ms
step:1396/2330 train_time:85265ms step_avg:61.08ms
step:1397/2330 train_time:85325ms step_avg:61.08ms
step:1398/2330 train_time:85387ms step_avg:61.08ms
step:1399/2330 train_time:85447ms step_avg:61.08ms
step:1400/2330 train_time:85510ms step_avg:61.08ms
step:1401/2330 train_time:85570ms step_avg:61.08ms
step:1402/2330 train_time:85633ms step_avg:61.08ms
step:1403/2330 train_time:85692ms step_avg:61.08ms
step:1404/2330 train_time:85756ms step_avg:61.08ms
step:1405/2330 train_time:85816ms step_avg:61.08ms
step:1406/2330 train_time:85878ms step_avg:61.08ms
step:1407/2330 train_time:85938ms step_avg:61.08ms
step:1408/2330 train_time:86000ms step_avg:61.08ms
step:1409/2330 train_time:86060ms step_avg:61.08ms
step:1410/2330 train_time:86123ms step_avg:61.08ms
step:1411/2330 train_time:86182ms step_avg:61.08ms
step:1412/2330 train_time:86245ms step_avg:61.08ms
step:1413/2330 train_time:86305ms step_avg:61.08ms
step:1414/2330 train_time:86368ms step_avg:61.08ms
step:1415/2330 train_time:86427ms step_avg:61.08ms
step:1416/2330 train_time:86491ms step_avg:61.08ms
step:1417/2330 train_time:86551ms step_avg:61.08ms
step:1418/2330 train_time:86614ms step_avg:61.08ms
step:1419/2330 train_time:86673ms step_avg:61.08ms
step:1420/2330 train_time:86737ms step_avg:61.08ms
step:1421/2330 train_time:86796ms step_avg:61.08ms
step:1422/2330 train_time:86860ms step_avg:61.08ms
step:1423/2330 train_time:86919ms step_avg:61.08ms
step:1424/2330 train_time:86981ms step_avg:61.08ms
step:1425/2330 train_time:87041ms step_avg:61.08ms
step:1426/2330 train_time:87104ms step_avg:61.08ms
step:1427/2330 train_time:87163ms step_avg:61.08ms
step:1428/2330 train_time:87226ms step_avg:61.08ms
step:1429/2330 train_time:87286ms step_avg:61.08ms
step:1430/2330 train_time:87349ms step_avg:61.08ms
step:1431/2330 train_time:87409ms step_avg:61.08ms
step:1432/2330 train_time:87471ms step_avg:61.08ms
step:1433/2330 train_time:87531ms step_avg:61.08ms
step:1434/2330 train_time:87594ms step_avg:61.08ms
step:1435/2330 train_time:87654ms step_avg:61.08ms
step:1436/2330 train_time:87716ms step_avg:61.08ms
step:1437/2330 train_time:87776ms step_avg:61.08ms
step:1438/2330 train_time:87839ms step_avg:61.08ms
step:1439/2330 train_time:87898ms step_avg:61.08ms
step:1440/2330 train_time:87961ms step_avg:61.08ms
step:1441/2330 train_time:88021ms step_avg:61.08ms
step:1442/2330 train_time:88083ms step_avg:61.08ms
step:1443/2330 train_time:88143ms step_avg:61.08ms
step:1444/2330 train_time:88206ms step_avg:61.08ms
step:1445/2330 train_time:88265ms step_avg:61.08ms
step:1446/2330 train_time:88328ms step_avg:61.08ms
step:1447/2330 train_time:88389ms step_avg:61.08ms
step:1448/2330 train_time:88452ms step_avg:61.09ms
step:1449/2330 train_time:88511ms step_avg:61.08ms
step:1450/2330 train_time:88574ms step_avg:61.09ms
step:1451/2330 train_time:88633ms step_avg:61.08ms
step:1452/2330 train_time:88696ms step_avg:61.09ms
step:1453/2330 train_time:88755ms step_avg:61.08ms
step:1454/2330 train_time:88818ms step_avg:61.09ms
step:1455/2330 train_time:88878ms step_avg:61.08ms
step:1456/2330 train_time:88940ms step_avg:61.09ms
step:1457/2330 train_time:89000ms step_avg:61.08ms
step:1458/2330 train_time:89062ms step_avg:61.09ms
step:1459/2330 train_time:89122ms step_avg:61.08ms
step:1460/2330 train_time:89185ms step_avg:61.09ms
step:1461/2330 train_time:89245ms step_avg:61.08ms
step:1462/2330 train_time:89308ms step_avg:61.09ms
step:1463/2330 train_time:89368ms step_avg:61.09ms
step:1464/2330 train_time:89430ms step_avg:61.09ms
step:1465/2330 train_time:89490ms step_avg:61.09ms
step:1466/2330 train_time:89553ms step_avg:61.09ms
step:1467/2330 train_time:89613ms step_avg:61.09ms
step:1468/2330 train_time:89675ms step_avg:61.09ms
step:1469/2330 train_time:89735ms step_avg:61.09ms
step:1470/2330 train_time:89798ms step_avg:61.09ms
step:1471/2330 train_time:89857ms step_avg:61.09ms
step:1472/2330 train_time:89920ms step_avg:61.09ms
step:1473/2330 train_time:89979ms step_avg:61.09ms
step:1474/2330 train_time:90042ms step_avg:61.09ms
step:1475/2330 train_time:90102ms step_avg:61.09ms
step:1476/2330 train_time:90164ms step_avg:61.09ms
step:1477/2330 train_time:90224ms step_avg:61.09ms
step:1478/2330 train_time:90288ms step_avg:61.09ms
step:1479/2330 train_time:90348ms step_avg:61.09ms
step:1480/2330 train_time:90411ms step_avg:61.09ms
step:1481/2330 train_time:90470ms step_avg:61.09ms
step:1482/2330 train_time:90533ms step_avg:61.09ms
step:1483/2330 train_time:90593ms step_avg:61.09ms
step:1484/2330 train_time:90656ms step_avg:61.09ms
step:1485/2330 train_time:90716ms step_avg:61.09ms
step:1486/2330 train_time:90778ms step_avg:61.09ms
step:1487/2330 train_time:90838ms step_avg:61.09ms
step:1488/2330 train_time:90900ms step_avg:61.09ms
step:1489/2330 train_time:90960ms step_avg:61.09ms
step:1490/2330 train_time:91023ms step_avg:61.09ms
step:1491/2330 train_time:91083ms step_avg:61.09ms
step:1492/2330 train_time:91146ms step_avg:61.09ms
step:1493/2330 train_time:91206ms step_avg:61.09ms
step:1494/2330 train_time:91268ms step_avg:61.09ms
step:1495/2330 train_time:91329ms step_avg:61.09ms
step:1496/2330 train_time:91391ms step_avg:61.09ms
step:1497/2330 train_time:91451ms step_avg:61.09ms
step:1498/2330 train_time:91514ms step_avg:61.09ms
step:1499/2330 train_time:91574ms step_avg:61.09ms
step:1500/2330 train_time:91637ms step_avg:61.09ms
step:1500/2330 val_loss:3.4814 train_time:91701ms step_avg:61.13ms
step:1501/2330 train_time:91723ms step_avg:61.11ms
step:1502/2330 train_time:91763ms step_avg:61.09ms
step:1503/2330 train_time:91827ms step_avg:61.10ms
step:1504/2330 train_time:91891ms step_avg:61.10ms
step:1505/2330 train_time:91952ms step_avg:61.10ms
step:1506/2330 train_time:92016ms step_avg:61.10ms
step:1507/2330 train_time:92075ms step_avg:61.10ms
step:1508/2330 train_time:92137ms step_avg:61.10ms
step:1509/2330 train_time:92197ms step_avg:61.10ms
step:1510/2330 train_time:92259ms step_avg:61.10ms
step:1511/2330 train_time:92318ms step_avg:61.10ms
step:1512/2330 train_time:92380ms step_avg:61.10ms
step:1513/2330 train_time:92439ms step_avg:61.10ms
step:1514/2330 train_time:92502ms step_avg:61.10ms
step:1515/2330 train_time:92560ms step_avg:61.10ms
step:1516/2330 train_time:92623ms step_avg:61.10ms
step:1517/2330 train_time:92683ms step_avg:61.10ms
step:1518/2330 train_time:92747ms step_avg:61.10ms
step:1519/2330 train_time:92807ms step_avg:61.10ms
step:1520/2330 train_time:92871ms step_avg:61.10ms
step:1521/2330 train_time:92932ms step_avg:61.10ms
step:1522/2330 train_time:92995ms step_avg:61.10ms
step:1523/2330 train_time:93055ms step_avg:61.10ms
step:1524/2330 train_time:93118ms step_avg:61.10ms
step:1525/2330 train_time:93177ms step_avg:61.10ms
step:1526/2330 train_time:93240ms step_avg:61.10ms
step:1527/2330 train_time:93298ms step_avg:61.10ms
step:1528/2330 train_time:93361ms step_avg:61.10ms
step:1529/2330 train_time:93420ms step_avg:61.10ms
step:1530/2330 train_time:93483ms step_avg:61.10ms
step:1531/2330 train_time:93543ms step_avg:61.10ms
step:1532/2330 train_time:93606ms step_avg:61.10ms
step:1533/2330 train_time:93666ms step_avg:61.10ms
step:1534/2330 train_time:93730ms step_avg:61.10ms
step:1535/2330 train_time:93791ms step_avg:61.10ms
step:1536/2330 train_time:93854ms step_avg:61.10ms
step:1537/2330 train_time:93915ms step_avg:61.10ms
step:1538/2330 train_time:93979ms step_avg:61.10ms
step:1539/2330 train_time:94039ms step_avg:61.10ms
step:1540/2330 train_time:94102ms step_avg:61.11ms
step:1541/2330 train_time:94162ms step_avg:61.10ms
step:1542/2330 train_time:94224ms step_avg:61.11ms
step:1543/2330 train_time:94285ms step_avg:61.10ms
step:1544/2330 train_time:94348ms step_avg:61.11ms
step:1545/2330 train_time:94408ms step_avg:61.11ms
step:1546/2330 train_time:94471ms step_avg:61.11ms
step:1547/2330 train_time:94531ms step_avg:61.11ms
step:1548/2330 train_time:94595ms step_avg:61.11ms
step:1549/2330 train_time:94656ms step_avg:61.11ms
step:1550/2330 train_time:94720ms step_avg:61.11ms
step:1551/2330 train_time:94780ms step_avg:61.11ms
step:1552/2330 train_time:94843ms step_avg:61.11ms
step:1553/2330 train_time:94904ms step_avg:61.11ms
step:1554/2330 train_time:94966ms step_avg:61.11ms
step:1555/2330 train_time:95027ms step_avg:61.11ms
step:1556/2330 train_time:95090ms step_avg:61.11ms
step:1557/2330 train_time:95151ms step_avg:61.11ms
step:1558/2330 train_time:95215ms step_avg:61.11ms
step:1559/2330 train_time:95275ms step_avg:61.11ms
step:1560/2330 train_time:95338ms step_avg:61.11ms
step:1561/2330 train_time:95398ms step_avg:61.11ms
step:1562/2330 train_time:95461ms step_avg:61.11ms
step:1563/2330 train_time:95520ms step_avg:61.11ms
step:1564/2330 train_time:95584ms step_avg:61.12ms
step:1565/2330 train_time:95644ms step_avg:61.11ms
step:1566/2330 train_time:95707ms step_avg:61.12ms
step:1567/2330 train_time:95767ms step_avg:61.12ms
step:1568/2330 train_time:95831ms step_avg:61.12ms
step:1569/2330 train_time:95891ms step_avg:61.12ms
step:1570/2330 train_time:95955ms step_avg:61.12ms
step:1571/2330 train_time:96014ms step_avg:61.12ms
step:1572/2330 train_time:96078ms step_avg:61.12ms
step:1573/2330 train_time:96138ms step_avg:61.12ms
step:1574/2330 train_time:96201ms step_avg:61.12ms
step:1575/2330 train_time:96261ms step_avg:61.12ms
step:1576/2330 train_time:96324ms step_avg:61.12ms
step:1577/2330 train_time:96384ms step_avg:61.12ms
step:1578/2330 train_time:96447ms step_avg:61.12ms
step:1579/2330 train_time:96508ms step_avg:61.12ms
step:1580/2330 train_time:96571ms step_avg:61.12ms
step:1581/2330 train_time:96632ms step_avg:61.12ms
step:1582/2330 train_time:96696ms step_avg:61.12ms
step:1583/2330 train_time:96756ms step_avg:61.12ms
step:1584/2330 train_time:96819ms step_avg:61.12ms
step:1585/2330 train_time:96879ms step_avg:61.12ms
step:1586/2330 train_time:96942ms step_avg:61.12ms
step:1587/2330 train_time:97002ms step_avg:61.12ms
step:1588/2330 train_time:97065ms step_avg:61.12ms
step:1589/2330 train_time:97126ms step_avg:61.12ms
step:1590/2330 train_time:97190ms step_avg:61.13ms
step:1591/2330 train_time:97250ms step_avg:61.13ms
step:1592/2330 train_time:97313ms step_avg:61.13ms
step:1593/2330 train_time:97374ms step_avg:61.13ms
step:1594/2330 train_time:97438ms step_avg:61.13ms
step:1595/2330 train_time:97498ms step_avg:61.13ms
step:1596/2330 train_time:97560ms step_avg:61.13ms
step:1597/2330 train_time:97621ms step_avg:61.13ms
step:1598/2330 train_time:97684ms step_avg:61.13ms
step:1599/2330 train_time:97744ms step_avg:61.13ms
step:1600/2330 train_time:97807ms step_avg:61.13ms
step:1601/2330 train_time:97868ms step_avg:61.13ms
step:1602/2330 train_time:97931ms step_avg:61.13ms
step:1603/2330 train_time:97991ms step_avg:61.13ms
step:1604/2330 train_time:98055ms step_avg:61.13ms
step:1605/2330 train_time:98115ms step_avg:61.13ms
step:1606/2330 train_time:98179ms step_avg:61.13ms
step:1607/2330 train_time:98238ms step_avg:61.13ms
step:1608/2330 train_time:98301ms step_avg:61.13ms
step:1609/2330 train_time:98361ms step_avg:61.13ms
step:1610/2330 train_time:98424ms step_avg:61.13ms
step:1611/2330 train_time:98484ms step_avg:61.13ms
step:1612/2330 train_time:98547ms step_avg:61.13ms
step:1613/2330 train_time:98607ms step_avg:61.13ms
step:1614/2330 train_time:98671ms step_avg:61.13ms
step:1615/2330 train_time:98732ms step_avg:61.13ms
step:1616/2330 train_time:98795ms step_avg:61.14ms
step:1617/2330 train_time:98855ms step_avg:61.13ms
step:1618/2330 train_time:98918ms step_avg:61.14ms
step:1619/2330 train_time:98979ms step_avg:61.14ms
step:1620/2330 train_time:99042ms step_avg:61.14ms
step:1621/2330 train_time:99102ms step_avg:61.14ms
step:1622/2330 train_time:99165ms step_avg:61.14ms
step:1623/2330 train_time:99225ms step_avg:61.14ms
step:1624/2330 train_time:99288ms step_avg:61.14ms
step:1625/2330 train_time:99349ms step_avg:61.14ms
step:1626/2330 train_time:99412ms step_avg:61.14ms
step:1627/2330 train_time:99473ms step_avg:61.14ms
step:1628/2330 train_time:99536ms step_avg:61.14ms
step:1629/2330 train_time:99596ms step_avg:61.14ms
step:1630/2330 train_time:99659ms step_avg:61.14ms
step:1631/2330 train_time:99720ms step_avg:61.14ms
step:1632/2330 train_time:99783ms step_avg:61.14ms
step:1633/2330 train_time:99843ms step_avg:61.14ms
step:1634/2330 train_time:99906ms step_avg:61.14ms
step:1635/2330 train_time:99966ms step_avg:61.14ms
step:1636/2330 train_time:100030ms step_avg:61.14ms
step:1637/2330 train_time:100090ms step_avg:61.14ms
step:1638/2330 train_time:100153ms step_avg:61.14ms
step:1639/2330 train_time:100214ms step_avg:61.14ms
step:1640/2330 train_time:100277ms step_avg:61.14ms
step:1641/2330 train_time:100338ms step_avg:61.14ms
step:1642/2330 train_time:100401ms step_avg:61.15ms
step:1643/2330 train_time:100461ms step_avg:61.15ms
step:1644/2330 train_time:100525ms step_avg:61.15ms
step:1645/2330 train_time:100585ms step_avg:61.15ms
step:1646/2330 train_time:100648ms step_avg:61.15ms
step:1647/2330 train_time:100709ms step_avg:61.15ms
step:1648/2330 train_time:100772ms step_avg:61.15ms
step:1649/2330 train_time:100833ms step_avg:61.15ms
step:1650/2330 train_time:100896ms step_avg:61.15ms
step:1651/2330 train_time:100956ms step_avg:61.15ms
step:1652/2330 train_time:101019ms step_avg:61.15ms
step:1653/2330 train_time:101079ms step_avg:61.15ms
step:1654/2330 train_time:101142ms step_avg:61.15ms
step:1655/2330 train_time:101202ms step_avg:61.15ms
step:1656/2330 train_time:101265ms step_avg:61.15ms
step:1657/2330 train_time:101325ms step_avg:61.15ms
step:1658/2330 train_time:101388ms step_avg:61.15ms
step:1659/2330 train_time:101449ms step_avg:61.15ms
step:1660/2330 train_time:101512ms step_avg:61.15ms
step:1661/2330 train_time:101573ms step_avg:61.15ms
step:1662/2330 train_time:101636ms step_avg:61.15ms
step:1663/2330 train_time:101696ms step_avg:61.15ms
step:1664/2330 train_time:101759ms step_avg:61.15ms
step:1665/2330 train_time:101820ms step_avg:61.15ms
step:1666/2330 train_time:101883ms step_avg:61.15ms
step:1667/2330 train_time:101943ms step_avg:61.15ms
step:1668/2330 train_time:102006ms step_avg:61.15ms
step:1669/2330 train_time:102067ms step_avg:61.15ms
step:1670/2330 train_time:102130ms step_avg:61.16ms
step:1671/2330 train_time:102190ms step_avg:61.16ms
step:1672/2330 train_time:102254ms step_avg:61.16ms
step:1673/2330 train_time:102314ms step_avg:61.16ms
step:1674/2330 train_time:102377ms step_avg:61.16ms
step:1675/2330 train_time:102439ms step_avg:61.16ms
step:1676/2330 train_time:102502ms step_avg:61.16ms
step:1677/2330 train_time:102562ms step_avg:61.16ms
step:1678/2330 train_time:102626ms step_avg:61.16ms
step:1679/2330 train_time:102686ms step_avg:61.16ms
step:1680/2330 train_time:102749ms step_avg:61.16ms
step:1681/2330 train_time:102809ms step_avg:61.16ms
step:1682/2330 train_time:102873ms step_avg:61.16ms
step:1683/2330 train_time:102934ms step_avg:61.16ms
step:1684/2330 train_time:102997ms step_avg:61.16ms
step:1685/2330 train_time:103057ms step_avg:61.16ms
step:1686/2330 train_time:103120ms step_avg:61.16ms
step:1687/2330 train_time:103180ms step_avg:61.16ms
step:1688/2330 train_time:103243ms step_avg:61.16ms
step:1689/2330 train_time:103303ms step_avg:61.16ms
step:1690/2330 train_time:103366ms step_avg:61.16ms
step:1691/2330 train_time:103427ms step_avg:61.16ms
step:1692/2330 train_time:103491ms step_avg:61.16ms
step:1693/2330 train_time:103551ms step_avg:61.16ms
step:1694/2330 train_time:103615ms step_avg:61.17ms
step:1695/2330 train_time:103675ms step_avg:61.17ms
step:1696/2330 train_time:103738ms step_avg:61.17ms
step:1697/2330 train_time:103798ms step_avg:61.17ms
step:1698/2330 train_time:103861ms step_avg:61.17ms
step:1699/2330 train_time:103921ms step_avg:61.17ms
step:1700/2330 train_time:103984ms step_avg:61.17ms
step:1701/2330 train_time:104044ms step_avg:61.17ms
step:1702/2330 train_time:104108ms step_avg:61.17ms
step:1703/2330 train_time:104167ms step_avg:61.17ms
step:1704/2330 train_time:104231ms step_avg:61.17ms
step:1705/2330 train_time:104291ms step_avg:61.17ms
step:1706/2330 train_time:104354ms step_avg:61.17ms
step:1707/2330 train_time:104415ms step_avg:61.17ms
step:1708/2330 train_time:104478ms step_avg:61.17ms
step:1709/2330 train_time:104538ms step_avg:61.17ms
step:1710/2330 train_time:104601ms step_avg:61.17ms
step:1711/2330 train_time:104661ms step_avg:61.17ms
step:1712/2330 train_time:104724ms step_avg:61.17ms
step:1713/2330 train_time:104784ms step_avg:61.17ms
step:1714/2330 train_time:104848ms step_avg:61.17ms
step:1715/2330 train_time:104908ms step_avg:61.17ms
step:1716/2330 train_time:104971ms step_avg:61.17ms
step:1717/2330 train_time:105032ms step_avg:61.17ms
step:1718/2330 train_time:105095ms step_avg:61.17ms
step:1719/2330 train_time:105155ms step_avg:61.17ms
step:1720/2330 train_time:105218ms step_avg:61.17ms
step:1721/2330 train_time:105278ms step_avg:61.17ms
step:1722/2330 train_time:105342ms step_avg:61.17ms
step:1723/2330 train_time:105402ms step_avg:61.17ms
step:1724/2330 train_time:105465ms step_avg:61.17ms
step:1725/2330 train_time:105525ms step_avg:61.17ms
step:1726/2330 train_time:105588ms step_avg:61.18ms
step:1727/2330 train_time:105649ms step_avg:61.17ms
step:1728/2330 train_time:105712ms step_avg:61.18ms
step:1729/2330 train_time:105773ms step_avg:61.18ms
step:1730/2330 train_time:105837ms step_avg:61.18ms
step:1731/2330 train_time:105897ms step_avg:61.18ms
step:1732/2330 train_time:105960ms step_avg:61.18ms
step:1733/2330 train_time:106022ms step_avg:61.18ms
step:1734/2330 train_time:106085ms step_avg:61.18ms
step:1735/2330 train_time:106144ms step_avg:61.18ms
step:1736/2330 train_time:106207ms step_avg:61.18ms
step:1737/2330 train_time:106268ms step_avg:61.18ms
step:1738/2330 train_time:106332ms step_avg:61.18ms
step:1739/2330 train_time:106392ms step_avg:61.18ms
step:1740/2330 train_time:106456ms step_avg:61.18ms
step:1741/2330 train_time:106517ms step_avg:61.18ms
step:1742/2330 train_time:106581ms step_avg:61.18ms
step:1743/2330 train_time:106641ms step_avg:61.18ms
step:1744/2330 train_time:106703ms step_avg:61.18ms
step:1745/2330 train_time:106763ms step_avg:61.18ms
step:1746/2330 train_time:106826ms step_avg:61.18ms
step:1747/2330 train_time:106887ms step_avg:61.18ms
step:1748/2330 train_time:106949ms step_avg:61.18ms
step:1749/2330 train_time:107010ms step_avg:61.18ms
step:1750/2330 train_time:107073ms step_avg:61.18ms
step:1750/2330 val_loss:3.4372 train_time:107138ms step_avg:61.22ms
step:1751/2330 train_time:107161ms step_avg:61.20ms
step:1752/2330 train_time:107201ms step_avg:61.19ms
step:1753/2330 train_time:107270ms step_avg:61.19ms
step:1754/2330 train_time:107337ms step_avg:61.20ms
step:1755/2330 train_time:107397ms step_avg:61.19ms
step:1756/2330 train_time:107461ms step_avg:61.20ms
step:1757/2330 train_time:107520ms step_avg:61.20ms
step:1758/2330 train_time:107583ms step_avg:61.20ms
step:1759/2330 train_time:107642ms step_avg:61.19ms
step:1760/2330 train_time:107704ms step_avg:61.20ms
step:1761/2330 train_time:107763ms step_avg:61.19ms
step:1762/2330 train_time:107825ms step_avg:61.19ms
step:1763/2330 train_time:107885ms step_avg:61.19ms
step:1764/2330 train_time:107947ms step_avg:61.19ms
step:1765/2330 train_time:108007ms step_avg:61.19ms
step:1766/2330 train_time:108070ms step_avg:61.19ms
step:1767/2330 train_time:108131ms step_avg:61.19ms
step:1768/2330 train_time:108196ms step_avg:61.20ms
step:1769/2330 train_time:108259ms step_avg:61.20ms
step:1770/2330 train_time:108323ms step_avg:61.20ms
step:1771/2330 train_time:108384ms step_avg:61.20ms
step:1772/2330 train_time:108446ms step_avg:61.20ms
step:1773/2330 train_time:108507ms step_avg:61.20ms
step:1774/2330 train_time:108571ms step_avg:61.20ms
step:1775/2330 train_time:108631ms step_avg:61.20ms
step:1776/2330 train_time:108695ms step_avg:61.20ms
step:1777/2330 train_time:108755ms step_avg:61.20ms
step:1778/2330 train_time:108818ms step_avg:61.20ms
step:1779/2330 train_time:108879ms step_avg:61.20ms
step:1780/2330 train_time:108941ms step_avg:61.20ms
step:1781/2330 train_time:109002ms step_avg:61.20ms
step:1782/2330 train_time:109064ms step_avg:61.20ms
step:1783/2330 train_time:109125ms step_avg:61.20ms
step:1784/2330 train_time:109189ms step_avg:61.20ms
step:1785/2330 train_time:109249ms step_avg:61.20ms
step:1786/2330 train_time:109313ms step_avg:61.21ms
step:1787/2330 train_time:109373ms step_avg:61.20ms
step:1788/2330 train_time:109437ms step_avg:61.21ms
step:1789/2330 train_time:109497ms step_avg:61.21ms
step:1790/2330 train_time:109561ms step_avg:61.21ms
step:1791/2330 train_time:109620ms step_avg:61.21ms
step:1792/2330 train_time:109684ms step_avg:61.21ms
step:1793/2330 train_time:109744ms step_avg:61.21ms
step:1794/2330 train_time:109807ms step_avg:61.21ms
step:1795/2330 train_time:109867ms step_avg:61.21ms
step:1796/2330 train_time:109930ms step_avg:61.21ms
step:1797/2330 train_time:109990ms step_avg:61.21ms
step:1798/2330 train_time:110053ms step_avg:61.21ms
step:1799/2330 train_time:110113ms step_avg:61.21ms
step:1800/2330 train_time:110177ms step_avg:61.21ms
step:1801/2330 train_time:110237ms step_avg:61.21ms
step:1802/2330 train_time:110301ms step_avg:61.21ms
step:1803/2330 train_time:110362ms step_avg:61.21ms
step:1804/2330 train_time:110426ms step_avg:61.21ms
step:1805/2330 train_time:110486ms step_avg:61.21ms
step:1806/2330 train_time:110549ms step_avg:61.21ms
step:1807/2330 train_time:110609ms step_avg:61.21ms
step:1808/2330 train_time:110673ms step_avg:61.21ms
step:1809/2330 train_time:110733ms step_avg:61.21ms
step:1810/2330 train_time:110796ms step_avg:61.21ms
step:1811/2330 train_time:110856ms step_avg:61.21ms
step:1812/2330 train_time:110920ms step_avg:61.21ms
step:1813/2330 train_time:110979ms step_avg:61.21ms
step:1814/2330 train_time:111043ms step_avg:61.21ms
step:1815/2330 train_time:111103ms step_avg:61.21ms
step:1816/2330 train_time:111166ms step_avg:61.21ms
step:1817/2330 train_time:111226ms step_avg:61.21ms
step:1818/2330 train_time:111289ms step_avg:61.21ms
step:1819/2330 train_time:111350ms step_avg:61.21ms
step:1820/2330 train_time:111413ms step_avg:61.22ms
step:1821/2330 train_time:111473ms step_avg:61.22ms
step:1822/2330 train_time:111537ms step_avg:61.22ms
step:1823/2330 train_time:111597ms step_avg:61.22ms
step:1824/2330 train_time:111660ms step_avg:61.22ms
step:1825/2330 train_time:111720ms step_avg:61.22ms
step:1826/2330 train_time:111783ms step_avg:61.22ms
step:1827/2330 train_time:111843ms step_avg:61.22ms
step:1828/2330 train_time:111907ms step_avg:61.22ms
step:1829/2330 train_time:111967ms step_avg:61.22ms
step:1830/2330 train_time:112030ms step_avg:61.22ms
step:1831/2330 train_time:112090ms step_avg:61.22ms
step:1832/2330 train_time:112154ms step_avg:61.22ms
step:1833/2330 train_time:112215ms step_avg:61.22ms
step:1834/2330 train_time:112278ms step_avg:61.22ms
step:1835/2330 train_time:112338ms step_avg:61.22ms
step:1836/2330 train_time:112402ms step_avg:61.22ms
step:1837/2330 train_time:112462ms step_avg:61.22ms
step:1838/2330 train_time:112525ms step_avg:61.22ms
step:1839/2330 train_time:112585ms step_avg:61.22ms
step:1840/2330 train_time:112648ms step_avg:61.22ms
step:1841/2330 train_time:112709ms step_avg:61.22ms
step:1842/2330 train_time:112773ms step_avg:61.22ms
step:1843/2330 train_time:112833ms step_avg:61.22ms
step:1844/2330 train_time:112897ms step_avg:61.22ms
step:1845/2330 train_time:112958ms step_avg:61.22ms
step:1846/2330 train_time:113021ms step_avg:61.23ms
step:1847/2330 train_time:113082ms step_avg:61.22ms
step:1848/2330 train_time:113144ms step_avg:61.23ms
step:1849/2330 train_time:113205ms step_avg:61.22ms
step:1850/2330 train_time:113268ms step_avg:61.23ms
step:1851/2330 train_time:113328ms step_avg:61.23ms
step:1852/2330 train_time:113392ms step_avg:61.23ms
step:1853/2330 train_time:113452ms step_avg:61.23ms
step:1854/2330 train_time:113516ms step_avg:61.23ms
step:1855/2330 train_time:113576ms step_avg:61.23ms
step:1856/2330 train_time:113640ms step_avg:61.23ms
step:1857/2330 train_time:113700ms step_avg:61.23ms
step:1858/2330 train_time:113763ms step_avg:61.23ms
step:1859/2330 train_time:113823ms step_avg:61.23ms
step:1860/2330 train_time:113886ms step_avg:61.23ms
step:1861/2330 train_time:113947ms step_avg:61.23ms
step:1862/2330 train_time:114009ms step_avg:61.23ms
step:1863/2330 train_time:114069ms step_avg:61.23ms
step:1864/2330 train_time:114133ms step_avg:61.23ms
step:1865/2330 train_time:114193ms step_avg:61.23ms
step:1866/2330 train_time:114257ms step_avg:61.23ms
step:1867/2330 train_time:114317ms step_avg:61.23ms
step:1868/2330 train_time:114380ms step_avg:61.23ms
step:1869/2330 train_time:114440ms step_avg:61.23ms
step:1870/2330 train_time:114503ms step_avg:61.23ms
step:1871/2330 train_time:114563ms step_avg:61.23ms
step:1872/2330 train_time:114626ms step_avg:61.23ms
step:1873/2330 train_time:114686ms step_avg:61.23ms
step:1874/2330 train_time:114749ms step_avg:61.23ms
step:1875/2330 train_time:114809ms step_avg:61.23ms
step:1876/2330 train_time:114873ms step_avg:61.23ms
step:1877/2330 train_time:114933ms step_avg:61.23ms
step:1878/2330 train_time:114997ms step_avg:61.23ms
step:1879/2330 train_time:115057ms step_avg:61.23ms
step:1880/2330 train_time:115120ms step_avg:61.23ms
step:1881/2330 train_time:115180ms step_avg:61.23ms
step:1882/2330 train_time:115243ms step_avg:61.23ms
step:1883/2330 train_time:115304ms step_avg:61.23ms
step:1884/2330 train_time:115367ms step_avg:61.24ms
step:1885/2330 train_time:115427ms step_avg:61.23ms
step:1886/2330 train_time:115491ms step_avg:61.24ms
step:1887/2330 train_time:115551ms step_avg:61.24ms
step:1888/2330 train_time:115615ms step_avg:61.24ms
step:1889/2330 train_time:115675ms step_avg:61.24ms
step:1890/2330 train_time:115739ms step_avg:61.24ms
step:1891/2330 train_time:115800ms step_avg:61.24ms
step:1892/2330 train_time:115862ms step_avg:61.24ms
step:1893/2330 train_time:115923ms step_avg:61.24ms
step:1894/2330 train_time:115986ms step_avg:61.24ms
step:1895/2330 train_time:116046ms step_avg:61.24ms
step:1896/2330 train_time:116109ms step_avg:61.24ms
step:1897/2330 train_time:116169ms step_avg:61.24ms
step:1898/2330 train_time:116232ms step_avg:61.24ms
step:1899/2330 train_time:116292ms step_avg:61.24ms
step:1900/2330 train_time:116356ms step_avg:61.24ms
step:1901/2330 train_time:116416ms step_avg:61.24ms
step:1902/2330 train_time:116479ms step_avg:61.24ms
step:1903/2330 train_time:116539ms step_avg:61.24ms
step:1904/2330 train_time:116602ms step_avg:61.24ms
step:1905/2330 train_time:116663ms step_avg:61.24ms
step:1906/2330 train_time:116726ms step_avg:61.24ms
step:1907/2330 train_time:116786ms step_avg:61.24ms
step:1908/2330 train_time:116849ms step_avg:61.24ms
step:1909/2330 train_time:116909ms step_avg:61.24ms
step:1910/2330 train_time:116973ms step_avg:61.24ms
step:1911/2330 train_time:117033ms step_avg:61.24ms
step:1912/2330 train_time:117097ms step_avg:61.24ms
step:1913/2330 train_time:117157ms step_avg:61.24ms
step:1914/2330 train_time:117220ms step_avg:61.24ms
step:1915/2330 train_time:117280ms step_avg:61.24ms
step:1916/2330 train_time:117343ms step_avg:61.24ms
step:1917/2330 train_time:117404ms step_avg:61.24ms
step:1918/2330 train_time:117467ms step_avg:61.24ms
step:1919/2330 train_time:117527ms step_avg:61.24ms
step:1920/2330 train_time:117590ms step_avg:61.24ms
step:1921/2330 train_time:117650ms step_avg:61.24ms
step:1922/2330 train_time:117714ms step_avg:61.25ms
step:1923/2330 train_time:117775ms step_avg:61.25ms
step:1924/2330 train_time:117838ms step_avg:61.25ms
step:1925/2330 train_time:117899ms step_avg:61.25ms
step:1926/2330 train_time:117962ms step_avg:61.25ms
step:1927/2330 train_time:118022ms step_avg:61.25ms
step:1928/2330 train_time:118086ms step_avg:61.25ms
step:1929/2330 train_time:118147ms step_avg:61.25ms
step:1930/2330 train_time:118209ms step_avg:61.25ms
step:1931/2330 train_time:118269ms step_avg:61.25ms
step:1932/2330 train_time:118333ms step_avg:61.25ms
step:1933/2330 train_time:118393ms step_avg:61.25ms
step:1934/2330 train_time:118457ms step_avg:61.25ms
step:1935/2330 train_time:118518ms step_avg:61.25ms
step:1936/2330 train_time:118581ms step_avg:61.25ms
step:1937/2330 train_time:118641ms step_avg:61.25ms
step:1938/2330 train_time:118704ms step_avg:61.25ms
step:1939/2330 train_time:118764ms step_avg:61.25ms
step:1940/2330 train_time:118828ms step_avg:61.25ms
step:1941/2330 train_time:118888ms step_avg:61.25ms
step:1942/2330 train_time:118952ms step_avg:61.25ms
step:1943/2330 train_time:119012ms step_avg:61.25ms
step:1944/2330 train_time:119075ms step_avg:61.25ms
step:1945/2330 train_time:119136ms step_avg:61.25ms
step:1946/2330 train_time:119199ms step_avg:61.25ms
step:1947/2330 train_time:119260ms step_avg:61.25ms
step:1948/2330 train_time:119323ms step_avg:61.25ms
step:1949/2330 train_time:119383ms step_avg:61.25ms
step:1950/2330 train_time:119447ms step_avg:61.25ms
step:1951/2330 train_time:119507ms step_avg:61.25ms
step:1952/2330 train_time:119571ms step_avg:61.26ms
step:1953/2330 train_time:119631ms step_avg:61.25ms
step:1954/2330 train_time:119695ms step_avg:61.26ms
step:1955/2330 train_time:119756ms step_avg:61.26ms
step:1956/2330 train_time:119819ms step_avg:61.26ms
step:1957/2330 train_time:119880ms step_avg:61.26ms
step:1958/2330 train_time:119943ms step_avg:61.26ms
step:1959/2330 train_time:120003ms step_avg:61.26ms
step:1960/2330 train_time:120066ms step_avg:61.26ms
step:1961/2330 train_time:120126ms step_avg:61.26ms
step:1962/2330 train_time:120190ms step_avg:61.26ms
step:1963/2330 train_time:120249ms step_avg:61.26ms
step:1964/2330 train_time:120313ms step_avg:61.26ms
step:1965/2330 train_time:120372ms step_avg:61.26ms
step:1966/2330 train_time:120436ms step_avg:61.26ms
step:1967/2330 train_time:120497ms step_avg:61.26ms
step:1968/2330 train_time:120560ms step_avg:61.26ms
step:1969/2330 train_time:120621ms step_avg:61.26ms
step:1970/2330 train_time:120684ms step_avg:61.26ms
step:1971/2330 train_time:120744ms step_avg:61.26ms
step:1972/2330 train_time:120808ms step_avg:61.26ms
step:1973/2330 train_time:120868ms step_avg:61.26ms
step:1974/2330 train_time:120931ms step_avg:61.26ms
step:1975/2330 train_time:120991ms step_avg:61.26ms
step:1976/2330 train_time:121054ms step_avg:61.26ms
step:1977/2330 train_time:121115ms step_avg:61.26ms
step:1978/2330 train_time:121178ms step_avg:61.26ms
step:1979/2330 train_time:121238ms step_avg:61.26ms
step:1980/2330 train_time:121301ms step_avg:61.26ms
step:1981/2330 train_time:121361ms step_avg:61.26ms
step:1982/2330 train_time:121424ms step_avg:61.26ms
step:1983/2330 train_time:121485ms step_avg:61.26ms
step:1984/2330 train_time:121548ms step_avg:61.26ms
step:1985/2330 train_time:121607ms step_avg:61.26ms
step:1986/2330 train_time:121671ms step_avg:61.26ms
step:1987/2330 train_time:121731ms step_avg:61.26ms
step:1988/2330 train_time:121795ms step_avg:61.27ms
step:1989/2330 train_time:121855ms step_avg:61.26ms
step:1990/2330 train_time:121919ms step_avg:61.27ms
step:1991/2330 train_time:121979ms step_avg:61.27ms
step:1992/2330 train_time:122043ms step_avg:61.27ms
step:1993/2330 train_time:122103ms step_avg:61.27ms
step:1994/2330 train_time:122167ms step_avg:61.27ms
step:1995/2330 train_time:122227ms step_avg:61.27ms
step:1996/2330 train_time:122290ms step_avg:61.27ms
step:1997/2330 train_time:122351ms step_avg:61.27ms
step:1998/2330 train_time:122414ms step_avg:61.27ms
step:1999/2330 train_time:122475ms step_avg:61.27ms
step:2000/2330 train_time:122538ms step_avg:61.27ms
step:2000/2330 val_loss:3.4101 train_time:122603ms step_avg:61.30ms
step:2001/2330 train_time:122626ms step_avg:61.28ms
step:2002/2330 train_time:122666ms step_avg:61.27ms
step:2003/2330 train_time:122732ms step_avg:61.27ms
step:2004/2330 train_time:122796ms step_avg:61.28ms
step:2005/2330 train_time:122857ms step_avg:61.28ms
step:2006/2330 train_time:122922ms step_avg:61.28ms
step:2007/2330 train_time:122981ms step_avg:61.28ms
step:2008/2330 train_time:123044ms step_avg:61.28ms
step:2009/2330 train_time:123104ms step_avg:61.28ms
step:2010/2330 train_time:123167ms step_avg:61.28ms
step:2011/2330 train_time:123226ms step_avg:61.28ms
step:2012/2330 train_time:123288ms step_avg:61.28ms
step:2013/2330 train_time:123348ms step_avg:61.28ms
step:2014/2330 train_time:123410ms step_avg:61.28ms
step:2015/2330 train_time:123469ms step_avg:61.28ms
step:2016/2330 train_time:123533ms step_avg:61.28ms
step:2017/2330 train_time:123594ms step_avg:61.28ms
step:2018/2330 train_time:123659ms step_avg:61.28ms
step:2019/2330 train_time:123720ms step_avg:61.28ms
step:2020/2330 train_time:123784ms step_avg:61.28ms
step:2021/2330 train_time:123846ms step_avg:61.28ms
step:2022/2330 train_time:123909ms step_avg:61.28ms
step:2023/2330 train_time:123970ms step_avg:61.28ms
step:2024/2330 train_time:124033ms step_avg:61.28ms
step:2025/2330 train_time:124093ms step_avg:61.28ms
step:2026/2330 train_time:124156ms step_avg:61.28ms
step:2027/2330 train_time:124216ms step_avg:61.28ms
step:2028/2330 train_time:124279ms step_avg:61.28ms
step:2029/2330 train_time:124339ms step_avg:61.28ms
step:2030/2330 train_time:124402ms step_avg:61.28ms
step:2031/2330 train_time:124461ms step_avg:61.28ms
step:2032/2330 train_time:124525ms step_avg:61.28ms
step:2033/2330 train_time:124586ms step_avg:61.28ms
step:2034/2330 train_time:124649ms step_avg:61.28ms
step:2035/2330 train_time:124709ms step_avg:61.28ms
step:2036/2330 train_time:124773ms step_avg:61.28ms
step:2037/2330 train_time:124834ms step_avg:61.28ms
step:2038/2330 train_time:124898ms step_avg:61.28ms
step:2039/2330 train_time:124958ms step_avg:61.28ms
step:2040/2330 train_time:125021ms step_avg:61.28ms
step:2041/2330 train_time:125082ms step_avg:61.28ms
step:2042/2330 train_time:125145ms step_avg:61.29ms
step:2043/2330 train_time:125205ms step_avg:61.28ms
step:2044/2330 train_time:125268ms step_avg:61.29ms
step:2045/2330 train_time:125328ms step_avg:61.29ms
step:2046/2330 train_time:125391ms step_avg:61.29ms
step:2047/2330 train_time:125450ms step_avg:61.28ms
step:2048/2330 train_time:125513ms step_avg:61.29ms
step:2049/2330 train_time:125574ms step_avg:61.29ms
step:2050/2330 train_time:125638ms step_avg:61.29ms
step:2051/2330 train_time:125698ms step_avg:61.29ms
step:2052/2330 train_time:125762ms step_avg:61.29ms
step:2053/2330 train_time:125823ms step_avg:61.29ms
step:2054/2330 train_time:125887ms step_avg:61.29ms
step:2055/2330 train_time:125947ms step_avg:61.29ms
step:2056/2330 train_time:126011ms step_avg:61.29ms
step:2057/2330 train_time:126071ms step_avg:61.29ms
step:2058/2330 train_time:126134ms step_avg:61.29ms
step:2059/2330 train_time:126195ms step_avg:61.29ms
step:2060/2330 train_time:126258ms step_avg:61.29ms
step:2061/2330 train_time:126318ms step_avg:61.29ms
step:2062/2330 train_time:126382ms step_avg:61.29ms
step:2063/2330 train_time:126442ms step_avg:61.29ms
step:2064/2330 train_time:126505ms step_avg:61.29ms
step:2065/2330 train_time:126566ms step_avg:61.29ms
step:2066/2330 train_time:126629ms step_avg:61.29ms
step:2067/2330 train_time:126689ms step_avg:61.29ms
step:2068/2330 train_time:126752ms step_avg:61.29ms
step:2069/2330 train_time:126813ms step_avg:61.29ms
step:2070/2330 train_time:126877ms step_avg:61.29ms
step:2071/2330 train_time:126937ms step_avg:61.29ms
step:2072/2330 train_time:127001ms step_avg:61.29ms
step:2073/2330 train_time:127061ms step_avg:61.29ms
step:2074/2330 train_time:127126ms step_avg:61.29ms
step:2075/2330 train_time:127186ms step_avg:61.29ms
step:2076/2330 train_time:127249ms step_avg:61.30ms
step:2077/2330 train_time:127310ms step_avg:61.29ms
step:2078/2330 train_time:127373ms step_avg:61.30ms
step:2079/2330 train_time:127433ms step_avg:61.30ms
step:2080/2330 train_time:127496ms step_avg:61.30ms
step:2081/2330 train_time:127557ms step_avg:61.30ms
step:2082/2330 train_time:127620ms step_avg:61.30ms
step:2083/2330 train_time:127680ms step_avg:61.30ms
step:2084/2330 train_time:127743ms step_avg:61.30ms
step:2085/2330 train_time:127804ms step_avg:61.30ms
step:2086/2330 train_time:127867ms step_avg:61.30ms
step:2087/2330 train_time:127928ms step_avg:61.30ms
step:2088/2330 train_time:127991ms step_avg:61.30ms
step:2089/2330 train_time:128051ms step_avg:61.30ms
step:2090/2330 train_time:128114ms step_avg:61.30ms
step:2091/2330 train_time:128175ms step_avg:61.30ms
step:2092/2330 train_time:128238ms step_avg:61.30ms
step:2093/2330 train_time:128298ms step_avg:61.30ms
step:2094/2330 train_time:128362ms step_avg:61.30ms
step:2095/2330 train_time:128422ms step_avg:61.30ms
step:2096/2330 train_time:128485ms step_avg:61.30ms
step:2097/2330 train_time:128545ms step_avg:61.30ms
step:2098/2330 train_time:128609ms step_avg:61.30ms
step:2099/2330 train_time:128668ms step_avg:61.30ms
step:2100/2330 train_time:128731ms step_avg:61.30ms
step:2101/2330 train_time:128792ms step_avg:61.30ms
step:2102/2330 train_time:128855ms step_avg:61.30ms
step:2103/2330 train_time:128915ms step_avg:61.30ms
step:2104/2330 train_time:128979ms step_avg:61.30ms
step:2105/2330 train_time:129039ms step_avg:61.30ms
step:2106/2330 train_time:129102ms step_avg:61.30ms
step:2107/2330 train_time:129164ms step_avg:61.30ms
step:2108/2330 train_time:129227ms step_avg:61.30ms
step:2109/2330 train_time:129287ms step_avg:61.30ms
step:2110/2330 train_time:129350ms step_avg:61.30ms
step:2111/2330 train_time:129410ms step_avg:61.30ms
step:2112/2330 train_time:129473ms step_avg:61.30ms
step:2113/2330 train_time:129533ms step_avg:61.30ms
step:2114/2330 train_time:129597ms step_avg:61.30ms
step:2115/2330 train_time:129657ms step_avg:61.30ms
step:2116/2330 train_time:129721ms step_avg:61.30ms
step:2117/2330 train_time:129781ms step_avg:61.30ms
step:2118/2330 train_time:129845ms step_avg:61.31ms
step:2119/2330 train_time:129905ms step_avg:61.30ms
step:2120/2330 train_time:129968ms step_avg:61.31ms
step:2121/2330 train_time:130028ms step_avg:61.31ms
step:2122/2330 train_time:130091ms step_avg:61.31ms
step:2123/2330 train_time:130152ms step_avg:61.31ms
step:2124/2330 train_time:130215ms step_avg:61.31ms
step:2125/2330 train_time:130275ms step_avg:61.31ms
step:2126/2330 train_time:130338ms step_avg:61.31ms
step:2127/2330 train_time:130398ms step_avg:61.31ms
step:2128/2330 train_time:130462ms step_avg:61.31ms
step:2129/2330 train_time:130522ms step_avg:61.31ms
step:2130/2330 train_time:130585ms step_avg:61.31ms
step:2131/2330 train_time:130646ms step_avg:61.31ms
step:2132/2330 train_time:130709ms step_avg:61.31ms
step:2133/2330 train_time:130769ms step_avg:61.31ms
step:2134/2330 train_time:130832ms step_avg:61.31ms
step:2135/2330 train_time:130892ms step_avg:61.31ms
step:2136/2330 train_time:130955ms step_avg:61.31ms
step:2137/2330 train_time:131015ms step_avg:61.31ms
step:2138/2330 train_time:131080ms step_avg:61.31ms
step:2139/2330 train_time:131140ms step_avg:61.31ms
step:2140/2330 train_time:131204ms step_avg:61.31ms
step:2141/2330 train_time:131264ms step_avg:61.31ms
step:2142/2330 train_time:131327ms step_avg:61.31ms
step:2143/2330 train_time:131387ms step_avg:61.31ms
step:2144/2330 train_time:131451ms step_avg:61.31ms
step:2145/2330 train_time:131511ms step_avg:61.31ms
step:2146/2330 train_time:131574ms step_avg:61.31ms
step:2147/2330 train_time:131634ms step_avg:61.31ms
step:2148/2330 train_time:131698ms step_avg:61.31ms
step:2149/2330 train_time:131758ms step_avg:61.31ms
step:2150/2330 train_time:131821ms step_avg:61.31ms
step:2151/2330 train_time:131882ms step_avg:61.31ms
step:2152/2330 train_time:131944ms step_avg:61.31ms
step:2153/2330 train_time:132005ms step_avg:61.31ms
step:2154/2330 train_time:132068ms step_avg:61.31ms
step:2155/2330 train_time:132129ms step_avg:61.31ms
step:2156/2330 train_time:132192ms step_avg:61.31ms
step:2157/2330 train_time:132253ms step_avg:61.31ms
step:2158/2330 train_time:132316ms step_avg:61.31ms
step:2159/2330 train_time:132377ms step_avg:61.31ms
step:2160/2330 train_time:132440ms step_avg:61.31ms
step:2161/2330 train_time:132500ms step_avg:61.31ms
step:2162/2330 train_time:132564ms step_avg:61.32ms
step:2163/2330 train_time:132624ms step_avg:61.31ms
step:2164/2330 train_time:132687ms step_avg:61.32ms
step:2165/2330 train_time:132748ms step_avg:61.32ms
step:2166/2330 train_time:132811ms step_avg:61.32ms
step:2167/2330 train_time:132870ms step_avg:61.32ms
step:2168/2330 train_time:132933ms step_avg:61.32ms
step:2169/2330 train_time:132994ms step_avg:61.32ms
step:2170/2330 train_time:133057ms step_avg:61.32ms
step:2171/2330 train_time:133117ms step_avg:61.32ms
step:2172/2330 train_time:133181ms step_avg:61.32ms
step:2173/2330 train_time:133242ms step_avg:61.32ms
step:2174/2330 train_time:133305ms step_avg:61.32ms
step:2175/2330 train_time:133365ms step_avg:61.32ms
step:2176/2330 train_time:133428ms step_avg:61.32ms
step:2177/2330 train_time:133488ms step_avg:61.32ms
step:2178/2330 train_time:133551ms step_avg:61.32ms
step:2179/2330 train_time:133611ms step_avg:61.32ms
step:2180/2330 train_time:133674ms step_avg:61.32ms
step:2181/2330 train_time:133735ms step_avg:61.32ms
step:2182/2330 train_time:133799ms step_avg:61.32ms
step:2183/2330 train_time:133859ms step_avg:61.32ms
step:2184/2330 train_time:133923ms step_avg:61.32ms
step:2185/2330 train_time:133983ms step_avg:61.32ms
step:2186/2330 train_time:134047ms step_avg:61.32ms
step:2187/2330 train_time:134107ms step_avg:61.32ms
step:2188/2330 train_time:134170ms step_avg:61.32ms
step:2189/2330 train_time:134231ms step_avg:61.32ms
step:2190/2330 train_time:134294ms step_avg:61.32ms
step:2191/2330 train_time:134354ms step_avg:61.32ms
step:2192/2330 train_time:134418ms step_avg:61.32ms
step:2193/2330 train_time:134478ms step_avg:61.32ms
step:2194/2330 train_time:134541ms step_avg:61.32ms
step:2195/2330 train_time:134601ms step_avg:61.32ms
step:2196/2330 train_time:134665ms step_avg:61.32ms
step:2197/2330 train_time:134725ms step_avg:61.32ms
step:2198/2330 train_time:134788ms step_avg:61.32ms
step:2199/2330 train_time:134849ms step_avg:61.32ms
step:2200/2330 train_time:134912ms step_avg:61.32ms
step:2201/2330 train_time:134973ms step_avg:61.32ms
step:2202/2330 train_time:135035ms step_avg:61.32ms
step:2203/2330 train_time:135095ms step_avg:61.32ms
step:2204/2330 train_time:135158ms step_avg:61.32ms
step:2205/2330 train_time:135219ms step_avg:61.32ms
step:2206/2330 train_time:135283ms step_avg:61.32ms
step:2207/2330 train_time:135343ms step_avg:61.32ms
step:2208/2330 train_time:135406ms step_avg:61.32ms
step:2209/2330 train_time:135466ms step_avg:61.32ms
step:2210/2330 train_time:135529ms step_avg:61.33ms
step:2211/2330 train_time:135590ms step_avg:61.33ms
step:2212/2330 train_time:135653ms step_avg:61.33ms
step:2213/2330 train_time:135713ms step_avg:61.33ms
step:2214/2330 train_time:135777ms step_avg:61.33ms
step:2215/2330 train_time:135837ms step_avg:61.33ms
step:2216/2330 train_time:135901ms step_avg:61.33ms
step:2217/2330 train_time:135962ms step_avg:61.33ms
step:2218/2330 train_time:136025ms step_avg:61.33ms
step:2219/2330 train_time:136085ms step_avg:61.33ms
step:2220/2330 train_time:136148ms step_avg:61.33ms
step:2221/2330 train_time:136208ms step_avg:61.33ms
step:2222/2330 train_time:136271ms step_avg:61.33ms
step:2223/2330 train_time:136331ms step_avg:61.33ms
step:2224/2330 train_time:136394ms step_avg:61.33ms
step:2225/2330 train_time:136454ms step_avg:61.33ms
step:2226/2330 train_time:136518ms step_avg:61.33ms
step:2227/2330 train_time:136578ms step_avg:61.33ms
step:2228/2330 train_time:136642ms step_avg:61.33ms
step:2229/2330 train_time:136702ms step_avg:61.33ms
step:2230/2330 train_time:136765ms step_avg:61.33ms
step:2231/2330 train_time:136825ms step_avg:61.33ms
step:2232/2330 train_time:136889ms step_avg:61.33ms
step:2233/2330 train_time:136949ms step_avg:61.33ms
step:2234/2330 train_time:137012ms step_avg:61.33ms
step:2235/2330 train_time:137073ms step_avg:61.33ms
step:2236/2330 train_time:137136ms step_avg:61.33ms
step:2237/2330 train_time:137196ms step_avg:61.33ms
step:2238/2330 train_time:137259ms step_avg:61.33ms
step:2239/2330 train_time:137320ms step_avg:61.33ms
step:2240/2330 train_time:137383ms step_avg:61.33ms
step:2241/2330 train_time:137444ms step_avg:61.33ms
step:2242/2330 train_time:137508ms step_avg:61.33ms
step:2243/2330 train_time:137567ms step_avg:61.33ms
step:2244/2330 train_time:137631ms step_avg:61.33ms
step:2245/2330 train_time:137691ms step_avg:61.33ms
step:2246/2330 train_time:137753ms step_avg:61.33ms
step:2247/2330 train_time:137813ms step_avg:61.33ms
step:2248/2330 train_time:137877ms step_avg:61.33ms
step:2249/2330 train_time:137937ms step_avg:61.33ms
step:2250/2330 train_time:138000ms step_avg:61.33ms
step:2250/2330 val_loss:3.3873 train_time:138066ms step_avg:61.36ms
step:2251/2330 train_time:138088ms step_avg:61.35ms
step:2252/2330 train_time:138128ms step_avg:61.34ms
step:2253/2330 train_time:138194ms step_avg:61.34ms
step:2254/2330 train_time:138259ms step_avg:61.34ms
step:2255/2330 train_time:138320ms step_avg:61.34ms
step:2256/2330 train_time:138384ms step_avg:61.34ms
step:2257/2330 train_time:138444ms step_avg:61.34ms
step:2258/2330 train_time:138507ms step_avg:61.34ms
step:2259/2330 train_time:138567ms step_avg:61.34ms
step:2260/2330 train_time:138629ms step_avg:61.34ms
step:2261/2330 train_time:138688ms step_avg:61.34ms
step:2262/2330 train_time:138751ms step_avg:61.34ms
step:2263/2330 train_time:138811ms step_avg:61.34ms
step:2264/2330 train_time:138873ms step_avg:61.34ms
step:2265/2330 train_time:138932ms step_avg:61.34ms
step:2266/2330 train_time:138995ms step_avg:61.34ms
step:2267/2330 train_time:139056ms step_avg:61.34ms
step:2268/2330 train_time:139120ms step_avg:61.34ms
step:2269/2330 train_time:139182ms step_avg:61.34ms
step:2270/2330 train_time:139246ms step_avg:61.34ms
step:2271/2330 train_time:139307ms step_avg:61.34ms
step:2272/2330 train_time:139371ms step_avg:61.34ms
step:2273/2330 train_time:139432ms step_avg:61.34ms
step:2274/2330 train_time:139495ms step_avg:61.34ms
step:2275/2330 train_time:139555ms step_avg:61.34ms
step:2276/2330 train_time:139618ms step_avg:61.34ms
step:2277/2330 train_time:139678ms step_avg:61.34ms
step:2278/2330 train_time:139741ms step_avg:61.34ms
step:2279/2330 train_time:139802ms step_avg:61.34ms
step:2280/2330 train_time:139865ms step_avg:61.34ms
step:2281/2330 train_time:139925ms step_avg:61.34ms
step:2282/2330 train_time:139987ms step_avg:61.34ms
step:2283/2330 train_time:140048ms step_avg:61.34ms
step:2284/2330 train_time:140112ms step_avg:61.35ms
step:2285/2330 train_time:140174ms step_avg:61.35ms
step:2286/2330 train_time:140237ms step_avg:61.35ms
step:2287/2330 train_time:140299ms step_avg:61.35ms
step:2288/2330 train_time:140362ms step_avg:61.35ms
step:2289/2330 train_time:140423ms step_avg:61.35ms
step:2290/2330 train_time:140487ms step_avg:61.35ms
step:2291/2330 train_time:140547ms step_avg:61.35ms
step:2292/2330 train_time:140610ms step_avg:61.35ms
step:2293/2330 train_time:140670ms step_avg:61.35ms
step:2294/2330 train_time:140734ms step_avg:61.35ms
step:2295/2330 train_time:140793ms step_avg:61.35ms
step:2296/2330 train_time:140856ms step_avg:61.35ms
step:2297/2330 train_time:140916ms step_avg:61.35ms
step:2298/2330 train_time:140980ms step_avg:61.35ms
step:2299/2330 train_time:141041ms step_avg:61.35ms
step:2300/2330 train_time:141105ms step_avg:61.35ms
step:2301/2330 train_time:141165ms step_avg:61.35ms
step:2302/2330 train_time:141228ms step_avg:61.35ms
step:2303/2330 train_time:141289ms step_avg:61.35ms
step:2304/2330 train_time:141353ms step_avg:61.35ms
step:2305/2330 train_time:141413ms step_avg:61.35ms
step:2306/2330 train_time:141477ms step_avg:61.35ms
step:2307/2330 train_time:141537ms step_avg:61.35ms
step:2308/2330 train_time:141601ms step_avg:61.35ms
step:2309/2330 train_time:141661ms step_avg:61.35ms
step:2310/2330 train_time:141724ms step_avg:61.35ms
step:2311/2330 train_time:141784ms step_avg:61.35ms
step:2312/2330 train_time:141847ms step_avg:61.35ms
step:2313/2330 train_time:141908ms step_avg:61.35ms
step:2314/2330 train_time:141971ms step_avg:61.35ms
step:2315/2330 train_time:142031ms step_avg:61.35ms
step:2316/2330 train_time:142094ms step_avg:61.35ms
step:2317/2330 train_time:142155ms step_avg:61.35ms
step:2318/2330 train_time:142219ms step_avg:61.35ms
step:2319/2330 train_time:142280ms step_avg:61.35ms
step:2320/2330 train_time:142344ms step_avg:61.36ms
step:2321/2330 train_time:142404ms step_avg:61.35ms
step:2322/2330 train_time:142467ms step_avg:61.36ms
step:2323/2330 train_time:142527ms step_avg:61.35ms
step:2324/2330 train_time:142591ms step_avg:61.36ms
step:2325/2330 train_time:142652ms step_avg:61.36ms
step:2326/2330 train_time:142715ms step_avg:61.36ms
step:2327/2330 train_time:142774ms step_avg:61.36ms
step:2328/2330 train_time:142838ms step_avg:61.36ms
step:2329/2330 train_time:142898ms step_avg:61.36ms
step:2330/2330 train_time:142962ms step_avg:61.36ms
step:2330/2330 val_loss:3.3579 train_time:143027ms step_avg:61.38ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
