import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-3"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:19:41 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:84ms step_avg:84.16ms
step:2/2330 train_time:178ms step_avg:88.84ms
step:3/2330 train_time:199ms step_avg:66.41ms
step:4/2330 train_time:235ms step_avg:58.72ms
step:5/2330 train_time:292ms step_avg:58.42ms
step:6/2330 train_time:353ms step_avg:58.88ms
step:7/2330 train_time:412ms step_avg:58.83ms
step:8/2330 train_time:473ms step_avg:59.16ms
step:9/2330 train_time:531ms step_avg:59.05ms
step:10/2330 train_time:593ms step_avg:59.34ms
step:11/2330 train_time:652ms step_avg:59.25ms
step:12/2330 train_time:713ms step_avg:59.45ms
step:13/2330 train_time:773ms step_avg:59.44ms
step:14/2330 train_time:834ms step_avg:59.60ms
step:15/2330 train_time:893ms step_avg:59.54ms
step:16/2330 train_time:955ms step_avg:59.71ms
step:17/2330 train_time:1018ms step_avg:59.86ms
step:18/2330 train_time:1082ms step_avg:60.13ms
step:19/2330 train_time:1145ms step_avg:60.24ms
step:20/2330 train_time:1208ms step_avg:60.39ms
step:21/2330 train_time:1268ms step_avg:60.36ms
step:22/2330 train_time:1330ms step_avg:60.47ms
step:23/2330 train_time:1390ms step_avg:60.43ms
step:24/2330 train_time:1452ms step_avg:60.48ms
step:25/2330 train_time:1510ms step_avg:60.41ms
step:26/2330 train_time:1572ms step_avg:60.47ms
step:27/2330 train_time:1631ms step_avg:60.40ms
step:28/2330 train_time:1693ms step_avg:60.48ms
step:29/2330 train_time:1753ms step_avg:60.45ms
step:30/2330 train_time:1815ms step_avg:60.51ms
step:31/2330 train_time:1874ms step_avg:60.44ms
step:32/2330 train_time:1937ms step_avg:60.53ms
step:33/2330 train_time:1996ms step_avg:60.49ms
step:34/2330 train_time:2059ms step_avg:60.57ms
step:35/2330 train_time:2119ms step_avg:60.56ms
step:36/2330 train_time:2182ms step_avg:60.62ms
step:37/2330 train_time:2243ms step_avg:60.61ms
step:38/2330 train_time:2305ms step_avg:60.67ms
step:39/2330 train_time:2365ms step_avg:60.64ms
step:40/2330 train_time:2427ms step_avg:60.67ms
step:41/2330 train_time:2487ms step_avg:60.65ms
step:42/2330 train_time:2548ms step_avg:60.68ms
step:43/2330 train_time:2608ms step_avg:60.65ms
step:44/2330 train_time:2670ms step_avg:60.68ms
step:45/2330 train_time:2730ms step_avg:60.66ms
step:46/2330 train_time:2792ms step_avg:60.70ms
step:47/2330 train_time:2852ms step_avg:60.68ms
step:48/2330 train_time:2914ms step_avg:60.72ms
step:49/2330 train_time:2974ms step_avg:60.70ms
step:50/2330 train_time:3037ms step_avg:60.75ms
step:51/2330 train_time:3098ms step_avg:60.74ms
step:52/2330 train_time:3161ms step_avg:60.78ms
step:53/2330 train_time:3220ms step_avg:60.76ms
step:54/2330 train_time:3283ms step_avg:60.80ms
step:55/2330 train_time:3343ms step_avg:60.78ms
step:56/2330 train_time:3405ms step_avg:60.80ms
step:57/2330 train_time:3465ms step_avg:60.79ms
step:58/2330 train_time:3527ms step_avg:60.81ms
step:59/2330 train_time:3586ms step_avg:60.78ms
step:60/2330 train_time:3648ms step_avg:60.80ms
step:61/2330 train_time:3707ms step_avg:60.77ms
step:62/2330 train_time:3769ms step_avg:60.79ms
step:63/2330 train_time:3828ms step_avg:60.77ms
step:64/2330 train_time:3891ms step_avg:60.80ms
step:65/2330 train_time:3951ms step_avg:60.79ms
step:66/2330 train_time:4015ms step_avg:60.83ms
step:67/2330 train_time:4075ms step_avg:60.82ms
step:68/2330 train_time:4138ms step_avg:60.86ms
step:69/2330 train_time:4198ms step_avg:60.84ms
step:70/2330 train_time:4260ms step_avg:60.86ms
step:71/2330 train_time:4319ms step_avg:60.84ms
step:72/2330 train_time:4381ms step_avg:60.85ms
step:73/2330 train_time:4442ms step_avg:60.84ms
step:74/2330 train_time:4504ms step_avg:60.87ms
step:75/2330 train_time:4564ms step_avg:60.85ms
step:76/2330 train_time:4625ms step_avg:60.86ms
step:77/2330 train_time:4685ms step_avg:60.84ms
step:78/2330 train_time:4747ms step_avg:60.86ms
step:79/2330 train_time:4806ms step_avg:60.84ms
step:80/2330 train_time:4869ms step_avg:60.86ms
step:81/2330 train_time:4929ms step_avg:60.85ms
step:82/2330 train_time:4993ms step_avg:60.88ms
step:83/2330 train_time:5053ms step_avg:60.88ms
step:84/2330 train_time:5117ms step_avg:60.91ms
step:85/2330 train_time:5176ms step_avg:60.89ms
step:86/2330 train_time:5240ms step_avg:60.93ms
step:87/2330 train_time:5299ms step_avg:60.91ms
step:88/2330 train_time:5362ms step_avg:60.93ms
step:89/2330 train_time:5421ms step_avg:60.91ms
step:90/2330 train_time:5483ms step_avg:60.92ms
step:91/2330 train_time:5543ms step_avg:60.91ms
step:92/2330 train_time:5604ms step_avg:60.92ms
step:93/2330 train_time:5664ms step_avg:60.90ms
step:94/2330 train_time:5726ms step_avg:60.91ms
step:95/2330 train_time:5785ms step_avg:60.89ms
step:96/2330 train_time:5847ms step_avg:60.90ms
step:97/2330 train_time:5907ms step_avg:60.90ms
step:98/2330 train_time:5969ms step_avg:60.91ms
step:99/2330 train_time:6029ms step_avg:60.90ms
step:100/2330 train_time:6093ms step_avg:60.93ms
step:101/2330 train_time:6153ms step_avg:60.92ms
step:102/2330 train_time:6216ms step_avg:60.94ms
step:103/2330 train_time:6276ms step_avg:60.93ms
step:104/2330 train_time:6338ms step_avg:60.95ms
step:105/2330 train_time:6398ms step_avg:60.93ms
step:106/2330 train_time:6460ms step_avg:60.94ms
step:107/2330 train_time:6519ms step_avg:60.92ms
step:108/2330 train_time:6582ms step_avg:60.94ms
step:109/2330 train_time:6641ms step_avg:60.93ms
step:110/2330 train_time:6703ms step_avg:60.94ms
step:111/2330 train_time:6763ms step_avg:60.93ms
step:112/2330 train_time:6825ms step_avg:60.94ms
step:113/2330 train_time:6885ms step_avg:60.93ms
step:114/2330 train_time:6947ms step_avg:60.94ms
step:115/2330 train_time:7007ms step_avg:60.93ms
step:116/2330 train_time:7069ms step_avg:60.94ms
step:117/2330 train_time:7129ms step_avg:60.93ms
step:118/2330 train_time:7192ms step_avg:60.95ms
step:119/2330 train_time:7253ms step_avg:60.95ms
step:120/2330 train_time:7316ms step_avg:60.96ms
step:121/2330 train_time:7375ms step_avg:60.95ms
step:122/2330 train_time:7438ms step_avg:60.97ms
step:123/2330 train_time:7498ms step_avg:60.96ms
step:124/2330 train_time:7561ms step_avg:60.97ms
step:125/2330 train_time:7620ms step_avg:60.96ms
step:126/2330 train_time:7683ms step_avg:60.98ms
step:127/2330 train_time:7743ms step_avg:60.97ms
step:128/2330 train_time:7805ms step_avg:60.98ms
step:129/2330 train_time:7865ms step_avg:60.97ms
step:130/2330 train_time:7927ms step_avg:60.97ms
step:131/2330 train_time:7987ms step_avg:60.97ms
step:132/2330 train_time:8050ms step_avg:60.98ms
step:133/2330 train_time:8109ms step_avg:60.97ms
step:134/2330 train_time:8171ms step_avg:60.98ms
step:135/2330 train_time:8232ms step_avg:60.97ms
step:136/2330 train_time:8296ms step_avg:61.00ms
step:137/2330 train_time:8355ms step_avg:60.99ms
step:138/2330 train_time:8417ms step_avg:61.00ms
step:139/2330 train_time:8477ms step_avg:60.99ms
step:140/2330 train_time:8540ms step_avg:61.00ms
step:141/2330 train_time:8599ms step_avg:60.99ms
step:142/2330 train_time:8662ms step_avg:61.00ms
step:143/2330 train_time:8722ms step_avg:60.99ms
step:144/2330 train_time:8785ms step_avg:61.00ms
step:145/2330 train_time:8843ms step_avg:60.99ms
step:146/2330 train_time:8906ms step_avg:61.00ms
step:147/2330 train_time:8965ms step_avg:60.99ms
step:148/2330 train_time:9027ms step_avg:61.00ms
step:149/2330 train_time:9087ms step_avg:60.99ms
step:150/2330 train_time:9150ms step_avg:61.00ms
step:151/2330 train_time:9210ms step_avg:60.99ms
step:152/2330 train_time:9273ms step_avg:61.01ms
step:153/2330 train_time:9333ms step_avg:61.00ms
step:154/2330 train_time:9396ms step_avg:61.02ms
step:155/2330 train_time:9456ms step_avg:61.01ms
step:156/2330 train_time:9519ms step_avg:61.02ms
step:157/2330 train_time:9579ms step_avg:61.01ms
step:158/2330 train_time:9642ms step_avg:61.02ms
step:159/2330 train_time:9701ms step_avg:61.01ms
step:160/2330 train_time:9763ms step_avg:61.02ms
step:161/2330 train_time:9822ms step_avg:61.01ms
step:162/2330 train_time:9884ms step_avg:61.02ms
step:163/2330 train_time:9945ms step_avg:61.01ms
step:164/2330 train_time:10007ms step_avg:61.02ms
step:165/2330 train_time:10066ms step_avg:61.01ms
step:166/2330 train_time:10128ms step_avg:61.01ms
step:167/2330 train_time:10188ms step_avg:61.00ms
step:168/2330 train_time:10250ms step_avg:61.01ms
step:169/2330 train_time:10309ms step_avg:61.00ms
step:170/2330 train_time:10372ms step_avg:61.01ms
step:171/2330 train_time:10432ms step_avg:61.01ms
step:172/2330 train_time:10495ms step_avg:61.02ms
step:173/2330 train_time:10556ms step_avg:61.02ms
step:174/2330 train_time:10618ms step_avg:61.02ms
step:175/2330 train_time:10679ms step_avg:61.02ms
step:176/2330 train_time:10741ms step_avg:61.03ms
step:177/2330 train_time:10801ms step_avg:61.02ms
step:178/2330 train_time:10863ms step_avg:61.03ms
step:179/2330 train_time:10923ms step_avg:61.02ms
step:180/2330 train_time:10985ms step_avg:61.03ms
step:181/2330 train_time:11044ms step_avg:61.02ms
step:182/2330 train_time:11107ms step_avg:61.03ms
step:183/2330 train_time:11166ms step_avg:61.01ms
step:184/2330 train_time:11228ms step_avg:61.02ms
step:185/2330 train_time:11288ms step_avg:61.01ms
step:186/2330 train_time:11351ms step_avg:61.03ms
step:187/2330 train_time:11411ms step_avg:61.02ms
step:188/2330 train_time:11474ms step_avg:61.03ms
step:189/2330 train_time:11535ms step_avg:61.03ms
step:190/2330 train_time:11598ms step_avg:61.04ms
step:191/2330 train_time:11657ms step_avg:61.03ms
step:192/2330 train_time:11720ms step_avg:61.04ms
step:193/2330 train_time:11779ms step_avg:61.03ms
step:194/2330 train_time:11843ms step_avg:61.05ms
step:195/2330 train_time:11902ms step_avg:61.04ms
step:196/2330 train_time:11965ms step_avg:61.04ms
step:197/2330 train_time:12023ms step_avg:61.03ms
step:198/2330 train_time:12086ms step_avg:61.04ms
step:199/2330 train_time:12146ms step_avg:61.03ms
step:200/2330 train_time:12208ms step_avg:61.04ms
step:201/2330 train_time:12267ms step_avg:61.03ms
step:202/2330 train_time:12330ms step_avg:61.04ms
step:203/2330 train_time:12390ms step_avg:61.03ms
step:204/2330 train_time:12453ms step_avg:61.04ms
step:205/2330 train_time:12513ms step_avg:61.04ms
step:206/2330 train_time:12577ms step_avg:61.05ms
step:207/2330 train_time:12636ms step_avg:61.05ms
step:208/2330 train_time:12700ms step_avg:61.06ms
step:209/2330 train_time:12759ms step_avg:61.05ms
step:210/2330 train_time:12821ms step_avg:61.05ms
step:211/2330 train_time:12881ms step_avg:61.05ms
step:212/2330 train_time:12944ms step_avg:61.06ms
step:213/2330 train_time:13002ms step_avg:61.04ms
step:214/2330 train_time:13065ms step_avg:61.05ms
step:215/2330 train_time:13124ms step_avg:61.04ms
step:216/2330 train_time:13186ms step_avg:61.05ms
step:217/2330 train_time:13245ms step_avg:61.04ms
step:218/2330 train_time:13307ms step_avg:61.04ms
step:219/2330 train_time:13367ms step_avg:61.03ms
step:220/2330 train_time:13429ms step_avg:61.04ms
step:221/2330 train_time:13490ms step_avg:61.04ms
step:222/2330 train_time:13554ms step_avg:61.05ms
step:223/2330 train_time:13614ms step_avg:61.05ms
step:224/2330 train_time:13677ms step_avg:61.06ms
step:225/2330 train_time:13737ms step_avg:61.05ms
step:226/2330 train_time:13799ms step_avg:61.06ms
step:227/2330 train_time:13859ms step_avg:61.05ms
step:228/2330 train_time:13922ms step_avg:61.06ms
step:229/2330 train_time:13982ms step_avg:61.06ms
step:230/2330 train_time:14044ms step_avg:61.06ms
step:231/2330 train_time:14104ms step_avg:61.05ms
step:232/2330 train_time:14166ms step_avg:61.06ms
step:233/2330 train_time:14225ms step_avg:61.05ms
step:234/2330 train_time:14286ms step_avg:61.05ms
step:235/2330 train_time:14346ms step_avg:61.05ms
step:236/2330 train_time:14408ms step_avg:61.05ms
step:237/2330 train_time:14469ms step_avg:61.05ms
step:238/2330 train_time:14532ms step_avg:61.06ms
step:239/2330 train_time:14594ms step_avg:61.06ms
step:240/2330 train_time:14657ms step_avg:61.07ms
step:241/2330 train_time:14717ms step_avg:61.06ms
step:242/2330 train_time:14779ms step_avg:61.07ms
step:243/2330 train_time:14838ms step_avg:61.06ms
step:244/2330 train_time:14901ms step_avg:61.07ms
step:245/2330 train_time:14961ms step_avg:61.06ms
step:246/2330 train_time:15023ms step_avg:61.07ms
step:247/2330 train_time:15083ms step_avg:61.06ms
step:248/2330 train_time:15145ms step_avg:61.07ms
step:249/2330 train_time:15205ms step_avg:61.06ms
step:250/2330 train_time:15267ms step_avg:61.07ms
step:250/2330 val_loss:4.1767 train_time:15331ms step_avg:61.32ms
step:251/2330 train_time:15354ms step_avg:61.17ms
step:252/2330 train_time:15390ms step_avg:61.07ms
step:253/2330 train_time:15454ms step_avg:61.08ms
step:254/2330 train_time:15520ms step_avg:61.10ms
step:255/2330 train_time:15580ms step_avg:61.10ms
step:256/2330 train_time:15642ms step_avg:61.10ms
step:257/2330 train_time:15703ms step_avg:61.10ms
step:258/2330 train_time:15765ms step_avg:61.10ms
step:259/2330 train_time:15823ms step_avg:61.09ms
step:260/2330 train_time:15885ms step_avg:61.10ms
step:261/2330 train_time:15944ms step_avg:61.09ms
step:262/2330 train_time:16006ms step_avg:61.09ms
step:263/2330 train_time:16064ms step_avg:61.08ms
step:264/2330 train_time:16126ms step_avg:61.08ms
step:265/2330 train_time:16184ms step_avg:61.07ms
step:266/2330 train_time:16246ms step_avg:61.08ms
step:267/2330 train_time:16306ms step_avg:61.07ms
step:268/2330 train_time:16369ms step_avg:61.08ms
step:269/2330 train_time:16429ms step_avg:61.08ms
step:270/2330 train_time:16494ms step_avg:61.09ms
step:271/2330 train_time:16556ms step_avg:61.09ms
step:272/2330 train_time:16619ms step_avg:61.10ms
step:273/2330 train_time:16678ms step_avg:61.09ms
step:274/2330 train_time:16740ms step_avg:61.10ms
step:275/2330 train_time:16799ms step_avg:61.09ms
step:276/2330 train_time:16862ms step_avg:61.09ms
step:277/2330 train_time:16921ms step_avg:61.09ms
step:278/2330 train_time:16983ms step_avg:61.09ms
step:279/2330 train_time:17042ms step_avg:61.08ms
step:280/2330 train_time:17104ms step_avg:61.09ms
step:281/2330 train_time:17163ms step_avg:61.08ms
step:282/2330 train_time:17225ms step_avg:61.08ms
step:283/2330 train_time:17284ms step_avg:61.07ms
step:284/2330 train_time:17347ms step_avg:61.08ms
step:285/2330 train_time:17407ms step_avg:61.08ms
step:286/2330 train_time:17471ms step_avg:61.09ms
step:287/2330 train_time:17532ms step_avg:61.09ms
step:288/2330 train_time:17596ms step_avg:61.10ms
step:289/2330 train_time:17656ms step_avg:61.09ms
step:290/2330 train_time:17718ms step_avg:61.10ms
step:291/2330 train_time:17777ms step_avg:61.09ms
step:292/2330 train_time:17839ms step_avg:61.09ms
step:293/2330 train_time:17899ms step_avg:61.09ms
step:294/2330 train_time:17962ms step_avg:61.10ms
step:295/2330 train_time:18021ms step_avg:61.09ms
step:296/2330 train_time:18083ms step_avg:61.09ms
step:297/2330 train_time:18142ms step_avg:61.08ms
step:298/2330 train_time:18205ms step_avg:61.09ms
step:299/2330 train_time:18264ms step_avg:61.08ms
step:300/2330 train_time:18326ms step_avg:61.09ms
step:301/2330 train_time:18386ms step_avg:61.08ms
step:302/2330 train_time:18449ms step_avg:61.09ms
step:303/2330 train_time:18509ms step_avg:61.09ms
step:304/2330 train_time:18573ms step_avg:61.10ms
step:305/2330 train_time:18633ms step_avg:61.09ms
step:306/2330 train_time:18697ms step_avg:61.10ms
step:307/2330 train_time:18756ms step_avg:61.10ms
step:308/2330 train_time:18819ms step_avg:61.10ms
step:309/2330 train_time:18879ms step_avg:61.10ms
step:310/2330 train_time:18941ms step_avg:61.10ms
step:311/2330 train_time:19001ms step_avg:61.10ms
step:312/2330 train_time:19064ms step_avg:61.10ms
step:313/2330 train_time:19123ms step_avg:61.10ms
step:314/2330 train_time:19185ms step_avg:61.10ms
step:315/2330 train_time:19244ms step_avg:61.09ms
step:316/2330 train_time:19306ms step_avg:61.10ms
step:317/2330 train_time:19366ms step_avg:61.09ms
step:318/2330 train_time:19428ms step_avg:61.09ms
step:319/2330 train_time:19488ms step_avg:61.09ms
step:320/2330 train_time:19551ms step_avg:61.10ms
step:321/2330 train_time:19610ms step_avg:61.09ms
step:322/2330 train_time:19674ms step_avg:61.10ms
step:323/2330 train_time:19734ms step_avg:61.09ms
step:324/2330 train_time:19798ms step_avg:61.10ms
step:325/2330 train_time:19858ms step_avg:61.10ms
step:326/2330 train_time:19921ms step_avg:61.11ms
step:327/2330 train_time:19980ms step_avg:61.10ms
step:328/2330 train_time:20043ms step_avg:61.11ms
step:329/2330 train_time:20102ms step_avg:61.10ms
step:330/2330 train_time:20164ms step_avg:61.10ms
step:331/2330 train_time:20223ms step_avg:61.10ms
step:332/2330 train_time:20286ms step_avg:61.10ms
step:333/2330 train_time:20346ms step_avg:61.10ms
step:334/2330 train_time:20408ms step_avg:61.10ms
step:335/2330 train_time:20467ms step_avg:61.10ms
step:336/2330 train_time:20529ms step_avg:61.10ms
step:337/2330 train_time:20589ms step_avg:61.10ms
step:338/2330 train_time:20653ms step_avg:61.10ms
step:339/2330 train_time:20713ms step_avg:61.10ms
step:340/2330 train_time:20776ms step_avg:61.11ms
step:341/2330 train_time:20836ms step_avg:61.10ms
step:342/2330 train_time:20898ms step_avg:61.11ms
step:343/2330 train_time:20958ms step_avg:61.10ms
step:344/2330 train_time:21020ms step_avg:61.11ms
step:345/2330 train_time:21080ms step_avg:61.10ms
step:346/2330 train_time:21142ms step_avg:61.11ms
step:347/2330 train_time:21201ms step_avg:61.10ms
step:348/2330 train_time:21264ms step_avg:61.10ms
step:349/2330 train_time:21323ms step_avg:61.10ms
step:350/2330 train_time:21386ms step_avg:61.10ms
step:351/2330 train_time:21445ms step_avg:61.10ms
step:352/2330 train_time:21507ms step_avg:61.10ms
step:353/2330 train_time:21566ms step_avg:61.09ms
step:354/2330 train_time:21629ms step_avg:61.10ms
step:355/2330 train_time:21689ms step_avg:61.10ms
step:356/2330 train_time:21752ms step_avg:61.10ms
step:357/2330 train_time:21812ms step_avg:61.10ms
step:358/2330 train_time:21875ms step_avg:61.10ms
step:359/2330 train_time:21935ms step_avg:61.10ms
step:360/2330 train_time:21999ms step_avg:61.11ms
step:361/2330 train_time:22058ms step_avg:61.10ms
step:362/2330 train_time:22121ms step_avg:61.11ms
step:363/2330 train_time:22180ms step_avg:61.10ms
step:364/2330 train_time:22242ms step_avg:61.11ms
step:365/2330 train_time:22303ms step_avg:61.10ms
step:366/2330 train_time:22365ms step_avg:61.11ms
step:367/2330 train_time:22424ms step_avg:61.10ms
step:368/2330 train_time:22486ms step_avg:61.10ms
step:369/2330 train_time:22545ms step_avg:61.10ms
step:370/2330 train_time:22608ms step_avg:61.10ms
step:371/2330 train_time:22668ms step_avg:61.10ms
step:372/2330 train_time:22730ms step_avg:61.10ms
step:373/2330 train_time:22790ms step_avg:61.10ms
step:374/2330 train_time:22854ms step_avg:61.11ms
step:375/2330 train_time:22914ms step_avg:61.10ms
step:376/2330 train_time:22977ms step_avg:61.11ms
step:377/2330 train_time:23037ms step_avg:61.11ms
step:378/2330 train_time:23100ms step_avg:61.11ms
step:379/2330 train_time:23159ms step_avg:61.11ms
step:380/2330 train_time:23222ms step_avg:61.11ms
step:381/2330 train_time:23281ms step_avg:61.11ms
step:382/2330 train_time:23344ms step_avg:61.11ms
step:383/2330 train_time:23403ms step_avg:61.11ms
step:384/2330 train_time:23465ms step_avg:61.11ms
step:385/2330 train_time:23526ms step_avg:61.11ms
step:386/2330 train_time:23588ms step_avg:61.11ms
step:387/2330 train_time:23648ms step_avg:61.11ms
step:388/2330 train_time:23710ms step_avg:61.11ms
step:389/2330 train_time:23769ms step_avg:61.10ms
step:390/2330 train_time:23833ms step_avg:61.11ms
step:391/2330 train_time:23893ms step_avg:61.11ms
step:392/2330 train_time:23956ms step_avg:61.11ms
step:393/2330 train_time:24017ms step_avg:61.11ms
step:394/2330 train_time:24080ms step_avg:61.12ms
step:395/2330 train_time:24140ms step_avg:61.11ms
step:396/2330 train_time:24202ms step_avg:61.12ms
step:397/2330 train_time:24262ms step_avg:61.11ms
step:398/2330 train_time:24324ms step_avg:61.11ms
step:399/2330 train_time:24383ms step_avg:61.11ms
step:400/2330 train_time:24445ms step_avg:61.11ms
step:401/2330 train_time:24505ms step_avg:61.11ms
step:402/2330 train_time:24567ms step_avg:61.11ms
step:403/2330 train_time:24626ms step_avg:61.11ms
step:404/2330 train_time:24688ms step_avg:61.11ms
step:405/2330 train_time:24747ms step_avg:61.10ms
step:406/2330 train_time:24810ms step_avg:61.11ms
step:407/2330 train_time:24870ms step_avg:61.10ms
step:408/2330 train_time:24933ms step_avg:61.11ms
step:409/2330 train_time:24994ms step_avg:61.11ms
step:410/2330 train_time:25057ms step_avg:61.12ms
step:411/2330 train_time:25118ms step_avg:61.11ms
step:412/2330 train_time:25180ms step_avg:61.12ms
step:413/2330 train_time:25240ms step_avg:61.11ms
step:414/2330 train_time:25303ms step_avg:61.12ms
step:415/2330 train_time:25362ms step_avg:61.11ms
step:416/2330 train_time:25424ms step_avg:61.12ms
step:417/2330 train_time:25484ms step_avg:61.11ms
step:418/2330 train_time:25546ms step_avg:61.11ms
step:419/2330 train_time:25605ms step_avg:61.11ms
step:420/2330 train_time:25668ms step_avg:61.11ms
step:421/2330 train_time:25727ms step_avg:61.11ms
step:422/2330 train_time:25790ms step_avg:61.11ms
step:423/2330 train_time:25850ms step_avg:61.11ms
step:424/2330 train_time:25913ms step_avg:61.12ms
step:425/2330 train_time:25973ms step_avg:61.11ms
step:426/2330 train_time:26037ms step_avg:61.12ms
step:427/2330 train_time:26097ms step_avg:61.12ms
step:428/2330 train_time:26159ms step_avg:61.12ms
step:429/2330 train_time:26218ms step_avg:61.12ms
step:430/2330 train_time:26281ms step_avg:61.12ms
step:431/2330 train_time:26340ms step_avg:61.11ms
step:432/2330 train_time:26402ms step_avg:61.12ms
step:433/2330 train_time:26462ms step_avg:61.11ms
step:434/2330 train_time:26525ms step_avg:61.12ms
step:435/2330 train_time:26585ms step_avg:61.11ms
step:436/2330 train_time:26646ms step_avg:61.12ms
step:437/2330 train_time:26706ms step_avg:61.11ms
step:438/2330 train_time:26768ms step_avg:61.11ms
step:439/2330 train_time:26827ms step_avg:61.11ms
step:440/2330 train_time:26889ms step_avg:61.11ms
step:441/2330 train_time:26949ms step_avg:61.11ms
step:442/2330 train_time:27012ms step_avg:61.11ms
step:443/2330 train_time:27073ms step_avg:61.11ms
step:444/2330 train_time:27136ms step_avg:61.12ms
step:445/2330 train_time:27197ms step_avg:61.12ms
step:446/2330 train_time:27259ms step_avg:61.12ms
step:447/2330 train_time:27318ms step_avg:61.11ms
step:448/2330 train_time:27380ms step_avg:61.12ms
step:449/2330 train_time:27440ms step_avg:61.11ms
step:450/2330 train_time:27503ms step_avg:61.12ms
step:451/2330 train_time:27563ms step_avg:61.11ms
step:452/2330 train_time:27625ms step_avg:61.12ms
step:453/2330 train_time:27684ms step_avg:61.11ms
step:454/2330 train_time:27746ms step_avg:61.11ms
step:455/2330 train_time:27806ms step_avg:61.11ms
step:456/2330 train_time:27868ms step_avg:61.11ms
step:457/2330 train_time:27927ms step_avg:61.11ms
step:458/2330 train_time:27990ms step_avg:61.11ms
step:459/2330 train_time:28050ms step_avg:61.11ms
step:460/2330 train_time:28114ms step_avg:61.12ms
step:461/2330 train_time:28173ms step_avg:61.11ms
step:462/2330 train_time:28237ms step_avg:61.12ms
step:463/2330 train_time:28297ms step_avg:61.12ms
step:464/2330 train_time:28359ms step_avg:61.12ms
step:465/2330 train_time:28419ms step_avg:61.12ms
step:466/2330 train_time:28482ms step_avg:61.12ms
step:467/2330 train_time:28542ms step_avg:61.12ms
step:468/2330 train_time:28605ms step_avg:61.12ms
step:469/2330 train_time:28664ms step_avg:61.12ms
step:470/2330 train_time:28726ms step_avg:61.12ms
step:471/2330 train_time:28785ms step_avg:61.12ms
step:472/2330 train_time:28848ms step_avg:61.12ms
step:473/2330 train_time:28907ms step_avg:61.11ms
step:474/2330 train_time:28970ms step_avg:61.12ms
step:475/2330 train_time:29029ms step_avg:61.11ms
step:476/2330 train_time:29092ms step_avg:61.12ms
step:477/2330 train_time:29152ms step_avg:61.12ms
step:478/2330 train_time:29215ms step_avg:61.12ms
step:479/2330 train_time:29275ms step_avg:61.12ms
step:480/2330 train_time:29338ms step_avg:61.12ms
step:481/2330 train_time:29398ms step_avg:61.12ms
step:482/2330 train_time:29461ms step_avg:61.12ms
step:483/2330 train_time:29520ms step_avg:61.12ms
step:484/2330 train_time:29583ms step_avg:61.12ms
step:485/2330 train_time:29643ms step_avg:61.12ms
step:486/2330 train_time:29705ms step_avg:61.12ms
step:487/2330 train_time:29765ms step_avg:61.12ms
step:488/2330 train_time:29827ms step_avg:61.12ms
step:489/2330 train_time:29886ms step_avg:61.12ms
step:490/2330 train_time:29949ms step_avg:61.12ms
step:491/2330 train_time:30009ms step_avg:61.12ms
step:492/2330 train_time:30071ms step_avg:61.12ms
step:493/2330 train_time:30131ms step_avg:61.12ms
step:494/2330 train_time:30194ms step_avg:61.12ms
step:495/2330 train_time:30255ms step_avg:61.12ms
step:496/2330 train_time:30318ms step_avg:61.12ms
step:497/2330 train_time:30377ms step_avg:61.12ms
step:498/2330 train_time:30440ms step_avg:61.12ms
step:499/2330 train_time:30499ms step_avg:61.12ms
step:500/2330 train_time:30561ms step_avg:61.12ms
step:500/2330 val_loss:3.8542 train_time:30625ms step_avg:61.25ms
step:501/2330 train_time:30647ms step_avg:61.17ms
step:502/2330 train_time:30685ms step_avg:61.13ms
step:503/2330 train_time:30748ms step_avg:61.13ms
step:504/2330 train_time:30814ms step_avg:61.14ms
step:505/2330 train_time:30874ms step_avg:61.14ms
step:506/2330 train_time:30937ms step_avg:61.14ms
step:507/2330 train_time:30997ms step_avg:61.14ms
step:508/2330 train_time:31059ms step_avg:61.14ms
step:509/2330 train_time:31118ms step_avg:61.14ms
step:510/2330 train_time:31180ms step_avg:61.14ms
step:511/2330 train_time:31239ms step_avg:61.13ms
step:512/2330 train_time:31301ms step_avg:61.13ms
step:513/2330 train_time:31360ms step_avg:61.13ms
step:514/2330 train_time:31422ms step_avg:61.13ms
step:515/2330 train_time:31480ms step_avg:61.13ms
step:516/2330 train_time:31544ms step_avg:61.13ms
step:517/2330 train_time:31603ms step_avg:61.13ms
step:518/2330 train_time:31666ms step_avg:61.13ms
step:519/2330 train_time:31727ms step_avg:61.13ms
step:520/2330 train_time:31791ms step_avg:61.14ms
step:521/2330 train_time:31851ms step_avg:61.13ms
step:522/2330 train_time:31914ms step_avg:61.14ms
step:523/2330 train_time:31974ms step_avg:61.14ms
step:524/2330 train_time:32036ms step_avg:61.14ms
step:525/2330 train_time:32096ms step_avg:61.14ms
step:526/2330 train_time:32158ms step_avg:61.14ms
step:527/2330 train_time:32218ms step_avg:61.13ms
step:528/2330 train_time:32279ms step_avg:61.14ms
step:529/2330 train_time:32339ms step_avg:61.13ms
step:530/2330 train_time:32401ms step_avg:61.13ms
step:531/2330 train_time:32460ms step_avg:61.13ms
step:532/2330 train_time:32523ms step_avg:61.13ms
step:533/2330 train_time:32582ms step_avg:61.13ms
step:534/2330 train_time:32646ms step_avg:61.13ms
step:535/2330 train_time:32706ms step_avg:61.13ms
step:536/2330 train_time:32768ms step_avg:61.13ms
step:537/2330 train_time:32828ms step_avg:61.13ms
step:538/2330 train_time:32891ms step_avg:61.14ms
step:539/2330 train_time:32951ms step_avg:61.13ms
step:540/2330 train_time:33014ms step_avg:61.14ms
step:541/2330 train_time:33074ms step_avg:61.13ms
step:542/2330 train_time:33137ms step_avg:61.14ms
step:543/2330 train_time:33197ms step_avg:61.14ms
step:544/2330 train_time:33259ms step_avg:61.14ms
step:545/2330 train_time:33318ms step_avg:61.13ms
step:546/2330 train_time:33381ms step_avg:61.14ms
step:547/2330 train_time:33440ms step_avg:61.13ms
step:548/2330 train_time:33502ms step_avg:61.14ms
step:549/2330 train_time:33561ms step_avg:61.13ms
step:550/2330 train_time:33623ms step_avg:61.13ms
step:551/2330 train_time:33684ms step_avg:61.13ms
step:552/2330 train_time:33746ms step_avg:61.13ms
step:553/2330 train_time:33806ms step_avg:61.13ms
step:554/2330 train_time:33869ms step_avg:61.13ms
step:555/2330 train_time:33928ms step_avg:61.13ms
step:556/2330 train_time:33991ms step_avg:61.13ms
step:557/2330 train_time:34051ms step_avg:61.13ms
step:558/2330 train_time:34114ms step_avg:61.14ms
step:559/2330 train_time:34173ms step_avg:61.13ms
step:560/2330 train_time:34237ms step_avg:61.14ms
step:561/2330 train_time:34296ms step_avg:61.13ms
step:562/2330 train_time:34359ms step_avg:61.14ms
step:563/2330 train_time:34418ms step_avg:61.13ms
step:564/2330 train_time:34481ms step_avg:61.14ms
step:565/2330 train_time:34542ms step_avg:61.14ms
step:566/2330 train_time:34603ms step_avg:61.14ms
step:567/2330 train_time:34663ms step_avg:61.13ms
step:568/2330 train_time:34725ms step_avg:61.14ms
step:569/2330 train_time:34785ms step_avg:61.13ms
step:570/2330 train_time:34847ms step_avg:61.14ms
step:571/2330 train_time:34907ms step_avg:61.13ms
step:572/2330 train_time:34969ms step_avg:61.13ms
step:573/2330 train_time:35029ms step_avg:61.13ms
step:574/2330 train_time:35092ms step_avg:61.14ms
step:575/2330 train_time:35151ms step_avg:61.13ms
step:576/2330 train_time:35214ms step_avg:61.14ms
step:577/2330 train_time:35274ms step_avg:61.13ms
step:578/2330 train_time:35338ms step_avg:61.14ms
step:579/2330 train_time:35398ms step_avg:61.14ms
step:580/2330 train_time:35461ms step_avg:61.14ms
step:581/2330 train_time:35520ms step_avg:61.14ms
step:582/2330 train_time:35583ms step_avg:61.14ms
step:583/2330 train_time:35643ms step_avg:61.14ms
step:584/2330 train_time:35705ms step_avg:61.14ms
step:585/2330 train_time:35765ms step_avg:61.14ms
step:586/2330 train_time:35827ms step_avg:61.14ms
step:587/2330 train_time:35887ms step_avg:61.14ms
step:588/2330 train_time:35949ms step_avg:61.14ms
step:589/2330 train_time:36008ms step_avg:61.13ms
step:590/2330 train_time:36071ms step_avg:61.14ms
step:591/2330 train_time:36131ms step_avg:61.13ms
step:592/2330 train_time:36193ms step_avg:61.14ms
step:593/2330 train_time:36253ms step_avg:61.14ms
step:594/2330 train_time:36316ms step_avg:61.14ms
step:595/2330 train_time:36376ms step_avg:61.14ms
step:596/2330 train_time:36440ms step_avg:61.14ms
step:597/2330 train_time:36500ms step_avg:61.14ms
step:598/2330 train_time:36563ms step_avg:61.14ms
step:599/2330 train_time:36622ms step_avg:61.14ms
step:600/2330 train_time:36685ms step_avg:61.14ms
step:601/2330 train_time:36745ms step_avg:61.14ms
step:602/2330 train_time:36807ms step_avg:61.14ms
step:603/2330 train_time:36866ms step_avg:61.14ms
step:604/2330 train_time:36928ms step_avg:61.14ms
step:605/2330 train_time:36987ms step_avg:61.14ms
step:606/2330 train_time:37049ms step_avg:61.14ms
step:607/2330 train_time:37108ms step_avg:61.13ms
step:608/2330 train_time:37171ms step_avg:61.14ms
step:609/2330 train_time:37231ms step_avg:61.14ms
step:610/2330 train_time:37295ms step_avg:61.14ms
step:611/2330 train_time:37355ms step_avg:61.14ms
step:612/2330 train_time:37418ms step_avg:61.14ms
step:613/2330 train_time:37478ms step_avg:61.14ms
step:614/2330 train_time:37540ms step_avg:61.14ms
step:615/2330 train_time:37600ms step_avg:61.14ms
step:616/2330 train_time:37662ms step_avg:61.14ms
step:617/2330 train_time:37723ms step_avg:61.14ms
step:618/2330 train_time:37785ms step_avg:61.14ms
step:619/2330 train_time:37844ms step_avg:61.14ms
step:620/2330 train_time:37906ms step_avg:61.14ms
step:621/2330 train_time:37966ms step_avg:61.14ms
step:622/2330 train_time:38028ms step_avg:61.14ms
step:623/2330 train_time:38088ms step_avg:61.14ms
step:624/2330 train_time:38150ms step_avg:61.14ms
step:625/2330 train_time:38210ms step_avg:61.14ms
step:626/2330 train_time:38272ms step_avg:61.14ms
step:627/2330 train_time:38332ms step_avg:61.14ms
step:628/2330 train_time:38396ms step_avg:61.14ms
step:629/2330 train_time:38456ms step_avg:61.14ms
step:630/2330 train_time:38519ms step_avg:61.14ms
step:631/2330 train_time:38578ms step_avg:61.14ms
step:632/2330 train_time:38641ms step_avg:61.14ms
step:633/2330 train_time:38701ms step_avg:61.14ms
step:634/2330 train_time:38763ms step_avg:61.14ms
step:635/2330 train_time:38823ms step_avg:61.14ms
step:636/2330 train_time:38885ms step_avg:61.14ms
step:637/2330 train_time:38945ms step_avg:61.14ms
step:638/2330 train_time:39007ms step_avg:61.14ms
step:639/2330 train_time:39067ms step_avg:61.14ms
step:640/2330 train_time:39129ms step_avg:61.14ms
step:641/2330 train_time:39189ms step_avg:61.14ms
step:642/2330 train_time:39251ms step_avg:61.14ms
step:643/2330 train_time:39311ms step_avg:61.14ms
step:644/2330 train_time:39374ms step_avg:61.14ms
step:645/2330 train_time:39434ms step_avg:61.14ms
step:646/2330 train_time:39498ms step_avg:61.14ms
step:647/2330 train_time:39558ms step_avg:61.14ms
step:648/2330 train_time:39621ms step_avg:61.14ms
step:649/2330 train_time:39680ms step_avg:61.14ms
step:650/2330 train_time:39744ms step_avg:61.14ms
step:651/2330 train_time:39803ms step_avg:61.14ms
step:652/2330 train_time:39866ms step_avg:61.14ms
step:653/2330 train_time:39925ms step_avg:61.14ms
step:654/2330 train_time:39987ms step_avg:61.14ms
step:655/2330 train_time:40047ms step_avg:61.14ms
step:656/2330 train_time:40109ms step_avg:61.14ms
step:657/2330 train_time:40169ms step_avg:61.14ms
step:658/2330 train_time:40231ms step_avg:61.14ms
step:659/2330 train_time:40290ms step_avg:61.14ms
step:660/2330 train_time:40353ms step_avg:61.14ms
step:661/2330 train_time:40412ms step_avg:61.14ms
step:662/2330 train_time:40476ms step_avg:61.14ms
step:663/2330 train_time:40536ms step_avg:61.14ms
step:664/2330 train_time:40599ms step_avg:61.14ms
step:665/2330 train_time:40659ms step_avg:61.14ms
step:666/2330 train_time:40721ms step_avg:61.14ms
step:667/2330 train_time:40781ms step_avg:61.14ms
step:668/2330 train_time:40844ms step_avg:61.14ms
step:669/2330 train_time:40903ms step_avg:61.14ms
step:670/2330 train_time:40965ms step_avg:61.14ms
step:671/2330 train_time:41025ms step_avg:61.14ms
step:672/2330 train_time:41087ms step_avg:61.14ms
step:673/2330 train_time:41147ms step_avg:61.14ms
step:674/2330 train_time:41210ms step_avg:61.14ms
step:675/2330 train_time:41268ms step_avg:61.14ms
step:676/2330 train_time:41331ms step_avg:61.14ms
step:677/2330 train_time:41391ms step_avg:61.14ms
step:678/2330 train_time:41454ms step_avg:61.14ms
step:679/2330 train_time:41515ms step_avg:61.14ms
step:680/2330 train_time:41578ms step_avg:61.14ms
step:681/2330 train_time:41638ms step_avg:61.14ms
step:682/2330 train_time:41701ms step_avg:61.15ms
step:683/2330 train_time:41761ms step_avg:61.14ms
step:684/2330 train_time:41824ms step_avg:61.15ms
step:685/2330 train_time:41883ms step_avg:61.14ms
step:686/2330 train_time:41946ms step_avg:61.15ms
step:687/2330 train_time:42004ms step_avg:61.14ms
step:688/2330 train_time:42067ms step_avg:61.14ms
step:689/2330 train_time:42126ms step_avg:61.14ms
step:690/2330 train_time:42189ms step_avg:61.14ms
step:691/2330 train_time:42248ms step_avg:61.14ms
step:692/2330 train_time:42311ms step_avg:61.14ms
step:693/2330 train_time:42370ms step_avg:61.14ms
step:694/2330 train_time:42433ms step_avg:61.14ms
step:695/2330 train_time:42494ms step_avg:61.14ms
step:696/2330 train_time:42557ms step_avg:61.14ms
step:697/2330 train_time:42617ms step_avg:61.14ms
step:698/2330 train_time:42679ms step_avg:61.15ms
step:699/2330 train_time:42739ms step_avg:61.14ms
step:700/2330 train_time:42802ms step_avg:61.15ms
step:701/2330 train_time:42861ms step_avg:61.14ms
step:702/2330 train_time:42924ms step_avg:61.15ms
step:703/2330 train_time:42983ms step_avg:61.14ms
step:704/2330 train_time:43045ms step_avg:61.14ms
step:705/2330 train_time:43105ms step_avg:61.14ms
step:706/2330 train_time:43167ms step_avg:61.14ms
step:707/2330 train_time:43226ms step_avg:61.14ms
step:708/2330 train_time:43288ms step_avg:61.14ms
step:709/2330 train_time:43348ms step_avg:61.14ms
step:710/2330 train_time:43411ms step_avg:61.14ms
step:711/2330 train_time:43470ms step_avg:61.14ms
step:712/2330 train_time:43534ms step_avg:61.14ms
step:713/2330 train_time:43594ms step_avg:61.14ms
step:714/2330 train_time:43657ms step_avg:61.14ms
step:715/2330 train_time:43717ms step_avg:61.14ms
step:716/2330 train_time:43780ms step_avg:61.15ms
step:717/2330 train_time:43840ms step_avg:61.14ms
step:718/2330 train_time:43902ms step_avg:61.14ms
step:719/2330 train_time:43962ms step_avg:61.14ms
step:720/2330 train_time:44025ms step_avg:61.15ms
step:721/2330 train_time:44084ms step_avg:61.14ms
step:722/2330 train_time:44147ms step_avg:61.14ms
step:723/2330 train_time:44206ms step_avg:61.14ms
step:724/2330 train_time:44268ms step_avg:61.14ms
step:725/2330 train_time:44327ms step_avg:61.14ms
step:726/2330 train_time:44389ms step_avg:61.14ms
step:727/2330 train_time:44449ms step_avg:61.14ms
step:728/2330 train_time:44511ms step_avg:61.14ms
step:729/2330 train_time:44571ms step_avg:61.14ms
step:730/2330 train_time:44634ms step_avg:61.14ms
step:731/2330 train_time:44694ms step_avg:61.14ms
step:732/2330 train_time:44757ms step_avg:61.14ms
step:733/2330 train_time:44817ms step_avg:61.14ms
step:734/2330 train_time:44880ms step_avg:61.14ms
step:735/2330 train_time:44940ms step_avg:61.14ms
step:736/2330 train_time:45002ms step_avg:61.14ms
step:737/2330 train_time:45061ms step_avg:61.14ms
step:738/2330 train_time:45124ms step_avg:61.14ms
step:739/2330 train_time:45183ms step_avg:61.14ms
step:740/2330 train_time:45246ms step_avg:61.14ms
step:741/2330 train_time:45305ms step_avg:61.14ms
step:742/2330 train_time:45367ms step_avg:61.14ms
step:743/2330 train_time:45427ms step_avg:61.14ms
step:744/2330 train_time:45489ms step_avg:61.14ms
step:745/2330 train_time:45548ms step_avg:61.14ms
step:746/2330 train_time:45611ms step_avg:61.14ms
step:747/2330 train_time:45670ms step_avg:61.14ms
step:748/2330 train_time:45733ms step_avg:61.14ms
step:749/2330 train_time:45793ms step_avg:61.14ms
step:750/2330 train_time:45857ms step_avg:61.14ms
step:750/2330 val_loss:3.7174 train_time:45921ms step_avg:61.23ms
step:751/2330 train_time:45944ms step_avg:61.18ms
step:752/2330 train_time:45981ms step_avg:61.15ms
step:753/2330 train_time:46047ms step_avg:61.15ms
step:754/2330 train_time:46113ms step_avg:61.16ms
step:755/2330 train_time:46173ms step_avg:61.16ms
step:756/2330 train_time:46236ms step_avg:61.16ms
step:757/2330 train_time:46294ms step_avg:61.16ms
step:758/2330 train_time:46356ms step_avg:61.16ms
step:759/2330 train_time:46415ms step_avg:61.15ms
step:760/2330 train_time:46477ms step_avg:61.15ms
step:761/2330 train_time:46536ms step_avg:61.15ms
step:762/2330 train_time:46597ms step_avg:61.15ms
step:763/2330 train_time:46656ms step_avg:61.15ms
step:764/2330 train_time:46718ms step_avg:61.15ms
step:765/2330 train_time:46776ms step_avg:61.15ms
step:766/2330 train_time:46839ms step_avg:61.15ms
step:767/2330 train_time:46900ms step_avg:61.15ms
step:768/2330 train_time:46963ms step_avg:61.15ms
step:769/2330 train_time:47026ms step_avg:61.15ms
step:770/2330 train_time:47091ms step_avg:61.16ms
step:771/2330 train_time:47153ms step_avg:61.16ms
step:772/2330 train_time:47216ms step_avg:61.16ms
step:773/2330 train_time:47277ms step_avg:61.16ms
step:774/2330 train_time:47340ms step_avg:61.16ms
step:775/2330 train_time:47400ms step_avg:61.16ms
step:776/2330 train_time:47463ms step_avg:61.16ms
step:777/2330 train_time:47523ms step_avg:61.16ms
step:778/2330 train_time:47586ms step_avg:61.16ms
step:779/2330 train_time:47646ms step_avg:61.16ms
step:780/2330 train_time:47710ms step_avg:61.17ms
step:781/2330 train_time:47770ms step_avg:61.17ms
step:782/2330 train_time:47833ms step_avg:61.17ms
step:783/2330 train_time:47893ms step_avg:61.17ms
step:784/2330 train_time:47956ms step_avg:61.17ms
step:785/2330 train_time:48018ms step_avg:61.17ms
step:786/2330 train_time:48081ms step_avg:61.17ms
step:787/2330 train_time:48142ms step_avg:61.17ms
step:788/2330 train_time:48207ms step_avg:61.18ms
step:789/2330 train_time:48267ms step_avg:61.18ms
step:790/2330 train_time:48331ms step_avg:61.18ms
step:791/2330 train_time:48391ms step_avg:61.18ms
step:792/2330 train_time:48453ms step_avg:61.18ms
step:793/2330 train_time:48514ms step_avg:61.18ms
step:794/2330 train_time:48579ms step_avg:61.18ms
step:795/2330 train_time:48639ms step_avg:61.18ms
step:796/2330 train_time:48701ms step_avg:61.18ms
step:797/2330 train_time:48761ms step_avg:61.18ms
step:798/2330 train_time:48826ms step_avg:61.18ms
step:799/2330 train_time:48886ms step_avg:61.18ms
step:800/2330 train_time:48949ms step_avg:61.19ms
step:801/2330 train_time:49010ms step_avg:61.19ms
step:802/2330 train_time:49074ms step_avg:61.19ms
step:803/2330 train_time:49134ms step_avg:61.19ms
step:804/2330 train_time:49198ms step_avg:61.19ms
step:805/2330 train_time:49258ms step_avg:61.19ms
step:806/2330 train_time:49321ms step_avg:61.19ms
step:807/2330 train_time:49382ms step_avg:61.19ms
step:808/2330 train_time:49445ms step_avg:61.19ms
step:809/2330 train_time:49507ms step_avg:61.20ms
step:810/2330 train_time:49571ms step_avg:61.20ms
step:811/2330 train_time:49631ms step_avg:61.20ms
step:812/2330 train_time:49693ms step_avg:61.20ms
step:813/2330 train_time:49754ms step_avg:61.20ms
step:814/2330 train_time:49817ms step_avg:61.20ms
step:815/2330 train_time:49877ms step_avg:61.20ms
step:816/2330 train_time:49940ms step_avg:61.20ms
step:817/2330 train_time:50000ms step_avg:61.20ms
step:818/2330 train_time:50063ms step_avg:61.20ms
step:819/2330 train_time:50124ms step_avg:61.20ms
step:820/2330 train_time:50188ms step_avg:61.20ms
step:821/2330 train_time:50249ms step_avg:61.20ms
step:822/2330 train_time:50313ms step_avg:61.21ms
step:823/2330 train_time:50373ms step_avg:61.21ms
step:824/2330 train_time:50436ms step_avg:61.21ms
step:825/2330 train_time:50496ms step_avg:61.21ms
step:826/2330 train_time:50559ms step_avg:61.21ms
step:827/2330 train_time:50619ms step_avg:61.21ms
step:828/2330 train_time:50682ms step_avg:61.21ms
step:829/2330 train_time:50742ms step_avg:61.21ms
step:830/2330 train_time:50806ms step_avg:61.21ms
step:831/2330 train_time:50866ms step_avg:61.21ms
step:832/2330 train_time:50930ms step_avg:61.21ms
step:833/2330 train_time:50990ms step_avg:61.21ms
step:834/2330 train_time:51053ms step_avg:61.21ms
step:835/2330 train_time:51113ms step_avg:61.21ms
step:836/2330 train_time:51177ms step_avg:61.22ms
step:837/2330 train_time:51237ms step_avg:61.22ms
step:838/2330 train_time:51300ms step_avg:61.22ms
step:839/2330 train_time:51361ms step_avg:61.22ms
step:840/2330 train_time:51424ms step_avg:61.22ms
step:841/2330 train_time:51484ms step_avg:61.22ms
step:842/2330 train_time:51548ms step_avg:61.22ms
step:843/2330 train_time:51609ms step_avg:61.22ms
step:844/2330 train_time:51672ms step_avg:61.22ms
step:845/2330 train_time:51732ms step_avg:61.22ms
step:846/2330 train_time:51795ms step_avg:61.22ms
step:847/2330 train_time:51855ms step_avg:61.22ms
step:848/2330 train_time:51918ms step_avg:61.22ms
step:849/2330 train_time:51978ms step_avg:61.22ms
step:850/2330 train_time:52041ms step_avg:61.22ms
step:851/2330 train_time:52101ms step_avg:61.22ms
step:852/2330 train_time:52165ms step_avg:61.23ms
step:853/2330 train_time:52226ms step_avg:61.23ms
step:854/2330 train_time:52289ms step_avg:61.23ms
step:855/2330 train_time:52350ms step_avg:61.23ms
step:856/2330 train_time:52413ms step_avg:61.23ms
step:857/2330 train_time:52473ms step_avg:61.23ms
step:858/2330 train_time:52536ms step_avg:61.23ms
step:859/2330 train_time:52597ms step_avg:61.23ms
step:860/2330 train_time:52660ms step_avg:61.23ms
step:861/2330 train_time:52720ms step_avg:61.23ms
step:862/2330 train_time:52784ms step_avg:61.23ms
step:863/2330 train_time:52844ms step_avg:61.23ms
step:864/2330 train_time:52908ms step_avg:61.24ms
step:865/2330 train_time:52968ms step_avg:61.23ms
step:866/2330 train_time:53032ms step_avg:61.24ms
step:867/2330 train_time:53092ms step_avg:61.24ms
step:868/2330 train_time:53156ms step_avg:61.24ms
step:869/2330 train_time:53216ms step_avg:61.24ms
step:870/2330 train_time:53279ms step_avg:61.24ms
step:871/2330 train_time:53340ms step_avg:61.24ms
step:872/2330 train_time:53404ms step_avg:61.24ms
step:873/2330 train_time:53464ms step_avg:61.24ms
step:874/2330 train_time:53528ms step_avg:61.25ms
step:875/2330 train_time:53589ms step_avg:61.24ms
step:876/2330 train_time:53651ms step_avg:61.25ms
step:877/2330 train_time:53711ms step_avg:61.24ms
step:878/2330 train_time:53774ms step_avg:61.25ms
step:879/2330 train_time:53835ms step_avg:61.25ms
step:880/2330 train_time:53898ms step_avg:61.25ms
step:881/2330 train_time:53958ms step_avg:61.25ms
step:882/2330 train_time:54021ms step_avg:61.25ms
step:883/2330 train_time:54081ms step_avg:61.25ms
step:884/2330 train_time:54144ms step_avg:61.25ms
step:885/2330 train_time:54205ms step_avg:61.25ms
step:886/2330 train_time:54269ms step_avg:61.25ms
step:887/2330 train_time:54330ms step_avg:61.25ms
step:888/2330 train_time:54393ms step_avg:61.25ms
step:889/2330 train_time:54453ms step_avg:61.25ms
step:890/2330 train_time:54516ms step_avg:61.25ms
step:891/2330 train_time:54576ms step_avg:61.25ms
step:892/2330 train_time:54639ms step_avg:61.25ms
step:893/2330 train_time:54700ms step_avg:61.25ms
step:894/2330 train_time:54763ms step_avg:61.26ms
step:895/2330 train_time:54824ms step_avg:61.26ms
step:896/2330 train_time:54887ms step_avg:61.26ms
step:897/2330 train_time:54947ms step_avg:61.26ms
step:898/2330 train_time:55010ms step_avg:61.26ms
step:899/2330 train_time:55071ms step_avg:61.26ms
step:900/2330 train_time:55133ms step_avg:61.26ms
step:901/2330 train_time:55194ms step_avg:61.26ms
step:902/2330 train_time:55257ms step_avg:61.26ms
step:903/2330 train_time:55317ms step_avg:61.26ms
step:904/2330 train_time:55380ms step_avg:61.26ms
step:905/2330 train_time:55440ms step_avg:61.26ms
step:906/2330 train_time:55504ms step_avg:61.26ms
step:907/2330 train_time:55565ms step_avg:61.26ms
step:908/2330 train_time:55628ms step_avg:61.26ms
step:909/2330 train_time:55688ms step_avg:61.26ms
step:910/2330 train_time:55752ms step_avg:61.27ms
step:911/2330 train_time:55812ms step_avg:61.26ms
step:912/2330 train_time:55875ms step_avg:61.27ms
step:913/2330 train_time:55935ms step_avg:61.27ms
step:914/2330 train_time:55998ms step_avg:61.27ms
step:915/2330 train_time:56058ms step_avg:61.27ms
step:916/2330 train_time:56122ms step_avg:61.27ms
step:917/2330 train_time:56182ms step_avg:61.27ms
step:918/2330 train_time:56245ms step_avg:61.27ms
step:919/2330 train_time:56306ms step_avg:61.27ms
step:920/2330 train_time:56370ms step_avg:61.27ms
step:921/2330 train_time:56430ms step_avg:61.27ms
step:922/2330 train_time:56493ms step_avg:61.27ms
step:923/2330 train_time:56554ms step_avg:61.27ms
step:924/2330 train_time:56617ms step_avg:61.27ms
step:925/2330 train_time:56677ms step_avg:61.27ms
step:926/2330 train_time:56740ms step_avg:61.27ms
step:927/2330 train_time:56800ms step_avg:61.27ms
step:928/2330 train_time:56864ms step_avg:61.28ms
step:929/2330 train_time:56924ms step_avg:61.27ms
step:930/2330 train_time:56988ms step_avg:61.28ms
step:931/2330 train_time:57048ms step_avg:61.28ms
step:932/2330 train_time:57111ms step_avg:61.28ms
step:933/2330 train_time:57172ms step_avg:61.28ms
step:934/2330 train_time:57235ms step_avg:61.28ms
step:935/2330 train_time:57295ms step_avg:61.28ms
step:936/2330 train_time:57357ms step_avg:61.28ms
step:937/2330 train_time:57418ms step_avg:61.28ms
step:938/2330 train_time:57481ms step_avg:61.28ms
step:939/2330 train_time:57541ms step_avg:61.28ms
step:940/2330 train_time:57605ms step_avg:61.28ms
step:941/2330 train_time:57666ms step_avg:61.28ms
step:942/2330 train_time:57729ms step_avg:61.28ms
step:943/2330 train_time:57789ms step_avg:61.28ms
step:944/2330 train_time:57852ms step_avg:61.28ms
step:945/2330 train_time:57912ms step_avg:61.28ms
step:946/2330 train_time:57975ms step_avg:61.28ms
step:947/2330 train_time:58036ms step_avg:61.28ms
step:948/2330 train_time:58098ms step_avg:61.28ms
step:949/2330 train_time:58158ms step_avg:61.28ms
step:950/2330 train_time:58221ms step_avg:61.29ms
step:951/2330 train_time:58282ms step_avg:61.28ms
step:952/2330 train_time:58345ms step_avg:61.29ms
step:953/2330 train_time:58406ms step_avg:61.29ms
step:954/2330 train_time:58469ms step_avg:61.29ms
step:955/2330 train_time:58530ms step_avg:61.29ms
step:956/2330 train_time:58593ms step_avg:61.29ms
step:957/2330 train_time:58653ms step_avg:61.29ms
step:958/2330 train_time:58716ms step_avg:61.29ms
step:959/2330 train_time:58777ms step_avg:61.29ms
step:960/2330 train_time:58840ms step_avg:61.29ms
step:961/2330 train_time:58900ms step_avg:61.29ms
step:962/2330 train_time:58962ms step_avg:61.29ms
step:963/2330 train_time:59023ms step_avg:61.29ms
step:964/2330 train_time:59086ms step_avg:61.29ms
step:965/2330 train_time:59146ms step_avg:61.29ms
step:966/2330 train_time:59210ms step_avg:61.29ms
step:967/2330 train_time:59271ms step_avg:61.29ms
step:968/2330 train_time:59334ms step_avg:61.30ms
step:969/2330 train_time:59394ms step_avg:61.29ms
step:970/2330 train_time:59457ms step_avg:61.30ms
step:971/2330 train_time:59518ms step_avg:61.30ms
step:972/2330 train_time:59581ms step_avg:61.30ms
step:973/2330 train_time:59641ms step_avg:61.30ms
step:974/2330 train_time:59705ms step_avg:61.30ms
step:975/2330 train_time:59765ms step_avg:61.30ms
step:976/2330 train_time:59829ms step_avg:61.30ms
step:977/2330 train_time:59888ms step_avg:61.30ms
step:978/2330 train_time:59952ms step_avg:61.30ms
step:979/2330 train_time:60012ms step_avg:61.30ms
step:980/2330 train_time:60075ms step_avg:61.30ms
step:981/2330 train_time:60135ms step_avg:61.30ms
step:982/2330 train_time:60199ms step_avg:61.30ms
step:983/2330 train_time:60258ms step_avg:61.30ms
step:984/2330 train_time:60322ms step_avg:61.30ms
step:985/2330 train_time:60382ms step_avg:61.30ms
step:986/2330 train_time:60446ms step_avg:61.30ms
step:987/2330 train_time:60506ms step_avg:61.30ms
step:988/2330 train_time:60571ms step_avg:61.31ms
step:989/2330 train_time:60631ms step_avg:61.30ms
step:990/2330 train_time:60694ms step_avg:61.31ms
step:991/2330 train_time:60754ms step_avg:61.31ms
step:992/2330 train_time:60818ms step_avg:61.31ms
step:993/2330 train_time:60877ms step_avg:61.31ms
step:994/2330 train_time:60940ms step_avg:61.31ms
step:995/2330 train_time:61000ms step_avg:61.31ms
step:996/2330 train_time:61064ms step_avg:61.31ms
step:997/2330 train_time:61125ms step_avg:61.31ms
step:998/2330 train_time:61188ms step_avg:61.31ms
step:999/2330 train_time:61249ms step_avg:61.31ms
step:1000/2330 train_time:61312ms step_avg:61.31ms
step:1000/2330 val_loss:3.6050 train_time:61377ms step_avg:61.38ms
step:1001/2330 train_time:61399ms step_avg:61.34ms
step:1002/2330 train_time:61440ms step_avg:61.32ms
step:1003/2330 train_time:61507ms step_avg:61.32ms
step:1004/2330 train_time:61573ms step_avg:61.33ms
step:1005/2330 train_time:61634ms step_avg:61.33ms
step:1006/2330 train_time:61698ms step_avg:61.33ms
step:1007/2330 train_time:61758ms step_avg:61.33ms
step:1008/2330 train_time:61822ms step_avg:61.33ms
step:1009/2330 train_time:61881ms step_avg:61.33ms
step:1010/2330 train_time:61943ms step_avg:61.33ms
step:1011/2330 train_time:62003ms step_avg:61.33ms
step:1012/2330 train_time:62065ms step_avg:61.33ms
step:1013/2330 train_time:62124ms step_avg:61.33ms
step:1014/2330 train_time:62186ms step_avg:61.33ms
step:1015/2330 train_time:62245ms step_avg:61.33ms
step:1016/2330 train_time:62311ms step_avg:61.33ms
step:1017/2330 train_time:62372ms step_avg:61.33ms
step:1018/2330 train_time:62436ms step_avg:61.33ms
step:1019/2330 train_time:62498ms step_avg:61.33ms
step:1020/2330 train_time:62562ms step_avg:61.34ms
step:1021/2330 train_time:62623ms step_avg:61.34ms
step:1022/2330 train_time:62686ms step_avg:61.34ms
step:1023/2330 train_time:62747ms step_avg:61.34ms
step:1024/2330 train_time:62809ms step_avg:61.34ms
step:1025/2330 train_time:62869ms step_avg:61.34ms
step:1026/2330 train_time:62932ms step_avg:61.34ms
step:1027/2330 train_time:62992ms step_avg:61.34ms
step:1028/2330 train_time:63055ms step_avg:61.34ms
step:1029/2330 train_time:63116ms step_avg:61.34ms
step:1030/2330 train_time:63179ms step_avg:61.34ms
step:1031/2330 train_time:63239ms step_avg:61.34ms
step:1032/2330 train_time:63304ms step_avg:61.34ms
step:1033/2330 train_time:63364ms step_avg:61.34ms
step:1034/2330 train_time:63428ms step_avg:61.34ms
step:1035/2330 train_time:63489ms step_avg:61.34ms
step:1036/2330 train_time:63552ms step_avg:61.34ms
step:1037/2330 train_time:63612ms step_avg:61.34ms
step:1038/2330 train_time:63676ms step_avg:61.35ms
step:1039/2330 train_time:63737ms step_avg:61.34ms
step:1040/2330 train_time:63800ms step_avg:61.35ms
step:1041/2330 train_time:63860ms step_avg:61.34ms
step:1042/2330 train_time:63923ms step_avg:61.35ms
step:1043/2330 train_time:63983ms step_avg:61.34ms
step:1044/2330 train_time:64045ms step_avg:61.35ms
step:1045/2330 train_time:64106ms step_avg:61.35ms
step:1046/2330 train_time:64169ms step_avg:61.35ms
step:1047/2330 train_time:64229ms step_avg:61.35ms
step:1048/2330 train_time:64292ms step_avg:61.35ms
step:1049/2330 train_time:64352ms step_avg:61.35ms
step:1050/2330 train_time:64415ms step_avg:61.35ms
step:1051/2330 train_time:64477ms step_avg:61.35ms
step:1052/2330 train_time:64541ms step_avg:61.35ms
step:1053/2330 train_time:64602ms step_avg:61.35ms
step:1054/2330 train_time:64665ms step_avg:61.35ms
step:1055/2330 train_time:64725ms step_avg:61.35ms
step:1056/2330 train_time:64788ms step_avg:61.35ms
step:1057/2330 train_time:64847ms step_avg:61.35ms
step:1058/2330 train_time:64910ms step_avg:61.35ms
step:1059/2330 train_time:64970ms step_avg:61.35ms
step:1060/2330 train_time:65033ms step_avg:61.35ms
step:1061/2330 train_time:65094ms step_avg:61.35ms
step:1062/2330 train_time:65158ms step_avg:61.35ms
step:1063/2330 train_time:65218ms step_avg:61.35ms
step:1064/2330 train_time:65281ms step_avg:61.35ms
step:1065/2330 train_time:65341ms step_avg:61.35ms
step:1066/2330 train_time:65404ms step_avg:61.35ms
step:1067/2330 train_time:65464ms step_avg:61.35ms
step:1068/2330 train_time:65527ms step_avg:61.36ms
step:1069/2330 train_time:65587ms step_avg:61.35ms
step:1070/2330 train_time:65651ms step_avg:61.36ms
step:1071/2330 train_time:65711ms step_avg:61.35ms
step:1072/2330 train_time:65774ms step_avg:61.36ms
step:1073/2330 train_time:65835ms step_avg:61.36ms
step:1074/2330 train_time:65899ms step_avg:61.36ms
step:1075/2330 train_time:65959ms step_avg:61.36ms
step:1076/2330 train_time:66022ms step_avg:61.36ms
step:1077/2330 train_time:66082ms step_avg:61.36ms
step:1078/2330 train_time:66145ms step_avg:61.36ms
step:1079/2330 train_time:66205ms step_avg:61.36ms
step:1080/2330 train_time:66268ms step_avg:61.36ms
step:1081/2330 train_time:66328ms step_avg:61.36ms
step:1082/2330 train_time:66391ms step_avg:61.36ms
step:1083/2330 train_time:66451ms step_avg:61.36ms
step:1084/2330 train_time:66515ms step_avg:61.36ms
step:1085/2330 train_time:66576ms step_avg:61.36ms
step:1086/2330 train_time:66640ms step_avg:61.36ms
step:1087/2330 train_time:66700ms step_avg:61.36ms
step:1088/2330 train_time:66763ms step_avg:61.36ms
step:1089/2330 train_time:66824ms step_avg:61.36ms
step:1090/2330 train_time:66886ms step_avg:61.36ms
step:1091/2330 train_time:66947ms step_avg:61.36ms
step:1092/2330 train_time:67009ms step_avg:61.36ms
step:1093/2330 train_time:67069ms step_avg:61.36ms
step:1094/2330 train_time:67133ms step_avg:61.36ms
step:1095/2330 train_time:67193ms step_avg:61.36ms
step:1096/2330 train_time:67257ms step_avg:61.37ms
step:1097/2330 train_time:67317ms step_avg:61.36ms
step:1098/2330 train_time:67380ms step_avg:61.37ms
step:1099/2330 train_time:67440ms step_avg:61.37ms
step:1100/2330 train_time:67504ms step_avg:61.37ms
step:1101/2330 train_time:67564ms step_avg:61.37ms
step:1102/2330 train_time:67628ms step_avg:61.37ms
step:1103/2330 train_time:67688ms step_avg:61.37ms
step:1104/2330 train_time:67751ms step_avg:61.37ms
step:1105/2330 train_time:67811ms step_avg:61.37ms
step:1106/2330 train_time:67875ms step_avg:61.37ms
step:1107/2330 train_time:67935ms step_avg:61.37ms
step:1108/2330 train_time:67998ms step_avg:61.37ms
step:1109/2330 train_time:68059ms step_avg:61.37ms
step:1110/2330 train_time:68122ms step_avg:61.37ms
step:1111/2330 train_time:68182ms step_avg:61.37ms
step:1112/2330 train_time:68245ms step_avg:61.37ms
step:1113/2330 train_time:68305ms step_avg:61.37ms
step:1114/2330 train_time:68369ms step_avg:61.37ms
step:1115/2330 train_time:68429ms step_avg:61.37ms
step:1116/2330 train_time:68493ms step_avg:61.37ms
step:1117/2330 train_time:68553ms step_avg:61.37ms
step:1118/2330 train_time:68617ms step_avg:61.37ms
step:1119/2330 train_time:68678ms step_avg:61.37ms
step:1120/2330 train_time:68741ms step_avg:61.38ms
step:1121/2330 train_time:68802ms step_avg:61.38ms
step:1122/2330 train_time:68865ms step_avg:61.38ms
step:1123/2330 train_time:68925ms step_avg:61.38ms
step:1124/2330 train_time:68988ms step_avg:61.38ms
step:1125/2330 train_time:69047ms step_avg:61.38ms
step:1126/2330 train_time:69110ms step_avg:61.38ms
step:1127/2330 train_time:69170ms step_avg:61.38ms
step:1128/2330 train_time:69233ms step_avg:61.38ms
step:1129/2330 train_time:69294ms step_avg:61.38ms
step:1130/2330 train_time:69357ms step_avg:61.38ms
step:1131/2330 train_time:69418ms step_avg:61.38ms
step:1132/2330 train_time:69482ms step_avg:61.38ms
step:1133/2330 train_time:69542ms step_avg:61.38ms
step:1134/2330 train_time:69605ms step_avg:61.38ms
step:1135/2330 train_time:69665ms step_avg:61.38ms
step:1136/2330 train_time:69729ms step_avg:61.38ms
step:1137/2330 train_time:69789ms step_avg:61.38ms
step:1138/2330 train_time:69852ms step_avg:61.38ms
step:1139/2330 train_time:69913ms step_avg:61.38ms
step:1140/2330 train_time:69976ms step_avg:61.38ms
step:1141/2330 train_time:70037ms step_avg:61.38ms
step:1142/2330 train_time:70100ms step_avg:61.38ms
step:1143/2330 train_time:70161ms step_avg:61.38ms
step:1144/2330 train_time:70225ms step_avg:61.39ms
step:1145/2330 train_time:70284ms step_avg:61.38ms
step:1146/2330 train_time:70347ms step_avg:61.38ms
step:1147/2330 train_time:70407ms step_avg:61.38ms
step:1148/2330 train_time:70470ms step_avg:61.39ms
step:1149/2330 train_time:70530ms step_avg:61.38ms
step:1150/2330 train_time:70595ms step_avg:61.39ms
step:1151/2330 train_time:70655ms step_avg:61.39ms
step:1152/2330 train_time:70719ms step_avg:61.39ms
step:1153/2330 train_time:70779ms step_avg:61.39ms
step:1154/2330 train_time:70843ms step_avg:61.39ms
step:1155/2330 train_time:70903ms step_avg:61.39ms
step:1156/2330 train_time:70966ms step_avg:61.39ms
step:1157/2330 train_time:71026ms step_avg:61.39ms
step:1158/2330 train_time:71089ms step_avg:61.39ms
step:1159/2330 train_time:71149ms step_avg:61.39ms
step:1160/2330 train_time:71211ms step_avg:61.39ms
step:1161/2330 train_time:71272ms step_avg:61.39ms
step:1162/2330 train_time:71335ms step_avg:61.39ms
step:1163/2330 train_time:71396ms step_avg:61.39ms
step:1164/2330 train_time:71459ms step_avg:61.39ms
step:1165/2330 train_time:71520ms step_avg:61.39ms
step:1166/2330 train_time:71583ms step_avg:61.39ms
step:1167/2330 train_time:71643ms step_avg:61.39ms
step:1168/2330 train_time:71706ms step_avg:61.39ms
step:1169/2330 train_time:71767ms step_avg:61.39ms
step:1170/2330 train_time:71830ms step_avg:61.39ms
step:1171/2330 train_time:71890ms step_avg:61.39ms
step:1172/2330 train_time:71954ms step_avg:61.39ms
step:1173/2330 train_time:72014ms step_avg:61.39ms
step:1174/2330 train_time:72077ms step_avg:61.39ms
step:1175/2330 train_time:72137ms step_avg:61.39ms
step:1176/2330 train_time:72201ms step_avg:61.40ms
step:1177/2330 train_time:72262ms step_avg:61.39ms
step:1178/2330 train_time:72325ms step_avg:61.40ms
step:1179/2330 train_time:72384ms step_avg:61.39ms
step:1180/2330 train_time:72448ms step_avg:61.40ms
step:1181/2330 train_time:72507ms step_avg:61.39ms
step:1182/2330 train_time:72570ms step_avg:61.40ms
step:1183/2330 train_time:72630ms step_avg:61.39ms
step:1184/2330 train_time:72694ms step_avg:61.40ms
step:1185/2330 train_time:72754ms step_avg:61.40ms
step:1186/2330 train_time:72818ms step_avg:61.40ms
step:1187/2330 train_time:72880ms step_avg:61.40ms
step:1188/2330 train_time:72943ms step_avg:61.40ms
step:1189/2330 train_time:73002ms step_avg:61.40ms
step:1190/2330 train_time:73066ms step_avg:61.40ms
step:1191/2330 train_time:73127ms step_avg:61.40ms
step:1192/2330 train_time:73190ms step_avg:61.40ms
step:1193/2330 train_time:73250ms step_avg:61.40ms
step:1194/2330 train_time:73313ms step_avg:61.40ms
step:1195/2330 train_time:73373ms step_avg:61.40ms
step:1196/2330 train_time:73437ms step_avg:61.40ms
step:1197/2330 train_time:73497ms step_avg:61.40ms
step:1198/2330 train_time:73561ms step_avg:61.40ms
step:1199/2330 train_time:73621ms step_avg:61.40ms
step:1200/2330 train_time:73684ms step_avg:61.40ms
step:1201/2330 train_time:73743ms step_avg:61.40ms
step:1202/2330 train_time:73807ms step_avg:61.40ms
step:1203/2330 train_time:73867ms step_avg:61.40ms
step:1204/2330 train_time:73930ms step_avg:61.40ms
step:1205/2330 train_time:73990ms step_avg:61.40ms
step:1206/2330 train_time:74054ms step_avg:61.40ms
step:1207/2330 train_time:74115ms step_avg:61.40ms
step:1208/2330 train_time:74178ms step_avg:61.41ms
step:1209/2330 train_time:74238ms step_avg:61.40ms
step:1210/2330 train_time:74301ms step_avg:61.41ms
step:1211/2330 train_time:74362ms step_avg:61.41ms
step:1212/2330 train_time:74426ms step_avg:61.41ms
step:1213/2330 train_time:74485ms step_avg:61.41ms
step:1214/2330 train_time:74548ms step_avg:61.41ms
step:1215/2330 train_time:74608ms step_avg:61.41ms
step:1216/2330 train_time:74671ms step_avg:61.41ms
step:1217/2330 train_time:74731ms step_avg:61.41ms
step:1218/2330 train_time:74795ms step_avg:61.41ms
step:1219/2330 train_time:74855ms step_avg:61.41ms
step:1220/2330 train_time:74919ms step_avg:61.41ms
step:1221/2330 train_time:74979ms step_avg:61.41ms
step:1222/2330 train_time:75042ms step_avg:61.41ms
step:1223/2330 train_time:75102ms step_avg:61.41ms
step:1224/2330 train_time:75165ms step_avg:61.41ms
step:1225/2330 train_time:75225ms step_avg:61.41ms
step:1226/2330 train_time:75289ms step_avg:61.41ms
step:1227/2330 train_time:75349ms step_avg:61.41ms
step:1228/2330 train_time:75412ms step_avg:61.41ms
step:1229/2330 train_time:75472ms step_avg:61.41ms
step:1230/2330 train_time:75536ms step_avg:61.41ms
step:1231/2330 train_time:75596ms step_avg:61.41ms
step:1232/2330 train_time:75660ms step_avg:61.41ms
step:1233/2330 train_time:75721ms step_avg:61.41ms
step:1234/2330 train_time:75784ms step_avg:61.41ms
step:1235/2330 train_time:75844ms step_avg:61.41ms
step:1236/2330 train_time:75908ms step_avg:61.41ms
step:1237/2330 train_time:75967ms step_avg:61.41ms
step:1238/2330 train_time:76031ms step_avg:61.41ms
step:1239/2330 train_time:76091ms step_avg:61.41ms
step:1240/2330 train_time:76154ms step_avg:61.41ms
step:1241/2330 train_time:76215ms step_avg:61.41ms
step:1242/2330 train_time:76278ms step_avg:61.42ms
step:1243/2330 train_time:76339ms step_avg:61.41ms
step:1244/2330 train_time:76402ms step_avg:61.42ms
step:1245/2330 train_time:76462ms step_avg:61.42ms
step:1246/2330 train_time:76525ms step_avg:61.42ms
step:1247/2330 train_time:76585ms step_avg:61.42ms
step:1248/2330 train_time:76649ms step_avg:61.42ms
step:1249/2330 train_time:76708ms step_avg:61.42ms
step:1250/2330 train_time:76771ms step_avg:61.42ms
step:1250/2330 val_loss:3.5458 train_time:76836ms step_avg:61.47ms
step:1251/2330 train_time:76859ms step_avg:61.44ms
step:1252/2330 train_time:76898ms step_avg:61.42ms
step:1253/2330 train_time:76961ms step_avg:61.42ms
step:1254/2330 train_time:77026ms step_avg:61.42ms
step:1255/2330 train_time:77086ms step_avg:61.42ms
step:1256/2330 train_time:77150ms step_avg:61.42ms
step:1257/2330 train_time:77209ms step_avg:61.42ms
step:1258/2330 train_time:77272ms step_avg:61.42ms
step:1259/2330 train_time:77331ms step_avg:61.42ms
step:1260/2330 train_time:77394ms step_avg:61.42ms
step:1261/2330 train_time:77454ms step_avg:61.42ms
step:1262/2330 train_time:77516ms step_avg:61.42ms
step:1263/2330 train_time:77575ms step_avg:61.42ms
step:1264/2330 train_time:77638ms step_avg:61.42ms
step:1265/2330 train_time:77697ms step_avg:61.42ms
step:1266/2330 train_time:77761ms step_avg:61.42ms
step:1267/2330 train_time:77823ms step_avg:61.42ms
step:1268/2330 train_time:77887ms step_avg:61.42ms
step:1269/2330 train_time:77949ms step_avg:61.43ms
step:1270/2330 train_time:78013ms step_avg:61.43ms
step:1271/2330 train_time:78074ms step_avg:61.43ms
step:1272/2330 train_time:78137ms step_avg:61.43ms
step:1273/2330 train_time:78198ms step_avg:61.43ms
step:1274/2330 train_time:78261ms step_avg:61.43ms
step:1275/2330 train_time:78320ms step_avg:61.43ms
step:1276/2330 train_time:78383ms step_avg:61.43ms
step:1277/2330 train_time:78443ms step_avg:61.43ms
step:1278/2330 train_time:78507ms step_avg:61.43ms
step:1279/2330 train_time:78567ms step_avg:61.43ms
step:1280/2330 train_time:78631ms step_avg:61.43ms
step:1281/2330 train_time:78691ms step_avg:61.43ms
step:1282/2330 train_time:78755ms step_avg:61.43ms
step:1283/2330 train_time:78815ms step_avg:61.43ms
step:1284/2330 train_time:78879ms step_avg:61.43ms
step:1285/2330 train_time:78939ms step_avg:61.43ms
step:1286/2330 train_time:79004ms step_avg:61.43ms
step:1287/2330 train_time:79065ms step_avg:61.43ms
step:1288/2330 train_time:79129ms step_avg:61.44ms
step:1289/2330 train_time:79191ms step_avg:61.44ms
step:1290/2330 train_time:79255ms step_avg:61.44ms
step:1291/2330 train_time:79314ms step_avg:61.44ms
step:1292/2330 train_time:79377ms step_avg:61.44ms
step:1293/2330 train_time:79437ms step_avg:61.44ms
step:1294/2330 train_time:79500ms step_avg:61.44ms
step:1295/2330 train_time:79560ms step_avg:61.44ms
step:1296/2330 train_time:79623ms step_avg:61.44ms
step:1297/2330 train_time:79683ms step_avg:61.44ms
step:1298/2330 train_time:79747ms step_avg:61.44ms
step:1299/2330 train_time:79808ms step_avg:61.44ms
step:1300/2330 train_time:79872ms step_avg:61.44ms
step:1301/2330 train_time:79933ms step_avg:61.44ms
step:1302/2330 train_time:79996ms step_avg:61.44ms
step:1303/2330 train_time:80057ms step_avg:61.44ms
step:1304/2330 train_time:80121ms step_avg:61.44ms
step:1305/2330 train_time:80180ms step_avg:61.44ms
step:1306/2330 train_time:80243ms step_avg:61.44ms
step:1307/2330 train_time:80303ms step_avg:61.44ms
step:1308/2330 train_time:80367ms step_avg:61.44ms
step:1309/2330 train_time:80428ms step_avg:61.44ms
step:1310/2330 train_time:80492ms step_avg:61.44ms
step:1311/2330 train_time:80552ms step_avg:61.44ms
step:1312/2330 train_time:80614ms step_avg:61.44ms
step:1313/2330 train_time:80674ms step_avg:61.44ms
step:1314/2330 train_time:80737ms step_avg:61.44ms
step:1315/2330 train_time:80798ms step_avg:61.44ms
step:1316/2330 train_time:80861ms step_avg:61.44ms
step:1317/2330 train_time:80921ms step_avg:61.44ms
step:1318/2330 train_time:80985ms step_avg:61.45ms
step:1319/2330 train_time:81046ms step_avg:61.44ms
step:1320/2330 train_time:81110ms step_avg:61.45ms
step:1321/2330 train_time:81170ms step_avg:61.45ms
step:1322/2330 train_time:81233ms step_avg:61.45ms
step:1323/2330 train_time:81293ms step_avg:61.45ms
step:1324/2330 train_time:81356ms step_avg:61.45ms
step:1325/2330 train_time:81416ms step_avg:61.45ms
step:1326/2330 train_time:81479ms step_avg:61.45ms
step:1327/2330 train_time:81539ms step_avg:61.45ms
step:1328/2330 train_time:81602ms step_avg:61.45ms
step:1329/2330 train_time:81663ms step_avg:61.45ms
step:1330/2330 train_time:81726ms step_avg:61.45ms
step:1331/2330 train_time:81786ms step_avg:61.45ms
step:1332/2330 train_time:81849ms step_avg:61.45ms
step:1333/2330 train_time:81909ms step_avg:61.45ms
step:1334/2330 train_time:81973ms step_avg:61.45ms
step:1335/2330 train_time:82033ms step_avg:61.45ms
step:1336/2330 train_time:82096ms step_avg:61.45ms
step:1337/2330 train_time:82157ms step_avg:61.45ms
step:1338/2330 train_time:82220ms step_avg:61.45ms
step:1339/2330 train_time:82280ms step_avg:61.45ms
step:1340/2330 train_time:82343ms step_avg:61.45ms
step:1341/2330 train_time:82403ms step_avg:61.45ms
step:1342/2330 train_time:82467ms step_avg:61.45ms
step:1343/2330 train_time:82527ms step_avg:61.45ms
step:1344/2330 train_time:82591ms step_avg:61.45ms
step:1345/2330 train_time:82652ms step_avg:61.45ms
step:1346/2330 train_time:82715ms step_avg:61.45ms
step:1347/2330 train_time:82774ms step_avg:61.45ms
step:1348/2330 train_time:82837ms step_avg:61.45ms
step:1349/2330 train_time:82898ms step_avg:61.45ms
step:1350/2330 train_time:82961ms step_avg:61.45ms
step:1351/2330 train_time:83021ms step_avg:61.45ms
step:1352/2330 train_time:83084ms step_avg:61.45ms
step:1353/2330 train_time:83144ms step_avg:61.45ms
step:1354/2330 train_time:83208ms step_avg:61.45ms
step:1355/2330 train_time:83269ms step_avg:61.45ms
step:1356/2330 train_time:83332ms step_avg:61.45ms
step:1357/2330 train_time:83392ms step_avg:61.45ms
step:1358/2330 train_time:83456ms step_avg:61.46ms
step:1359/2330 train_time:83516ms step_avg:61.45ms
step:1360/2330 train_time:83579ms step_avg:61.46ms
step:1361/2330 train_time:83639ms step_avg:61.45ms
step:1362/2330 train_time:83702ms step_avg:61.46ms
step:1363/2330 train_time:83762ms step_avg:61.45ms
step:1364/2330 train_time:83826ms step_avg:61.46ms
step:1365/2330 train_time:83886ms step_avg:61.46ms
step:1366/2330 train_time:83951ms step_avg:61.46ms
step:1367/2330 train_time:84012ms step_avg:61.46ms
step:1368/2330 train_time:84075ms step_avg:61.46ms
step:1369/2330 train_time:84135ms step_avg:61.46ms
step:1370/2330 train_time:84199ms step_avg:61.46ms
step:1371/2330 train_time:84259ms step_avg:61.46ms
step:1372/2330 train_time:84322ms step_avg:61.46ms
step:1373/2330 train_time:84382ms step_avg:61.46ms
step:1374/2330 train_time:84446ms step_avg:61.46ms
step:1375/2330 train_time:84506ms step_avg:61.46ms
step:1376/2330 train_time:84569ms step_avg:61.46ms
step:1377/2330 train_time:84630ms step_avg:61.46ms
step:1378/2330 train_time:84693ms step_avg:61.46ms
step:1379/2330 train_time:84753ms step_avg:61.46ms
step:1380/2330 train_time:84817ms step_avg:61.46ms
step:1381/2330 train_time:84877ms step_avg:61.46ms
step:1382/2330 train_time:84939ms step_avg:61.46ms
step:1383/2330 train_time:84999ms step_avg:61.46ms
step:1384/2330 train_time:85063ms step_avg:61.46ms
step:1385/2330 train_time:85123ms step_avg:61.46ms
step:1386/2330 train_time:85187ms step_avg:61.46ms
step:1387/2330 train_time:85248ms step_avg:61.46ms
step:1388/2330 train_time:85311ms step_avg:61.46ms
step:1389/2330 train_time:85371ms step_avg:61.46ms
step:1390/2330 train_time:85434ms step_avg:61.46ms
step:1391/2330 train_time:85494ms step_avg:61.46ms
step:1392/2330 train_time:85557ms step_avg:61.46ms
step:1393/2330 train_time:85617ms step_avg:61.46ms
step:1394/2330 train_time:85680ms step_avg:61.46ms
step:1395/2330 train_time:85740ms step_avg:61.46ms
step:1396/2330 train_time:85803ms step_avg:61.46ms
step:1397/2330 train_time:85863ms step_avg:61.46ms
step:1398/2330 train_time:85927ms step_avg:61.46ms
step:1399/2330 train_time:85987ms step_avg:61.46ms
step:1400/2330 train_time:86051ms step_avg:61.46ms
step:1401/2330 train_time:86110ms step_avg:61.46ms
step:1402/2330 train_time:86173ms step_avg:61.46ms
step:1403/2330 train_time:86233ms step_avg:61.46ms
step:1404/2330 train_time:86297ms step_avg:61.46ms
step:1405/2330 train_time:86357ms step_avg:61.46ms
step:1406/2330 train_time:86420ms step_avg:61.47ms
step:1407/2330 train_time:86479ms step_avg:61.46ms
step:1408/2330 train_time:86542ms step_avg:61.46ms
step:1409/2330 train_time:86603ms step_avg:61.46ms
step:1410/2330 train_time:86666ms step_avg:61.47ms
step:1411/2330 train_time:86727ms step_avg:61.46ms
step:1412/2330 train_time:86792ms step_avg:61.47ms
step:1413/2330 train_time:86852ms step_avg:61.47ms
step:1414/2330 train_time:86915ms step_avg:61.47ms
step:1415/2330 train_time:86975ms step_avg:61.47ms
step:1416/2330 train_time:87038ms step_avg:61.47ms
step:1417/2330 train_time:87098ms step_avg:61.47ms
step:1418/2330 train_time:87162ms step_avg:61.47ms
step:1419/2330 train_time:87222ms step_avg:61.47ms
step:1420/2330 train_time:87286ms step_avg:61.47ms
step:1421/2330 train_time:87346ms step_avg:61.47ms
step:1422/2330 train_time:87410ms step_avg:61.47ms
step:1423/2330 train_time:87471ms step_avg:61.47ms
step:1424/2330 train_time:87534ms step_avg:61.47ms
step:1425/2330 train_time:87594ms step_avg:61.47ms
step:1426/2330 train_time:87657ms step_avg:61.47ms
step:1427/2330 train_time:87717ms step_avg:61.47ms
step:1428/2330 train_time:87780ms step_avg:61.47ms
step:1429/2330 train_time:87840ms step_avg:61.47ms
step:1430/2330 train_time:87903ms step_avg:61.47ms
step:1431/2330 train_time:87964ms step_avg:61.47ms
step:1432/2330 train_time:88028ms step_avg:61.47ms
step:1433/2330 train_time:88088ms step_avg:61.47ms
step:1434/2330 train_time:88152ms step_avg:61.47ms
step:1435/2330 train_time:88212ms step_avg:61.47ms
step:1436/2330 train_time:88275ms step_avg:61.47ms
step:1437/2330 train_time:88336ms step_avg:61.47ms
step:1438/2330 train_time:88398ms step_avg:61.47ms
step:1439/2330 train_time:88459ms step_avg:61.47ms
step:1440/2330 train_time:88521ms step_avg:61.47ms
step:1441/2330 train_time:88582ms step_avg:61.47ms
step:1442/2330 train_time:88646ms step_avg:61.47ms
step:1443/2330 train_time:88706ms step_avg:61.47ms
step:1444/2330 train_time:88769ms step_avg:61.47ms
step:1445/2330 train_time:88830ms step_avg:61.47ms
step:1446/2330 train_time:88893ms step_avg:61.47ms
step:1447/2330 train_time:88953ms step_avg:61.47ms
step:1448/2330 train_time:89017ms step_avg:61.48ms
step:1449/2330 train_time:89077ms step_avg:61.47ms
step:1450/2330 train_time:89140ms step_avg:61.48ms
step:1451/2330 train_time:89199ms step_avg:61.47ms
step:1452/2330 train_time:89263ms step_avg:61.48ms
step:1453/2330 train_time:89324ms step_avg:61.48ms
step:1454/2330 train_time:89388ms step_avg:61.48ms
step:1455/2330 train_time:89448ms step_avg:61.48ms
step:1456/2330 train_time:89512ms step_avg:61.48ms
step:1457/2330 train_time:89572ms step_avg:61.48ms
step:1458/2330 train_time:89636ms step_avg:61.48ms
step:1459/2330 train_time:89695ms step_avg:61.48ms
step:1460/2330 train_time:89759ms step_avg:61.48ms
step:1461/2330 train_time:89819ms step_avg:61.48ms
step:1462/2330 train_time:89881ms step_avg:61.48ms
step:1463/2330 train_time:89942ms step_avg:61.48ms
step:1464/2330 train_time:90006ms step_avg:61.48ms
step:1465/2330 train_time:90066ms step_avg:61.48ms
step:1466/2330 train_time:90130ms step_avg:61.48ms
step:1467/2330 train_time:90190ms step_avg:61.48ms
step:1468/2330 train_time:90254ms step_avg:61.48ms
step:1469/2330 train_time:90314ms step_avg:61.48ms
step:1470/2330 train_time:90377ms step_avg:61.48ms
step:1471/2330 train_time:90437ms step_avg:61.48ms
step:1472/2330 train_time:90501ms step_avg:61.48ms
step:1473/2330 train_time:90561ms step_avg:61.48ms
step:1474/2330 train_time:90624ms step_avg:61.48ms
step:1475/2330 train_time:90684ms step_avg:61.48ms
step:1476/2330 train_time:90748ms step_avg:61.48ms
step:1477/2330 train_time:90809ms step_avg:61.48ms
step:1478/2330 train_time:90873ms step_avg:61.48ms
step:1479/2330 train_time:90933ms step_avg:61.48ms
step:1480/2330 train_time:90996ms step_avg:61.48ms
step:1481/2330 train_time:91056ms step_avg:61.48ms
step:1482/2330 train_time:91119ms step_avg:61.48ms
step:1483/2330 train_time:91179ms step_avg:61.48ms
step:1484/2330 train_time:91242ms step_avg:61.48ms
step:1485/2330 train_time:91302ms step_avg:61.48ms
step:1486/2330 train_time:91365ms step_avg:61.48ms
step:1487/2330 train_time:91426ms step_avg:61.48ms
step:1488/2330 train_time:91489ms step_avg:61.48ms
step:1489/2330 train_time:91551ms step_avg:61.48ms
step:1490/2330 train_time:91614ms step_avg:61.49ms
step:1491/2330 train_time:91674ms step_avg:61.48ms
step:1492/2330 train_time:91738ms step_avg:61.49ms
step:1493/2330 train_time:91797ms step_avg:61.49ms
step:1494/2330 train_time:91861ms step_avg:61.49ms
step:1495/2330 train_time:91921ms step_avg:61.49ms
step:1496/2330 train_time:91984ms step_avg:61.49ms
step:1497/2330 train_time:92044ms step_avg:61.49ms
step:1498/2330 train_time:92108ms step_avg:61.49ms
step:1499/2330 train_time:92168ms step_avg:61.49ms
step:1500/2330 train_time:92232ms step_avg:61.49ms
step:1500/2330 val_loss:3.5013 train_time:92297ms step_avg:61.53ms
step:1501/2330 train_time:92319ms step_avg:61.50ms
step:1502/2330 train_time:92359ms step_avg:61.49ms
step:1503/2330 train_time:92423ms step_avg:61.49ms
step:1504/2330 train_time:92488ms step_avg:61.49ms
step:1505/2330 train_time:92549ms step_avg:61.49ms
step:1506/2330 train_time:92612ms step_avg:61.50ms
step:1507/2330 train_time:92672ms step_avg:61.49ms
step:1508/2330 train_time:92735ms step_avg:61.50ms
step:1509/2330 train_time:92795ms step_avg:61.49ms
step:1510/2330 train_time:92857ms step_avg:61.49ms
step:1511/2330 train_time:92916ms step_avg:61.49ms
step:1512/2330 train_time:92978ms step_avg:61.49ms
step:1513/2330 train_time:93037ms step_avg:61.49ms
step:1514/2330 train_time:93100ms step_avg:61.49ms
step:1515/2330 train_time:93159ms step_avg:61.49ms
step:1516/2330 train_time:93223ms step_avg:61.49ms
step:1517/2330 train_time:93285ms step_avg:61.49ms
step:1518/2330 train_time:93349ms step_avg:61.49ms
step:1519/2330 train_time:93411ms step_avg:61.49ms
step:1520/2330 train_time:93476ms step_avg:61.50ms
step:1521/2330 train_time:93537ms step_avg:61.50ms
step:1522/2330 train_time:93600ms step_avg:61.50ms
step:1523/2330 train_time:93661ms step_avg:61.50ms
step:1524/2330 train_time:93724ms step_avg:61.50ms
step:1525/2330 train_time:93784ms step_avg:61.50ms
step:1526/2330 train_time:93847ms step_avg:61.50ms
step:1527/2330 train_time:93907ms step_avg:61.50ms
step:1528/2330 train_time:93969ms step_avg:61.50ms
step:1529/2330 train_time:94029ms step_avg:61.50ms
step:1530/2330 train_time:94092ms step_avg:61.50ms
step:1531/2330 train_time:94153ms step_avg:61.50ms
step:1532/2330 train_time:94217ms step_avg:61.50ms
step:1533/2330 train_time:94278ms step_avg:61.50ms
step:1534/2330 train_time:94341ms step_avg:61.50ms
step:1535/2330 train_time:94403ms step_avg:61.50ms
step:1536/2330 train_time:94466ms step_avg:61.50ms
step:1537/2330 train_time:94527ms step_avg:61.50ms
step:1538/2330 train_time:94592ms step_avg:61.50ms
step:1539/2330 train_time:94653ms step_avg:61.50ms
step:1540/2330 train_time:94718ms step_avg:61.50ms
step:1541/2330 train_time:94780ms step_avg:61.51ms
step:1542/2330 train_time:94843ms step_avg:61.51ms
step:1543/2330 train_time:94904ms step_avg:61.51ms
step:1544/2330 train_time:94967ms step_avg:61.51ms
step:1545/2330 train_time:95027ms step_avg:61.51ms
step:1546/2330 train_time:95090ms step_avg:61.51ms
step:1547/2330 train_time:95151ms step_avg:61.51ms
step:1548/2330 train_time:95215ms step_avg:61.51ms
step:1549/2330 train_time:95276ms step_avg:61.51ms
step:1550/2330 train_time:95341ms step_avg:61.51ms
step:1551/2330 train_time:95401ms step_avg:61.51ms
step:1552/2330 train_time:95465ms step_avg:61.51ms
step:1553/2330 train_time:95525ms step_avg:61.51ms
step:1554/2330 train_time:95589ms step_avg:61.51ms
step:1555/2330 train_time:95650ms step_avg:61.51ms
step:1556/2330 train_time:95715ms step_avg:61.51ms
step:1557/2330 train_time:95776ms step_avg:61.51ms
step:1558/2330 train_time:95841ms step_avg:61.52ms
step:1559/2330 train_time:95901ms step_avg:61.51ms
step:1560/2330 train_time:95964ms step_avg:61.52ms
step:1561/2330 train_time:96024ms step_avg:61.51ms
step:1562/2330 train_time:96087ms step_avg:61.52ms
step:1563/2330 train_time:96148ms step_avg:61.52ms
step:1564/2330 train_time:96212ms step_avg:61.52ms
step:1565/2330 train_time:96273ms step_avg:61.52ms
step:1566/2330 train_time:96337ms step_avg:61.52ms
step:1567/2330 train_time:96398ms step_avg:61.52ms
step:1568/2330 train_time:96462ms step_avg:61.52ms
step:1569/2330 train_time:96522ms step_avg:61.52ms
step:1570/2330 train_time:96587ms step_avg:61.52ms
step:1571/2330 train_time:96648ms step_avg:61.52ms
step:1572/2330 train_time:96712ms step_avg:61.52ms
step:1573/2330 train_time:96774ms step_avg:61.52ms
step:1574/2330 train_time:96838ms step_avg:61.52ms
step:1575/2330 train_time:96899ms step_avg:61.52ms
step:1576/2330 train_time:96962ms step_avg:61.52ms
step:1577/2330 train_time:97022ms step_avg:61.52ms
step:1578/2330 train_time:97086ms step_avg:61.52ms
step:1579/2330 train_time:97147ms step_avg:61.52ms
step:1580/2330 train_time:97210ms step_avg:61.53ms
step:1581/2330 train_time:97270ms step_avg:61.52ms
step:1582/2330 train_time:97335ms step_avg:61.53ms
step:1583/2330 train_time:97396ms step_avg:61.53ms
step:1584/2330 train_time:97460ms step_avg:61.53ms
step:1585/2330 train_time:97520ms step_avg:61.53ms
step:1586/2330 train_time:97584ms step_avg:61.53ms
step:1587/2330 train_time:97645ms step_avg:61.53ms
step:1588/2330 train_time:97708ms step_avg:61.53ms
step:1589/2330 train_time:97769ms step_avg:61.53ms
step:1590/2330 train_time:97834ms step_avg:61.53ms
step:1591/2330 train_time:97896ms step_avg:61.53ms
step:1592/2330 train_time:97959ms step_avg:61.53ms
step:1593/2330 train_time:98020ms step_avg:61.53ms
step:1594/2330 train_time:98084ms step_avg:61.53ms
step:1595/2330 train_time:98144ms step_avg:61.53ms
step:1596/2330 train_time:98208ms step_avg:61.53ms
step:1597/2330 train_time:98268ms step_avg:61.53ms
step:1598/2330 train_time:98332ms step_avg:61.53ms
step:1599/2330 train_time:98393ms step_avg:61.53ms
step:1600/2330 train_time:98457ms step_avg:61.54ms
step:1601/2330 train_time:98518ms step_avg:61.54ms
step:1602/2330 train_time:98582ms step_avg:61.54ms
step:1603/2330 train_time:98643ms step_avg:61.54ms
step:1604/2330 train_time:98706ms step_avg:61.54ms
step:1605/2330 train_time:98767ms step_avg:61.54ms
step:1606/2330 train_time:98830ms step_avg:61.54ms
step:1607/2330 train_time:98891ms step_avg:61.54ms
step:1608/2330 train_time:98956ms step_avg:61.54ms
step:1609/2330 train_time:99017ms step_avg:61.54ms
step:1610/2330 train_time:99081ms step_avg:61.54ms
step:1611/2330 train_time:99141ms step_avg:61.54ms
step:1612/2330 train_time:99205ms step_avg:61.54ms
step:1613/2330 train_time:99265ms step_avg:61.54ms
step:1614/2330 train_time:99329ms step_avg:61.54ms
step:1615/2330 train_time:99390ms step_avg:61.54ms
step:1616/2330 train_time:99454ms step_avg:61.54ms
step:1617/2330 train_time:99515ms step_avg:61.54ms
step:1618/2330 train_time:99579ms step_avg:61.54ms
step:1619/2330 train_time:99640ms step_avg:61.54ms
step:1620/2330 train_time:99703ms step_avg:61.55ms
step:1621/2330 train_time:99765ms step_avg:61.55ms
step:1622/2330 train_time:99828ms step_avg:61.55ms
step:1623/2330 train_time:99889ms step_avg:61.55ms
step:1624/2330 train_time:99953ms step_avg:61.55ms
step:1625/2330 train_time:100015ms step_avg:61.55ms
step:1626/2330 train_time:100079ms step_avg:61.55ms
step:1627/2330 train_time:100140ms step_avg:61.55ms
step:1628/2330 train_time:100204ms step_avg:61.55ms
step:1629/2330 train_time:100265ms step_avg:61.55ms
step:1630/2330 train_time:100328ms step_avg:61.55ms
step:1631/2330 train_time:100388ms step_avg:61.55ms
step:1632/2330 train_time:100452ms step_avg:61.55ms
step:1633/2330 train_time:100513ms step_avg:61.55ms
step:1634/2330 train_time:100578ms step_avg:61.55ms
step:1635/2330 train_time:100639ms step_avg:61.55ms
step:1636/2330 train_time:100703ms step_avg:61.55ms
step:1637/2330 train_time:100764ms step_avg:61.55ms
step:1638/2330 train_time:100828ms step_avg:61.56ms
step:1639/2330 train_time:100888ms step_avg:61.55ms
step:1640/2330 train_time:100952ms step_avg:61.56ms
step:1641/2330 train_time:101014ms step_avg:61.56ms
step:1642/2330 train_time:101078ms step_avg:61.56ms
step:1643/2330 train_time:101139ms step_avg:61.56ms
step:1644/2330 train_time:101203ms step_avg:61.56ms
step:1645/2330 train_time:101263ms step_avg:61.56ms
step:1646/2330 train_time:101327ms step_avg:61.56ms
step:1647/2330 train_time:101388ms step_avg:61.56ms
step:1648/2330 train_time:101451ms step_avg:61.56ms
step:1649/2330 train_time:101512ms step_avg:61.56ms
step:1650/2330 train_time:101575ms step_avg:61.56ms
step:1651/2330 train_time:101636ms step_avg:61.56ms
step:1652/2330 train_time:101700ms step_avg:61.56ms
step:1653/2330 train_time:101761ms step_avg:61.56ms
step:1654/2330 train_time:101824ms step_avg:61.56ms
step:1655/2330 train_time:101885ms step_avg:61.56ms
step:1656/2330 train_time:101948ms step_avg:61.56ms
step:1657/2330 train_time:102009ms step_avg:61.56ms
step:1658/2330 train_time:102072ms step_avg:61.56ms
step:1659/2330 train_time:102133ms step_avg:61.56ms
step:1660/2330 train_time:102197ms step_avg:61.56ms
step:1661/2330 train_time:102258ms step_avg:61.56ms
step:1662/2330 train_time:102321ms step_avg:61.57ms
step:1663/2330 train_time:102382ms step_avg:61.56ms
step:1664/2330 train_time:102446ms step_avg:61.57ms
step:1665/2330 train_time:102507ms step_avg:61.57ms
step:1666/2330 train_time:102569ms step_avg:61.57ms
step:1667/2330 train_time:102630ms step_avg:61.57ms
step:1668/2330 train_time:102694ms step_avg:61.57ms
step:1669/2330 train_time:102756ms step_avg:61.57ms
step:1670/2330 train_time:102819ms step_avg:61.57ms
step:1671/2330 train_time:102879ms step_avg:61.57ms
step:1672/2330 train_time:102943ms step_avg:61.57ms
step:1673/2330 train_time:103004ms step_avg:61.57ms
step:1674/2330 train_time:103067ms step_avg:61.57ms
step:1675/2330 train_time:103128ms step_avg:61.57ms
step:1676/2330 train_time:103192ms step_avg:61.57ms
step:1677/2330 train_time:103252ms step_avg:61.57ms
step:1678/2330 train_time:103317ms step_avg:61.57ms
step:1679/2330 train_time:103377ms step_avg:61.57ms
step:1680/2330 train_time:103441ms step_avg:61.57ms
step:1681/2330 train_time:103501ms step_avg:61.57ms
step:1682/2330 train_time:103565ms step_avg:61.57ms
step:1683/2330 train_time:103626ms step_avg:61.57ms
step:1684/2330 train_time:103689ms step_avg:61.57ms
step:1685/2330 train_time:103749ms step_avg:61.57ms
step:1686/2330 train_time:103813ms step_avg:61.57ms
step:1687/2330 train_time:103875ms step_avg:61.57ms
step:1688/2330 train_time:103939ms step_avg:61.58ms
step:1689/2330 train_time:103999ms step_avg:61.57ms
step:1690/2330 train_time:104062ms step_avg:61.58ms
step:1691/2330 train_time:104123ms step_avg:61.57ms
step:1692/2330 train_time:104187ms step_avg:61.58ms
step:1693/2330 train_time:104248ms step_avg:61.58ms
step:1694/2330 train_time:104312ms step_avg:61.58ms
step:1695/2330 train_time:104373ms step_avg:61.58ms
step:1696/2330 train_time:104436ms step_avg:61.58ms
step:1697/2330 train_time:104497ms step_avg:61.58ms
step:1698/2330 train_time:104560ms step_avg:61.58ms
step:1699/2330 train_time:104620ms step_avg:61.58ms
step:1700/2330 train_time:104684ms step_avg:61.58ms
step:1701/2330 train_time:104745ms step_avg:61.58ms
step:1702/2330 train_time:104809ms step_avg:61.58ms
step:1703/2330 train_time:104870ms step_avg:61.58ms
step:1704/2330 train_time:104934ms step_avg:61.58ms
step:1705/2330 train_time:104995ms step_avg:61.58ms
step:1706/2330 train_time:105059ms step_avg:61.58ms
step:1707/2330 train_time:105120ms step_avg:61.58ms
step:1708/2330 train_time:105184ms step_avg:61.58ms
step:1709/2330 train_time:105244ms step_avg:61.58ms
step:1710/2330 train_time:105307ms step_avg:61.58ms
step:1711/2330 train_time:105368ms step_avg:61.58ms
step:1712/2330 train_time:105432ms step_avg:61.58ms
step:1713/2330 train_time:105494ms step_avg:61.58ms
step:1714/2330 train_time:105557ms step_avg:61.59ms
step:1715/2330 train_time:105619ms step_avg:61.59ms
step:1716/2330 train_time:105683ms step_avg:61.59ms
step:1717/2330 train_time:105744ms step_avg:61.59ms
step:1718/2330 train_time:105807ms step_avg:61.59ms
step:1719/2330 train_time:105868ms step_avg:61.59ms
step:1720/2330 train_time:105932ms step_avg:61.59ms
step:1721/2330 train_time:105992ms step_avg:61.59ms
step:1722/2330 train_time:106056ms step_avg:61.59ms
step:1723/2330 train_time:106117ms step_avg:61.59ms
step:1724/2330 train_time:106181ms step_avg:61.59ms
step:1725/2330 train_time:106241ms step_avg:61.59ms
step:1726/2330 train_time:106304ms step_avg:61.59ms
step:1727/2330 train_time:106365ms step_avg:61.59ms
step:1728/2330 train_time:106429ms step_avg:61.59ms
step:1729/2330 train_time:106490ms step_avg:61.59ms
step:1730/2330 train_time:106554ms step_avg:61.59ms
step:1731/2330 train_time:106615ms step_avg:61.59ms
step:1732/2330 train_time:106679ms step_avg:61.59ms
step:1733/2330 train_time:106739ms step_avg:61.59ms
step:1734/2330 train_time:106803ms step_avg:61.59ms
step:1735/2330 train_time:106864ms step_avg:61.59ms
step:1736/2330 train_time:106928ms step_avg:61.59ms
step:1737/2330 train_time:106988ms step_avg:61.59ms
step:1738/2330 train_time:107053ms step_avg:61.60ms
step:1739/2330 train_time:107114ms step_avg:61.60ms
step:1740/2330 train_time:107179ms step_avg:61.60ms
step:1741/2330 train_time:107239ms step_avg:61.60ms
step:1742/2330 train_time:107302ms step_avg:61.60ms
step:1743/2330 train_time:107363ms step_avg:61.60ms
step:1744/2330 train_time:107427ms step_avg:61.60ms
step:1745/2330 train_time:107487ms step_avg:61.60ms
step:1746/2330 train_time:107551ms step_avg:61.60ms
step:1747/2330 train_time:107612ms step_avg:61.60ms
step:1748/2330 train_time:107676ms step_avg:61.60ms
step:1749/2330 train_time:107737ms step_avg:61.60ms
step:1750/2330 train_time:107801ms step_avg:61.60ms
step:1750/2330 val_loss:3.4612 train_time:107866ms step_avg:61.64ms
step:1751/2330 train_time:107889ms step_avg:61.62ms
step:1752/2330 train_time:107928ms step_avg:61.60ms
step:1753/2330 train_time:107996ms step_avg:61.61ms
step:1754/2330 train_time:108062ms step_avg:61.61ms
step:1755/2330 train_time:108123ms step_avg:61.61ms
step:1756/2330 train_time:108188ms step_avg:61.61ms
step:1757/2330 train_time:108247ms step_avg:61.61ms
step:1758/2330 train_time:108310ms step_avg:61.61ms
step:1759/2330 train_time:108369ms step_avg:61.61ms
step:1760/2330 train_time:108432ms step_avg:61.61ms
step:1761/2330 train_time:108491ms step_avg:61.61ms
step:1762/2330 train_time:108554ms step_avg:61.61ms
step:1763/2330 train_time:108613ms step_avg:61.61ms
step:1764/2330 train_time:108676ms step_avg:61.61ms
step:1765/2330 train_time:108736ms step_avg:61.61ms
step:1766/2330 train_time:108801ms step_avg:61.61ms
step:1767/2330 train_time:108864ms step_avg:61.61ms
step:1768/2330 train_time:108928ms step_avg:61.61ms
step:1769/2330 train_time:108990ms step_avg:61.61ms
step:1770/2330 train_time:109054ms step_avg:61.61ms
step:1771/2330 train_time:109116ms step_avg:61.61ms
step:1772/2330 train_time:109180ms step_avg:61.61ms
step:1773/2330 train_time:109240ms step_avg:61.61ms
step:1774/2330 train_time:109304ms step_avg:61.61ms
step:1775/2330 train_time:109365ms step_avg:61.61ms
step:1776/2330 train_time:109428ms step_avg:61.61ms
step:1777/2330 train_time:109488ms step_avg:61.61ms
step:1778/2330 train_time:109551ms step_avg:61.61ms
step:1779/2330 train_time:109611ms step_avg:61.61ms
step:1780/2330 train_time:109674ms step_avg:61.61ms
step:1781/2330 train_time:109734ms step_avg:61.61ms
step:1782/2330 train_time:109797ms step_avg:61.61ms
step:1783/2330 train_time:109859ms step_avg:61.61ms
step:1784/2330 train_time:109923ms step_avg:61.62ms
step:1785/2330 train_time:109984ms step_avg:61.62ms
step:1786/2330 train_time:110049ms step_avg:61.62ms
step:1787/2330 train_time:110111ms step_avg:61.62ms
step:1788/2330 train_time:110175ms step_avg:61.62ms
step:1789/2330 train_time:110235ms step_avg:61.62ms
step:1790/2330 train_time:110299ms step_avg:61.62ms
step:1791/2330 train_time:110359ms step_avg:61.62ms
step:1792/2330 train_time:110422ms step_avg:61.62ms
step:1793/2330 train_time:110483ms step_avg:61.62ms
step:1794/2330 train_time:110546ms step_avg:61.62ms
step:1795/2330 train_time:110606ms step_avg:61.62ms
step:1796/2330 train_time:110669ms step_avg:61.62ms
step:1797/2330 train_time:110730ms step_avg:61.62ms
step:1798/2330 train_time:110794ms step_avg:61.62ms
step:1799/2330 train_time:110854ms step_avg:61.62ms
step:1800/2330 train_time:110918ms step_avg:61.62ms
step:1801/2330 train_time:110979ms step_avg:61.62ms
step:1802/2330 train_time:111044ms step_avg:61.62ms
step:1803/2330 train_time:111105ms step_avg:61.62ms
step:1804/2330 train_time:111169ms step_avg:61.62ms
step:1805/2330 train_time:111230ms step_avg:61.62ms
step:1806/2330 train_time:111293ms step_avg:61.62ms
step:1807/2330 train_time:111354ms step_avg:61.62ms
step:1808/2330 train_time:111418ms step_avg:61.63ms
step:1809/2330 train_time:111479ms step_avg:61.62ms
step:1810/2330 train_time:111543ms step_avg:61.63ms
step:1811/2330 train_time:111603ms step_avg:61.62ms
step:1812/2330 train_time:111666ms step_avg:61.63ms
step:1813/2330 train_time:111726ms step_avg:61.63ms
step:1814/2330 train_time:111790ms step_avg:61.63ms
step:1815/2330 train_time:111850ms step_avg:61.63ms
step:1816/2330 train_time:111914ms step_avg:61.63ms
step:1817/2330 train_time:111974ms step_avg:61.63ms
step:1818/2330 train_time:112038ms step_avg:61.63ms
step:1819/2330 train_time:112099ms step_avg:61.63ms
step:1820/2330 train_time:112162ms step_avg:61.63ms
step:1821/2330 train_time:112223ms step_avg:61.63ms
step:1822/2330 train_time:112287ms step_avg:61.63ms
step:1823/2330 train_time:112348ms step_avg:61.63ms
step:1824/2330 train_time:112411ms step_avg:61.63ms
step:1825/2330 train_time:112472ms step_avg:61.63ms
step:1826/2330 train_time:112536ms step_avg:61.63ms
step:1827/2330 train_time:112596ms step_avg:61.63ms
step:1828/2330 train_time:112660ms step_avg:61.63ms
step:1829/2330 train_time:112721ms step_avg:61.63ms
step:1830/2330 train_time:112784ms step_avg:61.63ms
step:1831/2330 train_time:112845ms step_avg:61.63ms
step:1832/2330 train_time:112908ms step_avg:61.63ms
step:1833/2330 train_time:112969ms step_avg:61.63ms
step:1834/2330 train_time:113032ms step_avg:61.63ms
step:1835/2330 train_time:113093ms step_avg:61.63ms
step:1836/2330 train_time:113156ms step_avg:61.63ms
step:1837/2330 train_time:113217ms step_avg:61.63ms
step:1838/2330 train_time:113281ms step_avg:61.63ms
step:1839/2330 train_time:113342ms step_avg:61.63ms
step:1840/2330 train_time:113406ms step_avg:61.63ms
step:1841/2330 train_time:113466ms step_avg:61.63ms
step:1842/2330 train_time:113529ms step_avg:61.63ms
step:1843/2330 train_time:113590ms step_avg:61.63ms
step:1844/2330 train_time:113653ms step_avg:61.63ms
step:1845/2330 train_time:113713ms step_avg:61.63ms
step:1846/2330 train_time:113777ms step_avg:61.63ms
step:1847/2330 train_time:113839ms step_avg:61.63ms
step:1848/2330 train_time:113902ms step_avg:61.64ms
step:1849/2330 train_time:113964ms step_avg:61.64ms
step:1850/2330 train_time:114027ms step_avg:61.64ms
step:1851/2330 train_time:114087ms step_avg:61.64ms
step:1852/2330 train_time:114151ms step_avg:61.64ms
step:1853/2330 train_time:114212ms step_avg:61.64ms
step:1854/2330 train_time:114276ms step_avg:61.64ms
step:1855/2330 train_time:114336ms step_avg:61.64ms
step:1856/2330 train_time:114401ms step_avg:61.64ms
step:1857/2330 train_time:114461ms step_avg:61.64ms
step:1858/2330 train_time:114525ms step_avg:61.64ms
step:1859/2330 train_time:114586ms step_avg:61.64ms
step:1860/2330 train_time:114649ms step_avg:61.64ms
step:1861/2330 train_time:114710ms step_avg:61.64ms
step:1862/2330 train_time:114773ms step_avg:61.64ms
step:1863/2330 train_time:114834ms step_avg:61.64ms
step:1864/2330 train_time:114897ms step_avg:61.64ms
step:1865/2330 train_time:114958ms step_avg:61.64ms
step:1866/2330 train_time:115023ms step_avg:61.64ms
step:1867/2330 train_time:115086ms step_avg:61.64ms
step:1868/2330 train_time:115149ms step_avg:61.64ms
step:1869/2330 train_time:115209ms step_avg:61.64ms
step:1870/2330 train_time:115272ms step_avg:61.64ms
step:1871/2330 train_time:115333ms step_avg:61.64ms
step:1872/2330 train_time:115396ms step_avg:61.64ms
step:1873/2330 train_time:115457ms step_avg:61.64ms
step:1874/2330 train_time:115521ms step_avg:61.64ms
step:1875/2330 train_time:115582ms step_avg:61.64ms
step:1876/2330 train_time:115646ms step_avg:61.64ms
step:1877/2330 train_time:115705ms step_avg:61.64ms
step:1878/2330 train_time:115769ms step_avg:61.64ms
step:1879/2330 train_time:115830ms step_avg:61.64ms
step:1880/2330 train_time:115894ms step_avg:61.65ms
step:1881/2330 train_time:115954ms step_avg:61.64ms
step:1882/2330 train_time:116018ms step_avg:61.65ms
step:1883/2330 train_time:116079ms step_avg:61.65ms
step:1884/2330 train_time:116143ms step_avg:61.65ms
step:1885/2330 train_time:116205ms step_avg:61.65ms
step:1886/2330 train_time:116268ms step_avg:61.65ms
step:1887/2330 train_time:116328ms step_avg:61.65ms
step:1888/2330 train_time:116392ms step_avg:61.65ms
step:1889/2330 train_time:116453ms step_avg:61.65ms
step:1890/2330 train_time:116515ms step_avg:61.65ms
step:1891/2330 train_time:116577ms step_avg:61.65ms
step:1892/2330 train_time:116641ms step_avg:61.65ms
step:1893/2330 train_time:116701ms step_avg:61.65ms
step:1894/2330 train_time:116766ms step_avg:61.65ms
step:1895/2330 train_time:116826ms step_avg:61.65ms
step:1896/2330 train_time:116890ms step_avg:61.65ms
step:1897/2330 train_time:116951ms step_avg:61.65ms
step:1898/2330 train_time:117014ms step_avg:61.65ms
step:1899/2330 train_time:117075ms step_avg:61.65ms
step:1900/2330 train_time:117139ms step_avg:61.65ms
step:1901/2330 train_time:117200ms step_avg:61.65ms
step:1902/2330 train_time:117263ms step_avg:61.65ms
step:1903/2330 train_time:117324ms step_avg:61.65ms
step:1904/2330 train_time:117388ms step_avg:61.65ms
step:1905/2330 train_time:117448ms step_avg:61.65ms
step:1906/2330 train_time:117512ms step_avg:61.65ms
step:1907/2330 train_time:117572ms step_avg:61.65ms
step:1908/2330 train_time:117636ms step_avg:61.65ms
step:1909/2330 train_time:117696ms step_avg:61.65ms
step:1910/2330 train_time:117761ms step_avg:61.65ms
step:1911/2330 train_time:117822ms step_avg:61.65ms
step:1912/2330 train_time:117885ms step_avg:61.66ms
step:1913/2330 train_time:117946ms step_avg:61.65ms
step:1914/2330 train_time:118009ms step_avg:61.66ms
step:1915/2330 train_time:118070ms step_avg:61.66ms
step:1916/2330 train_time:118133ms step_avg:61.66ms
step:1917/2330 train_time:118194ms step_avg:61.66ms
step:1918/2330 train_time:118257ms step_avg:61.66ms
step:1919/2330 train_time:118318ms step_avg:61.66ms
step:1920/2330 train_time:118382ms step_avg:61.66ms
step:1921/2330 train_time:118442ms step_avg:61.66ms
step:1922/2330 train_time:118506ms step_avg:61.66ms
step:1923/2330 train_time:118566ms step_avg:61.66ms
step:1924/2330 train_time:118630ms step_avg:61.66ms
step:1925/2330 train_time:118691ms step_avg:61.66ms
step:1926/2330 train_time:118755ms step_avg:61.66ms
step:1927/2330 train_time:118815ms step_avg:61.66ms
step:1928/2330 train_time:118879ms step_avg:61.66ms
step:1929/2330 train_time:118940ms step_avg:61.66ms
step:1930/2330 train_time:119004ms step_avg:61.66ms
step:1931/2330 train_time:119065ms step_avg:61.66ms
step:1932/2330 train_time:119129ms step_avg:61.66ms
step:1933/2330 train_time:119189ms step_avg:61.66ms
step:1934/2330 train_time:119253ms step_avg:61.66ms
step:1935/2330 train_time:119313ms step_avg:61.66ms
step:1936/2330 train_time:119377ms step_avg:61.66ms
step:1937/2330 train_time:119437ms step_avg:61.66ms
step:1938/2330 train_time:119501ms step_avg:61.66ms
step:1939/2330 train_time:119562ms step_avg:61.66ms
step:1940/2330 train_time:119626ms step_avg:61.66ms
step:1941/2330 train_time:119686ms step_avg:61.66ms
step:1942/2330 train_time:119750ms step_avg:61.66ms
step:1943/2330 train_time:119811ms step_avg:61.66ms
step:1944/2330 train_time:119874ms step_avg:61.66ms
step:1945/2330 train_time:119935ms step_avg:61.66ms
step:1946/2330 train_time:119998ms step_avg:61.66ms
step:1947/2330 train_time:120059ms step_avg:61.66ms
step:1948/2330 train_time:120123ms step_avg:61.66ms
step:1949/2330 train_time:120184ms step_avg:61.66ms
step:1950/2330 train_time:120247ms step_avg:61.67ms
step:1951/2330 train_time:120307ms step_avg:61.66ms
step:1952/2330 train_time:120370ms step_avg:61.66ms
step:1953/2330 train_time:120431ms step_avg:61.66ms
step:1954/2330 train_time:120494ms step_avg:61.67ms
step:1955/2330 train_time:120554ms step_avg:61.66ms
step:1956/2330 train_time:120618ms step_avg:61.67ms
step:1957/2330 train_time:120679ms step_avg:61.67ms
step:1958/2330 train_time:120743ms step_avg:61.67ms
step:1959/2330 train_time:120804ms step_avg:61.67ms
step:1960/2330 train_time:120867ms step_avg:61.67ms
step:1961/2330 train_time:120927ms step_avg:61.67ms
step:1962/2330 train_time:120991ms step_avg:61.67ms
step:1963/2330 train_time:121051ms step_avg:61.67ms
step:1964/2330 train_time:121114ms step_avg:61.67ms
step:1965/2330 train_time:121175ms step_avg:61.67ms
step:1966/2330 train_time:121239ms step_avg:61.67ms
step:1967/2330 train_time:121300ms step_avg:61.67ms
step:1968/2330 train_time:121364ms step_avg:61.67ms
step:1969/2330 train_time:121424ms step_avg:61.67ms
step:1970/2330 train_time:121488ms step_avg:61.67ms
step:1971/2330 train_time:121548ms step_avg:61.67ms
step:1972/2330 train_time:121611ms step_avg:61.67ms
step:1973/2330 train_time:121672ms step_avg:61.67ms
step:1974/2330 train_time:121735ms step_avg:61.67ms
step:1975/2330 train_time:121796ms step_avg:61.67ms
step:1976/2330 train_time:121859ms step_avg:61.67ms
step:1977/2330 train_time:121920ms step_avg:61.67ms
step:1978/2330 train_time:121984ms step_avg:61.67ms
step:1979/2330 train_time:122045ms step_avg:61.67ms
step:1980/2330 train_time:122108ms step_avg:61.67ms
step:1981/2330 train_time:122169ms step_avg:61.67ms
step:1982/2330 train_time:122232ms step_avg:61.67ms
step:1983/2330 train_time:122293ms step_avg:61.67ms
step:1984/2330 train_time:122356ms step_avg:61.67ms
step:1985/2330 train_time:122417ms step_avg:61.67ms
step:1986/2330 train_time:122481ms step_avg:61.67ms
step:1987/2330 train_time:122542ms step_avg:61.67ms
step:1988/2330 train_time:122606ms step_avg:61.67ms
step:1989/2330 train_time:122666ms step_avg:61.67ms
step:1990/2330 train_time:122730ms step_avg:61.67ms
step:1991/2330 train_time:122791ms step_avg:61.67ms
step:1992/2330 train_time:122855ms step_avg:61.67ms
step:1993/2330 train_time:122916ms step_avg:61.67ms
step:1994/2330 train_time:122980ms step_avg:61.67ms
step:1995/2330 train_time:123041ms step_avg:61.67ms
step:1996/2330 train_time:123105ms step_avg:61.68ms
step:1997/2330 train_time:123166ms step_avg:61.68ms
step:1998/2330 train_time:123228ms step_avg:61.68ms
step:1999/2330 train_time:123289ms step_avg:61.68ms
step:2000/2330 train_time:123354ms step_avg:61.68ms
step:2000/2330 val_loss:3.4316 train_time:123418ms step_avg:61.71ms
step:2001/2330 train_time:123441ms step_avg:61.69ms
step:2002/2330 train_time:123482ms step_avg:61.68ms
step:2003/2330 train_time:123548ms step_avg:61.68ms
step:2004/2330 train_time:123614ms step_avg:61.68ms
step:2005/2330 train_time:123675ms step_avg:61.68ms
step:2006/2330 train_time:123738ms step_avg:61.68ms
step:2007/2330 train_time:123797ms step_avg:61.68ms
step:2008/2330 train_time:123861ms step_avg:61.68ms
step:2009/2330 train_time:123920ms step_avg:61.68ms
step:2010/2330 train_time:123983ms step_avg:61.68ms
step:2011/2330 train_time:124044ms step_avg:61.68ms
step:2012/2330 train_time:124106ms step_avg:61.68ms
step:2013/2330 train_time:124166ms step_avg:61.68ms
step:2014/2330 train_time:124229ms step_avg:61.68ms
step:2015/2330 train_time:124288ms step_avg:61.68ms
step:2016/2330 train_time:124351ms step_avg:61.68ms
step:2017/2330 train_time:124412ms step_avg:61.68ms
step:2018/2330 train_time:124478ms step_avg:61.68ms
step:2019/2330 train_time:124540ms step_avg:61.68ms
step:2020/2330 train_time:124605ms step_avg:61.69ms
step:2021/2330 train_time:124666ms step_avg:61.69ms
step:2022/2330 train_time:124730ms step_avg:61.69ms
step:2023/2330 train_time:124790ms step_avg:61.69ms
step:2024/2330 train_time:124854ms step_avg:61.69ms
step:2025/2330 train_time:124915ms step_avg:61.69ms
step:2026/2330 train_time:124978ms step_avg:61.69ms
step:2027/2330 train_time:125037ms step_avg:61.69ms
step:2028/2330 train_time:125101ms step_avg:61.69ms
step:2029/2330 train_time:125161ms step_avg:61.69ms
step:2030/2330 train_time:125224ms step_avg:61.69ms
step:2031/2330 train_time:125284ms step_avg:61.69ms
step:2032/2330 train_time:125347ms step_avg:61.69ms
step:2033/2330 train_time:125408ms step_avg:61.69ms
step:2034/2330 train_time:125473ms step_avg:61.69ms
step:2035/2330 train_time:125534ms step_avg:61.69ms
step:2036/2330 train_time:125598ms step_avg:61.69ms
step:2037/2330 train_time:125659ms step_avg:61.69ms
step:2038/2330 train_time:125724ms step_avg:61.69ms
step:2039/2330 train_time:125784ms step_avg:61.69ms
step:2040/2330 train_time:125848ms step_avg:61.69ms
step:2041/2330 train_time:125908ms step_avg:61.69ms
step:2042/2330 train_time:125972ms step_avg:61.69ms
step:2043/2330 train_time:126032ms step_avg:61.69ms
step:2044/2330 train_time:126096ms step_avg:61.69ms
step:2045/2330 train_time:126156ms step_avg:61.69ms
step:2046/2330 train_time:126219ms step_avg:61.69ms
step:2047/2330 train_time:126280ms step_avg:61.69ms
step:2048/2330 train_time:126343ms step_avg:61.69ms
step:2049/2330 train_time:126405ms step_avg:61.69ms
step:2050/2330 train_time:126469ms step_avg:61.69ms
step:2051/2330 train_time:126530ms step_avg:61.69ms
step:2052/2330 train_time:126594ms step_avg:61.69ms
step:2053/2330 train_time:126655ms step_avg:61.69ms
step:2054/2330 train_time:126720ms step_avg:61.69ms
step:2055/2330 train_time:126780ms step_avg:61.69ms
step:2056/2330 train_time:126844ms step_avg:61.69ms
step:2057/2330 train_time:126906ms step_avg:61.69ms
step:2058/2330 train_time:126970ms step_avg:61.70ms
step:2059/2330 train_time:127030ms step_avg:61.70ms
step:2060/2330 train_time:127093ms step_avg:61.70ms
step:2061/2330 train_time:127154ms step_avg:61.70ms
step:2062/2330 train_time:127218ms step_avg:61.70ms
step:2063/2330 train_time:127279ms step_avg:61.70ms
step:2064/2330 train_time:127342ms step_avg:61.70ms
step:2065/2330 train_time:127403ms step_avg:61.70ms
step:2066/2330 train_time:127468ms step_avg:61.70ms
step:2067/2330 train_time:127529ms step_avg:61.70ms
step:2068/2330 train_time:127592ms step_avg:61.70ms
step:2069/2330 train_time:127653ms step_avg:61.70ms
step:2070/2330 train_time:127716ms step_avg:61.70ms
step:2071/2330 train_time:127777ms step_avg:61.70ms
step:2072/2330 train_time:127841ms step_avg:61.70ms
step:2073/2330 train_time:127902ms step_avg:61.70ms
step:2074/2330 train_time:127966ms step_avg:61.70ms
step:2075/2330 train_time:128027ms step_avg:61.70ms
step:2076/2330 train_time:128091ms step_avg:61.70ms
step:2077/2330 train_time:128151ms step_avg:61.70ms
step:2078/2330 train_time:128215ms step_avg:61.70ms
step:2079/2330 train_time:128276ms step_avg:61.70ms
step:2080/2330 train_time:128339ms step_avg:61.70ms
step:2081/2330 train_time:128400ms step_avg:61.70ms
step:2082/2330 train_time:128464ms step_avg:61.70ms
step:2083/2330 train_time:128525ms step_avg:61.70ms
step:2084/2330 train_time:128589ms step_avg:61.70ms
step:2085/2330 train_time:128650ms step_avg:61.70ms
step:2086/2330 train_time:128714ms step_avg:61.70ms
step:2087/2330 train_time:128774ms step_avg:61.70ms
step:2088/2330 train_time:128837ms step_avg:61.70ms
step:2089/2330 train_time:128898ms step_avg:61.70ms
step:2090/2330 train_time:128962ms step_avg:61.70ms
step:2091/2330 train_time:129023ms step_avg:61.70ms
step:2092/2330 train_time:129087ms step_avg:61.71ms
step:2093/2330 train_time:129148ms step_avg:61.70ms
step:2094/2330 train_time:129212ms step_avg:61.71ms
step:2095/2330 train_time:129272ms step_avg:61.71ms
step:2096/2330 train_time:129335ms step_avg:61.71ms
step:2097/2330 train_time:129396ms step_avg:61.71ms
step:2098/2330 train_time:129459ms step_avg:61.71ms
step:2099/2330 train_time:129520ms step_avg:61.71ms
step:2100/2330 train_time:129584ms step_avg:61.71ms
step:2101/2330 train_time:129645ms step_avg:61.71ms
step:2102/2330 train_time:129709ms step_avg:61.71ms
step:2103/2330 train_time:129769ms step_avg:61.71ms
step:2104/2330 train_time:129833ms step_avg:61.71ms
step:2105/2330 train_time:129893ms step_avg:61.71ms
step:2106/2330 train_time:129957ms step_avg:61.71ms
step:2107/2330 train_time:130018ms step_avg:61.71ms
step:2108/2330 train_time:130082ms step_avg:61.71ms
step:2109/2330 train_time:130142ms step_avg:61.71ms
step:2110/2330 train_time:130207ms step_avg:61.71ms
step:2111/2330 train_time:130268ms step_avg:61.71ms
step:2112/2330 train_time:130332ms step_avg:61.71ms
step:2113/2330 train_time:130392ms step_avg:61.71ms
step:2114/2330 train_time:130455ms step_avg:61.71ms
step:2115/2330 train_time:130516ms step_avg:61.71ms
step:2116/2330 train_time:130579ms step_avg:61.71ms
step:2117/2330 train_time:130640ms step_avg:61.71ms
step:2118/2330 train_time:130705ms step_avg:61.71ms
step:2119/2330 train_time:130766ms step_avg:61.71ms
step:2120/2330 train_time:130829ms step_avg:61.71ms
step:2121/2330 train_time:130889ms step_avg:61.71ms
step:2122/2330 train_time:130953ms step_avg:61.71ms
step:2123/2330 train_time:131015ms step_avg:61.71ms
step:2124/2330 train_time:131079ms step_avg:61.71ms
step:2125/2330 train_time:131139ms step_avg:61.71ms
step:2126/2330 train_time:131203ms step_avg:61.71ms
step:2127/2330 train_time:131263ms step_avg:61.71ms
step:2128/2330 train_time:131328ms step_avg:61.71ms
step:2129/2330 train_time:131388ms step_avg:61.71ms
step:2130/2330 train_time:131452ms step_avg:61.71ms
step:2131/2330 train_time:131513ms step_avg:61.71ms
step:2132/2330 train_time:131576ms step_avg:61.72ms
step:2133/2330 train_time:131637ms step_avg:61.71ms
step:2134/2330 train_time:131701ms step_avg:61.72ms
step:2135/2330 train_time:131762ms step_avg:61.72ms
step:2136/2330 train_time:131826ms step_avg:61.72ms
step:2137/2330 train_time:131887ms step_avg:61.72ms
step:2138/2330 train_time:131950ms step_avg:61.72ms
step:2139/2330 train_time:132011ms step_avg:61.72ms
step:2140/2330 train_time:132075ms step_avg:61.72ms
step:2141/2330 train_time:132136ms step_avg:61.72ms
step:2142/2330 train_time:132199ms step_avg:61.72ms
step:2143/2330 train_time:132259ms step_avg:61.72ms
step:2144/2330 train_time:132323ms step_avg:61.72ms
step:2145/2330 train_time:132384ms step_avg:61.72ms
step:2146/2330 train_time:132448ms step_avg:61.72ms
step:2147/2330 train_time:132509ms step_avg:61.72ms
step:2148/2330 train_time:132573ms step_avg:61.72ms
step:2149/2330 train_time:132633ms step_avg:61.72ms
step:2150/2330 train_time:132696ms step_avg:61.72ms
step:2151/2330 train_time:132757ms step_avg:61.72ms
step:2152/2330 train_time:132821ms step_avg:61.72ms
step:2153/2330 train_time:132881ms step_avg:61.72ms
step:2154/2330 train_time:132946ms step_avg:61.72ms
step:2155/2330 train_time:133007ms step_avg:61.72ms
step:2156/2330 train_time:133070ms step_avg:61.72ms
step:2157/2330 train_time:133131ms step_avg:61.72ms
step:2158/2330 train_time:133194ms step_avg:61.72ms
step:2159/2330 train_time:133256ms step_avg:61.72ms
step:2160/2330 train_time:133319ms step_avg:61.72ms
step:2161/2330 train_time:133380ms step_avg:61.72ms
step:2162/2330 train_time:133444ms step_avg:61.72ms
step:2163/2330 train_time:133505ms step_avg:61.72ms
step:2164/2330 train_time:133569ms step_avg:61.72ms
step:2165/2330 train_time:133629ms step_avg:61.72ms
step:2166/2330 train_time:133693ms step_avg:61.72ms
step:2167/2330 train_time:133754ms step_avg:61.72ms
step:2168/2330 train_time:133817ms step_avg:61.72ms
step:2169/2330 train_time:133878ms step_avg:61.72ms
step:2170/2330 train_time:133941ms step_avg:61.72ms
step:2171/2330 train_time:134002ms step_avg:61.72ms
step:2172/2330 train_time:134066ms step_avg:61.72ms
step:2173/2330 train_time:134127ms step_avg:61.72ms
step:2174/2330 train_time:134190ms step_avg:61.72ms
step:2175/2330 train_time:134251ms step_avg:61.72ms
step:2176/2330 train_time:134314ms step_avg:61.73ms
step:2177/2330 train_time:134375ms step_avg:61.73ms
step:2178/2330 train_time:134439ms step_avg:61.73ms
step:2179/2330 train_time:134499ms step_avg:61.73ms
step:2180/2330 train_time:134563ms step_avg:61.73ms
step:2181/2330 train_time:134624ms step_avg:61.73ms
step:2182/2330 train_time:134688ms step_avg:61.73ms
step:2183/2330 train_time:134749ms step_avg:61.73ms
step:2184/2330 train_time:134813ms step_avg:61.73ms
step:2185/2330 train_time:134874ms step_avg:61.73ms
step:2186/2330 train_time:134937ms step_avg:61.73ms
step:2187/2330 train_time:134998ms step_avg:61.73ms
step:2188/2330 train_time:135062ms step_avg:61.73ms
step:2189/2330 train_time:135122ms step_avg:61.73ms
step:2190/2330 train_time:135186ms step_avg:61.73ms
step:2191/2330 train_time:135247ms step_avg:61.73ms
step:2192/2330 train_time:135311ms step_avg:61.73ms
step:2193/2330 train_time:135371ms step_avg:61.73ms
step:2194/2330 train_time:135434ms step_avg:61.73ms
step:2195/2330 train_time:135494ms step_avg:61.73ms
step:2196/2330 train_time:135558ms step_avg:61.73ms
step:2197/2330 train_time:135618ms step_avg:61.73ms
step:2198/2330 train_time:135682ms step_avg:61.73ms
step:2199/2330 train_time:135743ms step_avg:61.73ms
step:2200/2330 train_time:135808ms step_avg:61.73ms
step:2201/2330 train_time:135868ms step_avg:61.73ms
step:2202/2330 train_time:135932ms step_avg:61.73ms
step:2203/2330 train_time:135992ms step_avg:61.73ms
step:2204/2330 train_time:136055ms step_avg:61.73ms
step:2205/2330 train_time:136116ms step_avg:61.73ms
step:2206/2330 train_time:136180ms step_avg:61.73ms
step:2207/2330 train_time:136241ms step_avg:61.73ms
step:2208/2330 train_time:136305ms step_avg:61.73ms
step:2209/2330 train_time:136366ms step_avg:61.73ms
step:2210/2330 train_time:136430ms step_avg:61.73ms
step:2211/2330 train_time:136490ms step_avg:61.73ms
step:2212/2330 train_time:136554ms step_avg:61.73ms
step:2213/2330 train_time:136615ms step_avg:61.73ms
step:2214/2330 train_time:136678ms step_avg:61.73ms
step:2215/2330 train_time:136739ms step_avg:61.73ms
step:2216/2330 train_time:136804ms step_avg:61.73ms
step:2217/2330 train_time:136871ms step_avg:61.74ms
step:2218/2330 train_time:136929ms step_avg:61.74ms
step:2219/2330 train_time:136989ms step_avg:61.73ms
step:2220/2330 train_time:137052ms step_avg:61.74ms
step:2221/2330 train_time:137113ms step_avg:61.73ms
step:2222/2330 train_time:137177ms step_avg:61.74ms
step:2223/2330 train_time:137237ms step_avg:61.73ms
step:2224/2330 train_time:137300ms step_avg:61.74ms
step:2225/2330 train_time:137361ms step_avg:61.74ms
step:2226/2330 train_time:137425ms step_avg:61.74ms
step:2227/2330 train_time:137486ms step_avg:61.74ms
step:2228/2330 train_time:137549ms step_avg:61.74ms
step:2229/2330 train_time:137610ms step_avg:61.74ms
step:2230/2330 train_time:137674ms step_avg:61.74ms
step:2231/2330 train_time:137735ms step_avg:61.74ms
step:2232/2330 train_time:137798ms step_avg:61.74ms
step:2233/2330 train_time:137859ms step_avg:61.74ms
step:2234/2330 train_time:137923ms step_avg:61.74ms
step:2235/2330 train_time:137984ms step_avg:61.74ms
step:2236/2330 train_time:138048ms step_avg:61.74ms
step:2237/2330 train_time:138109ms step_avg:61.74ms
step:2238/2330 train_time:138172ms step_avg:61.74ms
step:2239/2330 train_time:138232ms step_avg:61.74ms
step:2240/2330 train_time:138295ms step_avg:61.74ms
step:2241/2330 train_time:138355ms step_avg:61.74ms
step:2242/2330 train_time:138419ms step_avg:61.74ms
step:2243/2330 train_time:138480ms step_avg:61.74ms
step:2244/2330 train_time:138544ms step_avg:61.74ms
step:2245/2330 train_time:138605ms step_avg:61.74ms
step:2246/2330 train_time:138669ms step_avg:61.74ms
step:2247/2330 train_time:138729ms step_avg:61.74ms
step:2248/2330 train_time:138792ms step_avg:61.74ms
step:2249/2330 train_time:138853ms step_avg:61.74ms
step:2250/2330 train_time:138916ms step_avg:61.74ms
step:2250/2330 val_loss:3.4063 train_time:138982ms step_avg:61.77ms
step:2251/2330 train_time:139007ms step_avg:61.75ms
step:2252/2330 train_time:139045ms step_avg:61.74ms
step:2253/2330 train_time:139111ms step_avg:61.74ms
step:2254/2330 train_time:139177ms step_avg:61.75ms
step:2255/2330 train_time:139239ms step_avg:61.75ms
step:2256/2330 train_time:139304ms step_avg:61.75ms
step:2257/2330 train_time:139364ms step_avg:61.75ms
step:2258/2330 train_time:139427ms step_avg:61.75ms
step:2259/2330 train_time:139487ms step_avg:61.75ms
step:2260/2330 train_time:139550ms step_avg:61.75ms
step:2261/2330 train_time:139609ms step_avg:61.75ms
step:2262/2330 train_time:139672ms step_avg:61.75ms
step:2263/2330 train_time:139732ms step_avg:61.75ms
step:2264/2330 train_time:139794ms step_avg:61.75ms
step:2265/2330 train_time:139854ms step_avg:61.75ms
step:2266/2330 train_time:139917ms step_avg:61.75ms
step:2267/2330 train_time:139978ms step_avg:61.75ms
step:2268/2330 train_time:140044ms step_avg:61.75ms
step:2269/2330 train_time:140106ms step_avg:61.75ms
step:2270/2330 train_time:140171ms step_avg:61.75ms
step:2271/2330 train_time:140233ms step_avg:61.75ms
step:2272/2330 train_time:140297ms step_avg:61.75ms
step:2273/2330 train_time:140358ms step_avg:61.75ms
step:2274/2330 train_time:140422ms step_avg:61.75ms
step:2275/2330 train_time:140482ms step_avg:61.75ms
step:2276/2330 train_time:140546ms step_avg:61.75ms
step:2277/2330 train_time:140607ms step_avg:61.75ms
step:2278/2330 train_time:140670ms step_avg:61.75ms
step:2279/2330 train_time:140730ms step_avg:61.75ms
step:2280/2330 train_time:140793ms step_avg:61.75ms
step:2281/2330 train_time:140853ms step_avg:61.75ms
step:2282/2330 train_time:140916ms step_avg:61.75ms
step:2283/2330 train_time:140977ms step_avg:61.75ms
step:2284/2330 train_time:141041ms step_avg:61.75ms
step:2285/2330 train_time:141102ms step_avg:61.75ms
step:2286/2330 train_time:141167ms step_avg:61.75ms
step:2287/2330 train_time:141229ms step_avg:61.75ms
step:2288/2330 train_time:141293ms step_avg:61.75ms
step:2289/2330 train_time:141354ms step_avg:61.75ms
step:2290/2330 train_time:141419ms step_avg:61.75ms
step:2291/2330 train_time:141479ms step_avg:61.75ms
step:2292/2330 train_time:141543ms step_avg:61.76ms
step:2293/2330 train_time:141603ms step_avg:61.75ms
step:2294/2330 train_time:141667ms step_avg:61.76ms
step:2295/2330 train_time:141727ms step_avg:61.75ms
step:2296/2330 train_time:141791ms step_avg:61.76ms
step:2297/2330 train_time:141851ms step_avg:61.75ms
step:2298/2330 train_time:141914ms step_avg:61.76ms
step:2299/2330 train_time:141974ms step_avg:61.75ms
step:2300/2330 train_time:142038ms step_avg:61.76ms
step:2301/2330 train_time:142099ms step_avg:61.76ms
step:2302/2330 train_time:142163ms step_avg:61.76ms
step:2303/2330 train_time:142225ms step_avg:61.76ms
step:2304/2330 train_time:142289ms step_avg:61.76ms
step:2305/2330 train_time:142350ms step_avg:61.76ms
step:2306/2330 train_time:142414ms step_avg:61.76ms
step:2307/2330 train_time:142474ms step_avg:61.76ms
step:2308/2330 train_time:142537ms step_avg:61.76ms
step:2309/2330 train_time:142597ms step_avg:61.76ms
step:2310/2330 train_time:142661ms step_avg:61.76ms
step:2311/2330 train_time:142721ms step_avg:61.76ms
step:2312/2330 train_time:142785ms step_avg:61.76ms
step:2313/2330 train_time:142845ms step_avg:61.76ms
step:2314/2330 train_time:142909ms step_avg:61.76ms
step:2315/2330 train_time:142969ms step_avg:61.76ms
step:2316/2330 train_time:143033ms step_avg:61.76ms
step:2317/2330 train_time:143094ms step_avg:61.76ms
step:2318/2330 train_time:143157ms step_avg:61.76ms
step:2319/2330 train_time:143218ms step_avg:61.76ms
step:2320/2330 train_time:143283ms step_avg:61.76ms
step:2321/2330 train_time:143345ms step_avg:61.76ms
step:2322/2330 train_time:143409ms step_avg:61.76ms
step:2323/2330 train_time:143469ms step_avg:61.76ms
step:2324/2330 train_time:143534ms step_avg:61.76ms
step:2325/2330 train_time:143595ms step_avg:61.76ms
step:2326/2330 train_time:143658ms step_avg:61.76ms
step:2327/2330 train_time:143719ms step_avg:61.76ms
step:2328/2330 train_time:143783ms step_avg:61.76ms
step:2329/2330 train_time:143844ms step_avg:61.76ms
step:2330/2330 train_time:143908ms step_avg:61.76ms
step:2330/2330 val_loss:3.3807 train_time:143974ms step_avg:61.79ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
