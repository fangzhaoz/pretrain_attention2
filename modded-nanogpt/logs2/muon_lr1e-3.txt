import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-3"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:06:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:76ms step_avg:75.80ms
step:2/2330 train_time:191ms step_avg:95.27ms
step:3/2330 train_time:213ms step_avg:71.02ms
step:4/2330 train_time:248ms step_avg:61.92ms
step:5/2330 train_time:305ms step_avg:60.97ms
step:6/2330 train_time:366ms step_avg:61.02ms
step:7/2330 train_time:425ms step_avg:60.76ms
step:8/2330 train_time:488ms step_avg:60.98ms
step:9/2330 train_time:546ms step_avg:60.70ms
step:10/2330 train_time:608ms step_avg:60.84ms
step:11/2330 train_time:667ms step_avg:60.62ms
step:12/2330 train_time:729ms step_avg:60.72ms
step:13/2330 train_time:787ms step_avg:60.58ms
step:14/2330 train_time:850ms step_avg:60.70ms
step:15/2330 train_time:909ms step_avg:60.60ms
step:16/2330 train_time:971ms step_avg:60.72ms
step:17/2330 train_time:1032ms step_avg:60.70ms
step:18/2330 train_time:1098ms step_avg:60.98ms
step:19/2330 train_time:1161ms step_avg:61.11ms
step:20/2330 train_time:1226ms step_avg:61.29ms
step:21/2330 train_time:1287ms step_avg:61.28ms
step:22/2330 train_time:1350ms step_avg:61.36ms
step:23/2330 train_time:1409ms step_avg:61.28ms
step:24/2330 train_time:1472ms step_avg:61.33ms
step:25/2330 train_time:1531ms step_avg:61.25ms
step:26/2330 train_time:1593ms step_avg:61.29ms
step:27/2330 train_time:1653ms step_avg:61.21ms
step:28/2330 train_time:1715ms step_avg:61.24ms
step:29/2330 train_time:1774ms step_avg:61.16ms
step:30/2330 train_time:1837ms step_avg:61.22ms
step:31/2330 train_time:1896ms step_avg:61.15ms
step:32/2330 train_time:1958ms step_avg:61.19ms
step:33/2330 train_time:2017ms step_avg:61.12ms
step:34/2330 train_time:2081ms step_avg:61.20ms
step:35/2330 train_time:2141ms step_avg:61.18ms
step:36/2330 train_time:2204ms step_avg:61.23ms
step:37/2330 train_time:2264ms step_avg:61.19ms
step:38/2330 train_time:2327ms step_avg:61.23ms
step:39/2330 train_time:2387ms step_avg:61.20ms
step:40/2330 train_time:2450ms step_avg:61.24ms
step:41/2330 train_time:2509ms step_avg:61.20ms
step:42/2330 train_time:2572ms step_avg:61.23ms
step:43/2330 train_time:2632ms step_avg:61.20ms
step:44/2330 train_time:2694ms step_avg:61.22ms
step:45/2330 train_time:2753ms step_avg:61.18ms
step:46/2330 train_time:2815ms step_avg:61.20ms
step:47/2330 train_time:2875ms step_avg:61.17ms
step:48/2330 train_time:2937ms step_avg:61.19ms
step:49/2330 train_time:2998ms step_avg:61.17ms
step:50/2330 train_time:3061ms step_avg:61.22ms
step:51/2330 train_time:3120ms step_avg:61.18ms
step:52/2330 train_time:3184ms step_avg:61.23ms
step:53/2330 train_time:3243ms step_avg:61.19ms
step:54/2330 train_time:3306ms step_avg:61.23ms
step:55/2330 train_time:3365ms step_avg:61.19ms
step:56/2330 train_time:3428ms step_avg:61.21ms
step:57/2330 train_time:3487ms step_avg:61.17ms
step:58/2330 train_time:3550ms step_avg:61.20ms
step:59/2330 train_time:3609ms step_avg:61.17ms
step:60/2330 train_time:3672ms step_avg:61.20ms
step:61/2330 train_time:3732ms step_avg:61.17ms
step:62/2330 train_time:3794ms step_avg:61.20ms
step:63/2330 train_time:3854ms step_avg:61.18ms
step:64/2330 train_time:3916ms step_avg:61.19ms
step:65/2330 train_time:3977ms step_avg:61.18ms
step:66/2330 train_time:4039ms step_avg:61.20ms
step:67/2330 train_time:4099ms step_avg:61.17ms
step:68/2330 train_time:4162ms step_avg:61.20ms
step:69/2330 train_time:4222ms step_avg:61.18ms
step:70/2330 train_time:4284ms step_avg:61.21ms
step:71/2330 train_time:4344ms step_avg:61.18ms
step:72/2330 train_time:4407ms step_avg:61.20ms
step:73/2330 train_time:4465ms step_avg:61.17ms
step:74/2330 train_time:4528ms step_avg:61.18ms
step:75/2330 train_time:4588ms step_avg:61.17ms
step:76/2330 train_time:4650ms step_avg:61.19ms
step:77/2330 train_time:4710ms step_avg:61.17ms
step:78/2330 train_time:4773ms step_avg:61.19ms
step:79/2330 train_time:4833ms step_avg:61.18ms
step:80/2330 train_time:4896ms step_avg:61.20ms
step:81/2330 train_time:4956ms step_avg:61.19ms
step:82/2330 train_time:5019ms step_avg:61.21ms
step:83/2330 train_time:5079ms step_avg:61.19ms
step:84/2330 train_time:5142ms step_avg:61.21ms
step:85/2330 train_time:5201ms step_avg:61.19ms
step:86/2330 train_time:5264ms step_avg:61.21ms
step:87/2330 train_time:5324ms step_avg:61.20ms
step:88/2330 train_time:5387ms step_avg:61.22ms
step:89/2330 train_time:5446ms step_avg:61.19ms
step:90/2330 train_time:5509ms step_avg:61.21ms
step:91/2330 train_time:5568ms step_avg:61.19ms
step:92/2330 train_time:5631ms step_avg:61.21ms
step:93/2330 train_time:5690ms step_avg:61.19ms
step:94/2330 train_time:5753ms step_avg:61.20ms
step:95/2330 train_time:5813ms step_avg:61.19ms
step:96/2330 train_time:5876ms step_avg:61.20ms
step:97/2330 train_time:5936ms step_avg:61.20ms
step:98/2330 train_time:5999ms step_avg:61.22ms
step:99/2330 train_time:6059ms step_avg:61.20ms
step:100/2330 train_time:6122ms step_avg:61.22ms
step:101/2330 train_time:6182ms step_avg:61.21ms
step:102/2330 train_time:6245ms step_avg:61.22ms
step:103/2330 train_time:6304ms step_avg:61.20ms
step:104/2330 train_time:6366ms step_avg:61.22ms
step:105/2330 train_time:6426ms step_avg:61.20ms
step:106/2330 train_time:6488ms step_avg:61.21ms
step:107/2330 train_time:6548ms step_avg:61.20ms
step:108/2330 train_time:6610ms step_avg:61.21ms
step:109/2330 train_time:6669ms step_avg:61.19ms
step:110/2330 train_time:6732ms step_avg:61.20ms
step:111/2330 train_time:6792ms step_avg:61.19ms
step:112/2330 train_time:6855ms step_avg:61.20ms
step:113/2330 train_time:6915ms step_avg:61.19ms
step:114/2330 train_time:6978ms step_avg:61.21ms
step:115/2330 train_time:7038ms step_avg:61.20ms
step:116/2330 train_time:7101ms step_avg:61.21ms
step:117/2330 train_time:7161ms step_avg:61.21ms
step:118/2330 train_time:7224ms step_avg:61.22ms
step:119/2330 train_time:7284ms step_avg:61.21ms
step:120/2330 train_time:7346ms step_avg:61.22ms
step:121/2330 train_time:7405ms step_avg:61.20ms
step:122/2330 train_time:7467ms step_avg:61.20ms
step:123/2330 train_time:7527ms step_avg:61.19ms
step:124/2330 train_time:7589ms step_avg:61.20ms
step:125/2330 train_time:7648ms step_avg:61.18ms
step:126/2330 train_time:7711ms step_avg:61.20ms
step:127/2330 train_time:7770ms step_avg:61.18ms
step:128/2330 train_time:7833ms step_avg:61.19ms
step:129/2330 train_time:7893ms step_avg:61.19ms
step:130/2330 train_time:7956ms step_avg:61.20ms
step:131/2330 train_time:8016ms step_avg:61.19ms
step:132/2330 train_time:8080ms step_avg:61.21ms
step:133/2330 train_time:8140ms step_avg:61.21ms
step:134/2330 train_time:8203ms step_avg:61.21ms
step:135/2330 train_time:8262ms step_avg:61.20ms
step:136/2330 train_time:8325ms step_avg:61.21ms
step:137/2330 train_time:8385ms step_avg:61.20ms
step:138/2330 train_time:8447ms step_avg:61.21ms
step:139/2330 train_time:8506ms step_avg:61.20ms
step:140/2330 train_time:8569ms step_avg:61.20ms
step:141/2330 train_time:8628ms step_avg:61.19ms
step:142/2330 train_time:8690ms step_avg:61.20ms
step:143/2330 train_time:8750ms step_avg:61.19ms
step:144/2330 train_time:8812ms step_avg:61.19ms
step:145/2330 train_time:8872ms step_avg:61.19ms
step:146/2330 train_time:8935ms step_avg:61.20ms
step:147/2330 train_time:8996ms step_avg:61.20ms
step:148/2330 train_time:9060ms step_avg:61.21ms
step:149/2330 train_time:9120ms step_avg:61.21ms
step:150/2330 train_time:9184ms step_avg:61.22ms
step:151/2330 train_time:9243ms step_avg:61.21ms
step:152/2330 train_time:9306ms step_avg:61.22ms
step:153/2330 train_time:9365ms step_avg:61.21ms
step:154/2330 train_time:9428ms step_avg:61.22ms
step:155/2330 train_time:9488ms step_avg:61.21ms
step:156/2330 train_time:9550ms step_avg:61.22ms
step:157/2330 train_time:9610ms step_avg:61.21ms
step:158/2330 train_time:9672ms step_avg:61.22ms
step:159/2330 train_time:9732ms step_avg:61.21ms
step:160/2330 train_time:9794ms step_avg:61.21ms
step:161/2330 train_time:9854ms step_avg:61.21ms
step:162/2330 train_time:9917ms step_avg:61.22ms
step:163/2330 train_time:9977ms step_avg:61.21ms
step:164/2330 train_time:10040ms step_avg:61.22ms
step:165/2330 train_time:10099ms step_avg:61.21ms
step:166/2330 train_time:10162ms step_avg:61.22ms
step:167/2330 train_time:10221ms step_avg:61.21ms
step:168/2330 train_time:10284ms step_avg:61.22ms
step:169/2330 train_time:10343ms step_avg:61.20ms
step:170/2330 train_time:10406ms step_avg:61.21ms
step:171/2330 train_time:10466ms step_avg:61.20ms
step:172/2330 train_time:10528ms step_avg:61.21ms
step:173/2330 train_time:10588ms step_avg:61.20ms
step:174/2330 train_time:10650ms step_avg:61.21ms
step:175/2330 train_time:10709ms step_avg:61.20ms
step:176/2330 train_time:10772ms step_avg:61.21ms
step:177/2330 train_time:10833ms step_avg:61.20ms
step:178/2330 train_time:10896ms step_avg:61.21ms
step:179/2330 train_time:10956ms step_avg:61.20ms
step:180/2330 train_time:11019ms step_avg:61.22ms
step:181/2330 train_time:11078ms step_avg:61.21ms
step:182/2330 train_time:11141ms step_avg:61.22ms
step:183/2330 train_time:11201ms step_avg:61.21ms
step:184/2330 train_time:11264ms step_avg:61.22ms
step:185/2330 train_time:11323ms step_avg:61.21ms
step:186/2330 train_time:11386ms step_avg:61.22ms
step:187/2330 train_time:11445ms step_avg:61.21ms
step:188/2330 train_time:11508ms step_avg:61.21ms
step:189/2330 train_time:11567ms step_avg:61.20ms
step:190/2330 train_time:11629ms step_avg:61.21ms
step:191/2330 train_time:11688ms step_avg:61.20ms
step:192/2330 train_time:11751ms step_avg:61.21ms
step:193/2330 train_time:11811ms step_avg:61.20ms
step:194/2330 train_time:11874ms step_avg:61.21ms
step:195/2330 train_time:11935ms step_avg:61.20ms
step:196/2330 train_time:11998ms step_avg:61.22ms
step:197/2330 train_time:12058ms step_avg:61.21ms
step:198/2330 train_time:12121ms step_avg:61.22ms
step:199/2330 train_time:12181ms step_avg:61.21ms
step:200/2330 train_time:12244ms step_avg:61.22ms
step:201/2330 train_time:12303ms step_avg:61.21ms
step:202/2330 train_time:12366ms step_avg:61.22ms
step:203/2330 train_time:12425ms step_avg:61.21ms
step:204/2330 train_time:12488ms step_avg:61.22ms
step:205/2330 train_time:12547ms step_avg:61.20ms
step:206/2330 train_time:12610ms step_avg:61.21ms
step:207/2330 train_time:12669ms step_avg:61.20ms
step:208/2330 train_time:12732ms step_avg:61.21ms
step:209/2330 train_time:12792ms step_avg:61.21ms
step:210/2330 train_time:12855ms step_avg:61.22ms
step:211/2330 train_time:12916ms step_avg:61.21ms
step:212/2330 train_time:12979ms step_avg:61.22ms
step:213/2330 train_time:13040ms step_avg:61.22ms
step:214/2330 train_time:13103ms step_avg:61.23ms
step:215/2330 train_time:13163ms step_avg:61.22ms
step:216/2330 train_time:13225ms step_avg:61.23ms
step:217/2330 train_time:13285ms step_avg:61.22ms
step:218/2330 train_time:13348ms step_avg:61.23ms
step:219/2330 train_time:13407ms step_avg:61.22ms
step:220/2330 train_time:13470ms step_avg:61.23ms
step:221/2330 train_time:13530ms step_avg:61.22ms
step:222/2330 train_time:13592ms step_avg:61.23ms
step:223/2330 train_time:13651ms step_avg:61.22ms
step:224/2330 train_time:13715ms step_avg:61.23ms
step:225/2330 train_time:13775ms step_avg:61.22ms
step:226/2330 train_time:13838ms step_avg:61.23ms
step:227/2330 train_time:13898ms step_avg:61.23ms
step:228/2330 train_time:13961ms step_avg:61.23ms
step:229/2330 train_time:14020ms step_avg:61.22ms
step:230/2330 train_time:14083ms step_avg:61.23ms
step:231/2330 train_time:14142ms step_avg:61.22ms
step:232/2330 train_time:14205ms step_avg:61.23ms
step:233/2330 train_time:14264ms step_avg:61.22ms
step:234/2330 train_time:14326ms step_avg:61.22ms
step:235/2330 train_time:14386ms step_avg:61.22ms
step:236/2330 train_time:14448ms step_avg:61.22ms
step:237/2330 train_time:14508ms step_avg:61.22ms
step:238/2330 train_time:14571ms step_avg:61.22ms
step:239/2330 train_time:14630ms step_avg:61.21ms
step:240/2330 train_time:14693ms step_avg:61.22ms
step:241/2330 train_time:14753ms step_avg:61.22ms
step:242/2330 train_time:14816ms step_avg:61.22ms
step:243/2330 train_time:14876ms step_avg:61.22ms
step:244/2330 train_time:14939ms step_avg:61.23ms
step:245/2330 train_time:14999ms step_avg:61.22ms
step:246/2330 train_time:15061ms step_avg:61.23ms
step:247/2330 train_time:15122ms step_avg:61.22ms
step:248/2330 train_time:15185ms step_avg:61.23ms
step:249/2330 train_time:15244ms step_avg:61.22ms
step:250/2330 train_time:15306ms step_avg:61.22ms
step:250/2330 val_loss:4.7487 train_time:15371ms step_avg:61.48ms
step:251/2330 train_time:15395ms step_avg:61.34ms
step:252/2330 train_time:15431ms step_avg:61.24ms
step:253/2330 train_time:15497ms step_avg:61.25ms
step:254/2330 train_time:15563ms step_avg:61.27ms
step:255/2330 train_time:15623ms step_avg:61.27ms
step:256/2330 train_time:15685ms step_avg:61.27ms
step:257/2330 train_time:15744ms step_avg:61.26ms
step:258/2330 train_time:15806ms step_avg:61.26ms
step:259/2330 train_time:15865ms step_avg:61.26ms
step:260/2330 train_time:15927ms step_avg:61.26ms
step:261/2330 train_time:15986ms step_avg:61.25ms
step:262/2330 train_time:16048ms step_avg:61.25ms
step:263/2330 train_time:16107ms step_avg:61.24ms
step:264/2330 train_time:16170ms step_avg:61.25ms
step:265/2330 train_time:16229ms step_avg:61.24ms
step:266/2330 train_time:16292ms step_avg:61.25ms
step:267/2330 train_time:16352ms step_avg:61.24ms
step:268/2330 train_time:16415ms step_avg:61.25ms
step:269/2330 train_time:16476ms step_avg:61.25ms
step:270/2330 train_time:16541ms step_avg:61.26ms
step:271/2330 train_time:16601ms step_avg:61.26ms
step:272/2330 train_time:16664ms step_avg:61.27ms
step:273/2330 train_time:16724ms step_avg:61.26ms
step:274/2330 train_time:16786ms step_avg:61.26ms
step:275/2330 train_time:16846ms step_avg:61.26ms
step:276/2330 train_time:16909ms step_avg:61.26ms
step:277/2330 train_time:16968ms step_avg:61.26ms
step:278/2330 train_time:17030ms step_avg:61.26ms
step:279/2330 train_time:17089ms step_avg:61.25ms
step:280/2330 train_time:17151ms step_avg:61.26ms
step:281/2330 train_time:17210ms step_avg:61.25ms
step:282/2330 train_time:17273ms step_avg:61.25ms
step:283/2330 train_time:17332ms step_avg:61.24ms
step:284/2330 train_time:17395ms step_avg:61.25ms
step:285/2330 train_time:17455ms step_avg:61.25ms
step:286/2330 train_time:17518ms step_avg:61.25ms
step:287/2330 train_time:17579ms step_avg:61.25ms
step:288/2330 train_time:17642ms step_avg:61.26ms
step:289/2330 train_time:17702ms step_avg:61.25ms
step:290/2330 train_time:17765ms step_avg:61.26ms
step:291/2330 train_time:17825ms step_avg:61.25ms
step:292/2330 train_time:17888ms step_avg:61.26ms
step:293/2330 train_time:17948ms step_avg:61.25ms
step:294/2330 train_time:18011ms step_avg:61.26ms
step:295/2330 train_time:18070ms step_avg:61.25ms
step:296/2330 train_time:18132ms step_avg:61.26ms
step:297/2330 train_time:18191ms step_avg:61.25ms
step:298/2330 train_time:18254ms step_avg:61.25ms
step:299/2330 train_time:18314ms step_avg:61.25ms
step:300/2330 train_time:18376ms step_avg:61.25ms
step:301/2330 train_time:18436ms step_avg:61.25ms
step:302/2330 train_time:18499ms step_avg:61.26ms
step:303/2330 train_time:18560ms step_avg:61.25ms
step:304/2330 train_time:18623ms step_avg:61.26ms
step:305/2330 train_time:18683ms step_avg:61.26ms
step:306/2330 train_time:18746ms step_avg:61.26ms
step:307/2330 train_time:18805ms step_avg:61.26ms
step:308/2330 train_time:18868ms step_avg:61.26ms
step:309/2330 train_time:18927ms step_avg:61.25ms
step:310/2330 train_time:18990ms step_avg:61.26ms
step:311/2330 train_time:19050ms step_avg:61.25ms
step:312/2330 train_time:19112ms step_avg:61.26ms
step:313/2330 train_time:19171ms step_avg:61.25ms
step:314/2330 train_time:19233ms step_avg:61.25ms
step:315/2330 train_time:19293ms step_avg:61.25ms
step:316/2330 train_time:19355ms step_avg:61.25ms
step:317/2330 train_time:19424ms step_avg:61.27ms
step:318/2330 train_time:19478ms step_avg:61.25ms
step:319/2330 train_time:19538ms step_avg:61.25ms
step:320/2330 train_time:19601ms step_avg:61.25ms
step:321/2330 train_time:19661ms step_avg:61.25ms
step:322/2330 train_time:19724ms step_avg:61.26ms
step:323/2330 train_time:19784ms step_avg:61.25ms
step:324/2330 train_time:19848ms step_avg:61.26ms
step:325/2330 train_time:19907ms step_avg:61.25ms
step:326/2330 train_time:19970ms step_avg:61.26ms
step:327/2330 train_time:20029ms step_avg:61.25ms
step:328/2330 train_time:20092ms step_avg:61.26ms
step:329/2330 train_time:20152ms step_avg:61.25ms
step:330/2330 train_time:20214ms step_avg:61.25ms
step:331/2330 train_time:20273ms step_avg:61.25ms
step:332/2330 train_time:20336ms step_avg:61.25ms
step:333/2330 train_time:20396ms step_avg:61.25ms
step:334/2330 train_time:20459ms step_avg:61.25ms
step:335/2330 train_time:20519ms step_avg:61.25ms
step:336/2330 train_time:20581ms step_avg:61.25ms
step:337/2330 train_time:20641ms step_avg:61.25ms
step:338/2330 train_time:20704ms step_avg:61.26ms
step:339/2330 train_time:20764ms step_avg:61.25ms
step:340/2330 train_time:20827ms step_avg:61.26ms
step:341/2330 train_time:20888ms step_avg:61.25ms
step:342/2330 train_time:20951ms step_avg:61.26ms
step:343/2330 train_time:21011ms step_avg:61.26ms
step:344/2330 train_time:21073ms step_avg:61.26ms
step:345/2330 train_time:21133ms step_avg:61.25ms
step:346/2330 train_time:21195ms step_avg:61.26ms
step:347/2330 train_time:21255ms step_avg:61.25ms
step:348/2330 train_time:21317ms step_avg:61.26ms
step:349/2330 train_time:21377ms step_avg:61.25ms
step:350/2330 train_time:21440ms step_avg:61.26ms
step:351/2330 train_time:21500ms step_avg:61.25ms
step:352/2330 train_time:21563ms step_avg:61.26ms
step:353/2330 train_time:21623ms step_avg:61.26ms
step:354/2330 train_time:21686ms step_avg:61.26ms
step:355/2330 train_time:21747ms step_avg:61.26ms
step:356/2330 train_time:21810ms step_avg:61.26ms
step:357/2330 train_time:21869ms step_avg:61.26ms
step:358/2330 train_time:21932ms step_avg:61.26ms
step:359/2330 train_time:21991ms step_avg:61.26ms
step:360/2330 train_time:22054ms step_avg:61.26ms
step:361/2330 train_time:22113ms step_avg:61.26ms
step:362/2330 train_time:22176ms step_avg:61.26ms
step:363/2330 train_time:22236ms step_avg:61.26ms
step:364/2330 train_time:22299ms step_avg:61.26ms
step:365/2330 train_time:22358ms step_avg:61.25ms
step:366/2330 train_time:22420ms step_avg:61.26ms
step:367/2330 train_time:22480ms step_avg:61.25ms
step:368/2330 train_time:22544ms step_avg:61.26ms
step:369/2330 train_time:22604ms step_avg:61.26ms
step:370/2330 train_time:22666ms step_avg:61.26ms
step:371/2330 train_time:22726ms step_avg:61.26ms
step:372/2330 train_time:22789ms step_avg:61.26ms
step:373/2330 train_time:22849ms step_avg:61.26ms
step:374/2330 train_time:22911ms step_avg:61.26ms
step:375/2330 train_time:22971ms step_avg:61.26ms
step:376/2330 train_time:23034ms step_avg:61.26ms
step:377/2330 train_time:23094ms step_avg:61.26ms
step:378/2330 train_time:23156ms step_avg:61.26ms
step:379/2330 train_time:23216ms step_avg:61.25ms
step:380/2330 train_time:23278ms step_avg:61.26ms
step:381/2330 train_time:23337ms step_avg:61.25ms
step:382/2330 train_time:23401ms step_avg:61.26ms
step:383/2330 train_time:23460ms step_avg:61.25ms
step:384/2330 train_time:23524ms step_avg:61.26ms
step:385/2330 train_time:23584ms step_avg:61.26ms
step:386/2330 train_time:23647ms step_avg:61.26ms
step:387/2330 train_time:23707ms step_avg:61.26ms
step:388/2330 train_time:23770ms step_avg:61.26ms
step:389/2330 train_time:23830ms step_avg:61.26ms
step:390/2330 train_time:23892ms step_avg:61.26ms
step:391/2330 train_time:23952ms step_avg:61.26ms
step:392/2330 train_time:24015ms step_avg:61.26ms
step:393/2330 train_time:24075ms step_avg:61.26ms
step:394/2330 train_time:24138ms step_avg:61.26ms
step:395/2330 train_time:24198ms step_avg:61.26ms
step:396/2330 train_time:24261ms step_avg:61.26ms
step:397/2330 train_time:24321ms step_avg:61.26ms
step:398/2330 train_time:24383ms step_avg:61.26ms
step:399/2330 train_time:24443ms step_avg:61.26ms
step:400/2330 train_time:24507ms step_avg:61.27ms
step:401/2330 train_time:24567ms step_avg:61.26ms
step:402/2330 train_time:24630ms step_avg:61.27ms
step:403/2330 train_time:24689ms step_avg:61.26ms
step:404/2330 train_time:24752ms step_avg:61.27ms
step:405/2330 train_time:24812ms step_avg:61.26ms
step:406/2330 train_time:24875ms step_avg:61.27ms
step:407/2330 train_time:24935ms step_avg:61.27ms
step:408/2330 train_time:24998ms step_avg:61.27ms
step:409/2330 train_time:25058ms step_avg:61.27ms
step:410/2330 train_time:25120ms step_avg:61.27ms
step:411/2330 train_time:25180ms step_avg:61.27ms
step:412/2330 train_time:25244ms step_avg:61.27ms
step:413/2330 train_time:25303ms step_avg:61.27ms
step:414/2330 train_time:25366ms step_avg:61.27ms
step:415/2330 train_time:25425ms step_avg:61.27ms
step:416/2330 train_time:25488ms step_avg:61.27ms
step:417/2330 train_time:25547ms step_avg:61.26ms
step:418/2330 train_time:25611ms step_avg:61.27ms
step:419/2330 train_time:25670ms step_avg:61.27ms
step:420/2330 train_time:25733ms step_avg:61.27ms
step:421/2330 train_time:25792ms step_avg:61.26ms
step:422/2330 train_time:25855ms step_avg:61.27ms
step:423/2330 train_time:25915ms step_avg:61.26ms
step:424/2330 train_time:25977ms step_avg:61.27ms
step:425/2330 train_time:26037ms step_avg:61.26ms
step:426/2330 train_time:26101ms step_avg:61.27ms
step:427/2330 train_time:26161ms step_avg:61.27ms
step:428/2330 train_time:26224ms step_avg:61.27ms
step:429/2330 train_time:26283ms step_avg:61.27ms
step:430/2330 train_time:26347ms step_avg:61.27ms
step:431/2330 train_time:26406ms step_avg:61.27ms
step:432/2330 train_time:26469ms step_avg:61.27ms
step:433/2330 train_time:26529ms step_avg:61.27ms
step:434/2330 train_time:26592ms step_avg:61.27ms
step:435/2330 train_time:26652ms step_avg:61.27ms
step:436/2330 train_time:26715ms step_avg:61.27ms
step:437/2330 train_time:26775ms step_avg:61.27ms
step:438/2330 train_time:26838ms step_avg:61.27ms
step:439/2330 train_time:26898ms step_avg:61.27ms
step:440/2330 train_time:26961ms step_avg:61.27ms
step:441/2330 train_time:27021ms step_avg:61.27ms
step:442/2330 train_time:27084ms step_avg:61.28ms
step:443/2330 train_time:27143ms step_avg:61.27ms
step:444/2330 train_time:27206ms step_avg:61.28ms
step:445/2330 train_time:27265ms step_avg:61.27ms
step:446/2330 train_time:27328ms step_avg:61.27ms
step:447/2330 train_time:27388ms step_avg:61.27ms
step:448/2330 train_time:27451ms step_avg:61.27ms
step:449/2330 train_time:27511ms step_avg:61.27ms
step:450/2330 train_time:27574ms step_avg:61.28ms
step:451/2330 train_time:27634ms step_avg:61.27ms
step:452/2330 train_time:27697ms step_avg:61.28ms
step:453/2330 train_time:27757ms step_avg:61.27ms
step:454/2330 train_time:27820ms step_avg:61.28ms
step:455/2330 train_time:27880ms step_avg:61.27ms
step:456/2330 train_time:27944ms step_avg:61.28ms
step:457/2330 train_time:28004ms step_avg:61.28ms
step:458/2330 train_time:28066ms step_avg:61.28ms
step:459/2330 train_time:28126ms step_avg:61.28ms
step:460/2330 train_time:28189ms step_avg:61.28ms
step:461/2330 train_time:28248ms step_avg:61.28ms
step:462/2330 train_time:28311ms step_avg:61.28ms
step:463/2330 train_time:28371ms step_avg:61.28ms
step:464/2330 train_time:28433ms step_avg:61.28ms
step:465/2330 train_time:28493ms step_avg:61.28ms
step:466/2330 train_time:28556ms step_avg:61.28ms
step:467/2330 train_time:28615ms step_avg:61.27ms
step:468/2330 train_time:28678ms step_avg:61.28ms
step:469/2330 train_time:28738ms step_avg:61.28ms
step:470/2330 train_time:28801ms step_avg:61.28ms
step:471/2330 train_time:28862ms step_avg:61.28ms
step:472/2330 train_time:28924ms step_avg:61.28ms
step:473/2330 train_time:28985ms step_avg:61.28ms
step:474/2330 train_time:29048ms step_avg:61.28ms
step:475/2330 train_time:29107ms step_avg:61.28ms
step:476/2330 train_time:29170ms step_avg:61.28ms
step:477/2330 train_time:29229ms step_avg:61.28ms
step:478/2330 train_time:29292ms step_avg:61.28ms
step:479/2330 train_time:29352ms step_avg:61.28ms
step:480/2330 train_time:29415ms step_avg:61.28ms
step:481/2330 train_time:29474ms step_avg:61.28ms
step:482/2330 train_time:29537ms step_avg:61.28ms
step:483/2330 train_time:29597ms step_avg:61.28ms
step:484/2330 train_time:29660ms step_avg:61.28ms
step:485/2330 train_time:29720ms step_avg:61.28ms
step:486/2330 train_time:29783ms step_avg:61.28ms
step:487/2330 train_time:29843ms step_avg:61.28ms
step:488/2330 train_time:29906ms step_avg:61.28ms
step:489/2330 train_time:29966ms step_avg:61.28ms
step:490/2330 train_time:30029ms step_avg:61.28ms
step:491/2330 train_time:30089ms step_avg:61.28ms
step:492/2330 train_time:30152ms step_avg:61.29ms
step:493/2330 train_time:30212ms step_avg:61.28ms
step:494/2330 train_time:30274ms step_avg:61.28ms
step:495/2330 train_time:30334ms step_avg:61.28ms
step:496/2330 train_time:30396ms step_avg:61.28ms
step:497/2330 train_time:30456ms step_avg:61.28ms
step:498/2330 train_time:30519ms step_avg:61.28ms
step:499/2330 train_time:30579ms step_avg:61.28ms
step:500/2330 train_time:30643ms step_avg:61.29ms
step:500/2330 val_loss:4.1608 train_time:30708ms step_avg:61.42ms
step:501/2330 train_time:30731ms step_avg:61.34ms
step:502/2330 train_time:30769ms step_avg:61.29ms
step:503/2330 train_time:30832ms step_avg:61.30ms
step:504/2330 train_time:30897ms step_avg:61.30ms
step:505/2330 train_time:30957ms step_avg:61.30ms
step:506/2330 train_time:31020ms step_avg:61.31ms
step:507/2330 train_time:31080ms step_avg:61.30ms
step:508/2330 train_time:31143ms step_avg:61.31ms
step:509/2330 train_time:31203ms step_avg:61.30ms
step:510/2330 train_time:31265ms step_avg:61.30ms
step:511/2330 train_time:31323ms step_avg:61.30ms
step:512/2330 train_time:31386ms step_avg:61.30ms
step:513/2330 train_time:31445ms step_avg:61.30ms
step:514/2330 train_time:31508ms step_avg:61.30ms
step:515/2330 train_time:31567ms step_avg:61.29ms
step:516/2330 train_time:31630ms step_avg:61.30ms
step:517/2330 train_time:31690ms step_avg:61.30ms
step:518/2330 train_time:31754ms step_avg:61.30ms
step:519/2330 train_time:31815ms step_avg:61.30ms
step:520/2330 train_time:31878ms step_avg:61.30ms
step:521/2330 train_time:31939ms step_avg:61.30ms
step:522/2330 train_time:32004ms step_avg:61.31ms
step:523/2330 train_time:32063ms step_avg:61.31ms
step:524/2330 train_time:32127ms step_avg:61.31ms
step:525/2330 train_time:32187ms step_avg:61.31ms
step:526/2330 train_time:32250ms step_avg:61.31ms
step:527/2330 train_time:32308ms step_avg:61.31ms
step:528/2330 train_time:32371ms step_avg:61.31ms
step:529/2330 train_time:32430ms step_avg:61.31ms
step:530/2330 train_time:32492ms step_avg:61.31ms
step:531/2330 train_time:32551ms step_avg:61.30ms
step:532/2330 train_time:32614ms step_avg:61.30ms
step:533/2330 train_time:32673ms step_avg:61.30ms
step:534/2330 train_time:32736ms step_avg:61.30ms
step:535/2330 train_time:32796ms step_avg:61.30ms
step:536/2330 train_time:32859ms step_avg:61.30ms
step:537/2330 train_time:32919ms step_avg:61.30ms
step:538/2330 train_time:32983ms step_avg:61.31ms
step:539/2330 train_time:33043ms step_avg:61.30ms
step:540/2330 train_time:33106ms step_avg:61.31ms
step:541/2330 train_time:33166ms step_avg:61.31ms
step:542/2330 train_time:33229ms step_avg:61.31ms
step:543/2330 train_time:33288ms step_avg:61.30ms
step:544/2330 train_time:33351ms step_avg:61.31ms
step:545/2330 train_time:33410ms step_avg:61.30ms
step:546/2330 train_time:33472ms step_avg:61.30ms
step:547/2330 train_time:33531ms step_avg:61.30ms
step:548/2330 train_time:33593ms step_avg:61.30ms
step:549/2330 train_time:33654ms step_avg:61.30ms
step:550/2330 train_time:33716ms step_avg:61.30ms
step:551/2330 train_time:33776ms step_avg:61.30ms
step:552/2330 train_time:33839ms step_avg:61.30ms
step:553/2330 train_time:33899ms step_avg:61.30ms
step:554/2330 train_time:33963ms step_avg:61.30ms
step:555/2330 train_time:34023ms step_avg:61.30ms
step:556/2330 train_time:34087ms step_avg:61.31ms
step:557/2330 train_time:34147ms step_avg:61.30ms
step:558/2330 train_time:34210ms step_avg:61.31ms
step:559/2330 train_time:34269ms step_avg:61.30ms
step:560/2330 train_time:34332ms step_avg:61.31ms
step:561/2330 train_time:34391ms step_avg:61.30ms
step:562/2330 train_time:34454ms step_avg:61.31ms
step:563/2330 train_time:34512ms step_avg:61.30ms
step:564/2330 train_time:34575ms step_avg:61.30ms
step:565/2330 train_time:34634ms step_avg:61.30ms
step:566/2330 train_time:34697ms step_avg:61.30ms
step:567/2330 train_time:34757ms step_avg:61.30ms
step:568/2330 train_time:34819ms step_avg:61.30ms
step:569/2330 train_time:34880ms step_avg:61.30ms
step:570/2330 train_time:34943ms step_avg:61.30ms
step:571/2330 train_time:35003ms step_avg:61.30ms
step:572/2330 train_time:35066ms step_avg:61.30ms
step:573/2330 train_time:35126ms step_avg:61.30ms
step:574/2330 train_time:35190ms step_avg:61.31ms
step:575/2330 train_time:35250ms step_avg:61.30ms
step:576/2330 train_time:35313ms step_avg:61.31ms
step:577/2330 train_time:35373ms step_avg:61.30ms
step:578/2330 train_time:35436ms step_avg:61.31ms
step:579/2330 train_time:35495ms step_avg:61.30ms
step:580/2330 train_time:35558ms step_avg:61.31ms
step:581/2330 train_time:35618ms step_avg:61.30ms
step:582/2330 train_time:35681ms step_avg:61.31ms
step:583/2330 train_time:35740ms step_avg:61.30ms
step:584/2330 train_time:35803ms step_avg:61.31ms
step:585/2330 train_time:35863ms step_avg:61.30ms
step:586/2330 train_time:35926ms step_avg:61.31ms
step:587/2330 train_time:35986ms step_avg:61.30ms
step:588/2330 train_time:36049ms step_avg:61.31ms
step:589/2330 train_time:36109ms step_avg:61.31ms
step:590/2330 train_time:36173ms step_avg:61.31ms
step:591/2330 train_time:36233ms step_avg:61.31ms
step:592/2330 train_time:36295ms step_avg:61.31ms
step:593/2330 train_time:36355ms step_avg:61.31ms
step:594/2330 train_time:36418ms step_avg:61.31ms
step:595/2330 train_time:36478ms step_avg:61.31ms
step:596/2330 train_time:36541ms step_avg:61.31ms
step:597/2330 train_time:36600ms step_avg:61.31ms
step:598/2330 train_time:36663ms step_avg:61.31ms
step:599/2330 train_time:36723ms step_avg:61.31ms
step:600/2330 train_time:36786ms step_avg:61.31ms
step:601/2330 train_time:36846ms step_avg:61.31ms
step:602/2330 train_time:36908ms step_avg:61.31ms
step:603/2330 train_time:36968ms step_avg:61.31ms
step:604/2330 train_time:37031ms step_avg:61.31ms
step:605/2330 train_time:37090ms step_avg:61.31ms
step:606/2330 train_time:37153ms step_avg:61.31ms
step:607/2330 train_time:37213ms step_avg:61.31ms
step:608/2330 train_time:37275ms step_avg:61.31ms
step:609/2330 train_time:37334ms step_avg:61.30ms
step:610/2330 train_time:37397ms step_avg:61.31ms
step:611/2330 train_time:37457ms step_avg:61.30ms
step:612/2330 train_time:37520ms step_avg:61.31ms
step:613/2330 train_time:37580ms step_avg:61.30ms
step:614/2330 train_time:37642ms step_avg:61.31ms
step:615/2330 train_time:37703ms step_avg:61.31ms
step:616/2330 train_time:37766ms step_avg:61.31ms
step:617/2330 train_time:37826ms step_avg:61.31ms
step:618/2330 train_time:37889ms step_avg:61.31ms
step:619/2330 train_time:37949ms step_avg:61.31ms
step:620/2330 train_time:38012ms step_avg:61.31ms
step:621/2330 train_time:38072ms step_avg:61.31ms
step:622/2330 train_time:38134ms step_avg:61.31ms
step:623/2330 train_time:38194ms step_avg:61.31ms
step:624/2330 train_time:38257ms step_avg:61.31ms
step:625/2330 train_time:38317ms step_avg:61.31ms
step:626/2330 train_time:38379ms step_avg:61.31ms
step:627/2330 train_time:38439ms step_avg:61.31ms
step:628/2330 train_time:38501ms step_avg:61.31ms
step:629/2330 train_time:38561ms step_avg:61.30ms
step:630/2330 train_time:38624ms step_avg:61.31ms
step:631/2330 train_time:38685ms step_avg:61.31ms
step:632/2330 train_time:38747ms step_avg:61.31ms
step:633/2330 train_time:38807ms step_avg:61.31ms
step:634/2330 train_time:38869ms step_avg:61.31ms
step:635/2330 train_time:38929ms step_avg:61.31ms
step:636/2330 train_time:38992ms step_avg:61.31ms
step:637/2330 train_time:39051ms step_avg:61.30ms
step:638/2330 train_time:39114ms step_avg:61.31ms
step:639/2330 train_time:39174ms step_avg:61.30ms
step:640/2330 train_time:39236ms step_avg:61.31ms
step:641/2330 train_time:39296ms step_avg:61.30ms
step:642/2330 train_time:39359ms step_avg:61.31ms
step:643/2330 train_time:39418ms step_avg:61.30ms
step:644/2330 train_time:39481ms step_avg:61.31ms
step:645/2330 train_time:39540ms step_avg:61.30ms
step:646/2330 train_time:39604ms step_avg:61.31ms
step:647/2330 train_time:39664ms step_avg:61.30ms
step:648/2330 train_time:39728ms step_avg:61.31ms
step:649/2330 train_time:39787ms step_avg:61.31ms
step:650/2330 train_time:39851ms step_avg:61.31ms
step:651/2330 train_time:39910ms step_avg:61.31ms
step:652/2330 train_time:39973ms step_avg:61.31ms
step:653/2330 train_time:40033ms step_avg:61.31ms
step:654/2330 train_time:40096ms step_avg:61.31ms
step:655/2330 train_time:40156ms step_avg:61.31ms
step:656/2330 train_time:40219ms step_avg:61.31ms
step:657/2330 train_time:40278ms step_avg:61.31ms
step:658/2330 train_time:40341ms step_avg:61.31ms
step:659/2330 train_time:40401ms step_avg:61.31ms
step:660/2330 train_time:40463ms step_avg:61.31ms
step:661/2330 train_time:40523ms step_avg:61.31ms
step:662/2330 train_time:40587ms step_avg:61.31ms
step:663/2330 train_time:40647ms step_avg:61.31ms
step:664/2330 train_time:40710ms step_avg:61.31ms
step:665/2330 train_time:40769ms step_avg:61.31ms
step:666/2330 train_time:40832ms step_avg:61.31ms
step:667/2330 train_time:40892ms step_avg:61.31ms
step:668/2330 train_time:40955ms step_avg:61.31ms
step:669/2330 train_time:41015ms step_avg:61.31ms
step:670/2330 train_time:41078ms step_avg:61.31ms
step:671/2330 train_time:41138ms step_avg:61.31ms
step:672/2330 train_time:41201ms step_avg:61.31ms
step:673/2330 train_time:41261ms step_avg:61.31ms
step:674/2330 train_time:41324ms step_avg:61.31ms
step:675/2330 train_time:41384ms step_avg:61.31ms
step:676/2330 train_time:41446ms step_avg:61.31ms
step:677/2330 train_time:41506ms step_avg:61.31ms
step:678/2330 train_time:41569ms step_avg:61.31ms
step:679/2330 train_time:41629ms step_avg:61.31ms
step:680/2330 train_time:41692ms step_avg:61.31ms
step:681/2330 train_time:41752ms step_avg:61.31ms
step:682/2330 train_time:41814ms step_avg:61.31ms
step:683/2330 train_time:41874ms step_avg:61.31ms
step:684/2330 train_time:41936ms step_avg:61.31ms
step:685/2330 train_time:41997ms step_avg:61.31ms
step:686/2330 train_time:42060ms step_avg:61.31ms
step:687/2330 train_time:42120ms step_avg:61.31ms
step:688/2330 train_time:42184ms step_avg:61.31ms
step:689/2330 train_time:42244ms step_avg:61.31ms
step:690/2330 train_time:42307ms step_avg:61.31ms
step:691/2330 train_time:42367ms step_avg:61.31ms
step:692/2330 train_time:42430ms step_avg:61.31ms
step:693/2330 train_time:42489ms step_avg:61.31ms
step:694/2330 train_time:42553ms step_avg:61.32ms
step:695/2330 train_time:42612ms step_avg:61.31ms
step:696/2330 train_time:42675ms step_avg:61.31ms
step:697/2330 train_time:42735ms step_avg:61.31ms
step:698/2330 train_time:42798ms step_avg:61.31ms
step:699/2330 train_time:42858ms step_avg:61.31ms
step:700/2330 train_time:42921ms step_avg:61.32ms
step:701/2330 train_time:42981ms step_avg:61.31ms
step:702/2330 train_time:43044ms step_avg:61.32ms
step:703/2330 train_time:43104ms step_avg:61.31ms
step:704/2330 train_time:43167ms step_avg:61.32ms
step:705/2330 train_time:43227ms step_avg:61.31ms
step:706/2330 train_time:43290ms step_avg:61.32ms
step:707/2330 train_time:43349ms step_avg:61.31ms
step:708/2330 train_time:43412ms step_avg:61.32ms
step:709/2330 train_time:43471ms step_avg:61.31ms
step:710/2330 train_time:43534ms step_avg:61.32ms
step:711/2330 train_time:43594ms step_avg:61.31ms
step:712/2330 train_time:43657ms step_avg:61.32ms
step:713/2330 train_time:43716ms step_avg:61.31ms
step:714/2330 train_time:43779ms step_avg:61.31ms
step:715/2330 train_time:43839ms step_avg:61.31ms
step:716/2330 train_time:43902ms step_avg:61.32ms
step:717/2330 train_time:43961ms step_avg:61.31ms
step:718/2330 train_time:44025ms step_avg:61.32ms
step:719/2330 train_time:44086ms step_avg:61.32ms
step:720/2330 train_time:44149ms step_avg:61.32ms
step:721/2330 train_time:44209ms step_avg:61.32ms
step:722/2330 train_time:44272ms step_avg:61.32ms
step:723/2330 train_time:44332ms step_avg:61.32ms
step:724/2330 train_time:44395ms step_avg:61.32ms
step:725/2330 train_time:44454ms step_avg:61.32ms
step:726/2330 train_time:44517ms step_avg:61.32ms
step:727/2330 train_time:44576ms step_avg:61.31ms
step:728/2330 train_time:44639ms step_avg:61.32ms
step:729/2330 train_time:44699ms step_avg:61.32ms
step:730/2330 train_time:44762ms step_avg:61.32ms
step:731/2330 train_time:44823ms step_avg:61.32ms
step:732/2330 train_time:44886ms step_avg:61.32ms
step:733/2330 train_time:44945ms step_avg:61.32ms
step:734/2330 train_time:45008ms step_avg:61.32ms
step:735/2330 train_time:45068ms step_avg:61.32ms
step:736/2330 train_time:45131ms step_avg:61.32ms
step:737/2330 train_time:45191ms step_avg:61.32ms
step:738/2330 train_time:45254ms step_avg:61.32ms
step:739/2330 train_time:45313ms step_avg:61.32ms
step:740/2330 train_time:45376ms step_avg:61.32ms
step:741/2330 train_time:45436ms step_avg:61.32ms
step:742/2330 train_time:45499ms step_avg:61.32ms
step:743/2330 train_time:45559ms step_avg:61.32ms
step:744/2330 train_time:45622ms step_avg:61.32ms
step:745/2330 train_time:45682ms step_avg:61.32ms
step:746/2330 train_time:45745ms step_avg:61.32ms
step:747/2330 train_time:45805ms step_avg:61.32ms
step:748/2330 train_time:45868ms step_avg:61.32ms
step:749/2330 train_time:45927ms step_avg:61.32ms
step:750/2330 train_time:45990ms step_avg:61.32ms
step:750/2330 val_loss:3.9504 train_time:46053ms step_avg:61.40ms
step:751/2330 train_time:46076ms step_avg:61.35ms
step:752/2330 train_time:46114ms step_avg:61.32ms
step:753/2330 train_time:46179ms step_avg:61.33ms
step:754/2330 train_time:46246ms step_avg:61.33ms
step:755/2330 train_time:46306ms step_avg:61.33ms
step:756/2330 train_time:46370ms step_avg:61.34ms
step:757/2330 train_time:46429ms step_avg:61.33ms
step:758/2330 train_time:46491ms step_avg:61.33ms
step:759/2330 train_time:46550ms step_avg:61.33ms
step:760/2330 train_time:46612ms step_avg:61.33ms
step:761/2330 train_time:46671ms step_avg:61.33ms
step:762/2330 train_time:46733ms step_avg:61.33ms
step:763/2330 train_time:46792ms step_avg:61.33ms
step:764/2330 train_time:46854ms step_avg:61.33ms
step:765/2330 train_time:46913ms step_avg:61.32ms
step:766/2330 train_time:46977ms step_avg:61.33ms
step:767/2330 train_time:47038ms step_avg:61.33ms
step:768/2330 train_time:47104ms step_avg:61.33ms
step:769/2330 train_time:47167ms step_avg:61.34ms
step:770/2330 train_time:47231ms step_avg:61.34ms
step:771/2330 train_time:47293ms step_avg:61.34ms
step:772/2330 train_time:47357ms step_avg:61.34ms
step:773/2330 train_time:47418ms step_avg:61.34ms
step:774/2330 train_time:47483ms step_avg:61.35ms
step:775/2330 train_time:47544ms step_avg:61.35ms
step:776/2330 train_time:47607ms step_avg:61.35ms
step:777/2330 train_time:47667ms step_avg:61.35ms
step:778/2330 train_time:47730ms step_avg:61.35ms
step:779/2330 train_time:47790ms step_avg:61.35ms
step:780/2330 train_time:47854ms step_avg:61.35ms
step:781/2330 train_time:47913ms step_avg:61.35ms
step:782/2330 train_time:47976ms step_avg:61.35ms
step:783/2330 train_time:48037ms step_avg:61.35ms
step:784/2330 train_time:48101ms step_avg:61.35ms
step:785/2330 train_time:48163ms step_avg:61.35ms
step:786/2330 train_time:48228ms step_avg:61.36ms
step:787/2330 train_time:48289ms step_avg:61.36ms
step:788/2330 train_time:48354ms step_avg:61.36ms
step:789/2330 train_time:48413ms step_avg:61.36ms
step:790/2330 train_time:48477ms step_avg:61.36ms
step:791/2330 train_time:48538ms step_avg:61.36ms
step:792/2330 train_time:48602ms step_avg:61.37ms
step:793/2330 train_time:48663ms step_avg:61.37ms
step:794/2330 train_time:48727ms step_avg:61.37ms
step:795/2330 train_time:48787ms step_avg:61.37ms
step:796/2330 train_time:48851ms step_avg:61.37ms
step:797/2330 train_time:48911ms step_avg:61.37ms
step:798/2330 train_time:48974ms step_avg:61.37ms
step:799/2330 train_time:49033ms step_avg:61.37ms
step:800/2330 train_time:49097ms step_avg:61.37ms
step:801/2330 train_time:49159ms step_avg:61.37ms
step:802/2330 train_time:49223ms step_avg:61.38ms
step:803/2330 train_time:49285ms step_avg:61.38ms
step:804/2330 train_time:49349ms step_avg:61.38ms
step:805/2330 train_time:49409ms step_avg:61.38ms
step:806/2330 train_time:49474ms step_avg:61.38ms
step:807/2330 train_time:49534ms step_avg:61.38ms
step:808/2330 train_time:49598ms step_avg:61.38ms
step:809/2330 train_time:49659ms step_avg:61.38ms
step:810/2330 train_time:49723ms step_avg:61.39ms
step:811/2330 train_time:49784ms step_avg:61.39ms
step:812/2330 train_time:49847ms step_avg:61.39ms
step:813/2330 train_time:49908ms step_avg:61.39ms
step:814/2330 train_time:49972ms step_avg:61.39ms
step:815/2330 train_time:50032ms step_avg:61.39ms
step:816/2330 train_time:50095ms step_avg:61.39ms
step:817/2330 train_time:50156ms step_avg:61.39ms
step:818/2330 train_time:50219ms step_avg:61.39ms
step:819/2330 train_time:50281ms step_avg:61.39ms
step:820/2330 train_time:50346ms step_avg:61.40ms
step:821/2330 train_time:50407ms step_avg:61.40ms
step:822/2330 train_time:50470ms step_avg:61.40ms
step:823/2330 train_time:50531ms step_avg:61.40ms
step:824/2330 train_time:50596ms step_avg:61.40ms
step:825/2330 train_time:50657ms step_avg:61.40ms
step:826/2330 train_time:50721ms step_avg:61.41ms
step:827/2330 train_time:50782ms step_avg:61.40ms
step:828/2330 train_time:50845ms step_avg:61.41ms
step:829/2330 train_time:50906ms step_avg:61.41ms
step:830/2330 train_time:50970ms step_avg:61.41ms
step:831/2330 train_time:51031ms step_avg:61.41ms
step:832/2330 train_time:51094ms step_avg:61.41ms
step:833/2330 train_time:51154ms step_avg:61.41ms
step:834/2330 train_time:51217ms step_avg:61.41ms
step:835/2330 train_time:51278ms step_avg:61.41ms
step:836/2330 train_time:51343ms step_avg:61.41ms
step:837/2330 train_time:51404ms step_avg:61.41ms
step:838/2330 train_time:51467ms step_avg:61.42ms
step:839/2330 train_time:51528ms step_avg:61.42ms
step:840/2330 train_time:51592ms step_avg:61.42ms
step:841/2330 train_time:51653ms step_avg:61.42ms
step:842/2330 train_time:51716ms step_avg:61.42ms
step:843/2330 train_time:51777ms step_avg:61.42ms
step:844/2330 train_time:51841ms step_avg:61.42ms
step:845/2330 train_time:51902ms step_avg:61.42ms
step:846/2330 train_time:51967ms step_avg:61.43ms
step:847/2330 train_time:52028ms step_avg:61.43ms
step:848/2330 train_time:52092ms step_avg:61.43ms
step:849/2330 train_time:52152ms step_avg:61.43ms
step:850/2330 train_time:52216ms step_avg:61.43ms
step:851/2330 train_time:52276ms step_avg:61.43ms
step:852/2330 train_time:52340ms step_avg:61.43ms
step:853/2330 train_time:52400ms step_avg:61.43ms
step:854/2330 train_time:52465ms step_avg:61.43ms
step:855/2330 train_time:52525ms step_avg:61.43ms
step:856/2330 train_time:52589ms step_avg:61.44ms
step:857/2330 train_time:52650ms step_avg:61.44ms
step:858/2330 train_time:52714ms step_avg:61.44ms
step:859/2330 train_time:52774ms step_avg:61.44ms
step:860/2330 train_time:52838ms step_avg:61.44ms
step:861/2330 train_time:52899ms step_avg:61.44ms
step:862/2330 train_time:52963ms step_avg:61.44ms
step:863/2330 train_time:53025ms step_avg:61.44ms
step:864/2330 train_time:53089ms step_avg:61.45ms
step:865/2330 train_time:53149ms step_avg:61.44ms
step:866/2330 train_time:53212ms step_avg:61.45ms
step:867/2330 train_time:53273ms step_avg:61.44ms
step:868/2330 train_time:53336ms step_avg:61.45ms
step:869/2330 train_time:53396ms step_avg:61.45ms
step:870/2330 train_time:53461ms step_avg:61.45ms
step:871/2330 train_time:53522ms step_avg:61.45ms
step:872/2330 train_time:53586ms step_avg:61.45ms
step:873/2330 train_time:53647ms step_avg:61.45ms
step:874/2330 train_time:53710ms step_avg:61.45ms
step:875/2330 train_time:53771ms step_avg:61.45ms
step:876/2330 train_time:53834ms step_avg:61.45ms
step:877/2330 train_time:53895ms step_avg:61.45ms
step:878/2330 train_time:53959ms step_avg:61.46ms
step:879/2330 train_time:54020ms step_avg:61.46ms
step:880/2330 train_time:54084ms step_avg:61.46ms
step:881/2330 train_time:54145ms step_avg:61.46ms
step:882/2330 train_time:54210ms step_avg:61.46ms
step:883/2330 train_time:54270ms step_avg:61.46ms
step:884/2330 train_time:54333ms step_avg:61.46ms
step:885/2330 train_time:54394ms step_avg:61.46ms
step:886/2330 train_time:54457ms step_avg:61.46ms
step:887/2330 train_time:54518ms step_avg:61.46ms
step:888/2330 train_time:54582ms step_avg:61.47ms
step:889/2330 train_time:54643ms step_avg:61.47ms
step:890/2330 train_time:54707ms step_avg:61.47ms
step:891/2330 train_time:54767ms step_avg:61.47ms
step:892/2330 train_time:54831ms step_avg:61.47ms
step:893/2330 train_time:54892ms step_avg:61.47ms
step:894/2330 train_time:54955ms step_avg:61.47ms
step:895/2330 train_time:55015ms step_avg:61.47ms
step:896/2330 train_time:55079ms step_avg:61.47ms
step:897/2330 train_time:55141ms step_avg:61.47ms
step:898/2330 train_time:55206ms step_avg:61.48ms
step:899/2330 train_time:55267ms step_avg:61.48ms
step:900/2330 train_time:55331ms step_avg:61.48ms
step:901/2330 train_time:55391ms step_avg:61.48ms
step:902/2330 train_time:55454ms step_avg:61.48ms
step:903/2330 train_time:55514ms step_avg:61.48ms
step:904/2330 train_time:55577ms step_avg:61.48ms
step:905/2330 train_time:55638ms step_avg:61.48ms
step:906/2330 train_time:55702ms step_avg:61.48ms
step:907/2330 train_time:55764ms step_avg:61.48ms
step:908/2330 train_time:55828ms step_avg:61.48ms
step:909/2330 train_time:55889ms step_avg:61.48ms
step:910/2330 train_time:55952ms step_avg:61.49ms
step:911/2330 train_time:56012ms step_avg:61.48ms
step:912/2330 train_time:56076ms step_avg:61.49ms
step:913/2330 train_time:56137ms step_avg:61.49ms
step:914/2330 train_time:56201ms step_avg:61.49ms
step:915/2330 train_time:56263ms step_avg:61.49ms
step:916/2330 train_time:56327ms step_avg:61.49ms
step:917/2330 train_time:56387ms step_avg:61.49ms
step:918/2330 train_time:56451ms step_avg:61.49ms
step:919/2330 train_time:56511ms step_avg:61.49ms
step:920/2330 train_time:56574ms step_avg:61.49ms
step:921/2330 train_time:56635ms step_avg:61.49ms
step:922/2330 train_time:56699ms step_avg:61.50ms
step:923/2330 train_time:56760ms step_avg:61.50ms
step:924/2330 train_time:56825ms step_avg:61.50ms
step:925/2330 train_time:56886ms step_avg:61.50ms
step:926/2330 train_time:56949ms step_avg:61.50ms
step:927/2330 train_time:57010ms step_avg:61.50ms
step:928/2330 train_time:57073ms step_avg:61.50ms
step:929/2330 train_time:57134ms step_avg:61.50ms
step:930/2330 train_time:57198ms step_avg:61.50ms
step:931/2330 train_time:57258ms step_avg:61.50ms
step:932/2330 train_time:57322ms step_avg:61.50ms
step:933/2330 train_time:57384ms step_avg:61.50ms
step:934/2330 train_time:57447ms step_avg:61.51ms
step:935/2330 train_time:57507ms step_avg:61.50ms
step:936/2330 train_time:57570ms step_avg:61.51ms
step:937/2330 train_time:57631ms step_avg:61.51ms
step:938/2330 train_time:57694ms step_avg:61.51ms
step:939/2330 train_time:57755ms step_avg:61.51ms
step:940/2330 train_time:57818ms step_avg:61.51ms
step:941/2330 train_time:57879ms step_avg:61.51ms
step:942/2330 train_time:57943ms step_avg:61.51ms
step:943/2330 train_time:58005ms step_avg:61.51ms
step:944/2330 train_time:58068ms step_avg:61.51ms
step:945/2330 train_time:58129ms step_avg:61.51ms
step:946/2330 train_time:58193ms step_avg:61.51ms
step:947/2330 train_time:58253ms step_avg:61.51ms
step:948/2330 train_time:58316ms step_avg:61.52ms
step:949/2330 train_time:58377ms step_avg:61.51ms
step:950/2330 train_time:58441ms step_avg:61.52ms
step:951/2330 train_time:58502ms step_avg:61.52ms
step:952/2330 train_time:58566ms step_avg:61.52ms
step:953/2330 train_time:58627ms step_avg:61.52ms
step:954/2330 train_time:58691ms step_avg:61.52ms
step:955/2330 train_time:58751ms step_avg:61.52ms
step:956/2330 train_time:58814ms step_avg:61.52ms
step:957/2330 train_time:58875ms step_avg:61.52ms
step:958/2330 train_time:58938ms step_avg:61.52ms
step:959/2330 train_time:58999ms step_avg:61.52ms
step:960/2330 train_time:59064ms step_avg:61.52ms
step:961/2330 train_time:59125ms step_avg:61.52ms
step:962/2330 train_time:59189ms step_avg:61.53ms
step:963/2330 train_time:59250ms step_avg:61.53ms
step:964/2330 train_time:59313ms step_avg:61.53ms
step:965/2330 train_time:59373ms step_avg:61.53ms
step:966/2330 train_time:59437ms step_avg:61.53ms
step:967/2330 train_time:59497ms step_avg:61.53ms
step:968/2330 train_time:59561ms step_avg:61.53ms
step:969/2330 train_time:59622ms step_avg:61.53ms
step:970/2330 train_time:59686ms step_avg:61.53ms
step:971/2330 train_time:59747ms step_avg:61.53ms
step:972/2330 train_time:59810ms step_avg:61.53ms
step:973/2330 train_time:59869ms step_avg:61.53ms
step:974/2330 train_time:59933ms step_avg:61.53ms
step:975/2330 train_time:59993ms step_avg:61.53ms
step:976/2330 train_time:60056ms step_avg:61.53ms
step:977/2330 train_time:60117ms step_avg:61.53ms
step:978/2330 train_time:60181ms step_avg:61.54ms
step:979/2330 train_time:60242ms step_avg:61.53ms
step:980/2330 train_time:60306ms step_avg:61.54ms
step:981/2330 train_time:60366ms step_avg:61.53ms
step:982/2330 train_time:60429ms step_avg:61.54ms
step:983/2330 train_time:60490ms step_avg:61.54ms
step:984/2330 train_time:60554ms step_avg:61.54ms
step:985/2330 train_time:60615ms step_avg:61.54ms
step:986/2330 train_time:60679ms step_avg:61.54ms
step:987/2330 train_time:60739ms step_avg:61.54ms
step:988/2330 train_time:60803ms step_avg:61.54ms
step:989/2330 train_time:60864ms step_avg:61.54ms
step:990/2330 train_time:60927ms step_avg:61.54ms
step:991/2330 train_time:60987ms step_avg:61.54ms
step:992/2330 train_time:61051ms step_avg:61.54ms
step:993/2330 train_time:61112ms step_avg:61.54ms
step:994/2330 train_time:61175ms step_avg:61.54ms
step:995/2330 train_time:61235ms step_avg:61.54ms
step:996/2330 train_time:61300ms step_avg:61.55ms
step:997/2330 train_time:61361ms step_avg:61.55ms
step:998/2330 train_time:61425ms step_avg:61.55ms
step:999/2330 train_time:61485ms step_avg:61.55ms
step:1000/2330 train_time:61549ms step_avg:61.55ms
step:1000/2330 val_loss:3.7926 train_time:61615ms step_avg:61.61ms
step:1001/2330 train_time:61638ms step_avg:61.58ms
step:1002/2330 train_time:61675ms step_avg:61.55ms
step:1003/2330 train_time:61738ms step_avg:61.55ms
step:1004/2330 train_time:61807ms step_avg:61.56ms
step:1005/2330 train_time:61870ms step_avg:61.56ms
step:1006/2330 train_time:61934ms step_avg:61.56ms
step:1007/2330 train_time:61994ms step_avg:61.56ms
step:1008/2330 train_time:62056ms step_avg:61.56ms
step:1009/2330 train_time:62117ms step_avg:61.56ms
step:1010/2330 train_time:62180ms step_avg:61.56ms
step:1011/2330 train_time:62239ms step_avg:61.56ms
step:1012/2330 train_time:62302ms step_avg:61.56ms
step:1013/2330 train_time:62362ms step_avg:61.56ms
step:1014/2330 train_time:62425ms step_avg:61.56ms
step:1015/2330 train_time:62484ms step_avg:61.56ms
step:1016/2330 train_time:62549ms step_avg:61.56ms
step:1017/2330 train_time:62613ms step_avg:61.57ms
step:1018/2330 train_time:62679ms step_avg:61.57ms
step:1019/2330 train_time:62741ms step_avg:61.57ms
step:1020/2330 train_time:62805ms step_avg:61.57ms
step:1021/2330 train_time:62866ms step_avg:61.57ms
step:1022/2330 train_time:62930ms step_avg:61.58ms
step:1023/2330 train_time:62990ms step_avg:61.57ms
step:1024/2330 train_time:63054ms step_avg:61.58ms
step:1025/2330 train_time:63115ms step_avg:61.58ms
step:1026/2330 train_time:63179ms step_avg:61.58ms
step:1027/2330 train_time:63239ms step_avg:61.58ms
step:1028/2330 train_time:63302ms step_avg:61.58ms
step:1029/2330 train_time:63361ms step_avg:61.58ms
step:1030/2330 train_time:63425ms step_avg:61.58ms
step:1031/2330 train_time:63485ms step_avg:61.58ms
step:1032/2330 train_time:63549ms step_avg:61.58ms
step:1033/2330 train_time:63611ms step_avg:61.58ms
step:1034/2330 train_time:63677ms step_avg:61.58ms
step:1035/2330 train_time:63739ms step_avg:61.58ms
step:1036/2330 train_time:63803ms step_avg:61.59ms
step:1037/2330 train_time:63864ms step_avg:61.59ms
step:1038/2330 train_time:63928ms step_avg:61.59ms
step:1039/2330 train_time:63988ms step_avg:61.59ms
step:1040/2330 train_time:64052ms step_avg:61.59ms
step:1041/2330 train_time:64113ms step_avg:61.59ms
step:1042/2330 train_time:64177ms step_avg:61.59ms
step:1043/2330 train_time:64237ms step_avg:61.59ms
step:1044/2330 train_time:64301ms step_avg:61.59ms
step:1045/2330 train_time:64361ms step_avg:61.59ms
step:1046/2330 train_time:64425ms step_avg:61.59ms
step:1047/2330 train_time:64485ms step_avg:61.59ms
step:1048/2330 train_time:64549ms step_avg:61.59ms
step:1049/2330 train_time:64610ms step_avg:61.59ms
step:1050/2330 train_time:64676ms step_avg:61.60ms
step:1051/2330 train_time:64737ms step_avg:61.60ms
step:1052/2330 train_time:64801ms step_avg:61.60ms
step:1053/2330 train_time:64862ms step_avg:61.60ms
step:1054/2330 train_time:64925ms step_avg:61.60ms
step:1055/2330 train_time:64986ms step_avg:61.60ms
step:1056/2330 train_time:65050ms step_avg:61.60ms
step:1057/2330 train_time:65111ms step_avg:61.60ms
step:1058/2330 train_time:65175ms step_avg:61.60ms
step:1059/2330 train_time:65236ms step_avg:61.60ms
step:1060/2330 train_time:65300ms step_avg:61.60ms
step:1061/2330 train_time:65360ms step_avg:61.60ms
step:1062/2330 train_time:65423ms step_avg:61.60ms
step:1063/2330 train_time:65483ms step_avg:61.60ms
step:1064/2330 train_time:65547ms step_avg:61.60ms
step:1065/2330 train_time:65607ms step_avg:61.60ms
step:1066/2330 train_time:65673ms step_avg:61.61ms
step:1067/2330 train_time:65735ms step_avg:61.61ms
step:1068/2330 train_time:65800ms step_avg:61.61ms
step:1069/2330 train_time:65860ms step_avg:61.61ms
step:1070/2330 train_time:65924ms step_avg:61.61ms
step:1071/2330 train_time:65984ms step_avg:61.61ms
step:1072/2330 train_time:66047ms step_avg:61.61ms
step:1073/2330 train_time:66108ms step_avg:61.61ms
step:1074/2330 train_time:66173ms step_avg:61.61ms
step:1075/2330 train_time:66234ms step_avg:61.61ms
step:1076/2330 train_time:66299ms step_avg:61.62ms
step:1077/2330 train_time:66359ms step_avg:61.61ms
step:1078/2330 train_time:66423ms step_avg:61.62ms
step:1079/2330 train_time:66483ms step_avg:61.62ms
step:1080/2330 train_time:66546ms step_avg:61.62ms
step:1081/2330 train_time:66607ms step_avg:61.62ms
step:1082/2330 train_time:66671ms step_avg:61.62ms
step:1083/2330 train_time:66733ms step_avg:61.62ms
step:1084/2330 train_time:66797ms step_avg:61.62ms
step:1085/2330 train_time:66858ms step_avg:61.62ms
step:1086/2330 train_time:66922ms step_avg:61.62ms
step:1087/2330 train_time:66983ms step_avg:61.62ms
step:1088/2330 train_time:67046ms step_avg:61.62ms
step:1089/2330 train_time:67107ms step_avg:61.62ms
step:1090/2330 train_time:67171ms step_avg:61.62ms
step:1091/2330 train_time:67231ms step_avg:61.62ms
step:1092/2330 train_time:67296ms step_avg:61.63ms
step:1093/2330 train_time:67357ms step_avg:61.63ms
step:1094/2330 train_time:67421ms step_avg:61.63ms
step:1095/2330 train_time:67481ms step_avg:61.63ms
step:1096/2330 train_time:67545ms step_avg:61.63ms
step:1097/2330 train_time:67606ms step_avg:61.63ms
step:1098/2330 train_time:67669ms step_avg:61.63ms
step:1099/2330 train_time:67731ms step_avg:61.63ms
step:1100/2330 train_time:67796ms step_avg:61.63ms
step:1101/2330 train_time:67857ms step_avg:61.63ms
step:1102/2330 train_time:67922ms step_avg:61.63ms
step:1103/2330 train_time:67982ms step_avg:61.63ms
step:1104/2330 train_time:68046ms step_avg:61.64ms
step:1105/2330 train_time:68106ms step_avg:61.63ms
step:1106/2330 train_time:68170ms step_avg:61.64ms
step:1107/2330 train_time:68231ms step_avg:61.64ms
step:1108/2330 train_time:68295ms step_avg:61.64ms
step:1109/2330 train_time:68356ms step_avg:61.64ms
step:1110/2330 train_time:68421ms step_avg:61.64ms
step:1111/2330 train_time:68482ms step_avg:61.64ms
step:1112/2330 train_time:68545ms step_avg:61.64ms
step:1113/2330 train_time:68605ms step_avg:61.64ms
step:1114/2330 train_time:68668ms step_avg:61.64ms
step:1115/2330 train_time:68729ms step_avg:61.64ms
step:1116/2330 train_time:68794ms step_avg:61.64ms
step:1117/2330 train_time:68855ms step_avg:61.64ms
step:1118/2330 train_time:68919ms step_avg:61.64ms
step:1119/2330 train_time:68980ms step_avg:61.64ms
step:1120/2330 train_time:69043ms step_avg:61.65ms
step:1121/2330 train_time:69104ms step_avg:61.64ms
step:1122/2330 train_time:69167ms step_avg:61.65ms
step:1123/2330 train_time:69228ms step_avg:61.65ms
step:1124/2330 train_time:69292ms step_avg:61.65ms
step:1125/2330 train_time:69353ms step_avg:61.65ms
step:1126/2330 train_time:69419ms step_avg:61.65ms
step:1127/2330 train_time:69479ms step_avg:61.65ms
step:1128/2330 train_time:69543ms step_avg:61.65ms
step:1129/2330 train_time:69603ms step_avg:61.65ms
step:1130/2330 train_time:69667ms step_avg:61.65ms
step:1131/2330 train_time:69728ms step_avg:61.65ms
step:1132/2330 train_time:69792ms step_avg:61.65ms
step:1133/2330 train_time:69854ms step_avg:61.65ms
step:1134/2330 train_time:69918ms step_avg:61.66ms
step:1135/2330 train_time:69979ms step_avg:61.66ms
step:1136/2330 train_time:70043ms step_avg:61.66ms
step:1137/2330 train_time:70104ms step_avg:61.66ms
step:1138/2330 train_time:70167ms step_avg:61.66ms
step:1139/2330 train_time:70227ms step_avg:61.66ms
step:1140/2330 train_time:70291ms step_avg:61.66ms
step:1141/2330 train_time:70353ms step_avg:61.66ms
step:1142/2330 train_time:70417ms step_avg:61.66ms
step:1143/2330 train_time:70478ms step_avg:61.66ms
step:1144/2330 train_time:70542ms step_avg:61.66ms
step:1145/2330 train_time:70603ms step_avg:61.66ms
step:1146/2330 train_time:70666ms step_avg:61.66ms
step:1147/2330 train_time:70727ms step_avg:61.66ms
step:1148/2330 train_time:70790ms step_avg:61.66ms
step:1149/2330 train_time:70852ms step_avg:61.66ms
step:1150/2330 train_time:70917ms step_avg:61.67ms
step:1151/2330 train_time:70978ms step_avg:61.67ms
step:1152/2330 train_time:71042ms step_avg:61.67ms
step:1153/2330 train_time:71102ms step_avg:61.67ms
step:1154/2330 train_time:71165ms step_avg:61.67ms
step:1155/2330 train_time:71226ms step_avg:61.67ms
step:1156/2330 train_time:71290ms step_avg:61.67ms
step:1157/2330 train_time:71351ms step_avg:61.67ms
step:1158/2330 train_time:71416ms step_avg:61.67ms
step:1159/2330 train_time:71477ms step_avg:61.67ms
step:1160/2330 train_time:71541ms step_avg:61.67ms
step:1161/2330 train_time:71601ms step_avg:61.67ms
step:1162/2330 train_time:71664ms step_avg:61.67ms
step:1163/2330 train_time:71725ms step_avg:61.67ms
step:1164/2330 train_time:71788ms step_avg:61.67ms
step:1165/2330 train_time:71849ms step_avg:61.67ms
step:1166/2330 train_time:71914ms step_avg:61.68ms
step:1167/2330 train_time:71976ms step_avg:61.68ms
step:1168/2330 train_time:72039ms step_avg:61.68ms
step:1169/2330 train_time:72100ms step_avg:61.68ms
step:1170/2330 train_time:72164ms step_avg:61.68ms
step:1171/2330 train_time:72225ms step_avg:61.68ms
step:1172/2330 train_time:72288ms step_avg:61.68ms
step:1173/2330 train_time:72348ms step_avg:61.68ms
step:1174/2330 train_time:72412ms step_avg:61.68ms
step:1175/2330 train_time:72473ms step_avg:61.68ms
step:1176/2330 train_time:72537ms step_avg:61.68ms
step:1177/2330 train_time:72598ms step_avg:61.68ms
step:1178/2330 train_time:72662ms step_avg:61.68ms
step:1179/2330 train_time:72722ms step_avg:61.68ms
step:1180/2330 train_time:72786ms step_avg:61.68ms
step:1181/2330 train_time:72846ms step_avg:61.68ms
step:1182/2330 train_time:72909ms step_avg:61.68ms
step:1183/2330 train_time:72970ms step_avg:61.68ms
step:1184/2330 train_time:73035ms step_avg:61.69ms
step:1185/2330 train_time:73097ms step_avg:61.69ms
step:1186/2330 train_time:73162ms step_avg:61.69ms
step:1187/2330 train_time:73223ms step_avg:61.69ms
step:1188/2330 train_time:73286ms step_avg:61.69ms
step:1189/2330 train_time:73346ms step_avg:61.69ms
step:1190/2330 train_time:73410ms step_avg:61.69ms
step:1191/2330 train_time:73471ms step_avg:61.69ms
step:1192/2330 train_time:73535ms step_avg:61.69ms
step:1193/2330 train_time:73596ms step_avg:61.69ms
step:1194/2330 train_time:73661ms step_avg:61.69ms
step:1195/2330 train_time:73721ms step_avg:61.69ms
step:1196/2330 train_time:73784ms step_avg:61.69ms
step:1197/2330 train_time:73844ms step_avg:61.69ms
step:1198/2330 train_time:73908ms step_avg:61.69ms
step:1199/2330 train_time:73969ms step_avg:61.69ms
step:1200/2330 train_time:74033ms step_avg:61.69ms
step:1201/2330 train_time:74094ms step_avg:61.69ms
step:1202/2330 train_time:74158ms step_avg:61.70ms
step:1203/2330 train_time:74219ms step_avg:61.70ms
step:1204/2330 train_time:74283ms step_avg:61.70ms
step:1205/2330 train_time:74343ms step_avg:61.70ms
step:1206/2330 train_time:74407ms step_avg:61.70ms
step:1207/2330 train_time:74467ms step_avg:61.70ms
step:1208/2330 train_time:74532ms step_avg:61.70ms
step:1209/2330 train_time:74594ms step_avg:61.70ms
step:1210/2330 train_time:74659ms step_avg:61.70ms
step:1211/2330 train_time:74720ms step_avg:61.70ms
step:1212/2330 train_time:74784ms step_avg:61.70ms
step:1213/2330 train_time:74844ms step_avg:61.70ms
step:1214/2330 train_time:74908ms step_avg:61.70ms
step:1215/2330 train_time:74968ms step_avg:61.70ms
step:1216/2330 train_time:75032ms step_avg:61.70ms
step:1217/2330 train_time:75094ms step_avg:61.70ms
step:1218/2330 train_time:75159ms step_avg:61.71ms
step:1219/2330 train_time:75220ms step_avg:61.71ms
step:1220/2330 train_time:75283ms step_avg:61.71ms
step:1221/2330 train_time:75343ms step_avg:61.71ms
step:1222/2330 train_time:75406ms step_avg:61.71ms
step:1223/2330 train_time:75467ms step_avg:61.71ms
step:1224/2330 train_time:75531ms step_avg:61.71ms
step:1225/2330 train_time:75592ms step_avg:61.71ms
step:1226/2330 train_time:75658ms step_avg:61.71ms
step:1227/2330 train_time:75719ms step_avg:61.71ms
step:1228/2330 train_time:75782ms step_avg:61.71ms
step:1229/2330 train_time:75843ms step_avg:61.71ms
step:1230/2330 train_time:75906ms step_avg:61.71ms
step:1231/2330 train_time:75967ms step_avg:61.71ms
step:1232/2330 train_time:76030ms step_avg:61.71ms
step:1233/2330 train_time:76091ms step_avg:61.71ms
step:1234/2330 train_time:76156ms step_avg:61.71ms
step:1235/2330 train_time:76218ms step_avg:61.71ms
step:1236/2330 train_time:76282ms step_avg:61.72ms
step:1237/2330 train_time:76343ms step_avg:61.72ms
step:1238/2330 train_time:76406ms step_avg:61.72ms
step:1239/2330 train_time:76466ms step_avg:61.72ms
step:1240/2330 train_time:76530ms step_avg:61.72ms
step:1241/2330 train_time:76591ms step_avg:61.72ms
step:1242/2330 train_time:76656ms step_avg:61.72ms
step:1243/2330 train_time:76718ms step_avg:61.72ms
step:1244/2330 train_time:76782ms step_avg:61.72ms
step:1245/2330 train_time:76843ms step_avg:61.72ms
step:1246/2330 train_time:76906ms step_avg:61.72ms
step:1247/2330 train_time:76967ms step_avg:61.72ms
step:1248/2330 train_time:77031ms step_avg:61.72ms
step:1249/2330 train_time:77091ms step_avg:61.72ms
step:1250/2330 train_time:77155ms step_avg:61.72ms
step:1250/2330 val_loss:3.7112 train_time:77220ms step_avg:61.78ms
step:1251/2330 train_time:77241ms step_avg:61.74ms
step:1252/2330 train_time:77283ms step_avg:61.73ms
step:1253/2330 train_time:77349ms step_avg:61.73ms
step:1254/2330 train_time:77414ms step_avg:61.73ms
step:1255/2330 train_time:77475ms step_avg:61.73ms
step:1256/2330 train_time:77539ms step_avg:61.74ms
step:1257/2330 train_time:77599ms step_avg:61.73ms
step:1258/2330 train_time:77663ms step_avg:61.74ms
step:1259/2330 train_time:77723ms step_avg:61.73ms
step:1260/2330 train_time:77786ms step_avg:61.73ms
step:1261/2330 train_time:77847ms step_avg:61.73ms
step:1262/2330 train_time:77910ms step_avg:61.73ms
step:1263/2330 train_time:77969ms step_avg:61.73ms
step:1264/2330 train_time:78032ms step_avg:61.73ms
step:1265/2330 train_time:78091ms step_avg:61.73ms
step:1266/2330 train_time:78155ms step_avg:61.73ms
step:1267/2330 train_time:78216ms step_avg:61.73ms
step:1268/2330 train_time:78283ms step_avg:61.74ms
step:1269/2330 train_time:78346ms step_avg:61.74ms
step:1270/2330 train_time:78411ms step_avg:61.74ms
step:1271/2330 train_time:78472ms step_avg:61.74ms
step:1272/2330 train_time:78535ms step_avg:61.74ms
step:1273/2330 train_time:78596ms step_avg:61.74ms
step:1274/2330 train_time:78660ms step_avg:61.74ms
step:1275/2330 train_time:78721ms step_avg:61.74ms
step:1276/2330 train_time:78785ms step_avg:61.74ms
step:1277/2330 train_time:78845ms step_avg:61.74ms
step:1278/2330 train_time:78909ms step_avg:61.74ms
step:1279/2330 train_time:78968ms step_avg:61.74ms
step:1280/2330 train_time:79031ms step_avg:61.74ms
step:1281/2330 train_time:79091ms step_avg:61.74ms
step:1282/2330 train_time:79154ms step_avg:61.74ms
step:1283/2330 train_time:79216ms step_avg:61.74ms
step:1284/2330 train_time:79281ms step_avg:61.75ms
step:1285/2330 train_time:79343ms step_avg:61.75ms
step:1286/2330 train_time:79408ms step_avg:61.75ms
step:1287/2330 train_time:79469ms step_avg:61.75ms
step:1288/2330 train_time:79533ms step_avg:61.75ms
step:1289/2330 train_time:79594ms step_avg:61.75ms
step:1290/2330 train_time:79657ms step_avg:61.75ms
step:1291/2330 train_time:79719ms step_avg:61.75ms
step:1292/2330 train_time:79783ms step_avg:61.75ms
step:1293/2330 train_time:79844ms step_avg:61.75ms
step:1294/2330 train_time:79908ms step_avg:61.75ms
step:1295/2330 train_time:79968ms step_avg:61.75ms
step:1296/2330 train_time:80032ms step_avg:61.75ms
step:1297/2330 train_time:80092ms step_avg:61.75ms
step:1298/2330 train_time:80156ms step_avg:61.75ms
step:1299/2330 train_time:80217ms step_avg:61.75ms
step:1300/2330 train_time:80282ms step_avg:61.76ms
step:1301/2330 train_time:80344ms step_avg:61.76ms
step:1302/2330 train_time:80408ms step_avg:61.76ms
step:1303/2330 train_time:80469ms step_avg:61.76ms
step:1304/2330 train_time:80533ms step_avg:61.76ms
step:1305/2330 train_time:80593ms step_avg:61.76ms
step:1306/2330 train_time:80657ms step_avg:61.76ms
step:1307/2330 train_time:80718ms step_avg:61.76ms
step:1308/2330 train_time:80783ms step_avg:61.76ms
step:1309/2330 train_time:80845ms step_avg:61.76ms
step:1310/2330 train_time:80909ms step_avg:61.76ms
step:1311/2330 train_time:80969ms step_avg:61.76ms
step:1312/2330 train_time:81033ms step_avg:61.76ms
step:1313/2330 train_time:81093ms step_avg:61.76ms
step:1314/2330 train_time:81156ms step_avg:61.76ms
step:1315/2330 train_time:81217ms step_avg:61.76ms
step:1316/2330 train_time:81282ms step_avg:61.76ms
step:1317/2330 train_time:81343ms step_avg:61.76ms
step:1318/2330 train_time:81407ms step_avg:61.77ms
step:1319/2330 train_time:81468ms step_avg:61.76ms
step:1320/2330 train_time:81532ms step_avg:61.77ms
step:1321/2330 train_time:81593ms step_avg:61.77ms
step:1322/2330 train_time:81657ms step_avg:61.77ms
step:1323/2330 train_time:81717ms step_avg:61.77ms
step:1324/2330 train_time:81782ms step_avg:61.77ms
step:1325/2330 train_time:81843ms step_avg:61.77ms
step:1326/2330 train_time:81907ms step_avg:61.77ms
step:1327/2330 train_time:81967ms step_avg:61.77ms
step:1328/2330 train_time:82030ms step_avg:61.77ms
step:1329/2330 train_time:82091ms step_avg:61.77ms
step:1330/2330 train_time:82154ms step_avg:61.77ms
step:1331/2330 train_time:82215ms step_avg:61.77ms
step:1332/2330 train_time:82279ms step_avg:61.77ms
step:1333/2330 train_time:82341ms step_avg:61.77ms
step:1334/2330 train_time:82405ms step_avg:61.77ms
step:1335/2330 train_time:82466ms step_avg:61.77ms
step:1336/2330 train_time:82530ms step_avg:61.77ms
step:1337/2330 train_time:82590ms step_avg:61.77ms
step:1338/2330 train_time:82654ms step_avg:61.77ms
step:1339/2330 train_time:82714ms step_avg:61.77ms
step:1340/2330 train_time:82779ms step_avg:61.78ms
step:1341/2330 train_time:82840ms step_avg:61.77ms
step:1342/2330 train_time:82904ms step_avg:61.78ms
step:1343/2330 train_time:82965ms step_avg:61.78ms
step:1344/2330 train_time:83029ms step_avg:61.78ms
step:1345/2330 train_time:83089ms step_avg:61.78ms
step:1346/2330 train_time:83152ms step_avg:61.78ms
step:1347/2330 train_time:83213ms step_avg:61.78ms
step:1348/2330 train_time:83276ms step_avg:61.78ms
step:1349/2330 train_time:83337ms step_avg:61.78ms
step:1350/2330 train_time:83402ms step_avg:61.78ms
step:1351/2330 train_time:83463ms step_avg:61.78ms
step:1352/2330 train_time:83527ms step_avg:61.78ms
step:1353/2330 train_time:83588ms step_avg:61.78ms
step:1354/2330 train_time:83652ms step_avg:61.78ms
step:1355/2330 train_time:83713ms step_avg:61.78ms
step:1356/2330 train_time:83776ms step_avg:61.78ms
step:1357/2330 train_time:83836ms step_avg:61.78ms
step:1358/2330 train_time:83901ms step_avg:61.78ms
step:1359/2330 train_time:83963ms step_avg:61.78ms
step:1360/2330 train_time:84027ms step_avg:61.78ms
step:1361/2330 train_time:84087ms step_avg:61.78ms
step:1362/2330 train_time:84151ms step_avg:61.79ms
step:1363/2330 train_time:84211ms step_avg:61.78ms
step:1364/2330 train_time:84275ms step_avg:61.79ms
step:1365/2330 train_time:84335ms step_avg:61.78ms
step:1366/2330 train_time:84400ms step_avg:61.79ms
step:1367/2330 train_time:84462ms step_avg:61.79ms
step:1368/2330 train_time:84526ms step_avg:61.79ms
step:1369/2330 train_time:84586ms step_avg:61.79ms
step:1370/2330 train_time:84650ms step_avg:61.79ms
step:1371/2330 train_time:84710ms step_avg:61.79ms
step:1372/2330 train_time:84774ms step_avg:61.79ms
step:1373/2330 train_time:84834ms step_avg:61.79ms
step:1374/2330 train_time:84898ms step_avg:61.79ms
step:1375/2330 train_time:84959ms step_avg:61.79ms
step:1376/2330 train_time:85024ms step_avg:61.79ms
step:1377/2330 train_time:85085ms step_avg:61.79ms
step:1378/2330 train_time:85149ms step_avg:61.79ms
step:1379/2330 train_time:85209ms step_avg:61.79ms
step:1380/2330 train_time:85272ms step_avg:61.79ms
step:1381/2330 train_time:85333ms step_avg:61.79ms
step:1382/2330 train_time:85396ms step_avg:61.79ms
step:1383/2330 train_time:85457ms step_avg:61.79ms
step:1384/2330 train_time:85522ms step_avg:61.79ms
step:1385/2330 train_time:85583ms step_avg:61.79ms
step:1386/2330 train_time:85648ms step_avg:61.79ms
step:1387/2330 train_time:85708ms step_avg:61.79ms
step:1388/2330 train_time:85772ms step_avg:61.80ms
step:1389/2330 train_time:85832ms step_avg:61.79ms
step:1390/2330 train_time:85896ms step_avg:61.80ms
step:1391/2330 train_time:85957ms step_avg:61.80ms
step:1392/2330 train_time:86022ms step_avg:61.80ms
step:1393/2330 train_time:86083ms step_avg:61.80ms
step:1394/2330 train_time:86147ms step_avg:61.80ms
step:1395/2330 train_time:86208ms step_avg:61.80ms
step:1396/2330 train_time:86271ms step_avg:61.80ms
step:1397/2330 train_time:86332ms step_avg:61.80ms
step:1398/2330 train_time:86395ms step_avg:61.80ms
step:1399/2330 train_time:86456ms step_avg:61.80ms
step:1400/2330 train_time:86521ms step_avg:61.80ms
step:1401/2330 train_time:86582ms step_avg:61.80ms
step:1402/2330 train_time:86647ms step_avg:61.80ms
step:1403/2330 train_time:86707ms step_avg:61.80ms
step:1404/2330 train_time:86771ms step_avg:61.80ms
step:1405/2330 train_time:86832ms step_avg:61.80ms
step:1406/2330 train_time:86895ms step_avg:61.80ms
step:1407/2330 train_time:86956ms step_avg:61.80ms
step:1408/2330 train_time:87021ms step_avg:61.80ms
step:1409/2330 train_time:87082ms step_avg:61.80ms
step:1410/2330 train_time:87147ms step_avg:61.81ms
step:1411/2330 train_time:87208ms step_avg:61.81ms
step:1412/2330 train_time:87272ms step_avg:61.81ms
step:1413/2330 train_time:87331ms step_avg:61.81ms
step:1414/2330 train_time:87395ms step_avg:61.81ms
step:1415/2330 train_time:87455ms step_avg:61.81ms
step:1416/2330 train_time:87521ms step_avg:61.81ms
step:1417/2330 train_time:87582ms step_avg:61.81ms
step:1418/2330 train_time:87647ms step_avg:61.81ms
step:1419/2330 train_time:87708ms step_avg:61.81ms
step:1420/2330 train_time:87771ms step_avg:61.81ms
step:1421/2330 train_time:87831ms step_avg:61.81ms
step:1422/2330 train_time:87895ms step_avg:61.81ms
step:1423/2330 train_time:87956ms step_avg:61.81ms
step:1424/2330 train_time:88020ms step_avg:61.81ms
step:1425/2330 train_time:88081ms step_avg:61.81ms
step:1426/2330 train_time:88146ms step_avg:61.81ms
step:1427/2330 train_time:88206ms step_avg:61.81ms
step:1428/2330 train_time:88270ms step_avg:61.81ms
step:1429/2330 train_time:88331ms step_avg:61.81ms
step:1430/2330 train_time:88394ms step_avg:61.81ms
step:1431/2330 train_time:88455ms step_avg:61.81ms
step:1432/2330 train_time:88519ms step_avg:61.82ms
step:1433/2330 train_time:88580ms step_avg:61.81ms
step:1434/2330 train_time:88644ms step_avg:61.82ms
step:1435/2330 train_time:88705ms step_avg:61.82ms
step:1436/2330 train_time:88770ms step_avg:61.82ms
step:1437/2330 train_time:88830ms step_avg:61.82ms
step:1438/2330 train_time:88893ms step_avg:61.82ms
step:1439/2330 train_time:88954ms step_avg:61.82ms
step:1440/2330 train_time:89018ms step_avg:61.82ms
step:1441/2330 train_time:89079ms step_avg:61.82ms
step:1442/2330 train_time:89144ms step_avg:61.82ms
step:1443/2330 train_time:89205ms step_avg:61.82ms
step:1444/2330 train_time:89269ms step_avg:61.82ms
step:1445/2330 train_time:89330ms step_avg:61.82ms
step:1446/2330 train_time:89393ms step_avg:61.82ms
step:1447/2330 train_time:89454ms step_avg:61.82ms
step:1448/2330 train_time:89518ms step_avg:61.82ms
step:1449/2330 train_time:89579ms step_avg:61.82ms
step:1450/2330 train_time:89645ms step_avg:61.82ms
step:1451/2330 train_time:89706ms step_avg:61.82ms
step:1452/2330 train_time:89770ms step_avg:61.82ms
step:1453/2330 train_time:89830ms step_avg:61.82ms
step:1454/2330 train_time:89894ms step_avg:61.83ms
step:1455/2330 train_time:89954ms step_avg:61.82ms
step:1456/2330 train_time:90018ms step_avg:61.83ms
step:1457/2330 train_time:90079ms step_avg:61.83ms
step:1458/2330 train_time:90145ms step_avg:61.83ms
step:1459/2330 train_time:90205ms step_avg:61.83ms
step:1460/2330 train_time:90269ms step_avg:61.83ms
step:1461/2330 train_time:90329ms step_avg:61.83ms
step:1462/2330 train_time:90393ms step_avg:61.83ms
step:1463/2330 train_time:90454ms step_avg:61.83ms
step:1464/2330 train_time:90517ms step_avg:61.83ms
step:1465/2330 train_time:90578ms step_avg:61.83ms
step:1466/2330 train_time:90644ms step_avg:61.83ms
step:1467/2330 train_time:90704ms step_avg:61.83ms
step:1468/2330 train_time:90768ms step_avg:61.83ms
step:1469/2330 train_time:90830ms step_avg:61.83ms
step:1470/2330 train_time:90894ms step_avg:61.83ms
step:1471/2330 train_time:90954ms step_avg:61.83ms
step:1472/2330 train_time:91018ms step_avg:61.83ms
step:1473/2330 train_time:91079ms step_avg:61.83ms
step:1474/2330 train_time:91144ms step_avg:61.83ms
step:1475/2330 train_time:91205ms step_avg:61.83ms
step:1476/2330 train_time:91269ms step_avg:61.84ms
step:1477/2330 train_time:91330ms step_avg:61.83ms
step:1478/2330 train_time:91394ms step_avg:61.84ms
step:1479/2330 train_time:91454ms step_avg:61.83ms
step:1480/2330 train_time:91518ms step_avg:61.84ms
step:1481/2330 train_time:91579ms step_avg:61.84ms
step:1482/2330 train_time:91643ms step_avg:61.84ms
step:1483/2330 train_time:91704ms step_avg:61.84ms
step:1484/2330 train_time:91767ms step_avg:61.84ms
step:1485/2330 train_time:91828ms step_avg:61.84ms
step:1486/2330 train_time:91891ms step_avg:61.84ms
step:1487/2330 train_time:91952ms step_avg:61.84ms
step:1488/2330 train_time:92015ms step_avg:61.84ms
step:1489/2330 train_time:92076ms step_avg:61.84ms
step:1490/2330 train_time:92140ms step_avg:61.84ms
step:1491/2330 train_time:92201ms step_avg:61.84ms
step:1492/2330 train_time:92266ms step_avg:61.84ms
step:1493/2330 train_time:92326ms step_avg:61.84ms
step:1494/2330 train_time:92390ms step_avg:61.84ms
step:1495/2330 train_time:92451ms step_avg:61.84ms
step:1496/2330 train_time:92514ms step_avg:61.84ms
step:1497/2330 train_time:92574ms step_avg:61.84ms
step:1498/2330 train_time:92638ms step_avg:61.84ms
step:1499/2330 train_time:92701ms step_avg:61.84ms
step:1500/2330 train_time:92765ms step_avg:61.84ms
step:1500/2330 val_loss:3.6591 train_time:92830ms step_avg:61.89ms
step:1501/2330 train_time:92853ms step_avg:61.86ms
step:1502/2330 train_time:92894ms step_avg:61.85ms
step:1503/2330 train_time:92958ms step_avg:61.85ms
step:1504/2330 train_time:93024ms step_avg:61.85ms
step:1505/2330 train_time:93085ms step_avg:61.85ms
step:1506/2330 train_time:93149ms step_avg:61.85ms
step:1507/2330 train_time:93209ms step_avg:61.85ms
step:1508/2330 train_time:93273ms step_avg:61.85ms
step:1509/2330 train_time:93333ms step_avg:61.85ms
step:1510/2330 train_time:93395ms step_avg:61.85ms
step:1511/2330 train_time:93455ms step_avg:61.85ms
step:1512/2330 train_time:93518ms step_avg:61.85ms
step:1513/2330 train_time:93578ms step_avg:61.85ms
step:1514/2330 train_time:93641ms step_avg:61.85ms
step:1515/2330 train_time:93701ms step_avg:61.85ms
step:1516/2330 train_time:93764ms step_avg:61.85ms
step:1517/2330 train_time:93827ms step_avg:61.85ms
step:1518/2330 train_time:93893ms step_avg:61.85ms
step:1519/2330 train_time:93954ms step_avg:61.85ms
step:1520/2330 train_time:94019ms step_avg:61.85ms
step:1521/2330 train_time:94080ms step_avg:61.85ms
step:1522/2330 train_time:94144ms step_avg:61.86ms
step:1523/2330 train_time:94204ms step_avg:61.85ms
step:1524/2330 train_time:94268ms step_avg:61.86ms
step:1525/2330 train_time:94329ms step_avg:61.85ms
step:1526/2330 train_time:94393ms step_avg:61.86ms
step:1527/2330 train_time:94453ms step_avg:61.86ms
step:1528/2330 train_time:94516ms step_avg:61.86ms
step:1529/2330 train_time:94576ms step_avg:61.85ms
step:1530/2330 train_time:94640ms step_avg:61.86ms
step:1531/2330 train_time:94700ms step_avg:61.86ms
step:1532/2330 train_time:94765ms step_avg:61.86ms
step:1533/2330 train_time:94826ms step_avg:61.86ms
step:1534/2330 train_time:94892ms step_avg:61.86ms
step:1535/2330 train_time:94954ms step_avg:61.86ms
step:1536/2330 train_time:95018ms step_avg:61.86ms
step:1537/2330 train_time:95080ms step_avg:61.86ms
step:1538/2330 train_time:95144ms step_avg:61.86ms
step:1539/2330 train_time:95205ms step_avg:61.86ms
step:1540/2330 train_time:95270ms step_avg:61.86ms
step:1541/2330 train_time:95331ms step_avg:61.86ms
step:1542/2330 train_time:95394ms step_avg:61.86ms
step:1543/2330 train_time:95455ms step_avg:61.86ms
step:1544/2330 train_time:95519ms step_avg:61.86ms
step:1545/2330 train_time:95579ms step_avg:61.86ms
step:1546/2330 train_time:95643ms step_avg:61.87ms
step:1547/2330 train_time:95704ms step_avg:61.86ms
step:1548/2330 train_time:95768ms step_avg:61.87ms
step:1549/2330 train_time:95830ms step_avg:61.87ms
step:1550/2330 train_time:95895ms step_avg:61.87ms
step:1551/2330 train_time:95956ms step_avg:61.87ms
step:1552/2330 train_time:96021ms step_avg:61.87ms
step:1553/2330 train_time:96082ms step_avg:61.87ms
step:1554/2330 train_time:96146ms step_avg:61.87ms
step:1555/2330 train_time:96208ms step_avg:61.87ms
step:1556/2330 train_time:96273ms step_avg:61.87ms
step:1557/2330 train_time:96334ms step_avg:61.87ms
step:1558/2330 train_time:96398ms step_avg:61.87ms
step:1559/2330 train_time:96459ms step_avg:61.87ms
step:1560/2330 train_time:96523ms step_avg:61.87ms
step:1561/2330 train_time:96584ms step_avg:61.87ms
step:1562/2330 train_time:96648ms step_avg:61.87ms
step:1563/2330 train_time:96709ms step_avg:61.87ms
step:1564/2330 train_time:96773ms step_avg:61.88ms
step:1565/2330 train_time:96835ms step_avg:61.88ms
step:1566/2330 train_time:96899ms step_avg:61.88ms
step:1567/2330 train_time:96961ms step_avg:61.88ms
step:1568/2330 train_time:97025ms step_avg:61.88ms
step:1569/2330 train_time:97086ms step_avg:61.88ms
step:1570/2330 train_time:97151ms step_avg:61.88ms
step:1571/2330 train_time:97213ms step_avg:61.88ms
step:1572/2330 train_time:97278ms step_avg:61.88ms
step:1573/2330 train_time:97338ms step_avg:61.88ms
step:1574/2330 train_time:97402ms step_avg:61.88ms
step:1575/2330 train_time:97463ms step_avg:61.88ms
step:1576/2330 train_time:97528ms step_avg:61.88ms
step:1577/2330 train_time:97589ms step_avg:61.88ms
step:1578/2330 train_time:97654ms step_avg:61.88ms
step:1579/2330 train_time:97715ms step_avg:61.88ms
step:1580/2330 train_time:97779ms step_avg:61.89ms
step:1581/2330 train_time:97841ms step_avg:61.89ms
step:1582/2330 train_time:97905ms step_avg:61.89ms
step:1583/2330 train_time:97967ms step_avg:61.89ms
step:1584/2330 train_time:98032ms step_avg:61.89ms
step:1585/2330 train_time:98092ms step_avg:61.89ms
step:1586/2330 train_time:98157ms step_avg:61.89ms
step:1587/2330 train_time:98218ms step_avg:61.89ms
step:1588/2330 train_time:98282ms step_avg:61.89ms
step:1589/2330 train_time:98344ms step_avg:61.89ms
step:1590/2330 train_time:98408ms step_avg:61.89ms
step:1591/2330 train_time:98470ms step_avg:61.89ms
step:1592/2330 train_time:98534ms step_avg:61.89ms
step:1593/2330 train_time:98594ms step_avg:61.89ms
step:1594/2330 train_time:98658ms step_avg:61.89ms
step:1595/2330 train_time:98719ms step_avg:61.89ms
step:1596/2330 train_time:98783ms step_avg:61.89ms
step:1597/2330 train_time:98845ms step_avg:61.89ms
step:1598/2330 train_time:98908ms step_avg:61.90ms
step:1599/2330 train_time:98970ms step_avg:61.89ms
step:1600/2330 train_time:99034ms step_avg:61.90ms
step:1601/2330 train_time:99096ms step_avg:61.90ms
step:1602/2330 train_time:99160ms step_avg:61.90ms
step:1603/2330 train_time:99220ms step_avg:61.90ms
step:1604/2330 train_time:99285ms step_avg:61.90ms
step:1605/2330 train_time:99346ms step_avg:61.90ms
step:1606/2330 train_time:99410ms step_avg:61.90ms
step:1607/2330 train_time:99471ms step_avg:61.90ms
step:1608/2330 train_time:99535ms step_avg:61.90ms
step:1609/2330 train_time:99596ms step_avg:61.90ms
step:1610/2330 train_time:99660ms step_avg:61.90ms
step:1611/2330 train_time:99721ms step_avg:61.90ms
step:1612/2330 train_time:99785ms step_avg:61.90ms
step:1613/2330 train_time:99846ms step_avg:61.90ms
step:1614/2330 train_time:99911ms step_avg:61.90ms
step:1615/2330 train_time:99973ms step_avg:61.90ms
step:1616/2330 train_time:100038ms step_avg:61.90ms
step:1617/2330 train_time:100099ms step_avg:61.90ms
step:1618/2330 train_time:100164ms step_avg:61.91ms
step:1619/2330 train_time:100225ms step_avg:61.91ms
step:1620/2330 train_time:100290ms step_avg:61.91ms
step:1621/2330 train_time:100352ms step_avg:61.91ms
step:1622/2330 train_time:100416ms step_avg:61.91ms
step:1623/2330 train_time:100477ms step_avg:61.91ms
step:1624/2330 train_time:100542ms step_avg:61.91ms
step:1625/2330 train_time:100602ms step_avg:61.91ms
step:1626/2330 train_time:100667ms step_avg:61.91ms
step:1627/2330 train_time:100728ms step_avg:61.91ms
step:1628/2330 train_time:100791ms step_avg:61.91ms
step:1629/2330 train_time:100853ms step_avg:61.91ms
step:1630/2330 train_time:100917ms step_avg:61.91ms
step:1631/2330 train_time:100979ms step_avg:61.91ms
step:1632/2330 train_time:101044ms step_avg:61.91ms
step:1633/2330 train_time:101105ms step_avg:61.91ms
step:1634/2330 train_time:101170ms step_avg:61.92ms
step:1635/2330 train_time:101231ms step_avg:61.92ms
step:1636/2330 train_time:101295ms step_avg:61.92ms
step:1637/2330 train_time:101356ms step_avg:61.92ms
step:1638/2330 train_time:101420ms step_avg:61.92ms
step:1639/2330 train_time:101481ms step_avg:61.92ms
step:1640/2330 train_time:101545ms step_avg:61.92ms
step:1641/2330 train_time:101606ms step_avg:61.92ms
step:1642/2330 train_time:101671ms step_avg:61.92ms
step:1643/2330 train_time:101732ms step_avg:61.92ms
step:1644/2330 train_time:101796ms step_avg:61.92ms
step:1645/2330 train_time:101857ms step_avg:61.92ms
step:1646/2330 train_time:101922ms step_avg:61.92ms
step:1647/2330 train_time:101983ms step_avg:61.92ms
step:1648/2330 train_time:102047ms step_avg:61.92ms
step:1649/2330 train_time:102109ms step_avg:61.92ms
step:1650/2330 train_time:102173ms step_avg:61.92ms
step:1651/2330 train_time:102235ms step_avg:61.92ms
step:1652/2330 train_time:102299ms step_avg:61.92ms
step:1653/2330 train_time:102360ms step_avg:61.92ms
step:1654/2330 train_time:102424ms step_avg:61.92ms
step:1655/2330 train_time:102485ms step_avg:61.92ms
step:1656/2330 train_time:102549ms step_avg:61.93ms
step:1657/2330 train_time:102611ms step_avg:61.93ms
step:1658/2330 train_time:102675ms step_avg:61.93ms
step:1659/2330 train_time:102736ms step_avg:61.93ms
step:1660/2330 train_time:102800ms step_avg:61.93ms
step:1661/2330 train_time:102861ms step_avg:61.93ms
step:1662/2330 train_time:102925ms step_avg:61.93ms
step:1663/2330 train_time:102987ms step_avg:61.93ms
step:1664/2330 train_time:103051ms step_avg:61.93ms
step:1665/2330 train_time:103113ms step_avg:61.93ms
step:1666/2330 train_time:103178ms step_avg:61.93ms
step:1667/2330 train_time:103238ms step_avg:61.93ms
step:1668/2330 train_time:103303ms step_avg:61.93ms
step:1669/2330 train_time:103364ms step_avg:61.93ms
step:1670/2330 train_time:103429ms step_avg:61.93ms
step:1671/2330 train_time:103489ms step_avg:61.93ms
step:1672/2330 train_time:103553ms step_avg:61.93ms
step:1673/2330 train_time:103615ms step_avg:61.93ms
step:1674/2330 train_time:103679ms step_avg:61.93ms
step:1675/2330 train_time:103740ms step_avg:61.93ms
step:1676/2330 train_time:103804ms step_avg:61.94ms
step:1677/2330 train_time:103865ms step_avg:61.94ms
step:1678/2330 train_time:103930ms step_avg:61.94ms
step:1679/2330 train_time:103991ms step_avg:61.94ms
step:1680/2330 train_time:104056ms step_avg:61.94ms
step:1681/2330 train_time:104118ms step_avg:61.94ms
step:1682/2330 train_time:104182ms step_avg:61.94ms
step:1683/2330 train_time:104243ms step_avg:61.94ms
step:1684/2330 train_time:104307ms step_avg:61.94ms
step:1685/2330 train_time:104369ms step_avg:61.94ms
step:1686/2330 train_time:104434ms step_avg:61.94ms
step:1687/2330 train_time:104495ms step_avg:61.94ms
step:1688/2330 train_time:104559ms step_avg:61.94ms
step:1689/2330 train_time:104620ms step_avg:61.94ms
step:1690/2330 train_time:104685ms step_avg:61.94ms
step:1691/2330 train_time:104746ms step_avg:61.94ms
step:1692/2330 train_time:104811ms step_avg:61.94ms
step:1693/2330 train_time:104872ms step_avg:61.94ms
step:1694/2330 train_time:104936ms step_avg:61.95ms
step:1695/2330 train_time:104997ms step_avg:61.95ms
step:1696/2330 train_time:105061ms step_avg:61.95ms
step:1697/2330 train_time:105122ms step_avg:61.95ms
step:1698/2330 train_time:105186ms step_avg:61.95ms
step:1699/2330 train_time:105247ms step_avg:61.95ms
step:1700/2330 train_time:105311ms step_avg:61.95ms
step:1701/2330 train_time:105373ms step_avg:61.95ms
step:1702/2330 train_time:105438ms step_avg:61.95ms
step:1703/2330 train_time:105499ms step_avg:61.95ms
step:1704/2330 train_time:105563ms step_avg:61.95ms
step:1705/2330 train_time:105624ms step_avg:61.95ms
step:1706/2330 train_time:105689ms step_avg:61.95ms
step:1707/2330 train_time:105750ms step_avg:61.95ms
step:1708/2330 train_time:105814ms step_avg:61.95ms
step:1709/2330 train_time:105875ms step_avg:61.95ms
step:1710/2330 train_time:105939ms step_avg:61.95ms
step:1711/2330 train_time:106000ms step_avg:61.95ms
step:1712/2330 train_time:106065ms step_avg:61.95ms
step:1713/2330 train_time:106126ms step_avg:61.95ms
step:1714/2330 train_time:106191ms step_avg:61.95ms
step:1715/2330 train_time:106252ms step_avg:61.95ms
step:1716/2330 train_time:106316ms step_avg:61.96ms
step:1717/2330 train_time:106377ms step_avg:61.96ms
step:1718/2330 train_time:106441ms step_avg:61.96ms
step:1719/2330 train_time:106502ms step_avg:61.96ms
step:1720/2330 train_time:106567ms step_avg:61.96ms
step:1721/2330 train_time:106627ms step_avg:61.96ms
step:1722/2330 train_time:106691ms step_avg:61.96ms
step:1723/2330 train_time:106753ms step_avg:61.96ms
step:1724/2330 train_time:106817ms step_avg:61.96ms
step:1725/2330 train_time:106878ms step_avg:61.96ms
step:1726/2330 train_time:106942ms step_avg:61.96ms
step:1727/2330 train_time:107003ms step_avg:61.96ms
step:1728/2330 train_time:107066ms step_avg:61.96ms
step:1729/2330 train_time:107128ms step_avg:61.96ms
step:1730/2330 train_time:107192ms step_avg:61.96ms
step:1731/2330 train_time:107254ms step_avg:61.96ms
step:1732/2330 train_time:107318ms step_avg:61.96ms
step:1733/2330 train_time:107379ms step_avg:61.96ms
step:1734/2330 train_time:107443ms step_avg:61.96ms
step:1735/2330 train_time:107504ms step_avg:61.96ms
step:1736/2330 train_time:107569ms step_avg:61.96ms
step:1737/2330 train_time:107631ms step_avg:61.96ms
step:1738/2330 train_time:107695ms step_avg:61.96ms
step:1739/2330 train_time:107757ms step_avg:61.96ms
step:1740/2330 train_time:107821ms step_avg:61.97ms
step:1741/2330 train_time:107882ms step_avg:61.97ms
step:1742/2330 train_time:107946ms step_avg:61.97ms
step:1743/2330 train_time:108007ms step_avg:61.97ms
step:1744/2330 train_time:108072ms step_avg:61.97ms
step:1745/2330 train_time:108133ms step_avg:61.97ms
step:1746/2330 train_time:108197ms step_avg:61.97ms
step:1747/2330 train_time:108259ms step_avg:61.97ms
step:1748/2330 train_time:108323ms step_avg:61.97ms
step:1749/2330 train_time:108384ms step_avg:61.97ms
step:1750/2330 train_time:108449ms step_avg:61.97ms
step:1750/2330 val_loss:3.6094 train_time:108515ms step_avg:62.01ms
step:1751/2330 train_time:108537ms step_avg:61.99ms
step:1752/2330 train_time:108576ms step_avg:61.97ms
step:1753/2330 train_time:108644ms step_avg:61.98ms
step:1754/2330 train_time:108712ms step_avg:61.98ms
step:1755/2330 train_time:108773ms step_avg:61.98ms
step:1756/2330 train_time:108837ms step_avg:61.98ms
step:1757/2330 train_time:108897ms step_avg:61.98ms
step:1758/2330 train_time:108961ms step_avg:61.98ms
step:1759/2330 train_time:109020ms step_avg:61.98ms
step:1760/2330 train_time:109084ms step_avg:61.98ms
step:1761/2330 train_time:109144ms step_avg:61.98ms
step:1762/2330 train_time:109208ms step_avg:61.98ms
step:1763/2330 train_time:109268ms step_avg:61.98ms
step:1764/2330 train_time:109331ms step_avg:61.98ms
step:1765/2330 train_time:109393ms step_avg:61.98ms
step:1766/2330 train_time:109461ms step_avg:61.98ms
step:1767/2330 train_time:109524ms step_avg:61.98ms
step:1768/2330 train_time:109589ms step_avg:61.98ms
step:1769/2330 train_time:109652ms step_avg:61.99ms
step:1770/2330 train_time:109717ms step_avg:61.99ms
step:1771/2330 train_time:109779ms step_avg:61.99ms
step:1772/2330 train_time:109843ms step_avg:61.99ms
step:1773/2330 train_time:109904ms step_avg:61.99ms
step:1774/2330 train_time:109968ms step_avg:61.99ms
step:1775/2330 train_time:110028ms step_avg:61.99ms
step:1776/2330 train_time:110092ms step_avg:61.99ms
step:1777/2330 train_time:110153ms step_avg:61.99ms
step:1778/2330 train_time:110216ms step_avg:61.99ms
step:1779/2330 train_time:110277ms step_avg:61.99ms
step:1780/2330 train_time:110341ms step_avg:61.99ms
step:1781/2330 train_time:110402ms step_avg:61.99ms
step:1782/2330 train_time:110468ms step_avg:61.99ms
step:1783/2330 train_time:110530ms step_avg:61.99ms
step:1784/2330 train_time:110595ms step_avg:61.99ms
step:1785/2330 train_time:110656ms step_avg:61.99ms
step:1786/2330 train_time:110721ms step_avg:61.99ms
step:1787/2330 train_time:110783ms step_avg:61.99ms
step:1788/2330 train_time:110848ms step_avg:62.00ms
step:1789/2330 train_time:110909ms step_avg:61.99ms
step:1790/2330 train_time:110974ms step_avg:62.00ms
step:1791/2330 train_time:111034ms step_avg:62.00ms
step:1792/2330 train_time:111097ms step_avg:62.00ms
step:1793/2330 train_time:111158ms step_avg:62.00ms
step:1794/2330 train_time:111221ms step_avg:62.00ms
step:1795/2330 train_time:111282ms step_avg:62.00ms
step:1796/2330 train_time:111346ms step_avg:62.00ms
step:1797/2330 train_time:111407ms step_avg:62.00ms
step:1798/2330 train_time:111471ms step_avg:62.00ms
step:1799/2330 train_time:111533ms step_avg:62.00ms
step:1800/2330 train_time:111598ms step_avg:62.00ms
step:1801/2330 train_time:111660ms step_avg:62.00ms
step:1802/2330 train_time:111724ms step_avg:62.00ms
step:1803/2330 train_time:111785ms step_avg:62.00ms
step:1804/2330 train_time:111850ms step_avg:62.00ms
step:1805/2330 train_time:111912ms step_avg:62.00ms
step:1806/2330 train_time:111976ms step_avg:62.00ms
step:1807/2330 train_time:112038ms step_avg:62.00ms
step:1808/2330 train_time:112101ms step_avg:62.00ms
step:1809/2330 train_time:112162ms step_avg:62.00ms
step:1810/2330 train_time:112226ms step_avg:62.00ms
step:1811/2330 train_time:112287ms step_avg:62.00ms
step:1812/2330 train_time:112352ms step_avg:62.00ms
step:1813/2330 train_time:112413ms step_avg:62.00ms
step:1814/2330 train_time:112478ms step_avg:62.01ms
step:1815/2330 train_time:112539ms step_avg:62.00ms
step:1816/2330 train_time:112604ms step_avg:62.01ms
step:1817/2330 train_time:112666ms step_avg:62.01ms
step:1818/2330 train_time:112731ms step_avg:62.01ms
step:1819/2330 train_time:112792ms step_avg:62.01ms
step:1820/2330 train_time:112857ms step_avg:62.01ms
step:1821/2330 train_time:112919ms step_avg:62.01ms
step:1822/2330 train_time:112983ms step_avg:62.01ms
step:1823/2330 train_time:113044ms step_avg:62.01ms
step:1824/2330 train_time:113108ms step_avg:62.01ms
step:1825/2330 train_time:113169ms step_avg:62.01ms
step:1826/2330 train_time:113233ms step_avg:62.01ms
step:1827/2330 train_time:113294ms step_avg:62.01ms
step:1828/2330 train_time:113357ms step_avg:62.01ms
step:1829/2330 train_time:113419ms step_avg:62.01ms
step:1830/2330 train_time:113484ms step_avg:62.01ms
step:1831/2330 train_time:113545ms step_avg:62.01ms
step:1832/2330 train_time:113610ms step_avg:62.01ms
step:1833/2330 train_time:113672ms step_avg:62.01ms
step:1834/2330 train_time:113736ms step_avg:62.02ms
step:1835/2330 train_time:113798ms step_avg:62.02ms
step:1836/2330 train_time:113863ms step_avg:62.02ms
step:1837/2330 train_time:113924ms step_avg:62.02ms
step:1838/2330 train_time:113989ms step_avg:62.02ms
step:1839/2330 train_time:114051ms step_avg:62.02ms
step:1840/2330 train_time:114115ms step_avg:62.02ms
step:1841/2330 train_time:114175ms step_avg:62.02ms
step:1842/2330 train_time:114239ms step_avg:62.02ms
step:1843/2330 train_time:114300ms step_avg:62.02ms
step:1844/2330 train_time:114364ms step_avg:62.02ms
step:1845/2330 train_time:114425ms step_avg:62.02ms
step:1846/2330 train_time:114489ms step_avg:62.02ms
step:1847/2330 train_time:114550ms step_avg:62.02ms
step:1848/2330 train_time:114615ms step_avg:62.02ms
step:1849/2330 train_time:114677ms step_avg:62.02ms
step:1850/2330 train_time:114741ms step_avg:62.02ms
step:1851/2330 train_time:114803ms step_avg:62.02ms
step:1852/2330 train_time:114867ms step_avg:62.02ms
step:1853/2330 train_time:114929ms step_avg:62.02ms
step:1854/2330 train_time:114994ms step_avg:62.02ms
step:1855/2330 train_time:115055ms step_avg:62.02ms
step:1856/2330 train_time:115119ms step_avg:62.03ms
step:1857/2330 train_time:115180ms step_avg:62.03ms
step:1858/2330 train_time:115244ms step_avg:62.03ms
step:1859/2330 train_time:115305ms step_avg:62.03ms
step:1860/2330 train_time:115371ms step_avg:62.03ms
step:1861/2330 train_time:115432ms step_avg:62.03ms
step:1862/2330 train_time:115496ms step_avg:62.03ms
step:1863/2330 train_time:115557ms step_avg:62.03ms
step:1864/2330 train_time:115621ms step_avg:62.03ms
step:1865/2330 train_time:115682ms step_avg:62.03ms
step:1866/2330 train_time:115746ms step_avg:62.03ms
step:1867/2330 train_time:115807ms step_avg:62.03ms
step:1868/2330 train_time:115871ms step_avg:62.03ms
step:1869/2330 train_time:115932ms step_avg:62.03ms
step:1870/2330 train_time:115996ms step_avg:62.03ms
step:1871/2330 train_time:116057ms step_avg:62.03ms
step:1872/2330 train_time:116122ms step_avg:62.03ms
step:1873/2330 train_time:116183ms step_avg:62.03ms
step:1874/2330 train_time:116248ms step_avg:62.03ms
step:1875/2330 train_time:116310ms step_avg:62.03ms
step:1876/2330 train_time:116374ms step_avg:62.03ms
step:1877/2330 train_time:116435ms step_avg:62.03ms
step:1878/2330 train_time:116499ms step_avg:62.03ms
step:1879/2330 train_time:116560ms step_avg:62.03ms
step:1880/2330 train_time:116624ms step_avg:62.03ms
step:1881/2330 train_time:116685ms step_avg:62.03ms
step:1882/2330 train_time:116749ms step_avg:62.03ms
step:1883/2330 train_time:116810ms step_avg:62.03ms
step:1884/2330 train_time:116874ms step_avg:62.04ms
step:1885/2330 train_time:116935ms step_avg:62.03ms
step:1886/2330 train_time:116999ms step_avg:62.04ms
step:1887/2330 train_time:117060ms step_avg:62.04ms
step:1888/2330 train_time:117124ms step_avg:62.04ms
step:1889/2330 train_time:117185ms step_avg:62.04ms
step:1890/2330 train_time:117250ms step_avg:62.04ms
step:1891/2330 train_time:117311ms step_avg:62.04ms
step:1892/2330 train_time:117376ms step_avg:62.04ms
step:1893/2330 train_time:117437ms step_avg:62.04ms
step:1894/2330 train_time:117502ms step_avg:62.04ms
step:1895/2330 train_time:117562ms step_avg:62.04ms
step:1896/2330 train_time:117626ms step_avg:62.04ms
step:1897/2330 train_time:117687ms step_avg:62.04ms
step:1898/2330 train_time:117752ms step_avg:62.04ms
step:1899/2330 train_time:117813ms step_avg:62.04ms
step:1900/2330 train_time:117879ms step_avg:62.04ms
step:1901/2330 train_time:117940ms step_avg:62.04ms
step:1902/2330 train_time:118004ms step_avg:62.04ms
step:1903/2330 train_time:118064ms step_avg:62.04ms
step:1904/2330 train_time:118129ms step_avg:62.04ms
step:1905/2330 train_time:118190ms step_avg:62.04ms
step:1906/2330 train_time:118254ms step_avg:62.04ms
step:1907/2330 train_time:118316ms step_avg:62.04ms
step:1908/2330 train_time:118380ms step_avg:62.04ms
step:1909/2330 train_time:118441ms step_avg:62.04ms
step:1910/2330 train_time:118505ms step_avg:62.04ms
step:1911/2330 train_time:118566ms step_avg:62.04ms
step:1912/2330 train_time:118630ms step_avg:62.04ms
step:1913/2330 train_time:118691ms step_avg:62.04ms
step:1914/2330 train_time:118755ms step_avg:62.05ms
step:1915/2330 train_time:118816ms step_avg:62.04ms
step:1916/2330 train_time:118880ms step_avg:62.05ms
step:1917/2330 train_time:118941ms step_avg:62.05ms
step:1918/2330 train_time:119005ms step_avg:62.05ms
step:1919/2330 train_time:119066ms step_avg:62.05ms
step:1920/2330 train_time:119130ms step_avg:62.05ms
step:1921/2330 train_time:119191ms step_avg:62.05ms
step:1922/2330 train_time:119256ms step_avg:62.05ms
step:1923/2330 train_time:119317ms step_avg:62.05ms
step:1924/2330 train_time:119382ms step_avg:62.05ms
step:1925/2330 train_time:119443ms step_avg:62.05ms
step:1926/2330 train_time:119508ms step_avg:62.05ms
step:1927/2330 train_time:119569ms step_avg:62.05ms
step:1928/2330 train_time:119634ms step_avg:62.05ms
step:1929/2330 train_time:119694ms step_avg:62.05ms
step:1930/2330 train_time:119758ms step_avg:62.05ms
step:1931/2330 train_time:119818ms step_avg:62.05ms
step:1932/2330 train_time:119883ms step_avg:62.05ms
step:1933/2330 train_time:119945ms step_avg:62.05ms
step:1934/2330 train_time:120009ms step_avg:62.05ms
step:1935/2330 train_time:120071ms step_avg:62.05ms
step:1936/2330 train_time:120135ms step_avg:62.05ms
step:1937/2330 train_time:120195ms step_avg:62.05ms
step:1938/2330 train_time:120259ms step_avg:62.05ms
step:1939/2330 train_time:120320ms step_avg:62.05ms
step:1940/2330 train_time:120385ms step_avg:62.05ms
step:1941/2330 train_time:120446ms step_avg:62.05ms
step:1942/2330 train_time:120511ms step_avg:62.06ms
step:1943/2330 train_time:120573ms step_avg:62.06ms
step:1944/2330 train_time:120637ms step_avg:62.06ms
step:1945/2330 train_time:120698ms step_avg:62.06ms
step:1946/2330 train_time:120762ms step_avg:62.06ms
step:1947/2330 train_time:120823ms step_avg:62.06ms
step:1948/2330 train_time:120887ms step_avg:62.06ms
step:1949/2330 train_time:120948ms step_avg:62.06ms
step:1950/2330 train_time:121013ms step_avg:62.06ms
step:1951/2330 train_time:121074ms step_avg:62.06ms
step:1952/2330 train_time:121138ms step_avg:62.06ms
step:1953/2330 train_time:121199ms step_avg:62.06ms
step:1954/2330 train_time:121263ms step_avg:62.06ms
step:1955/2330 train_time:121324ms step_avg:62.06ms
step:1956/2330 train_time:121388ms step_avg:62.06ms
step:1957/2330 train_time:121450ms step_avg:62.06ms
step:1958/2330 train_time:121515ms step_avg:62.06ms
step:1959/2330 train_time:121576ms step_avg:62.06ms
step:1960/2330 train_time:121640ms step_avg:62.06ms
step:1961/2330 train_time:121700ms step_avg:62.06ms
step:1962/2330 train_time:121764ms step_avg:62.06ms
step:1963/2330 train_time:121826ms step_avg:62.06ms
step:1964/2330 train_time:121890ms step_avg:62.06ms
step:1965/2330 train_time:121951ms step_avg:62.06ms
step:1966/2330 train_time:122016ms step_avg:62.06ms
step:1967/2330 train_time:122078ms step_avg:62.06ms
step:1968/2330 train_time:122142ms step_avg:62.06ms
step:1969/2330 train_time:122203ms step_avg:62.06ms
step:1970/2330 train_time:122266ms step_avg:62.06ms
step:1971/2330 train_time:122327ms step_avg:62.06ms
step:1972/2330 train_time:122391ms step_avg:62.06ms
step:1973/2330 train_time:122453ms step_avg:62.06ms
step:1974/2330 train_time:122517ms step_avg:62.07ms
step:1975/2330 train_time:122578ms step_avg:62.07ms
step:1976/2330 train_time:122642ms step_avg:62.07ms
step:1977/2330 train_time:122703ms step_avg:62.07ms
step:1978/2330 train_time:122767ms step_avg:62.07ms
step:1979/2330 train_time:122829ms step_avg:62.07ms
step:1980/2330 train_time:122893ms step_avg:62.07ms
step:1981/2330 train_time:122955ms step_avg:62.07ms
step:1982/2330 train_time:123020ms step_avg:62.07ms
step:1983/2330 train_time:123081ms step_avg:62.07ms
step:1984/2330 train_time:123145ms step_avg:62.07ms
step:1985/2330 train_time:123206ms step_avg:62.07ms
step:1986/2330 train_time:123270ms step_avg:62.07ms
step:1987/2330 train_time:123331ms step_avg:62.07ms
step:1988/2330 train_time:123396ms step_avg:62.07ms
step:1989/2330 train_time:123457ms step_avg:62.07ms
step:1990/2330 train_time:123521ms step_avg:62.07ms
step:1991/2330 train_time:123582ms step_avg:62.07ms
step:1992/2330 train_time:123646ms step_avg:62.07ms
step:1993/2330 train_time:123707ms step_avg:62.07ms
step:1994/2330 train_time:123772ms step_avg:62.07ms
step:1995/2330 train_time:123834ms step_avg:62.07ms
step:1996/2330 train_time:123898ms step_avg:62.07ms
step:1997/2330 train_time:123960ms step_avg:62.07ms
step:1998/2330 train_time:124024ms step_avg:62.07ms
step:1999/2330 train_time:124085ms step_avg:62.07ms
step:2000/2330 train_time:124149ms step_avg:62.07ms
step:2000/2330 val_loss:3.5639 train_time:124214ms step_avg:62.11ms
step:2001/2330 train_time:124236ms step_avg:62.09ms
step:2002/2330 train_time:124277ms step_avg:62.08ms
step:2003/2330 train_time:124344ms step_avg:62.08ms
step:2004/2330 train_time:124410ms step_avg:62.08ms
step:2005/2330 train_time:124471ms step_avg:62.08ms
step:2006/2330 train_time:124535ms step_avg:62.08ms
step:2007/2330 train_time:124596ms step_avg:62.08ms
step:2008/2330 train_time:124659ms step_avg:62.08ms
step:2009/2330 train_time:124719ms step_avg:62.08ms
step:2010/2330 train_time:124782ms step_avg:62.08ms
step:2011/2330 train_time:124843ms step_avg:62.08ms
step:2012/2330 train_time:124907ms step_avg:62.08ms
step:2013/2330 train_time:124968ms step_avg:62.08ms
step:2014/2330 train_time:125031ms step_avg:62.08ms
step:2015/2330 train_time:125091ms step_avg:62.08ms
step:2016/2330 train_time:125156ms step_avg:62.08ms
step:2017/2330 train_time:125219ms step_avg:62.08ms
step:2018/2330 train_time:125284ms step_avg:62.08ms
step:2019/2330 train_time:125348ms step_avg:62.08ms
step:2020/2330 train_time:125413ms step_avg:62.09ms
step:2021/2330 train_time:125474ms step_avg:62.09ms
step:2022/2330 train_time:125538ms step_avg:62.09ms
step:2023/2330 train_time:125599ms step_avg:62.09ms
step:2024/2330 train_time:125663ms step_avg:62.09ms
step:2025/2330 train_time:125724ms step_avg:62.09ms
step:2026/2330 train_time:125788ms step_avg:62.09ms
step:2027/2330 train_time:125848ms step_avg:62.09ms
step:2028/2330 train_time:125911ms step_avg:62.09ms
step:2029/2330 train_time:125971ms step_avg:62.09ms
step:2030/2330 train_time:126034ms step_avg:62.09ms
step:2031/2330 train_time:126095ms step_avg:62.09ms
step:2032/2330 train_time:126160ms step_avg:62.09ms
step:2033/2330 train_time:126221ms step_avg:62.09ms
step:2034/2330 train_time:126287ms step_avg:62.09ms
step:2035/2330 train_time:126349ms step_avg:62.09ms
step:2036/2330 train_time:126414ms step_avg:62.09ms
step:2037/2330 train_time:126476ms step_avg:62.09ms
step:2038/2330 train_time:126540ms step_avg:62.09ms
step:2039/2330 train_time:126601ms step_avg:62.09ms
step:2040/2330 train_time:126665ms step_avg:62.09ms
step:2041/2330 train_time:126727ms step_avg:62.09ms
step:2042/2330 train_time:126791ms step_avg:62.09ms
step:2043/2330 train_time:126851ms step_avg:62.09ms
step:2044/2330 train_time:126914ms step_avg:62.09ms
step:2045/2330 train_time:126975ms step_avg:62.09ms
step:2046/2330 train_time:127039ms step_avg:62.09ms
step:2047/2330 train_time:127100ms step_avg:62.09ms
step:2048/2330 train_time:127164ms step_avg:62.09ms
step:2049/2330 train_time:127225ms step_avg:62.09ms
step:2050/2330 train_time:127290ms step_avg:62.09ms
step:2051/2330 train_time:127352ms step_avg:62.09ms
step:2052/2330 train_time:127417ms step_avg:62.09ms
step:2053/2330 train_time:127479ms step_avg:62.09ms
step:2054/2330 train_time:127543ms step_avg:62.09ms
step:2055/2330 train_time:127604ms step_avg:62.09ms
step:2056/2330 train_time:127668ms step_avg:62.10ms
step:2057/2330 train_time:127729ms step_avg:62.09ms
step:2058/2330 train_time:127794ms step_avg:62.10ms
step:2059/2330 train_time:127855ms step_avg:62.10ms
step:2060/2330 train_time:127918ms step_avg:62.10ms
step:2061/2330 train_time:127979ms step_avg:62.10ms
step:2062/2330 train_time:128042ms step_avg:62.10ms
step:2063/2330 train_time:128103ms step_avg:62.10ms
step:2064/2330 train_time:128167ms step_avg:62.10ms
step:2065/2330 train_time:128229ms step_avg:62.10ms
step:2066/2330 train_time:128294ms step_avg:62.10ms
step:2067/2330 train_time:128355ms step_avg:62.10ms
step:2068/2330 train_time:128419ms step_avg:62.10ms
step:2069/2330 train_time:128480ms step_avg:62.10ms
step:2070/2330 train_time:128544ms step_avg:62.10ms
step:2071/2330 train_time:128605ms step_avg:62.10ms
step:2072/2330 train_time:128670ms step_avg:62.10ms
step:2073/2330 train_time:128731ms step_avg:62.10ms
step:2074/2330 train_time:128795ms step_avg:62.10ms
step:2075/2330 train_time:128856ms step_avg:62.10ms
step:2076/2330 train_time:128919ms step_avg:62.10ms
step:2077/2330 train_time:128980ms step_avg:62.10ms
step:2078/2330 train_time:129045ms step_avg:62.10ms
step:2079/2330 train_time:129106ms step_avg:62.10ms
step:2080/2330 train_time:129170ms step_avg:62.10ms
step:2081/2330 train_time:129232ms step_avg:62.10ms
step:2082/2330 train_time:129297ms step_avg:62.10ms
step:2083/2330 train_time:129357ms step_avg:62.10ms
step:2084/2330 train_time:129421ms step_avg:62.10ms
step:2085/2330 train_time:129482ms step_avg:62.10ms
step:2086/2330 train_time:129547ms step_avg:62.10ms
step:2087/2330 train_time:129608ms step_avg:62.10ms
step:2088/2330 train_time:129673ms step_avg:62.10ms
step:2089/2330 train_time:129734ms step_avg:62.10ms
step:2090/2330 train_time:129798ms step_avg:62.10ms
step:2091/2330 train_time:129859ms step_avg:62.10ms
step:2092/2330 train_time:129922ms step_avg:62.10ms
step:2093/2330 train_time:129984ms step_avg:62.10ms
step:2094/2330 train_time:130048ms step_avg:62.11ms
step:2095/2330 train_time:130109ms step_avg:62.10ms
step:2096/2330 train_time:130173ms step_avg:62.11ms
step:2097/2330 train_time:130235ms step_avg:62.11ms
step:2098/2330 train_time:130300ms step_avg:62.11ms
step:2099/2330 train_time:130360ms step_avg:62.11ms
step:2100/2330 train_time:130424ms step_avg:62.11ms
step:2101/2330 train_time:130486ms step_avg:62.11ms
step:2102/2330 train_time:130550ms step_avg:62.11ms
step:2103/2330 train_time:130612ms step_avg:62.11ms
step:2104/2330 train_time:130676ms step_avg:62.11ms
step:2105/2330 train_time:130736ms step_avg:62.11ms
step:2106/2330 train_time:130801ms step_avg:62.11ms
step:2107/2330 train_time:130862ms step_avg:62.11ms
step:2108/2330 train_time:130927ms step_avg:62.11ms
step:2109/2330 train_time:130987ms step_avg:62.11ms
step:2110/2330 train_time:131052ms step_avg:62.11ms
step:2111/2330 train_time:131112ms step_avg:62.11ms
step:2112/2330 train_time:131176ms step_avg:62.11ms
step:2113/2330 train_time:131238ms step_avg:62.11ms
step:2114/2330 train_time:131302ms step_avg:62.11ms
step:2115/2330 train_time:131362ms step_avg:62.11ms
step:2116/2330 train_time:131427ms step_avg:62.11ms
step:2117/2330 train_time:131489ms step_avg:62.11ms
step:2118/2330 train_time:131553ms step_avg:62.11ms
step:2119/2330 train_time:131614ms step_avg:62.11ms
step:2120/2330 train_time:131678ms step_avg:62.11ms
step:2121/2330 train_time:131739ms step_avg:62.11ms
step:2122/2330 train_time:131803ms step_avg:62.11ms
step:2123/2330 train_time:131864ms step_avg:62.11ms
step:2124/2330 train_time:131928ms step_avg:62.11ms
step:2125/2330 train_time:131989ms step_avg:62.11ms
step:2126/2330 train_time:132053ms step_avg:62.11ms
step:2127/2330 train_time:132114ms step_avg:62.11ms
step:2128/2330 train_time:132179ms step_avg:62.11ms
step:2129/2330 train_time:132240ms step_avg:62.11ms
step:2130/2330 train_time:132304ms step_avg:62.11ms
step:2131/2330 train_time:132367ms step_avg:62.11ms
step:2132/2330 train_time:132431ms step_avg:62.12ms
step:2133/2330 train_time:132493ms step_avg:62.12ms
step:2134/2330 train_time:132558ms step_avg:62.12ms
step:2135/2330 train_time:132618ms step_avg:62.12ms
step:2136/2330 train_time:132683ms step_avg:62.12ms
step:2137/2330 train_time:132743ms step_avg:62.12ms
step:2138/2330 train_time:132807ms step_avg:62.12ms
step:2139/2330 train_time:132868ms step_avg:62.12ms
step:2140/2330 train_time:132932ms step_avg:62.12ms
step:2141/2330 train_time:132993ms step_avg:62.12ms
step:2142/2330 train_time:133057ms step_avg:62.12ms
step:2143/2330 train_time:133118ms step_avg:62.12ms
step:2144/2330 train_time:133182ms step_avg:62.12ms
step:2145/2330 train_time:133243ms step_avg:62.12ms
step:2146/2330 train_time:133307ms step_avg:62.12ms
step:2147/2330 train_time:133369ms step_avg:62.12ms
step:2148/2330 train_time:133433ms step_avg:62.12ms
step:2149/2330 train_time:133494ms step_avg:62.12ms
step:2150/2330 train_time:133559ms step_avg:62.12ms
step:2151/2330 train_time:133620ms step_avg:62.12ms
step:2152/2330 train_time:133684ms step_avg:62.12ms
step:2153/2330 train_time:133745ms step_avg:62.12ms
step:2154/2330 train_time:133809ms step_avg:62.12ms
step:2155/2330 train_time:133871ms step_avg:62.12ms
step:2156/2330 train_time:133935ms step_avg:62.12ms
step:2157/2330 train_time:133995ms step_avg:62.12ms
step:2158/2330 train_time:134059ms step_avg:62.12ms
step:2159/2330 train_time:134121ms step_avg:62.12ms
step:2160/2330 train_time:134185ms step_avg:62.12ms
step:2161/2330 train_time:134245ms step_avg:62.12ms
step:2162/2330 train_time:134310ms step_avg:62.12ms
step:2163/2330 train_time:134372ms step_avg:62.12ms
step:2164/2330 train_time:134436ms step_avg:62.12ms
step:2165/2330 train_time:134496ms step_avg:62.12ms
step:2166/2330 train_time:134561ms step_avg:62.12ms
step:2167/2330 train_time:134622ms step_avg:62.12ms
step:2168/2330 train_time:134686ms step_avg:62.12ms
step:2169/2330 train_time:134748ms step_avg:62.12ms
step:2170/2330 train_time:134813ms step_avg:62.13ms
step:2171/2330 train_time:134874ms step_avg:62.13ms
step:2172/2330 train_time:134938ms step_avg:62.13ms
step:2173/2330 train_time:134999ms step_avg:62.13ms
step:2174/2330 train_time:135064ms step_avg:62.13ms
step:2175/2330 train_time:135125ms step_avg:62.13ms
step:2176/2330 train_time:135189ms step_avg:62.13ms
step:2177/2330 train_time:135250ms step_avg:62.13ms
step:2178/2330 train_time:135314ms step_avg:62.13ms
step:2179/2330 train_time:135375ms step_avg:62.13ms
step:2180/2330 train_time:135440ms step_avg:62.13ms
step:2181/2330 train_time:135501ms step_avg:62.13ms
step:2182/2330 train_time:135566ms step_avg:62.13ms
step:2183/2330 train_time:135627ms step_avg:62.13ms
step:2184/2330 train_time:135691ms step_avg:62.13ms
step:2185/2330 train_time:135752ms step_avg:62.13ms
step:2186/2330 train_time:135816ms step_avg:62.13ms
step:2187/2330 train_time:135878ms step_avg:62.13ms
step:2188/2330 train_time:135941ms step_avg:62.13ms
step:2189/2330 train_time:136002ms step_avg:62.13ms
step:2190/2330 train_time:136067ms step_avg:62.13ms
step:2191/2330 train_time:136129ms step_avg:62.13ms
step:2192/2330 train_time:136194ms step_avg:62.13ms
step:2193/2330 train_time:136255ms step_avg:62.13ms
step:2194/2330 train_time:136319ms step_avg:62.13ms
step:2195/2330 train_time:136379ms step_avg:62.13ms
step:2196/2330 train_time:136443ms step_avg:62.13ms
step:2197/2330 train_time:136504ms step_avg:62.13ms
step:2198/2330 train_time:136568ms step_avg:62.13ms
step:2199/2330 train_time:136629ms step_avg:62.13ms
step:2200/2330 train_time:136694ms step_avg:62.13ms
step:2201/2330 train_time:136755ms step_avg:62.13ms
step:2202/2330 train_time:136819ms step_avg:62.13ms
step:2203/2330 train_time:136880ms step_avg:62.13ms
step:2204/2330 train_time:136943ms step_avg:62.13ms
step:2205/2330 train_time:137004ms step_avg:62.13ms
step:2206/2330 train_time:137068ms step_avg:62.13ms
step:2207/2330 train_time:137130ms step_avg:62.13ms
step:2208/2330 train_time:137194ms step_avg:62.14ms
step:2209/2330 train_time:137256ms step_avg:62.13ms
step:2210/2330 train_time:137319ms step_avg:62.14ms
step:2211/2330 train_time:137380ms step_avg:62.13ms
step:2212/2330 train_time:137446ms step_avg:62.14ms
step:2213/2330 train_time:137507ms step_avg:62.14ms
step:2214/2330 train_time:137571ms step_avg:62.14ms
step:2215/2330 train_time:137633ms step_avg:62.14ms
step:2216/2330 train_time:137697ms step_avg:62.14ms
step:2217/2330 train_time:137768ms step_avg:62.14ms
step:2218/2330 train_time:137822ms step_avg:62.14ms
step:2219/2330 train_time:137883ms step_avg:62.14ms
step:2220/2330 train_time:137948ms step_avg:62.14ms
step:2221/2330 train_time:138008ms step_avg:62.14ms
step:2222/2330 train_time:138072ms step_avg:62.14ms
step:2223/2330 train_time:138134ms step_avg:62.14ms
step:2224/2330 train_time:138198ms step_avg:62.14ms
step:2225/2330 train_time:138259ms step_avg:62.14ms
step:2226/2330 train_time:138323ms step_avg:62.14ms
step:2227/2330 train_time:138384ms step_avg:62.14ms
step:2228/2330 train_time:138448ms step_avg:62.14ms
step:2229/2330 train_time:138509ms step_avg:62.14ms
step:2230/2330 train_time:138573ms step_avg:62.14ms
step:2231/2330 train_time:138635ms step_avg:62.14ms
step:2232/2330 train_time:138699ms step_avg:62.14ms
step:2233/2330 train_time:138759ms step_avg:62.14ms
step:2234/2330 train_time:138823ms step_avg:62.14ms
step:2235/2330 train_time:138884ms step_avg:62.14ms
step:2236/2330 train_time:138949ms step_avg:62.14ms
step:2237/2330 train_time:139010ms step_avg:62.14ms
step:2238/2330 train_time:139075ms step_avg:62.14ms
step:2239/2330 train_time:139136ms step_avg:62.14ms
step:2240/2330 train_time:139201ms step_avg:62.14ms
step:2241/2330 train_time:139261ms step_avg:62.14ms
step:2242/2330 train_time:139325ms step_avg:62.14ms
step:2243/2330 train_time:139386ms step_avg:62.14ms
step:2244/2330 train_time:139450ms step_avg:62.14ms
step:2245/2330 train_time:139511ms step_avg:62.14ms
step:2246/2330 train_time:139575ms step_avg:62.14ms
step:2247/2330 train_time:139637ms step_avg:62.14ms
step:2248/2330 train_time:139701ms step_avg:62.14ms
step:2249/2330 train_time:139762ms step_avg:62.14ms
step:2250/2330 train_time:139826ms step_avg:62.14ms
step:2250/2330 val_loss:3.5325 train_time:139893ms step_avg:62.17ms
step:2251/2330 train_time:139915ms step_avg:62.16ms
step:2252/2330 train_time:139956ms step_avg:62.15ms
step:2253/2330 train_time:140022ms step_avg:62.15ms
step:2254/2330 train_time:140088ms step_avg:62.15ms
step:2255/2330 train_time:140149ms step_avg:62.15ms
step:2256/2330 train_time:140213ms step_avg:62.15ms
step:2257/2330 train_time:140274ms step_avg:62.15ms
step:2258/2330 train_time:140337ms step_avg:62.15ms
step:2259/2330 train_time:140397ms step_avg:62.15ms
step:2260/2330 train_time:140461ms step_avg:62.15ms
step:2261/2330 train_time:140521ms step_avg:62.15ms
step:2262/2330 train_time:140585ms step_avg:62.15ms
step:2263/2330 train_time:140645ms step_avg:62.15ms
step:2264/2330 train_time:140709ms step_avg:62.15ms
step:2265/2330 train_time:140769ms step_avg:62.15ms
step:2266/2330 train_time:140833ms step_avg:62.15ms
step:2267/2330 train_time:140896ms step_avg:62.15ms
step:2268/2330 train_time:140961ms step_avg:62.15ms
step:2269/2330 train_time:141024ms step_avg:62.15ms
step:2270/2330 train_time:141090ms step_avg:62.15ms
step:2271/2330 train_time:141151ms step_avg:62.15ms
step:2272/2330 train_time:141215ms step_avg:62.15ms
step:2273/2330 train_time:141276ms step_avg:62.15ms
step:2274/2330 train_time:141340ms step_avg:62.15ms
step:2275/2330 train_time:141401ms step_avg:62.15ms
step:2276/2330 train_time:141465ms step_avg:62.15ms
step:2277/2330 train_time:141525ms step_avg:62.15ms
step:2278/2330 train_time:141588ms step_avg:62.15ms
step:2279/2330 train_time:141649ms step_avg:62.15ms
step:2280/2330 train_time:141713ms step_avg:62.15ms
step:2281/2330 train_time:141774ms step_avg:62.15ms
step:2282/2330 train_time:141838ms step_avg:62.16ms
step:2283/2330 train_time:141900ms step_avg:62.15ms
step:2284/2330 train_time:141964ms step_avg:62.16ms
step:2285/2330 train_time:142026ms step_avg:62.16ms
step:2286/2330 train_time:142092ms step_avg:62.16ms
step:2287/2330 train_time:142154ms step_avg:62.16ms
step:2288/2330 train_time:142218ms step_avg:62.16ms
step:2289/2330 train_time:142278ms step_avg:62.16ms
step:2290/2330 train_time:142342ms step_avg:62.16ms
step:2291/2330 train_time:142403ms step_avg:62.16ms
step:2292/2330 train_time:142467ms step_avg:62.16ms
step:2293/2330 train_time:142528ms step_avg:62.16ms
step:2294/2330 train_time:142592ms step_avg:62.16ms
step:2295/2330 train_time:142652ms step_avg:62.16ms
step:2296/2330 train_time:142716ms step_avg:62.16ms
step:2297/2330 train_time:142777ms step_avg:62.16ms
step:2298/2330 train_time:142842ms step_avg:62.16ms
step:2299/2330 train_time:142902ms step_avg:62.16ms
step:2300/2330 train_time:142967ms step_avg:62.16ms
step:2301/2330 train_time:143030ms step_avg:62.16ms
step:2302/2330 train_time:143095ms step_avg:62.16ms
step:2303/2330 train_time:143156ms step_avg:62.16ms
step:2304/2330 train_time:143220ms step_avg:62.16ms
step:2305/2330 train_time:143281ms step_avg:62.16ms
step:2306/2330 train_time:143345ms step_avg:62.16ms
step:2307/2330 train_time:143407ms step_avg:62.16ms
step:2308/2330 train_time:143470ms step_avg:62.16ms
step:2309/2330 train_time:143531ms step_avg:62.16ms
step:2310/2330 train_time:143595ms step_avg:62.16ms
step:2311/2330 train_time:143657ms step_avg:62.16ms
step:2312/2330 train_time:143720ms step_avg:62.16ms
step:2313/2330 train_time:143781ms step_avg:62.16ms
step:2314/2330 train_time:143844ms step_avg:62.16ms
step:2315/2330 train_time:143906ms step_avg:62.16ms
step:2316/2330 train_time:143970ms step_avg:62.16ms
step:2317/2330 train_time:144032ms step_avg:62.16ms
step:2318/2330 train_time:144096ms step_avg:62.16ms
step:2319/2330 train_time:144158ms step_avg:62.16ms
step:2320/2330 train_time:144223ms step_avg:62.16ms
step:2321/2330 train_time:144284ms step_avg:62.16ms
step:2322/2330 train_time:144349ms step_avg:62.17ms
step:2323/2330 train_time:144411ms step_avg:62.17ms
step:2324/2330 train_time:144475ms step_avg:62.17ms
step:2325/2330 train_time:144536ms step_avg:62.17ms
step:2326/2330 train_time:144600ms step_avg:62.17ms
step:2327/2330 train_time:144660ms step_avg:62.17ms
step:2328/2330 train_time:144724ms step_avg:62.17ms
step:2329/2330 train_time:144785ms step_avg:62.17ms
step:2330/2330 train_time:144849ms step_avg:62.17ms
step:2330/2330 val_loss:3.5090 train_time:144915ms step_avg:62.20ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
