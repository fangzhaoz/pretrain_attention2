import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-3"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:10:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:93ms step_avg:92.96ms
step:2/2330 train_time:186ms step_avg:93.08ms
step:3/2330 train_time:208ms step_avg:69.27ms
step:4/2330 train_time:244ms step_avg:60.89ms
step:5/2330 train_time:301ms step_avg:60.17ms
step:6/2330 train_time:362ms step_avg:60.36ms
step:7/2330 train_time:421ms step_avg:60.08ms
step:8/2330 train_time:483ms step_avg:60.36ms
step:9/2330 train_time:542ms step_avg:60.18ms
step:10/2330 train_time:604ms step_avg:60.37ms
step:11/2330 train_time:662ms step_avg:60.18ms
step:12/2330 train_time:724ms step_avg:60.35ms
step:13/2330 train_time:783ms step_avg:60.24ms
step:14/2330 train_time:845ms step_avg:60.35ms
step:15/2330 train_time:904ms step_avg:60.27ms
step:16/2330 train_time:966ms step_avg:60.37ms
step:17/2330 train_time:1027ms step_avg:60.39ms
step:18/2330 train_time:1091ms step_avg:60.62ms
step:19/2330 train_time:1153ms step_avg:60.68ms
step:20/2330 train_time:1216ms step_avg:60.81ms
step:21/2330 train_time:1276ms step_avg:60.77ms
step:22/2330 train_time:1339ms step_avg:60.86ms
step:23/2330 train_time:1398ms step_avg:60.80ms
step:24/2330 train_time:1460ms step_avg:60.85ms
step:25/2330 train_time:1520ms step_avg:60.78ms
step:26/2330 train_time:1583ms step_avg:60.88ms
step:27/2330 train_time:1642ms step_avg:60.81ms
step:28/2330 train_time:1704ms step_avg:60.85ms
step:29/2330 train_time:1762ms step_avg:60.77ms
step:30/2330 train_time:1825ms step_avg:60.83ms
step:31/2330 train_time:1884ms step_avg:60.79ms
step:32/2330 train_time:1947ms step_avg:60.84ms
step:33/2330 train_time:2007ms step_avg:60.83ms
step:34/2330 train_time:2071ms step_avg:60.91ms
step:35/2330 train_time:2132ms step_avg:60.91ms
step:36/2330 train_time:2195ms step_avg:60.97ms
step:37/2330 train_time:2255ms step_avg:60.94ms
step:38/2330 train_time:2318ms step_avg:60.99ms
step:39/2330 train_time:2377ms step_avg:60.96ms
step:40/2330 train_time:2440ms step_avg:61.00ms
step:41/2330 train_time:2499ms step_avg:60.96ms
step:42/2330 train_time:2562ms step_avg:61.00ms
step:43/2330 train_time:2621ms step_avg:60.96ms
step:44/2330 train_time:2684ms step_avg:60.99ms
step:45/2330 train_time:2743ms step_avg:60.95ms
step:46/2330 train_time:2805ms step_avg:60.97ms
step:47/2330 train_time:2864ms step_avg:60.93ms
step:48/2330 train_time:2927ms step_avg:60.98ms
step:49/2330 train_time:2987ms step_avg:60.96ms
step:50/2330 train_time:3050ms step_avg:61.00ms
step:51/2330 train_time:3111ms step_avg:60.99ms
step:52/2330 train_time:3173ms step_avg:61.02ms
step:53/2330 train_time:3233ms step_avg:61.00ms
step:54/2330 train_time:3296ms step_avg:61.03ms
step:55/2330 train_time:3355ms step_avg:61.00ms
step:56/2330 train_time:3417ms step_avg:61.03ms
step:57/2330 train_time:3477ms step_avg:61.00ms
step:58/2330 train_time:3540ms step_avg:61.03ms
step:59/2330 train_time:3599ms step_avg:60.99ms
step:60/2330 train_time:3661ms step_avg:61.02ms
step:61/2330 train_time:3721ms step_avg:61.00ms
step:62/2330 train_time:3784ms step_avg:61.03ms
step:63/2330 train_time:3843ms step_avg:61.00ms
step:64/2330 train_time:3906ms step_avg:61.02ms
step:65/2330 train_time:3966ms step_avg:61.01ms
step:66/2330 train_time:4029ms step_avg:61.05ms
step:67/2330 train_time:4089ms step_avg:61.03ms
step:68/2330 train_time:4152ms step_avg:61.06ms
step:69/2330 train_time:4211ms step_avg:61.03ms
step:70/2330 train_time:4274ms step_avg:61.06ms
step:71/2330 train_time:4334ms step_avg:61.05ms
step:72/2330 train_time:4397ms step_avg:61.07ms
step:73/2330 train_time:4456ms step_avg:61.05ms
step:74/2330 train_time:4519ms step_avg:61.07ms
step:75/2330 train_time:4579ms step_avg:61.05ms
step:76/2330 train_time:4641ms step_avg:61.07ms
step:77/2330 train_time:4701ms step_avg:61.05ms
step:78/2330 train_time:4763ms step_avg:61.07ms
step:79/2330 train_time:4823ms step_avg:61.05ms
step:80/2330 train_time:4885ms step_avg:61.07ms
step:81/2330 train_time:4945ms step_avg:61.05ms
step:82/2330 train_time:5008ms step_avg:61.07ms
step:83/2330 train_time:5068ms step_avg:61.06ms
step:84/2330 train_time:5131ms step_avg:61.08ms
step:85/2330 train_time:5190ms step_avg:61.06ms
step:86/2330 train_time:5253ms step_avg:61.08ms
step:87/2330 train_time:5313ms step_avg:61.06ms
step:88/2330 train_time:5375ms step_avg:61.08ms
step:89/2330 train_time:5434ms step_avg:61.06ms
step:90/2330 train_time:5496ms step_avg:61.07ms
step:91/2330 train_time:5556ms step_avg:61.05ms
step:92/2330 train_time:5618ms step_avg:61.07ms
step:93/2330 train_time:5677ms step_avg:61.05ms
step:94/2330 train_time:5740ms step_avg:61.06ms
step:95/2330 train_time:5800ms step_avg:61.05ms
step:96/2330 train_time:5863ms step_avg:61.07ms
step:97/2330 train_time:5923ms step_avg:61.06ms
step:98/2330 train_time:5987ms step_avg:61.09ms
step:99/2330 train_time:6047ms step_avg:61.08ms
step:100/2330 train_time:6109ms step_avg:61.09ms
step:101/2330 train_time:6169ms step_avg:61.08ms
step:102/2330 train_time:6232ms step_avg:61.10ms
step:103/2330 train_time:6291ms step_avg:61.08ms
step:104/2330 train_time:6354ms step_avg:61.09ms
step:105/2330 train_time:6415ms step_avg:61.09ms
step:106/2330 train_time:6477ms step_avg:61.10ms
step:107/2330 train_time:6535ms step_avg:61.08ms
step:108/2330 train_time:6598ms step_avg:61.09ms
step:109/2330 train_time:6657ms step_avg:61.08ms
step:110/2330 train_time:6720ms step_avg:61.09ms
step:111/2330 train_time:6780ms step_avg:61.08ms
step:112/2330 train_time:6843ms step_avg:61.09ms
step:113/2330 train_time:6902ms step_avg:61.08ms
step:114/2330 train_time:6965ms step_avg:61.10ms
step:115/2330 train_time:7026ms step_avg:61.10ms
step:116/2330 train_time:7089ms step_avg:61.11ms
step:117/2330 train_time:7149ms step_avg:61.10ms
step:118/2330 train_time:7211ms step_avg:61.11ms
step:119/2330 train_time:7271ms step_avg:61.10ms
step:120/2330 train_time:7334ms step_avg:61.12ms
step:121/2330 train_time:7393ms step_avg:61.10ms
step:122/2330 train_time:7455ms step_avg:61.11ms
step:123/2330 train_time:7515ms step_avg:61.10ms
step:124/2330 train_time:7576ms step_avg:61.10ms
step:125/2330 train_time:7636ms step_avg:61.09ms
step:126/2330 train_time:7698ms step_avg:61.10ms
step:127/2330 train_time:7758ms step_avg:61.09ms
step:128/2330 train_time:7821ms step_avg:61.10ms
step:129/2330 train_time:7881ms step_avg:61.09ms
step:130/2330 train_time:7945ms step_avg:61.11ms
step:131/2330 train_time:8005ms step_avg:61.11ms
step:132/2330 train_time:8068ms step_avg:61.12ms
step:133/2330 train_time:8128ms step_avg:61.12ms
step:134/2330 train_time:8191ms step_avg:61.13ms
step:135/2330 train_time:8251ms step_avg:61.12ms
step:136/2330 train_time:8314ms step_avg:61.13ms
step:137/2330 train_time:8373ms step_avg:61.12ms
step:138/2330 train_time:8435ms step_avg:61.13ms
step:139/2330 train_time:8495ms step_avg:61.11ms
step:140/2330 train_time:8557ms step_avg:61.12ms
step:141/2330 train_time:8616ms step_avg:61.10ms
step:142/2330 train_time:8678ms step_avg:61.11ms
step:143/2330 train_time:8738ms step_avg:61.10ms
step:144/2330 train_time:8800ms step_avg:61.11ms
step:145/2330 train_time:8860ms step_avg:61.10ms
step:146/2330 train_time:8923ms step_avg:61.12ms
step:147/2330 train_time:8984ms step_avg:61.11ms
step:148/2330 train_time:9047ms step_avg:61.13ms
step:149/2330 train_time:9108ms step_avg:61.13ms
step:150/2330 train_time:9170ms step_avg:61.14ms
step:151/2330 train_time:9230ms step_avg:61.13ms
step:152/2330 train_time:9293ms step_avg:61.14ms
step:153/2330 train_time:9353ms step_avg:61.13ms
step:154/2330 train_time:9415ms step_avg:61.14ms
step:155/2330 train_time:9475ms step_avg:61.13ms
step:156/2330 train_time:9537ms step_avg:61.14ms
step:157/2330 train_time:9596ms step_avg:61.12ms
step:158/2330 train_time:9658ms step_avg:61.13ms
step:159/2330 train_time:9718ms step_avg:61.12ms
step:160/2330 train_time:9780ms step_avg:61.13ms
step:161/2330 train_time:9840ms step_avg:61.12ms
step:162/2330 train_time:9903ms step_avg:61.13ms
step:163/2330 train_time:9963ms step_avg:61.12ms
step:164/2330 train_time:10027ms step_avg:61.14ms
step:165/2330 train_time:10087ms step_avg:61.13ms
step:166/2330 train_time:10150ms step_avg:61.14ms
step:167/2330 train_time:10210ms step_avg:61.14ms
step:168/2330 train_time:10273ms step_avg:61.15ms
step:169/2330 train_time:10332ms step_avg:61.14ms
step:170/2330 train_time:10394ms step_avg:61.14ms
step:171/2330 train_time:10454ms step_avg:61.13ms
step:172/2330 train_time:10516ms step_avg:61.14ms
step:173/2330 train_time:10575ms step_avg:61.13ms
step:174/2330 train_time:10637ms step_avg:61.13ms
step:175/2330 train_time:10697ms step_avg:61.13ms
step:176/2330 train_time:10760ms step_avg:61.13ms
step:177/2330 train_time:10819ms step_avg:61.13ms
step:178/2330 train_time:10883ms step_avg:61.14ms
step:179/2330 train_time:10943ms step_avg:61.13ms
step:180/2330 train_time:11006ms step_avg:61.14ms
step:181/2330 train_time:11066ms step_avg:61.14ms
step:182/2330 train_time:11129ms step_avg:61.15ms
step:183/2330 train_time:11189ms step_avg:61.14ms
step:184/2330 train_time:11252ms step_avg:61.15ms
step:185/2330 train_time:11311ms step_avg:61.14ms
step:186/2330 train_time:11373ms step_avg:61.15ms
step:187/2330 train_time:11433ms step_avg:61.14ms
step:188/2330 train_time:11495ms step_avg:61.14ms
step:189/2330 train_time:11554ms step_avg:61.13ms
step:190/2330 train_time:11616ms step_avg:61.14ms
step:191/2330 train_time:11676ms step_avg:61.13ms
step:192/2330 train_time:11738ms step_avg:61.14ms
step:193/2330 train_time:11798ms step_avg:61.13ms
step:194/2330 train_time:11861ms step_avg:61.14ms
step:195/2330 train_time:11921ms step_avg:61.13ms
step:196/2330 train_time:11984ms step_avg:61.14ms
step:197/2330 train_time:12044ms step_avg:61.14ms
step:198/2330 train_time:12107ms step_avg:61.15ms
step:199/2330 train_time:12167ms step_avg:61.14ms
step:200/2330 train_time:12231ms step_avg:61.15ms
step:201/2330 train_time:12290ms step_avg:61.15ms
step:202/2330 train_time:12353ms step_avg:61.15ms
step:203/2330 train_time:12412ms step_avg:61.14ms
step:204/2330 train_time:12474ms step_avg:61.15ms
step:205/2330 train_time:12534ms step_avg:61.14ms
step:206/2330 train_time:12596ms step_avg:61.15ms
step:207/2330 train_time:12656ms step_avg:61.14ms
step:208/2330 train_time:12718ms step_avg:61.14ms
step:209/2330 train_time:12777ms step_avg:61.14ms
step:210/2330 train_time:12840ms step_avg:61.14ms
step:211/2330 train_time:12901ms step_avg:61.14ms
step:212/2330 train_time:12964ms step_avg:61.15ms
step:213/2330 train_time:13024ms step_avg:61.14ms
step:214/2330 train_time:13087ms step_avg:61.15ms
step:215/2330 train_time:13147ms step_avg:61.15ms
step:216/2330 train_time:13210ms step_avg:61.16ms
step:217/2330 train_time:13270ms step_avg:61.15ms
step:218/2330 train_time:13334ms step_avg:61.16ms
step:219/2330 train_time:13393ms step_avg:61.15ms
step:220/2330 train_time:13456ms step_avg:61.16ms
step:221/2330 train_time:13515ms step_avg:61.15ms
step:222/2330 train_time:13577ms step_avg:61.16ms
step:223/2330 train_time:13637ms step_avg:61.15ms
step:224/2330 train_time:13699ms step_avg:61.16ms
step:225/2330 train_time:13758ms step_avg:61.15ms
step:226/2330 train_time:13822ms step_avg:61.16ms
step:227/2330 train_time:13881ms step_avg:61.15ms
step:228/2330 train_time:13944ms step_avg:61.16ms
step:229/2330 train_time:14004ms step_avg:61.15ms
step:230/2330 train_time:14068ms step_avg:61.16ms
step:231/2330 train_time:14128ms step_avg:61.16ms
step:232/2330 train_time:14190ms step_avg:61.17ms
step:233/2330 train_time:14251ms step_avg:61.16ms
step:234/2330 train_time:14313ms step_avg:61.17ms
step:235/2330 train_time:14372ms step_avg:61.16ms
step:236/2330 train_time:14435ms step_avg:61.16ms
step:237/2330 train_time:14494ms step_avg:61.16ms
step:238/2330 train_time:14556ms step_avg:61.16ms
step:239/2330 train_time:14615ms step_avg:61.15ms
step:240/2330 train_time:14678ms step_avg:61.16ms
step:241/2330 train_time:14737ms step_avg:61.15ms
step:242/2330 train_time:14800ms step_avg:61.16ms
step:243/2330 train_time:14859ms step_avg:61.15ms
step:244/2330 train_time:14923ms step_avg:61.16ms
step:245/2330 train_time:14983ms step_avg:61.16ms
step:246/2330 train_time:15046ms step_avg:61.16ms
step:247/2330 train_time:15106ms step_avg:61.16ms
step:248/2330 train_time:15169ms step_avg:61.17ms
step:249/2330 train_time:15228ms step_avg:61.16ms
step:250/2330 train_time:15291ms step_avg:61.16ms
step:250/2330 val_loss:4.3305 train_time:15356ms step_avg:61.42ms
step:251/2330 train_time:15378ms step_avg:61.27ms
step:252/2330 train_time:15415ms step_avg:61.17ms
step:253/2330 train_time:15481ms step_avg:61.19ms
step:254/2330 train_time:15547ms step_avg:61.21ms
step:255/2330 train_time:15608ms step_avg:61.21ms
step:256/2330 train_time:15671ms step_avg:61.22ms
step:257/2330 train_time:15731ms step_avg:61.21ms
step:258/2330 train_time:15793ms step_avg:61.22ms
step:259/2330 train_time:15853ms step_avg:61.21ms
step:260/2330 train_time:15914ms step_avg:61.21ms
step:261/2330 train_time:15973ms step_avg:61.20ms
step:262/2330 train_time:16036ms step_avg:61.21ms
step:263/2330 train_time:16094ms step_avg:61.20ms
step:264/2330 train_time:16157ms step_avg:61.20ms
step:265/2330 train_time:16215ms step_avg:61.19ms
step:266/2330 train_time:16278ms step_avg:61.19ms
step:267/2330 train_time:16338ms step_avg:61.19ms
step:268/2330 train_time:16401ms step_avg:61.20ms
step:269/2330 train_time:16462ms step_avg:61.20ms
step:270/2330 train_time:16525ms step_avg:61.21ms
step:271/2330 train_time:16586ms step_avg:61.20ms
step:272/2330 train_time:16649ms step_avg:61.21ms
step:273/2330 train_time:16709ms step_avg:61.21ms
step:274/2330 train_time:16772ms step_avg:61.21ms
step:275/2330 train_time:16832ms step_avg:61.21ms
step:276/2330 train_time:16894ms step_avg:61.21ms
step:277/2330 train_time:16954ms step_avg:61.21ms
step:278/2330 train_time:17016ms step_avg:61.21ms
step:279/2330 train_time:17075ms step_avg:61.20ms
step:280/2330 train_time:17137ms step_avg:61.21ms
step:281/2330 train_time:17196ms step_avg:61.20ms
step:282/2330 train_time:17258ms step_avg:61.20ms
step:283/2330 train_time:17318ms step_avg:61.19ms
step:284/2330 train_time:17381ms step_avg:61.20ms
step:285/2330 train_time:17443ms step_avg:61.20ms
step:286/2330 train_time:17506ms step_avg:61.21ms
step:287/2330 train_time:17566ms step_avg:61.20ms
step:288/2330 train_time:17629ms step_avg:61.21ms
step:289/2330 train_time:17689ms step_avg:61.21ms
step:290/2330 train_time:17752ms step_avg:61.21ms
step:291/2330 train_time:17811ms step_avg:61.21ms
step:292/2330 train_time:17874ms step_avg:61.21ms
step:293/2330 train_time:17934ms step_avg:61.21ms
step:294/2330 train_time:17997ms step_avg:61.21ms
step:295/2330 train_time:18056ms step_avg:61.21ms
step:296/2330 train_time:18119ms step_avg:61.21ms
step:297/2330 train_time:18178ms step_avg:61.21ms
step:298/2330 train_time:18241ms step_avg:61.21ms
step:299/2330 train_time:18301ms step_avg:61.21ms
step:300/2330 train_time:18363ms step_avg:61.21ms
step:301/2330 train_time:18423ms step_avg:61.21ms
step:302/2330 train_time:18486ms step_avg:61.21ms
step:303/2330 train_time:18546ms step_avg:61.21ms
step:304/2330 train_time:18609ms step_avg:61.21ms
step:305/2330 train_time:18669ms step_avg:61.21ms
step:306/2330 train_time:18732ms step_avg:61.22ms
step:307/2330 train_time:18792ms step_avg:61.21ms
step:308/2330 train_time:18854ms step_avg:61.22ms
step:309/2330 train_time:18914ms step_avg:61.21ms
step:310/2330 train_time:18978ms step_avg:61.22ms
step:311/2330 train_time:19037ms step_avg:61.21ms
step:312/2330 train_time:19100ms step_avg:61.22ms
step:313/2330 train_time:19160ms step_avg:61.21ms
step:314/2330 train_time:19223ms step_avg:61.22ms
step:315/2330 train_time:19283ms step_avg:61.21ms
step:316/2330 train_time:19346ms step_avg:61.22ms
step:317/2330 train_time:19405ms step_avg:61.22ms
step:318/2330 train_time:19468ms step_avg:61.22ms
step:319/2330 train_time:19528ms step_avg:61.21ms
step:320/2330 train_time:19590ms step_avg:61.22ms
step:321/2330 train_time:19650ms step_avg:61.21ms
step:322/2330 train_time:19712ms step_avg:61.22ms
step:323/2330 train_time:19772ms step_avg:61.21ms
step:324/2330 train_time:19835ms step_avg:61.22ms
step:325/2330 train_time:19895ms step_avg:61.21ms
step:326/2330 train_time:19957ms step_avg:61.22ms
step:327/2330 train_time:20018ms step_avg:61.22ms
step:328/2330 train_time:20081ms step_avg:61.22ms
step:329/2330 train_time:20140ms step_avg:61.22ms
step:330/2330 train_time:20202ms step_avg:61.22ms
step:331/2330 train_time:20262ms step_avg:61.21ms
step:332/2330 train_time:20324ms step_avg:61.22ms
step:333/2330 train_time:20384ms step_avg:61.21ms
step:334/2330 train_time:20446ms step_avg:61.22ms
step:335/2330 train_time:20506ms step_avg:61.21ms
step:336/2330 train_time:20568ms step_avg:61.21ms
step:337/2330 train_time:20627ms step_avg:61.21ms
step:338/2330 train_time:20690ms step_avg:61.21ms
step:339/2330 train_time:20749ms step_avg:61.21ms
step:340/2330 train_time:20812ms step_avg:61.21ms
step:341/2330 train_time:20872ms step_avg:61.21ms
step:342/2330 train_time:20936ms step_avg:61.22ms
step:343/2330 train_time:20996ms step_avg:61.21ms
step:344/2330 train_time:21059ms step_avg:61.22ms
step:345/2330 train_time:21119ms step_avg:61.21ms
step:346/2330 train_time:21182ms step_avg:61.22ms
step:347/2330 train_time:21242ms step_avg:61.21ms
step:348/2330 train_time:21304ms step_avg:61.22ms
step:349/2330 train_time:21364ms step_avg:61.21ms
step:350/2330 train_time:21426ms step_avg:61.22ms
step:351/2330 train_time:21485ms step_avg:61.21ms
step:352/2330 train_time:21548ms step_avg:61.22ms
step:353/2330 train_time:21607ms step_avg:61.21ms
step:354/2330 train_time:21670ms step_avg:61.21ms
step:355/2330 train_time:21729ms step_avg:61.21ms
step:356/2330 train_time:21792ms step_avg:61.21ms
step:357/2330 train_time:21852ms step_avg:61.21ms
step:358/2330 train_time:21916ms step_avg:61.22ms
step:359/2330 train_time:21977ms step_avg:61.22ms
step:360/2330 train_time:22040ms step_avg:61.22ms
step:361/2330 train_time:22100ms step_avg:61.22ms
step:362/2330 train_time:22162ms step_avg:61.22ms
step:363/2330 train_time:22222ms step_avg:61.22ms
step:364/2330 train_time:22285ms step_avg:61.22ms
step:365/2330 train_time:22344ms step_avg:61.22ms
step:366/2330 train_time:22406ms step_avg:61.22ms
step:367/2330 train_time:22466ms step_avg:61.22ms
step:368/2330 train_time:22528ms step_avg:61.22ms
step:369/2330 train_time:22588ms step_avg:61.22ms
step:370/2330 train_time:22651ms step_avg:61.22ms
step:371/2330 train_time:22710ms step_avg:61.21ms
step:372/2330 train_time:22772ms step_avg:61.22ms
step:373/2330 train_time:22833ms step_avg:61.21ms
step:374/2330 train_time:22896ms step_avg:61.22ms
step:375/2330 train_time:22956ms step_avg:61.22ms
step:376/2330 train_time:23019ms step_avg:61.22ms
step:377/2330 train_time:23080ms step_avg:61.22ms
step:378/2330 train_time:23143ms step_avg:61.23ms
step:379/2330 train_time:23202ms step_avg:61.22ms
step:380/2330 train_time:23265ms step_avg:61.22ms
step:381/2330 train_time:23324ms step_avg:61.22ms
step:382/2330 train_time:23387ms step_avg:61.22ms
step:383/2330 train_time:23446ms step_avg:61.22ms
step:384/2330 train_time:23508ms step_avg:61.22ms
step:385/2330 train_time:23567ms step_avg:61.21ms
step:386/2330 train_time:23630ms step_avg:61.22ms
step:387/2330 train_time:23690ms step_avg:61.21ms
step:388/2330 train_time:23752ms step_avg:61.22ms
step:389/2330 train_time:23813ms step_avg:61.21ms
step:390/2330 train_time:23876ms step_avg:61.22ms
step:391/2330 train_time:23936ms step_avg:61.22ms
step:392/2330 train_time:23999ms step_avg:61.22ms
step:393/2330 train_time:24059ms step_avg:61.22ms
step:394/2330 train_time:24122ms step_avg:61.22ms
step:395/2330 train_time:24182ms step_avg:61.22ms
step:396/2330 train_time:24245ms step_avg:61.22ms
step:397/2330 train_time:24304ms step_avg:61.22ms
step:398/2330 train_time:24367ms step_avg:61.22ms
step:399/2330 train_time:24426ms step_avg:61.22ms
step:400/2330 train_time:24488ms step_avg:61.22ms
step:401/2330 train_time:24548ms step_avg:61.22ms
step:402/2330 train_time:24610ms step_avg:61.22ms
step:403/2330 train_time:24670ms step_avg:61.22ms
step:404/2330 train_time:24733ms step_avg:61.22ms
step:405/2330 train_time:24793ms step_avg:61.22ms
step:406/2330 train_time:24856ms step_avg:61.22ms
step:407/2330 train_time:24916ms step_avg:61.22ms
step:408/2330 train_time:24980ms step_avg:61.22ms
step:409/2330 train_time:25041ms step_avg:61.22ms
step:410/2330 train_time:25103ms step_avg:61.23ms
step:411/2330 train_time:25164ms step_avg:61.23ms
step:412/2330 train_time:25226ms step_avg:61.23ms
step:413/2330 train_time:25285ms step_avg:61.22ms
step:414/2330 train_time:25348ms step_avg:61.23ms
step:415/2330 train_time:25407ms step_avg:61.22ms
step:416/2330 train_time:25470ms step_avg:61.23ms
step:417/2330 train_time:25530ms step_avg:61.22ms
step:418/2330 train_time:25593ms step_avg:61.23ms
step:419/2330 train_time:25652ms step_avg:61.22ms
step:420/2330 train_time:25715ms step_avg:61.23ms
step:421/2330 train_time:25776ms step_avg:61.22ms
step:422/2330 train_time:25839ms step_avg:61.23ms
step:423/2330 train_time:25898ms step_avg:61.23ms
step:424/2330 train_time:25961ms step_avg:61.23ms
step:425/2330 train_time:26021ms step_avg:61.23ms
step:426/2330 train_time:26084ms step_avg:61.23ms
step:427/2330 train_time:26144ms step_avg:61.23ms
step:428/2330 train_time:26206ms step_avg:61.23ms
step:429/2330 train_time:26265ms step_avg:61.22ms
step:430/2330 train_time:26327ms step_avg:61.23ms
step:431/2330 train_time:26387ms step_avg:61.22ms
step:432/2330 train_time:26450ms step_avg:61.23ms
step:433/2330 train_time:26509ms step_avg:61.22ms
step:434/2330 train_time:26572ms step_avg:61.23ms
step:435/2330 train_time:26632ms step_avg:61.22ms
step:436/2330 train_time:26695ms step_avg:61.23ms
step:437/2330 train_time:26755ms step_avg:61.22ms
step:438/2330 train_time:26818ms step_avg:61.23ms
step:439/2330 train_time:26878ms step_avg:61.23ms
step:440/2330 train_time:26942ms step_avg:61.23ms
step:441/2330 train_time:27000ms step_avg:61.22ms
step:442/2330 train_time:27063ms step_avg:61.23ms
step:443/2330 train_time:27123ms step_avg:61.23ms
step:444/2330 train_time:27186ms step_avg:61.23ms
step:445/2330 train_time:27245ms step_avg:61.23ms
step:446/2330 train_time:27308ms step_avg:61.23ms
step:447/2330 train_time:27367ms step_avg:61.22ms
step:448/2330 train_time:27429ms step_avg:61.23ms
step:449/2330 train_time:27489ms step_avg:61.22ms
step:450/2330 train_time:27551ms step_avg:61.22ms
step:451/2330 train_time:27611ms step_avg:61.22ms
step:452/2330 train_time:27674ms step_avg:61.23ms
step:453/2330 train_time:27735ms step_avg:61.22ms
step:454/2330 train_time:27798ms step_avg:61.23ms
step:455/2330 train_time:27858ms step_avg:61.23ms
step:456/2330 train_time:27921ms step_avg:61.23ms
step:457/2330 train_time:27981ms step_avg:61.23ms
step:458/2330 train_time:28044ms step_avg:61.23ms
step:459/2330 train_time:28103ms step_avg:61.23ms
step:460/2330 train_time:28165ms step_avg:61.23ms
step:461/2330 train_time:28226ms step_avg:61.23ms
step:462/2330 train_time:28288ms step_avg:61.23ms
step:463/2330 train_time:28348ms step_avg:61.23ms
step:464/2330 train_time:28410ms step_avg:61.23ms
step:465/2330 train_time:28469ms step_avg:61.22ms
step:466/2330 train_time:28532ms step_avg:61.23ms
step:467/2330 train_time:28592ms step_avg:61.22ms
step:468/2330 train_time:28654ms step_avg:61.23ms
step:469/2330 train_time:28715ms step_avg:61.23ms
step:470/2330 train_time:28779ms step_avg:61.23ms
step:471/2330 train_time:28839ms step_avg:61.23ms
step:472/2330 train_time:28901ms step_avg:61.23ms
step:473/2330 train_time:28961ms step_avg:61.23ms
step:474/2330 train_time:29024ms step_avg:61.23ms
step:475/2330 train_time:29083ms step_avg:61.23ms
step:476/2330 train_time:29146ms step_avg:61.23ms
step:477/2330 train_time:29205ms step_avg:61.23ms
step:478/2330 train_time:29267ms step_avg:61.23ms
step:479/2330 train_time:29327ms step_avg:61.22ms
step:480/2330 train_time:29389ms step_avg:61.23ms
step:481/2330 train_time:29449ms step_avg:61.22ms
step:482/2330 train_time:29511ms step_avg:61.23ms
step:483/2330 train_time:29571ms step_avg:61.22ms
step:484/2330 train_time:29634ms step_avg:61.23ms
step:485/2330 train_time:29694ms step_avg:61.23ms
step:486/2330 train_time:29757ms step_avg:61.23ms
step:487/2330 train_time:29818ms step_avg:61.23ms
step:488/2330 train_time:29882ms step_avg:61.23ms
step:489/2330 train_time:29941ms step_avg:61.23ms
step:490/2330 train_time:30004ms step_avg:61.23ms
step:491/2330 train_time:30064ms step_avg:61.23ms
step:492/2330 train_time:30126ms step_avg:61.23ms
step:493/2330 train_time:30186ms step_avg:61.23ms
step:494/2330 train_time:30248ms step_avg:61.23ms
step:495/2330 train_time:30307ms step_avg:61.23ms
step:496/2330 train_time:30371ms step_avg:61.23ms
step:497/2330 train_time:30430ms step_avg:61.23ms
step:498/2330 train_time:30492ms step_avg:61.23ms
step:499/2330 train_time:30551ms step_avg:61.23ms
step:500/2330 train_time:30615ms step_avg:61.23ms
step:500/2330 val_loss:3.9309 train_time:30679ms step_avg:61.36ms
step:501/2330 train_time:30702ms step_avg:61.28ms
step:502/2330 train_time:30741ms step_avg:61.24ms
step:503/2330 train_time:30805ms step_avg:61.24ms
step:504/2330 train_time:30870ms step_avg:61.25ms
step:505/2330 train_time:30930ms step_avg:61.25ms
step:506/2330 train_time:30993ms step_avg:61.25ms
step:507/2330 train_time:31051ms step_avg:61.25ms
step:508/2330 train_time:31113ms step_avg:61.25ms
step:509/2330 train_time:31172ms step_avg:61.24ms
step:510/2330 train_time:31234ms step_avg:61.24ms
step:511/2330 train_time:31293ms step_avg:61.24ms
step:512/2330 train_time:31356ms step_avg:61.24ms
step:513/2330 train_time:31416ms step_avg:61.24ms
step:514/2330 train_time:31479ms step_avg:61.24ms
step:515/2330 train_time:31539ms step_avg:61.24ms
step:516/2330 train_time:31601ms step_avg:61.24ms
step:517/2330 train_time:31661ms step_avg:61.24ms
step:518/2330 train_time:31724ms step_avg:61.24ms
step:519/2330 train_time:31786ms step_avg:61.24ms
step:520/2330 train_time:31849ms step_avg:61.25ms
step:521/2330 train_time:31909ms step_avg:61.25ms
step:522/2330 train_time:31972ms step_avg:61.25ms
step:523/2330 train_time:32031ms step_avg:61.24ms
step:524/2330 train_time:32094ms step_avg:61.25ms
step:525/2330 train_time:32153ms step_avg:61.24ms
step:526/2330 train_time:32215ms step_avg:61.25ms
step:527/2330 train_time:32274ms step_avg:61.24ms
step:528/2330 train_time:32337ms step_avg:61.24ms
step:529/2330 train_time:32396ms step_avg:61.24ms
step:530/2330 train_time:32460ms step_avg:61.24ms
step:531/2330 train_time:32520ms step_avg:61.24ms
step:532/2330 train_time:32582ms step_avg:61.24ms
step:533/2330 train_time:32642ms step_avg:61.24ms
step:534/2330 train_time:32705ms step_avg:61.25ms
step:535/2330 train_time:32765ms step_avg:61.24ms
step:536/2330 train_time:32829ms step_avg:61.25ms
step:537/2330 train_time:32888ms step_avg:61.24ms
step:538/2330 train_time:32951ms step_avg:61.25ms
step:539/2330 train_time:33011ms step_avg:61.25ms
step:540/2330 train_time:33074ms step_avg:61.25ms
step:541/2330 train_time:33133ms step_avg:61.24ms
step:542/2330 train_time:33196ms step_avg:61.25ms
step:543/2330 train_time:33255ms step_avg:61.24ms
step:544/2330 train_time:33319ms step_avg:61.25ms
step:545/2330 train_time:33378ms step_avg:61.24ms
step:546/2330 train_time:33441ms step_avg:61.25ms
step:547/2330 train_time:33501ms step_avg:61.24ms
step:548/2330 train_time:33564ms step_avg:61.25ms
step:549/2330 train_time:33624ms step_avg:61.25ms
step:550/2330 train_time:33687ms step_avg:61.25ms
step:551/2330 train_time:33747ms step_avg:61.25ms
step:552/2330 train_time:33810ms step_avg:61.25ms
step:553/2330 train_time:33869ms step_avg:61.25ms
step:554/2330 train_time:33933ms step_avg:61.25ms
step:555/2330 train_time:33992ms step_avg:61.25ms
step:556/2330 train_time:34056ms step_avg:61.25ms
step:557/2330 train_time:34115ms step_avg:61.25ms
step:558/2330 train_time:34178ms step_avg:61.25ms
step:559/2330 train_time:34238ms step_avg:61.25ms
step:560/2330 train_time:34300ms step_avg:61.25ms
step:561/2330 train_time:34360ms step_avg:61.25ms
step:562/2330 train_time:34424ms step_avg:61.25ms
step:563/2330 train_time:34484ms step_avg:61.25ms
step:564/2330 train_time:34546ms step_avg:61.25ms
step:565/2330 train_time:34605ms step_avg:61.25ms
step:566/2330 train_time:34668ms step_avg:61.25ms
step:567/2330 train_time:34728ms step_avg:61.25ms
step:568/2330 train_time:34790ms step_avg:61.25ms
step:569/2330 train_time:34850ms step_avg:61.25ms
step:570/2330 train_time:34912ms step_avg:61.25ms
step:571/2330 train_time:34972ms step_avg:61.25ms
step:572/2330 train_time:35034ms step_avg:61.25ms
step:573/2330 train_time:35094ms step_avg:61.25ms
step:574/2330 train_time:35157ms step_avg:61.25ms
step:575/2330 train_time:35217ms step_avg:61.25ms
step:576/2330 train_time:35280ms step_avg:61.25ms
step:577/2330 train_time:35340ms step_avg:61.25ms
step:578/2330 train_time:35403ms step_avg:61.25ms
step:579/2330 train_time:35463ms step_avg:61.25ms
step:580/2330 train_time:35526ms step_avg:61.25ms
step:581/2330 train_time:35585ms step_avg:61.25ms
step:582/2330 train_time:35648ms step_avg:61.25ms
step:583/2330 train_time:35707ms step_avg:61.25ms
step:584/2330 train_time:35770ms step_avg:61.25ms
step:585/2330 train_time:35829ms step_avg:61.25ms
step:586/2330 train_time:35892ms step_avg:61.25ms
step:587/2330 train_time:35952ms step_avg:61.25ms
step:588/2330 train_time:36015ms step_avg:61.25ms
step:589/2330 train_time:36075ms step_avg:61.25ms
step:590/2330 train_time:36137ms step_avg:61.25ms
step:591/2330 train_time:36197ms step_avg:61.25ms
step:592/2330 train_time:36260ms step_avg:61.25ms
step:593/2330 train_time:36320ms step_avg:61.25ms
step:594/2330 train_time:36383ms step_avg:61.25ms
step:595/2330 train_time:36444ms step_avg:61.25ms
step:596/2330 train_time:36507ms step_avg:61.25ms
step:597/2330 train_time:36567ms step_avg:61.25ms
step:598/2330 train_time:36630ms step_avg:61.25ms
step:599/2330 train_time:36689ms step_avg:61.25ms
step:600/2330 train_time:36751ms step_avg:61.25ms
step:601/2330 train_time:36810ms step_avg:61.25ms
step:602/2330 train_time:36873ms step_avg:61.25ms
step:603/2330 train_time:36932ms step_avg:61.25ms
step:604/2330 train_time:36995ms step_avg:61.25ms
step:605/2330 train_time:37055ms step_avg:61.25ms
step:606/2330 train_time:37118ms step_avg:61.25ms
step:607/2330 train_time:37178ms step_avg:61.25ms
step:608/2330 train_time:37241ms step_avg:61.25ms
step:609/2330 train_time:37301ms step_avg:61.25ms
step:610/2330 train_time:37364ms step_avg:61.25ms
step:611/2330 train_time:37423ms step_avg:61.25ms
step:612/2330 train_time:37487ms step_avg:61.25ms
step:613/2330 train_time:37546ms step_avg:61.25ms
step:614/2330 train_time:37610ms step_avg:61.25ms
step:615/2330 train_time:37669ms step_avg:61.25ms
step:616/2330 train_time:37732ms step_avg:61.25ms
step:617/2330 train_time:37791ms step_avg:61.25ms
step:618/2330 train_time:37853ms step_avg:61.25ms
step:619/2330 train_time:37912ms step_avg:61.25ms
step:620/2330 train_time:37975ms step_avg:61.25ms
step:621/2330 train_time:38035ms step_avg:61.25ms
step:622/2330 train_time:38098ms step_avg:61.25ms
step:623/2330 train_time:38158ms step_avg:61.25ms
step:624/2330 train_time:38221ms step_avg:61.25ms
step:625/2330 train_time:38281ms step_avg:61.25ms
step:626/2330 train_time:38344ms step_avg:61.25ms
step:627/2330 train_time:38404ms step_avg:61.25ms
step:628/2330 train_time:38467ms step_avg:61.25ms
step:629/2330 train_time:38527ms step_avg:61.25ms
step:630/2330 train_time:38589ms step_avg:61.25ms
step:631/2330 train_time:38649ms step_avg:61.25ms
step:632/2330 train_time:38711ms step_avg:61.25ms
step:633/2330 train_time:38771ms step_avg:61.25ms
step:634/2330 train_time:38834ms step_avg:61.25ms
step:635/2330 train_time:38893ms step_avg:61.25ms
step:636/2330 train_time:38956ms step_avg:61.25ms
step:637/2330 train_time:39015ms step_avg:61.25ms
step:638/2330 train_time:39079ms step_avg:61.25ms
step:639/2330 train_time:39139ms step_avg:61.25ms
step:640/2330 train_time:39201ms step_avg:61.25ms
step:641/2330 train_time:39261ms step_avg:61.25ms
step:642/2330 train_time:39325ms step_avg:61.25ms
step:643/2330 train_time:39385ms step_avg:61.25ms
step:644/2330 train_time:39448ms step_avg:61.25ms
step:645/2330 train_time:39508ms step_avg:61.25ms
step:646/2330 train_time:39571ms step_avg:61.25ms
step:647/2330 train_time:39630ms step_avg:61.25ms
step:648/2330 train_time:39693ms step_avg:61.25ms
step:649/2330 train_time:39753ms step_avg:61.25ms
step:650/2330 train_time:39816ms step_avg:61.26ms
step:651/2330 train_time:39876ms step_avg:61.25ms
step:652/2330 train_time:39938ms step_avg:61.25ms
step:653/2330 train_time:39998ms step_avg:61.25ms
step:654/2330 train_time:40061ms step_avg:61.26ms
step:655/2330 train_time:40121ms step_avg:61.25ms
step:656/2330 train_time:40183ms step_avg:61.26ms
step:657/2330 train_time:40243ms step_avg:61.25ms
step:658/2330 train_time:40305ms step_avg:61.25ms
step:659/2330 train_time:40366ms step_avg:61.25ms
step:660/2330 train_time:40429ms step_avg:61.26ms
step:661/2330 train_time:40489ms step_avg:61.25ms
step:662/2330 train_time:40551ms step_avg:61.26ms
step:663/2330 train_time:40610ms step_avg:61.25ms
step:664/2330 train_time:40673ms step_avg:61.25ms
step:665/2330 train_time:40733ms step_avg:61.25ms
step:666/2330 train_time:40796ms step_avg:61.26ms
step:667/2330 train_time:40856ms step_avg:61.25ms
step:668/2330 train_time:40918ms step_avg:61.26ms
step:669/2330 train_time:40978ms step_avg:61.25ms
step:670/2330 train_time:41041ms step_avg:61.26ms
step:671/2330 train_time:41101ms step_avg:61.25ms
step:672/2330 train_time:41164ms step_avg:61.26ms
step:673/2330 train_time:41224ms step_avg:61.25ms
step:674/2330 train_time:41286ms step_avg:61.26ms
step:675/2330 train_time:41346ms step_avg:61.25ms
step:676/2330 train_time:41409ms step_avg:61.26ms
step:677/2330 train_time:41469ms step_avg:61.25ms
step:678/2330 train_time:41532ms step_avg:61.26ms
step:679/2330 train_time:41591ms step_avg:61.25ms
step:680/2330 train_time:41653ms step_avg:61.25ms
step:681/2330 train_time:41713ms step_avg:61.25ms
step:682/2330 train_time:41776ms step_avg:61.26ms
step:683/2330 train_time:41836ms step_avg:61.25ms
step:684/2330 train_time:41899ms step_avg:61.26ms
step:685/2330 train_time:41958ms step_avg:61.25ms
step:686/2330 train_time:42022ms step_avg:61.26ms
step:687/2330 train_time:42082ms step_avg:61.25ms
step:688/2330 train_time:42144ms step_avg:61.26ms
step:689/2330 train_time:42204ms step_avg:61.25ms
step:690/2330 train_time:42267ms step_avg:61.26ms
step:691/2330 train_time:42327ms step_avg:61.25ms
step:692/2330 train_time:42390ms step_avg:61.26ms
step:693/2330 train_time:42449ms step_avg:61.25ms
step:694/2330 train_time:42512ms step_avg:61.26ms
step:695/2330 train_time:42571ms step_avg:61.25ms
step:696/2330 train_time:42634ms step_avg:61.26ms
step:697/2330 train_time:42693ms step_avg:61.25ms
step:698/2330 train_time:42756ms step_avg:61.25ms
step:699/2330 train_time:42815ms step_avg:61.25ms
step:700/2330 train_time:42878ms step_avg:61.25ms
step:701/2330 train_time:42938ms step_avg:61.25ms
step:702/2330 train_time:43001ms step_avg:61.25ms
step:703/2330 train_time:43061ms step_avg:61.25ms
step:704/2330 train_time:43124ms step_avg:61.26ms
step:705/2330 train_time:43185ms step_avg:61.25ms
step:706/2330 train_time:43247ms step_avg:61.26ms
step:707/2330 train_time:43307ms step_avg:61.25ms
step:708/2330 train_time:43369ms step_avg:61.26ms
step:709/2330 train_time:43429ms step_avg:61.25ms
step:710/2330 train_time:43491ms step_avg:61.26ms
step:711/2330 train_time:43551ms step_avg:61.25ms
step:712/2330 train_time:43614ms step_avg:61.26ms
step:713/2330 train_time:43673ms step_avg:61.25ms
step:714/2330 train_time:43736ms step_avg:61.26ms
step:715/2330 train_time:43796ms step_avg:61.25ms
step:716/2330 train_time:43859ms step_avg:61.26ms
step:717/2330 train_time:43920ms step_avg:61.26ms
step:718/2330 train_time:43983ms step_avg:61.26ms
step:719/2330 train_time:44042ms step_avg:61.25ms
step:720/2330 train_time:44105ms step_avg:61.26ms
step:721/2330 train_time:44165ms step_avg:61.26ms
step:722/2330 train_time:44228ms step_avg:61.26ms
step:723/2330 train_time:44288ms step_avg:61.26ms
step:724/2330 train_time:44350ms step_avg:61.26ms
step:725/2330 train_time:44409ms step_avg:61.25ms
step:726/2330 train_time:44471ms step_avg:61.26ms
step:727/2330 train_time:44531ms step_avg:61.25ms
step:728/2330 train_time:44593ms step_avg:61.25ms
step:729/2330 train_time:44653ms step_avg:61.25ms
step:730/2330 train_time:44716ms step_avg:61.26ms
step:731/2330 train_time:44777ms step_avg:61.25ms
step:732/2330 train_time:44840ms step_avg:61.26ms
step:733/2330 train_time:44900ms step_avg:61.25ms
step:734/2330 train_time:44963ms step_avg:61.26ms
step:735/2330 train_time:45023ms step_avg:61.26ms
step:736/2330 train_time:45086ms step_avg:61.26ms
step:737/2330 train_time:45145ms step_avg:61.26ms
step:738/2330 train_time:45208ms step_avg:61.26ms
step:739/2330 train_time:45267ms step_avg:61.25ms
step:740/2330 train_time:45330ms step_avg:61.26ms
step:741/2330 train_time:45389ms step_avg:61.25ms
step:742/2330 train_time:45452ms step_avg:61.26ms
step:743/2330 train_time:45511ms step_avg:61.25ms
step:744/2330 train_time:45574ms step_avg:61.26ms
step:745/2330 train_time:45633ms step_avg:61.25ms
step:746/2330 train_time:45696ms step_avg:61.26ms
step:747/2330 train_time:45756ms step_avg:61.25ms
step:748/2330 train_time:45820ms step_avg:61.26ms
step:749/2330 train_time:45880ms step_avg:61.25ms
step:750/2330 train_time:45943ms step_avg:61.26ms
step:750/2330 val_loss:3.7817 train_time:46007ms step_avg:61.34ms
step:751/2330 train_time:46030ms step_avg:61.29ms
step:752/2330 train_time:46067ms step_avg:61.26ms
step:753/2330 train_time:46133ms step_avg:61.27ms
step:754/2330 train_time:46200ms step_avg:61.27ms
step:755/2330 train_time:46260ms step_avg:61.27ms
step:756/2330 train_time:46323ms step_avg:61.27ms
step:757/2330 train_time:46382ms step_avg:61.27ms
step:758/2330 train_time:46444ms step_avg:61.27ms
step:759/2330 train_time:46504ms step_avg:61.27ms
step:760/2330 train_time:46566ms step_avg:61.27ms
step:761/2330 train_time:46625ms step_avg:61.27ms
step:762/2330 train_time:46687ms step_avg:61.27ms
step:763/2330 train_time:46746ms step_avg:61.27ms
step:764/2330 train_time:46808ms step_avg:61.27ms
step:765/2330 train_time:46867ms step_avg:61.26ms
step:766/2330 train_time:46931ms step_avg:61.27ms
step:767/2330 train_time:46993ms step_avg:61.27ms
step:768/2330 train_time:47058ms step_avg:61.27ms
step:769/2330 train_time:47119ms step_avg:61.27ms
step:770/2330 train_time:47184ms step_avg:61.28ms
step:771/2330 train_time:47246ms step_avg:61.28ms
step:772/2330 train_time:47310ms step_avg:61.28ms
step:773/2330 train_time:47371ms step_avg:61.28ms
step:774/2330 train_time:47435ms step_avg:61.29ms
step:775/2330 train_time:47495ms step_avg:61.28ms
step:776/2330 train_time:47559ms step_avg:61.29ms
step:777/2330 train_time:47619ms step_avg:61.29ms
step:778/2330 train_time:47682ms step_avg:61.29ms
step:779/2330 train_time:47741ms step_avg:61.29ms
step:780/2330 train_time:47805ms step_avg:61.29ms
step:781/2330 train_time:47865ms step_avg:61.29ms
step:782/2330 train_time:47928ms step_avg:61.29ms
step:783/2330 train_time:47989ms step_avg:61.29ms
step:784/2330 train_time:48053ms step_avg:61.29ms
step:785/2330 train_time:48115ms step_avg:61.29ms
step:786/2330 train_time:48180ms step_avg:61.30ms
step:787/2330 train_time:48241ms step_avg:61.30ms
step:788/2330 train_time:48305ms step_avg:61.30ms
step:789/2330 train_time:48365ms step_avg:61.30ms
step:790/2330 train_time:48429ms step_avg:61.30ms
step:791/2330 train_time:48490ms step_avg:61.30ms
step:792/2330 train_time:48554ms step_avg:61.31ms
step:793/2330 train_time:48615ms step_avg:61.30ms
step:794/2330 train_time:48678ms step_avg:61.31ms
step:795/2330 train_time:48738ms step_avg:61.31ms
step:796/2330 train_time:48802ms step_avg:61.31ms
step:797/2330 train_time:48863ms step_avg:61.31ms
step:798/2330 train_time:48926ms step_avg:61.31ms
step:799/2330 train_time:48986ms step_avg:61.31ms
step:800/2330 train_time:49050ms step_avg:61.31ms
step:801/2330 train_time:49112ms step_avg:61.31ms
step:802/2330 train_time:49176ms step_avg:61.32ms
step:803/2330 train_time:49237ms step_avg:61.32ms
step:804/2330 train_time:49301ms step_avg:61.32ms
step:805/2330 train_time:49363ms step_avg:61.32ms
step:806/2330 train_time:49426ms step_avg:61.32ms
step:807/2330 train_time:49487ms step_avg:61.32ms
step:808/2330 train_time:49550ms step_avg:61.32ms
step:809/2330 train_time:49610ms step_avg:61.32ms
step:810/2330 train_time:49674ms step_avg:61.33ms
step:811/2330 train_time:49735ms step_avg:61.33ms
step:812/2330 train_time:49798ms step_avg:61.33ms
step:813/2330 train_time:49859ms step_avg:61.33ms
step:814/2330 train_time:49922ms step_avg:61.33ms
step:815/2330 train_time:49983ms step_avg:61.33ms
step:816/2330 train_time:50046ms step_avg:61.33ms
step:817/2330 train_time:50107ms step_avg:61.33ms
step:818/2330 train_time:50171ms step_avg:61.33ms
step:819/2330 train_time:50231ms step_avg:61.33ms
step:820/2330 train_time:50296ms step_avg:61.34ms
step:821/2330 train_time:50356ms step_avg:61.34ms
step:822/2330 train_time:50420ms step_avg:61.34ms
step:823/2330 train_time:50481ms step_avg:61.34ms
step:824/2330 train_time:50544ms step_avg:61.34ms
step:825/2330 train_time:50604ms step_avg:61.34ms
step:826/2330 train_time:50668ms step_avg:61.34ms
step:827/2330 train_time:50728ms step_avg:61.34ms
step:828/2330 train_time:50792ms step_avg:61.34ms
step:829/2330 train_time:50853ms step_avg:61.34ms
step:830/2330 train_time:50917ms step_avg:61.35ms
step:831/2330 train_time:50977ms step_avg:61.34ms
step:832/2330 train_time:51041ms step_avg:61.35ms
step:833/2330 train_time:51101ms step_avg:61.35ms
step:834/2330 train_time:51165ms step_avg:61.35ms
step:835/2330 train_time:51225ms step_avg:61.35ms
step:836/2330 train_time:51288ms step_avg:61.35ms
step:837/2330 train_time:51350ms step_avg:61.35ms
step:838/2330 train_time:51414ms step_avg:61.35ms
step:839/2330 train_time:51474ms step_avg:61.35ms
step:840/2330 train_time:51538ms step_avg:61.35ms
step:841/2330 train_time:51598ms step_avg:61.35ms
step:842/2330 train_time:51662ms step_avg:61.36ms
step:843/2330 train_time:51724ms step_avg:61.36ms
step:844/2330 train_time:51787ms step_avg:61.36ms
step:845/2330 train_time:51847ms step_avg:61.36ms
step:846/2330 train_time:51911ms step_avg:61.36ms
step:847/2330 train_time:51973ms step_avg:61.36ms
step:848/2330 train_time:52036ms step_avg:61.36ms
step:849/2330 train_time:52096ms step_avg:61.36ms
step:850/2330 train_time:52160ms step_avg:61.36ms
step:851/2330 train_time:52220ms step_avg:61.36ms
step:852/2330 train_time:52284ms step_avg:61.37ms
step:853/2330 train_time:52344ms step_avg:61.37ms
step:854/2330 train_time:52408ms step_avg:61.37ms
step:855/2330 train_time:52468ms step_avg:61.37ms
step:856/2330 train_time:52532ms step_avg:61.37ms
step:857/2330 train_time:52594ms step_avg:61.37ms
step:858/2330 train_time:52658ms step_avg:61.37ms
step:859/2330 train_time:52719ms step_avg:61.37ms
step:860/2330 train_time:52783ms step_avg:61.38ms
step:861/2330 train_time:52843ms step_avg:61.37ms
step:862/2330 train_time:52906ms step_avg:61.38ms
step:863/2330 train_time:52966ms step_avg:61.37ms
step:864/2330 train_time:53029ms step_avg:61.38ms
step:865/2330 train_time:53090ms step_avg:61.38ms
step:866/2330 train_time:53155ms step_avg:61.38ms
step:867/2330 train_time:53216ms step_avg:61.38ms
step:868/2330 train_time:53280ms step_avg:61.38ms
step:869/2330 train_time:53340ms step_avg:61.38ms
step:870/2330 train_time:53404ms step_avg:61.38ms
step:871/2330 train_time:53465ms step_avg:61.38ms
step:872/2330 train_time:53528ms step_avg:61.39ms
step:873/2330 train_time:53589ms step_avg:61.38ms
step:874/2330 train_time:53654ms step_avg:61.39ms
step:875/2330 train_time:53716ms step_avg:61.39ms
step:876/2330 train_time:53779ms step_avg:61.39ms
step:877/2330 train_time:53840ms step_avg:61.39ms
step:878/2330 train_time:53904ms step_avg:61.39ms
step:879/2330 train_time:53964ms step_avg:61.39ms
step:880/2330 train_time:54027ms step_avg:61.39ms
step:881/2330 train_time:54088ms step_avg:61.39ms
step:882/2330 train_time:54151ms step_avg:61.40ms
step:883/2330 train_time:54212ms step_avg:61.40ms
step:884/2330 train_time:54276ms step_avg:61.40ms
step:885/2330 train_time:54337ms step_avg:61.40ms
step:886/2330 train_time:54401ms step_avg:61.40ms
step:887/2330 train_time:54462ms step_avg:61.40ms
step:888/2330 train_time:54525ms step_avg:61.40ms
step:889/2330 train_time:54585ms step_avg:61.40ms
step:890/2330 train_time:54650ms step_avg:61.40ms
step:891/2330 train_time:54710ms step_avg:61.40ms
step:892/2330 train_time:54775ms step_avg:61.41ms
step:893/2330 train_time:54835ms step_avg:61.41ms
step:894/2330 train_time:54900ms step_avg:61.41ms
step:895/2330 train_time:54960ms step_avg:61.41ms
step:896/2330 train_time:55024ms step_avg:61.41ms
step:897/2330 train_time:55084ms step_avg:61.41ms
step:898/2330 train_time:55148ms step_avg:61.41ms
step:899/2330 train_time:55208ms step_avg:61.41ms
step:900/2330 train_time:55272ms step_avg:61.41ms
step:901/2330 train_time:55333ms step_avg:61.41ms
step:902/2330 train_time:55398ms step_avg:61.42ms
step:903/2330 train_time:55458ms step_avg:61.42ms
step:904/2330 train_time:55522ms step_avg:61.42ms
step:905/2330 train_time:55582ms step_avg:61.42ms
step:906/2330 train_time:55646ms step_avg:61.42ms
step:907/2330 train_time:55706ms step_avg:61.42ms
step:908/2330 train_time:55770ms step_avg:61.42ms
step:909/2330 train_time:55831ms step_avg:61.42ms
step:910/2330 train_time:55896ms step_avg:61.42ms
step:911/2330 train_time:55957ms step_avg:61.42ms
step:912/2330 train_time:56020ms step_avg:61.43ms
step:913/2330 train_time:56081ms step_avg:61.42ms
step:914/2330 train_time:56144ms step_avg:61.43ms
step:915/2330 train_time:56205ms step_avg:61.43ms
step:916/2330 train_time:56269ms step_avg:61.43ms
step:917/2330 train_time:56329ms step_avg:61.43ms
step:918/2330 train_time:56393ms step_avg:61.43ms
step:919/2330 train_time:56454ms step_avg:61.43ms
step:920/2330 train_time:56518ms step_avg:61.43ms
step:921/2330 train_time:56579ms step_avg:61.43ms
step:922/2330 train_time:56642ms step_avg:61.43ms
step:923/2330 train_time:56703ms step_avg:61.43ms
step:924/2330 train_time:56767ms step_avg:61.44ms
step:925/2330 train_time:56828ms step_avg:61.44ms
step:926/2330 train_time:56892ms step_avg:61.44ms
step:927/2330 train_time:56954ms step_avg:61.44ms
step:928/2330 train_time:57017ms step_avg:61.44ms
step:929/2330 train_time:57078ms step_avg:61.44ms
step:930/2330 train_time:57142ms step_avg:61.44ms
step:931/2330 train_time:57202ms step_avg:61.44ms
step:932/2330 train_time:57266ms step_avg:61.44ms
step:933/2330 train_time:57326ms step_avg:61.44ms
step:934/2330 train_time:57390ms step_avg:61.45ms
step:935/2330 train_time:57452ms step_avg:61.45ms
step:936/2330 train_time:57516ms step_avg:61.45ms
step:937/2330 train_time:57577ms step_avg:61.45ms
step:938/2330 train_time:57640ms step_avg:61.45ms
step:939/2330 train_time:57701ms step_avg:61.45ms
step:940/2330 train_time:57765ms step_avg:61.45ms
step:941/2330 train_time:57825ms step_avg:61.45ms
step:942/2330 train_time:57888ms step_avg:61.45ms
step:943/2330 train_time:57949ms step_avg:61.45ms
step:944/2330 train_time:58013ms step_avg:61.45ms
step:945/2330 train_time:58075ms step_avg:61.45ms
step:946/2330 train_time:58139ms step_avg:61.46ms
step:947/2330 train_time:58200ms step_avg:61.46ms
step:948/2330 train_time:58264ms step_avg:61.46ms
step:949/2330 train_time:58324ms step_avg:61.46ms
step:950/2330 train_time:58387ms step_avg:61.46ms
step:951/2330 train_time:58448ms step_avg:61.46ms
step:952/2330 train_time:58512ms step_avg:61.46ms
step:953/2330 train_time:58574ms step_avg:61.46ms
step:954/2330 train_time:58638ms step_avg:61.46ms
step:955/2330 train_time:58698ms step_avg:61.46ms
step:956/2330 train_time:58763ms step_avg:61.47ms
step:957/2330 train_time:58823ms step_avg:61.47ms
step:958/2330 train_time:58886ms step_avg:61.47ms
step:959/2330 train_time:58947ms step_avg:61.47ms
step:960/2330 train_time:59010ms step_avg:61.47ms
step:961/2330 train_time:59072ms step_avg:61.47ms
step:962/2330 train_time:59135ms step_avg:61.47ms
step:963/2330 train_time:59196ms step_avg:61.47ms
step:964/2330 train_time:59260ms step_avg:61.47ms
step:965/2330 train_time:59321ms step_avg:61.47ms
step:966/2330 train_time:59384ms step_avg:61.47ms
step:967/2330 train_time:59444ms step_avg:61.47ms
step:968/2330 train_time:59508ms step_avg:61.48ms
step:969/2330 train_time:59569ms step_avg:61.47ms
step:970/2330 train_time:59632ms step_avg:61.48ms
step:971/2330 train_time:59693ms step_avg:61.48ms
step:972/2330 train_time:59757ms step_avg:61.48ms
step:973/2330 train_time:59818ms step_avg:61.48ms
step:974/2330 train_time:59882ms step_avg:61.48ms
step:975/2330 train_time:59943ms step_avg:61.48ms
step:976/2330 train_time:60006ms step_avg:61.48ms
step:977/2330 train_time:60067ms step_avg:61.48ms
step:978/2330 train_time:60131ms step_avg:61.48ms
step:979/2330 train_time:60193ms step_avg:61.48ms
step:980/2330 train_time:60257ms step_avg:61.49ms
step:981/2330 train_time:60318ms step_avg:61.49ms
step:982/2330 train_time:60382ms step_avg:61.49ms
step:983/2330 train_time:60443ms step_avg:61.49ms
step:984/2330 train_time:60506ms step_avg:61.49ms
step:985/2330 train_time:60566ms step_avg:61.49ms
step:986/2330 train_time:60629ms step_avg:61.49ms
step:987/2330 train_time:60691ms step_avg:61.49ms
step:988/2330 train_time:60755ms step_avg:61.49ms
step:989/2330 train_time:60815ms step_avg:61.49ms
step:990/2330 train_time:60879ms step_avg:61.49ms
step:991/2330 train_time:60940ms step_avg:61.49ms
step:992/2330 train_time:61004ms step_avg:61.50ms
step:993/2330 train_time:61064ms step_avg:61.49ms
step:994/2330 train_time:61128ms step_avg:61.50ms
step:995/2330 train_time:61189ms step_avg:61.50ms
step:996/2330 train_time:61254ms step_avg:61.50ms
step:997/2330 train_time:61315ms step_avg:61.50ms
step:998/2330 train_time:61379ms step_avg:61.50ms
step:999/2330 train_time:61440ms step_avg:61.50ms
step:1000/2330 train_time:61503ms step_avg:61.50ms
step:1000/2330 val_loss:3.6706 train_time:61569ms step_avg:61.57ms
step:1001/2330 train_time:61593ms step_avg:61.53ms
step:1002/2330 train_time:61630ms step_avg:61.51ms
step:1003/2330 train_time:61697ms step_avg:61.51ms
step:1004/2330 train_time:61765ms step_avg:61.52ms
step:1005/2330 train_time:61825ms step_avg:61.52ms
step:1006/2330 train_time:61889ms step_avg:61.52ms
step:1007/2330 train_time:61949ms step_avg:61.52ms
step:1008/2330 train_time:62013ms step_avg:61.52ms
step:1009/2330 train_time:62072ms step_avg:61.52ms
step:1010/2330 train_time:62135ms step_avg:61.52ms
step:1011/2330 train_time:62194ms step_avg:61.52ms
step:1012/2330 train_time:62257ms step_avg:61.52ms
step:1013/2330 train_time:62317ms step_avg:61.52ms
step:1014/2330 train_time:62379ms step_avg:61.52ms
step:1015/2330 train_time:62438ms step_avg:61.52ms
step:1016/2330 train_time:62502ms step_avg:61.52ms
step:1017/2330 train_time:62564ms step_avg:61.52ms
step:1018/2330 train_time:62629ms step_avg:61.52ms
step:1019/2330 train_time:62691ms step_avg:61.52ms
step:1020/2330 train_time:62755ms step_avg:61.52ms
step:1021/2330 train_time:62816ms step_avg:61.52ms
step:1022/2330 train_time:62879ms step_avg:61.53ms
step:1023/2330 train_time:62940ms step_avg:61.52ms
step:1024/2330 train_time:63002ms step_avg:61.53ms
step:1025/2330 train_time:63062ms step_avg:61.52ms
step:1026/2330 train_time:63125ms step_avg:61.53ms
step:1027/2330 train_time:63185ms step_avg:61.52ms
step:1028/2330 train_time:63249ms step_avg:61.53ms
step:1029/2330 train_time:63309ms step_avg:61.52ms
step:1030/2330 train_time:63372ms step_avg:61.53ms
step:1031/2330 train_time:63432ms step_avg:61.52ms
step:1032/2330 train_time:63496ms step_avg:61.53ms
step:1033/2330 train_time:63557ms step_avg:61.53ms
step:1034/2330 train_time:63620ms step_avg:61.53ms
step:1035/2330 train_time:63681ms step_avg:61.53ms
step:1036/2330 train_time:63744ms step_avg:61.53ms
step:1037/2330 train_time:63805ms step_avg:61.53ms
step:1038/2330 train_time:63868ms step_avg:61.53ms
step:1039/2330 train_time:63928ms step_avg:61.53ms
step:1040/2330 train_time:63992ms step_avg:61.53ms
step:1041/2330 train_time:64053ms step_avg:61.53ms
step:1042/2330 train_time:64116ms step_avg:61.53ms
step:1043/2330 train_time:64176ms step_avg:61.53ms
step:1044/2330 train_time:64239ms step_avg:61.53ms
step:1045/2330 train_time:64299ms step_avg:61.53ms
step:1046/2330 train_time:64363ms step_avg:61.53ms
step:1047/2330 train_time:64424ms step_avg:61.53ms
step:1048/2330 train_time:64486ms step_avg:61.53ms
step:1049/2330 train_time:64547ms step_avg:61.53ms
step:1050/2330 train_time:64612ms step_avg:61.53ms
step:1051/2330 train_time:64673ms step_avg:61.53ms
step:1052/2330 train_time:64736ms step_avg:61.54ms
step:1053/2330 train_time:64796ms step_avg:61.53ms
step:1054/2330 train_time:64860ms step_avg:61.54ms
step:1055/2330 train_time:64919ms step_avg:61.53ms
step:1056/2330 train_time:64982ms step_avg:61.54ms
step:1057/2330 train_time:65043ms step_avg:61.54ms
step:1058/2330 train_time:65107ms step_avg:61.54ms
step:1059/2330 train_time:65168ms step_avg:61.54ms
step:1060/2330 train_time:65231ms step_avg:61.54ms
step:1061/2330 train_time:65292ms step_avg:61.54ms
step:1062/2330 train_time:65356ms step_avg:61.54ms
step:1063/2330 train_time:65416ms step_avg:61.54ms
step:1064/2330 train_time:65479ms step_avg:61.54ms
step:1065/2330 train_time:65540ms step_avg:61.54ms
step:1066/2330 train_time:65603ms step_avg:61.54ms
step:1067/2330 train_time:65664ms step_avg:61.54ms
step:1068/2330 train_time:65728ms step_avg:61.54ms
step:1069/2330 train_time:65789ms step_avg:61.54ms
step:1070/2330 train_time:65853ms step_avg:61.55ms
step:1071/2330 train_time:65913ms step_avg:61.54ms
step:1072/2330 train_time:65977ms step_avg:61.55ms
step:1073/2330 train_time:66037ms step_avg:61.54ms
step:1074/2330 train_time:66100ms step_avg:61.55ms
step:1075/2330 train_time:66161ms step_avg:61.55ms
step:1076/2330 train_time:66225ms step_avg:61.55ms
step:1077/2330 train_time:66285ms step_avg:61.55ms
step:1078/2330 train_time:66348ms step_avg:61.55ms
step:1079/2330 train_time:66409ms step_avg:61.55ms
step:1080/2330 train_time:66472ms step_avg:61.55ms
step:1081/2330 train_time:66533ms step_avg:61.55ms
step:1082/2330 train_time:66596ms step_avg:61.55ms
step:1083/2330 train_time:66656ms step_avg:61.55ms
step:1084/2330 train_time:66720ms step_avg:61.55ms
step:1085/2330 train_time:66780ms step_avg:61.55ms
step:1086/2330 train_time:66844ms step_avg:61.55ms
step:1087/2330 train_time:66905ms step_avg:61.55ms
step:1088/2330 train_time:66969ms step_avg:61.55ms
step:1089/2330 train_time:67029ms step_avg:61.55ms
step:1090/2330 train_time:67093ms step_avg:61.55ms
step:1091/2330 train_time:67153ms step_avg:61.55ms
step:1092/2330 train_time:67217ms step_avg:61.55ms
step:1093/2330 train_time:67277ms step_avg:61.55ms
step:1094/2330 train_time:67341ms step_avg:61.55ms
step:1095/2330 train_time:67401ms step_avg:61.55ms
step:1096/2330 train_time:67465ms step_avg:61.56ms
step:1097/2330 train_time:67525ms step_avg:61.55ms
step:1098/2330 train_time:67589ms step_avg:61.56ms
step:1099/2330 train_time:67649ms step_avg:61.56ms
step:1100/2330 train_time:67714ms step_avg:61.56ms
step:1101/2330 train_time:67774ms step_avg:61.56ms
step:1102/2330 train_time:67838ms step_avg:61.56ms
step:1103/2330 train_time:67898ms step_avg:61.56ms
step:1104/2330 train_time:67962ms step_avg:61.56ms
step:1105/2330 train_time:68023ms step_avg:61.56ms
step:1106/2330 train_time:68085ms step_avg:61.56ms
step:1107/2330 train_time:68147ms step_avg:61.56ms
step:1108/2330 train_time:68212ms step_avg:61.56ms
step:1109/2330 train_time:68273ms step_avg:61.56ms
step:1110/2330 train_time:68336ms step_avg:61.56ms
step:1111/2330 train_time:68396ms step_avg:61.56ms
step:1112/2330 train_time:68460ms step_avg:61.56ms
step:1113/2330 train_time:68521ms step_avg:61.56ms
step:1114/2330 train_time:68585ms step_avg:61.57ms
step:1115/2330 train_time:68646ms step_avg:61.57ms
step:1116/2330 train_time:68710ms step_avg:61.57ms
step:1117/2330 train_time:68770ms step_avg:61.57ms
step:1118/2330 train_time:68834ms step_avg:61.57ms
step:1119/2330 train_time:68894ms step_avg:61.57ms
step:1120/2330 train_time:68958ms step_avg:61.57ms
step:1121/2330 train_time:69019ms step_avg:61.57ms
step:1122/2330 train_time:69082ms step_avg:61.57ms
step:1123/2330 train_time:69142ms step_avg:61.57ms
step:1124/2330 train_time:69206ms step_avg:61.57ms
step:1125/2330 train_time:69266ms step_avg:61.57ms
step:1126/2330 train_time:69330ms step_avg:61.57ms
step:1127/2330 train_time:69391ms step_avg:61.57ms
step:1128/2330 train_time:69455ms step_avg:61.57ms
step:1129/2330 train_time:69515ms step_avg:61.57ms
step:1130/2330 train_time:69579ms step_avg:61.57ms
step:1131/2330 train_time:69640ms step_avg:61.57ms
step:1132/2330 train_time:69704ms step_avg:61.58ms
step:1133/2330 train_time:69764ms step_avg:61.57ms
step:1134/2330 train_time:69828ms step_avg:61.58ms
step:1135/2330 train_time:69890ms step_avg:61.58ms
step:1136/2330 train_time:69954ms step_avg:61.58ms
step:1137/2330 train_time:70014ms step_avg:61.58ms
step:1138/2330 train_time:70078ms step_avg:61.58ms
step:1139/2330 train_time:70138ms step_avg:61.58ms
step:1140/2330 train_time:70201ms step_avg:61.58ms
step:1141/2330 train_time:70262ms step_avg:61.58ms
step:1142/2330 train_time:70325ms step_avg:61.58ms
step:1143/2330 train_time:70386ms step_avg:61.58ms
step:1144/2330 train_time:70450ms step_avg:61.58ms
step:1145/2330 train_time:70511ms step_avg:61.58ms
step:1146/2330 train_time:70574ms step_avg:61.58ms
step:1147/2330 train_time:70634ms step_avg:61.58ms
step:1148/2330 train_time:70698ms step_avg:61.58ms
step:1149/2330 train_time:70759ms step_avg:61.58ms
step:1150/2330 train_time:70823ms step_avg:61.59ms
step:1151/2330 train_time:70884ms step_avg:61.58ms
step:1152/2330 train_time:70947ms step_avg:61.59ms
step:1153/2330 train_time:71007ms step_avg:61.58ms
step:1154/2330 train_time:71071ms step_avg:61.59ms
step:1155/2330 train_time:71131ms step_avg:61.59ms
step:1156/2330 train_time:71195ms step_avg:61.59ms
step:1157/2330 train_time:71256ms step_avg:61.59ms
step:1158/2330 train_time:71319ms step_avg:61.59ms
step:1159/2330 train_time:71380ms step_avg:61.59ms
step:1160/2330 train_time:71443ms step_avg:61.59ms
step:1161/2330 train_time:71503ms step_avg:61.59ms
step:1162/2330 train_time:71566ms step_avg:61.59ms
step:1163/2330 train_time:71627ms step_avg:61.59ms
step:1164/2330 train_time:71690ms step_avg:61.59ms
step:1165/2330 train_time:71751ms step_avg:61.59ms
step:1166/2330 train_time:71815ms step_avg:61.59ms
step:1167/2330 train_time:71876ms step_avg:61.59ms
step:1168/2330 train_time:71939ms step_avg:61.59ms
step:1169/2330 train_time:72000ms step_avg:61.59ms
step:1170/2330 train_time:72064ms step_avg:61.59ms
step:1171/2330 train_time:72124ms step_avg:61.59ms
step:1172/2330 train_time:72188ms step_avg:61.59ms
step:1173/2330 train_time:72249ms step_avg:61.59ms
step:1174/2330 train_time:72314ms step_avg:61.60ms
step:1175/2330 train_time:72374ms step_avg:61.59ms
step:1176/2330 train_time:72437ms step_avg:61.60ms
step:1177/2330 train_time:72498ms step_avg:61.60ms
step:1178/2330 train_time:72562ms step_avg:61.60ms
step:1179/2330 train_time:72622ms step_avg:61.60ms
step:1180/2330 train_time:72685ms step_avg:61.60ms
step:1181/2330 train_time:72745ms step_avg:61.60ms
step:1182/2330 train_time:72809ms step_avg:61.60ms
step:1183/2330 train_time:72870ms step_avg:61.60ms
step:1184/2330 train_time:72934ms step_avg:61.60ms
step:1185/2330 train_time:72995ms step_avg:61.60ms
step:1186/2330 train_time:73059ms step_avg:61.60ms
step:1187/2330 train_time:73120ms step_avg:61.60ms
step:1188/2330 train_time:73183ms step_avg:61.60ms
step:1189/2330 train_time:73244ms step_avg:61.60ms
step:1190/2330 train_time:73307ms step_avg:61.60ms
step:1191/2330 train_time:73368ms step_avg:61.60ms
step:1192/2330 train_time:73432ms step_avg:61.60ms
step:1193/2330 train_time:73493ms step_avg:61.60ms
step:1194/2330 train_time:73556ms step_avg:61.60ms
step:1195/2330 train_time:73616ms step_avg:61.60ms
step:1196/2330 train_time:73680ms step_avg:61.61ms
step:1197/2330 train_time:73741ms step_avg:61.60ms
step:1198/2330 train_time:73804ms step_avg:61.61ms
step:1199/2330 train_time:73864ms step_avg:61.60ms
step:1200/2330 train_time:73928ms step_avg:61.61ms
step:1201/2330 train_time:73989ms step_avg:61.61ms
step:1202/2330 train_time:74054ms step_avg:61.61ms
step:1203/2330 train_time:74114ms step_avg:61.61ms
step:1204/2330 train_time:74177ms step_avg:61.61ms
step:1205/2330 train_time:74237ms step_avg:61.61ms
step:1206/2330 train_time:74300ms step_avg:61.61ms
step:1207/2330 train_time:74362ms step_avg:61.61ms
step:1208/2330 train_time:74425ms step_avg:61.61ms
step:1209/2330 train_time:74485ms step_avg:61.61ms
step:1210/2330 train_time:74549ms step_avg:61.61ms
step:1211/2330 train_time:74611ms step_avg:61.61ms
step:1212/2330 train_time:74674ms step_avg:61.61ms
step:1213/2330 train_time:74734ms step_avg:61.61ms
step:1214/2330 train_time:74798ms step_avg:61.61ms
step:1215/2330 train_time:74858ms step_avg:61.61ms
step:1216/2330 train_time:74921ms step_avg:61.61ms
step:1217/2330 train_time:74981ms step_avg:61.61ms
step:1218/2330 train_time:75044ms step_avg:61.61ms
step:1219/2330 train_time:75104ms step_avg:61.61ms
step:1220/2330 train_time:75168ms step_avg:61.61ms
step:1221/2330 train_time:75228ms step_avg:61.61ms
step:1222/2330 train_time:75293ms step_avg:61.61ms
step:1223/2330 train_time:75353ms step_avg:61.61ms
step:1224/2330 train_time:75416ms step_avg:61.61ms
step:1225/2330 train_time:75476ms step_avg:61.61ms
step:1226/2330 train_time:75540ms step_avg:61.61ms
step:1227/2330 train_time:75601ms step_avg:61.61ms
step:1228/2330 train_time:75664ms step_avg:61.62ms
step:1229/2330 train_time:75725ms step_avg:61.61ms
step:1230/2330 train_time:75788ms step_avg:61.62ms
step:1231/2330 train_time:75850ms step_avg:61.62ms
step:1232/2330 train_time:75914ms step_avg:61.62ms
step:1233/2330 train_time:75974ms step_avg:61.62ms
step:1234/2330 train_time:76037ms step_avg:61.62ms
step:1235/2330 train_time:76097ms step_avg:61.62ms
step:1236/2330 train_time:76160ms step_avg:61.62ms
step:1237/2330 train_time:76220ms step_avg:61.62ms
step:1238/2330 train_time:76284ms step_avg:61.62ms
step:1239/2330 train_time:76345ms step_avg:61.62ms
step:1240/2330 train_time:76409ms step_avg:61.62ms
step:1241/2330 train_time:76470ms step_avg:61.62ms
step:1242/2330 train_time:76533ms step_avg:61.62ms
step:1243/2330 train_time:76594ms step_avg:61.62ms
step:1244/2330 train_time:76657ms step_avg:61.62ms
step:1245/2330 train_time:76718ms step_avg:61.62ms
step:1246/2330 train_time:76782ms step_avg:61.62ms
step:1247/2330 train_time:76842ms step_avg:61.62ms
step:1248/2330 train_time:76906ms step_avg:61.62ms
step:1249/2330 train_time:76966ms step_avg:61.62ms
step:1250/2330 train_time:77030ms step_avg:61.62ms
step:1250/2330 val_loss:3.5931 train_time:77095ms step_avg:61.68ms
step:1251/2330 train_time:77116ms step_avg:61.64ms
step:1252/2330 train_time:77161ms step_avg:61.63ms
step:1253/2330 train_time:77226ms step_avg:61.63ms
step:1254/2330 train_time:77291ms step_avg:61.64ms
step:1255/2330 train_time:77350ms step_avg:61.63ms
step:1256/2330 train_time:77415ms step_avg:61.64ms
step:1257/2330 train_time:77474ms step_avg:61.63ms
step:1258/2330 train_time:77537ms step_avg:61.64ms
step:1259/2330 train_time:77597ms step_avg:61.63ms
step:1260/2330 train_time:77659ms step_avg:61.63ms
step:1261/2330 train_time:77719ms step_avg:61.63ms
step:1262/2330 train_time:77781ms step_avg:61.63ms
step:1263/2330 train_time:77841ms step_avg:61.63ms
step:1264/2330 train_time:77904ms step_avg:61.63ms
step:1265/2330 train_time:77963ms step_avg:61.63ms
step:1266/2330 train_time:78028ms step_avg:61.63ms
step:1267/2330 train_time:78090ms step_avg:61.63ms
step:1268/2330 train_time:78155ms step_avg:61.64ms
step:1269/2330 train_time:78219ms step_avg:61.64ms
step:1270/2330 train_time:78281ms step_avg:61.64ms
step:1271/2330 train_time:78342ms step_avg:61.64ms
step:1272/2330 train_time:78405ms step_avg:61.64ms
step:1273/2330 train_time:78466ms step_avg:61.64ms
step:1274/2330 train_time:78531ms step_avg:61.64ms
step:1275/2330 train_time:78591ms step_avg:61.64ms
step:1276/2330 train_time:78655ms step_avg:61.64ms
step:1277/2330 train_time:78715ms step_avg:61.64ms
step:1278/2330 train_time:78778ms step_avg:61.64ms
step:1279/2330 train_time:78838ms step_avg:61.64ms
step:1280/2330 train_time:78901ms step_avg:61.64ms
step:1281/2330 train_time:78962ms step_avg:61.64ms
step:1282/2330 train_time:79025ms step_avg:61.64ms
step:1283/2330 train_time:79086ms step_avg:61.64ms
step:1284/2330 train_time:79150ms step_avg:61.64ms
step:1285/2330 train_time:79212ms step_avg:61.64ms
step:1286/2330 train_time:79276ms step_avg:61.65ms
step:1287/2330 train_time:79337ms step_avg:61.64ms
step:1288/2330 train_time:79401ms step_avg:61.65ms
step:1289/2330 train_time:79461ms step_avg:61.65ms
step:1290/2330 train_time:79525ms step_avg:61.65ms
step:1291/2330 train_time:79585ms step_avg:61.65ms
step:1292/2330 train_time:79649ms step_avg:61.65ms
step:1293/2330 train_time:79709ms step_avg:61.65ms
step:1294/2330 train_time:79774ms step_avg:61.65ms
step:1295/2330 train_time:79835ms step_avg:61.65ms
step:1296/2330 train_time:79898ms step_avg:61.65ms
step:1297/2330 train_time:79958ms step_avg:61.65ms
step:1298/2330 train_time:80021ms step_avg:61.65ms
step:1299/2330 train_time:80081ms step_avg:61.65ms
step:1300/2330 train_time:80146ms step_avg:61.65ms
step:1301/2330 train_time:80206ms step_avg:61.65ms
step:1302/2330 train_time:80270ms step_avg:61.65ms
step:1303/2330 train_time:80331ms step_avg:61.65ms
step:1304/2330 train_time:80395ms step_avg:61.65ms
step:1305/2330 train_time:80455ms step_avg:61.65ms
step:1306/2330 train_time:80519ms step_avg:61.65ms
step:1307/2330 train_time:80580ms step_avg:61.65ms
step:1308/2330 train_time:80643ms step_avg:61.65ms
step:1309/2330 train_time:80704ms step_avg:61.65ms
step:1310/2330 train_time:80767ms step_avg:61.65ms
step:1311/2330 train_time:80827ms step_avg:61.65ms
step:1312/2330 train_time:80892ms step_avg:61.66ms
step:1313/2330 train_time:80953ms step_avg:61.66ms
step:1314/2330 train_time:81017ms step_avg:61.66ms
step:1315/2330 train_time:81078ms step_avg:61.66ms
step:1316/2330 train_time:81141ms step_avg:61.66ms
step:1317/2330 train_time:81201ms step_avg:61.66ms
step:1318/2330 train_time:81264ms step_avg:61.66ms
step:1319/2330 train_time:81326ms step_avg:61.66ms
step:1320/2330 train_time:81390ms step_avg:61.66ms
step:1321/2330 train_time:81450ms step_avg:61.66ms
step:1322/2330 train_time:81514ms step_avg:61.66ms
step:1323/2330 train_time:81575ms step_avg:61.66ms
step:1324/2330 train_time:81638ms step_avg:61.66ms
step:1325/2330 train_time:81698ms step_avg:61.66ms
step:1326/2330 train_time:81762ms step_avg:61.66ms
step:1327/2330 train_time:81823ms step_avg:61.66ms
step:1328/2330 train_time:81886ms step_avg:61.66ms
step:1329/2330 train_time:81946ms step_avg:61.66ms
step:1330/2330 train_time:82010ms step_avg:61.66ms
step:1331/2330 train_time:82071ms step_avg:61.66ms
step:1332/2330 train_time:82135ms step_avg:61.66ms
step:1333/2330 train_time:82196ms step_avg:61.66ms
step:1334/2330 train_time:82259ms step_avg:61.66ms
step:1335/2330 train_time:82320ms step_avg:61.66ms
step:1336/2330 train_time:82383ms step_avg:61.66ms
step:1337/2330 train_time:82444ms step_avg:61.66ms
step:1338/2330 train_time:82508ms step_avg:61.67ms
step:1339/2330 train_time:82570ms step_avg:61.67ms
step:1340/2330 train_time:82634ms step_avg:61.67ms
step:1341/2330 train_time:82694ms step_avg:61.67ms
step:1342/2330 train_time:82757ms step_avg:61.67ms
step:1343/2330 train_time:82817ms step_avg:61.67ms
step:1344/2330 train_time:82881ms step_avg:61.67ms
step:1345/2330 train_time:82941ms step_avg:61.67ms
step:1346/2330 train_time:83004ms step_avg:61.67ms
step:1347/2330 train_time:83064ms step_avg:61.67ms
step:1348/2330 train_time:83127ms step_avg:61.67ms
step:1349/2330 train_time:83188ms step_avg:61.67ms
step:1350/2330 train_time:83251ms step_avg:61.67ms
step:1351/2330 train_time:83312ms step_avg:61.67ms
step:1352/2330 train_time:83376ms step_avg:61.67ms
step:1353/2330 train_time:83436ms step_avg:61.67ms
step:1354/2330 train_time:83500ms step_avg:61.67ms
step:1355/2330 train_time:83560ms step_avg:61.67ms
step:1356/2330 train_time:83623ms step_avg:61.67ms
step:1357/2330 train_time:83683ms step_avg:61.67ms
step:1358/2330 train_time:83747ms step_avg:61.67ms
step:1359/2330 train_time:83809ms step_avg:61.67ms
step:1360/2330 train_time:83873ms step_avg:61.67ms
step:1361/2330 train_time:83933ms step_avg:61.67ms
step:1362/2330 train_time:83996ms step_avg:61.67ms
step:1363/2330 train_time:84056ms step_avg:61.67ms
step:1364/2330 train_time:84119ms step_avg:61.67ms
step:1365/2330 train_time:84179ms step_avg:61.67ms
step:1366/2330 train_time:84242ms step_avg:61.67ms
step:1367/2330 train_time:84303ms step_avg:61.67ms
step:1368/2330 train_time:84366ms step_avg:61.67ms
step:1369/2330 train_time:84427ms step_avg:61.67ms
step:1370/2330 train_time:84491ms step_avg:61.67ms
step:1371/2330 train_time:84552ms step_avg:61.67ms
step:1372/2330 train_time:84615ms step_avg:61.67ms
step:1373/2330 train_time:84676ms step_avg:61.67ms
step:1374/2330 train_time:84740ms step_avg:61.67ms
step:1375/2330 train_time:84800ms step_avg:61.67ms
step:1376/2330 train_time:84863ms step_avg:61.67ms
step:1377/2330 train_time:84924ms step_avg:61.67ms
step:1378/2330 train_time:84988ms step_avg:61.68ms
step:1379/2330 train_time:85049ms step_avg:61.67ms
step:1380/2330 train_time:85113ms step_avg:61.68ms
step:1381/2330 train_time:85174ms step_avg:61.68ms
step:1382/2330 train_time:85237ms step_avg:61.68ms
step:1383/2330 train_time:85297ms step_avg:61.68ms
step:1384/2330 train_time:85360ms step_avg:61.68ms
step:1385/2330 train_time:85421ms step_avg:61.68ms
step:1386/2330 train_time:85484ms step_avg:61.68ms
step:1387/2330 train_time:85545ms step_avg:61.68ms
step:1388/2330 train_time:85609ms step_avg:61.68ms
step:1389/2330 train_time:85670ms step_avg:61.68ms
step:1390/2330 train_time:85734ms step_avg:61.68ms
step:1391/2330 train_time:85795ms step_avg:61.68ms
step:1392/2330 train_time:85858ms step_avg:61.68ms
step:1393/2330 train_time:85918ms step_avg:61.68ms
step:1394/2330 train_time:85982ms step_avg:61.68ms
step:1395/2330 train_time:86042ms step_avg:61.68ms
step:1396/2330 train_time:86105ms step_avg:61.68ms
step:1397/2330 train_time:86166ms step_avg:61.68ms
step:1398/2330 train_time:86231ms step_avg:61.68ms
step:1399/2330 train_time:86291ms step_avg:61.68ms
step:1400/2330 train_time:86355ms step_avg:61.68ms
step:1401/2330 train_time:86415ms step_avg:61.68ms
step:1402/2330 train_time:86479ms step_avg:61.68ms
step:1403/2330 train_time:86539ms step_avg:61.68ms
step:1404/2330 train_time:86603ms step_avg:61.68ms
step:1405/2330 train_time:86663ms step_avg:61.68ms
step:1406/2330 train_time:86726ms step_avg:61.68ms
step:1407/2330 train_time:86787ms step_avg:61.68ms
step:1408/2330 train_time:86851ms step_avg:61.68ms
step:1409/2330 train_time:86912ms step_avg:61.68ms
step:1410/2330 train_time:86976ms step_avg:61.69ms
step:1411/2330 train_time:87036ms step_avg:61.68ms
step:1412/2330 train_time:87099ms step_avg:61.69ms
step:1413/2330 train_time:87160ms step_avg:61.68ms
step:1414/2330 train_time:87223ms step_avg:61.69ms
step:1415/2330 train_time:87285ms step_avg:61.69ms
step:1416/2330 train_time:87348ms step_avg:61.69ms
step:1417/2330 train_time:87409ms step_avg:61.69ms
step:1418/2330 train_time:87474ms step_avg:61.69ms
step:1419/2330 train_time:87534ms step_avg:61.69ms
step:1420/2330 train_time:87597ms step_avg:61.69ms
step:1421/2330 train_time:87657ms step_avg:61.69ms
step:1422/2330 train_time:87721ms step_avg:61.69ms
step:1423/2330 train_time:87782ms step_avg:61.69ms
step:1424/2330 train_time:87846ms step_avg:61.69ms
step:1425/2330 train_time:87906ms step_avg:61.69ms
step:1426/2330 train_time:87970ms step_avg:61.69ms
step:1427/2330 train_time:88031ms step_avg:61.69ms
step:1428/2330 train_time:88094ms step_avg:61.69ms
step:1429/2330 train_time:88155ms step_avg:61.69ms
step:1430/2330 train_time:88218ms step_avg:61.69ms
step:1431/2330 train_time:88279ms step_avg:61.69ms
step:1432/2330 train_time:88342ms step_avg:61.69ms
step:1433/2330 train_time:88403ms step_avg:61.69ms
step:1434/2330 train_time:88466ms step_avg:61.69ms
step:1435/2330 train_time:88527ms step_avg:61.69ms
step:1436/2330 train_time:88590ms step_avg:61.69ms
step:1437/2330 train_time:88650ms step_avg:61.69ms
step:1438/2330 train_time:88714ms step_avg:61.69ms
step:1439/2330 train_time:88775ms step_avg:61.69ms
step:1440/2330 train_time:88838ms step_avg:61.69ms
step:1441/2330 train_time:88898ms step_avg:61.69ms
step:1442/2330 train_time:88961ms step_avg:61.69ms
step:1443/2330 train_time:89021ms step_avg:61.69ms
step:1444/2330 train_time:89084ms step_avg:61.69ms
step:1445/2330 train_time:89144ms step_avg:61.69ms
step:1446/2330 train_time:89208ms step_avg:61.69ms
step:1447/2330 train_time:89268ms step_avg:61.69ms
step:1448/2330 train_time:89333ms step_avg:61.69ms
step:1449/2330 train_time:89393ms step_avg:61.69ms
step:1450/2330 train_time:89456ms step_avg:61.69ms
step:1451/2330 train_time:89516ms step_avg:61.69ms
step:1452/2330 train_time:89580ms step_avg:61.69ms
step:1453/2330 train_time:89640ms step_avg:61.69ms
step:1454/2330 train_time:89703ms step_avg:61.69ms
step:1455/2330 train_time:89764ms step_avg:61.69ms
step:1456/2330 train_time:89828ms step_avg:61.69ms
step:1457/2330 train_time:89888ms step_avg:61.69ms
step:1458/2330 train_time:89952ms step_avg:61.70ms
step:1459/2330 train_time:90013ms step_avg:61.69ms
step:1460/2330 train_time:90077ms step_avg:61.70ms
step:1461/2330 train_time:90137ms step_avg:61.70ms
step:1462/2330 train_time:90200ms step_avg:61.70ms
step:1463/2330 train_time:90260ms step_avg:61.70ms
step:1464/2330 train_time:90324ms step_avg:61.70ms
step:1465/2330 train_time:90384ms step_avg:61.70ms
step:1466/2330 train_time:90449ms step_avg:61.70ms
step:1467/2330 train_time:90510ms step_avg:61.70ms
step:1468/2330 train_time:90574ms step_avg:61.70ms
step:1469/2330 train_time:90634ms step_avg:61.70ms
step:1470/2330 train_time:90698ms step_avg:61.70ms
step:1471/2330 train_time:90758ms step_avg:61.70ms
step:1472/2330 train_time:90821ms step_avg:61.70ms
step:1473/2330 train_time:90882ms step_avg:61.70ms
step:1474/2330 train_time:90945ms step_avg:61.70ms
step:1475/2330 train_time:91005ms step_avg:61.70ms
step:1476/2330 train_time:91069ms step_avg:61.70ms
step:1477/2330 train_time:91130ms step_avg:61.70ms
step:1478/2330 train_time:91194ms step_avg:61.70ms
step:1479/2330 train_time:91254ms step_avg:61.70ms
step:1480/2330 train_time:91319ms step_avg:61.70ms
step:1481/2330 train_time:91379ms step_avg:61.70ms
step:1482/2330 train_time:91442ms step_avg:61.70ms
step:1483/2330 train_time:91502ms step_avg:61.70ms
step:1484/2330 train_time:91566ms step_avg:61.70ms
step:1485/2330 train_time:91627ms step_avg:61.70ms
step:1486/2330 train_time:91691ms step_avg:61.70ms
step:1487/2330 train_time:91752ms step_avg:61.70ms
step:1488/2330 train_time:91815ms step_avg:61.70ms
step:1489/2330 train_time:91876ms step_avg:61.70ms
step:1490/2330 train_time:91939ms step_avg:61.70ms
step:1491/2330 train_time:91999ms step_avg:61.70ms
step:1492/2330 train_time:92062ms step_avg:61.70ms
step:1493/2330 train_time:92123ms step_avg:61.70ms
step:1494/2330 train_time:92186ms step_avg:61.70ms
step:1495/2330 train_time:92247ms step_avg:61.70ms
step:1496/2330 train_time:92311ms step_avg:61.71ms
step:1497/2330 train_time:92372ms step_avg:61.70ms
step:1498/2330 train_time:92436ms step_avg:61.71ms
step:1499/2330 train_time:92496ms step_avg:61.71ms
step:1500/2330 train_time:92559ms step_avg:61.71ms
step:1500/2330 val_loss:3.5442 train_time:92624ms step_avg:61.75ms
step:1501/2330 train_time:92646ms step_avg:61.72ms
step:1502/2330 train_time:92687ms step_avg:61.71ms
step:1503/2330 train_time:92753ms step_avg:61.71ms
step:1504/2330 train_time:92819ms step_avg:61.71ms
step:1505/2330 train_time:92880ms step_avg:61.71ms
step:1506/2330 train_time:92945ms step_avg:61.72ms
step:1507/2330 train_time:93005ms step_avg:61.72ms
step:1508/2330 train_time:93068ms step_avg:61.72ms
step:1509/2330 train_time:93129ms step_avg:61.72ms
step:1510/2330 train_time:93192ms step_avg:61.72ms
step:1511/2330 train_time:93251ms step_avg:61.71ms
step:1512/2330 train_time:93314ms step_avg:61.72ms
step:1513/2330 train_time:93373ms step_avg:61.71ms
step:1514/2330 train_time:93436ms step_avg:61.71ms
step:1515/2330 train_time:93495ms step_avg:61.71ms
step:1516/2330 train_time:93560ms step_avg:61.72ms
step:1517/2330 train_time:93621ms step_avg:61.71ms
step:1518/2330 train_time:93685ms step_avg:61.72ms
step:1519/2330 train_time:93747ms step_avg:61.72ms
step:1520/2330 train_time:93812ms step_avg:61.72ms
step:1521/2330 train_time:93873ms step_avg:61.72ms
step:1522/2330 train_time:93937ms step_avg:61.72ms
step:1523/2330 train_time:93998ms step_avg:61.72ms
step:1524/2330 train_time:94061ms step_avg:61.72ms
step:1525/2330 train_time:94121ms step_avg:61.72ms
step:1526/2330 train_time:94184ms step_avg:61.72ms
step:1527/2330 train_time:94244ms step_avg:61.72ms
step:1528/2330 train_time:94308ms step_avg:61.72ms
step:1529/2330 train_time:94369ms step_avg:61.72ms
step:1530/2330 train_time:94433ms step_avg:61.72ms
step:1531/2330 train_time:94495ms step_avg:61.72ms
step:1532/2330 train_time:94558ms step_avg:61.72ms
step:1533/2330 train_time:94619ms step_avg:61.72ms
step:1534/2330 train_time:94683ms step_avg:61.72ms
step:1535/2330 train_time:94744ms step_avg:61.72ms
step:1536/2330 train_time:94808ms step_avg:61.72ms
step:1537/2330 train_time:94870ms step_avg:61.72ms
step:1538/2330 train_time:94934ms step_avg:61.73ms
step:1539/2330 train_time:94995ms step_avg:61.73ms
step:1540/2330 train_time:95059ms step_avg:61.73ms
step:1541/2330 train_time:95119ms step_avg:61.73ms
step:1542/2330 train_time:95183ms step_avg:61.73ms
step:1543/2330 train_time:95243ms step_avg:61.73ms
step:1544/2330 train_time:95307ms step_avg:61.73ms
step:1545/2330 train_time:95367ms step_avg:61.73ms
step:1546/2330 train_time:95431ms step_avg:61.73ms
step:1547/2330 train_time:95492ms step_avg:61.73ms
step:1548/2330 train_time:95556ms step_avg:61.73ms
step:1549/2330 train_time:95617ms step_avg:61.73ms
step:1550/2330 train_time:95681ms step_avg:61.73ms
step:1551/2330 train_time:95742ms step_avg:61.73ms
step:1552/2330 train_time:95807ms step_avg:61.73ms
step:1553/2330 train_time:95869ms step_avg:61.73ms
step:1554/2330 train_time:95933ms step_avg:61.73ms
step:1555/2330 train_time:95994ms step_avg:61.73ms
step:1556/2330 train_time:96057ms step_avg:61.73ms
step:1557/2330 train_time:96118ms step_avg:61.73ms
step:1558/2330 train_time:96181ms step_avg:61.73ms
step:1559/2330 train_time:96242ms step_avg:61.73ms
step:1560/2330 train_time:96306ms step_avg:61.73ms
step:1561/2330 train_time:96367ms step_avg:61.73ms
step:1562/2330 train_time:96432ms step_avg:61.74ms
step:1563/2330 train_time:96493ms step_avg:61.74ms
step:1564/2330 train_time:96557ms step_avg:61.74ms
step:1565/2330 train_time:96618ms step_avg:61.74ms
step:1566/2330 train_time:96681ms step_avg:61.74ms
step:1567/2330 train_time:96742ms step_avg:61.74ms
step:1568/2330 train_time:96806ms step_avg:61.74ms
step:1569/2330 train_time:96868ms step_avg:61.74ms
step:1570/2330 train_time:96932ms step_avg:61.74ms
step:1571/2330 train_time:96994ms step_avg:61.74ms
step:1572/2330 train_time:97058ms step_avg:61.74ms
step:1573/2330 train_time:97119ms step_avg:61.74ms
step:1574/2330 train_time:97182ms step_avg:61.74ms
step:1575/2330 train_time:97243ms step_avg:61.74ms
step:1576/2330 train_time:97307ms step_avg:61.74ms
step:1577/2330 train_time:97368ms step_avg:61.74ms
step:1578/2330 train_time:97433ms step_avg:61.74ms
step:1579/2330 train_time:97495ms step_avg:61.74ms
step:1580/2330 train_time:97559ms step_avg:61.75ms
step:1581/2330 train_time:97619ms step_avg:61.75ms
step:1582/2330 train_time:97683ms step_avg:61.75ms
step:1583/2330 train_time:97744ms step_avg:61.75ms
step:1584/2330 train_time:97809ms step_avg:61.75ms
step:1585/2330 train_time:97871ms step_avg:61.75ms
step:1586/2330 train_time:97935ms step_avg:61.75ms
step:1587/2330 train_time:97996ms step_avg:61.75ms
step:1588/2330 train_time:98060ms step_avg:61.75ms
step:1589/2330 train_time:98121ms step_avg:61.75ms
step:1590/2330 train_time:98185ms step_avg:61.75ms
step:1591/2330 train_time:98246ms step_avg:61.75ms
step:1592/2330 train_time:98310ms step_avg:61.75ms
step:1593/2330 train_time:98371ms step_avg:61.75ms
step:1594/2330 train_time:98435ms step_avg:61.75ms
step:1595/2330 train_time:98497ms step_avg:61.75ms
step:1596/2330 train_time:98560ms step_avg:61.75ms
step:1597/2330 train_time:98621ms step_avg:61.75ms
step:1598/2330 train_time:98684ms step_avg:61.75ms
step:1599/2330 train_time:98746ms step_avg:61.75ms
step:1600/2330 train_time:98810ms step_avg:61.76ms
step:1601/2330 train_time:98872ms step_avg:61.76ms
step:1602/2330 train_time:98936ms step_avg:61.76ms
step:1603/2330 train_time:98997ms step_avg:61.76ms
step:1604/2330 train_time:99062ms step_avg:61.76ms
step:1605/2330 train_time:99122ms step_avg:61.76ms
step:1606/2330 train_time:99186ms step_avg:61.76ms
step:1607/2330 train_time:99246ms step_avg:61.76ms
step:1608/2330 train_time:99310ms step_avg:61.76ms
step:1609/2330 train_time:99371ms step_avg:61.76ms
step:1610/2330 train_time:99435ms step_avg:61.76ms
step:1611/2330 train_time:99496ms step_avg:61.76ms
step:1612/2330 train_time:99560ms step_avg:61.76ms
step:1613/2330 train_time:99620ms step_avg:61.76ms
step:1614/2330 train_time:99684ms step_avg:61.76ms
step:1615/2330 train_time:99746ms step_avg:61.76ms
step:1616/2330 train_time:99810ms step_avg:61.76ms
step:1617/2330 train_time:99873ms step_avg:61.76ms
step:1618/2330 train_time:99937ms step_avg:61.77ms
step:1619/2330 train_time:99999ms step_avg:61.77ms
step:1620/2330 train_time:100063ms step_avg:61.77ms
step:1621/2330 train_time:100123ms step_avg:61.77ms
step:1622/2330 train_time:100186ms step_avg:61.77ms
step:1623/2330 train_time:100247ms step_avg:61.77ms
step:1624/2330 train_time:100311ms step_avg:61.77ms
step:1625/2330 train_time:100372ms step_avg:61.77ms
step:1626/2330 train_time:100437ms step_avg:61.77ms
step:1627/2330 train_time:100499ms step_avg:61.77ms
step:1628/2330 train_time:100562ms step_avg:61.77ms
step:1629/2330 train_time:100623ms step_avg:61.77ms
step:1630/2330 train_time:100688ms step_avg:61.77ms
step:1631/2330 train_time:100750ms step_avg:61.77ms
step:1632/2330 train_time:100813ms step_avg:61.77ms
step:1633/2330 train_time:100874ms step_avg:61.77ms
step:1634/2330 train_time:100938ms step_avg:61.77ms
step:1635/2330 train_time:100999ms step_avg:61.77ms
step:1636/2330 train_time:101062ms step_avg:61.77ms
step:1637/2330 train_time:101123ms step_avg:61.77ms
step:1638/2330 train_time:101188ms step_avg:61.78ms
step:1639/2330 train_time:101250ms step_avg:61.78ms
step:1640/2330 train_time:101313ms step_avg:61.78ms
step:1641/2330 train_time:101374ms step_avg:61.78ms
step:1642/2330 train_time:101438ms step_avg:61.78ms
step:1643/2330 train_time:101498ms step_avg:61.78ms
step:1644/2330 train_time:101563ms step_avg:61.78ms
step:1645/2330 train_time:101624ms step_avg:61.78ms
step:1646/2330 train_time:101688ms step_avg:61.78ms
step:1647/2330 train_time:101751ms step_avg:61.78ms
step:1648/2330 train_time:101815ms step_avg:61.78ms
step:1649/2330 train_time:101875ms step_avg:61.78ms
step:1650/2330 train_time:101939ms step_avg:61.78ms
step:1651/2330 train_time:102000ms step_avg:61.78ms
step:1652/2330 train_time:102063ms step_avg:61.78ms
step:1653/2330 train_time:102124ms step_avg:61.78ms
step:1654/2330 train_time:102189ms step_avg:61.78ms
step:1655/2330 train_time:102249ms step_avg:61.78ms
step:1656/2330 train_time:102313ms step_avg:61.78ms
step:1657/2330 train_time:102374ms step_avg:61.78ms
step:1658/2330 train_time:102438ms step_avg:61.78ms
step:1659/2330 train_time:102498ms step_avg:61.78ms
step:1660/2330 train_time:102563ms step_avg:61.79ms
step:1661/2330 train_time:102624ms step_avg:61.78ms
step:1662/2330 train_time:102689ms step_avg:61.79ms
step:1663/2330 train_time:102750ms step_avg:61.79ms
step:1664/2330 train_time:102814ms step_avg:61.79ms
step:1665/2330 train_time:102875ms step_avg:61.79ms
step:1666/2330 train_time:102939ms step_avg:61.79ms
step:1667/2330 train_time:102999ms step_avg:61.79ms
step:1668/2330 train_time:103063ms step_avg:61.79ms
step:1669/2330 train_time:103124ms step_avg:61.79ms
step:1670/2330 train_time:103189ms step_avg:61.79ms
step:1671/2330 train_time:103250ms step_avg:61.79ms
step:1672/2330 train_time:103314ms step_avg:61.79ms
step:1673/2330 train_time:103375ms step_avg:61.79ms
step:1674/2330 train_time:103439ms step_avg:61.79ms
step:1675/2330 train_time:103499ms step_avg:61.79ms
step:1676/2330 train_time:103563ms step_avg:61.79ms
step:1677/2330 train_time:103624ms step_avg:61.79ms
step:1678/2330 train_time:103688ms step_avg:61.79ms
step:1679/2330 train_time:103750ms step_avg:61.79ms
step:1680/2330 train_time:103814ms step_avg:61.79ms
step:1681/2330 train_time:103875ms step_avg:61.79ms
step:1682/2330 train_time:103939ms step_avg:61.79ms
step:1683/2330 train_time:103999ms step_avg:61.79ms
step:1684/2330 train_time:104064ms step_avg:61.80ms
step:1685/2330 train_time:104126ms step_avg:61.80ms
step:1686/2330 train_time:104191ms step_avg:61.80ms
step:1687/2330 train_time:104251ms step_avg:61.80ms
step:1688/2330 train_time:104315ms step_avg:61.80ms
step:1689/2330 train_time:104376ms step_avg:61.80ms
step:1690/2330 train_time:104439ms step_avg:61.80ms
step:1691/2330 train_time:104500ms step_avg:61.80ms
step:1692/2330 train_time:104564ms step_avg:61.80ms
step:1693/2330 train_time:104626ms step_avg:61.80ms
step:1694/2330 train_time:104690ms step_avg:61.80ms
step:1695/2330 train_time:104751ms step_avg:61.80ms
step:1696/2330 train_time:104815ms step_avg:61.80ms
step:1697/2330 train_time:104875ms step_avg:61.80ms
step:1698/2330 train_time:104939ms step_avg:61.80ms
step:1699/2330 train_time:105001ms step_avg:61.80ms
step:1700/2330 train_time:105065ms step_avg:61.80ms
step:1701/2330 train_time:105126ms step_avg:61.80ms
step:1702/2330 train_time:105190ms step_avg:61.80ms
step:1703/2330 train_time:105251ms step_avg:61.80ms
step:1704/2330 train_time:105315ms step_avg:61.80ms
step:1705/2330 train_time:105376ms step_avg:61.80ms
step:1706/2330 train_time:105439ms step_avg:61.81ms
step:1707/2330 train_time:105501ms step_avg:61.80ms
step:1708/2330 train_time:105565ms step_avg:61.81ms
step:1709/2330 train_time:105626ms step_avg:61.81ms
step:1710/2330 train_time:105691ms step_avg:61.81ms
step:1711/2330 train_time:105752ms step_avg:61.81ms
step:1712/2330 train_time:105816ms step_avg:61.81ms
step:1713/2330 train_time:105876ms step_avg:61.81ms
step:1714/2330 train_time:105940ms step_avg:61.81ms
step:1715/2330 train_time:106001ms step_avg:61.81ms
step:1716/2330 train_time:106066ms step_avg:61.81ms
step:1717/2330 train_time:106127ms step_avg:61.81ms
step:1718/2330 train_time:106191ms step_avg:61.81ms
step:1719/2330 train_time:106253ms step_avg:61.81ms
step:1720/2330 train_time:106317ms step_avg:61.81ms
step:1721/2330 train_time:106378ms step_avg:61.81ms
step:1722/2330 train_time:106442ms step_avg:61.81ms
step:1723/2330 train_time:106503ms step_avg:61.81ms
step:1724/2330 train_time:106567ms step_avg:61.81ms
step:1725/2330 train_time:106628ms step_avg:61.81ms
step:1726/2330 train_time:106693ms step_avg:61.82ms
step:1727/2330 train_time:106754ms step_avg:61.81ms
step:1728/2330 train_time:106818ms step_avg:61.82ms
step:1729/2330 train_time:106879ms step_avg:61.82ms
step:1730/2330 train_time:106942ms step_avg:61.82ms
step:1731/2330 train_time:107003ms step_avg:61.82ms
step:1732/2330 train_time:107067ms step_avg:61.82ms
step:1733/2330 train_time:107129ms step_avg:61.82ms
step:1734/2330 train_time:107193ms step_avg:61.82ms
step:1735/2330 train_time:107254ms step_avg:61.82ms
step:1736/2330 train_time:107319ms step_avg:61.82ms
step:1737/2330 train_time:107380ms step_avg:61.82ms
step:1738/2330 train_time:107443ms step_avg:61.82ms
step:1739/2330 train_time:107504ms step_avg:61.82ms
step:1740/2330 train_time:107568ms step_avg:61.82ms
step:1741/2330 train_time:107629ms step_avg:61.82ms
step:1742/2330 train_time:107694ms step_avg:61.82ms
step:1743/2330 train_time:107755ms step_avg:61.82ms
step:1744/2330 train_time:107819ms step_avg:61.82ms
step:1745/2330 train_time:107880ms step_avg:61.82ms
step:1746/2330 train_time:107943ms step_avg:61.82ms
step:1747/2330 train_time:108004ms step_avg:61.82ms
step:1748/2330 train_time:108068ms step_avg:61.82ms
step:1749/2330 train_time:108129ms step_avg:61.82ms
step:1750/2330 train_time:108193ms step_avg:61.82ms
step:1750/2330 val_loss:3.5206 train_time:108259ms step_avg:61.86ms
step:1751/2330 train_time:108281ms step_avg:61.84ms
step:1752/2330 train_time:108321ms step_avg:61.83ms
step:1753/2330 train_time:108388ms step_avg:61.83ms
step:1754/2330 train_time:108456ms step_avg:61.83ms
step:1755/2330 train_time:108516ms step_avg:61.83ms
step:1756/2330 train_time:108580ms step_avg:61.83ms
step:1757/2330 train_time:108640ms step_avg:61.83ms
step:1758/2330 train_time:108704ms step_avg:61.83ms
step:1759/2330 train_time:108763ms step_avg:61.83ms
step:1760/2330 train_time:108826ms step_avg:61.83ms
step:1761/2330 train_time:108886ms step_avg:61.83ms
step:1762/2330 train_time:108950ms step_avg:61.83ms
step:1763/2330 train_time:109010ms step_avg:61.83ms
step:1764/2330 train_time:109072ms step_avg:61.83ms
step:1765/2330 train_time:109132ms step_avg:61.83ms
step:1766/2330 train_time:109197ms step_avg:61.83ms
step:1767/2330 train_time:109258ms step_avg:61.83ms
step:1768/2330 train_time:109323ms step_avg:61.83ms
step:1769/2330 train_time:109387ms step_avg:61.84ms
step:1770/2330 train_time:109452ms step_avg:61.84ms
step:1771/2330 train_time:109513ms step_avg:61.84ms
step:1772/2330 train_time:109577ms step_avg:61.84ms
step:1773/2330 train_time:109638ms step_avg:61.84ms
step:1774/2330 train_time:109701ms step_avg:61.84ms
step:1775/2330 train_time:109762ms step_avg:61.84ms
step:1776/2330 train_time:109825ms step_avg:61.84ms
step:1777/2330 train_time:109885ms step_avg:61.84ms
step:1778/2330 train_time:109949ms step_avg:61.84ms
step:1779/2330 train_time:110009ms step_avg:61.84ms
step:1780/2330 train_time:110072ms step_avg:61.84ms
step:1781/2330 train_time:110133ms step_avg:61.84ms
step:1782/2330 train_time:110197ms step_avg:61.84ms
step:1783/2330 train_time:110258ms step_avg:61.84ms
step:1784/2330 train_time:110323ms step_avg:61.84ms
step:1785/2330 train_time:110385ms step_avg:61.84ms
step:1786/2330 train_time:110450ms step_avg:61.84ms
step:1787/2330 train_time:110512ms step_avg:61.84ms
step:1788/2330 train_time:110577ms step_avg:61.84ms
step:1789/2330 train_time:110637ms step_avg:61.84ms
step:1790/2330 train_time:110701ms step_avg:61.84ms
step:1791/2330 train_time:110762ms step_avg:61.84ms
step:1792/2330 train_time:110825ms step_avg:61.84ms
step:1793/2330 train_time:110885ms step_avg:61.84ms
step:1794/2330 train_time:110949ms step_avg:61.84ms
step:1795/2330 train_time:111009ms step_avg:61.84ms
step:1796/2330 train_time:111072ms step_avg:61.84ms
step:1797/2330 train_time:111133ms step_avg:61.84ms
step:1798/2330 train_time:111197ms step_avg:61.84ms
step:1799/2330 train_time:111258ms step_avg:61.84ms
step:1800/2330 train_time:111322ms step_avg:61.85ms
step:1801/2330 train_time:111383ms step_avg:61.85ms
step:1802/2330 train_time:111448ms step_avg:61.85ms
step:1803/2330 train_time:111510ms step_avg:61.85ms
step:1804/2330 train_time:111574ms step_avg:61.85ms
step:1805/2330 train_time:111635ms step_avg:61.85ms
step:1806/2330 train_time:111699ms step_avg:61.85ms
step:1807/2330 train_time:111760ms step_avg:61.85ms
step:1808/2330 train_time:111823ms step_avg:61.85ms
step:1809/2330 train_time:111885ms step_avg:61.85ms
step:1810/2330 train_time:111948ms step_avg:61.85ms
step:1811/2330 train_time:112009ms step_avg:61.85ms
step:1812/2330 train_time:112073ms step_avg:61.85ms
step:1813/2330 train_time:112133ms step_avg:61.85ms
step:1814/2330 train_time:112197ms step_avg:61.85ms
step:1815/2330 train_time:112257ms step_avg:61.85ms
step:1816/2330 train_time:112321ms step_avg:61.85ms
step:1817/2330 train_time:112383ms step_avg:61.85ms
step:1818/2330 train_time:112446ms step_avg:61.85ms
step:1819/2330 train_time:112508ms step_avg:61.85ms
step:1820/2330 train_time:112572ms step_avg:61.85ms
step:1821/2330 train_time:112634ms step_avg:61.85ms
step:1822/2330 train_time:112698ms step_avg:61.85ms
step:1823/2330 train_time:112759ms step_avg:61.85ms
step:1824/2330 train_time:112823ms step_avg:61.85ms
step:1825/2330 train_time:112883ms step_avg:61.85ms
step:1826/2330 train_time:112947ms step_avg:61.86ms
step:1827/2330 train_time:113008ms step_avg:61.85ms
step:1828/2330 train_time:113072ms step_avg:61.86ms
step:1829/2330 train_time:113132ms step_avg:61.85ms
step:1830/2330 train_time:113196ms step_avg:61.86ms
step:1831/2330 train_time:113257ms step_avg:61.86ms
step:1832/2330 train_time:113321ms step_avg:61.86ms
step:1833/2330 train_time:113382ms step_avg:61.86ms
step:1834/2330 train_time:113446ms step_avg:61.86ms
step:1835/2330 train_time:113508ms step_avg:61.86ms
step:1836/2330 train_time:113572ms step_avg:61.86ms
step:1837/2330 train_time:113634ms step_avg:61.86ms
step:1838/2330 train_time:113697ms step_avg:61.86ms
step:1839/2330 train_time:113758ms step_avg:61.86ms
step:1840/2330 train_time:113822ms step_avg:61.86ms
step:1841/2330 train_time:113883ms step_avg:61.86ms
step:1842/2330 train_time:113947ms step_avg:61.86ms
step:1843/2330 train_time:114007ms step_avg:61.86ms
step:1844/2330 train_time:114070ms step_avg:61.86ms
step:1845/2330 train_time:114131ms step_avg:61.86ms
step:1846/2330 train_time:114196ms step_avg:61.86ms
step:1847/2330 train_time:114257ms step_avg:61.86ms
step:1848/2330 train_time:114321ms step_avg:61.86ms
step:1849/2330 train_time:114382ms step_avg:61.86ms
step:1850/2330 train_time:114446ms step_avg:61.86ms
step:1851/2330 train_time:114507ms step_avg:61.86ms
step:1852/2330 train_time:114571ms step_avg:61.86ms
step:1853/2330 train_time:114633ms step_avg:61.86ms
step:1854/2330 train_time:114697ms step_avg:61.86ms
step:1855/2330 train_time:114757ms step_avg:61.86ms
step:1856/2330 train_time:114821ms step_avg:61.86ms
step:1857/2330 train_time:114882ms step_avg:61.86ms
step:1858/2330 train_time:114946ms step_avg:61.87ms
step:1859/2330 train_time:115007ms step_avg:61.86ms
step:1860/2330 train_time:115071ms step_avg:61.87ms
step:1861/2330 train_time:115133ms step_avg:61.87ms
step:1862/2330 train_time:115197ms step_avg:61.87ms
step:1863/2330 train_time:115258ms step_avg:61.87ms
step:1864/2330 train_time:115321ms step_avg:61.87ms
step:1865/2330 train_time:115382ms step_avg:61.87ms
step:1866/2330 train_time:115446ms step_avg:61.87ms
step:1867/2330 train_time:115508ms step_avg:61.87ms
step:1868/2330 train_time:115572ms step_avg:61.87ms
step:1869/2330 train_time:115633ms step_avg:61.87ms
step:1870/2330 train_time:115697ms step_avg:61.87ms
step:1871/2330 train_time:115758ms step_avg:61.87ms
step:1872/2330 train_time:115822ms step_avg:61.87ms
step:1873/2330 train_time:115882ms step_avg:61.87ms
step:1874/2330 train_time:115946ms step_avg:61.87ms
step:1875/2330 train_time:116006ms step_avg:61.87ms
step:1876/2330 train_time:116070ms step_avg:61.87ms
step:1877/2330 train_time:116131ms step_avg:61.87ms
step:1878/2330 train_time:116196ms step_avg:61.87ms
step:1879/2330 train_time:116256ms step_avg:61.87ms
step:1880/2330 train_time:116320ms step_avg:61.87ms
step:1881/2330 train_time:116381ms step_avg:61.87ms
step:1882/2330 train_time:116445ms step_avg:61.87ms
step:1883/2330 train_time:116505ms step_avg:61.87ms
step:1884/2330 train_time:116570ms step_avg:61.87ms
step:1885/2330 train_time:116631ms step_avg:61.87ms
step:1886/2330 train_time:116696ms step_avg:61.87ms
step:1887/2330 train_time:116756ms step_avg:61.87ms
step:1888/2330 train_time:116819ms step_avg:61.87ms
step:1889/2330 train_time:116880ms step_avg:61.87ms
step:1890/2330 train_time:116944ms step_avg:61.87ms
step:1891/2330 train_time:117005ms step_avg:61.87ms
step:1892/2330 train_time:117069ms step_avg:61.88ms
step:1893/2330 train_time:117131ms step_avg:61.88ms
step:1894/2330 train_time:117195ms step_avg:61.88ms
step:1895/2330 train_time:117256ms step_avg:61.88ms
step:1896/2330 train_time:117319ms step_avg:61.88ms
step:1897/2330 train_time:117380ms step_avg:61.88ms
step:1898/2330 train_time:117443ms step_avg:61.88ms
step:1899/2330 train_time:117504ms step_avg:61.88ms
step:1900/2330 train_time:117568ms step_avg:61.88ms
step:1901/2330 train_time:117630ms step_avg:61.88ms
step:1902/2330 train_time:117694ms step_avg:61.88ms
step:1903/2330 train_time:117755ms step_avg:61.88ms
step:1904/2330 train_time:117819ms step_avg:61.88ms
step:1905/2330 train_time:117880ms step_avg:61.88ms
step:1906/2330 train_time:117943ms step_avg:61.88ms
step:1907/2330 train_time:118004ms step_avg:61.88ms
step:1908/2330 train_time:118068ms step_avg:61.88ms
step:1909/2330 train_time:118129ms step_avg:61.88ms
step:1910/2330 train_time:118193ms step_avg:61.88ms
step:1911/2330 train_time:118254ms step_avg:61.88ms
step:1912/2330 train_time:118318ms step_avg:61.88ms
step:1913/2330 train_time:118378ms step_avg:61.88ms
step:1914/2330 train_time:118442ms step_avg:61.88ms
step:1915/2330 train_time:118503ms step_avg:61.88ms
step:1916/2330 train_time:118567ms step_avg:61.88ms
step:1917/2330 train_time:118627ms step_avg:61.88ms
step:1918/2330 train_time:118692ms step_avg:61.88ms
step:1919/2330 train_time:118753ms step_avg:61.88ms
step:1920/2330 train_time:118817ms step_avg:61.88ms
step:1921/2330 train_time:118878ms step_avg:61.88ms
step:1922/2330 train_time:118943ms step_avg:61.89ms
step:1923/2330 train_time:119003ms step_avg:61.88ms
step:1924/2330 train_time:119067ms step_avg:61.89ms
step:1925/2330 train_time:119128ms step_avg:61.88ms
step:1926/2330 train_time:119192ms step_avg:61.89ms
step:1927/2330 train_time:119254ms step_avg:61.89ms
step:1928/2330 train_time:119317ms step_avg:61.89ms
step:1929/2330 train_time:119378ms step_avg:61.89ms
step:1930/2330 train_time:119442ms step_avg:61.89ms
step:1931/2330 train_time:119503ms step_avg:61.89ms
step:1932/2330 train_time:119568ms step_avg:61.89ms
step:1933/2330 train_time:119629ms step_avg:61.89ms
step:1934/2330 train_time:119694ms step_avg:61.89ms
step:1935/2330 train_time:119754ms step_avg:61.89ms
step:1936/2330 train_time:119818ms step_avg:61.89ms
step:1937/2330 train_time:119879ms step_avg:61.89ms
step:1938/2330 train_time:119942ms step_avg:61.89ms
step:1939/2330 train_time:120003ms step_avg:61.89ms
step:1940/2330 train_time:120067ms step_avg:61.89ms
step:1941/2330 train_time:120128ms step_avg:61.89ms
step:1942/2330 train_time:120193ms step_avg:61.89ms
step:1943/2330 train_time:120253ms step_avg:61.89ms
step:1944/2330 train_time:120317ms step_avg:61.89ms
step:1945/2330 train_time:120377ms step_avg:61.89ms
step:1946/2330 train_time:120441ms step_avg:61.89ms
step:1947/2330 train_time:120503ms step_avg:61.89ms
step:1948/2330 train_time:120567ms step_avg:61.89ms
step:1949/2330 train_time:120628ms step_avg:61.89ms
step:1950/2330 train_time:120692ms step_avg:61.89ms
step:1951/2330 train_time:120753ms step_avg:61.89ms
step:1952/2330 train_time:120817ms step_avg:61.89ms
step:1953/2330 train_time:120878ms step_avg:61.89ms
step:1954/2330 train_time:120941ms step_avg:61.89ms
step:1955/2330 train_time:121002ms step_avg:61.89ms
step:1956/2330 train_time:121066ms step_avg:61.89ms
step:1957/2330 train_time:121127ms step_avg:61.89ms
step:1958/2330 train_time:121192ms step_avg:61.90ms
step:1959/2330 train_time:121253ms step_avg:61.90ms
step:1960/2330 train_time:121317ms step_avg:61.90ms
step:1961/2330 train_time:121378ms step_avg:61.90ms
step:1962/2330 train_time:121441ms step_avg:61.90ms
step:1963/2330 train_time:121503ms step_avg:61.90ms
step:1964/2330 train_time:121567ms step_avg:61.90ms
step:1965/2330 train_time:121627ms step_avg:61.90ms
step:1966/2330 train_time:121692ms step_avg:61.90ms
step:1967/2330 train_time:121753ms step_avg:61.90ms
step:1968/2330 train_time:121817ms step_avg:61.90ms
step:1969/2330 train_time:121878ms step_avg:61.90ms
step:1970/2330 train_time:121942ms step_avg:61.90ms
step:1971/2330 train_time:122003ms step_avg:61.90ms
step:1972/2330 train_time:122066ms step_avg:61.90ms
step:1973/2330 train_time:122127ms step_avg:61.90ms
step:1974/2330 train_time:122192ms step_avg:61.90ms
step:1975/2330 train_time:122253ms step_avg:61.90ms
step:1976/2330 train_time:122317ms step_avg:61.90ms
step:1977/2330 train_time:122378ms step_avg:61.90ms
step:1978/2330 train_time:122442ms step_avg:61.90ms
step:1979/2330 train_time:122503ms step_avg:61.90ms
step:1980/2330 train_time:122567ms step_avg:61.90ms
step:1981/2330 train_time:122629ms step_avg:61.90ms
step:1982/2330 train_time:122693ms step_avg:61.90ms
step:1983/2330 train_time:122754ms step_avg:61.90ms
step:1984/2330 train_time:122818ms step_avg:61.90ms
step:1985/2330 train_time:122878ms step_avg:61.90ms
step:1986/2330 train_time:122941ms step_avg:61.90ms
step:1987/2330 train_time:123002ms step_avg:61.90ms
step:1988/2330 train_time:123066ms step_avg:61.90ms
step:1989/2330 train_time:123127ms step_avg:61.90ms
step:1990/2330 train_time:123192ms step_avg:61.91ms
step:1991/2330 train_time:123253ms step_avg:61.91ms
step:1992/2330 train_time:123317ms step_avg:61.91ms
step:1993/2330 train_time:123378ms step_avg:61.91ms
step:1994/2330 train_time:123442ms step_avg:61.91ms
step:1995/2330 train_time:123503ms step_avg:61.91ms
step:1996/2330 train_time:123567ms step_avg:61.91ms
step:1997/2330 train_time:123628ms step_avg:61.91ms
step:1998/2330 train_time:123693ms step_avg:61.91ms
step:1999/2330 train_time:123753ms step_avg:61.91ms
step:2000/2330 train_time:123817ms step_avg:61.91ms
step:2000/2330 val_loss:3.4709 train_time:123882ms step_avg:61.94ms
step:2001/2330 train_time:123905ms step_avg:61.92ms
step:2002/2330 train_time:123944ms step_avg:61.91ms
step:2003/2330 train_time:124012ms step_avg:61.91ms
step:2004/2330 train_time:124081ms step_avg:61.92ms
step:2005/2330 train_time:124143ms step_avg:61.92ms
step:2006/2330 train_time:124208ms step_avg:61.92ms
step:2007/2330 train_time:124268ms step_avg:61.92ms
step:2008/2330 train_time:124331ms step_avg:61.92ms
step:2009/2330 train_time:124391ms step_avg:61.92ms
step:2010/2330 train_time:124453ms step_avg:61.92ms
step:2011/2330 train_time:124514ms step_avg:61.92ms
step:2012/2330 train_time:124576ms step_avg:61.92ms
step:2013/2330 train_time:124636ms step_avg:61.92ms
step:2014/2330 train_time:124699ms step_avg:61.92ms
step:2015/2330 train_time:124759ms step_avg:61.91ms
step:2016/2330 train_time:124822ms step_avg:61.92ms
step:2017/2330 train_time:124883ms step_avg:61.91ms
step:2018/2330 train_time:124948ms step_avg:61.92ms
step:2019/2330 train_time:125011ms step_avg:61.92ms
step:2020/2330 train_time:125076ms step_avg:61.92ms
step:2021/2330 train_time:125138ms step_avg:61.92ms
step:2022/2330 train_time:125203ms step_avg:61.92ms
step:2023/2330 train_time:125264ms step_avg:61.92ms
step:2024/2330 train_time:125328ms step_avg:61.92ms
step:2025/2330 train_time:125388ms step_avg:61.92ms
step:2026/2330 train_time:125452ms step_avg:61.92ms
step:2027/2330 train_time:125512ms step_avg:61.92ms
step:2028/2330 train_time:125575ms step_avg:61.92ms
step:2029/2330 train_time:125635ms step_avg:61.92ms
step:2030/2330 train_time:125698ms step_avg:61.92ms
step:2031/2330 train_time:125758ms step_avg:61.92ms
step:2032/2330 train_time:125822ms step_avg:61.92ms
step:2033/2330 train_time:125882ms step_avg:61.92ms
step:2034/2330 train_time:125947ms step_avg:61.92ms
step:2035/2330 train_time:126009ms step_avg:61.92ms
step:2036/2330 train_time:126073ms step_avg:61.92ms
step:2037/2330 train_time:126135ms step_avg:61.92ms
step:2038/2330 train_time:126198ms step_avg:61.92ms
step:2039/2330 train_time:126260ms step_avg:61.92ms
step:2040/2330 train_time:126324ms step_avg:61.92ms
step:2041/2330 train_time:126386ms step_avg:61.92ms
step:2042/2330 train_time:126450ms step_avg:61.92ms
step:2043/2330 train_time:126510ms step_avg:61.92ms
step:2044/2330 train_time:126574ms step_avg:61.92ms
step:2045/2330 train_time:126634ms step_avg:61.92ms
step:2046/2330 train_time:126697ms step_avg:61.92ms
step:2047/2330 train_time:126757ms step_avg:61.92ms
step:2048/2330 train_time:126820ms step_avg:61.92ms
step:2049/2330 train_time:126881ms step_avg:61.92ms
step:2050/2330 train_time:126945ms step_avg:61.92ms
step:2051/2330 train_time:127007ms step_avg:61.92ms
step:2052/2330 train_time:127072ms step_avg:61.93ms
step:2053/2330 train_time:127133ms step_avg:61.93ms
step:2054/2330 train_time:127198ms step_avg:61.93ms
step:2055/2330 train_time:127259ms step_avg:61.93ms
step:2056/2330 train_time:127324ms step_avg:61.93ms
step:2057/2330 train_time:127386ms step_avg:61.93ms
step:2058/2330 train_time:127449ms step_avg:61.93ms
step:2059/2330 train_time:127511ms step_avg:61.93ms
step:2060/2330 train_time:127574ms step_avg:61.93ms
step:2061/2330 train_time:127635ms step_avg:61.93ms
step:2062/2330 train_time:127698ms step_avg:61.93ms
step:2063/2330 train_time:127758ms step_avg:61.93ms
step:2064/2330 train_time:127821ms step_avg:61.93ms
step:2065/2330 train_time:127882ms step_avg:61.93ms
step:2066/2330 train_time:127947ms step_avg:61.93ms
step:2067/2330 train_time:128009ms step_avg:61.93ms
step:2068/2330 train_time:128074ms step_avg:61.93ms
step:2069/2330 train_time:128135ms step_avg:61.93ms
step:2070/2330 train_time:128199ms step_avg:61.93ms
step:2071/2330 train_time:128260ms step_avg:61.93ms
step:2072/2330 train_time:128324ms step_avg:61.93ms
step:2073/2330 train_time:128386ms step_avg:61.93ms
step:2074/2330 train_time:128450ms step_avg:61.93ms
step:2075/2330 train_time:128510ms step_avg:61.93ms
step:2076/2330 train_time:128575ms step_avg:61.93ms
step:2077/2330 train_time:128635ms step_avg:61.93ms
step:2078/2330 train_time:128698ms step_avg:61.93ms
step:2079/2330 train_time:128758ms step_avg:61.93ms
step:2080/2330 train_time:128822ms step_avg:61.93ms
step:2081/2330 train_time:128882ms step_avg:61.93ms
step:2082/2330 train_time:128947ms step_avg:61.93ms
step:2083/2330 train_time:129008ms step_avg:61.93ms
step:2084/2330 train_time:129073ms step_avg:61.94ms
step:2085/2330 train_time:129134ms step_avg:61.93ms
step:2086/2330 train_time:129198ms step_avg:61.94ms
step:2087/2330 train_time:129259ms step_avg:61.94ms
step:2088/2330 train_time:129323ms step_avg:61.94ms
step:2089/2330 train_time:129385ms step_avg:61.94ms
step:2090/2330 train_time:129449ms step_avg:61.94ms
step:2091/2330 train_time:129511ms step_avg:61.94ms
step:2092/2330 train_time:129574ms step_avg:61.94ms
step:2093/2330 train_time:129635ms step_avg:61.94ms
step:2094/2330 train_time:129698ms step_avg:61.94ms
step:2095/2330 train_time:129758ms step_avg:61.94ms
step:2096/2330 train_time:129822ms step_avg:61.94ms
step:2097/2330 train_time:129883ms step_avg:61.94ms
step:2098/2330 train_time:129948ms step_avg:61.94ms
step:2099/2330 train_time:130009ms step_avg:61.94ms
step:2100/2330 train_time:130073ms step_avg:61.94ms
step:2101/2330 train_time:130133ms step_avg:61.94ms
step:2102/2330 train_time:130197ms step_avg:61.94ms
step:2103/2330 train_time:130258ms step_avg:61.94ms
step:2104/2330 train_time:130322ms step_avg:61.94ms
step:2105/2330 train_time:130383ms step_avg:61.94ms
step:2106/2330 train_time:130448ms step_avg:61.94ms
step:2107/2330 train_time:130510ms step_avg:61.94ms
step:2108/2330 train_time:130574ms step_avg:61.94ms
step:2109/2330 train_time:130635ms step_avg:61.94ms
step:2110/2330 train_time:130698ms step_avg:61.94ms
step:2111/2330 train_time:130758ms step_avg:61.94ms
step:2112/2330 train_time:130822ms step_avg:61.94ms
step:2113/2330 train_time:130883ms step_avg:61.94ms
step:2114/2330 train_time:130948ms step_avg:61.94ms
step:2115/2330 train_time:131010ms step_avg:61.94ms
step:2116/2330 train_time:131073ms step_avg:61.94ms
step:2117/2330 train_time:131134ms step_avg:61.94ms
step:2118/2330 train_time:131198ms step_avg:61.94ms
step:2119/2330 train_time:131258ms step_avg:61.94ms
step:2120/2330 train_time:131322ms step_avg:61.94ms
step:2121/2330 train_time:131383ms step_avg:61.94ms
step:2122/2330 train_time:131447ms step_avg:61.94ms
step:2123/2330 train_time:131509ms step_avg:61.95ms
step:2124/2330 train_time:131573ms step_avg:61.95ms
step:2125/2330 train_time:131633ms step_avg:61.95ms
step:2126/2330 train_time:131697ms step_avg:61.95ms
step:2127/2330 train_time:131757ms step_avg:61.95ms
step:2128/2330 train_time:131821ms step_avg:61.95ms
step:2129/2330 train_time:131882ms step_avg:61.95ms
step:2130/2330 train_time:131946ms step_avg:61.95ms
step:2131/2330 train_time:132008ms step_avg:61.95ms
step:2132/2330 train_time:132072ms step_avg:61.95ms
step:2133/2330 train_time:132133ms step_avg:61.95ms
step:2134/2330 train_time:132197ms step_avg:61.95ms
step:2135/2330 train_time:132257ms step_avg:61.95ms
step:2136/2330 train_time:132321ms step_avg:61.95ms
step:2137/2330 train_time:132382ms step_avg:61.95ms
step:2138/2330 train_time:132447ms step_avg:61.95ms
step:2139/2330 train_time:132509ms step_avg:61.95ms
step:2140/2330 train_time:132573ms step_avg:61.95ms
step:2141/2330 train_time:132635ms step_avg:61.95ms
step:2142/2330 train_time:132699ms step_avg:61.95ms
step:2143/2330 train_time:132760ms step_avg:61.95ms
step:2144/2330 train_time:132823ms step_avg:61.95ms
step:2145/2330 train_time:132884ms step_avg:61.95ms
step:2146/2330 train_time:132949ms step_avg:61.95ms
step:2147/2330 train_time:133011ms step_avg:61.95ms
step:2148/2330 train_time:133074ms step_avg:61.95ms
step:2149/2330 train_time:133135ms step_avg:61.95ms
step:2150/2330 train_time:133198ms step_avg:61.95ms
step:2151/2330 train_time:133259ms step_avg:61.95ms
step:2152/2330 train_time:133322ms step_avg:61.95ms
step:2153/2330 train_time:133383ms step_avg:61.95ms
step:2154/2330 train_time:133447ms step_avg:61.95ms
step:2155/2330 train_time:133509ms step_avg:61.95ms
step:2156/2330 train_time:133574ms step_avg:61.95ms
step:2157/2330 train_time:133634ms step_avg:61.95ms
step:2158/2330 train_time:133697ms step_avg:61.95ms
step:2159/2330 train_time:133758ms step_avg:61.95ms
step:2160/2330 train_time:133821ms step_avg:61.95ms
step:2161/2330 train_time:133882ms step_avg:61.95ms
step:2162/2330 train_time:133947ms step_avg:61.95ms
step:2163/2330 train_time:134007ms step_avg:61.95ms
step:2164/2330 train_time:134072ms step_avg:61.96ms
step:2165/2330 train_time:134132ms step_avg:61.95ms
step:2166/2330 train_time:134196ms step_avg:61.96ms
step:2167/2330 train_time:134257ms step_avg:61.96ms
step:2168/2330 train_time:134320ms step_avg:61.96ms
step:2169/2330 train_time:134381ms step_avg:61.96ms
step:2170/2330 train_time:134446ms step_avg:61.96ms
step:2171/2330 train_time:134507ms step_avg:61.96ms
step:2172/2330 train_time:134571ms step_avg:61.96ms
step:2173/2330 train_time:134632ms step_avg:61.96ms
step:2174/2330 train_time:134696ms step_avg:61.96ms
step:2175/2330 train_time:134757ms step_avg:61.96ms
step:2176/2330 train_time:134821ms step_avg:61.96ms
step:2177/2330 train_time:134881ms step_avg:61.96ms
step:2178/2330 train_time:134946ms step_avg:61.96ms
step:2179/2330 train_time:135007ms step_avg:61.96ms
step:2180/2330 train_time:135071ms step_avg:61.96ms
step:2181/2330 train_time:135132ms step_avg:61.96ms
step:2182/2330 train_time:135196ms step_avg:61.96ms
step:2183/2330 train_time:135257ms step_avg:61.96ms
step:2184/2330 train_time:135321ms step_avg:61.96ms
step:2185/2330 train_time:135381ms step_avg:61.96ms
step:2186/2330 train_time:135446ms step_avg:61.96ms
step:2187/2330 train_time:135507ms step_avg:61.96ms
step:2188/2330 train_time:135572ms step_avg:61.96ms
step:2189/2330 train_time:135633ms step_avg:61.96ms
step:2190/2330 train_time:135696ms step_avg:61.96ms
step:2191/2330 train_time:135757ms step_avg:61.96ms
step:2192/2330 train_time:135821ms step_avg:61.96ms
step:2193/2330 train_time:135882ms step_avg:61.96ms
step:2194/2330 train_time:135946ms step_avg:61.96ms
step:2195/2330 train_time:136007ms step_avg:61.96ms
step:2196/2330 train_time:136072ms step_avg:61.96ms
step:2197/2330 train_time:136132ms step_avg:61.96ms
step:2198/2330 train_time:136196ms step_avg:61.96ms
step:2199/2330 train_time:136257ms step_avg:61.96ms
step:2200/2330 train_time:136321ms step_avg:61.96ms
step:2201/2330 train_time:136382ms step_avg:61.96ms
step:2202/2330 train_time:136446ms step_avg:61.96ms
step:2203/2330 train_time:136508ms step_avg:61.96ms
step:2204/2330 train_time:136572ms step_avg:61.97ms
step:2205/2330 train_time:136632ms step_avg:61.96ms
step:2206/2330 train_time:136696ms step_avg:61.97ms
step:2207/2330 train_time:136756ms step_avg:61.96ms
step:2208/2330 train_time:136820ms step_avg:61.97ms
step:2209/2330 train_time:136881ms step_avg:61.97ms
step:2210/2330 train_time:136945ms step_avg:61.97ms
step:2211/2330 train_time:137006ms step_avg:61.97ms
step:2212/2330 train_time:137071ms step_avg:61.97ms
step:2213/2330 train_time:137132ms step_avg:61.97ms
step:2214/2330 train_time:137195ms step_avg:61.97ms
step:2215/2330 train_time:137256ms step_avg:61.97ms
step:2216/2330 train_time:137320ms step_avg:61.97ms
step:2217/2330 train_time:137381ms step_avg:61.97ms
step:2218/2330 train_time:137445ms step_avg:61.97ms
step:2219/2330 train_time:137507ms step_avg:61.97ms
step:2220/2330 train_time:137571ms step_avg:61.97ms
step:2221/2330 train_time:137632ms step_avg:61.97ms
step:2222/2330 train_time:137696ms step_avg:61.97ms
step:2223/2330 train_time:137756ms step_avg:61.97ms
step:2224/2330 train_time:137820ms step_avg:61.97ms
step:2225/2330 train_time:137881ms step_avg:61.97ms
step:2226/2330 train_time:137945ms step_avg:61.97ms
step:2227/2330 train_time:138007ms step_avg:61.97ms
step:2228/2330 train_time:138071ms step_avg:61.97ms
step:2229/2330 train_time:138131ms step_avg:61.97ms
step:2230/2330 train_time:138195ms step_avg:61.97ms
step:2231/2330 train_time:138256ms step_avg:61.97ms
step:2232/2330 train_time:138320ms step_avg:61.97ms
step:2233/2330 train_time:138381ms step_avg:61.97ms
step:2234/2330 train_time:138444ms step_avg:61.97ms
step:2235/2330 train_time:138506ms step_avg:61.97ms
step:2236/2330 train_time:138571ms step_avg:61.97ms
step:2237/2330 train_time:138631ms step_avg:61.97ms
step:2238/2330 train_time:138695ms step_avg:61.97ms
step:2239/2330 train_time:138755ms step_avg:61.97ms
step:2240/2330 train_time:138819ms step_avg:61.97ms
step:2241/2330 train_time:138880ms step_avg:61.97ms
step:2242/2330 train_time:138943ms step_avg:61.97ms
step:2243/2330 train_time:139005ms step_avg:61.97ms
step:2244/2330 train_time:139069ms step_avg:61.97ms
step:2245/2330 train_time:139130ms step_avg:61.97ms
step:2246/2330 train_time:139194ms step_avg:61.97ms
step:2247/2330 train_time:139254ms step_avg:61.97ms
step:2248/2330 train_time:139318ms step_avg:61.97ms
step:2249/2330 train_time:139379ms step_avg:61.97ms
step:2250/2330 train_time:139442ms step_avg:61.97ms
step:2250/2330 val_loss:3.4445 train_time:139508ms step_avg:62.00ms
step:2251/2330 train_time:139529ms step_avg:61.99ms
step:2252/2330 train_time:139570ms step_avg:61.98ms
step:2253/2330 train_time:139638ms step_avg:61.98ms
step:2254/2330 train_time:139703ms step_avg:61.98ms
step:2255/2330 train_time:139764ms step_avg:61.98ms
step:2256/2330 train_time:139829ms step_avg:61.98ms
step:2257/2330 train_time:139888ms step_avg:61.98ms
step:2258/2330 train_time:139951ms step_avg:61.98ms
step:2259/2330 train_time:140011ms step_avg:61.98ms
step:2260/2330 train_time:140074ms step_avg:61.98ms
step:2261/2330 train_time:140135ms step_avg:61.98ms
step:2262/2330 train_time:140198ms step_avg:61.98ms
step:2263/2330 train_time:140258ms step_avg:61.98ms
step:2264/2330 train_time:140321ms step_avg:61.98ms
step:2265/2330 train_time:140381ms step_avg:61.98ms
step:2266/2330 train_time:140445ms step_avg:61.98ms
step:2267/2330 train_time:140508ms step_avg:61.98ms
step:2268/2330 train_time:140574ms step_avg:61.98ms
step:2269/2330 train_time:140635ms step_avg:61.98ms
step:2270/2330 train_time:140700ms step_avg:61.98ms
step:2271/2330 train_time:140761ms step_avg:61.98ms
step:2272/2330 train_time:140826ms step_avg:61.98ms
step:2273/2330 train_time:140887ms step_avg:61.98ms
step:2274/2330 train_time:140950ms step_avg:61.98ms
step:2275/2330 train_time:141011ms step_avg:61.98ms
step:2276/2330 train_time:141074ms step_avg:61.98ms
step:2277/2330 train_time:141135ms step_avg:61.98ms
step:2278/2330 train_time:141198ms step_avg:61.98ms
step:2279/2330 train_time:141258ms step_avg:61.98ms
step:2280/2330 train_time:141321ms step_avg:61.98ms
step:2281/2330 train_time:141382ms step_avg:61.98ms
step:2282/2330 train_time:141446ms step_avg:61.98ms
step:2283/2330 train_time:141507ms step_avg:61.98ms
step:2284/2330 train_time:141572ms step_avg:61.98ms
step:2285/2330 train_time:141633ms step_avg:61.98ms
step:2286/2330 train_time:141697ms step_avg:61.98ms
step:2287/2330 train_time:141759ms step_avg:61.98ms
step:2288/2330 train_time:141824ms step_avg:61.99ms
step:2289/2330 train_time:141885ms step_avg:61.99ms
step:2290/2330 train_time:141949ms step_avg:61.99ms
step:2291/2330 train_time:142010ms step_avg:61.99ms
step:2292/2330 train_time:142073ms step_avg:61.99ms
step:2293/2330 train_time:142134ms step_avg:61.99ms
step:2294/2330 train_time:142198ms step_avg:61.99ms
step:2295/2330 train_time:142258ms step_avg:61.99ms
step:2296/2330 train_time:142321ms step_avg:61.99ms
step:2297/2330 train_time:142382ms step_avg:61.99ms
step:2298/2330 train_time:142446ms step_avg:61.99ms
step:2299/2330 train_time:142507ms step_avg:61.99ms
step:2300/2330 train_time:142571ms step_avg:61.99ms
step:2301/2330 train_time:142633ms step_avg:61.99ms
step:2302/2330 train_time:142697ms step_avg:61.99ms
step:2303/2330 train_time:142758ms step_avg:61.99ms
step:2304/2330 train_time:142822ms step_avg:61.99ms
step:2305/2330 train_time:142883ms step_avg:61.99ms
step:2306/2330 train_time:142947ms step_avg:61.99ms
step:2307/2330 train_time:143008ms step_avg:61.99ms
step:2308/2330 train_time:143073ms step_avg:61.99ms
step:2309/2330 train_time:143134ms step_avg:61.99ms
step:2310/2330 train_time:143197ms step_avg:61.99ms
step:2311/2330 train_time:143258ms step_avg:61.99ms
step:2312/2330 train_time:143321ms step_avg:61.99ms
step:2313/2330 train_time:143382ms step_avg:61.99ms
step:2314/2330 train_time:143446ms step_avg:61.99ms
step:2315/2330 train_time:143507ms step_avg:61.99ms
step:2316/2330 train_time:143572ms step_avg:61.99ms
step:2317/2330 train_time:143632ms step_avg:61.99ms
step:2318/2330 train_time:143696ms step_avg:61.99ms
step:2319/2330 train_time:143757ms step_avg:61.99ms
step:2320/2330 train_time:143821ms step_avg:61.99ms
step:2321/2330 train_time:143882ms step_avg:61.99ms
step:2322/2330 train_time:143948ms step_avg:61.99ms
step:2323/2330 train_time:144009ms step_avg:61.99ms
step:2324/2330 train_time:144072ms step_avg:61.99ms
step:2325/2330 train_time:144133ms step_avg:61.99ms
step:2326/2330 train_time:144197ms step_avg:61.99ms
step:2327/2330 train_time:144257ms step_avg:61.99ms
step:2328/2330 train_time:144321ms step_avg:61.99ms
step:2329/2330 train_time:144382ms step_avg:61.99ms
step:2330/2330 train_time:144445ms step_avg:61.99ms
step:2330/2330 val_loss:3.4202 train_time:144511ms step_avg:62.02ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
