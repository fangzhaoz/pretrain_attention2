import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-4"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:49:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             107W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:89ms step_avg:88.75ms
step:2/2330 train_time:187ms step_avg:93.56ms
step:3/2330 train_time:210ms step_avg:70.16ms
step:4/2330 train_time:244ms step_avg:60.99ms
step:5/2330 train_time:301ms step_avg:60.23ms
step:6/2330 train_time:362ms step_avg:60.38ms
step:7/2330 train_time:421ms step_avg:60.12ms
step:8/2330 train_time:482ms step_avg:60.26ms
step:9/2330 train_time:541ms step_avg:60.06ms
step:10/2330 train_time:602ms step_avg:60.20ms
step:11/2330 train_time:660ms step_avg:60.04ms
step:12/2330 train_time:723ms step_avg:60.23ms
step:13/2330 train_time:781ms step_avg:60.09ms
step:14/2330 train_time:843ms step_avg:60.19ms
step:15/2330 train_time:901ms step_avg:60.09ms
step:16/2330 train_time:963ms step_avg:60.21ms
step:17/2330 train_time:1023ms step_avg:60.18ms
step:18/2330 train_time:1088ms step_avg:60.42ms
step:19/2330 train_time:1151ms step_avg:60.59ms
step:20/2330 train_time:1216ms step_avg:60.78ms
step:21/2330 train_time:1276ms step_avg:60.74ms
step:22/2330 train_time:1338ms step_avg:60.83ms
step:23/2330 train_time:1398ms step_avg:60.79ms
step:24/2330 train_time:1462ms step_avg:60.90ms
step:25/2330 train_time:1520ms step_avg:60.82ms
step:26/2330 train_time:1583ms step_avg:60.88ms
step:27/2330 train_time:1641ms step_avg:60.78ms
step:28/2330 train_time:1703ms step_avg:60.82ms
step:29/2330 train_time:1762ms step_avg:60.75ms
step:30/2330 train_time:1823ms step_avg:60.78ms
step:31/2330 train_time:1882ms step_avg:60.71ms
step:32/2330 train_time:1944ms step_avg:60.74ms
step:33/2330 train_time:2003ms step_avg:60.71ms
step:34/2330 train_time:2067ms step_avg:60.80ms
step:35/2330 train_time:2129ms step_avg:60.84ms
step:36/2330 train_time:2193ms step_avg:60.91ms
step:37/2330 train_time:2253ms step_avg:60.90ms
step:38/2330 train_time:2316ms step_avg:60.94ms
step:39/2330 train_time:2375ms step_avg:60.90ms
step:40/2330 train_time:2438ms step_avg:60.95ms
step:41/2330 train_time:2498ms step_avg:60.92ms
step:42/2330 train_time:2560ms step_avg:60.96ms
step:43/2330 train_time:2621ms step_avg:60.95ms
step:44/2330 train_time:2683ms step_avg:60.97ms
step:45/2330 train_time:2742ms step_avg:60.92ms
step:46/2330 train_time:2803ms step_avg:60.94ms
step:47/2330 train_time:2862ms step_avg:60.90ms
step:48/2330 train_time:2924ms step_avg:60.92ms
step:49/2330 train_time:2983ms step_avg:60.88ms
step:50/2330 train_time:3046ms step_avg:60.91ms
step:51/2330 train_time:3106ms step_avg:60.90ms
step:52/2330 train_time:3169ms step_avg:60.95ms
step:53/2330 train_time:3230ms step_avg:60.93ms
step:54/2330 train_time:3292ms step_avg:60.97ms
step:55/2330 train_time:3352ms step_avg:60.94ms
step:56/2330 train_time:3415ms step_avg:60.98ms
step:57/2330 train_time:3474ms step_avg:60.95ms
step:58/2330 train_time:3537ms step_avg:60.99ms
step:59/2330 train_time:3597ms step_avg:60.96ms
step:60/2330 train_time:3659ms step_avg:60.99ms
step:61/2330 train_time:3719ms step_avg:60.96ms
step:62/2330 train_time:3781ms step_avg:60.99ms
step:63/2330 train_time:3841ms step_avg:60.97ms
step:64/2330 train_time:3904ms step_avg:60.99ms
step:65/2330 train_time:3963ms step_avg:60.97ms
step:66/2330 train_time:4026ms step_avg:61.00ms
step:67/2330 train_time:4086ms step_avg:60.98ms
step:68/2330 train_time:4148ms step_avg:61.01ms
step:69/2330 train_time:4209ms step_avg:61.00ms
step:70/2330 train_time:4271ms step_avg:61.02ms
step:71/2330 train_time:4330ms step_avg:60.99ms
step:72/2330 train_time:4393ms step_avg:61.01ms
step:73/2330 train_time:4453ms step_avg:60.99ms
step:74/2330 train_time:4515ms step_avg:61.02ms
step:75/2330 train_time:4575ms step_avg:60.99ms
step:76/2330 train_time:4637ms step_avg:61.02ms
step:77/2330 train_time:4696ms step_avg:60.99ms
step:78/2330 train_time:4759ms step_avg:61.02ms
step:79/2330 train_time:4820ms step_avg:61.01ms
step:80/2330 train_time:4882ms step_avg:61.03ms
step:81/2330 train_time:4942ms step_avg:61.01ms
step:82/2330 train_time:5004ms step_avg:61.03ms
step:83/2330 train_time:5064ms step_avg:61.01ms
step:84/2330 train_time:5127ms step_avg:61.03ms
step:85/2330 train_time:5186ms step_avg:61.01ms
step:86/2330 train_time:5248ms step_avg:61.03ms
step:87/2330 train_time:5309ms step_avg:61.02ms
step:88/2330 train_time:5372ms step_avg:61.04ms
step:89/2330 train_time:5431ms step_avg:61.02ms
step:90/2330 train_time:5494ms step_avg:61.04ms
step:91/2330 train_time:5552ms step_avg:61.02ms
step:92/2330 train_time:5615ms step_avg:61.03ms
step:93/2330 train_time:5674ms step_avg:61.01ms
step:94/2330 train_time:5737ms step_avg:61.03ms
step:95/2330 train_time:5797ms step_avg:61.02ms
step:96/2330 train_time:5861ms step_avg:61.05ms
step:97/2330 train_time:5921ms step_avg:61.04ms
step:98/2330 train_time:5983ms step_avg:61.05ms
step:99/2330 train_time:6044ms step_avg:61.05ms
step:100/2330 train_time:6106ms step_avg:61.06ms
step:101/2330 train_time:6166ms step_avg:61.05ms
step:102/2330 train_time:6229ms step_avg:61.07ms
step:103/2330 train_time:6288ms step_avg:61.05ms
step:104/2330 train_time:6351ms step_avg:61.06ms
step:105/2330 train_time:6411ms step_avg:61.06ms
step:106/2330 train_time:6473ms step_avg:61.07ms
step:107/2330 train_time:6533ms step_avg:61.05ms
step:108/2330 train_time:6595ms step_avg:61.07ms
step:109/2330 train_time:6655ms step_avg:61.05ms
step:110/2330 train_time:6717ms step_avg:61.07ms
step:111/2330 train_time:6776ms step_avg:61.05ms
step:112/2330 train_time:6839ms step_avg:61.06ms
step:113/2330 train_time:6899ms step_avg:61.05ms
step:114/2330 train_time:6962ms step_avg:61.07ms
step:115/2330 train_time:7022ms step_avg:61.06ms
step:116/2330 train_time:7085ms step_avg:61.08ms
step:117/2330 train_time:7145ms step_avg:61.07ms
step:118/2330 train_time:7207ms step_avg:61.08ms
step:119/2330 train_time:7267ms step_avg:61.07ms
step:120/2330 train_time:7329ms step_avg:61.08ms
step:121/2330 train_time:7389ms step_avg:61.07ms
step:122/2330 train_time:7451ms step_avg:61.08ms
step:123/2330 train_time:7511ms step_avg:61.06ms
step:124/2330 train_time:7573ms step_avg:61.07ms
step:125/2330 train_time:7633ms step_avg:61.06ms
step:126/2330 train_time:7695ms step_avg:61.07ms
step:127/2330 train_time:7754ms step_avg:61.05ms
step:128/2330 train_time:7817ms step_avg:61.07ms
step:129/2330 train_time:7876ms step_avg:61.06ms
step:130/2330 train_time:7939ms step_avg:61.07ms
step:131/2330 train_time:8000ms step_avg:61.07ms
step:132/2330 train_time:8063ms step_avg:61.08ms
step:133/2330 train_time:8122ms step_avg:61.07ms
step:134/2330 train_time:8185ms step_avg:61.08ms
step:135/2330 train_time:8244ms step_avg:61.07ms
step:136/2330 train_time:8307ms step_avg:61.08ms
step:137/2330 train_time:8367ms step_avg:61.07ms
step:138/2330 train_time:8429ms step_avg:61.08ms
step:139/2330 train_time:8489ms step_avg:61.07ms
step:140/2330 train_time:8552ms step_avg:61.08ms
step:141/2330 train_time:8611ms step_avg:61.07ms
step:142/2330 train_time:8674ms step_avg:61.08ms
step:143/2330 train_time:8733ms step_avg:61.07ms
step:144/2330 train_time:8796ms step_avg:61.08ms
step:145/2330 train_time:8855ms step_avg:61.07ms
step:146/2330 train_time:8918ms step_avg:61.08ms
step:147/2330 train_time:8978ms step_avg:61.08ms
step:148/2330 train_time:9041ms step_avg:61.09ms
step:149/2330 train_time:9101ms step_avg:61.08ms
step:150/2330 train_time:9165ms step_avg:61.10ms
step:151/2330 train_time:9225ms step_avg:61.09ms
step:152/2330 train_time:9287ms step_avg:61.10ms
step:153/2330 train_time:9347ms step_avg:61.09ms
step:154/2330 train_time:9409ms step_avg:61.10ms
step:155/2330 train_time:9468ms step_avg:61.09ms
step:156/2330 train_time:9531ms step_avg:61.09ms
step:157/2330 train_time:9590ms step_avg:61.08ms
step:158/2330 train_time:9653ms step_avg:61.09ms
step:159/2330 train_time:9713ms step_avg:61.09ms
step:160/2330 train_time:9776ms step_avg:61.10ms
step:161/2330 train_time:9835ms step_avg:61.09ms
step:162/2330 train_time:9897ms step_avg:61.10ms
step:163/2330 train_time:9957ms step_avg:61.09ms
step:164/2330 train_time:10020ms step_avg:61.10ms
step:165/2330 train_time:10080ms step_avg:61.09ms
step:166/2330 train_time:10143ms step_avg:61.10ms
step:167/2330 train_time:10204ms step_avg:61.10ms
step:168/2330 train_time:10267ms step_avg:61.11ms
step:169/2330 train_time:10327ms step_avg:61.11ms
step:170/2330 train_time:10389ms step_avg:61.11ms
step:171/2330 train_time:10448ms step_avg:61.10ms
step:172/2330 train_time:10510ms step_avg:61.11ms
step:173/2330 train_time:10569ms step_avg:61.09ms
step:174/2330 train_time:10632ms step_avg:61.10ms
step:175/2330 train_time:10692ms step_avg:61.09ms
step:176/2330 train_time:10755ms step_avg:61.11ms
step:177/2330 train_time:10814ms step_avg:61.09ms
step:178/2330 train_time:10876ms step_avg:61.10ms
step:179/2330 train_time:10936ms step_avg:61.09ms
step:180/2330 train_time:10999ms step_avg:61.10ms
step:181/2330 train_time:11059ms step_avg:61.10ms
step:182/2330 train_time:11123ms step_avg:61.11ms
step:183/2330 train_time:11182ms step_avg:61.11ms
step:184/2330 train_time:11245ms step_avg:61.11ms
step:185/2330 train_time:11306ms step_avg:61.11ms
step:186/2330 train_time:11369ms step_avg:61.12ms
step:187/2330 train_time:11428ms step_avg:61.11ms
step:188/2330 train_time:11491ms step_avg:61.12ms
step:189/2330 train_time:11550ms step_avg:61.11ms
step:190/2330 train_time:11612ms step_avg:61.12ms
step:191/2330 train_time:11672ms step_avg:61.11ms
step:192/2330 train_time:11735ms step_avg:61.12ms
step:193/2330 train_time:11795ms step_avg:61.11ms
step:194/2330 train_time:11857ms step_avg:61.12ms
step:195/2330 train_time:11916ms step_avg:61.11ms
step:196/2330 train_time:11979ms step_avg:61.12ms
step:197/2330 train_time:12039ms step_avg:61.11ms
step:198/2330 train_time:12102ms step_avg:61.12ms
step:199/2330 train_time:12163ms step_avg:61.12ms
step:200/2330 train_time:12226ms step_avg:61.13ms
step:201/2330 train_time:12286ms step_avg:61.12ms
step:202/2330 train_time:12349ms step_avg:61.13ms
step:203/2330 train_time:12409ms step_avg:61.13ms
step:204/2330 train_time:12471ms step_avg:61.13ms
step:205/2330 train_time:12530ms step_avg:61.12ms
step:206/2330 train_time:12593ms step_avg:61.13ms
step:207/2330 train_time:12652ms step_avg:61.12ms
step:208/2330 train_time:12714ms step_avg:61.13ms
step:209/2330 train_time:12774ms step_avg:61.12ms
step:210/2330 train_time:12836ms step_avg:61.13ms
step:211/2330 train_time:12896ms step_avg:61.12ms
step:212/2330 train_time:12959ms step_avg:61.13ms
step:213/2330 train_time:13019ms step_avg:61.12ms
step:214/2330 train_time:13082ms step_avg:61.13ms
step:215/2330 train_time:13142ms step_avg:61.12ms
step:216/2330 train_time:13205ms step_avg:61.14ms
step:217/2330 train_time:13266ms step_avg:61.13ms
step:218/2330 train_time:13329ms step_avg:61.14ms
step:219/2330 train_time:13389ms step_avg:61.14ms
step:220/2330 train_time:13452ms step_avg:61.14ms
step:221/2330 train_time:13511ms step_avg:61.14ms
step:222/2330 train_time:13574ms step_avg:61.14ms
step:223/2330 train_time:13633ms step_avg:61.13ms
step:224/2330 train_time:13696ms step_avg:61.14ms
step:225/2330 train_time:13755ms step_avg:61.13ms
step:226/2330 train_time:13817ms step_avg:61.14ms
step:227/2330 train_time:13877ms step_avg:61.13ms
step:228/2330 train_time:13939ms step_avg:61.14ms
step:229/2330 train_time:13999ms step_avg:61.13ms
step:230/2330 train_time:14062ms step_avg:61.14ms
step:231/2330 train_time:14122ms step_avg:61.14ms
step:232/2330 train_time:14185ms step_avg:61.14ms
step:233/2330 train_time:14246ms step_avg:61.14ms
step:234/2330 train_time:14309ms step_avg:61.15ms
step:235/2330 train_time:14368ms step_avg:61.14ms
step:236/2330 train_time:14432ms step_avg:61.15ms
step:237/2330 train_time:14491ms step_avg:61.14ms
step:238/2330 train_time:14554ms step_avg:61.15ms
step:239/2330 train_time:14613ms step_avg:61.14ms
step:240/2330 train_time:14675ms step_avg:61.15ms
step:241/2330 train_time:14735ms step_avg:61.14ms
step:242/2330 train_time:14797ms step_avg:61.15ms
step:243/2330 train_time:14857ms step_avg:61.14ms
step:244/2330 train_time:14920ms step_avg:61.15ms
step:245/2330 train_time:14980ms step_avg:61.14ms
step:246/2330 train_time:15042ms step_avg:61.15ms
step:247/2330 train_time:15102ms step_avg:61.14ms
step:248/2330 train_time:15165ms step_avg:61.15ms
step:249/2330 train_time:15226ms step_avg:61.15ms
step:250/2330 train_time:15288ms step_avg:61.15ms
step:250/2330 val_loss:5.3929 train_time:15352ms step_avg:61.41ms
step:251/2330 train_time:15377ms step_avg:61.26ms
step:252/2330 train_time:15415ms step_avg:61.17ms
step:253/2330 train_time:15478ms step_avg:61.18ms
step:254/2330 train_time:15544ms step_avg:61.20ms
step:255/2330 train_time:15606ms step_avg:61.20ms
step:256/2330 train_time:15668ms step_avg:61.21ms
step:257/2330 train_time:15728ms step_avg:61.20ms
step:258/2330 train_time:15790ms step_avg:61.20ms
step:259/2330 train_time:15849ms step_avg:61.19ms
step:260/2330 train_time:15911ms step_avg:61.20ms
step:261/2330 train_time:15970ms step_avg:61.19ms
step:262/2330 train_time:16033ms step_avg:61.20ms
step:263/2330 train_time:16092ms step_avg:61.19ms
step:264/2330 train_time:16154ms step_avg:61.19ms
step:265/2330 train_time:16213ms step_avg:61.18ms
step:266/2330 train_time:16277ms step_avg:61.19ms
step:267/2330 train_time:16337ms step_avg:61.19ms
step:268/2330 train_time:16399ms step_avg:61.19ms
step:269/2330 train_time:16460ms step_avg:61.19ms
step:270/2330 train_time:16523ms step_avg:61.20ms
step:271/2330 train_time:16585ms step_avg:61.20ms
step:272/2330 train_time:16649ms step_avg:61.21ms
step:273/2330 train_time:16709ms step_avg:61.20ms
step:274/2330 train_time:16771ms step_avg:61.21ms
step:275/2330 train_time:16831ms step_avg:61.20ms
step:276/2330 train_time:16894ms step_avg:61.21ms
step:277/2330 train_time:16953ms step_avg:61.20ms
step:278/2330 train_time:17016ms step_avg:61.21ms
step:279/2330 train_time:17075ms step_avg:61.20ms
step:280/2330 train_time:17137ms step_avg:61.20ms
step:281/2330 train_time:17197ms step_avg:61.20ms
step:282/2330 train_time:17259ms step_avg:61.20ms
step:283/2330 train_time:17318ms step_avg:61.19ms
step:284/2330 train_time:17381ms step_avg:61.20ms
step:285/2330 train_time:17440ms step_avg:61.19ms
step:286/2330 train_time:17504ms step_avg:61.20ms
step:287/2330 train_time:17564ms step_avg:61.20ms
step:288/2330 train_time:17629ms step_avg:61.21ms
step:289/2330 train_time:17688ms step_avg:61.21ms
step:290/2330 train_time:17751ms step_avg:61.21ms
step:291/2330 train_time:17810ms step_avg:61.20ms
step:292/2330 train_time:17872ms step_avg:61.21ms
step:293/2330 train_time:17932ms step_avg:61.20ms
step:294/2330 train_time:17994ms step_avg:61.20ms
step:295/2330 train_time:18054ms step_avg:61.20ms
step:296/2330 train_time:18116ms step_avg:61.20ms
step:297/2330 train_time:18175ms step_avg:61.20ms
step:298/2330 train_time:18237ms step_avg:61.20ms
step:299/2330 train_time:18297ms step_avg:61.19ms
step:300/2330 train_time:18359ms step_avg:61.20ms
step:301/2330 train_time:18420ms step_avg:61.20ms
step:302/2330 train_time:18483ms step_avg:61.20ms
step:303/2330 train_time:18543ms step_avg:61.20ms
step:304/2330 train_time:18607ms step_avg:61.21ms
step:305/2330 train_time:18667ms step_avg:61.20ms
step:306/2330 train_time:18731ms step_avg:61.21ms
step:307/2330 train_time:18791ms step_avg:61.21ms
step:308/2330 train_time:18854ms step_avg:61.21ms
step:309/2330 train_time:18913ms step_avg:61.21ms
step:310/2330 train_time:18976ms step_avg:61.21ms
step:311/2330 train_time:19035ms step_avg:61.21ms
step:312/2330 train_time:19097ms step_avg:61.21ms
step:313/2330 train_time:19156ms step_avg:61.20ms
step:314/2330 train_time:19219ms step_avg:61.21ms
step:315/2330 train_time:19279ms step_avg:61.20ms
step:316/2330 train_time:19341ms step_avg:61.21ms
step:317/2330 train_time:19401ms step_avg:61.20ms
step:318/2330 train_time:19464ms step_avg:61.21ms
step:319/2330 train_time:19524ms step_avg:61.20ms
step:320/2330 train_time:19587ms step_avg:61.21ms
step:321/2330 train_time:19647ms step_avg:61.21ms
step:322/2330 train_time:19711ms step_avg:61.21ms
step:323/2330 train_time:19771ms step_avg:61.21ms
step:324/2330 train_time:19834ms step_avg:61.22ms
step:325/2330 train_time:19894ms step_avg:61.21ms
step:326/2330 train_time:19957ms step_avg:61.22ms
step:327/2330 train_time:20016ms step_avg:61.21ms
step:328/2330 train_time:20078ms step_avg:61.21ms
step:329/2330 train_time:20137ms step_avg:61.21ms
step:330/2330 train_time:20199ms step_avg:61.21ms
step:331/2330 train_time:20259ms step_avg:61.20ms
step:332/2330 train_time:20322ms step_avg:61.21ms
step:333/2330 train_time:20381ms step_avg:61.20ms
step:334/2330 train_time:20444ms step_avg:61.21ms
step:335/2330 train_time:20504ms step_avg:61.20ms
step:336/2330 train_time:20567ms step_avg:61.21ms
step:337/2330 train_time:20627ms step_avg:61.21ms
step:338/2330 train_time:20690ms step_avg:61.21ms
step:339/2330 train_time:20750ms step_avg:61.21ms
step:340/2330 train_time:20813ms step_avg:61.21ms
step:341/2330 train_time:20872ms step_avg:61.21ms
step:342/2330 train_time:20935ms step_avg:61.21ms
step:343/2330 train_time:20995ms step_avg:61.21ms
step:344/2330 train_time:21057ms step_avg:61.21ms
step:345/2330 train_time:21116ms step_avg:61.21ms
step:346/2330 train_time:21178ms step_avg:61.21ms
step:347/2330 train_time:21238ms step_avg:61.20ms
step:348/2330 train_time:21300ms step_avg:61.21ms
step:349/2330 train_time:21359ms step_avg:61.20ms
step:350/2330 train_time:21423ms step_avg:61.21ms
step:351/2330 train_time:21482ms step_avg:61.20ms
step:352/2330 train_time:21546ms step_avg:61.21ms
step:353/2330 train_time:21606ms step_avg:61.21ms
step:354/2330 train_time:21670ms step_avg:61.21ms
step:355/2330 train_time:21730ms step_avg:61.21ms
step:356/2330 train_time:21793ms step_avg:61.22ms
step:357/2330 train_time:21853ms step_avg:61.21ms
step:358/2330 train_time:21915ms step_avg:61.22ms
step:359/2330 train_time:21975ms step_avg:61.21ms
step:360/2330 train_time:22037ms step_avg:61.21ms
step:361/2330 train_time:22096ms step_avg:61.21ms
step:362/2330 train_time:22158ms step_avg:61.21ms
step:363/2330 train_time:22218ms step_avg:61.21ms
step:364/2330 train_time:22281ms step_avg:61.21ms
step:365/2330 train_time:22341ms step_avg:61.21ms
step:366/2330 train_time:22404ms step_avg:61.21ms
step:367/2330 train_time:22463ms step_avg:61.21ms
step:368/2330 train_time:22526ms step_avg:61.21ms
step:369/2330 train_time:22586ms step_avg:61.21ms
step:370/2330 train_time:22650ms step_avg:61.22ms
step:371/2330 train_time:22710ms step_avg:61.21ms
step:372/2330 train_time:22773ms step_avg:61.22ms
step:373/2330 train_time:22832ms step_avg:61.21ms
step:374/2330 train_time:22895ms step_avg:61.22ms
step:375/2330 train_time:22954ms step_avg:61.21ms
step:376/2330 train_time:23017ms step_avg:61.22ms
step:377/2330 train_time:23076ms step_avg:61.21ms
step:378/2330 train_time:23138ms step_avg:61.21ms
step:379/2330 train_time:23198ms step_avg:61.21ms
step:380/2330 train_time:23261ms step_avg:61.21ms
step:381/2330 train_time:23321ms step_avg:61.21ms
step:382/2330 train_time:23384ms step_avg:61.21ms
step:383/2330 train_time:23445ms step_avg:61.21ms
step:384/2330 train_time:23508ms step_avg:61.22ms
step:385/2330 train_time:23568ms step_avg:61.21ms
step:386/2330 train_time:23630ms step_avg:61.22ms
step:387/2330 train_time:23690ms step_avg:61.22ms
step:388/2330 train_time:23754ms step_avg:61.22ms
step:389/2330 train_time:23814ms step_avg:61.22ms
step:390/2330 train_time:23876ms step_avg:61.22ms
step:391/2330 train_time:23935ms step_avg:61.22ms
step:392/2330 train_time:23998ms step_avg:61.22ms
step:393/2330 train_time:24057ms step_avg:61.21ms
step:394/2330 train_time:24119ms step_avg:61.22ms
step:395/2330 train_time:24179ms step_avg:61.21ms
step:396/2330 train_time:24242ms step_avg:61.22ms
step:397/2330 train_time:24301ms step_avg:61.21ms
step:398/2330 train_time:24364ms step_avg:61.22ms
step:399/2330 train_time:24424ms step_avg:61.21ms
step:400/2330 train_time:24487ms step_avg:61.22ms
step:401/2330 train_time:24547ms step_avg:61.21ms
step:402/2330 train_time:24610ms step_avg:61.22ms
step:403/2330 train_time:24670ms step_avg:61.22ms
step:404/2330 train_time:24733ms step_avg:61.22ms
step:405/2330 train_time:24792ms step_avg:61.22ms
step:406/2330 train_time:24855ms step_avg:61.22ms
step:407/2330 train_time:24915ms step_avg:61.22ms
step:408/2330 train_time:24978ms step_avg:61.22ms
step:409/2330 train_time:25038ms step_avg:61.22ms
step:410/2330 train_time:25101ms step_avg:61.22ms
step:411/2330 train_time:25161ms step_avg:61.22ms
step:412/2330 train_time:25223ms step_avg:61.22ms
step:413/2330 train_time:25283ms step_avg:61.22ms
step:414/2330 train_time:25345ms step_avg:61.22ms
step:415/2330 train_time:25405ms step_avg:61.22ms
step:416/2330 train_time:25468ms step_avg:61.22ms
step:417/2330 train_time:25528ms step_avg:61.22ms
step:418/2330 train_time:25590ms step_avg:61.22ms
step:419/2330 train_time:25651ms step_avg:61.22ms
step:420/2330 train_time:25714ms step_avg:61.22ms
step:421/2330 train_time:25773ms step_avg:61.22ms
step:422/2330 train_time:25836ms step_avg:61.22ms
step:423/2330 train_time:25896ms step_avg:61.22ms
step:424/2330 train_time:25958ms step_avg:61.22ms
step:425/2330 train_time:26017ms step_avg:61.22ms
step:426/2330 train_time:26080ms step_avg:61.22ms
step:427/2330 train_time:26140ms step_avg:61.22ms
step:428/2330 train_time:26203ms step_avg:61.22ms
step:429/2330 train_time:26262ms step_avg:61.22ms
step:430/2330 train_time:26325ms step_avg:61.22ms
step:431/2330 train_time:26385ms step_avg:61.22ms
step:432/2330 train_time:26448ms step_avg:61.22ms
step:433/2330 train_time:26508ms step_avg:61.22ms
step:434/2330 train_time:26570ms step_avg:61.22ms
step:435/2330 train_time:26631ms step_avg:61.22ms
step:436/2330 train_time:26694ms step_avg:61.23ms
step:437/2330 train_time:26754ms step_avg:61.22ms
step:438/2330 train_time:26817ms step_avg:61.23ms
step:439/2330 train_time:26877ms step_avg:61.22ms
step:440/2330 train_time:26939ms step_avg:61.23ms
step:441/2330 train_time:26999ms step_avg:61.22ms
step:442/2330 train_time:27061ms step_avg:61.22ms
step:443/2330 train_time:27121ms step_avg:61.22ms
step:444/2330 train_time:27184ms step_avg:61.22ms
step:445/2330 train_time:27243ms step_avg:61.22ms
step:446/2330 train_time:27306ms step_avg:61.23ms
step:447/2330 train_time:27367ms step_avg:61.22ms
step:448/2330 train_time:27430ms step_avg:61.23ms
step:449/2330 train_time:27489ms step_avg:61.22ms
step:450/2330 train_time:27552ms step_avg:61.23ms
step:451/2330 train_time:27612ms step_avg:61.22ms
step:452/2330 train_time:27674ms step_avg:61.23ms
step:453/2330 train_time:27734ms step_avg:61.22ms
step:454/2330 train_time:27796ms step_avg:61.23ms
step:455/2330 train_time:27856ms step_avg:61.22ms
step:456/2330 train_time:27919ms step_avg:61.23ms
step:457/2330 train_time:27979ms step_avg:61.22ms
step:458/2330 train_time:28042ms step_avg:61.23ms
step:459/2330 train_time:28101ms step_avg:61.22ms
step:460/2330 train_time:28164ms step_avg:61.23ms
step:461/2330 train_time:28223ms step_avg:61.22ms
step:462/2330 train_time:28286ms step_avg:61.23ms
step:463/2330 train_time:28347ms step_avg:61.22ms
step:464/2330 train_time:28409ms step_avg:61.23ms
step:465/2330 train_time:28469ms step_avg:61.22ms
step:466/2330 train_time:28532ms step_avg:61.23ms
step:467/2330 train_time:28592ms step_avg:61.22ms
step:468/2330 train_time:28655ms step_avg:61.23ms
step:469/2330 train_time:28715ms step_avg:61.23ms
step:470/2330 train_time:28777ms step_avg:61.23ms
step:471/2330 train_time:28837ms step_avg:61.23ms
step:472/2330 train_time:28900ms step_avg:61.23ms
step:473/2330 train_time:28960ms step_avg:61.23ms
step:474/2330 train_time:29023ms step_avg:61.23ms
step:475/2330 train_time:29082ms step_avg:61.23ms
step:476/2330 train_time:29145ms step_avg:61.23ms
step:477/2330 train_time:29207ms step_avg:61.23ms
step:478/2330 train_time:29268ms step_avg:61.23ms
step:479/2330 train_time:29328ms step_avg:61.23ms
step:480/2330 train_time:29390ms step_avg:61.23ms
step:481/2330 train_time:29451ms step_avg:61.23ms
step:482/2330 train_time:29514ms step_avg:61.23ms
step:483/2330 train_time:29574ms step_avg:61.23ms
step:484/2330 train_time:29636ms step_avg:61.23ms
step:485/2330 train_time:29695ms step_avg:61.23ms
step:486/2330 train_time:29758ms step_avg:61.23ms
step:487/2330 train_time:29818ms step_avg:61.23ms
step:488/2330 train_time:29881ms step_avg:61.23ms
step:489/2330 train_time:29941ms step_avg:61.23ms
step:490/2330 train_time:30004ms step_avg:61.23ms
step:491/2330 train_time:30064ms step_avg:61.23ms
step:492/2330 train_time:30127ms step_avg:61.23ms
step:493/2330 train_time:30187ms step_avg:61.23ms
step:494/2330 train_time:30250ms step_avg:61.23ms
step:495/2330 train_time:30309ms step_avg:61.23ms
step:496/2330 train_time:30372ms step_avg:61.23ms
step:497/2330 train_time:30432ms step_avg:61.23ms
step:498/2330 train_time:30495ms step_avg:61.23ms
step:499/2330 train_time:30555ms step_avg:61.23ms
step:500/2330 train_time:30618ms step_avg:61.24ms
step:500/2330 val_loss:4.8244 train_time:30682ms step_avg:61.36ms
step:501/2330 train_time:30706ms step_avg:61.29ms
step:502/2330 train_time:30741ms step_avg:61.24ms
step:503/2330 train_time:30805ms step_avg:61.24ms
step:504/2330 train_time:30873ms step_avg:61.26ms
step:505/2330 train_time:30934ms step_avg:61.25ms
step:506/2330 train_time:30997ms step_avg:61.26ms
step:507/2330 train_time:31057ms step_avg:61.26ms
step:508/2330 train_time:31120ms step_avg:61.26ms
step:509/2330 train_time:31179ms step_avg:61.26ms
step:510/2330 train_time:31241ms step_avg:61.26ms
step:511/2330 train_time:31300ms step_avg:61.25ms
step:512/2330 train_time:31362ms step_avg:61.25ms
step:513/2330 train_time:31421ms step_avg:61.25ms
step:514/2330 train_time:31483ms step_avg:61.25ms
step:515/2330 train_time:31542ms step_avg:61.25ms
step:516/2330 train_time:31604ms step_avg:61.25ms
step:517/2330 train_time:31665ms step_avg:61.25ms
step:518/2330 train_time:31727ms step_avg:61.25ms
step:519/2330 train_time:31789ms step_avg:61.25ms
step:520/2330 train_time:31852ms step_avg:61.25ms
step:521/2330 train_time:31913ms step_avg:61.25ms
step:522/2330 train_time:31976ms step_avg:61.26ms
step:523/2330 train_time:32037ms step_avg:61.26ms
step:524/2330 train_time:32100ms step_avg:61.26ms
step:525/2330 train_time:32159ms step_avg:61.26ms
step:526/2330 train_time:32221ms step_avg:61.26ms
step:527/2330 train_time:32280ms step_avg:61.25ms
step:528/2330 train_time:32342ms step_avg:61.25ms
step:529/2330 train_time:32402ms step_avg:61.25ms
step:530/2330 train_time:32464ms step_avg:61.25ms
step:531/2330 train_time:32524ms step_avg:61.25ms
step:532/2330 train_time:32586ms step_avg:61.25ms
step:533/2330 train_time:32646ms step_avg:61.25ms
step:534/2330 train_time:32710ms step_avg:61.25ms
step:535/2330 train_time:32769ms step_avg:61.25ms
step:536/2330 train_time:32832ms step_avg:61.25ms
step:537/2330 train_time:32892ms step_avg:61.25ms
step:538/2330 train_time:32955ms step_avg:61.25ms
step:539/2330 train_time:33016ms step_avg:61.25ms
step:540/2330 train_time:33079ms step_avg:61.26ms
step:541/2330 train_time:33139ms step_avg:61.25ms
step:542/2330 train_time:33201ms step_avg:61.26ms
step:543/2330 train_time:33261ms step_avg:61.25ms
step:544/2330 train_time:33324ms step_avg:61.26ms
step:545/2330 train_time:33384ms step_avg:61.25ms
step:546/2330 train_time:33446ms step_avg:61.26ms
step:547/2330 train_time:33505ms step_avg:61.25ms
step:548/2330 train_time:33568ms step_avg:61.25ms
step:549/2330 train_time:33627ms step_avg:61.25ms
step:550/2330 train_time:33690ms step_avg:61.25ms
step:551/2330 train_time:33749ms step_avg:61.25ms
step:552/2330 train_time:33813ms step_avg:61.25ms
step:553/2330 train_time:33872ms step_avg:61.25ms
step:554/2330 train_time:33936ms step_avg:61.26ms
step:555/2330 train_time:33996ms step_avg:61.25ms
step:556/2330 train_time:34059ms step_avg:61.26ms
step:557/2330 train_time:34120ms step_avg:61.26ms
step:558/2330 train_time:34183ms step_avg:61.26ms
step:559/2330 train_time:34243ms step_avg:61.26ms
step:560/2330 train_time:34306ms step_avg:61.26ms
step:561/2330 train_time:34365ms step_avg:61.26ms
step:562/2330 train_time:34427ms step_avg:61.26ms
step:563/2330 train_time:34486ms step_avg:61.25ms
step:564/2330 train_time:34549ms step_avg:61.26ms
step:565/2330 train_time:34608ms step_avg:61.25ms
step:566/2330 train_time:34670ms step_avg:61.26ms
step:567/2330 train_time:34731ms step_avg:61.25ms
step:568/2330 train_time:34794ms step_avg:61.26ms
step:569/2330 train_time:34853ms step_avg:61.25ms
step:570/2330 train_time:34917ms step_avg:61.26ms
step:571/2330 train_time:34977ms step_avg:61.26ms
step:572/2330 train_time:35041ms step_avg:61.26ms
step:573/2330 train_time:35101ms step_avg:61.26ms
step:574/2330 train_time:35164ms step_avg:61.26ms
step:575/2330 train_time:35224ms step_avg:61.26ms
step:576/2330 train_time:35287ms step_avg:61.26ms
step:577/2330 train_time:35346ms step_avg:61.26ms
step:578/2330 train_time:35409ms step_avg:61.26ms
step:579/2330 train_time:35468ms step_avg:61.26ms
step:580/2330 train_time:35531ms step_avg:61.26ms
step:581/2330 train_time:35590ms step_avg:61.26ms
step:582/2330 train_time:35653ms step_avg:61.26ms
step:583/2330 train_time:35713ms step_avg:61.26ms
step:584/2330 train_time:35776ms step_avg:61.26ms
step:585/2330 train_time:35836ms step_avg:61.26ms
step:586/2330 train_time:35899ms step_avg:61.26ms
step:587/2330 train_time:35958ms step_avg:61.26ms
step:588/2330 train_time:36021ms step_avg:61.26ms
step:589/2330 train_time:36082ms step_avg:61.26ms
step:590/2330 train_time:36145ms step_avg:61.26ms
step:591/2330 train_time:36205ms step_avg:61.26ms
step:592/2330 train_time:36268ms step_avg:61.26ms
step:593/2330 train_time:36328ms step_avg:61.26ms
step:594/2330 train_time:36391ms step_avg:61.26ms
step:595/2330 train_time:36451ms step_avg:61.26ms
step:596/2330 train_time:36513ms step_avg:61.26ms
step:597/2330 train_time:36574ms step_avg:61.26ms
step:598/2330 train_time:36637ms step_avg:61.27ms
step:599/2330 train_time:36697ms step_avg:61.26ms
step:600/2330 train_time:36759ms step_avg:61.27ms
step:601/2330 train_time:36819ms step_avg:61.26ms
step:602/2330 train_time:36882ms step_avg:61.27ms
step:603/2330 train_time:36941ms step_avg:61.26ms
step:604/2330 train_time:37004ms step_avg:61.26ms
step:605/2330 train_time:37063ms step_avg:61.26ms
step:606/2330 train_time:37126ms step_avg:61.26ms
step:607/2330 train_time:37187ms step_avg:61.26ms
step:608/2330 train_time:37249ms step_avg:61.27ms
step:609/2330 train_time:37309ms step_avg:61.26ms
step:610/2330 train_time:37372ms step_avg:61.27ms
step:611/2330 train_time:37431ms step_avg:61.26ms
step:612/2330 train_time:37494ms step_avg:61.27ms
step:613/2330 train_time:37554ms step_avg:61.26ms
step:614/2330 train_time:37617ms step_avg:61.27ms
step:615/2330 train_time:37676ms step_avg:61.26ms
step:616/2330 train_time:37739ms step_avg:61.27ms
step:617/2330 train_time:37799ms step_avg:61.26ms
step:618/2330 train_time:37862ms step_avg:61.27ms
step:619/2330 train_time:37922ms step_avg:61.26ms
step:620/2330 train_time:37985ms step_avg:61.27ms
step:621/2330 train_time:38045ms step_avg:61.26ms
step:622/2330 train_time:38108ms step_avg:61.27ms
step:623/2330 train_time:38167ms step_avg:61.26ms
step:624/2330 train_time:38230ms step_avg:61.27ms
step:625/2330 train_time:38290ms step_avg:61.26ms
step:626/2330 train_time:38353ms step_avg:61.27ms
step:627/2330 train_time:38413ms step_avg:61.26ms
step:628/2330 train_time:38476ms step_avg:61.27ms
step:629/2330 train_time:38536ms step_avg:61.27ms
step:630/2330 train_time:38599ms step_avg:61.27ms
step:631/2330 train_time:38658ms step_avg:61.26ms
step:632/2330 train_time:38721ms step_avg:61.27ms
step:633/2330 train_time:38782ms step_avg:61.27ms
step:634/2330 train_time:38845ms step_avg:61.27ms
step:635/2330 train_time:38904ms step_avg:61.27ms
step:636/2330 train_time:38966ms step_avg:61.27ms
step:637/2330 train_time:39026ms step_avg:61.27ms
step:638/2330 train_time:39089ms step_avg:61.27ms
step:639/2330 train_time:39148ms step_avg:61.27ms
step:640/2330 train_time:39211ms step_avg:61.27ms
step:641/2330 train_time:39270ms step_avg:61.26ms
step:642/2330 train_time:39334ms step_avg:61.27ms
step:643/2330 train_time:39393ms step_avg:61.27ms
step:644/2330 train_time:39456ms step_avg:61.27ms
step:645/2330 train_time:39516ms step_avg:61.27ms
step:646/2330 train_time:39579ms step_avg:61.27ms
step:647/2330 train_time:39639ms step_avg:61.27ms
step:648/2330 train_time:39702ms step_avg:61.27ms
step:649/2330 train_time:39762ms step_avg:61.27ms
step:650/2330 train_time:39825ms step_avg:61.27ms
step:651/2330 train_time:39885ms step_avg:61.27ms
step:652/2330 train_time:39948ms step_avg:61.27ms
step:653/2330 train_time:40007ms step_avg:61.27ms
step:654/2330 train_time:40070ms step_avg:61.27ms
step:655/2330 train_time:40130ms step_avg:61.27ms
step:656/2330 train_time:40193ms step_avg:61.27ms
step:657/2330 train_time:40253ms step_avg:61.27ms
step:658/2330 train_time:40315ms step_avg:61.27ms
step:659/2330 train_time:40375ms step_avg:61.27ms
step:660/2330 train_time:40438ms step_avg:61.27ms
step:661/2330 train_time:40498ms step_avg:61.27ms
step:662/2330 train_time:40560ms step_avg:61.27ms
step:663/2330 train_time:40620ms step_avg:61.27ms
step:664/2330 train_time:40683ms step_avg:61.27ms
step:665/2330 train_time:40743ms step_avg:61.27ms
step:666/2330 train_time:40805ms step_avg:61.27ms
step:667/2330 train_time:40865ms step_avg:61.27ms
step:668/2330 train_time:40927ms step_avg:61.27ms
step:669/2330 train_time:40987ms step_avg:61.27ms
step:670/2330 train_time:41050ms step_avg:61.27ms
step:671/2330 train_time:41110ms step_avg:61.27ms
step:672/2330 train_time:41173ms step_avg:61.27ms
step:673/2330 train_time:41232ms step_avg:61.27ms
step:674/2330 train_time:41295ms step_avg:61.27ms
step:675/2330 train_time:41356ms step_avg:61.27ms
step:676/2330 train_time:41419ms step_avg:61.27ms
step:677/2330 train_time:41479ms step_avg:61.27ms
step:678/2330 train_time:41543ms step_avg:61.27ms
step:679/2330 train_time:41603ms step_avg:61.27ms
step:680/2330 train_time:41665ms step_avg:61.27ms
step:681/2330 train_time:41725ms step_avg:61.27ms
step:682/2330 train_time:41788ms step_avg:61.27ms
step:683/2330 train_time:41847ms step_avg:61.27ms
step:684/2330 train_time:41910ms step_avg:61.27ms
step:685/2330 train_time:41970ms step_avg:61.27ms
step:686/2330 train_time:42033ms step_avg:61.27ms
step:687/2330 train_time:42093ms step_avg:61.27ms
step:688/2330 train_time:42156ms step_avg:61.27ms
step:689/2330 train_time:42216ms step_avg:61.27ms
step:690/2330 train_time:42279ms step_avg:61.27ms
step:691/2330 train_time:42339ms step_avg:61.27ms
step:692/2330 train_time:42402ms step_avg:61.27ms
step:693/2330 train_time:42461ms step_avg:61.27ms
step:694/2330 train_time:42524ms step_avg:61.27ms
step:695/2330 train_time:42584ms step_avg:61.27ms
step:696/2330 train_time:42647ms step_avg:61.27ms
step:697/2330 train_time:42706ms step_avg:61.27ms
step:698/2330 train_time:42769ms step_avg:61.27ms
step:699/2330 train_time:42828ms step_avg:61.27ms
step:700/2330 train_time:42891ms step_avg:61.27ms
step:701/2330 train_time:42951ms step_avg:61.27ms
step:702/2330 train_time:43014ms step_avg:61.27ms
step:703/2330 train_time:43074ms step_avg:61.27ms
step:704/2330 train_time:43138ms step_avg:61.28ms
step:705/2330 train_time:43198ms step_avg:61.27ms
step:706/2330 train_time:43261ms step_avg:61.28ms
step:707/2330 train_time:43320ms step_avg:61.27ms
step:708/2330 train_time:43384ms step_avg:61.28ms
step:709/2330 train_time:43443ms step_avg:61.27ms
step:710/2330 train_time:43506ms step_avg:61.28ms
step:711/2330 train_time:43565ms step_avg:61.27ms
step:712/2330 train_time:43628ms step_avg:61.28ms
step:713/2330 train_time:43688ms step_avg:61.27ms
step:714/2330 train_time:43750ms step_avg:61.27ms
step:715/2330 train_time:43810ms step_avg:61.27ms
step:716/2330 train_time:43873ms step_avg:61.28ms
step:717/2330 train_time:43933ms step_avg:61.27ms
step:718/2330 train_time:43996ms step_avg:61.28ms
step:719/2330 train_time:44056ms step_avg:61.27ms
step:720/2330 train_time:44119ms step_avg:61.28ms
step:721/2330 train_time:44179ms step_avg:61.27ms
step:722/2330 train_time:44242ms step_avg:61.28ms
step:723/2330 train_time:44302ms step_avg:61.27ms
step:724/2330 train_time:44365ms step_avg:61.28ms
step:725/2330 train_time:44425ms step_avg:61.28ms
step:726/2330 train_time:44488ms step_avg:61.28ms
step:727/2330 train_time:44548ms step_avg:61.28ms
step:728/2330 train_time:44610ms step_avg:61.28ms
step:729/2330 train_time:44670ms step_avg:61.28ms
step:730/2330 train_time:44733ms step_avg:61.28ms
step:731/2330 train_time:44793ms step_avg:61.28ms
step:732/2330 train_time:44856ms step_avg:61.28ms
step:733/2330 train_time:44915ms step_avg:61.28ms
step:734/2330 train_time:44978ms step_avg:61.28ms
step:735/2330 train_time:45039ms step_avg:61.28ms
step:736/2330 train_time:45101ms step_avg:61.28ms
step:737/2330 train_time:45160ms step_avg:61.28ms
step:738/2330 train_time:45224ms step_avg:61.28ms
step:739/2330 train_time:45284ms step_avg:61.28ms
step:740/2330 train_time:45347ms step_avg:61.28ms
step:741/2330 train_time:45406ms step_avg:61.28ms
step:742/2330 train_time:45468ms step_avg:61.28ms
step:743/2330 train_time:45528ms step_avg:61.28ms
step:744/2330 train_time:45592ms step_avg:61.28ms
step:745/2330 train_time:45651ms step_avg:61.28ms
step:746/2330 train_time:45714ms step_avg:61.28ms
step:747/2330 train_time:45775ms step_avg:61.28ms
step:748/2330 train_time:45839ms step_avg:61.28ms
step:749/2330 train_time:45898ms step_avg:61.28ms
step:750/2330 train_time:45961ms step_avg:61.28ms
step:750/2330 val_loss:4.5090 train_time:46025ms step_avg:61.37ms
step:751/2330 train_time:46049ms step_avg:61.32ms
step:752/2330 train_time:46086ms step_avg:61.28ms
step:753/2330 train_time:46154ms step_avg:61.29ms
step:754/2330 train_time:46222ms step_avg:61.30ms
step:755/2330 train_time:46283ms step_avg:61.30ms
step:756/2330 train_time:46347ms step_avg:61.31ms
step:757/2330 train_time:46407ms step_avg:61.30ms
step:758/2330 train_time:46470ms step_avg:61.31ms
step:759/2330 train_time:46529ms step_avg:61.30ms
step:760/2330 train_time:46591ms step_avg:61.30ms
step:761/2330 train_time:46650ms step_avg:61.30ms
step:762/2330 train_time:46712ms step_avg:61.30ms
step:763/2330 train_time:46771ms step_avg:61.30ms
step:764/2330 train_time:46834ms step_avg:61.30ms
step:765/2330 train_time:46893ms step_avg:61.30ms
step:766/2330 train_time:46956ms step_avg:61.30ms
step:767/2330 train_time:47017ms step_avg:61.30ms
step:768/2330 train_time:47080ms step_avg:61.30ms
step:769/2330 train_time:47144ms step_avg:61.31ms
step:770/2330 train_time:47209ms step_avg:61.31ms
step:771/2330 train_time:47272ms step_avg:61.31ms
step:772/2330 train_time:47336ms step_avg:61.32ms
step:773/2330 train_time:47397ms step_avg:61.32ms
step:774/2330 train_time:47461ms step_avg:61.32ms
step:775/2330 train_time:47521ms step_avg:61.32ms
step:776/2330 train_time:47585ms step_avg:61.32ms
step:777/2330 train_time:47645ms step_avg:61.32ms
step:778/2330 train_time:47709ms step_avg:61.32ms
step:779/2330 train_time:47770ms step_avg:61.32ms
step:780/2330 train_time:47833ms step_avg:61.32ms
step:781/2330 train_time:47894ms step_avg:61.32ms
step:782/2330 train_time:47957ms step_avg:61.33ms
step:783/2330 train_time:48017ms step_avg:61.33ms
step:784/2330 train_time:48081ms step_avg:61.33ms
step:785/2330 train_time:48142ms step_avg:61.33ms
step:786/2330 train_time:48207ms step_avg:61.33ms
step:787/2330 train_time:48269ms step_avg:61.33ms
step:788/2330 train_time:48333ms step_avg:61.34ms
step:789/2330 train_time:48394ms step_avg:61.34ms
step:790/2330 train_time:48458ms step_avg:61.34ms
step:791/2330 train_time:48519ms step_avg:61.34ms
step:792/2330 train_time:48582ms step_avg:61.34ms
step:793/2330 train_time:48642ms step_avg:61.34ms
step:794/2330 train_time:48707ms step_avg:61.34ms
step:795/2330 train_time:48768ms step_avg:61.34ms
step:796/2330 train_time:48832ms step_avg:61.35ms
step:797/2330 train_time:48893ms step_avg:61.35ms
step:798/2330 train_time:48956ms step_avg:61.35ms
step:799/2330 train_time:49017ms step_avg:61.35ms
step:800/2330 train_time:49080ms step_avg:61.35ms
step:801/2330 train_time:49141ms step_avg:61.35ms
step:802/2330 train_time:49205ms step_avg:61.35ms
step:803/2330 train_time:49267ms step_avg:61.35ms
step:804/2330 train_time:49331ms step_avg:61.36ms
step:805/2330 train_time:49392ms step_avg:61.36ms
step:806/2330 train_time:49455ms step_avg:61.36ms
step:807/2330 train_time:49517ms step_avg:61.36ms
step:808/2330 train_time:49580ms step_avg:61.36ms
step:809/2330 train_time:49641ms step_avg:61.36ms
step:810/2330 train_time:49704ms step_avg:61.36ms
step:811/2330 train_time:49765ms step_avg:61.36ms
step:812/2330 train_time:49829ms step_avg:61.37ms
step:813/2330 train_time:49889ms step_avg:61.36ms
step:814/2330 train_time:49953ms step_avg:61.37ms
step:815/2330 train_time:50014ms step_avg:61.37ms
step:816/2330 train_time:50078ms step_avg:61.37ms
step:817/2330 train_time:50138ms step_avg:61.37ms
step:818/2330 train_time:50201ms step_avg:61.37ms
step:819/2330 train_time:50263ms step_avg:61.37ms
step:820/2330 train_time:50327ms step_avg:61.37ms
step:821/2330 train_time:50389ms step_avg:61.38ms
step:822/2330 train_time:50454ms step_avg:61.38ms
step:823/2330 train_time:50514ms step_avg:61.38ms
step:824/2330 train_time:50578ms step_avg:61.38ms
step:825/2330 train_time:50638ms step_avg:61.38ms
step:826/2330 train_time:50702ms step_avg:61.38ms
step:827/2330 train_time:50762ms step_avg:61.38ms
step:828/2330 train_time:50825ms step_avg:61.38ms
step:829/2330 train_time:50886ms step_avg:61.38ms
step:830/2330 train_time:50950ms step_avg:61.39ms
step:831/2330 train_time:51011ms step_avg:61.39ms
step:832/2330 train_time:51075ms step_avg:61.39ms
step:833/2330 train_time:51136ms step_avg:61.39ms
step:834/2330 train_time:51199ms step_avg:61.39ms
step:835/2330 train_time:51260ms step_avg:61.39ms
step:836/2330 train_time:51324ms step_avg:61.39ms
step:837/2330 train_time:51385ms step_avg:61.39ms
step:838/2330 train_time:51449ms step_avg:61.39ms
step:839/2330 train_time:51510ms step_avg:61.39ms
step:840/2330 train_time:51574ms step_avg:61.40ms
step:841/2330 train_time:51635ms step_avg:61.40ms
step:842/2330 train_time:51699ms step_avg:61.40ms
step:843/2330 train_time:51759ms step_avg:61.40ms
step:844/2330 train_time:51822ms step_avg:61.40ms
step:845/2330 train_time:51883ms step_avg:61.40ms
step:846/2330 train_time:51948ms step_avg:61.40ms
step:847/2330 train_time:52010ms step_avg:61.40ms
step:848/2330 train_time:52074ms step_avg:61.41ms
step:849/2330 train_time:52134ms step_avg:61.41ms
step:850/2330 train_time:52198ms step_avg:61.41ms
step:851/2330 train_time:52259ms step_avg:61.41ms
step:852/2330 train_time:52322ms step_avg:61.41ms
step:853/2330 train_time:52383ms step_avg:61.41ms
step:854/2330 train_time:52447ms step_avg:61.41ms
step:855/2330 train_time:52509ms step_avg:61.41ms
step:856/2330 train_time:52573ms step_avg:61.42ms
step:857/2330 train_time:52634ms step_avg:61.42ms
step:858/2330 train_time:52697ms step_avg:61.42ms
step:859/2330 train_time:52758ms step_avg:61.42ms
step:860/2330 train_time:52822ms step_avg:61.42ms
step:861/2330 train_time:52882ms step_avg:61.42ms
step:862/2330 train_time:52946ms step_avg:61.42ms
step:863/2330 train_time:53008ms step_avg:61.42ms
step:864/2330 train_time:53072ms step_avg:61.43ms
step:865/2330 train_time:53132ms step_avg:61.42ms
step:866/2330 train_time:53196ms step_avg:61.43ms
step:867/2330 train_time:53257ms step_avg:61.43ms
step:868/2330 train_time:53321ms step_avg:61.43ms
step:869/2330 train_time:53381ms step_avg:61.43ms
step:870/2330 train_time:53445ms step_avg:61.43ms
step:871/2330 train_time:53507ms step_avg:61.43ms
step:872/2330 train_time:53572ms step_avg:61.44ms
step:873/2330 train_time:53633ms step_avg:61.43ms
step:874/2330 train_time:53696ms step_avg:61.44ms
step:875/2330 train_time:53756ms step_avg:61.44ms
step:876/2330 train_time:53820ms step_avg:61.44ms
step:877/2330 train_time:53880ms step_avg:61.44ms
step:878/2330 train_time:53944ms step_avg:61.44ms
step:879/2330 train_time:54005ms step_avg:61.44ms
step:880/2330 train_time:54069ms step_avg:61.44ms
step:881/2330 train_time:54130ms step_avg:61.44ms
step:882/2330 train_time:54194ms step_avg:61.44ms
step:883/2330 train_time:54254ms step_avg:61.44ms
step:884/2330 train_time:54319ms step_avg:61.45ms
step:885/2330 train_time:54379ms step_avg:61.45ms
step:886/2330 train_time:54442ms step_avg:61.45ms
step:887/2330 train_time:54503ms step_avg:61.45ms
step:888/2330 train_time:54568ms step_avg:61.45ms
step:889/2330 train_time:54629ms step_avg:61.45ms
step:890/2330 train_time:54693ms step_avg:61.45ms
step:891/2330 train_time:54753ms step_avg:61.45ms
step:892/2330 train_time:54817ms step_avg:61.45ms
step:893/2330 train_time:54877ms step_avg:61.45ms
step:894/2330 train_time:54941ms step_avg:61.46ms
step:895/2330 train_time:55001ms step_avg:61.45ms
step:896/2330 train_time:55065ms step_avg:61.46ms
step:897/2330 train_time:55126ms step_avg:61.46ms
step:898/2330 train_time:55191ms step_avg:61.46ms
step:899/2330 train_time:55252ms step_avg:61.46ms
step:900/2330 train_time:55316ms step_avg:61.46ms
step:901/2330 train_time:55377ms step_avg:61.46ms
step:902/2330 train_time:55441ms step_avg:61.46ms
step:903/2330 train_time:55501ms step_avg:61.46ms
step:904/2330 train_time:55565ms step_avg:61.47ms
step:905/2330 train_time:55626ms step_avg:61.47ms
step:906/2330 train_time:55690ms step_avg:61.47ms
step:907/2330 train_time:55752ms step_avg:61.47ms
step:908/2330 train_time:55816ms step_avg:61.47ms
step:909/2330 train_time:55876ms step_avg:61.47ms
step:910/2330 train_time:55940ms step_avg:61.47ms
step:911/2330 train_time:56000ms step_avg:61.47ms
step:912/2330 train_time:56063ms step_avg:61.47ms
step:913/2330 train_time:56124ms step_avg:61.47ms
step:914/2330 train_time:56188ms step_avg:61.48ms
step:915/2330 train_time:56250ms step_avg:61.48ms
step:916/2330 train_time:56315ms step_avg:61.48ms
step:917/2330 train_time:56375ms step_avg:61.48ms
step:918/2330 train_time:56439ms step_avg:61.48ms
step:919/2330 train_time:56499ms step_avg:61.48ms
step:920/2330 train_time:56562ms step_avg:61.48ms
step:921/2330 train_time:56623ms step_avg:61.48ms
step:922/2330 train_time:56687ms step_avg:61.48ms
step:923/2330 train_time:56749ms step_avg:61.48ms
step:924/2330 train_time:56814ms step_avg:61.49ms
step:925/2330 train_time:56874ms step_avg:61.49ms
step:926/2330 train_time:56938ms step_avg:61.49ms
step:927/2330 train_time:56998ms step_avg:61.49ms
step:928/2330 train_time:57061ms step_avg:61.49ms
step:929/2330 train_time:57122ms step_avg:61.49ms
step:930/2330 train_time:57186ms step_avg:61.49ms
step:931/2330 train_time:57248ms step_avg:61.49ms
step:932/2330 train_time:57312ms step_avg:61.49ms
step:933/2330 train_time:57373ms step_avg:61.49ms
step:934/2330 train_time:57436ms step_avg:61.49ms
step:935/2330 train_time:57497ms step_avg:61.49ms
step:936/2330 train_time:57561ms step_avg:61.50ms
step:937/2330 train_time:57621ms step_avg:61.50ms
step:938/2330 train_time:57685ms step_avg:61.50ms
step:939/2330 train_time:57746ms step_avg:61.50ms
step:940/2330 train_time:57810ms step_avg:61.50ms
step:941/2330 train_time:57871ms step_avg:61.50ms
step:942/2330 train_time:57935ms step_avg:61.50ms
step:943/2330 train_time:57995ms step_avg:61.50ms
step:944/2330 train_time:58058ms step_avg:61.50ms
step:945/2330 train_time:58119ms step_avg:61.50ms
step:946/2330 train_time:58182ms step_avg:61.50ms
step:947/2330 train_time:58244ms step_avg:61.50ms
step:948/2330 train_time:58309ms step_avg:61.51ms
step:949/2330 train_time:58369ms step_avg:61.51ms
step:950/2330 train_time:58433ms step_avg:61.51ms
step:951/2330 train_time:58493ms step_avg:61.51ms
step:952/2330 train_time:58556ms step_avg:61.51ms
step:953/2330 train_time:58618ms step_avg:61.51ms
step:954/2330 train_time:58681ms step_avg:61.51ms
step:955/2330 train_time:58741ms step_avg:61.51ms
step:956/2330 train_time:58806ms step_avg:61.51ms
step:957/2330 train_time:58868ms step_avg:61.51ms
step:958/2330 train_time:58932ms step_avg:61.52ms
step:959/2330 train_time:58993ms step_avg:61.52ms
step:960/2330 train_time:59057ms step_avg:61.52ms
step:961/2330 train_time:59118ms step_avg:61.52ms
step:962/2330 train_time:59181ms step_avg:61.52ms
step:963/2330 train_time:59242ms step_avg:61.52ms
step:964/2330 train_time:59307ms step_avg:61.52ms
step:965/2330 train_time:59368ms step_avg:61.52ms
step:966/2330 train_time:59432ms step_avg:61.52ms
step:967/2330 train_time:59492ms step_avg:61.52ms
step:968/2330 train_time:59556ms step_avg:61.52ms
step:969/2330 train_time:59616ms step_avg:61.52ms
step:970/2330 train_time:59680ms step_avg:61.53ms
step:971/2330 train_time:59740ms step_avg:61.52ms
step:972/2330 train_time:59803ms step_avg:61.53ms
step:973/2330 train_time:59864ms step_avg:61.52ms
step:974/2330 train_time:59929ms step_avg:61.53ms
step:975/2330 train_time:59990ms step_avg:61.53ms
step:976/2330 train_time:60053ms step_avg:61.53ms
step:977/2330 train_time:60113ms step_avg:61.53ms
step:978/2330 train_time:60178ms step_avg:61.53ms
step:979/2330 train_time:60239ms step_avg:61.53ms
step:980/2330 train_time:60302ms step_avg:61.53ms
step:981/2330 train_time:60363ms step_avg:61.53ms
step:982/2330 train_time:60427ms step_avg:61.53ms
step:983/2330 train_time:60488ms step_avg:61.53ms
step:984/2330 train_time:60552ms step_avg:61.54ms
step:985/2330 train_time:60613ms step_avg:61.54ms
step:986/2330 train_time:60677ms step_avg:61.54ms
step:987/2330 train_time:60737ms step_avg:61.54ms
step:988/2330 train_time:60801ms step_avg:61.54ms
step:989/2330 train_time:60861ms step_avg:61.54ms
step:990/2330 train_time:60925ms step_avg:61.54ms
step:991/2330 train_time:60986ms step_avg:61.54ms
step:992/2330 train_time:61051ms step_avg:61.54ms
step:993/2330 train_time:61112ms step_avg:61.54ms
step:994/2330 train_time:61176ms step_avg:61.55ms
step:995/2330 train_time:61237ms step_avg:61.54ms
step:996/2330 train_time:61301ms step_avg:61.55ms
step:997/2330 train_time:61362ms step_avg:61.55ms
step:998/2330 train_time:61426ms step_avg:61.55ms
step:999/2330 train_time:61488ms step_avg:61.55ms
step:1000/2330 train_time:61552ms step_avg:61.55ms
step:1000/2330 val_loss:4.2812 train_time:61617ms step_avg:61.62ms
step:1001/2330 train_time:61642ms step_avg:61.58ms
step:1002/2330 train_time:61678ms step_avg:61.56ms
step:1003/2330 train_time:61745ms step_avg:61.56ms
step:1004/2330 train_time:61810ms step_avg:61.56ms
step:1005/2330 train_time:61870ms step_avg:61.56ms
step:1006/2330 train_time:61934ms step_avg:61.56ms
step:1007/2330 train_time:61994ms step_avg:61.56ms
step:1008/2330 train_time:62057ms step_avg:61.56ms
step:1009/2330 train_time:62117ms step_avg:61.56ms
step:1010/2330 train_time:62180ms step_avg:61.56ms
step:1011/2330 train_time:62241ms step_avg:61.56ms
step:1012/2330 train_time:62304ms step_avg:61.56ms
step:1013/2330 train_time:62363ms step_avg:61.56ms
step:1014/2330 train_time:62426ms step_avg:61.56ms
step:1015/2330 train_time:62486ms step_avg:61.56ms
step:1016/2330 train_time:62553ms step_avg:61.57ms
step:1017/2330 train_time:62617ms step_avg:61.57ms
step:1018/2330 train_time:62682ms step_avg:61.57ms
step:1019/2330 train_time:62743ms step_avg:61.57ms
step:1020/2330 train_time:62807ms step_avg:61.58ms
step:1021/2330 train_time:62868ms step_avg:61.57ms
step:1022/2330 train_time:62932ms step_avg:61.58ms
step:1023/2330 train_time:62993ms step_avg:61.58ms
step:1024/2330 train_time:63056ms step_avg:61.58ms
step:1025/2330 train_time:63116ms step_avg:61.58ms
step:1026/2330 train_time:63179ms step_avg:61.58ms
step:1027/2330 train_time:63239ms step_avg:61.58ms
step:1028/2330 train_time:63302ms step_avg:61.58ms
step:1029/2330 train_time:63362ms step_avg:61.58ms
step:1030/2330 train_time:63425ms step_avg:61.58ms
step:1031/2330 train_time:63485ms step_avg:61.58ms
step:1032/2330 train_time:63549ms step_avg:61.58ms
step:1033/2330 train_time:63612ms step_avg:61.58ms
step:1034/2330 train_time:63676ms step_avg:61.58ms
step:1035/2330 train_time:63738ms step_avg:61.58ms
step:1036/2330 train_time:63802ms step_avg:61.58ms
step:1037/2330 train_time:63862ms step_avg:61.58ms
step:1038/2330 train_time:63926ms step_avg:61.59ms
step:1039/2330 train_time:63986ms step_avg:61.58ms
step:1040/2330 train_time:64050ms step_avg:61.59ms
step:1041/2330 train_time:64112ms step_avg:61.59ms
step:1042/2330 train_time:64176ms step_avg:61.59ms
step:1043/2330 train_time:64236ms step_avg:61.59ms
step:1044/2330 train_time:64299ms step_avg:61.59ms
step:1045/2330 train_time:64359ms step_avg:61.59ms
step:1046/2330 train_time:64423ms step_avg:61.59ms
step:1047/2330 train_time:64483ms step_avg:61.59ms
step:1048/2330 train_time:64546ms step_avg:61.59ms
step:1049/2330 train_time:64608ms step_avg:61.59ms
step:1050/2330 train_time:64673ms step_avg:61.59ms
step:1051/2330 train_time:64734ms step_avg:61.59ms
step:1052/2330 train_time:64798ms step_avg:61.59ms
step:1053/2330 train_time:64858ms step_avg:61.59ms
step:1054/2330 train_time:64921ms step_avg:61.60ms
step:1055/2330 train_time:64982ms step_avg:61.59ms
step:1056/2330 train_time:65046ms step_avg:61.60ms
step:1057/2330 train_time:65106ms step_avg:61.60ms
step:1058/2330 train_time:65170ms step_avg:61.60ms
step:1059/2330 train_time:65231ms step_avg:61.60ms
step:1060/2330 train_time:65295ms step_avg:61.60ms
step:1061/2330 train_time:65355ms step_avg:61.60ms
step:1062/2330 train_time:65418ms step_avg:61.60ms
step:1063/2330 train_time:65478ms step_avg:61.60ms
step:1064/2330 train_time:65542ms step_avg:61.60ms
step:1065/2330 train_time:65602ms step_avg:61.60ms
step:1066/2330 train_time:65666ms step_avg:61.60ms
step:1067/2330 train_time:65727ms step_avg:61.60ms
step:1068/2330 train_time:65792ms step_avg:61.60ms
step:1069/2330 train_time:65854ms step_avg:61.60ms
step:1070/2330 train_time:65917ms step_avg:61.61ms
step:1071/2330 train_time:65978ms step_avg:61.60ms
step:1072/2330 train_time:66041ms step_avg:61.61ms
step:1073/2330 train_time:66102ms step_avg:61.60ms
step:1074/2330 train_time:66166ms step_avg:61.61ms
step:1075/2330 train_time:66225ms step_avg:61.61ms
step:1076/2330 train_time:66289ms step_avg:61.61ms
step:1077/2330 train_time:66351ms step_avg:61.61ms
step:1078/2330 train_time:66415ms step_avg:61.61ms
step:1079/2330 train_time:66475ms step_avg:61.61ms
step:1080/2330 train_time:66539ms step_avg:61.61ms
step:1081/2330 train_time:66600ms step_avg:61.61ms
step:1082/2330 train_time:66663ms step_avg:61.61ms
step:1083/2330 train_time:66724ms step_avg:61.61ms
step:1084/2330 train_time:66787ms step_avg:61.61ms
step:1085/2330 train_time:66848ms step_avg:61.61ms
step:1086/2330 train_time:66913ms step_avg:61.61ms
step:1087/2330 train_time:66974ms step_avg:61.61ms
step:1088/2330 train_time:67038ms step_avg:61.62ms
step:1089/2330 train_time:67098ms step_avg:61.61ms
step:1090/2330 train_time:67163ms step_avg:61.62ms
step:1091/2330 train_time:67223ms step_avg:61.62ms
step:1092/2330 train_time:67286ms step_avg:61.62ms
step:1093/2330 train_time:67346ms step_avg:61.62ms
step:1094/2330 train_time:67411ms step_avg:61.62ms
step:1095/2330 train_time:67472ms step_avg:61.62ms
step:1096/2330 train_time:67537ms step_avg:61.62ms
step:1097/2330 train_time:67597ms step_avg:61.62ms
step:1098/2330 train_time:67661ms step_avg:61.62ms
step:1099/2330 train_time:67722ms step_avg:61.62ms
step:1100/2330 train_time:67786ms step_avg:61.62ms
step:1101/2330 train_time:67846ms step_avg:61.62ms
step:1102/2330 train_time:67911ms step_avg:61.63ms
step:1103/2330 train_time:67972ms step_avg:61.62ms
step:1104/2330 train_time:68037ms step_avg:61.63ms
step:1105/2330 train_time:68097ms step_avg:61.63ms
step:1106/2330 train_time:68160ms step_avg:61.63ms
step:1107/2330 train_time:68222ms step_avg:61.63ms
step:1108/2330 train_time:68285ms step_avg:61.63ms
step:1109/2330 train_time:68346ms step_avg:61.63ms
step:1110/2330 train_time:68409ms step_avg:61.63ms
step:1111/2330 train_time:68471ms step_avg:61.63ms
step:1112/2330 train_time:68536ms step_avg:61.63ms
step:1113/2330 train_time:68596ms step_avg:61.63ms
step:1114/2330 train_time:68660ms step_avg:61.63ms
step:1115/2330 train_time:68721ms step_avg:61.63ms
step:1116/2330 train_time:68785ms step_avg:61.64ms
step:1117/2330 train_time:68846ms step_avg:61.63ms
step:1118/2330 train_time:68910ms step_avg:61.64ms
step:1119/2330 train_time:68971ms step_avg:61.64ms
step:1120/2330 train_time:69035ms step_avg:61.64ms
step:1121/2330 train_time:69096ms step_avg:61.64ms
step:1122/2330 train_time:69159ms step_avg:61.64ms
step:1123/2330 train_time:69219ms step_avg:61.64ms
step:1124/2330 train_time:69283ms step_avg:61.64ms
step:1125/2330 train_time:69343ms step_avg:61.64ms
step:1126/2330 train_time:69406ms step_avg:61.64ms
step:1127/2330 train_time:69467ms step_avg:61.64ms
step:1128/2330 train_time:69532ms step_avg:61.64ms
step:1129/2330 train_time:69593ms step_avg:61.64ms
step:1130/2330 train_time:69657ms step_avg:61.64ms
step:1131/2330 train_time:69717ms step_avg:61.64ms
step:1132/2330 train_time:69781ms step_avg:61.64ms
step:1133/2330 train_time:69842ms step_avg:61.64ms
step:1134/2330 train_time:69906ms step_avg:61.65ms
step:1135/2330 train_time:69967ms step_avg:61.64ms
step:1136/2330 train_time:70031ms step_avg:61.65ms
step:1137/2330 train_time:70093ms step_avg:61.65ms
step:1138/2330 train_time:70156ms step_avg:61.65ms
step:1139/2330 train_time:70217ms step_avg:61.65ms
step:1140/2330 train_time:70280ms step_avg:61.65ms
step:1141/2330 train_time:70341ms step_avg:61.65ms
step:1142/2330 train_time:70404ms step_avg:61.65ms
step:1143/2330 train_time:70464ms step_avg:61.65ms
step:1144/2330 train_time:70527ms step_avg:61.65ms
step:1145/2330 train_time:70588ms step_avg:61.65ms
step:1146/2330 train_time:70653ms step_avg:61.65ms
step:1147/2330 train_time:70714ms step_avg:61.65ms
step:1148/2330 train_time:70778ms step_avg:61.65ms
step:1149/2330 train_time:70839ms step_avg:61.65ms
step:1150/2330 train_time:70903ms step_avg:61.65ms
step:1151/2330 train_time:70963ms step_avg:61.65ms
step:1152/2330 train_time:71027ms step_avg:61.66ms
step:1153/2330 train_time:71088ms step_avg:61.65ms
step:1154/2330 train_time:71152ms step_avg:61.66ms
step:1155/2330 train_time:71213ms step_avg:61.66ms
step:1156/2330 train_time:71277ms step_avg:61.66ms
step:1157/2330 train_time:71339ms step_avg:61.66ms
step:1158/2330 train_time:71402ms step_avg:61.66ms
step:1159/2330 train_time:71462ms step_avg:61.66ms
step:1160/2330 train_time:71526ms step_avg:61.66ms
step:1161/2330 train_time:71586ms step_avg:61.66ms
step:1162/2330 train_time:71650ms step_avg:61.66ms
step:1163/2330 train_time:71712ms step_avg:61.66ms
step:1164/2330 train_time:71776ms step_avg:61.66ms
step:1165/2330 train_time:71837ms step_avg:61.66ms
step:1166/2330 train_time:71901ms step_avg:61.66ms
step:1167/2330 train_time:71962ms step_avg:61.66ms
step:1168/2330 train_time:72025ms step_avg:61.67ms
step:1169/2330 train_time:72085ms step_avg:61.66ms
step:1170/2330 train_time:72148ms step_avg:61.67ms
step:1171/2330 train_time:72209ms step_avg:61.66ms
step:1172/2330 train_time:72274ms step_avg:61.67ms
step:1173/2330 train_time:72335ms step_avg:61.67ms
step:1174/2330 train_time:72399ms step_avg:61.67ms
step:1175/2330 train_time:72459ms step_avg:61.67ms
step:1176/2330 train_time:72523ms step_avg:61.67ms
step:1177/2330 train_time:72583ms step_avg:61.67ms
step:1178/2330 train_time:72647ms step_avg:61.67ms
step:1179/2330 train_time:72707ms step_avg:61.67ms
step:1180/2330 train_time:72771ms step_avg:61.67ms
step:1181/2330 train_time:72833ms step_avg:61.67ms
step:1182/2330 train_time:72897ms step_avg:61.67ms
step:1183/2330 train_time:72957ms step_avg:61.67ms
step:1184/2330 train_time:73021ms step_avg:61.67ms
step:1185/2330 train_time:73082ms step_avg:61.67ms
step:1186/2330 train_time:73145ms step_avg:61.67ms
step:1187/2330 train_time:73206ms step_avg:61.67ms
step:1188/2330 train_time:73271ms step_avg:61.68ms
step:1189/2330 train_time:73333ms step_avg:61.68ms
step:1190/2330 train_time:73397ms step_avg:61.68ms
step:1191/2330 train_time:73457ms step_avg:61.68ms
step:1192/2330 train_time:73520ms step_avg:61.68ms
step:1193/2330 train_time:73581ms step_avg:61.68ms
step:1194/2330 train_time:73645ms step_avg:61.68ms
step:1195/2330 train_time:73705ms step_avg:61.68ms
step:1196/2330 train_time:73769ms step_avg:61.68ms
step:1197/2330 train_time:73830ms step_avg:61.68ms
step:1198/2330 train_time:73895ms step_avg:61.68ms
step:1199/2330 train_time:73956ms step_avg:61.68ms
step:1200/2330 train_time:74019ms step_avg:61.68ms
step:1201/2330 train_time:74080ms step_avg:61.68ms
step:1202/2330 train_time:74144ms step_avg:61.68ms
step:1203/2330 train_time:74204ms step_avg:61.68ms
step:1204/2330 train_time:74267ms step_avg:61.68ms
step:1205/2330 train_time:74328ms step_avg:61.68ms
step:1206/2330 train_time:74393ms step_avg:61.69ms
step:1207/2330 train_time:74454ms step_avg:61.69ms
step:1208/2330 train_time:74518ms step_avg:61.69ms
step:1209/2330 train_time:74578ms step_avg:61.69ms
step:1210/2330 train_time:74642ms step_avg:61.69ms
step:1211/2330 train_time:74702ms step_avg:61.69ms
step:1212/2330 train_time:74765ms step_avg:61.69ms
step:1213/2330 train_time:74826ms step_avg:61.69ms
step:1214/2330 train_time:74890ms step_avg:61.69ms
step:1215/2330 train_time:74952ms step_avg:61.69ms
step:1216/2330 train_time:75017ms step_avg:61.69ms
step:1217/2330 train_time:75078ms step_avg:61.69ms
step:1218/2330 train_time:75142ms step_avg:61.69ms
step:1219/2330 train_time:75202ms step_avg:61.69ms
step:1220/2330 train_time:75266ms step_avg:61.69ms
step:1221/2330 train_time:75326ms step_avg:61.69ms
step:1222/2330 train_time:75390ms step_avg:61.69ms
step:1223/2330 train_time:75451ms step_avg:61.69ms
step:1224/2330 train_time:75515ms step_avg:61.70ms
step:1225/2330 train_time:75576ms step_avg:61.70ms
step:1226/2330 train_time:75640ms step_avg:61.70ms
step:1227/2330 train_time:75701ms step_avg:61.70ms
step:1228/2330 train_time:75765ms step_avg:61.70ms
step:1229/2330 train_time:75825ms step_avg:61.70ms
step:1230/2330 train_time:75888ms step_avg:61.70ms
step:1231/2330 train_time:75950ms step_avg:61.70ms
step:1232/2330 train_time:76014ms step_avg:61.70ms
step:1233/2330 train_time:76075ms step_avg:61.70ms
step:1234/2330 train_time:76139ms step_avg:61.70ms
step:1235/2330 train_time:76200ms step_avg:61.70ms
step:1236/2330 train_time:76263ms step_avg:61.70ms
step:1237/2330 train_time:76323ms step_avg:61.70ms
step:1238/2330 train_time:76387ms step_avg:61.70ms
step:1239/2330 train_time:76448ms step_avg:61.70ms
step:1240/2330 train_time:76512ms step_avg:61.70ms
step:1241/2330 train_time:76574ms step_avg:61.70ms
step:1242/2330 train_time:76638ms step_avg:61.71ms
step:1243/2330 train_time:76700ms step_avg:61.71ms
step:1244/2330 train_time:76763ms step_avg:61.71ms
step:1245/2330 train_time:76824ms step_avg:61.71ms
step:1246/2330 train_time:76887ms step_avg:61.71ms
step:1247/2330 train_time:76948ms step_avg:61.71ms
step:1248/2330 train_time:77013ms step_avg:61.71ms
step:1249/2330 train_time:77074ms step_avg:61.71ms
step:1250/2330 train_time:77139ms step_avg:61.71ms
step:1250/2330 val_loss:4.1662 train_time:77204ms step_avg:61.76ms
step:1251/2330 train_time:77230ms step_avg:61.73ms
step:1252/2330 train_time:77267ms step_avg:61.71ms
step:1253/2330 train_time:77334ms step_avg:61.72ms
step:1254/2330 train_time:77400ms step_avg:61.72ms
step:1255/2330 train_time:77462ms step_avg:61.72ms
step:1256/2330 train_time:77525ms step_avg:61.72ms
step:1257/2330 train_time:77584ms step_avg:61.72ms
step:1258/2330 train_time:77648ms step_avg:61.72ms
step:1259/2330 train_time:77708ms step_avg:61.72ms
step:1260/2330 train_time:77770ms step_avg:61.72ms
step:1261/2330 train_time:77831ms step_avg:61.72ms
step:1262/2330 train_time:77894ms step_avg:61.72ms
step:1263/2330 train_time:77954ms step_avg:61.72ms
step:1264/2330 train_time:78016ms step_avg:61.72ms
step:1265/2330 train_time:78076ms step_avg:61.72ms
step:1266/2330 train_time:78140ms step_avg:61.72ms
step:1267/2330 train_time:78201ms step_avg:61.72ms
step:1268/2330 train_time:78266ms step_avg:61.72ms
step:1269/2330 train_time:78328ms step_avg:61.72ms
step:1270/2330 train_time:78394ms step_avg:61.73ms
step:1271/2330 train_time:78455ms step_avg:61.73ms
step:1272/2330 train_time:78519ms step_avg:61.73ms
step:1273/2330 train_time:78580ms step_avg:61.73ms
step:1274/2330 train_time:78643ms step_avg:61.73ms
step:1275/2330 train_time:78704ms step_avg:61.73ms
step:1276/2330 train_time:78767ms step_avg:61.73ms
step:1277/2330 train_time:78827ms step_avg:61.73ms
step:1278/2330 train_time:78890ms step_avg:61.73ms
step:1279/2330 train_time:78950ms step_avg:61.73ms
step:1280/2330 train_time:79014ms step_avg:61.73ms
step:1281/2330 train_time:79075ms step_avg:61.73ms
step:1282/2330 train_time:79139ms step_avg:61.73ms
step:1283/2330 train_time:79200ms step_avg:61.73ms
step:1284/2330 train_time:79265ms step_avg:61.73ms
step:1285/2330 train_time:79327ms step_avg:61.73ms
step:1286/2330 train_time:79391ms step_avg:61.73ms
step:1287/2330 train_time:79453ms step_avg:61.74ms
step:1288/2330 train_time:79518ms step_avg:61.74ms
step:1289/2330 train_time:79579ms step_avg:61.74ms
step:1290/2330 train_time:79642ms step_avg:61.74ms
step:1291/2330 train_time:79703ms step_avg:61.74ms
step:1292/2330 train_time:79766ms step_avg:61.74ms
step:1293/2330 train_time:79826ms step_avg:61.74ms
step:1294/2330 train_time:79889ms step_avg:61.74ms
step:1295/2330 train_time:79949ms step_avg:61.74ms
step:1296/2330 train_time:80014ms step_avg:61.74ms
step:1297/2330 train_time:80075ms step_avg:61.74ms
step:1298/2330 train_time:80139ms step_avg:61.74ms
step:1299/2330 train_time:80200ms step_avg:61.74ms
step:1300/2330 train_time:80264ms step_avg:61.74ms
step:1301/2330 train_time:80324ms step_avg:61.74ms
step:1302/2330 train_time:80388ms step_avg:61.74ms
step:1303/2330 train_time:80450ms step_avg:61.74ms
step:1304/2330 train_time:80514ms step_avg:61.74ms
step:1305/2330 train_time:80576ms step_avg:61.74ms
step:1306/2330 train_time:80640ms step_avg:61.75ms
step:1307/2330 train_time:80701ms step_avg:61.75ms
step:1308/2330 train_time:80765ms step_avg:61.75ms
step:1309/2330 train_time:80826ms step_avg:61.75ms
step:1310/2330 train_time:80889ms step_avg:61.75ms
step:1311/2330 train_time:80949ms step_avg:61.75ms
step:1312/2330 train_time:81013ms step_avg:61.75ms
step:1313/2330 train_time:81074ms step_avg:61.75ms
step:1314/2330 train_time:81138ms step_avg:61.75ms
step:1315/2330 train_time:81198ms step_avg:61.75ms
step:1316/2330 train_time:81262ms step_avg:61.75ms
step:1317/2330 train_time:81322ms step_avg:61.75ms
step:1318/2330 train_time:81386ms step_avg:61.75ms
step:1319/2330 train_time:81446ms step_avg:61.75ms
step:1320/2330 train_time:81511ms step_avg:61.75ms
step:1321/2330 train_time:81572ms step_avg:61.75ms
step:1322/2330 train_time:81637ms step_avg:61.75ms
step:1323/2330 train_time:81698ms step_avg:61.75ms
step:1324/2330 train_time:81762ms step_avg:61.75ms
step:1325/2330 train_time:81822ms step_avg:61.75ms
step:1326/2330 train_time:81886ms step_avg:61.75ms
step:1327/2330 train_time:81946ms step_avg:61.75ms
step:1328/2330 train_time:82009ms step_avg:61.75ms
step:1329/2330 train_time:82070ms step_avg:61.75ms
step:1330/2330 train_time:82134ms step_avg:61.75ms
step:1331/2330 train_time:82196ms step_avg:61.75ms
step:1332/2330 train_time:82260ms step_avg:61.76ms
step:1333/2330 train_time:82320ms step_avg:61.76ms
step:1334/2330 train_time:82384ms step_avg:61.76ms
step:1335/2330 train_time:82445ms step_avg:61.76ms
step:1336/2330 train_time:82508ms step_avg:61.76ms
step:1337/2330 train_time:82569ms step_avg:61.76ms
step:1338/2330 train_time:82634ms step_avg:61.76ms
step:1339/2330 train_time:82696ms step_avg:61.76ms
step:1340/2330 train_time:82760ms step_avg:61.76ms
step:1341/2330 train_time:82820ms step_avg:61.76ms
step:1342/2330 train_time:82884ms step_avg:61.76ms
step:1343/2330 train_time:82944ms step_avg:61.76ms
step:1344/2330 train_time:83007ms step_avg:61.76ms
step:1345/2330 train_time:83067ms step_avg:61.76ms
step:1346/2330 train_time:83132ms step_avg:61.76ms
step:1347/2330 train_time:83192ms step_avg:61.76ms
step:1348/2330 train_time:83257ms step_avg:61.76ms
step:1349/2330 train_time:83318ms step_avg:61.76ms
step:1350/2330 train_time:83381ms step_avg:61.76ms
step:1351/2330 train_time:83441ms step_avg:61.76ms
step:1352/2330 train_time:83505ms step_avg:61.76ms
step:1353/2330 train_time:83566ms step_avg:61.76ms
step:1354/2330 train_time:83629ms step_avg:61.76ms
step:1355/2330 train_time:83691ms step_avg:61.76ms
step:1356/2330 train_time:83755ms step_avg:61.77ms
step:1357/2330 train_time:83816ms step_avg:61.77ms
step:1358/2330 train_time:83879ms step_avg:61.77ms
step:1359/2330 train_time:83940ms step_avg:61.77ms
step:1360/2330 train_time:84003ms step_avg:61.77ms
step:1361/2330 train_time:84063ms step_avg:61.77ms
step:1362/2330 train_time:84126ms step_avg:61.77ms
step:1363/2330 train_time:84187ms step_avg:61.77ms
step:1364/2330 train_time:84252ms step_avg:61.77ms
step:1365/2330 train_time:84313ms step_avg:61.77ms
step:1366/2330 train_time:84377ms step_avg:61.77ms
step:1367/2330 train_time:84437ms step_avg:61.77ms
step:1368/2330 train_time:84501ms step_avg:61.77ms
step:1369/2330 train_time:84563ms step_avg:61.77ms
step:1370/2330 train_time:84626ms step_avg:61.77ms
step:1371/2330 train_time:84687ms step_avg:61.77ms
step:1372/2330 train_time:84751ms step_avg:61.77ms
step:1373/2330 train_time:84813ms step_avg:61.77ms
step:1374/2330 train_time:84876ms step_avg:61.77ms
step:1375/2330 train_time:84937ms step_avg:61.77ms
step:1376/2330 train_time:85001ms step_avg:61.77ms
step:1377/2330 train_time:85061ms step_avg:61.77ms
step:1378/2330 train_time:85125ms step_avg:61.77ms
step:1379/2330 train_time:85185ms step_avg:61.77ms
step:1380/2330 train_time:85248ms step_avg:61.77ms
step:1381/2330 train_time:85309ms step_avg:61.77ms
step:1382/2330 train_time:85373ms step_avg:61.78ms
step:1383/2330 train_time:85435ms step_avg:61.77ms
step:1384/2330 train_time:85499ms step_avg:61.78ms
step:1385/2330 train_time:85560ms step_avg:61.78ms
step:1386/2330 train_time:85624ms step_avg:61.78ms
step:1387/2330 train_time:85684ms step_avg:61.78ms
step:1388/2330 train_time:85748ms step_avg:61.78ms
step:1389/2330 train_time:85810ms step_avg:61.78ms
step:1390/2330 train_time:85874ms step_avg:61.78ms
step:1391/2330 train_time:85936ms step_avg:61.78ms
step:1392/2330 train_time:86000ms step_avg:61.78ms
step:1393/2330 train_time:86060ms step_avg:61.78ms
step:1394/2330 train_time:86124ms step_avg:61.78ms
step:1395/2330 train_time:86184ms step_avg:61.78ms
step:1396/2330 train_time:86247ms step_avg:61.78ms
step:1397/2330 train_time:86308ms step_avg:61.78ms
step:1398/2330 train_time:86372ms step_avg:61.78ms
step:1399/2330 train_time:86433ms step_avg:61.78ms
step:1400/2330 train_time:86497ms step_avg:61.78ms
step:1401/2330 train_time:86558ms step_avg:61.78ms
step:1402/2330 train_time:86622ms step_avg:61.78ms
step:1403/2330 train_time:86682ms step_avg:61.78ms
step:1404/2330 train_time:86746ms step_avg:61.78ms
step:1405/2330 train_time:86806ms step_avg:61.78ms
step:1406/2330 train_time:86869ms step_avg:61.78ms
step:1407/2330 train_time:86931ms step_avg:61.78ms
step:1408/2330 train_time:86995ms step_avg:61.79ms
step:1409/2330 train_time:87056ms step_avg:61.79ms
step:1410/2330 train_time:87120ms step_avg:61.79ms
step:1411/2330 train_time:87181ms step_avg:61.79ms
step:1412/2330 train_time:87244ms step_avg:61.79ms
step:1413/2330 train_time:87305ms step_avg:61.79ms
step:1414/2330 train_time:87368ms step_avg:61.79ms
step:1415/2330 train_time:87428ms step_avg:61.79ms
step:1416/2330 train_time:87493ms step_avg:61.79ms
step:1417/2330 train_time:87554ms step_avg:61.79ms
step:1418/2330 train_time:87618ms step_avg:61.79ms
step:1419/2330 train_time:87679ms step_avg:61.79ms
step:1420/2330 train_time:87742ms step_avg:61.79ms
step:1421/2330 train_time:87802ms step_avg:61.79ms
step:1422/2330 train_time:87867ms step_avg:61.79ms
step:1423/2330 train_time:87927ms step_avg:61.79ms
step:1424/2330 train_time:87992ms step_avg:61.79ms
step:1425/2330 train_time:88054ms step_avg:61.79ms
step:1426/2330 train_time:88118ms step_avg:61.79ms
step:1427/2330 train_time:88179ms step_avg:61.79ms
step:1428/2330 train_time:88242ms step_avg:61.79ms
step:1429/2330 train_time:88302ms step_avg:61.79ms
step:1430/2330 train_time:88366ms step_avg:61.79ms
step:1431/2330 train_time:88426ms step_avg:61.79ms
step:1432/2330 train_time:88490ms step_avg:61.79ms
step:1433/2330 train_time:88552ms step_avg:61.79ms
step:1434/2330 train_time:88616ms step_avg:61.80ms
step:1435/2330 train_time:88677ms step_avg:61.80ms
step:1436/2330 train_time:88740ms step_avg:61.80ms
step:1437/2330 train_time:88801ms step_avg:61.80ms
step:1438/2330 train_time:88865ms step_avg:61.80ms
step:1439/2330 train_time:88925ms step_avg:61.80ms
step:1440/2330 train_time:88989ms step_avg:61.80ms
step:1441/2330 train_time:89049ms step_avg:61.80ms
step:1442/2330 train_time:89114ms step_avg:61.80ms
step:1443/2330 train_time:89176ms step_avg:61.80ms
step:1444/2330 train_time:89240ms step_avg:61.80ms
step:1445/2330 train_time:89300ms step_avg:61.80ms
step:1446/2330 train_time:89364ms step_avg:61.80ms
step:1447/2330 train_time:89424ms step_avg:61.80ms
step:1448/2330 train_time:89487ms step_avg:61.80ms
step:1449/2330 train_time:89548ms step_avg:61.80ms
step:1450/2330 train_time:89613ms step_avg:61.80ms
step:1451/2330 train_time:89674ms step_avg:61.80ms
step:1452/2330 train_time:89738ms step_avg:61.80ms
step:1453/2330 train_time:89799ms step_avg:61.80ms
step:1454/2330 train_time:89863ms step_avg:61.80ms
step:1455/2330 train_time:89923ms step_avg:61.80ms
step:1456/2330 train_time:89987ms step_avg:61.80ms
step:1457/2330 train_time:90047ms step_avg:61.80ms
step:1458/2330 train_time:90111ms step_avg:61.80ms
step:1459/2330 train_time:90172ms step_avg:61.80ms
step:1460/2330 train_time:90237ms step_avg:61.81ms
step:1461/2330 train_time:90298ms step_avg:61.81ms
step:1462/2330 train_time:90362ms step_avg:61.81ms
step:1463/2330 train_time:90422ms step_avg:61.81ms
step:1464/2330 train_time:90486ms step_avg:61.81ms
step:1465/2330 train_time:90546ms step_avg:61.81ms
step:1466/2330 train_time:90610ms step_avg:61.81ms
step:1467/2330 train_time:90671ms step_avg:61.81ms
step:1468/2330 train_time:90736ms step_avg:61.81ms
step:1469/2330 train_time:90797ms step_avg:61.81ms
step:1470/2330 train_time:90861ms step_avg:61.81ms
step:1471/2330 train_time:90921ms step_avg:61.81ms
step:1472/2330 train_time:90984ms step_avg:61.81ms
step:1473/2330 train_time:91045ms step_avg:61.81ms
step:1474/2330 train_time:91108ms step_avg:61.81ms
step:1475/2330 train_time:91169ms step_avg:61.81ms
step:1476/2330 train_time:91234ms step_avg:61.81ms
step:1477/2330 train_time:91295ms step_avg:61.81ms
step:1478/2330 train_time:91359ms step_avg:61.81ms
step:1479/2330 train_time:91419ms step_avg:61.81ms
step:1480/2330 train_time:91483ms step_avg:61.81ms
step:1481/2330 train_time:91545ms step_avg:61.81ms
step:1482/2330 train_time:91608ms step_avg:61.81ms
step:1483/2330 train_time:91669ms step_avg:61.81ms
step:1484/2330 train_time:91734ms step_avg:61.82ms
step:1485/2330 train_time:91795ms step_avg:61.81ms
step:1486/2330 train_time:91859ms step_avg:61.82ms
step:1487/2330 train_time:91920ms step_avg:61.82ms
step:1488/2330 train_time:91983ms step_avg:61.82ms
step:1489/2330 train_time:92044ms step_avg:61.82ms
step:1490/2330 train_time:92107ms step_avg:61.82ms
step:1491/2330 train_time:92168ms step_avg:61.82ms
step:1492/2330 train_time:92233ms step_avg:61.82ms
step:1493/2330 train_time:92294ms step_avg:61.82ms
step:1494/2330 train_time:92359ms step_avg:61.82ms
step:1495/2330 train_time:92419ms step_avg:61.82ms
step:1496/2330 train_time:92482ms step_avg:61.82ms
step:1497/2330 train_time:92543ms step_avg:61.82ms
step:1498/2330 train_time:92606ms step_avg:61.82ms
step:1499/2330 train_time:92668ms step_avg:61.82ms
step:1500/2330 train_time:92732ms step_avg:61.82ms
step:1500/2330 val_loss:4.0736 train_time:92798ms step_avg:61.87ms
step:1501/2330 train_time:92821ms step_avg:61.84ms
step:1502/2330 train_time:92862ms step_avg:61.83ms
step:1503/2330 train_time:92928ms step_avg:61.83ms
step:1504/2330 train_time:92995ms step_avg:61.83ms
step:1505/2330 train_time:93055ms step_avg:61.83ms
step:1506/2330 train_time:93119ms step_avg:61.83ms
step:1507/2330 train_time:93179ms step_avg:61.83ms
step:1508/2330 train_time:93242ms step_avg:61.83ms
step:1509/2330 train_time:93302ms step_avg:61.83ms
step:1510/2330 train_time:93365ms step_avg:61.83ms
step:1511/2330 train_time:93424ms step_avg:61.83ms
step:1512/2330 train_time:93487ms step_avg:61.83ms
step:1513/2330 train_time:93547ms step_avg:61.83ms
step:1514/2330 train_time:93610ms step_avg:61.83ms
step:1515/2330 train_time:93670ms step_avg:61.83ms
step:1516/2330 train_time:93735ms step_avg:61.83ms
step:1517/2330 train_time:93797ms step_avg:61.83ms
step:1518/2330 train_time:93862ms step_avg:61.83ms
step:1519/2330 train_time:93925ms step_avg:61.83ms
step:1520/2330 train_time:93991ms step_avg:61.84ms
step:1521/2330 train_time:94053ms step_avg:61.84ms
step:1522/2330 train_time:94116ms step_avg:61.84ms
step:1523/2330 train_time:94177ms step_avg:61.84ms
step:1524/2330 train_time:94241ms step_avg:61.84ms
step:1525/2330 train_time:94301ms step_avg:61.84ms
step:1526/2330 train_time:94364ms step_avg:61.84ms
step:1527/2330 train_time:94424ms step_avg:61.84ms
step:1528/2330 train_time:94487ms step_avg:61.84ms
step:1529/2330 train_time:94547ms step_avg:61.84ms
step:1530/2330 train_time:94610ms step_avg:61.84ms
step:1531/2330 train_time:94672ms step_avg:61.84ms
step:1532/2330 train_time:94737ms step_avg:61.84ms
step:1533/2330 train_time:94799ms step_avg:61.84ms
step:1534/2330 train_time:94865ms step_avg:61.84ms
step:1535/2330 train_time:94928ms step_avg:61.84ms
step:1536/2330 train_time:94994ms step_avg:61.84ms
step:1537/2330 train_time:95056ms step_avg:61.85ms
step:1538/2330 train_time:95120ms step_avg:61.85ms
step:1539/2330 train_time:95181ms step_avg:61.85ms
step:1540/2330 train_time:95245ms step_avg:61.85ms
step:1541/2330 train_time:95306ms step_avg:61.85ms
step:1542/2330 train_time:95370ms step_avg:61.85ms
step:1543/2330 train_time:95431ms step_avg:61.85ms
step:1544/2330 train_time:95495ms step_avg:61.85ms
step:1545/2330 train_time:95555ms step_avg:61.85ms
step:1546/2330 train_time:95618ms step_avg:61.85ms
step:1547/2330 train_time:95679ms step_avg:61.85ms
step:1548/2330 train_time:95744ms step_avg:61.85ms
step:1549/2330 train_time:95805ms step_avg:61.85ms
step:1550/2330 train_time:95871ms step_avg:61.85ms
step:1551/2330 train_time:95933ms step_avg:61.85ms
step:1552/2330 train_time:95998ms step_avg:61.85ms
step:1553/2330 train_time:96060ms step_avg:61.85ms
step:1554/2330 train_time:96124ms step_avg:61.86ms
step:1555/2330 train_time:96185ms step_avg:61.86ms
step:1556/2330 train_time:96250ms step_avg:61.86ms
step:1557/2330 train_time:96312ms step_avg:61.86ms
step:1558/2330 train_time:96376ms step_avg:61.86ms
step:1559/2330 train_time:96437ms step_avg:61.86ms
step:1560/2330 train_time:96500ms step_avg:61.86ms
step:1561/2330 train_time:96561ms step_avg:61.86ms
step:1562/2330 train_time:96625ms step_avg:61.86ms
step:1563/2330 train_time:96686ms step_avg:61.86ms
step:1564/2330 train_time:96750ms step_avg:61.86ms
step:1565/2330 train_time:96812ms step_avg:61.86ms
step:1566/2330 train_time:96877ms step_avg:61.86ms
step:1567/2330 train_time:96939ms step_avg:61.86ms
step:1568/2330 train_time:97003ms step_avg:61.86ms
step:1569/2330 train_time:97065ms step_avg:61.86ms
step:1570/2330 train_time:97130ms step_avg:61.87ms
step:1571/2330 train_time:97191ms step_avg:61.87ms
step:1572/2330 train_time:97255ms step_avg:61.87ms
step:1573/2330 train_time:97316ms step_avg:61.87ms
step:1574/2330 train_time:97381ms step_avg:61.87ms
step:1575/2330 train_time:97441ms step_avg:61.87ms
step:1576/2330 train_time:97505ms step_avg:61.87ms
step:1577/2330 train_time:97566ms step_avg:61.87ms
step:1578/2330 train_time:97631ms step_avg:61.87ms
step:1579/2330 train_time:97692ms step_avg:61.87ms
step:1580/2330 train_time:97757ms step_avg:61.87ms
step:1581/2330 train_time:97818ms step_avg:61.87ms
step:1582/2330 train_time:97883ms step_avg:61.87ms
step:1583/2330 train_time:97945ms step_avg:61.87ms
step:1584/2330 train_time:98010ms step_avg:61.87ms
step:1585/2330 train_time:98071ms step_avg:61.87ms
step:1586/2330 train_time:98136ms step_avg:61.88ms
step:1587/2330 train_time:98197ms step_avg:61.88ms
step:1588/2330 train_time:98262ms step_avg:61.88ms
step:1589/2330 train_time:98323ms step_avg:61.88ms
step:1590/2330 train_time:98388ms step_avg:61.88ms
step:1591/2330 train_time:98450ms step_avg:61.88ms
step:1592/2330 train_time:98514ms step_avg:61.88ms
step:1593/2330 train_time:98575ms step_avg:61.88ms
step:1594/2330 train_time:98639ms step_avg:61.88ms
step:1595/2330 train_time:98699ms step_avg:61.88ms
step:1596/2330 train_time:98764ms step_avg:61.88ms
step:1597/2330 train_time:98825ms step_avg:61.88ms
step:1598/2330 train_time:98890ms step_avg:61.88ms
step:1599/2330 train_time:98952ms step_avg:61.88ms
step:1600/2330 train_time:99016ms step_avg:61.89ms
step:1601/2330 train_time:99078ms step_avg:61.88ms
step:1602/2330 train_time:99142ms step_avg:61.89ms
step:1603/2330 train_time:99202ms step_avg:61.89ms
step:1604/2330 train_time:99267ms step_avg:61.89ms
step:1605/2330 train_time:99329ms step_avg:61.89ms
step:1606/2330 train_time:99394ms step_avg:61.89ms
step:1607/2330 train_time:99455ms step_avg:61.89ms
step:1608/2330 train_time:99519ms step_avg:61.89ms
step:1609/2330 train_time:99580ms step_avg:61.89ms
step:1610/2330 train_time:99643ms step_avg:61.89ms
step:1611/2330 train_time:99705ms step_avg:61.89ms
step:1612/2330 train_time:99770ms step_avg:61.89ms
step:1613/2330 train_time:99831ms step_avg:61.89ms
step:1614/2330 train_time:99895ms step_avg:61.89ms
step:1615/2330 train_time:99957ms step_avg:61.89ms
step:1616/2330 train_time:100021ms step_avg:61.89ms
step:1617/2330 train_time:100083ms step_avg:61.89ms
step:1618/2330 train_time:100147ms step_avg:61.90ms
step:1619/2330 train_time:100209ms step_avg:61.90ms
step:1620/2330 train_time:100273ms step_avg:61.90ms
step:1621/2330 train_time:100335ms step_avg:61.90ms
step:1622/2330 train_time:100399ms step_avg:61.90ms
step:1623/2330 train_time:100460ms step_avg:61.90ms
step:1624/2330 train_time:100524ms step_avg:61.90ms
step:1625/2330 train_time:100585ms step_avg:61.90ms
step:1626/2330 train_time:100650ms step_avg:61.90ms
step:1627/2330 train_time:100712ms step_avg:61.90ms
step:1628/2330 train_time:100776ms step_avg:61.90ms
step:1629/2330 train_time:100838ms step_avg:61.90ms
step:1630/2330 train_time:100901ms step_avg:61.90ms
step:1631/2330 train_time:100962ms step_avg:61.90ms
step:1632/2330 train_time:101027ms step_avg:61.90ms
step:1633/2330 train_time:101089ms step_avg:61.90ms
step:1634/2330 train_time:101153ms step_avg:61.91ms
step:1635/2330 train_time:101214ms step_avg:61.90ms
step:1636/2330 train_time:101278ms step_avg:61.91ms
step:1637/2330 train_time:101339ms step_avg:61.91ms
step:1638/2330 train_time:101404ms step_avg:61.91ms
step:1639/2330 train_time:101465ms step_avg:61.91ms
step:1640/2330 train_time:101530ms step_avg:61.91ms
step:1641/2330 train_time:101591ms step_avg:61.91ms
step:1642/2330 train_time:101656ms step_avg:61.91ms
step:1643/2330 train_time:101717ms step_avg:61.91ms
step:1644/2330 train_time:101782ms step_avg:61.91ms
step:1645/2330 train_time:101843ms step_avg:61.91ms
step:1646/2330 train_time:101908ms step_avg:61.91ms
step:1647/2330 train_time:101970ms step_avg:61.91ms
step:1648/2330 train_time:102034ms step_avg:61.91ms
step:1649/2330 train_time:102095ms step_avg:61.91ms
step:1650/2330 train_time:102159ms step_avg:61.91ms
step:1651/2330 train_time:102220ms step_avg:61.91ms
step:1652/2330 train_time:102285ms step_avg:61.92ms
step:1653/2330 train_time:102346ms step_avg:61.92ms
step:1654/2330 train_time:102410ms step_avg:61.92ms
step:1655/2330 train_time:102472ms step_avg:61.92ms
step:1656/2330 train_time:102536ms step_avg:61.92ms
step:1657/2330 train_time:102597ms step_avg:61.92ms
step:1658/2330 train_time:102661ms step_avg:61.92ms
step:1659/2330 train_time:102722ms step_avg:61.92ms
step:1660/2330 train_time:102787ms step_avg:61.92ms
step:1661/2330 train_time:102849ms step_avg:61.92ms
step:1662/2330 train_time:102913ms step_avg:61.92ms
step:1663/2330 train_time:102975ms step_avg:61.92ms
step:1664/2330 train_time:103039ms step_avg:61.92ms
step:1665/2330 train_time:103100ms step_avg:61.92ms
step:1666/2330 train_time:103164ms step_avg:61.92ms
step:1667/2330 train_time:103225ms step_avg:61.92ms
step:1668/2330 train_time:103290ms step_avg:61.92ms
step:1669/2330 train_time:103352ms step_avg:61.92ms
step:1670/2330 train_time:103417ms step_avg:61.93ms
step:1671/2330 train_time:103478ms step_avg:61.93ms
step:1672/2330 train_time:103542ms step_avg:61.93ms
step:1673/2330 train_time:103603ms step_avg:61.93ms
step:1674/2330 train_time:103669ms step_avg:61.93ms
step:1675/2330 train_time:103730ms step_avg:61.93ms
step:1676/2330 train_time:103794ms step_avg:61.93ms
step:1677/2330 train_time:103855ms step_avg:61.93ms
step:1678/2330 train_time:103919ms step_avg:61.93ms
step:1679/2330 train_time:103980ms step_avg:61.93ms
step:1680/2330 train_time:104044ms step_avg:61.93ms
step:1681/2330 train_time:104105ms step_avg:61.93ms
step:1682/2330 train_time:104170ms step_avg:61.93ms
step:1683/2330 train_time:104231ms step_avg:61.93ms
step:1684/2330 train_time:104295ms step_avg:61.93ms
step:1685/2330 train_time:104356ms step_avg:61.93ms
step:1686/2330 train_time:104420ms step_avg:61.93ms
step:1687/2330 train_time:104481ms step_avg:61.93ms
step:1688/2330 train_time:104546ms step_avg:61.93ms
step:1689/2330 train_time:104607ms step_avg:61.93ms
step:1690/2330 train_time:104672ms step_avg:61.94ms
step:1691/2330 train_time:104733ms step_avg:61.94ms
step:1692/2330 train_time:104798ms step_avg:61.94ms
step:1693/2330 train_time:104859ms step_avg:61.94ms
step:1694/2330 train_time:104923ms step_avg:61.94ms
step:1695/2330 train_time:104985ms step_avg:61.94ms
step:1696/2330 train_time:105049ms step_avg:61.94ms
step:1697/2330 train_time:105110ms step_avg:61.94ms
step:1698/2330 train_time:105175ms step_avg:61.94ms
step:1699/2330 train_time:105237ms step_avg:61.94ms
step:1700/2330 train_time:105301ms step_avg:61.94ms
step:1701/2330 train_time:105362ms step_avg:61.94ms
step:1702/2330 train_time:105426ms step_avg:61.94ms
step:1703/2330 train_time:105486ms step_avg:61.94ms
step:1704/2330 train_time:105551ms step_avg:61.94ms
step:1705/2330 train_time:105612ms step_avg:61.94ms
step:1706/2330 train_time:105677ms step_avg:61.94ms
step:1707/2330 train_time:105739ms step_avg:61.94ms
step:1708/2330 train_time:105803ms step_avg:61.95ms
step:1709/2330 train_time:105864ms step_avg:61.95ms
step:1710/2330 train_time:105929ms step_avg:61.95ms
step:1711/2330 train_time:105990ms step_avg:61.95ms
step:1712/2330 train_time:106054ms step_avg:61.95ms
step:1713/2330 train_time:106116ms step_avg:61.95ms
step:1714/2330 train_time:106181ms step_avg:61.95ms
step:1715/2330 train_time:106242ms step_avg:61.95ms
step:1716/2330 train_time:106306ms step_avg:61.95ms
step:1717/2330 train_time:106368ms step_avg:61.95ms
step:1718/2330 train_time:106432ms step_avg:61.95ms
step:1719/2330 train_time:106493ms step_avg:61.95ms
step:1720/2330 train_time:106557ms step_avg:61.95ms
step:1721/2330 train_time:106619ms step_avg:61.95ms
step:1722/2330 train_time:106683ms step_avg:61.95ms
step:1723/2330 train_time:106744ms step_avg:61.95ms
step:1724/2330 train_time:106809ms step_avg:61.95ms
step:1725/2330 train_time:106871ms step_avg:61.95ms
step:1726/2330 train_time:106935ms step_avg:61.96ms
step:1727/2330 train_time:106996ms step_avg:61.95ms
step:1728/2330 train_time:107060ms step_avg:61.96ms
step:1729/2330 train_time:107121ms step_avg:61.96ms
step:1730/2330 train_time:107185ms step_avg:61.96ms
step:1731/2330 train_time:107246ms step_avg:61.96ms
step:1732/2330 train_time:107311ms step_avg:61.96ms
step:1733/2330 train_time:107372ms step_avg:61.96ms
step:1734/2330 train_time:107437ms step_avg:61.96ms
step:1735/2330 train_time:107497ms step_avg:61.96ms
step:1736/2330 train_time:107562ms step_avg:61.96ms
step:1737/2330 train_time:107623ms step_avg:61.96ms
step:1738/2330 train_time:107688ms step_avg:61.96ms
step:1739/2330 train_time:107749ms step_avg:61.96ms
step:1740/2330 train_time:107814ms step_avg:61.96ms
step:1741/2330 train_time:107876ms step_avg:61.96ms
step:1742/2330 train_time:107940ms step_avg:61.96ms
step:1743/2330 train_time:108001ms step_avg:61.96ms
step:1744/2330 train_time:108066ms step_avg:61.96ms
step:1745/2330 train_time:108128ms step_avg:61.96ms
step:1746/2330 train_time:108192ms step_avg:61.97ms
step:1747/2330 train_time:108253ms step_avg:61.97ms
step:1748/2330 train_time:108317ms step_avg:61.97ms
step:1749/2330 train_time:108377ms step_avg:61.97ms
step:1750/2330 train_time:108441ms step_avg:61.97ms
step:1750/2330 val_loss:3.9845 train_time:108508ms step_avg:62.00ms
step:1751/2330 train_time:108530ms step_avg:61.98ms
step:1752/2330 train_time:108569ms step_avg:61.97ms
step:1753/2330 train_time:108635ms step_avg:61.97ms
step:1754/2330 train_time:108701ms step_avg:61.97ms
step:1755/2330 train_time:108761ms step_avg:61.97ms
step:1756/2330 train_time:108824ms step_avg:61.97ms
step:1757/2330 train_time:108884ms step_avg:61.97ms
step:1758/2330 train_time:108947ms step_avg:61.97ms
step:1759/2330 train_time:109007ms step_avg:61.97ms
step:1760/2330 train_time:109071ms step_avg:61.97ms
step:1761/2330 train_time:109131ms step_avg:61.97ms
step:1762/2330 train_time:109194ms step_avg:61.97ms
step:1763/2330 train_time:109255ms step_avg:61.97ms
step:1764/2330 train_time:109318ms step_avg:61.97ms
step:1765/2330 train_time:109379ms step_avg:61.97ms
step:1766/2330 train_time:109447ms step_avg:61.97ms
step:1767/2330 train_time:109512ms step_avg:61.98ms
step:1768/2330 train_time:109577ms step_avg:61.98ms
step:1769/2330 train_time:109638ms step_avg:61.98ms
step:1770/2330 train_time:109703ms step_avg:61.98ms
step:1771/2330 train_time:109763ms step_avg:61.98ms
step:1772/2330 train_time:109827ms step_avg:61.98ms
step:1773/2330 train_time:109888ms step_avg:61.98ms
step:1774/2330 train_time:109952ms step_avg:61.98ms
step:1775/2330 train_time:110012ms step_avg:61.98ms
step:1776/2330 train_time:110076ms step_avg:61.98ms
step:1777/2330 train_time:110136ms step_avg:61.98ms
step:1778/2330 train_time:110200ms step_avg:61.98ms
step:1779/2330 train_time:110260ms step_avg:61.98ms
step:1780/2330 train_time:110324ms step_avg:61.98ms
step:1781/2330 train_time:110385ms step_avg:61.98ms
step:1782/2330 train_time:110451ms step_avg:61.98ms
step:1783/2330 train_time:110513ms step_avg:61.98ms
step:1784/2330 train_time:110579ms step_avg:61.98ms
step:1785/2330 train_time:110641ms step_avg:61.98ms
step:1786/2330 train_time:110705ms step_avg:61.98ms
step:1787/2330 train_time:110766ms step_avg:61.98ms
step:1788/2330 train_time:110831ms step_avg:61.99ms
step:1789/2330 train_time:110892ms step_avg:61.99ms
step:1790/2330 train_time:110956ms step_avg:61.99ms
step:1791/2330 train_time:111016ms step_avg:61.99ms
step:1792/2330 train_time:111080ms step_avg:61.99ms
step:1793/2330 train_time:111140ms step_avg:61.99ms
step:1794/2330 train_time:111204ms step_avg:61.99ms
step:1795/2330 train_time:111264ms step_avg:61.99ms
step:1796/2330 train_time:111328ms step_avg:61.99ms
step:1797/2330 train_time:111389ms step_avg:61.99ms
step:1798/2330 train_time:111454ms step_avg:61.99ms
step:1799/2330 train_time:111517ms step_avg:61.99ms
step:1800/2330 train_time:111582ms step_avg:61.99ms
step:1801/2330 train_time:111643ms step_avg:61.99ms
step:1802/2330 train_time:111707ms step_avg:61.99ms
step:1803/2330 train_time:111767ms step_avg:61.99ms
step:1804/2330 train_time:111831ms step_avg:61.99ms
step:1805/2330 train_time:111892ms step_avg:61.99ms
step:1806/2330 train_time:111957ms step_avg:61.99ms
step:1807/2330 train_time:112018ms step_avg:61.99ms
step:1808/2330 train_time:112082ms step_avg:61.99ms
step:1809/2330 train_time:112142ms step_avg:61.99ms
step:1810/2330 train_time:112205ms step_avg:61.99ms
step:1811/2330 train_time:112266ms step_avg:61.99ms
step:1812/2330 train_time:112330ms step_avg:61.99ms
step:1813/2330 train_time:112392ms step_avg:61.99ms
step:1814/2330 train_time:112456ms step_avg:61.99ms
step:1815/2330 train_time:112517ms step_avg:61.99ms
step:1816/2330 train_time:112582ms step_avg:61.99ms
step:1817/2330 train_time:112642ms step_avg:61.99ms
step:1818/2330 train_time:112706ms step_avg:61.99ms
step:1819/2330 train_time:112768ms step_avg:61.99ms
step:1820/2330 train_time:112832ms step_avg:62.00ms
step:1821/2330 train_time:112895ms step_avg:62.00ms
step:1822/2330 train_time:112959ms step_avg:62.00ms
step:1823/2330 train_time:113021ms step_avg:62.00ms
step:1824/2330 train_time:113085ms step_avg:62.00ms
step:1825/2330 train_time:113145ms step_avg:62.00ms
step:1826/2330 train_time:113209ms step_avg:62.00ms
step:1827/2330 train_time:113270ms step_avg:62.00ms
step:1828/2330 train_time:113334ms step_avg:62.00ms
step:1829/2330 train_time:113396ms step_avg:62.00ms
step:1830/2330 train_time:113461ms step_avg:62.00ms
step:1831/2330 train_time:113522ms step_avg:62.00ms
step:1832/2330 train_time:113586ms step_avg:62.00ms
step:1833/2330 train_time:113648ms step_avg:62.00ms
step:1834/2330 train_time:113712ms step_avg:62.00ms
step:1835/2330 train_time:113773ms step_avg:62.00ms
step:1836/2330 train_time:113838ms step_avg:62.00ms
step:1837/2330 train_time:113899ms step_avg:62.00ms
step:1838/2330 train_time:113963ms step_avg:62.00ms
step:1839/2330 train_time:114024ms step_avg:62.00ms
step:1840/2330 train_time:114088ms step_avg:62.00ms
step:1841/2330 train_time:114148ms step_avg:62.00ms
step:1842/2330 train_time:114212ms step_avg:62.00ms
step:1843/2330 train_time:114273ms step_avg:62.00ms
step:1844/2330 train_time:114337ms step_avg:62.01ms
step:1845/2330 train_time:114399ms step_avg:62.00ms
step:1846/2330 train_time:114463ms step_avg:62.01ms
step:1847/2330 train_time:114524ms step_avg:62.01ms
step:1848/2330 train_time:114588ms step_avg:62.01ms
step:1849/2330 train_time:114649ms step_avg:62.01ms
step:1850/2330 train_time:114714ms step_avg:62.01ms
step:1851/2330 train_time:114775ms step_avg:62.01ms
step:1852/2330 train_time:114839ms step_avg:62.01ms
step:1853/2330 train_time:114900ms step_avg:62.01ms
step:1854/2330 train_time:114964ms step_avg:62.01ms
step:1855/2330 train_time:115026ms step_avg:62.01ms
step:1856/2330 train_time:115090ms step_avg:62.01ms
step:1857/2330 train_time:115151ms step_avg:62.01ms
step:1858/2330 train_time:115215ms step_avg:62.01ms
step:1859/2330 train_time:115276ms step_avg:62.01ms
step:1860/2330 train_time:115340ms step_avg:62.01ms
step:1861/2330 train_time:115401ms step_avg:62.01ms
step:1862/2330 train_time:115465ms step_avg:62.01ms
step:1863/2330 train_time:115526ms step_avg:62.01ms
step:1864/2330 train_time:115592ms step_avg:62.01ms
step:1865/2330 train_time:115653ms step_avg:62.01ms
step:1866/2330 train_time:115718ms step_avg:62.01ms
step:1867/2330 train_time:115779ms step_avg:62.01ms
step:1868/2330 train_time:115843ms step_avg:62.01ms
step:1869/2330 train_time:115904ms step_avg:62.01ms
step:1870/2330 train_time:115968ms step_avg:62.02ms
step:1871/2330 train_time:116029ms step_avg:62.01ms
step:1872/2330 train_time:116093ms step_avg:62.02ms
step:1873/2330 train_time:116155ms step_avg:62.02ms
step:1874/2330 train_time:116219ms step_avg:62.02ms
step:1875/2330 train_time:116280ms step_avg:62.02ms
step:1876/2330 train_time:116344ms step_avg:62.02ms
step:1877/2330 train_time:116405ms step_avg:62.02ms
step:1878/2330 train_time:116469ms step_avg:62.02ms
step:1879/2330 train_time:116530ms step_avg:62.02ms
step:1880/2330 train_time:116595ms step_avg:62.02ms
step:1881/2330 train_time:116656ms step_avg:62.02ms
step:1882/2330 train_time:116721ms step_avg:62.02ms
step:1883/2330 train_time:116782ms step_avg:62.02ms
step:1884/2330 train_time:116846ms step_avg:62.02ms
step:1885/2330 train_time:116907ms step_avg:62.02ms
step:1886/2330 train_time:116972ms step_avg:62.02ms
step:1887/2330 train_time:117033ms step_avg:62.02ms
step:1888/2330 train_time:117097ms step_avg:62.02ms
step:1889/2330 train_time:117158ms step_avg:62.02ms
step:1890/2330 train_time:117222ms step_avg:62.02ms
step:1891/2330 train_time:117283ms step_avg:62.02ms
step:1892/2330 train_time:117347ms step_avg:62.02ms
step:1893/2330 train_time:117408ms step_avg:62.02ms
step:1894/2330 train_time:117472ms step_avg:62.02ms
step:1895/2330 train_time:117534ms step_avg:62.02ms
step:1896/2330 train_time:117598ms step_avg:62.02ms
step:1897/2330 train_time:117659ms step_avg:62.02ms
step:1898/2330 train_time:117724ms step_avg:62.03ms
step:1899/2330 train_time:117785ms step_avg:62.02ms
step:1900/2330 train_time:117849ms step_avg:62.03ms
step:1901/2330 train_time:117911ms step_avg:62.03ms
step:1902/2330 train_time:117975ms step_avg:62.03ms
step:1903/2330 train_time:118036ms step_avg:62.03ms
step:1904/2330 train_time:118100ms step_avg:62.03ms
step:1905/2330 train_time:118160ms step_avg:62.03ms
step:1906/2330 train_time:118224ms step_avg:62.03ms
step:1907/2330 train_time:118286ms step_avg:62.03ms
step:1908/2330 train_time:118350ms step_avg:62.03ms
step:1909/2330 train_time:118410ms step_avg:62.03ms
step:1910/2330 train_time:118475ms step_avg:62.03ms
step:1911/2330 train_time:118535ms step_avg:62.03ms
step:1912/2330 train_time:118600ms step_avg:62.03ms
step:1913/2330 train_time:118661ms step_avg:62.03ms
step:1914/2330 train_time:118726ms step_avg:62.03ms
step:1915/2330 train_time:118787ms step_avg:62.03ms
step:1916/2330 train_time:118852ms step_avg:62.03ms
step:1917/2330 train_time:118914ms step_avg:62.03ms
step:1918/2330 train_time:118979ms step_avg:62.03ms
step:1919/2330 train_time:119039ms step_avg:62.03ms
step:1920/2330 train_time:119104ms step_avg:62.03ms
step:1921/2330 train_time:119165ms step_avg:62.03ms
step:1922/2330 train_time:119229ms step_avg:62.03ms
step:1923/2330 train_time:119290ms step_avg:62.03ms
step:1924/2330 train_time:119354ms step_avg:62.03ms
step:1925/2330 train_time:119415ms step_avg:62.03ms
step:1926/2330 train_time:119479ms step_avg:62.03ms
step:1927/2330 train_time:119540ms step_avg:62.03ms
step:1928/2330 train_time:119604ms step_avg:62.04ms
step:1929/2330 train_time:119666ms step_avg:62.04ms
step:1930/2330 train_time:119730ms step_avg:62.04ms
step:1931/2330 train_time:119791ms step_avg:62.04ms
step:1932/2330 train_time:119855ms step_avg:62.04ms
step:1933/2330 train_time:119917ms step_avg:62.04ms
step:1934/2330 train_time:119981ms step_avg:62.04ms
step:1935/2330 train_time:120042ms step_avg:62.04ms
step:1936/2330 train_time:120106ms step_avg:62.04ms
step:1937/2330 train_time:120167ms step_avg:62.04ms
step:1938/2330 train_time:120232ms step_avg:62.04ms
step:1939/2330 train_time:120293ms step_avg:62.04ms
step:1940/2330 train_time:120357ms step_avg:62.04ms
step:1941/2330 train_time:120419ms step_avg:62.04ms
step:1942/2330 train_time:120483ms step_avg:62.04ms
step:1943/2330 train_time:120543ms step_avg:62.04ms
step:1944/2330 train_time:120607ms step_avg:62.04ms
step:1945/2330 train_time:120668ms step_avg:62.04ms
step:1946/2330 train_time:120732ms step_avg:62.04ms
step:1947/2330 train_time:120794ms step_avg:62.04ms
step:1948/2330 train_time:120858ms step_avg:62.04ms
step:1949/2330 train_time:120920ms step_avg:62.04ms
step:1950/2330 train_time:120984ms step_avg:62.04ms
step:1951/2330 train_time:121045ms step_avg:62.04ms
step:1952/2330 train_time:121109ms step_avg:62.04ms
step:1953/2330 train_time:121170ms step_avg:62.04ms
step:1954/2330 train_time:121234ms step_avg:62.04ms
step:1955/2330 train_time:121295ms step_avg:62.04ms
step:1956/2330 train_time:121360ms step_avg:62.04ms
step:1957/2330 train_time:121422ms step_avg:62.04ms
step:1958/2330 train_time:121487ms step_avg:62.05ms
step:1959/2330 train_time:121548ms step_avg:62.05ms
step:1960/2330 train_time:121612ms step_avg:62.05ms
step:1961/2330 train_time:121674ms step_avg:62.05ms
step:1962/2330 train_time:121738ms step_avg:62.05ms
step:1963/2330 train_time:121799ms step_avg:62.05ms
step:1964/2330 train_time:121863ms step_avg:62.05ms
step:1965/2330 train_time:121925ms step_avg:62.05ms
step:1966/2330 train_time:121989ms step_avg:62.05ms
step:1967/2330 train_time:122049ms step_avg:62.05ms
step:1968/2330 train_time:122114ms step_avg:62.05ms
step:1969/2330 train_time:122175ms step_avg:62.05ms
step:1970/2330 train_time:122239ms step_avg:62.05ms
step:1971/2330 train_time:122300ms step_avg:62.05ms
step:1972/2330 train_time:122364ms step_avg:62.05ms
step:1973/2330 train_time:122425ms step_avg:62.05ms
step:1974/2330 train_time:122490ms step_avg:62.05ms
step:1975/2330 train_time:122551ms step_avg:62.05ms
step:1976/2330 train_time:122616ms step_avg:62.05ms
step:1977/2330 train_time:122676ms step_avg:62.05ms
step:1978/2330 train_time:122740ms step_avg:62.05ms
step:1979/2330 train_time:122801ms step_avg:62.05ms
step:1980/2330 train_time:122865ms step_avg:62.05ms
step:1981/2330 train_time:122926ms step_avg:62.05ms
step:1982/2330 train_time:122991ms step_avg:62.05ms
step:1983/2330 train_time:123052ms step_avg:62.05ms
step:1984/2330 train_time:123116ms step_avg:62.05ms
step:1985/2330 train_time:123177ms step_avg:62.05ms
step:1986/2330 train_time:123241ms step_avg:62.06ms
step:1987/2330 train_time:123302ms step_avg:62.05ms
step:1988/2330 train_time:123366ms step_avg:62.06ms
step:1989/2330 train_time:123427ms step_avg:62.05ms
step:1990/2330 train_time:123492ms step_avg:62.06ms
step:1991/2330 train_time:123553ms step_avg:62.06ms
step:1992/2330 train_time:123617ms step_avg:62.06ms
step:1993/2330 train_time:123678ms step_avg:62.06ms
step:1994/2330 train_time:123742ms step_avg:62.06ms
step:1995/2330 train_time:123803ms step_avg:62.06ms
step:1996/2330 train_time:123866ms step_avg:62.06ms
step:1997/2330 train_time:123927ms step_avg:62.06ms
step:1998/2330 train_time:123992ms step_avg:62.06ms
step:1999/2330 train_time:124054ms step_avg:62.06ms
step:2000/2330 train_time:124119ms step_avg:62.06ms
step:2000/2330 val_loss:3.9495 train_time:124185ms step_avg:62.09ms
step:2001/2330 train_time:124208ms step_avg:62.07ms
step:2002/2330 train_time:124248ms step_avg:62.06ms
step:2003/2330 train_time:124314ms step_avg:62.06ms
step:2004/2330 train_time:124379ms step_avg:62.07ms
step:2005/2330 train_time:124441ms step_avg:62.07ms
step:2006/2330 train_time:124507ms step_avg:62.07ms
step:2007/2330 train_time:124568ms step_avg:62.07ms
step:2008/2330 train_time:124632ms step_avg:62.07ms
step:2009/2330 train_time:124692ms step_avg:62.07ms
step:2010/2330 train_time:124756ms step_avg:62.07ms
step:2011/2330 train_time:124816ms step_avg:62.07ms
step:2012/2330 train_time:124879ms step_avg:62.07ms
step:2013/2330 train_time:124939ms step_avg:62.07ms
step:2014/2330 train_time:125002ms step_avg:62.07ms
step:2015/2330 train_time:125062ms step_avg:62.07ms
step:2016/2330 train_time:125127ms step_avg:62.07ms
step:2017/2330 train_time:125189ms step_avg:62.07ms
step:2018/2330 train_time:125256ms step_avg:62.07ms
step:2019/2330 train_time:125317ms step_avg:62.07ms
step:2020/2330 train_time:125382ms step_avg:62.07ms
step:2021/2330 train_time:125444ms step_avg:62.07ms
step:2022/2330 train_time:125510ms step_avg:62.07ms
step:2023/2330 train_time:125571ms step_avg:62.07ms
step:2024/2330 train_time:125636ms step_avg:62.07ms
step:2025/2330 train_time:125696ms step_avg:62.07ms
step:2026/2330 train_time:125760ms step_avg:62.07ms
step:2027/2330 train_time:125820ms step_avg:62.07ms
step:2028/2330 train_time:125884ms step_avg:62.07ms
step:2029/2330 train_time:125944ms step_avg:62.07ms
step:2030/2330 train_time:126008ms step_avg:62.07ms
step:2031/2330 train_time:126069ms step_avg:62.07ms
step:2032/2330 train_time:126133ms step_avg:62.07ms
step:2033/2330 train_time:126196ms step_avg:62.07ms
step:2034/2330 train_time:126260ms step_avg:62.07ms
step:2035/2330 train_time:126322ms step_avg:62.07ms
step:2036/2330 train_time:126387ms step_avg:62.08ms
step:2037/2330 train_time:126449ms step_avg:62.08ms
step:2038/2330 train_time:126514ms step_avg:62.08ms
step:2039/2330 train_time:126576ms step_avg:62.08ms
step:2040/2330 train_time:126640ms step_avg:62.08ms
step:2041/2330 train_time:126702ms step_avg:62.08ms
step:2042/2330 train_time:126766ms step_avg:62.08ms
step:2043/2330 train_time:126828ms step_avg:62.08ms
step:2044/2330 train_time:126891ms step_avg:62.08ms
step:2045/2330 train_time:126952ms step_avg:62.08ms
step:2046/2330 train_time:127016ms step_avg:62.08ms
step:2047/2330 train_time:127077ms step_avg:62.08ms
step:2048/2330 train_time:127141ms step_avg:62.08ms
step:2049/2330 train_time:127202ms step_avg:62.08ms
step:2050/2330 train_time:127267ms step_avg:62.08ms
step:2051/2330 train_time:127329ms step_avg:62.08ms
step:2052/2330 train_time:127394ms step_avg:62.08ms
step:2053/2330 train_time:127455ms step_avg:62.08ms
step:2054/2330 train_time:127520ms step_avg:62.08ms
step:2055/2330 train_time:127581ms step_avg:62.08ms
step:2056/2330 train_time:127647ms step_avg:62.08ms
step:2057/2330 train_time:127708ms step_avg:62.08ms
step:2058/2330 train_time:127773ms step_avg:62.09ms
step:2059/2330 train_time:127834ms step_avg:62.09ms
step:2060/2330 train_time:127898ms step_avg:62.09ms
step:2061/2330 train_time:127959ms step_avg:62.09ms
step:2062/2330 train_time:128022ms step_avg:62.09ms
step:2063/2330 train_time:128084ms step_avg:62.09ms
step:2064/2330 train_time:128148ms step_avg:62.09ms
step:2065/2330 train_time:128209ms step_avg:62.09ms
step:2066/2330 train_time:128274ms step_avg:62.09ms
step:2067/2330 train_time:128336ms step_avg:62.09ms
step:2068/2330 train_time:128400ms step_avg:62.09ms
step:2069/2330 train_time:128461ms step_avg:62.09ms
step:2070/2330 train_time:128526ms step_avg:62.09ms
step:2071/2330 train_time:128588ms step_avg:62.09ms
step:2072/2330 train_time:128652ms step_avg:62.09ms
step:2073/2330 train_time:128713ms step_avg:62.09ms
step:2074/2330 train_time:128777ms step_avg:62.09ms
step:2075/2330 train_time:128837ms step_avg:62.09ms
step:2076/2330 train_time:128901ms step_avg:62.09ms
step:2077/2330 train_time:128962ms step_avg:62.09ms
step:2078/2330 train_time:129026ms step_avg:62.09ms
step:2079/2330 train_time:129087ms step_avg:62.09ms
step:2080/2330 train_time:129152ms step_avg:62.09ms
step:2081/2330 train_time:129212ms step_avg:62.09ms
step:2082/2330 train_time:129277ms step_avg:62.09ms
step:2083/2330 train_time:129339ms step_avg:62.09ms
step:2084/2330 train_time:129403ms step_avg:62.09ms
step:2085/2330 train_time:129464ms step_avg:62.09ms
step:2086/2330 train_time:129530ms step_avg:62.09ms
step:2087/2330 train_time:129591ms step_avg:62.09ms
step:2088/2330 train_time:129655ms step_avg:62.10ms
step:2089/2330 train_time:129717ms step_avg:62.10ms
step:2090/2330 train_time:129780ms step_avg:62.10ms
step:2091/2330 train_time:129842ms step_avg:62.10ms
step:2092/2330 train_time:129906ms step_avg:62.10ms
step:2093/2330 train_time:129967ms step_avg:62.10ms
step:2094/2330 train_time:130032ms step_avg:62.10ms
step:2095/2330 train_time:130093ms step_avg:62.10ms
step:2096/2330 train_time:130158ms step_avg:62.10ms
step:2097/2330 train_time:130219ms step_avg:62.10ms
step:2098/2330 train_time:130283ms step_avg:62.10ms
step:2099/2330 train_time:130344ms step_avg:62.10ms
step:2100/2330 train_time:130408ms step_avg:62.10ms
step:2101/2330 train_time:130470ms step_avg:62.10ms
step:2102/2330 train_time:130535ms step_avg:62.10ms
step:2103/2330 train_time:130597ms step_avg:62.10ms
step:2104/2330 train_time:130660ms step_avg:62.10ms
step:2105/2330 train_time:130722ms step_avg:62.10ms
step:2106/2330 train_time:130787ms step_avg:62.10ms
step:2107/2330 train_time:130849ms step_avg:62.10ms
step:2108/2330 train_time:130913ms step_avg:62.10ms
step:2109/2330 train_time:130974ms step_avg:62.10ms
step:2110/2330 train_time:131038ms step_avg:62.10ms
step:2111/2330 train_time:131099ms step_avg:62.10ms
step:2112/2330 train_time:131164ms step_avg:62.10ms
step:2113/2330 train_time:131225ms step_avg:62.10ms
step:2114/2330 train_time:131289ms step_avg:62.10ms
step:2115/2330 train_time:131350ms step_avg:62.10ms
step:2116/2330 train_time:131414ms step_avg:62.10ms
step:2117/2330 train_time:131476ms step_avg:62.10ms
step:2118/2330 train_time:131540ms step_avg:62.11ms
step:2119/2330 train_time:131601ms step_avg:62.11ms
step:2120/2330 train_time:131666ms step_avg:62.11ms
step:2121/2330 train_time:131727ms step_avg:62.11ms
step:2122/2330 train_time:131793ms step_avg:62.11ms
step:2123/2330 train_time:131853ms step_avg:62.11ms
step:2124/2330 train_time:131917ms step_avg:62.11ms
step:2125/2330 train_time:131977ms step_avg:62.11ms
step:2126/2330 train_time:132041ms step_avg:62.11ms
step:2127/2330 train_time:132103ms step_avg:62.11ms
step:2128/2330 train_time:132167ms step_avg:62.11ms
step:2129/2330 train_time:132228ms step_avg:62.11ms
step:2130/2330 train_time:132293ms step_avg:62.11ms
step:2131/2330 train_time:132354ms step_avg:62.11ms
step:2132/2330 train_time:132418ms step_avg:62.11ms
step:2133/2330 train_time:132480ms step_avg:62.11ms
step:2134/2330 train_time:132544ms step_avg:62.11ms
step:2135/2330 train_time:132605ms step_avg:62.11ms
step:2136/2330 train_time:132669ms step_avg:62.11ms
step:2137/2330 train_time:132731ms step_avg:62.11ms
step:2138/2330 train_time:132795ms step_avg:62.11ms
step:2139/2330 train_time:132857ms step_avg:62.11ms
step:2140/2330 train_time:132920ms step_avg:62.11ms
step:2141/2330 train_time:132981ms step_avg:62.11ms
step:2142/2330 train_time:133046ms step_avg:62.11ms
step:2143/2330 train_time:133107ms step_avg:62.11ms
step:2144/2330 train_time:133171ms step_avg:62.11ms
step:2145/2330 train_time:133233ms step_avg:62.11ms
step:2146/2330 train_time:133297ms step_avg:62.11ms
step:2147/2330 train_time:133357ms step_avg:62.11ms
step:2148/2330 train_time:133422ms step_avg:62.11ms
step:2149/2330 train_time:133483ms step_avg:62.11ms
step:2150/2330 train_time:133548ms step_avg:62.12ms
step:2151/2330 train_time:133610ms step_avg:62.12ms
step:2152/2330 train_time:133674ms step_avg:62.12ms
step:2153/2330 train_time:133736ms step_avg:62.12ms
step:2154/2330 train_time:133800ms step_avg:62.12ms
step:2155/2330 train_time:133861ms step_avg:62.12ms
step:2156/2330 train_time:133925ms step_avg:62.12ms
step:2157/2330 train_time:133986ms step_avg:62.12ms
step:2158/2330 train_time:134050ms step_avg:62.12ms
step:2159/2330 train_time:134112ms step_avg:62.12ms
step:2160/2330 train_time:134176ms step_avg:62.12ms
step:2161/2330 train_time:134238ms step_avg:62.12ms
step:2162/2330 train_time:134302ms step_avg:62.12ms
step:2163/2330 train_time:134364ms step_avg:62.12ms
step:2164/2330 train_time:134428ms step_avg:62.12ms
step:2165/2330 train_time:134489ms step_avg:62.12ms
step:2166/2330 train_time:134553ms step_avg:62.12ms
step:2167/2330 train_time:134614ms step_avg:62.12ms
step:2168/2330 train_time:134679ms step_avg:62.12ms
step:2169/2330 train_time:134740ms step_avg:62.12ms
step:2170/2330 train_time:134804ms step_avg:62.12ms
step:2171/2330 train_time:134865ms step_avg:62.12ms
step:2172/2330 train_time:134930ms step_avg:62.12ms
step:2173/2330 train_time:134991ms step_avg:62.12ms
step:2174/2330 train_time:135055ms step_avg:62.12ms
step:2175/2330 train_time:135116ms step_avg:62.12ms
step:2176/2330 train_time:135180ms step_avg:62.12ms
step:2177/2330 train_time:135241ms step_avg:62.12ms
step:2178/2330 train_time:135306ms step_avg:62.12ms
step:2179/2330 train_time:135368ms step_avg:62.12ms
step:2180/2330 train_time:135432ms step_avg:62.12ms
step:2181/2330 train_time:135494ms step_avg:62.12ms
step:2182/2330 train_time:135558ms step_avg:62.13ms
step:2183/2330 train_time:135619ms step_avg:62.13ms
step:2184/2330 train_time:135683ms step_avg:62.13ms
step:2185/2330 train_time:135744ms step_avg:62.13ms
step:2186/2330 train_time:135808ms step_avg:62.13ms
step:2187/2330 train_time:135870ms step_avg:62.13ms
step:2188/2330 train_time:135934ms step_avg:62.13ms
step:2189/2330 train_time:135995ms step_avg:62.13ms
step:2190/2330 train_time:136060ms step_avg:62.13ms
step:2191/2330 train_time:136121ms step_avg:62.13ms
step:2192/2330 train_time:136186ms step_avg:62.13ms
step:2193/2330 train_time:136247ms step_avg:62.13ms
step:2194/2330 train_time:136311ms step_avg:62.13ms
step:2195/2330 train_time:136373ms step_avg:62.13ms
step:2196/2330 train_time:136437ms step_avg:62.13ms
step:2197/2330 train_time:136498ms step_avg:62.13ms
step:2198/2330 train_time:136562ms step_avg:62.13ms
step:2199/2330 train_time:136623ms step_avg:62.13ms
step:2200/2330 train_time:136687ms step_avg:62.13ms
step:2201/2330 train_time:136748ms step_avg:62.13ms
step:2202/2330 train_time:136813ms step_avg:62.13ms
step:2203/2330 train_time:136875ms step_avg:62.13ms
step:2204/2330 train_time:136939ms step_avg:62.13ms
step:2205/2330 train_time:137000ms step_avg:62.13ms
step:2206/2330 train_time:137065ms step_avg:62.13ms
step:2207/2330 train_time:137126ms step_avg:62.13ms
step:2208/2330 train_time:137191ms step_avg:62.13ms
step:2209/2330 train_time:137252ms step_avg:62.13ms
step:2210/2330 train_time:137316ms step_avg:62.13ms
step:2211/2330 train_time:137377ms step_avg:62.13ms
step:2212/2330 train_time:137441ms step_avg:62.13ms
step:2213/2330 train_time:137503ms step_avg:62.13ms
step:2214/2330 train_time:137567ms step_avg:62.14ms
step:2215/2330 train_time:137629ms step_avg:62.14ms
step:2216/2330 train_time:137694ms step_avg:62.14ms
step:2217/2330 train_time:137756ms step_avg:62.14ms
step:2218/2330 train_time:137819ms step_avg:62.14ms
step:2219/2330 train_time:137881ms step_avg:62.14ms
step:2220/2330 train_time:137945ms step_avg:62.14ms
step:2221/2330 train_time:138006ms step_avg:62.14ms
step:2222/2330 train_time:138071ms step_avg:62.14ms
step:2223/2330 train_time:138132ms step_avg:62.14ms
step:2224/2330 train_time:138195ms step_avg:62.14ms
step:2225/2330 train_time:138256ms step_avg:62.14ms
step:2226/2330 train_time:138321ms step_avg:62.14ms
step:2227/2330 train_time:138382ms step_avg:62.14ms
step:2228/2330 train_time:138446ms step_avg:62.14ms
step:2229/2330 train_time:138507ms step_avg:62.14ms
step:2230/2330 train_time:138572ms step_avg:62.14ms
step:2231/2330 train_time:138633ms step_avg:62.14ms
step:2232/2330 train_time:138698ms step_avg:62.14ms
step:2233/2330 train_time:138758ms step_avg:62.14ms
step:2234/2330 train_time:138823ms step_avg:62.14ms
step:2235/2330 train_time:138884ms step_avg:62.14ms
step:2236/2330 train_time:138948ms step_avg:62.14ms
step:2237/2330 train_time:139010ms step_avg:62.14ms
step:2238/2330 train_time:139074ms step_avg:62.14ms
step:2239/2330 train_time:139135ms step_avg:62.14ms
step:2240/2330 train_time:139199ms step_avg:62.14ms
step:2241/2330 train_time:139261ms step_avg:62.14ms
step:2242/2330 train_time:139325ms step_avg:62.14ms
step:2243/2330 train_time:139386ms step_avg:62.14ms
step:2244/2330 train_time:139450ms step_avg:62.14ms
step:2245/2330 train_time:139511ms step_avg:62.14ms
step:2246/2330 train_time:139575ms step_avg:62.14ms
step:2247/2330 train_time:139636ms step_avg:62.14ms
step:2248/2330 train_time:139700ms step_avg:62.14ms
step:2249/2330 train_time:139760ms step_avg:62.14ms
step:2250/2330 train_time:139825ms step_avg:62.14ms
step:2250/2330 val_loss:3.8803 train_time:139891ms step_avg:62.17ms
step:2251/2330 train_time:139914ms step_avg:62.16ms
step:2252/2330 train_time:139954ms step_avg:62.15ms
step:2253/2330 train_time:140021ms step_avg:62.15ms
step:2254/2330 train_time:140089ms step_avg:62.15ms
step:2255/2330 train_time:140151ms step_avg:62.15ms
step:2256/2330 train_time:140215ms step_avg:62.15ms
step:2257/2330 train_time:140275ms step_avg:62.15ms
step:2258/2330 train_time:140339ms step_avg:62.15ms
step:2259/2330 train_time:140399ms step_avg:62.15ms
step:2260/2330 train_time:140463ms step_avg:62.15ms
step:2261/2330 train_time:140523ms step_avg:62.15ms
step:2262/2330 train_time:140587ms step_avg:62.15ms
step:2263/2330 train_time:140647ms step_avg:62.15ms
step:2264/2330 train_time:140710ms step_avg:62.15ms
step:2265/2330 train_time:140771ms step_avg:62.15ms
step:2266/2330 train_time:140834ms step_avg:62.15ms
step:2267/2330 train_time:140896ms step_avg:62.15ms
step:2268/2330 train_time:140964ms step_avg:62.15ms
step:2269/2330 train_time:141027ms step_avg:62.15ms
step:2270/2330 train_time:141093ms step_avg:62.16ms
step:2271/2330 train_time:141155ms step_avg:62.16ms
step:2272/2330 train_time:141219ms step_avg:62.16ms
step:2273/2330 train_time:141281ms step_avg:62.16ms
step:2274/2330 train_time:141344ms step_avg:62.16ms
step:2275/2330 train_time:141405ms step_avg:62.16ms
step:2276/2330 train_time:141468ms step_avg:62.16ms
step:2277/2330 train_time:141529ms step_avg:62.16ms
step:2278/2330 train_time:141592ms step_avg:62.16ms
step:2279/2330 train_time:141653ms step_avg:62.16ms
step:2280/2330 train_time:141716ms step_avg:62.16ms
step:2281/2330 train_time:141776ms step_avg:62.16ms
step:2282/2330 train_time:141841ms step_avg:62.16ms
step:2283/2330 train_time:141903ms step_avg:62.16ms
step:2284/2330 train_time:141968ms step_avg:62.16ms
step:2285/2330 train_time:142031ms step_avg:62.16ms
step:2286/2330 train_time:142096ms step_avg:62.16ms
step:2287/2330 train_time:142158ms step_avg:62.16ms
step:2288/2330 train_time:142224ms step_avg:62.16ms
step:2289/2330 train_time:142285ms step_avg:62.16ms
step:2290/2330 train_time:142349ms step_avg:62.16ms
step:2291/2330 train_time:142409ms step_avg:62.16ms
step:2292/2330 train_time:142473ms step_avg:62.16ms
step:2293/2330 train_time:142533ms step_avg:62.16ms
step:2294/2330 train_time:142597ms step_avg:62.16ms
step:2295/2330 train_time:142658ms step_avg:62.16ms
step:2296/2330 train_time:142721ms step_avg:62.16ms
step:2297/2330 train_time:142783ms step_avg:62.16ms
step:2298/2330 train_time:142847ms step_avg:62.16ms
step:2299/2330 train_time:142908ms step_avg:62.16ms
step:2300/2330 train_time:142972ms step_avg:62.16ms
step:2301/2330 train_time:143034ms step_avg:62.16ms
step:2302/2330 train_time:143098ms step_avg:62.16ms
step:2303/2330 train_time:143160ms step_avg:62.16ms
step:2304/2330 train_time:143225ms step_avg:62.16ms
step:2305/2330 train_time:143287ms step_avg:62.16ms
step:2306/2330 train_time:143351ms step_avg:62.16ms
step:2307/2330 train_time:143412ms step_avg:62.16ms
step:2308/2330 train_time:143476ms step_avg:62.16ms
step:2309/2330 train_time:143536ms step_avg:62.16ms
step:2310/2330 train_time:143600ms step_avg:62.16ms
step:2311/2330 train_time:143662ms step_avg:62.16ms
step:2312/2330 train_time:143725ms step_avg:62.16ms
step:2313/2330 train_time:143786ms step_avg:62.16ms
step:2314/2330 train_time:143850ms step_avg:62.17ms
step:2315/2330 train_time:143911ms step_avg:62.16ms
step:2316/2330 train_time:143975ms step_avg:62.17ms
step:2317/2330 train_time:144037ms step_avg:62.17ms
step:2318/2330 train_time:144101ms step_avg:62.17ms
step:2319/2330 train_time:144163ms step_avg:62.17ms
step:2320/2330 train_time:144229ms step_avg:62.17ms
step:2321/2330 train_time:144291ms step_avg:62.17ms
step:2322/2330 train_time:144355ms step_avg:62.17ms
step:2323/2330 train_time:144416ms step_avg:62.17ms
step:2324/2330 train_time:144481ms step_avg:62.17ms
step:2325/2330 train_time:144542ms step_avg:62.17ms
step:2326/2330 train_time:144606ms step_avg:62.17ms
step:2327/2330 train_time:144666ms step_avg:62.17ms
step:2328/2330 train_time:144730ms step_avg:62.17ms
step:2329/2330 train_time:144791ms step_avg:62.17ms
step:2330/2330 train_time:144855ms step_avg:62.17ms
step:2330/2330 val_loss:4.7190 train_time:144921ms step_avg:62.20ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
