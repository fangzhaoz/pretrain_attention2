import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-4"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:02:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:88ms step_avg:87.55ms
step:2/2330 train_time:191ms step_avg:95.30ms
step:3/2330 train_time:213ms step_avg:70.87ms
step:4/2330 train_time:248ms step_avg:62.07ms
step:5/2330 train_time:305ms step_avg:61.08ms
step:6/2330 train_time:367ms step_avg:61.11ms
step:7/2330 train_time:425ms step_avg:60.74ms
step:8/2330 train_time:487ms step_avg:60.87ms
step:9/2330 train_time:547ms step_avg:60.75ms
step:10/2330 train_time:608ms step_avg:60.85ms
step:11/2330 train_time:667ms step_avg:60.61ms
step:12/2330 train_time:728ms step_avg:60.70ms
step:13/2330 train_time:787ms step_avg:60.51ms
step:14/2330 train_time:849ms step_avg:60.61ms
step:15/2330 train_time:907ms step_avg:60.47ms
step:16/2330 train_time:969ms step_avg:60.58ms
step:17/2330 train_time:1030ms step_avg:60.58ms
step:18/2330 train_time:1095ms step_avg:60.82ms
step:19/2330 train_time:1158ms step_avg:60.95ms
step:20/2330 train_time:1222ms step_avg:61.10ms
step:21/2330 train_time:1282ms step_avg:61.06ms
step:22/2330 train_time:1345ms step_avg:61.14ms
step:23/2330 train_time:1405ms step_avg:61.09ms
step:24/2330 train_time:1467ms step_avg:61.14ms
step:25/2330 train_time:1526ms step_avg:61.05ms
step:26/2330 train_time:1588ms step_avg:61.08ms
step:27/2330 train_time:1647ms step_avg:61.00ms
step:28/2330 train_time:1709ms step_avg:61.04ms
step:29/2330 train_time:1768ms step_avg:60.97ms
step:30/2330 train_time:1830ms step_avg:61.00ms
step:31/2330 train_time:1889ms step_avg:60.93ms
step:32/2330 train_time:1951ms step_avg:60.97ms
step:33/2330 train_time:2010ms step_avg:60.92ms
step:34/2330 train_time:2074ms step_avg:61.01ms
step:35/2330 train_time:2136ms step_avg:61.03ms
step:36/2330 train_time:2200ms step_avg:61.10ms
step:37/2330 train_time:2260ms step_avg:61.07ms
step:38/2330 train_time:2322ms step_avg:61.12ms
step:39/2330 train_time:2382ms step_avg:61.08ms
step:40/2330 train_time:2445ms step_avg:61.14ms
step:41/2330 train_time:2505ms step_avg:61.09ms
step:42/2330 train_time:2567ms step_avg:61.11ms
step:43/2330 train_time:2626ms step_avg:61.08ms
step:44/2330 train_time:2688ms step_avg:61.10ms
step:45/2330 train_time:2747ms step_avg:61.05ms
step:46/2330 train_time:2810ms step_avg:61.08ms
step:47/2330 train_time:2868ms step_avg:61.03ms
step:48/2330 train_time:2931ms step_avg:61.05ms
step:49/2330 train_time:2990ms step_avg:61.02ms
step:50/2330 train_time:3053ms step_avg:61.07ms
step:51/2330 train_time:3114ms step_avg:61.05ms
step:52/2330 train_time:3177ms step_avg:61.10ms
step:53/2330 train_time:3237ms step_avg:61.08ms
step:54/2330 train_time:3300ms step_avg:61.12ms
step:55/2330 train_time:3361ms step_avg:61.10ms
step:56/2330 train_time:3423ms step_avg:61.13ms
step:57/2330 train_time:3482ms step_avg:61.08ms
step:58/2330 train_time:3545ms step_avg:61.11ms
step:59/2330 train_time:3604ms step_avg:61.08ms
step:60/2330 train_time:3666ms step_avg:61.10ms
step:61/2330 train_time:3726ms step_avg:61.08ms
step:62/2330 train_time:3788ms step_avg:61.10ms
step:63/2330 train_time:3848ms step_avg:61.07ms
step:64/2330 train_time:3910ms step_avg:61.10ms
step:65/2330 train_time:3970ms step_avg:61.08ms
step:66/2330 train_time:4033ms step_avg:61.10ms
step:67/2330 train_time:4092ms step_avg:61.08ms
step:68/2330 train_time:4155ms step_avg:61.11ms
step:69/2330 train_time:4216ms step_avg:61.10ms
step:70/2330 train_time:4279ms step_avg:61.12ms
step:71/2330 train_time:4338ms step_avg:61.10ms
step:72/2330 train_time:4401ms step_avg:61.12ms
step:73/2330 train_time:4460ms step_avg:61.10ms
step:74/2330 train_time:4523ms step_avg:61.12ms
step:75/2330 train_time:4582ms step_avg:61.09ms
step:76/2330 train_time:4644ms step_avg:61.10ms
step:77/2330 train_time:4703ms step_avg:61.08ms
step:78/2330 train_time:4766ms step_avg:61.10ms
step:79/2330 train_time:4826ms step_avg:61.09ms
step:80/2330 train_time:4888ms step_avg:61.10ms
step:81/2330 train_time:4949ms step_avg:61.10ms
step:82/2330 train_time:5012ms step_avg:61.12ms
step:83/2330 train_time:5071ms step_avg:61.10ms
step:84/2330 train_time:5134ms step_avg:61.12ms
step:85/2330 train_time:5195ms step_avg:61.12ms
step:86/2330 train_time:5258ms step_avg:61.14ms
step:87/2330 train_time:5318ms step_avg:61.12ms
step:88/2330 train_time:5380ms step_avg:61.14ms
step:89/2330 train_time:5440ms step_avg:61.12ms
step:90/2330 train_time:5502ms step_avg:61.14ms
step:91/2330 train_time:5562ms step_avg:61.12ms
step:92/2330 train_time:5624ms step_avg:61.13ms
step:93/2330 train_time:5683ms step_avg:61.11ms
step:94/2330 train_time:5746ms step_avg:61.13ms
step:95/2330 train_time:5807ms step_avg:61.13ms
step:96/2330 train_time:5869ms step_avg:61.14ms
step:97/2330 train_time:5929ms step_avg:61.12ms
step:98/2330 train_time:5991ms step_avg:61.14ms
step:99/2330 train_time:6052ms step_avg:61.13ms
step:100/2330 train_time:6115ms step_avg:61.15ms
step:101/2330 train_time:6174ms step_avg:61.13ms
step:102/2330 train_time:6238ms step_avg:61.16ms
step:103/2330 train_time:6297ms step_avg:61.14ms
step:104/2330 train_time:6360ms step_avg:61.15ms
step:105/2330 train_time:6419ms step_avg:61.13ms
step:106/2330 train_time:6482ms step_avg:61.15ms
step:107/2330 train_time:6541ms step_avg:61.13ms
step:108/2330 train_time:6603ms step_avg:61.14ms
step:109/2330 train_time:6663ms step_avg:61.13ms
step:110/2330 train_time:6726ms step_avg:61.14ms
step:111/2330 train_time:6786ms step_avg:61.13ms
step:112/2330 train_time:6849ms step_avg:61.15ms
step:113/2330 train_time:6909ms step_avg:61.14ms
step:114/2330 train_time:6971ms step_avg:61.15ms
step:115/2330 train_time:7031ms step_avg:61.14ms
step:116/2330 train_time:7095ms step_avg:61.16ms
step:117/2330 train_time:7155ms step_avg:61.15ms
step:118/2330 train_time:7217ms step_avg:61.16ms
step:119/2330 train_time:7277ms step_avg:61.15ms
step:120/2330 train_time:7340ms step_avg:61.16ms
step:121/2330 train_time:7399ms step_avg:61.15ms
step:122/2330 train_time:7462ms step_avg:61.17ms
step:123/2330 train_time:7522ms step_avg:61.15ms
step:124/2330 train_time:7584ms step_avg:61.16ms
step:125/2330 train_time:7644ms step_avg:61.15ms
step:126/2330 train_time:7706ms step_avg:61.16ms
step:127/2330 train_time:7766ms step_avg:61.15ms
step:128/2330 train_time:7828ms step_avg:61.16ms
step:129/2330 train_time:7888ms step_avg:61.15ms
step:130/2330 train_time:7951ms step_avg:61.16ms
step:131/2330 train_time:8012ms step_avg:61.16ms
step:132/2330 train_time:8074ms step_avg:61.17ms
step:133/2330 train_time:8134ms step_avg:61.16ms
step:134/2330 train_time:8197ms step_avg:61.17ms
step:135/2330 train_time:8257ms step_avg:61.16ms
step:136/2330 train_time:8320ms step_avg:61.18ms
step:137/2330 train_time:8379ms step_avg:61.16ms
step:138/2330 train_time:8441ms step_avg:61.17ms
step:139/2330 train_time:8501ms step_avg:61.16ms
step:140/2330 train_time:8564ms step_avg:61.17ms
step:141/2330 train_time:8624ms step_avg:61.16ms
step:142/2330 train_time:8687ms step_avg:61.18ms
step:143/2330 train_time:8747ms step_avg:61.17ms
step:144/2330 train_time:8809ms step_avg:61.18ms
step:145/2330 train_time:8869ms step_avg:61.17ms
step:146/2330 train_time:8932ms step_avg:61.18ms
step:147/2330 train_time:8991ms step_avg:61.16ms
step:148/2330 train_time:9054ms step_avg:61.18ms
step:149/2330 train_time:9114ms step_avg:61.17ms
step:150/2330 train_time:9177ms step_avg:61.18ms
step:151/2330 train_time:9237ms step_avg:61.17ms
step:152/2330 train_time:9299ms step_avg:61.18ms
step:153/2330 train_time:9359ms step_avg:61.17ms
step:154/2330 train_time:9421ms step_avg:61.17ms
step:155/2330 train_time:9481ms step_avg:61.17ms
step:156/2330 train_time:9543ms step_avg:61.18ms
step:157/2330 train_time:9603ms step_avg:61.17ms
step:158/2330 train_time:9666ms step_avg:61.18ms
step:159/2330 train_time:9726ms step_avg:61.17ms
step:160/2330 train_time:9789ms step_avg:61.18ms
step:161/2330 train_time:9849ms step_avg:61.17ms
step:162/2330 train_time:9911ms step_avg:61.18ms
step:163/2330 train_time:9971ms step_avg:61.17ms
step:164/2330 train_time:10033ms step_avg:61.18ms
step:165/2330 train_time:10093ms step_avg:61.17ms
step:166/2330 train_time:10157ms step_avg:61.19ms
step:167/2330 train_time:10217ms step_avg:61.18ms
step:168/2330 train_time:10279ms step_avg:61.18ms
step:169/2330 train_time:10339ms step_avg:61.18ms
step:170/2330 train_time:10401ms step_avg:61.18ms
step:171/2330 train_time:10460ms step_avg:61.17ms
step:172/2330 train_time:10522ms step_avg:61.18ms
step:173/2330 train_time:10582ms step_avg:61.17ms
step:174/2330 train_time:10644ms step_avg:61.18ms
step:175/2330 train_time:10704ms step_avg:61.16ms
step:176/2330 train_time:10767ms step_avg:61.17ms
step:177/2330 train_time:10826ms step_avg:61.17ms
step:178/2330 train_time:10890ms step_avg:61.18ms
step:179/2330 train_time:10951ms step_avg:61.18ms
step:180/2330 train_time:11013ms step_avg:61.19ms
step:181/2330 train_time:11074ms step_avg:61.18ms
step:182/2330 train_time:11137ms step_avg:61.19ms
step:183/2330 train_time:11197ms step_avg:61.18ms
step:184/2330 train_time:11259ms step_avg:61.19ms
step:185/2330 train_time:11318ms step_avg:61.18ms
step:186/2330 train_time:11380ms step_avg:61.18ms
step:187/2330 train_time:11439ms step_avg:61.17ms
step:188/2330 train_time:11502ms step_avg:61.18ms
step:189/2330 train_time:11562ms step_avg:61.17ms
step:190/2330 train_time:11624ms step_avg:61.18ms
step:191/2330 train_time:11684ms step_avg:61.17ms
step:192/2330 train_time:11747ms step_avg:61.18ms
step:193/2330 train_time:11808ms step_avg:61.18ms
step:194/2330 train_time:11871ms step_avg:61.19ms
step:195/2330 train_time:11931ms step_avg:61.18ms
step:196/2330 train_time:11994ms step_avg:61.19ms
step:197/2330 train_time:12055ms step_avg:61.19ms
step:198/2330 train_time:12117ms step_avg:61.20ms
step:199/2330 train_time:12176ms step_avg:61.19ms
step:200/2330 train_time:12239ms step_avg:61.19ms
step:201/2330 train_time:12298ms step_avg:61.18ms
step:202/2330 train_time:12361ms step_avg:61.19ms
step:203/2330 train_time:12420ms step_avg:61.18ms
step:204/2330 train_time:12482ms step_avg:61.19ms
step:205/2330 train_time:12542ms step_avg:61.18ms
step:206/2330 train_time:12605ms step_avg:61.19ms
step:207/2330 train_time:12664ms step_avg:61.18ms
step:208/2330 train_time:12727ms step_avg:61.19ms
step:209/2330 train_time:12786ms step_avg:61.18ms
step:210/2330 train_time:12850ms step_avg:61.19ms
step:211/2330 train_time:12910ms step_avg:61.18ms
step:212/2330 train_time:12973ms step_avg:61.19ms
step:213/2330 train_time:13034ms step_avg:61.19ms
step:214/2330 train_time:13096ms step_avg:61.20ms
step:215/2330 train_time:13156ms step_avg:61.19ms
step:216/2330 train_time:13219ms step_avg:61.20ms
step:217/2330 train_time:13279ms step_avg:61.19ms
step:218/2330 train_time:13341ms step_avg:61.20ms
step:219/2330 train_time:13400ms step_avg:61.19ms
step:220/2330 train_time:13464ms step_avg:61.20ms
step:221/2330 train_time:13524ms step_avg:61.19ms
step:222/2330 train_time:13586ms step_avg:61.20ms
step:223/2330 train_time:13645ms step_avg:61.19ms
step:224/2330 train_time:13709ms step_avg:61.20ms
step:225/2330 train_time:13769ms step_avg:61.19ms
step:226/2330 train_time:13831ms step_avg:61.20ms
step:227/2330 train_time:13891ms step_avg:61.19ms
step:228/2330 train_time:13954ms step_avg:61.20ms
step:229/2330 train_time:14013ms step_avg:61.19ms
step:230/2330 train_time:14076ms step_avg:61.20ms
step:231/2330 train_time:14136ms step_avg:61.19ms
step:232/2330 train_time:14198ms step_avg:61.20ms
step:233/2330 train_time:14258ms step_avg:61.19ms
step:234/2330 train_time:14320ms step_avg:61.20ms
step:235/2330 train_time:14379ms step_avg:61.19ms
step:236/2330 train_time:14441ms step_avg:61.19ms
step:237/2330 train_time:14501ms step_avg:61.19ms
step:238/2330 train_time:14563ms step_avg:61.19ms
step:239/2330 train_time:14624ms step_avg:61.19ms
step:240/2330 train_time:14686ms step_avg:61.19ms
step:241/2330 train_time:14747ms step_avg:61.19ms
step:242/2330 train_time:14811ms step_avg:61.20ms
step:243/2330 train_time:14870ms step_avg:61.20ms
step:244/2330 train_time:14934ms step_avg:61.20ms
step:245/2330 train_time:14993ms step_avg:61.20ms
step:246/2330 train_time:15057ms step_avg:61.21ms
step:247/2330 train_time:15117ms step_avg:61.20ms
step:248/2330 train_time:15179ms step_avg:61.21ms
step:249/2330 train_time:15239ms step_avg:61.20ms
step:250/2330 train_time:15301ms step_avg:61.21ms
step:250/2330 val_loss:4.8966 train_time:15365ms step_avg:61.46ms
step:251/2330 train_time:15389ms step_avg:61.31ms
step:252/2330 train_time:15426ms step_avg:61.21ms
step:253/2330 train_time:15493ms step_avg:61.24ms
step:254/2330 train_time:15562ms step_avg:61.27ms
step:255/2330 train_time:15622ms step_avg:61.26ms
step:256/2330 train_time:15686ms step_avg:61.27ms
step:257/2330 train_time:15746ms step_avg:61.27ms
step:258/2330 train_time:15809ms step_avg:61.27ms
step:259/2330 train_time:15869ms step_avg:61.27ms
step:260/2330 train_time:15931ms step_avg:61.27ms
step:261/2330 train_time:15989ms step_avg:61.26ms
step:262/2330 train_time:16051ms step_avg:61.26ms
step:263/2330 train_time:16110ms step_avg:61.25ms
step:264/2330 train_time:16172ms step_avg:61.26ms
step:265/2330 train_time:16231ms step_avg:61.25ms
step:266/2330 train_time:16293ms step_avg:61.25ms
step:267/2330 train_time:16353ms step_avg:61.25ms
step:268/2330 train_time:16418ms step_avg:61.26ms
step:269/2330 train_time:16479ms step_avg:61.26ms
step:270/2330 train_time:16542ms step_avg:61.27ms
step:271/2330 train_time:16603ms step_avg:61.27ms
step:272/2330 train_time:16666ms step_avg:61.27ms
step:273/2330 train_time:16726ms step_avg:61.27ms
step:274/2330 train_time:16789ms step_avg:61.28ms
step:275/2330 train_time:16850ms step_avg:61.27ms
step:276/2330 train_time:16913ms step_avg:61.28ms
step:277/2330 train_time:16972ms step_avg:61.27ms
step:278/2330 train_time:17035ms step_avg:61.28ms
step:279/2330 train_time:17094ms step_avg:61.27ms
step:280/2330 train_time:17156ms step_avg:61.27ms
step:281/2330 train_time:17215ms step_avg:61.26ms
step:282/2330 train_time:17277ms step_avg:61.27ms
step:283/2330 train_time:17337ms step_avg:61.26ms
step:284/2330 train_time:17399ms step_avg:61.26ms
step:285/2330 train_time:17459ms step_avg:61.26ms
step:286/2330 train_time:17522ms step_avg:61.27ms
step:287/2330 train_time:17582ms step_avg:61.26ms
step:288/2330 train_time:17646ms step_avg:61.27ms
step:289/2330 train_time:17706ms step_avg:61.27ms
step:290/2330 train_time:17769ms step_avg:61.27ms
step:291/2330 train_time:17829ms step_avg:61.27ms
step:292/2330 train_time:17892ms step_avg:61.27ms
step:293/2330 train_time:17953ms step_avg:61.27ms
step:294/2330 train_time:18015ms step_avg:61.28ms
step:295/2330 train_time:18075ms step_avg:61.27ms
step:296/2330 train_time:18137ms step_avg:61.27ms
step:297/2330 train_time:18196ms step_avg:61.26ms
step:298/2330 train_time:18258ms step_avg:61.27ms
step:299/2330 train_time:18317ms step_avg:61.26ms
step:300/2330 train_time:18380ms step_avg:61.27ms
step:301/2330 train_time:18439ms step_avg:61.26ms
step:302/2330 train_time:18502ms step_avg:61.26ms
step:303/2330 train_time:18562ms step_avg:61.26ms
step:304/2330 train_time:18625ms step_avg:61.27ms
step:305/2330 train_time:18685ms step_avg:61.26ms
step:306/2330 train_time:18749ms step_avg:61.27ms
step:307/2330 train_time:18809ms step_avg:61.27ms
step:308/2330 train_time:18872ms step_avg:61.27ms
step:309/2330 train_time:18931ms step_avg:61.27ms
step:310/2330 train_time:18994ms step_avg:61.27ms
step:311/2330 train_time:19053ms step_avg:61.26ms
step:312/2330 train_time:19116ms step_avg:61.27ms
step:313/2330 train_time:19175ms step_avg:61.26ms
step:314/2330 train_time:19237ms step_avg:61.27ms
step:315/2330 train_time:19298ms step_avg:61.26ms
step:316/2330 train_time:19360ms step_avg:61.27ms
step:317/2330 train_time:19419ms step_avg:61.26ms
step:318/2330 train_time:19482ms step_avg:61.26ms
step:319/2330 train_time:19541ms step_avg:61.26ms
step:320/2330 train_time:19604ms step_avg:61.26ms
step:321/2330 train_time:19664ms step_avg:61.26ms
step:322/2330 train_time:19727ms step_avg:61.26ms
step:323/2330 train_time:19787ms step_avg:61.26ms
step:324/2330 train_time:19851ms step_avg:61.27ms
step:325/2330 train_time:19910ms step_avg:61.26ms
step:326/2330 train_time:19973ms step_avg:61.27ms
step:327/2330 train_time:20032ms step_avg:61.26ms
step:328/2330 train_time:20095ms step_avg:61.27ms
step:329/2330 train_time:20155ms step_avg:61.26ms
step:330/2330 train_time:20218ms step_avg:61.27ms
step:331/2330 train_time:20278ms step_avg:61.26ms
step:332/2330 train_time:20340ms step_avg:61.27ms
step:333/2330 train_time:20399ms step_avg:61.26ms
step:334/2330 train_time:20461ms step_avg:61.26ms
step:335/2330 train_time:20521ms step_avg:61.26ms
step:336/2330 train_time:20584ms step_avg:61.26ms
step:337/2330 train_time:20643ms step_avg:61.26ms
step:338/2330 train_time:20707ms step_avg:61.26ms
step:339/2330 train_time:20767ms step_avg:61.26ms
step:340/2330 train_time:20830ms step_avg:61.27ms
step:341/2330 train_time:20891ms step_avg:61.26ms
step:342/2330 train_time:20954ms step_avg:61.27ms
step:343/2330 train_time:21013ms step_avg:61.26ms
step:344/2330 train_time:21076ms step_avg:61.27ms
step:345/2330 train_time:21135ms step_avg:61.26ms
step:346/2330 train_time:21198ms step_avg:61.27ms
step:347/2330 train_time:21258ms step_avg:61.26ms
step:348/2330 train_time:21320ms step_avg:61.26ms
step:349/2330 train_time:21379ms step_avg:61.26ms
step:350/2330 train_time:21442ms step_avg:61.26ms
step:351/2330 train_time:21502ms step_avg:61.26ms
step:352/2330 train_time:21564ms step_avg:61.26ms
step:353/2330 train_time:21624ms step_avg:61.26ms
step:354/2330 train_time:21687ms step_avg:61.26ms
step:355/2330 train_time:21747ms step_avg:61.26ms
step:356/2330 train_time:21811ms step_avg:61.27ms
step:357/2330 train_time:21871ms step_avg:61.26ms
step:358/2330 train_time:21934ms step_avg:61.27ms
step:359/2330 train_time:21995ms step_avg:61.27ms
step:360/2330 train_time:22058ms step_avg:61.27ms
step:361/2330 train_time:22118ms step_avg:61.27ms
step:362/2330 train_time:22180ms step_avg:61.27ms
step:363/2330 train_time:22240ms step_avg:61.27ms
step:364/2330 train_time:22302ms step_avg:61.27ms
step:365/2330 train_time:22362ms step_avg:61.27ms
step:366/2330 train_time:22425ms step_avg:61.27ms
step:367/2330 train_time:22485ms step_avg:61.27ms
step:368/2330 train_time:22547ms step_avg:61.27ms
step:369/2330 train_time:22607ms step_avg:61.26ms
step:370/2330 train_time:22670ms step_avg:61.27ms
step:371/2330 train_time:22730ms step_avg:61.27ms
step:372/2330 train_time:22793ms step_avg:61.27ms
step:373/2330 train_time:22852ms step_avg:61.27ms
step:374/2330 train_time:22916ms step_avg:61.27ms
step:375/2330 train_time:22975ms step_avg:61.27ms
step:376/2330 train_time:23037ms step_avg:61.27ms
step:377/2330 train_time:23097ms step_avg:61.26ms
step:378/2330 train_time:23159ms step_avg:61.27ms
step:379/2330 train_time:23218ms step_avg:61.26ms
step:380/2330 train_time:23281ms step_avg:61.27ms
step:381/2330 train_time:23341ms step_avg:61.26ms
step:382/2330 train_time:23403ms step_avg:61.27ms
step:383/2330 train_time:23463ms step_avg:61.26ms
step:384/2330 train_time:23526ms step_avg:61.27ms
step:385/2330 train_time:23586ms step_avg:61.26ms
step:386/2330 train_time:23649ms step_avg:61.27ms
step:387/2330 train_time:23710ms step_avg:61.26ms
step:388/2330 train_time:23773ms step_avg:61.27ms
step:389/2330 train_time:23833ms step_avg:61.27ms
step:390/2330 train_time:23895ms step_avg:61.27ms
step:391/2330 train_time:23955ms step_avg:61.27ms
step:392/2330 train_time:24019ms step_avg:61.27ms
step:393/2330 train_time:24078ms step_avg:61.27ms
step:394/2330 train_time:24140ms step_avg:61.27ms
step:395/2330 train_time:24200ms step_avg:61.27ms
step:396/2330 train_time:24262ms step_avg:61.27ms
step:397/2330 train_time:24322ms step_avg:61.26ms
step:398/2330 train_time:24385ms step_avg:61.27ms
step:399/2330 train_time:24444ms step_avg:61.26ms
step:400/2330 train_time:24507ms step_avg:61.27ms
step:401/2330 train_time:24567ms step_avg:61.26ms
step:402/2330 train_time:24630ms step_avg:61.27ms
step:403/2330 train_time:24691ms step_avg:61.27ms
step:404/2330 train_time:24754ms step_avg:61.27ms
step:405/2330 train_time:24814ms step_avg:61.27ms
step:406/2330 train_time:24877ms step_avg:61.27ms
step:407/2330 train_time:24938ms step_avg:61.27ms
step:408/2330 train_time:25001ms step_avg:61.28ms
step:409/2330 train_time:25061ms step_avg:61.27ms
step:410/2330 train_time:25124ms step_avg:61.28ms
step:411/2330 train_time:25183ms step_avg:61.27ms
step:412/2330 train_time:25246ms step_avg:61.28ms
step:413/2330 train_time:25306ms step_avg:61.27ms
step:414/2330 train_time:25369ms step_avg:61.28ms
step:415/2330 train_time:25429ms step_avg:61.27ms
step:416/2330 train_time:25491ms step_avg:61.28ms
step:417/2330 train_time:25551ms step_avg:61.27ms
step:418/2330 train_time:25613ms step_avg:61.28ms
step:419/2330 train_time:25673ms step_avg:61.27ms
step:420/2330 train_time:25736ms step_avg:61.28ms
step:421/2330 train_time:25795ms step_avg:61.27ms
step:422/2330 train_time:25858ms step_avg:61.27ms
step:423/2330 train_time:25918ms step_avg:61.27ms
step:424/2330 train_time:25981ms step_avg:61.27ms
step:425/2330 train_time:26040ms step_avg:61.27ms
step:426/2330 train_time:26104ms step_avg:61.28ms
step:427/2330 train_time:26164ms step_avg:61.27ms
step:428/2330 train_time:26227ms step_avg:61.28ms
step:429/2330 train_time:26288ms step_avg:61.28ms
step:430/2330 train_time:26350ms step_avg:61.28ms
step:431/2330 train_time:26409ms step_avg:61.27ms
step:432/2330 train_time:26472ms step_avg:61.28ms
step:433/2330 train_time:26532ms step_avg:61.27ms
step:434/2330 train_time:26594ms step_avg:61.28ms
step:435/2330 train_time:26655ms step_avg:61.27ms
step:436/2330 train_time:26717ms step_avg:61.28ms
step:437/2330 train_time:26777ms step_avg:61.27ms
step:438/2330 train_time:26839ms step_avg:61.28ms
step:439/2330 train_time:26899ms step_avg:61.27ms
step:440/2330 train_time:26961ms step_avg:61.27ms
step:441/2330 train_time:27021ms step_avg:61.27ms
step:442/2330 train_time:27084ms step_avg:61.28ms
step:443/2330 train_time:27144ms step_avg:61.27ms
step:444/2330 train_time:27207ms step_avg:61.28ms
step:445/2330 train_time:27267ms step_avg:61.27ms
step:446/2330 train_time:27330ms step_avg:61.28ms
step:447/2330 train_time:27390ms step_avg:61.28ms
step:448/2330 train_time:27453ms step_avg:61.28ms
step:449/2330 train_time:27514ms step_avg:61.28ms
step:450/2330 train_time:27577ms step_avg:61.28ms
step:451/2330 train_time:27637ms step_avg:61.28ms
step:452/2330 train_time:27700ms step_avg:61.28ms
step:453/2330 train_time:27760ms step_avg:61.28ms
step:454/2330 train_time:27823ms step_avg:61.28ms
step:455/2330 train_time:27882ms step_avg:61.28ms
step:456/2330 train_time:27945ms step_avg:61.28ms
step:457/2330 train_time:28005ms step_avg:61.28ms
step:458/2330 train_time:28069ms step_avg:61.29ms
step:459/2330 train_time:28129ms step_avg:61.28ms
step:460/2330 train_time:28192ms step_avg:61.29ms
step:461/2330 train_time:28251ms step_avg:61.28ms
step:462/2330 train_time:28314ms step_avg:61.29ms
step:463/2330 train_time:28373ms step_avg:61.28ms
step:464/2330 train_time:28436ms step_avg:61.29ms
step:465/2330 train_time:28496ms step_avg:61.28ms
step:466/2330 train_time:28558ms step_avg:61.28ms
step:467/2330 train_time:28619ms step_avg:61.28ms
step:468/2330 train_time:28682ms step_avg:61.29ms
step:469/2330 train_time:28742ms step_avg:61.28ms
step:470/2330 train_time:28805ms step_avg:61.29ms
step:471/2330 train_time:28866ms step_avg:61.29ms
step:472/2330 train_time:28928ms step_avg:61.29ms
step:473/2330 train_time:28988ms step_avg:61.29ms
step:474/2330 train_time:29050ms step_avg:61.29ms
step:475/2330 train_time:29110ms step_avg:61.28ms
step:476/2330 train_time:29172ms step_avg:61.29ms
step:477/2330 train_time:29232ms step_avg:61.28ms
step:478/2330 train_time:29294ms step_avg:61.29ms
step:479/2330 train_time:29354ms step_avg:61.28ms
step:480/2330 train_time:29417ms step_avg:61.29ms
step:481/2330 train_time:29477ms step_avg:61.28ms
step:482/2330 train_time:29540ms step_avg:61.29ms
step:483/2330 train_time:29600ms step_avg:61.28ms
step:484/2330 train_time:29662ms step_avg:61.29ms
step:485/2330 train_time:29722ms step_avg:61.28ms
step:486/2330 train_time:29785ms step_avg:61.29ms
step:487/2330 train_time:29845ms step_avg:61.28ms
step:488/2330 train_time:29909ms step_avg:61.29ms
step:489/2330 train_time:29969ms step_avg:61.29ms
step:490/2330 train_time:30032ms step_avg:61.29ms
step:491/2330 train_time:30091ms step_avg:61.29ms
step:492/2330 train_time:30155ms step_avg:61.29ms
step:493/2330 train_time:30215ms step_avg:61.29ms
step:494/2330 train_time:30277ms step_avg:61.29ms
step:495/2330 train_time:30337ms step_avg:61.29ms
step:496/2330 train_time:30400ms step_avg:61.29ms
step:497/2330 train_time:30459ms step_avg:61.29ms
step:498/2330 train_time:30522ms step_avg:61.29ms
step:499/2330 train_time:30582ms step_avg:61.29ms
step:500/2330 train_time:30645ms step_avg:61.29ms
step:500/2330 val_loss:4.2732 train_time:30709ms step_avg:61.42ms
step:501/2330 train_time:30733ms step_avg:61.34ms
step:502/2330 train_time:30770ms step_avg:61.29ms
step:503/2330 train_time:30834ms step_avg:61.30ms
step:504/2330 train_time:30900ms step_avg:61.31ms
step:505/2330 train_time:30960ms step_avg:61.31ms
step:506/2330 train_time:31023ms step_avg:61.31ms
step:507/2330 train_time:31082ms step_avg:61.31ms
step:508/2330 train_time:31144ms step_avg:61.31ms
step:509/2330 train_time:31204ms step_avg:61.30ms
step:510/2330 train_time:31265ms step_avg:61.30ms
step:511/2330 train_time:31324ms step_avg:61.30ms
step:512/2330 train_time:31386ms step_avg:61.30ms
step:513/2330 train_time:31445ms step_avg:61.30ms
step:514/2330 train_time:31508ms step_avg:61.30ms
step:515/2330 train_time:31567ms step_avg:61.29ms
step:516/2330 train_time:31631ms step_avg:61.30ms
step:517/2330 train_time:31692ms step_avg:61.30ms
step:518/2330 train_time:31755ms step_avg:61.30ms
step:519/2330 train_time:31816ms step_avg:61.30ms
step:520/2330 train_time:31880ms step_avg:61.31ms
step:521/2330 train_time:31940ms step_avg:61.31ms
step:522/2330 train_time:32003ms step_avg:61.31ms
step:523/2330 train_time:32063ms step_avg:61.31ms
step:524/2330 train_time:32126ms step_avg:61.31ms
step:525/2330 train_time:32185ms step_avg:61.30ms
step:526/2330 train_time:32248ms step_avg:61.31ms
step:527/2330 train_time:32306ms step_avg:61.30ms
step:528/2330 train_time:32369ms step_avg:61.30ms
step:529/2330 train_time:32428ms step_avg:61.30ms
step:530/2330 train_time:32490ms step_avg:61.30ms
step:531/2330 train_time:32549ms step_avg:61.30ms
step:532/2330 train_time:32612ms step_avg:61.30ms
step:533/2330 train_time:32673ms step_avg:61.30ms
step:534/2330 train_time:32737ms step_avg:61.30ms
step:535/2330 train_time:32798ms step_avg:61.30ms
step:536/2330 train_time:32861ms step_avg:61.31ms
step:537/2330 train_time:32921ms step_avg:61.31ms
step:538/2330 train_time:32984ms step_avg:61.31ms
step:539/2330 train_time:33043ms step_avg:61.30ms
step:540/2330 train_time:33106ms step_avg:61.31ms
step:541/2330 train_time:33165ms step_avg:61.30ms
step:542/2330 train_time:33228ms step_avg:61.31ms
step:543/2330 train_time:33287ms step_avg:61.30ms
step:544/2330 train_time:33350ms step_avg:61.30ms
step:545/2330 train_time:33409ms step_avg:61.30ms
step:546/2330 train_time:33471ms step_avg:61.30ms
step:547/2330 train_time:33530ms step_avg:61.30ms
step:548/2330 train_time:33592ms step_avg:61.30ms
step:549/2330 train_time:33652ms step_avg:61.30ms
step:550/2330 train_time:33716ms step_avg:61.30ms
step:551/2330 train_time:33777ms step_avg:61.30ms
step:552/2330 train_time:33841ms step_avg:61.31ms
step:553/2330 train_time:33901ms step_avg:61.30ms
step:554/2330 train_time:33963ms step_avg:61.31ms
step:555/2330 train_time:34024ms step_avg:61.30ms
step:556/2330 train_time:34087ms step_avg:61.31ms
step:557/2330 train_time:34147ms step_avg:61.30ms
step:558/2330 train_time:34209ms step_avg:61.31ms
step:559/2330 train_time:34269ms step_avg:61.30ms
step:560/2330 train_time:34331ms step_avg:61.31ms
step:561/2330 train_time:34391ms step_avg:61.30ms
step:562/2330 train_time:34453ms step_avg:61.31ms
step:563/2330 train_time:34513ms step_avg:61.30ms
step:564/2330 train_time:34576ms step_avg:61.30ms
step:565/2330 train_time:34635ms step_avg:61.30ms
step:566/2330 train_time:34698ms step_avg:61.30ms
step:567/2330 train_time:34758ms step_avg:61.30ms
step:568/2330 train_time:34821ms step_avg:61.31ms
step:569/2330 train_time:34881ms step_avg:61.30ms
step:570/2330 train_time:34944ms step_avg:61.31ms
step:571/2330 train_time:35004ms step_avg:61.30ms
step:572/2330 train_time:35067ms step_avg:61.31ms
step:573/2330 train_time:35127ms step_avg:61.30ms
step:574/2330 train_time:35190ms step_avg:61.31ms
step:575/2330 train_time:35250ms step_avg:61.30ms
step:576/2330 train_time:35312ms step_avg:61.31ms
step:577/2330 train_time:35372ms step_avg:61.30ms
step:578/2330 train_time:35434ms step_avg:61.31ms
step:579/2330 train_time:35494ms step_avg:61.30ms
step:580/2330 train_time:35557ms step_avg:61.30ms
step:581/2330 train_time:35616ms step_avg:61.30ms
step:582/2330 train_time:35680ms step_avg:61.31ms
step:583/2330 train_time:35740ms step_avg:61.30ms
step:584/2330 train_time:35804ms step_avg:61.31ms
step:585/2330 train_time:35863ms step_avg:61.31ms
step:586/2330 train_time:35927ms step_avg:61.31ms
step:587/2330 train_time:35987ms step_avg:61.31ms
step:588/2330 train_time:36050ms step_avg:61.31ms
step:589/2330 train_time:36109ms step_avg:61.31ms
step:590/2330 train_time:36172ms step_avg:61.31ms
step:591/2330 train_time:36232ms step_avg:61.31ms
step:592/2330 train_time:36295ms step_avg:61.31ms
step:593/2330 train_time:36355ms step_avg:61.31ms
step:594/2330 train_time:36418ms step_avg:61.31ms
step:595/2330 train_time:36477ms step_avg:61.31ms
step:596/2330 train_time:36540ms step_avg:61.31ms
step:597/2330 train_time:36600ms step_avg:61.31ms
step:598/2330 train_time:36662ms step_avg:61.31ms
step:599/2330 train_time:36722ms step_avg:61.31ms
step:600/2330 train_time:36785ms step_avg:61.31ms
step:601/2330 train_time:36845ms step_avg:61.31ms
step:602/2330 train_time:36909ms step_avg:61.31ms
step:603/2330 train_time:36968ms step_avg:61.31ms
step:604/2330 train_time:37030ms step_avg:61.31ms
step:605/2330 train_time:37090ms step_avg:61.31ms
step:606/2330 train_time:37153ms step_avg:61.31ms
step:607/2330 train_time:37213ms step_avg:61.31ms
step:608/2330 train_time:37276ms step_avg:61.31ms
step:609/2330 train_time:37335ms step_avg:61.31ms
step:610/2330 train_time:37398ms step_avg:61.31ms
step:611/2330 train_time:37459ms step_avg:61.31ms
step:612/2330 train_time:37522ms step_avg:61.31ms
step:613/2330 train_time:37582ms step_avg:61.31ms
step:614/2330 train_time:37646ms step_avg:61.31ms
step:615/2330 train_time:37706ms step_avg:61.31ms
step:616/2330 train_time:37769ms step_avg:61.31ms
step:617/2330 train_time:37828ms step_avg:61.31ms
step:618/2330 train_time:37891ms step_avg:61.31ms
step:619/2330 train_time:37952ms step_avg:61.31ms
step:620/2330 train_time:38015ms step_avg:61.31ms
step:621/2330 train_time:38075ms step_avg:61.31ms
step:622/2330 train_time:38138ms step_avg:61.31ms
step:623/2330 train_time:38198ms step_avg:61.31ms
step:624/2330 train_time:38261ms step_avg:61.32ms
step:625/2330 train_time:38320ms step_avg:61.31ms
step:626/2330 train_time:38382ms step_avg:61.31ms
step:627/2330 train_time:38442ms step_avg:61.31ms
step:628/2330 train_time:38505ms step_avg:61.31ms
step:629/2330 train_time:38565ms step_avg:61.31ms
step:630/2330 train_time:38628ms step_avg:61.31ms
step:631/2330 train_time:38687ms step_avg:61.31ms
step:632/2330 train_time:38750ms step_avg:61.31ms
step:633/2330 train_time:38810ms step_avg:61.31ms
step:634/2330 train_time:38873ms step_avg:61.31ms
step:635/2330 train_time:38933ms step_avg:61.31ms
step:636/2330 train_time:38997ms step_avg:61.32ms
step:637/2330 train_time:39057ms step_avg:61.31ms
step:638/2330 train_time:39120ms step_avg:61.32ms
step:639/2330 train_time:39180ms step_avg:61.31ms
step:640/2330 train_time:39243ms step_avg:61.32ms
step:641/2330 train_time:39302ms step_avg:61.31ms
step:642/2330 train_time:39364ms step_avg:61.31ms
step:643/2330 train_time:39424ms step_avg:61.31ms
step:644/2330 train_time:39486ms step_avg:61.31ms
step:645/2330 train_time:39546ms step_avg:61.31ms
step:646/2330 train_time:39609ms step_avg:61.31ms
step:647/2330 train_time:39668ms step_avg:61.31ms
step:648/2330 train_time:39731ms step_avg:61.31ms
step:649/2330 train_time:39790ms step_avg:61.31ms
step:650/2330 train_time:39854ms step_avg:61.31ms
step:651/2330 train_time:39913ms step_avg:61.31ms
step:652/2330 train_time:39976ms step_avg:61.31ms
step:653/2330 train_time:40037ms step_avg:61.31ms
step:654/2330 train_time:40100ms step_avg:61.32ms
step:655/2330 train_time:40160ms step_avg:61.31ms
step:656/2330 train_time:40223ms step_avg:61.32ms
step:657/2330 train_time:40283ms step_avg:61.31ms
step:658/2330 train_time:40346ms step_avg:61.32ms
step:659/2330 train_time:40406ms step_avg:61.31ms
step:660/2330 train_time:40469ms step_avg:61.32ms
step:661/2330 train_time:40528ms step_avg:61.31ms
step:662/2330 train_time:40591ms step_avg:61.32ms
step:663/2330 train_time:40650ms step_avg:61.31ms
step:664/2330 train_time:40713ms step_avg:61.32ms
step:665/2330 train_time:40773ms step_avg:61.31ms
step:666/2330 train_time:40837ms step_avg:61.32ms
step:667/2330 train_time:40897ms step_avg:61.31ms
step:668/2330 train_time:40960ms step_avg:61.32ms
step:669/2330 train_time:41019ms step_avg:61.31ms
step:670/2330 train_time:41082ms step_avg:61.32ms
step:671/2330 train_time:41141ms step_avg:61.31ms
step:672/2330 train_time:41205ms step_avg:61.32ms
step:673/2330 train_time:41264ms step_avg:61.31ms
step:674/2330 train_time:41327ms step_avg:61.32ms
step:675/2330 train_time:41387ms step_avg:61.31ms
step:676/2330 train_time:41451ms step_avg:61.32ms
step:677/2330 train_time:41510ms step_avg:61.31ms
step:678/2330 train_time:41573ms step_avg:61.32ms
step:679/2330 train_time:41633ms step_avg:61.32ms
step:680/2330 train_time:41697ms step_avg:61.32ms
step:681/2330 train_time:41757ms step_avg:61.32ms
step:682/2330 train_time:41819ms step_avg:61.32ms
step:683/2330 train_time:41880ms step_avg:61.32ms
step:684/2330 train_time:41944ms step_avg:61.32ms
step:685/2330 train_time:42003ms step_avg:61.32ms
step:686/2330 train_time:42066ms step_avg:61.32ms
step:687/2330 train_time:42125ms step_avg:61.32ms
step:688/2330 train_time:42188ms step_avg:61.32ms
step:689/2330 train_time:42248ms step_avg:61.32ms
step:690/2330 train_time:42310ms step_avg:61.32ms
step:691/2330 train_time:42370ms step_avg:61.32ms
step:692/2330 train_time:42433ms step_avg:61.32ms
step:693/2330 train_time:42494ms step_avg:61.32ms
step:694/2330 train_time:42557ms step_avg:61.32ms
step:695/2330 train_time:42617ms step_avg:61.32ms
step:696/2330 train_time:42680ms step_avg:61.32ms
step:697/2330 train_time:42740ms step_avg:61.32ms
step:698/2330 train_time:42802ms step_avg:61.32ms
step:699/2330 train_time:42862ms step_avg:61.32ms
step:700/2330 train_time:42924ms step_avg:61.32ms
step:701/2330 train_time:42985ms step_avg:61.32ms
step:702/2330 train_time:43048ms step_avg:61.32ms
step:703/2330 train_time:43107ms step_avg:61.32ms
step:704/2330 train_time:43170ms step_avg:61.32ms
step:705/2330 train_time:43229ms step_avg:61.32ms
step:706/2330 train_time:43292ms step_avg:61.32ms
step:707/2330 train_time:43353ms step_avg:61.32ms
step:708/2330 train_time:43415ms step_avg:61.32ms
step:709/2330 train_time:43475ms step_avg:61.32ms
step:710/2330 train_time:43538ms step_avg:61.32ms
step:711/2330 train_time:43598ms step_avg:61.32ms
step:712/2330 train_time:43661ms step_avg:61.32ms
step:713/2330 train_time:43720ms step_avg:61.32ms
step:714/2330 train_time:43783ms step_avg:61.32ms
step:715/2330 train_time:43844ms step_avg:61.32ms
step:716/2330 train_time:43907ms step_avg:61.32ms
step:717/2330 train_time:43967ms step_avg:61.32ms
step:718/2330 train_time:44029ms step_avg:61.32ms
step:719/2330 train_time:44089ms step_avg:61.32ms
step:720/2330 train_time:44151ms step_avg:61.32ms
step:721/2330 train_time:44211ms step_avg:61.32ms
step:722/2330 train_time:44274ms step_avg:61.32ms
step:723/2330 train_time:44335ms step_avg:61.32ms
step:724/2330 train_time:44398ms step_avg:61.32ms
step:725/2330 train_time:44458ms step_avg:61.32ms
step:726/2330 train_time:44521ms step_avg:61.32ms
step:727/2330 train_time:44581ms step_avg:61.32ms
step:728/2330 train_time:44644ms step_avg:61.32ms
step:729/2330 train_time:44704ms step_avg:61.32ms
step:730/2330 train_time:44767ms step_avg:61.32ms
step:731/2330 train_time:44826ms step_avg:61.32ms
step:732/2330 train_time:44889ms step_avg:61.32ms
step:733/2330 train_time:44949ms step_avg:61.32ms
step:734/2330 train_time:45013ms step_avg:61.33ms
step:735/2330 train_time:45073ms step_avg:61.32ms
step:736/2330 train_time:45136ms step_avg:61.33ms
step:737/2330 train_time:45195ms step_avg:61.32ms
step:738/2330 train_time:45258ms step_avg:61.33ms
step:739/2330 train_time:45318ms step_avg:61.32ms
step:740/2330 train_time:45381ms step_avg:61.33ms
step:741/2330 train_time:45440ms step_avg:61.32ms
step:742/2330 train_time:45504ms step_avg:61.33ms
step:743/2330 train_time:45563ms step_avg:61.32ms
step:744/2330 train_time:45626ms step_avg:61.33ms
step:745/2330 train_time:45686ms step_avg:61.32ms
step:746/2330 train_time:45749ms step_avg:61.33ms
step:747/2330 train_time:45809ms step_avg:61.32ms
step:748/2330 train_time:45871ms step_avg:61.32ms
step:749/2330 train_time:45931ms step_avg:61.32ms
step:750/2330 train_time:45994ms step_avg:61.33ms
step:750/2330 val_loss:4.0180 train_time:46059ms step_avg:61.41ms
step:751/2330 train_time:46083ms step_avg:61.36ms
step:752/2330 train_time:46120ms step_avg:61.33ms
step:753/2330 train_time:46184ms step_avg:61.33ms
step:754/2330 train_time:46250ms step_avg:61.34ms
step:755/2330 train_time:46310ms step_avg:61.34ms
step:756/2330 train_time:46374ms step_avg:61.34ms
step:757/2330 train_time:46433ms step_avg:61.34ms
step:758/2330 train_time:46496ms step_avg:61.34ms
step:759/2330 train_time:46556ms step_avg:61.34ms
step:760/2330 train_time:46618ms step_avg:61.34ms
step:761/2330 train_time:46678ms step_avg:61.34ms
step:762/2330 train_time:46740ms step_avg:61.34ms
step:763/2330 train_time:46798ms step_avg:61.33ms
step:764/2330 train_time:46860ms step_avg:61.34ms
step:765/2330 train_time:46920ms step_avg:61.33ms
step:766/2330 train_time:46984ms step_avg:61.34ms
step:767/2330 train_time:47045ms step_avg:61.34ms
step:768/2330 train_time:47109ms step_avg:61.34ms
step:769/2330 train_time:47171ms step_avg:61.34ms
step:770/2330 train_time:47235ms step_avg:61.34ms
step:771/2330 train_time:47297ms step_avg:61.34ms
step:772/2330 train_time:47360ms step_avg:61.35ms
step:773/2330 train_time:47421ms step_avg:61.35ms
step:774/2330 train_time:47484ms step_avg:61.35ms
step:775/2330 train_time:47545ms step_avg:61.35ms
step:776/2330 train_time:47609ms step_avg:61.35ms
step:777/2330 train_time:47669ms step_avg:61.35ms
step:778/2330 train_time:47733ms step_avg:61.35ms
step:779/2330 train_time:47794ms step_avg:61.35ms
step:780/2330 train_time:47857ms step_avg:61.36ms
step:781/2330 train_time:47917ms step_avg:61.35ms
step:782/2330 train_time:47981ms step_avg:61.36ms
step:783/2330 train_time:48042ms step_avg:61.36ms
step:784/2330 train_time:48105ms step_avg:61.36ms
step:785/2330 train_time:48166ms step_avg:61.36ms
step:786/2330 train_time:48232ms step_avg:61.36ms
step:787/2330 train_time:48293ms step_avg:61.36ms
step:788/2330 train_time:48356ms step_avg:61.37ms
step:789/2330 train_time:48417ms step_avg:61.37ms
step:790/2330 train_time:48481ms step_avg:61.37ms
step:791/2330 train_time:48542ms step_avg:61.37ms
step:792/2330 train_time:48605ms step_avg:61.37ms
step:793/2330 train_time:48665ms step_avg:61.37ms
step:794/2330 train_time:48729ms step_avg:61.37ms
step:795/2330 train_time:48790ms step_avg:61.37ms
step:796/2330 train_time:48854ms step_avg:61.37ms
step:797/2330 train_time:48915ms step_avg:61.37ms
step:798/2330 train_time:48979ms step_avg:61.38ms
step:799/2330 train_time:49039ms step_avg:61.38ms
step:800/2330 train_time:49103ms step_avg:61.38ms
step:801/2330 train_time:49162ms step_avg:61.38ms
step:802/2330 train_time:49226ms step_avg:61.38ms
step:803/2330 train_time:49287ms step_avg:61.38ms
step:804/2330 train_time:49352ms step_avg:61.38ms
step:805/2330 train_time:49413ms step_avg:61.38ms
step:806/2330 train_time:49477ms step_avg:61.39ms
step:807/2330 train_time:49538ms step_avg:61.39ms
step:808/2330 train_time:49601ms step_avg:61.39ms
step:809/2330 train_time:49662ms step_avg:61.39ms
step:810/2330 train_time:49725ms step_avg:61.39ms
step:811/2330 train_time:49785ms step_avg:61.39ms
step:812/2330 train_time:49849ms step_avg:61.39ms
step:813/2330 train_time:49910ms step_avg:61.39ms
step:814/2330 train_time:49974ms step_avg:61.39ms
step:815/2330 train_time:50035ms step_avg:61.39ms
step:816/2330 train_time:50099ms step_avg:61.40ms
step:817/2330 train_time:50159ms step_avg:61.39ms
step:818/2330 train_time:50222ms step_avg:61.40ms
step:819/2330 train_time:50283ms step_avg:61.40ms
step:820/2330 train_time:50346ms step_avg:61.40ms
step:821/2330 train_time:50408ms step_avg:61.40ms
step:822/2330 train_time:50472ms step_avg:61.40ms
step:823/2330 train_time:50534ms step_avg:61.40ms
step:824/2330 train_time:50597ms step_avg:61.40ms
step:825/2330 train_time:50658ms step_avg:61.40ms
step:826/2330 train_time:50722ms step_avg:61.41ms
step:827/2330 train_time:50783ms step_avg:61.41ms
step:828/2330 train_time:50847ms step_avg:61.41ms
step:829/2330 train_time:50907ms step_avg:61.41ms
step:830/2330 train_time:50971ms step_avg:61.41ms
step:831/2330 train_time:51032ms step_avg:61.41ms
step:832/2330 train_time:51095ms step_avg:61.41ms
step:833/2330 train_time:51155ms step_avg:61.41ms
step:834/2330 train_time:51218ms step_avg:61.41ms
step:835/2330 train_time:51279ms step_avg:61.41ms
step:836/2330 train_time:51342ms step_avg:61.41ms
step:837/2330 train_time:51402ms step_avg:61.41ms
step:838/2330 train_time:51466ms step_avg:61.42ms
step:839/2330 train_time:51527ms step_avg:61.41ms
step:840/2330 train_time:51592ms step_avg:61.42ms
step:841/2330 train_time:51654ms step_avg:61.42ms
step:842/2330 train_time:51718ms step_avg:61.42ms
step:843/2330 train_time:51779ms step_avg:61.42ms
step:844/2330 train_time:51842ms step_avg:61.42ms
step:845/2330 train_time:51902ms step_avg:61.42ms
step:846/2330 train_time:51966ms step_avg:61.42ms
step:847/2330 train_time:52026ms step_avg:61.42ms
step:848/2330 train_time:52090ms step_avg:61.43ms
step:849/2330 train_time:52151ms step_avg:61.43ms
step:850/2330 train_time:52214ms step_avg:61.43ms
step:851/2330 train_time:52275ms step_avg:61.43ms
step:852/2330 train_time:52339ms step_avg:61.43ms
step:853/2330 train_time:52399ms step_avg:61.43ms
step:854/2330 train_time:52462ms step_avg:61.43ms
step:855/2330 train_time:52523ms step_avg:61.43ms
step:856/2330 train_time:52586ms step_avg:61.43ms
step:857/2330 train_time:52648ms step_avg:61.43ms
step:858/2330 train_time:52712ms step_avg:61.44ms
step:859/2330 train_time:52773ms step_avg:61.44ms
step:860/2330 train_time:52837ms step_avg:61.44ms
step:861/2330 train_time:52897ms step_avg:61.44ms
step:862/2330 train_time:52961ms step_avg:61.44ms
step:863/2330 train_time:53022ms step_avg:61.44ms
step:864/2330 train_time:53085ms step_avg:61.44ms
step:865/2330 train_time:53145ms step_avg:61.44ms
step:866/2330 train_time:53210ms step_avg:61.44ms
step:867/2330 train_time:53271ms step_avg:61.44ms
step:868/2330 train_time:53336ms step_avg:61.45ms
step:869/2330 train_time:53396ms step_avg:61.45ms
step:870/2330 train_time:53460ms step_avg:61.45ms
step:871/2330 train_time:53520ms step_avg:61.45ms
step:872/2330 train_time:53584ms step_avg:61.45ms
step:873/2330 train_time:53644ms step_avg:61.45ms
step:874/2330 train_time:53708ms step_avg:61.45ms
step:875/2330 train_time:53769ms step_avg:61.45ms
step:876/2330 train_time:53834ms step_avg:61.45ms
step:877/2330 train_time:53895ms step_avg:61.45ms
step:878/2330 train_time:53958ms step_avg:61.46ms
step:879/2330 train_time:54019ms step_avg:61.45ms
step:880/2330 train_time:54083ms step_avg:61.46ms
step:881/2330 train_time:54143ms step_avg:61.46ms
step:882/2330 train_time:54206ms step_avg:61.46ms
step:883/2330 train_time:54268ms step_avg:61.46ms
step:884/2330 train_time:54333ms step_avg:61.46ms
step:885/2330 train_time:54394ms step_avg:61.46ms
step:886/2330 train_time:54458ms step_avg:61.47ms
step:887/2330 train_time:54519ms step_avg:61.46ms
step:888/2330 train_time:54582ms step_avg:61.47ms
step:889/2330 train_time:54642ms step_avg:61.47ms
step:890/2330 train_time:54706ms step_avg:61.47ms
step:891/2330 train_time:54767ms step_avg:61.47ms
step:892/2330 train_time:54832ms step_avg:61.47ms
step:893/2330 train_time:54894ms step_avg:61.47ms
step:894/2330 train_time:54958ms step_avg:61.47ms
step:895/2330 train_time:55018ms step_avg:61.47ms
step:896/2330 train_time:55082ms step_avg:61.48ms
step:897/2330 train_time:55142ms step_avg:61.47ms
step:898/2330 train_time:55206ms step_avg:61.48ms
step:899/2330 train_time:55266ms step_avg:61.48ms
step:900/2330 train_time:55330ms step_avg:61.48ms
step:901/2330 train_time:55391ms step_avg:61.48ms
step:902/2330 train_time:55456ms step_avg:61.48ms
step:903/2330 train_time:55516ms step_avg:61.48ms
step:904/2330 train_time:55579ms step_avg:61.48ms
step:905/2330 train_time:55640ms step_avg:61.48ms
step:906/2330 train_time:55703ms step_avg:61.48ms
step:907/2330 train_time:55763ms step_avg:61.48ms
step:908/2330 train_time:55827ms step_avg:61.48ms
step:909/2330 train_time:55889ms step_avg:61.48ms
step:910/2330 train_time:55953ms step_avg:61.49ms
step:911/2330 train_time:56014ms step_avg:61.49ms
step:912/2330 train_time:56078ms step_avg:61.49ms
step:913/2330 train_time:56138ms step_avg:61.49ms
step:914/2330 train_time:56202ms step_avg:61.49ms
step:915/2330 train_time:56263ms step_avg:61.49ms
step:916/2330 train_time:56326ms step_avg:61.49ms
step:917/2330 train_time:56387ms step_avg:61.49ms
step:918/2330 train_time:56452ms step_avg:61.49ms
step:919/2330 train_time:56513ms step_avg:61.49ms
step:920/2330 train_time:56576ms step_avg:61.50ms
step:921/2330 train_time:56637ms step_avg:61.49ms
step:922/2330 train_time:56700ms step_avg:61.50ms
step:923/2330 train_time:56760ms step_avg:61.50ms
step:924/2330 train_time:56824ms step_avg:61.50ms
step:925/2330 train_time:56885ms step_avg:61.50ms
step:926/2330 train_time:56950ms step_avg:61.50ms
step:927/2330 train_time:57010ms step_avg:61.50ms
step:928/2330 train_time:57075ms step_avg:61.50ms
step:929/2330 train_time:57136ms step_avg:61.50ms
step:930/2330 train_time:57199ms step_avg:61.50ms
step:931/2330 train_time:57260ms step_avg:61.50ms
step:932/2330 train_time:57323ms step_avg:61.50ms
step:933/2330 train_time:57383ms step_avg:61.50ms
step:934/2330 train_time:57447ms step_avg:61.51ms
step:935/2330 train_time:57508ms step_avg:61.51ms
step:936/2330 train_time:57572ms step_avg:61.51ms
step:937/2330 train_time:57633ms step_avg:61.51ms
step:938/2330 train_time:57696ms step_avg:61.51ms
step:939/2330 train_time:57757ms step_avg:61.51ms
step:940/2330 train_time:57821ms step_avg:61.51ms
step:941/2330 train_time:57882ms step_avg:61.51ms
step:942/2330 train_time:57946ms step_avg:61.51ms
step:943/2330 train_time:58007ms step_avg:61.51ms
step:944/2330 train_time:58072ms step_avg:61.52ms
step:945/2330 train_time:58133ms step_avg:61.52ms
step:946/2330 train_time:58196ms step_avg:61.52ms
step:947/2330 train_time:58257ms step_avg:61.52ms
step:948/2330 train_time:58320ms step_avg:61.52ms
step:949/2330 train_time:58381ms step_avg:61.52ms
step:950/2330 train_time:58445ms step_avg:61.52ms
step:951/2330 train_time:58505ms step_avg:61.52ms
step:952/2330 train_time:58569ms step_avg:61.52ms
step:953/2330 train_time:58631ms step_avg:61.52ms
step:954/2330 train_time:58693ms step_avg:61.52ms
step:955/2330 train_time:58754ms step_avg:61.52ms
step:956/2330 train_time:58818ms step_avg:61.53ms
step:957/2330 train_time:58880ms step_avg:61.53ms
step:958/2330 train_time:58943ms step_avg:61.53ms
step:959/2330 train_time:59003ms step_avg:61.53ms
step:960/2330 train_time:59067ms step_avg:61.53ms
step:961/2330 train_time:59128ms step_avg:61.53ms
step:962/2330 train_time:59192ms step_avg:61.53ms
step:963/2330 train_time:59254ms step_avg:61.53ms
step:964/2330 train_time:59318ms step_avg:61.53ms
step:965/2330 train_time:59378ms step_avg:61.53ms
step:966/2330 train_time:59442ms step_avg:61.53ms
step:967/2330 train_time:59502ms step_avg:61.53ms
step:968/2330 train_time:59566ms step_avg:61.53ms
step:969/2330 train_time:59626ms step_avg:61.53ms
step:970/2330 train_time:59690ms step_avg:61.54ms
step:971/2330 train_time:59751ms step_avg:61.54ms
step:972/2330 train_time:59815ms step_avg:61.54ms
step:973/2330 train_time:59876ms step_avg:61.54ms
step:974/2330 train_time:59940ms step_avg:61.54ms
step:975/2330 train_time:60000ms step_avg:61.54ms
step:976/2330 train_time:60063ms step_avg:61.54ms
step:977/2330 train_time:60124ms step_avg:61.54ms
step:978/2330 train_time:60188ms step_avg:61.54ms
step:979/2330 train_time:60248ms step_avg:61.54ms
step:980/2330 train_time:60312ms step_avg:61.54ms
step:981/2330 train_time:60373ms step_avg:61.54ms
step:982/2330 train_time:60437ms step_avg:61.54ms
step:983/2330 train_time:60498ms step_avg:61.54ms
step:984/2330 train_time:60562ms step_avg:61.55ms
step:985/2330 train_time:60622ms step_avg:61.55ms
step:986/2330 train_time:60685ms step_avg:61.55ms
step:987/2330 train_time:60747ms step_avg:61.55ms
step:988/2330 train_time:60811ms step_avg:61.55ms
step:989/2330 train_time:60872ms step_avg:61.55ms
step:990/2330 train_time:60937ms step_avg:61.55ms
step:991/2330 train_time:60997ms step_avg:61.55ms
step:992/2330 train_time:61060ms step_avg:61.55ms
step:993/2330 train_time:61121ms step_avg:61.55ms
step:994/2330 train_time:61184ms step_avg:61.55ms
step:995/2330 train_time:61245ms step_avg:61.55ms
step:996/2330 train_time:61308ms step_avg:61.55ms
step:997/2330 train_time:61370ms step_avg:61.55ms
step:998/2330 train_time:61434ms step_avg:61.56ms
step:999/2330 train_time:61494ms step_avg:61.56ms
step:1000/2330 train_time:61558ms step_avg:61.56ms
step:1000/2330 val_loss:3.8442 train_time:61623ms step_avg:61.62ms
step:1001/2330 train_time:61647ms step_avg:61.59ms
step:1002/2330 train_time:61683ms step_avg:61.56ms
step:1003/2330 train_time:61749ms step_avg:61.56ms
step:1004/2330 train_time:61815ms step_avg:61.57ms
step:1005/2330 train_time:61876ms step_avg:61.57ms
step:1006/2330 train_time:61939ms step_avg:61.57ms
step:1007/2330 train_time:62000ms step_avg:61.57ms
step:1008/2330 train_time:62063ms step_avg:61.57ms
step:1009/2330 train_time:62123ms step_avg:61.57ms
step:1010/2330 train_time:62186ms step_avg:61.57ms
step:1011/2330 train_time:62245ms step_avg:61.57ms
step:1012/2330 train_time:62308ms step_avg:61.57ms
step:1013/2330 train_time:62368ms step_avg:61.57ms
step:1014/2330 train_time:62430ms step_avg:61.57ms
step:1015/2330 train_time:62490ms step_avg:61.57ms
step:1016/2330 train_time:62556ms step_avg:61.57ms
step:1017/2330 train_time:62619ms step_avg:61.57ms
step:1018/2330 train_time:62684ms step_avg:61.58ms
step:1019/2330 train_time:62746ms step_avg:61.58ms
step:1020/2330 train_time:62811ms step_avg:61.58ms
step:1021/2330 train_time:62871ms step_avg:61.58ms
step:1022/2330 train_time:62935ms step_avg:61.58ms
step:1023/2330 train_time:62996ms step_avg:61.58ms
step:1024/2330 train_time:63061ms step_avg:61.58ms
step:1025/2330 train_time:63121ms step_avg:61.58ms
step:1026/2330 train_time:63185ms step_avg:61.58ms
step:1027/2330 train_time:63245ms step_avg:61.58ms
step:1028/2330 train_time:63308ms step_avg:61.58ms
step:1029/2330 train_time:63368ms step_avg:61.58ms
step:1030/2330 train_time:63430ms step_avg:61.58ms
step:1031/2330 train_time:63491ms step_avg:61.58ms
step:1032/2330 train_time:63555ms step_avg:61.58ms
step:1033/2330 train_time:63617ms step_avg:61.58ms
step:1034/2330 train_time:63681ms step_avg:61.59ms
step:1035/2330 train_time:63743ms step_avg:61.59ms
step:1036/2330 train_time:63808ms step_avg:61.59ms
step:1037/2330 train_time:63869ms step_avg:61.59ms
step:1038/2330 train_time:63932ms step_avg:61.59ms
step:1039/2330 train_time:63994ms step_avg:61.59ms
step:1040/2330 train_time:64058ms step_avg:61.59ms
step:1041/2330 train_time:64120ms step_avg:61.59ms
step:1042/2330 train_time:64183ms step_avg:61.60ms
step:1043/2330 train_time:64244ms step_avg:61.59ms
step:1044/2330 train_time:64308ms step_avg:61.60ms
step:1045/2330 train_time:64367ms step_avg:61.60ms
step:1046/2330 train_time:64430ms step_avg:61.60ms
step:1047/2330 train_time:64490ms step_avg:61.60ms
step:1048/2330 train_time:64554ms step_avg:61.60ms
step:1049/2330 train_time:64615ms step_avg:61.60ms
step:1050/2330 train_time:64679ms step_avg:61.60ms
step:1051/2330 train_time:64741ms step_avg:61.60ms
step:1052/2330 train_time:64806ms step_avg:61.60ms
step:1053/2330 train_time:64866ms step_avg:61.60ms
step:1054/2330 train_time:64929ms step_avg:61.60ms
step:1055/2330 train_time:64990ms step_avg:61.60ms
step:1056/2330 train_time:65054ms step_avg:61.60ms
step:1057/2330 train_time:65116ms step_avg:61.60ms
step:1058/2330 train_time:65180ms step_avg:61.61ms
step:1059/2330 train_time:65241ms step_avg:61.61ms
step:1060/2330 train_time:65305ms step_avg:61.61ms
step:1061/2330 train_time:65365ms step_avg:61.61ms
step:1062/2330 train_time:65428ms step_avg:61.61ms
step:1063/2330 train_time:65488ms step_avg:61.61ms
step:1064/2330 train_time:65551ms step_avg:61.61ms
step:1065/2330 train_time:65611ms step_avg:61.61ms
step:1066/2330 train_time:65676ms step_avg:61.61ms
step:1067/2330 train_time:65737ms step_avg:61.61ms
step:1068/2330 train_time:65802ms step_avg:61.61ms
step:1069/2330 train_time:65863ms step_avg:61.61ms
step:1070/2330 train_time:65927ms step_avg:61.61ms
step:1071/2330 train_time:65987ms step_avg:61.61ms
step:1072/2330 train_time:66051ms step_avg:61.61ms
step:1073/2330 train_time:66112ms step_avg:61.61ms
step:1074/2330 train_time:66178ms step_avg:61.62ms
step:1075/2330 train_time:66238ms step_avg:61.62ms
step:1076/2330 train_time:66302ms step_avg:61.62ms
step:1077/2330 train_time:66362ms step_avg:61.62ms
step:1078/2330 train_time:66425ms step_avg:61.62ms
step:1079/2330 train_time:66486ms step_avg:61.62ms
step:1080/2330 train_time:66549ms step_avg:61.62ms
step:1081/2330 train_time:66609ms step_avg:61.62ms
step:1082/2330 train_time:66672ms step_avg:61.62ms
step:1083/2330 train_time:66735ms step_avg:61.62ms
step:1084/2330 train_time:66799ms step_avg:61.62ms
step:1085/2330 train_time:66860ms step_avg:61.62ms
step:1086/2330 train_time:66924ms step_avg:61.62ms
step:1087/2330 train_time:66985ms step_avg:61.62ms
step:1088/2330 train_time:67048ms step_avg:61.63ms
step:1089/2330 train_time:67108ms step_avg:61.62ms
step:1090/2330 train_time:67172ms step_avg:61.63ms
step:1091/2330 train_time:67234ms step_avg:61.63ms
step:1092/2330 train_time:67299ms step_avg:61.63ms
step:1093/2330 train_time:67360ms step_avg:61.63ms
step:1094/2330 train_time:67424ms step_avg:61.63ms
step:1095/2330 train_time:67484ms step_avg:61.63ms
step:1096/2330 train_time:67548ms step_avg:61.63ms
step:1097/2330 train_time:67609ms step_avg:61.63ms
step:1098/2330 train_time:67672ms step_avg:61.63ms
step:1099/2330 train_time:67732ms step_avg:61.63ms
step:1100/2330 train_time:67797ms step_avg:61.63ms
step:1101/2330 train_time:67859ms step_avg:61.63ms
step:1102/2330 train_time:67923ms step_avg:61.64ms
step:1103/2330 train_time:67984ms step_avg:61.64ms
step:1104/2330 train_time:68048ms step_avg:61.64ms
step:1105/2330 train_time:68108ms step_avg:61.64ms
step:1106/2330 train_time:68171ms step_avg:61.64ms
step:1107/2330 train_time:68232ms step_avg:61.64ms
step:1108/2330 train_time:68296ms step_avg:61.64ms
step:1109/2330 train_time:68357ms step_avg:61.64ms
step:1110/2330 train_time:68421ms step_avg:61.64ms
step:1111/2330 train_time:68482ms step_avg:61.64ms
step:1112/2330 train_time:68546ms step_avg:61.64ms
step:1113/2330 train_time:68607ms step_avg:61.64ms
step:1114/2330 train_time:68670ms step_avg:61.64ms
step:1115/2330 train_time:68730ms step_avg:61.64ms
step:1116/2330 train_time:68794ms step_avg:61.64ms
step:1117/2330 train_time:68856ms step_avg:61.64ms
step:1118/2330 train_time:68920ms step_avg:61.65ms
step:1119/2330 train_time:68981ms step_avg:61.65ms
step:1120/2330 train_time:69045ms step_avg:61.65ms
step:1121/2330 train_time:69105ms step_avg:61.65ms
step:1122/2330 train_time:69169ms step_avg:61.65ms
step:1123/2330 train_time:69229ms step_avg:61.65ms
step:1124/2330 train_time:69292ms step_avg:61.65ms
step:1125/2330 train_time:69353ms step_avg:61.65ms
step:1126/2330 train_time:69418ms step_avg:61.65ms
step:1127/2330 train_time:69481ms step_avg:61.65ms
step:1128/2330 train_time:69545ms step_avg:61.65ms
step:1129/2330 train_time:69605ms step_avg:61.65ms
step:1130/2330 train_time:69669ms step_avg:61.65ms
step:1131/2330 train_time:69729ms step_avg:61.65ms
step:1132/2330 train_time:69792ms step_avg:61.65ms
step:1133/2330 train_time:69854ms step_avg:61.65ms
step:1134/2330 train_time:69918ms step_avg:61.66ms
step:1135/2330 train_time:69980ms step_avg:61.66ms
step:1136/2330 train_time:70044ms step_avg:61.66ms
step:1137/2330 train_time:70104ms step_avg:61.66ms
step:1138/2330 train_time:70167ms step_avg:61.66ms
step:1139/2330 train_time:70228ms step_avg:61.66ms
step:1140/2330 train_time:70291ms step_avg:61.66ms
step:1141/2330 train_time:70351ms step_avg:61.66ms
step:1142/2330 train_time:70416ms step_avg:61.66ms
step:1143/2330 train_time:70477ms step_avg:61.66ms
step:1144/2330 train_time:70541ms step_avg:61.66ms
step:1145/2330 train_time:70602ms step_avg:61.66ms
step:1146/2330 train_time:70665ms step_avg:61.66ms
step:1147/2330 train_time:70726ms step_avg:61.66ms
step:1148/2330 train_time:70789ms step_avg:61.66ms
step:1149/2330 train_time:70849ms step_avg:61.66ms
step:1150/2330 train_time:70914ms step_avg:61.66ms
step:1151/2330 train_time:70976ms step_avg:61.66ms
step:1152/2330 train_time:71040ms step_avg:61.67ms
step:1153/2330 train_time:71102ms step_avg:61.67ms
step:1154/2330 train_time:71166ms step_avg:61.67ms
step:1155/2330 train_time:71226ms step_avg:61.67ms
step:1156/2330 train_time:71290ms step_avg:61.67ms
step:1157/2330 train_time:71350ms step_avg:61.67ms
step:1158/2330 train_time:71414ms step_avg:61.67ms
step:1159/2330 train_time:71475ms step_avg:61.67ms
step:1160/2330 train_time:71540ms step_avg:61.67ms
step:1161/2330 train_time:71601ms step_avg:61.67ms
step:1162/2330 train_time:71665ms step_avg:61.67ms
step:1163/2330 train_time:71725ms step_avg:61.67ms
step:1164/2330 train_time:71788ms step_avg:61.67ms
step:1165/2330 train_time:71849ms step_avg:61.67ms
step:1166/2330 train_time:71912ms step_avg:61.67ms
step:1167/2330 train_time:71973ms step_avg:61.67ms
step:1168/2330 train_time:72038ms step_avg:61.68ms
step:1169/2330 train_time:72099ms step_avg:61.68ms
step:1170/2330 train_time:72163ms step_avg:61.68ms
step:1171/2330 train_time:72224ms step_avg:61.68ms
step:1172/2330 train_time:72287ms step_avg:61.68ms
step:1173/2330 train_time:72348ms step_avg:61.68ms
step:1174/2330 train_time:72411ms step_avg:61.68ms
step:1175/2330 train_time:72471ms step_avg:61.68ms
step:1176/2330 train_time:72535ms step_avg:61.68ms
step:1177/2330 train_time:72597ms step_avg:61.68ms
step:1178/2330 train_time:72662ms step_avg:61.68ms
step:1179/2330 train_time:72723ms step_avg:61.68ms
step:1180/2330 train_time:72786ms step_avg:61.68ms
step:1181/2330 train_time:72846ms step_avg:61.68ms
step:1182/2330 train_time:72909ms step_avg:61.68ms
step:1183/2330 train_time:72970ms step_avg:61.68ms
step:1184/2330 train_time:73033ms step_avg:61.68ms
step:1185/2330 train_time:73094ms step_avg:61.68ms
step:1186/2330 train_time:73160ms step_avg:61.69ms
step:1187/2330 train_time:73222ms step_avg:61.69ms
step:1188/2330 train_time:73285ms step_avg:61.69ms
step:1189/2330 train_time:73345ms step_avg:61.69ms
step:1190/2330 train_time:73409ms step_avg:61.69ms
step:1191/2330 train_time:73469ms step_avg:61.69ms
step:1192/2330 train_time:73533ms step_avg:61.69ms
step:1193/2330 train_time:73594ms step_avg:61.69ms
step:1194/2330 train_time:73659ms step_avg:61.69ms
step:1195/2330 train_time:73720ms step_avg:61.69ms
step:1196/2330 train_time:73784ms step_avg:61.69ms
step:1197/2330 train_time:73844ms step_avg:61.69ms
step:1198/2330 train_time:73907ms step_avg:61.69ms
step:1199/2330 train_time:73967ms step_avg:61.69ms
step:1200/2330 train_time:74030ms step_avg:61.69ms
step:1201/2330 train_time:74091ms step_avg:61.69ms
step:1202/2330 train_time:74156ms step_avg:61.69ms
step:1203/2330 train_time:74217ms step_avg:61.69ms
step:1204/2330 train_time:74282ms step_avg:61.70ms
step:1205/2330 train_time:74343ms step_avg:61.70ms
step:1206/2330 train_time:74407ms step_avg:61.70ms
step:1207/2330 train_time:74467ms step_avg:61.70ms
step:1208/2330 train_time:74530ms step_avg:61.70ms
step:1209/2330 train_time:74591ms step_avg:61.70ms
step:1210/2330 train_time:74654ms step_avg:61.70ms
step:1211/2330 train_time:74716ms step_avg:61.70ms
step:1212/2330 train_time:74780ms step_avg:61.70ms
step:1213/2330 train_time:74842ms step_avg:61.70ms
step:1214/2330 train_time:74906ms step_avg:61.70ms
step:1215/2330 train_time:74966ms step_avg:61.70ms
step:1216/2330 train_time:75030ms step_avg:61.70ms
step:1217/2330 train_time:75090ms step_avg:61.70ms
step:1218/2330 train_time:75153ms step_avg:61.70ms
step:1219/2330 train_time:75214ms step_avg:61.70ms
step:1220/2330 train_time:75278ms step_avg:61.70ms
step:1221/2330 train_time:75340ms step_avg:61.70ms
step:1222/2330 train_time:75404ms step_avg:61.71ms
step:1223/2330 train_time:75465ms step_avg:61.70ms
step:1224/2330 train_time:75529ms step_avg:61.71ms
step:1225/2330 train_time:75588ms step_avg:61.70ms
step:1226/2330 train_time:75652ms step_avg:61.71ms
step:1227/2330 train_time:75713ms step_avg:61.71ms
step:1228/2330 train_time:75778ms step_avg:61.71ms
step:1229/2330 train_time:75839ms step_avg:61.71ms
step:1230/2330 train_time:75904ms step_avg:61.71ms
step:1231/2330 train_time:75964ms step_avg:61.71ms
step:1232/2330 train_time:76028ms step_avg:61.71ms
step:1233/2330 train_time:76088ms step_avg:61.71ms
step:1234/2330 train_time:76151ms step_avg:61.71ms
step:1235/2330 train_time:76211ms step_avg:61.71ms
step:1236/2330 train_time:76275ms step_avg:61.71ms
step:1237/2330 train_time:76336ms step_avg:61.71ms
step:1238/2330 train_time:76401ms step_avg:61.71ms
step:1239/2330 train_time:76462ms step_avg:61.71ms
step:1240/2330 train_time:76526ms step_avg:61.71ms
step:1241/2330 train_time:76586ms step_avg:61.71ms
step:1242/2330 train_time:76649ms step_avg:61.71ms
step:1243/2330 train_time:76710ms step_avg:61.71ms
step:1244/2330 train_time:76774ms step_avg:61.72ms
step:1245/2330 train_time:76835ms step_avg:61.71ms
step:1246/2330 train_time:76900ms step_avg:61.72ms
step:1247/2330 train_time:76961ms step_avg:61.72ms
step:1248/2330 train_time:77024ms step_avg:61.72ms
step:1249/2330 train_time:77085ms step_avg:61.72ms
step:1250/2330 train_time:77149ms step_avg:61.72ms
step:1250/2330 val_loss:3.7581 train_time:77213ms step_avg:61.77ms
step:1251/2330 train_time:77237ms step_avg:61.74ms
step:1252/2330 train_time:77276ms step_avg:61.72ms
step:1253/2330 train_time:77339ms step_avg:61.72ms
step:1254/2330 train_time:77406ms step_avg:61.73ms
step:1255/2330 train_time:77467ms step_avg:61.73ms
step:1256/2330 train_time:77530ms step_avg:61.73ms
step:1257/2330 train_time:77590ms step_avg:61.73ms
step:1258/2330 train_time:77653ms step_avg:61.73ms
step:1259/2330 train_time:77713ms step_avg:61.73ms
step:1260/2330 train_time:77776ms step_avg:61.73ms
step:1261/2330 train_time:77836ms step_avg:61.73ms
step:1262/2330 train_time:77898ms step_avg:61.73ms
step:1263/2330 train_time:77958ms step_avg:61.72ms
step:1264/2330 train_time:78022ms step_avg:61.73ms
step:1265/2330 train_time:78081ms step_avg:61.72ms
step:1266/2330 train_time:78146ms step_avg:61.73ms
step:1267/2330 train_time:78208ms step_avg:61.73ms
step:1268/2330 train_time:78273ms step_avg:61.73ms
step:1269/2330 train_time:78336ms step_avg:61.73ms
step:1270/2330 train_time:78401ms step_avg:61.73ms
step:1271/2330 train_time:78463ms step_avg:61.73ms
step:1272/2330 train_time:78527ms step_avg:61.74ms
step:1273/2330 train_time:78588ms step_avg:61.73ms
step:1274/2330 train_time:78652ms step_avg:61.74ms
step:1275/2330 train_time:78712ms step_avg:61.74ms
step:1276/2330 train_time:78776ms step_avg:61.74ms
step:1277/2330 train_time:78835ms step_avg:61.73ms
step:1278/2330 train_time:78898ms step_avg:61.74ms
step:1279/2330 train_time:78957ms step_avg:61.73ms
step:1280/2330 train_time:79020ms step_avg:61.73ms
step:1281/2330 train_time:79081ms step_avg:61.73ms
step:1282/2330 train_time:79144ms step_avg:61.74ms
step:1283/2330 train_time:79206ms step_avg:61.73ms
step:1284/2330 train_time:79271ms step_avg:61.74ms
step:1285/2330 train_time:79333ms step_avg:61.74ms
step:1286/2330 train_time:79397ms step_avg:61.74ms
step:1287/2330 train_time:79458ms step_avg:61.74ms
step:1288/2330 train_time:79523ms step_avg:61.74ms
step:1289/2330 train_time:79584ms step_avg:61.74ms
step:1290/2330 train_time:79648ms step_avg:61.74ms
step:1291/2330 train_time:79709ms step_avg:61.74ms
step:1292/2330 train_time:79772ms step_avg:61.74ms
step:1293/2330 train_time:79833ms step_avg:61.74ms
step:1294/2330 train_time:79895ms step_avg:61.74ms
step:1295/2330 train_time:79955ms step_avg:61.74ms
step:1296/2330 train_time:80018ms step_avg:61.74ms
step:1297/2330 train_time:80079ms step_avg:61.74ms
step:1298/2330 train_time:80142ms step_avg:61.74ms
step:1299/2330 train_time:80203ms step_avg:61.74ms
step:1300/2330 train_time:80268ms step_avg:61.74ms
step:1301/2330 train_time:80330ms step_avg:61.74ms
step:1302/2330 train_time:80394ms step_avg:61.75ms
step:1303/2330 train_time:80455ms step_avg:61.75ms
step:1304/2330 train_time:80519ms step_avg:61.75ms
step:1305/2330 train_time:80580ms step_avg:61.75ms
step:1306/2330 train_time:80646ms step_avg:61.75ms
step:1307/2330 train_time:80707ms step_avg:61.75ms
step:1308/2330 train_time:80772ms step_avg:61.75ms
step:1309/2330 train_time:80833ms step_avg:61.75ms
step:1310/2330 train_time:80896ms step_avg:61.75ms
step:1311/2330 train_time:80956ms step_avg:61.75ms
step:1312/2330 train_time:81019ms step_avg:61.75ms
step:1313/2330 train_time:81079ms step_avg:61.75ms
step:1314/2330 train_time:81143ms step_avg:61.75ms
step:1315/2330 train_time:81204ms step_avg:61.75ms
step:1316/2330 train_time:81268ms step_avg:61.75ms
step:1317/2330 train_time:81329ms step_avg:61.75ms
step:1318/2330 train_time:81393ms step_avg:61.76ms
step:1319/2330 train_time:81454ms step_avg:61.75ms
step:1320/2330 train_time:81518ms step_avg:61.76ms
step:1321/2330 train_time:81580ms step_avg:61.76ms
step:1322/2330 train_time:81645ms step_avg:61.76ms
step:1323/2330 train_time:81708ms step_avg:61.76ms
step:1324/2330 train_time:81772ms step_avg:61.76ms
step:1325/2330 train_time:81833ms step_avg:61.76ms
step:1326/2330 train_time:81896ms step_avg:61.76ms
step:1327/2330 train_time:81956ms step_avg:61.76ms
step:1328/2330 train_time:82020ms step_avg:61.76ms
step:1329/2330 train_time:82081ms step_avg:61.76ms
step:1330/2330 train_time:82144ms step_avg:61.76ms
step:1331/2330 train_time:82205ms step_avg:61.76ms
step:1332/2330 train_time:82269ms step_avg:61.76ms
step:1333/2330 train_time:82330ms step_avg:61.76ms
step:1334/2330 train_time:82394ms step_avg:61.76ms
step:1335/2330 train_time:82455ms step_avg:61.76ms
step:1336/2330 train_time:82519ms step_avg:61.77ms
step:1337/2330 train_time:82580ms step_avg:61.77ms
step:1338/2330 train_time:82645ms step_avg:61.77ms
step:1339/2330 train_time:82707ms step_avg:61.77ms
step:1340/2330 train_time:82772ms step_avg:61.77ms
step:1341/2330 train_time:82832ms step_avg:61.77ms
step:1342/2330 train_time:82895ms step_avg:61.77ms
step:1343/2330 train_time:82955ms step_avg:61.77ms
step:1344/2330 train_time:83018ms step_avg:61.77ms
step:1345/2330 train_time:83079ms step_avg:61.77ms
step:1346/2330 train_time:83143ms step_avg:61.77ms
step:1347/2330 train_time:83203ms step_avg:61.77ms
step:1348/2330 train_time:83267ms step_avg:61.77ms
step:1349/2330 train_time:83328ms step_avg:61.77ms
step:1350/2330 train_time:83392ms step_avg:61.77ms
step:1351/2330 train_time:83453ms step_avg:61.77ms
step:1352/2330 train_time:83516ms step_avg:61.77ms
step:1353/2330 train_time:83577ms step_avg:61.77ms
step:1354/2330 train_time:83641ms step_avg:61.77ms
step:1355/2330 train_time:83703ms step_avg:61.77ms
step:1356/2330 train_time:83767ms step_avg:61.78ms
step:1357/2330 train_time:83829ms step_avg:61.78ms
step:1358/2330 train_time:83893ms step_avg:61.78ms
step:1359/2330 train_time:83953ms step_avg:61.78ms
step:1360/2330 train_time:84017ms step_avg:61.78ms
step:1361/2330 train_time:84077ms step_avg:61.78ms
step:1362/2330 train_time:84141ms step_avg:61.78ms
step:1363/2330 train_time:84203ms step_avg:61.78ms
step:1364/2330 train_time:84267ms step_avg:61.78ms
step:1365/2330 train_time:84328ms step_avg:61.78ms
step:1366/2330 train_time:84392ms step_avg:61.78ms
step:1367/2330 train_time:84452ms step_avg:61.78ms
step:1368/2330 train_time:84516ms step_avg:61.78ms
step:1369/2330 train_time:84577ms step_avg:61.78ms
step:1370/2330 train_time:84641ms step_avg:61.78ms
step:1371/2330 train_time:84703ms step_avg:61.78ms
step:1372/2330 train_time:84768ms step_avg:61.78ms
step:1373/2330 train_time:84828ms step_avg:61.78ms
step:1374/2330 train_time:84892ms step_avg:61.78ms
step:1375/2330 train_time:84952ms step_avg:61.78ms
step:1376/2330 train_time:85016ms step_avg:61.78ms
step:1377/2330 train_time:85076ms step_avg:61.78ms
step:1378/2330 train_time:85139ms step_avg:61.78ms
step:1379/2330 train_time:85200ms step_avg:61.78ms
step:1380/2330 train_time:85264ms step_avg:61.79ms
step:1381/2330 train_time:85325ms step_avg:61.79ms
step:1382/2330 train_time:85389ms step_avg:61.79ms
step:1383/2330 train_time:85450ms step_avg:61.79ms
step:1384/2330 train_time:85514ms step_avg:61.79ms
step:1385/2330 train_time:85574ms step_avg:61.79ms
step:1386/2330 train_time:85638ms step_avg:61.79ms
step:1387/2330 train_time:85699ms step_avg:61.79ms
step:1388/2330 train_time:85764ms step_avg:61.79ms
step:1389/2330 train_time:85826ms step_avg:61.79ms
step:1390/2330 train_time:85891ms step_avg:61.79ms
step:1391/2330 train_time:85951ms step_avg:61.79ms
step:1392/2330 train_time:86015ms step_avg:61.79ms
step:1393/2330 train_time:86076ms step_avg:61.79ms
step:1394/2330 train_time:86139ms step_avg:61.79ms
step:1395/2330 train_time:86200ms step_avg:61.79ms
step:1396/2330 train_time:86264ms step_avg:61.79ms
step:1397/2330 train_time:86325ms step_avg:61.79ms
step:1398/2330 train_time:86389ms step_avg:61.79ms
step:1399/2330 train_time:86450ms step_avg:61.79ms
step:1400/2330 train_time:86514ms step_avg:61.80ms
step:1401/2330 train_time:86574ms step_avg:61.79ms
step:1402/2330 train_time:86638ms step_avg:61.80ms
step:1403/2330 train_time:86699ms step_avg:61.80ms
step:1404/2330 train_time:86764ms step_avg:61.80ms
step:1405/2330 train_time:86826ms step_avg:61.80ms
step:1406/2330 train_time:86890ms step_avg:61.80ms
step:1407/2330 train_time:86951ms step_avg:61.80ms
step:1408/2330 train_time:87015ms step_avg:61.80ms
step:1409/2330 train_time:87075ms step_avg:61.80ms
step:1410/2330 train_time:87139ms step_avg:61.80ms
step:1411/2330 train_time:87200ms step_avg:61.80ms
step:1412/2330 train_time:87264ms step_avg:61.80ms
step:1413/2330 train_time:87325ms step_avg:61.80ms
step:1414/2330 train_time:87389ms step_avg:61.80ms
step:1415/2330 train_time:87450ms step_avg:61.80ms
step:1416/2330 train_time:87514ms step_avg:61.80ms
step:1417/2330 train_time:87575ms step_avg:61.80ms
step:1418/2330 train_time:87639ms step_avg:61.80ms
step:1419/2330 train_time:87700ms step_avg:61.80ms
step:1420/2330 train_time:87764ms step_avg:61.81ms
step:1421/2330 train_time:87825ms step_avg:61.81ms
step:1422/2330 train_time:87890ms step_avg:61.81ms
step:1423/2330 train_time:87951ms step_avg:61.81ms
step:1424/2330 train_time:88015ms step_avg:61.81ms
step:1425/2330 train_time:88075ms step_avg:61.81ms
step:1426/2330 train_time:88138ms step_avg:61.81ms
step:1427/2330 train_time:88199ms step_avg:61.81ms
step:1428/2330 train_time:88263ms step_avg:61.81ms
step:1429/2330 train_time:88325ms step_avg:61.81ms
step:1430/2330 train_time:88390ms step_avg:61.81ms
step:1431/2330 train_time:88450ms step_avg:61.81ms
step:1432/2330 train_time:88514ms step_avg:61.81ms
step:1433/2330 train_time:88574ms step_avg:61.81ms
step:1434/2330 train_time:88638ms step_avg:61.81ms
step:1435/2330 train_time:88699ms step_avg:61.81ms
step:1436/2330 train_time:88763ms step_avg:61.81ms
step:1437/2330 train_time:88826ms step_avg:61.81ms
step:1438/2330 train_time:88890ms step_avg:61.81ms
step:1439/2330 train_time:88950ms step_avg:61.81ms
step:1440/2330 train_time:89014ms step_avg:61.82ms
step:1441/2330 train_time:89075ms step_avg:61.81ms
step:1442/2330 train_time:89139ms step_avg:61.82ms
step:1443/2330 train_time:89200ms step_avg:61.82ms
step:1444/2330 train_time:89264ms step_avg:61.82ms
step:1445/2330 train_time:89326ms step_avg:61.82ms
step:1446/2330 train_time:89389ms step_avg:61.82ms
step:1447/2330 train_time:89450ms step_avg:61.82ms
step:1448/2330 train_time:89514ms step_avg:61.82ms
step:1449/2330 train_time:89574ms step_avg:61.82ms
step:1450/2330 train_time:89638ms step_avg:61.82ms
step:1451/2330 train_time:89698ms step_avg:61.82ms
step:1452/2330 train_time:89762ms step_avg:61.82ms
step:1453/2330 train_time:89825ms step_avg:61.82ms
step:1454/2330 train_time:89889ms step_avg:61.82ms
step:1455/2330 train_time:89949ms step_avg:61.82ms
step:1456/2330 train_time:90013ms step_avg:61.82ms
step:1457/2330 train_time:90074ms step_avg:61.82ms
step:1458/2330 train_time:90138ms step_avg:61.82ms
step:1459/2330 train_time:90198ms step_avg:61.82ms
step:1460/2330 train_time:90262ms step_avg:61.82ms
step:1461/2330 train_time:90323ms step_avg:61.82ms
step:1462/2330 train_time:90388ms step_avg:61.83ms
step:1463/2330 train_time:90449ms step_avg:61.82ms
step:1464/2330 train_time:90512ms step_avg:61.83ms
step:1465/2330 train_time:90573ms step_avg:61.82ms
step:1466/2330 train_time:90637ms step_avg:61.83ms
step:1467/2330 train_time:90697ms step_avg:61.82ms
step:1468/2330 train_time:90761ms step_avg:61.83ms
step:1469/2330 train_time:90822ms step_avg:61.83ms
step:1470/2330 train_time:90886ms step_avg:61.83ms
step:1471/2330 train_time:90948ms step_avg:61.83ms
step:1472/2330 train_time:91011ms step_avg:61.83ms
step:1473/2330 train_time:91072ms step_avg:61.83ms
step:1474/2330 train_time:91136ms step_avg:61.83ms
step:1475/2330 train_time:91196ms step_avg:61.83ms
step:1476/2330 train_time:91259ms step_avg:61.83ms
step:1477/2330 train_time:91320ms step_avg:61.83ms
step:1478/2330 train_time:91385ms step_avg:61.83ms
step:1479/2330 train_time:91446ms step_avg:61.83ms
step:1480/2330 train_time:91510ms step_avg:61.83ms
step:1481/2330 train_time:91571ms step_avg:61.83ms
step:1482/2330 train_time:91636ms step_avg:61.83ms
step:1483/2330 train_time:91696ms step_avg:61.83ms
step:1484/2330 train_time:91759ms step_avg:61.83ms
step:1485/2330 train_time:91819ms step_avg:61.83ms
step:1486/2330 train_time:91883ms step_avg:61.83ms
step:1487/2330 train_time:91944ms step_avg:61.83ms
step:1488/2330 train_time:92008ms step_avg:61.83ms
step:1489/2330 train_time:92069ms step_avg:61.83ms
step:1490/2330 train_time:92133ms step_avg:61.83ms
step:1491/2330 train_time:92194ms step_avg:61.83ms
step:1492/2330 train_time:92257ms step_avg:61.83ms
step:1493/2330 train_time:92318ms step_avg:61.83ms
step:1494/2330 train_time:92382ms step_avg:61.84ms
step:1495/2330 train_time:92443ms step_avg:61.83ms
step:1496/2330 train_time:92508ms step_avg:61.84ms
step:1497/2330 train_time:92569ms step_avg:61.84ms
step:1498/2330 train_time:92634ms step_avg:61.84ms
step:1499/2330 train_time:92694ms step_avg:61.84ms
step:1500/2330 train_time:92757ms step_avg:61.84ms
step:1500/2330 val_loss:3.6980 train_time:92821ms step_avg:61.88ms
step:1501/2330 train_time:92845ms step_avg:61.86ms
step:1502/2330 train_time:92883ms step_avg:61.84ms
step:1503/2330 train_time:92949ms step_avg:61.84ms
step:1504/2330 train_time:93016ms step_avg:61.85ms
step:1505/2330 train_time:93076ms step_avg:61.84ms
step:1506/2330 train_time:93139ms step_avg:61.85ms
step:1507/2330 train_time:93200ms step_avg:61.84ms
step:1508/2330 train_time:93263ms step_avg:61.85ms
step:1509/2330 train_time:93323ms step_avg:61.84ms
step:1510/2330 train_time:93386ms step_avg:61.85ms
step:1511/2330 train_time:93446ms step_avg:61.84ms
step:1512/2330 train_time:93510ms step_avg:61.85ms
step:1513/2330 train_time:93570ms step_avg:61.84ms
step:1514/2330 train_time:93633ms step_avg:61.84ms
step:1515/2330 train_time:93693ms step_avg:61.84ms
step:1516/2330 train_time:93757ms step_avg:61.84ms
step:1517/2330 train_time:93818ms step_avg:61.84ms
step:1518/2330 train_time:93883ms step_avg:61.85ms
step:1519/2330 train_time:93946ms step_avg:61.85ms
step:1520/2330 train_time:94012ms step_avg:61.85ms
step:1521/2330 train_time:94073ms step_avg:61.85ms
step:1522/2330 train_time:94136ms step_avg:61.85ms
step:1523/2330 train_time:94197ms step_avg:61.85ms
step:1524/2330 train_time:94261ms step_avg:61.85ms
step:1525/2330 train_time:94320ms step_avg:61.85ms
step:1526/2330 train_time:94383ms step_avg:61.85ms
step:1527/2330 train_time:94443ms step_avg:61.85ms
step:1528/2330 train_time:94506ms step_avg:61.85ms
step:1529/2330 train_time:94567ms step_avg:61.85ms
step:1530/2330 train_time:94630ms step_avg:61.85ms
step:1531/2330 train_time:94692ms step_avg:61.85ms
step:1532/2330 train_time:94757ms step_avg:61.85ms
step:1533/2330 train_time:94818ms step_avg:61.85ms
step:1534/2330 train_time:94883ms step_avg:61.85ms
step:1535/2330 train_time:94947ms step_avg:61.85ms
step:1536/2330 train_time:95012ms step_avg:61.86ms
step:1537/2330 train_time:95074ms step_avg:61.86ms
step:1538/2330 train_time:95138ms step_avg:61.86ms
step:1539/2330 train_time:95200ms step_avg:61.86ms
step:1540/2330 train_time:95264ms step_avg:61.86ms
step:1541/2330 train_time:95326ms step_avg:61.86ms
step:1542/2330 train_time:95390ms step_avg:61.86ms
step:1543/2330 train_time:95450ms step_avg:61.86ms
step:1544/2330 train_time:95513ms step_avg:61.86ms
step:1545/2330 train_time:95574ms step_avg:61.86ms
step:1546/2330 train_time:95637ms step_avg:61.86ms
step:1547/2330 train_time:95698ms step_avg:61.86ms
step:1548/2330 train_time:95762ms step_avg:61.86ms
step:1549/2330 train_time:95823ms step_avg:61.86ms
step:1550/2330 train_time:95889ms step_avg:61.86ms
step:1551/2330 train_time:95951ms step_avg:61.86ms
step:1552/2330 train_time:96016ms step_avg:61.87ms
step:1553/2330 train_time:96079ms step_avg:61.87ms
step:1554/2330 train_time:96144ms step_avg:61.87ms
step:1555/2330 train_time:96206ms step_avg:61.87ms
step:1556/2330 train_time:96270ms step_avg:61.87ms
step:1557/2330 train_time:96332ms step_avg:61.87ms
step:1558/2330 train_time:96396ms step_avg:61.87ms
step:1559/2330 train_time:96457ms step_avg:61.87ms
step:1560/2330 train_time:96521ms step_avg:61.87ms
step:1561/2330 train_time:96581ms step_avg:61.87ms
step:1562/2330 train_time:96645ms step_avg:61.87ms
step:1563/2330 train_time:96705ms step_avg:61.87ms
step:1564/2330 train_time:96769ms step_avg:61.87ms
step:1565/2330 train_time:96832ms step_avg:61.87ms
step:1566/2330 train_time:96896ms step_avg:61.87ms
step:1567/2330 train_time:96958ms step_avg:61.87ms
step:1568/2330 train_time:97022ms step_avg:61.88ms
step:1569/2330 train_time:97084ms step_avg:61.88ms
step:1570/2330 train_time:97149ms step_avg:61.88ms
step:1571/2330 train_time:97211ms step_avg:61.88ms
step:1572/2330 train_time:97275ms step_avg:61.88ms
step:1573/2330 train_time:97336ms step_avg:61.88ms
step:1574/2330 train_time:97400ms step_avg:61.88ms
step:1575/2330 train_time:97461ms step_avg:61.88ms
step:1576/2330 train_time:97526ms step_avg:61.88ms
step:1577/2330 train_time:97587ms step_avg:61.88ms
step:1578/2330 train_time:97651ms step_avg:61.88ms
step:1579/2330 train_time:97712ms step_avg:61.88ms
step:1580/2330 train_time:97776ms step_avg:61.88ms
step:1581/2330 train_time:97837ms step_avg:61.88ms
step:1582/2330 train_time:97902ms step_avg:61.88ms
step:1583/2330 train_time:97964ms step_avg:61.89ms
step:1584/2330 train_time:98029ms step_avg:61.89ms
step:1585/2330 train_time:98091ms step_avg:61.89ms
step:1586/2330 train_time:98156ms step_avg:61.89ms
step:1587/2330 train_time:98217ms step_avg:61.89ms
step:1588/2330 train_time:98282ms step_avg:61.89ms
step:1589/2330 train_time:98343ms step_avg:61.89ms
step:1590/2330 train_time:98408ms step_avg:61.89ms
step:1591/2330 train_time:98469ms step_avg:61.89ms
step:1592/2330 train_time:98533ms step_avg:61.89ms
step:1593/2330 train_time:98594ms step_avg:61.89ms
step:1594/2330 train_time:98658ms step_avg:61.89ms
step:1595/2330 train_time:98719ms step_avg:61.89ms
step:1596/2330 train_time:98783ms step_avg:61.89ms
step:1597/2330 train_time:98844ms step_avg:61.89ms
step:1598/2330 train_time:98908ms step_avg:61.90ms
step:1599/2330 train_time:98970ms step_avg:61.89ms
step:1600/2330 train_time:99035ms step_avg:61.90ms
step:1601/2330 train_time:99096ms step_avg:61.90ms
step:1602/2330 train_time:99160ms step_avg:61.90ms
step:1603/2330 train_time:99221ms step_avg:61.90ms
step:1604/2330 train_time:99285ms step_avg:61.90ms
step:1605/2330 train_time:99346ms step_avg:61.90ms
step:1606/2330 train_time:99410ms step_avg:61.90ms
step:1607/2330 train_time:99472ms step_avg:61.90ms
step:1608/2330 train_time:99536ms step_avg:61.90ms
step:1609/2330 train_time:99597ms step_avg:61.90ms
step:1610/2330 train_time:99661ms step_avg:61.90ms
step:1611/2330 train_time:99721ms step_avg:61.90ms
step:1612/2330 train_time:99785ms step_avg:61.90ms
step:1613/2330 train_time:99847ms step_avg:61.90ms
step:1614/2330 train_time:99911ms step_avg:61.90ms
step:1615/2330 train_time:99973ms step_avg:61.90ms
step:1616/2330 train_time:100037ms step_avg:61.90ms
step:1617/2330 train_time:100099ms step_avg:61.90ms
step:1618/2330 train_time:100164ms step_avg:61.91ms
step:1619/2330 train_time:100226ms step_avg:61.91ms
step:1620/2330 train_time:100290ms step_avg:61.91ms
step:1621/2330 train_time:100351ms step_avg:61.91ms
step:1622/2330 train_time:100415ms step_avg:61.91ms
step:1623/2330 train_time:100476ms step_avg:61.91ms
step:1624/2330 train_time:100540ms step_avg:61.91ms
step:1625/2330 train_time:100601ms step_avg:61.91ms
step:1626/2330 train_time:100665ms step_avg:61.91ms
step:1627/2330 train_time:100726ms step_avg:61.91ms
step:1628/2330 train_time:100791ms step_avg:61.91ms
step:1629/2330 train_time:100853ms step_avg:61.91ms
step:1630/2330 train_time:100918ms step_avg:61.91ms
step:1631/2330 train_time:100978ms step_avg:61.91ms
step:1632/2330 train_time:101043ms step_avg:61.91ms
step:1633/2330 train_time:101105ms step_avg:61.91ms
step:1634/2330 train_time:101169ms step_avg:61.92ms
step:1635/2330 train_time:101231ms step_avg:61.91ms
step:1636/2330 train_time:101295ms step_avg:61.92ms
step:1637/2330 train_time:101357ms step_avg:61.92ms
step:1638/2330 train_time:101420ms step_avg:61.92ms
step:1639/2330 train_time:101481ms step_avg:61.92ms
step:1640/2330 train_time:101545ms step_avg:61.92ms
step:1641/2330 train_time:101607ms step_avg:61.92ms
step:1642/2330 train_time:101671ms step_avg:61.92ms
step:1643/2330 train_time:101733ms step_avg:61.92ms
step:1644/2330 train_time:101798ms step_avg:61.92ms
step:1645/2330 train_time:101859ms step_avg:61.92ms
step:1646/2330 train_time:101924ms step_avg:61.92ms
step:1647/2330 train_time:101986ms step_avg:61.92ms
step:1648/2330 train_time:102051ms step_avg:61.92ms
step:1649/2330 train_time:102112ms step_avg:61.92ms
step:1650/2330 train_time:102177ms step_avg:61.93ms
step:1651/2330 train_time:102238ms step_avg:61.93ms
step:1652/2330 train_time:102303ms step_avg:61.93ms
step:1653/2330 train_time:102365ms step_avg:61.93ms
step:1654/2330 train_time:102429ms step_avg:61.93ms
step:1655/2330 train_time:102490ms step_avg:61.93ms
step:1656/2330 train_time:102554ms step_avg:61.93ms
step:1657/2330 train_time:102614ms step_avg:61.93ms
step:1658/2330 train_time:102678ms step_avg:61.93ms
step:1659/2330 train_time:102739ms step_avg:61.93ms
step:1660/2330 train_time:102803ms step_avg:61.93ms
step:1661/2330 train_time:102865ms step_avg:61.93ms
step:1662/2330 train_time:102929ms step_avg:61.93ms
step:1663/2330 train_time:102990ms step_avg:61.93ms
step:1664/2330 train_time:103055ms step_avg:61.93ms
step:1665/2330 train_time:103116ms step_avg:61.93ms
step:1666/2330 train_time:103180ms step_avg:61.93ms
step:1667/2330 train_time:103241ms step_avg:61.93ms
step:1668/2330 train_time:103306ms step_avg:61.93ms
step:1669/2330 train_time:103367ms step_avg:61.93ms
step:1670/2330 train_time:103431ms step_avg:61.93ms
step:1671/2330 train_time:103492ms step_avg:61.93ms
step:1672/2330 train_time:103557ms step_avg:61.94ms
step:1673/2330 train_time:103618ms step_avg:61.94ms
step:1674/2330 train_time:103682ms step_avg:61.94ms
step:1675/2330 train_time:103744ms step_avg:61.94ms
step:1676/2330 train_time:103808ms step_avg:61.94ms
step:1677/2330 train_time:103869ms step_avg:61.94ms
step:1678/2330 train_time:103935ms step_avg:61.94ms
step:1679/2330 train_time:103996ms step_avg:61.94ms
step:1680/2330 train_time:104060ms step_avg:61.94ms
step:1681/2330 train_time:104120ms step_avg:61.94ms
step:1682/2330 train_time:104184ms step_avg:61.94ms
step:1683/2330 train_time:104245ms step_avg:61.94ms
step:1684/2330 train_time:104309ms step_avg:61.94ms
step:1685/2330 train_time:104370ms step_avg:61.94ms
step:1686/2330 train_time:104434ms step_avg:61.94ms
step:1687/2330 train_time:104495ms step_avg:61.94ms
step:1688/2330 train_time:104560ms step_avg:61.94ms
step:1689/2330 train_time:104621ms step_avg:61.94ms
step:1690/2330 train_time:104685ms step_avg:61.94ms
step:1691/2330 train_time:104746ms step_avg:61.94ms
step:1692/2330 train_time:104810ms step_avg:61.94ms
step:1693/2330 train_time:104871ms step_avg:61.94ms
step:1694/2330 train_time:104936ms step_avg:61.95ms
step:1695/2330 train_time:104997ms step_avg:61.95ms
step:1696/2330 train_time:105061ms step_avg:61.95ms
step:1697/2330 train_time:105122ms step_avg:61.95ms
step:1698/2330 train_time:105186ms step_avg:61.95ms
step:1699/2330 train_time:105247ms step_avg:61.95ms
step:1700/2330 train_time:105312ms step_avg:61.95ms
step:1701/2330 train_time:105373ms step_avg:61.95ms
step:1702/2330 train_time:105436ms step_avg:61.95ms
step:1703/2330 train_time:105498ms step_avg:61.95ms
step:1704/2330 train_time:105562ms step_avg:61.95ms
step:1705/2330 train_time:105623ms step_avg:61.95ms
step:1706/2330 train_time:105687ms step_avg:61.95ms
step:1707/2330 train_time:105749ms step_avg:61.95ms
step:1708/2330 train_time:105813ms step_avg:61.95ms
step:1709/2330 train_time:105874ms step_avg:61.95ms
step:1710/2330 train_time:105938ms step_avg:61.95ms
step:1711/2330 train_time:105999ms step_avg:61.95ms
step:1712/2330 train_time:106064ms step_avg:61.95ms
step:1713/2330 train_time:106125ms step_avg:61.95ms
step:1714/2330 train_time:106190ms step_avg:61.95ms
step:1715/2330 train_time:106251ms step_avg:61.95ms
step:1716/2330 train_time:106315ms step_avg:61.96ms
step:1717/2330 train_time:106376ms step_avg:61.95ms
step:1718/2330 train_time:106440ms step_avg:61.96ms
step:1719/2330 train_time:106501ms step_avg:61.96ms
step:1720/2330 train_time:106565ms step_avg:61.96ms
step:1721/2330 train_time:106626ms step_avg:61.96ms
step:1722/2330 train_time:106691ms step_avg:61.96ms
step:1723/2330 train_time:106753ms step_avg:61.96ms
step:1724/2330 train_time:106817ms step_avg:61.96ms
step:1725/2330 train_time:106879ms step_avg:61.96ms
step:1726/2330 train_time:106944ms step_avg:61.96ms
step:1727/2330 train_time:107006ms step_avg:61.96ms
step:1728/2330 train_time:107070ms step_avg:61.96ms
step:1729/2330 train_time:107131ms step_avg:61.96ms
step:1730/2330 train_time:107196ms step_avg:61.96ms
step:1731/2330 train_time:107257ms step_avg:61.96ms
step:1732/2330 train_time:107321ms step_avg:61.96ms
step:1733/2330 train_time:107382ms step_avg:61.96ms
step:1734/2330 train_time:107446ms step_avg:61.96ms
step:1735/2330 train_time:107507ms step_avg:61.96ms
step:1736/2330 train_time:107571ms step_avg:61.96ms
step:1737/2330 train_time:107632ms step_avg:61.96ms
step:1738/2330 train_time:107696ms step_avg:61.97ms
step:1739/2330 train_time:107758ms step_avg:61.97ms
step:1740/2330 train_time:107822ms step_avg:61.97ms
step:1741/2330 train_time:107884ms step_avg:61.97ms
step:1742/2330 train_time:107949ms step_avg:61.97ms
step:1743/2330 train_time:108011ms step_avg:61.97ms
step:1744/2330 train_time:108075ms step_avg:61.97ms
step:1745/2330 train_time:108137ms step_avg:61.97ms
step:1746/2330 train_time:108202ms step_avg:61.97ms
step:1747/2330 train_time:108264ms step_avg:61.97ms
step:1748/2330 train_time:108328ms step_avg:61.97ms
step:1749/2330 train_time:108390ms step_avg:61.97ms
step:1750/2330 train_time:108453ms step_avg:61.97ms
step:1750/2330 val_loss:3.6543 train_time:108519ms step_avg:62.01ms
step:1751/2330 train_time:108542ms step_avg:61.99ms
step:1752/2330 train_time:108581ms step_avg:61.98ms
step:1753/2330 train_time:108649ms step_avg:61.98ms
step:1754/2330 train_time:108714ms step_avg:61.98ms
step:1755/2330 train_time:108774ms step_avg:61.98ms
step:1756/2330 train_time:108838ms step_avg:61.98ms
step:1757/2330 train_time:108898ms step_avg:61.98ms
step:1758/2330 train_time:108962ms step_avg:61.98ms
step:1759/2330 train_time:109022ms step_avg:61.98ms
step:1760/2330 train_time:109086ms step_avg:61.98ms
step:1761/2330 train_time:109145ms step_avg:61.98ms
step:1762/2330 train_time:109209ms step_avg:61.98ms
step:1763/2330 train_time:109269ms step_avg:61.98ms
step:1764/2330 train_time:109332ms step_avg:61.98ms
step:1765/2330 train_time:109393ms step_avg:61.98ms
step:1766/2330 train_time:109462ms step_avg:61.98ms
step:1767/2330 train_time:109525ms step_avg:61.98ms
step:1768/2330 train_time:109590ms step_avg:61.99ms
step:1769/2330 train_time:109652ms step_avg:61.99ms
step:1770/2330 train_time:109716ms step_avg:61.99ms
step:1771/2330 train_time:109777ms step_avg:61.99ms
step:1772/2330 train_time:109841ms step_avg:61.99ms
step:1773/2330 train_time:109901ms step_avg:61.99ms
step:1774/2330 train_time:109965ms step_avg:61.99ms
step:1775/2330 train_time:110026ms step_avg:61.99ms
step:1776/2330 train_time:110089ms step_avg:61.99ms
step:1777/2330 train_time:110150ms step_avg:61.99ms
step:1778/2330 train_time:110214ms step_avg:61.99ms
step:1779/2330 train_time:110274ms step_avg:61.99ms
step:1780/2330 train_time:110338ms step_avg:61.99ms
step:1781/2330 train_time:110401ms step_avg:61.99ms
step:1782/2330 train_time:110468ms step_avg:61.99ms
step:1783/2330 train_time:110529ms step_avg:61.99ms
step:1784/2330 train_time:110595ms step_avg:61.99ms
step:1785/2330 train_time:110657ms step_avg:61.99ms
step:1786/2330 train_time:110722ms step_avg:61.99ms
step:1787/2330 train_time:110783ms step_avg:61.99ms
step:1788/2330 train_time:110847ms step_avg:61.99ms
step:1789/2330 train_time:110908ms step_avg:61.99ms
step:1790/2330 train_time:110973ms step_avg:62.00ms
step:1791/2330 train_time:111033ms step_avg:62.00ms
step:1792/2330 train_time:111097ms step_avg:62.00ms
step:1793/2330 train_time:111157ms step_avg:62.00ms
step:1794/2330 train_time:111222ms step_avg:62.00ms
step:1795/2330 train_time:111283ms step_avg:62.00ms
step:1796/2330 train_time:111348ms step_avg:62.00ms
step:1797/2330 train_time:111410ms step_avg:62.00ms
step:1798/2330 train_time:111475ms step_avg:62.00ms
step:1799/2330 train_time:111537ms step_avg:62.00ms
step:1800/2330 train_time:111602ms step_avg:62.00ms
step:1801/2330 train_time:111665ms step_avg:62.00ms
step:1802/2330 train_time:111729ms step_avg:62.00ms
step:1803/2330 train_time:111790ms step_avg:62.00ms
step:1804/2330 train_time:111855ms step_avg:62.00ms
step:1805/2330 train_time:111916ms step_avg:62.00ms
step:1806/2330 train_time:111979ms step_avg:62.00ms
step:1807/2330 train_time:112040ms step_avg:62.00ms
step:1808/2330 train_time:112103ms step_avg:62.00ms
step:1809/2330 train_time:112164ms step_avg:62.00ms
step:1810/2330 train_time:112228ms step_avg:62.00ms
step:1811/2330 train_time:112288ms step_avg:62.00ms
step:1812/2330 train_time:112352ms step_avg:62.00ms
step:1813/2330 train_time:112415ms step_avg:62.01ms
step:1814/2330 train_time:112480ms step_avg:62.01ms
step:1815/2330 train_time:112542ms step_avg:62.01ms
step:1816/2330 train_time:112607ms step_avg:62.01ms
step:1817/2330 train_time:112669ms step_avg:62.01ms
step:1818/2330 train_time:112734ms step_avg:62.01ms
step:1819/2330 train_time:112795ms step_avg:62.01ms
step:1820/2330 train_time:112860ms step_avg:62.01ms
step:1821/2330 train_time:112921ms step_avg:62.01ms
step:1822/2330 train_time:112985ms step_avg:62.01ms
step:1823/2330 train_time:113045ms step_avg:62.01ms
step:1824/2330 train_time:113109ms step_avg:62.01ms
step:1825/2330 train_time:113169ms step_avg:62.01ms
step:1826/2330 train_time:113234ms step_avg:62.01ms
step:1827/2330 train_time:113294ms step_avg:62.01ms
step:1828/2330 train_time:113359ms step_avg:62.01ms
step:1829/2330 train_time:113420ms step_avg:62.01ms
step:1830/2330 train_time:113485ms step_avg:62.01ms
step:1831/2330 train_time:113547ms step_avg:62.01ms
step:1832/2330 train_time:113611ms step_avg:62.01ms
step:1833/2330 train_time:113672ms step_avg:62.01ms
step:1834/2330 train_time:113736ms step_avg:62.02ms
step:1835/2330 train_time:113798ms step_avg:62.02ms
step:1836/2330 train_time:113863ms step_avg:62.02ms
step:1837/2330 train_time:113924ms step_avg:62.02ms
step:1838/2330 train_time:113988ms step_avg:62.02ms
step:1839/2330 train_time:114049ms step_avg:62.02ms
step:1840/2330 train_time:114113ms step_avg:62.02ms
step:1841/2330 train_time:114174ms step_avg:62.02ms
step:1842/2330 train_time:114238ms step_avg:62.02ms
step:1843/2330 train_time:114299ms step_avg:62.02ms
step:1844/2330 train_time:114364ms step_avg:62.02ms
step:1845/2330 train_time:114425ms step_avg:62.02ms
step:1846/2330 train_time:114490ms step_avg:62.02ms
step:1847/2330 train_time:114551ms step_avg:62.02ms
step:1848/2330 train_time:114616ms step_avg:62.02ms
step:1849/2330 train_time:114677ms step_avg:62.02ms
step:1850/2330 train_time:114742ms step_avg:62.02ms
step:1851/2330 train_time:114804ms step_avg:62.02ms
step:1852/2330 train_time:114869ms step_avg:62.02ms
step:1853/2330 train_time:114930ms step_avg:62.02ms
step:1854/2330 train_time:114995ms step_avg:62.03ms
step:1855/2330 train_time:115055ms step_avg:62.02ms
step:1856/2330 train_time:115120ms step_avg:62.03ms
step:1857/2330 train_time:115181ms step_avg:62.03ms
step:1858/2330 train_time:115246ms step_avg:62.03ms
step:1859/2330 train_time:115307ms step_avg:62.03ms
step:1860/2330 train_time:115372ms step_avg:62.03ms
step:1861/2330 train_time:115433ms step_avg:62.03ms
step:1862/2330 train_time:115498ms step_avg:62.03ms
step:1863/2330 train_time:115559ms step_avg:62.03ms
step:1864/2330 train_time:115624ms step_avg:62.03ms
step:1865/2330 train_time:115685ms step_avg:62.03ms
step:1866/2330 train_time:115749ms step_avg:62.03ms
step:1867/2330 train_time:115810ms step_avg:62.03ms
step:1868/2330 train_time:115875ms step_avg:62.03ms
step:1869/2330 train_time:115936ms step_avg:62.03ms
step:1870/2330 train_time:116001ms step_avg:62.03ms
step:1871/2330 train_time:116063ms step_avg:62.03ms
step:1872/2330 train_time:116127ms step_avg:62.03ms
step:1873/2330 train_time:116188ms step_avg:62.03ms
step:1874/2330 train_time:116252ms step_avg:62.03ms
step:1875/2330 train_time:116312ms step_avg:62.03ms
step:1876/2330 train_time:116377ms step_avg:62.03ms
step:1877/2330 train_time:116438ms step_avg:62.03ms
step:1878/2330 train_time:116503ms step_avg:62.04ms
step:1879/2330 train_time:116564ms step_avg:62.03ms
step:1880/2330 train_time:116628ms step_avg:62.04ms
step:1881/2330 train_time:116689ms step_avg:62.04ms
step:1882/2330 train_time:116753ms step_avg:62.04ms
step:1883/2330 train_time:116815ms step_avg:62.04ms
step:1884/2330 train_time:116880ms step_avg:62.04ms
step:1885/2330 train_time:116941ms step_avg:62.04ms
step:1886/2330 train_time:117006ms step_avg:62.04ms
step:1887/2330 train_time:117067ms step_avg:62.04ms
step:1888/2330 train_time:117131ms step_avg:62.04ms
step:1889/2330 train_time:117192ms step_avg:62.04ms
step:1890/2330 train_time:117256ms step_avg:62.04ms
step:1891/2330 train_time:117317ms step_avg:62.04ms
step:1892/2330 train_time:117381ms step_avg:62.04ms
step:1893/2330 train_time:117442ms step_avg:62.04ms
step:1894/2330 train_time:117506ms step_avg:62.04ms
step:1895/2330 train_time:117568ms step_avg:62.04ms
step:1896/2330 train_time:117633ms step_avg:62.04ms
step:1897/2330 train_time:117694ms step_avg:62.04ms
step:1898/2330 train_time:117758ms step_avg:62.04ms
step:1899/2330 train_time:117820ms step_avg:62.04ms
step:1900/2330 train_time:117885ms step_avg:62.04ms
step:1901/2330 train_time:117946ms step_avg:62.04ms
step:1902/2330 train_time:118010ms step_avg:62.05ms
step:1903/2330 train_time:118071ms step_avg:62.04ms
step:1904/2330 train_time:118134ms step_avg:62.05ms
step:1905/2330 train_time:118195ms step_avg:62.04ms
step:1906/2330 train_time:118259ms step_avg:62.05ms
step:1907/2330 train_time:118321ms step_avg:62.05ms
step:1908/2330 train_time:118385ms step_avg:62.05ms
step:1909/2330 train_time:118446ms step_avg:62.05ms
step:1910/2330 train_time:118511ms step_avg:62.05ms
step:1911/2330 train_time:118572ms step_avg:62.05ms
step:1912/2330 train_time:118635ms step_avg:62.05ms
step:1913/2330 train_time:118696ms step_avg:62.05ms
step:1914/2330 train_time:118761ms step_avg:62.05ms
step:1915/2330 train_time:118823ms step_avg:62.05ms
step:1916/2330 train_time:118888ms step_avg:62.05ms
step:1917/2330 train_time:118950ms step_avg:62.05ms
step:1918/2330 train_time:119014ms step_avg:62.05ms
step:1919/2330 train_time:119075ms step_avg:62.05ms
step:1920/2330 train_time:119139ms step_avg:62.05ms
step:1921/2330 train_time:119201ms step_avg:62.05ms
step:1922/2330 train_time:119266ms step_avg:62.05ms
step:1923/2330 train_time:119326ms step_avg:62.05ms
step:1924/2330 train_time:119390ms step_avg:62.05ms
step:1925/2330 train_time:119450ms step_avg:62.05ms
step:1926/2330 train_time:119514ms step_avg:62.05ms
step:1927/2330 train_time:119576ms step_avg:62.05ms
step:1928/2330 train_time:119640ms step_avg:62.05ms
step:1929/2330 train_time:119702ms step_avg:62.05ms
step:1930/2330 train_time:119768ms step_avg:62.06ms
step:1931/2330 train_time:119829ms step_avg:62.06ms
step:1932/2330 train_time:119894ms step_avg:62.06ms
step:1933/2330 train_time:119956ms step_avg:62.06ms
step:1934/2330 train_time:120020ms step_avg:62.06ms
step:1935/2330 train_time:120081ms step_avg:62.06ms
step:1936/2330 train_time:120145ms step_avg:62.06ms
step:1937/2330 train_time:120207ms step_avg:62.06ms
step:1938/2330 train_time:120271ms step_avg:62.06ms
step:1939/2330 train_time:120332ms step_avg:62.06ms
step:1940/2330 train_time:120396ms step_avg:62.06ms
step:1941/2330 train_time:120458ms step_avg:62.06ms
step:1942/2330 train_time:120523ms step_avg:62.06ms
step:1943/2330 train_time:120583ms step_avg:62.06ms
step:1944/2330 train_time:120648ms step_avg:62.06ms
step:1945/2330 train_time:120709ms step_avg:62.06ms
step:1946/2330 train_time:120773ms step_avg:62.06ms
step:1947/2330 train_time:120834ms step_avg:62.06ms
step:1948/2330 train_time:120898ms step_avg:62.06ms
step:1949/2330 train_time:120960ms step_avg:62.06ms
step:1950/2330 train_time:121025ms step_avg:62.06ms
step:1951/2330 train_time:121086ms step_avg:62.06ms
step:1952/2330 train_time:121151ms step_avg:62.06ms
step:1953/2330 train_time:121212ms step_avg:62.06ms
step:1954/2330 train_time:121276ms step_avg:62.07ms
step:1955/2330 train_time:121337ms step_avg:62.07ms
step:1956/2330 train_time:121402ms step_avg:62.07ms
step:1957/2330 train_time:121463ms step_avg:62.07ms
step:1958/2330 train_time:121528ms step_avg:62.07ms
step:1959/2330 train_time:121589ms step_avg:62.07ms
step:1960/2330 train_time:121653ms step_avg:62.07ms
step:1961/2330 train_time:121714ms step_avg:62.07ms
step:1962/2330 train_time:121778ms step_avg:62.07ms
step:1963/2330 train_time:121840ms step_avg:62.07ms
step:1964/2330 train_time:121904ms step_avg:62.07ms
step:1965/2330 train_time:121965ms step_avg:62.07ms
step:1966/2330 train_time:122029ms step_avg:62.07ms
step:1967/2330 train_time:122089ms step_avg:62.07ms
step:1968/2330 train_time:122154ms step_avg:62.07ms
step:1969/2330 train_time:122214ms step_avg:62.07ms
step:1970/2330 train_time:122279ms step_avg:62.07ms
step:1971/2330 train_time:122341ms step_avg:62.07ms
step:1972/2330 train_time:122405ms step_avg:62.07ms
step:1973/2330 train_time:122466ms step_avg:62.07ms
step:1974/2330 train_time:122530ms step_avg:62.07ms
step:1975/2330 train_time:122591ms step_avg:62.07ms
step:1976/2330 train_time:122655ms step_avg:62.07ms
step:1977/2330 train_time:122716ms step_avg:62.07ms
step:1978/2330 train_time:122781ms step_avg:62.07ms
step:1979/2330 train_time:122842ms step_avg:62.07ms
step:1980/2330 train_time:122907ms step_avg:62.07ms
step:1981/2330 train_time:122969ms step_avg:62.07ms
step:1982/2330 train_time:123033ms step_avg:62.08ms
step:1983/2330 train_time:123094ms step_avg:62.07ms
step:1984/2330 train_time:123157ms step_avg:62.08ms
step:1985/2330 train_time:123218ms step_avg:62.07ms
step:1986/2330 train_time:123283ms step_avg:62.08ms
step:1987/2330 train_time:123345ms step_avg:62.08ms
step:1988/2330 train_time:123409ms step_avg:62.08ms
step:1989/2330 train_time:123471ms step_avg:62.08ms
step:1990/2330 train_time:123534ms step_avg:62.08ms
step:1991/2330 train_time:123595ms step_avg:62.08ms
step:1992/2330 train_time:123661ms step_avg:62.08ms
step:1993/2330 train_time:123721ms step_avg:62.08ms
step:1994/2330 train_time:123786ms step_avg:62.08ms
step:1995/2330 train_time:123848ms step_avg:62.08ms
step:1996/2330 train_time:123912ms step_avg:62.08ms
step:1997/2330 train_time:123974ms step_avg:62.08ms
step:1998/2330 train_time:124038ms step_avg:62.08ms
step:1999/2330 train_time:124099ms step_avg:62.08ms
step:2000/2330 train_time:124164ms step_avg:62.08ms
step:2000/2330 val_loss:3.6002 train_time:124229ms step_avg:62.11ms
step:2001/2330 train_time:124252ms step_avg:62.10ms
step:2002/2330 train_time:124292ms step_avg:62.08ms
step:2003/2330 train_time:124358ms step_avg:62.09ms
step:2004/2330 train_time:124425ms step_avg:62.09ms
step:2005/2330 train_time:124487ms step_avg:62.09ms
step:2006/2330 train_time:124550ms step_avg:62.09ms
step:2007/2330 train_time:124611ms step_avg:62.09ms
step:2008/2330 train_time:124674ms step_avg:62.09ms
step:2009/2330 train_time:124734ms step_avg:62.09ms
step:2010/2330 train_time:124798ms step_avg:62.09ms
step:2011/2330 train_time:124859ms step_avg:62.09ms
step:2012/2330 train_time:124923ms step_avg:62.09ms
step:2013/2330 train_time:124983ms step_avg:62.09ms
step:2014/2330 train_time:125046ms step_avg:62.09ms
step:2015/2330 train_time:125106ms step_avg:62.09ms
step:2016/2330 train_time:125170ms step_avg:62.09ms
step:2017/2330 train_time:125232ms step_avg:62.09ms
step:2018/2330 train_time:125299ms step_avg:62.09ms
step:2019/2330 train_time:125362ms step_avg:62.09ms
step:2020/2330 train_time:125427ms step_avg:62.09ms
step:2021/2330 train_time:125489ms step_avg:62.09ms
step:2022/2330 train_time:125553ms step_avg:62.09ms
step:2023/2330 train_time:125614ms step_avg:62.09ms
step:2024/2330 train_time:125678ms step_avg:62.09ms
step:2025/2330 train_time:125739ms step_avg:62.09ms
step:2026/2330 train_time:125802ms step_avg:62.09ms
step:2027/2330 train_time:125863ms step_avg:62.09ms
step:2028/2330 train_time:125927ms step_avg:62.09ms
step:2029/2330 train_time:125989ms step_avg:62.09ms
step:2030/2330 train_time:126052ms step_avg:62.09ms
step:2031/2330 train_time:126113ms step_avg:62.09ms
step:2032/2330 train_time:126178ms step_avg:62.10ms
step:2033/2330 train_time:126240ms step_avg:62.10ms
step:2034/2330 train_time:126306ms step_avg:62.10ms
step:2035/2330 train_time:126369ms step_avg:62.10ms
step:2036/2330 train_time:126434ms step_avg:62.10ms
step:2037/2330 train_time:126496ms step_avg:62.10ms
step:2038/2330 train_time:126560ms step_avg:62.10ms
step:2039/2330 train_time:126621ms step_avg:62.10ms
step:2040/2330 train_time:126686ms step_avg:62.10ms
step:2041/2330 train_time:126747ms step_avg:62.10ms
step:2042/2330 train_time:126811ms step_avg:62.10ms
step:2043/2330 train_time:126872ms step_avg:62.10ms
step:2044/2330 train_time:126936ms step_avg:62.10ms
step:2045/2330 train_time:126996ms step_avg:62.10ms
step:2046/2330 train_time:127060ms step_avg:62.10ms
step:2047/2330 train_time:127121ms step_avg:62.10ms
step:2048/2330 train_time:127186ms step_avg:62.10ms
step:2049/2330 train_time:127248ms step_avg:62.10ms
step:2050/2330 train_time:127313ms step_avg:62.10ms
step:2051/2330 train_time:127376ms step_avg:62.10ms
step:2052/2330 train_time:127441ms step_avg:62.11ms
step:2053/2330 train_time:127502ms step_avg:62.11ms
step:2054/2330 train_time:127566ms step_avg:62.11ms
step:2055/2330 train_time:127629ms step_avg:62.11ms
step:2056/2330 train_time:127693ms step_avg:62.11ms
step:2057/2330 train_time:127753ms step_avg:62.11ms
step:2058/2330 train_time:127817ms step_avg:62.11ms
step:2059/2330 train_time:127880ms step_avg:62.11ms
step:2060/2330 train_time:127944ms step_avg:62.11ms
step:2061/2330 train_time:128004ms step_avg:62.11ms
step:2062/2330 train_time:128069ms step_avg:62.11ms
step:2063/2330 train_time:128129ms step_avg:62.11ms
step:2064/2330 train_time:128194ms step_avg:62.11ms
step:2065/2330 train_time:128255ms step_avg:62.11ms
step:2066/2330 train_time:128320ms step_avg:62.11ms
step:2067/2330 train_time:128382ms step_avg:62.11ms
step:2068/2330 train_time:128447ms step_avg:62.11ms
step:2069/2330 train_time:128508ms step_avg:62.11ms
step:2070/2330 train_time:128573ms step_avg:62.11ms
step:2071/2330 train_time:128634ms step_avg:62.11ms
step:2072/2330 train_time:128698ms step_avg:62.11ms
step:2073/2330 train_time:128759ms step_avg:62.11ms
step:2074/2330 train_time:128824ms step_avg:62.11ms
step:2075/2330 train_time:128885ms step_avg:62.11ms
step:2076/2330 train_time:128949ms step_avg:62.11ms
step:2077/2330 train_time:129009ms step_avg:62.11ms
step:2078/2330 train_time:129074ms step_avg:62.11ms
step:2079/2330 train_time:129136ms step_avg:62.11ms
step:2080/2330 train_time:129200ms step_avg:62.12ms
step:2081/2330 train_time:129261ms step_avg:62.11ms
step:2082/2330 train_time:129325ms step_avg:62.12ms
step:2083/2330 train_time:129387ms step_avg:62.12ms
step:2084/2330 train_time:129451ms step_avg:62.12ms
step:2085/2330 train_time:129512ms step_avg:62.12ms
step:2086/2330 train_time:129577ms step_avg:62.12ms
step:2087/2330 train_time:129638ms step_avg:62.12ms
step:2088/2330 train_time:129703ms step_avg:62.12ms
step:2089/2330 train_time:129765ms step_avg:62.12ms
step:2090/2330 train_time:129829ms step_avg:62.12ms
step:2091/2330 train_time:129890ms step_avg:62.12ms
step:2092/2330 train_time:129953ms step_avg:62.12ms
step:2093/2330 train_time:130015ms step_avg:62.12ms
step:2094/2330 train_time:130080ms step_avg:62.12ms
step:2095/2330 train_time:130142ms step_avg:62.12ms
step:2096/2330 train_time:130206ms step_avg:62.12ms
step:2097/2330 train_time:130267ms step_avg:62.12ms
step:2098/2330 train_time:130332ms step_avg:62.12ms
step:2099/2330 train_time:130394ms step_avg:62.12ms
step:2100/2330 train_time:130458ms step_avg:62.12ms
step:2101/2330 train_time:130520ms step_avg:62.12ms
step:2102/2330 train_time:130585ms step_avg:62.12ms
step:2103/2330 train_time:130646ms step_avg:62.12ms
step:2104/2330 train_time:130710ms step_avg:62.12ms
step:2105/2330 train_time:130771ms step_avg:62.12ms
step:2106/2330 train_time:130835ms step_avg:62.12ms
step:2107/2330 train_time:130896ms step_avg:62.12ms
step:2108/2330 train_time:130961ms step_avg:62.13ms
step:2109/2330 train_time:131022ms step_avg:62.12ms
step:2110/2330 train_time:131087ms step_avg:62.13ms
step:2111/2330 train_time:131148ms step_avg:62.13ms
step:2112/2330 train_time:131212ms step_avg:62.13ms
step:2113/2330 train_time:131273ms step_avg:62.13ms
step:2114/2330 train_time:131337ms step_avg:62.13ms
step:2115/2330 train_time:131398ms step_avg:62.13ms
step:2116/2330 train_time:131463ms step_avg:62.13ms
step:2117/2330 train_time:131524ms step_avg:62.13ms
step:2118/2330 train_time:131589ms step_avg:62.13ms
step:2119/2330 train_time:131650ms step_avg:62.13ms
step:2120/2330 train_time:131715ms step_avg:62.13ms
step:2121/2330 train_time:131776ms step_avg:62.13ms
step:2122/2330 train_time:131840ms step_avg:62.13ms
step:2123/2330 train_time:131901ms step_avg:62.13ms
step:2124/2330 train_time:131966ms step_avg:62.13ms
step:2125/2330 train_time:132027ms step_avg:62.13ms
step:2126/2330 train_time:132091ms step_avg:62.13ms
step:2127/2330 train_time:132153ms step_avg:62.13ms
step:2128/2330 train_time:132217ms step_avg:62.13ms
step:2129/2330 train_time:132279ms step_avg:62.13ms
step:2130/2330 train_time:132343ms step_avg:62.13ms
step:2131/2330 train_time:132404ms step_avg:62.13ms
step:2132/2330 train_time:132469ms step_avg:62.13ms
step:2133/2330 train_time:132530ms step_avg:62.13ms
step:2134/2330 train_time:132594ms step_avg:62.13ms
step:2135/2330 train_time:132655ms step_avg:62.13ms
step:2136/2330 train_time:132720ms step_avg:62.13ms
step:2137/2330 train_time:132782ms step_avg:62.13ms
step:2138/2330 train_time:132845ms step_avg:62.14ms
step:2139/2330 train_time:132906ms step_avg:62.13ms
step:2140/2330 train_time:132971ms step_avg:62.14ms
step:2141/2330 train_time:133032ms step_avg:62.14ms
step:2142/2330 train_time:133097ms step_avg:62.14ms
step:2143/2330 train_time:133158ms step_avg:62.14ms
step:2144/2330 train_time:133222ms step_avg:62.14ms
step:2145/2330 train_time:133285ms step_avg:62.14ms
step:2146/2330 train_time:133349ms step_avg:62.14ms
step:2147/2330 train_time:133409ms step_avg:62.14ms
step:2148/2330 train_time:133474ms step_avg:62.14ms
step:2149/2330 train_time:133535ms step_avg:62.14ms
step:2150/2330 train_time:133600ms step_avg:62.14ms
step:2151/2330 train_time:133661ms step_avg:62.14ms
step:2152/2330 train_time:133726ms step_avg:62.14ms
step:2153/2330 train_time:133787ms step_avg:62.14ms
step:2154/2330 train_time:133851ms step_avg:62.14ms
step:2155/2330 train_time:133912ms step_avg:62.14ms
step:2156/2330 train_time:133977ms step_avg:62.14ms
step:2157/2330 train_time:134039ms step_avg:62.14ms
step:2158/2330 train_time:134103ms step_avg:62.14ms
step:2159/2330 train_time:134164ms step_avg:62.14ms
step:2160/2330 train_time:134229ms step_avg:62.14ms
step:2161/2330 train_time:134290ms step_avg:62.14ms
step:2162/2330 train_time:134354ms step_avg:62.14ms
step:2163/2330 train_time:134416ms step_avg:62.14ms
step:2164/2330 train_time:134480ms step_avg:62.14ms
step:2165/2330 train_time:134541ms step_avg:62.14ms
step:2166/2330 train_time:134606ms step_avg:62.14ms
step:2167/2330 train_time:134667ms step_avg:62.14ms
step:2168/2330 train_time:134732ms step_avg:62.15ms
step:2169/2330 train_time:134794ms step_avg:62.15ms
step:2170/2330 train_time:134859ms step_avg:62.15ms
step:2171/2330 train_time:134920ms step_avg:62.15ms
step:2172/2330 train_time:134985ms step_avg:62.15ms
step:2173/2330 train_time:135047ms step_avg:62.15ms
step:2174/2330 train_time:135111ms step_avg:62.15ms
step:2175/2330 train_time:135172ms step_avg:62.15ms
step:2176/2330 train_time:135236ms step_avg:62.15ms
step:2177/2330 train_time:135297ms step_avg:62.15ms
step:2178/2330 train_time:135362ms step_avg:62.15ms
step:2179/2330 train_time:135423ms step_avg:62.15ms
step:2180/2330 train_time:135488ms step_avg:62.15ms
step:2181/2330 train_time:135549ms step_avg:62.15ms
step:2182/2330 train_time:135613ms step_avg:62.15ms
step:2183/2330 train_time:135675ms step_avg:62.15ms
step:2184/2330 train_time:135740ms step_avg:62.15ms
step:2185/2330 train_time:135802ms step_avg:62.15ms
step:2186/2330 train_time:135866ms step_avg:62.15ms
step:2187/2330 train_time:135928ms step_avg:62.15ms
step:2188/2330 train_time:135992ms step_avg:62.15ms
step:2189/2330 train_time:136053ms step_avg:62.15ms
step:2190/2330 train_time:136116ms step_avg:62.15ms
step:2191/2330 train_time:136178ms step_avg:62.15ms
step:2192/2330 train_time:136243ms step_avg:62.15ms
step:2193/2330 train_time:136303ms step_avg:62.15ms
step:2194/2330 train_time:136367ms step_avg:62.15ms
step:2195/2330 train_time:136429ms step_avg:62.15ms
step:2196/2330 train_time:136493ms step_avg:62.16ms
step:2197/2330 train_time:136554ms step_avg:62.15ms
step:2198/2330 train_time:136619ms step_avg:62.16ms
step:2199/2330 train_time:136681ms step_avg:62.16ms
step:2200/2330 train_time:136745ms step_avg:62.16ms
step:2201/2330 train_time:136806ms step_avg:62.16ms
step:2202/2330 train_time:136871ms step_avg:62.16ms
step:2203/2330 train_time:136932ms step_avg:62.16ms
step:2204/2330 train_time:136996ms step_avg:62.16ms
step:2205/2330 train_time:137057ms step_avg:62.16ms
step:2206/2330 train_time:137123ms step_avg:62.16ms
step:2207/2330 train_time:137185ms step_avg:62.16ms
step:2208/2330 train_time:137249ms step_avg:62.16ms
step:2209/2330 train_time:137310ms step_avg:62.16ms
step:2210/2330 train_time:137374ms step_avg:62.16ms
step:2211/2330 train_time:137435ms step_avg:62.16ms
step:2212/2330 train_time:137499ms step_avg:62.16ms
step:2213/2330 train_time:137560ms step_avg:62.16ms
step:2214/2330 train_time:137625ms step_avg:62.16ms
step:2215/2330 train_time:137686ms step_avg:62.16ms
step:2216/2330 train_time:137750ms step_avg:62.16ms
step:2217/2330 train_time:137820ms step_avg:62.17ms
step:2218/2330 train_time:137876ms step_avg:62.16ms
step:2219/2330 train_time:137937ms step_avg:62.16ms
step:2220/2330 train_time:138002ms step_avg:62.16ms
step:2221/2330 train_time:138064ms step_avg:62.16ms
step:2222/2330 train_time:138128ms step_avg:62.16ms
step:2223/2330 train_time:138189ms step_avg:62.16ms
step:2224/2330 train_time:138253ms step_avg:62.16ms
step:2225/2330 train_time:138313ms step_avg:62.16ms
step:2226/2330 train_time:138378ms step_avg:62.16ms
step:2227/2330 train_time:138440ms step_avg:62.16ms
step:2228/2330 train_time:138504ms step_avg:62.17ms
step:2229/2330 train_time:138565ms step_avg:62.16ms
step:2230/2330 train_time:138629ms step_avg:62.17ms
step:2231/2330 train_time:138690ms step_avg:62.17ms
step:2232/2330 train_time:138755ms step_avg:62.17ms
step:2233/2330 train_time:138816ms step_avg:62.17ms
step:2234/2330 train_time:138881ms step_avg:62.17ms
step:2235/2330 train_time:138942ms step_avg:62.17ms
step:2236/2330 train_time:139007ms step_avg:62.17ms
step:2237/2330 train_time:139069ms step_avg:62.17ms
step:2238/2330 train_time:139133ms step_avg:62.17ms
step:2239/2330 train_time:139194ms step_avg:62.17ms
step:2240/2330 train_time:139258ms step_avg:62.17ms
step:2241/2330 train_time:139318ms step_avg:62.17ms
step:2242/2330 train_time:139383ms step_avg:62.17ms
step:2243/2330 train_time:139444ms step_avg:62.17ms
step:2244/2330 train_time:139508ms step_avg:62.17ms
step:2245/2330 train_time:139569ms step_avg:62.17ms
step:2246/2330 train_time:139634ms step_avg:62.17ms
step:2247/2330 train_time:139695ms step_avg:62.17ms
step:2248/2330 train_time:139759ms step_avg:62.17ms
step:2249/2330 train_time:139821ms step_avg:62.17ms
step:2250/2330 train_time:139885ms step_avg:62.17ms
step:2250/2330 val_loss:3.5766 train_time:139951ms step_avg:62.20ms
step:2251/2330 train_time:139974ms step_avg:62.18ms
step:2252/2330 train_time:140014ms step_avg:62.17ms
step:2253/2330 train_time:140080ms step_avg:62.17ms
step:2254/2330 train_time:140144ms step_avg:62.18ms
step:2255/2330 train_time:140206ms step_avg:62.18ms
step:2256/2330 train_time:140270ms step_avg:62.18ms
step:2257/2330 train_time:140330ms step_avg:62.18ms
step:2258/2330 train_time:140393ms step_avg:62.18ms
step:2259/2330 train_time:140453ms step_avg:62.17ms
step:2260/2330 train_time:140516ms step_avg:62.18ms
step:2261/2330 train_time:140577ms step_avg:62.17ms
step:2262/2330 train_time:140640ms step_avg:62.18ms
step:2263/2330 train_time:140701ms step_avg:62.17ms
step:2264/2330 train_time:140765ms step_avg:62.18ms
step:2265/2330 train_time:140826ms step_avg:62.17ms
step:2266/2330 train_time:140891ms step_avg:62.18ms
step:2267/2330 train_time:140953ms step_avg:62.18ms
step:2268/2330 train_time:141018ms step_avg:62.18ms
step:2269/2330 train_time:141081ms step_avg:62.18ms
step:2270/2330 train_time:141147ms step_avg:62.18ms
step:2271/2330 train_time:141209ms step_avg:62.18ms
step:2272/2330 train_time:141272ms step_avg:62.18ms
step:2273/2330 train_time:141333ms step_avg:62.18ms
step:2274/2330 train_time:141396ms step_avg:62.18ms
step:2275/2330 train_time:141456ms step_avg:62.18ms
step:2276/2330 train_time:141520ms step_avg:62.18ms
step:2277/2330 train_time:141580ms step_avg:62.18ms
step:2278/2330 train_time:141644ms step_avg:62.18ms
step:2279/2330 train_time:141705ms step_avg:62.18ms
step:2280/2330 train_time:141768ms step_avg:62.18ms
step:2281/2330 train_time:141829ms step_avg:62.18ms
step:2282/2330 train_time:141893ms step_avg:62.18ms
step:2283/2330 train_time:141954ms step_avg:62.18ms
step:2284/2330 train_time:142019ms step_avg:62.18ms
step:2285/2330 train_time:142081ms step_avg:62.18ms
step:2286/2330 train_time:142147ms step_avg:62.18ms
step:2287/2330 train_time:142209ms step_avg:62.18ms
step:2288/2330 train_time:142273ms step_avg:62.18ms
step:2289/2330 train_time:142334ms step_avg:62.18ms
step:2290/2330 train_time:142398ms step_avg:62.18ms
step:2291/2330 train_time:142459ms step_avg:62.18ms
step:2292/2330 train_time:142522ms step_avg:62.18ms
step:2293/2330 train_time:142583ms step_avg:62.18ms
step:2294/2330 train_time:142646ms step_avg:62.18ms
step:2295/2330 train_time:142707ms step_avg:62.18ms
step:2296/2330 train_time:142771ms step_avg:62.18ms
step:2297/2330 train_time:142832ms step_avg:62.18ms
step:2298/2330 train_time:142896ms step_avg:62.18ms
step:2299/2330 train_time:142958ms step_avg:62.18ms
step:2300/2330 train_time:143023ms step_avg:62.18ms
step:2301/2330 train_time:143086ms step_avg:62.18ms
step:2302/2330 train_time:143151ms step_avg:62.19ms
step:2303/2330 train_time:143212ms step_avg:62.19ms
step:2304/2330 train_time:143277ms step_avg:62.19ms
step:2305/2330 train_time:143338ms step_avg:62.19ms
step:2306/2330 train_time:143402ms step_avg:62.19ms
step:2307/2330 train_time:143463ms step_avg:62.19ms
step:2308/2330 train_time:143527ms step_avg:62.19ms
step:2309/2330 train_time:143587ms step_avg:62.19ms
step:2310/2330 train_time:143651ms step_avg:62.19ms
step:2311/2330 train_time:143713ms step_avg:62.19ms
step:2312/2330 train_time:143776ms step_avg:62.19ms
step:2313/2330 train_time:143837ms step_avg:62.19ms
step:2314/2330 train_time:143901ms step_avg:62.19ms
step:2315/2330 train_time:143962ms step_avg:62.19ms
step:2316/2330 train_time:144027ms step_avg:62.19ms
step:2317/2330 train_time:144088ms step_avg:62.19ms
step:2318/2330 train_time:144153ms step_avg:62.19ms
step:2319/2330 train_time:144214ms step_avg:62.19ms
step:2320/2330 train_time:144279ms step_avg:62.19ms
step:2321/2330 train_time:144339ms step_avg:62.19ms
step:2322/2330 train_time:144404ms step_avg:62.19ms
step:2323/2330 train_time:144465ms step_avg:62.19ms
step:2324/2330 train_time:144530ms step_avg:62.19ms
step:2325/2330 train_time:144591ms step_avg:62.19ms
step:2326/2330 train_time:144655ms step_avg:62.19ms
step:2327/2330 train_time:144715ms step_avg:62.19ms
step:2328/2330 train_time:144779ms step_avg:62.19ms
step:2329/2330 train_time:144840ms step_avg:62.19ms
step:2330/2330 train_time:144905ms step_avg:62.19ms
step:2330/2330 val_loss:3.5495 train_time:144971ms step_avg:62.22ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
