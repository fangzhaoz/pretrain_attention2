import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr7e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:19:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:61ms step_avg:61.10ms
step:2/2330 train_time:153ms step_avg:76.29ms
step:3/2330 train_time:170ms step_avg:56.68ms
step:4/2330 train_time:182ms step_avg:45.58ms
step:5/2330 train_time:193ms step_avg:38.59ms
step:6/2330 train_time:285ms step_avg:47.50ms
step:7/2330 train_time:306ms step_avg:43.72ms
step:8/2330 train_time:360ms step_avg:45.02ms
step:9/2330 train_time:382ms step_avg:42.44ms
step:10/2330 train_time:438ms step_avg:43.78ms
step:11/2330 train_time:460ms step_avg:41.80ms
step:12/2330 train_time:516ms step_avg:42.99ms
step:13/2330 train_time:538ms step_avg:41.37ms
step:14/2330 train_time:594ms step_avg:42.41ms
step:15/2330 train_time:615ms step_avg:41.03ms
step:16/2330 train_time:671ms step_avg:41.95ms
step:17/2330 train_time:693ms step_avg:40.75ms
step:18/2330 train_time:748ms step_avg:41.58ms
step:19/2330 train_time:770ms step_avg:40.54ms
step:20/2330 train_time:826ms step_avg:41.30ms
step:21/2330 train_time:848ms step_avg:40.37ms
step:22/2330 train_time:903ms step_avg:41.04ms
step:23/2330 train_time:925ms step_avg:40.22ms
step:24/2330 train_time:980ms step_avg:40.85ms
step:25/2330 train_time:1003ms step_avg:40.11ms
step:26/2330 train_time:1058ms step_avg:40.71ms
step:27/2330 train_time:1080ms step_avg:40.01ms
step:28/2330 train_time:1138ms step_avg:40.64ms
step:29/2330 train_time:1165ms step_avg:40.16ms
step:30/2330 train_time:1225ms step_avg:40.84ms
step:31/2330 train_time:1250ms step_avg:40.31ms
step:32/2330 train_time:1308ms step_avg:40.88ms
step:33/2330 train_time:1331ms step_avg:40.35ms
step:34/2330 train_time:1389ms step_avg:40.84ms
step:35/2330 train_time:1411ms step_avg:40.33ms
step:36/2330 train_time:1468ms step_avg:40.77ms
step:37/2330 train_time:1490ms step_avg:40.27ms
step:38/2330 train_time:1546ms step_avg:40.70ms
step:39/2330 train_time:1569ms step_avg:40.22ms
step:40/2330 train_time:1624ms step_avg:40.60ms
step:41/2330 train_time:1647ms step_avg:40.16ms
step:42/2330 train_time:1702ms step_avg:40.53ms
step:43/2330 train_time:1725ms step_avg:40.13ms
step:44/2330 train_time:1781ms step_avg:40.47ms
step:45/2330 train_time:1803ms step_avg:40.07ms
step:46/2330 train_time:1859ms step_avg:40.40ms
step:47/2330 train_time:1881ms step_avg:40.01ms
step:48/2330 train_time:1936ms step_avg:40.34ms
step:49/2330 train_time:1959ms step_avg:39.98ms
step:50/2330 train_time:2014ms step_avg:40.29ms
step:51/2330 train_time:2036ms step_avg:39.93ms
step:52/2330 train_time:2094ms step_avg:40.27ms
step:53/2330 train_time:2117ms step_avg:39.93ms
step:54/2330 train_time:2175ms step_avg:40.27ms
step:55/2330 train_time:2198ms step_avg:39.97ms
step:56/2330 train_time:2258ms step_avg:40.32ms
step:57/2330 train_time:2283ms step_avg:40.04ms
step:58/2330 train_time:2340ms step_avg:40.34ms
step:59/2330 train_time:2363ms step_avg:40.06ms
step:60/2330 train_time:2420ms step_avg:40.33ms
step:61/2330 train_time:2443ms step_avg:40.05ms
step:62/2330 train_time:2499ms step_avg:40.30ms
step:63/2330 train_time:2522ms step_avg:40.04ms
step:64/2330 train_time:2578ms step_avg:40.29ms
step:65/2330 train_time:2601ms step_avg:40.01ms
step:66/2330 train_time:2657ms step_avg:40.25ms
step:67/2330 train_time:2680ms step_avg:40.00ms
step:68/2330 train_time:2736ms step_avg:40.24ms
step:69/2330 train_time:2758ms step_avg:39.98ms
step:70/2330 train_time:2814ms step_avg:40.20ms
step:71/2330 train_time:2836ms step_avg:39.95ms
step:72/2330 train_time:2892ms step_avg:40.17ms
step:73/2330 train_time:2914ms step_avg:39.92ms
step:74/2330 train_time:2970ms step_avg:40.14ms
step:75/2330 train_time:2992ms step_avg:39.89ms
step:76/2330 train_time:3048ms step_avg:40.11ms
step:77/2330 train_time:3070ms step_avg:39.87ms
step:78/2330 train_time:3127ms step_avg:40.09ms
step:79/2330 train_time:3149ms step_avg:39.86ms
step:80/2330 train_time:3207ms step_avg:40.09ms
step:81/2330 train_time:3229ms step_avg:39.87ms
step:82/2330 train_time:3287ms step_avg:40.08ms
step:83/2330 train_time:3309ms step_avg:39.87ms
step:84/2330 train_time:3367ms step_avg:40.08ms
step:85/2330 train_time:3389ms step_avg:39.87ms
step:86/2330 train_time:3446ms step_avg:40.07ms
step:87/2330 train_time:3469ms step_avg:39.87ms
step:88/2330 train_time:3525ms step_avg:40.06ms
step:89/2330 train_time:3548ms step_avg:39.86ms
step:90/2330 train_time:3603ms step_avg:40.04ms
step:91/2330 train_time:3626ms step_avg:39.85ms
step:92/2330 train_time:3683ms step_avg:40.03ms
step:93/2330 train_time:3706ms step_avg:39.84ms
step:94/2330 train_time:3761ms step_avg:40.02ms
step:95/2330 train_time:3784ms step_avg:39.83ms
step:96/2330 train_time:3841ms step_avg:40.01ms
step:97/2330 train_time:3863ms step_avg:39.83ms
step:98/2330 train_time:3919ms step_avg:39.99ms
step:99/2330 train_time:3942ms step_avg:39.81ms
step:100/2330 train_time:3997ms step_avg:39.97ms
step:101/2330 train_time:4020ms step_avg:39.81ms
step:102/2330 train_time:4077ms step_avg:39.97ms
step:103/2330 train_time:4100ms step_avg:39.80ms
step:104/2330 train_time:4157ms step_avg:39.97ms
step:105/2330 train_time:4181ms step_avg:39.82ms
step:106/2330 train_time:4238ms step_avg:39.98ms
step:107/2330 train_time:4261ms step_avg:39.82ms
step:108/2330 train_time:4317ms step_avg:39.98ms
step:109/2330 train_time:4341ms step_avg:39.82ms
step:110/2330 train_time:4398ms step_avg:39.99ms
step:111/2330 train_time:4422ms step_avg:39.84ms
step:112/2330 train_time:4478ms step_avg:39.99ms
step:113/2330 train_time:4501ms step_avg:39.83ms
step:114/2330 train_time:4557ms step_avg:39.98ms
step:115/2330 train_time:4580ms step_avg:39.83ms
step:116/2330 train_time:4637ms step_avg:39.97ms
step:117/2330 train_time:4659ms step_avg:39.82ms
step:118/2330 train_time:4715ms step_avg:39.96ms
step:119/2330 train_time:4737ms step_avg:39.81ms
step:120/2330 train_time:4794ms step_avg:39.95ms
step:121/2330 train_time:4816ms step_avg:39.80ms
step:122/2330 train_time:4873ms step_avg:39.94ms
step:123/2330 train_time:4895ms step_avg:39.80ms
step:124/2330 train_time:4951ms step_avg:39.93ms
step:125/2330 train_time:4974ms step_avg:39.79ms
step:126/2330 train_time:5030ms step_avg:39.92ms
step:127/2330 train_time:5052ms step_avg:39.78ms
step:128/2330 train_time:5109ms step_avg:39.91ms
step:129/2330 train_time:5131ms step_avg:39.78ms
step:130/2330 train_time:5188ms step_avg:39.91ms
step:131/2330 train_time:5210ms step_avg:39.77ms
step:132/2330 train_time:5267ms step_avg:39.90ms
step:133/2330 train_time:5289ms step_avg:39.77ms
step:134/2330 train_time:5347ms step_avg:39.90ms
step:135/2330 train_time:5369ms step_avg:39.77ms
step:136/2330 train_time:5425ms step_avg:39.89ms
step:137/2330 train_time:5447ms step_avg:39.76ms
step:138/2330 train_time:5503ms step_avg:39.88ms
step:139/2330 train_time:5526ms step_avg:39.76ms
step:140/2330 train_time:5583ms step_avg:39.88ms
step:141/2330 train_time:5606ms step_avg:39.76ms
step:142/2330 train_time:5663ms step_avg:39.88ms
step:143/2330 train_time:5687ms step_avg:39.77ms
step:144/2330 train_time:5743ms step_avg:39.88ms
step:145/2330 train_time:5766ms step_avg:39.77ms
step:146/2330 train_time:5822ms step_avg:39.87ms
step:147/2330 train_time:5845ms step_avg:39.76ms
step:148/2330 train_time:5901ms step_avg:39.87ms
step:149/2330 train_time:5924ms step_avg:39.76ms
step:150/2330 train_time:5980ms step_avg:39.87ms
step:151/2330 train_time:6003ms step_avg:39.75ms
step:152/2330 train_time:6059ms step_avg:39.86ms
step:153/2330 train_time:6082ms step_avg:39.75ms
step:154/2330 train_time:6139ms step_avg:39.86ms
step:155/2330 train_time:6162ms step_avg:39.76ms
step:156/2330 train_time:6219ms step_avg:39.87ms
step:157/2330 train_time:6241ms step_avg:39.75ms
step:158/2330 train_time:6298ms step_avg:39.86ms
step:159/2330 train_time:6321ms step_avg:39.75ms
step:160/2330 train_time:6378ms step_avg:39.86ms
step:161/2330 train_time:6401ms step_avg:39.76ms
step:162/2330 train_time:6457ms step_avg:39.86ms
step:163/2330 train_time:6481ms step_avg:39.76ms
step:164/2330 train_time:6537ms step_avg:39.86ms
step:165/2330 train_time:6560ms step_avg:39.76ms
step:166/2330 train_time:6617ms step_avg:39.86ms
step:167/2330 train_time:6640ms step_avg:39.76ms
step:168/2330 train_time:6698ms step_avg:39.87ms
step:169/2330 train_time:6720ms step_avg:39.76ms
step:170/2330 train_time:6777ms step_avg:39.86ms
step:171/2330 train_time:6799ms step_avg:39.76ms
step:172/2330 train_time:6856ms step_avg:39.86ms
step:173/2330 train_time:6878ms step_avg:39.76ms
step:174/2330 train_time:6935ms step_avg:39.85ms
step:175/2330 train_time:6957ms step_avg:39.75ms
step:176/2330 train_time:7014ms step_avg:39.85ms
step:177/2330 train_time:7036ms step_avg:39.75ms
step:178/2330 train_time:7093ms step_avg:39.85ms
step:179/2330 train_time:7115ms step_avg:39.75ms
step:180/2330 train_time:7173ms step_avg:39.85ms
step:181/2330 train_time:7195ms step_avg:39.75ms
step:182/2330 train_time:7252ms step_avg:39.85ms
step:183/2330 train_time:7274ms step_avg:39.75ms
step:184/2330 train_time:7331ms step_avg:39.84ms
step:185/2330 train_time:7354ms step_avg:39.75ms
step:186/2330 train_time:7410ms step_avg:39.84ms
step:187/2330 train_time:7433ms step_avg:39.75ms
step:188/2330 train_time:7490ms step_avg:39.84ms
step:189/2330 train_time:7511ms step_avg:39.74ms
step:190/2330 train_time:7568ms step_avg:39.83ms
step:191/2330 train_time:7591ms step_avg:39.74ms
step:192/2330 train_time:7648ms step_avg:39.83ms
step:193/2330 train_time:7670ms step_avg:39.74ms
step:194/2330 train_time:7727ms step_avg:39.83ms
step:195/2330 train_time:7748ms step_avg:39.74ms
step:196/2330 train_time:7805ms step_avg:39.82ms
step:197/2330 train_time:7827ms step_avg:39.73ms
step:198/2330 train_time:7883ms step_avg:39.81ms
step:199/2330 train_time:7907ms step_avg:39.73ms
step:200/2330 train_time:7963ms step_avg:39.81ms
step:201/2330 train_time:7986ms step_avg:39.73ms
step:202/2330 train_time:8042ms step_avg:39.81ms
step:203/2330 train_time:8066ms step_avg:39.73ms
step:204/2330 train_time:8122ms step_avg:39.81ms
step:205/2330 train_time:8145ms step_avg:39.73ms
step:206/2330 train_time:8201ms step_avg:39.81ms
step:207/2330 train_time:8223ms step_avg:39.73ms
step:208/2330 train_time:8279ms step_avg:39.80ms
step:209/2330 train_time:8302ms step_avg:39.72ms
step:210/2330 train_time:8359ms step_avg:39.80ms
step:211/2330 train_time:8382ms step_avg:39.72ms
step:212/2330 train_time:8438ms step_avg:39.80ms
step:213/2330 train_time:8462ms step_avg:39.73ms
step:214/2330 train_time:8518ms step_avg:39.80ms
step:215/2330 train_time:8541ms step_avg:39.72ms
step:216/2330 train_time:8598ms step_avg:39.80ms
step:217/2330 train_time:8621ms step_avg:39.73ms
step:218/2330 train_time:8678ms step_avg:39.81ms
step:219/2330 train_time:8701ms step_avg:39.73ms
step:220/2330 train_time:8758ms step_avg:39.81ms
step:221/2330 train_time:8780ms step_avg:39.73ms
step:222/2330 train_time:8837ms step_avg:39.81ms
step:223/2330 train_time:8859ms step_avg:39.73ms
step:224/2330 train_time:8915ms step_avg:39.80ms
step:225/2330 train_time:8938ms step_avg:39.72ms
step:226/2330 train_time:8995ms step_avg:39.80ms
step:227/2330 train_time:9016ms step_avg:39.72ms
step:228/2330 train_time:9074ms step_avg:39.80ms
step:229/2330 train_time:9096ms step_avg:39.72ms
step:230/2330 train_time:9152ms step_avg:39.79ms
step:231/2330 train_time:9175ms step_avg:39.72ms
step:232/2330 train_time:9231ms step_avg:39.79ms
step:233/2330 train_time:9253ms step_avg:39.71ms
step:234/2330 train_time:9310ms step_avg:39.79ms
step:235/2330 train_time:9332ms step_avg:39.71ms
step:236/2330 train_time:9389ms step_avg:39.78ms
step:237/2330 train_time:9412ms step_avg:39.71ms
step:238/2330 train_time:9468ms step_avg:39.78ms
step:239/2330 train_time:9490ms step_avg:39.71ms
step:240/2330 train_time:9546ms step_avg:39.78ms
step:241/2330 train_time:9569ms step_avg:39.70ms
step:242/2330 train_time:9625ms step_avg:39.77ms
step:243/2330 train_time:9647ms step_avg:39.70ms
step:244/2330 train_time:9703ms step_avg:39.76ms
step:245/2330 train_time:9726ms step_avg:39.70ms
step:246/2330 train_time:9782ms step_avg:39.76ms
step:247/2330 train_time:9806ms step_avg:39.70ms
step:248/2330 train_time:9862ms step_avg:39.77ms
step:249/2330 train_time:9885ms step_avg:39.70ms
step:250/2330 train_time:9940ms step_avg:39.76ms
step:250/2330 val_loss:5.5223 train_time:10036ms step_avg:40.14ms
step:251/2330 train_time:10050ms step_avg:40.04ms
step:252/2330 train_time:10062ms step_avg:39.93ms
step:253/2330 train_time:10073ms step_avg:39.81ms
step:254/2330 train_time:10101ms step_avg:39.77ms
step:255/2330 train_time:10123ms step_avg:39.70ms
step:256/2330 train_time:10179ms step_avg:39.76ms
step:257/2330 train_time:10200ms step_avg:39.69ms
step:258/2330 train_time:10256ms step_avg:39.75ms
step:259/2330 train_time:10278ms step_avg:39.68ms
step:260/2330 train_time:10335ms step_avg:39.75ms
step:261/2330 train_time:10358ms step_avg:39.69ms
step:262/2330 train_time:10419ms step_avg:39.77ms
step:263/2330 train_time:10446ms step_avg:39.72ms
step:264/2330 train_time:10504ms step_avg:39.79ms
step:265/2330 train_time:10528ms step_avg:39.73ms
step:266/2330 train_time:10584ms step_avg:39.79ms
step:267/2330 train_time:10606ms step_avg:39.72ms
step:268/2330 train_time:10662ms step_avg:39.78ms
step:269/2330 train_time:10685ms step_avg:39.72ms
step:270/2330 train_time:10740ms step_avg:39.78ms
step:271/2330 train_time:10764ms step_avg:39.72ms
step:272/2330 train_time:10819ms step_avg:39.78ms
step:273/2330 train_time:10841ms step_avg:39.71ms
step:274/2330 train_time:10898ms step_avg:39.77ms
step:275/2330 train_time:10919ms step_avg:39.71ms
step:276/2330 train_time:10977ms step_avg:39.77ms
step:277/2330 train_time:10999ms step_avg:39.71ms
step:278/2330 train_time:11057ms step_avg:39.77ms
step:279/2330 train_time:11079ms step_avg:39.71ms
step:280/2330 train_time:11135ms step_avg:39.77ms
step:281/2330 train_time:11157ms step_avg:39.70ms
step:282/2330 train_time:11213ms step_avg:39.76ms
step:283/2330 train_time:11235ms step_avg:39.70ms
step:284/2330 train_time:11291ms step_avg:39.76ms
step:285/2330 train_time:11314ms step_avg:39.70ms
step:286/2330 train_time:11372ms step_avg:39.76ms
step:287/2330 train_time:11396ms step_avg:39.71ms
step:288/2330 train_time:11455ms step_avg:39.77ms
step:289/2330 train_time:11478ms step_avg:39.72ms
step:290/2330 train_time:11535ms step_avg:39.78ms
step:291/2330 train_time:11558ms step_avg:39.72ms
step:292/2330 train_time:11615ms step_avg:39.78ms
step:293/2330 train_time:11637ms step_avg:39.72ms
step:294/2330 train_time:11694ms step_avg:39.77ms
step:295/2330 train_time:11716ms step_avg:39.71ms
step:296/2330 train_time:11772ms step_avg:39.77ms
step:297/2330 train_time:11794ms step_avg:39.71ms
step:298/2330 train_time:11850ms step_avg:39.77ms
step:299/2330 train_time:11872ms step_avg:39.71ms
step:300/2330 train_time:11928ms step_avg:39.76ms
step:301/2330 train_time:11951ms step_avg:39.70ms
step:302/2330 train_time:12007ms step_avg:39.76ms
step:303/2330 train_time:12029ms step_avg:39.70ms
step:304/2330 train_time:12085ms step_avg:39.75ms
step:305/2330 train_time:12108ms step_avg:39.70ms
step:306/2330 train_time:12163ms step_avg:39.75ms
step:307/2330 train_time:12186ms step_avg:39.69ms
step:308/2330 train_time:12242ms step_avg:39.75ms
step:309/2330 train_time:12265ms step_avg:39.69ms
step:310/2330 train_time:12322ms step_avg:39.75ms
step:311/2330 train_time:12344ms step_avg:39.69ms
step:312/2330 train_time:12402ms step_avg:39.75ms
step:313/2330 train_time:12425ms step_avg:39.70ms
step:314/2330 train_time:12482ms step_avg:39.75ms
step:315/2330 train_time:12505ms step_avg:39.70ms
step:316/2330 train_time:12562ms step_avg:39.75ms
step:317/2330 train_time:12584ms step_avg:39.70ms
step:318/2330 train_time:12641ms step_avg:39.75ms
step:319/2330 train_time:12663ms step_avg:39.70ms
step:320/2330 train_time:12719ms step_avg:39.75ms
step:321/2330 train_time:12742ms step_avg:39.70ms
step:322/2330 train_time:12799ms step_avg:39.75ms
step:323/2330 train_time:12822ms step_avg:39.70ms
step:324/2330 train_time:12879ms step_avg:39.75ms
step:325/2330 train_time:12902ms step_avg:39.70ms
step:326/2330 train_time:12958ms step_avg:39.75ms
step:327/2330 train_time:12981ms step_avg:39.70ms
step:328/2330 train_time:13037ms step_avg:39.75ms
step:329/2330 train_time:13059ms step_avg:39.69ms
step:330/2330 train_time:13115ms step_avg:39.74ms
step:331/2330 train_time:13137ms step_avg:39.69ms
step:332/2330 train_time:13194ms step_avg:39.74ms
step:333/2330 train_time:13216ms step_avg:39.69ms
step:334/2330 train_time:13273ms step_avg:39.74ms
step:335/2330 train_time:13295ms step_avg:39.69ms
step:336/2330 train_time:13352ms step_avg:39.74ms
step:337/2330 train_time:13375ms step_avg:39.69ms
step:338/2330 train_time:13432ms step_avg:39.74ms
step:339/2330 train_time:13454ms step_avg:39.69ms
step:340/2330 train_time:13511ms step_avg:39.74ms
step:341/2330 train_time:13534ms step_avg:39.69ms
step:342/2330 train_time:13590ms step_avg:39.74ms
step:343/2330 train_time:13613ms step_avg:39.69ms
step:344/2330 train_time:13669ms step_avg:39.74ms
step:345/2330 train_time:13691ms step_avg:39.68ms
step:346/2330 train_time:13748ms step_avg:39.73ms
step:347/2330 train_time:13770ms step_avg:39.68ms
step:348/2330 train_time:13826ms step_avg:39.73ms
step:349/2330 train_time:13849ms step_avg:39.68ms
step:350/2330 train_time:13905ms step_avg:39.73ms
step:351/2330 train_time:13928ms step_avg:39.68ms
step:352/2330 train_time:13985ms step_avg:39.73ms
step:353/2330 train_time:14008ms step_avg:39.68ms
step:354/2330 train_time:14064ms step_avg:39.73ms
step:355/2330 train_time:14086ms step_avg:39.68ms
step:356/2330 train_time:14142ms step_avg:39.72ms
step:357/2330 train_time:14165ms step_avg:39.68ms
step:358/2330 train_time:14221ms step_avg:39.72ms
step:359/2330 train_time:14243ms step_avg:39.68ms
step:360/2330 train_time:14300ms step_avg:39.72ms
step:361/2330 train_time:14323ms step_avg:39.68ms
step:362/2330 train_time:14380ms step_avg:39.72ms
step:363/2330 train_time:14403ms step_avg:39.68ms
step:364/2330 train_time:14460ms step_avg:39.73ms
step:365/2330 train_time:14483ms step_avg:39.68ms
step:366/2330 train_time:14540ms step_avg:39.73ms
step:367/2330 train_time:14563ms step_avg:39.68ms
step:368/2330 train_time:14619ms step_avg:39.73ms
step:369/2330 train_time:14641ms step_avg:39.68ms
step:370/2330 train_time:14699ms step_avg:39.73ms
step:371/2330 train_time:14721ms step_avg:39.68ms
step:372/2330 train_time:14778ms step_avg:39.73ms
step:373/2330 train_time:14801ms step_avg:39.68ms
step:374/2330 train_time:14858ms step_avg:39.73ms
step:375/2330 train_time:14880ms step_avg:39.68ms
step:376/2330 train_time:14936ms step_avg:39.72ms
step:377/2330 train_time:14959ms step_avg:39.68ms
step:378/2330 train_time:15016ms step_avg:39.73ms
step:379/2330 train_time:15038ms step_avg:39.68ms
step:380/2330 train_time:15095ms step_avg:39.72ms
step:381/2330 train_time:15117ms step_avg:39.68ms
step:382/2330 train_time:15173ms step_avg:39.72ms
step:383/2330 train_time:15195ms step_avg:39.67ms
step:384/2330 train_time:15252ms step_avg:39.72ms
step:385/2330 train_time:15274ms step_avg:39.67ms
step:386/2330 train_time:15331ms step_avg:39.72ms
step:387/2330 train_time:15353ms step_avg:39.67ms
step:388/2330 train_time:15410ms step_avg:39.72ms
step:389/2330 train_time:15432ms step_avg:39.67ms
step:390/2330 train_time:15489ms step_avg:39.71ms
step:391/2330 train_time:15511ms step_avg:39.67ms
step:392/2330 train_time:15567ms step_avg:39.71ms
step:393/2330 train_time:15590ms step_avg:39.67ms
step:394/2330 train_time:15646ms step_avg:39.71ms
step:395/2330 train_time:15669ms step_avg:39.67ms
step:396/2330 train_time:15726ms step_avg:39.71ms
step:397/2330 train_time:15749ms step_avg:39.67ms
step:398/2330 train_time:15806ms step_avg:39.71ms
step:399/2330 train_time:15830ms step_avg:39.67ms
step:400/2330 train_time:15886ms step_avg:39.72ms
step:401/2330 train_time:15909ms step_avg:39.67ms
step:402/2330 train_time:15965ms step_avg:39.71ms
step:403/2330 train_time:15988ms step_avg:39.67ms
step:404/2330 train_time:16044ms step_avg:39.71ms
step:405/2330 train_time:16067ms step_avg:39.67ms
step:406/2330 train_time:16124ms step_avg:39.71ms
step:407/2330 train_time:16146ms step_avg:39.67ms
step:408/2330 train_time:16202ms step_avg:39.71ms
step:409/2330 train_time:16225ms step_avg:39.67ms
step:410/2330 train_time:16282ms step_avg:39.71ms
step:411/2330 train_time:16304ms step_avg:39.67ms
step:412/2330 train_time:16360ms step_avg:39.71ms
step:413/2330 train_time:16383ms step_avg:39.67ms
step:414/2330 train_time:16440ms step_avg:39.71ms
step:415/2330 train_time:16463ms step_avg:39.67ms
step:416/2330 train_time:16520ms step_avg:39.71ms
step:417/2330 train_time:16542ms step_avg:39.67ms
step:418/2330 train_time:16599ms step_avg:39.71ms
step:419/2330 train_time:16621ms step_avg:39.67ms
step:420/2330 train_time:16677ms step_avg:39.71ms
step:421/2330 train_time:16700ms step_avg:39.67ms
step:422/2330 train_time:16758ms step_avg:39.71ms
step:423/2330 train_time:16780ms step_avg:39.67ms
step:424/2330 train_time:16837ms step_avg:39.71ms
step:425/2330 train_time:16860ms step_avg:39.67ms
step:426/2330 train_time:16917ms step_avg:39.71ms
step:427/2330 train_time:16939ms step_avg:39.67ms
step:428/2330 train_time:16996ms step_avg:39.71ms
step:429/2330 train_time:17018ms step_avg:39.67ms
step:430/2330 train_time:17076ms step_avg:39.71ms
step:431/2330 train_time:17098ms step_avg:39.67ms
step:432/2330 train_time:17155ms step_avg:39.71ms
step:433/2330 train_time:17177ms step_avg:39.67ms
step:434/2330 train_time:17233ms step_avg:39.71ms
step:435/2330 train_time:17255ms step_avg:39.67ms
step:436/2330 train_time:17312ms step_avg:39.71ms
step:437/2330 train_time:17334ms step_avg:39.67ms
step:438/2330 train_time:17390ms step_avg:39.70ms
step:439/2330 train_time:17412ms step_avg:39.66ms
step:440/2330 train_time:17469ms step_avg:39.70ms
step:441/2330 train_time:17490ms step_avg:39.66ms
step:442/2330 train_time:17546ms step_avg:39.70ms
step:443/2330 train_time:17570ms step_avg:39.66ms
step:444/2330 train_time:17625ms step_avg:39.70ms
step:445/2330 train_time:17648ms step_avg:39.66ms
step:446/2330 train_time:17704ms step_avg:39.69ms
step:447/2330 train_time:17727ms step_avg:39.66ms
step:448/2330 train_time:17784ms step_avg:39.70ms
step:449/2330 train_time:17807ms step_avg:39.66ms
step:450/2330 train_time:17864ms step_avg:39.70ms
step:451/2330 train_time:17887ms step_avg:39.66ms
step:452/2330 train_time:17944ms step_avg:39.70ms
step:453/2330 train_time:17967ms step_avg:39.66ms
step:454/2330 train_time:18024ms step_avg:39.70ms
step:455/2330 train_time:18047ms step_avg:39.66ms
step:456/2330 train_time:18103ms step_avg:39.70ms
step:457/2330 train_time:18125ms step_avg:39.66ms
step:458/2330 train_time:18181ms step_avg:39.70ms
step:459/2330 train_time:18204ms step_avg:39.66ms
step:460/2330 train_time:18261ms step_avg:39.70ms
step:461/2330 train_time:18283ms step_avg:39.66ms
step:462/2330 train_time:18340ms step_avg:39.70ms
step:463/2330 train_time:18363ms step_avg:39.66ms
step:464/2330 train_time:18419ms step_avg:39.70ms
step:465/2330 train_time:18442ms step_avg:39.66ms
step:466/2330 train_time:18498ms step_avg:39.70ms
step:467/2330 train_time:18521ms step_avg:39.66ms
step:468/2330 train_time:18577ms step_avg:39.69ms
step:469/2330 train_time:18599ms step_avg:39.66ms
step:470/2330 train_time:18656ms step_avg:39.69ms
step:471/2330 train_time:18679ms step_avg:39.66ms
step:472/2330 train_time:18735ms step_avg:39.69ms
step:473/2330 train_time:18758ms step_avg:39.66ms
step:474/2330 train_time:18815ms step_avg:39.69ms
step:475/2330 train_time:18837ms step_avg:39.66ms
step:476/2330 train_time:18895ms step_avg:39.69ms
step:477/2330 train_time:18917ms step_avg:39.66ms
step:478/2330 train_time:18975ms step_avg:39.70ms
step:479/2330 train_time:18997ms step_avg:39.66ms
step:480/2330 train_time:19054ms step_avg:39.70ms
step:481/2330 train_time:19076ms step_avg:39.66ms
step:482/2330 train_time:19132ms step_avg:39.69ms
step:483/2330 train_time:19155ms step_avg:39.66ms
step:484/2330 train_time:19211ms step_avg:39.69ms
step:485/2330 train_time:19233ms step_avg:39.66ms
step:486/2330 train_time:19290ms step_avg:39.69ms
step:487/2330 train_time:19312ms step_avg:39.65ms
step:488/2330 train_time:19367ms step_avg:39.69ms
step:489/2330 train_time:19390ms step_avg:39.65ms
step:490/2330 train_time:19446ms step_avg:39.69ms
step:491/2330 train_time:19469ms step_avg:39.65ms
step:492/2330 train_time:19525ms step_avg:39.69ms
step:493/2330 train_time:19549ms step_avg:39.65ms
step:494/2330 train_time:19604ms step_avg:39.69ms
step:495/2330 train_time:19627ms step_avg:39.65ms
step:496/2330 train_time:19684ms step_avg:39.69ms
step:497/2330 train_time:19707ms step_avg:39.65ms
step:498/2330 train_time:19763ms step_avg:39.68ms
step:499/2330 train_time:19786ms step_avg:39.65ms
step:500/2330 train_time:19842ms step_avg:39.68ms
step:500/2330 val_loss:5.3516 train_time:19939ms step_avg:39.88ms
step:501/2330 train_time:19951ms step_avg:39.82ms
step:502/2330 train_time:19963ms step_avg:39.77ms
step:503/2330 train_time:19972ms step_avg:39.71ms
step:504/2330 train_time:20001ms step_avg:39.69ms
step:505/2330 train_time:20023ms step_avg:39.65ms
step:506/2330 train_time:20078ms step_avg:39.68ms
step:507/2330 train_time:20100ms step_avg:39.64ms
step:508/2330 train_time:20155ms step_avg:39.68ms
step:509/2330 train_time:20177ms step_avg:39.64ms
step:510/2330 train_time:20234ms step_avg:39.68ms
step:511/2330 train_time:20259ms step_avg:39.65ms
step:512/2330 train_time:20321ms step_avg:39.69ms
step:513/2330 train_time:20345ms step_avg:39.66ms
step:514/2330 train_time:20403ms step_avg:39.69ms
step:515/2330 train_time:20426ms step_avg:39.66ms
step:516/2330 train_time:20483ms step_avg:39.69ms
step:517/2330 train_time:20505ms step_avg:39.66ms
step:518/2330 train_time:20561ms step_avg:39.69ms
step:519/2330 train_time:20584ms step_avg:39.66ms
step:520/2330 train_time:20640ms step_avg:39.69ms
step:521/2330 train_time:20662ms step_avg:39.66ms
step:522/2330 train_time:20718ms step_avg:39.69ms
step:523/2330 train_time:20740ms step_avg:39.66ms
step:524/2330 train_time:20796ms step_avg:39.69ms
step:525/2330 train_time:20817ms step_avg:39.65ms
step:526/2330 train_time:20874ms step_avg:39.68ms
step:527/2330 train_time:20896ms step_avg:39.65ms
step:528/2330 train_time:20952ms step_avg:39.68ms
step:529/2330 train_time:20974ms step_avg:39.65ms
step:530/2330 train_time:21029ms step_avg:39.68ms
step:531/2330 train_time:21052ms step_avg:39.65ms
step:532/2330 train_time:21108ms step_avg:39.68ms
step:533/2330 train_time:21130ms step_avg:39.64ms
step:534/2330 train_time:21187ms step_avg:39.68ms
step:535/2330 train_time:21210ms step_avg:39.65ms
step:536/2330 train_time:21267ms step_avg:39.68ms
step:537/2330 train_time:21290ms step_avg:39.65ms
step:538/2330 train_time:21348ms step_avg:39.68ms
step:539/2330 train_time:21372ms step_avg:39.65ms
step:540/2330 train_time:21429ms step_avg:39.68ms
step:541/2330 train_time:21453ms step_avg:39.65ms
step:542/2330 train_time:21510ms step_avg:39.69ms
step:543/2330 train_time:21533ms step_avg:39.65ms
step:544/2330 train_time:21589ms step_avg:39.69ms
step:545/2330 train_time:21612ms step_avg:39.65ms
step:546/2330 train_time:21668ms step_avg:39.68ms
step:547/2330 train_time:21690ms step_avg:39.65ms
step:548/2330 train_time:21746ms step_avg:39.68ms
step:549/2330 train_time:21769ms step_avg:39.65ms
step:550/2330 train_time:21825ms step_avg:39.68ms
step:551/2330 train_time:21848ms step_avg:39.65ms
step:552/2330 train_time:21904ms step_avg:39.68ms
step:553/2330 train_time:21926ms step_avg:39.65ms
step:554/2330 train_time:21982ms step_avg:39.68ms
step:555/2330 train_time:22005ms step_avg:39.65ms
step:556/2330 train_time:22061ms step_avg:39.68ms
step:557/2330 train_time:22083ms step_avg:39.65ms
step:558/2330 train_time:22140ms step_avg:39.68ms
step:559/2330 train_time:22162ms step_avg:39.65ms
step:560/2330 train_time:22219ms step_avg:39.68ms
step:561/2330 train_time:22241ms step_avg:39.65ms
step:562/2330 train_time:22299ms step_avg:39.68ms
step:563/2330 train_time:22322ms step_avg:39.65ms
step:564/2330 train_time:22381ms step_avg:39.68ms
step:565/2330 train_time:22403ms step_avg:39.65ms
step:566/2330 train_time:22461ms step_avg:39.68ms
step:567/2330 train_time:22484ms step_avg:39.65ms
step:568/2330 train_time:22541ms step_avg:39.69ms
step:569/2330 train_time:22563ms step_avg:39.65ms
step:570/2330 train_time:22620ms step_avg:39.68ms
step:571/2330 train_time:22642ms step_avg:39.65ms
step:572/2330 train_time:22698ms step_avg:39.68ms
step:573/2330 train_time:22721ms step_avg:39.65ms
step:574/2330 train_time:22777ms step_avg:39.68ms
step:575/2330 train_time:22799ms step_avg:39.65ms
step:576/2330 train_time:22856ms step_avg:39.68ms
step:577/2330 train_time:22878ms step_avg:39.65ms
step:578/2330 train_time:22933ms step_avg:39.68ms
step:579/2330 train_time:22955ms step_avg:39.65ms
step:580/2330 train_time:23011ms step_avg:39.67ms
step:581/2330 train_time:23033ms step_avg:39.64ms
step:582/2330 train_time:23088ms step_avg:39.67ms
step:583/2330 train_time:23112ms step_avg:39.64ms
step:584/2330 train_time:23168ms step_avg:39.67ms
step:585/2330 train_time:23191ms step_avg:39.64ms
step:586/2330 train_time:23248ms step_avg:39.67ms
step:587/2330 train_time:23271ms step_avg:39.64ms
step:588/2330 train_time:23329ms step_avg:39.67ms
step:589/2330 train_time:23352ms step_avg:39.65ms
step:590/2330 train_time:23409ms step_avg:39.68ms
step:591/2330 train_time:23433ms step_avg:39.65ms
step:592/2330 train_time:23490ms step_avg:39.68ms
step:593/2330 train_time:23513ms step_avg:39.65ms
step:594/2330 train_time:23570ms step_avg:39.68ms
step:595/2330 train_time:23593ms step_avg:39.65ms
step:596/2330 train_time:23649ms step_avg:39.68ms
step:597/2330 train_time:23672ms step_avg:39.65ms
step:598/2330 train_time:23728ms step_avg:39.68ms
step:599/2330 train_time:23751ms step_avg:39.65ms
step:600/2330 train_time:23807ms step_avg:39.68ms
step:601/2330 train_time:23829ms step_avg:39.65ms
step:602/2330 train_time:23885ms step_avg:39.68ms
step:603/2330 train_time:23908ms step_avg:39.65ms
step:604/2330 train_time:23964ms step_avg:39.68ms
step:605/2330 train_time:23987ms step_avg:39.65ms
step:606/2330 train_time:24043ms step_avg:39.67ms
step:607/2330 train_time:24065ms step_avg:39.65ms
step:608/2330 train_time:24122ms step_avg:39.67ms
step:609/2330 train_time:24144ms step_avg:39.64ms
step:610/2330 train_time:24201ms step_avg:39.67ms
step:611/2330 train_time:24223ms step_avg:39.65ms
step:612/2330 train_time:24280ms step_avg:39.67ms
step:613/2330 train_time:24302ms step_avg:39.64ms
step:614/2330 train_time:24359ms step_avg:39.67ms
step:615/2330 train_time:24382ms step_avg:39.65ms
step:616/2330 train_time:24440ms step_avg:39.68ms
step:617/2330 train_time:24462ms step_avg:39.65ms
step:618/2330 train_time:24520ms step_avg:39.68ms
step:619/2330 train_time:24542ms step_avg:39.65ms
step:620/2330 train_time:24599ms step_avg:39.68ms
step:621/2330 train_time:24622ms step_avg:39.65ms
step:622/2330 train_time:24679ms step_avg:39.68ms
step:623/2330 train_time:24701ms step_avg:39.65ms
step:624/2330 train_time:24758ms step_avg:39.68ms
step:625/2330 train_time:24780ms step_avg:39.65ms
step:626/2330 train_time:24837ms step_avg:39.68ms
step:627/2330 train_time:24859ms step_avg:39.65ms
step:628/2330 train_time:24916ms step_avg:39.67ms
step:629/2330 train_time:24937ms step_avg:39.65ms
step:630/2330 train_time:24993ms step_avg:39.67ms
step:631/2330 train_time:25015ms step_avg:39.64ms
step:632/2330 train_time:25071ms step_avg:39.67ms
step:633/2330 train_time:25094ms step_avg:39.64ms
step:634/2330 train_time:25149ms step_avg:39.67ms
step:635/2330 train_time:25173ms step_avg:39.64ms
step:636/2330 train_time:25229ms step_avg:39.67ms
step:637/2330 train_time:25253ms step_avg:39.64ms
step:638/2330 train_time:25309ms step_avg:39.67ms
step:639/2330 train_time:25333ms step_avg:39.64ms
step:640/2330 train_time:25389ms step_avg:39.67ms
step:641/2330 train_time:25412ms step_avg:39.64ms
step:642/2330 train_time:25469ms step_avg:39.67ms
step:643/2330 train_time:25492ms step_avg:39.65ms
step:644/2330 train_time:25548ms step_avg:39.67ms
step:645/2330 train_time:25571ms step_avg:39.64ms
step:646/2330 train_time:25627ms step_avg:39.67ms
step:647/2330 train_time:25650ms step_avg:39.64ms
step:648/2330 train_time:25706ms step_avg:39.67ms
step:649/2330 train_time:25729ms step_avg:39.64ms
step:650/2330 train_time:25786ms step_avg:39.67ms
step:651/2330 train_time:25809ms step_avg:39.64ms
step:652/2330 train_time:25865ms step_avg:39.67ms
step:653/2330 train_time:25887ms step_avg:39.64ms
step:654/2330 train_time:25944ms step_avg:39.67ms
step:655/2330 train_time:25966ms step_avg:39.64ms
step:656/2330 train_time:26022ms step_avg:39.67ms
step:657/2330 train_time:26045ms step_avg:39.64ms
step:658/2330 train_time:26101ms step_avg:39.67ms
step:659/2330 train_time:26123ms step_avg:39.64ms
step:660/2330 train_time:26180ms step_avg:39.67ms
step:661/2330 train_time:26202ms step_avg:39.64ms
step:662/2330 train_time:26260ms step_avg:39.67ms
step:663/2330 train_time:26282ms step_avg:39.64ms
step:664/2330 train_time:26339ms step_avg:39.67ms
step:665/2330 train_time:26361ms step_avg:39.64ms
step:666/2330 train_time:26418ms step_avg:39.67ms
step:667/2330 train_time:26441ms step_avg:39.64ms
step:668/2330 train_time:26499ms step_avg:39.67ms
step:669/2330 train_time:26521ms step_avg:39.64ms
step:670/2330 train_time:26578ms step_avg:39.67ms
step:671/2330 train_time:26600ms step_avg:39.64ms
step:672/2330 train_time:26657ms step_avg:39.67ms
step:673/2330 train_time:26679ms step_avg:39.64ms
step:674/2330 train_time:26736ms step_avg:39.67ms
step:675/2330 train_time:26758ms step_avg:39.64ms
step:676/2330 train_time:26815ms step_avg:39.67ms
step:677/2330 train_time:26837ms step_avg:39.64ms
step:678/2330 train_time:26893ms step_avg:39.67ms
step:679/2330 train_time:26916ms step_avg:39.64ms
step:680/2330 train_time:26972ms step_avg:39.66ms
step:681/2330 train_time:26994ms step_avg:39.64ms
step:682/2330 train_time:27050ms step_avg:39.66ms
step:683/2330 train_time:27073ms step_avg:39.64ms
step:684/2330 train_time:27129ms step_avg:39.66ms
step:685/2330 train_time:27152ms step_avg:39.64ms
step:686/2330 train_time:27208ms step_avg:39.66ms
step:687/2330 train_time:27231ms step_avg:39.64ms
step:688/2330 train_time:27287ms step_avg:39.66ms
step:689/2330 train_time:27310ms step_avg:39.64ms
step:690/2330 train_time:27366ms step_avg:39.66ms
step:691/2330 train_time:27389ms step_avg:39.64ms
step:692/2330 train_time:27446ms step_avg:39.66ms
step:693/2330 train_time:27468ms step_avg:39.64ms
step:694/2330 train_time:27525ms step_avg:39.66ms
step:695/2330 train_time:27547ms step_avg:39.64ms
step:696/2330 train_time:27604ms step_avg:39.66ms
step:697/2330 train_time:27627ms step_avg:39.64ms
step:698/2330 train_time:27684ms step_avg:39.66ms
step:699/2330 train_time:27708ms step_avg:39.64ms
step:700/2330 train_time:27764ms step_avg:39.66ms
step:701/2330 train_time:27787ms step_avg:39.64ms
step:702/2330 train_time:27844ms step_avg:39.66ms
step:703/2330 train_time:27866ms step_avg:39.64ms
step:704/2330 train_time:27923ms step_avg:39.66ms
step:705/2330 train_time:27946ms step_avg:39.64ms
step:706/2330 train_time:28002ms step_avg:39.66ms
step:707/2330 train_time:28026ms step_avg:39.64ms
step:708/2330 train_time:28082ms step_avg:39.66ms
step:709/2330 train_time:28104ms step_avg:39.64ms
step:710/2330 train_time:28161ms step_avg:39.66ms
step:711/2330 train_time:28183ms step_avg:39.64ms
step:712/2330 train_time:28240ms step_avg:39.66ms
step:713/2330 train_time:28262ms step_avg:39.64ms
step:714/2330 train_time:28319ms step_avg:39.66ms
step:715/2330 train_time:28341ms step_avg:39.64ms
step:716/2330 train_time:28398ms step_avg:39.66ms
step:717/2330 train_time:28420ms step_avg:39.64ms
step:718/2330 train_time:28477ms step_avg:39.66ms
step:719/2330 train_time:28499ms step_avg:39.64ms
step:720/2330 train_time:28556ms step_avg:39.66ms
step:721/2330 train_time:28578ms step_avg:39.64ms
step:722/2330 train_time:28634ms step_avg:39.66ms
step:723/2330 train_time:28657ms step_avg:39.64ms
step:724/2330 train_time:28713ms step_avg:39.66ms
step:725/2330 train_time:28735ms step_avg:39.63ms
step:726/2330 train_time:28791ms step_avg:39.66ms
step:727/2330 train_time:28814ms step_avg:39.63ms
step:728/2330 train_time:28870ms step_avg:39.66ms
step:729/2330 train_time:28894ms step_avg:39.63ms
step:730/2330 train_time:28950ms step_avg:39.66ms
step:731/2330 train_time:28973ms step_avg:39.63ms
step:732/2330 train_time:29029ms step_avg:39.66ms
step:733/2330 train_time:29053ms step_avg:39.64ms
step:734/2330 train_time:29109ms step_avg:39.66ms
step:735/2330 train_time:29131ms step_avg:39.63ms
step:736/2330 train_time:29188ms step_avg:39.66ms
step:737/2330 train_time:29211ms step_avg:39.64ms
step:738/2330 train_time:29268ms step_avg:39.66ms
step:739/2330 train_time:29291ms step_avg:39.64ms
step:740/2330 train_time:29347ms step_avg:39.66ms
step:741/2330 train_time:29370ms step_avg:39.64ms
step:742/2330 train_time:29426ms step_avg:39.66ms
step:743/2330 train_time:29449ms step_avg:39.64ms
step:744/2330 train_time:29505ms step_avg:39.66ms
step:745/2330 train_time:29528ms step_avg:39.63ms
step:746/2330 train_time:29585ms step_avg:39.66ms
step:747/2330 train_time:29608ms step_avg:39.64ms
step:748/2330 train_time:29664ms step_avg:39.66ms
step:749/2330 train_time:29686ms step_avg:39.63ms
step:750/2330 train_time:29744ms step_avg:39.66ms
step:750/2330 val_loss:5.2719 train_time:29840ms step_avg:39.79ms
step:751/2330 train_time:29853ms step_avg:39.75ms
step:752/2330 train_time:29864ms step_avg:39.71ms
step:753/2330 train_time:29875ms step_avg:39.67ms
step:754/2330 train_time:29904ms step_avg:39.66ms
step:755/2330 train_time:29926ms step_avg:39.64ms
step:756/2330 train_time:29981ms step_avg:39.66ms
step:757/2330 train_time:30003ms step_avg:39.63ms
step:758/2330 train_time:30058ms step_avg:39.65ms
step:759/2330 train_time:30080ms step_avg:39.63ms
step:760/2330 train_time:30136ms step_avg:39.65ms
step:761/2330 train_time:30163ms step_avg:39.64ms
step:762/2330 train_time:30224ms step_avg:39.66ms
step:763/2330 train_time:30247ms step_avg:39.64ms
step:764/2330 train_time:30305ms step_avg:39.67ms
step:765/2330 train_time:30329ms step_avg:39.65ms
step:766/2330 train_time:30385ms step_avg:39.67ms
step:767/2330 train_time:30407ms step_avg:39.64ms
step:768/2330 train_time:30463ms step_avg:39.66ms
step:769/2330 train_time:30485ms step_avg:39.64ms
step:770/2330 train_time:30541ms step_avg:39.66ms
step:771/2330 train_time:30563ms step_avg:39.64ms
step:772/2330 train_time:30619ms step_avg:39.66ms
step:773/2330 train_time:30640ms step_avg:39.64ms
step:774/2330 train_time:30695ms step_avg:39.66ms
step:775/2330 train_time:30718ms step_avg:39.64ms
step:776/2330 train_time:30774ms step_avg:39.66ms
step:777/2330 train_time:30798ms step_avg:39.64ms
step:778/2330 train_time:30854ms step_avg:39.66ms
step:779/2330 train_time:30877ms step_avg:39.64ms
step:780/2330 train_time:30932ms step_avg:39.66ms
step:781/2330 train_time:30955ms step_avg:39.63ms
step:782/2330 train_time:31011ms step_avg:39.66ms
step:783/2330 train_time:31033ms step_avg:39.63ms
step:784/2330 train_time:31090ms step_avg:39.66ms
step:785/2330 train_time:31113ms step_avg:39.63ms
step:786/2330 train_time:31171ms step_avg:39.66ms
step:787/2330 train_time:31194ms step_avg:39.64ms
step:788/2330 train_time:31251ms step_avg:39.66ms
step:789/2330 train_time:31274ms step_avg:39.64ms
step:790/2330 train_time:31330ms step_avg:39.66ms
step:791/2330 train_time:31354ms step_avg:39.64ms
step:792/2330 train_time:31410ms step_avg:39.66ms
step:793/2330 train_time:31433ms step_avg:39.64ms
step:794/2330 train_time:31489ms step_avg:39.66ms
step:795/2330 train_time:31512ms step_avg:39.64ms
step:796/2330 train_time:31568ms step_avg:39.66ms
step:797/2330 train_time:31592ms step_avg:39.64ms
step:798/2330 train_time:31648ms step_avg:39.66ms
step:799/2330 train_time:31671ms step_avg:39.64ms
step:800/2330 train_time:31728ms step_avg:39.66ms
step:801/2330 train_time:31750ms step_avg:39.64ms
step:802/2330 train_time:31807ms step_avg:39.66ms
step:803/2330 train_time:31829ms step_avg:39.64ms
step:804/2330 train_time:31885ms step_avg:39.66ms
step:805/2330 train_time:31907ms step_avg:39.64ms
step:806/2330 train_time:31963ms step_avg:39.66ms
step:807/2330 train_time:31986ms step_avg:39.64ms
step:808/2330 train_time:32042ms step_avg:39.66ms
step:809/2330 train_time:32064ms step_avg:39.63ms
step:810/2330 train_time:32121ms step_avg:39.66ms
step:811/2330 train_time:32143ms step_avg:39.63ms
step:812/2330 train_time:32200ms step_avg:39.66ms
step:813/2330 train_time:32223ms step_avg:39.64ms
step:814/2330 train_time:32280ms step_avg:39.66ms
step:815/2330 train_time:32302ms step_avg:39.63ms
step:816/2330 train_time:32359ms step_avg:39.66ms
step:817/2330 train_time:32381ms step_avg:39.63ms
step:818/2330 train_time:32437ms step_avg:39.65ms
step:819/2330 train_time:32459ms step_avg:39.63ms
step:820/2330 train_time:32514ms step_avg:39.65ms
step:821/2330 train_time:32537ms step_avg:39.63ms
step:822/2330 train_time:32593ms step_avg:39.65ms
step:823/2330 train_time:32616ms step_avg:39.63ms
step:824/2330 train_time:32672ms step_avg:39.65ms
step:825/2330 train_time:32694ms step_avg:39.63ms
step:826/2330 train_time:32750ms step_avg:39.65ms
step:827/2330 train_time:32773ms step_avg:39.63ms
step:828/2330 train_time:32829ms step_avg:39.65ms
step:829/2330 train_time:32852ms step_avg:39.63ms
step:830/2330 train_time:32908ms step_avg:39.65ms
step:831/2330 train_time:32931ms step_avg:39.63ms
step:832/2330 train_time:32988ms step_avg:39.65ms
step:833/2330 train_time:33010ms step_avg:39.63ms
step:834/2330 train_time:33067ms step_avg:39.65ms
step:835/2330 train_time:33089ms step_avg:39.63ms
step:836/2330 train_time:33146ms step_avg:39.65ms
step:837/2330 train_time:33169ms step_avg:39.63ms
step:838/2330 train_time:33225ms step_avg:39.65ms
step:839/2330 train_time:33247ms step_avg:39.63ms
step:840/2330 train_time:33304ms step_avg:39.65ms
step:841/2330 train_time:33326ms step_avg:39.63ms
step:842/2330 train_time:33383ms step_avg:39.65ms
step:843/2330 train_time:33405ms step_avg:39.63ms
step:844/2330 train_time:33462ms step_avg:39.65ms
step:845/2330 train_time:33485ms step_avg:39.63ms
step:846/2330 train_time:33542ms step_avg:39.65ms
step:847/2330 train_time:33564ms step_avg:39.63ms
step:848/2330 train_time:33621ms step_avg:39.65ms
step:849/2330 train_time:33643ms step_avg:39.63ms
step:850/2330 train_time:33700ms step_avg:39.65ms
step:851/2330 train_time:33722ms step_avg:39.63ms
step:852/2330 train_time:33779ms step_avg:39.65ms
step:853/2330 train_time:33801ms step_avg:39.63ms
step:854/2330 train_time:33856ms step_avg:39.64ms
step:855/2330 train_time:33879ms step_avg:39.62ms
step:856/2330 train_time:33935ms step_avg:39.64ms
step:857/2330 train_time:33958ms step_avg:39.62ms
step:858/2330 train_time:34014ms step_avg:39.64ms
step:859/2330 train_time:34037ms step_avg:39.62ms
step:860/2330 train_time:34093ms step_avg:39.64ms
step:861/2330 train_time:34116ms step_avg:39.62ms
step:862/2330 train_time:34172ms step_avg:39.64ms
step:863/2330 train_time:34195ms step_avg:39.62ms
step:864/2330 train_time:34252ms step_avg:39.64ms
step:865/2330 train_time:34274ms step_avg:39.62ms
step:866/2330 train_time:34331ms step_avg:39.64ms
step:867/2330 train_time:34354ms step_avg:39.62ms
step:868/2330 train_time:34410ms step_avg:39.64ms
step:869/2330 train_time:34432ms step_avg:39.62ms
step:870/2330 train_time:34489ms step_avg:39.64ms
step:871/2330 train_time:34512ms step_avg:39.62ms
step:872/2330 train_time:34568ms step_avg:39.64ms
step:873/2330 train_time:34592ms step_avg:39.62ms
step:874/2330 train_time:34649ms step_avg:39.64ms
step:875/2330 train_time:34672ms step_avg:39.62ms
step:876/2330 train_time:34728ms step_avg:39.64ms
step:877/2330 train_time:34751ms step_avg:39.62ms
step:878/2330 train_time:34808ms step_avg:39.64ms
step:879/2330 train_time:34830ms step_avg:39.62ms
step:880/2330 train_time:34887ms step_avg:39.64ms
step:881/2330 train_time:34909ms step_avg:39.62ms
step:882/2330 train_time:34966ms step_avg:39.64ms
step:883/2330 train_time:34988ms step_avg:39.62ms
step:884/2330 train_time:35045ms step_avg:39.64ms
step:885/2330 train_time:35067ms step_avg:39.62ms
step:886/2330 train_time:35124ms step_avg:39.64ms
step:887/2330 train_time:35146ms step_avg:39.62ms
step:888/2330 train_time:35203ms step_avg:39.64ms
step:889/2330 train_time:35225ms step_avg:39.62ms
step:890/2330 train_time:35281ms step_avg:39.64ms
step:891/2330 train_time:35303ms step_avg:39.62ms
step:892/2330 train_time:35359ms step_avg:39.64ms
step:893/2330 train_time:35381ms step_avg:39.62ms
step:894/2330 train_time:35437ms step_avg:39.64ms
step:895/2330 train_time:35460ms step_avg:39.62ms
step:896/2330 train_time:35516ms step_avg:39.64ms
step:897/2330 train_time:35539ms step_avg:39.62ms
step:898/2330 train_time:35594ms step_avg:39.64ms
step:899/2330 train_time:35617ms step_avg:39.62ms
step:900/2330 train_time:35673ms step_avg:39.64ms
step:901/2330 train_time:35696ms step_avg:39.62ms
step:902/2330 train_time:35752ms step_avg:39.64ms
step:903/2330 train_time:35775ms step_avg:39.62ms
step:904/2330 train_time:35831ms step_avg:39.64ms
step:905/2330 train_time:35854ms step_avg:39.62ms
step:906/2330 train_time:35910ms step_avg:39.64ms
step:907/2330 train_time:35933ms step_avg:39.62ms
step:908/2330 train_time:35990ms step_avg:39.64ms
step:909/2330 train_time:36012ms step_avg:39.62ms
step:910/2330 train_time:36069ms step_avg:39.64ms
step:911/2330 train_time:36091ms step_avg:39.62ms
step:912/2330 train_time:36148ms step_avg:39.64ms
step:913/2330 train_time:36171ms step_avg:39.62ms
step:914/2330 train_time:36228ms step_avg:39.64ms
step:915/2330 train_time:36251ms step_avg:39.62ms
step:916/2330 train_time:36307ms step_avg:39.64ms
step:917/2330 train_time:36330ms step_avg:39.62ms
step:918/2330 train_time:36387ms step_avg:39.64ms
step:919/2330 train_time:36409ms step_avg:39.62ms
step:920/2330 train_time:36466ms step_avg:39.64ms
step:921/2330 train_time:36488ms step_avg:39.62ms
step:922/2330 train_time:36545ms step_avg:39.64ms
step:923/2330 train_time:36567ms step_avg:39.62ms
step:924/2330 train_time:36624ms step_avg:39.64ms
step:925/2330 train_time:36647ms step_avg:39.62ms
step:926/2330 train_time:36704ms step_avg:39.64ms
step:927/2330 train_time:36726ms step_avg:39.62ms
step:928/2330 train_time:36783ms step_avg:39.64ms
step:929/2330 train_time:36806ms step_avg:39.62ms
step:930/2330 train_time:36863ms step_avg:39.64ms
step:931/2330 train_time:36885ms step_avg:39.62ms
step:932/2330 train_time:36942ms step_avg:39.64ms
step:933/2330 train_time:36964ms step_avg:39.62ms
step:934/2330 train_time:37020ms step_avg:39.64ms
step:935/2330 train_time:37043ms step_avg:39.62ms
step:936/2330 train_time:37099ms step_avg:39.64ms
step:937/2330 train_time:37121ms step_avg:39.62ms
step:938/2330 train_time:37178ms step_avg:39.63ms
step:939/2330 train_time:37200ms step_avg:39.62ms
step:940/2330 train_time:37256ms step_avg:39.63ms
step:941/2330 train_time:37279ms step_avg:39.62ms
step:942/2330 train_time:37335ms step_avg:39.63ms
step:943/2330 train_time:37358ms step_avg:39.62ms
step:944/2330 train_time:37414ms step_avg:39.63ms
step:945/2330 train_time:37437ms step_avg:39.62ms
step:946/2330 train_time:37493ms step_avg:39.63ms
step:947/2330 train_time:37516ms step_avg:39.62ms
step:948/2330 train_time:37572ms step_avg:39.63ms
step:949/2330 train_time:37595ms step_avg:39.62ms
step:950/2330 train_time:37652ms step_avg:39.63ms
step:951/2330 train_time:37674ms step_avg:39.61ms
step:952/2330 train_time:37730ms step_avg:39.63ms
step:953/2330 train_time:37753ms step_avg:39.61ms
step:954/2330 train_time:37809ms step_avg:39.63ms
step:955/2330 train_time:37832ms step_avg:39.61ms
step:956/2330 train_time:37888ms step_avg:39.63ms
step:957/2330 train_time:37910ms step_avg:39.61ms
step:958/2330 train_time:37966ms step_avg:39.63ms
step:959/2330 train_time:37989ms step_avg:39.61ms
step:960/2330 train_time:38046ms step_avg:39.63ms
step:961/2330 train_time:38068ms step_avg:39.61ms
step:962/2330 train_time:38125ms step_avg:39.63ms
step:963/2330 train_time:38147ms step_avg:39.61ms
step:964/2330 train_time:38203ms step_avg:39.63ms
step:965/2330 train_time:38226ms step_avg:39.61ms
step:966/2330 train_time:38282ms step_avg:39.63ms
step:967/2330 train_time:38305ms step_avg:39.61ms
step:968/2330 train_time:38361ms step_avg:39.63ms
step:969/2330 train_time:38385ms step_avg:39.61ms
step:970/2330 train_time:38441ms step_avg:39.63ms
step:971/2330 train_time:38463ms step_avg:39.61ms
step:972/2330 train_time:38521ms step_avg:39.63ms
step:973/2330 train_time:38543ms step_avg:39.61ms
step:974/2330 train_time:38600ms step_avg:39.63ms
step:975/2330 train_time:38622ms step_avg:39.61ms
step:976/2330 train_time:38678ms step_avg:39.63ms
step:977/2330 train_time:38701ms step_avg:39.61ms
step:978/2330 train_time:38756ms step_avg:39.63ms
step:979/2330 train_time:38779ms step_avg:39.61ms
step:980/2330 train_time:38835ms step_avg:39.63ms
step:981/2330 train_time:38858ms step_avg:39.61ms
step:982/2330 train_time:38914ms step_avg:39.63ms
step:983/2330 train_time:38937ms step_avg:39.61ms
step:984/2330 train_time:38993ms step_avg:39.63ms
step:985/2330 train_time:39016ms step_avg:39.61ms
step:986/2330 train_time:39073ms step_avg:39.63ms
step:987/2330 train_time:39096ms step_avg:39.61ms
step:988/2330 train_time:39152ms step_avg:39.63ms
step:989/2330 train_time:39174ms step_avg:39.61ms
step:990/2330 train_time:39230ms step_avg:39.63ms
step:991/2330 train_time:39252ms step_avg:39.61ms
step:992/2330 train_time:39309ms step_avg:39.63ms
step:993/2330 train_time:39331ms step_avg:39.61ms
step:994/2330 train_time:39387ms step_avg:39.63ms
step:995/2330 train_time:39411ms step_avg:39.61ms
step:996/2330 train_time:39467ms step_avg:39.63ms
step:997/2330 train_time:39490ms step_avg:39.61ms
step:998/2330 train_time:39546ms step_avg:39.63ms
step:999/2330 train_time:39569ms step_avg:39.61ms
step:1000/2330 train_time:39626ms step_avg:39.63ms
step:1000/2330 val_loss:5.2291 train_time:39722ms step_avg:39.72ms
step:1001/2330 train_time:39735ms step_avg:39.70ms
step:1002/2330 train_time:39746ms step_avg:39.67ms
step:1003/2330 train_time:39756ms step_avg:39.64ms
step:1004/2330 train_time:39784ms step_avg:39.63ms
step:1005/2330 train_time:39805ms step_avg:39.61ms
step:1006/2330 train_time:39860ms step_avg:39.62ms
step:1007/2330 train_time:39882ms step_avg:39.60ms
step:1008/2330 train_time:39938ms step_avg:39.62ms
step:1009/2330 train_time:39959ms step_avg:39.60ms
step:1010/2330 train_time:40015ms step_avg:39.62ms
step:1011/2330 train_time:40043ms step_avg:39.61ms
step:1012/2330 train_time:40104ms step_avg:39.63ms
step:1013/2330 train_time:40127ms step_avg:39.61ms
step:1014/2330 train_time:40185ms step_avg:39.63ms
step:1015/2330 train_time:40207ms step_avg:39.61ms
step:1016/2330 train_time:40263ms step_avg:39.63ms
step:1017/2330 train_time:40285ms step_avg:39.61ms
step:1018/2330 train_time:40341ms step_avg:39.63ms
step:1019/2330 train_time:40363ms step_avg:39.61ms
step:1020/2330 train_time:40419ms step_avg:39.63ms
step:1021/2330 train_time:40441ms step_avg:39.61ms
step:1022/2330 train_time:40496ms step_avg:39.62ms
step:1023/2330 train_time:40519ms step_avg:39.61ms
step:1024/2330 train_time:40574ms step_avg:39.62ms
step:1025/2330 train_time:40597ms step_avg:39.61ms
step:1026/2330 train_time:40655ms step_avg:39.63ms
step:1027/2330 train_time:40680ms step_avg:39.61ms
step:1028/2330 train_time:40737ms step_avg:39.63ms
step:1029/2330 train_time:40759ms step_avg:39.61ms
step:1030/2330 train_time:40815ms step_avg:39.63ms
step:1031/2330 train_time:40838ms step_avg:39.61ms
step:1032/2330 train_time:40893ms step_avg:39.62ms
step:1033/2330 train_time:40915ms step_avg:39.61ms
step:1034/2330 train_time:40972ms step_avg:39.62ms
step:1035/2330 train_time:40996ms step_avg:39.61ms
step:1036/2330 train_time:41053ms step_avg:39.63ms
step:1037/2330 train_time:41077ms step_avg:39.61ms
step:1038/2330 train_time:41134ms step_avg:39.63ms
step:1039/2330 train_time:41158ms step_avg:39.61ms
step:1040/2330 train_time:41214ms step_avg:39.63ms
step:1041/2330 train_time:41237ms step_avg:39.61ms
step:1042/2330 train_time:41293ms step_avg:39.63ms
step:1043/2330 train_time:41316ms step_avg:39.61ms
step:1044/2330 train_time:41372ms step_avg:39.63ms
step:1045/2330 train_time:41395ms step_avg:39.61ms
step:1046/2330 train_time:41451ms step_avg:39.63ms
step:1047/2330 train_time:41473ms step_avg:39.61ms
step:1048/2330 train_time:41529ms step_avg:39.63ms
step:1049/2330 train_time:41552ms step_avg:39.61ms
step:1050/2330 train_time:41608ms step_avg:39.63ms
step:1051/2330 train_time:41630ms step_avg:39.61ms
step:1052/2330 train_time:41687ms step_avg:39.63ms
step:1053/2330 train_time:41710ms step_avg:39.61ms
step:1054/2330 train_time:41766ms step_avg:39.63ms
step:1055/2330 train_time:41789ms step_avg:39.61ms
step:1056/2330 train_time:41845ms step_avg:39.63ms
step:1057/2330 train_time:41867ms step_avg:39.61ms
step:1058/2330 train_time:41924ms step_avg:39.63ms
step:1059/2330 train_time:41947ms step_avg:39.61ms
step:1060/2330 train_time:42004ms step_avg:39.63ms
step:1061/2330 train_time:42027ms step_avg:39.61ms
step:1062/2330 train_time:42083ms step_avg:39.63ms
step:1063/2330 train_time:42106ms step_avg:39.61ms
step:1064/2330 train_time:42163ms step_avg:39.63ms
step:1065/2330 train_time:42185ms step_avg:39.61ms
step:1066/2330 train_time:42242ms step_avg:39.63ms
step:1067/2330 train_time:42264ms step_avg:39.61ms
step:1068/2330 train_time:42320ms step_avg:39.63ms
step:1069/2330 train_time:42343ms step_avg:39.61ms
step:1070/2330 train_time:42400ms step_avg:39.63ms
step:1071/2330 train_time:42421ms step_avg:39.61ms
step:1072/2330 train_time:42477ms step_avg:39.62ms
step:1073/2330 train_time:42500ms step_avg:39.61ms
step:1074/2330 train_time:42557ms step_avg:39.62ms
step:1075/2330 train_time:42579ms step_avg:39.61ms
step:1076/2330 train_time:42635ms step_avg:39.62ms
step:1077/2330 train_time:42657ms step_avg:39.61ms
step:1078/2330 train_time:42714ms step_avg:39.62ms
step:1079/2330 train_time:42736ms step_avg:39.61ms
step:1080/2330 train_time:42791ms step_avg:39.62ms
step:1081/2330 train_time:42814ms step_avg:39.61ms
step:1082/2330 train_time:42871ms step_avg:39.62ms
step:1083/2330 train_time:42894ms step_avg:39.61ms
step:1084/2330 train_time:42949ms step_avg:39.62ms
step:1085/2330 train_time:42973ms step_avg:39.61ms
step:1086/2330 train_time:43029ms step_avg:39.62ms
step:1087/2330 train_time:43052ms step_avg:39.61ms
step:1088/2330 train_time:43108ms step_avg:39.62ms
step:1089/2330 train_time:43132ms step_avg:39.61ms
step:1090/2330 train_time:43189ms step_avg:39.62ms
step:1091/2330 train_time:43212ms step_avg:39.61ms
step:1092/2330 train_time:43268ms step_avg:39.62ms
step:1093/2330 train_time:43291ms step_avg:39.61ms
step:1094/2330 train_time:43347ms step_avg:39.62ms
step:1095/2330 train_time:43370ms step_avg:39.61ms
step:1096/2330 train_time:43426ms step_avg:39.62ms
step:1097/2330 train_time:43449ms step_avg:39.61ms
step:1098/2330 train_time:43505ms step_avg:39.62ms
step:1099/2330 train_time:43528ms step_avg:39.61ms
step:1100/2330 train_time:43585ms step_avg:39.62ms
step:1101/2330 train_time:43608ms step_avg:39.61ms
step:1102/2330 train_time:43664ms step_avg:39.62ms
step:1103/2330 train_time:43686ms step_avg:39.61ms
step:1104/2330 train_time:43743ms step_avg:39.62ms
step:1105/2330 train_time:43765ms step_avg:39.61ms
step:1106/2330 train_time:43821ms step_avg:39.62ms
step:1107/2330 train_time:43844ms step_avg:39.61ms
step:1108/2330 train_time:43900ms step_avg:39.62ms
step:1109/2330 train_time:43922ms step_avg:39.61ms
step:1110/2330 train_time:43980ms step_avg:39.62ms
step:1111/2330 train_time:44002ms step_avg:39.61ms
step:1112/2330 train_time:44059ms step_avg:39.62ms
step:1113/2330 train_time:44081ms step_avg:39.61ms
step:1114/2330 train_time:44139ms step_avg:39.62ms
step:1115/2330 train_time:44162ms step_avg:39.61ms
step:1116/2330 train_time:44218ms step_avg:39.62ms
step:1117/2330 train_time:44240ms step_avg:39.61ms
step:1118/2330 train_time:44297ms step_avg:39.62ms
step:1119/2330 train_time:44319ms step_avg:39.61ms
step:1120/2330 train_time:44376ms step_avg:39.62ms
step:1121/2330 train_time:44398ms step_avg:39.61ms
step:1122/2330 train_time:44455ms step_avg:39.62ms
step:1123/2330 train_time:44477ms step_avg:39.61ms
step:1124/2330 train_time:44533ms step_avg:39.62ms
step:1125/2330 train_time:44556ms step_avg:39.61ms
step:1126/2330 train_time:44612ms step_avg:39.62ms
step:1127/2330 train_time:44635ms step_avg:39.61ms
step:1128/2330 train_time:44691ms step_avg:39.62ms
step:1129/2330 train_time:44714ms step_avg:39.60ms
step:1130/2330 train_time:44769ms step_avg:39.62ms
step:1131/2330 train_time:44792ms step_avg:39.60ms
step:1132/2330 train_time:44847ms step_avg:39.62ms
step:1133/2330 train_time:44871ms step_avg:39.60ms
step:1134/2330 train_time:44927ms step_avg:39.62ms
step:1135/2330 train_time:44950ms step_avg:39.60ms
step:1136/2330 train_time:45006ms step_avg:39.62ms
step:1137/2330 train_time:45030ms step_avg:39.60ms
step:1138/2330 train_time:45086ms step_avg:39.62ms
step:1139/2330 train_time:45109ms step_avg:39.60ms
step:1140/2330 train_time:45165ms step_avg:39.62ms
step:1141/2330 train_time:45188ms step_avg:39.60ms
step:1142/2330 train_time:45244ms step_avg:39.62ms
step:1143/2330 train_time:45267ms step_avg:39.60ms
step:1144/2330 train_time:45324ms step_avg:39.62ms
step:1145/2330 train_time:45346ms step_avg:39.60ms
step:1146/2330 train_time:45403ms step_avg:39.62ms
step:1147/2330 train_time:45426ms step_avg:39.60ms
step:1148/2330 train_time:45483ms step_avg:39.62ms
step:1149/2330 train_time:45505ms step_avg:39.60ms
step:1150/2330 train_time:45560ms step_avg:39.62ms
step:1151/2330 train_time:45583ms step_avg:39.60ms
step:1152/2330 train_time:45640ms step_avg:39.62ms
step:1153/2330 train_time:45662ms step_avg:39.60ms
step:1154/2330 train_time:45718ms step_avg:39.62ms
step:1155/2330 train_time:45741ms step_avg:39.60ms
step:1156/2330 train_time:45798ms step_avg:39.62ms
step:1157/2330 train_time:45820ms step_avg:39.60ms
step:1158/2330 train_time:45877ms step_avg:39.62ms
step:1159/2330 train_time:45898ms step_avg:39.60ms
step:1160/2330 train_time:45955ms step_avg:39.62ms
step:1161/2330 train_time:45977ms step_avg:39.60ms
step:1162/2330 train_time:46033ms step_avg:39.62ms
step:1163/2330 train_time:46055ms step_avg:39.60ms
step:1164/2330 train_time:46111ms step_avg:39.61ms
step:1165/2330 train_time:46134ms step_avg:39.60ms
step:1166/2330 train_time:46191ms step_avg:39.61ms
step:1167/2330 train_time:46214ms step_avg:39.60ms
step:1168/2330 train_time:46270ms step_avg:39.61ms
step:1169/2330 train_time:46293ms step_avg:39.60ms
step:1170/2330 train_time:46350ms step_avg:39.61ms
step:1171/2330 train_time:46373ms step_avg:39.60ms
step:1172/2330 train_time:46429ms step_avg:39.62ms
step:1173/2330 train_time:46452ms step_avg:39.60ms
step:1174/2330 train_time:46508ms step_avg:39.62ms
step:1175/2330 train_time:46531ms step_avg:39.60ms
step:1176/2330 train_time:46587ms step_avg:39.62ms
step:1177/2330 train_time:46610ms step_avg:39.60ms
step:1178/2330 train_time:46666ms step_avg:39.61ms
step:1179/2330 train_time:46688ms step_avg:39.60ms
step:1180/2330 train_time:46744ms step_avg:39.61ms
step:1181/2330 train_time:46767ms step_avg:39.60ms
step:1182/2330 train_time:46824ms step_avg:39.61ms
step:1183/2330 train_time:46847ms step_avg:39.60ms
step:1184/2330 train_time:46904ms step_avg:39.61ms
step:1185/2330 train_time:46926ms step_avg:39.60ms
step:1186/2330 train_time:46982ms step_avg:39.61ms
step:1187/2330 train_time:47004ms step_avg:39.60ms
step:1188/2330 train_time:47061ms step_avg:39.61ms
step:1189/2330 train_time:47083ms step_avg:39.60ms
step:1190/2330 train_time:47140ms step_avg:39.61ms
step:1191/2330 train_time:47162ms step_avg:39.60ms
step:1192/2330 train_time:47219ms step_avg:39.61ms
step:1193/2330 train_time:47241ms step_avg:39.60ms
step:1194/2330 train_time:47298ms step_avg:39.61ms
step:1195/2330 train_time:47320ms step_avg:39.60ms
step:1196/2330 train_time:47376ms step_avg:39.61ms
step:1197/2330 train_time:47399ms step_avg:39.60ms
step:1198/2330 train_time:47456ms step_avg:39.61ms
step:1199/2330 train_time:47478ms step_avg:39.60ms
step:1200/2330 train_time:47534ms step_avg:39.61ms
step:1201/2330 train_time:47556ms step_avg:39.60ms
step:1202/2330 train_time:47612ms step_avg:39.61ms
step:1203/2330 train_time:47635ms step_avg:39.60ms
step:1204/2330 train_time:47691ms step_avg:39.61ms
step:1205/2330 train_time:47715ms step_avg:39.60ms
step:1206/2330 train_time:47771ms step_avg:39.61ms
step:1207/2330 train_time:47795ms step_avg:39.60ms
step:1208/2330 train_time:47851ms step_avg:39.61ms
step:1209/2330 train_time:47874ms step_avg:39.60ms
step:1210/2330 train_time:47930ms step_avg:39.61ms
step:1211/2330 train_time:47953ms step_avg:39.60ms
step:1212/2330 train_time:48009ms step_avg:39.61ms
step:1213/2330 train_time:48032ms step_avg:39.60ms
step:1214/2330 train_time:48088ms step_avg:39.61ms
step:1215/2330 train_time:48111ms step_avg:39.60ms
step:1216/2330 train_time:48167ms step_avg:39.61ms
step:1217/2330 train_time:48190ms step_avg:39.60ms
step:1218/2330 train_time:48247ms step_avg:39.61ms
step:1219/2330 train_time:48269ms step_avg:39.60ms
step:1220/2330 train_time:48325ms step_avg:39.61ms
step:1221/2330 train_time:48349ms step_avg:39.60ms
step:1222/2330 train_time:48405ms step_avg:39.61ms
step:1223/2330 train_time:48428ms step_avg:39.60ms
step:1224/2330 train_time:48484ms step_avg:39.61ms
step:1225/2330 train_time:48507ms step_avg:39.60ms
step:1226/2330 train_time:48563ms step_avg:39.61ms
step:1227/2330 train_time:48585ms step_avg:39.60ms
step:1228/2330 train_time:48642ms step_avg:39.61ms
step:1229/2330 train_time:48665ms step_avg:39.60ms
step:1230/2330 train_time:48722ms step_avg:39.61ms
step:1231/2330 train_time:48744ms step_avg:39.60ms
step:1232/2330 train_time:48800ms step_avg:39.61ms
step:1233/2330 train_time:48823ms step_avg:39.60ms
step:1234/2330 train_time:48879ms step_avg:39.61ms
step:1235/2330 train_time:48901ms step_avg:39.60ms
step:1236/2330 train_time:48957ms step_avg:39.61ms
step:1237/2330 train_time:48980ms step_avg:39.60ms
step:1238/2330 train_time:49035ms step_avg:39.61ms
step:1239/2330 train_time:49057ms step_avg:39.59ms
step:1240/2330 train_time:49114ms step_avg:39.61ms
step:1241/2330 train_time:49136ms step_avg:39.59ms
step:1242/2330 train_time:49191ms step_avg:39.61ms
step:1243/2330 train_time:49214ms step_avg:39.59ms
step:1244/2330 train_time:49270ms step_avg:39.61ms
step:1245/2330 train_time:49293ms step_avg:39.59ms
step:1246/2330 train_time:49350ms step_avg:39.61ms
step:1247/2330 train_time:49373ms step_avg:39.59ms
step:1248/2330 train_time:49430ms step_avg:39.61ms
step:1249/2330 train_time:49453ms step_avg:39.59ms
step:1250/2330 train_time:49510ms step_avg:39.61ms
step:1250/2330 val_loss:5.1979 train_time:49608ms step_avg:39.69ms
step:1251/2330 train_time:49620ms step_avg:39.66ms
step:1252/2330 train_time:49631ms step_avg:39.64ms
step:1253/2330 train_time:49641ms step_avg:39.62ms
step:1254/2330 train_time:49673ms step_avg:39.61ms
step:1255/2330 train_time:49694ms step_avg:39.60ms
step:1256/2330 train_time:49749ms step_avg:39.61ms
step:1257/2330 train_time:49770ms step_avg:39.59ms
step:1258/2330 train_time:49826ms step_avg:39.61ms
step:1259/2330 train_time:49848ms step_avg:39.59ms
step:1260/2330 train_time:49904ms step_avg:39.61ms
step:1261/2330 train_time:49930ms step_avg:39.60ms
step:1262/2330 train_time:49991ms step_avg:39.61ms
step:1263/2330 train_time:50014ms step_avg:39.60ms
step:1264/2330 train_time:50071ms step_avg:39.61ms
step:1265/2330 train_time:50093ms step_avg:39.60ms
step:1266/2330 train_time:50149ms step_avg:39.61ms
step:1267/2330 train_time:50171ms step_avg:39.60ms
step:1268/2330 train_time:50228ms step_avg:39.61ms
step:1269/2330 train_time:50250ms step_avg:39.60ms
step:1270/2330 train_time:50306ms step_avg:39.61ms
step:1271/2330 train_time:50328ms step_avg:39.60ms
step:1272/2330 train_time:50383ms step_avg:39.61ms
step:1273/2330 train_time:50406ms step_avg:39.60ms
step:1274/2330 train_time:50462ms step_avg:39.61ms
step:1275/2330 train_time:50483ms step_avg:39.59ms
step:1276/2330 train_time:50539ms step_avg:39.61ms
step:1277/2330 train_time:50562ms step_avg:39.59ms
step:1278/2330 train_time:50618ms step_avg:39.61ms
step:1279/2330 train_time:50642ms step_avg:39.59ms
step:1280/2330 train_time:50697ms step_avg:39.61ms
step:1281/2330 train_time:50720ms step_avg:39.59ms
step:1282/2330 train_time:50775ms step_avg:39.61ms
step:1283/2330 train_time:50797ms step_avg:39.59ms
step:1284/2330 train_time:50853ms step_avg:39.61ms
step:1285/2330 train_time:50876ms step_avg:39.59ms
step:1286/2330 train_time:50935ms step_avg:39.61ms
step:1287/2330 train_time:50959ms step_avg:39.60ms
step:1288/2330 train_time:51016ms step_avg:39.61ms
step:1289/2330 train_time:51040ms step_avg:39.60ms
step:1290/2330 train_time:51097ms step_avg:39.61ms
step:1291/2330 train_time:51120ms step_avg:39.60ms
step:1292/2330 train_time:51176ms step_avg:39.61ms
step:1293/2330 train_time:51198ms step_avg:39.60ms
step:1294/2330 train_time:51254ms step_avg:39.61ms
step:1295/2330 train_time:51276ms step_avg:39.60ms
step:1296/2330 train_time:51332ms step_avg:39.61ms
step:1297/2330 train_time:51355ms step_avg:39.60ms
step:1298/2330 train_time:51411ms step_avg:39.61ms
step:1299/2330 train_time:51433ms step_avg:39.59ms
step:1300/2330 train_time:51489ms step_avg:39.61ms
step:1301/2330 train_time:51511ms step_avg:39.59ms
step:1302/2330 train_time:51568ms step_avg:39.61ms
step:1303/2330 train_time:51590ms step_avg:39.59ms
step:1304/2330 train_time:51646ms step_avg:39.61ms
step:1305/2330 train_time:51668ms step_avg:39.59ms
step:1306/2330 train_time:51725ms step_avg:39.61ms
step:1307/2330 train_time:51747ms step_avg:39.59ms
step:1308/2330 train_time:51803ms step_avg:39.60ms
step:1309/2330 train_time:51825ms step_avg:39.59ms
step:1310/2330 train_time:51882ms step_avg:39.60ms
step:1311/2330 train_time:51905ms step_avg:39.59ms
step:1312/2330 train_time:51961ms step_avg:39.60ms
step:1313/2330 train_time:51984ms step_avg:39.59ms
step:1314/2330 train_time:52040ms step_avg:39.60ms
step:1315/2330 train_time:52063ms step_avg:39.59ms
step:1316/2330 train_time:52119ms step_avg:39.60ms
step:1317/2330 train_time:52142ms step_avg:39.59ms
step:1318/2330 train_time:52198ms step_avg:39.60ms
step:1319/2330 train_time:52221ms step_avg:39.59ms
step:1320/2330 train_time:52278ms step_avg:39.60ms
step:1321/2330 train_time:52300ms step_avg:39.59ms
step:1322/2330 train_time:52357ms step_avg:39.60ms
step:1323/2330 train_time:52379ms step_avg:39.59ms
step:1324/2330 train_time:52435ms step_avg:39.60ms
step:1325/2330 train_time:52458ms step_avg:39.59ms
step:1326/2330 train_time:52514ms step_avg:39.60ms
step:1327/2330 train_time:52537ms step_avg:39.59ms
step:1328/2330 train_time:52593ms step_avg:39.60ms
step:1329/2330 train_time:52615ms step_avg:39.59ms
step:1330/2330 train_time:52672ms step_avg:39.60ms
step:1331/2330 train_time:52694ms step_avg:39.59ms
step:1332/2330 train_time:52750ms step_avg:39.60ms
step:1333/2330 train_time:52772ms step_avg:39.59ms
step:1334/2330 train_time:52829ms step_avg:39.60ms
step:1335/2330 train_time:52852ms step_avg:39.59ms
step:1336/2330 train_time:52909ms step_avg:39.60ms
step:1337/2330 train_time:52931ms step_avg:39.59ms
step:1338/2330 train_time:52988ms step_avg:39.60ms
step:1339/2330 train_time:53010ms step_avg:39.59ms
step:1340/2330 train_time:53067ms step_avg:39.60ms
step:1341/2330 train_time:53089ms step_avg:39.59ms
step:1342/2330 train_time:53147ms step_avg:39.60ms
step:1343/2330 train_time:53169ms step_avg:39.59ms
step:1344/2330 train_time:53226ms step_avg:39.60ms
step:1345/2330 train_time:53249ms step_avg:39.59ms
step:1346/2330 train_time:53306ms step_avg:39.60ms
step:1347/2330 train_time:53328ms step_avg:39.59ms
step:1348/2330 train_time:53385ms step_avg:39.60ms
step:1349/2330 train_time:53407ms step_avg:39.59ms
step:1350/2330 train_time:53463ms step_avg:39.60ms
step:1351/2330 train_time:53485ms step_avg:39.59ms
step:1352/2330 train_time:53541ms step_avg:39.60ms
step:1353/2330 train_time:53563ms step_avg:39.59ms
step:1354/2330 train_time:53619ms step_avg:39.60ms
step:1355/2330 train_time:53642ms step_avg:39.59ms
step:1356/2330 train_time:53698ms step_avg:39.60ms
step:1357/2330 train_time:53721ms step_avg:39.59ms
step:1358/2330 train_time:53778ms step_avg:39.60ms
step:1359/2330 train_time:53801ms step_avg:39.59ms
step:1360/2330 train_time:53857ms step_avg:39.60ms
step:1361/2330 train_time:53880ms step_avg:39.59ms
step:1362/2330 train_time:53936ms step_avg:39.60ms
step:1363/2330 train_time:53959ms step_avg:39.59ms
step:1364/2330 train_time:54015ms step_avg:39.60ms
step:1365/2330 train_time:54038ms step_avg:39.59ms
step:1366/2330 train_time:54094ms step_avg:39.60ms
step:1367/2330 train_time:54117ms step_avg:39.59ms
step:1368/2330 train_time:54173ms step_avg:39.60ms
step:1369/2330 train_time:54196ms step_avg:39.59ms
step:1370/2330 train_time:54252ms step_avg:39.60ms
step:1371/2330 train_time:54275ms step_avg:39.59ms
step:1372/2330 train_time:54332ms step_avg:39.60ms
step:1373/2330 train_time:54354ms step_avg:39.59ms
step:1374/2330 train_time:54411ms step_avg:39.60ms
step:1375/2330 train_time:54433ms step_avg:39.59ms
step:1376/2330 train_time:54489ms step_avg:39.60ms
step:1377/2330 train_time:54511ms step_avg:39.59ms
step:1378/2330 train_time:54568ms step_avg:39.60ms
step:1379/2330 train_time:54590ms step_avg:39.59ms
step:1380/2330 train_time:54646ms step_avg:39.60ms
step:1381/2330 train_time:54668ms step_avg:39.59ms
step:1382/2330 train_time:54725ms step_avg:39.60ms
step:1383/2330 train_time:54747ms step_avg:39.59ms
step:1384/2330 train_time:54804ms step_avg:39.60ms
step:1385/2330 train_time:54826ms step_avg:39.59ms
step:1386/2330 train_time:54883ms step_avg:39.60ms
step:1387/2330 train_time:54905ms step_avg:39.59ms
step:1388/2330 train_time:54961ms step_avg:39.60ms
step:1389/2330 train_time:54983ms step_avg:39.58ms
step:1390/2330 train_time:55039ms step_avg:39.60ms
step:1391/2330 train_time:55063ms step_avg:39.59ms
step:1392/2330 train_time:55119ms step_avg:39.60ms
step:1393/2330 train_time:55142ms step_avg:39.58ms
step:1394/2330 train_time:55198ms step_avg:39.60ms
step:1395/2330 train_time:55222ms step_avg:39.59ms
step:1396/2330 train_time:55278ms step_avg:39.60ms
step:1397/2330 train_time:55301ms step_avg:39.59ms
step:1398/2330 train_time:55357ms step_avg:39.60ms
step:1399/2330 train_time:55380ms step_avg:39.59ms
step:1400/2330 train_time:55436ms step_avg:39.60ms
step:1401/2330 train_time:55459ms step_avg:39.59ms
step:1402/2330 train_time:55516ms step_avg:39.60ms
step:1403/2330 train_time:55538ms step_avg:39.59ms
step:1404/2330 train_time:55594ms step_avg:39.60ms
step:1405/2330 train_time:55617ms step_avg:39.58ms
step:1406/2330 train_time:55673ms step_avg:39.60ms
step:1407/2330 train_time:55695ms step_avg:39.58ms
step:1408/2330 train_time:55751ms step_avg:39.60ms
step:1409/2330 train_time:55773ms step_avg:39.58ms
step:1410/2330 train_time:55830ms step_avg:39.60ms
step:1411/2330 train_time:55853ms step_avg:39.58ms
step:1412/2330 train_time:55909ms step_avg:39.60ms
step:1413/2330 train_time:55931ms step_avg:39.58ms
step:1414/2330 train_time:55988ms step_avg:39.60ms
step:1415/2330 train_time:56010ms step_avg:39.58ms
step:1416/2330 train_time:56067ms step_avg:39.60ms
step:1417/2330 train_time:56089ms step_avg:39.58ms
step:1418/2330 train_time:56147ms step_avg:39.60ms
step:1419/2330 train_time:56169ms step_avg:39.58ms
step:1420/2330 train_time:56225ms step_avg:39.60ms
step:1421/2330 train_time:56248ms step_avg:39.58ms
step:1422/2330 train_time:56305ms step_avg:39.60ms
step:1423/2330 train_time:56327ms step_avg:39.58ms
step:1424/2330 train_time:56384ms step_avg:39.60ms
step:1425/2330 train_time:56406ms step_avg:39.58ms
step:1426/2330 train_time:56463ms step_avg:39.60ms
step:1427/2330 train_time:56485ms step_avg:39.58ms
step:1428/2330 train_time:56541ms step_avg:39.59ms
step:1429/2330 train_time:56563ms step_avg:39.58ms
step:1430/2330 train_time:56619ms step_avg:39.59ms
step:1431/2330 train_time:56642ms step_avg:39.58ms
step:1432/2330 train_time:56698ms step_avg:39.59ms
step:1433/2330 train_time:56721ms step_avg:39.58ms
step:1434/2330 train_time:56777ms step_avg:39.59ms
step:1435/2330 train_time:56801ms step_avg:39.58ms
step:1436/2330 train_time:56858ms step_avg:39.59ms
step:1437/2330 train_time:56882ms step_avg:39.58ms
step:1438/2330 train_time:56937ms step_avg:39.59ms
step:1439/2330 train_time:56960ms step_avg:39.58ms
step:1440/2330 train_time:57016ms step_avg:39.59ms
step:1441/2330 train_time:57039ms step_avg:39.58ms
step:1442/2330 train_time:57095ms step_avg:39.59ms
step:1443/2330 train_time:57118ms step_avg:39.58ms
step:1444/2330 train_time:57174ms step_avg:39.59ms
step:1445/2330 train_time:57196ms step_avg:39.58ms
step:1446/2330 train_time:57252ms step_avg:39.59ms
step:1447/2330 train_time:57275ms step_avg:39.58ms
step:1448/2330 train_time:57330ms step_avg:39.59ms
step:1449/2330 train_time:57353ms step_avg:39.58ms
step:1450/2330 train_time:57410ms step_avg:39.59ms
step:1451/2330 train_time:57432ms step_avg:39.58ms
step:1452/2330 train_time:57489ms step_avg:39.59ms
step:1453/2330 train_time:57511ms step_avg:39.58ms
step:1454/2330 train_time:57567ms step_avg:39.59ms
step:1455/2330 train_time:57590ms step_avg:39.58ms
step:1456/2330 train_time:57647ms step_avg:39.59ms
step:1457/2330 train_time:57669ms step_avg:39.58ms
step:1458/2330 train_time:57726ms step_avg:39.59ms
step:1459/2330 train_time:57748ms step_avg:39.58ms
step:1460/2330 train_time:57805ms step_avg:39.59ms
step:1461/2330 train_time:57827ms step_avg:39.58ms
step:1462/2330 train_time:57883ms step_avg:39.59ms
step:1463/2330 train_time:57905ms step_avg:39.58ms
step:1464/2330 train_time:57961ms step_avg:39.59ms
step:1465/2330 train_time:57983ms step_avg:39.58ms
step:1466/2330 train_time:58039ms step_avg:39.59ms
step:1467/2330 train_time:58062ms step_avg:39.58ms
step:1468/2330 train_time:58118ms step_avg:39.59ms
step:1469/2330 train_time:58141ms step_avg:39.58ms
step:1470/2330 train_time:58197ms step_avg:39.59ms
step:1471/2330 train_time:58220ms step_avg:39.58ms
step:1472/2330 train_time:58277ms step_avg:39.59ms
step:1473/2330 train_time:58300ms step_avg:39.58ms
step:1474/2330 train_time:58356ms step_avg:39.59ms
step:1475/2330 train_time:58378ms step_avg:39.58ms
step:1476/2330 train_time:58434ms step_avg:39.59ms
step:1477/2330 train_time:58457ms step_avg:39.58ms
step:1478/2330 train_time:58512ms step_avg:39.59ms
step:1479/2330 train_time:58535ms step_avg:39.58ms
step:1480/2330 train_time:58592ms step_avg:39.59ms
step:1481/2330 train_time:58615ms step_avg:39.58ms
step:1482/2330 train_time:58671ms step_avg:39.59ms
step:1483/2330 train_time:58694ms step_avg:39.58ms
step:1484/2330 train_time:58750ms step_avg:39.59ms
step:1485/2330 train_time:58773ms step_avg:39.58ms
step:1486/2330 train_time:58830ms step_avg:39.59ms
step:1487/2330 train_time:58852ms step_avg:39.58ms
step:1488/2330 train_time:58909ms step_avg:39.59ms
step:1489/2330 train_time:58931ms step_avg:39.58ms
step:1490/2330 train_time:58988ms step_avg:39.59ms
step:1491/2330 train_time:59010ms step_avg:39.58ms
step:1492/2330 train_time:59066ms step_avg:39.59ms
step:1493/2330 train_time:59089ms step_avg:39.58ms
step:1494/2330 train_time:59147ms step_avg:39.59ms
step:1495/2330 train_time:59170ms step_avg:39.58ms
step:1496/2330 train_time:59227ms step_avg:39.59ms
step:1497/2330 train_time:59250ms step_avg:39.58ms
step:1498/2330 train_time:59306ms step_avg:39.59ms
step:1499/2330 train_time:59329ms step_avg:39.58ms
step:1500/2330 train_time:59386ms step_avg:39.59ms
step:1500/2330 val_loss:5.1758 train_time:59479ms step_avg:39.65ms
step:1501/2330 train_time:59491ms step_avg:39.63ms
step:1502/2330 train_time:59503ms step_avg:39.62ms
step:1503/2330 train_time:59513ms step_avg:39.60ms
step:1504/2330 train_time:59543ms step_avg:39.59ms
step:1505/2330 train_time:59564ms step_avg:39.58ms
step:1506/2330 train_time:59619ms step_avg:39.59ms
step:1507/2330 train_time:59641ms step_avg:39.58ms
step:1508/2330 train_time:59696ms step_avg:39.59ms
step:1509/2330 train_time:59717ms step_avg:39.57ms
step:1510/2330 train_time:59773ms step_avg:39.58ms
step:1511/2330 train_time:59798ms step_avg:39.58ms
step:1512/2330 train_time:59858ms step_avg:39.59ms
step:1513/2330 train_time:59880ms step_avg:39.58ms
step:1514/2330 train_time:59938ms step_avg:39.59ms
step:1515/2330 train_time:59960ms step_avg:39.58ms
step:1516/2330 train_time:60016ms step_avg:39.59ms
step:1517/2330 train_time:60038ms step_avg:39.58ms
step:1518/2330 train_time:60094ms step_avg:39.59ms
step:1519/2330 train_time:60115ms step_avg:39.58ms
step:1520/2330 train_time:60171ms step_avg:39.59ms
step:1521/2330 train_time:60193ms step_avg:39.57ms
step:1522/2330 train_time:60248ms step_avg:39.58ms
step:1523/2330 train_time:60270ms step_avg:39.57ms
step:1524/2330 train_time:60326ms step_avg:39.58ms
step:1525/2330 train_time:60348ms step_avg:39.57ms
step:1526/2330 train_time:60405ms step_avg:39.58ms
step:1527/2330 train_time:60428ms step_avg:39.57ms
step:1528/2330 train_time:60484ms step_avg:39.58ms
step:1529/2330 train_time:60508ms step_avg:39.57ms
step:1530/2330 train_time:60565ms step_avg:39.58ms
step:1531/2330 train_time:60586ms step_avg:39.57ms
step:1532/2330 train_time:60642ms step_avg:39.58ms
step:1533/2330 train_time:60664ms step_avg:39.57ms
step:1534/2330 train_time:60720ms step_avg:39.58ms
step:1535/2330 train_time:60743ms step_avg:39.57ms
step:1536/2330 train_time:60799ms step_avg:39.58ms
step:1537/2330 train_time:60824ms step_avg:39.57ms
step:1538/2330 train_time:60881ms step_avg:39.58ms
step:1539/2330 train_time:60904ms step_avg:39.57ms
step:1540/2330 train_time:60960ms step_avg:39.58ms
step:1541/2330 train_time:60983ms step_avg:39.57ms
step:1542/2330 train_time:61039ms step_avg:39.58ms
step:1543/2330 train_time:61062ms step_avg:39.57ms
step:1544/2330 train_time:61118ms step_avg:39.58ms
step:1545/2330 train_time:61141ms step_avg:39.57ms
step:1546/2330 train_time:61197ms step_avg:39.58ms
step:1547/2330 train_time:61219ms step_avg:39.57ms
step:1548/2330 train_time:61275ms step_avg:39.58ms
step:1549/2330 train_time:61297ms step_avg:39.57ms
step:1550/2330 train_time:61353ms step_avg:39.58ms
step:1551/2330 train_time:61376ms step_avg:39.57ms
step:1552/2330 train_time:61433ms step_avg:39.58ms
step:1553/2330 train_time:61455ms step_avg:39.57ms
step:1554/2330 train_time:61511ms step_avg:39.58ms
step:1555/2330 train_time:61534ms step_avg:39.57ms
step:1556/2330 train_time:61590ms step_avg:39.58ms
step:1557/2330 train_time:61612ms step_avg:39.57ms
step:1558/2330 train_time:61669ms step_avg:39.58ms
step:1559/2330 train_time:61692ms step_avg:39.57ms
step:1560/2330 train_time:61749ms step_avg:39.58ms
step:1561/2330 train_time:61772ms step_avg:39.57ms
step:1562/2330 train_time:61828ms step_avg:39.58ms
step:1563/2330 train_time:61851ms step_avg:39.57ms
step:1564/2330 train_time:61907ms step_avg:39.58ms
step:1565/2330 train_time:61930ms step_avg:39.57ms
step:1566/2330 train_time:61987ms step_avg:39.58ms
step:1567/2330 train_time:62010ms step_avg:39.57ms
step:1568/2330 train_time:62066ms step_avg:39.58ms
step:1569/2330 train_time:62090ms step_avg:39.57ms
step:1570/2330 train_time:62146ms step_avg:39.58ms
step:1571/2330 train_time:62169ms step_avg:39.57ms
step:1572/2330 train_time:62225ms step_avg:39.58ms
step:1573/2330 train_time:62248ms step_avg:39.57ms
step:1574/2330 train_time:62304ms step_avg:39.58ms
step:1575/2330 train_time:62327ms step_avg:39.57ms
step:1576/2330 train_time:62383ms step_avg:39.58ms
step:1577/2330 train_time:62406ms step_avg:39.57ms
step:1578/2330 train_time:62461ms step_avg:39.58ms
step:1579/2330 train_time:62484ms step_avg:39.57ms
step:1580/2330 train_time:62540ms step_avg:39.58ms
step:1581/2330 train_time:62563ms step_avg:39.57ms
step:1582/2330 train_time:62619ms step_avg:39.58ms
step:1583/2330 train_time:62642ms step_avg:39.57ms
step:1584/2330 train_time:62699ms step_avg:39.58ms
step:1585/2330 train_time:62721ms step_avg:39.57ms
step:1586/2330 train_time:62778ms step_avg:39.58ms
step:1587/2330 train_time:62800ms step_avg:39.57ms
step:1588/2330 train_time:62857ms step_avg:39.58ms
step:1589/2330 train_time:62879ms step_avg:39.57ms
step:1590/2330 train_time:62935ms step_avg:39.58ms
step:1591/2330 train_time:62957ms step_avg:39.57ms
step:1592/2330 train_time:63014ms step_avg:39.58ms
step:1593/2330 train_time:63037ms step_avg:39.57ms
step:1594/2330 train_time:63094ms step_avg:39.58ms
step:1595/2330 train_time:63116ms step_avg:39.57ms
step:1596/2330 train_time:63172ms step_avg:39.58ms
step:1597/2330 train_time:63195ms step_avg:39.57ms
step:1598/2330 train_time:63252ms step_avg:39.58ms
step:1599/2330 train_time:63275ms step_avg:39.57ms
step:1600/2330 train_time:63331ms step_avg:39.58ms
step:1601/2330 train_time:63353ms step_avg:39.57ms
step:1602/2330 train_time:63408ms step_avg:39.58ms
step:1603/2330 train_time:63430ms step_avg:39.57ms
step:1604/2330 train_time:63486ms step_avg:39.58ms
step:1605/2330 train_time:63509ms step_avg:39.57ms
step:1606/2330 train_time:63565ms step_avg:39.58ms
step:1607/2330 train_time:63589ms step_avg:39.57ms
step:1608/2330 train_time:63645ms step_avg:39.58ms
step:1609/2330 train_time:63667ms step_avg:39.57ms
step:1610/2330 train_time:63725ms step_avg:39.58ms
step:1611/2330 train_time:63748ms step_avg:39.57ms
step:1612/2330 train_time:63804ms step_avg:39.58ms
step:1613/2330 train_time:63828ms step_avg:39.57ms
step:1614/2330 train_time:63884ms step_avg:39.58ms
step:1615/2330 train_time:63906ms step_avg:39.57ms
step:1616/2330 train_time:63962ms step_avg:39.58ms
step:1617/2330 train_time:63985ms step_avg:39.57ms
step:1618/2330 train_time:64042ms step_avg:39.58ms
step:1619/2330 train_time:64064ms step_avg:39.57ms
step:1620/2330 train_time:64121ms step_avg:39.58ms
step:1621/2330 train_time:64143ms step_avg:39.57ms
step:1622/2330 train_time:64200ms step_avg:39.58ms
step:1623/2330 train_time:64223ms step_avg:39.57ms
step:1624/2330 train_time:64279ms step_avg:39.58ms
step:1625/2330 train_time:64302ms step_avg:39.57ms
step:1626/2330 train_time:64358ms step_avg:39.58ms
step:1627/2330 train_time:64380ms step_avg:39.57ms
step:1628/2330 train_time:64436ms step_avg:39.58ms
step:1629/2330 train_time:64458ms step_avg:39.57ms
step:1630/2330 train_time:64514ms step_avg:39.58ms
step:1631/2330 train_time:64536ms step_avg:39.57ms
step:1632/2330 train_time:64593ms step_avg:39.58ms
step:1633/2330 train_time:64615ms step_avg:39.57ms
step:1634/2330 train_time:64672ms step_avg:39.58ms
step:1635/2330 train_time:64695ms step_avg:39.57ms
step:1636/2330 train_time:64752ms step_avg:39.58ms
step:1637/2330 train_time:64774ms step_avg:39.57ms
step:1638/2330 train_time:64830ms step_avg:39.58ms
step:1639/2330 train_time:64853ms step_avg:39.57ms
step:1640/2330 train_time:64908ms step_avg:39.58ms
step:1641/2330 train_time:64931ms step_avg:39.57ms
step:1642/2330 train_time:64987ms step_avg:39.58ms
step:1643/2330 train_time:65010ms step_avg:39.57ms
step:1644/2330 train_time:65066ms step_avg:39.58ms
step:1645/2330 train_time:65090ms step_avg:39.57ms
step:1646/2330 train_time:65146ms step_avg:39.58ms
step:1647/2330 train_time:65169ms step_avg:39.57ms
step:1648/2330 train_time:65225ms step_avg:39.58ms
step:1649/2330 train_time:65248ms step_avg:39.57ms
step:1650/2330 train_time:65304ms step_avg:39.58ms
step:1651/2330 train_time:65327ms step_avg:39.57ms
step:1652/2330 train_time:65383ms step_avg:39.58ms
step:1653/2330 train_time:65405ms step_avg:39.57ms
step:1654/2330 train_time:65461ms step_avg:39.58ms
step:1655/2330 train_time:65484ms step_avg:39.57ms
step:1656/2330 train_time:65540ms step_avg:39.58ms
step:1657/2330 train_time:65563ms step_avg:39.57ms
step:1658/2330 train_time:65619ms step_avg:39.58ms
step:1659/2330 train_time:65642ms step_avg:39.57ms
step:1660/2330 train_time:65698ms step_avg:39.58ms
step:1661/2330 train_time:65720ms step_avg:39.57ms
step:1662/2330 train_time:65776ms step_avg:39.58ms
step:1663/2330 train_time:65798ms step_avg:39.57ms
step:1664/2330 train_time:65856ms step_avg:39.58ms
step:1665/2330 train_time:65878ms step_avg:39.57ms
step:1666/2330 train_time:65935ms step_avg:39.58ms
step:1667/2330 train_time:65957ms step_avg:39.57ms
step:1668/2330 train_time:66014ms step_avg:39.58ms
step:1669/2330 train_time:66036ms step_avg:39.57ms
step:1670/2330 train_time:66094ms step_avg:39.58ms
step:1671/2330 train_time:66116ms step_avg:39.57ms
step:1672/2330 train_time:66171ms step_avg:39.58ms
step:1673/2330 train_time:66194ms step_avg:39.57ms
step:1674/2330 train_time:66250ms step_avg:39.58ms
step:1675/2330 train_time:66272ms step_avg:39.57ms
step:1676/2330 train_time:66328ms step_avg:39.57ms
step:1677/2330 train_time:66350ms step_avg:39.56ms
step:1678/2330 train_time:66406ms step_avg:39.57ms
step:1679/2330 train_time:66429ms step_avg:39.56ms
step:1680/2330 train_time:66486ms step_avg:39.57ms
step:1681/2330 train_time:66509ms step_avg:39.56ms
step:1682/2330 train_time:66565ms step_avg:39.57ms
step:1683/2330 train_time:66588ms step_avg:39.57ms
step:1684/2330 train_time:66645ms step_avg:39.58ms
step:1685/2330 train_time:66668ms step_avg:39.57ms
step:1686/2330 train_time:66725ms step_avg:39.58ms
step:1687/2330 train_time:66749ms step_avg:39.57ms
step:1688/2330 train_time:66804ms step_avg:39.58ms
step:1689/2330 train_time:66827ms step_avg:39.57ms
step:1690/2330 train_time:66882ms step_avg:39.58ms
step:1691/2330 train_time:66905ms step_avg:39.57ms
step:1692/2330 train_time:66962ms step_avg:39.58ms
step:1693/2330 train_time:66984ms step_avg:39.57ms
step:1694/2330 train_time:67040ms step_avg:39.57ms
step:1695/2330 train_time:67063ms step_avg:39.57ms
step:1696/2330 train_time:67119ms step_avg:39.57ms
step:1697/2330 train_time:67142ms step_avg:39.57ms
step:1698/2330 train_time:67199ms step_avg:39.58ms
step:1699/2330 train_time:67221ms step_avg:39.56ms
step:1700/2330 train_time:67277ms step_avg:39.57ms
step:1701/2330 train_time:67299ms step_avg:39.56ms
step:1702/2330 train_time:67356ms step_avg:39.57ms
step:1703/2330 train_time:67378ms step_avg:39.56ms
step:1704/2330 train_time:67435ms step_avg:39.57ms
step:1705/2330 train_time:67457ms step_avg:39.56ms
step:1706/2330 train_time:67514ms step_avg:39.57ms
step:1707/2330 train_time:67536ms step_avg:39.56ms
step:1708/2330 train_time:67593ms step_avg:39.57ms
step:1709/2330 train_time:67616ms step_avg:39.56ms
step:1710/2330 train_time:67673ms step_avg:39.57ms
step:1711/2330 train_time:67695ms step_avg:39.56ms
step:1712/2330 train_time:67751ms step_avg:39.57ms
step:1713/2330 train_time:67773ms step_avg:39.56ms
step:1714/2330 train_time:67829ms step_avg:39.57ms
step:1715/2330 train_time:67852ms step_avg:39.56ms
step:1716/2330 train_time:67908ms step_avg:39.57ms
step:1717/2330 train_time:67931ms step_avg:39.56ms
step:1718/2330 train_time:67987ms step_avg:39.57ms
step:1719/2330 train_time:68010ms step_avg:39.56ms
step:1720/2330 train_time:68066ms step_avg:39.57ms
step:1721/2330 train_time:68089ms step_avg:39.56ms
step:1722/2330 train_time:68146ms step_avg:39.57ms
step:1723/2330 train_time:68168ms step_avg:39.56ms
step:1724/2330 train_time:68224ms step_avg:39.57ms
step:1725/2330 train_time:68247ms step_avg:39.56ms
step:1726/2330 train_time:68303ms step_avg:39.57ms
step:1727/2330 train_time:68325ms step_avg:39.56ms
step:1728/2330 train_time:68380ms step_avg:39.57ms
step:1729/2330 train_time:68404ms step_avg:39.56ms
step:1730/2330 train_time:68460ms step_avg:39.57ms
step:1731/2330 train_time:68482ms step_avg:39.56ms
step:1732/2330 train_time:68539ms step_avg:39.57ms
step:1733/2330 train_time:68562ms step_avg:39.56ms
step:1734/2330 train_time:68618ms step_avg:39.57ms
step:1735/2330 train_time:68640ms step_avg:39.56ms
step:1736/2330 train_time:68696ms step_avg:39.57ms
step:1737/2330 train_time:68719ms step_avg:39.56ms
step:1738/2330 train_time:68775ms step_avg:39.57ms
step:1739/2330 train_time:68798ms step_avg:39.56ms
step:1740/2330 train_time:68855ms step_avg:39.57ms
step:1741/2330 train_time:68877ms step_avg:39.56ms
step:1742/2330 train_time:68933ms step_avg:39.57ms
step:1743/2330 train_time:68956ms step_avg:39.56ms
step:1744/2330 train_time:69013ms step_avg:39.57ms
step:1745/2330 train_time:69036ms step_avg:39.56ms
step:1746/2330 train_time:69093ms step_avg:39.57ms
step:1747/2330 train_time:69115ms step_avg:39.56ms
step:1748/2330 train_time:69171ms step_avg:39.57ms
step:1749/2330 train_time:69193ms step_avg:39.56ms
step:1750/2330 train_time:69249ms step_avg:39.57ms
step:1750/2330 val_loss:5.1582 train_time:69344ms step_avg:39.63ms
step:1751/2330 train_time:69358ms step_avg:39.61ms
step:1752/2330 train_time:69370ms step_avg:39.59ms
step:1753/2330 train_time:69380ms step_avg:39.58ms
step:1754/2330 train_time:69408ms step_avg:39.57ms
step:1755/2330 train_time:69430ms step_avg:39.56ms
step:1756/2330 train_time:69485ms step_avg:39.57ms
step:1757/2330 train_time:69507ms step_avg:39.56ms
step:1758/2330 train_time:69562ms step_avg:39.57ms
step:1759/2330 train_time:69584ms step_avg:39.56ms
step:1760/2330 train_time:69640ms step_avg:39.57ms
step:1761/2330 train_time:69664ms step_avg:39.56ms
step:1762/2330 train_time:69723ms step_avg:39.57ms
step:1763/2330 train_time:69749ms step_avg:39.56ms
step:1764/2330 train_time:69806ms step_avg:39.57ms
step:1765/2330 train_time:69829ms step_avg:39.56ms
step:1766/2330 train_time:69885ms step_avg:39.57ms
step:1767/2330 train_time:69908ms step_avg:39.56ms
step:1768/2330 train_time:69964ms step_avg:39.57ms
step:1769/2330 train_time:69986ms step_avg:39.56ms
step:1770/2330 train_time:70042ms step_avg:39.57ms
step:1771/2330 train_time:70065ms step_avg:39.56ms
step:1772/2330 train_time:70120ms step_avg:39.57ms
step:1773/2330 train_time:70142ms step_avg:39.56ms
step:1774/2330 train_time:70199ms step_avg:39.57ms
step:1775/2330 train_time:70221ms step_avg:39.56ms
step:1776/2330 train_time:70278ms step_avg:39.57ms
step:1777/2330 train_time:70300ms step_avg:39.56ms
step:1778/2330 train_time:70358ms step_avg:39.57ms
step:1779/2330 train_time:70380ms step_avg:39.56ms
step:1780/2330 train_time:70437ms step_avg:39.57ms
step:1781/2330 train_time:70459ms step_avg:39.56ms
step:1782/2330 train_time:70515ms step_avg:39.57ms
step:1783/2330 train_time:70537ms step_avg:39.56ms
step:1784/2330 train_time:70593ms step_avg:39.57ms
step:1785/2330 train_time:70616ms step_avg:39.56ms
step:1786/2330 train_time:70673ms step_avg:39.57ms
step:1787/2330 train_time:70695ms step_avg:39.56ms
step:1788/2330 train_time:70751ms step_avg:39.57ms
step:1789/2330 train_time:70774ms step_avg:39.56ms
step:1790/2330 train_time:70830ms step_avg:39.57ms
step:1791/2330 train_time:70853ms step_avg:39.56ms
step:1792/2330 train_time:70909ms step_avg:39.57ms
step:1793/2330 train_time:70932ms step_avg:39.56ms
step:1794/2330 train_time:70987ms step_avg:39.57ms
step:1795/2330 train_time:71011ms step_avg:39.56ms
step:1796/2330 train_time:71067ms step_avg:39.57ms
step:1797/2330 train_time:71090ms step_avg:39.56ms
step:1798/2330 train_time:71146ms step_avg:39.57ms
step:1799/2330 train_time:71169ms step_avg:39.56ms
step:1800/2330 train_time:71225ms step_avg:39.57ms
step:1801/2330 train_time:71248ms step_avg:39.56ms
step:1802/2330 train_time:71304ms step_avg:39.57ms
step:1803/2330 train_time:71326ms step_avg:39.56ms
step:1804/2330 train_time:71382ms step_avg:39.57ms
step:1805/2330 train_time:71405ms step_avg:39.56ms
step:1806/2330 train_time:71461ms step_avg:39.57ms
step:1807/2330 train_time:71483ms step_avg:39.56ms
step:1808/2330 train_time:71539ms step_avg:39.57ms
step:1809/2330 train_time:71562ms step_avg:39.56ms
step:1810/2330 train_time:71619ms step_avg:39.57ms
step:1811/2330 train_time:71641ms step_avg:39.56ms
step:1812/2330 train_time:71697ms step_avg:39.57ms
step:1813/2330 train_time:71720ms step_avg:39.56ms
step:1814/2330 train_time:71777ms step_avg:39.57ms
step:1815/2330 train_time:71800ms step_avg:39.56ms
step:1816/2330 train_time:71857ms step_avg:39.57ms
step:1817/2330 train_time:71879ms step_avg:39.56ms
step:1818/2330 train_time:71935ms step_avg:39.57ms
step:1819/2330 train_time:71958ms step_avg:39.56ms
step:1820/2330 train_time:72014ms step_avg:39.57ms
step:1821/2330 train_time:72036ms step_avg:39.56ms
step:1822/2330 train_time:72092ms step_avg:39.57ms
step:1823/2330 train_time:72114ms step_avg:39.56ms
step:1824/2330 train_time:72170ms step_avg:39.57ms
step:1825/2330 train_time:72193ms step_avg:39.56ms
step:1826/2330 train_time:72249ms step_avg:39.57ms
step:1827/2330 train_time:72271ms step_avg:39.56ms
step:1828/2330 train_time:72327ms step_avg:39.57ms
step:1829/2330 train_time:72350ms step_avg:39.56ms
step:1830/2330 train_time:72407ms step_avg:39.57ms
step:1831/2330 train_time:72431ms step_avg:39.56ms
step:1832/2330 train_time:72487ms step_avg:39.57ms
step:1833/2330 train_time:72511ms step_avg:39.56ms
step:1834/2330 train_time:72567ms step_avg:39.57ms
step:1835/2330 train_time:72589ms step_avg:39.56ms
step:1836/2330 train_time:72646ms step_avg:39.57ms
step:1837/2330 train_time:72669ms step_avg:39.56ms
step:1838/2330 train_time:72725ms step_avg:39.57ms
step:1839/2330 train_time:72748ms step_avg:39.56ms
step:1840/2330 train_time:72804ms step_avg:39.57ms
step:1841/2330 train_time:72827ms step_avg:39.56ms
step:1842/2330 train_time:72882ms step_avg:39.57ms
step:1843/2330 train_time:72906ms step_avg:39.56ms
step:1844/2330 train_time:72962ms step_avg:39.57ms
step:1845/2330 train_time:72986ms step_avg:39.56ms
step:1846/2330 train_time:73042ms step_avg:39.57ms
step:1847/2330 train_time:73065ms step_avg:39.56ms
step:1848/2330 train_time:73121ms step_avg:39.57ms
step:1849/2330 train_time:73144ms step_avg:39.56ms
step:1850/2330 train_time:73200ms step_avg:39.57ms
step:1851/2330 train_time:73222ms step_avg:39.56ms
step:1852/2330 train_time:73279ms step_avg:39.57ms
step:1853/2330 train_time:73301ms step_avg:39.56ms
step:1854/2330 train_time:73357ms step_avg:39.57ms
step:1855/2330 train_time:73379ms step_avg:39.56ms
step:1856/2330 train_time:73436ms step_avg:39.57ms
step:1857/2330 train_time:73458ms step_avg:39.56ms
step:1858/2330 train_time:73515ms step_avg:39.57ms
step:1859/2330 train_time:73537ms step_avg:39.56ms
step:1860/2330 train_time:73593ms step_avg:39.57ms
step:1861/2330 train_time:73615ms step_avg:39.56ms
step:1862/2330 train_time:73671ms step_avg:39.57ms
step:1863/2330 train_time:73694ms step_avg:39.56ms
step:1864/2330 train_time:73750ms step_avg:39.57ms
step:1865/2330 train_time:73773ms step_avg:39.56ms
step:1866/2330 train_time:73829ms step_avg:39.57ms
step:1867/2330 train_time:73852ms step_avg:39.56ms
step:1868/2330 train_time:73908ms step_avg:39.57ms
step:1869/2330 train_time:73931ms step_avg:39.56ms
step:1870/2330 train_time:73987ms step_avg:39.57ms
step:1871/2330 train_time:74010ms step_avg:39.56ms
step:1872/2330 train_time:74066ms step_avg:39.57ms
step:1873/2330 train_time:74089ms step_avg:39.56ms
step:1874/2330 train_time:74145ms step_avg:39.57ms
step:1875/2330 train_time:74168ms step_avg:39.56ms
step:1876/2330 train_time:74224ms step_avg:39.57ms
step:1877/2330 train_time:74247ms step_avg:39.56ms
step:1878/2330 train_time:74302ms step_avg:39.56ms
step:1879/2330 train_time:74325ms step_avg:39.56ms
step:1880/2330 train_time:74381ms step_avg:39.56ms
step:1881/2330 train_time:74404ms step_avg:39.56ms
step:1882/2330 train_time:74461ms step_avg:39.56ms
step:1883/2330 train_time:74483ms step_avg:39.56ms
step:1884/2330 train_time:74540ms step_avg:39.56ms
step:1885/2330 train_time:74562ms step_avg:39.56ms
step:1886/2330 train_time:74618ms step_avg:39.56ms
step:1887/2330 train_time:74641ms step_avg:39.56ms
step:1888/2330 train_time:74698ms step_avg:39.56ms
step:1889/2330 train_time:74720ms step_avg:39.56ms
step:1890/2330 train_time:74777ms step_avg:39.56ms
step:1891/2330 train_time:74799ms step_avg:39.56ms
step:1892/2330 train_time:74856ms step_avg:39.56ms
step:1893/2330 train_time:74878ms step_avg:39.56ms
step:1894/2330 train_time:74935ms step_avg:39.56ms
step:1895/2330 train_time:74957ms step_avg:39.55ms
step:1896/2330 train_time:75013ms step_avg:39.56ms
step:1897/2330 train_time:75036ms step_avg:39.55ms
step:1898/2330 train_time:75093ms step_avg:39.56ms
step:1899/2330 train_time:75115ms step_avg:39.55ms
step:1900/2330 train_time:75171ms step_avg:39.56ms
step:1901/2330 train_time:75193ms step_avg:39.55ms
step:1902/2330 train_time:75249ms step_avg:39.56ms
step:1903/2330 train_time:75272ms step_avg:39.55ms
step:1904/2330 train_time:75328ms step_avg:39.56ms
step:1905/2330 train_time:75352ms step_avg:39.55ms
step:1906/2330 train_time:75408ms step_avg:39.56ms
step:1907/2330 train_time:75431ms step_avg:39.55ms
step:1908/2330 train_time:75487ms step_avg:39.56ms
step:1909/2330 train_time:75509ms step_avg:39.55ms
step:1910/2330 train_time:75565ms step_avg:39.56ms
step:1911/2330 train_time:75588ms step_avg:39.55ms
step:1912/2330 train_time:75644ms step_avg:39.56ms
step:1913/2330 train_time:75666ms step_avg:39.55ms
step:1914/2330 train_time:75723ms step_avg:39.56ms
step:1915/2330 train_time:75746ms step_avg:39.55ms
step:1916/2330 train_time:75803ms step_avg:39.56ms
step:1917/2330 train_time:75826ms step_avg:39.55ms
step:1918/2330 train_time:75882ms step_avg:39.56ms
step:1919/2330 train_time:75904ms step_avg:39.55ms
step:1920/2330 train_time:75961ms step_avg:39.56ms
step:1921/2330 train_time:75985ms step_avg:39.55ms
step:1922/2330 train_time:76041ms step_avg:39.56ms
step:1923/2330 train_time:76063ms step_avg:39.55ms
step:1924/2330 train_time:76119ms step_avg:39.56ms
step:1925/2330 train_time:76142ms step_avg:39.55ms
step:1926/2330 train_time:76198ms step_avg:39.56ms
step:1927/2330 train_time:76220ms step_avg:39.55ms
step:1928/2330 train_time:76278ms step_avg:39.56ms
step:1929/2330 train_time:76300ms step_avg:39.55ms
step:1930/2330 train_time:76356ms step_avg:39.56ms
step:1931/2330 train_time:76378ms step_avg:39.55ms
step:1932/2330 train_time:76435ms step_avg:39.56ms
step:1933/2330 train_time:76457ms step_avg:39.55ms
step:1934/2330 train_time:76513ms step_avg:39.56ms
step:1935/2330 train_time:76535ms step_avg:39.55ms
step:1936/2330 train_time:76591ms step_avg:39.56ms
step:1937/2330 train_time:76614ms step_avg:39.55ms
step:1938/2330 train_time:76669ms step_avg:39.56ms
step:1939/2330 train_time:76692ms step_avg:39.55ms
step:1940/2330 train_time:76749ms step_avg:39.56ms
step:1941/2330 train_time:76772ms step_avg:39.55ms
step:1942/2330 train_time:76828ms step_avg:39.56ms
step:1943/2330 train_time:76851ms step_avg:39.55ms
step:1944/2330 train_time:76908ms step_avg:39.56ms
step:1945/2330 train_time:76930ms step_avg:39.55ms
step:1946/2330 train_time:76987ms step_avg:39.56ms
step:1947/2330 train_time:77009ms step_avg:39.55ms
step:1948/2330 train_time:77066ms step_avg:39.56ms
step:1949/2330 train_time:77088ms step_avg:39.55ms
step:1950/2330 train_time:77144ms step_avg:39.56ms
step:1951/2330 train_time:77167ms step_avg:39.55ms
step:1952/2330 train_time:77223ms step_avg:39.56ms
step:1953/2330 train_time:77246ms step_avg:39.55ms
step:1954/2330 train_time:77302ms step_avg:39.56ms
step:1955/2330 train_time:77325ms step_avg:39.55ms
step:1956/2330 train_time:77381ms step_avg:39.56ms
step:1957/2330 train_time:77404ms step_avg:39.55ms
step:1958/2330 train_time:77461ms step_avg:39.56ms
step:1959/2330 train_time:77483ms step_avg:39.55ms
step:1960/2330 train_time:77539ms step_avg:39.56ms
step:1961/2330 train_time:77561ms step_avg:39.55ms
step:1962/2330 train_time:77617ms step_avg:39.56ms
step:1963/2330 train_time:77639ms step_avg:39.55ms
step:1964/2330 train_time:77696ms step_avg:39.56ms
step:1965/2330 train_time:77719ms step_avg:39.55ms
step:1966/2330 train_time:77775ms step_avg:39.56ms
step:1967/2330 train_time:77797ms step_avg:39.55ms
step:1968/2330 train_time:77854ms step_avg:39.56ms
step:1969/2330 train_time:77876ms step_avg:39.55ms
step:1970/2330 train_time:77933ms step_avg:39.56ms
step:1971/2330 train_time:77956ms step_avg:39.55ms
step:1972/2330 train_time:78011ms step_avg:39.56ms
step:1973/2330 train_time:78034ms step_avg:39.55ms
step:1974/2330 train_time:78090ms step_avg:39.56ms
step:1975/2330 train_time:78112ms step_avg:39.55ms
step:1976/2330 train_time:78168ms step_avg:39.56ms
step:1977/2330 train_time:78191ms step_avg:39.55ms
step:1978/2330 train_time:78248ms step_avg:39.56ms
step:1979/2330 train_time:78272ms step_avg:39.55ms
step:1980/2330 train_time:78328ms step_avg:39.56ms
step:1981/2330 train_time:78351ms step_avg:39.55ms
step:1982/2330 train_time:78407ms step_avg:39.56ms
step:1983/2330 train_time:78430ms step_avg:39.55ms
step:1984/2330 train_time:78486ms step_avg:39.56ms
step:1985/2330 train_time:78508ms step_avg:39.55ms
step:1986/2330 train_time:78564ms step_avg:39.56ms
step:1987/2330 train_time:78587ms step_avg:39.55ms
step:1988/2330 train_time:78643ms step_avg:39.56ms
step:1989/2330 train_time:78665ms step_avg:39.55ms
step:1990/2330 train_time:78721ms step_avg:39.56ms
step:1991/2330 train_time:78744ms step_avg:39.55ms
step:1992/2330 train_time:78801ms step_avg:39.56ms
step:1993/2330 train_time:78824ms step_avg:39.55ms
step:1994/2330 train_time:78880ms step_avg:39.56ms
step:1995/2330 train_time:78903ms step_avg:39.55ms
step:1996/2330 train_time:78959ms step_avg:39.56ms
step:1997/2330 train_time:78982ms step_avg:39.55ms
step:1998/2330 train_time:79038ms step_avg:39.56ms
step:1999/2330 train_time:79060ms step_avg:39.55ms
step:2000/2330 train_time:79117ms step_avg:39.56ms
step:2000/2330 val_loss:5.1433 train_time:79213ms step_avg:39.61ms
step:2001/2330 train_time:79226ms step_avg:39.59ms
step:2002/2330 train_time:79238ms step_avg:39.58ms
step:2003/2330 train_time:79248ms step_avg:39.56ms
step:2004/2330 train_time:79277ms step_avg:39.56ms
step:2005/2330 train_time:79298ms step_avg:39.55ms
step:2006/2330 train_time:79353ms step_avg:39.56ms
step:2007/2330 train_time:79375ms step_avg:39.55ms
step:2008/2330 train_time:79430ms step_avg:39.56ms
step:2009/2330 train_time:79452ms step_avg:39.55ms
step:2010/2330 train_time:79508ms step_avg:39.56ms
step:2011/2330 train_time:79532ms step_avg:39.55ms
step:2012/2330 train_time:79593ms step_avg:39.56ms
step:2013/2330 train_time:79617ms step_avg:39.55ms
step:2014/2330 train_time:79674ms step_avg:39.56ms
step:2015/2330 train_time:79698ms step_avg:39.55ms
step:2016/2330 train_time:79754ms step_avg:39.56ms
step:2017/2330 train_time:79776ms step_avg:39.55ms
step:2018/2330 train_time:79832ms step_avg:39.56ms
step:2019/2330 train_time:79855ms step_avg:39.55ms
step:2020/2330 train_time:79910ms step_avg:39.56ms
step:2021/2330 train_time:79933ms step_avg:39.55ms
step:2022/2330 train_time:79988ms step_avg:39.56ms
step:2023/2330 train_time:80009ms step_avg:39.55ms
step:2024/2330 train_time:80065ms step_avg:39.56ms
step:2025/2330 train_time:80087ms step_avg:39.55ms
step:2026/2330 train_time:80142ms step_avg:39.56ms
step:2027/2330 train_time:80164ms step_avg:39.55ms
step:2028/2330 train_time:80220ms step_avg:39.56ms
step:2029/2330 train_time:80242ms step_avg:39.55ms
step:2030/2330 train_time:80297ms step_avg:39.56ms
step:2031/2330 train_time:80319ms step_avg:39.55ms
step:2032/2330 train_time:80375ms step_avg:39.55ms
step:2033/2330 train_time:80397ms step_avg:39.55ms
step:2034/2330 train_time:80453ms step_avg:39.55ms
step:2035/2330 train_time:80476ms step_avg:39.55ms
step:2036/2330 train_time:80534ms step_avg:39.55ms
step:2037/2330 train_time:80557ms step_avg:39.55ms
step:2038/2330 train_time:80613ms step_avg:39.56ms
step:2039/2330 train_time:80636ms step_avg:39.55ms
step:2040/2330 train_time:80693ms step_avg:39.56ms
step:2041/2330 train_time:80716ms step_avg:39.55ms
step:2042/2330 train_time:80772ms step_avg:39.56ms
step:2043/2330 train_time:80795ms step_avg:39.55ms
step:2044/2330 train_time:80852ms step_avg:39.56ms
step:2045/2330 train_time:80874ms step_avg:39.55ms
step:2046/2330 train_time:80930ms step_avg:39.56ms
step:2047/2330 train_time:80952ms step_avg:39.55ms
step:2048/2330 train_time:81008ms step_avg:39.55ms
step:2049/2330 train_time:81031ms step_avg:39.55ms
step:2050/2330 train_time:81086ms step_avg:39.55ms
step:2051/2330 train_time:81108ms step_avg:39.55ms
step:2052/2330 train_time:81164ms step_avg:39.55ms
step:2053/2330 train_time:81187ms step_avg:39.55ms
step:2054/2330 train_time:81243ms step_avg:39.55ms
step:2055/2330 train_time:81265ms step_avg:39.54ms
step:2056/2330 train_time:81321ms step_avg:39.55ms
step:2057/2330 train_time:81343ms step_avg:39.54ms
step:2058/2330 train_time:81399ms step_avg:39.55ms
step:2059/2330 train_time:81421ms step_avg:39.54ms
step:2060/2330 train_time:81477ms step_avg:39.55ms
step:2061/2330 train_time:81499ms step_avg:39.54ms
step:2062/2330 train_time:81555ms step_avg:39.55ms
step:2063/2330 train_time:81578ms step_avg:39.54ms
step:2064/2330 train_time:81636ms step_avg:39.55ms
step:2065/2330 train_time:81659ms step_avg:39.54ms
step:2066/2330 train_time:81715ms step_avg:39.55ms
step:2067/2330 train_time:81738ms step_avg:39.54ms
step:2068/2330 train_time:81793ms step_avg:39.55ms
step:2069/2330 train_time:81816ms step_avg:39.54ms
step:2070/2330 train_time:81872ms step_avg:39.55ms
step:2071/2330 train_time:81895ms step_avg:39.54ms
step:2072/2330 train_time:81950ms step_avg:39.55ms
step:2073/2330 train_time:81973ms step_avg:39.54ms
step:2074/2330 train_time:82029ms step_avg:39.55ms
step:2075/2330 train_time:82052ms step_avg:39.54ms
step:2076/2330 train_time:82108ms step_avg:39.55ms
step:2077/2330 train_time:82131ms step_avg:39.54ms
step:2078/2330 train_time:82187ms step_avg:39.55ms
step:2079/2330 train_time:82209ms step_avg:39.54ms
step:2080/2330 train_time:82266ms step_avg:39.55ms
step:2081/2330 train_time:82288ms step_avg:39.54ms
step:2082/2330 train_time:82345ms step_avg:39.55ms
step:2083/2330 train_time:82366ms step_avg:39.54ms
step:2084/2330 train_time:82423ms step_avg:39.55ms
step:2085/2330 train_time:82445ms step_avg:39.54ms
step:2086/2330 train_time:82502ms step_avg:39.55ms
step:2087/2330 train_time:82525ms step_avg:39.54ms
step:2088/2330 train_time:82581ms step_avg:39.55ms
step:2089/2330 train_time:82604ms step_avg:39.54ms
step:2090/2330 train_time:82660ms step_avg:39.55ms
step:2091/2330 train_time:82683ms step_avg:39.54ms
step:2092/2330 train_time:82739ms step_avg:39.55ms
step:2093/2330 train_time:82762ms step_avg:39.54ms
step:2094/2330 train_time:82817ms step_avg:39.55ms
step:2095/2330 train_time:82840ms step_avg:39.54ms
step:2096/2330 train_time:82896ms step_avg:39.55ms
step:2097/2330 train_time:82919ms step_avg:39.54ms
step:2098/2330 train_time:82975ms step_avg:39.55ms
step:2099/2330 train_time:82998ms step_avg:39.54ms
step:2100/2330 train_time:83054ms step_avg:39.55ms
step:2101/2330 train_time:83077ms step_avg:39.54ms
step:2102/2330 train_time:83133ms step_avg:39.55ms
step:2103/2330 train_time:83155ms step_avg:39.54ms
step:2104/2330 train_time:83211ms step_avg:39.55ms
step:2105/2330 train_time:83234ms step_avg:39.54ms
step:2106/2330 train_time:83290ms step_avg:39.55ms
step:2107/2330 train_time:83313ms step_avg:39.54ms
step:2108/2330 train_time:83369ms step_avg:39.55ms
step:2109/2330 train_time:83392ms step_avg:39.54ms
step:2110/2330 train_time:83449ms step_avg:39.55ms
step:2111/2330 train_time:83471ms step_avg:39.54ms
step:2112/2330 train_time:83528ms step_avg:39.55ms
step:2113/2330 train_time:83551ms step_avg:39.54ms
step:2114/2330 train_time:83607ms step_avg:39.55ms
step:2115/2330 train_time:83630ms step_avg:39.54ms
step:2116/2330 train_time:83686ms step_avg:39.55ms
step:2117/2330 train_time:83709ms step_avg:39.54ms
step:2118/2330 train_time:83766ms step_avg:39.55ms
step:2119/2330 train_time:83788ms step_avg:39.54ms
step:2120/2330 train_time:83845ms step_avg:39.55ms
step:2121/2330 train_time:83867ms step_avg:39.54ms
step:2122/2330 train_time:83924ms step_avg:39.55ms
step:2123/2330 train_time:83947ms step_avg:39.54ms
step:2124/2330 train_time:84004ms step_avg:39.55ms
step:2125/2330 train_time:84025ms step_avg:39.54ms
step:2126/2330 train_time:84082ms step_avg:39.55ms
step:2127/2330 train_time:84103ms step_avg:39.54ms
step:2128/2330 train_time:84159ms step_avg:39.55ms
step:2129/2330 train_time:84181ms step_avg:39.54ms
step:2130/2330 train_time:84237ms step_avg:39.55ms
step:2131/2330 train_time:84259ms step_avg:39.54ms
step:2132/2330 train_time:84316ms step_avg:39.55ms
step:2133/2330 train_time:84339ms step_avg:39.54ms
step:2134/2330 train_time:84395ms step_avg:39.55ms
step:2135/2330 train_time:84418ms step_avg:39.54ms
step:2136/2330 train_time:84474ms step_avg:39.55ms
step:2137/2330 train_time:84497ms step_avg:39.54ms
step:2138/2330 train_time:84553ms step_avg:39.55ms
step:2139/2330 train_time:84575ms step_avg:39.54ms
step:2140/2330 train_time:84631ms step_avg:39.55ms
step:2141/2330 train_time:84654ms step_avg:39.54ms
step:2142/2330 train_time:84711ms step_avg:39.55ms
step:2143/2330 train_time:84734ms step_avg:39.54ms
step:2144/2330 train_time:84790ms step_avg:39.55ms
step:2145/2330 train_time:84813ms step_avg:39.54ms
step:2146/2330 train_time:84869ms step_avg:39.55ms
step:2147/2330 train_time:84892ms step_avg:39.54ms
step:2148/2330 train_time:84948ms step_avg:39.55ms
step:2149/2330 train_time:84970ms step_avg:39.54ms
step:2150/2330 train_time:85026ms step_avg:39.55ms
step:2151/2330 train_time:85049ms step_avg:39.54ms
step:2152/2330 train_time:85106ms step_avg:39.55ms
step:2153/2330 train_time:85128ms step_avg:39.54ms
step:2154/2330 train_time:85184ms step_avg:39.55ms
step:2155/2330 train_time:85206ms step_avg:39.54ms
step:2156/2330 train_time:85263ms step_avg:39.55ms
step:2157/2330 train_time:85284ms step_avg:39.54ms
step:2158/2330 train_time:85341ms step_avg:39.55ms
step:2159/2330 train_time:85364ms step_avg:39.54ms
step:2160/2330 train_time:85421ms step_avg:39.55ms
step:2161/2330 train_time:85442ms step_avg:39.54ms
step:2162/2330 train_time:85499ms step_avg:39.55ms
step:2163/2330 train_time:85521ms step_avg:39.54ms
step:2164/2330 train_time:85577ms step_avg:39.55ms
step:2165/2330 train_time:85600ms step_avg:39.54ms
step:2166/2330 train_time:85656ms step_avg:39.55ms
step:2167/2330 train_time:85679ms step_avg:39.54ms
step:2168/2330 train_time:85736ms step_avg:39.55ms
step:2169/2330 train_time:85759ms step_avg:39.54ms
step:2170/2330 train_time:85814ms step_avg:39.55ms
step:2171/2330 train_time:85837ms step_avg:39.54ms
step:2172/2330 train_time:85894ms step_avg:39.55ms
step:2173/2330 train_time:85916ms step_avg:39.54ms
step:2174/2330 train_time:85971ms step_avg:39.55ms
step:2175/2330 train_time:85994ms step_avg:39.54ms
step:2176/2330 train_time:86051ms step_avg:39.55ms
step:2177/2330 train_time:86073ms step_avg:39.54ms
step:2178/2330 train_time:86129ms step_avg:39.55ms
step:2179/2330 train_time:86153ms step_avg:39.54ms
step:2180/2330 train_time:86209ms step_avg:39.55ms
step:2181/2330 train_time:86231ms step_avg:39.54ms
step:2182/2330 train_time:86288ms step_avg:39.55ms
step:2183/2330 train_time:86311ms step_avg:39.54ms
step:2184/2330 train_time:86367ms step_avg:39.55ms
step:2185/2330 train_time:86389ms step_avg:39.54ms
step:2186/2330 train_time:86446ms step_avg:39.55ms
step:2187/2330 train_time:86468ms step_avg:39.54ms
step:2188/2330 train_time:86525ms step_avg:39.55ms
step:2189/2330 train_time:86547ms step_avg:39.54ms
step:2190/2330 train_time:86604ms step_avg:39.55ms
step:2191/2330 train_time:86627ms step_avg:39.54ms
step:2192/2330 train_time:86684ms step_avg:39.55ms
step:2193/2330 train_time:86706ms step_avg:39.54ms
step:2194/2330 train_time:86763ms step_avg:39.55ms
step:2195/2330 train_time:86785ms step_avg:39.54ms
step:2196/2330 train_time:86841ms step_avg:39.54ms
step:2197/2330 train_time:86863ms step_avg:39.54ms
step:2198/2330 train_time:86919ms step_avg:39.54ms
step:2199/2330 train_time:86941ms step_avg:39.54ms
step:2200/2330 train_time:86997ms step_avg:39.54ms
step:2201/2330 train_time:87020ms step_avg:39.54ms
step:2202/2330 train_time:87076ms step_avg:39.54ms
step:2203/2330 train_time:87098ms step_avg:39.54ms
step:2204/2330 train_time:87155ms step_avg:39.54ms
step:2205/2330 train_time:87178ms step_avg:39.54ms
step:2206/2330 train_time:87234ms step_avg:39.54ms
step:2207/2330 train_time:87257ms step_avg:39.54ms
step:2208/2330 train_time:87312ms step_avg:39.54ms
step:2209/2330 train_time:87335ms step_avg:39.54ms
step:2210/2330 train_time:87390ms step_avg:39.54ms
step:2211/2330 train_time:87413ms step_avg:39.54ms
step:2212/2330 train_time:87469ms step_avg:39.54ms
step:2213/2330 train_time:87492ms step_avg:39.54ms
step:2214/2330 train_time:87548ms step_avg:39.54ms
step:2215/2330 train_time:87571ms step_avg:39.54ms
step:2216/2330 train_time:87628ms step_avg:39.54ms
step:2217/2330 train_time:87650ms step_avg:39.54ms
step:2218/2330 train_time:87706ms step_avg:39.54ms
step:2219/2330 train_time:87728ms step_avg:39.54ms
step:2220/2330 train_time:87785ms step_avg:39.54ms
step:2221/2330 train_time:87807ms step_avg:39.53ms
step:2222/2330 train_time:87864ms step_avg:39.54ms
step:2223/2330 train_time:87886ms step_avg:39.53ms
step:2224/2330 train_time:87943ms step_avg:39.54ms
step:2225/2330 train_time:87964ms step_avg:39.53ms
step:2226/2330 train_time:88020ms step_avg:39.54ms
step:2227/2330 train_time:88043ms step_avg:39.53ms
step:2228/2330 train_time:88099ms step_avg:39.54ms
step:2229/2330 train_time:88122ms step_avg:39.53ms
step:2230/2330 train_time:88177ms step_avg:39.54ms
step:2231/2330 train_time:88201ms step_avg:39.53ms
step:2232/2330 train_time:88257ms step_avg:39.54ms
step:2233/2330 train_time:88280ms step_avg:39.53ms
step:2234/2330 train_time:88336ms step_avg:39.54ms
step:2235/2330 train_time:88359ms step_avg:39.53ms
step:2236/2330 train_time:88415ms step_avg:39.54ms
step:2237/2330 train_time:88438ms step_avg:39.53ms
step:2238/2330 train_time:88494ms step_avg:39.54ms
step:2239/2330 train_time:88517ms step_avg:39.53ms
step:2240/2330 train_time:88573ms step_avg:39.54ms
step:2241/2330 train_time:88596ms step_avg:39.53ms
step:2242/2330 train_time:88653ms step_avg:39.54ms
step:2243/2330 train_time:88676ms step_avg:39.53ms
step:2244/2330 train_time:88732ms step_avg:39.54ms
step:2245/2330 train_time:88754ms step_avg:39.53ms
step:2246/2330 train_time:88810ms step_avg:39.54ms
step:2247/2330 train_time:88833ms step_avg:39.53ms
step:2248/2330 train_time:88889ms step_avg:39.54ms
step:2249/2330 train_time:88912ms step_avg:39.53ms
step:2250/2330 train_time:88969ms step_avg:39.54ms
step:2250/2330 val_loss:5.1316 train_time:89064ms step_avg:39.58ms
step:2251/2330 train_time:89077ms step_avg:39.57ms
step:2252/2330 train_time:89088ms step_avg:39.56ms
step:2253/2330 train_time:89098ms step_avg:39.55ms
step:2254/2330 train_time:89127ms step_avg:39.54ms
step:2255/2330 train_time:89148ms step_avg:39.53ms
step:2256/2330 train_time:89203ms step_avg:39.54ms
step:2257/2330 train_time:89225ms step_avg:39.53ms
step:2258/2330 train_time:89280ms step_avg:39.54ms
step:2259/2330 train_time:89302ms step_avg:39.53ms
step:2260/2330 train_time:89360ms step_avg:39.54ms
step:2261/2330 train_time:89385ms step_avg:39.53ms
step:2262/2330 train_time:89444ms step_avg:39.54ms
step:2263/2330 train_time:89468ms step_avg:39.54ms
step:2264/2330 train_time:89525ms step_avg:39.54ms
step:2265/2330 train_time:89548ms step_avg:39.54ms
step:2266/2330 train_time:89604ms step_avg:39.54ms
step:2267/2330 train_time:89626ms step_avg:39.54ms
step:2268/2330 train_time:89682ms step_avg:39.54ms
step:2269/2330 train_time:89704ms step_avg:39.53ms
step:2270/2330 train_time:89759ms step_avg:39.54ms
step:2271/2330 train_time:89781ms step_avg:39.53ms
step:2272/2330 train_time:89836ms step_avg:39.54ms
step:2273/2330 train_time:89858ms step_avg:39.53ms
step:2274/2330 train_time:89913ms step_avg:39.54ms
step:2275/2330 train_time:89935ms step_avg:39.53ms
step:2276/2330 train_time:89992ms step_avg:39.54ms
step:2277/2330 train_time:90015ms step_avg:39.53ms
step:2278/2330 train_time:90072ms step_avg:39.54ms
step:2279/2330 train_time:90094ms step_avg:39.53ms
step:2280/2330 train_time:90151ms step_avg:39.54ms
step:2281/2330 train_time:90173ms step_avg:39.53ms
step:2282/2330 train_time:90229ms step_avg:39.54ms
step:2283/2330 train_time:90251ms step_avg:39.53ms
step:2284/2330 train_time:90307ms step_avg:39.54ms
step:2285/2330 train_time:90330ms step_avg:39.53ms
step:2286/2330 train_time:90387ms step_avg:39.54ms
step:2287/2330 train_time:90410ms step_avg:39.53ms
step:2288/2330 train_time:90466ms step_avg:39.54ms
step:2289/2330 train_time:90489ms step_avg:39.53ms
step:2290/2330 train_time:90545ms step_avg:39.54ms
step:2291/2330 train_time:90568ms step_avg:39.53ms
step:2292/2330 train_time:90624ms step_avg:39.54ms
step:2293/2330 train_time:90646ms step_avg:39.53ms
step:2294/2330 train_time:90701ms step_avg:39.54ms
step:2295/2330 train_time:90724ms step_avg:39.53ms
step:2296/2330 train_time:90779ms step_avg:39.54ms
step:2297/2330 train_time:90801ms step_avg:39.53ms
step:2298/2330 train_time:90857ms step_avg:39.54ms
step:2299/2330 train_time:90879ms step_avg:39.53ms
step:2300/2330 train_time:90935ms step_avg:39.54ms
step:2301/2330 train_time:90957ms step_avg:39.53ms
step:2302/2330 train_time:91013ms step_avg:39.54ms
step:2303/2330 train_time:91036ms step_avg:39.53ms
step:2304/2330 train_time:91092ms step_avg:39.54ms
step:2305/2330 train_time:91114ms step_avg:39.53ms
step:2306/2330 train_time:91170ms step_avg:39.54ms
step:2307/2330 train_time:91192ms step_avg:39.53ms
step:2308/2330 train_time:91248ms step_avg:39.54ms
step:2309/2330 train_time:91271ms step_avg:39.53ms
step:2310/2330 train_time:91327ms step_avg:39.54ms
step:2311/2330 train_time:91349ms step_avg:39.53ms
step:2312/2330 train_time:91405ms step_avg:39.54ms
step:2313/2330 train_time:91427ms step_avg:39.53ms
step:2314/2330 train_time:91483ms step_avg:39.53ms
step:2315/2330 train_time:91506ms step_avg:39.53ms
step:2316/2330 train_time:91563ms step_avg:39.53ms
step:2317/2330 train_time:91585ms step_avg:39.53ms
step:2318/2330 train_time:91641ms step_avg:39.53ms
step:2319/2330 train_time:91664ms step_avg:39.53ms
step:2320/2330 train_time:91720ms step_avg:39.53ms
step:2321/2330 train_time:91742ms step_avg:39.53ms
step:2322/2330 train_time:91798ms step_avg:39.53ms
step:2323/2330 train_time:91820ms step_avg:39.53ms
step:2324/2330 train_time:91877ms step_avg:39.53ms
step:2325/2330 train_time:91899ms step_avg:39.53ms
step:2326/2330 train_time:91955ms step_avg:39.53ms
step:2327/2330 train_time:91977ms step_avg:39.53ms
step:2328/2330 train_time:92033ms step_avg:39.53ms
step:2329/2330 train_time:92056ms step_avg:39.53ms
step:2330/2330 train_time:92112ms step_avg:39.53ms
step:2330/2330 val_loss:5.1218 train_time:92209ms step_avg:39.57ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
