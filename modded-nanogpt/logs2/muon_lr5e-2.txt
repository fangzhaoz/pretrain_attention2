import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-2"
    os.makedirs("logs2", exist_ok=True)
    logfile = f"logs2/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs2/{run_id}", exist_ok=True)
            torch.save(log, f"logs2/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 06:31:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:87.33ms
step:2/2330 train_time:183ms step_avg:91.35ms
step:3/2330 train_time:206ms step_avg:68.53ms
step:4/2330 train_time:240ms step_avg:60.08ms
step:5/2330 train_time:298ms step_avg:59.57ms
step:6/2330 train_time:360ms step_avg:59.95ms
step:7/2330 train_time:419ms step_avg:59.82ms
step:8/2330 train_time:481ms step_avg:60.07ms
step:9/2330 train_time:539ms step_avg:59.91ms
step:10/2330 train_time:601ms step_avg:60.11ms
step:11/2330 train_time:660ms step_avg:59.96ms
step:12/2330 train_time:721ms step_avg:60.11ms
step:13/2330 train_time:780ms step_avg:59.98ms
step:14/2330 train_time:842ms step_avg:60.11ms
step:15/2330 train_time:900ms step_avg:60.01ms
step:16/2330 train_time:962ms step_avg:60.13ms
step:17/2330 train_time:1022ms step_avg:60.09ms
step:18/2330 train_time:1085ms step_avg:60.30ms
step:19/2330 train_time:1148ms step_avg:60.41ms
step:20/2330 train_time:1213ms step_avg:60.64ms
step:21/2330 train_time:1273ms step_avg:60.64ms
step:22/2330 train_time:1336ms step_avg:60.74ms
step:23/2330 train_time:1396ms step_avg:60.67ms
step:24/2330 train_time:1458ms step_avg:60.74ms
step:25/2330 train_time:1517ms step_avg:60.69ms
step:26/2330 train_time:1579ms step_avg:60.74ms
step:27/2330 train_time:1638ms step_avg:60.68ms
step:28/2330 train_time:1701ms step_avg:60.75ms
step:29/2330 train_time:1760ms step_avg:60.69ms
step:30/2330 train_time:1822ms step_avg:60.73ms
step:31/2330 train_time:1880ms step_avg:60.66ms
step:32/2330 train_time:1942ms step_avg:60.69ms
step:33/2330 train_time:2001ms step_avg:60.64ms
step:34/2330 train_time:2064ms step_avg:60.70ms
step:35/2330 train_time:2125ms step_avg:60.72ms
step:36/2330 train_time:2189ms step_avg:60.81ms
step:37/2330 train_time:2250ms step_avg:60.80ms
step:38/2330 train_time:2312ms step_avg:60.85ms
step:39/2330 train_time:2374ms step_avg:60.86ms
step:40/2330 train_time:2436ms step_avg:60.89ms
step:41/2330 train_time:2494ms step_avg:60.83ms
step:42/2330 train_time:2557ms step_avg:60.88ms
step:43/2330 train_time:2617ms step_avg:60.85ms
step:44/2330 train_time:2679ms step_avg:60.88ms
step:45/2330 train_time:2738ms step_avg:60.85ms
step:46/2330 train_time:2800ms step_avg:60.88ms
step:47/2330 train_time:2859ms step_avg:60.83ms
step:48/2330 train_time:2921ms step_avg:60.85ms
step:49/2330 train_time:2980ms step_avg:60.81ms
step:50/2330 train_time:3042ms step_avg:60.84ms
step:51/2330 train_time:3103ms step_avg:60.84ms
step:52/2330 train_time:3166ms step_avg:60.88ms
step:53/2330 train_time:3225ms step_avg:60.86ms
step:54/2330 train_time:3289ms step_avg:60.91ms
step:55/2330 train_time:3350ms step_avg:60.90ms
step:56/2330 train_time:3412ms step_avg:60.93ms
step:57/2330 train_time:3472ms step_avg:60.91ms
step:58/2330 train_time:3534ms step_avg:60.94ms
step:59/2330 train_time:3593ms step_avg:60.90ms
step:60/2330 train_time:3656ms step_avg:60.93ms
step:61/2330 train_time:3715ms step_avg:60.90ms
step:62/2330 train_time:3777ms step_avg:60.92ms
step:63/2330 train_time:3836ms step_avg:60.89ms
step:64/2330 train_time:3898ms step_avg:60.90ms
step:65/2330 train_time:3957ms step_avg:60.87ms
step:66/2330 train_time:4020ms step_avg:60.90ms
step:67/2330 train_time:4080ms step_avg:60.89ms
step:68/2330 train_time:4142ms step_avg:60.91ms
step:69/2330 train_time:4202ms step_avg:60.89ms
step:70/2330 train_time:4264ms step_avg:60.92ms
step:71/2330 train_time:4324ms step_avg:60.90ms
step:72/2330 train_time:4386ms step_avg:60.92ms
step:73/2330 train_time:4446ms step_avg:60.91ms
step:74/2330 train_time:4509ms step_avg:60.93ms
step:75/2330 train_time:4569ms step_avg:60.91ms
step:76/2330 train_time:4631ms step_avg:60.93ms
step:77/2330 train_time:4691ms step_avg:60.92ms
step:78/2330 train_time:4753ms step_avg:60.94ms
step:79/2330 train_time:4813ms step_avg:60.93ms
step:80/2330 train_time:4876ms step_avg:60.95ms
step:81/2330 train_time:4935ms step_avg:60.93ms
step:82/2330 train_time:4997ms step_avg:60.94ms
step:83/2330 train_time:5056ms step_avg:60.92ms
step:84/2330 train_time:5118ms step_avg:60.93ms
step:85/2330 train_time:5178ms step_avg:60.91ms
step:86/2330 train_time:5240ms step_avg:60.93ms
step:87/2330 train_time:5300ms step_avg:60.92ms
step:88/2330 train_time:5362ms step_avg:60.93ms
step:89/2330 train_time:5422ms step_avg:60.92ms
step:90/2330 train_time:5485ms step_avg:60.94ms
step:91/2330 train_time:5544ms step_avg:60.92ms
step:92/2330 train_time:5606ms step_avg:60.93ms
step:93/2330 train_time:5665ms step_avg:60.92ms
step:94/2330 train_time:5728ms step_avg:60.93ms
step:95/2330 train_time:5787ms step_avg:60.92ms
step:96/2330 train_time:5851ms step_avg:60.94ms
step:97/2330 train_time:5911ms step_avg:60.94ms
step:98/2330 train_time:5974ms step_avg:60.96ms
step:99/2330 train_time:6034ms step_avg:60.95ms
step:100/2330 train_time:6097ms step_avg:60.97ms
step:101/2330 train_time:6157ms step_avg:60.96ms
step:102/2330 train_time:6219ms step_avg:60.97ms
step:103/2330 train_time:6278ms step_avg:60.95ms
step:104/2330 train_time:6340ms step_avg:60.97ms
step:105/2330 train_time:6399ms step_avg:60.94ms
step:106/2330 train_time:6463ms step_avg:60.97ms
step:107/2330 train_time:6522ms step_avg:60.95ms
step:108/2330 train_time:6583ms step_avg:60.96ms
step:109/2330 train_time:6643ms step_avg:60.95ms
step:110/2330 train_time:6705ms step_avg:60.96ms
step:111/2330 train_time:6765ms step_avg:60.94ms
step:112/2330 train_time:6827ms step_avg:60.96ms
step:113/2330 train_time:6887ms step_avg:60.95ms
step:114/2330 train_time:6950ms step_avg:60.96ms
step:115/2330 train_time:7009ms step_avg:60.95ms
step:116/2330 train_time:7072ms step_avg:60.96ms
step:117/2330 train_time:7131ms step_avg:60.95ms
step:118/2330 train_time:7194ms step_avg:60.96ms
step:119/2330 train_time:7254ms step_avg:60.96ms
step:120/2330 train_time:7317ms step_avg:60.97ms
step:121/2330 train_time:7376ms step_avg:60.96ms
step:122/2330 train_time:7438ms step_avg:60.96ms
step:123/2330 train_time:7498ms step_avg:60.96ms
step:124/2330 train_time:7560ms step_avg:60.97ms
step:125/2330 train_time:7619ms step_avg:60.95ms
step:126/2330 train_time:7681ms step_avg:60.96ms
step:127/2330 train_time:7741ms step_avg:60.95ms
step:128/2330 train_time:7802ms step_avg:60.96ms
step:129/2330 train_time:7862ms step_avg:60.95ms
step:130/2330 train_time:7924ms step_avg:60.96ms
step:131/2330 train_time:7984ms step_avg:60.94ms
step:132/2330 train_time:8046ms step_avg:60.96ms
step:133/2330 train_time:8106ms step_avg:60.95ms
step:134/2330 train_time:8170ms step_avg:60.97ms
step:135/2330 train_time:8230ms step_avg:60.97ms
step:136/2330 train_time:8293ms step_avg:60.98ms
step:137/2330 train_time:8352ms step_avg:60.96ms
step:138/2330 train_time:8415ms step_avg:60.98ms
step:139/2330 train_time:8474ms step_avg:60.97ms
step:140/2330 train_time:8537ms step_avg:60.98ms
step:141/2330 train_time:8596ms step_avg:60.96ms
step:142/2330 train_time:8657ms step_avg:60.97ms
step:143/2330 train_time:8716ms step_avg:60.95ms
step:144/2330 train_time:8779ms step_avg:60.96ms
step:145/2330 train_time:8839ms step_avg:60.96ms
step:146/2330 train_time:8901ms step_avg:60.97ms
step:147/2330 train_time:8961ms step_avg:60.96ms
step:148/2330 train_time:9023ms step_avg:60.96ms
step:149/2330 train_time:9082ms step_avg:60.95ms
step:150/2330 train_time:9144ms step_avg:60.96ms
step:151/2330 train_time:9203ms step_avg:60.95ms
step:152/2330 train_time:9266ms step_avg:60.96ms
step:153/2330 train_time:9325ms step_avg:60.95ms
step:154/2330 train_time:9388ms step_avg:60.96ms
step:155/2330 train_time:9448ms step_avg:60.95ms
step:156/2330 train_time:9510ms step_avg:60.96ms
step:157/2330 train_time:9570ms step_avg:60.95ms
step:158/2330 train_time:9632ms step_avg:60.96ms
step:159/2330 train_time:9691ms step_avg:60.95ms
step:160/2330 train_time:9754ms step_avg:60.96ms
step:161/2330 train_time:9814ms step_avg:60.96ms
step:162/2330 train_time:9876ms step_avg:60.96ms
step:163/2330 train_time:9935ms step_avg:60.95ms
step:164/2330 train_time:9997ms step_avg:60.96ms
step:165/2330 train_time:10057ms step_avg:60.95ms
step:166/2330 train_time:10119ms step_avg:60.96ms
step:167/2330 train_time:10178ms step_avg:60.95ms
step:168/2330 train_time:10240ms step_avg:60.95ms
step:169/2330 train_time:10300ms step_avg:60.94ms
step:170/2330 train_time:10362ms step_avg:60.95ms
step:171/2330 train_time:10422ms step_avg:60.94ms
step:172/2330 train_time:10484ms step_avg:60.95ms
step:173/2330 train_time:10543ms step_avg:60.94ms
step:174/2330 train_time:10605ms step_avg:60.95ms
step:175/2330 train_time:10665ms step_avg:60.94ms
step:176/2330 train_time:10728ms step_avg:60.95ms
step:177/2330 train_time:10787ms step_avg:60.95ms
step:178/2330 train_time:10850ms step_avg:60.95ms
step:179/2330 train_time:10909ms step_avg:60.94ms
step:180/2330 train_time:10971ms step_avg:60.95ms
step:181/2330 train_time:11031ms step_avg:60.94ms
step:182/2330 train_time:11093ms step_avg:60.95ms
step:183/2330 train_time:11152ms step_avg:60.94ms
step:184/2330 train_time:11214ms step_avg:60.95ms
step:185/2330 train_time:11273ms step_avg:60.94ms
step:186/2330 train_time:11335ms step_avg:60.94ms
step:187/2330 train_time:11395ms step_avg:60.93ms
step:188/2330 train_time:11457ms step_avg:60.94ms
step:189/2330 train_time:11516ms step_avg:60.93ms
step:190/2330 train_time:11579ms step_avg:60.94ms
step:191/2330 train_time:11639ms step_avg:60.93ms
step:192/2330 train_time:11700ms step_avg:60.94ms
step:193/2330 train_time:11760ms step_avg:60.93ms
step:194/2330 train_time:11822ms step_avg:60.94ms
step:195/2330 train_time:11881ms step_avg:60.93ms
step:196/2330 train_time:11943ms step_avg:60.93ms
step:197/2330 train_time:12002ms step_avg:60.92ms
step:198/2330 train_time:12064ms step_avg:60.93ms
step:199/2330 train_time:12124ms step_avg:60.93ms
step:200/2330 train_time:12187ms step_avg:60.94ms
step:201/2330 train_time:12246ms step_avg:60.93ms
step:202/2330 train_time:12310ms step_avg:60.94ms
step:203/2330 train_time:12369ms step_avg:60.93ms
step:204/2330 train_time:12432ms step_avg:60.94ms
step:205/2330 train_time:12491ms step_avg:60.93ms
step:206/2330 train_time:12553ms step_avg:60.94ms
step:207/2330 train_time:12613ms step_avg:60.93ms
step:208/2330 train_time:12675ms step_avg:60.94ms
step:209/2330 train_time:12735ms step_avg:60.93ms
step:210/2330 train_time:12797ms step_avg:60.94ms
step:211/2330 train_time:12856ms step_avg:60.93ms
step:212/2330 train_time:12918ms step_avg:60.94ms
step:213/2330 train_time:12977ms step_avg:60.93ms
step:214/2330 train_time:13039ms step_avg:60.93ms
step:215/2330 train_time:13098ms step_avg:60.92ms
step:216/2330 train_time:13161ms step_avg:60.93ms
step:217/2330 train_time:13220ms step_avg:60.92ms
step:218/2330 train_time:13282ms step_avg:60.93ms
step:219/2330 train_time:13341ms step_avg:60.92ms
step:220/2330 train_time:13404ms step_avg:60.93ms
step:221/2330 train_time:13463ms step_avg:60.92ms
step:222/2330 train_time:13526ms step_avg:60.93ms
step:223/2330 train_time:13585ms step_avg:60.92ms
step:224/2330 train_time:13648ms step_avg:60.93ms
step:225/2330 train_time:13708ms step_avg:60.92ms
step:226/2330 train_time:13771ms step_avg:60.93ms
step:227/2330 train_time:13831ms step_avg:60.93ms
step:228/2330 train_time:13893ms step_avg:60.93ms
step:229/2330 train_time:13952ms step_avg:60.93ms
step:230/2330 train_time:14015ms step_avg:60.93ms
step:231/2330 train_time:14074ms step_avg:60.93ms
step:232/2330 train_time:14136ms step_avg:60.93ms
step:233/2330 train_time:14195ms step_avg:60.92ms
step:234/2330 train_time:14257ms step_avg:60.93ms
step:235/2330 train_time:14317ms step_avg:60.92ms
step:236/2330 train_time:14379ms step_avg:60.93ms
step:237/2330 train_time:14439ms step_avg:60.92ms
step:238/2330 train_time:14500ms step_avg:60.93ms
step:239/2330 train_time:14560ms step_avg:60.92ms
step:240/2330 train_time:14622ms step_avg:60.93ms
step:241/2330 train_time:14682ms step_avg:60.92ms
step:242/2330 train_time:14744ms step_avg:60.92ms
step:243/2330 train_time:14803ms step_avg:60.92ms
step:244/2330 train_time:14866ms step_avg:60.92ms
step:245/2330 train_time:14925ms step_avg:60.92ms
step:246/2330 train_time:14988ms step_avg:60.93ms
step:247/2330 train_time:15047ms step_avg:60.92ms
step:248/2330 train_time:15110ms step_avg:60.93ms
step:249/2330 train_time:15170ms step_avg:60.92ms
step:250/2330 train_time:15232ms step_avg:60.93ms
step:250/2330 val_loss:4.0787 train_time:15297ms step_avg:61.19ms
step:251/2330 train_time:15320ms step_avg:61.04ms
step:252/2330 train_time:15357ms step_avg:60.94ms
step:253/2330 train_time:15423ms step_avg:60.96ms
step:254/2330 train_time:15489ms step_avg:60.98ms
step:255/2330 train_time:15548ms step_avg:60.97ms
step:256/2330 train_time:15611ms step_avg:60.98ms
step:257/2330 train_time:15670ms step_avg:60.97ms
step:258/2330 train_time:15732ms step_avg:60.98ms
step:259/2330 train_time:15790ms step_avg:60.97ms
step:260/2330 train_time:15852ms step_avg:60.97ms
step:261/2330 train_time:15910ms step_avg:60.96ms
step:262/2330 train_time:15972ms step_avg:60.96ms
step:263/2330 train_time:16030ms step_avg:60.95ms
step:264/2330 train_time:16091ms step_avg:60.95ms
step:265/2330 train_time:16149ms step_avg:60.94ms
step:266/2330 train_time:16210ms step_avg:60.94ms
step:267/2330 train_time:16270ms step_avg:60.94ms
step:268/2330 train_time:16333ms step_avg:60.95ms
step:269/2330 train_time:16394ms step_avg:60.95ms
step:270/2330 train_time:16458ms step_avg:60.96ms
step:271/2330 train_time:16517ms step_avg:60.95ms
step:272/2330 train_time:16580ms step_avg:60.96ms
step:273/2330 train_time:16640ms step_avg:60.95ms
step:274/2330 train_time:16702ms step_avg:60.96ms
step:275/2330 train_time:16762ms step_avg:60.95ms
step:276/2330 train_time:16824ms step_avg:60.96ms
step:277/2330 train_time:16884ms step_avg:60.95ms
step:278/2330 train_time:16946ms step_avg:60.96ms
step:279/2330 train_time:17005ms step_avg:60.95ms
step:280/2330 train_time:17067ms step_avg:60.95ms
step:281/2330 train_time:17125ms step_avg:60.94ms
step:282/2330 train_time:17187ms step_avg:60.95ms
step:283/2330 train_time:17246ms step_avg:60.94ms
step:284/2330 train_time:17309ms step_avg:60.95ms
step:285/2330 train_time:17368ms step_avg:60.94ms
step:286/2330 train_time:17431ms step_avg:60.95ms
step:287/2330 train_time:17490ms step_avg:60.94ms
step:288/2330 train_time:17553ms step_avg:60.95ms
step:289/2330 train_time:17612ms step_avg:60.94ms
step:290/2330 train_time:17675ms step_avg:60.95ms
step:291/2330 train_time:17734ms step_avg:60.94ms
step:292/2330 train_time:17796ms step_avg:60.94ms
step:293/2330 train_time:17855ms step_avg:60.94ms
step:294/2330 train_time:17917ms step_avg:60.94ms
step:295/2330 train_time:17976ms step_avg:60.94ms
step:296/2330 train_time:18038ms step_avg:60.94ms
step:297/2330 train_time:18097ms step_avg:60.93ms
step:298/2330 train_time:18160ms step_avg:60.94ms
step:299/2330 train_time:18220ms step_avg:60.94ms
step:300/2330 train_time:18283ms step_avg:60.94ms
step:301/2330 train_time:18343ms step_avg:60.94ms
step:302/2330 train_time:18405ms step_avg:60.94ms
step:303/2330 train_time:18465ms step_avg:60.94ms
step:304/2330 train_time:18527ms step_avg:60.95ms
step:305/2330 train_time:18586ms step_avg:60.94ms
step:306/2330 train_time:18649ms step_avg:60.94ms
step:307/2330 train_time:18708ms step_avg:60.94ms
step:308/2330 train_time:18771ms step_avg:60.94ms
step:309/2330 train_time:18830ms step_avg:60.94ms
step:310/2330 train_time:18892ms step_avg:60.94ms
step:311/2330 train_time:18951ms step_avg:60.93ms
step:312/2330 train_time:19013ms step_avg:60.94ms
step:313/2330 train_time:19073ms step_avg:60.93ms
step:314/2330 train_time:19134ms step_avg:60.94ms
step:315/2330 train_time:19193ms step_avg:60.93ms
step:316/2330 train_time:19255ms step_avg:60.93ms
step:317/2330 train_time:19315ms step_avg:60.93ms
step:318/2330 train_time:19377ms step_avg:60.93ms
step:319/2330 train_time:19437ms step_avg:60.93ms
step:320/2330 train_time:19500ms step_avg:60.94ms
step:321/2330 train_time:19559ms step_avg:60.93ms
step:322/2330 train_time:19622ms step_avg:60.94ms
step:323/2330 train_time:19682ms step_avg:60.93ms
step:324/2330 train_time:19744ms step_avg:60.94ms
step:325/2330 train_time:19803ms step_avg:60.93ms
step:326/2330 train_time:19866ms step_avg:60.94ms
step:327/2330 train_time:19926ms step_avg:60.94ms
step:328/2330 train_time:19988ms step_avg:60.94ms
step:329/2330 train_time:20046ms step_avg:60.93ms
step:330/2330 train_time:20108ms step_avg:60.93ms
step:331/2330 train_time:20168ms step_avg:60.93ms
step:332/2330 train_time:20231ms step_avg:60.94ms
step:333/2330 train_time:20290ms step_avg:60.93ms
step:334/2330 train_time:20352ms step_avg:60.93ms
step:335/2330 train_time:20412ms step_avg:60.93ms
step:336/2330 train_time:20474ms step_avg:60.93ms
step:337/2330 train_time:20532ms step_avg:60.93ms
step:338/2330 train_time:20595ms step_avg:60.93ms
step:339/2330 train_time:20654ms step_avg:60.93ms
step:340/2330 train_time:20716ms step_avg:60.93ms
step:341/2330 train_time:20776ms step_avg:60.93ms
step:342/2330 train_time:20839ms step_avg:60.93ms
step:343/2330 train_time:20898ms step_avg:60.93ms
step:344/2330 train_time:20961ms step_avg:60.93ms
step:345/2330 train_time:21021ms step_avg:60.93ms
step:346/2330 train_time:21084ms step_avg:60.94ms
step:347/2330 train_time:21143ms step_avg:60.93ms
step:348/2330 train_time:21206ms step_avg:60.94ms
step:349/2330 train_time:21265ms step_avg:60.93ms
step:350/2330 train_time:21327ms step_avg:60.93ms
step:351/2330 train_time:21386ms step_avg:60.93ms
step:352/2330 train_time:21448ms step_avg:60.93ms
step:353/2330 train_time:21507ms step_avg:60.93ms
step:354/2330 train_time:21569ms step_avg:60.93ms
step:355/2330 train_time:21629ms step_avg:60.93ms
step:356/2330 train_time:21691ms step_avg:60.93ms
step:357/2330 train_time:21750ms step_avg:60.93ms
step:358/2330 train_time:21813ms step_avg:60.93ms
step:359/2330 train_time:21873ms step_avg:60.93ms
step:360/2330 train_time:21935ms step_avg:60.93ms
step:361/2330 train_time:21994ms step_avg:60.93ms
step:362/2330 train_time:22056ms step_avg:60.93ms
step:363/2330 train_time:22116ms step_avg:60.93ms
step:364/2330 train_time:22179ms step_avg:60.93ms
step:365/2330 train_time:22237ms step_avg:60.92ms
step:366/2330 train_time:22300ms step_avg:60.93ms
step:367/2330 train_time:22359ms step_avg:60.92ms
step:368/2330 train_time:22422ms step_avg:60.93ms
step:369/2330 train_time:22482ms step_avg:60.93ms
step:370/2330 train_time:22544ms step_avg:60.93ms
step:371/2330 train_time:22605ms step_avg:60.93ms
step:372/2330 train_time:22667ms step_avg:60.93ms
step:373/2330 train_time:22726ms step_avg:60.93ms
step:374/2330 train_time:22788ms step_avg:60.93ms
step:375/2330 train_time:22847ms step_avg:60.92ms
step:376/2330 train_time:22909ms step_avg:60.93ms
step:377/2330 train_time:22969ms step_avg:60.92ms
step:378/2330 train_time:23031ms step_avg:60.93ms
step:379/2330 train_time:23090ms step_avg:60.92ms
step:380/2330 train_time:23152ms step_avg:60.93ms
step:381/2330 train_time:23211ms step_avg:60.92ms
step:382/2330 train_time:23274ms step_avg:60.93ms
step:383/2330 train_time:23332ms step_avg:60.92ms
step:384/2330 train_time:23394ms step_avg:60.92ms
step:385/2330 train_time:23455ms step_avg:60.92ms
step:386/2330 train_time:23517ms step_avg:60.93ms
step:387/2330 train_time:23577ms step_avg:60.92ms
step:388/2330 train_time:23640ms step_avg:60.93ms
step:389/2330 train_time:23699ms step_avg:60.92ms
step:390/2330 train_time:23762ms step_avg:60.93ms
step:391/2330 train_time:23822ms step_avg:60.93ms
step:392/2330 train_time:23885ms step_avg:60.93ms
step:393/2330 train_time:23944ms step_avg:60.93ms
step:394/2330 train_time:24007ms step_avg:60.93ms
step:395/2330 train_time:24066ms step_avg:60.93ms
step:396/2330 train_time:24128ms step_avg:60.93ms
step:397/2330 train_time:24187ms step_avg:60.92ms
step:398/2330 train_time:24249ms step_avg:60.93ms
step:399/2330 train_time:24308ms step_avg:60.92ms
step:400/2330 train_time:24370ms step_avg:60.92ms
step:401/2330 train_time:24429ms step_avg:60.92ms
step:402/2330 train_time:24491ms step_avg:60.92ms
step:403/2330 train_time:24551ms step_avg:60.92ms
step:404/2330 train_time:24613ms step_avg:60.92ms
step:405/2330 train_time:24673ms step_avg:60.92ms
step:406/2330 train_time:24735ms step_avg:60.92ms
step:407/2330 train_time:24795ms step_avg:60.92ms
step:408/2330 train_time:24857ms step_avg:60.92ms
step:409/2330 train_time:24916ms step_avg:60.92ms
step:410/2330 train_time:24978ms step_avg:60.92ms
step:411/2330 train_time:25038ms step_avg:60.92ms
step:412/2330 train_time:25101ms step_avg:60.93ms
step:413/2330 train_time:25161ms step_avg:60.92ms
step:414/2330 train_time:25225ms step_avg:60.93ms
step:415/2330 train_time:25284ms step_avg:60.93ms
step:416/2330 train_time:25346ms step_avg:60.93ms
step:417/2330 train_time:25405ms step_avg:60.92ms
step:418/2330 train_time:25467ms step_avg:60.93ms
step:419/2330 train_time:25527ms step_avg:60.92ms
step:420/2330 train_time:25589ms step_avg:60.93ms
step:421/2330 train_time:25648ms step_avg:60.92ms
step:422/2330 train_time:25711ms step_avg:60.93ms
step:423/2330 train_time:25770ms step_avg:60.92ms
step:424/2330 train_time:25833ms step_avg:60.93ms
step:425/2330 train_time:25891ms step_avg:60.92ms
step:426/2330 train_time:25953ms step_avg:60.92ms
step:427/2330 train_time:26012ms step_avg:60.92ms
step:428/2330 train_time:26074ms step_avg:60.92ms
step:429/2330 train_time:26133ms step_avg:60.92ms
step:430/2330 train_time:26195ms step_avg:60.92ms
step:431/2330 train_time:26254ms step_avg:60.91ms
step:432/2330 train_time:26317ms step_avg:60.92ms
step:433/2330 train_time:26376ms step_avg:60.92ms
step:434/2330 train_time:26439ms step_avg:60.92ms
step:435/2330 train_time:26498ms step_avg:60.92ms
step:436/2330 train_time:26562ms step_avg:60.92ms
step:437/2330 train_time:26621ms step_avg:60.92ms
step:438/2330 train_time:26684ms step_avg:60.92ms
step:439/2330 train_time:26743ms step_avg:60.92ms
step:440/2330 train_time:26806ms step_avg:60.92ms
step:441/2330 train_time:26866ms step_avg:60.92ms
step:442/2330 train_time:26928ms step_avg:60.92ms
step:443/2330 train_time:26987ms step_avg:60.92ms
step:444/2330 train_time:27049ms step_avg:60.92ms
step:445/2330 train_time:27108ms step_avg:60.92ms
step:446/2330 train_time:27170ms step_avg:60.92ms
step:447/2330 train_time:27230ms step_avg:60.92ms
step:448/2330 train_time:27292ms step_avg:60.92ms
step:449/2330 train_time:27350ms step_avg:60.91ms
step:450/2330 train_time:27412ms step_avg:60.92ms
step:451/2330 train_time:27471ms step_avg:60.91ms
step:452/2330 train_time:27533ms step_avg:60.91ms
step:453/2330 train_time:27592ms step_avg:60.91ms
step:454/2330 train_time:27655ms step_avg:60.91ms
step:455/2330 train_time:27714ms step_avg:60.91ms
step:456/2330 train_time:27777ms step_avg:60.91ms
step:457/2330 train_time:27836ms step_avg:60.91ms
step:458/2330 train_time:27899ms step_avg:60.91ms
step:459/2330 train_time:27958ms step_avg:60.91ms
step:460/2330 train_time:28021ms step_avg:60.92ms
step:461/2330 train_time:28081ms step_avg:60.91ms
step:462/2330 train_time:28144ms step_avg:60.92ms
step:463/2330 train_time:28203ms step_avg:60.91ms
step:464/2330 train_time:28265ms step_avg:60.92ms
step:465/2330 train_time:28325ms step_avg:60.91ms
step:466/2330 train_time:28387ms step_avg:60.92ms
step:467/2330 train_time:28446ms step_avg:60.91ms
step:468/2330 train_time:28508ms step_avg:60.91ms
step:469/2330 train_time:28568ms step_avg:60.91ms
step:470/2330 train_time:28630ms step_avg:60.91ms
step:471/2330 train_time:28689ms step_avg:60.91ms
step:472/2330 train_time:28750ms step_avg:60.91ms
step:473/2330 train_time:28810ms step_avg:60.91ms
step:474/2330 train_time:28873ms step_avg:60.91ms
step:475/2330 train_time:28932ms step_avg:60.91ms
step:476/2330 train_time:28994ms step_avg:60.91ms
step:477/2330 train_time:29054ms step_avg:60.91ms
step:478/2330 train_time:29116ms step_avg:60.91ms
step:479/2330 train_time:29176ms step_avg:60.91ms
step:480/2330 train_time:29238ms step_avg:60.91ms
step:481/2330 train_time:29297ms step_avg:60.91ms
step:482/2330 train_time:29360ms step_avg:60.91ms
step:483/2330 train_time:29419ms step_avg:60.91ms
step:484/2330 train_time:29482ms step_avg:60.91ms
step:485/2330 train_time:29542ms step_avg:60.91ms
step:486/2330 train_time:29605ms step_avg:60.91ms
step:487/2330 train_time:29664ms step_avg:60.91ms
step:488/2330 train_time:29727ms step_avg:60.92ms
step:489/2330 train_time:29786ms step_avg:60.91ms
step:490/2330 train_time:29849ms step_avg:60.92ms
step:491/2330 train_time:29907ms step_avg:60.91ms
step:492/2330 train_time:29969ms step_avg:60.91ms
step:493/2330 train_time:30029ms step_avg:60.91ms
step:494/2330 train_time:30091ms step_avg:60.91ms
step:495/2330 train_time:30150ms step_avg:60.91ms
step:496/2330 train_time:30212ms step_avg:60.91ms
step:497/2330 train_time:30271ms step_avg:60.91ms
step:498/2330 train_time:30333ms step_avg:60.91ms
step:499/2330 train_time:30392ms step_avg:60.91ms
step:500/2330 train_time:30454ms step_avg:60.91ms
step:500/2330 val_loss:3.8153 train_time:30519ms step_avg:61.04ms
step:501/2330 train_time:30542ms step_avg:60.96ms
step:502/2330 train_time:30580ms step_avg:60.92ms
step:503/2330 train_time:30645ms step_avg:60.92ms
step:504/2330 train_time:30712ms step_avg:60.94ms
step:505/2330 train_time:30771ms step_avg:60.93ms
step:506/2330 train_time:30834ms step_avg:60.94ms
step:507/2330 train_time:30892ms step_avg:60.93ms
step:508/2330 train_time:30954ms step_avg:60.93ms
step:509/2330 train_time:31012ms step_avg:60.93ms
step:510/2330 train_time:31073ms step_avg:60.93ms
step:511/2330 train_time:31131ms step_avg:60.92ms
step:512/2330 train_time:31193ms step_avg:60.92ms
step:513/2330 train_time:31251ms step_avg:60.92ms
step:514/2330 train_time:31313ms step_avg:60.92ms
step:515/2330 train_time:31371ms step_avg:60.91ms
step:516/2330 train_time:31433ms step_avg:60.92ms
step:517/2330 train_time:31492ms step_avg:60.91ms
step:518/2330 train_time:31558ms step_avg:60.92ms
step:519/2330 train_time:31619ms step_avg:60.92ms
step:520/2330 train_time:31683ms step_avg:60.93ms
step:521/2330 train_time:31743ms step_avg:60.93ms
step:522/2330 train_time:31806ms step_avg:60.93ms
step:523/2330 train_time:31865ms step_avg:60.93ms
step:524/2330 train_time:31927ms step_avg:60.93ms
step:525/2330 train_time:31986ms step_avg:60.93ms
step:526/2330 train_time:32048ms step_avg:60.93ms
step:527/2330 train_time:32107ms step_avg:60.92ms
step:528/2330 train_time:32168ms step_avg:60.92ms
step:529/2330 train_time:32227ms step_avg:60.92ms
step:530/2330 train_time:32288ms step_avg:60.92ms
step:531/2330 train_time:32347ms step_avg:60.92ms
step:532/2330 train_time:32409ms step_avg:60.92ms
step:533/2330 train_time:32468ms step_avg:60.92ms
step:534/2330 train_time:32531ms step_avg:60.92ms
step:535/2330 train_time:32592ms step_avg:60.92ms
step:536/2330 train_time:32654ms step_avg:60.92ms
step:537/2330 train_time:32714ms step_avg:60.92ms
step:538/2330 train_time:32777ms step_avg:60.92ms
step:539/2330 train_time:32836ms step_avg:60.92ms
step:540/2330 train_time:32899ms step_avg:60.92ms
step:541/2330 train_time:32958ms step_avg:60.92ms
step:542/2330 train_time:33021ms step_avg:60.92ms
step:543/2330 train_time:33079ms step_avg:60.92ms
step:544/2330 train_time:33142ms step_avg:60.92ms
step:545/2330 train_time:33201ms step_avg:60.92ms
step:546/2330 train_time:33263ms step_avg:60.92ms
step:547/2330 train_time:33322ms step_avg:60.92ms
step:548/2330 train_time:33384ms step_avg:60.92ms
step:549/2330 train_time:33443ms step_avg:60.92ms
step:550/2330 train_time:33505ms step_avg:60.92ms
step:551/2330 train_time:33565ms step_avg:60.92ms
step:552/2330 train_time:33627ms step_avg:60.92ms
step:553/2330 train_time:33687ms step_avg:60.92ms
step:554/2330 train_time:33750ms step_avg:60.92ms
step:555/2330 train_time:33809ms step_avg:60.92ms
step:556/2330 train_time:33871ms step_avg:60.92ms
step:557/2330 train_time:33930ms step_avg:60.92ms
step:558/2330 train_time:33993ms step_avg:60.92ms
step:559/2330 train_time:34053ms step_avg:60.92ms
step:560/2330 train_time:34115ms step_avg:60.92ms
step:561/2330 train_time:34174ms step_avg:60.92ms
step:562/2330 train_time:34236ms step_avg:60.92ms
step:563/2330 train_time:34296ms step_avg:60.92ms
step:564/2330 train_time:34359ms step_avg:60.92ms
step:565/2330 train_time:34418ms step_avg:60.92ms
step:566/2330 train_time:34480ms step_avg:60.92ms
step:567/2330 train_time:34540ms step_avg:60.92ms
step:568/2330 train_time:34602ms step_avg:60.92ms
step:569/2330 train_time:34662ms step_avg:60.92ms
step:570/2330 train_time:34724ms step_avg:60.92ms
step:571/2330 train_time:34784ms step_avg:60.92ms
step:572/2330 train_time:34846ms step_avg:60.92ms
step:573/2330 train_time:34905ms step_avg:60.92ms
step:574/2330 train_time:34968ms step_avg:60.92ms
step:575/2330 train_time:35027ms step_avg:60.92ms
step:576/2330 train_time:35089ms step_avg:60.92ms
step:577/2330 train_time:35148ms step_avg:60.91ms
step:578/2330 train_time:35210ms step_avg:60.92ms
step:579/2330 train_time:35270ms step_avg:60.91ms
step:580/2330 train_time:35332ms step_avg:60.92ms
step:581/2330 train_time:35391ms step_avg:60.91ms
step:582/2330 train_time:35454ms step_avg:60.92ms
step:583/2330 train_time:35513ms step_avg:60.91ms
step:584/2330 train_time:35575ms step_avg:60.92ms
step:585/2330 train_time:35635ms step_avg:60.91ms
step:586/2330 train_time:35698ms step_avg:60.92ms
step:587/2330 train_time:35759ms step_avg:60.92ms
step:588/2330 train_time:35821ms step_avg:60.92ms
step:589/2330 train_time:35880ms step_avg:60.92ms
step:590/2330 train_time:35943ms step_avg:60.92ms
step:591/2330 train_time:36003ms step_avg:60.92ms
step:592/2330 train_time:36065ms step_avg:60.92ms
step:593/2330 train_time:36124ms step_avg:60.92ms
step:594/2330 train_time:36185ms step_avg:60.92ms
step:595/2330 train_time:36245ms step_avg:60.92ms
step:596/2330 train_time:36306ms step_avg:60.92ms
step:597/2330 train_time:36366ms step_avg:60.91ms
step:598/2330 train_time:36428ms step_avg:60.92ms
step:599/2330 train_time:36488ms step_avg:60.92ms
step:600/2330 train_time:36550ms step_avg:60.92ms
step:601/2330 train_time:36610ms step_avg:60.92ms
step:602/2330 train_time:36672ms step_avg:60.92ms
step:603/2330 train_time:36731ms step_avg:60.91ms
step:604/2330 train_time:36793ms step_avg:60.92ms
step:605/2330 train_time:36852ms step_avg:60.91ms
step:606/2330 train_time:36915ms step_avg:60.92ms
step:607/2330 train_time:36974ms step_avg:60.91ms
step:608/2330 train_time:37036ms step_avg:60.91ms
step:609/2330 train_time:37096ms step_avg:60.91ms
step:610/2330 train_time:37159ms step_avg:60.92ms
step:611/2330 train_time:37218ms step_avg:60.91ms
step:612/2330 train_time:37281ms step_avg:60.92ms
step:613/2330 train_time:37340ms step_avg:60.91ms
step:614/2330 train_time:37403ms step_avg:60.92ms
step:615/2330 train_time:37462ms step_avg:60.91ms
step:616/2330 train_time:37525ms step_avg:60.92ms
step:617/2330 train_time:37584ms step_avg:60.91ms
step:618/2330 train_time:37646ms step_avg:60.92ms
step:619/2330 train_time:37705ms step_avg:60.91ms
step:620/2330 train_time:37768ms step_avg:60.92ms
step:621/2330 train_time:37827ms step_avg:60.91ms
step:622/2330 train_time:37889ms step_avg:60.91ms
step:623/2330 train_time:37948ms step_avg:60.91ms
step:624/2330 train_time:38011ms step_avg:60.92ms
step:625/2330 train_time:38070ms step_avg:60.91ms
step:626/2330 train_time:38133ms step_avg:60.92ms
step:627/2330 train_time:38192ms step_avg:60.91ms
step:628/2330 train_time:38255ms step_avg:60.91ms
step:629/2330 train_time:38314ms step_avg:60.91ms
step:630/2330 train_time:38377ms step_avg:60.92ms
step:631/2330 train_time:38437ms step_avg:60.91ms
step:632/2330 train_time:38499ms step_avg:60.92ms
step:633/2330 train_time:38559ms step_avg:60.91ms
step:634/2330 train_time:38622ms step_avg:60.92ms
step:635/2330 train_time:38682ms step_avg:60.92ms
step:636/2330 train_time:38744ms step_avg:60.92ms
step:637/2330 train_time:38804ms step_avg:60.92ms
step:638/2330 train_time:38866ms step_avg:60.92ms
step:639/2330 train_time:38925ms step_avg:60.92ms
step:640/2330 train_time:38987ms step_avg:60.92ms
step:641/2330 train_time:39046ms step_avg:60.91ms
step:642/2330 train_time:39109ms step_avg:60.92ms
step:643/2330 train_time:39168ms step_avg:60.91ms
step:644/2330 train_time:39231ms step_avg:60.92ms
step:645/2330 train_time:39291ms step_avg:60.92ms
step:646/2330 train_time:39352ms step_avg:60.92ms
step:647/2330 train_time:39412ms step_avg:60.91ms
step:648/2330 train_time:39474ms step_avg:60.92ms
step:649/2330 train_time:39533ms step_avg:60.91ms
step:650/2330 train_time:39595ms step_avg:60.92ms
step:651/2330 train_time:39655ms step_avg:60.91ms
step:652/2330 train_time:39718ms step_avg:60.92ms
step:653/2330 train_time:39778ms step_avg:60.92ms
step:654/2330 train_time:39840ms step_avg:60.92ms
step:655/2330 train_time:39900ms step_avg:60.92ms
step:656/2330 train_time:39963ms step_avg:60.92ms
step:657/2330 train_time:40023ms step_avg:60.92ms
step:658/2330 train_time:40085ms step_avg:60.92ms
step:659/2330 train_time:40143ms step_avg:60.92ms
step:660/2330 train_time:40206ms step_avg:60.92ms
step:661/2330 train_time:40265ms step_avg:60.92ms
step:662/2330 train_time:40328ms step_avg:60.92ms
step:663/2330 train_time:40387ms step_avg:60.92ms
step:664/2330 train_time:40450ms step_avg:60.92ms
step:665/2330 train_time:40510ms step_avg:60.92ms
step:666/2330 train_time:40571ms step_avg:60.92ms
step:667/2330 train_time:40631ms step_avg:60.92ms
step:668/2330 train_time:40693ms step_avg:60.92ms
step:669/2330 train_time:40752ms step_avg:60.92ms
step:670/2330 train_time:40815ms step_avg:60.92ms
step:671/2330 train_time:40874ms step_avg:60.91ms
step:672/2330 train_time:40936ms step_avg:60.92ms
step:673/2330 train_time:40997ms step_avg:60.92ms
step:674/2330 train_time:41060ms step_avg:60.92ms
step:675/2330 train_time:41119ms step_avg:60.92ms
step:676/2330 train_time:41182ms step_avg:60.92ms
step:677/2330 train_time:41242ms step_avg:60.92ms
step:678/2330 train_time:41305ms step_avg:60.92ms
step:679/2330 train_time:41364ms step_avg:60.92ms
step:680/2330 train_time:41426ms step_avg:60.92ms
step:681/2330 train_time:41486ms step_avg:60.92ms
step:682/2330 train_time:41548ms step_avg:60.92ms
step:683/2330 train_time:41609ms step_avg:60.92ms
step:684/2330 train_time:41670ms step_avg:60.92ms
step:685/2330 train_time:41730ms step_avg:60.92ms
step:686/2330 train_time:41792ms step_avg:60.92ms
step:687/2330 train_time:41851ms step_avg:60.92ms
step:688/2330 train_time:41913ms step_avg:60.92ms
step:689/2330 train_time:41972ms step_avg:60.92ms
step:690/2330 train_time:42034ms step_avg:60.92ms
step:691/2330 train_time:42094ms step_avg:60.92ms
step:692/2330 train_time:42157ms step_avg:60.92ms
step:693/2330 train_time:42217ms step_avg:60.92ms
step:694/2330 train_time:42280ms step_avg:60.92ms
step:695/2330 train_time:42339ms step_avg:60.92ms
step:696/2330 train_time:42402ms step_avg:60.92ms
step:697/2330 train_time:42461ms step_avg:60.92ms
step:698/2330 train_time:42524ms step_avg:60.92ms
step:699/2330 train_time:42583ms step_avg:60.92ms
step:700/2330 train_time:42645ms step_avg:60.92ms
step:701/2330 train_time:42705ms step_avg:60.92ms
step:702/2330 train_time:42767ms step_avg:60.92ms
step:703/2330 train_time:42826ms step_avg:60.92ms
step:704/2330 train_time:42888ms step_avg:60.92ms
step:705/2330 train_time:42947ms step_avg:60.92ms
step:706/2330 train_time:43010ms step_avg:60.92ms
step:707/2330 train_time:43069ms step_avg:60.92ms
step:708/2330 train_time:43132ms step_avg:60.92ms
step:709/2330 train_time:43192ms step_avg:60.92ms
step:710/2330 train_time:43255ms step_avg:60.92ms
step:711/2330 train_time:43314ms step_avg:60.92ms
step:712/2330 train_time:43376ms step_avg:60.92ms
step:713/2330 train_time:43435ms step_avg:60.92ms
step:714/2330 train_time:43498ms step_avg:60.92ms
step:715/2330 train_time:43557ms step_avg:60.92ms
step:716/2330 train_time:43620ms step_avg:60.92ms
step:717/2330 train_time:43680ms step_avg:60.92ms
step:718/2330 train_time:43742ms step_avg:60.92ms
step:719/2330 train_time:43802ms step_avg:60.92ms
step:720/2330 train_time:43864ms step_avg:60.92ms
step:721/2330 train_time:43923ms step_avg:60.92ms
step:722/2330 train_time:43985ms step_avg:60.92ms
step:723/2330 train_time:44044ms step_avg:60.92ms
step:724/2330 train_time:44107ms step_avg:60.92ms
step:725/2330 train_time:44166ms step_avg:60.92ms
step:726/2330 train_time:44229ms step_avg:60.92ms
step:727/2330 train_time:44288ms step_avg:60.92ms
step:728/2330 train_time:44351ms step_avg:60.92ms
step:729/2330 train_time:44411ms step_avg:60.92ms
step:730/2330 train_time:44473ms step_avg:60.92ms
step:731/2330 train_time:44532ms step_avg:60.92ms
step:732/2330 train_time:44594ms step_avg:60.92ms
step:733/2330 train_time:44653ms step_avg:60.92ms
step:734/2330 train_time:44716ms step_avg:60.92ms
step:735/2330 train_time:44776ms step_avg:60.92ms
step:736/2330 train_time:44838ms step_avg:60.92ms
step:737/2330 train_time:44897ms step_avg:60.92ms
step:738/2330 train_time:44960ms step_avg:60.92ms
step:739/2330 train_time:45020ms step_avg:60.92ms
step:740/2330 train_time:45082ms step_avg:60.92ms
step:741/2330 train_time:45142ms step_avg:60.92ms
step:742/2330 train_time:45205ms step_avg:60.92ms
step:743/2330 train_time:45265ms step_avg:60.92ms
step:744/2330 train_time:45326ms step_avg:60.92ms
step:745/2330 train_time:45386ms step_avg:60.92ms
step:746/2330 train_time:45448ms step_avg:60.92ms
step:747/2330 train_time:45508ms step_avg:60.92ms
step:748/2330 train_time:45570ms step_avg:60.92ms
step:749/2330 train_time:45629ms step_avg:60.92ms
step:750/2330 train_time:45690ms step_avg:60.92ms
step:750/2330 val_loss:3.6874 train_time:45755ms step_avg:61.01ms
step:751/2330 train_time:45779ms step_avg:60.96ms
step:752/2330 train_time:45815ms step_avg:60.92ms
step:753/2330 train_time:45877ms step_avg:60.93ms
step:754/2330 train_time:45946ms step_avg:60.94ms
step:755/2330 train_time:46007ms step_avg:60.94ms
step:756/2330 train_time:46070ms step_avg:60.94ms
step:757/2330 train_time:46129ms step_avg:60.94ms
step:758/2330 train_time:46191ms step_avg:60.94ms
step:759/2330 train_time:46250ms step_avg:60.94ms
step:760/2330 train_time:46312ms step_avg:60.94ms
step:761/2330 train_time:46370ms step_avg:60.93ms
step:762/2330 train_time:46432ms step_avg:60.93ms
step:763/2330 train_time:46490ms step_avg:60.93ms
step:764/2330 train_time:46551ms step_avg:60.93ms
step:765/2330 train_time:46609ms step_avg:60.93ms
step:766/2330 train_time:46672ms step_avg:60.93ms
step:767/2330 train_time:46732ms step_avg:60.93ms
step:768/2330 train_time:46796ms step_avg:60.93ms
step:769/2330 train_time:46858ms step_avg:60.93ms
step:770/2330 train_time:46922ms step_avg:60.94ms
step:771/2330 train_time:46984ms step_avg:60.94ms
step:772/2330 train_time:47048ms step_avg:60.94ms
step:773/2330 train_time:47107ms step_avg:60.94ms
step:774/2330 train_time:47171ms step_avg:60.94ms
step:775/2330 train_time:47231ms step_avg:60.94ms
step:776/2330 train_time:47294ms step_avg:60.95ms
step:777/2330 train_time:47352ms step_avg:60.94ms
step:778/2330 train_time:47414ms step_avg:60.94ms
step:779/2330 train_time:47473ms step_avg:60.94ms
step:780/2330 train_time:47536ms step_avg:60.94ms
step:781/2330 train_time:47595ms step_avg:60.94ms
step:782/2330 train_time:47658ms step_avg:60.94ms
step:783/2330 train_time:47717ms step_avg:60.94ms
step:784/2330 train_time:47780ms step_avg:60.94ms
step:785/2330 train_time:47841ms step_avg:60.94ms
step:786/2330 train_time:47905ms step_avg:60.95ms
step:787/2330 train_time:47965ms step_avg:60.95ms
step:788/2330 train_time:48029ms step_avg:60.95ms
step:789/2330 train_time:48090ms step_avg:60.95ms
step:790/2330 train_time:48153ms step_avg:60.95ms
step:791/2330 train_time:48213ms step_avg:60.95ms
step:792/2330 train_time:48276ms step_avg:60.95ms
step:793/2330 train_time:48335ms step_avg:60.95ms
step:794/2330 train_time:48397ms step_avg:60.95ms
step:795/2330 train_time:48457ms step_avg:60.95ms
step:796/2330 train_time:48520ms step_avg:60.95ms
step:797/2330 train_time:48579ms step_avg:60.95ms
step:798/2330 train_time:48642ms step_avg:60.96ms
step:799/2330 train_time:48702ms step_avg:60.95ms
step:800/2330 train_time:48765ms step_avg:60.96ms
step:801/2330 train_time:48825ms step_avg:60.95ms
step:802/2330 train_time:48888ms step_avg:60.96ms
step:803/2330 train_time:48949ms step_avg:60.96ms
step:804/2330 train_time:49012ms step_avg:60.96ms
step:805/2330 train_time:49072ms step_avg:60.96ms
step:806/2330 train_time:49135ms step_avg:60.96ms
step:807/2330 train_time:49195ms step_avg:60.96ms
step:808/2330 train_time:49257ms step_avg:60.96ms
step:809/2330 train_time:49317ms step_avg:60.96ms
step:810/2330 train_time:49380ms step_avg:60.96ms
step:811/2330 train_time:49440ms step_avg:60.96ms
step:812/2330 train_time:49503ms step_avg:60.96ms
step:813/2330 train_time:49563ms step_avg:60.96ms
step:814/2330 train_time:49625ms step_avg:60.96ms
step:815/2330 train_time:49685ms step_avg:60.96ms
step:816/2330 train_time:49748ms step_avg:60.97ms
step:817/2330 train_time:49807ms step_avg:60.96ms
step:818/2330 train_time:49870ms step_avg:60.97ms
step:819/2330 train_time:49931ms step_avg:60.97ms
step:820/2330 train_time:49994ms step_avg:60.97ms
step:821/2330 train_time:50054ms step_avg:60.97ms
step:822/2330 train_time:50117ms step_avg:60.97ms
step:823/2330 train_time:50177ms step_avg:60.97ms
step:824/2330 train_time:50240ms step_avg:60.97ms
step:825/2330 train_time:50300ms step_avg:60.97ms
step:826/2330 train_time:50363ms step_avg:60.97ms
step:827/2330 train_time:50423ms step_avg:60.97ms
step:828/2330 train_time:50486ms step_avg:60.97ms
step:829/2330 train_time:50545ms step_avg:60.97ms
step:830/2330 train_time:50608ms step_avg:60.97ms
step:831/2330 train_time:50669ms step_avg:60.97ms
step:832/2330 train_time:50732ms step_avg:60.98ms
step:833/2330 train_time:50792ms step_avg:60.97ms
step:834/2330 train_time:50854ms step_avg:60.98ms
step:835/2330 train_time:50914ms step_avg:60.98ms
step:836/2330 train_time:50978ms step_avg:60.98ms
step:837/2330 train_time:51038ms step_avg:60.98ms
step:838/2330 train_time:51100ms step_avg:60.98ms
step:839/2330 train_time:51160ms step_avg:60.98ms
step:840/2330 train_time:51223ms step_avg:60.98ms
step:841/2330 train_time:51283ms step_avg:60.98ms
step:842/2330 train_time:51346ms step_avg:60.98ms
step:843/2330 train_time:51405ms step_avg:60.98ms
step:844/2330 train_time:51469ms step_avg:60.98ms
step:845/2330 train_time:51529ms step_avg:60.98ms
step:846/2330 train_time:51592ms step_avg:60.98ms
step:847/2330 train_time:51652ms step_avg:60.98ms
step:848/2330 train_time:51714ms step_avg:60.98ms
step:849/2330 train_time:51774ms step_avg:60.98ms
step:850/2330 train_time:51837ms step_avg:60.98ms
step:851/2330 train_time:51896ms step_avg:60.98ms
step:852/2330 train_time:51959ms step_avg:60.99ms
step:853/2330 train_time:52019ms step_avg:60.98ms
step:854/2330 train_time:52082ms step_avg:60.99ms
step:855/2330 train_time:52142ms step_avg:60.98ms
step:856/2330 train_time:52205ms step_avg:60.99ms
step:857/2330 train_time:52264ms step_avg:60.99ms
step:858/2330 train_time:52327ms step_avg:60.99ms
step:859/2330 train_time:52388ms step_avg:60.99ms
step:860/2330 train_time:52451ms step_avg:60.99ms
step:861/2330 train_time:52510ms step_avg:60.99ms
step:862/2330 train_time:52573ms step_avg:60.99ms
step:863/2330 train_time:52633ms step_avg:60.99ms
step:864/2330 train_time:52696ms step_avg:60.99ms
step:865/2330 train_time:52755ms step_avg:60.99ms
step:866/2330 train_time:52818ms step_avg:60.99ms
step:867/2330 train_time:52878ms step_avg:60.99ms
step:868/2330 train_time:52942ms step_avg:60.99ms
step:869/2330 train_time:53001ms step_avg:60.99ms
step:870/2330 train_time:53064ms step_avg:60.99ms
step:871/2330 train_time:53124ms step_avg:60.99ms
step:872/2330 train_time:53188ms step_avg:60.99ms
step:873/2330 train_time:53247ms step_avg:60.99ms
step:874/2330 train_time:53310ms step_avg:61.00ms
step:875/2330 train_time:53370ms step_avg:60.99ms
step:876/2330 train_time:53433ms step_avg:61.00ms
step:877/2330 train_time:53493ms step_avg:61.00ms
step:878/2330 train_time:53556ms step_avg:61.00ms
step:879/2330 train_time:53616ms step_avg:61.00ms
step:880/2330 train_time:53679ms step_avg:61.00ms
step:881/2330 train_time:53739ms step_avg:61.00ms
step:882/2330 train_time:53801ms step_avg:61.00ms
step:883/2330 train_time:53861ms step_avg:61.00ms
step:884/2330 train_time:53924ms step_avg:61.00ms
step:885/2330 train_time:53984ms step_avg:61.00ms
step:886/2330 train_time:54047ms step_avg:61.00ms
step:887/2330 train_time:54107ms step_avg:61.00ms
step:888/2330 train_time:54171ms step_avg:61.00ms
step:889/2330 train_time:54231ms step_avg:61.00ms
step:890/2330 train_time:54293ms step_avg:61.00ms
step:891/2330 train_time:54353ms step_avg:61.00ms
step:892/2330 train_time:54416ms step_avg:61.00ms
step:893/2330 train_time:54475ms step_avg:61.00ms
step:894/2330 train_time:54538ms step_avg:61.00ms
step:895/2330 train_time:54598ms step_avg:61.00ms
step:896/2330 train_time:54661ms step_avg:61.01ms
step:897/2330 train_time:54720ms step_avg:61.00ms
step:898/2330 train_time:54783ms step_avg:61.01ms
step:899/2330 train_time:54843ms step_avg:61.00ms
step:900/2330 train_time:54906ms step_avg:61.01ms
step:901/2330 train_time:54966ms step_avg:61.01ms
step:902/2330 train_time:55030ms step_avg:61.01ms
step:903/2330 train_time:55090ms step_avg:61.01ms
step:904/2330 train_time:55152ms step_avg:61.01ms
step:905/2330 train_time:55212ms step_avg:61.01ms
step:906/2330 train_time:55274ms step_avg:61.01ms
step:907/2330 train_time:55334ms step_avg:61.01ms
step:908/2330 train_time:55397ms step_avg:61.01ms
step:909/2330 train_time:55456ms step_avg:61.01ms
step:910/2330 train_time:55519ms step_avg:61.01ms
step:911/2330 train_time:55578ms step_avg:61.01ms
step:912/2330 train_time:55642ms step_avg:61.01ms
step:913/2330 train_time:55701ms step_avg:61.01ms
step:914/2330 train_time:55764ms step_avg:61.01ms
step:915/2330 train_time:55824ms step_avg:61.01ms
step:916/2330 train_time:55887ms step_avg:61.01ms
step:917/2330 train_time:55947ms step_avg:61.01ms
step:918/2330 train_time:56010ms step_avg:61.01ms
step:919/2330 train_time:56071ms step_avg:61.01ms
step:920/2330 train_time:56133ms step_avg:61.01ms
step:921/2330 train_time:56192ms step_avg:61.01ms
step:922/2330 train_time:56255ms step_avg:61.01ms
step:923/2330 train_time:56315ms step_avg:61.01ms
step:924/2330 train_time:56378ms step_avg:61.01ms
step:925/2330 train_time:56437ms step_avg:61.01ms
step:926/2330 train_time:56500ms step_avg:61.02ms
step:927/2330 train_time:56560ms step_avg:61.01ms
step:928/2330 train_time:56622ms step_avg:61.02ms
step:929/2330 train_time:56682ms step_avg:61.01ms
step:930/2330 train_time:56745ms step_avg:61.02ms
step:931/2330 train_time:56804ms step_avg:61.01ms
step:932/2330 train_time:56867ms step_avg:61.02ms
step:933/2330 train_time:56928ms step_avg:61.02ms
step:934/2330 train_time:56991ms step_avg:61.02ms
step:935/2330 train_time:57051ms step_avg:61.02ms
step:936/2330 train_time:57114ms step_avg:61.02ms
step:937/2330 train_time:57175ms step_avg:61.02ms
step:938/2330 train_time:57238ms step_avg:61.02ms
step:939/2330 train_time:57298ms step_avg:61.02ms
step:940/2330 train_time:57360ms step_avg:61.02ms
step:941/2330 train_time:57420ms step_avg:61.02ms
step:942/2330 train_time:57484ms step_avg:61.02ms
step:943/2330 train_time:57544ms step_avg:61.02ms
step:944/2330 train_time:57607ms step_avg:61.02ms
step:945/2330 train_time:57666ms step_avg:61.02ms
step:946/2330 train_time:57730ms step_avg:61.02ms
step:947/2330 train_time:57790ms step_avg:61.02ms
step:948/2330 train_time:57852ms step_avg:61.03ms
step:949/2330 train_time:57912ms step_avg:61.02ms
step:950/2330 train_time:57975ms step_avg:61.03ms
step:951/2330 train_time:58034ms step_avg:61.02ms
step:952/2330 train_time:58097ms step_avg:61.03ms
step:953/2330 train_time:58158ms step_avg:61.03ms
step:954/2330 train_time:58220ms step_avg:61.03ms
step:955/2330 train_time:58279ms step_avg:61.03ms
step:956/2330 train_time:58343ms step_avg:61.03ms
step:957/2330 train_time:58403ms step_avg:61.03ms
step:958/2330 train_time:58466ms step_avg:61.03ms
step:959/2330 train_time:58526ms step_avg:61.03ms
step:960/2330 train_time:58589ms step_avg:61.03ms
step:961/2330 train_time:58649ms step_avg:61.03ms
step:962/2330 train_time:58712ms step_avg:61.03ms
step:963/2330 train_time:58772ms step_avg:61.03ms
step:964/2330 train_time:58835ms step_avg:61.03ms
step:965/2330 train_time:58895ms step_avg:61.03ms
step:966/2330 train_time:58957ms step_avg:61.03ms
step:967/2330 train_time:59017ms step_avg:61.03ms
step:968/2330 train_time:59080ms step_avg:61.03ms
step:969/2330 train_time:59140ms step_avg:61.03ms
step:970/2330 train_time:59203ms step_avg:61.03ms
step:971/2330 train_time:59262ms step_avg:61.03ms
step:972/2330 train_time:59325ms step_avg:61.03ms
step:973/2330 train_time:59385ms step_avg:61.03ms
step:974/2330 train_time:59449ms step_avg:61.04ms
step:975/2330 train_time:59508ms step_avg:61.03ms
step:976/2330 train_time:59572ms step_avg:61.04ms
step:977/2330 train_time:59632ms step_avg:61.04ms
step:978/2330 train_time:59695ms step_avg:61.04ms
step:979/2330 train_time:59754ms step_avg:61.04ms
step:980/2330 train_time:59817ms step_avg:61.04ms
step:981/2330 train_time:59876ms step_avg:61.04ms
step:982/2330 train_time:59940ms step_avg:61.04ms
step:983/2330 train_time:60000ms step_avg:61.04ms
step:984/2330 train_time:60063ms step_avg:61.04ms
step:985/2330 train_time:60123ms step_avg:61.04ms
step:986/2330 train_time:60186ms step_avg:61.04ms
step:987/2330 train_time:60246ms step_avg:61.04ms
step:988/2330 train_time:60310ms step_avg:61.04ms
step:989/2330 train_time:60370ms step_avg:61.04ms
step:990/2330 train_time:60434ms step_avg:61.04ms
step:991/2330 train_time:60493ms step_avg:61.04ms
step:992/2330 train_time:60556ms step_avg:61.04ms
step:993/2330 train_time:60616ms step_avg:61.04ms
step:994/2330 train_time:60679ms step_avg:61.04ms
step:995/2330 train_time:60739ms step_avg:61.04ms
step:996/2330 train_time:60801ms step_avg:61.05ms
step:997/2330 train_time:60862ms step_avg:61.04ms
step:998/2330 train_time:60925ms step_avg:61.05ms
step:999/2330 train_time:60985ms step_avg:61.05ms
step:1000/2330 train_time:61049ms step_avg:61.05ms
step:1000/2330 val_loss:3.5757 train_time:61113ms step_avg:61.11ms
step:1001/2330 train_time:61137ms step_avg:61.08ms
step:1002/2330 train_time:61174ms step_avg:61.05ms
step:1003/2330 train_time:61239ms step_avg:61.06ms
step:1004/2330 train_time:61307ms step_avg:61.06ms
step:1005/2330 train_time:61367ms step_avg:61.06ms
step:1006/2330 train_time:61430ms step_avg:61.06ms
step:1007/2330 train_time:61489ms step_avg:61.06ms
step:1008/2330 train_time:61551ms step_avg:61.06ms
step:1009/2330 train_time:61610ms step_avg:61.06ms
step:1010/2330 train_time:61673ms step_avg:61.06ms
step:1011/2330 train_time:61732ms step_avg:61.06ms
step:1012/2330 train_time:61794ms step_avg:61.06ms
step:1013/2330 train_time:61852ms step_avg:61.06ms
step:1014/2330 train_time:61915ms step_avg:61.06ms
step:1015/2330 train_time:61974ms step_avg:61.06ms
step:1016/2330 train_time:62038ms step_avg:61.06ms
step:1017/2330 train_time:62099ms step_avg:61.06ms
step:1018/2330 train_time:62164ms step_avg:61.06ms
step:1019/2330 train_time:62226ms step_avg:61.07ms
step:1020/2330 train_time:62291ms step_avg:61.07ms
step:1021/2330 train_time:62352ms step_avg:61.07ms
step:1022/2330 train_time:62415ms step_avg:61.07ms
step:1023/2330 train_time:62475ms step_avg:61.07ms
step:1024/2330 train_time:62537ms step_avg:61.07ms
step:1025/2330 train_time:62597ms step_avg:61.07ms
step:1026/2330 train_time:62659ms step_avg:61.07ms
step:1027/2330 train_time:62719ms step_avg:61.07ms
step:1028/2330 train_time:62782ms step_avg:61.07ms
step:1029/2330 train_time:62841ms step_avg:61.07ms
step:1030/2330 train_time:62904ms step_avg:61.07ms
step:1031/2330 train_time:62963ms step_avg:61.07ms
step:1032/2330 train_time:63026ms step_avg:61.07ms
step:1033/2330 train_time:63086ms step_avg:61.07ms
step:1034/2330 train_time:63149ms step_avg:61.07ms
step:1035/2330 train_time:63210ms step_avg:61.07ms
step:1036/2330 train_time:63274ms step_avg:61.08ms
step:1037/2330 train_time:63335ms step_avg:61.08ms
step:1038/2330 train_time:63398ms step_avg:61.08ms
step:1039/2330 train_time:63457ms step_avg:61.08ms
step:1040/2330 train_time:63520ms step_avg:61.08ms
step:1041/2330 train_time:63581ms step_avg:61.08ms
step:1042/2330 train_time:63643ms step_avg:61.08ms
step:1043/2330 train_time:63703ms step_avg:61.08ms
step:1044/2330 train_time:63766ms step_avg:61.08ms
step:1045/2330 train_time:63826ms step_avg:61.08ms
step:1046/2330 train_time:63888ms step_avg:61.08ms
step:1047/2330 train_time:63948ms step_avg:61.08ms
step:1048/2330 train_time:64011ms step_avg:61.08ms
step:1049/2330 train_time:64071ms step_avg:61.08ms
step:1050/2330 train_time:64134ms step_avg:61.08ms
step:1051/2330 train_time:64195ms step_avg:61.08ms
step:1052/2330 train_time:64257ms step_avg:61.08ms
step:1053/2330 train_time:64318ms step_avg:61.08ms
step:1054/2330 train_time:64381ms step_avg:61.08ms
step:1055/2330 train_time:64441ms step_avg:61.08ms
step:1056/2330 train_time:64504ms step_avg:61.08ms
step:1057/2330 train_time:64563ms step_avg:61.08ms
step:1058/2330 train_time:64627ms step_avg:61.08ms
step:1059/2330 train_time:64686ms step_avg:61.08ms
step:1060/2330 train_time:64749ms step_avg:61.08ms
step:1061/2330 train_time:64809ms step_avg:61.08ms
step:1062/2330 train_time:64873ms step_avg:61.09ms
step:1063/2330 train_time:64932ms step_avg:61.08ms
step:1064/2330 train_time:64995ms step_avg:61.09ms
step:1065/2330 train_time:65055ms step_avg:61.08ms
step:1066/2330 train_time:65118ms step_avg:61.09ms
step:1067/2330 train_time:65177ms step_avg:61.08ms
step:1068/2330 train_time:65240ms step_avg:61.09ms
step:1069/2330 train_time:65300ms step_avg:61.09ms
step:1070/2330 train_time:65363ms step_avg:61.09ms
step:1071/2330 train_time:65423ms step_avg:61.09ms
step:1072/2330 train_time:65486ms step_avg:61.09ms
step:1073/2330 train_time:65545ms step_avg:61.09ms
step:1074/2330 train_time:65608ms step_avg:61.09ms
step:1075/2330 train_time:65668ms step_avg:61.09ms
step:1076/2330 train_time:65731ms step_avg:61.09ms
step:1077/2330 train_time:65790ms step_avg:61.09ms
step:1078/2330 train_time:65853ms step_avg:61.09ms
step:1079/2330 train_time:65913ms step_avg:61.09ms
step:1080/2330 train_time:65976ms step_avg:61.09ms
step:1081/2330 train_time:66036ms step_avg:61.09ms
step:1082/2330 train_time:66099ms step_avg:61.09ms
step:1083/2330 train_time:66158ms step_avg:61.09ms
step:1084/2330 train_time:66221ms step_avg:61.09ms
step:1085/2330 train_time:66281ms step_avg:61.09ms
step:1086/2330 train_time:66343ms step_avg:61.09ms
step:1087/2330 train_time:66403ms step_avg:61.09ms
step:1088/2330 train_time:66466ms step_avg:61.09ms
step:1089/2330 train_time:66526ms step_avg:61.09ms
step:1090/2330 train_time:66588ms step_avg:61.09ms
step:1091/2330 train_time:66648ms step_avg:61.09ms
step:1092/2330 train_time:66711ms step_avg:61.09ms
step:1093/2330 train_time:66771ms step_avg:61.09ms
step:1094/2330 train_time:66834ms step_avg:61.09ms
step:1095/2330 train_time:66894ms step_avg:61.09ms
step:1096/2330 train_time:66956ms step_avg:61.09ms
step:1097/2330 train_time:67017ms step_avg:61.09ms
step:1098/2330 train_time:67080ms step_avg:61.09ms
step:1099/2330 train_time:67139ms step_avg:61.09ms
step:1100/2330 train_time:67202ms step_avg:61.09ms
step:1101/2330 train_time:67262ms step_avg:61.09ms
step:1102/2330 train_time:67325ms step_avg:61.09ms
step:1103/2330 train_time:67385ms step_avg:61.09ms
step:1104/2330 train_time:67448ms step_avg:61.09ms
step:1105/2330 train_time:67507ms step_avg:61.09ms
step:1106/2330 train_time:67570ms step_avg:61.09ms
step:1107/2330 train_time:67630ms step_avg:61.09ms
step:1108/2330 train_time:67693ms step_avg:61.09ms
step:1109/2330 train_time:67753ms step_avg:61.09ms
step:1110/2330 train_time:67817ms step_avg:61.10ms
step:1111/2330 train_time:67876ms step_avg:61.09ms
step:1112/2330 train_time:67939ms step_avg:61.10ms
step:1113/2330 train_time:67998ms step_avg:61.09ms
step:1114/2330 train_time:68061ms step_avg:61.10ms
step:1115/2330 train_time:68121ms step_avg:61.10ms
step:1116/2330 train_time:68184ms step_avg:61.10ms
step:1117/2330 train_time:68244ms step_avg:61.10ms
step:1118/2330 train_time:68307ms step_avg:61.10ms
step:1119/2330 train_time:68367ms step_avg:61.10ms
step:1120/2330 train_time:68430ms step_avg:61.10ms
step:1121/2330 train_time:68490ms step_avg:61.10ms
step:1122/2330 train_time:68552ms step_avg:61.10ms
step:1123/2330 train_time:68612ms step_avg:61.10ms
step:1124/2330 train_time:68676ms step_avg:61.10ms
step:1125/2330 train_time:68736ms step_avg:61.10ms
step:1126/2330 train_time:68799ms step_avg:61.10ms
step:1127/2330 train_time:68858ms step_avg:61.10ms
step:1128/2330 train_time:68921ms step_avg:61.10ms
step:1129/2330 train_time:68981ms step_avg:61.10ms
step:1130/2330 train_time:69044ms step_avg:61.10ms
step:1131/2330 train_time:69103ms step_avg:61.10ms
step:1132/2330 train_time:69167ms step_avg:61.10ms
step:1133/2330 train_time:69226ms step_avg:61.10ms
step:1134/2330 train_time:69289ms step_avg:61.10ms
step:1135/2330 train_time:69349ms step_avg:61.10ms
step:1136/2330 train_time:69413ms step_avg:61.10ms
step:1137/2330 train_time:69472ms step_avg:61.10ms
step:1138/2330 train_time:69535ms step_avg:61.10ms
step:1139/2330 train_time:69595ms step_avg:61.10ms
step:1140/2330 train_time:69657ms step_avg:61.10ms
step:1141/2330 train_time:69717ms step_avg:61.10ms
step:1142/2330 train_time:69780ms step_avg:61.10ms
step:1143/2330 train_time:69839ms step_avg:61.10ms
step:1144/2330 train_time:69902ms step_avg:61.10ms
step:1145/2330 train_time:69962ms step_avg:61.10ms
step:1146/2330 train_time:70026ms step_avg:61.10ms
step:1147/2330 train_time:70085ms step_avg:61.10ms
step:1148/2330 train_time:70148ms step_avg:61.10ms
step:1149/2330 train_time:70208ms step_avg:61.10ms
step:1150/2330 train_time:70271ms step_avg:61.10ms
step:1151/2330 train_time:70330ms step_avg:61.10ms
step:1152/2330 train_time:70394ms step_avg:61.11ms
step:1153/2330 train_time:70454ms step_avg:61.11ms
step:1154/2330 train_time:70518ms step_avg:61.11ms
step:1155/2330 train_time:70577ms step_avg:61.11ms
step:1156/2330 train_time:70640ms step_avg:61.11ms
step:1157/2330 train_time:70699ms step_avg:61.11ms
step:1158/2330 train_time:70763ms step_avg:61.11ms
step:1159/2330 train_time:70823ms step_avg:61.11ms
step:1160/2330 train_time:70885ms step_avg:61.11ms
step:1161/2330 train_time:70944ms step_avg:61.11ms
step:1162/2330 train_time:71007ms step_avg:61.11ms
step:1163/2330 train_time:71068ms step_avg:61.11ms
step:1164/2330 train_time:71131ms step_avg:61.11ms
step:1165/2330 train_time:71191ms step_avg:61.11ms
step:1166/2330 train_time:71254ms step_avg:61.11ms
step:1167/2330 train_time:71315ms step_avg:61.11ms
step:1168/2330 train_time:71377ms step_avg:61.11ms
step:1169/2330 train_time:71437ms step_avg:61.11ms
step:1170/2330 train_time:71499ms step_avg:61.11ms
step:1171/2330 train_time:71559ms step_avg:61.11ms
step:1172/2330 train_time:71622ms step_avg:61.11ms
step:1173/2330 train_time:71681ms step_avg:61.11ms
step:1174/2330 train_time:71744ms step_avg:61.11ms
step:1175/2330 train_time:71804ms step_avg:61.11ms
step:1176/2330 train_time:71867ms step_avg:61.11ms
step:1177/2330 train_time:71927ms step_avg:61.11ms
step:1178/2330 train_time:71990ms step_avg:61.11ms
step:1179/2330 train_time:72051ms step_avg:61.11ms
step:1180/2330 train_time:72114ms step_avg:61.11ms
step:1181/2330 train_time:72174ms step_avg:61.11ms
step:1182/2330 train_time:72237ms step_avg:61.11ms
step:1183/2330 train_time:72296ms step_avg:61.11ms
step:1184/2330 train_time:72359ms step_avg:61.11ms
step:1185/2330 train_time:72419ms step_avg:61.11ms
step:1186/2330 train_time:72481ms step_avg:61.11ms
step:1187/2330 train_time:72541ms step_avg:61.11ms
step:1188/2330 train_time:72604ms step_avg:61.11ms
step:1189/2330 train_time:72663ms step_avg:61.11ms
step:1190/2330 train_time:72726ms step_avg:61.11ms
step:1191/2330 train_time:72786ms step_avg:61.11ms
step:1192/2330 train_time:72849ms step_avg:61.11ms
step:1193/2330 train_time:72909ms step_avg:61.11ms
step:1194/2330 train_time:72972ms step_avg:61.12ms
step:1195/2330 train_time:73032ms step_avg:61.11ms
step:1196/2330 train_time:73095ms step_avg:61.12ms
step:1197/2330 train_time:73156ms step_avg:61.12ms
step:1198/2330 train_time:73218ms step_avg:61.12ms
step:1199/2330 train_time:73278ms step_avg:61.12ms
step:1200/2330 train_time:73341ms step_avg:61.12ms
step:1201/2330 train_time:73401ms step_avg:61.12ms
step:1202/2330 train_time:73464ms step_avg:61.12ms
step:1203/2330 train_time:73523ms step_avg:61.12ms
step:1204/2330 train_time:73586ms step_avg:61.12ms
step:1205/2330 train_time:73646ms step_avg:61.12ms
step:1206/2330 train_time:73709ms step_avg:61.12ms
step:1207/2330 train_time:73769ms step_avg:61.12ms
step:1208/2330 train_time:73832ms step_avg:61.12ms
step:1209/2330 train_time:73892ms step_avg:61.12ms
step:1210/2330 train_time:73955ms step_avg:61.12ms
step:1211/2330 train_time:74016ms step_avg:61.12ms
step:1212/2330 train_time:74080ms step_avg:61.12ms
step:1213/2330 train_time:74139ms step_avg:61.12ms
step:1214/2330 train_time:74202ms step_avg:61.12ms
step:1215/2330 train_time:74262ms step_avg:61.12ms
step:1216/2330 train_time:74326ms step_avg:61.12ms
step:1217/2330 train_time:74386ms step_avg:61.12ms
step:1218/2330 train_time:74449ms step_avg:61.12ms
step:1219/2330 train_time:74508ms step_avg:61.12ms
step:1220/2330 train_time:74572ms step_avg:61.12ms
step:1221/2330 train_time:74631ms step_avg:61.12ms
step:1222/2330 train_time:74694ms step_avg:61.12ms
step:1223/2330 train_time:74754ms step_avg:61.12ms
step:1224/2330 train_time:74817ms step_avg:61.12ms
step:1225/2330 train_time:74877ms step_avg:61.12ms
step:1226/2330 train_time:74939ms step_avg:61.13ms
step:1227/2330 train_time:74999ms step_avg:61.12ms
step:1228/2330 train_time:75062ms step_avg:61.13ms
step:1229/2330 train_time:75121ms step_avg:61.12ms
step:1230/2330 train_time:75184ms step_avg:61.13ms
step:1231/2330 train_time:75244ms step_avg:61.12ms
step:1232/2330 train_time:75307ms step_avg:61.13ms
step:1233/2330 train_time:75368ms step_avg:61.13ms
step:1234/2330 train_time:75431ms step_avg:61.13ms
step:1235/2330 train_time:75491ms step_avg:61.13ms
step:1236/2330 train_time:75554ms step_avg:61.13ms
step:1237/2330 train_time:75613ms step_avg:61.13ms
step:1238/2330 train_time:75676ms step_avg:61.13ms
step:1239/2330 train_time:75736ms step_avg:61.13ms
step:1240/2330 train_time:75798ms step_avg:61.13ms
step:1241/2330 train_time:75858ms step_avg:61.13ms
step:1242/2330 train_time:75921ms step_avg:61.13ms
step:1243/2330 train_time:75981ms step_avg:61.13ms
step:1244/2330 train_time:76044ms step_avg:61.13ms
step:1245/2330 train_time:76104ms step_avg:61.13ms
step:1246/2330 train_time:76166ms step_avg:61.13ms
step:1247/2330 train_time:76226ms step_avg:61.13ms
step:1248/2330 train_time:76289ms step_avg:61.13ms
step:1249/2330 train_time:76348ms step_avg:61.13ms
step:1250/2330 train_time:76412ms step_avg:61.13ms
step:1250/2330 val_loss:3.5174 train_time:76477ms step_avg:61.18ms
step:1251/2330 train_time:76500ms step_avg:61.15ms
step:1252/2330 train_time:76539ms step_avg:61.13ms
step:1253/2330 train_time:76605ms step_avg:61.14ms
step:1254/2330 train_time:76670ms step_avg:61.14ms
step:1255/2330 train_time:76730ms step_avg:61.14ms
step:1256/2330 train_time:76795ms step_avg:61.14ms
step:1257/2330 train_time:76854ms step_avg:61.14ms
step:1258/2330 train_time:76917ms step_avg:61.14ms
step:1259/2330 train_time:76977ms step_avg:61.14ms
step:1260/2330 train_time:77040ms step_avg:61.14ms
step:1261/2330 train_time:77100ms step_avg:61.14ms
step:1262/2330 train_time:77162ms step_avg:61.14ms
step:1263/2330 train_time:77221ms step_avg:61.14ms
step:1264/2330 train_time:77283ms step_avg:61.14ms
step:1265/2330 train_time:77342ms step_avg:61.14ms
step:1266/2330 train_time:77404ms step_avg:61.14ms
step:1267/2330 train_time:77464ms step_avg:61.14ms
step:1268/2330 train_time:77528ms step_avg:61.14ms
step:1269/2330 train_time:77588ms step_avg:61.14ms
step:1270/2330 train_time:77652ms step_avg:61.14ms
step:1271/2330 train_time:77713ms step_avg:61.14ms
step:1272/2330 train_time:77776ms step_avg:61.14ms
step:1273/2330 train_time:77836ms step_avg:61.14ms
step:1274/2330 train_time:77899ms step_avg:61.14ms
step:1275/2330 train_time:77959ms step_avg:61.14ms
step:1276/2330 train_time:78022ms step_avg:61.15ms
step:1277/2330 train_time:78081ms step_avg:61.14ms
step:1278/2330 train_time:78143ms step_avg:61.15ms
step:1279/2330 train_time:78203ms step_avg:61.14ms
step:1280/2330 train_time:78265ms step_avg:61.14ms
step:1281/2330 train_time:78325ms step_avg:61.14ms
step:1282/2330 train_time:78387ms step_avg:61.14ms
step:1283/2330 train_time:78447ms step_avg:61.14ms
step:1284/2330 train_time:78510ms step_avg:61.14ms
step:1285/2330 train_time:78570ms step_avg:61.14ms
step:1286/2330 train_time:78634ms step_avg:61.15ms
step:1287/2330 train_time:78695ms step_avg:61.15ms
step:1288/2330 train_time:78759ms step_avg:61.15ms
step:1289/2330 train_time:78818ms step_avg:61.15ms
step:1290/2330 train_time:78881ms step_avg:61.15ms
step:1291/2330 train_time:78942ms step_avg:61.15ms
step:1292/2330 train_time:79005ms step_avg:61.15ms
step:1293/2330 train_time:79064ms step_avg:61.15ms
step:1294/2330 train_time:79126ms step_avg:61.15ms
step:1295/2330 train_time:79186ms step_avg:61.15ms
step:1296/2330 train_time:79248ms step_avg:61.15ms
step:1297/2330 train_time:79307ms step_avg:61.15ms
step:1298/2330 train_time:79370ms step_avg:61.15ms
step:1299/2330 train_time:79430ms step_avg:61.15ms
step:1300/2330 train_time:79493ms step_avg:61.15ms
step:1301/2330 train_time:79553ms step_avg:61.15ms
step:1302/2330 train_time:79617ms step_avg:61.15ms
step:1303/2330 train_time:79677ms step_avg:61.15ms
step:1304/2330 train_time:79740ms step_avg:61.15ms
step:1305/2330 train_time:79800ms step_avg:61.15ms
step:1306/2330 train_time:79864ms step_avg:61.15ms
step:1307/2330 train_time:79923ms step_avg:61.15ms
step:1308/2330 train_time:79986ms step_avg:61.15ms
step:1309/2330 train_time:80045ms step_avg:61.15ms
step:1310/2330 train_time:80108ms step_avg:61.15ms
step:1311/2330 train_time:80168ms step_avg:61.15ms
step:1312/2330 train_time:80231ms step_avg:61.15ms
step:1313/2330 train_time:80292ms step_avg:61.15ms
step:1314/2330 train_time:80355ms step_avg:61.15ms
step:1315/2330 train_time:80415ms step_avg:61.15ms
step:1316/2330 train_time:80478ms step_avg:61.15ms
step:1317/2330 train_time:80538ms step_avg:61.15ms
step:1318/2330 train_time:80601ms step_avg:61.15ms
step:1319/2330 train_time:80661ms step_avg:61.15ms
step:1320/2330 train_time:80724ms step_avg:61.15ms
step:1321/2330 train_time:80784ms step_avg:61.15ms
step:1322/2330 train_time:80848ms step_avg:61.16ms
step:1323/2330 train_time:80907ms step_avg:61.15ms
step:1324/2330 train_time:80970ms step_avg:61.16ms
step:1325/2330 train_time:81030ms step_avg:61.15ms
step:1326/2330 train_time:81093ms step_avg:61.16ms
step:1327/2330 train_time:81153ms step_avg:61.16ms
step:1328/2330 train_time:81216ms step_avg:61.16ms
step:1329/2330 train_time:81277ms step_avg:61.16ms
step:1330/2330 train_time:81340ms step_avg:61.16ms
step:1331/2330 train_time:81400ms step_avg:61.16ms
step:1332/2330 train_time:81462ms step_avg:61.16ms
step:1333/2330 train_time:81522ms step_avg:61.16ms
step:1334/2330 train_time:81585ms step_avg:61.16ms
step:1335/2330 train_time:81646ms step_avg:61.16ms
step:1336/2330 train_time:81709ms step_avg:61.16ms
step:1337/2330 train_time:81769ms step_avg:61.16ms
step:1338/2330 train_time:81832ms step_avg:61.16ms
step:1339/2330 train_time:81892ms step_avg:61.16ms
step:1340/2330 train_time:81956ms step_avg:61.16ms
step:1341/2330 train_time:82016ms step_avg:61.16ms
step:1342/2330 train_time:82079ms step_avg:61.16ms
step:1343/2330 train_time:82139ms step_avg:61.16ms
step:1344/2330 train_time:82202ms step_avg:61.16ms
step:1345/2330 train_time:82261ms step_avg:61.16ms
step:1346/2330 train_time:82324ms step_avg:61.16ms
step:1347/2330 train_time:82384ms step_avg:61.16ms
step:1348/2330 train_time:82446ms step_avg:61.16ms
step:1349/2330 train_time:82506ms step_avg:61.16ms
step:1350/2330 train_time:82568ms step_avg:61.16ms
step:1351/2330 train_time:82628ms step_avg:61.16ms
step:1352/2330 train_time:82692ms step_avg:61.16ms
step:1353/2330 train_time:82752ms step_avg:61.16ms
step:1354/2330 train_time:82815ms step_avg:61.16ms
step:1355/2330 train_time:82876ms step_avg:61.16ms
step:1356/2330 train_time:82939ms step_avg:61.16ms
step:1357/2330 train_time:83000ms step_avg:61.16ms
step:1358/2330 train_time:83062ms step_avg:61.17ms
step:1359/2330 train_time:83122ms step_avg:61.16ms
step:1360/2330 train_time:83184ms step_avg:61.16ms
step:1361/2330 train_time:83243ms step_avg:61.16ms
step:1362/2330 train_time:83306ms step_avg:61.16ms
step:1363/2330 train_time:83366ms step_avg:61.16ms
step:1364/2330 train_time:83428ms step_avg:61.16ms
step:1365/2330 train_time:83488ms step_avg:61.16ms
step:1366/2330 train_time:83551ms step_avg:61.16ms
step:1367/2330 train_time:83610ms step_avg:61.16ms
step:1368/2330 train_time:83674ms step_avg:61.16ms
step:1369/2330 train_time:83734ms step_avg:61.16ms
step:1370/2330 train_time:83797ms step_avg:61.17ms
step:1371/2330 train_time:83857ms step_avg:61.17ms
step:1372/2330 train_time:83921ms step_avg:61.17ms
step:1373/2330 train_time:83981ms step_avg:61.17ms
step:1374/2330 train_time:84044ms step_avg:61.17ms
step:1375/2330 train_time:84104ms step_avg:61.17ms
step:1376/2330 train_time:84167ms step_avg:61.17ms
step:1377/2330 train_time:84227ms step_avg:61.17ms
step:1378/2330 train_time:84289ms step_avg:61.17ms
step:1379/2330 train_time:84349ms step_avg:61.17ms
step:1380/2330 train_time:84412ms step_avg:61.17ms
step:1381/2330 train_time:84472ms step_avg:61.17ms
step:1382/2330 train_time:84536ms step_avg:61.17ms
step:1383/2330 train_time:84596ms step_avg:61.17ms
step:1384/2330 train_time:84659ms step_avg:61.17ms
step:1385/2330 train_time:84719ms step_avg:61.17ms
step:1386/2330 train_time:84782ms step_avg:61.17ms
step:1387/2330 train_time:84842ms step_avg:61.17ms
step:1388/2330 train_time:84905ms step_avg:61.17ms
step:1389/2330 train_time:84965ms step_avg:61.17ms
step:1390/2330 train_time:85028ms step_avg:61.17ms
step:1391/2330 train_time:85088ms step_avg:61.17ms
step:1392/2330 train_time:85152ms step_avg:61.17ms
step:1393/2330 train_time:85211ms step_avg:61.17ms
step:1394/2330 train_time:85275ms step_avg:61.17ms
step:1395/2330 train_time:85336ms step_avg:61.17ms
step:1396/2330 train_time:85399ms step_avg:61.17ms
step:1397/2330 train_time:85458ms step_avg:61.17ms
step:1398/2330 train_time:85521ms step_avg:61.17ms
step:1399/2330 train_time:85581ms step_avg:61.17ms
step:1400/2330 train_time:85644ms step_avg:61.17ms
step:1401/2330 train_time:85704ms step_avg:61.17ms
step:1402/2330 train_time:85766ms step_avg:61.17ms
step:1403/2330 train_time:85826ms step_avg:61.17ms
step:1404/2330 train_time:85890ms step_avg:61.17ms
step:1405/2330 train_time:85950ms step_avg:61.17ms
step:1406/2330 train_time:86013ms step_avg:61.18ms
step:1407/2330 train_time:86073ms step_avg:61.17ms
step:1408/2330 train_time:86136ms step_avg:61.18ms
step:1409/2330 train_time:86196ms step_avg:61.18ms
step:1410/2330 train_time:86259ms step_avg:61.18ms
step:1411/2330 train_time:86319ms step_avg:61.18ms
step:1412/2330 train_time:86382ms step_avg:61.18ms
step:1413/2330 train_time:86442ms step_avg:61.18ms
step:1414/2330 train_time:86505ms step_avg:61.18ms
step:1415/2330 train_time:86565ms step_avg:61.18ms
step:1416/2330 train_time:86628ms step_avg:61.18ms
step:1417/2330 train_time:86688ms step_avg:61.18ms
step:1418/2330 train_time:86752ms step_avg:61.18ms
step:1419/2330 train_time:86812ms step_avg:61.18ms
step:1420/2330 train_time:86875ms step_avg:61.18ms
step:1421/2330 train_time:86936ms step_avg:61.18ms
step:1422/2330 train_time:86999ms step_avg:61.18ms
step:1423/2330 train_time:87058ms step_avg:61.18ms
step:1424/2330 train_time:87121ms step_avg:61.18ms
step:1425/2330 train_time:87181ms step_avg:61.18ms
step:1426/2330 train_time:87244ms step_avg:61.18ms
step:1427/2330 train_time:87303ms step_avg:61.18ms
step:1428/2330 train_time:87366ms step_avg:61.18ms
step:1429/2330 train_time:87426ms step_avg:61.18ms
step:1430/2330 train_time:87489ms step_avg:61.18ms
step:1431/2330 train_time:87548ms step_avg:61.18ms
step:1432/2330 train_time:87612ms step_avg:61.18ms
step:1433/2330 train_time:87672ms step_avg:61.18ms
step:1434/2330 train_time:87735ms step_avg:61.18ms
step:1435/2330 train_time:87796ms step_avg:61.18ms
step:1436/2330 train_time:87859ms step_avg:61.18ms
step:1437/2330 train_time:87920ms step_avg:61.18ms
step:1438/2330 train_time:87983ms step_avg:61.18ms
step:1439/2330 train_time:88043ms step_avg:61.18ms
step:1440/2330 train_time:88106ms step_avg:61.18ms
step:1441/2330 train_time:88165ms step_avg:61.18ms
step:1442/2330 train_time:88228ms step_avg:61.18ms
step:1443/2330 train_time:88288ms step_avg:61.18ms
step:1444/2330 train_time:88351ms step_avg:61.18ms
step:1445/2330 train_time:88410ms step_avg:61.18ms
step:1446/2330 train_time:88473ms step_avg:61.18ms
step:1447/2330 train_time:88533ms step_avg:61.18ms
step:1448/2330 train_time:88597ms step_avg:61.19ms
step:1449/2330 train_time:88658ms step_avg:61.19ms
step:1450/2330 train_time:88721ms step_avg:61.19ms
step:1451/2330 train_time:88781ms step_avg:61.19ms
step:1452/2330 train_time:88844ms step_avg:61.19ms
step:1453/2330 train_time:88903ms step_avg:61.19ms
step:1454/2330 train_time:88966ms step_avg:61.19ms
step:1455/2330 train_time:89026ms step_avg:61.19ms
step:1456/2330 train_time:89089ms step_avg:61.19ms
step:1457/2330 train_time:89148ms step_avg:61.19ms
step:1458/2330 train_time:89211ms step_avg:61.19ms
step:1459/2330 train_time:89270ms step_avg:61.19ms
step:1460/2330 train_time:89333ms step_avg:61.19ms
step:1461/2330 train_time:89394ms step_avg:61.19ms
step:1462/2330 train_time:89457ms step_avg:61.19ms
step:1463/2330 train_time:89517ms step_avg:61.19ms
step:1464/2330 train_time:89580ms step_avg:61.19ms
step:1465/2330 train_time:89640ms step_avg:61.19ms
step:1466/2330 train_time:89703ms step_avg:61.19ms
step:1467/2330 train_time:89762ms step_avg:61.19ms
step:1468/2330 train_time:89825ms step_avg:61.19ms
step:1469/2330 train_time:89885ms step_avg:61.19ms
step:1470/2330 train_time:89948ms step_avg:61.19ms
step:1471/2330 train_time:90008ms step_avg:61.19ms
step:1472/2330 train_time:90072ms step_avg:61.19ms
step:1473/2330 train_time:90132ms step_avg:61.19ms
step:1474/2330 train_time:90195ms step_avg:61.19ms
step:1475/2330 train_time:90255ms step_avg:61.19ms
step:1476/2330 train_time:90318ms step_avg:61.19ms
step:1477/2330 train_time:90379ms step_avg:61.19ms
step:1478/2330 train_time:90442ms step_avg:61.19ms
step:1479/2330 train_time:90502ms step_avg:61.19ms
step:1480/2330 train_time:90564ms step_avg:61.19ms
step:1481/2330 train_time:90624ms step_avg:61.19ms
step:1482/2330 train_time:90687ms step_avg:61.19ms
step:1483/2330 train_time:90747ms step_avg:61.19ms
step:1484/2330 train_time:90809ms step_avg:61.19ms
step:1485/2330 train_time:90869ms step_avg:61.19ms
step:1486/2330 train_time:90932ms step_avg:61.19ms
step:1487/2330 train_time:90993ms step_avg:61.19ms
step:1488/2330 train_time:91056ms step_avg:61.19ms
step:1489/2330 train_time:91116ms step_avg:61.19ms
step:1490/2330 train_time:91179ms step_avg:61.19ms
step:1491/2330 train_time:91239ms step_avg:61.19ms
step:1492/2330 train_time:91302ms step_avg:61.19ms
step:1493/2330 train_time:91361ms step_avg:61.19ms
step:1494/2330 train_time:91425ms step_avg:61.19ms
step:1495/2330 train_time:91485ms step_avg:61.19ms
step:1496/2330 train_time:91548ms step_avg:61.20ms
step:1497/2330 train_time:91608ms step_avg:61.19ms
step:1498/2330 train_time:91671ms step_avg:61.20ms
step:1499/2330 train_time:91731ms step_avg:61.20ms
step:1500/2330 train_time:91795ms step_avg:61.20ms
step:1500/2330 val_loss:3.4759 train_time:91860ms step_avg:61.24ms
step:1501/2330 train_time:91882ms step_avg:61.21ms
step:1502/2330 train_time:91921ms step_avg:61.20ms
step:1503/2330 train_time:91985ms step_avg:61.20ms
step:1504/2330 train_time:92051ms step_avg:61.20ms
step:1505/2330 train_time:92113ms step_avg:61.20ms
step:1506/2330 train_time:92177ms step_avg:61.21ms
step:1507/2330 train_time:92237ms step_avg:61.21ms
step:1508/2330 train_time:92300ms step_avg:61.21ms
step:1509/2330 train_time:92359ms step_avg:61.21ms
step:1510/2330 train_time:92421ms step_avg:61.21ms
step:1511/2330 train_time:92481ms step_avg:61.20ms
step:1512/2330 train_time:92543ms step_avg:61.21ms
step:1513/2330 train_time:92602ms step_avg:61.20ms
step:1514/2330 train_time:92664ms step_avg:61.20ms
step:1515/2330 train_time:92723ms step_avg:61.20ms
step:1516/2330 train_time:92785ms step_avg:61.20ms
step:1517/2330 train_time:92845ms step_avg:61.20ms
step:1518/2330 train_time:92909ms step_avg:61.21ms
step:1519/2330 train_time:92970ms step_avg:61.21ms
step:1520/2330 train_time:93034ms step_avg:61.21ms
step:1521/2330 train_time:93095ms step_avg:61.21ms
step:1522/2330 train_time:93160ms step_avg:61.21ms
step:1523/2330 train_time:93220ms step_avg:61.21ms
step:1524/2330 train_time:93283ms step_avg:61.21ms
step:1525/2330 train_time:93343ms step_avg:61.21ms
step:1526/2330 train_time:93405ms step_avg:61.21ms
step:1527/2330 train_time:93465ms step_avg:61.21ms
step:1528/2330 train_time:93527ms step_avg:61.21ms
step:1529/2330 train_time:93586ms step_avg:61.21ms
step:1530/2330 train_time:93649ms step_avg:61.21ms
step:1531/2330 train_time:93710ms step_avg:61.21ms
step:1532/2330 train_time:93773ms step_avg:61.21ms
step:1533/2330 train_time:93833ms step_avg:61.21ms
step:1534/2330 train_time:93898ms step_avg:61.21ms
step:1535/2330 train_time:93960ms step_avg:61.21ms
step:1536/2330 train_time:94024ms step_avg:61.21ms
step:1537/2330 train_time:94085ms step_avg:61.21ms
step:1538/2330 train_time:94148ms step_avg:61.21ms
step:1539/2330 train_time:94208ms step_avg:61.21ms
step:1540/2330 train_time:94273ms step_avg:61.22ms
step:1541/2330 train_time:94333ms step_avg:61.22ms
step:1542/2330 train_time:94398ms step_avg:61.22ms
step:1543/2330 train_time:94460ms step_avg:61.22ms
step:1544/2330 train_time:94522ms step_avg:61.22ms
step:1545/2330 train_time:94582ms step_avg:61.22ms
step:1546/2330 train_time:94645ms step_avg:61.22ms
step:1547/2330 train_time:94705ms step_avg:61.22ms
step:1548/2330 train_time:94768ms step_avg:61.22ms
step:1549/2330 train_time:94828ms step_avg:61.22ms
step:1550/2330 train_time:94892ms step_avg:61.22ms
step:1551/2330 train_time:94953ms step_avg:61.22ms
step:1552/2330 train_time:95017ms step_avg:61.22ms
step:1553/2330 train_time:95079ms step_avg:61.22ms
step:1554/2330 train_time:95143ms step_avg:61.22ms
step:1555/2330 train_time:95203ms step_avg:61.22ms
step:1556/2330 train_time:95267ms step_avg:61.23ms
step:1557/2330 train_time:95328ms step_avg:61.23ms
step:1558/2330 train_time:95391ms step_avg:61.23ms
step:1559/2330 train_time:95451ms step_avg:61.23ms
step:1560/2330 train_time:95515ms step_avg:61.23ms
step:1561/2330 train_time:95575ms step_avg:61.23ms
step:1562/2330 train_time:95639ms step_avg:61.23ms
step:1563/2330 train_time:95699ms step_avg:61.23ms
step:1564/2330 train_time:95763ms step_avg:61.23ms
step:1565/2330 train_time:95824ms step_avg:61.23ms
step:1566/2330 train_time:95887ms step_avg:61.23ms
step:1567/2330 train_time:95947ms step_avg:61.23ms
step:1568/2330 train_time:96010ms step_avg:61.23ms
step:1569/2330 train_time:96071ms step_avg:61.23ms
step:1570/2330 train_time:96134ms step_avg:61.23ms
step:1571/2330 train_time:96196ms step_avg:61.23ms
step:1572/2330 train_time:96260ms step_avg:61.23ms
step:1573/2330 train_time:96320ms step_avg:61.23ms
step:1574/2330 train_time:96382ms step_avg:61.23ms
step:1575/2330 train_time:96443ms step_avg:61.23ms
step:1576/2330 train_time:96506ms step_avg:61.23ms
step:1577/2330 train_time:96566ms step_avg:61.23ms
step:1578/2330 train_time:96629ms step_avg:61.24ms
step:1579/2330 train_time:96689ms step_avg:61.23ms
step:1580/2330 train_time:96753ms step_avg:61.24ms
step:1581/2330 train_time:96814ms step_avg:61.24ms
step:1582/2330 train_time:96877ms step_avg:61.24ms
step:1583/2330 train_time:96937ms step_avg:61.24ms
step:1584/2330 train_time:97001ms step_avg:61.24ms
step:1585/2330 train_time:97062ms step_avg:61.24ms
step:1586/2330 train_time:97126ms step_avg:61.24ms
step:1587/2330 train_time:97186ms step_avg:61.24ms
step:1588/2330 train_time:97250ms step_avg:61.24ms
step:1589/2330 train_time:97310ms step_avg:61.24ms
step:1590/2330 train_time:97375ms step_avg:61.24ms
step:1591/2330 train_time:97436ms step_avg:61.24ms
step:1592/2330 train_time:97499ms step_avg:61.24ms
step:1593/2330 train_time:97560ms step_avg:61.24ms
step:1594/2330 train_time:97623ms step_avg:61.24ms
step:1595/2330 train_time:97684ms step_avg:61.24ms
step:1596/2330 train_time:97747ms step_avg:61.25ms
step:1597/2330 train_time:97808ms step_avg:61.25ms
step:1598/2330 train_time:97871ms step_avg:61.25ms
step:1599/2330 train_time:97932ms step_avg:61.25ms
step:1600/2330 train_time:97996ms step_avg:61.25ms
step:1601/2330 train_time:98057ms step_avg:61.25ms
step:1602/2330 train_time:98120ms step_avg:61.25ms
step:1603/2330 train_time:98180ms step_avg:61.25ms
step:1604/2330 train_time:98243ms step_avg:61.25ms
step:1605/2330 train_time:98304ms step_avg:61.25ms
step:1606/2330 train_time:98368ms step_avg:61.25ms
step:1607/2330 train_time:98428ms step_avg:61.25ms
step:1608/2330 train_time:98492ms step_avg:61.25ms
step:1609/2330 train_time:98552ms step_avg:61.25ms
step:1610/2330 train_time:98616ms step_avg:61.25ms
step:1611/2330 train_time:98677ms step_avg:61.25ms
step:1612/2330 train_time:98740ms step_avg:61.25ms
step:1613/2330 train_time:98801ms step_avg:61.25ms
step:1614/2330 train_time:98864ms step_avg:61.25ms
step:1615/2330 train_time:98925ms step_avg:61.25ms
step:1616/2330 train_time:98988ms step_avg:61.25ms
step:1617/2330 train_time:99048ms step_avg:61.25ms
step:1618/2330 train_time:99111ms step_avg:61.26ms
step:1619/2330 train_time:99171ms step_avg:61.25ms
step:1620/2330 train_time:99235ms step_avg:61.26ms
step:1621/2330 train_time:99296ms step_avg:61.26ms
step:1622/2330 train_time:99360ms step_avg:61.26ms
step:1623/2330 train_time:99420ms step_avg:61.26ms
step:1624/2330 train_time:99483ms step_avg:61.26ms
step:1625/2330 train_time:99544ms step_avg:61.26ms
step:1626/2330 train_time:99607ms step_avg:61.26ms
step:1627/2330 train_time:99667ms step_avg:61.26ms
step:1628/2330 train_time:99731ms step_avg:61.26ms
step:1629/2330 train_time:99791ms step_avg:61.26ms
step:1630/2330 train_time:99855ms step_avg:61.26ms
step:1631/2330 train_time:99916ms step_avg:61.26ms
step:1632/2330 train_time:99979ms step_avg:61.26ms
step:1633/2330 train_time:100040ms step_avg:61.26ms
step:1634/2330 train_time:100103ms step_avg:61.26ms
step:1635/2330 train_time:100163ms step_avg:61.26ms
step:1636/2330 train_time:100226ms step_avg:61.26ms
step:1637/2330 train_time:100286ms step_avg:61.26ms
step:1638/2330 train_time:100350ms step_avg:61.26ms
step:1639/2330 train_time:100411ms step_avg:61.26ms
step:1640/2330 train_time:100476ms step_avg:61.27ms
step:1641/2330 train_time:100537ms step_avg:61.27ms
step:1642/2330 train_time:100600ms step_avg:61.27ms
step:1643/2330 train_time:100661ms step_avg:61.27ms
step:1644/2330 train_time:100725ms step_avg:61.27ms
step:1645/2330 train_time:100785ms step_avg:61.27ms
step:1646/2330 train_time:100849ms step_avg:61.27ms
step:1647/2330 train_time:100909ms step_avg:61.27ms
step:1648/2330 train_time:100973ms step_avg:61.27ms
step:1649/2330 train_time:101033ms step_avg:61.27ms
step:1650/2330 train_time:101097ms step_avg:61.27ms
step:1651/2330 train_time:101157ms step_avg:61.27ms
step:1652/2330 train_time:101221ms step_avg:61.27ms
step:1653/2330 train_time:101281ms step_avg:61.27ms
step:1654/2330 train_time:101345ms step_avg:61.27ms
step:1655/2330 train_time:101404ms step_avg:61.27ms
step:1656/2330 train_time:101468ms step_avg:61.27ms
step:1657/2330 train_time:101528ms step_avg:61.27ms
step:1658/2330 train_time:101591ms step_avg:61.27ms
step:1659/2330 train_time:101652ms step_avg:61.27ms
step:1660/2330 train_time:101716ms step_avg:61.27ms
step:1661/2330 train_time:101776ms step_avg:61.27ms
step:1662/2330 train_time:101839ms step_avg:61.28ms
step:1663/2330 train_time:101899ms step_avg:61.27ms
step:1664/2330 train_time:101963ms step_avg:61.28ms
step:1665/2330 train_time:102023ms step_avg:61.28ms
step:1666/2330 train_time:102087ms step_avg:61.28ms
step:1667/2330 train_time:102147ms step_avg:61.28ms
step:1668/2330 train_time:102210ms step_avg:61.28ms
step:1669/2330 train_time:102271ms step_avg:61.28ms
step:1670/2330 train_time:102334ms step_avg:61.28ms
step:1671/2330 train_time:102395ms step_avg:61.28ms
step:1672/2330 train_time:102460ms step_avg:61.28ms
step:1673/2330 train_time:102520ms step_avg:61.28ms
step:1674/2330 train_time:102583ms step_avg:61.28ms
step:1675/2330 train_time:102644ms step_avg:61.28ms
step:1676/2330 train_time:102707ms step_avg:61.28ms
step:1677/2330 train_time:102767ms step_avg:61.28ms
step:1678/2330 train_time:102830ms step_avg:61.28ms
step:1679/2330 train_time:102891ms step_avg:61.28ms
step:1680/2330 train_time:102955ms step_avg:61.28ms
step:1681/2330 train_time:103015ms step_avg:61.28ms
step:1682/2330 train_time:103079ms step_avg:61.28ms
step:1683/2330 train_time:103139ms step_avg:61.28ms
step:1684/2330 train_time:103203ms step_avg:61.28ms
step:1685/2330 train_time:103263ms step_avg:61.28ms
step:1686/2330 train_time:103327ms step_avg:61.29ms
step:1687/2330 train_time:103387ms step_avg:61.28ms
step:1688/2330 train_time:103451ms step_avg:61.29ms
step:1689/2330 train_time:103511ms step_avg:61.29ms
step:1690/2330 train_time:103575ms step_avg:61.29ms
step:1691/2330 train_time:103636ms step_avg:61.29ms
step:1692/2330 train_time:103699ms step_avg:61.29ms
step:1693/2330 train_time:103759ms step_avg:61.29ms
step:1694/2330 train_time:103822ms step_avg:61.29ms
step:1695/2330 train_time:103882ms step_avg:61.29ms
step:1696/2330 train_time:103945ms step_avg:61.29ms
step:1697/2330 train_time:104005ms step_avg:61.29ms
step:1698/2330 train_time:104069ms step_avg:61.29ms
step:1699/2330 train_time:104129ms step_avg:61.29ms
step:1700/2330 train_time:104194ms step_avg:61.29ms
step:1701/2330 train_time:104254ms step_avg:61.29ms
step:1702/2330 train_time:104318ms step_avg:61.29ms
step:1703/2330 train_time:104378ms step_avg:61.29ms
step:1704/2330 train_time:104441ms step_avg:61.29ms
step:1705/2330 train_time:104501ms step_avg:61.29ms
step:1706/2330 train_time:104565ms step_avg:61.29ms
step:1707/2330 train_time:104625ms step_avg:61.29ms
step:1708/2330 train_time:104688ms step_avg:61.29ms
step:1709/2330 train_time:104748ms step_avg:61.29ms
step:1710/2330 train_time:104812ms step_avg:61.29ms
step:1711/2330 train_time:104873ms step_avg:61.29ms
step:1712/2330 train_time:104937ms step_avg:61.30ms
step:1713/2330 train_time:104998ms step_avg:61.29ms
step:1714/2330 train_time:105062ms step_avg:61.30ms
step:1715/2330 train_time:105121ms step_avg:61.30ms
step:1716/2330 train_time:105185ms step_avg:61.30ms
step:1717/2330 train_time:105245ms step_avg:61.30ms
step:1718/2330 train_time:105308ms step_avg:61.30ms
step:1719/2330 train_time:105369ms step_avg:61.30ms
step:1720/2330 train_time:105432ms step_avg:61.30ms
step:1721/2330 train_time:105494ms step_avg:61.30ms
step:1722/2330 train_time:105558ms step_avg:61.30ms
step:1723/2330 train_time:105618ms step_avg:61.30ms
step:1724/2330 train_time:105681ms step_avg:61.30ms
step:1725/2330 train_time:105741ms step_avg:61.30ms
step:1726/2330 train_time:105805ms step_avg:61.30ms
step:1727/2330 train_time:105865ms step_avg:61.30ms
step:1728/2330 train_time:105928ms step_avg:61.30ms
step:1729/2330 train_time:105989ms step_avg:61.30ms
step:1730/2330 train_time:106053ms step_avg:61.30ms
step:1731/2330 train_time:106113ms step_avg:61.30ms
step:1732/2330 train_time:106177ms step_avg:61.30ms
step:1733/2330 train_time:106238ms step_avg:61.30ms
step:1734/2330 train_time:106301ms step_avg:61.30ms
step:1735/2330 train_time:106362ms step_avg:61.30ms
step:1736/2330 train_time:106425ms step_avg:61.30ms
step:1737/2330 train_time:106486ms step_avg:61.30ms
step:1738/2330 train_time:106549ms step_avg:61.31ms
step:1739/2330 train_time:106610ms step_avg:61.31ms
step:1740/2330 train_time:106673ms step_avg:61.31ms
step:1741/2330 train_time:106734ms step_avg:61.31ms
step:1742/2330 train_time:106799ms step_avg:61.31ms
step:1743/2330 train_time:106860ms step_avg:61.31ms
step:1744/2330 train_time:106923ms step_avg:61.31ms
step:1745/2330 train_time:106983ms step_avg:61.31ms
step:1746/2330 train_time:107047ms step_avg:61.31ms
step:1747/2330 train_time:107107ms step_avg:61.31ms
step:1748/2330 train_time:107170ms step_avg:61.31ms
step:1749/2330 train_time:107231ms step_avg:61.31ms
step:1750/2330 train_time:107294ms step_avg:61.31ms
step:1750/2330 val_loss:3.4338 train_time:107360ms step_avg:61.35ms
step:1751/2330 train_time:107384ms step_avg:61.33ms
step:1752/2330 train_time:107423ms step_avg:61.31ms
step:1753/2330 train_time:107490ms step_avg:61.32ms
step:1754/2330 train_time:107554ms step_avg:61.32ms
step:1755/2330 train_time:107614ms step_avg:61.32ms
step:1756/2330 train_time:107677ms step_avg:61.32ms
step:1757/2330 train_time:107738ms step_avg:61.32ms
step:1758/2330 train_time:107800ms step_avg:61.32ms
step:1759/2330 train_time:107859ms step_avg:61.32ms
step:1760/2330 train_time:107922ms step_avg:61.32ms
step:1761/2330 train_time:107981ms step_avg:61.32ms
step:1762/2330 train_time:108044ms step_avg:61.32ms
step:1763/2330 train_time:108103ms step_avg:61.32ms
step:1764/2330 train_time:108165ms step_avg:61.32ms
step:1765/2330 train_time:108225ms step_avg:61.32ms
step:1766/2330 train_time:108290ms step_avg:61.32ms
step:1767/2330 train_time:108352ms step_avg:61.32ms
step:1768/2330 train_time:108416ms step_avg:61.32ms
step:1769/2330 train_time:108478ms step_avg:61.32ms
step:1770/2330 train_time:108542ms step_avg:61.32ms
step:1771/2330 train_time:108603ms step_avg:61.32ms
step:1772/2330 train_time:108667ms step_avg:61.32ms
step:1773/2330 train_time:108728ms step_avg:61.32ms
step:1774/2330 train_time:108792ms step_avg:61.33ms
step:1775/2330 train_time:108852ms step_avg:61.32ms
step:1776/2330 train_time:108915ms step_avg:61.33ms
step:1777/2330 train_time:108975ms step_avg:61.33ms
step:1778/2330 train_time:109038ms step_avg:61.33ms
step:1779/2330 train_time:109098ms step_avg:61.33ms
step:1780/2330 train_time:109161ms step_avg:61.33ms
step:1781/2330 train_time:109220ms step_avg:61.33ms
step:1782/2330 train_time:109284ms step_avg:61.33ms
step:1783/2330 train_time:109346ms step_avg:61.33ms
step:1784/2330 train_time:109409ms step_avg:61.33ms
step:1785/2330 train_time:109470ms step_avg:61.33ms
step:1786/2330 train_time:109534ms step_avg:61.33ms
step:1787/2330 train_time:109595ms step_avg:61.33ms
step:1788/2330 train_time:109658ms step_avg:61.33ms
step:1789/2330 train_time:109719ms step_avg:61.33ms
step:1790/2330 train_time:109782ms step_avg:61.33ms
step:1791/2330 train_time:109842ms step_avg:61.33ms
step:1792/2330 train_time:109905ms step_avg:61.33ms
step:1793/2330 train_time:109965ms step_avg:61.33ms
step:1794/2330 train_time:110028ms step_avg:61.33ms
step:1795/2330 train_time:110088ms step_avg:61.33ms
step:1796/2330 train_time:110152ms step_avg:61.33ms
step:1797/2330 train_time:110212ms step_avg:61.33ms
step:1798/2330 train_time:110276ms step_avg:61.33ms
step:1799/2330 train_time:110336ms step_avg:61.33ms
step:1800/2330 train_time:110400ms step_avg:61.33ms
step:1801/2330 train_time:110461ms step_avg:61.33ms
step:1802/2330 train_time:110524ms step_avg:61.33ms
step:1803/2330 train_time:110584ms step_avg:61.33ms
step:1804/2330 train_time:110647ms step_avg:61.33ms
step:1805/2330 train_time:110707ms step_avg:61.33ms
step:1806/2330 train_time:110771ms step_avg:61.33ms
step:1807/2330 train_time:110832ms step_avg:61.33ms
step:1808/2330 train_time:110896ms step_avg:61.34ms
step:1809/2330 train_time:110956ms step_avg:61.34ms
step:1810/2330 train_time:111019ms step_avg:61.34ms
step:1811/2330 train_time:111079ms step_avg:61.34ms
step:1812/2330 train_time:111142ms step_avg:61.34ms
step:1813/2330 train_time:111202ms step_avg:61.34ms
step:1814/2330 train_time:111265ms step_avg:61.34ms
step:1815/2330 train_time:111325ms step_avg:61.34ms
step:1816/2330 train_time:111388ms step_avg:61.34ms
step:1817/2330 train_time:111448ms step_avg:61.34ms
step:1818/2330 train_time:111512ms step_avg:61.34ms
step:1819/2330 train_time:111572ms step_avg:61.34ms
step:1820/2330 train_time:111636ms step_avg:61.34ms
step:1821/2330 train_time:111697ms step_avg:61.34ms
step:1822/2330 train_time:111760ms step_avg:61.34ms
step:1823/2330 train_time:111820ms step_avg:61.34ms
step:1824/2330 train_time:111883ms step_avg:61.34ms
step:1825/2330 train_time:111943ms step_avg:61.34ms
step:1826/2330 train_time:112006ms step_avg:61.34ms
step:1827/2330 train_time:112066ms step_avg:61.34ms
step:1828/2330 train_time:112129ms step_avg:61.34ms
step:1829/2330 train_time:112190ms step_avg:61.34ms
step:1830/2330 train_time:112253ms step_avg:61.34ms
step:1831/2330 train_time:112313ms step_avg:61.34ms
step:1832/2330 train_time:112376ms step_avg:61.34ms
step:1833/2330 train_time:112436ms step_avg:61.34ms
step:1834/2330 train_time:112499ms step_avg:61.34ms
step:1835/2330 train_time:112560ms step_avg:61.34ms
step:1836/2330 train_time:112623ms step_avg:61.34ms
step:1837/2330 train_time:112683ms step_avg:61.34ms
step:1838/2330 train_time:112746ms step_avg:61.34ms
step:1839/2330 train_time:112807ms step_avg:61.34ms
step:1840/2330 train_time:112870ms step_avg:61.34ms
step:1841/2330 train_time:112930ms step_avg:61.34ms
step:1842/2330 train_time:112994ms step_avg:61.34ms
step:1843/2330 train_time:113055ms step_avg:61.34ms
step:1844/2330 train_time:113118ms step_avg:61.34ms
step:1845/2330 train_time:113178ms step_avg:61.34ms
step:1846/2330 train_time:113241ms step_avg:61.34ms
step:1847/2330 train_time:113301ms step_avg:61.34ms
step:1848/2330 train_time:113364ms step_avg:61.34ms
step:1849/2330 train_time:113425ms step_avg:61.34ms
step:1850/2330 train_time:113488ms step_avg:61.35ms
step:1851/2330 train_time:113548ms step_avg:61.34ms
step:1852/2330 train_time:113612ms step_avg:61.35ms
step:1853/2330 train_time:113672ms step_avg:61.34ms
step:1854/2330 train_time:113735ms step_avg:61.35ms
step:1855/2330 train_time:113796ms step_avg:61.35ms
step:1856/2330 train_time:113859ms step_avg:61.35ms
step:1857/2330 train_time:113919ms step_avg:61.35ms
step:1858/2330 train_time:113983ms step_avg:61.35ms
step:1859/2330 train_time:114044ms step_avg:61.35ms
step:1860/2330 train_time:114107ms step_avg:61.35ms
step:1861/2330 train_time:114167ms step_avg:61.35ms
step:1862/2330 train_time:114231ms step_avg:61.35ms
step:1863/2330 train_time:114291ms step_avg:61.35ms
step:1864/2330 train_time:114354ms step_avg:61.35ms
step:1865/2330 train_time:114415ms step_avg:61.35ms
step:1866/2330 train_time:114478ms step_avg:61.35ms
step:1867/2330 train_time:114538ms step_avg:61.35ms
step:1868/2330 train_time:114602ms step_avg:61.35ms
step:1869/2330 train_time:114662ms step_avg:61.35ms
step:1870/2330 train_time:114725ms step_avg:61.35ms
step:1871/2330 train_time:114785ms step_avg:61.35ms
step:1872/2330 train_time:114849ms step_avg:61.35ms
step:1873/2330 train_time:114909ms step_avg:61.35ms
step:1874/2330 train_time:114973ms step_avg:61.35ms
step:1875/2330 train_time:115034ms step_avg:61.35ms
step:1876/2330 train_time:115098ms step_avg:61.35ms
step:1877/2330 train_time:115158ms step_avg:61.35ms
step:1878/2330 train_time:115221ms step_avg:61.35ms
step:1879/2330 train_time:115281ms step_avg:61.35ms
step:1880/2330 train_time:115345ms step_avg:61.35ms
step:1881/2330 train_time:115404ms step_avg:61.35ms
step:1882/2330 train_time:115468ms step_avg:61.35ms
step:1883/2330 train_time:115528ms step_avg:61.35ms
step:1884/2330 train_time:115592ms step_avg:61.35ms
step:1885/2330 train_time:115652ms step_avg:61.35ms
step:1886/2330 train_time:115715ms step_avg:61.35ms
step:1887/2330 train_time:115776ms step_avg:61.35ms
step:1888/2330 train_time:115839ms step_avg:61.36ms
step:1889/2330 train_time:115899ms step_avg:61.35ms
step:1890/2330 train_time:115963ms step_avg:61.36ms
step:1891/2330 train_time:116024ms step_avg:61.36ms
step:1892/2330 train_time:116087ms step_avg:61.36ms
step:1893/2330 train_time:116147ms step_avg:61.36ms
step:1894/2330 train_time:116210ms step_avg:61.36ms
step:1895/2330 train_time:116271ms step_avg:61.36ms
step:1896/2330 train_time:116335ms step_avg:61.36ms
step:1897/2330 train_time:116396ms step_avg:61.36ms
step:1898/2330 train_time:116458ms step_avg:61.36ms
step:1899/2330 train_time:116518ms step_avg:61.36ms
step:1900/2330 train_time:116582ms step_avg:61.36ms
step:1901/2330 train_time:116642ms step_avg:61.36ms
step:1902/2330 train_time:116706ms step_avg:61.36ms
step:1903/2330 train_time:116766ms step_avg:61.36ms
step:1904/2330 train_time:116829ms step_avg:61.36ms
step:1905/2330 train_time:116890ms step_avg:61.36ms
step:1906/2330 train_time:116953ms step_avg:61.36ms
step:1907/2330 train_time:117014ms step_avg:61.36ms
step:1908/2330 train_time:117077ms step_avg:61.36ms
step:1909/2330 train_time:117137ms step_avg:61.36ms
step:1910/2330 train_time:117201ms step_avg:61.36ms
step:1911/2330 train_time:117261ms step_avg:61.36ms
step:1912/2330 train_time:117325ms step_avg:61.36ms
step:1913/2330 train_time:117385ms step_avg:61.36ms
step:1914/2330 train_time:117448ms step_avg:61.36ms
step:1915/2330 train_time:117508ms step_avg:61.36ms
step:1916/2330 train_time:117572ms step_avg:61.36ms
step:1917/2330 train_time:117632ms step_avg:61.36ms
step:1918/2330 train_time:117696ms step_avg:61.36ms
step:1919/2330 train_time:117756ms step_avg:61.36ms
step:1920/2330 train_time:117819ms step_avg:61.36ms
step:1921/2330 train_time:117880ms step_avg:61.36ms
step:1922/2330 train_time:117943ms step_avg:61.36ms
step:1923/2330 train_time:118003ms step_avg:61.36ms
step:1924/2330 train_time:118066ms step_avg:61.36ms
step:1925/2330 train_time:118126ms step_avg:61.36ms
step:1926/2330 train_time:118191ms step_avg:61.37ms
step:1927/2330 train_time:118252ms step_avg:61.37ms
step:1928/2330 train_time:118316ms step_avg:61.37ms
step:1929/2330 train_time:118376ms step_avg:61.37ms
step:1930/2330 train_time:118439ms step_avg:61.37ms
step:1931/2330 train_time:118500ms step_avg:61.37ms
step:1932/2330 train_time:118564ms step_avg:61.37ms
step:1933/2330 train_time:118624ms step_avg:61.37ms
step:1934/2330 train_time:118687ms step_avg:61.37ms
step:1935/2330 train_time:118747ms step_avg:61.37ms
step:1936/2330 train_time:118810ms step_avg:61.37ms
step:1937/2330 train_time:118870ms step_avg:61.37ms
step:1938/2330 train_time:118934ms step_avg:61.37ms
step:1939/2330 train_time:118995ms step_avg:61.37ms
step:1940/2330 train_time:119057ms step_avg:61.37ms
step:1941/2330 train_time:119118ms step_avg:61.37ms
step:1942/2330 train_time:119181ms step_avg:61.37ms
step:1943/2330 train_time:119241ms step_avg:61.37ms
step:1944/2330 train_time:119304ms step_avg:61.37ms
step:1945/2330 train_time:119364ms step_avg:61.37ms
step:1946/2330 train_time:119427ms step_avg:61.37ms
step:1947/2330 train_time:119487ms step_avg:61.37ms
step:1948/2330 train_time:119550ms step_avg:61.37ms
step:1949/2330 train_time:119611ms step_avg:61.37ms
step:1950/2330 train_time:119675ms step_avg:61.37ms
step:1951/2330 train_time:119735ms step_avg:61.37ms
step:1952/2330 train_time:119798ms step_avg:61.37ms
step:1953/2330 train_time:119858ms step_avg:61.37ms
step:1954/2330 train_time:119921ms step_avg:61.37ms
step:1955/2330 train_time:119981ms step_avg:61.37ms
step:1956/2330 train_time:120045ms step_avg:61.37ms
step:1957/2330 train_time:120105ms step_avg:61.37ms
step:1958/2330 train_time:120169ms step_avg:61.37ms
step:1959/2330 train_time:120229ms step_avg:61.37ms
step:1960/2330 train_time:120292ms step_avg:61.37ms
step:1961/2330 train_time:120352ms step_avg:61.37ms
step:1962/2330 train_time:120415ms step_avg:61.37ms
step:1963/2330 train_time:120477ms step_avg:61.37ms
step:1964/2330 train_time:120540ms step_avg:61.37ms
step:1965/2330 train_time:120600ms step_avg:61.37ms
step:1966/2330 train_time:120664ms step_avg:61.38ms
step:1967/2330 train_time:120724ms step_avg:61.37ms
step:1968/2330 train_time:120787ms step_avg:61.38ms
step:1969/2330 train_time:120848ms step_avg:61.38ms
step:1970/2330 train_time:120911ms step_avg:61.38ms
step:1971/2330 train_time:120972ms step_avg:61.38ms
step:1972/2330 train_time:121036ms step_avg:61.38ms
step:1973/2330 train_time:121097ms step_avg:61.38ms
step:1974/2330 train_time:121159ms step_avg:61.38ms
step:1975/2330 train_time:121220ms step_avg:61.38ms
step:1976/2330 train_time:121284ms step_avg:61.38ms
step:1977/2330 train_time:121344ms step_avg:61.38ms
step:1978/2330 train_time:121407ms step_avg:61.38ms
step:1979/2330 train_time:121468ms step_avg:61.38ms
step:1980/2330 train_time:121531ms step_avg:61.38ms
step:1981/2330 train_time:121592ms step_avg:61.38ms
step:1982/2330 train_time:121656ms step_avg:61.38ms
step:1983/2330 train_time:121715ms step_avg:61.38ms
step:1984/2330 train_time:121779ms step_avg:61.38ms
step:1985/2330 train_time:121840ms step_avg:61.38ms
step:1986/2330 train_time:121903ms step_avg:61.38ms
step:1987/2330 train_time:121963ms step_avg:61.38ms
step:1988/2330 train_time:122026ms step_avg:61.38ms
step:1989/2330 train_time:122086ms step_avg:61.38ms
step:1990/2330 train_time:122150ms step_avg:61.38ms
step:1991/2330 train_time:122210ms step_avg:61.38ms
step:1992/2330 train_time:122274ms step_avg:61.38ms
step:1993/2330 train_time:122334ms step_avg:61.38ms
step:1994/2330 train_time:122398ms step_avg:61.38ms
step:1995/2330 train_time:122458ms step_avg:61.38ms
step:1996/2330 train_time:122522ms step_avg:61.38ms
step:1997/2330 train_time:122582ms step_avg:61.38ms
step:1998/2330 train_time:122645ms step_avg:61.38ms
step:1999/2330 train_time:122705ms step_avg:61.38ms
step:2000/2330 train_time:122769ms step_avg:61.38ms
step:2000/2330 val_loss:3.4064 train_time:122833ms step_avg:61.42ms
step:2001/2330 train_time:122856ms step_avg:61.40ms
step:2002/2330 train_time:122896ms step_avg:61.39ms
step:2003/2330 train_time:122961ms step_avg:61.39ms
step:2004/2330 train_time:123026ms step_avg:61.39ms
step:2005/2330 train_time:123086ms step_avg:61.39ms
step:2006/2330 train_time:123149ms step_avg:61.39ms
step:2007/2330 train_time:123209ms step_avg:61.39ms
step:2008/2330 train_time:123272ms step_avg:61.39ms
step:2009/2330 train_time:123332ms step_avg:61.39ms
step:2010/2330 train_time:123394ms step_avg:61.39ms
step:2011/2330 train_time:123454ms step_avg:61.39ms
step:2012/2330 train_time:123516ms step_avg:61.39ms
step:2013/2330 train_time:123576ms step_avg:61.39ms
step:2014/2330 train_time:123639ms step_avg:61.39ms
step:2015/2330 train_time:123698ms step_avg:61.39ms
step:2016/2330 train_time:123762ms step_avg:61.39ms
step:2017/2330 train_time:123823ms step_avg:61.39ms
step:2018/2330 train_time:123888ms step_avg:61.39ms
step:2019/2330 train_time:123950ms step_avg:61.39ms
step:2020/2330 train_time:124015ms step_avg:61.39ms
step:2021/2330 train_time:124076ms step_avg:61.39ms
step:2022/2330 train_time:124140ms step_avg:61.39ms
step:2023/2330 train_time:124200ms step_avg:61.39ms
step:2024/2330 train_time:124263ms step_avg:61.39ms
step:2025/2330 train_time:124323ms step_avg:61.39ms
step:2026/2330 train_time:124386ms step_avg:61.39ms
step:2027/2330 train_time:124445ms step_avg:61.39ms
step:2028/2330 train_time:124508ms step_avg:61.39ms
step:2029/2330 train_time:124568ms step_avg:61.39ms
step:2030/2330 train_time:124631ms step_avg:61.39ms
step:2031/2330 train_time:124691ms step_avg:61.39ms
step:2032/2330 train_time:124754ms step_avg:61.39ms
step:2033/2330 train_time:124815ms step_avg:61.39ms
step:2034/2330 train_time:124879ms step_avg:61.40ms
step:2035/2330 train_time:124939ms step_avg:61.40ms
step:2036/2330 train_time:125003ms step_avg:61.40ms
step:2037/2330 train_time:125064ms step_avg:61.40ms
step:2038/2330 train_time:125127ms step_avg:61.40ms
step:2039/2330 train_time:125187ms step_avg:61.40ms
step:2040/2330 train_time:125250ms step_avg:61.40ms
step:2041/2330 train_time:125310ms step_avg:61.40ms
step:2042/2330 train_time:125373ms step_avg:61.40ms
step:2043/2330 train_time:125433ms step_avg:61.40ms
step:2044/2330 train_time:125496ms step_avg:61.40ms
step:2045/2330 train_time:125557ms step_avg:61.40ms
step:2046/2330 train_time:125620ms step_avg:61.40ms
step:2047/2330 train_time:125680ms step_avg:61.40ms
step:2048/2330 train_time:125743ms step_avg:61.40ms
step:2049/2330 train_time:125803ms step_avg:61.40ms
step:2050/2330 train_time:125867ms step_avg:61.40ms
step:2051/2330 train_time:125928ms step_avg:61.40ms
step:2052/2330 train_time:125991ms step_avg:61.40ms
step:2053/2330 train_time:126052ms step_avg:61.40ms
step:2054/2330 train_time:126116ms step_avg:61.40ms
step:2055/2330 train_time:126178ms step_avg:61.40ms
step:2056/2330 train_time:126241ms step_avg:61.40ms
step:2057/2330 train_time:126301ms step_avg:61.40ms
step:2058/2330 train_time:126364ms step_avg:61.40ms
step:2059/2330 train_time:126424ms step_avg:61.40ms
step:2060/2330 train_time:126487ms step_avg:61.40ms
step:2061/2330 train_time:126547ms step_avg:61.40ms
step:2062/2330 train_time:126611ms step_avg:61.40ms
step:2063/2330 train_time:126671ms step_avg:61.40ms
step:2064/2330 train_time:126734ms step_avg:61.40ms
step:2065/2330 train_time:126794ms step_avg:61.40ms
step:2066/2330 train_time:126858ms step_avg:61.40ms
step:2067/2330 train_time:126919ms step_avg:61.40ms
step:2068/2330 train_time:126982ms step_avg:61.40ms
step:2069/2330 train_time:127042ms step_avg:61.40ms
step:2070/2330 train_time:127105ms step_avg:61.40ms
step:2071/2330 train_time:127166ms step_avg:61.40ms
step:2072/2330 train_time:127229ms step_avg:61.40ms
step:2073/2330 train_time:127288ms step_avg:61.40ms
step:2074/2330 train_time:127351ms step_avg:61.40ms
step:2075/2330 train_time:127412ms step_avg:61.40ms
step:2076/2330 train_time:127475ms step_avg:61.40ms
step:2077/2330 train_time:127536ms step_avg:61.40ms
step:2078/2330 train_time:127599ms step_avg:61.40ms
step:2079/2330 train_time:127659ms step_avg:61.40ms
step:2080/2330 train_time:127722ms step_avg:61.40ms
step:2081/2330 train_time:127782ms step_avg:61.40ms
step:2082/2330 train_time:127845ms step_avg:61.40ms
step:2083/2330 train_time:127905ms step_avg:61.40ms
step:2084/2330 train_time:127969ms step_avg:61.41ms
step:2085/2330 train_time:128030ms step_avg:61.41ms
step:2086/2330 train_time:128093ms step_avg:61.41ms
step:2087/2330 train_time:128154ms step_avg:61.41ms
step:2088/2330 train_time:128218ms step_avg:61.41ms
step:2089/2330 train_time:128278ms step_avg:61.41ms
step:2090/2330 train_time:128341ms step_avg:61.41ms
step:2091/2330 train_time:128401ms step_avg:61.41ms
step:2092/2330 train_time:128464ms step_avg:61.41ms
step:2093/2330 train_time:128524ms step_avg:61.41ms
step:2094/2330 train_time:128587ms step_avg:61.41ms
step:2095/2330 train_time:128647ms step_avg:61.41ms
step:2096/2330 train_time:128710ms step_avg:61.41ms
step:2097/2330 train_time:128771ms step_avg:61.41ms
step:2098/2330 train_time:128835ms step_avg:61.41ms
step:2099/2330 train_time:128895ms step_avg:61.41ms
step:2100/2330 train_time:128960ms step_avg:61.41ms
step:2101/2330 train_time:129020ms step_avg:61.41ms
step:2102/2330 train_time:129083ms step_avg:61.41ms
step:2103/2330 train_time:129144ms step_avg:61.41ms
step:2104/2330 train_time:129207ms step_avg:61.41ms
step:2105/2330 train_time:129267ms step_avg:61.41ms
step:2106/2330 train_time:129330ms step_avg:61.41ms
step:2107/2330 train_time:129390ms step_avg:61.41ms
step:2108/2330 train_time:129454ms step_avg:61.41ms
step:2109/2330 train_time:129514ms step_avg:61.41ms
step:2110/2330 train_time:129578ms step_avg:61.41ms
step:2111/2330 train_time:129639ms step_avg:61.41ms
step:2112/2330 train_time:129702ms step_avg:61.41ms
step:2113/2330 train_time:129763ms step_avg:61.41ms
step:2114/2330 train_time:129826ms step_avg:61.41ms
step:2115/2330 train_time:129886ms step_avg:61.41ms
step:2116/2330 train_time:129949ms step_avg:61.41ms
step:2117/2330 train_time:130009ms step_avg:61.41ms
step:2118/2330 train_time:130073ms step_avg:61.41ms
step:2119/2330 train_time:130134ms step_avg:61.41ms
step:2120/2330 train_time:130198ms step_avg:61.41ms
step:2121/2330 train_time:130258ms step_avg:61.41ms
step:2122/2330 train_time:130322ms step_avg:61.41ms
step:2123/2330 train_time:130382ms step_avg:61.41ms
step:2124/2330 train_time:130444ms step_avg:61.41ms
step:2125/2330 train_time:130505ms step_avg:61.41ms
step:2126/2330 train_time:130568ms step_avg:61.41ms
step:2127/2330 train_time:130628ms step_avg:61.41ms
step:2128/2330 train_time:130691ms step_avg:61.42ms
step:2129/2330 train_time:130752ms step_avg:61.41ms
step:2130/2330 train_time:130816ms step_avg:61.42ms
step:2131/2330 train_time:130876ms step_avg:61.42ms
step:2132/2330 train_time:130940ms step_avg:61.42ms
step:2133/2330 train_time:131000ms step_avg:61.42ms
step:2134/2330 train_time:131064ms step_avg:61.42ms
step:2135/2330 train_time:131124ms step_avg:61.42ms
step:2136/2330 train_time:131186ms step_avg:61.42ms
step:2137/2330 train_time:131246ms step_avg:61.42ms
step:2138/2330 train_time:131309ms step_avg:61.42ms
step:2139/2330 train_time:131370ms step_avg:61.42ms
step:2140/2330 train_time:131433ms step_avg:61.42ms
step:2141/2330 train_time:131493ms step_avg:61.42ms
step:2142/2330 train_time:131557ms step_avg:61.42ms
step:2143/2330 train_time:131617ms step_avg:61.42ms
step:2144/2330 train_time:131681ms step_avg:61.42ms
step:2145/2330 train_time:131741ms step_avg:61.42ms
step:2146/2330 train_time:131804ms step_avg:61.42ms
step:2147/2330 train_time:131864ms step_avg:61.42ms
step:2148/2330 train_time:131927ms step_avg:61.42ms
step:2149/2330 train_time:131987ms step_avg:61.42ms
step:2150/2330 train_time:132051ms step_avg:61.42ms
step:2151/2330 train_time:132111ms step_avg:61.42ms
step:2152/2330 train_time:132174ms step_avg:61.42ms
step:2153/2330 train_time:132234ms step_avg:61.42ms
step:2154/2330 train_time:132298ms step_avg:61.42ms
step:2155/2330 train_time:132358ms step_avg:61.42ms
step:2156/2330 train_time:132421ms step_avg:61.42ms
step:2157/2330 train_time:132481ms step_avg:61.42ms
step:2158/2330 train_time:132545ms step_avg:61.42ms
step:2159/2330 train_time:132605ms step_avg:61.42ms
step:2160/2330 train_time:132668ms step_avg:61.42ms
step:2161/2330 train_time:132728ms step_avg:61.42ms
step:2162/2330 train_time:132791ms step_avg:61.42ms
step:2163/2330 train_time:132852ms step_avg:61.42ms
step:2164/2330 train_time:132916ms step_avg:61.42ms
step:2165/2330 train_time:132976ms step_avg:61.42ms
step:2166/2330 train_time:133040ms step_avg:61.42ms
step:2167/2330 train_time:133100ms step_avg:61.42ms
step:2168/2330 train_time:133163ms step_avg:61.42ms
step:2169/2330 train_time:133223ms step_avg:61.42ms
step:2170/2330 train_time:133286ms step_avg:61.42ms
step:2171/2330 train_time:133346ms step_avg:61.42ms
step:2172/2330 train_time:133409ms step_avg:61.42ms
step:2173/2330 train_time:133470ms step_avg:61.42ms
step:2174/2330 train_time:133534ms step_avg:61.42ms
step:2175/2330 train_time:133594ms step_avg:61.42ms
step:2176/2330 train_time:133658ms step_avg:61.42ms
step:2177/2330 train_time:133719ms step_avg:61.42ms
step:2178/2330 train_time:133782ms step_avg:61.42ms
step:2179/2330 train_time:133843ms step_avg:61.42ms
step:2180/2330 train_time:133906ms step_avg:61.42ms
step:2181/2330 train_time:133967ms step_avg:61.42ms
step:2182/2330 train_time:134030ms step_avg:61.43ms
step:2183/2330 train_time:134090ms step_avg:61.42ms
step:2184/2330 train_time:134154ms step_avg:61.43ms
step:2185/2330 train_time:134214ms step_avg:61.43ms
step:2186/2330 train_time:134278ms step_avg:61.43ms
step:2187/2330 train_time:134338ms step_avg:61.43ms
step:2188/2330 train_time:134401ms step_avg:61.43ms
step:2189/2330 train_time:134462ms step_avg:61.43ms
step:2190/2330 train_time:134525ms step_avg:61.43ms
step:2191/2330 train_time:134585ms step_avg:61.43ms
step:2192/2330 train_time:134648ms step_avg:61.43ms
step:2193/2330 train_time:134708ms step_avg:61.43ms
step:2194/2330 train_time:134771ms step_avg:61.43ms
step:2195/2330 train_time:134832ms step_avg:61.43ms
step:2196/2330 train_time:134895ms step_avg:61.43ms
step:2197/2330 train_time:134956ms step_avg:61.43ms
step:2198/2330 train_time:135020ms step_avg:61.43ms
step:2199/2330 train_time:135080ms step_avg:61.43ms
step:2200/2330 train_time:135143ms step_avg:61.43ms
step:2201/2330 train_time:135203ms step_avg:61.43ms
step:2202/2330 train_time:135266ms step_avg:61.43ms
step:2203/2330 train_time:135327ms step_avg:61.43ms
step:2204/2330 train_time:135389ms step_avg:61.43ms
step:2205/2330 train_time:135449ms step_avg:61.43ms
step:2206/2330 train_time:135513ms step_avg:61.43ms
step:2207/2330 train_time:135573ms step_avg:61.43ms
step:2208/2330 train_time:135637ms step_avg:61.43ms
step:2209/2330 train_time:135697ms step_avg:61.43ms
step:2210/2330 train_time:135761ms step_avg:61.43ms
step:2211/2330 train_time:135821ms step_avg:61.43ms
step:2212/2330 train_time:135884ms step_avg:61.43ms
step:2213/2330 train_time:135944ms step_avg:61.43ms
step:2214/2330 train_time:136007ms step_avg:61.43ms
step:2215/2330 train_time:136068ms step_avg:61.43ms
step:2216/2330 train_time:136132ms step_avg:61.43ms
step:2217/2330 train_time:136192ms step_avg:61.43ms
step:2218/2330 train_time:136255ms step_avg:61.43ms
step:2219/2330 train_time:136315ms step_avg:61.43ms
step:2220/2330 train_time:136379ms step_avg:61.43ms
step:2221/2330 train_time:136439ms step_avg:61.43ms
step:2222/2330 train_time:136503ms step_avg:61.43ms
step:2223/2330 train_time:136563ms step_avg:61.43ms
step:2224/2330 train_time:136626ms step_avg:61.43ms
step:2225/2330 train_time:136686ms step_avg:61.43ms
step:2226/2330 train_time:136750ms step_avg:61.43ms
step:2227/2330 train_time:136810ms step_avg:61.43ms
step:2228/2330 train_time:136874ms step_avg:61.43ms
step:2229/2330 train_time:136934ms step_avg:61.43ms
step:2230/2330 train_time:136998ms step_avg:61.43ms
step:2231/2330 train_time:137059ms step_avg:61.43ms
step:2232/2330 train_time:137122ms step_avg:61.43ms
step:2233/2330 train_time:137183ms step_avg:61.43ms
step:2234/2330 train_time:137247ms step_avg:61.44ms
step:2235/2330 train_time:137307ms step_avg:61.43ms
step:2236/2330 train_time:137370ms step_avg:61.44ms
step:2237/2330 train_time:137431ms step_avg:61.44ms
step:2238/2330 train_time:137494ms step_avg:61.44ms
step:2239/2330 train_time:137555ms step_avg:61.44ms
step:2240/2330 train_time:137620ms step_avg:61.44ms
step:2241/2330 train_time:137680ms step_avg:61.44ms
step:2242/2330 train_time:137744ms step_avg:61.44ms
step:2243/2330 train_time:137803ms step_avg:61.44ms
step:2244/2330 train_time:137866ms step_avg:61.44ms
step:2245/2330 train_time:137926ms step_avg:61.44ms
step:2246/2330 train_time:137990ms step_avg:61.44ms
step:2247/2330 train_time:138050ms step_avg:61.44ms
step:2248/2330 train_time:138113ms step_avg:61.44ms
step:2249/2330 train_time:138174ms step_avg:61.44ms
step:2250/2330 train_time:138238ms step_avg:61.44ms
step:2250/2330 val_loss:3.3846 train_time:138303ms step_avg:61.47ms
step:2251/2330 train_time:138325ms step_avg:61.45ms
step:2252/2330 train_time:138366ms step_avg:61.44ms
step:2253/2330 train_time:138434ms step_avg:61.44ms
step:2254/2330 train_time:138500ms step_avg:61.45ms
step:2255/2330 train_time:138560ms step_avg:61.45ms
step:2256/2330 train_time:138623ms step_avg:61.45ms
step:2257/2330 train_time:138683ms step_avg:61.45ms
step:2258/2330 train_time:138745ms step_avg:61.45ms
step:2259/2330 train_time:138805ms step_avg:61.45ms
step:2260/2330 train_time:138867ms step_avg:61.45ms
step:2261/2330 train_time:138927ms step_avg:61.44ms
step:2262/2330 train_time:138989ms step_avg:61.45ms
step:2263/2330 train_time:139049ms step_avg:61.44ms
step:2264/2330 train_time:139111ms step_avg:61.44ms
step:2265/2330 train_time:139171ms step_avg:61.44ms
step:2266/2330 train_time:139234ms step_avg:61.44ms
step:2267/2330 train_time:139296ms step_avg:61.44ms
step:2268/2330 train_time:139361ms step_avg:61.45ms
step:2269/2330 train_time:139423ms step_avg:61.45ms
step:2270/2330 train_time:139489ms step_avg:61.45ms
step:2271/2330 train_time:139550ms step_avg:61.45ms
step:2272/2330 train_time:139614ms step_avg:61.45ms
step:2273/2330 train_time:139675ms step_avg:61.45ms
step:2274/2330 train_time:139738ms step_avg:61.45ms
step:2275/2330 train_time:139798ms step_avg:61.45ms
step:2276/2330 train_time:139862ms step_avg:61.45ms
step:2277/2330 train_time:139921ms step_avg:61.45ms
step:2278/2330 train_time:139984ms step_avg:61.45ms
step:2279/2330 train_time:140044ms step_avg:61.45ms
step:2280/2330 train_time:140107ms step_avg:61.45ms
step:2281/2330 train_time:140166ms step_avg:61.45ms
step:2282/2330 train_time:140229ms step_avg:61.45ms
step:2283/2330 train_time:140289ms step_avg:61.45ms
step:2284/2330 train_time:140354ms step_avg:61.45ms
step:2285/2330 train_time:140414ms step_avg:61.45ms
step:2286/2330 train_time:140478ms step_avg:61.45ms
step:2287/2330 train_time:140539ms step_avg:61.45ms
step:2288/2330 train_time:140602ms step_avg:61.45ms
step:2289/2330 train_time:140663ms step_avg:61.45ms
step:2290/2330 train_time:140726ms step_avg:61.45ms
step:2291/2330 train_time:140786ms step_avg:61.45ms
step:2292/2330 train_time:140849ms step_avg:61.45ms
step:2293/2330 train_time:140909ms step_avg:61.45ms
step:2294/2330 train_time:140972ms step_avg:61.45ms
step:2295/2330 train_time:141032ms step_avg:61.45ms
step:2296/2330 train_time:141095ms step_avg:61.45ms
step:2297/2330 train_time:141156ms step_avg:61.45ms
step:2298/2330 train_time:141218ms step_avg:61.45ms
step:2299/2330 train_time:141279ms step_avg:61.45ms
step:2300/2330 train_time:141342ms step_avg:61.45ms
step:2301/2330 train_time:141403ms step_avg:61.45ms
step:2302/2330 train_time:141466ms step_avg:61.45ms
step:2303/2330 train_time:141526ms step_avg:61.45ms
step:2304/2330 train_time:141590ms step_avg:61.45ms
step:2305/2330 train_time:141651ms step_avg:61.45ms
step:2306/2330 train_time:141715ms step_avg:61.45ms
step:2307/2330 train_time:141776ms step_avg:61.45ms
step:2308/2330 train_time:141840ms step_avg:61.46ms
step:2309/2330 train_time:141900ms step_avg:61.46ms
step:2310/2330 train_time:141963ms step_avg:61.46ms
step:2311/2330 train_time:142023ms step_avg:61.46ms
step:2312/2330 train_time:142086ms step_avg:61.46ms
step:2313/2330 train_time:142146ms step_avg:61.46ms
step:2314/2330 train_time:142209ms step_avg:61.46ms
step:2315/2330 train_time:142269ms step_avg:61.46ms
step:2316/2330 train_time:142332ms step_avg:61.46ms
step:2317/2330 train_time:142393ms step_avg:61.46ms
step:2318/2330 train_time:142457ms step_avg:61.46ms
step:2319/2330 train_time:142517ms step_avg:61.46ms
step:2320/2330 train_time:142580ms step_avg:61.46ms
step:2321/2330 train_time:142641ms step_avg:61.46ms
step:2322/2330 train_time:142704ms step_avg:61.46ms
step:2323/2330 train_time:142764ms step_avg:61.46ms
step:2324/2330 train_time:142828ms step_avg:61.46ms
step:2325/2330 train_time:142888ms step_avg:61.46ms
step:2326/2330 train_time:142952ms step_avg:61.46ms
step:2327/2330 train_time:143011ms step_avg:61.46ms
step:2328/2330 train_time:143074ms step_avg:61.46ms
step:2329/2330 train_time:143135ms step_avg:61.46ms
step:2330/2330 train_time:143198ms step_avg:61.46ms
step:2330/2330 val_loss:3.3566 train_time:143263ms step_avg:61.49ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
