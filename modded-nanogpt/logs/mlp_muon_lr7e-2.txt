import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr7e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:19:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:89ms step_avg:88.61ms
step:2/2330 train_time:152ms step_avg:76.06ms
step:3/2330 train_time:166ms step_avg:55.33ms
step:4/2330 train_time:180ms step_avg:44.88ms
step:5/2330 train_time:193ms step_avg:38.54ms
step:6/2330 train_time:303ms step_avg:50.55ms
step:7/2330 train_time:336ms step_avg:48.06ms
step:8/2330 train_time:380ms step_avg:47.44ms
step:9/2330 train_time:414ms step_avg:46.01ms
step:10/2330 train_time:458ms step_avg:45.79ms
step:11/2330 train_time:493ms step_avg:44.78ms
step:12/2330 train_time:536ms step_avg:44.68ms
step:13/2330 train_time:570ms step_avg:43.87ms
step:14/2330 train_time:614ms step_avg:43.85ms
step:15/2330 train_time:649ms step_avg:43.25ms
step:16/2330 train_time:693ms step_avg:43.32ms
step:17/2330 train_time:728ms step_avg:42.83ms
step:18/2330 train_time:772ms step_avg:42.88ms
step:19/2330 train_time:806ms step_avg:42.41ms
step:20/2330 train_time:850ms step_avg:42.48ms
step:21/2330 train_time:884ms step_avg:42.12ms
step:22/2330 train_time:928ms step_avg:42.19ms
step:23/2330 train_time:963ms step_avg:41.85ms
step:24/2330 train_time:1006ms step_avg:41.93ms
step:25/2330 train_time:1041ms step_avg:41.64ms
step:26/2330 train_time:1085ms step_avg:41.75ms
step:27/2330 train_time:1120ms step_avg:41.48ms
step:28/2330 train_time:1169ms step_avg:41.75ms
step:29/2330 train_time:1211ms step_avg:41.76ms
step:30/2330 train_time:1261ms step_avg:42.03ms
step:31/2330 train_time:1299ms step_avg:41.89ms
step:32/2330 train_time:1344ms step_avg:42.00ms
step:33/2330 train_time:1380ms step_avg:41.80ms
step:34/2330 train_time:1423ms step_avg:41.87ms
step:35/2330 train_time:1459ms step_avg:41.68ms
step:36/2330 train_time:1502ms step_avg:41.73ms
step:37/2330 train_time:1538ms step_avg:41.55ms
step:38/2330 train_time:1581ms step_avg:41.61ms
step:39/2330 train_time:1616ms step_avg:41.43ms
step:40/2330 train_time:1660ms step_avg:41.50ms
step:41/2330 train_time:1695ms step_avg:41.35ms
step:42/2330 train_time:1740ms step_avg:41.43ms
step:43/2330 train_time:1774ms step_avg:41.27ms
step:44/2330 train_time:1819ms step_avg:41.34ms
step:45/2330 train_time:1854ms step_avg:41.19ms
step:46/2330 train_time:1898ms step_avg:41.26ms
step:47/2330 train_time:1932ms step_avg:41.12ms
step:48/2330 train_time:1978ms step_avg:41.20ms
step:49/2330 train_time:2012ms step_avg:41.07ms
step:50/2330 train_time:2056ms step_avg:41.13ms
step:51/2330 train_time:2093ms step_avg:41.04ms
step:52/2330 train_time:2139ms step_avg:41.14ms
step:53/2330 train_time:2176ms step_avg:41.05ms
step:54/2330 train_time:2222ms step_avg:41.16ms
step:55/2330 train_time:2258ms step_avg:41.06ms
step:56/2330 train_time:2304ms step_avg:41.15ms
step:57/2330 train_time:2340ms step_avg:41.06ms
step:58/2330 train_time:2384ms step_avg:41.10ms
step:59/2330 train_time:2419ms step_avg:41.00ms
step:60/2330 train_time:2463ms step_avg:41.05ms
step:61/2330 train_time:2498ms step_avg:40.95ms
step:62/2330 train_time:2542ms step_avg:41.01ms
step:63/2330 train_time:2577ms step_avg:40.91ms
step:64/2330 train_time:2621ms step_avg:40.95ms
step:65/2330 train_time:2656ms step_avg:40.86ms
step:66/2330 train_time:2700ms step_avg:40.92ms
step:67/2330 train_time:2735ms step_avg:40.82ms
step:68/2330 train_time:2779ms step_avg:40.87ms
step:69/2330 train_time:2814ms step_avg:40.78ms
step:70/2330 train_time:2858ms step_avg:40.83ms
step:71/2330 train_time:2893ms step_avg:40.74ms
step:72/2330 train_time:2937ms step_avg:40.79ms
step:73/2330 train_time:2972ms step_avg:40.72ms
step:74/2330 train_time:3017ms step_avg:40.77ms
step:75/2330 train_time:3052ms step_avg:40.69ms
step:76/2330 train_time:3098ms step_avg:40.76ms
step:77/2330 train_time:3134ms step_avg:40.70ms
step:78/2330 train_time:3180ms step_avg:40.77ms
step:79/2330 train_time:3216ms step_avg:40.71ms
step:80/2330 train_time:3261ms step_avg:40.77ms
step:81/2330 train_time:3297ms step_avg:40.70ms
step:82/2330 train_time:3342ms step_avg:40.75ms
step:83/2330 train_time:3377ms step_avg:40.68ms
step:84/2330 train_time:3421ms step_avg:40.72ms
step:85/2330 train_time:3456ms step_avg:40.66ms
step:86/2330 train_time:3501ms step_avg:40.71ms
step:87/2330 train_time:3537ms step_avg:40.65ms
step:88/2330 train_time:3581ms step_avg:40.69ms
step:89/2330 train_time:3615ms step_avg:40.62ms
step:90/2330 train_time:3659ms step_avg:40.66ms
step:91/2330 train_time:3694ms step_avg:40.60ms
step:92/2330 train_time:3739ms step_avg:40.64ms
step:93/2330 train_time:3773ms step_avg:40.57ms
step:94/2330 train_time:3817ms step_avg:40.60ms
step:95/2330 train_time:3852ms step_avg:40.55ms
step:96/2330 train_time:3897ms step_avg:40.59ms
step:97/2330 train_time:3931ms step_avg:40.53ms
step:98/2330 train_time:3976ms step_avg:40.57ms
step:99/2330 train_time:4011ms step_avg:40.51ms
step:100/2330 train_time:4055ms step_avg:40.55ms
step:101/2330 train_time:4092ms step_avg:40.52ms
step:102/2330 train_time:4138ms step_avg:40.57ms
step:103/2330 train_time:4173ms step_avg:40.52ms
step:104/2330 train_time:4218ms step_avg:40.56ms
step:105/2330 train_time:4254ms step_avg:40.51ms
step:106/2330 train_time:4299ms step_avg:40.56ms
step:107/2330 train_time:4335ms step_avg:40.51ms
step:108/2330 train_time:4380ms step_avg:40.55ms
step:109/2330 train_time:4415ms step_avg:40.50ms
step:110/2330 train_time:4459ms step_avg:40.53ms
step:111/2330 train_time:4495ms step_avg:40.49ms
step:112/2330 train_time:4539ms step_avg:40.53ms
step:113/2330 train_time:4574ms step_avg:40.48ms
step:114/2330 train_time:4618ms step_avg:40.51ms
step:115/2330 train_time:4653ms step_avg:40.46ms
step:116/2330 train_time:4697ms step_avg:40.49ms
step:117/2330 train_time:4732ms step_avg:40.45ms
step:118/2330 train_time:4776ms step_avg:40.47ms
step:119/2330 train_time:4811ms step_avg:40.43ms
step:120/2330 train_time:4855ms step_avg:40.46ms
step:121/2330 train_time:4891ms step_avg:40.42ms
step:122/2330 train_time:4935ms step_avg:40.45ms
step:123/2330 train_time:4969ms step_avg:40.40ms
step:124/2330 train_time:5014ms step_avg:40.43ms
step:125/2330 train_time:5049ms step_avg:40.40ms
step:126/2330 train_time:5095ms step_avg:40.43ms
step:127/2330 train_time:5130ms step_avg:40.39ms
step:128/2330 train_time:5175ms step_avg:40.43ms
step:129/2330 train_time:5210ms step_avg:40.38ms
step:130/2330 train_time:5254ms step_avg:40.42ms
step:131/2330 train_time:5290ms step_avg:40.38ms
step:132/2330 train_time:5335ms step_avg:40.42ms
step:133/2330 train_time:5371ms step_avg:40.38ms
step:134/2330 train_time:5416ms step_avg:40.42ms
step:135/2330 train_time:5451ms step_avg:40.38ms
step:136/2330 train_time:5496ms step_avg:40.41ms
step:137/2330 train_time:5531ms step_avg:40.37ms
step:138/2330 train_time:5575ms step_avg:40.40ms
step:139/2330 train_time:5609ms step_avg:40.36ms
step:140/2330 train_time:5653ms step_avg:40.38ms
step:141/2330 train_time:5688ms step_avg:40.34ms
step:142/2330 train_time:5733ms step_avg:40.37ms
step:143/2330 train_time:5767ms step_avg:40.33ms
step:144/2330 train_time:5811ms step_avg:40.35ms
step:145/2330 train_time:5846ms step_avg:40.31ms
step:146/2330 train_time:5890ms step_avg:40.34ms
step:147/2330 train_time:5925ms step_avg:40.30ms
step:148/2330 train_time:5969ms step_avg:40.33ms
step:149/2330 train_time:6003ms step_avg:40.29ms
step:150/2330 train_time:6047ms step_avg:40.31ms
step:151/2330 train_time:6082ms step_avg:40.28ms
step:152/2330 train_time:6127ms step_avg:40.31ms
step:153/2330 train_time:6162ms step_avg:40.28ms
step:154/2330 train_time:6207ms step_avg:40.30ms
step:155/2330 train_time:6243ms step_avg:40.27ms
step:156/2330 train_time:6287ms step_avg:40.30ms
step:157/2330 train_time:6323ms step_avg:40.27ms
step:158/2330 train_time:6368ms step_avg:40.30ms
step:159/2330 train_time:6404ms step_avg:40.27ms
step:160/2330 train_time:6448ms step_avg:40.30ms
step:161/2330 train_time:6483ms step_avg:40.27ms
step:162/2330 train_time:6528ms step_avg:40.30ms
step:163/2330 train_time:6564ms step_avg:40.27ms
step:164/2330 train_time:6608ms step_avg:40.29ms
step:165/2330 train_time:6643ms step_avg:40.26ms
step:166/2330 train_time:6687ms step_avg:40.28ms
step:167/2330 train_time:6721ms step_avg:40.25ms
step:168/2330 train_time:6766ms step_avg:40.27ms
step:169/2330 train_time:6801ms step_avg:40.24ms
step:170/2330 train_time:6845ms step_avg:40.26ms
step:171/2330 train_time:6880ms step_avg:40.23ms
step:172/2330 train_time:6924ms step_avg:40.26ms
step:173/2330 train_time:6959ms step_avg:40.23ms
step:174/2330 train_time:7003ms step_avg:40.25ms
step:175/2330 train_time:7038ms step_avg:40.22ms
step:176/2330 train_time:7082ms step_avg:40.24ms
step:177/2330 train_time:7117ms step_avg:40.21ms
step:178/2330 train_time:7161ms step_avg:40.23ms
step:179/2330 train_time:7197ms step_avg:40.21ms
step:180/2330 train_time:7241ms step_avg:40.23ms
step:181/2330 train_time:7277ms step_avg:40.20ms
step:182/2330 train_time:7322ms step_avg:40.23ms
step:183/2330 train_time:7356ms step_avg:40.20ms
step:184/2330 train_time:7402ms step_avg:40.23ms
step:185/2330 train_time:7437ms step_avg:40.20ms
step:186/2330 train_time:7482ms step_avg:40.22ms
step:187/2330 train_time:7517ms step_avg:40.20ms
step:188/2330 train_time:7561ms step_avg:40.22ms
step:189/2330 train_time:7596ms step_avg:40.19ms
step:190/2330 train_time:7641ms step_avg:40.22ms
step:191/2330 train_time:7675ms step_avg:40.19ms
step:192/2330 train_time:7720ms step_avg:40.21ms
step:193/2330 train_time:7755ms step_avg:40.18ms
step:194/2330 train_time:7799ms step_avg:40.20ms
step:195/2330 train_time:7834ms step_avg:40.18ms
step:196/2330 train_time:7878ms step_avg:40.20ms
step:197/2330 train_time:7914ms step_avg:40.17ms
step:198/2330 train_time:7958ms step_avg:40.19ms
step:199/2330 train_time:7993ms step_avg:40.17ms
step:200/2330 train_time:8038ms step_avg:40.19ms
step:201/2330 train_time:8073ms step_avg:40.16ms
step:202/2330 train_time:8117ms step_avg:40.18ms
step:203/2330 train_time:8153ms step_avg:40.16ms
step:204/2330 train_time:8198ms step_avg:40.19ms
step:205/2330 train_time:8235ms step_avg:40.17ms
step:206/2330 train_time:8279ms step_avg:40.19ms
step:207/2330 train_time:8315ms step_avg:40.17ms
step:208/2330 train_time:8360ms step_avg:40.19ms
step:209/2330 train_time:8395ms step_avg:40.17ms
step:210/2330 train_time:8440ms step_avg:40.19ms
step:211/2330 train_time:8475ms step_avg:40.17ms
step:212/2330 train_time:8520ms step_avg:40.19ms
step:213/2330 train_time:8555ms step_avg:40.16ms
step:214/2330 train_time:8599ms step_avg:40.18ms
step:215/2330 train_time:8635ms step_avg:40.16ms
step:216/2330 train_time:8679ms step_avg:40.18ms
step:217/2330 train_time:8714ms step_avg:40.16ms
step:218/2330 train_time:8759ms step_avg:40.18ms
step:219/2330 train_time:8793ms step_avg:40.15ms
step:220/2330 train_time:8839ms step_avg:40.18ms
step:221/2330 train_time:8873ms step_avg:40.15ms
step:222/2330 train_time:8918ms step_avg:40.17ms
step:223/2330 train_time:8953ms step_avg:40.15ms
step:224/2330 train_time:8997ms step_avg:40.17ms
step:225/2330 train_time:9032ms step_avg:40.14ms
step:226/2330 train_time:9077ms step_avg:40.16ms
step:227/2330 train_time:9112ms step_avg:40.14ms
step:228/2330 train_time:9156ms step_avg:40.16ms
step:229/2330 train_time:9192ms step_avg:40.14ms
step:230/2330 train_time:9237ms step_avg:40.16ms
step:231/2330 train_time:9273ms step_avg:40.14ms
step:232/2330 train_time:9317ms step_avg:40.16ms
step:233/2330 train_time:9353ms step_avg:40.14ms
step:234/2330 train_time:9399ms step_avg:40.17ms
step:235/2330 train_time:9434ms step_avg:40.15ms
step:236/2330 train_time:9479ms step_avg:40.16ms
step:237/2330 train_time:9514ms step_avg:40.14ms
step:238/2330 train_time:9558ms step_avg:40.16ms
step:239/2330 train_time:9594ms step_avg:40.14ms
step:240/2330 train_time:9639ms step_avg:40.16ms
step:241/2330 train_time:9674ms step_avg:40.14ms
step:242/2330 train_time:9719ms step_avg:40.16ms
step:243/2330 train_time:9754ms step_avg:40.14ms
step:244/2330 train_time:9798ms step_avg:40.16ms
step:245/2330 train_time:9834ms step_avg:40.14ms
step:246/2330 train_time:9878ms step_avg:40.16ms
step:247/2330 train_time:9914ms step_avg:40.14ms
step:248/2330 train_time:9958ms step_avg:40.15ms
step:249/2330 train_time:9993ms step_avg:40.13ms
step:250/2330 train_time:10038ms step_avg:40.15ms
step:250/2330 val_loss:5.4251 train_time:10124ms step_avg:40.49ms
step:251/2330 train_time:10137ms step_avg:40.39ms
step:252/2330 train_time:10149ms step_avg:40.28ms
step:253/2330 train_time:10161ms step_avg:40.16ms
step:254/2330 train_time:10196ms step_avg:40.14ms
step:255/2330 train_time:10231ms step_avg:40.12ms
step:256/2330 train_time:10274ms step_avg:40.13ms
step:257/2330 train_time:10308ms step_avg:40.11ms
step:258/2330 train_time:10352ms step_avg:40.12ms
step:259/2330 train_time:10387ms step_avg:40.10ms
step:260/2330 train_time:10432ms step_avg:40.12ms
step:261/2330 train_time:10473ms step_avg:40.13ms
step:262/2330 train_time:10523ms step_avg:40.16ms
step:263/2330 train_time:10560ms step_avg:40.15ms
step:264/2330 train_time:10604ms step_avg:40.17ms
step:265/2330 train_time:10640ms step_avg:40.15ms
step:266/2330 train_time:10684ms step_avg:40.17ms
step:267/2330 train_time:10719ms step_avg:40.15ms
step:268/2330 train_time:10764ms step_avg:40.16ms
step:269/2330 train_time:10798ms step_avg:40.14ms
step:270/2330 train_time:10842ms step_avg:40.16ms
step:271/2330 train_time:10877ms step_avg:40.14ms
step:272/2330 train_time:10921ms step_avg:40.15ms
step:273/2330 train_time:10956ms step_avg:40.13ms
step:274/2330 train_time:11000ms step_avg:40.14ms
step:275/2330 train_time:11035ms step_avg:40.13ms
step:276/2330 train_time:11080ms step_avg:40.15ms
step:277/2330 train_time:11115ms step_avg:40.13ms
step:278/2330 train_time:11159ms step_avg:40.14ms
step:279/2330 train_time:11193ms step_avg:40.12ms
step:280/2330 train_time:11238ms step_avg:40.13ms
step:281/2330 train_time:11272ms step_avg:40.12ms
step:282/2330 train_time:11317ms step_avg:40.13ms
step:283/2330 train_time:11351ms step_avg:40.11ms
step:284/2330 train_time:11397ms step_avg:40.13ms
step:285/2330 train_time:11432ms step_avg:40.11ms
step:286/2330 train_time:11479ms step_avg:40.14ms
step:287/2330 train_time:11515ms step_avg:40.12ms
step:288/2330 train_time:11561ms step_avg:40.14ms
step:289/2330 train_time:11597ms step_avg:40.13ms
step:290/2330 train_time:11641ms step_avg:40.14ms
step:291/2330 train_time:11677ms step_avg:40.13ms
step:292/2330 train_time:11721ms step_avg:40.14ms
step:293/2330 train_time:11756ms step_avg:40.12ms
step:294/2330 train_time:11800ms step_avg:40.14ms
step:295/2330 train_time:11836ms step_avg:40.12ms
step:296/2330 train_time:11879ms step_avg:40.13ms
step:297/2330 train_time:11913ms step_avg:40.11ms
step:298/2330 train_time:11957ms step_avg:40.12ms
step:299/2330 train_time:11992ms step_avg:40.11ms
step:300/2330 train_time:12037ms step_avg:40.12ms
step:301/2330 train_time:12072ms step_avg:40.11ms
step:302/2330 train_time:12116ms step_avg:40.12ms
step:303/2330 train_time:12151ms step_avg:40.10ms
step:304/2330 train_time:12194ms step_avg:40.11ms
step:305/2330 train_time:12230ms step_avg:40.10ms
step:306/2330 train_time:12274ms step_avg:40.11ms
step:307/2330 train_time:12309ms step_avg:40.09ms
step:308/2330 train_time:12353ms step_avg:40.11ms
step:309/2330 train_time:12389ms step_avg:40.09ms
step:310/2330 train_time:12433ms step_avg:40.11ms
step:311/2330 train_time:12469ms step_avg:40.09ms
step:312/2330 train_time:12514ms step_avg:40.11ms
step:313/2330 train_time:12551ms step_avg:40.10ms
step:314/2330 train_time:12596ms step_avg:40.11ms
step:315/2330 train_time:12631ms step_avg:40.10ms
step:316/2330 train_time:12677ms step_avg:40.12ms
step:317/2330 train_time:12713ms step_avg:40.10ms
step:318/2330 train_time:12757ms step_avg:40.12ms
step:319/2330 train_time:12792ms step_avg:40.10ms
step:320/2330 train_time:12837ms step_avg:40.11ms
step:321/2330 train_time:12872ms step_avg:40.10ms
step:322/2330 train_time:12916ms step_avg:40.11ms
step:323/2330 train_time:12950ms step_avg:40.09ms
step:324/2330 train_time:12995ms step_avg:40.11ms
step:325/2330 train_time:13030ms step_avg:40.09ms
step:326/2330 train_time:13074ms step_avg:40.11ms
step:327/2330 train_time:13109ms step_avg:40.09ms
step:328/2330 train_time:13153ms step_avg:40.10ms
step:329/2330 train_time:13188ms step_avg:40.08ms
step:330/2330 train_time:13233ms step_avg:40.10ms
step:331/2330 train_time:13267ms step_avg:40.08ms
step:332/2330 train_time:13311ms step_avg:40.09ms
step:333/2330 train_time:13346ms step_avg:40.08ms
step:334/2330 train_time:13392ms step_avg:40.10ms
step:335/2330 train_time:13427ms step_avg:40.08ms
step:336/2330 train_time:13472ms step_avg:40.09ms
step:337/2330 train_time:13507ms step_avg:40.08ms
step:338/2330 train_time:13552ms step_avg:40.09ms
step:339/2330 train_time:13587ms step_avg:40.08ms
step:340/2330 train_time:13633ms step_avg:40.10ms
step:341/2330 train_time:13669ms step_avg:40.09ms
step:342/2330 train_time:13714ms step_avg:40.10ms
step:343/2330 train_time:13749ms step_avg:40.09ms
step:344/2330 train_time:13794ms step_avg:40.10ms
step:345/2330 train_time:13829ms step_avg:40.08ms
step:346/2330 train_time:13873ms step_avg:40.10ms
step:347/2330 train_time:13907ms step_avg:40.08ms
step:348/2330 train_time:13953ms step_avg:40.09ms
step:349/2330 train_time:13988ms step_avg:40.08ms
step:350/2330 train_time:14033ms step_avg:40.09ms
step:351/2330 train_time:14068ms step_avg:40.08ms
step:352/2330 train_time:14111ms step_avg:40.09ms
step:353/2330 train_time:14146ms step_avg:40.07ms
step:354/2330 train_time:14190ms step_avg:40.09ms
step:355/2330 train_time:14225ms step_avg:40.07ms
step:356/2330 train_time:14269ms step_avg:40.08ms
step:357/2330 train_time:14304ms step_avg:40.07ms
step:358/2330 train_time:14347ms step_avg:40.08ms
step:359/2330 train_time:14382ms step_avg:40.06ms
step:360/2330 train_time:14427ms step_avg:40.08ms
step:361/2330 train_time:14463ms step_avg:40.06ms
step:362/2330 train_time:14507ms step_avg:40.07ms
step:363/2330 train_time:14543ms step_avg:40.06ms
step:364/2330 train_time:14588ms step_avg:40.08ms
step:365/2330 train_time:14624ms step_avg:40.06ms
step:366/2330 train_time:14668ms step_avg:40.08ms
step:367/2330 train_time:14704ms step_avg:40.06ms
step:368/2330 train_time:14748ms step_avg:40.08ms
step:369/2330 train_time:14784ms step_avg:40.06ms
step:370/2330 train_time:14829ms step_avg:40.08ms
step:371/2330 train_time:14864ms step_avg:40.06ms
step:372/2330 train_time:14909ms step_avg:40.08ms
step:373/2330 train_time:14944ms step_avg:40.07ms
step:374/2330 train_time:14988ms step_avg:40.08ms
step:375/2330 train_time:15023ms step_avg:40.06ms
step:376/2330 train_time:15068ms step_avg:40.07ms
step:377/2330 train_time:15103ms step_avg:40.06ms
step:378/2330 train_time:15147ms step_avg:40.07ms
step:379/2330 train_time:15182ms step_avg:40.06ms
step:380/2330 train_time:15226ms step_avg:40.07ms
step:381/2330 train_time:15261ms step_avg:40.06ms
step:382/2330 train_time:15305ms step_avg:40.07ms
step:383/2330 train_time:15340ms step_avg:40.05ms
step:384/2330 train_time:15385ms step_avg:40.06ms
step:385/2330 train_time:15420ms step_avg:40.05ms
step:386/2330 train_time:15465ms step_avg:40.06ms
step:387/2330 train_time:15500ms step_avg:40.05ms
step:388/2330 train_time:15545ms step_avg:40.06ms
step:389/2330 train_time:15580ms step_avg:40.05ms
step:390/2330 train_time:15624ms step_avg:40.06ms
step:391/2330 train_time:15659ms step_avg:40.05ms
step:392/2330 train_time:15703ms step_avg:40.06ms
step:393/2330 train_time:15738ms step_avg:40.05ms
step:394/2330 train_time:15783ms step_avg:40.06ms
step:395/2330 train_time:15818ms step_avg:40.05ms
step:396/2330 train_time:15862ms step_avg:40.06ms
step:397/2330 train_time:15898ms step_avg:40.04ms
step:398/2330 train_time:15942ms step_avg:40.06ms
step:399/2330 train_time:15978ms step_avg:40.04ms
step:400/2330 train_time:16021ms step_avg:40.05ms
step:401/2330 train_time:16057ms step_avg:40.04ms
step:402/2330 train_time:16101ms step_avg:40.05ms
step:403/2330 train_time:16136ms step_avg:40.04ms
step:404/2330 train_time:16180ms step_avg:40.05ms
step:405/2330 train_time:16215ms step_avg:40.04ms
step:406/2330 train_time:16259ms step_avg:40.05ms
step:407/2330 train_time:16294ms step_avg:40.03ms
step:408/2330 train_time:16339ms step_avg:40.05ms
step:409/2330 train_time:16373ms step_avg:40.03ms
step:410/2330 train_time:16418ms step_avg:40.04ms
step:411/2330 train_time:16453ms step_avg:40.03ms
step:412/2330 train_time:16498ms step_avg:40.04ms
step:413/2330 train_time:16534ms step_avg:40.03ms
step:414/2330 train_time:16579ms step_avg:40.05ms
step:415/2330 train_time:16613ms step_avg:40.03ms
step:416/2330 train_time:16658ms step_avg:40.04ms
step:417/2330 train_time:16693ms step_avg:40.03ms
step:418/2330 train_time:16738ms step_avg:40.04ms
step:419/2330 train_time:16773ms step_avg:40.03ms
step:420/2330 train_time:16817ms step_avg:40.04ms
step:421/2330 train_time:16853ms step_avg:40.03ms
step:422/2330 train_time:16898ms step_avg:40.04ms
step:423/2330 train_time:16932ms step_avg:40.03ms
step:424/2330 train_time:16977ms step_avg:40.04ms
step:425/2330 train_time:17012ms step_avg:40.03ms
step:426/2330 train_time:17056ms step_avg:40.04ms
step:427/2330 train_time:17092ms step_avg:40.03ms
step:428/2330 train_time:17136ms step_avg:40.04ms
step:429/2330 train_time:17172ms step_avg:40.03ms
step:430/2330 train_time:17217ms step_avg:40.04ms
step:431/2330 train_time:17251ms step_avg:40.03ms
step:432/2330 train_time:17296ms step_avg:40.04ms
step:433/2330 train_time:17331ms step_avg:40.02ms
step:434/2330 train_time:17376ms step_avg:40.04ms
step:435/2330 train_time:17411ms step_avg:40.02ms
step:436/2330 train_time:17455ms step_avg:40.03ms
step:437/2330 train_time:17490ms step_avg:40.02ms
step:438/2330 train_time:17535ms step_avg:40.03ms
step:439/2330 train_time:17570ms step_avg:40.02ms
step:440/2330 train_time:17615ms step_avg:40.03ms
step:441/2330 train_time:17651ms step_avg:40.02ms
step:442/2330 train_time:17696ms step_avg:40.04ms
step:443/2330 train_time:17731ms step_avg:40.03ms
step:444/2330 train_time:17776ms step_avg:40.04ms
step:445/2330 train_time:17812ms step_avg:40.03ms
step:446/2330 train_time:17856ms step_avg:40.04ms
step:447/2330 train_time:17892ms step_avg:40.03ms
step:448/2330 train_time:17936ms step_avg:40.04ms
step:449/2330 train_time:17971ms step_avg:40.03ms
step:450/2330 train_time:18016ms step_avg:40.04ms
step:451/2330 train_time:18051ms step_avg:40.03ms
step:452/2330 train_time:18096ms step_avg:40.04ms
step:453/2330 train_time:18132ms step_avg:40.03ms
step:454/2330 train_time:18176ms step_avg:40.04ms
step:455/2330 train_time:18211ms step_avg:40.02ms
step:456/2330 train_time:18256ms step_avg:40.03ms
step:457/2330 train_time:18291ms step_avg:40.02ms
step:458/2330 train_time:18337ms step_avg:40.04ms
step:459/2330 train_time:18371ms step_avg:40.03ms
step:460/2330 train_time:18416ms step_avg:40.04ms
step:461/2330 train_time:18452ms step_avg:40.03ms
step:462/2330 train_time:18497ms step_avg:40.04ms
step:463/2330 train_time:18532ms step_avg:40.03ms
step:464/2330 train_time:18576ms step_avg:40.04ms
step:465/2330 train_time:18612ms step_avg:40.02ms
step:466/2330 train_time:18656ms step_avg:40.03ms
step:467/2330 train_time:18692ms step_avg:40.02ms
step:468/2330 train_time:18736ms step_avg:40.03ms
step:469/2330 train_time:18771ms step_avg:40.02ms
step:470/2330 train_time:18815ms step_avg:40.03ms
step:471/2330 train_time:18851ms step_avg:40.02ms
step:472/2330 train_time:18896ms step_avg:40.03ms
step:473/2330 train_time:18932ms step_avg:40.02ms
step:474/2330 train_time:18977ms step_avg:40.04ms
step:475/2330 train_time:19012ms step_avg:40.02ms
step:476/2330 train_time:19056ms step_avg:40.03ms
step:477/2330 train_time:19092ms step_avg:40.02ms
step:478/2330 train_time:19137ms step_avg:40.04ms
step:479/2330 train_time:19172ms step_avg:40.03ms
step:480/2330 train_time:19216ms step_avg:40.03ms
step:481/2330 train_time:19251ms step_avg:40.02ms
step:482/2330 train_time:19296ms step_avg:40.03ms
step:483/2330 train_time:19332ms step_avg:40.02ms
step:484/2330 train_time:19377ms step_avg:40.03ms
step:485/2330 train_time:19412ms step_avg:40.02ms
step:486/2330 train_time:19456ms step_avg:40.03ms
step:487/2330 train_time:19491ms step_avg:40.02ms
step:488/2330 train_time:19536ms step_avg:40.03ms
step:489/2330 train_time:19572ms step_avg:40.02ms
step:490/2330 train_time:19616ms step_avg:40.03ms
step:491/2330 train_time:19651ms step_avg:40.02ms
step:492/2330 train_time:19696ms step_avg:40.03ms
step:493/2330 train_time:19731ms step_avg:40.02ms
step:494/2330 train_time:19775ms step_avg:40.03ms
step:495/2330 train_time:19811ms step_avg:40.02ms
step:496/2330 train_time:19856ms step_avg:40.03ms
step:497/2330 train_time:19891ms step_avg:40.02ms
step:498/2330 train_time:19936ms step_avg:40.03ms
step:499/2330 train_time:19971ms step_avg:40.02ms
step:500/2330 train_time:20016ms step_avg:40.03ms
step:500/2330 val_loss:5.3028 train_time:20103ms step_avg:40.21ms
step:501/2330 train_time:20116ms step_avg:40.15ms
step:502/2330 train_time:20127ms step_avg:40.09ms
step:503/2330 train_time:20138ms step_avg:40.04ms
step:504/2330 train_time:20176ms step_avg:40.03ms
step:505/2330 train_time:20210ms step_avg:40.02ms
step:506/2330 train_time:20254ms step_avg:40.03ms
step:507/2330 train_time:20289ms step_avg:40.02ms
step:508/2330 train_time:20332ms step_avg:40.02ms
step:509/2330 train_time:20366ms step_avg:40.01ms
step:510/2330 train_time:20410ms step_avg:40.02ms
step:511/2330 train_time:20453ms step_avg:40.03ms
step:512/2330 train_time:20500ms step_avg:40.04ms
step:513/2330 train_time:20535ms step_avg:40.03ms
step:514/2330 train_time:20579ms step_avg:40.04ms
step:515/2330 train_time:20614ms step_avg:40.03ms
step:516/2330 train_time:20658ms step_avg:40.03ms
step:517/2330 train_time:20693ms step_avg:40.03ms
step:518/2330 train_time:20737ms step_avg:40.03ms
step:519/2330 train_time:20771ms step_avg:40.02ms
step:520/2330 train_time:20816ms step_avg:40.03ms
step:521/2330 train_time:20850ms step_avg:40.02ms
step:522/2330 train_time:20894ms step_avg:40.03ms
step:523/2330 train_time:20928ms step_avg:40.02ms
step:524/2330 train_time:20972ms step_avg:40.02ms
step:525/2330 train_time:21007ms step_avg:40.01ms
step:526/2330 train_time:21053ms step_avg:40.02ms
step:527/2330 train_time:21090ms step_avg:40.02ms
step:528/2330 train_time:21134ms step_avg:40.03ms
step:529/2330 train_time:21169ms step_avg:40.02ms
step:530/2330 train_time:21213ms step_avg:40.02ms
step:531/2330 train_time:21248ms step_avg:40.02ms
step:532/2330 train_time:21293ms step_avg:40.02ms
step:533/2330 train_time:21329ms step_avg:40.02ms
step:534/2330 train_time:21374ms step_avg:40.03ms
step:535/2330 train_time:21410ms step_avg:40.02ms
step:536/2330 train_time:21456ms step_avg:40.03ms
step:537/2330 train_time:21492ms step_avg:40.02ms
step:538/2330 train_time:21537ms step_avg:40.03ms
step:539/2330 train_time:21573ms step_avg:40.02ms
step:540/2330 train_time:21618ms step_avg:40.03ms
step:541/2330 train_time:21654ms step_avg:40.03ms
step:542/2330 train_time:21698ms step_avg:40.03ms
step:543/2330 train_time:21734ms step_avg:40.03ms
step:544/2330 train_time:21777ms step_avg:40.03ms
step:545/2330 train_time:21812ms step_avg:40.02ms
step:546/2330 train_time:21856ms step_avg:40.03ms
step:547/2330 train_time:21891ms step_avg:40.02ms
step:548/2330 train_time:21936ms step_avg:40.03ms
step:549/2330 train_time:21971ms step_avg:40.02ms
step:550/2330 train_time:22015ms step_avg:40.03ms
step:551/2330 train_time:22051ms step_avg:40.02ms
step:552/2330 train_time:22095ms step_avg:40.03ms
step:553/2330 train_time:22130ms step_avg:40.02ms
step:554/2330 train_time:22175ms step_avg:40.03ms
step:555/2330 train_time:22209ms step_avg:40.02ms
step:556/2330 train_time:22255ms step_avg:40.03ms
step:557/2330 train_time:22290ms step_avg:40.02ms
step:558/2330 train_time:22334ms step_avg:40.03ms
step:559/2330 train_time:22369ms step_avg:40.02ms
step:560/2330 train_time:22415ms step_avg:40.03ms
step:561/2330 train_time:22451ms step_avg:40.02ms
step:562/2330 train_time:22496ms step_avg:40.03ms
step:563/2330 train_time:22532ms step_avg:40.02ms
step:564/2330 train_time:22576ms step_avg:40.03ms
step:565/2330 train_time:22612ms step_avg:40.02ms
step:566/2330 train_time:22657ms step_avg:40.03ms
step:567/2330 train_time:22692ms step_avg:40.02ms
step:568/2330 train_time:22736ms step_avg:40.03ms
step:569/2330 train_time:22771ms step_avg:40.02ms
step:570/2330 train_time:22816ms step_avg:40.03ms
step:571/2330 train_time:22851ms step_avg:40.02ms
step:572/2330 train_time:22895ms step_avg:40.03ms
step:573/2330 train_time:22930ms step_avg:40.02ms
step:574/2330 train_time:22974ms step_avg:40.02ms
step:575/2330 train_time:23009ms step_avg:40.02ms
step:576/2330 train_time:23053ms step_avg:40.02ms
step:577/2330 train_time:23089ms step_avg:40.02ms
step:578/2330 train_time:23134ms step_avg:40.02ms
step:579/2330 train_time:23168ms step_avg:40.01ms
step:580/2330 train_time:23212ms step_avg:40.02ms
step:581/2330 train_time:23247ms step_avg:40.01ms
step:582/2330 train_time:23292ms step_avg:40.02ms
step:583/2330 train_time:23326ms step_avg:40.01ms
step:584/2330 train_time:23371ms step_avg:40.02ms
step:585/2330 train_time:23407ms step_avg:40.01ms
step:586/2330 train_time:23452ms step_avg:40.02ms
step:587/2330 train_time:23488ms step_avg:40.01ms
step:588/2330 train_time:23532ms step_avg:40.02ms
step:589/2330 train_time:23568ms step_avg:40.01ms
step:590/2330 train_time:23613ms step_avg:40.02ms
step:591/2330 train_time:23649ms step_avg:40.02ms
step:592/2330 train_time:23693ms step_avg:40.02ms
step:593/2330 train_time:23729ms step_avg:40.01ms
step:594/2330 train_time:23774ms step_avg:40.02ms
step:595/2330 train_time:23809ms step_avg:40.02ms
step:596/2330 train_time:23854ms step_avg:40.02ms
step:597/2330 train_time:23889ms step_avg:40.02ms
step:598/2330 train_time:23934ms step_avg:40.02ms
step:599/2330 train_time:23969ms step_avg:40.01ms
step:600/2330 train_time:24013ms step_avg:40.02ms
step:601/2330 train_time:24049ms step_avg:40.01ms
step:602/2330 train_time:24093ms step_avg:40.02ms
step:603/2330 train_time:24128ms step_avg:40.01ms
step:604/2330 train_time:24173ms step_avg:40.02ms
step:605/2330 train_time:24208ms step_avg:40.01ms
step:606/2330 train_time:24252ms step_avg:40.02ms
step:607/2330 train_time:24287ms step_avg:40.01ms
step:608/2330 train_time:24331ms step_avg:40.02ms
step:609/2330 train_time:24367ms step_avg:40.01ms
step:610/2330 train_time:24412ms step_avg:40.02ms
step:611/2330 train_time:24448ms step_avg:40.01ms
step:612/2330 train_time:24493ms step_avg:40.02ms
step:613/2330 train_time:24529ms step_avg:40.02ms
step:614/2330 train_time:24574ms step_avg:40.02ms
step:615/2330 train_time:24609ms step_avg:40.01ms
step:616/2330 train_time:24655ms step_avg:40.02ms
step:617/2330 train_time:24690ms step_avg:40.02ms
step:618/2330 train_time:24735ms step_avg:40.02ms
step:619/2330 train_time:24770ms step_avg:40.02ms
step:620/2330 train_time:24814ms step_avg:40.02ms
step:621/2330 train_time:24849ms step_avg:40.02ms
step:622/2330 train_time:24894ms step_avg:40.02ms
step:623/2330 train_time:24930ms step_avg:40.02ms
step:624/2330 train_time:24974ms step_avg:40.02ms
step:625/2330 train_time:25008ms step_avg:40.01ms
step:626/2330 train_time:25053ms step_avg:40.02ms
step:627/2330 train_time:25089ms step_avg:40.01ms
step:628/2330 train_time:25133ms step_avg:40.02ms
step:629/2330 train_time:25168ms step_avg:40.01ms
step:630/2330 train_time:25212ms step_avg:40.02ms
step:631/2330 train_time:25247ms step_avg:40.01ms
step:632/2330 train_time:25291ms step_avg:40.02ms
step:633/2330 train_time:25327ms step_avg:40.01ms
step:634/2330 train_time:25371ms step_avg:40.02ms
step:635/2330 train_time:25406ms step_avg:40.01ms
step:636/2330 train_time:25451ms step_avg:40.02ms
step:637/2330 train_time:25486ms step_avg:40.01ms
step:638/2330 train_time:25530ms step_avg:40.02ms
step:639/2330 train_time:25564ms step_avg:40.01ms
step:640/2330 train_time:25609ms step_avg:40.01ms
step:641/2330 train_time:25645ms step_avg:40.01ms
step:642/2330 train_time:25689ms step_avg:40.01ms
step:643/2330 train_time:25724ms step_avg:40.01ms
step:644/2330 train_time:25768ms step_avg:40.01ms
step:645/2330 train_time:25804ms step_avg:40.01ms
step:646/2330 train_time:25849ms step_avg:40.01ms
step:647/2330 train_time:25884ms step_avg:40.01ms
step:648/2330 train_time:25928ms step_avg:40.01ms
step:649/2330 train_time:25963ms step_avg:40.00ms
step:650/2330 train_time:26008ms step_avg:40.01ms
step:651/2330 train_time:26043ms step_avg:40.00ms
step:652/2330 train_time:26087ms step_avg:40.01ms
step:653/2330 train_time:26122ms step_avg:40.00ms
step:654/2330 train_time:26167ms step_avg:40.01ms
step:655/2330 train_time:26202ms step_avg:40.00ms
step:656/2330 train_time:26246ms step_avg:40.01ms
step:657/2330 train_time:26281ms step_avg:40.00ms
step:658/2330 train_time:26325ms step_avg:40.01ms
step:659/2330 train_time:26360ms step_avg:40.00ms
step:660/2330 train_time:26405ms step_avg:40.01ms
step:661/2330 train_time:26440ms step_avg:40.00ms
step:662/2330 train_time:26484ms step_avg:40.01ms
step:663/2330 train_time:26519ms step_avg:40.00ms
step:664/2330 train_time:26563ms step_avg:40.00ms
step:665/2330 train_time:26597ms step_avg:40.00ms
step:666/2330 train_time:26642ms step_avg:40.00ms
step:667/2330 train_time:26677ms step_avg:40.00ms
step:668/2330 train_time:26721ms step_avg:40.00ms
step:669/2330 train_time:26757ms step_avg:40.00ms
step:670/2330 train_time:26802ms step_avg:40.00ms
step:671/2330 train_time:26837ms step_avg:39.99ms
step:672/2330 train_time:26881ms step_avg:40.00ms
step:673/2330 train_time:26915ms step_avg:39.99ms
step:674/2330 train_time:26959ms step_avg:40.00ms
step:675/2330 train_time:26995ms step_avg:39.99ms
step:676/2330 train_time:27039ms step_avg:40.00ms
step:677/2330 train_time:27074ms step_avg:39.99ms
step:678/2330 train_time:27118ms step_avg:40.00ms
step:679/2330 train_time:27153ms step_avg:39.99ms
step:680/2330 train_time:27198ms step_avg:40.00ms
step:681/2330 train_time:27233ms step_avg:39.99ms
step:682/2330 train_time:27278ms step_avg:40.00ms
step:683/2330 train_time:27313ms step_avg:39.99ms
step:684/2330 train_time:27358ms step_avg:40.00ms
step:685/2330 train_time:27392ms step_avg:39.99ms
step:686/2330 train_time:27436ms step_avg:39.99ms
step:687/2330 train_time:27472ms step_avg:39.99ms
step:688/2330 train_time:27516ms step_avg:39.99ms
step:689/2330 train_time:27552ms step_avg:39.99ms
step:690/2330 train_time:27596ms step_avg:39.99ms
step:691/2330 train_time:27631ms step_avg:39.99ms
step:692/2330 train_time:27675ms step_avg:39.99ms
step:693/2330 train_time:27710ms step_avg:39.99ms
step:694/2330 train_time:27756ms step_avg:39.99ms
step:695/2330 train_time:27792ms step_avg:39.99ms
step:696/2330 train_time:27836ms step_avg:39.99ms
step:697/2330 train_time:27871ms step_avg:39.99ms
step:698/2330 train_time:27915ms step_avg:39.99ms
step:699/2330 train_time:27951ms step_avg:39.99ms
step:700/2330 train_time:27995ms step_avg:39.99ms
step:701/2330 train_time:28031ms step_avg:39.99ms
step:702/2330 train_time:28076ms step_avg:39.99ms
step:703/2330 train_time:28110ms step_avg:39.99ms
step:704/2330 train_time:28155ms step_avg:39.99ms
step:705/2330 train_time:28190ms step_avg:39.99ms
step:706/2330 train_time:28235ms step_avg:39.99ms
step:707/2330 train_time:28270ms step_avg:39.99ms
step:708/2330 train_time:28315ms step_avg:39.99ms
step:709/2330 train_time:28350ms step_avg:39.99ms
step:710/2330 train_time:28394ms step_avg:39.99ms
step:711/2330 train_time:28430ms step_avg:39.99ms
step:712/2330 train_time:28476ms step_avg:39.99ms
step:713/2330 train_time:28511ms step_avg:39.99ms
step:714/2330 train_time:28556ms step_avg:39.99ms
step:715/2330 train_time:28591ms step_avg:39.99ms
step:716/2330 train_time:28636ms step_avg:39.99ms
step:717/2330 train_time:28671ms step_avg:39.99ms
step:718/2330 train_time:28716ms step_avg:39.99ms
step:719/2330 train_time:28751ms step_avg:39.99ms
step:720/2330 train_time:28795ms step_avg:39.99ms
step:721/2330 train_time:28830ms step_avg:39.99ms
step:722/2330 train_time:28874ms step_avg:39.99ms
step:723/2330 train_time:28909ms step_avg:39.99ms
step:724/2330 train_time:28955ms step_avg:39.99ms
step:725/2330 train_time:28990ms step_avg:39.99ms
step:726/2330 train_time:29035ms step_avg:39.99ms
step:727/2330 train_time:29070ms step_avg:39.99ms
step:728/2330 train_time:29114ms step_avg:39.99ms
step:729/2330 train_time:29149ms step_avg:39.99ms
step:730/2330 train_time:29195ms step_avg:39.99ms
step:731/2330 train_time:29230ms step_avg:39.99ms
step:732/2330 train_time:29274ms step_avg:39.99ms
step:733/2330 train_time:29309ms step_avg:39.99ms
step:734/2330 train_time:29354ms step_avg:39.99ms
step:735/2330 train_time:29388ms step_avg:39.98ms
step:736/2330 train_time:29433ms step_avg:39.99ms
step:737/2330 train_time:29468ms step_avg:39.98ms
step:738/2330 train_time:29513ms step_avg:39.99ms
step:739/2330 train_time:29548ms step_avg:39.98ms
step:740/2330 train_time:29592ms step_avg:39.99ms
step:741/2330 train_time:29627ms step_avg:39.98ms
step:742/2330 train_time:29672ms step_avg:39.99ms
step:743/2330 train_time:29706ms step_avg:39.98ms
step:744/2330 train_time:29750ms step_avg:39.99ms
step:745/2330 train_time:29786ms step_avg:39.98ms
step:746/2330 train_time:29831ms step_avg:39.99ms
step:747/2330 train_time:29866ms step_avg:39.98ms
step:748/2330 train_time:29910ms step_avg:39.99ms
step:749/2330 train_time:29946ms step_avg:39.98ms
step:750/2330 train_time:29990ms step_avg:39.99ms
step:750/2330 val_loss:5.2398 train_time:30077ms step_avg:40.10ms
step:751/2330 train_time:30091ms step_avg:40.07ms
step:752/2330 train_time:30103ms step_avg:40.03ms
step:753/2330 train_time:30114ms step_avg:39.99ms
step:754/2330 train_time:30150ms step_avg:39.99ms
step:755/2330 train_time:30184ms step_avg:39.98ms
step:756/2330 train_time:30227ms step_avg:39.98ms
step:757/2330 train_time:30262ms step_avg:39.98ms
step:758/2330 train_time:30305ms step_avg:39.98ms
step:759/2330 train_time:30339ms step_avg:39.97ms
step:760/2330 train_time:30382ms step_avg:39.98ms
step:761/2330 train_time:30421ms step_avg:39.97ms
step:762/2330 train_time:30467ms step_avg:39.98ms
step:763/2330 train_time:30503ms step_avg:39.98ms
step:764/2330 train_time:30549ms step_avg:39.98ms
step:765/2330 train_time:30586ms step_avg:39.98ms
step:766/2330 train_time:30630ms step_avg:39.99ms
step:767/2330 train_time:30665ms step_avg:39.98ms
step:768/2330 train_time:30708ms step_avg:39.98ms
step:769/2330 train_time:30744ms step_avg:39.98ms
step:770/2330 train_time:30788ms step_avg:39.98ms
step:771/2330 train_time:30823ms step_avg:39.98ms
step:772/2330 train_time:30866ms step_avg:39.98ms
step:773/2330 train_time:30901ms step_avg:39.98ms
step:774/2330 train_time:30945ms step_avg:39.98ms
step:775/2330 train_time:30979ms step_avg:39.97ms
step:776/2330 train_time:31024ms step_avg:39.98ms
step:777/2330 train_time:31059ms step_avg:39.97ms
step:778/2330 train_time:31104ms step_avg:39.98ms
step:779/2330 train_time:31138ms step_avg:39.97ms
step:780/2330 train_time:31182ms step_avg:39.98ms
step:781/2330 train_time:31217ms step_avg:39.97ms
step:782/2330 train_time:31261ms step_avg:39.98ms
step:783/2330 train_time:31296ms step_avg:39.97ms
step:784/2330 train_time:31340ms step_avg:39.97ms
step:785/2330 train_time:31377ms step_avg:39.97ms
step:786/2330 train_time:31423ms step_avg:39.98ms
step:787/2330 train_time:31458ms step_avg:39.97ms
step:788/2330 train_time:31503ms step_avg:39.98ms
step:789/2330 train_time:31539ms step_avg:39.97ms
step:790/2330 train_time:31584ms step_avg:39.98ms
step:791/2330 train_time:31619ms step_avg:39.97ms
step:792/2330 train_time:31664ms step_avg:39.98ms
step:793/2330 train_time:31699ms step_avg:39.97ms
step:794/2330 train_time:31744ms step_avg:39.98ms
step:795/2330 train_time:31778ms step_avg:39.97ms
step:796/2330 train_time:31822ms step_avg:39.98ms
step:797/2330 train_time:31857ms step_avg:39.97ms
step:798/2330 train_time:31901ms step_avg:39.98ms
step:799/2330 train_time:31936ms step_avg:39.97ms
step:800/2330 train_time:31979ms step_avg:39.97ms
step:801/2330 train_time:32014ms step_avg:39.97ms
step:802/2330 train_time:32058ms step_avg:39.97ms
step:803/2330 train_time:32092ms step_avg:39.97ms
step:804/2330 train_time:32136ms step_avg:39.97ms
step:805/2330 train_time:32170ms step_avg:39.96ms
step:806/2330 train_time:32216ms step_avg:39.97ms
step:807/2330 train_time:32250ms step_avg:39.96ms
step:808/2330 train_time:32294ms step_avg:39.97ms
step:809/2330 train_time:32329ms step_avg:39.96ms
step:810/2330 train_time:32373ms step_avg:39.97ms
step:811/2330 train_time:32408ms step_avg:39.96ms
step:812/2330 train_time:32454ms step_avg:39.97ms
step:813/2330 train_time:32490ms step_avg:39.96ms
step:814/2330 train_time:32536ms step_avg:39.97ms
step:815/2330 train_time:32572ms step_avg:39.97ms
step:816/2330 train_time:32617ms step_avg:39.97ms
step:817/2330 train_time:32654ms step_avg:39.97ms
step:818/2330 train_time:32699ms step_avg:39.97ms
step:819/2330 train_time:32733ms step_avg:39.97ms
step:820/2330 train_time:32778ms step_avg:39.97ms
step:821/2330 train_time:32813ms step_avg:39.97ms
step:822/2330 train_time:32857ms step_avg:39.97ms
step:823/2330 train_time:32892ms step_avg:39.97ms
step:824/2330 train_time:32935ms step_avg:39.97ms
step:825/2330 train_time:32969ms step_avg:39.96ms
step:826/2330 train_time:33014ms step_avg:39.97ms
step:827/2330 train_time:33048ms step_avg:39.96ms
step:828/2330 train_time:33092ms step_avg:39.97ms
step:829/2330 train_time:33127ms step_avg:39.96ms
step:830/2330 train_time:33171ms step_avg:39.97ms
step:831/2330 train_time:33207ms step_avg:39.96ms
step:832/2330 train_time:33251ms step_avg:39.97ms
step:833/2330 train_time:33287ms step_avg:39.96ms
step:834/2330 train_time:33332ms step_avg:39.97ms
step:835/2330 train_time:33367ms step_avg:39.96ms
step:836/2330 train_time:33411ms step_avg:39.97ms
step:837/2330 train_time:33448ms step_avg:39.96ms
step:838/2330 train_time:33492ms step_avg:39.97ms
step:839/2330 train_time:33528ms step_avg:39.96ms
step:840/2330 train_time:33574ms step_avg:39.97ms
step:841/2330 train_time:33611ms step_avg:39.96ms
step:842/2330 train_time:33656ms step_avg:39.97ms
step:843/2330 train_time:33692ms step_avg:39.97ms
step:844/2330 train_time:33736ms step_avg:39.97ms
step:845/2330 train_time:33772ms step_avg:39.97ms
step:846/2330 train_time:33817ms step_avg:39.97ms
step:847/2330 train_time:33852ms step_avg:39.97ms
step:848/2330 train_time:33895ms step_avg:39.97ms
step:849/2330 train_time:33930ms step_avg:39.96ms
step:850/2330 train_time:33974ms step_avg:39.97ms
step:851/2330 train_time:34008ms step_avg:39.96ms
step:852/2330 train_time:34052ms step_avg:39.97ms
step:853/2330 train_time:34087ms step_avg:39.96ms
step:854/2330 train_time:34131ms step_avg:39.97ms
step:855/2330 train_time:34166ms step_avg:39.96ms
step:856/2330 train_time:34209ms step_avg:39.96ms
step:857/2330 train_time:34244ms step_avg:39.96ms
step:858/2330 train_time:34288ms step_avg:39.96ms
step:859/2330 train_time:34323ms step_avg:39.96ms
step:860/2330 train_time:34367ms step_avg:39.96ms
step:861/2330 train_time:34403ms step_avg:39.96ms
step:862/2330 train_time:34448ms step_avg:39.96ms
step:863/2330 train_time:34483ms step_avg:39.96ms
step:864/2330 train_time:34528ms step_avg:39.96ms
step:865/2330 train_time:34563ms step_avg:39.96ms
step:866/2330 train_time:34607ms step_avg:39.96ms
step:867/2330 train_time:34643ms step_avg:39.96ms
step:868/2330 train_time:34689ms step_avg:39.96ms
step:869/2330 train_time:34724ms step_avg:39.96ms
step:870/2330 train_time:34768ms step_avg:39.96ms
step:871/2330 train_time:34804ms step_avg:39.96ms
step:872/2330 train_time:34847ms step_avg:39.96ms
step:873/2330 train_time:34882ms step_avg:39.96ms
step:874/2330 train_time:34926ms step_avg:39.96ms
step:875/2330 train_time:34962ms step_avg:39.96ms
step:876/2330 train_time:35005ms step_avg:39.96ms
step:877/2330 train_time:35040ms step_avg:39.95ms
step:878/2330 train_time:35084ms step_avg:39.96ms
step:879/2330 train_time:35119ms step_avg:39.95ms
step:880/2330 train_time:35163ms step_avg:39.96ms
step:881/2330 train_time:35198ms step_avg:39.95ms
step:882/2330 train_time:35242ms step_avg:39.96ms
step:883/2330 train_time:35277ms step_avg:39.95ms
step:884/2330 train_time:35322ms step_avg:39.96ms
step:885/2330 train_time:35357ms step_avg:39.95ms
step:886/2330 train_time:35402ms step_avg:39.96ms
step:887/2330 train_time:35437ms step_avg:39.95ms
step:888/2330 train_time:35481ms step_avg:39.96ms
step:889/2330 train_time:35516ms step_avg:39.95ms
step:890/2330 train_time:35561ms step_avg:39.96ms
step:891/2330 train_time:35597ms step_avg:39.95ms
step:892/2330 train_time:35641ms step_avg:39.96ms
step:893/2330 train_time:35677ms step_avg:39.95ms
step:894/2330 train_time:35721ms step_avg:39.96ms
step:895/2330 train_time:35756ms step_avg:39.95ms
step:896/2330 train_time:35800ms step_avg:39.96ms
step:897/2330 train_time:35835ms step_avg:39.95ms
step:898/2330 train_time:35879ms step_avg:39.95ms
step:899/2330 train_time:35915ms step_avg:39.95ms
step:900/2330 train_time:35959ms step_avg:39.95ms
step:901/2330 train_time:35993ms step_avg:39.95ms
step:902/2330 train_time:36038ms step_avg:39.95ms
step:903/2330 train_time:36073ms step_avg:39.95ms
step:904/2330 train_time:36118ms step_avg:39.95ms
step:905/2330 train_time:36152ms step_avg:39.95ms
step:906/2330 train_time:36196ms step_avg:39.95ms
step:907/2330 train_time:36231ms step_avg:39.95ms
step:908/2330 train_time:36276ms step_avg:39.95ms
step:909/2330 train_time:36311ms step_avg:39.95ms
step:910/2330 train_time:36356ms step_avg:39.95ms
step:911/2330 train_time:36391ms step_avg:39.95ms
step:912/2330 train_time:36435ms step_avg:39.95ms
step:913/2330 train_time:36470ms step_avg:39.95ms
step:914/2330 train_time:36514ms step_avg:39.95ms
step:915/2330 train_time:36550ms step_avg:39.95ms
step:916/2330 train_time:36595ms step_avg:39.95ms
step:917/2330 train_time:36630ms step_avg:39.95ms
step:918/2330 train_time:36675ms step_avg:39.95ms
step:919/2330 train_time:36710ms step_avg:39.95ms
step:920/2330 train_time:36755ms step_avg:39.95ms
step:921/2330 train_time:36790ms step_avg:39.95ms
step:922/2330 train_time:36834ms step_avg:39.95ms
step:923/2330 train_time:36869ms step_avg:39.94ms
step:924/2330 train_time:36913ms step_avg:39.95ms
step:925/2330 train_time:36948ms step_avg:39.94ms
step:926/2330 train_time:36992ms step_avg:39.95ms
step:927/2330 train_time:37027ms step_avg:39.94ms
step:928/2330 train_time:37072ms step_avg:39.95ms
step:929/2330 train_time:37107ms step_avg:39.94ms
step:930/2330 train_time:37152ms step_avg:39.95ms
step:931/2330 train_time:37187ms step_avg:39.94ms
step:932/2330 train_time:37231ms step_avg:39.95ms
step:933/2330 train_time:37267ms step_avg:39.94ms
step:934/2330 train_time:37312ms step_avg:39.95ms
step:935/2330 train_time:37348ms step_avg:39.94ms
step:936/2330 train_time:37392ms step_avg:39.95ms
step:937/2330 train_time:37427ms step_avg:39.94ms
step:938/2330 train_time:37472ms step_avg:39.95ms
step:939/2330 train_time:37507ms step_avg:39.94ms
step:940/2330 train_time:37553ms step_avg:39.95ms
step:941/2330 train_time:37588ms step_avg:39.95ms
step:942/2330 train_time:37633ms step_avg:39.95ms
step:943/2330 train_time:37669ms step_avg:39.95ms
step:944/2330 train_time:37713ms step_avg:39.95ms
step:945/2330 train_time:37749ms step_avg:39.95ms
step:946/2330 train_time:37794ms step_avg:39.95ms
step:947/2330 train_time:37829ms step_avg:39.95ms
step:948/2330 train_time:37873ms step_avg:39.95ms
step:949/2330 train_time:37909ms step_avg:39.95ms
step:950/2330 train_time:37953ms step_avg:39.95ms
step:951/2330 train_time:37989ms step_avg:39.95ms
step:952/2330 train_time:38033ms step_avg:39.95ms
step:953/2330 train_time:38068ms step_avg:39.95ms
step:954/2330 train_time:38112ms step_avg:39.95ms
step:955/2330 train_time:38148ms step_avg:39.95ms
step:956/2330 train_time:38193ms step_avg:39.95ms
step:957/2330 train_time:38228ms step_avg:39.95ms
step:958/2330 train_time:38273ms step_avg:39.95ms
step:959/2330 train_time:38309ms step_avg:39.95ms
step:960/2330 train_time:38354ms step_avg:39.95ms
step:961/2330 train_time:38389ms step_avg:39.95ms
step:962/2330 train_time:38433ms step_avg:39.95ms
step:963/2330 train_time:38468ms step_avg:39.95ms
step:964/2330 train_time:38513ms step_avg:39.95ms
step:965/2330 train_time:38548ms step_avg:39.95ms
step:966/2330 train_time:38593ms step_avg:39.95ms
step:967/2330 train_time:38629ms step_avg:39.95ms
step:968/2330 train_time:38673ms step_avg:39.95ms
step:969/2330 train_time:38709ms step_avg:39.95ms
step:970/2330 train_time:38754ms step_avg:39.95ms
step:971/2330 train_time:38790ms step_avg:39.95ms
step:972/2330 train_time:38833ms step_avg:39.95ms
step:973/2330 train_time:38868ms step_avg:39.95ms
step:974/2330 train_time:38912ms step_avg:39.95ms
step:975/2330 train_time:38947ms step_avg:39.95ms
step:976/2330 train_time:38991ms step_avg:39.95ms
step:977/2330 train_time:39026ms step_avg:39.94ms
step:978/2330 train_time:39070ms step_avg:39.95ms
step:979/2330 train_time:39105ms step_avg:39.94ms
step:980/2330 train_time:39150ms step_avg:39.95ms
step:981/2330 train_time:39186ms step_avg:39.94ms
step:982/2330 train_time:39230ms step_avg:39.95ms
step:983/2330 train_time:39265ms step_avg:39.94ms
step:984/2330 train_time:39310ms step_avg:39.95ms
step:985/2330 train_time:39344ms step_avg:39.94ms
step:986/2330 train_time:39389ms step_avg:39.95ms
step:987/2330 train_time:39424ms step_avg:39.94ms
step:988/2330 train_time:39467ms step_avg:39.95ms
step:989/2330 train_time:39502ms step_avg:39.94ms
step:990/2330 train_time:39548ms step_avg:39.95ms
step:991/2330 train_time:39583ms step_avg:39.94ms
step:992/2330 train_time:39627ms step_avg:39.95ms
step:993/2330 train_time:39662ms step_avg:39.94ms
step:994/2330 train_time:39706ms step_avg:39.95ms
step:995/2330 train_time:39742ms step_avg:39.94ms
step:996/2330 train_time:39786ms step_avg:39.95ms
step:997/2330 train_time:39821ms step_avg:39.94ms
step:998/2330 train_time:39865ms step_avg:39.95ms
step:999/2330 train_time:39900ms step_avg:39.94ms
step:1000/2330 train_time:39944ms step_avg:39.94ms
step:1000/2330 val_loss:5.2039 train_time:40030ms step_avg:40.03ms
step:1001/2330 train_time:40044ms step_avg:40.00ms
step:1002/2330 train_time:40056ms step_avg:39.98ms
step:1003/2330 train_time:40067ms step_avg:39.95ms
step:1004/2330 train_time:40104ms step_avg:39.94ms
step:1005/2330 train_time:40138ms step_avg:39.94ms
step:1006/2330 train_time:40181ms step_avg:39.94ms
step:1007/2330 train_time:40215ms step_avg:39.94ms
step:1008/2330 train_time:40259ms step_avg:39.94ms
step:1009/2330 train_time:40293ms step_avg:39.93ms
step:1010/2330 train_time:40340ms step_avg:39.94ms
step:1011/2330 train_time:40376ms step_avg:39.94ms
step:1012/2330 train_time:40423ms step_avg:39.94ms
step:1013/2330 train_time:40460ms step_avg:39.94ms
step:1014/2330 train_time:40505ms step_avg:39.95ms
step:1015/2330 train_time:40542ms step_avg:39.94ms
step:1016/2330 train_time:40587ms step_avg:39.95ms
step:1017/2330 train_time:40622ms step_avg:39.94ms
step:1018/2330 train_time:40666ms step_avg:39.95ms
step:1019/2330 train_time:40701ms step_avg:39.94ms
step:1020/2330 train_time:40744ms step_avg:39.95ms
step:1021/2330 train_time:40779ms step_avg:39.94ms
step:1022/2330 train_time:40823ms step_avg:39.94ms
step:1023/2330 train_time:40857ms step_avg:39.94ms
step:1024/2330 train_time:40901ms step_avg:39.94ms
step:1025/2330 train_time:40938ms step_avg:39.94ms
step:1026/2330 train_time:40983ms step_avg:39.94ms
step:1027/2330 train_time:41019ms step_avg:39.94ms
step:1028/2330 train_time:41063ms step_avg:39.94ms
step:1029/2330 train_time:41098ms step_avg:39.94ms
step:1030/2330 train_time:41142ms step_avg:39.94ms
step:1031/2330 train_time:41176ms step_avg:39.94ms
step:1032/2330 train_time:41219ms step_avg:39.94ms
step:1033/2330 train_time:41255ms step_avg:39.94ms
step:1034/2330 train_time:41300ms step_avg:39.94ms
step:1035/2330 train_time:41335ms step_avg:39.94ms
step:1036/2330 train_time:41381ms step_avg:39.94ms
step:1037/2330 train_time:41417ms step_avg:39.94ms
step:1038/2330 train_time:41463ms step_avg:39.94ms
step:1039/2330 train_time:41498ms step_avg:39.94ms
step:1040/2330 train_time:41544ms step_avg:39.95ms
step:1041/2330 train_time:41578ms step_avg:39.94ms
step:1042/2330 train_time:41624ms step_avg:39.95ms
step:1043/2330 train_time:41658ms step_avg:39.94ms
step:1044/2330 train_time:41703ms step_avg:39.95ms
step:1045/2330 train_time:41737ms step_avg:39.94ms
step:1046/2330 train_time:41782ms step_avg:39.94ms
step:1047/2330 train_time:41816ms step_avg:39.94ms
step:1048/2330 train_time:41861ms step_avg:39.94ms
step:1049/2330 train_time:41895ms step_avg:39.94ms
step:1050/2330 train_time:41940ms step_avg:39.94ms
step:1051/2330 train_time:41974ms step_avg:39.94ms
step:1052/2330 train_time:42018ms step_avg:39.94ms
step:1053/2330 train_time:42053ms step_avg:39.94ms
step:1054/2330 train_time:42097ms step_avg:39.94ms
step:1055/2330 train_time:42132ms step_avg:39.94ms
step:1056/2330 train_time:42177ms step_avg:39.94ms
step:1057/2330 train_time:42211ms step_avg:39.93ms
step:1058/2330 train_time:42255ms step_avg:39.94ms
step:1059/2330 train_time:42290ms step_avg:39.93ms
step:1060/2330 train_time:42335ms step_avg:39.94ms
step:1061/2330 train_time:42371ms step_avg:39.93ms
step:1062/2330 train_time:42416ms step_avg:39.94ms
step:1063/2330 train_time:42452ms step_avg:39.94ms
step:1064/2330 train_time:42497ms step_avg:39.94ms
step:1065/2330 train_time:42533ms step_avg:39.94ms
step:1066/2330 train_time:42579ms step_avg:39.94ms
step:1067/2330 train_time:42614ms step_avg:39.94ms
step:1068/2330 train_time:42659ms step_avg:39.94ms
step:1069/2330 train_time:42694ms step_avg:39.94ms
step:1070/2330 train_time:42738ms step_avg:39.94ms
step:1071/2330 train_time:42773ms step_avg:39.94ms
step:1072/2330 train_time:42817ms step_avg:39.94ms
step:1073/2330 train_time:42852ms step_avg:39.94ms
step:1074/2330 train_time:42898ms step_avg:39.94ms
step:1075/2330 train_time:42933ms step_avg:39.94ms
step:1076/2330 train_time:42976ms step_avg:39.94ms
step:1077/2330 train_time:43011ms step_avg:39.94ms
step:1078/2330 train_time:43054ms step_avg:39.94ms
step:1079/2330 train_time:43089ms step_avg:39.93ms
step:1080/2330 train_time:43133ms step_avg:39.94ms
step:1081/2330 train_time:43168ms step_avg:39.93ms
step:1082/2330 train_time:43212ms step_avg:39.94ms
step:1083/2330 train_time:43247ms step_avg:39.93ms
step:1084/2330 train_time:43292ms step_avg:39.94ms
step:1085/2330 train_time:43328ms step_avg:39.93ms
step:1086/2330 train_time:43372ms step_avg:39.94ms
step:1087/2330 train_time:43408ms step_avg:39.93ms
step:1088/2330 train_time:43454ms step_avg:39.94ms
step:1089/2330 train_time:43490ms step_avg:39.94ms
step:1090/2330 train_time:43534ms step_avg:39.94ms
step:1091/2330 train_time:43570ms step_avg:39.94ms
step:1092/2330 train_time:43614ms step_avg:39.94ms
step:1093/2330 train_time:43650ms step_avg:39.94ms
step:1094/2330 train_time:43695ms step_avg:39.94ms
step:1095/2330 train_time:43730ms step_avg:39.94ms
step:1096/2330 train_time:43774ms step_avg:39.94ms
step:1097/2330 train_time:43810ms step_avg:39.94ms
step:1098/2330 train_time:43854ms step_avg:39.94ms
step:1099/2330 train_time:43889ms step_avg:39.94ms
step:1100/2330 train_time:43933ms step_avg:39.94ms
step:1101/2330 train_time:43968ms step_avg:39.93ms
step:1102/2330 train_time:44012ms step_avg:39.94ms
step:1103/2330 train_time:44047ms step_avg:39.93ms
step:1104/2330 train_time:44091ms step_avg:39.94ms
step:1105/2330 train_time:44125ms step_avg:39.93ms
step:1106/2330 train_time:44169ms step_avg:39.94ms
step:1107/2330 train_time:44205ms step_avg:39.93ms
step:1108/2330 train_time:44249ms step_avg:39.94ms
step:1109/2330 train_time:44284ms step_avg:39.93ms
step:1110/2330 train_time:44328ms step_avg:39.94ms
step:1111/2330 train_time:44363ms step_avg:39.93ms
step:1112/2330 train_time:44408ms step_avg:39.94ms
step:1113/2330 train_time:44443ms step_avg:39.93ms
step:1114/2330 train_time:44487ms step_avg:39.93ms
step:1115/2330 train_time:44522ms step_avg:39.93ms
step:1116/2330 train_time:44566ms step_avg:39.93ms
step:1117/2330 train_time:44602ms step_avg:39.93ms
step:1118/2330 train_time:44646ms step_avg:39.93ms
step:1119/2330 train_time:44681ms step_avg:39.93ms
step:1120/2330 train_time:44726ms step_avg:39.93ms
step:1121/2330 train_time:44761ms step_avg:39.93ms
step:1122/2330 train_time:44806ms step_avg:39.93ms
step:1123/2330 train_time:44841ms step_avg:39.93ms
step:1124/2330 train_time:44885ms step_avg:39.93ms
step:1125/2330 train_time:44920ms step_avg:39.93ms
step:1126/2330 train_time:44965ms step_avg:39.93ms
step:1127/2330 train_time:44999ms step_avg:39.93ms
step:1128/2330 train_time:45043ms step_avg:39.93ms
step:1129/2330 train_time:45079ms step_avg:39.93ms
step:1130/2330 train_time:45123ms step_avg:39.93ms
step:1131/2330 train_time:45158ms step_avg:39.93ms
step:1132/2330 train_time:45203ms step_avg:39.93ms
step:1133/2330 train_time:45239ms step_avg:39.93ms
step:1134/2330 train_time:45283ms step_avg:39.93ms
step:1135/2330 train_time:45318ms step_avg:39.93ms
step:1136/2330 train_time:45363ms step_avg:39.93ms
step:1137/2330 train_time:45398ms step_avg:39.93ms
step:1138/2330 train_time:45442ms step_avg:39.93ms
step:1139/2330 train_time:45477ms step_avg:39.93ms
step:1140/2330 train_time:45522ms step_avg:39.93ms
step:1141/2330 train_time:45557ms step_avg:39.93ms
step:1142/2330 train_time:45602ms step_avg:39.93ms
step:1143/2330 train_time:45636ms step_avg:39.93ms
step:1144/2330 train_time:45681ms step_avg:39.93ms
step:1145/2330 train_time:45715ms step_avg:39.93ms
step:1146/2330 train_time:45759ms step_avg:39.93ms
step:1147/2330 train_time:45795ms step_avg:39.93ms
step:1148/2330 train_time:45839ms step_avg:39.93ms
step:1149/2330 train_time:45874ms step_avg:39.93ms
step:1150/2330 train_time:45918ms step_avg:39.93ms
step:1151/2330 train_time:45954ms step_avg:39.92ms
step:1152/2330 train_time:45998ms step_avg:39.93ms
step:1153/2330 train_time:46033ms step_avg:39.92ms
step:1154/2330 train_time:46077ms step_avg:39.93ms
step:1155/2330 train_time:46111ms step_avg:39.92ms
step:1156/2330 train_time:46155ms step_avg:39.93ms
step:1157/2330 train_time:46190ms step_avg:39.92ms
step:1158/2330 train_time:46234ms step_avg:39.93ms
step:1159/2330 train_time:46269ms step_avg:39.92ms
step:1160/2330 train_time:46314ms step_avg:39.93ms
step:1161/2330 train_time:46349ms step_avg:39.92ms
step:1162/2330 train_time:46393ms step_avg:39.93ms
step:1163/2330 train_time:46429ms step_avg:39.92ms
step:1164/2330 train_time:46473ms step_avg:39.93ms
step:1165/2330 train_time:46508ms step_avg:39.92ms
step:1166/2330 train_time:46553ms step_avg:39.93ms
step:1167/2330 train_time:46588ms step_avg:39.92ms
step:1168/2330 train_time:46633ms step_avg:39.93ms
step:1169/2330 train_time:46669ms step_avg:39.92ms
step:1170/2330 train_time:46713ms step_avg:39.93ms
step:1171/2330 train_time:46749ms step_avg:39.92ms
step:1172/2330 train_time:46793ms step_avg:39.93ms
step:1173/2330 train_time:46829ms step_avg:39.92ms
step:1174/2330 train_time:46873ms step_avg:39.93ms
step:1175/2330 train_time:46908ms step_avg:39.92ms
step:1176/2330 train_time:46953ms step_avg:39.93ms
step:1177/2330 train_time:46989ms step_avg:39.92ms
step:1178/2330 train_time:47034ms step_avg:39.93ms
step:1179/2330 train_time:47069ms step_avg:39.92ms
step:1180/2330 train_time:47113ms step_avg:39.93ms
step:1181/2330 train_time:47149ms step_avg:39.92ms
step:1182/2330 train_time:47194ms step_avg:39.93ms
step:1183/2330 train_time:47229ms step_avg:39.92ms
step:1184/2330 train_time:47273ms step_avg:39.93ms
step:1185/2330 train_time:47308ms step_avg:39.92ms
step:1186/2330 train_time:47353ms step_avg:39.93ms
step:1187/2330 train_time:47389ms step_avg:39.92ms
step:1188/2330 train_time:47434ms step_avg:39.93ms
step:1189/2330 train_time:47469ms step_avg:39.92ms
step:1190/2330 train_time:47513ms step_avg:39.93ms
step:1191/2330 train_time:47549ms step_avg:39.92ms
step:1192/2330 train_time:47593ms step_avg:39.93ms
step:1193/2330 train_time:47629ms step_avg:39.92ms
step:1194/2330 train_time:47674ms step_avg:39.93ms
step:1195/2330 train_time:47709ms step_avg:39.92ms
step:1196/2330 train_time:47754ms step_avg:39.93ms
step:1197/2330 train_time:47789ms step_avg:39.92ms
step:1198/2330 train_time:47834ms step_avg:39.93ms
step:1199/2330 train_time:47869ms step_avg:39.92ms
step:1200/2330 train_time:47913ms step_avg:39.93ms
step:1201/2330 train_time:47949ms step_avg:39.92ms
step:1202/2330 train_time:47994ms step_avg:39.93ms
step:1203/2330 train_time:48029ms step_avg:39.92ms
step:1204/2330 train_time:48073ms step_avg:39.93ms
step:1205/2330 train_time:48109ms step_avg:39.92ms
step:1206/2330 train_time:48154ms step_avg:39.93ms
step:1207/2330 train_time:48189ms step_avg:39.92ms
step:1208/2330 train_time:48234ms step_avg:39.93ms
step:1209/2330 train_time:48269ms step_avg:39.92ms
step:1210/2330 train_time:48313ms step_avg:39.93ms
step:1211/2330 train_time:48348ms step_avg:39.92ms
step:1212/2330 train_time:48393ms step_avg:39.93ms
step:1213/2330 train_time:48429ms step_avg:39.92ms
step:1214/2330 train_time:48473ms step_avg:39.93ms
step:1215/2330 train_time:48509ms step_avg:39.92ms
step:1216/2330 train_time:48552ms step_avg:39.93ms
step:1217/2330 train_time:48587ms step_avg:39.92ms
step:1218/2330 train_time:48632ms step_avg:39.93ms
step:1219/2330 train_time:48668ms step_avg:39.92ms
step:1220/2330 train_time:48713ms step_avg:39.93ms
step:1221/2330 train_time:48749ms step_avg:39.93ms
step:1222/2330 train_time:48794ms step_avg:39.93ms
step:1223/2330 train_time:48829ms step_avg:39.93ms
step:1224/2330 train_time:48873ms step_avg:39.93ms
step:1225/2330 train_time:48909ms step_avg:39.93ms
step:1226/2330 train_time:48954ms step_avg:39.93ms
step:1227/2330 train_time:48989ms step_avg:39.93ms
step:1228/2330 train_time:49033ms step_avg:39.93ms
step:1229/2330 train_time:49069ms step_avg:39.93ms
step:1230/2330 train_time:49113ms step_avg:39.93ms
step:1231/2330 train_time:49149ms step_avg:39.93ms
step:1232/2330 train_time:49193ms step_avg:39.93ms
step:1233/2330 train_time:49228ms step_avg:39.93ms
step:1234/2330 train_time:49272ms step_avg:39.93ms
step:1235/2330 train_time:49308ms step_avg:39.93ms
step:1236/2330 train_time:49353ms step_avg:39.93ms
step:1237/2330 train_time:49389ms step_avg:39.93ms
step:1238/2330 train_time:49433ms step_avg:39.93ms
step:1239/2330 train_time:49469ms step_avg:39.93ms
step:1240/2330 train_time:49513ms step_avg:39.93ms
step:1241/2330 train_time:49549ms step_avg:39.93ms
step:1242/2330 train_time:49593ms step_avg:39.93ms
step:1243/2330 train_time:49628ms step_avg:39.93ms
step:1244/2330 train_time:49673ms step_avg:39.93ms
step:1245/2330 train_time:49709ms step_avg:39.93ms
step:1246/2330 train_time:49753ms step_avg:39.93ms
step:1247/2330 train_time:49789ms step_avg:39.93ms
step:1248/2330 train_time:49834ms step_avg:39.93ms
step:1249/2330 train_time:49869ms step_avg:39.93ms
step:1250/2330 train_time:49913ms step_avg:39.93ms
step:1250/2330 val_loss:5.1775 train_time:50002ms step_avg:40.00ms
step:1251/2330 train_time:50015ms step_avg:39.98ms
step:1252/2330 train_time:50026ms step_avg:39.96ms
step:1253/2330 train_time:50037ms step_avg:39.93ms
step:1254/2330 train_time:50075ms step_avg:39.93ms
step:1255/2330 train_time:50109ms step_avg:39.93ms
step:1256/2330 train_time:50153ms step_avg:39.93ms
step:1257/2330 train_time:50187ms step_avg:39.93ms
step:1258/2330 train_time:50231ms step_avg:39.93ms
step:1259/2330 train_time:50265ms step_avg:39.92ms
step:1260/2330 train_time:50311ms step_avg:39.93ms
step:1261/2330 train_time:50351ms step_avg:39.93ms
step:1262/2330 train_time:50398ms step_avg:39.93ms
step:1263/2330 train_time:50434ms step_avg:39.93ms
step:1264/2330 train_time:50479ms step_avg:39.94ms
step:1265/2330 train_time:50700ms step_avg:40.08ms
step:1266/2330 train_time:50713ms step_avg:40.06ms
step:1267/2330 train_time:50724ms step_avg:40.03ms
step:1268/2330 train_time:50759ms step_avg:40.03ms
step:1269/2330 train_time:50793ms step_avg:40.03ms
step:1270/2330 train_time:50836ms step_avg:40.03ms
step:1271/2330 train_time:50870ms step_avg:40.02ms
step:1272/2330 train_time:51105ms step_avg:40.18ms
step:1273/2330 train_time:51139ms step_avg:40.17ms
step:1274/2330 train_time:51182ms step_avg:40.17ms
step:1275/2330 train_time:51216ms step_avg:40.17ms
step:1276/2330 train_time:51369ms step_avg:40.26ms
step:1277/2330 train_time:51402ms step_avg:40.25ms
step:1278/2330 train_time:51445ms step_avg:40.25ms
step:1279/2330 train_time:51480ms step_avg:40.25ms
step:1280/2330 train_time:51523ms step_avg:40.25ms
step:1281/2330 train_time:51557ms step_avg:40.25ms
step:1282/2330 train_time:51601ms step_avg:40.25ms
step:1283/2330 train_time:51635ms step_avg:40.25ms
step:1284/2330 train_time:51678ms step_avg:40.25ms
step:1285/2330 train_time:51713ms step_avg:40.24ms
step:1286/2330 train_time:51756ms step_avg:40.25ms
step:1287/2330 train_time:51790ms step_avg:40.24ms
step:1288/2330 train_time:51834ms step_avg:40.24ms
step:1289/2330 train_time:51868ms step_avg:40.24ms
step:1290/2330 train_time:51911ms step_avg:40.24ms
step:1291/2330 train_time:51946ms step_avg:40.24ms
step:1292/2330 train_time:51989ms step_avg:40.24ms
step:1293/2330 train_time:52024ms step_avg:40.23ms
step:1294/2330 train_time:52067ms step_avg:40.24ms
step:1295/2330 train_time:52101ms step_avg:40.23ms
step:1296/2330 train_time:52145ms step_avg:40.24ms
step:1297/2330 train_time:52179ms step_avg:40.23ms
step:1298/2330 train_time:52226ms step_avg:40.24ms
step:1299/2330 train_time:52266ms step_avg:40.24ms
step:1300/2330 train_time:52312ms step_avg:40.24ms
step:1301/2330 train_time:52351ms step_avg:40.24ms
step:1302/2330 train_time:52397ms step_avg:40.24ms
step:1303/2330 train_time:52433ms step_avg:40.24ms
step:1304/2330 train_time:52478ms step_avg:40.24ms
step:1305/2330 train_time:52514ms step_avg:40.24ms
step:1306/2330 train_time:52559ms step_avg:40.24ms
step:1307/2330 train_time:52593ms step_avg:40.24ms
step:1308/2330 train_time:52637ms step_avg:40.24ms
step:1309/2330 train_time:52671ms step_avg:40.24ms
step:1310/2330 train_time:52715ms step_avg:40.24ms
step:1311/2330 train_time:52749ms step_avg:40.24ms
step:1312/2330 train_time:52792ms step_avg:40.24ms
step:1313/2330 train_time:52826ms step_avg:40.23ms
step:1314/2330 train_time:52870ms step_avg:40.24ms
step:1315/2330 train_time:52904ms step_avg:40.23ms
step:1316/2330 train_time:52947ms step_avg:40.23ms
step:1317/2330 train_time:52982ms step_avg:40.23ms
step:1318/2330 train_time:53025ms step_avg:40.23ms
step:1319/2330 train_time:53059ms step_avg:40.23ms
step:1320/2330 train_time:53103ms step_avg:40.23ms
step:1321/2330 train_time:53138ms step_avg:40.23ms
step:1322/2330 train_time:53184ms step_avg:40.23ms
step:1323/2330 train_time:53219ms step_avg:40.23ms
step:1324/2330 train_time:53265ms step_avg:40.23ms
step:1325/2330 train_time:53300ms step_avg:40.23ms
step:1326/2330 train_time:53345ms step_avg:40.23ms
step:1327/2330 train_time:53380ms step_avg:40.23ms
step:1328/2330 train_time:53424ms step_avg:40.23ms
step:1329/2330 train_time:53460ms step_avg:40.23ms
step:1330/2330 train_time:53504ms step_avg:40.23ms
step:1331/2330 train_time:53539ms step_avg:40.22ms
step:1332/2330 train_time:53583ms step_avg:40.23ms
step:1333/2330 train_time:53618ms step_avg:40.22ms
step:1334/2330 train_time:53661ms step_avg:40.23ms
step:1335/2330 train_time:53696ms step_avg:40.22ms
step:1336/2330 train_time:53741ms step_avg:40.23ms
step:1337/2330 train_time:53776ms step_avg:40.22ms
step:1338/2330 train_time:53820ms step_avg:40.22ms
step:1339/2330 train_time:53854ms step_avg:40.22ms
step:1340/2330 train_time:53899ms step_avg:40.22ms
step:1341/2330 train_time:53933ms step_avg:40.22ms
step:1342/2330 train_time:53977ms step_avg:40.22ms
step:1343/2330 train_time:54012ms step_avg:40.22ms
step:1344/2330 train_time:54056ms step_avg:40.22ms
step:1345/2330 train_time:54091ms step_avg:40.22ms
step:1346/2330 train_time:54135ms step_avg:40.22ms
step:1347/2330 train_time:54170ms step_avg:40.22ms
step:1348/2330 train_time:54216ms step_avg:40.22ms
step:1349/2330 train_time:54252ms step_avg:40.22ms
step:1350/2330 train_time:54297ms step_avg:40.22ms
step:1351/2330 train_time:54332ms step_avg:40.22ms
step:1352/2330 train_time:54377ms step_avg:40.22ms
step:1353/2330 train_time:54412ms step_avg:40.22ms
step:1354/2330 train_time:54458ms step_avg:40.22ms
step:1355/2330 train_time:54494ms step_avg:40.22ms
step:1356/2330 train_time:54538ms step_avg:40.22ms
step:1357/2330 train_time:54572ms step_avg:40.22ms
step:1358/2330 train_time:54617ms step_avg:40.22ms
step:1359/2330 train_time:54652ms step_avg:40.21ms
step:1360/2330 train_time:54696ms step_avg:40.22ms
step:1361/2330 train_time:54731ms step_avg:40.21ms
step:1362/2330 train_time:54774ms step_avg:40.22ms
step:1363/2330 train_time:54808ms step_avg:40.21ms
step:1364/2330 train_time:54852ms step_avg:40.21ms
step:1365/2330 train_time:54887ms step_avg:40.21ms
step:1366/2330 train_time:54931ms step_avg:40.21ms
step:1367/2330 train_time:54965ms step_avg:40.21ms
step:1368/2330 train_time:55009ms step_avg:40.21ms
step:1369/2330 train_time:55043ms step_avg:40.21ms
step:1370/2330 train_time:55087ms step_avg:40.21ms
step:1371/2330 train_time:55122ms step_avg:40.21ms
step:1372/2330 train_time:55166ms step_avg:40.21ms
step:1373/2330 train_time:55202ms step_avg:40.21ms
step:1374/2330 train_time:55246ms step_avg:40.21ms
step:1375/2330 train_time:55282ms step_avg:40.20ms
step:1376/2330 train_time:55327ms step_avg:40.21ms
step:1377/2330 train_time:55362ms step_avg:40.21ms
step:1378/2330 train_time:55408ms step_avg:40.21ms
step:1379/2330 train_time:55443ms step_avg:40.21ms
step:1380/2330 train_time:55488ms step_avg:40.21ms
step:1381/2330 train_time:55524ms step_avg:40.21ms
step:1382/2330 train_time:55568ms step_avg:40.21ms
step:1383/2330 train_time:55603ms step_avg:40.20ms
step:1384/2330 train_time:55648ms step_avg:40.21ms
step:1385/2330 train_time:55683ms step_avg:40.20ms
step:1386/2330 train_time:55728ms step_avg:40.21ms
step:1387/2330 train_time:55762ms step_avg:40.20ms
step:1388/2330 train_time:55807ms step_avg:40.21ms
step:1389/2330 train_time:55842ms step_avg:40.20ms
step:1390/2330 train_time:55886ms step_avg:40.21ms
step:1391/2330 train_time:55920ms step_avg:40.20ms
step:1392/2330 train_time:55964ms step_avg:40.20ms
step:1393/2330 train_time:55999ms step_avg:40.20ms
step:1394/2330 train_time:56043ms step_avg:40.20ms
step:1395/2330 train_time:56077ms step_avg:40.20ms
step:1396/2330 train_time:56121ms step_avg:40.20ms
step:1397/2330 train_time:56156ms step_avg:40.20ms
step:1398/2330 train_time:56201ms step_avg:40.20ms
step:1399/2330 train_time:56237ms step_avg:40.20ms
step:1400/2330 train_time:56281ms step_avg:40.20ms
step:1401/2330 train_time:56317ms step_avg:40.20ms
step:1402/2330 train_time:56362ms step_avg:40.20ms
step:1403/2330 train_time:56397ms step_avg:40.20ms
step:1404/2330 train_time:56442ms step_avg:40.20ms
step:1405/2330 train_time:56476ms step_avg:40.20ms
step:1406/2330 train_time:56521ms step_avg:40.20ms
step:1407/2330 train_time:56557ms step_avg:40.20ms
step:1408/2330 train_time:56601ms step_avg:40.20ms
step:1409/2330 train_time:56636ms step_avg:40.20ms
step:1410/2330 train_time:56681ms step_avg:40.20ms
step:1411/2330 train_time:56716ms step_avg:40.20ms
step:1412/2330 train_time:56760ms step_avg:40.20ms
step:1413/2330 train_time:56795ms step_avg:40.19ms
step:1414/2330 train_time:56839ms step_avg:40.20ms
step:1415/2330 train_time:56874ms step_avg:40.19ms
step:1416/2330 train_time:56918ms step_avg:40.20ms
step:1417/2330 train_time:56953ms step_avg:40.19ms
step:1418/2330 train_time:56997ms step_avg:40.20ms
step:1419/2330 train_time:57032ms step_avg:40.19ms
step:1420/2330 train_time:57076ms step_avg:40.19ms
step:1421/2330 train_time:57110ms step_avg:40.19ms
step:1422/2330 train_time:57154ms step_avg:40.19ms
step:1423/2330 train_time:57189ms step_avg:40.19ms
step:1424/2330 train_time:57234ms step_avg:40.19ms
step:1425/2330 train_time:57269ms step_avg:40.19ms
step:1426/2330 train_time:57314ms step_avg:40.19ms
step:1427/2330 train_time:57350ms step_avg:40.19ms
step:1428/2330 train_time:57394ms step_avg:40.19ms
step:1429/2330 train_time:57430ms step_avg:40.19ms
step:1430/2330 train_time:57475ms step_avg:40.19ms
step:1431/2330 train_time:57510ms step_avg:40.19ms
step:1432/2330 train_time:57554ms step_avg:40.19ms
step:1433/2330 train_time:57591ms step_avg:40.19ms
step:1434/2330 train_time:57635ms step_avg:40.19ms
step:1435/2330 train_time:57669ms step_avg:40.19ms
step:1436/2330 train_time:57713ms step_avg:40.19ms
step:1437/2330 train_time:57748ms step_avg:40.19ms
step:1438/2330 train_time:57791ms step_avg:40.19ms
step:1439/2330 train_time:57826ms step_avg:40.19ms
step:1440/2330 train_time:57871ms step_avg:40.19ms
step:1441/2330 train_time:57906ms step_avg:40.18ms
step:1442/2330 train_time:57951ms step_avg:40.19ms
step:1443/2330 train_time:57986ms step_avg:40.18ms
step:1444/2330 train_time:58031ms step_avg:40.19ms
step:1445/2330 train_time:58066ms step_avg:40.18ms
step:1446/2330 train_time:58110ms step_avg:40.19ms
step:1447/2330 train_time:58145ms step_avg:40.18ms
step:1448/2330 train_time:58189ms step_avg:40.19ms
step:1449/2330 train_time:58224ms step_avg:40.18ms
step:1450/2330 train_time:58269ms step_avg:40.19ms
step:1451/2330 train_time:58304ms step_avg:40.18ms
step:1452/2330 train_time:58348ms step_avg:40.18ms
step:1453/2330 train_time:58383ms step_avg:40.18ms
step:1454/2330 train_time:58428ms step_avg:40.18ms
step:1455/2330 train_time:58463ms step_avg:40.18ms
step:1456/2330 train_time:58508ms step_avg:40.18ms
step:1457/2330 train_time:58544ms step_avg:40.18ms
step:1458/2330 train_time:58589ms step_avg:40.18ms
step:1459/2330 train_time:58625ms step_avg:40.18ms
step:1460/2330 train_time:58669ms step_avg:40.18ms
step:1461/2330 train_time:58704ms step_avg:40.18ms
step:1462/2330 train_time:58748ms step_avg:40.18ms
step:1463/2330 train_time:58783ms step_avg:40.18ms
step:1464/2330 train_time:58828ms step_avg:40.18ms
step:1465/2330 train_time:58863ms step_avg:40.18ms
step:1466/2330 train_time:58907ms step_avg:40.18ms
step:1467/2330 train_time:58942ms step_avg:40.18ms
step:1468/2330 train_time:58986ms step_avg:40.18ms
step:1469/2330 train_time:59021ms step_avg:40.18ms
step:1470/2330 train_time:59064ms step_avg:40.18ms
step:1471/2330 train_time:59099ms step_avg:40.18ms
step:1472/2330 train_time:59143ms step_avg:40.18ms
step:1473/2330 train_time:59177ms step_avg:40.17ms
step:1474/2330 train_time:59222ms step_avg:40.18ms
step:1475/2330 train_time:59257ms step_avg:40.17ms
step:1476/2330 train_time:59302ms step_avg:40.18ms
step:1477/2330 train_time:59337ms step_avg:40.17ms
step:1478/2330 train_time:59381ms step_avg:40.18ms
step:1479/2330 train_time:59416ms step_avg:40.17ms
step:1480/2330 train_time:59461ms step_avg:40.18ms
step:1481/2330 train_time:59496ms step_avg:40.17ms
step:1482/2330 train_time:59541ms step_avg:40.18ms
step:1483/2330 train_time:59576ms step_avg:40.17ms
step:1484/2330 train_time:59620ms step_avg:40.18ms
step:1485/2330 train_time:59655ms step_avg:40.17ms
step:1486/2330 train_time:59699ms step_avg:40.17ms
step:1487/2330 train_time:59734ms step_avg:40.17ms
step:1488/2330 train_time:59779ms step_avg:40.17ms
step:1489/2330 train_time:59815ms step_avg:40.17ms
step:1490/2330 train_time:59860ms step_avg:40.17ms
step:1491/2330 train_time:59894ms step_avg:40.17ms
step:1492/2330 train_time:59939ms step_avg:40.17ms
step:1493/2330 train_time:59973ms step_avg:40.17ms
step:1494/2330 train_time:60018ms step_avg:40.17ms
step:1495/2330 train_time:60053ms step_avg:40.17ms
step:1496/2330 train_time:60097ms step_avg:40.17ms
step:1497/2330 train_time:60131ms step_avg:40.17ms
step:1498/2330 train_time:60176ms step_avg:40.17ms
step:1499/2330 train_time:60211ms step_avg:40.17ms
step:1500/2330 train_time:60255ms step_avg:40.17ms
step:1500/2330 val_loss:5.1406 train_time:60342ms step_avg:40.23ms
step:1501/2330 train_time:60355ms step_avg:40.21ms
step:1502/2330 train_time:60368ms step_avg:40.19ms
step:1503/2330 train_time:60379ms step_avg:40.17ms
step:1504/2330 train_time:60415ms step_avg:40.17ms
step:1505/2330 train_time:60449ms step_avg:40.17ms
step:1506/2330 train_time:60492ms step_avg:40.17ms
step:1507/2330 train_time:60526ms step_avg:40.16ms
step:1508/2330 train_time:60569ms step_avg:40.17ms
step:1509/2330 train_time:60604ms step_avg:40.16ms
step:1510/2330 train_time:60648ms step_avg:40.16ms
step:1511/2330 train_time:60686ms step_avg:40.16ms
step:1512/2330 train_time:60733ms step_avg:40.17ms
step:1513/2330 train_time:60770ms step_avg:40.17ms
step:1514/2330 train_time:60815ms step_avg:40.17ms
step:1515/2330 train_time:60851ms step_avg:40.17ms
step:1516/2330 train_time:60994ms step_avg:40.23ms
step:1517/2330 train_time:61027ms step_avg:40.23ms
step:1518/2330 train_time:61070ms step_avg:40.23ms
step:1519/2330 train_time:61104ms step_avg:40.23ms
step:1520/2330 train_time:61147ms step_avg:40.23ms
step:1521/2330 train_time:61182ms step_avg:40.22ms
step:1522/2330 train_time:61225ms step_avg:40.23ms
step:1523/2330 train_time:61259ms step_avg:40.22ms
step:1524/2330 train_time:61303ms step_avg:40.22ms
step:1525/2330 train_time:61337ms step_avg:40.22ms
step:1526/2330 train_time:61380ms step_avg:40.22ms
step:1527/2330 train_time:61414ms step_avg:40.22ms
step:1528/2330 train_time:61564ms step_avg:40.29ms
step:1529/2330 train_time:61599ms step_avg:40.29ms
step:1530/2330 train_time:61642ms step_avg:40.29ms
step:1531/2330 train_time:61676ms step_avg:40.28ms
step:1532/2330 train_time:61719ms step_avg:40.29ms
step:1533/2330 train_time:61754ms step_avg:40.28ms
step:1534/2330 train_time:61797ms step_avg:40.28ms
step:1535/2330 train_time:61832ms step_avg:40.28ms
step:1536/2330 train_time:61875ms step_avg:40.28ms
step:1537/2330 train_time:61910ms step_avg:40.28ms
step:1538/2330 train_time:61953ms step_avg:40.28ms
step:1539/2330 train_time:61988ms step_avg:40.28ms
step:1540/2330 train_time:62031ms step_avg:40.28ms
step:1541/2330 train_time:62065ms step_avg:40.28ms
step:1542/2330 train_time:62108ms step_avg:40.28ms
step:1543/2330 train_time:62143ms step_avg:40.27ms
step:1544/2330 train_time:62186ms step_avg:40.28ms
step:1545/2330 train_time:62220ms step_avg:40.27ms
step:1546/2330 train_time:62263ms step_avg:40.27ms
step:1547/2330 train_time:62297ms step_avg:40.27ms
step:1548/2330 train_time:62341ms step_avg:40.27ms
step:1549/2330 train_time:62378ms step_avg:40.27ms
step:1550/2330 train_time:62432ms step_avg:40.28ms
step:1551/2330 train_time:62469ms step_avg:40.28ms
step:1552/2330 train_time:62515ms step_avg:40.28ms
step:1553/2330 train_time:62551ms step_avg:40.28ms
step:1554/2330 train_time:62596ms step_avg:40.28ms
step:1555/2330 train_time:62631ms step_avg:40.28ms
step:1556/2330 train_time:62675ms step_avg:40.28ms
step:1557/2330 train_time:62710ms step_avg:40.28ms
step:1558/2330 train_time:62754ms step_avg:40.28ms
step:1559/2330 train_time:62789ms step_avg:40.27ms
step:1560/2330 train_time:62833ms step_avg:40.28ms
step:1561/2330 train_time:62867ms step_avg:40.27ms
step:1562/2330 train_time:62911ms step_avg:40.28ms
step:1563/2330 train_time:62946ms step_avg:40.27ms
step:1564/2330 train_time:62990ms step_avg:40.27ms
step:1565/2330 train_time:63024ms step_avg:40.27ms
step:1566/2330 train_time:63068ms step_avg:40.27ms
step:1567/2330 train_time:63102ms step_avg:40.27ms
step:1568/2330 train_time:63145ms step_avg:40.27ms
step:1569/2330 train_time:63180ms step_avg:40.27ms
step:1570/2330 train_time:63223ms step_avg:40.27ms
step:1571/2330 train_time:63258ms step_avg:40.27ms
step:1572/2330 train_time:63301ms step_avg:40.27ms
step:1573/2330 train_time:63338ms step_avg:40.27ms
step:1574/2330 train_time:63386ms step_avg:40.27ms
step:1575/2330 train_time:63424ms step_avg:40.27ms
step:1576/2330 train_time:63469ms step_avg:40.27ms
step:1577/2330 train_time:63505ms step_avg:40.27ms
step:1578/2330 train_time:63551ms step_avg:40.27ms
step:1579/2330 train_time:63586ms step_avg:40.27ms
step:1580/2330 train_time:63631ms step_avg:40.27ms
step:1581/2330 train_time:63666ms step_avg:40.27ms
step:1582/2330 train_time:63711ms step_avg:40.27ms
step:1583/2330 train_time:63746ms step_avg:40.27ms
step:1584/2330 train_time:63791ms step_avg:40.27ms
step:1585/2330 train_time:63826ms step_avg:40.27ms
step:1586/2330 train_time:63870ms step_avg:40.27ms
step:1587/2330 train_time:63904ms step_avg:40.27ms
step:1588/2330 train_time:63948ms step_avg:40.27ms
step:1589/2330 train_time:63982ms step_avg:40.27ms
step:1590/2330 train_time:64026ms step_avg:40.27ms
step:1591/2330 train_time:64060ms step_avg:40.26ms
step:1592/2330 train_time:64104ms step_avg:40.27ms
step:1593/2330 train_time:64138ms step_avg:40.26ms
step:1594/2330 train_time:64181ms step_avg:40.26ms
step:1595/2330 train_time:64216ms step_avg:40.26ms
step:1596/2330 train_time:64261ms step_avg:40.26ms
step:1597/2330 train_time:64296ms step_avg:40.26ms
step:1598/2330 train_time:64341ms step_avg:40.26ms
step:1599/2330 train_time:64376ms step_avg:40.26ms
step:1600/2330 train_time:64424ms step_avg:40.26ms
step:1601/2330 train_time:64459ms step_avg:40.26ms
step:1602/2330 train_time:64504ms step_avg:40.26ms
step:1603/2330 train_time:64540ms step_avg:40.26ms
step:1604/2330 train_time:64586ms step_avg:40.27ms
step:1605/2330 train_time:64622ms step_avg:40.26ms
step:1606/2330 train_time:64667ms step_avg:40.27ms
step:1607/2330 train_time:64702ms step_avg:40.26ms
step:1608/2330 train_time:64746ms step_avg:40.27ms
step:1609/2330 train_time:64782ms step_avg:40.26ms
step:1610/2330 train_time:64827ms step_avg:40.27ms
step:1611/2330 train_time:64862ms step_avg:40.26ms
step:1612/2330 train_time:64906ms step_avg:40.26ms
step:1613/2330 train_time:64941ms step_avg:40.26ms
step:1614/2330 train_time:64984ms step_avg:40.26ms
step:1615/2330 train_time:65019ms step_avg:40.26ms
step:1616/2330 train_time:65063ms step_avg:40.26ms
step:1617/2330 train_time:65097ms step_avg:40.26ms
step:1618/2330 train_time:65141ms step_avg:40.26ms
step:1619/2330 train_time:65175ms step_avg:40.26ms
step:1620/2330 train_time:65219ms step_avg:40.26ms
step:1621/2330 train_time:65254ms step_avg:40.26ms
step:1622/2330 train_time:65298ms step_avg:40.26ms
step:1623/2330 train_time:65333ms step_avg:40.25ms
step:1624/2330 train_time:65378ms step_avg:40.26ms
step:1625/2330 train_time:65414ms step_avg:40.25ms
step:1626/2330 train_time:65459ms step_avg:40.26ms
step:1627/2330 train_time:65496ms step_avg:40.26ms
step:1628/2330 train_time:65542ms step_avg:40.26ms
step:1629/2330 train_time:65578ms step_avg:40.26ms
step:1630/2330 train_time:65623ms step_avg:40.26ms
step:1631/2330 train_time:65658ms step_avg:40.26ms
step:1632/2330 train_time:65703ms step_avg:40.26ms
step:1633/2330 train_time:65738ms step_avg:40.26ms
step:1634/2330 train_time:65783ms step_avg:40.26ms
step:1635/2330 train_time:65819ms step_avg:40.26ms
step:1636/2330 train_time:65864ms step_avg:40.26ms
step:1637/2330 train_time:65899ms step_avg:40.26ms
step:1638/2330 train_time:65943ms step_avg:40.26ms
step:1639/2330 train_time:65978ms step_avg:40.26ms
step:1640/2330 train_time:66022ms step_avg:40.26ms
step:1641/2330 train_time:66056ms step_avg:40.25ms
step:1642/2330 train_time:66100ms step_avg:40.26ms
step:1643/2330 train_time:66134ms step_avg:40.25ms
step:1644/2330 train_time:66178ms step_avg:40.25ms
step:1645/2330 train_time:66213ms step_avg:40.25ms
step:1646/2330 train_time:66257ms step_avg:40.25ms
step:1647/2330 train_time:66292ms step_avg:40.25ms
step:1648/2330 train_time:66336ms step_avg:40.25ms
step:1649/2330 train_time:66372ms step_avg:40.25ms
step:1650/2330 train_time:66417ms step_avg:40.25ms
step:1651/2330 train_time:66453ms step_avg:40.25ms
step:1652/2330 train_time:66498ms step_avg:40.25ms
step:1653/2330 train_time:66535ms step_avg:40.25ms
step:1654/2330 train_time:66580ms step_avg:40.25ms
step:1655/2330 train_time:66617ms step_avg:40.25ms
step:1656/2330 train_time:66661ms step_avg:40.25ms
step:1657/2330 train_time:66697ms step_avg:40.25ms
step:1658/2330 train_time:66742ms step_avg:40.25ms
step:1659/2330 train_time:66778ms step_avg:40.25ms
step:1660/2330 train_time:66823ms step_avg:40.25ms
step:1661/2330 train_time:66857ms step_avg:40.25ms
step:1662/2330 train_time:66902ms step_avg:40.25ms
step:1663/2330 train_time:66937ms step_avg:40.25ms
step:1664/2330 train_time:66981ms step_avg:40.25ms
step:1665/2330 train_time:67016ms step_avg:40.25ms
step:1666/2330 train_time:67060ms step_avg:40.25ms
step:1667/2330 train_time:67093ms step_avg:40.25ms
step:1668/2330 train_time:67136ms step_avg:40.25ms
step:1669/2330 train_time:67171ms step_avg:40.25ms
step:1670/2330 train_time:67215ms step_avg:40.25ms
step:1671/2330 train_time:67250ms step_avg:40.25ms
step:1672/2330 train_time:67295ms step_avg:40.25ms
step:1673/2330 train_time:67330ms step_avg:40.24ms
step:1674/2330 train_time:67375ms step_avg:40.25ms
step:1675/2330 train_time:67410ms step_avg:40.24ms
step:1676/2330 train_time:67454ms step_avg:40.25ms
step:1677/2330 train_time:67489ms step_avg:40.24ms
step:1678/2330 train_time:67534ms step_avg:40.25ms
step:1679/2330 train_time:67569ms step_avg:40.24ms
step:1680/2330 train_time:67614ms step_avg:40.25ms
step:1681/2330 train_time:67649ms step_avg:40.24ms
step:1682/2330 train_time:67694ms step_avg:40.25ms
step:1683/2330 train_time:67730ms step_avg:40.24ms
step:1684/2330 train_time:67775ms step_avg:40.25ms
step:1685/2330 train_time:67810ms step_avg:40.24ms
step:1686/2330 train_time:67855ms step_avg:40.25ms
step:1687/2330 train_time:67890ms step_avg:40.24ms
step:1688/2330 train_time:67935ms step_avg:40.25ms
step:1689/2330 train_time:67969ms step_avg:40.24ms
step:1690/2330 train_time:68013ms step_avg:40.24ms
step:1691/2330 train_time:68048ms step_avg:40.24ms
step:1692/2330 train_time:68092ms step_avg:40.24ms
step:1693/2330 train_time:68127ms step_avg:40.24ms
step:1694/2330 train_time:68170ms step_avg:40.24ms
step:1695/2330 train_time:68205ms step_avg:40.24ms
step:1696/2330 train_time:68249ms step_avg:40.24ms
step:1697/2330 train_time:68283ms step_avg:40.24ms
step:1698/2330 train_time:68328ms step_avg:40.24ms
step:1699/2330 train_time:68363ms step_avg:40.24ms
step:1700/2330 train_time:68408ms step_avg:40.24ms
step:1701/2330 train_time:68443ms step_avg:40.24ms
step:1702/2330 train_time:68487ms step_avg:40.24ms
step:1703/2330 train_time:68523ms step_avg:40.24ms
step:1704/2330 train_time:68568ms step_avg:40.24ms
step:1705/2330 train_time:68603ms step_avg:40.24ms
step:1706/2330 train_time:68647ms step_avg:40.24ms
step:1707/2330 train_time:68683ms step_avg:40.24ms
step:1708/2330 train_time:68728ms step_avg:40.24ms
step:1709/2330 train_time:68763ms step_avg:40.24ms
step:1710/2330 train_time:68808ms step_avg:40.24ms
step:1711/2330 train_time:68843ms step_avg:40.24ms
step:1712/2330 train_time:68887ms step_avg:40.24ms
step:1713/2330 train_time:68922ms step_avg:40.23ms
step:1714/2330 train_time:68966ms step_avg:40.24ms
step:1715/2330 train_time:69000ms step_avg:40.23ms
step:1716/2330 train_time:69044ms step_avg:40.24ms
step:1717/2330 train_time:69079ms step_avg:40.23ms
step:1718/2330 train_time:69124ms step_avg:40.23ms
step:1719/2330 train_time:69158ms step_avg:40.23ms
step:1720/2330 train_time:69202ms step_avg:40.23ms
step:1721/2330 train_time:69237ms step_avg:40.23ms
step:1722/2330 train_time:69282ms step_avg:40.23ms
step:1723/2330 train_time:69317ms step_avg:40.23ms
step:1724/2330 train_time:69362ms step_avg:40.23ms
step:1725/2330 train_time:69396ms step_avg:40.23ms
step:1726/2330 train_time:69440ms step_avg:40.23ms
step:1727/2330 train_time:69476ms step_avg:40.23ms
step:1728/2330 train_time:69521ms step_avg:40.23ms
step:1729/2330 train_time:69556ms step_avg:40.23ms
step:1730/2330 train_time:69600ms step_avg:40.23ms
step:1731/2330 train_time:69635ms step_avg:40.23ms
step:1732/2330 train_time:69680ms step_avg:40.23ms
step:1733/2330 train_time:69715ms step_avg:40.23ms
step:1734/2330 train_time:69760ms step_avg:40.23ms
step:1735/2330 train_time:69795ms step_avg:40.23ms
step:1736/2330 train_time:69840ms step_avg:40.23ms
step:1737/2330 train_time:69876ms step_avg:40.23ms
step:1738/2330 train_time:69920ms step_avg:40.23ms
step:1739/2330 train_time:69954ms step_avg:40.23ms
step:1740/2330 train_time:69998ms step_avg:40.23ms
step:1741/2330 train_time:70033ms step_avg:40.23ms
step:1742/2330 train_time:70077ms step_avg:40.23ms
step:1743/2330 train_time:70113ms step_avg:40.23ms
step:1744/2330 train_time:70157ms step_avg:40.23ms
step:1745/2330 train_time:70192ms step_avg:40.22ms
step:1746/2330 train_time:70236ms step_avg:40.23ms
step:1747/2330 train_time:70272ms step_avg:40.22ms
step:1748/2330 train_time:70316ms step_avg:40.23ms
step:1749/2330 train_time:70351ms step_avg:40.22ms
step:1750/2330 train_time:70396ms step_avg:40.23ms
step:1750/2330 val_loss:5.1073 train_time:70485ms step_avg:40.28ms
step:1751/2330 train_time:70500ms step_avg:40.26ms
step:1752/2330 train_time:70513ms step_avg:40.25ms
step:1753/2330 train_time:70524ms step_avg:40.23ms
step:1754/2330 train_time:70557ms step_avg:40.23ms
step:1755/2330 train_time:70591ms step_avg:40.22ms
step:1756/2330 train_time:70634ms step_avg:40.22ms
step:1757/2330 train_time:70668ms step_avg:40.22ms
step:1758/2330 train_time:70712ms step_avg:40.22ms
step:1759/2330 train_time:70746ms step_avg:40.22ms
step:1760/2330 train_time:70789ms step_avg:40.22ms
step:1761/2330 train_time:70824ms step_avg:40.22ms
step:1762/2330 train_time:70869ms step_avg:40.22ms
step:1763/2330 train_time:70906ms step_avg:40.22ms
step:1764/2330 train_time:70951ms step_avg:40.22ms
step:1765/2330 train_time:70986ms step_avg:40.22ms
step:1766/2330 train_time:71030ms step_avg:40.22ms
step:1767/2330 train_time:71065ms step_avg:40.22ms
step:1768/2330 train_time:71108ms step_avg:40.22ms
step:1769/2330 train_time:71142ms step_avg:40.22ms
step:1770/2330 train_time:71186ms step_avg:40.22ms
step:1771/2330 train_time:71220ms step_avg:40.21ms
step:1772/2330 train_time:71263ms step_avg:40.22ms
step:1773/2330 train_time:71298ms step_avg:40.21ms
step:1774/2330 train_time:71342ms step_avg:40.22ms
step:1775/2330 train_time:71378ms step_avg:40.21ms
step:1776/2330 train_time:71425ms step_avg:40.22ms
step:1777/2330 train_time:71461ms step_avg:40.21ms
step:1778/2330 train_time:71507ms step_avg:40.22ms
step:1779/2330 train_time:71543ms step_avg:40.22ms
step:1780/2330 train_time:71588ms step_avg:40.22ms
step:1781/2330 train_time:71623ms step_avg:40.22ms
step:1782/2330 train_time:71667ms step_avg:40.22ms
step:1783/2330 train_time:71702ms step_avg:40.21ms
step:1784/2330 train_time:71745ms step_avg:40.22ms
step:1785/2330 train_time:71780ms step_avg:40.21ms
step:1786/2330 train_time:71825ms step_avg:40.22ms
step:1787/2330 train_time:71861ms step_avg:40.21ms
step:1788/2330 train_time:71906ms step_avg:40.22ms
step:1789/2330 train_time:71942ms step_avg:40.21ms
step:1790/2330 train_time:71986ms step_avg:40.22ms
step:1791/2330 train_time:72021ms step_avg:40.21ms
step:1792/2330 train_time:72065ms step_avg:40.22ms
step:1793/2330 train_time:72100ms step_avg:40.21ms
step:1794/2330 train_time:72144ms step_avg:40.21ms
step:1795/2330 train_time:72178ms step_avg:40.21ms
step:1796/2330 train_time:72221ms step_avg:40.21ms
step:1797/2330 train_time:72256ms step_avg:40.21ms
step:1798/2330 train_time:72300ms step_avg:40.21ms
step:1799/2330 train_time:72335ms step_avg:40.21ms
step:1800/2330 train_time:72380ms step_avg:40.21ms
step:1801/2330 train_time:72415ms step_avg:40.21ms
step:1802/2330 train_time:72460ms step_avg:40.21ms
step:1803/2330 train_time:72496ms step_avg:40.21ms
step:1804/2330 train_time:72541ms step_avg:40.21ms
step:1805/2330 train_time:72576ms step_avg:40.21ms
step:1806/2330 train_time:72620ms step_avg:40.21ms
step:1807/2330 train_time:72655ms step_avg:40.21ms
step:1808/2330 train_time:72700ms step_avg:40.21ms
step:1809/2330 train_time:72735ms step_avg:40.21ms
step:1810/2330 train_time:72779ms step_avg:40.21ms
step:1811/2330 train_time:72814ms step_avg:40.21ms
step:1812/2330 train_time:72859ms step_avg:40.21ms
step:1813/2330 train_time:72895ms step_avg:40.21ms
step:1814/2330 train_time:72939ms step_avg:40.21ms
step:1815/2330 train_time:72974ms step_avg:40.21ms
step:1816/2330 train_time:73019ms step_avg:40.21ms
step:1817/2330 train_time:73053ms step_avg:40.21ms
step:1818/2330 train_time:73098ms step_avg:40.21ms
step:1819/2330 train_time:73132ms step_avg:40.20ms
step:1820/2330 train_time:73176ms step_avg:40.21ms
step:1821/2330 train_time:73210ms step_avg:40.20ms
step:1822/2330 train_time:73254ms step_avg:40.21ms
step:1823/2330 train_time:73289ms step_avg:40.20ms
step:1824/2330 train_time:73333ms step_avg:40.20ms
step:1825/2330 train_time:73368ms step_avg:40.20ms
step:1826/2330 train_time:73412ms step_avg:40.20ms
step:1827/2330 train_time:73447ms step_avg:40.20ms
step:1828/2330 train_time:73492ms step_avg:40.20ms
step:1829/2330 train_time:73528ms step_avg:40.20ms
step:1830/2330 train_time:73572ms step_avg:40.20ms
step:1831/2330 train_time:73607ms step_avg:40.20ms
step:1832/2330 train_time:73653ms step_avg:40.20ms
step:1833/2330 train_time:73688ms step_avg:40.20ms
step:1834/2330 train_time:73732ms step_avg:40.20ms
step:1835/2330 train_time:73767ms step_avg:40.20ms
step:1836/2330 train_time:73812ms step_avg:40.20ms
step:1837/2330 train_time:73848ms step_avg:40.20ms
step:1838/2330 train_time:73893ms step_avg:40.20ms
step:1839/2330 train_time:73928ms step_avg:40.20ms
step:1840/2330 train_time:73973ms step_avg:40.20ms
step:1841/2330 train_time:74007ms step_avg:40.20ms
step:1842/2330 train_time:74051ms step_avg:40.20ms
step:1843/2330 train_time:74085ms step_avg:40.20ms
step:1844/2330 train_time:74129ms step_avg:40.20ms
step:1845/2330 train_time:74163ms step_avg:40.20ms
step:1846/2330 train_time:74207ms step_avg:40.20ms
step:1847/2330 train_time:74242ms step_avg:40.20ms
step:1848/2330 train_time:74286ms step_avg:40.20ms
step:1849/2330 train_time:74321ms step_avg:40.20ms
step:1850/2330 train_time:74365ms step_avg:40.20ms
step:1851/2330 train_time:74400ms step_avg:40.19ms
step:1852/2330 train_time:74444ms step_avg:40.20ms
step:1853/2330 train_time:74479ms step_avg:40.19ms
step:1854/2330 train_time:74523ms step_avg:40.20ms
step:1855/2330 train_time:74558ms step_avg:40.19ms
step:1856/2330 train_time:74602ms step_avg:40.20ms
step:1857/2330 train_time:74637ms step_avg:40.19ms
step:1858/2330 train_time:74681ms step_avg:40.19ms
step:1859/2330 train_time:74715ms step_avg:40.19ms
step:1860/2330 train_time:74760ms step_avg:40.19ms
step:1861/2330 train_time:74796ms step_avg:40.19ms
step:1862/2330 train_time:74841ms step_avg:40.19ms
step:1863/2330 train_time:74876ms step_avg:40.19ms
step:1864/2330 train_time:74921ms step_avg:40.19ms
step:1865/2330 train_time:74956ms step_avg:40.19ms
step:1866/2330 train_time:75001ms step_avg:40.19ms
step:1867/2330 train_time:75035ms step_avg:40.19ms
step:1868/2330 train_time:75080ms step_avg:40.19ms
step:1869/2330 train_time:75115ms step_avg:40.19ms
step:1870/2330 train_time:75159ms step_avg:40.19ms
step:1871/2330 train_time:75195ms step_avg:40.19ms
step:1872/2330 train_time:75239ms step_avg:40.19ms
step:1873/2330 train_time:75274ms step_avg:40.19ms
step:1874/2330 train_time:75319ms step_avg:40.19ms
step:1875/2330 train_time:75353ms step_avg:40.19ms
step:1876/2330 train_time:75397ms step_avg:40.19ms
step:1877/2330 train_time:75432ms step_avg:40.19ms
step:1878/2330 train_time:75477ms step_avg:40.19ms
step:1879/2330 train_time:75512ms step_avg:40.19ms
step:1880/2330 train_time:75556ms step_avg:40.19ms
step:1881/2330 train_time:75591ms step_avg:40.19ms
step:1882/2330 train_time:75636ms step_avg:40.19ms
step:1883/2330 train_time:75672ms step_avg:40.19ms
step:1884/2330 train_time:75718ms step_avg:40.19ms
step:1885/2330 train_time:75753ms step_avg:40.19ms
step:1886/2330 train_time:75798ms step_avg:40.19ms
step:1887/2330 train_time:75833ms step_avg:40.19ms
step:1888/2330 train_time:75878ms step_avg:40.19ms
step:1889/2330 train_time:75913ms step_avg:40.19ms
step:1890/2330 train_time:75958ms step_avg:40.19ms
step:1891/2330 train_time:75992ms step_avg:40.19ms
step:1892/2330 train_time:76037ms step_avg:40.19ms
step:1893/2330 train_time:76073ms step_avg:40.19ms
step:1894/2330 train_time:76117ms step_avg:40.19ms
step:1895/2330 train_time:76152ms step_avg:40.19ms
step:1896/2330 train_time:76196ms step_avg:40.19ms
step:1897/2330 train_time:76231ms step_avg:40.19ms
step:1898/2330 train_time:76276ms step_avg:40.19ms
step:1899/2330 train_time:76311ms step_avg:40.18ms
step:1900/2330 train_time:76355ms step_avg:40.19ms
step:1901/2330 train_time:76391ms step_avg:40.18ms
step:1902/2330 train_time:76434ms step_avg:40.19ms
step:1903/2330 train_time:76470ms step_avg:40.18ms
step:1904/2330 train_time:76514ms step_avg:40.19ms
step:1905/2330 train_time:76549ms step_avg:40.18ms
step:1906/2330 train_time:76594ms step_avg:40.19ms
step:1907/2330 train_time:76629ms step_avg:40.18ms
step:1908/2330 train_time:76673ms step_avg:40.19ms
step:1909/2330 train_time:76708ms step_avg:40.18ms
step:1910/2330 train_time:76752ms step_avg:40.18ms
step:1911/2330 train_time:76787ms step_avg:40.18ms
step:1912/2330 train_time:76832ms step_avg:40.18ms
step:1913/2330 train_time:76866ms step_avg:40.18ms
step:1914/2330 train_time:76910ms step_avg:40.18ms
step:1915/2330 train_time:76945ms step_avg:40.18ms
step:1916/2330 train_time:76990ms step_avg:40.18ms
step:1917/2330 train_time:77025ms step_avg:40.18ms
step:1918/2330 train_time:77069ms step_avg:40.18ms
step:1919/2330 train_time:77105ms step_avg:40.18ms
step:1920/2330 train_time:77149ms step_avg:40.18ms
step:1921/2330 train_time:77184ms step_avg:40.18ms
step:1922/2330 train_time:77229ms step_avg:40.18ms
step:1923/2330 train_time:77263ms step_avg:40.18ms
step:1924/2330 train_time:77308ms step_avg:40.18ms
step:1925/2330 train_time:77342ms step_avg:40.18ms
step:1926/2330 train_time:77387ms step_avg:40.18ms
step:1927/2330 train_time:77421ms step_avg:40.18ms
step:1928/2330 train_time:77466ms step_avg:40.18ms
step:1929/2330 train_time:77501ms step_avg:40.18ms
step:1930/2330 train_time:77546ms step_avg:40.18ms
step:1931/2330 train_time:77581ms step_avg:40.18ms
step:1932/2330 train_time:77625ms step_avg:40.18ms
step:1933/2330 train_time:77660ms step_avg:40.18ms
step:1934/2330 train_time:77704ms step_avg:40.18ms
step:1935/2330 train_time:77739ms step_avg:40.17ms
step:1936/2330 train_time:77783ms step_avg:40.18ms
step:1937/2330 train_time:77817ms step_avg:40.17ms
step:1938/2330 train_time:77862ms step_avg:40.18ms
step:1939/2330 train_time:77897ms step_avg:40.17ms
step:1940/2330 train_time:77941ms step_avg:40.18ms
step:1941/2330 train_time:77976ms step_avg:40.17ms
step:1942/2330 train_time:78020ms step_avg:40.18ms
step:1943/2330 train_time:78055ms step_avg:40.17ms
step:1944/2330 train_time:78100ms step_avg:40.17ms
step:1945/2330 train_time:78135ms step_avg:40.17ms
step:1946/2330 train_time:78180ms step_avg:40.17ms
step:1947/2330 train_time:78215ms step_avg:40.17ms
step:1948/2330 train_time:78259ms step_avg:40.17ms
step:1949/2330 train_time:78295ms step_avg:40.17ms
step:1950/2330 train_time:78339ms step_avg:40.17ms
step:1951/2330 train_time:78374ms step_avg:40.17ms
step:1952/2330 train_time:78419ms step_avg:40.17ms
step:1953/2330 train_time:78454ms step_avg:40.17ms
step:1954/2330 train_time:78499ms step_avg:40.17ms
step:1955/2330 train_time:78533ms step_avg:40.17ms
step:1956/2330 train_time:78578ms step_avg:40.17ms
step:1957/2330 train_time:78612ms step_avg:40.17ms
step:1958/2330 train_time:78657ms step_avg:40.17ms
step:1959/2330 train_time:78692ms step_avg:40.17ms
step:1960/2330 train_time:78737ms step_avg:40.17ms
step:1961/2330 train_time:78771ms step_avg:40.17ms
step:1962/2330 train_time:78816ms step_avg:40.17ms
step:1963/2330 train_time:78852ms step_avg:40.17ms
step:1964/2330 train_time:78896ms step_avg:40.17ms
step:1965/2330 train_time:78931ms step_avg:40.17ms
step:1966/2330 train_time:78975ms step_avg:40.17ms
step:1967/2330 train_time:79010ms step_avg:40.17ms
step:1968/2330 train_time:79055ms step_avg:40.17ms
step:1969/2330 train_time:79092ms step_avg:40.17ms
step:1970/2330 train_time:79136ms step_avg:40.17ms
step:1971/2330 train_time:79171ms step_avg:40.17ms
step:1972/2330 train_time:79215ms step_avg:40.17ms
step:1973/2330 train_time:79251ms step_avg:40.17ms
step:1974/2330 train_time:79296ms step_avg:40.17ms
step:1975/2330 train_time:79330ms step_avg:40.17ms
step:1976/2330 train_time:79375ms step_avg:40.17ms
step:1977/2330 train_time:79410ms step_avg:40.17ms
step:1978/2330 train_time:79454ms step_avg:40.17ms
step:1979/2330 train_time:79489ms step_avg:40.17ms
step:1980/2330 train_time:79534ms step_avg:40.17ms
step:1981/2330 train_time:79569ms step_avg:40.17ms
step:1982/2330 train_time:79613ms step_avg:40.17ms
step:1983/2330 train_time:79647ms step_avg:40.17ms
step:1984/2330 train_time:79692ms step_avg:40.17ms
step:1985/2330 train_time:79727ms step_avg:40.16ms
step:1986/2330 train_time:79771ms step_avg:40.17ms
step:1987/2330 train_time:79806ms step_avg:40.16ms
step:1988/2330 train_time:79850ms step_avg:40.17ms
step:1989/2330 train_time:79885ms step_avg:40.16ms
step:1990/2330 train_time:79928ms step_avg:40.17ms
step:1991/2330 train_time:79963ms step_avg:40.16ms
step:1992/2330 train_time:80008ms step_avg:40.16ms
step:1993/2330 train_time:80043ms step_avg:40.16ms
step:1994/2330 train_time:80087ms step_avg:40.16ms
step:1995/2330 train_time:80123ms step_avg:40.16ms
step:1996/2330 train_time:80167ms step_avg:40.16ms
step:1997/2330 train_time:80202ms step_avg:40.16ms
step:1998/2330 train_time:80246ms step_avg:40.16ms
step:1999/2330 train_time:80281ms step_avg:40.16ms
step:2000/2330 train_time:80325ms step_avg:40.16ms
step:2000/2330 val_loss:5.0771 train_time:80412ms step_avg:40.21ms
step:2001/2330 train_time:80426ms step_avg:40.19ms
step:2002/2330 train_time:80439ms step_avg:40.18ms
step:2003/2330 train_time:80450ms step_avg:40.16ms
step:2004/2330 train_time:80487ms step_avg:40.16ms
step:2005/2330 train_time:80521ms step_avg:40.16ms
step:2006/2330 train_time:80565ms step_avg:40.16ms
step:2007/2330 train_time:80599ms step_avg:40.16ms
step:2008/2330 train_time:80642ms step_avg:40.16ms
step:2009/2330 train_time:80677ms step_avg:40.16ms
step:2010/2330 train_time:80723ms step_avg:40.16ms
step:2011/2330 train_time:80760ms step_avg:40.16ms
step:2012/2330 train_time:80807ms step_avg:40.16ms
step:2013/2330 train_time:80844ms step_avg:40.16ms
step:2014/2330 train_time:80889ms step_avg:40.16ms
step:2015/2330 train_time:80923ms step_avg:40.16ms
step:2016/2330 train_time:80968ms step_avg:40.16ms
step:2017/2330 train_time:81003ms step_avg:40.16ms
step:2018/2330 train_time:81047ms step_avg:40.16ms
step:2019/2330 train_time:81081ms step_avg:40.16ms
step:2020/2330 train_time:81126ms step_avg:40.16ms
step:2021/2330 train_time:81159ms step_avg:40.16ms
step:2022/2330 train_time:81202ms step_avg:40.16ms
step:2023/2330 train_time:81237ms step_avg:40.16ms
step:2024/2330 train_time:81281ms step_avg:40.16ms
step:2025/2330 train_time:81316ms step_avg:40.16ms
step:2026/2330 train_time:81360ms step_avg:40.16ms
step:2027/2330 train_time:81396ms step_avg:40.16ms
step:2028/2330 train_time:81441ms step_avg:40.16ms
step:2029/2330 train_time:81476ms step_avg:40.16ms
step:2030/2330 train_time:81519ms step_avg:40.16ms
step:2031/2330 train_time:81554ms step_avg:40.15ms
step:2032/2330 train_time:81598ms step_avg:40.16ms
step:2033/2330 train_time:81633ms step_avg:40.15ms
step:2034/2330 train_time:81677ms step_avg:40.16ms
step:2035/2330 train_time:81712ms step_avg:40.15ms
step:2036/2330 train_time:81757ms step_avg:40.16ms
step:2037/2330 train_time:81792ms step_avg:40.15ms
step:2038/2330 train_time:81837ms step_avg:40.16ms
step:2039/2330 train_time:81873ms step_avg:40.15ms
step:2040/2330 train_time:81917ms step_avg:40.16ms
step:2041/2330 train_time:81951ms step_avg:40.15ms
step:2042/2330 train_time:81995ms step_avg:40.15ms
step:2043/2330 train_time:82030ms step_avg:40.15ms
step:2044/2330 train_time:82075ms step_avg:40.15ms
step:2045/2330 train_time:82110ms step_avg:40.15ms
step:2046/2330 train_time:82154ms step_avg:40.15ms
step:2047/2330 train_time:82188ms step_avg:40.15ms
step:2048/2330 train_time:82233ms step_avg:40.15ms
step:2049/2330 train_time:82267ms step_avg:40.15ms
step:2050/2330 train_time:82312ms step_avg:40.15ms
step:2051/2330 train_time:82346ms step_avg:40.15ms
step:2052/2330 train_time:82392ms step_avg:40.15ms
step:2053/2330 train_time:82426ms step_avg:40.15ms
step:2054/2330 train_time:82471ms step_avg:40.15ms
step:2055/2330 train_time:82506ms step_avg:40.15ms
step:2056/2330 train_time:82551ms step_avg:40.15ms
step:2057/2330 train_time:82585ms step_avg:40.15ms
step:2058/2330 train_time:82630ms step_avg:40.15ms
step:2059/2330 train_time:82666ms step_avg:40.15ms
step:2060/2330 train_time:82710ms step_avg:40.15ms
step:2061/2330 train_time:82746ms step_avg:40.15ms
step:2062/2330 train_time:82790ms step_avg:40.15ms
step:2063/2330 train_time:82826ms step_avg:40.15ms
step:2064/2330 train_time:82870ms step_avg:40.15ms
step:2065/2330 train_time:82906ms step_avg:40.15ms
step:2066/2330 train_time:82950ms step_avg:40.15ms
step:2067/2330 train_time:82985ms step_avg:40.15ms
step:2068/2330 train_time:83031ms step_avg:40.15ms
step:2069/2330 train_time:83066ms step_avg:40.15ms
step:2070/2330 train_time:83111ms step_avg:40.15ms
step:2071/2330 train_time:83146ms step_avg:40.15ms
step:2072/2330 train_time:83190ms step_avg:40.15ms
step:2073/2330 train_time:83225ms step_avg:40.15ms
step:2074/2330 train_time:83269ms step_avg:40.15ms
step:2075/2330 train_time:83304ms step_avg:40.15ms
step:2076/2330 train_time:83348ms step_avg:40.15ms
step:2077/2330 train_time:83384ms step_avg:40.15ms
step:2078/2330 train_time:83428ms step_avg:40.15ms
step:2079/2330 train_time:83463ms step_avg:40.15ms
step:2080/2330 train_time:83507ms step_avg:40.15ms
step:2081/2330 train_time:83543ms step_avg:40.15ms
step:2082/2330 train_time:83586ms step_avg:40.15ms
step:2083/2330 train_time:83621ms step_avg:40.14ms
step:2084/2330 train_time:83665ms step_avg:40.15ms
step:2085/2330 train_time:83701ms step_avg:40.14ms
step:2086/2330 train_time:83745ms step_avg:40.15ms
step:2087/2330 train_time:83780ms step_avg:40.14ms
step:2088/2330 train_time:83825ms step_avg:40.15ms
step:2089/2330 train_time:83860ms step_avg:40.14ms
step:2090/2330 train_time:83905ms step_avg:40.15ms
step:2091/2330 train_time:83940ms step_avg:40.14ms
step:2092/2330 train_time:83985ms step_avg:40.15ms
step:2093/2330 train_time:84020ms step_avg:40.14ms
step:2094/2330 train_time:84064ms step_avg:40.15ms
step:2095/2330 train_time:84100ms step_avg:40.14ms
step:2096/2330 train_time:84144ms step_avg:40.14ms
step:2097/2330 train_time:84179ms step_avg:40.14ms
step:2098/2330 train_time:84224ms step_avg:40.14ms
step:2099/2330 train_time:84259ms step_avg:40.14ms
step:2100/2330 train_time:84303ms step_avg:40.14ms
step:2101/2330 train_time:84338ms step_avg:40.14ms
step:2102/2330 train_time:84382ms step_avg:40.14ms
step:2103/2330 train_time:84417ms step_avg:40.14ms
step:2104/2330 train_time:84462ms step_avg:40.14ms
step:2105/2330 train_time:84496ms step_avg:40.14ms
step:2106/2330 train_time:84541ms step_avg:40.14ms
step:2107/2330 train_time:84576ms step_avg:40.14ms
step:2108/2330 train_time:84620ms step_avg:40.14ms
step:2109/2330 train_time:84655ms step_avg:40.14ms
step:2110/2330 train_time:84699ms step_avg:40.14ms
step:2111/2330 train_time:84734ms step_avg:40.14ms
step:2112/2330 train_time:84777ms step_avg:40.14ms
step:2113/2330 train_time:84813ms step_avg:40.14ms
step:2114/2330 train_time:84857ms step_avg:40.14ms
step:2115/2330 train_time:84892ms step_avg:40.14ms
step:2116/2330 train_time:84936ms step_avg:40.14ms
step:2117/2330 train_time:84971ms step_avg:40.14ms
step:2118/2330 train_time:85015ms step_avg:40.14ms
step:2119/2330 train_time:85050ms step_avg:40.14ms
step:2120/2330 train_time:85094ms step_avg:40.14ms
step:2121/2330 train_time:85130ms step_avg:40.14ms
step:2122/2330 train_time:85175ms step_avg:40.14ms
step:2123/2330 train_time:85211ms step_avg:40.14ms
step:2124/2330 train_time:85255ms step_avg:40.14ms
step:2125/2330 train_time:85290ms step_avg:40.14ms
step:2126/2330 train_time:85334ms step_avg:40.14ms
step:2127/2330 train_time:85370ms step_avg:40.14ms
step:2128/2330 train_time:85414ms step_avg:40.14ms
step:2129/2330 train_time:85448ms step_avg:40.14ms
step:2130/2330 train_time:85493ms step_avg:40.14ms
step:2131/2330 train_time:85528ms step_avg:40.14ms
step:2132/2330 train_time:85573ms step_avg:40.14ms
step:2133/2330 train_time:85608ms step_avg:40.13ms
step:2134/2330 train_time:85652ms step_avg:40.14ms
step:2135/2330 train_time:85686ms step_avg:40.13ms
step:2136/2330 train_time:85732ms step_avg:40.14ms
step:2137/2330 train_time:85766ms step_avg:40.13ms
step:2138/2330 train_time:85811ms step_avg:40.14ms
step:2139/2330 train_time:85846ms step_avg:40.13ms
step:2140/2330 train_time:85890ms step_avg:40.14ms
step:2141/2330 train_time:85925ms step_avg:40.13ms
step:2142/2330 train_time:85969ms step_avg:40.14ms
step:2143/2330 train_time:86005ms step_avg:40.13ms
step:2144/2330 train_time:86049ms step_avg:40.13ms
step:2145/2330 train_time:86084ms step_avg:40.13ms
step:2146/2330 train_time:86130ms step_avg:40.13ms
step:2147/2330 train_time:86165ms step_avg:40.13ms
step:2148/2330 train_time:86209ms step_avg:40.13ms
step:2149/2330 train_time:86245ms step_avg:40.13ms
step:2150/2330 train_time:86289ms step_avg:40.13ms
step:2151/2330 train_time:86324ms step_avg:40.13ms
step:2152/2330 train_time:86369ms step_avg:40.13ms
step:2153/2330 train_time:86404ms step_avg:40.13ms
step:2154/2330 train_time:86447ms step_avg:40.13ms
step:2155/2330 train_time:86482ms step_avg:40.13ms
step:2156/2330 train_time:86526ms step_avg:40.13ms
step:2157/2330 train_time:86561ms step_avg:40.13ms
step:2158/2330 train_time:86605ms step_avg:40.13ms
step:2159/2330 train_time:86639ms step_avg:40.13ms
step:2160/2330 train_time:86683ms step_avg:40.13ms
step:2161/2330 train_time:86718ms step_avg:40.13ms
step:2162/2330 train_time:86762ms step_avg:40.13ms
step:2163/2330 train_time:86798ms step_avg:40.13ms
step:2164/2330 train_time:86841ms step_avg:40.13ms
step:2165/2330 train_time:86876ms step_avg:40.13ms
step:2166/2330 train_time:86920ms step_avg:40.13ms
step:2167/2330 train_time:86955ms step_avg:40.13ms
step:2168/2330 train_time:87000ms step_avg:40.13ms
step:2169/2330 train_time:87035ms step_avg:40.13ms
step:2170/2330 train_time:87080ms step_avg:40.13ms
step:2171/2330 train_time:87115ms step_avg:40.13ms
step:2172/2330 train_time:87159ms step_avg:40.13ms
step:2173/2330 train_time:87194ms step_avg:40.13ms
step:2174/2330 train_time:87239ms step_avg:40.13ms
step:2175/2330 train_time:87274ms step_avg:40.13ms
step:2176/2330 train_time:87318ms step_avg:40.13ms
step:2177/2330 train_time:87354ms step_avg:40.13ms
step:2178/2330 train_time:87398ms step_avg:40.13ms
step:2179/2330 train_time:87433ms step_avg:40.13ms
step:2180/2330 train_time:87477ms step_avg:40.13ms
step:2181/2330 train_time:87512ms step_avg:40.12ms
step:2182/2330 train_time:87556ms step_avg:40.13ms
step:2183/2330 train_time:87590ms step_avg:40.12ms
step:2184/2330 train_time:87635ms step_avg:40.13ms
step:2185/2330 train_time:87670ms step_avg:40.12ms
step:2186/2330 train_time:87715ms step_avg:40.13ms
step:2187/2330 train_time:87750ms step_avg:40.12ms
step:2188/2330 train_time:87794ms step_avg:40.13ms
step:2189/2330 train_time:87829ms step_avg:40.12ms
step:2190/2330 train_time:87873ms step_avg:40.12ms
step:2191/2330 train_time:87908ms step_avg:40.12ms
step:2192/2330 train_time:87953ms step_avg:40.12ms
step:2193/2330 train_time:87988ms step_avg:40.12ms
step:2194/2330 train_time:88033ms step_avg:40.12ms
step:2195/2330 train_time:88068ms step_avg:40.12ms
step:2196/2330 train_time:88112ms step_avg:40.12ms
step:2197/2330 train_time:88147ms step_avg:40.12ms
step:2198/2330 train_time:88192ms step_avg:40.12ms
step:2199/2330 train_time:88228ms step_avg:40.12ms
step:2200/2330 train_time:88272ms step_avg:40.12ms
step:2201/2330 train_time:88306ms step_avg:40.12ms
step:2202/2330 train_time:88350ms step_avg:40.12ms
step:2203/2330 train_time:88386ms step_avg:40.12ms
step:2204/2330 train_time:88430ms step_avg:40.12ms
step:2205/2330 train_time:88466ms step_avg:40.12ms
step:2206/2330 train_time:88510ms step_avg:40.12ms
step:2207/2330 train_time:88545ms step_avg:40.12ms
step:2208/2330 train_time:88589ms step_avg:40.12ms
step:2209/2330 train_time:88624ms step_avg:40.12ms
step:2210/2330 train_time:88669ms step_avg:40.12ms
step:2211/2330 train_time:88704ms step_avg:40.12ms
step:2212/2330 train_time:88748ms step_avg:40.12ms
step:2213/2330 train_time:88783ms step_avg:40.12ms
step:2214/2330 train_time:88827ms step_avg:40.12ms
step:2215/2330 train_time:88863ms step_avg:40.12ms
step:2216/2330 train_time:88907ms step_avg:40.12ms
step:2217/2330 train_time:88942ms step_avg:40.12ms
step:2218/2330 train_time:88987ms step_avg:40.12ms
step:2219/2330 train_time:89022ms step_avg:40.12ms
step:2220/2330 train_time:89066ms step_avg:40.12ms
step:2221/2330 train_time:89100ms step_avg:40.12ms
step:2222/2330 train_time:89144ms step_avg:40.12ms
step:2223/2330 train_time:89178ms step_avg:40.12ms
step:2224/2330 train_time:89223ms step_avg:40.12ms
step:2225/2330 train_time:89258ms step_avg:40.12ms
step:2226/2330 train_time:89302ms step_avg:40.12ms
step:2227/2330 train_time:89336ms step_avg:40.12ms
step:2228/2330 train_time:89381ms step_avg:40.12ms
step:2229/2330 train_time:89416ms step_avg:40.11ms
step:2230/2330 train_time:89460ms step_avg:40.12ms
step:2231/2330 train_time:89495ms step_avg:40.11ms
step:2232/2330 train_time:89539ms step_avg:40.12ms
step:2233/2330 train_time:89574ms step_avg:40.11ms
step:2234/2330 train_time:89619ms step_avg:40.12ms
step:2235/2330 train_time:89653ms step_avg:40.11ms
step:2236/2330 train_time:89697ms step_avg:40.12ms
step:2237/2330 train_time:89732ms step_avg:40.11ms
step:2238/2330 train_time:89777ms step_avg:40.11ms
step:2239/2330 train_time:89812ms step_avg:40.11ms
step:2240/2330 train_time:89856ms step_avg:40.11ms
step:2241/2330 train_time:89891ms step_avg:40.11ms
step:2242/2330 train_time:89935ms step_avg:40.11ms
step:2243/2330 train_time:89970ms step_avg:40.11ms
step:2244/2330 train_time:90014ms step_avg:40.11ms
step:2245/2330 train_time:90049ms step_avg:40.11ms
step:2246/2330 train_time:90093ms step_avg:40.11ms
step:2247/2330 train_time:90129ms step_avg:40.11ms
step:2248/2330 train_time:90173ms step_avg:40.11ms
step:2249/2330 train_time:90209ms step_avg:40.11ms
step:2250/2330 train_time:90252ms step_avg:40.11ms
step:2250/2330 val_loss:5.0514 train_time:90338ms step_avg:40.15ms
step:2251/2330 train_time:90352ms step_avg:40.14ms
step:2252/2330 train_time:90364ms step_avg:40.13ms
step:2253/2330 train_time:90376ms step_avg:40.11ms
step:2254/2330 train_time:90411ms step_avg:40.11ms
step:2255/2330 train_time:90445ms step_avg:40.11ms
step:2256/2330 train_time:90488ms step_avg:40.11ms
step:2257/2330 train_time:90523ms step_avg:40.11ms
step:2258/2330 train_time:90567ms step_avg:40.11ms
step:2259/2330 train_time:90602ms step_avg:40.11ms
step:2260/2330 train_time:90647ms step_avg:40.11ms
step:2261/2330 train_time:90686ms step_avg:40.11ms
step:2262/2330 train_time:90734ms step_avg:40.11ms
step:2263/2330 train_time:90772ms step_avg:40.11ms
step:2264/2330 train_time:90816ms step_avg:40.11ms
step:2265/2330 train_time:90851ms step_avg:40.11ms
step:2266/2330 train_time:90896ms step_avg:40.11ms
step:2267/2330 train_time:90931ms step_avg:40.11ms
step:2268/2330 train_time:90975ms step_avg:40.11ms
step:2269/2330 train_time:91009ms step_avg:40.11ms
step:2270/2330 train_time:91053ms step_avg:40.11ms
step:2271/2330 train_time:91088ms step_avg:40.11ms
step:2272/2330 train_time:91132ms step_avg:40.11ms
step:2273/2330 train_time:91166ms step_avg:40.11ms
step:2274/2330 train_time:91209ms step_avg:40.11ms
step:2275/2330 train_time:91244ms step_avg:40.11ms
step:2276/2330 train_time:91288ms step_avg:40.11ms
step:2277/2330 train_time:91322ms step_avg:40.11ms
step:2278/2330 train_time:91366ms step_avg:40.11ms
step:2279/2330 train_time:91401ms step_avg:40.11ms
step:2280/2330 train_time:91445ms step_avg:40.11ms
step:2281/2330 train_time:91479ms step_avg:40.10ms
step:2282/2330 train_time:91523ms step_avg:40.11ms
step:2283/2330 train_time:91557ms step_avg:40.10ms
step:2284/2330 train_time:91602ms step_avg:40.11ms
step:2285/2330 train_time:91638ms step_avg:40.10ms
step:2286/2330 train_time:91683ms step_avg:40.11ms
step:2287/2330 train_time:91718ms step_avg:40.10ms
step:2288/2330 train_time:91762ms step_avg:40.11ms
step:2289/2330 train_time:91796ms step_avg:40.10ms
step:2290/2330 train_time:91841ms step_avg:40.11ms
step:2291/2330 train_time:91876ms step_avg:40.10ms
step:2292/2330 train_time:91921ms step_avg:40.10ms
step:2293/2330 train_time:91956ms step_avg:40.10ms
step:2294/2330 train_time:92000ms step_avg:40.10ms
step:2295/2330 train_time:92036ms step_avg:40.10ms
step:2296/2330 train_time:92080ms step_avg:40.10ms
step:2297/2330 train_time:92115ms step_avg:40.10ms
step:2298/2330 train_time:92159ms step_avg:40.10ms
step:2299/2330 train_time:92194ms step_avg:40.10ms
step:2300/2330 train_time:92239ms step_avg:40.10ms
step:2301/2330 train_time:92274ms step_avg:40.10ms
step:2302/2330 train_time:92318ms step_avg:40.10ms
step:2303/2330 train_time:92353ms step_avg:40.10ms
step:2304/2330 train_time:92398ms step_avg:40.10ms
step:2305/2330 train_time:92433ms step_avg:40.10ms
step:2306/2330 train_time:92478ms step_avg:40.10ms
step:2307/2330 train_time:92513ms step_avg:40.10ms
step:2308/2330 train_time:92557ms step_avg:40.10ms
step:2309/2330 train_time:92593ms step_avg:40.10ms
step:2310/2330 train_time:92638ms step_avg:40.10ms
step:2311/2330 train_time:92673ms step_avg:40.10ms
step:2312/2330 train_time:92717ms step_avg:40.10ms
step:2313/2330 train_time:92753ms step_avg:40.10ms
step:2314/2330 train_time:92798ms step_avg:40.10ms
step:2315/2330 train_time:92834ms step_avg:40.10ms
step:2316/2330 train_time:92878ms step_avg:40.10ms
step:2317/2330 train_time:92912ms step_avg:40.10ms
step:2318/2330 train_time:92957ms step_avg:40.10ms
step:2319/2330 train_time:92992ms step_avg:40.10ms
step:2320/2330 train_time:93037ms step_avg:40.10ms
step:2321/2330 train_time:93071ms step_avg:40.10ms
step:2322/2330 train_time:93116ms step_avg:40.10ms
step:2323/2330 train_time:93151ms step_avg:40.10ms
step:2324/2330 train_time:93195ms step_avg:40.10ms
step:2325/2330 train_time:93230ms step_avg:40.10ms
step:2326/2330 train_time:93274ms step_avg:40.10ms
step:2327/2330 train_time:93309ms step_avg:40.10ms
step:2328/2330 train_time:93352ms step_avg:40.10ms
step:2329/2330 train_time:93387ms step_avg:40.10ms
step:2330/2330 train_time:93431ms step_avg:40.10ms
step:2330/2330 val_loss:5.0447 train_time:93518ms step_avg:40.14ms
peak memory allocated: 29226 MiB reserved: 38968 MiB
