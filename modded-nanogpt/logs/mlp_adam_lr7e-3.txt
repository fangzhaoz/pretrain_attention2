import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr7e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=7e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:44:26 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:68ms step_avg:67.55ms
step:2/2330 train_time:137ms step_avg:68.37ms
step:3/2330 train_time:147ms step_avg:49.15ms
step:4/2330 train_time:159ms step_avg:39.67ms
step:5/2330 train_time:168ms step_avg:33.69ms
step:6/2330 train_time:179ms step_avg:29.87ms
step:7/2330 train_time:223ms step_avg:31.82ms
step:8/2330 train_time:263ms step_avg:32.84ms
step:9/2330 train_time:296ms step_avg:32.91ms
step:10/2330 train_time:336ms step_avg:33.61ms
step:11/2330 train_time:369ms step_avg:33.58ms
step:12/2330 train_time:409ms step_avg:34.10ms
step:13/2330 train_time:443ms step_avg:34.05ms
step:14/2330 train_time:483ms step_avg:34.47ms
step:15/2330 train_time:516ms step_avg:34.41ms
step:16/2330 train_time:556ms step_avg:34.75ms
step:17/2330 train_time:589ms step_avg:34.67ms
step:18/2330 train_time:629ms step_avg:34.97ms
step:19/2330 train_time:663ms step_avg:34.89ms
step:20/2330 train_time:703ms step_avg:35.15ms
step:21/2330 train_time:737ms step_avg:35.07ms
step:22/2330 train_time:777ms step_avg:35.31ms
step:23/2330 train_time:810ms step_avg:35.22ms
step:24/2330 train_time:850ms step_avg:35.42ms
step:25/2330 train_time:884ms step_avg:35.37ms
step:26/2330 train_time:924ms step_avg:35.55ms
step:27/2330 train_time:958ms step_avg:35.49ms
step:28/2330 train_time:998ms step_avg:35.66ms
step:29/2330 train_time:1033ms step_avg:35.61ms
step:30/2330 train_time:1073ms step_avg:35.76ms
step:31/2330 train_time:1109ms step_avg:35.78ms
step:32/2330 train_time:1150ms step_avg:35.93ms
step:33/2330 train_time:1185ms step_avg:35.91ms
step:34/2330 train_time:1225ms step_avg:36.04ms
step:35/2330 train_time:1260ms step_avg:36.01ms
step:36/2330 train_time:1301ms step_avg:36.13ms
step:37/2330 train_time:1335ms step_avg:36.09ms
step:38/2330 train_time:1375ms step_avg:36.19ms
step:39/2330 train_time:1410ms step_avg:36.14ms
step:40/2330 train_time:1450ms step_avg:36.25ms
step:41/2330 train_time:1485ms step_avg:36.21ms
step:42/2330 train_time:1525ms step_avg:36.31ms
step:43/2330 train_time:1559ms step_avg:36.26ms
step:44/2330 train_time:1599ms step_avg:36.35ms
step:45/2330 train_time:1633ms step_avg:36.29ms
step:46/2330 train_time:1673ms step_avg:36.38ms
step:47/2330 train_time:1707ms step_avg:36.32ms
step:48/2330 train_time:1747ms step_avg:36.41ms
step:49/2330 train_time:1781ms step_avg:36.35ms
step:50/2330 train_time:1821ms step_avg:36.43ms
step:51/2330 train_time:1855ms step_avg:36.38ms
step:52/2330 train_time:1895ms step_avg:36.45ms
step:53/2330 train_time:1929ms step_avg:36.40ms
step:54/2330 train_time:1970ms step_avg:36.47ms
step:55/2330 train_time:2004ms step_avg:36.43ms
step:56/2330 train_time:2044ms step_avg:36.51ms
step:57/2330 train_time:2079ms step_avg:36.47ms
step:58/2330 train_time:2119ms step_avg:36.54ms
step:59/2330 train_time:2154ms step_avg:36.51ms
step:60/2330 train_time:2195ms step_avg:36.58ms
step:61/2330 train_time:2229ms step_avg:36.54ms
step:62/2330 train_time:2269ms step_avg:36.60ms
step:63/2330 train_time:2304ms step_avg:36.57ms
step:64/2330 train_time:2344ms step_avg:36.63ms
step:65/2330 train_time:2379ms step_avg:36.60ms
step:66/2330 train_time:2419ms step_avg:36.65ms
step:67/2330 train_time:2453ms step_avg:36.62ms
step:68/2330 train_time:2494ms step_avg:36.67ms
step:69/2330 train_time:2528ms step_avg:36.64ms
step:70/2330 train_time:2568ms step_avg:36.69ms
step:71/2330 train_time:2603ms step_avg:36.66ms
step:72/2330 train_time:2643ms step_avg:36.71ms
step:73/2330 train_time:2677ms step_avg:36.68ms
step:74/2330 train_time:2718ms step_avg:36.73ms
step:75/2330 train_time:2751ms step_avg:36.69ms
step:76/2330 train_time:2792ms step_avg:36.73ms
step:77/2330 train_time:2826ms step_avg:36.70ms
step:78/2330 train_time:2866ms step_avg:36.75ms
step:79/2330 train_time:2900ms step_avg:36.71ms
step:80/2330 train_time:2941ms step_avg:36.76ms
step:81/2330 train_time:2975ms step_avg:36.73ms
step:82/2330 train_time:3015ms step_avg:36.77ms
step:83/2330 train_time:3050ms step_avg:36.75ms
step:84/2330 train_time:3090ms step_avg:36.79ms
step:85/2330 train_time:3125ms step_avg:36.76ms
step:86/2330 train_time:3165ms step_avg:36.80ms
step:87/2330 train_time:3200ms step_avg:36.78ms
step:88/2330 train_time:3241ms step_avg:36.83ms
step:89/2330 train_time:3275ms step_avg:36.80ms
step:90/2330 train_time:3316ms step_avg:36.85ms
step:91/2330 train_time:3351ms step_avg:36.82ms
step:92/2330 train_time:3391ms step_avg:36.86ms
step:93/2330 train_time:3426ms step_avg:36.84ms
step:94/2330 train_time:3467ms step_avg:36.88ms
step:95/2330 train_time:3502ms step_avg:36.86ms
step:96/2330 train_time:3542ms step_avg:36.90ms
step:97/2330 train_time:3577ms step_avg:36.88ms
step:98/2330 train_time:3618ms step_avg:36.91ms
step:99/2330 train_time:3653ms step_avg:36.90ms
step:100/2330 train_time:3693ms step_avg:36.93ms
step:101/2330 train_time:3728ms step_avg:36.91ms
step:102/2330 train_time:3768ms step_avg:36.94ms
step:103/2330 train_time:3803ms step_avg:36.92ms
step:104/2330 train_time:3843ms step_avg:36.95ms
step:105/2330 train_time:3877ms step_avg:36.93ms
step:106/2330 train_time:3918ms step_avg:36.96ms
step:107/2330 train_time:3952ms step_avg:36.94ms
step:108/2330 train_time:3993ms step_avg:36.97ms
step:109/2330 train_time:4027ms step_avg:36.95ms
step:110/2330 train_time:4068ms step_avg:36.98ms
step:111/2330 train_time:4102ms step_avg:36.96ms
step:112/2330 train_time:4143ms step_avg:36.99ms
step:113/2330 train_time:4177ms step_avg:36.97ms
step:114/2330 train_time:4218ms step_avg:37.00ms
step:115/2330 train_time:4253ms step_avg:36.98ms
step:116/2330 train_time:4293ms step_avg:37.01ms
step:117/2330 train_time:4328ms step_avg:36.99ms
step:118/2330 train_time:4369ms step_avg:37.03ms
step:119/2330 train_time:4404ms step_avg:37.01ms
step:120/2330 train_time:4444ms step_avg:37.04ms
step:121/2330 train_time:4478ms step_avg:37.01ms
step:122/2330 train_time:4519ms step_avg:37.04ms
step:123/2330 train_time:4554ms step_avg:37.03ms
step:124/2330 train_time:4595ms step_avg:37.06ms
step:125/2330 train_time:4629ms step_avg:37.03ms
step:126/2330 train_time:4670ms step_avg:37.06ms
step:127/2330 train_time:4704ms step_avg:37.04ms
step:128/2330 train_time:4745ms step_avg:37.07ms
step:129/2330 train_time:4780ms step_avg:37.05ms
step:130/2330 train_time:4820ms step_avg:37.08ms
step:131/2330 train_time:4855ms step_avg:37.06ms
step:132/2330 train_time:4896ms step_avg:37.09ms
step:133/2330 train_time:4930ms step_avg:37.07ms
step:134/2330 train_time:4970ms step_avg:37.09ms
step:135/2330 train_time:5005ms step_avg:37.08ms
step:136/2330 train_time:5046ms step_avg:37.10ms
step:137/2330 train_time:5080ms step_avg:37.08ms
step:138/2330 train_time:5120ms step_avg:37.10ms
step:139/2330 train_time:5155ms step_avg:37.09ms
step:140/2330 train_time:5196ms step_avg:37.11ms
step:141/2330 train_time:5230ms step_avg:37.09ms
step:142/2330 train_time:5271ms step_avg:37.12ms
step:143/2330 train_time:5306ms step_avg:37.10ms
step:144/2330 train_time:5346ms step_avg:37.13ms
step:145/2330 train_time:5381ms step_avg:37.11ms
step:146/2330 train_time:5421ms step_avg:37.13ms
step:147/2330 train_time:5456ms step_avg:37.12ms
step:148/2330 train_time:5497ms step_avg:37.14ms
step:149/2330 train_time:5532ms step_avg:37.13ms
step:150/2330 train_time:5573ms step_avg:37.15ms
step:151/2330 train_time:5608ms step_avg:37.14ms
step:152/2330 train_time:5648ms step_avg:37.16ms
step:153/2330 train_time:5683ms step_avg:37.14ms
step:154/2330 train_time:5724ms step_avg:37.17ms
step:155/2330 train_time:5758ms step_avg:37.15ms
step:156/2330 train_time:5799ms step_avg:37.17ms
step:157/2330 train_time:5833ms step_avg:37.15ms
step:158/2330 train_time:5874ms step_avg:37.18ms
step:159/2330 train_time:5908ms step_avg:37.16ms
step:160/2330 train_time:5949ms step_avg:37.18ms
step:161/2330 train_time:5983ms step_avg:37.16ms
step:162/2330 train_time:6024ms step_avg:37.18ms
step:163/2330 train_time:6058ms step_avg:37.17ms
step:164/2330 train_time:6099ms step_avg:37.19ms
step:165/2330 train_time:6134ms step_avg:37.18ms
step:166/2330 train_time:6174ms step_avg:37.20ms
step:167/2330 train_time:6209ms step_avg:37.18ms
step:168/2330 train_time:6250ms step_avg:37.21ms
step:169/2330 train_time:6285ms step_avg:37.19ms
step:170/2330 train_time:6325ms step_avg:37.21ms
step:171/2330 train_time:6360ms step_avg:37.19ms
step:172/2330 train_time:6401ms step_avg:37.21ms
step:173/2330 train_time:6435ms step_avg:37.20ms
step:174/2330 train_time:6476ms step_avg:37.22ms
step:175/2330 train_time:6510ms step_avg:37.20ms
step:176/2330 train_time:6551ms step_avg:37.22ms
step:177/2330 train_time:6586ms step_avg:37.21ms
step:178/2330 train_time:6626ms step_avg:37.23ms
step:179/2330 train_time:6662ms step_avg:37.22ms
step:180/2330 train_time:6702ms step_avg:37.24ms
step:181/2330 train_time:6737ms step_avg:37.22ms
step:182/2330 train_time:6777ms step_avg:37.24ms
step:183/2330 train_time:6812ms step_avg:37.22ms
step:184/2330 train_time:6852ms step_avg:37.24ms
step:185/2330 train_time:6887ms step_avg:37.23ms
step:186/2330 train_time:6928ms step_avg:37.25ms
step:187/2330 train_time:6962ms step_avg:37.23ms
step:188/2330 train_time:7003ms step_avg:37.25ms
step:189/2330 train_time:7037ms step_avg:37.23ms
step:190/2330 train_time:7078ms step_avg:37.25ms
step:191/2330 train_time:7112ms step_avg:37.24ms
step:192/2330 train_time:7153ms step_avg:37.25ms
step:193/2330 train_time:7188ms step_avg:37.24ms
step:194/2330 train_time:7229ms step_avg:37.26ms
step:195/2330 train_time:7264ms step_avg:37.25ms
step:196/2330 train_time:7305ms step_avg:37.27ms
step:197/2330 train_time:7339ms step_avg:37.25ms
step:198/2330 train_time:7379ms step_avg:37.27ms
step:199/2330 train_time:7414ms step_avg:37.26ms
step:200/2330 train_time:7455ms step_avg:37.27ms
step:201/2330 train_time:7489ms step_avg:37.26ms
step:202/2330 train_time:7530ms step_avg:37.28ms
step:203/2330 train_time:7565ms step_avg:37.27ms
step:204/2330 train_time:7606ms step_avg:37.28ms
step:205/2330 train_time:7640ms step_avg:37.27ms
step:206/2330 train_time:7681ms step_avg:37.29ms
step:207/2330 train_time:7716ms step_avg:37.28ms
step:208/2330 train_time:7757ms step_avg:37.29ms
step:209/2330 train_time:7792ms step_avg:37.28ms
step:210/2330 train_time:7832ms step_avg:37.30ms
step:211/2330 train_time:7868ms step_avg:37.29ms
step:212/2330 train_time:7909ms step_avg:37.30ms
step:213/2330 train_time:7943ms step_avg:37.29ms
step:214/2330 train_time:7983ms step_avg:37.30ms
step:215/2330 train_time:8018ms step_avg:37.29ms
step:216/2330 train_time:8059ms step_avg:37.31ms
step:217/2330 train_time:8094ms step_avg:37.30ms
step:218/2330 train_time:8135ms step_avg:37.31ms
step:219/2330 train_time:8169ms step_avg:37.30ms
step:220/2330 train_time:8210ms step_avg:37.32ms
step:221/2330 train_time:8244ms step_avg:37.30ms
step:222/2330 train_time:8284ms step_avg:37.32ms
step:223/2330 train_time:8319ms step_avg:37.31ms
step:224/2330 train_time:8360ms step_avg:37.32ms
step:225/2330 train_time:8395ms step_avg:37.31ms
step:226/2330 train_time:8435ms step_avg:37.32ms
step:227/2330 train_time:8470ms step_avg:37.31ms
step:228/2330 train_time:8511ms step_avg:37.33ms
step:229/2330 train_time:8545ms step_avg:37.31ms
step:230/2330 train_time:8586ms step_avg:37.33ms
step:231/2330 train_time:8621ms step_avg:37.32ms
step:232/2330 train_time:8661ms step_avg:37.33ms
step:233/2330 train_time:8696ms step_avg:37.32ms
step:234/2330 train_time:8737ms step_avg:37.34ms
step:235/2330 train_time:8772ms step_avg:37.33ms
step:236/2330 train_time:8812ms step_avg:37.34ms
step:237/2330 train_time:8847ms step_avg:37.33ms
step:238/2330 train_time:8888ms step_avg:37.34ms
step:239/2330 train_time:8923ms step_avg:37.33ms
step:240/2330 train_time:8963ms step_avg:37.35ms
step:241/2330 train_time:8998ms step_avg:37.34ms
step:242/2330 train_time:9039ms step_avg:37.35ms
step:243/2330 train_time:9073ms step_avg:37.34ms
step:244/2330 train_time:9114ms step_avg:37.35ms
step:245/2330 train_time:9148ms step_avg:37.34ms
step:246/2330 train_time:9188ms step_avg:37.35ms
step:247/2330 train_time:9223ms step_avg:37.34ms
step:248/2330 train_time:9264ms step_avg:37.35ms
step:249/2330 train_time:9299ms step_avg:37.34ms
step:250/2330 train_time:9339ms step_avg:37.36ms
step:250/2330 val_loss:5.8499 train_time:9450ms step_avg:37.80ms
step:251/2330 train_time:9461ms step_avg:37.69ms
step:252/2330 train_time:9471ms step_avg:37.59ms
step:253/2330 train_time:9480ms step_avg:37.47ms
step:254/2330 train_time:9492ms step_avg:37.37ms
step:255/2330 train_time:9528ms step_avg:37.36ms
step:256/2330 train_time:9568ms step_avg:37.37ms
step:257/2330 train_time:9602ms step_avg:37.36ms
step:258/2330 train_time:9642ms step_avg:37.37ms
step:259/2330 train_time:9677ms step_avg:37.36ms
step:260/2330 train_time:9717ms step_avg:37.37ms
step:261/2330 train_time:9752ms step_avg:37.36ms
step:262/2330 train_time:9793ms step_avg:37.38ms
step:263/2330 train_time:9833ms step_avg:37.39ms
step:264/2330 train_time:9874ms step_avg:37.40ms
step:265/2330 train_time:9910ms step_avg:37.40ms
step:266/2330 train_time:9951ms step_avg:37.41ms
step:267/2330 train_time:9987ms step_avg:37.40ms
step:268/2330 train_time:10027ms step_avg:37.41ms
step:269/2330 train_time:10062ms step_avg:37.40ms
step:270/2330 train_time:10102ms step_avg:37.41ms
step:271/2330 train_time:10136ms step_avg:37.40ms
step:272/2330 train_time:10177ms step_avg:37.41ms
step:273/2330 train_time:10211ms step_avg:37.40ms
step:274/2330 train_time:10251ms step_avg:37.41ms
step:275/2330 train_time:10287ms step_avg:37.41ms
step:276/2330 train_time:10327ms step_avg:37.42ms
step:277/2330 train_time:10362ms step_avg:37.41ms
step:278/2330 train_time:10403ms step_avg:37.42ms
step:279/2330 train_time:10438ms step_avg:37.41ms
step:280/2330 train_time:10479ms step_avg:37.43ms
step:281/2330 train_time:10514ms step_avg:37.42ms
step:282/2330 train_time:10555ms step_avg:37.43ms
step:283/2330 train_time:10590ms step_avg:37.42ms
step:284/2330 train_time:10631ms step_avg:37.43ms
step:285/2330 train_time:10665ms step_avg:37.42ms
step:286/2330 train_time:10706ms step_avg:37.43ms
step:287/2330 train_time:10742ms step_avg:37.43ms
step:288/2330 train_time:10782ms step_avg:37.44ms
step:289/2330 train_time:10818ms step_avg:37.43ms
step:290/2330 train_time:10859ms step_avg:37.44ms
step:291/2330 train_time:10895ms step_avg:37.44ms
step:292/2330 train_time:10936ms step_avg:37.45ms
step:293/2330 train_time:10971ms step_avg:37.44ms
step:294/2330 train_time:11012ms step_avg:37.45ms
step:295/2330 train_time:11047ms step_avg:37.45ms
step:296/2330 train_time:11088ms step_avg:37.46ms
step:297/2330 train_time:11123ms step_avg:37.45ms
step:298/2330 train_time:11163ms step_avg:37.46ms
step:299/2330 train_time:11198ms step_avg:37.45ms
step:300/2330 train_time:11239ms step_avg:37.46ms
step:301/2330 train_time:11273ms step_avg:37.45ms
step:302/2330 train_time:11314ms step_avg:37.46ms
step:303/2330 train_time:11348ms step_avg:37.45ms
step:304/2330 train_time:11388ms step_avg:37.46ms
step:305/2330 train_time:11423ms step_avg:37.45ms
step:306/2330 train_time:11463ms step_avg:37.46ms
step:307/2330 train_time:11498ms step_avg:37.45ms
step:308/2330 train_time:11539ms step_avg:37.47ms
step:309/2330 train_time:11574ms step_avg:37.46ms
step:310/2330 train_time:11615ms step_avg:37.47ms
step:311/2330 train_time:11649ms step_avg:37.46ms
step:312/2330 train_time:11691ms step_avg:37.47ms
step:313/2330 train_time:11725ms step_avg:37.46ms
step:314/2330 train_time:11766ms step_avg:37.47ms
step:315/2330 train_time:11801ms step_avg:37.46ms
step:316/2330 train_time:11842ms step_avg:37.47ms
step:317/2330 train_time:11877ms step_avg:37.47ms
step:318/2330 train_time:11918ms step_avg:37.48ms
step:319/2330 train_time:11954ms step_avg:37.47ms
step:320/2330 train_time:11994ms step_avg:37.48ms
step:321/2330 train_time:12030ms step_avg:37.48ms
step:322/2330 train_time:12071ms step_avg:37.49ms
step:323/2330 train_time:12107ms step_avg:37.48ms
step:324/2330 train_time:12147ms step_avg:37.49ms
step:325/2330 train_time:12182ms step_avg:37.48ms
step:326/2330 train_time:12222ms step_avg:37.49ms
step:327/2330 train_time:12257ms step_avg:37.48ms
step:328/2330 train_time:12298ms step_avg:37.49ms
step:329/2330 train_time:12332ms step_avg:37.48ms
step:330/2330 train_time:12373ms step_avg:37.49ms
step:331/2330 train_time:12408ms step_avg:37.48ms
step:332/2330 train_time:12448ms step_avg:37.49ms
step:333/2330 train_time:12483ms step_avg:37.49ms
step:334/2330 train_time:12523ms step_avg:37.50ms
step:335/2330 train_time:12558ms step_avg:37.49ms
step:336/2330 train_time:12599ms step_avg:37.50ms
step:337/2330 train_time:12634ms step_avg:37.49ms
step:338/2330 train_time:12674ms step_avg:37.50ms
step:339/2330 train_time:12710ms step_avg:37.49ms
step:340/2330 train_time:12751ms step_avg:37.50ms
step:341/2330 train_time:12786ms step_avg:37.50ms
step:342/2330 train_time:12827ms step_avg:37.51ms
step:343/2330 train_time:12862ms step_avg:37.50ms
step:344/2330 train_time:12903ms step_avg:37.51ms
step:345/2330 train_time:12938ms step_avg:37.50ms
step:346/2330 train_time:12978ms step_avg:37.51ms
step:347/2330 train_time:13014ms step_avg:37.50ms
step:348/2330 train_time:13055ms step_avg:37.51ms
step:349/2330 train_time:13090ms step_avg:37.51ms
step:350/2330 train_time:13131ms step_avg:37.52ms
step:351/2330 train_time:13167ms step_avg:37.51ms
step:352/2330 train_time:13207ms step_avg:37.52ms
step:353/2330 train_time:13242ms step_avg:37.51ms
step:354/2330 train_time:13282ms step_avg:37.52ms
step:355/2330 train_time:13317ms step_avg:37.51ms
step:356/2330 train_time:13358ms step_avg:37.52ms
step:357/2330 train_time:13393ms step_avg:37.51ms
step:358/2330 train_time:13433ms step_avg:37.52ms
step:359/2330 train_time:13468ms step_avg:37.52ms
step:360/2330 train_time:13508ms step_avg:37.52ms
step:361/2330 train_time:13545ms step_avg:37.52ms
step:362/2330 train_time:13585ms step_avg:37.53ms
step:363/2330 train_time:13620ms step_avg:37.52ms
step:364/2330 train_time:13661ms step_avg:37.53ms
step:365/2330 train_time:13696ms step_avg:37.52ms
step:366/2330 train_time:13737ms step_avg:37.53ms
step:367/2330 train_time:13772ms step_avg:37.53ms
step:368/2330 train_time:13813ms step_avg:37.53ms
step:369/2330 train_time:13849ms step_avg:37.53ms
step:370/2330 train_time:13889ms step_avg:37.54ms
step:371/2330 train_time:13925ms step_avg:37.53ms
step:372/2330 train_time:13965ms step_avg:37.54ms
step:373/2330 train_time:14000ms step_avg:37.53ms
step:374/2330 train_time:14041ms step_avg:37.54ms
step:375/2330 train_time:14076ms step_avg:37.54ms
step:376/2330 train_time:14117ms step_avg:37.54ms
step:377/2330 train_time:14152ms step_avg:37.54ms
step:378/2330 train_time:14193ms step_avg:37.55ms
step:379/2330 train_time:14228ms step_avg:37.54ms
step:380/2330 train_time:14269ms step_avg:37.55ms
step:381/2330 train_time:14304ms step_avg:37.54ms
step:382/2330 train_time:14344ms step_avg:37.55ms
step:383/2330 train_time:14379ms step_avg:37.54ms
step:384/2330 train_time:14420ms step_avg:37.55ms
step:385/2330 train_time:14455ms step_avg:37.55ms
step:386/2330 train_time:14496ms step_avg:37.55ms
step:387/2330 train_time:14531ms step_avg:37.55ms
step:388/2330 train_time:14572ms step_avg:37.56ms
step:389/2330 train_time:14607ms step_avg:37.55ms
step:390/2330 train_time:14647ms step_avg:37.56ms
step:391/2330 train_time:14682ms step_avg:37.55ms
step:392/2330 train_time:14722ms step_avg:37.56ms
step:393/2330 train_time:14758ms step_avg:37.55ms
step:394/2330 train_time:14799ms step_avg:37.56ms
step:395/2330 train_time:14833ms step_avg:37.55ms
step:396/2330 train_time:14874ms step_avg:37.56ms
step:397/2330 train_time:14909ms step_avg:37.55ms
step:398/2330 train_time:14950ms step_avg:37.56ms
step:399/2330 train_time:14986ms step_avg:37.56ms
step:400/2330 train_time:15027ms step_avg:37.57ms
step:401/2330 train_time:15063ms step_avg:37.56ms
step:402/2330 train_time:15104ms step_avg:37.57ms
step:403/2330 train_time:15139ms step_avg:37.57ms
step:404/2330 train_time:15180ms step_avg:37.57ms
step:405/2330 train_time:15215ms step_avg:37.57ms
step:406/2330 train_time:15255ms step_avg:37.58ms
step:407/2330 train_time:15291ms step_avg:37.57ms
step:408/2330 train_time:15332ms step_avg:37.58ms
step:409/2330 train_time:15368ms step_avg:37.57ms
step:410/2330 train_time:15408ms step_avg:37.58ms
step:411/2330 train_time:15443ms step_avg:37.58ms
step:412/2330 train_time:15484ms step_avg:37.58ms
step:413/2330 train_time:15519ms step_avg:37.58ms
step:414/2330 train_time:15560ms step_avg:37.58ms
step:415/2330 train_time:15595ms step_avg:37.58ms
step:416/2330 train_time:15636ms step_avg:37.59ms
step:417/2330 train_time:15670ms step_avg:37.58ms
step:418/2330 train_time:15711ms step_avg:37.59ms
step:419/2330 train_time:15745ms step_avg:37.58ms
step:420/2330 train_time:15786ms step_avg:37.59ms
step:421/2330 train_time:15820ms step_avg:37.58ms
step:422/2330 train_time:15861ms step_avg:37.59ms
step:423/2330 train_time:15896ms step_avg:37.58ms
step:424/2330 train_time:15936ms step_avg:37.59ms
step:425/2330 train_time:15972ms step_avg:37.58ms
step:426/2330 train_time:16013ms step_avg:37.59ms
step:427/2330 train_time:16049ms step_avg:37.58ms
step:428/2330 train_time:16090ms step_avg:37.59ms
step:429/2330 train_time:16126ms step_avg:37.59ms
step:430/2330 train_time:16167ms step_avg:37.60ms
step:431/2330 train_time:16202ms step_avg:37.59ms
step:432/2330 train_time:16243ms step_avg:37.60ms
step:433/2330 train_time:16278ms step_avg:37.59ms
step:434/2330 train_time:16319ms step_avg:37.60ms
step:435/2330 train_time:16355ms step_avg:37.60ms
step:436/2330 train_time:16395ms step_avg:37.60ms
step:437/2330 train_time:16430ms step_avg:37.60ms
step:438/2330 train_time:16470ms step_avg:37.60ms
step:439/2330 train_time:16505ms step_avg:37.60ms
step:440/2330 train_time:16546ms step_avg:37.60ms
step:441/2330 train_time:16580ms step_avg:37.60ms
step:442/2330 train_time:16621ms step_avg:37.60ms
step:443/2330 train_time:16656ms step_avg:37.60ms
step:444/2330 train_time:16697ms step_avg:37.61ms
step:445/2330 train_time:16731ms step_avg:37.60ms
step:446/2330 train_time:16772ms step_avg:37.61ms
step:447/2330 train_time:16807ms step_avg:37.60ms
step:448/2330 train_time:16849ms step_avg:37.61ms
step:449/2330 train_time:16883ms step_avg:37.60ms
step:450/2330 train_time:16924ms step_avg:37.61ms
step:451/2330 train_time:16959ms step_avg:37.60ms
step:452/2330 train_time:17000ms step_avg:37.61ms
step:453/2330 train_time:17034ms step_avg:37.60ms
step:454/2330 train_time:17075ms step_avg:37.61ms
step:455/2330 train_time:17112ms step_avg:37.61ms
step:456/2330 train_time:17153ms step_avg:37.62ms
step:457/2330 train_time:17188ms step_avg:37.61ms
step:458/2330 train_time:17229ms step_avg:37.62ms
step:459/2330 train_time:17265ms step_avg:37.61ms
step:460/2330 train_time:17306ms step_avg:37.62ms
step:461/2330 train_time:17342ms step_avg:37.62ms
step:462/2330 train_time:17382ms step_avg:37.62ms
step:463/2330 train_time:17417ms step_avg:37.62ms
step:464/2330 train_time:17458ms step_avg:37.63ms
step:465/2330 train_time:17493ms step_avg:37.62ms
step:466/2330 train_time:17534ms step_avg:37.63ms
step:467/2330 train_time:17568ms step_avg:37.62ms
step:468/2330 train_time:17609ms step_avg:37.63ms
step:469/2330 train_time:17644ms step_avg:37.62ms
step:470/2330 train_time:17684ms step_avg:37.63ms
step:471/2330 train_time:17719ms step_avg:37.62ms
step:472/2330 train_time:17760ms step_avg:37.63ms
step:473/2330 train_time:17795ms step_avg:37.62ms
step:474/2330 train_time:17836ms step_avg:37.63ms
step:475/2330 train_time:17872ms step_avg:37.63ms
step:476/2330 train_time:17913ms step_avg:37.63ms
step:477/2330 train_time:17949ms step_avg:37.63ms
step:478/2330 train_time:17990ms step_avg:37.63ms
step:479/2330 train_time:18025ms step_avg:37.63ms
step:480/2330 train_time:18066ms step_avg:37.64ms
step:481/2330 train_time:18101ms step_avg:37.63ms
step:482/2330 train_time:18141ms step_avg:37.64ms
step:483/2330 train_time:18177ms step_avg:37.63ms
step:484/2330 train_time:18217ms step_avg:37.64ms
step:485/2330 train_time:18254ms step_avg:37.64ms
step:486/2330 train_time:18294ms step_avg:37.64ms
step:487/2330 train_time:18330ms step_avg:37.64ms
step:488/2330 train_time:18372ms step_avg:37.65ms
step:489/2330 train_time:18405ms step_avg:37.64ms
step:490/2330 train_time:18446ms step_avg:37.65ms
step:491/2330 train_time:18481ms step_avg:37.64ms
step:492/2330 train_time:18521ms step_avg:37.65ms
step:493/2330 train_time:18556ms step_avg:37.64ms
step:494/2330 train_time:18597ms step_avg:37.65ms
step:495/2330 train_time:18633ms step_avg:37.64ms
step:496/2330 train_time:18673ms step_avg:37.65ms
step:497/2330 train_time:18709ms step_avg:37.64ms
step:498/2330 train_time:18749ms step_avg:37.65ms
step:499/2330 train_time:18784ms step_avg:37.64ms
step:500/2330 train_time:18824ms step_avg:37.65ms
step:500/2330 val_loss:5.6212 train_time:18935ms step_avg:37.87ms
step:501/2330 train_time:18947ms step_avg:37.82ms
step:502/2330 train_time:18958ms step_avg:37.76ms
step:503/2330 train_time:18967ms step_avg:37.71ms
step:504/2330 train_time:18979ms step_avg:37.66ms
step:505/2330 train_time:19013ms step_avg:37.65ms
step:506/2330 train_time:19053ms step_avg:37.65ms
step:507/2330 train_time:19087ms step_avg:37.65ms
step:508/2330 train_time:19128ms step_avg:37.65ms
step:509/2330 train_time:19163ms step_avg:37.65ms
step:510/2330 train_time:19203ms step_avg:37.65ms
step:511/2330 train_time:19238ms step_avg:37.65ms
step:512/2330 train_time:19280ms step_avg:37.66ms
step:513/2330 train_time:19317ms step_avg:37.66ms
step:514/2330 train_time:19358ms step_avg:37.66ms
step:515/2330 train_time:19394ms step_avg:37.66ms
step:516/2330 train_time:19435ms step_avg:37.66ms
step:517/2330 train_time:19470ms step_avg:37.66ms
step:518/2330 train_time:19510ms step_avg:37.67ms
step:519/2330 train_time:19546ms step_avg:37.66ms
step:520/2330 train_time:19586ms step_avg:37.67ms
step:521/2330 train_time:19621ms step_avg:37.66ms
step:522/2330 train_time:19661ms step_avg:37.67ms
step:523/2330 train_time:19696ms step_avg:37.66ms
step:524/2330 train_time:19737ms step_avg:37.67ms
step:525/2330 train_time:19772ms step_avg:37.66ms
step:526/2330 train_time:19812ms step_avg:37.67ms
step:527/2330 train_time:19847ms step_avg:37.66ms
step:528/2330 train_time:19887ms step_avg:37.67ms
step:529/2330 train_time:19923ms step_avg:37.66ms
step:530/2330 train_time:19963ms step_avg:37.67ms
step:531/2330 train_time:19998ms step_avg:37.66ms
step:532/2330 train_time:20039ms step_avg:37.67ms
step:533/2330 train_time:20073ms step_avg:37.66ms
step:534/2330 train_time:20115ms step_avg:37.67ms
step:535/2330 train_time:20149ms step_avg:37.66ms
step:536/2330 train_time:20190ms step_avg:37.67ms
step:537/2330 train_time:20225ms step_avg:37.66ms
step:538/2330 train_time:20266ms step_avg:37.67ms
step:539/2330 train_time:20302ms step_avg:37.67ms
step:540/2330 train_time:20343ms step_avg:37.67ms
step:541/2330 train_time:20378ms step_avg:37.67ms
step:542/2330 train_time:20419ms step_avg:37.67ms
step:543/2330 train_time:20454ms step_avg:37.67ms
step:544/2330 train_time:20495ms step_avg:37.67ms
step:545/2330 train_time:20530ms step_avg:37.67ms
step:546/2330 train_time:20571ms step_avg:37.68ms
step:547/2330 train_time:20607ms step_avg:37.67ms
step:548/2330 train_time:20647ms step_avg:37.68ms
step:549/2330 train_time:20683ms step_avg:37.67ms
step:550/2330 train_time:20724ms step_avg:37.68ms
step:551/2330 train_time:20758ms step_avg:37.67ms
step:552/2330 train_time:20799ms step_avg:37.68ms
step:553/2330 train_time:20834ms step_avg:37.67ms
step:554/2330 train_time:20875ms step_avg:37.68ms
step:555/2330 train_time:20909ms step_avg:37.67ms
step:556/2330 train_time:20951ms step_avg:37.68ms
step:557/2330 train_time:20985ms step_avg:37.67ms
step:558/2330 train_time:21026ms step_avg:37.68ms
step:559/2330 train_time:21061ms step_avg:37.68ms
step:560/2330 train_time:21101ms step_avg:37.68ms
step:561/2330 train_time:21136ms step_avg:37.68ms
step:562/2330 train_time:21178ms step_avg:37.68ms
step:563/2330 train_time:21212ms step_avg:37.68ms
step:564/2330 train_time:21253ms step_avg:37.68ms
step:565/2330 train_time:21289ms step_avg:37.68ms
step:566/2330 train_time:21329ms step_avg:37.68ms
step:567/2330 train_time:21365ms step_avg:37.68ms
step:568/2330 train_time:21406ms step_avg:37.69ms
step:569/2330 train_time:21442ms step_avg:37.68ms
step:570/2330 train_time:21482ms step_avg:37.69ms
step:571/2330 train_time:21517ms step_avg:37.68ms
step:572/2330 train_time:21558ms step_avg:37.69ms
step:573/2330 train_time:21593ms step_avg:37.68ms
step:574/2330 train_time:21634ms step_avg:37.69ms
step:575/2330 train_time:21670ms step_avg:37.69ms
step:576/2330 train_time:21711ms step_avg:37.69ms
step:577/2330 train_time:21746ms step_avg:37.69ms
step:578/2330 train_time:21787ms step_avg:37.69ms
step:579/2330 train_time:21822ms step_avg:37.69ms
step:580/2330 train_time:21862ms step_avg:37.69ms
step:581/2330 train_time:21897ms step_avg:37.69ms
step:582/2330 train_time:21938ms step_avg:37.69ms
step:583/2330 train_time:21973ms step_avg:37.69ms
step:584/2330 train_time:22013ms step_avg:37.69ms
step:585/2330 train_time:22049ms step_avg:37.69ms
step:586/2330 train_time:22090ms step_avg:37.70ms
step:587/2330 train_time:22124ms step_avg:37.69ms
step:588/2330 train_time:22165ms step_avg:37.70ms
step:589/2330 train_time:22200ms step_avg:37.69ms
step:590/2330 train_time:22240ms step_avg:37.70ms
step:591/2330 train_time:22276ms step_avg:37.69ms
step:592/2330 train_time:22317ms step_avg:37.70ms
step:593/2330 train_time:22353ms step_avg:37.69ms
step:594/2330 train_time:22394ms step_avg:37.70ms
step:595/2330 train_time:22429ms step_avg:37.70ms
step:596/2330 train_time:22470ms step_avg:37.70ms
step:597/2330 train_time:22505ms step_avg:37.70ms
step:598/2330 train_time:22546ms step_avg:37.70ms
step:599/2330 train_time:22581ms step_avg:37.70ms
step:600/2330 train_time:22621ms step_avg:37.70ms
step:601/2330 train_time:22656ms step_avg:37.70ms
step:602/2330 train_time:22697ms step_avg:37.70ms
step:603/2330 train_time:22733ms step_avg:37.70ms
step:604/2330 train_time:22774ms step_avg:37.70ms
step:605/2330 train_time:22809ms step_avg:37.70ms
step:606/2330 train_time:22850ms step_avg:37.71ms
step:607/2330 train_time:22885ms step_avg:37.70ms
step:608/2330 train_time:22925ms step_avg:37.71ms
step:609/2330 train_time:22960ms step_avg:37.70ms
step:610/2330 train_time:23001ms step_avg:37.71ms
step:611/2330 train_time:23036ms step_avg:37.70ms
step:612/2330 train_time:23078ms step_avg:37.71ms
step:613/2330 train_time:23112ms step_avg:37.70ms
step:614/2330 train_time:23153ms step_avg:37.71ms
step:615/2330 train_time:23188ms step_avg:37.70ms
step:616/2330 train_time:23229ms step_avg:37.71ms
step:617/2330 train_time:23263ms step_avg:37.70ms
step:618/2330 train_time:23304ms step_avg:37.71ms
step:619/2330 train_time:23339ms step_avg:37.70ms
step:620/2330 train_time:23380ms step_avg:37.71ms
step:621/2330 train_time:23415ms step_avg:37.71ms
step:622/2330 train_time:23456ms step_avg:37.71ms
step:623/2330 train_time:23491ms step_avg:37.71ms
step:624/2330 train_time:23532ms step_avg:37.71ms
step:625/2330 train_time:23567ms step_avg:37.71ms
step:626/2330 train_time:23608ms step_avg:37.71ms
step:627/2330 train_time:23643ms step_avg:37.71ms
step:628/2330 train_time:23684ms step_avg:37.71ms
step:629/2330 train_time:23719ms step_avg:37.71ms
step:630/2330 train_time:23760ms step_avg:37.71ms
step:631/2330 train_time:23795ms step_avg:37.71ms
step:632/2330 train_time:23836ms step_avg:37.72ms
step:633/2330 train_time:23872ms step_avg:37.71ms
step:634/2330 train_time:23913ms step_avg:37.72ms
step:635/2330 train_time:23948ms step_avg:37.71ms
step:636/2330 train_time:23989ms step_avg:37.72ms
step:637/2330 train_time:24024ms step_avg:37.71ms
step:638/2330 train_time:24064ms step_avg:37.72ms
step:639/2330 train_time:24098ms step_avg:37.71ms
step:640/2330 train_time:24140ms step_avg:37.72ms
step:641/2330 train_time:24174ms step_avg:37.71ms
step:642/2330 train_time:24216ms step_avg:37.72ms
step:643/2330 train_time:24250ms step_avg:37.71ms
step:644/2330 train_time:24291ms step_avg:37.72ms
step:645/2330 train_time:24326ms step_avg:37.72ms
step:646/2330 train_time:24367ms step_avg:37.72ms
step:647/2330 train_time:24402ms step_avg:37.72ms
step:648/2330 train_time:24442ms step_avg:37.72ms
step:649/2330 train_time:24477ms step_avg:37.72ms
step:650/2330 train_time:24518ms step_avg:37.72ms
step:651/2330 train_time:24553ms step_avg:37.72ms
step:652/2330 train_time:24594ms step_avg:37.72ms
step:653/2330 train_time:24630ms step_avg:37.72ms
step:654/2330 train_time:24671ms step_avg:37.72ms
step:655/2330 train_time:24705ms step_avg:37.72ms
step:656/2330 train_time:24746ms step_avg:37.72ms
step:657/2330 train_time:24781ms step_avg:37.72ms
step:658/2330 train_time:24821ms step_avg:37.72ms
step:659/2330 train_time:24856ms step_avg:37.72ms
step:660/2330 train_time:24897ms step_avg:37.72ms
step:661/2330 train_time:24932ms step_avg:37.72ms
step:662/2330 train_time:24973ms step_avg:37.72ms
step:663/2330 train_time:25009ms step_avg:37.72ms
step:664/2330 train_time:25049ms step_avg:37.72ms
step:665/2330 train_time:25086ms step_avg:37.72ms
step:666/2330 train_time:25127ms step_avg:37.73ms
step:667/2330 train_time:25162ms step_avg:37.72ms
step:668/2330 train_time:25202ms step_avg:37.73ms
step:669/2330 train_time:25237ms step_avg:37.72ms
step:670/2330 train_time:25278ms step_avg:37.73ms
step:671/2330 train_time:25313ms step_avg:37.72ms
step:672/2330 train_time:25354ms step_avg:37.73ms
step:673/2330 train_time:25389ms step_avg:37.73ms
step:674/2330 train_time:25431ms step_avg:37.73ms
step:675/2330 train_time:25466ms step_avg:37.73ms
step:676/2330 train_time:25507ms step_avg:37.73ms
step:677/2330 train_time:25542ms step_avg:37.73ms
step:678/2330 train_time:25583ms step_avg:37.73ms
step:679/2330 train_time:25619ms step_avg:37.73ms
step:680/2330 train_time:25660ms step_avg:37.73ms
step:681/2330 train_time:25694ms step_avg:37.73ms
step:682/2330 train_time:25736ms step_avg:37.74ms
step:683/2330 train_time:25771ms step_avg:37.73ms
step:684/2330 train_time:25812ms step_avg:37.74ms
step:685/2330 train_time:25847ms step_avg:37.73ms
step:686/2330 train_time:25888ms step_avg:37.74ms
step:687/2330 train_time:25923ms step_avg:37.73ms
step:688/2330 train_time:25964ms step_avg:37.74ms
step:689/2330 train_time:25999ms step_avg:37.73ms
step:690/2330 train_time:26040ms step_avg:37.74ms
step:691/2330 train_time:26075ms step_avg:37.74ms
step:692/2330 train_time:26116ms step_avg:37.74ms
step:693/2330 train_time:26152ms step_avg:37.74ms
step:694/2330 train_time:26192ms step_avg:37.74ms
step:695/2330 train_time:26227ms step_avg:37.74ms
step:696/2330 train_time:26268ms step_avg:37.74ms
step:697/2330 train_time:26303ms step_avg:37.74ms
step:698/2330 train_time:26344ms step_avg:37.74ms
step:699/2330 train_time:26379ms step_avg:37.74ms
step:700/2330 train_time:26420ms step_avg:37.74ms
step:701/2330 train_time:26455ms step_avg:37.74ms
step:702/2330 train_time:26496ms step_avg:37.74ms
step:703/2330 train_time:26531ms step_avg:37.74ms
step:704/2330 train_time:26572ms step_avg:37.74ms
step:705/2330 train_time:26607ms step_avg:37.74ms
step:706/2330 train_time:26648ms step_avg:37.74ms
step:707/2330 train_time:26683ms step_avg:37.74ms
step:708/2330 train_time:26724ms step_avg:37.75ms
step:709/2330 train_time:26758ms step_avg:37.74ms
step:710/2330 train_time:26799ms step_avg:37.75ms
step:711/2330 train_time:26834ms step_avg:37.74ms
step:712/2330 train_time:26875ms step_avg:37.75ms
step:713/2330 train_time:26910ms step_avg:37.74ms
step:714/2330 train_time:26951ms step_avg:37.75ms
step:715/2330 train_time:26987ms step_avg:37.74ms
step:716/2330 train_time:27028ms step_avg:37.75ms
step:717/2330 train_time:27064ms step_avg:37.75ms
step:718/2330 train_time:27104ms step_avg:37.75ms
step:719/2330 train_time:27139ms step_avg:37.75ms
step:720/2330 train_time:27180ms step_avg:37.75ms
step:721/2330 train_time:27214ms step_avg:37.74ms
step:722/2330 train_time:27255ms step_avg:37.75ms
step:723/2330 train_time:27290ms step_avg:37.75ms
step:724/2330 train_time:27331ms step_avg:37.75ms
step:725/2330 train_time:27367ms step_avg:37.75ms
step:726/2330 train_time:27407ms step_avg:37.75ms
step:727/2330 train_time:27443ms step_avg:37.75ms
step:728/2330 train_time:27483ms step_avg:37.75ms
step:729/2330 train_time:27518ms step_avg:37.75ms
step:730/2330 train_time:27559ms step_avg:37.75ms
step:731/2330 train_time:27595ms step_avg:37.75ms
step:732/2330 train_time:27636ms step_avg:37.75ms
step:733/2330 train_time:27671ms step_avg:37.75ms
step:734/2330 train_time:27711ms step_avg:37.75ms
step:735/2330 train_time:27746ms step_avg:37.75ms
step:736/2330 train_time:27787ms step_avg:37.75ms
step:737/2330 train_time:27823ms step_avg:37.75ms
step:738/2330 train_time:27863ms step_avg:37.75ms
step:739/2330 train_time:27898ms step_avg:37.75ms
step:740/2330 train_time:27939ms step_avg:37.76ms
step:741/2330 train_time:27973ms step_avg:37.75ms
step:742/2330 train_time:28014ms step_avg:37.75ms
step:743/2330 train_time:28048ms step_avg:37.75ms
step:744/2330 train_time:28089ms step_avg:37.75ms
step:745/2330 train_time:28124ms step_avg:37.75ms
step:746/2330 train_time:28164ms step_avg:37.75ms
step:747/2330 train_time:28199ms step_avg:37.75ms
step:748/2330 train_time:28240ms step_avg:37.75ms
step:749/2330 train_time:28275ms step_avg:37.75ms
step:750/2330 train_time:28316ms step_avg:37.75ms
step:750/2330 val_loss:5.4843 train_time:28427ms step_avg:37.90ms
step:751/2330 train_time:28438ms step_avg:37.87ms
step:752/2330 train_time:28448ms step_avg:37.83ms
step:753/2330 train_time:28457ms step_avg:37.79ms
step:754/2330 train_time:28470ms step_avg:37.76ms
step:755/2330 train_time:28504ms step_avg:37.75ms
step:756/2330 train_time:28544ms step_avg:37.76ms
step:757/2330 train_time:28579ms step_avg:37.75ms
step:758/2330 train_time:28619ms step_avg:37.76ms
step:759/2330 train_time:28653ms step_avg:37.75ms
step:760/2330 train_time:28694ms step_avg:37.76ms
step:761/2330 train_time:28729ms step_avg:37.75ms
step:762/2330 train_time:28770ms step_avg:37.76ms
step:763/2330 train_time:28808ms step_avg:37.76ms
step:764/2330 train_time:28849ms step_avg:37.76ms
step:765/2330 train_time:28885ms step_avg:37.76ms
step:766/2330 train_time:28927ms step_avg:37.76ms
step:767/2330 train_time:28961ms step_avg:37.76ms
step:768/2330 train_time:29001ms step_avg:37.76ms
step:769/2330 train_time:29036ms step_avg:37.76ms
step:770/2330 train_time:29076ms step_avg:37.76ms
step:771/2330 train_time:29111ms step_avg:37.76ms
step:772/2330 train_time:29152ms step_avg:37.76ms
step:773/2330 train_time:29186ms step_avg:37.76ms
step:774/2330 train_time:29227ms step_avg:37.76ms
step:775/2330 train_time:29261ms step_avg:37.76ms
step:776/2330 train_time:29302ms step_avg:37.76ms
step:777/2330 train_time:29337ms step_avg:37.76ms
step:778/2330 train_time:29377ms step_avg:37.76ms
step:779/2330 train_time:29413ms step_avg:37.76ms
step:780/2330 train_time:29455ms step_avg:37.76ms
step:781/2330 train_time:29489ms step_avg:37.76ms
step:782/2330 train_time:29530ms step_avg:37.76ms
step:783/2330 train_time:29565ms step_avg:37.76ms
step:784/2330 train_time:29605ms step_avg:37.76ms
step:785/2330 train_time:29640ms step_avg:37.76ms
step:786/2330 train_time:29680ms step_avg:37.76ms
step:787/2330 train_time:29716ms step_avg:37.76ms
step:788/2330 train_time:29757ms step_avg:37.76ms
step:789/2330 train_time:29793ms step_avg:37.76ms
step:790/2330 train_time:29833ms step_avg:37.76ms
step:791/2330 train_time:29869ms step_avg:37.76ms
step:792/2330 train_time:29911ms step_avg:37.77ms
step:793/2330 train_time:29946ms step_avg:37.76ms
step:794/2330 train_time:29986ms step_avg:37.77ms
step:795/2330 train_time:30022ms step_avg:37.76ms
step:796/2330 train_time:30062ms step_avg:37.77ms
step:797/2330 train_time:30097ms step_avg:37.76ms
step:798/2330 train_time:30138ms step_avg:37.77ms
step:799/2330 train_time:30172ms step_avg:37.76ms
step:800/2330 train_time:30213ms step_avg:37.77ms
step:801/2330 train_time:30248ms step_avg:37.76ms
step:802/2330 train_time:30288ms step_avg:37.77ms
step:803/2330 train_time:30324ms step_avg:37.76ms
step:804/2330 train_time:30365ms step_avg:37.77ms
step:805/2330 train_time:30399ms step_avg:37.76ms
step:806/2330 train_time:30440ms step_avg:37.77ms
step:807/2330 train_time:30474ms step_avg:37.76ms
step:808/2330 train_time:30515ms step_avg:37.77ms
step:809/2330 train_time:30549ms step_avg:37.76ms
step:810/2330 train_time:30590ms step_avg:37.77ms
step:811/2330 train_time:30625ms step_avg:37.76ms
step:812/2330 train_time:30665ms step_avg:37.77ms
step:813/2330 train_time:30700ms step_avg:37.76ms
step:814/2330 train_time:30741ms step_avg:37.77ms
step:815/2330 train_time:30775ms step_avg:37.76ms
step:816/2330 train_time:30816ms step_avg:37.77ms
step:817/2330 train_time:30851ms step_avg:37.76ms
step:818/2330 train_time:30892ms step_avg:37.77ms
step:819/2330 train_time:30928ms step_avg:37.76ms
step:820/2330 train_time:30969ms step_avg:37.77ms
step:821/2330 train_time:31004ms step_avg:37.76ms
step:822/2330 train_time:31044ms step_avg:37.77ms
step:823/2330 train_time:31079ms step_avg:37.76ms
step:824/2330 train_time:31119ms step_avg:37.77ms
step:825/2330 train_time:31154ms step_avg:37.76ms
step:826/2330 train_time:31195ms step_avg:37.77ms
step:827/2330 train_time:31230ms step_avg:37.76ms
step:828/2330 train_time:31271ms step_avg:37.77ms
step:829/2330 train_time:31306ms step_avg:37.76ms
step:830/2330 train_time:31347ms step_avg:37.77ms
step:831/2330 train_time:31382ms step_avg:37.76ms
step:832/2330 train_time:31423ms step_avg:37.77ms
step:833/2330 train_time:31458ms step_avg:37.76ms
step:834/2330 train_time:31498ms step_avg:37.77ms
step:835/2330 train_time:31533ms step_avg:37.76ms
step:836/2330 train_time:31574ms step_avg:37.77ms
step:837/2330 train_time:31609ms step_avg:37.76ms
step:838/2330 train_time:31650ms step_avg:37.77ms
step:839/2330 train_time:31685ms step_avg:37.77ms
step:840/2330 train_time:31726ms step_avg:37.77ms
step:841/2330 train_time:31762ms step_avg:37.77ms
step:842/2330 train_time:31802ms step_avg:37.77ms
step:843/2330 train_time:31838ms step_avg:37.77ms
step:844/2330 train_time:31878ms step_avg:37.77ms
step:845/2330 train_time:31913ms step_avg:37.77ms
step:846/2330 train_time:31954ms step_avg:37.77ms
step:847/2330 train_time:31989ms step_avg:37.77ms
step:848/2330 train_time:32029ms step_avg:37.77ms
step:849/2330 train_time:32065ms step_avg:37.77ms
step:850/2330 train_time:32106ms step_avg:37.77ms
step:851/2330 train_time:32140ms step_avg:37.77ms
step:852/2330 train_time:32181ms step_avg:37.77ms
step:853/2330 train_time:32216ms step_avg:37.77ms
step:854/2330 train_time:32257ms step_avg:37.77ms
step:855/2330 train_time:32291ms step_avg:37.77ms
step:856/2330 train_time:32332ms step_avg:37.77ms
step:857/2330 train_time:32367ms step_avg:37.77ms
step:858/2330 train_time:32408ms step_avg:37.77ms
step:859/2330 train_time:32443ms step_avg:37.77ms
step:860/2330 train_time:32484ms step_avg:37.77ms
step:861/2330 train_time:32518ms step_avg:37.77ms
step:862/2330 train_time:32559ms step_avg:37.77ms
step:863/2330 train_time:32594ms step_avg:37.77ms
step:864/2330 train_time:32635ms step_avg:37.77ms
step:865/2330 train_time:32669ms step_avg:37.77ms
step:866/2330 train_time:32710ms step_avg:37.77ms
step:867/2330 train_time:32746ms step_avg:37.77ms
step:868/2330 train_time:32786ms step_avg:37.77ms
step:869/2330 train_time:32822ms step_avg:37.77ms
step:870/2330 train_time:32862ms step_avg:37.77ms
step:871/2330 train_time:32898ms step_avg:37.77ms
step:872/2330 train_time:32939ms step_avg:37.77ms
step:873/2330 train_time:32974ms step_avg:37.77ms
step:874/2330 train_time:33015ms step_avg:37.77ms
step:875/2330 train_time:33049ms step_avg:37.77ms
step:876/2330 train_time:33090ms step_avg:37.77ms
step:877/2330 train_time:33125ms step_avg:37.77ms
step:878/2330 train_time:33165ms step_avg:37.77ms
step:879/2330 train_time:33200ms step_avg:37.77ms
step:880/2330 train_time:33240ms step_avg:37.77ms
step:881/2330 train_time:33275ms step_avg:37.77ms
step:882/2330 train_time:33316ms step_avg:37.77ms
step:883/2330 train_time:33351ms step_avg:37.77ms
step:884/2330 train_time:33392ms step_avg:37.77ms
step:885/2330 train_time:33428ms step_avg:37.77ms
step:886/2330 train_time:33468ms step_avg:37.77ms
step:887/2330 train_time:33504ms step_avg:37.77ms
step:888/2330 train_time:33545ms step_avg:37.78ms
step:889/2330 train_time:33580ms step_avg:37.77ms
step:890/2330 train_time:33621ms step_avg:37.78ms
step:891/2330 train_time:33655ms step_avg:37.77ms
step:892/2330 train_time:33696ms step_avg:37.78ms
step:893/2330 train_time:33731ms step_avg:37.77ms
step:894/2330 train_time:33772ms step_avg:37.78ms
step:895/2330 train_time:33807ms step_avg:37.77ms
step:896/2330 train_time:33848ms step_avg:37.78ms
step:897/2330 train_time:33884ms step_avg:37.77ms
step:898/2330 train_time:33924ms step_avg:37.78ms
step:899/2330 train_time:33959ms step_avg:37.77ms
step:900/2330 train_time:34000ms step_avg:37.78ms
step:901/2330 train_time:34035ms step_avg:37.78ms
step:902/2330 train_time:34076ms step_avg:37.78ms
step:903/2330 train_time:34111ms step_avg:37.78ms
step:904/2330 train_time:34152ms step_avg:37.78ms
step:905/2330 train_time:34187ms step_avg:37.78ms
step:906/2330 train_time:34227ms step_avg:37.78ms
step:907/2330 train_time:34263ms step_avg:37.78ms
step:908/2330 train_time:34303ms step_avg:37.78ms
step:909/2330 train_time:34338ms step_avg:37.78ms
step:910/2330 train_time:34379ms step_avg:37.78ms
step:911/2330 train_time:34413ms step_avg:37.78ms
step:912/2330 train_time:34454ms step_avg:37.78ms
step:913/2330 train_time:34490ms step_avg:37.78ms
step:914/2330 train_time:34531ms step_avg:37.78ms
step:915/2330 train_time:34567ms step_avg:37.78ms
step:916/2330 train_time:34607ms step_avg:37.78ms
step:917/2330 train_time:34643ms step_avg:37.78ms
step:918/2330 train_time:34684ms step_avg:37.78ms
step:919/2330 train_time:34718ms step_avg:37.78ms
step:920/2330 train_time:34759ms step_avg:37.78ms
step:921/2330 train_time:34794ms step_avg:37.78ms
step:922/2330 train_time:34834ms step_avg:37.78ms
step:923/2330 train_time:34871ms step_avg:37.78ms
step:924/2330 train_time:34911ms step_avg:37.78ms
step:925/2330 train_time:34946ms step_avg:37.78ms
step:926/2330 train_time:34987ms step_avg:37.78ms
step:927/2330 train_time:35022ms step_avg:37.78ms
step:928/2330 train_time:35063ms step_avg:37.78ms
step:929/2330 train_time:35098ms step_avg:37.78ms
step:930/2330 train_time:35138ms step_avg:37.78ms
step:931/2330 train_time:35173ms step_avg:37.78ms
step:932/2330 train_time:35213ms step_avg:37.78ms
step:933/2330 train_time:35248ms step_avg:37.78ms
step:934/2330 train_time:35289ms step_avg:37.78ms
step:935/2330 train_time:35324ms step_avg:37.78ms
step:936/2330 train_time:35365ms step_avg:37.78ms
step:937/2330 train_time:35401ms step_avg:37.78ms
step:938/2330 train_time:35441ms step_avg:37.78ms
step:939/2330 train_time:35477ms step_avg:37.78ms
step:940/2330 train_time:35517ms step_avg:37.78ms
step:941/2330 train_time:35552ms step_avg:37.78ms
step:942/2330 train_time:35593ms step_avg:37.78ms
step:943/2330 train_time:35628ms step_avg:37.78ms
step:944/2330 train_time:35669ms step_avg:37.78ms
step:945/2330 train_time:35704ms step_avg:37.78ms
step:946/2330 train_time:35745ms step_avg:37.79ms
step:947/2330 train_time:35781ms step_avg:37.78ms
step:948/2330 train_time:35821ms step_avg:37.79ms
step:949/2330 train_time:35857ms step_avg:37.78ms
step:950/2330 train_time:35898ms step_avg:37.79ms
step:951/2330 train_time:35933ms step_avg:37.78ms
step:952/2330 train_time:35974ms step_avg:37.79ms
step:953/2330 train_time:36010ms step_avg:37.79ms
step:954/2330 train_time:36050ms step_avg:37.79ms
step:955/2330 train_time:36085ms step_avg:37.79ms
step:956/2330 train_time:36126ms step_avg:37.79ms
step:957/2330 train_time:36161ms step_avg:37.79ms
step:958/2330 train_time:36202ms step_avg:37.79ms
step:959/2330 train_time:36236ms step_avg:37.79ms
step:960/2330 train_time:36277ms step_avg:37.79ms
step:961/2330 train_time:36313ms step_avg:37.79ms
step:962/2330 train_time:36355ms step_avg:37.79ms
step:963/2330 train_time:36389ms step_avg:37.79ms
step:964/2330 train_time:36430ms step_avg:37.79ms
step:965/2330 train_time:36465ms step_avg:37.79ms
step:966/2330 train_time:36505ms step_avg:37.79ms
step:967/2330 train_time:36540ms step_avg:37.79ms
step:968/2330 train_time:36580ms step_avg:37.79ms
step:969/2330 train_time:36615ms step_avg:37.79ms
step:970/2330 train_time:36656ms step_avg:37.79ms
step:971/2330 train_time:36691ms step_avg:37.79ms
step:972/2330 train_time:36731ms step_avg:37.79ms
step:973/2330 train_time:36767ms step_avg:37.79ms
step:974/2330 train_time:36808ms step_avg:37.79ms
step:975/2330 train_time:36844ms step_avg:37.79ms
step:976/2330 train_time:36884ms step_avg:37.79ms
step:977/2330 train_time:36920ms step_avg:37.79ms
step:978/2330 train_time:36961ms step_avg:37.79ms
step:979/2330 train_time:36996ms step_avg:37.79ms
step:980/2330 train_time:37037ms step_avg:37.79ms
step:981/2330 train_time:37072ms step_avg:37.79ms
step:982/2330 train_time:37113ms step_avg:37.79ms
step:983/2330 train_time:37148ms step_avg:37.79ms
step:984/2330 train_time:37189ms step_avg:37.79ms
step:985/2330 train_time:37224ms step_avg:37.79ms
step:986/2330 train_time:37265ms step_avg:37.79ms
step:987/2330 train_time:37300ms step_avg:37.79ms
step:988/2330 train_time:37341ms step_avg:37.79ms
step:989/2330 train_time:37376ms step_avg:37.79ms
step:990/2330 train_time:37417ms step_avg:37.79ms
step:991/2330 train_time:37451ms step_avg:37.79ms
step:992/2330 train_time:37492ms step_avg:37.79ms
step:993/2330 train_time:37526ms step_avg:37.79ms
step:994/2330 train_time:37567ms step_avg:37.79ms
step:995/2330 train_time:37601ms step_avg:37.79ms
step:996/2330 train_time:37642ms step_avg:37.79ms
step:997/2330 train_time:37676ms step_avg:37.79ms
step:998/2330 train_time:37717ms step_avg:37.79ms
step:999/2330 train_time:37753ms step_avg:37.79ms
step:1000/2330 train_time:37793ms step_avg:37.79ms
step:1000/2330 val_loss:5.4025 train_time:37905ms step_avg:37.90ms
step:1001/2330 train_time:37916ms step_avg:37.88ms
step:1002/2330 train_time:37926ms step_avg:37.85ms
step:1003/2330 train_time:37935ms step_avg:37.82ms
step:1004/2330 train_time:37947ms step_avg:37.80ms
step:1005/2330 train_time:37981ms step_avg:37.79ms
step:1006/2330 train_time:38022ms step_avg:37.79ms
step:1007/2330 train_time:38056ms step_avg:37.79ms
step:1008/2330 train_time:38096ms step_avg:37.79ms
step:1009/2330 train_time:38130ms step_avg:37.79ms
step:1010/2330 train_time:38171ms step_avg:37.79ms
step:1011/2330 train_time:38207ms step_avg:37.79ms
step:1012/2330 train_time:38248ms step_avg:37.79ms
step:1013/2330 train_time:38288ms step_avg:37.80ms
step:1014/2330 train_time:38328ms step_avg:37.80ms
step:1015/2330 train_time:38365ms step_avg:37.80ms
step:1016/2330 train_time:38405ms step_avg:37.80ms
step:1017/2330 train_time:38441ms step_avg:37.80ms
step:1018/2330 train_time:38482ms step_avg:37.80ms
step:1019/2330 train_time:38516ms step_avg:37.80ms
step:1020/2330 train_time:38556ms step_avg:37.80ms
step:1021/2330 train_time:38591ms step_avg:37.80ms
step:1022/2330 train_time:38632ms step_avg:37.80ms
step:1023/2330 train_time:38666ms step_avg:37.80ms
step:1024/2330 train_time:38706ms step_avg:37.80ms
step:1025/2330 train_time:38741ms step_avg:37.80ms
step:1026/2330 train_time:38782ms step_avg:37.80ms
step:1027/2330 train_time:38818ms step_avg:37.80ms
step:1028/2330 train_time:38859ms step_avg:37.80ms
step:1029/2330 train_time:38896ms step_avg:37.80ms
step:1030/2330 train_time:38937ms step_avg:37.80ms
step:1031/2330 train_time:38972ms step_avg:37.80ms
step:1032/2330 train_time:39013ms step_avg:37.80ms
step:1033/2330 train_time:39048ms step_avg:37.80ms
step:1034/2330 train_time:39088ms step_avg:37.80ms
step:1035/2330 train_time:39123ms step_avg:37.80ms
step:1036/2330 train_time:39164ms step_avg:37.80ms
step:1037/2330 train_time:39201ms step_avg:37.80ms
step:1038/2330 train_time:39241ms step_avg:37.80ms
step:1039/2330 train_time:39278ms step_avg:37.80ms
step:1040/2330 train_time:39318ms step_avg:37.81ms
step:1041/2330 train_time:39353ms step_avg:37.80ms
step:1042/2330 train_time:39394ms step_avg:37.81ms
step:1043/2330 train_time:39429ms step_avg:37.80ms
step:1044/2330 train_time:39469ms step_avg:37.81ms
step:1045/2330 train_time:39504ms step_avg:37.80ms
step:1046/2330 train_time:39545ms step_avg:37.81ms
step:1047/2330 train_time:39580ms step_avg:37.80ms
step:1048/2330 train_time:39620ms step_avg:37.81ms
step:1049/2330 train_time:39654ms step_avg:37.80ms
step:1050/2330 train_time:39695ms step_avg:37.80ms
step:1051/2330 train_time:39730ms step_avg:37.80ms
step:1052/2330 train_time:39771ms step_avg:37.80ms
step:1053/2330 train_time:39806ms step_avg:37.80ms
step:1054/2330 train_time:39846ms step_avg:37.80ms
step:1055/2330 train_time:39883ms step_avg:37.80ms
step:1056/2330 train_time:39924ms step_avg:37.81ms
step:1057/2330 train_time:39959ms step_avg:37.80ms
step:1058/2330 train_time:40000ms step_avg:37.81ms
step:1059/2330 train_time:40034ms step_avg:37.80ms
step:1060/2330 train_time:40075ms step_avg:37.81ms
step:1061/2330 train_time:40111ms step_avg:37.80ms
step:1062/2330 train_time:40152ms step_avg:37.81ms
step:1063/2330 train_time:40188ms step_avg:37.81ms
step:1064/2330 train_time:40229ms step_avg:37.81ms
step:1065/2330 train_time:40264ms step_avg:37.81ms
step:1066/2330 train_time:40305ms step_avg:37.81ms
step:1067/2330 train_time:40341ms step_avg:37.81ms
step:1068/2330 train_time:40382ms step_avg:37.81ms
step:1069/2330 train_time:40417ms step_avg:37.81ms
step:1070/2330 train_time:40457ms step_avg:37.81ms
step:1071/2330 train_time:40492ms step_avg:37.81ms
step:1072/2330 train_time:40533ms step_avg:37.81ms
step:1073/2330 train_time:40567ms step_avg:37.81ms
step:1074/2330 train_time:40608ms step_avg:37.81ms
step:1075/2330 train_time:40643ms step_avg:37.81ms
step:1076/2330 train_time:40684ms step_avg:37.81ms
step:1077/2330 train_time:40719ms step_avg:37.81ms
step:1078/2330 train_time:40759ms step_avg:37.81ms
step:1079/2330 train_time:40795ms step_avg:37.81ms
step:1080/2330 train_time:40836ms step_avg:37.81ms
step:1081/2330 train_time:40871ms step_avg:37.81ms
step:1082/2330 train_time:40912ms step_avg:37.81ms
step:1083/2330 train_time:40948ms step_avg:37.81ms
step:1084/2330 train_time:40989ms step_avg:37.81ms
step:1085/2330 train_time:41024ms step_avg:37.81ms
step:1086/2330 train_time:41064ms step_avg:37.81ms
step:1087/2330 train_time:41099ms step_avg:37.81ms
step:1088/2330 train_time:41140ms step_avg:37.81ms
step:1089/2330 train_time:41175ms step_avg:37.81ms
step:1090/2330 train_time:41216ms step_avg:37.81ms
step:1091/2330 train_time:41252ms step_avg:37.81ms
step:1092/2330 train_time:41293ms step_avg:37.81ms
step:1093/2330 train_time:41327ms step_avg:37.81ms
step:1094/2330 train_time:41368ms step_avg:37.81ms
step:1095/2330 train_time:41405ms step_avg:37.81ms
step:1096/2330 train_time:41446ms step_avg:37.82ms
step:1097/2330 train_time:41481ms step_avg:37.81ms
step:1098/2330 train_time:41522ms step_avg:37.82ms
step:1099/2330 train_time:41557ms step_avg:37.81ms
step:1100/2330 train_time:41597ms step_avg:37.82ms
step:1101/2330 train_time:41632ms step_avg:37.81ms
step:1102/2330 train_time:41673ms step_avg:37.82ms
step:1103/2330 train_time:41708ms step_avg:37.81ms
step:1104/2330 train_time:41749ms step_avg:37.82ms
step:1105/2330 train_time:41785ms step_avg:37.81ms
step:1106/2330 train_time:41825ms step_avg:37.82ms
step:1107/2330 train_time:41860ms step_avg:37.81ms
step:1108/2330 train_time:41900ms step_avg:37.82ms
step:1109/2330 train_time:41935ms step_avg:37.81ms
step:1110/2330 train_time:41976ms step_avg:37.82ms
step:1111/2330 train_time:42011ms step_avg:37.81ms
step:1112/2330 train_time:42052ms step_avg:37.82ms
step:1113/2330 train_time:42088ms step_avg:37.82ms
step:1114/2330 train_time:42129ms step_avg:37.82ms
step:1115/2330 train_time:42165ms step_avg:37.82ms
step:1116/2330 train_time:42206ms step_avg:37.82ms
step:1117/2330 train_time:42242ms step_avg:37.82ms
step:1118/2330 train_time:42282ms step_avg:37.82ms
step:1119/2330 train_time:42318ms step_avg:37.82ms
step:1120/2330 train_time:42358ms step_avg:37.82ms
step:1121/2330 train_time:42393ms step_avg:37.82ms
step:1122/2330 train_time:42434ms step_avg:37.82ms
step:1123/2330 train_time:42470ms step_avg:37.82ms
step:1124/2330 train_time:42511ms step_avg:37.82ms
step:1125/2330 train_time:42546ms step_avg:37.82ms
step:1126/2330 train_time:42588ms step_avg:37.82ms
step:1127/2330 train_time:42623ms step_avg:37.82ms
step:1128/2330 train_time:42664ms step_avg:37.82ms
step:1129/2330 train_time:42700ms step_avg:37.82ms
step:1130/2330 train_time:42740ms step_avg:37.82ms
step:1131/2330 train_time:42776ms step_avg:37.82ms
step:1132/2330 train_time:42817ms step_avg:37.82ms
step:1133/2330 train_time:42852ms step_avg:37.82ms
step:1134/2330 train_time:42893ms step_avg:37.82ms
step:1135/2330 train_time:42928ms step_avg:37.82ms
step:1136/2330 train_time:42969ms step_avg:37.82ms
step:1137/2330 train_time:43004ms step_avg:37.82ms
step:1138/2330 train_time:43045ms step_avg:37.82ms
step:1139/2330 train_time:43080ms step_avg:37.82ms
step:1140/2330 train_time:43120ms step_avg:37.82ms
step:1141/2330 train_time:43155ms step_avg:37.82ms
step:1142/2330 train_time:43196ms step_avg:37.82ms
step:1143/2330 train_time:43230ms step_avg:37.82ms
step:1144/2330 train_time:43271ms step_avg:37.82ms
step:1145/2330 train_time:43307ms step_avg:37.82ms
step:1146/2330 train_time:43348ms step_avg:37.83ms
step:1147/2330 train_time:43383ms step_avg:37.82ms
step:1148/2330 train_time:43424ms step_avg:37.83ms
step:1149/2330 train_time:43460ms step_avg:37.82ms
step:1150/2330 train_time:43501ms step_avg:37.83ms
step:1151/2330 train_time:43537ms step_avg:37.83ms
step:1152/2330 train_time:43577ms step_avg:37.83ms
step:1153/2330 train_time:43612ms step_avg:37.82ms
step:1154/2330 train_time:43653ms step_avg:37.83ms
step:1155/2330 train_time:43688ms step_avg:37.82ms
step:1156/2330 train_time:43729ms step_avg:37.83ms
step:1157/2330 train_time:43764ms step_avg:37.83ms
step:1158/2330 train_time:43805ms step_avg:37.83ms
step:1159/2330 train_time:43839ms step_avg:37.82ms
step:1160/2330 train_time:43880ms step_avg:37.83ms
step:1161/2330 train_time:43914ms step_avg:37.82ms
step:1162/2330 train_time:43955ms step_avg:37.83ms
step:1163/2330 train_time:43990ms step_avg:37.82ms
step:1164/2330 train_time:44032ms step_avg:37.83ms
step:1165/2330 train_time:44067ms step_avg:37.83ms
step:1166/2330 train_time:44108ms step_avg:37.83ms
step:1167/2330 train_time:44143ms step_avg:37.83ms
step:1168/2330 train_time:44184ms step_avg:37.83ms
step:1169/2330 train_time:44219ms step_avg:37.83ms
step:1170/2330 train_time:44260ms step_avg:37.83ms
step:1171/2330 train_time:44295ms step_avg:37.83ms
step:1172/2330 train_time:44336ms step_avg:37.83ms
step:1173/2330 train_time:44371ms step_avg:37.83ms
step:1174/2330 train_time:44412ms step_avg:37.83ms
step:1175/2330 train_time:44447ms step_avg:37.83ms
step:1176/2330 train_time:44488ms step_avg:37.83ms
step:1177/2330 train_time:44524ms step_avg:37.83ms
step:1178/2330 train_time:44565ms step_avg:37.83ms
step:1179/2330 train_time:44601ms step_avg:37.83ms
step:1180/2330 train_time:44642ms step_avg:37.83ms
step:1181/2330 train_time:44677ms step_avg:37.83ms
step:1182/2330 train_time:44718ms step_avg:37.83ms
step:1183/2330 train_time:44753ms step_avg:37.83ms
step:1184/2330 train_time:44794ms step_avg:37.83ms
step:1185/2330 train_time:44828ms step_avg:37.83ms
step:1186/2330 train_time:44869ms step_avg:37.83ms
step:1187/2330 train_time:44903ms step_avg:37.83ms
step:1188/2330 train_time:44945ms step_avg:37.83ms
step:1189/2330 train_time:44979ms step_avg:37.83ms
step:1190/2330 train_time:45020ms step_avg:37.83ms
step:1191/2330 train_time:45055ms step_avg:37.83ms
step:1192/2330 train_time:45095ms step_avg:37.83ms
step:1193/2330 train_time:45130ms step_avg:37.83ms
step:1194/2330 train_time:45171ms step_avg:37.83ms
step:1195/2330 train_time:45207ms step_avg:37.83ms
step:1196/2330 train_time:45248ms step_avg:37.83ms
step:1197/2330 train_time:45284ms step_avg:37.83ms
step:1198/2330 train_time:45324ms step_avg:37.83ms
step:1199/2330 train_time:45360ms step_avg:37.83ms
step:1200/2330 train_time:45401ms step_avg:37.83ms
step:1201/2330 train_time:45437ms step_avg:37.83ms
step:1202/2330 train_time:45477ms step_avg:37.83ms
step:1203/2330 train_time:45512ms step_avg:37.83ms
step:1204/2330 train_time:45554ms step_avg:37.84ms
step:1205/2330 train_time:45589ms step_avg:37.83ms
step:1206/2330 train_time:45630ms step_avg:37.84ms
step:1207/2330 train_time:45665ms step_avg:37.83ms
step:1208/2330 train_time:45706ms step_avg:37.84ms
step:1209/2330 train_time:45742ms step_avg:37.83ms
step:1210/2330 train_time:45782ms step_avg:37.84ms
step:1211/2330 train_time:45818ms step_avg:37.84ms
step:1212/2330 train_time:45859ms step_avg:37.84ms
step:1213/2330 train_time:45894ms step_avg:37.84ms
step:1214/2330 train_time:45935ms step_avg:37.84ms
step:1215/2330 train_time:45970ms step_avg:37.84ms
step:1216/2330 train_time:46011ms step_avg:37.84ms
step:1217/2330 train_time:46046ms step_avg:37.84ms
step:1218/2330 train_time:46088ms step_avg:37.84ms
step:1219/2330 train_time:46123ms step_avg:37.84ms
step:1220/2330 train_time:46164ms step_avg:37.84ms
step:1221/2330 train_time:46199ms step_avg:37.84ms
step:1222/2330 train_time:46240ms step_avg:37.84ms
step:1223/2330 train_time:46275ms step_avg:37.84ms
step:1224/2330 train_time:46316ms step_avg:37.84ms
step:1225/2330 train_time:46351ms step_avg:37.84ms
step:1226/2330 train_time:46392ms step_avg:37.84ms
step:1227/2330 train_time:46427ms step_avg:37.84ms
step:1228/2330 train_time:46468ms step_avg:37.84ms
step:1229/2330 train_time:46503ms step_avg:37.84ms
step:1230/2330 train_time:46544ms step_avg:37.84ms
step:1231/2330 train_time:46579ms step_avg:37.84ms
step:1232/2330 train_time:46620ms step_avg:37.84ms
step:1233/2330 train_time:46655ms step_avg:37.84ms
step:1234/2330 train_time:46696ms step_avg:37.84ms
step:1235/2330 train_time:46731ms step_avg:37.84ms
step:1236/2330 train_time:46772ms step_avg:37.84ms
step:1237/2330 train_time:46808ms step_avg:37.84ms
step:1238/2330 train_time:46849ms step_avg:37.84ms
step:1239/2330 train_time:46884ms step_avg:37.84ms
step:1240/2330 train_time:46925ms step_avg:37.84ms
step:1241/2330 train_time:46960ms step_avg:37.84ms
step:1242/2330 train_time:47001ms step_avg:37.84ms
step:1243/2330 train_time:47037ms step_avg:37.84ms
step:1244/2330 train_time:47077ms step_avg:37.84ms
step:1245/2330 train_time:47112ms step_avg:37.84ms
step:1246/2330 train_time:47153ms step_avg:37.84ms
step:1247/2330 train_time:47188ms step_avg:37.84ms
step:1248/2330 train_time:47229ms step_avg:37.84ms
step:1249/2330 train_time:47264ms step_avg:37.84ms
step:1250/2330 train_time:47305ms step_avg:37.84ms
step:1250/2330 val_loss:5.3561 train_time:47416ms step_avg:37.93ms
step:1251/2330 train_time:47428ms step_avg:37.91ms
step:1252/2330 train_time:47440ms step_avg:37.89ms
step:1253/2330 train_time:47449ms step_avg:37.87ms
step:1254/2330 train_time:47461ms step_avg:37.85ms
step:1255/2330 train_time:47494ms step_avg:37.84ms
step:1256/2330 train_time:47535ms step_avg:37.85ms
step:1257/2330 train_time:47569ms step_avg:37.84ms
step:1258/2330 train_time:47610ms step_avg:37.85ms
step:1259/2330 train_time:47644ms step_avg:37.84ms
step:1260/2330 train_time:47685ms step_avg:37.84ms
step:1261/2330 train_time:47720ms step_avg:37.84ms
step:1262/2330 train_time:47761ms step_avg:37.85ms
step:1263/2330 train_time:47801ms step_avg:37.85ms
step:1264/2330 train_time:47843ms step_avg:37.85ms
step:1265/2330 train_time:47879ms step_avg:37.85ms
step:1266/2330 train_time:47919ms step_avg:37.85ms
step:1267/2330 train_time:47955ms step_avg:37.85ms
step:1268/2330 train_time:47995ms step_avg:37.85ms
step:1269/2330 train_time:48031ms step_avg:37.85ms
step:1270/2330 train_time:48071ms step_avg:37.85ms
step:1271/2330 train_time:48106ms step_avg:37.85ms
step:1272/2330 train_time:48146ms step_avg:37.85ms
step:1273/2330 train_time:48180ms step_avg:37.85ms
step:1274/2330 train_time:48221ms step_avg:37.85ms
step:1275/2330 train_time:48256ms step_avg:37.85ms
step:1276/2330 train_time:48297ms step_avg:37.85ms
step:1277/2330 train_time:48331ms step_avg:37.85ms
step:1278/2330 train_time:48372ms step_avg:37.85ms
step:1279/2330 train_time:48408ms step_avg:37.85ms
step:1280/2330 train_time:48449ms step_avg:37.85ms
step:1281/2330 train_time:48484ms step_avg:37.85ms
step:1282/2330 train_time:48525ms step_avg:37.85ms
step:1283/2330 train_time:48559ms step_avg:37.85ms
step:1284/2330 train_time:48600ms step_avg:37.85ms
step:1285/2330 train_time:48635ms step_avg:37.85ms
step:1286/2330 train_time:48675ms step_avg:37.85ms
step:1287/2330 train_time:48713ms step_avg:37.85ms
step:1288/2330 train_time:48754ms step_avg:37.85ms
step:1289/2330 train_time:48790ms step_avg:37.85ms
step:1290/2330 train_time:48831ms step_avg:37.85ms
step:1291/2330 train_time:48867ms step_avg:37.85ms
step:1292/2330 train_time:48908ms step_avg:37.85ms
step:1293/2330 train_time:48944ms step_avg:37.85ms
step:1294/2330 train_time:48984ms step_avg:37.86ms
step:1295/2330 train_time:49020ms step_avg:37.85ms
step:1296/2330 train_time:49061ms step_avg:37.86ms
step:1297/2330 train_time:49096ms step_avg:37.85ms
step:1298/2330 train_time:49136ms step_avg:37.86ms
step:1299/2330 train_time:49171ms step_avg:37.85ms
step:1300/2330 train_time:49212ms step_avg:37.86ms
step:1301/2330 train_time:49246ms step_avg:37.85ms
step:1302/2330 train_time:49287ms step_avg:37.85ms
step:1303/2330 train_time:49322ms step_avg:37.85ms
step:1304/2330 train_time:49363ms step_avg:37.85ms
step:1305/2330 train_time:49398ms step_avg:37.85ms
step:1306/2330 train_time:49439ms step_avg:37.86ms
step:1307/2330 train_time:49474ms step_avg:37.85ms
step:1308/2330 train_time:49515ms step_avg:37.86ms
step:1309/2330 train_time:49549ms step_avg:37.85ms
step:1310/2330 train_time:49590ms step_avg:37.85ms
step:1311/2330 train_time:49624ms step_avg:37.85ms
step:1312/2330 train_time:49665ms step_avg:37.85ms
step:1313/2330 train_time:49699ms step_avg:37.85ms
step:1314/2330 train_time:49740ms step_avg:37.85ms
step:1315/2330 train_time:49776ms step_avg:37.85ms
step:1316/2330 train_time:49817ms step_avg:37.85ms
step:1317/2330 train_time:49853ms step_avg:37.85ms
step:1318/2330 train_time:49895ms step_avg:37.86ms
step:1319/2330 train_time:49930ms step_avg:37.85ms
step:1320/2330 train_time:49971ms step_avg:37.86ms
step:1321/2330 train_time:50006ms step_avg:37.85ms
step:1322/2330 train_time:50047ms step_avg:37.86ms
step:1323/2330 train_time:50082ms step_avg:37.85ms
step:1324/2330 train_time:50123ms step_avg:37.86ms
step:1325/2330 train_time:50157ms step_avg:37.85ms
step:1326/2330 train_time:50198ms step_avg:37.86ms
step:1327/2330 train_time:50232ms step_avg:37.85ms
step:1328/2330 train_time:50273ms step_avg:37.86ms
step:1329/2330 train_time:50308ms step_avg:37.85ms
step:1330/2330 train_time:50349ms step_avg:37.86ms
step:1331/2330 train_time:50383ms step_avg:37.85ms
step:1332/2330 train_time:50424ms step_avg:37.86ms
step:1333/2330 train_time:50458ms step_avg:37.85ms
step:1334/2330 train_time:50499ms step_avg:37.86ms
step:1335/2330 train_time:50535ms step_avg:37.85ms
step:1336/2330 train_time:50576ms step_avg:37.86ms
step:1337/2330 train_time:50610ms step_avg:37.85ms
step:1338/2330 train_time:50651ms step_avg:37.86ms
step:1339/2330 train_time:50686ms step_avg:37.85ms
step:1340/2330 train_time:50726ms step_avg:37.86ms
step:1341/2330 train_time:50762ms step_avg:37.85ms
step:1342/2330 train_time:50803ms step_avg:37.86ms
step:1343/2330 train_time:50838ms step_avg:37.85ms
step:1344/2330 train_time:50879ms step_avg:37.86ms
step:1345/2330 train_time:50915ms step_avg:37.86ms
step:1346/2330 train_time:50956ms step_avg:37.86ms
step:1347/2330 train_time:50992ms step_avg:37.86ms
step:1348/2330 train_time:51033ms step_avg:37.86ms
step:1349/2330 train_time:51069ms step_avg:37.86ms
step:1350/2330 train_time:51109ms step_avg:37.86ms
step:1351/2330 train_time:51145ms step_avg:37.86ms
step:1352/2330 train_time:51186ms step_avg:37.86ms
step:1353/2330 train_time:51220ms step_avg:37.86ms
step:1354/2330 train_time:51262ms step_avg:37.86ms
step:1355/2330 train_time:51297ms step_avg:37.86ms
step:1356/2330 train_time:51337ms step_avg:37.86ms
step:1357/2330 train_time:51374ms step_avg:37.86ms
step:1358/2330 train_time:51415ms step_avg:37.86ms
step:1359/2330 train_time:51450ms step_avg:37.86ms
step:1360/2330 train_time:51491ms step_avg:37.86ms
step:1361/2330 train_time:51526ms step_avg:37.86ms
step:1362/2330 train_time:51566ms step_avg:37.86ms
step:1363/2330 train_time:51601ms step_avg:37.86ms
step:1364/2330 train_time:51642ms step_avg:37.86ms
step:1365/2330 train_time:51677ms step_avg:37.86ms
step:1366/2330 train_time:51718ms step_avg:37.86ms
step:1367/2330 train_time:51753ms step_avg:37.86ms
step:1368/2330 train_time:51795ms step_avg:37.86ms
step:1369/2330 train_time:51829ms step_avg:37.86ms
step:1370/2330 train_time:51871ms step_avg:37.86ms
step:1371/2330 train_time:51906ms step_avg:37.86ms
step:1372/2330 train_time:51946ms step_avg:37.86ms
step:1373/2330 train_time:51982ms step_avg:37.86ms
step:1374/2330 train_time:52022ms step_avg:37.86ms
step:1375/2330 train_time:52058ms step_avg:37.86ms
step:1376/2330 train_time:52100ms step_avg:37.86ms
step:1377/2330 train_time:52134ms step_avg:37.86ms
step:1378/2330 train_time:52176ms step_avg:37.86ms
step:1379/2330 train_time:52210ms step_avg:37.86ms
step:1380/2330 train_time:52251ms step_avg:37.86ms
step:1381/2330 train_time:52286ms step_avg:37.86ms
step:1382/2330 train_time:52326ms step_avg:37.86ms
step:1383/2330 train_time:52361ms step_avg:37.86ms
step:1384/2330 train_time:52402ms step_avg:37.86ms
step:1385/2330 train_time:52437ms step_avg:37.86ms
step:1386/2330 train_time:52478ms step_avg:37.86ms
step:1387/2330 train_time:52514ms step_avg:37.86ms
step:1388/2330 train_time:52555ms step_avg:37.86ms
step:1389/2330 train_time:52590ms step_avg:37.86ms
step:1390/2330 train_time:52630ms step_avg:37.86ms
step:1391/2330 train_time:52666ms step_avg:37.86ms
step:1392/2330 train_time:52706ms step_avg:37.86ms
step:1393/2330 train_time:52741ms step_avg:37.86ms
step:1394/2330 train_time:52783ms step_avg:37.86ms
step:1395/2330 train_time:52817ms step_avg:37.86ms
step:1396/2330 train_time:52859ms step_avg:37.86ms
step:1397/2330 train_time:52894ms step_avg:37.86ms
step:1398/2330 train_time:52935ms step_avg:37.86ms
step:1399/2330 train_time:52969ms step_avg:37.86ms
step:1400/2330 train_time:53010ms step_avg:37.86ms
step:1401/2330 train_time:53045ms step_avg:37.86ms
step:1402/2330 train_time:53086ms step_avg:37.86ms
step:1403/2330 train_time:53121ms step_avg:37.86ms
step:1404/2330 train_time:53162ms step_avg:37.86ms
step:1405/2330 train_time:53197ms step_avg:37.86ms
step:1406/2330 train_time:53238ms step_avg:37.86ms
step:1407/2330 train_time:53273ms step_avg:37.86ms
step:1408/2330 train_time:53313ms step_avg:37.86ms
step:1409/2330 train_time:53349ms step_avg:37.86ms
step:1410/2330 train_time:53390ms step_avg:37.87ms
step:1411/2330 train_time:53425ms step_avg:37.86ms
step:1412/2330 train_time:53466ms step_avg:37.87ms
step:1413/2330 train_time:53501ms step_avg:37.86ms
step:1414/2330 train_time:53542ms step_avg:37.87ms
step:1415/2330 train_time:53577ms step_avg:37.86ms
step:1416/2330 train_time:53618ms step_avg:37.87ms
step:1417/2330 train_time:53653ms step_avg:37.86ms
step:1418/2330 train_time:53693ms step_avg:37.87ms
step:1419/2330 train_time:53730ms step_avg:37.86ms
step:1420/2330 train_time:53770ms step_avg:37.87ms
step:1421/2330 train_time:53806ms step_avg:37.86ms
step:1422/2330 train_time:53846ms step_avg:37.87ms
step:1423/2330 train_time:53882ms step_avg:37.86ms
step:1424/2330 train_time:53923ms step_avg:37.87ms
step:1425/2330 train_time:53958ms step_avg:37.87ms
step:1426/2330 train_time:53999ms step_avg:37.87ms
step:1427/2330 train_time:54035ms step_avg:37.87ms
step:1428/2330 train_time:54076ms step_avg:37.87ms
step:1429/2330 train_time:54110ms step_avg:37.87ms
step:1430/2330 train_time:54151ms step_avg:37.87ms
step:1431/2330 train_time:54186ms step_avg:37.87ms
step:1432/2330 train_time:54226ms step_avg:37.87ms
step:1433/2330 train_time:54262ms step_avg:37.87ms
step:1434/2330 train_time:54303ms step_avg:37.87ms
step:1435/2330 train_time:54337ms step_avg:37.87ms
step:1436/2330 train_time:54378ms step_avg:37.87ms
step:1437/2330 train_time:54415ms step_avg:37.87ms
step:1438/2330 train_time:54455ms step_avg:37.87ms
step:1439/2330 train_time:54491ms step_avg:37.87ms
step:1440/2330 train_time:54532ms step_avg:37.87ms
step:1441/2330 train_time:54568ms step_avg:37.87ms
step:1442/2330 train_time:54608ms step_avg:37.87ms
step:1443/2330 train_time:54643ms step_avg:37.87ms
step:1444/2330 train_time:54684ms step_avg:37.87ms
step:1445/2330 train_time:54719ms step_avg:37.87ms
step:1446/2330 train_time:54760ms step_avg:37.87ms
step:1447/2330 train_time:54795ms step_avg:37.87ms
step:1448/2330 train_time:54836ms step_avg:37.87ms
step:1449/2330 train_time:54873ms step_avg:37.87ms
step:1450/2330 train_time:54913ms step_avg:37.87ms
step:1451/2330 train_time:54949ms step_avg:37.87ms
step:1452/2330 train_time:54989ms step_avg:37.87ms
step:1453/2330 train_time:55025ms step_avg:37.87ms
step:1454/2330 train_time:55066ms step_avg:37.87ms
step:1455/2330 train_time:55100ms step_avg:37.87ms
step:1456/2330 train_time:55141ms step_avg:37.87ms
step:1457/2330 train_time:55177ms step_avg:37.87ms
step:1458/2330 train_time:55217ms step_avg:37.87ms
step:1459/2330 train_time:55253ms step_avg:37.87ms
step:1460/2330 train_time:55294ms step_avg:37.87ms
step:1461/2330 train_time:55330ms step_avg:37.87ms
step:1462/2330 train_time:55371ms step_avg:37.87ms
step:1463/2330 train_time:55405ms step_avg:37.87ms
step:1464/2330 train_time:55446ms step_avg:37.87ms
step:1465/2330 train_time:55481ms step_avg:37.87ms
step:1466/2330 train_time:55522ms step_avg:37.87ms
step:1467/2330 train_time:55558ms step_avg:37.87ms
step:1468/2330 train_time:55599ms step_avg:37.87ms
step:1469/2330 train_time:55634ms step_avg:37.87ms
step:1470/2330 train_time:55675ms step_avg:37.87ms
step:1471/2330 train_time:55710ms step_avg:37.87ms
step:1472/2330 train_time:55751ms step_avg:37.87ms
step:1473/2330 train_time:55786ms step_avg:37.87ms
step:1474/2330 train_time:55827ms step_avg:37.87ms
step:1475/2330 train_time:55862ms step_avg:37.87ms
step:1476/2330 train_time:55903ms step_avg:37.87ms
step:1477/2330 train_time:55938ms step_avg:37.87ms
step:1478/2330 train_time:55979ms step_avg:37.88ms
step:1479/2330 train_time:56015ms step_avg:37.87ms
step:1480/2330 train_time:56056ms step_avg:37.88ms
step:1481/2330 train_time:56091ms step_avg:37.87ms
step:1482/2330 train_time:56132ms step_avg:37.88ms
step:1483/2330 train_time:56167ms step_avg:37.87ms
step:1484/2330 train_time:56207ms step_avg:37.88ms
step:1485/2330 train_time:56243ms step_avg:37.87ms
step:1486/2330 train_time:56284ms step_avg:37.88ms
step:1487/2330 train_time:56319ms step_avg:37.87ms
step:1488/2330 train_time:56360ms step_avg:37.88ms
step:1489/2330 train_time:56395ms step_avg:37.87ms
step:1490/2330 train_time:56436ms step_avg:37.88ms
step:1491/2330 train_time:56471ms step_avg:37.87ms
step:1492/2330 train_time:56511ms step_avg:37.88ms
step:1493/2330 train_time:56547ms step_avg:37.87ms
step:1494/2330 train_time:56587ms step_avg:37.88ms
step:1495/2330 train_time:56623ms step_avg:37.87ms
step:1496/2330 train_time:56664ms step_avg:37.88ms
step:1497/2330 train_time:56699ms step_avg:37.87ms
step:1498/2330 train_time:56739ms step_avg:37.88ms
step:1499/2330 train_time:56774ms step_avg:37.87ms
step:1500/2330 train_time:56815ms step_avg:37.88ms
step:1500/2330 val_loss:5.3045 train_time:56928ms step_avg:37.95ms
step:1501/2330 train_time:56939ms step_avg:37.93ms
step:1502/2330 train_time:56949ms step_avg:37.92ms
step:1503/2330 train_time:56958ms step_avg:37.90ms
step:1504/2330 train_time:56970ms step_avg:37.88ms
step:1505/2330 train_time:57006ms step_avg:37.88ms
step:1506/2330 train_time:57047ms step_avg:37.88ms
step:1507/2330 train_time:57081ms step_avg:37.88ms
step:1508/2330 train_time:57122ms step_avg:37.88ms
step:1509/2330 train_time:57156ms step_avg:37.88ms
step:1510/2330 train_time:57197ms step_avg:37.88ms
step:1511/2330 train_time:57231ms step_avg:37.88ms
step:1512/2330 train_time:57272ms step_avg:37.88ms
step:1513/2330 train_time:57309ms step_avg:37.88ms
step:1514/2330 train_time:57349ms step_avg:37.88ms
step:1515/2330 train_time:57385ms step_avg:37.88ms
step:1516/2330 train_time:57426ms step_avg:37.88ms
step:1517/2330 train_time:57460ms step_avg:37.88ms
step:1518/2330 train_time:57501ms step_avg:37.88ms
step:1519/2330 train_time:57539ms step_avg:37.88ms
step:1520/2330 train_time:57579ms step_avg:37.88ms
step:1521/2330 train_time:57615ms step_avg:37.88ms
step:1522/2330 train_time:57655ms step_avg:37.88ms
step:1523/2330 train_time:57690ms step_avg:37.88ms
step:1524/2330 train_time:57730ms step_avg:37.88ms
step:1525/2330 train_time:57765ms step_avg:37.88ms
step:1526/2330 train_time:57805ms step_avg:37.88ms
step:1527/2330 train_time:57840ms step_avg:37.88ms
step:1528/2330 train_time:57881ms step_avg:37.88ms
step:1529/2330 train_time:57918ms step_avg:37.88ms
step:1530/2330 train_time:57959ms step_avg:37.88ms
step:1531/2330 train_time:57994ms step_avg:37.88ms
step:1532/2330 train_time:58034ms step_avg:37.88ms
step:1533/2330 train_time:58070ms step_avg:37.88ms
step:1534/2330 train_time:58110ms step_avg:37.88ms
step:1535/2330 train_time:58145ms step_avg:37.88ms
step:1536/2330 train_time:58186ms step_avg:37.88ms
step:1537/2330 train_time:58221ms step_avg:37.88ms
step:1538/2330 train_time:58263ms step_avg:37.88ms
step:1539/2330 train_time:58298ms step_avg:37.88ms
step:1540/2330 train_time:58339ms step_avg:37.88ms
step:1541/2330 train_time:58374ms step_avg:37.88ms
step:1542/2330 train_time:58416ms step_avg:37.88ms
step:1543/2330 train_time:58450ms step_avg:37.88ms
step:1544/2330 train_time:58491ms step_avg:37.88ms
step:1545/2330 train_time:58526ms step_avg:37.88ms
step:1546/2330 train_time:58566ms step_avg:37.88ms
step:1547/2330 train_time:58602ms step_avg:37.88ms
step:1548/2330 train_time:58642ms step_avg:37.88ms
step:1549/2330 train_time:58678ms step_avg:37.88ms
step:1550/2330 train_time:58718ms step_avg:37.88ms
step:1551/2330 train_time:58754ms step_avg:37.88ms
step:1552/2330 train_time:58795ms step_avg:37.88ms
step:1553/2330 train_time:58830ms step_avg:37.88ms
step:1554/2330 train_time:58871ms step_avg:37.88ms
step:1555/2330 train_time:58905ms step_avg:37.88ms
step:1556/2330 train_time:58946ms step_avg:37.88ms
step:1557/2330 train_time:58980ms step_avg:37.88ms
step:1558/2330 train_time:59022ms step_avg:37.88ms
step:1559/2330 train_time:59057ms step_avg:37.88ms
step:1560/2330 train_time:59098ms step_avg:37.88ms
step:1561/2330 train_time:59134ms step_avg:37.88ms
step:1562/2330 train_time:59174ms step_avg:37.88ms
step:1563/2330 train_time:59210ms step_avg:37.88ms
step:1564/2330 train_time:59251ms step_avg:37.88ms
step:1565/2330 train_time:59287ms step_avg:37.88ms
step:1566/2330 train_time:59327ms step_avg:37.88ms
step:1567/2330 train_time:59363ms step_avg:37.88ms
step:1568/2330 train_time:59404ms step_avg:37.88ms
step:1569/2330 train_time:59439ms step_avg:37.88ms
step:1570/2330 train_time:59480ms step_avg:37.89ms
step:1571/2330 train_time:59516ms step_avg:37.88ms
step:1572/2330 train_time:59557ms step_avg:37.89ms
step:1573/2330 train_time:59593ms step_avg:37.88ms
step:1574/2330 train_time:59633ms step_avg:37.89ms
step:1575/2330 train_time:59669ms step_avg:37.88ms
step:1576/2330 train_time:59709ms step_avg:37.89ms
step:1577/2330 train_time:59744ms step_avg:37.88ms
step:1578/2330 train_time:59785ms step_avg:37.89ms
step:1579/2330 train_time:59820ms step_avg:37.88ms
step:1580/2330 train_time:59860ms step_avg:37.89ms
step:1581/2330 train_time:59896ms step_avg:37.88ms
step:1582/2330 train_time:59937ms step_avg:37.89ms
step:1583/2330 train_time:59973ms step_avg:37.89ms
step:1584/2330 train_time:60013ms step_avg:37.89ms
step:1585/2330 train_time:60049ms step_avg:37.89ms
step:1586/2330 train_time:60089ms step_avg:37.89ms
step:1587/2330 train_time:60124ms step_avg:37.89ms
step:1588/2330 train_time:60165ms step_avg:37.89ms
step:1589/2330 train_time:60200ms step_avg:37.89ms
step:1590/2330 train_time:60241ms step_avg:37.89ms
step:1591/2330 train_time:60276ms step_avg:37.89ms
step:1592/2330 train_time:60317ms step_avg:37.89ms
step:1593/2330 train_time:60354ms step_avg:37.89ms
step:1594/2330 train_time:60394ms step_avg:37.89ms
step:1595/2330 train_time:60431ms step_avg:37.89ms
step:1596/2330 train_time:60471ms step_avg:37.89ms
step:1597/2330 train_time:60507ms step_avg:37.89ms
step:1598/2330 train_time:60547ms step_avg:37.89ms
step:1599/2330 train_time:60582ms step_avg:37.89ms
step:1600/2330 train_time:60623ms step_avg:37.89ms
step:1601/2330 train_time:60658ms step_avg:37.89ms
step:1602/2330 train_time:60699ms step_avg:37.89ms
step:1603/2330 train_time:60735ms step_avg:37.89ms
step:1604/2330 train_time:60775ms step_avg:37.89ms
step:1605/2330 train_time:60811ms step_avg:37.89ms
step:1606/2330 train_time:60851ms step_avg:37.89ms
step:1607/2330 train_time:60886ms step_avg:37.89ms
step:1608/2330 train_time:60927ms step_avg:37.89ms
step:1609/2330 train_time:60962ms step_avg:37.89ms
step:1610/2330 train_time:61003ms step_avg:37.89ms
step:1611/2330 train_time:61038ms step_avg:37.89ms
step:1612/2330 train_time:61079ms step_avg:37.89ms
step:1613/2330 train_time:61113ms step_avg:37.89ms
step:1614/2330 train_time:61154ms step_avg:37.89ms
step:1615/2330 train_time:61188ms step_avg:37.89ms
step:1616/2330 train_time:61229ms step_avg:37.89ms
step:1617/2330 train_time:61265ms step_avg:37.89ms
step:1618/2330 train_time:61306ms step_avg:37.89ms
step:1619/2330 train_time:61341ms step_avg:37.89ms
step:1620/2330 train_time:61382ms step_avg:37.89ms
step:1621/2330 train_time:61418ms step_avg:37.89ms
step:1622/2330 train_time:61459ms step_avg:37.89ms
step:1623/2330 train_time:61494ms step_avg:37.89ms
step:1624/2330 train_time:61535ms step_avg:37.89ms
step:1625/2330 train_time:61571ms step_avg:37.89ms
step:1626/2330 train_time:61612ms step_avg:37.89ms
step:1627/2330 train_time:61648ms step_avg:37.89ms
step:1628/2330 train_time:61688ms step_avg:37.89ms
step:1629/2330 train_time:61723ms step_avg:37.89ms
step:1630/2330 train_time:61764ms step_avg:37.89ms
step:1631/2330 train_time:61800ms step_avg:37.89ms
step:1632/2330 train_time:61840ms step_avg:37.89ms
step:1633/2330 train_time:61876ms step_avg:37.89ms
step:1634/2330 train_time:61917ms step_avg:37.89ms
step:1635/2330 train_time:61952ms step_avg:37.89ms
step:1636/2330 train_time:61992ms step_avg:37.89ms
step:1637/2330 train_time:62027ms step_avg:37.89ms
step:1638/2330 train_time:62068ms step_avg:37.89ms
step:1639/2330 train_time:62103ms step_avg:37.89ms
step:1640/2330 train_time:62144ms step_avg:37.89ms
step:1641/2330 train_time:62179ms step_avg:37.89ms
step:1642/2330 train_time:62219ms step_avg:37.89ms
step:1643/2330 train_time:62255ms step_avg:37.89ms
step:1644/2330 train_time:62296ms step_avg:37.89ms
step:1645/2330 train_time:62332ms step_avg:37.89ms
step:1646/2330 train_time:62372ms step_avg:37.89ms
step:1647/2330 train_time:62409ms step_avg:37.89ms
step:1648/2330 train_time:62449ms step_avg:37.89ms
step:1649/2330 train_time:62485ms step_avg:37.89ms
step:1650/2330 train_time:62526ms step_avg:37.89ms
step:1651/2330 train_time:62560ms step_avg:37.89ms
step:1652/2330 train_time:62601ms step_avg:37.89ms
step:1653/2330 train_time:62637ms step_avg:37.89ms
step:1654/2330 train_time:62678ms step_avg:37.89ms
step:1655/2330 train_time:62713ms step_avg:37.89ms
step:1656/2330 train_time:62754ms step_avg:37.89ms
step:1657/2330 train_time:62789ms step_avg:37.89ms
step:1658/2330 train_time:62829ms step_avg:37.89ms
step:1659/2330 train_time:62865ms step_avg:37.89ms
step:1660/2330 train_time:62906ms step_avg:37.89ms
step:1661/2330 train_time:62940ms step_avg:37.89ms
step:1662/2330 train_time:62981ms step_avg:37.89ms
step:1663/2330 train_time:63016ms step_avg:37.89ms
step:1664/2330 train_time:63056ms step_avg:37.89ms
step:1665/2330 train_time:63092ms step_avg:37.89ms
step:1666/2330 train_time:63133ms step_avg:37.89ms
step:1667/2330 train_time:63167ms step_avg:37.89ms
step:1668/2330 train_time:63208ms step_avg:37.89ms
step:1669/2330 train_time:63243ms step_avg:37.89ms
step:1670/2330 train_time:63284ms step_avg:37.89ms
step:1671/2330 train_time:63319ms step_avg:37.89ms
step:1672/2330 train_time:63360ms step_avg:37.89ms
step:1673/2330 train_time:63397ms step_avg:37.89ms
step:1674/2330 train_time:63437ms step_avg:37.90ms
step:1675/2330 train_time:63473ms step_avg:37.89ms
step:1676/2330 train_time:63514ms step_avg:37.90ms
step:1677/2330 train_time:63549ms step_avg:37.89ms
step:1678/2330 train_time:63589ms step_avg:37.90ms
step:1679/2330 train_time:63624ms step_avg:37.89ms
step:1680/2330 train_time:63666ms step_avg:37.90ms
step:1681/2330 train_time:63700ms step_avg:37.89ms
step:1682/2330 train_time:63741ms step_avg:37.90ms
step:1683/2330 train_time:63777ms step_avg:37.89ms
step:1684/2330 train_time:63818ms step_avg:37.90ms
step:1685/2330 train_time:63854ms step_avg:37.90ms
step:1686/2330 train_time:63894ms step_avg:37.90ms
step:1687/2330 train_time:63929ms step_avg:37.90ms
step:1688/2330 train_time:63970ms step_avg:37.90ms
step:1689/2330 train_time:64005ms step_avg:37.90ms
step:1690/2330 train_time:64046ms step_avg:37.90ms
step:1691/2330 train_time:64080ms step_avg:37.89ms
step:1692/2330 train_time:64121ms step_avg:37.90ms
step:1693/2330 train_time:64156ms step_avg:37.89ms
step:1694/2330 train_time:64196ms step_avg:37.90ms
step:1695/2330 train_time:64233ms step_avg:37.90ms
step:1696/2330 train_time:64273ms step_avg:37.90ms
step:1697/2330 train_time:64310ms step_avg:37.90ms
step:1698/2330 train_time:64350ms step_avg:37.90ms
step:1699/2330 train_time:64386ms step_avg:37.90ms
step:1700/2330 train_time:64427ms step_avg:37.90ms
step:1701/2330 train_time:64462ms step_avg:37.90ms
step:1702/2330 train_time:64503ms step_avg:37.90ms
step:1703/2330 train_time:64539ms step_avg:37.90ms
step:1704/2330 train_time:64580ms step_avg:37.90ms
step:1705/2330 train_time:64615ms step_avg:37.90ms
step:1706/2330 train_time:64656ms step_avg:37.90ms
step:1707/2330 train_time:64693ms step_avg:37.90ms
step:1708/2330 train_time:64733ms step_avg:37.90ms
step:1709/2330 train_time:64769ms step_avg:37.90ms
step:1710/2330 train_time:64809ms step_avg:37.90ms
step:1711/2330 train_time:64845ms step_avg:37.90ms
step:1712/2330 train_time:64885ms step_avg:37.90ms
step:1713/2330 train_time:64920ms step_avg:37.90ms
step:1714/2330 train_time:64961ms step_avg:37.90ms
step:1715/2330 train_time:64996ms step_avg:37.90ms
step:1716/2330 train_time:65037ms step_avg:37.90ms
step:1717/2330 train_time:65073ms step_avg:37.90ms
step:1718/2330 train_time:65114ms step_avg:37.90ms
step:1719/2330 train_time:65149ms step_avg:37.90ms
step:1720/2330 train_time:65189ms step_avg:37.90ms
step:1721/2330 train_time:65225ms step_avg:37.90ms
step:1722/2330 train_time:65266ms step_avg:37.90ms
step:1723/2330 train_time:65301ms step_avg:37.90ms
step:1724/2330 train_time:65342ms step_avg:37.90ms
step:1725/2330 train_time:65378ms step_avg:37.90ms
step:1726/2330 train_time:65419ms step_avg:37.90ms
step:1727/2330 train_time:65455ms step_avg:37.90ms
step:1728/2330 train_time:65495ms step_avg:37.90ms
step:1729/2330 train_time:65531ms step_avg:37.90ms
step:1730/2330 train_time:65571ms step_avg:37.90ms
step:1731/2330 train_time:65607ms step_avg:37.90ms
step:1732/2330 train_time:65648ms step_avg:37.90ms
step:1733/2330 train_time:65683ms step_avg:37.90ms
step:1734/2330 train_time:65724ms step_avg:37.90ms
step:1735/2330 train_time:65759ms step_avg:37.90ms
step:1736/2330 train_time:65800ms step_avg:37.90ms
step:1737/2330 train_time:65836ms step_avg:37.90ms
step:1738/2330 train_time:65876ms step_avg:37.90ms
step:1739/2330 train_time:65912ms step_avg:37.90ms
step:1740/2330 train_time:65953ms step_avg:37.90ms
step:1741/2330 train_time:65987ms step_avg:37.90ms
step:1742/2330 train_time:66028ms step_avg:37.90ms
step:1743/2330 train_time:66064ms step_avg:37.90ms
step:1744/2330 train_time:66105ms step_avg:37.90ms
step:1745/2330 train_time:66140ms step_avg:37.90ms
step:1746/2330 train_time:66180ms step_avg:37.90ms
step:1747/2330 train_time:66215ms step_avg:37.90ms
step:1748/2330 train_time:66256ms step_avg:37.90ms
step:1749/2330 train_time:66290ms step_avg:37.90ms
step:1750/2330 train_time:66331ms step_avg:37.90ms
step:1750/2330 val_loss:5.2630 train_time:66442ms step_avg:37.97ms
step:1751/2330 train_time:66453ms step_avg:37.95ms
step:1752/2330 train_time:66463ms step_avg:37.94ms
step:1753/2330 train_time:66472ms step_avg:37.92ms
step:1754/2330 train_time:66484ms step_avg:37.90ms
step:1755/2330 train_time:66520ms step_avg:37.90ms
step:1756/2330 train_time:66561ms step_avg:37.90ms
step:1757/2330 train_time:66595ms step_avg:37.90ms
step:1758/2330 train_time:66635ms step_avg:37.90ms
step:1759/2330 train_time:66670ms step_avg:37.90ms
step:1760/2330 train_time:66710ms step_avg:37.90ms
step:1761/2330 train_time:66746ms step_avg:37.90ms
step:1762/2330 train_time:66786ms step_avg:37.90ms
step:1763/2330 train_time:66825ms step_avg:37.90ms
step:1764/2330 train_time:66866ms step_avg:37.91ms
step:1765/2330 train_time:66902ms step_avg:37.90ms
step:1766/2330 train_time:66942ms step_avg:37.91ms
step:1767/2330 train_time:66977ms step_avg:37.90ms
step:1768/2330 train_time:67019ms step_avg:37.91ms
step:1769/2330 train_time:67055ms step_avg:37.91ms
step:1770/2330 train_time:67095ms step_avg:37.91ms
step:1771/2330 train_time:67130ms step_avg:37.91ms
step:1772/2330 train_time:67171ms step_avg:37.91ms
step:1773/2330 train_time:67205ms step_avg:37.90ms
step:1774/2330 train_time:67245ms step_avg:37.91ms
step:1775/2330 train_time:67280ms step_avg:37.90ms
step:1776/2330 train_time:67321ms step_avg:37.91ms
step:1777/2330 train_time:67356ms step_avg:37.90ms
step:1778/2330 train_time:67398ms step_avg:37.91ms
step:1779/2330 train_time:67433ms step_avg:37.91ms
step:1780/2330 train_time:67474ms step_avg:37.91ms
step:1781/2330 train_time:67509ms step_avg:37.90ms
step:1782/2330 train_time:67549ms step_avg:37.91ms
step:1783/2330 train_time:67585ms step_avg:37.91ms
step:1784/2330 train_time:67625ms step_avg:37.91ms
step:1785/2330 train_time:67660ms step_avg:37.90ms
step:1786/2330 train_time:67701ms step_avg:37.91ms
step:1787/2330 train_time:67737ms step_avg:37.91ms
step:1788/2330 train_time:67777ms step_avg:37.91ms
step:1789/2330 train_time:67813ms step_avg:37.91ms
step:1790/2330 train_time:67854ms step_avg:37.91ms
step:1791/2330 train_time:67890ms step_avg:37.91ms
step:1792/2330 train_time:67931ms step_avg:37.91ms
step:1793/2330 train_time:67967ms step_avg:37.91ms
step:1794/2330 train_time:68007ms step_avg:37.91ms
step:1795/2330 train_time:68042ms step_avg:37.91ms
step:1796/2330 train_time:68083ms step_avg:37.91ms
step:1797/2330 train_time:68118ms step_avg:37.91ms
step:1798/2330 train_time:68159ms step_avg:37.91ms
step:1799/2330 train_time:68194ms step_avg:37.91ms
step:1800/2330 train_time:68234ms step_avg:37.91ms
step:1801/2330 train_time:68269ms step_avg:37.91ms
step:1802/2330 train_time:68310ms step_avg:37.91ms
step:1803/2330 train_time:68346ms step_avg:37.91ms
step:1804/2330 train_time:68387ms step_avg:37.91ms
step:1805/2330 train_time:68422ms step_avg:37.91ms
step:1806/2330 train_time:68463ms step_avg:37.91ms
step:1807/2330 train_time:68498ms step_avg:37.91ms
step:1808/2330 train_time:68540ms step_avg:37.91ms
step:1809/2330 train_time:68574ms step_avg:37.91ms
step:1810/2330 train_time:68616ms step_avg:37.91ms
step:1811/2330 train_time:68651ms step_avg:37.91ms
step:1812/2330 train_time:68691ms step_avg:37.91ms
step:1813/2330 train_time:68728ms step_avg:37.91ms
step:1814/2330 train_time:68768ms step_avg:37.91ms
step:1815/2330 train_time:68803ms step_avg:37.91ms
step:1816/2330 train_time:68844ms step_avg:37.91ms
step:1817/2330 train_time:68880ms step_avg:37.91ms
step:1818/2330 train_time:68921ms step_avg:37.91ms
step:1819/2330 train_time:68956ms step_avg:37.91ms
step:1820/2330 train_time:68997ms step_avg:37.91ms
step:1821/2330 train_time:69033ms step_avg:37.91ms
step:1822/2330 train_time:69074ms step_avg:37.91ms
step:1823/2330 train_time:69109ms step_avg:37.91ms
step:1824/2330 train_time:69149ms step_avg:37.91ms
step:1825/2330 train_time:69184ms step_avg:37.91ms
step:1826/2330 train_time:69224ms step_avg:37.91ms
step:1827/2330 train_time:69260ms step_avg:37.91ms
step:1828/2330 train_time:69301ms step_avg:37.91ms
step:1829/2330 train_time:69335ms step_avg:37.91ms
step:1830/2330 train_time:69376ms step_avg:37.91ms
step:1831/2330 train_time:69412ms step_avg:37.91ms
step:1832/2330 train_time:69453ms step_avg:37.91ms
step:1833/2330 train_time:69489ms step_avg:37.91ms
step:1834/2330 train_time:69529ms step_avg:37.91ms
step:1835/2330 train_time:69564ms step_avg:37.91ms
step:1836/2330 train_time:69605ms step_avg:37.91ms
step:1837/2330 train_time:69640ms step_avg:37.91ms
step:1838/2330 train_time:69681ms step_avg:37.91ms
step:1839/2330 train_time:69716ms step_avg:37.91ms
step:1840/2330 train_time:69757ms step_avg:37.91ms
step:1841/2330 train_time:69794ms step_avg:37.91ms
step:1842/2330 train_time:69835ms step_avg:37.91ms
step:1843/2330 train_time:69870ms step_avg:37.91ms
step:1844/2330 train_time:69911ms step_avg:37.91ms
step:1845/2330 train_time:69946ms step_avg:37.91ms
step:1846/2330 train_time:69986ms step_avg:37.91ms
step:1847/2330 train_time:70022ms step_avg:37.91ms
step:1848/2330 train_time:70062ms step_avg:37.91ms
step:1849/2330 train_time:70097ms step_avg:37.91ms
step:1850/2330 train_time:70138ms step_avg:37.91ms
step:1851/2330 train_time:70173ms step_avg:37.91ms
step:1852/2330 train_time:70214ms step_avg:37.91ms
step:1853/2330 train_time:70249ms step_avg:37.91ms
step:1854/2330 train_time:70290ms step_avg:37.91ms
step:1855/2330 train_time:70325ms step_avg:37.91ms
step:1856/2330 train_time:70365ms step_avg:37.91ms
step:1857/2330 train_time:70400ms step_avg:37.91ms
step:1858/2330 train_time:70441ms step_avg:37.91ms
step:1859/2330 train_time:70476ms step_avg:37.91ms
step:1860/2330 train_time:70516ms step_avg:37.91ms
step:1861/2330 train_time:70553ms step_avg:37.91ms
step:1862/2330 train_time:70593ms step_avg:37.91ms
step:1863/2330 train_time:70628ms step_avg:37.91ms
step:1864/2330 train_time:70669ms step_avg:37.91ms
step:1865/2330 train_time:70704ms step_avg:37.91ms
step:1866/2330 train_time:70745ms step_avg:37.91ms
step:1867/2330 train_time:70780ms step_avg:37.91ms
step:1868/2330 train_time:70822ms step_avg:37.91ms
step:1869/2330 train_time:70856ms step_avg:37.91ms
step:1870/2330 train_time:70897ms step_avg:37.91ms
step:1871/2330 train_time:70933ms step_avg:37.91ms
step:1872/2330 train_time:70974ms step_avg:37.91ms
step:1873/2330 train_time:71010ms step_avg:37.91ms
step:1874/2330 train_time:71051ms step_avg:37.91ms
step:1875/2330 train_time:71086ms step_avg:37.91ms
step:1876/2330 train_time:71127ms step_avg:37.91ms
step:1877/2330 train_time:71162ms step_avg:37.91ms
step:1878/2330 train_time:71202ms step_avg:37.91ms
step:1879/2330 train_time:71238ms step_avg:37.91ms
step:1880/2330 train_time:71278ms step_avg:37.91ms
step:1881/2330 train_time:71314ms step_avg:37.91ms
step:1882/2330 train_time:71355ms step_avg:37.91ms
step:1883/2330 train_time:71390ms step_avg:37.91ms
step:1884/2330 train_time:71430ms step_avg:37.91ms
step:1885/2330 train_time:71465ms step_avg:37.91ms
step:1886/2330 train_time:71506ms step_avg:37.91ms
step:1887/2330 train_time:71541ms step_avg:37.91ms
step:1888/2330 train_time:71581ms step_avg:37.91ms
step:1889/2330 train_time:71618ms step_avg:37.91ms
step:1890/2330 train_time:71659ms step_avg:37.91ms
step:1891/2330 train_time:71695ms step_avg:37.91ms
step:1892/2330 train_time:71736ms step_avg:37.92ms
step:1893/2330 train_time:71771ms step_avg:37.91ms
step:1894/2330 train_time:71812ms step_avg:37.92ms
step:1895/2330 train_time:71848ms step_avg:37.91ms
step:1896/2330 train_time:71888ms step_avg:37.92ms
step:1897/2330 train_time:71925ms step_avg:37.91ms
step:1898/2330 train_time:71965ms step_avg:37.92ms
step:1899/2330 train_time:72001ms step_avg:37.91ms
step:1900/2330 train_time:72041ms step_avg:37.92ms
step:1901/2330 train_time:72076ms step_avg:37.91ms
step:1902/2330 train_time:72117ms step_avg:37.92ms
step:1903/2330 train_time:72152ms step_avg:37.92ms
step:1904/2330 train_time:72193ms step_avg:37.92ms
step:1905/2330 train_time:72229ms step_avg:37.92ms
step:1906/2330 train_time:72270ms step_avg:37.92ms
step:1907/2330 train_time:72306ms step_avg:37.92ms
step:1908/2330 train_time:72346ms step_avg:37.92ms
step:1909/2330 train_time:72381ms step_avg:37.92ms
step:1910/2330 train_time:72421ms step_avg:37.92ms
step:1911/2330 train_time:72456ms step_avg:37.92ms
step:1912/2330 train_time:72497ms step_avg:37.92ms
step:1913/2330 train_time:72533ms step_avg:37.92ms
step:1914/2330 train_time:72574ms step_avg:37.92ms
step:1915/2330 train_time:72609ms step_avg:37.92ms
step:1916/2330 train_time:72649ms step_avg:37.92ms
step:1917/2330 train_time:72685ms step_avg:37.92ms
step:1918/2330 train_time:72725ms step_avg:37.92ms
step:1919/2330 train_time:72760ms step_avg:37.92ms
step:1920/2330 train_time:72802ms step_avg:37.92ms
step:1921/2330 train_time:72837ms step_avg:37.92ms
step:1922/2330 train_time:72877ms step_avg:37.92ms
step:1923/2330 train_time:72913ms step_avg:37.92ms
step:1924/2330 train_time:72954ms step_avg:37.92ms
step:1925/2330 train_time:72990ms step_avg:37.92ms
step:1926/2330 train_time:73030ms step_avg:37.92ms
step:1927/2330 train_time:73067ms step_avg:37.92ms
step:1928/2330 train_time:73107ms step_avg:37.92ms
step:1929/2330 train_time:73143ms step_avg:37.92ms
step:1930/2330 train_time:73183ms step_avg:37.92ms
step:1931/2330 train_time:73219ms step_avg:37.92ms
step:1932/2330 train_time:73261ms step_avg:37.92ms
step:1933/2330 train_time:73296ms step_avg:37.92ms
step:1934/2330 train_time:73337ms step_avg:37.92ms
step:1935/2330 train_time:73372ms step_avg:37.92ms
step:1936/2330 train_time:73413ms step_avg:37.92ms
step:1937/2330 train_time:73447ms step_avg:37.92ms
step:1938/2330 train_time:73487ms step_avg:37.92ms
step:1939/2330 train_time:73522ms step_avg:37.92ms
step:1940/2330 train_time:73563ms step_avg:37.92ms
step:1941/2330 train_time:73598ms step_avg:37.92ms
step:1942/2330 train_time:73639ms step_avg:37.92ms
step:1943/2330 train_time:73674ms step_avg:37.92ms
step:1944/2330 train_time:73715ms step_avg:37.92ms
step:1945/2330 train_time:73750ms step_avg:37.92ms
step:1946/2330 train_time:73791ms step_avg:37.92ms
step:1947/2330 train_time:73825ms step_avg:37.92ms
step:1948/2330 train_time:73866ms step_avg:37.92ms
step:1949/2330 train_time:73900ms step_avg:37.92ms
step:1950/2330 train_time:73942ms step_avg:37.92ms
step:1951/2330 train_time:73976ms step_avg:37.92ms
step:1952/2330 train_time:74017ms step_avg:37.92ms
step:1953/2330 train_time:74054ms step_avg:37.92ms
step:1954/2330 train_time:74095ms step_avg:37.92ms
step:1955/2330 train_time:74130ms step_avg:37.92ms
step:1956/2330 train_time:74171ms step_avg:37.92ms
step:1957/2330 train_time:74206ms step_avg:37.92ms
step:1958/2330 train_time:74246ms step_avg:37.92ms
step:1959/2330 train_time:74282ms step_avg:37.92ms
step:1960/2330 train_time:74323ms step_avg:37.92ms
step:1961/2330 train_time:74358ms step_avg:37.92ms
step:1962/2330 train_time:74399ms step_avg:37.92ms
step:1963/2330 train_time:74433ms step_avg:37.92ms
step:1964/2330 train_time:74474ms step_avg:37.92ms
step:1965/2330 train_time:74510ms step_avg:37.92ms
step:1966/2330 train_time:74550ms step_avg:37.92ms
step:1967/2330 train_time:74586ms step_avg:37.92ms
step:1968/2330 train_time:74627ms step_avg:37.92ms
step:1969/2330 train_time:74662ms step_avg:37.92ms
step:1970/2330 train_time:74703ms step_avg:37.92ms
step:1971/2330 train_time:74737ms step_avg:37.92ms
step:1972/2330 train_time:74777ms step_avg:37.92ms
step:1973/2330 train_time:74813ms step_avg:37.92ms
step:1974/2330 train_time:74854ms step_avg:37.92ms
step:1975/2330 train_time:74890ms step_avg:37.92ms
step:1976/2330 train_time:74930ms step_avg:37.92ms
step:1977/2330 train_time:74967ms step_avg:37.92ms
step:1978/2330 train_time:75007ms step_avg:37.92ms
step:1979/2330 train_time:75044ms step_avg:37.92ms
step:1980/2330 train_time:75084ms step_avg:37.92ms
step:1981/2330 train_time:75120ms step_avg:37.92ms
step:1982/2330 train_time:75161ms step_avg:37.92ms
step:1983/2330 train_time:75197ms step_avg:37.92ms
step:1984/2330 train_time:75237ms step_avg:37.92ms
step:1985/2330 train_time:75273ms step_avg:37.92ms
step:1986/2330 train_time:75314ms step_avg:37.92ms
step:1987/2330 train_time:75349ms step_avg:37.92ms
step:1988/2330 train_time:75390ms step_avg:37.92ms
step:1989/2330 train_time:75425ms step_avg:37.92ms
step:1990/2330 train_time:75465ms step_avg:37.92ms
step:1991/2330 train_time:75501ms step_avg:37.92ms
step:1992/2330 train_time:75542ms step_avg:37.92ms
step:1993/2330 train_time:75577ms step_avg:37.92ms
step:1994/2330 train_time:75617ms step_avg:37.92ms
step:1995/2330 train_time:75654ms step_avg:37.92ms
step:1996/2330 train_time:75694ms step_avg:37.92ms
step:1997/2330 train_time:75730ms step_avg:37.92ms
step:1998/2330 train_time:75771ms step_avg:37.92ms
step:1999/2330 train_time:75805ms step_avg:37.92ms
step:2000/2330 train_time:75845ms step_avg:37.92ms
step:2000/2330 val_loss:5.2296 train_time:75957ms step_avg:37.98ms
step:2001/2330 train_time:75968ms step_avg:37.97ms
step:2002/2330 train_time:75980ms step_avg:37.95ms
step:2003/2330 train_time:75988ms step_avg:37.94ms
step:2004/2330 train_time:76000ms step_avg:37.92ms
step:2005/2330 train_time:76035ms step_avg:37.92ms
step:2006/2330 train_time:76076ms step_avg:37.92ms
step:2007/2330 train_time:76109ms step_avg:37.92ms
step:2008/2330 train_time:76150ms step_avg:37.92ms
step:2009/2330 train_time:76185ms step_avg:37.92ms
step:2010/2330 train_time:76225ms step_avg:37.92ms
step:2011/2330 train_time:76261ms step_avg:37.92ms
step:2012/2330 train_time:76302ms step_avg:37.92ms
step:2013/2330 train_time:76339ms step_avg:37.92ms
step:2014/2330 train_time:76380ms step_avg:37.92ms
step:2015/2330 train_time:76415ms step_avg:37.92ms
step:2016/2330 train_time:76456ms step_avg:37.92ms
step:2017/2330 train_time:76491ms step_avg:37.92ms
step:2018/2330 train_time:76531ms step_avg:37.92ms
step:2019/2330 train_time:76568ms step_avg:37.92ms
step:2020/2330 train_time:76608ms step_avg:37.92ms
step:2021/2330 train_time:76643ms step_avg:37.92ms
step:2022/2330 train_time:76683ms step_avg:37.92ms
step:2023/2330 train_time:76718ms step_avg:37.92ms
step:2024/2330 train_time:76758ms step_avg:37.92ms
step:2025/2330 train_time:76793ms step_avg:37.92ms
step:2026/2330 train_time:76834ms step_avg:37.92ms
step:2027/2330 train_time:76869ms step_avg:37.92ms
step:2028/2330 train_time:76910ms step_avg:37.92ms
step:2029/2330 train_time:76945ms step_avg:37.92ms
step:2030/2330 train_time:76986ms step_avg:37.92ms
step:2031/2330 train_time:77020ms step_avg:37.92ms
step:2032/2330 train_time:77061ms step_avg:37.92ms
step:2033/2330 train_time:77096ms step_avg:37.92ms
step:2034/2330 train_time:77137ms step_avg:37.92ms
step:2035/2330 train_time:77172ms step_avg:37.92ms
step:2036/2330 train_time:77213ms step_avg:37.92ms
step:2037/2330 train_time:77247ms step_avg:37.92ms
step:2038/2330 train_time:77288ms step_avg:37.92ms
step:2039/2330 train_time:77325ms step_avg:37.92ms
step:2040/2330 train_time:77366ms step_avg:37.92ms
step:2041/2330 train_time:77402ms step_avg:37.92ms
step:2042/2330 train_time:77442ms step_avg:37.92ms
step:2043/2330 train_time:77477ms step_avg:37.92ms
step:2044/2330 train_time:77518ms step_avg:37.92ms
step:2045/2330 train_time:77553ms step_avg:37.92ms
step:2046/2330 train_time:77593ms step_avg:37.92ms
step:2047/2330 train_time:77629ms step_avg:37.92ms
step:2048/2330 train_time:77670ms step_avg:37.92ms
step:2049/2330 train_time:77704ms step_avg:37.92ms
step:2050/2330 train_time:77745ms step_avg:37.92ms
step:2051/2330 train_time:77779ms step_avg:37.92ms
step:2052/2330 train_time:77820ms step_avg:37.92ms
step:2053/2330 train_time:77855ms step_avg:37.92ms
step:2054/2330 train_time:77896ms step_avg:37.92ms
step:2055/2330 train_time:77931ms step_avg:37.92ms
step:2056/2330 train_time:77972ms step_avg:37.92ms
step:2057/2330 train_time:78007ms step_avg:37.92ms
step:2058/2330 train_time:78048ms step_avg:37.92ms
step:2059/2330 train_time:78083ms step_avg:37.92ms
step:2060/2330 train_time:78124ms step_avg:37.92ms
step:2061/2330 train_time:78159ms step_avg:37.92ms
step:2062/2330 train_time:78200ms step_avg:37.92ms
step:2063/2330 train_time:78235ms step_avg:37.92ms
step:2064/2330 train_time:78276ms step_avg:37.92ms
step:2065/2330 train_time:78311ms step_avg:37.92ms
step:2066/2330 train_time:78352ms step_avg:37.92ms
step:2067/2330 train_time:78388ms step_avg:37.92ms
step:2068/2330 train_time:78428ms step_avg:37.92ms
step:2069/2330 train_time:78464ms step_avg:37.92ms
step:2070/2330 train_time:78504ms step_avg:37.92ms
step:2071/2330 train_time:78539ms step_avg:37.92ms
step:2072/2330 train_time:78579ms step_avg:37.92ms
step:2073/2330 train_time:78614ms step_avg:37.92ms
step:2074/2330 train_time:78654ms step_avg:37.92ms
step:2075/2330 train_time:78690ms step_avg:37.92ms
step:2076/2330 train_time:78730ms step_avg:37.92ms
step:2077/2330 train_time:78766ms step_avg:37.92ms
step:2078/2330 train_time:78806ms step_avg:37.92ms
step:2079/2330 train_time:78842ms step_avg:37.92ms
step:2080/2330 train_time:78882ms step_avg:37.92ms
step:2081/2330 train_time:78917ms step_avg:37.92ms
step:2082/2330 train_time:78957ms step_avg:37.92ms
step:2083/2330 train_time:78992ms step_avg:37.92ms
step:2084/2330 train_time:79033ms step_avg:37.92ms
step:2085/2330 train_time:79067ms step_avg:37.92ms
step:2086/2330 train_time:79108ms step_avg:37.92ms
step:2087/2330 train_time:79143ms step_avg:37.92ms
step:2088/2330 train_time:79183ms step_avg:37.92ms
step:2089/2330 train_time:79218ms step_avg:37.92ms
step:2090/2330 train_time:79259ms step_avg:37.92ms
step:2091/2330 train_time:79294ms step_avg:37.92ms
step:2092/2330 train_time:79336ms step_avg:37.92ms
step:2093/2330 train_time:79371ms step_avg:37.92ms
step:2094/2330 train_time:79412ms step_avg:37.92ms
step:2095/2330 train_time:79447ms step_avg:37.92ms
step:2096/2330 train_time:79488ms step_avg:37.92ms
step:2097/2330 train_time:79524ms step_avg:37.92ms
step:2098/2330 train_time:79564ms step_avg:37.92ms
step:2099/2330 train_time:79600ms step_avg:37.92ms
step:2100/2330 train_time:79640ms step_avg:37.92ms
step:2101/2330 train_time:79675ms step_avg:37.92ms
step:2102/2330 train_time:79716ms step_avg:37.92ms
step:2103/2330 train_time:79750ms step_avg:37.92ms
step:2104/2330 train_time:79791ms step_avg:37.92ms
step:2105/2330 train_time:79826ms step_avg:37.92ms
step:2106/2330 train_time:79867ms step_avg:37.92ms
step:2107/2330 train_time:79902ms step_avg:37.92ms
step:2108/2330 train_time:79943ms step_avg:37.92ms
step:2109/2330 train_time:79977ms step_avg:37.92ms
step:2110/2330 train_time:80018ms step_avg:37.92ms
step:2111/2330 train_time:80052ms step_avg:37.92ms
step:2112/2330 train_time:80093ms step_avg:37.92ms
step:2113/2330 train_time:80128ms step_avg:37.92ms
step:2114/2330 train_time:80169ms step_avg:37.92ms
step:2115/2330 train_time:80204ms step_avg:37.92ms
step:2116/2330 train_time:80245ms step_avg:37.92ms
step:2117/2330 train_time:80281ms step_avg:37.92ms
step:2118/2330 train_time:80322ms step_avg:37.92ms
step:2119/2330 train_time:80358ms step_avg:37.92ms
step:2120/2330 train_time:80398ms step_avg:37.92ms
step:2121/2330 train_time:80433ms step_avg:37.92ms
step:2122/2330 train_time:80474ms step_avg:37.92ms
step:2123/2330 train_time:80509ms step_avg:37.92ms
step:2124/2330 train_time:80550ms step_avg:37.92ms
step:2125/2330 train_time:80585ms step_avg:37.92ms
step:2126/2330 train_time:80626ms step_avg:37.92ms
step:2127/2330 train_time:80660ms step_avg:37.92ms
step:2128/2330 train_time:80701ms step_avg:37.92ms
step:2129/2330 train_time:80735ms step_avg:37.92ms
step:2130/2330 train_time:80776ms step_avg:37.92ms
step:2131/2330 train_time:80811ms step_avg:37.92ms
step:2132/2330 train_time:80851ms step_avg:37.92ms
step:2133/2330 train_time:80888ms step_avg:37.92ms
step:2134/2330 train_time:80928ms step_avg:37.92ms
step:2135/2330 train_time:80964ms step_avg:37.92ms
step:2136/2330 train_time:81005ms step_avg:37.92ms
step:2137/2330 train_time:81040ms step_avg:37.92ms
step:2138/2330 train_time:81080ms step_avg:37.92ms
step:2139/2330 train_time:81115ms step_avg:37.92ms
step:2140/2330 train_time:81156ms step_avg:37.92ms
step:2141/2330 train_time:81191ms step_avg:37.92ms
step:2142/2330 train_time:81232ms step_avg:37.92ms
step:2143/2330 train_time:81268ms step_avg:37.92ms
step:2144/2330 train_time:81309ms step_avg:37.92ms
step:2145/2330 train_time:81344ms step_avg:37.92ms
step:2146/2330 train_time:81385ms step_avg:37.92ms
step:2147/2330 train_time:81420ms step_avg:37.92ms
step:2148/2330 train_time:81461ms step_avg:37.92ms
step:2149/2330 train_time:81496ms step_avg:37.92ms
step:2150/2330 train_time:81537ms step_avg:37.92ms
step:2151/2330 train_time:81572ms step_avg:37.92ms
step:2152/2330 train_time:81613ms step_avg:37.92ms
step:2153/2330 train_time:81647ms step_avg:37.92ms
step:2154/2330 train_time:81688ms step_avg:37.92ms
step:2155/2330 train_time:81724ms step_avg:37.92ms
step:2156/2330 train_time:81765ms step_avg:37.92ms
step:2157/2330 train_time:81800ms step_avg:37.92ms
step:2158/2330 train_time:81840ms step_avg:37.92ms
step:2159/2330 train_time:81875ms step_avg:37.92ms
step:2160/2330 train_time:81916ms step_avg:37.92ms
step:2161/2330 train_time:81950ms step_avg:37.92ms
step:2162/2330 train_time:81991ms step_avg:37.92ms
step:2163/2330 train_time:82027ms step_avg:37.92ms
step:2164/2330 train_time:82068ms step_avg:37.92ms
step:2165/2330 train_time:82103ms step_avg:37.92ms
step:2166/2330 train_time:82143ms step_avg:37.92ms
step:2167/2330 train_time:82178ms step_avg:37.92ms
step:2168/2330 train_time:82219ms step_avg:37.92ms
step:2169/2330 train_time:82254ms step_avg:37.92ms
step:2170/2330 train_time:82295ms step_avg:37.92ms
step:2171/2330 train_time:82331ms step_avg:37.92ms
step:2172/2330 train_time:82372ms step_avg:37.92ms
step:2173/2330 train_time:82407ms step_avg:37.92ms
step:2174/2330 train_time:82448ms step_avg:37.92ms
step:2175/2330 train_time:82482ms step_avg:37.92ms
step:2176/2330 train_time:82523ms step_avg:37.92ms
step:2177/2330 train_time:82558ms step_avg:37.92ms
step:2178/2330 train_time:82598ms step_avg:37.92ms
step:2179/2330 train_time:82635ms step_avg:37.92ms
step:2180/2330 train_time:82677ms step_avg:37.93ms
step:2181/2330 train_time:82711ms step_avg:37.92ms
step:2182/2330 train_time:82751ms step_avg:37.92ms
step:2183/2330 train_time:82786ms step_avg:37.92ms
step:2184/2330 train_time:82827ms step_avg:37.92ms
step:2185/2330 train_time:82862ms step_avg:37.92ms
step:2186/2330 train_time:82902ms step_avg:37.92ms
step:2187/2330 train_time:82937ms step_avg:37.92ms
step:2188/2330 train_time:82978ms step_avg:37.92ms
step:2189/2330 train_time:83013ms step_avg:37.92ms
step:2190/2330 train_time:83053ms step_avg:37.92ms
step:2191/2330 train_time:83089ms step_avg:37.92ms
step:2192/2330 train_time:83129ms step_avg:37.92ms
step:2193/2330 train_time:83165ms step_avg:37.92ms
step:2194/2330 train_time:83205ms step_avg:37.92ms
step:2195/2330 train_time:83240ms step_avg:37.92ms
step:2196/2330 train_time:83281ms step_avg:37.92ms
step:2197/2330 train_time:83316ms step_avg:37.92ms
step:2198/2330 train_time:83357ms step_avg:37.92ms
step:2199/2330 train_time:83393ms step_avg:37.92ms
step:2200/2330 train_time:83433ms step_avg:37.92ms
step:2201/2330 train_time:83469ms step_avg:37.92ms
step:2202/2330 train_time:83510ms step_avg:37.92ms
step:2203/2330 train_time:83545ms step_avg:37.92ms
step:2204/2330 train_time:83585ms step_avg:37.92ms
step:2205/2330 train_time:83621ms step_avg:37.92ms
step:2206/2330 train_time:83661ms step_avg:37.92ms
step:2207/2330 train_time:83696ms step_avg:37.92ms
step:2208/2330 train_time:83738ms step_avg:37.92ms
step:2209/2330 train_time:83773ms step_avg:37.92ms
step:2210/2330 train_time:83814ms step_avg:37.92ms
step:2211/2330 train_time:83849ms step_avg:37.92ms
step:2212/2330 train_time:83890ms step_avg:37.92ms
step:2213/2330 train_time:83925ms step_avg:37.92ms
step:2214/2330 train_time:83965ms step_avg:37.92ms
step:2215/2330 train_time:84000ms step_avg:37.92ms
step:2216/2330 train_time:84040ms step_avg:37.92ms
step:2217/2330 train_time:84076ms step_avg:37.92ms
step:2218/2330 train_time:84116ms step_avg:37.92ms
step:2219/2330 train_time:84152ms step_avg:37.92ms
step:2220/2330 train_time:84193ms step_avg:37.92ms
step:2221/2330 train_time:84228ms step_avg:37.92ms
step:2222/2330 train_time:84269ms step_avg:37.92ms
step:2223/2330 train_time:84304ms step_avg:37.92ms
step:2224/2330 train_time:84345ms step_avg:37.92ms
step:2225/2330 train_time:84380ms step_avg:37.92ms
step:2226/2330 train_time:84420ms step_avg:37.92ms
step:2227/2330 train_time:84455ms step_avg:37.92ms
step:2228/2330 train_time:84496ms step_avg:37.92ms
step:2229/2330 train_time:84532ms step_avg:37.92ms
step:2230/2330 train_time:84573ms step_avg:37.92ms
step:2231/2330 train_time:84608ms step_avg:37.92ms
step:2232/2330 train_time:84649ms step_avg:37.93ms
step:2233/2330 train_time:84685ms step_avg:37.92ms
step:2234/2330 train_time:84725ms step_avg:37.93ms
step:2235/2330 train_time:84760ms step_avg:37.92ms
step:2236/2330 train_time:84800ms step_avg:37.93ms
step:2237/2330 train_time:84835ms step_avg:37.92ms
step:2238/2330 train_time:84877ms step_avg:37.93ms
step:2239/2330 train_time:84911ms step_avg:37.92ms
step:2240/2330 train_time:84951ms step_avg:37.92ms
step:2241/2330 train_time:84987ms step_avg:37.92ms
step:2242/2330 train_time:85028ms step_avg:37.92ms
step:2243/2330 train_time:85062ms step_avg:37.92ms
step:2244/2330 train_time:85103ms step_avg:37.92ms
step:2245/2330 train_time:85137ms step_avg:37.92ms
step:2246/2330 train_time:85178ms step_avg:37.92ms
step:2247/2330 train_time:85212ms step_avg:37.92ms
step:2248/2330 train_time:85253ms step_avg:37.92ms
step:2249/2330 train_time:85289ms step_avg:37.92ms
step:2250/2330 train_time:85330ms step_avg:37.92ms
step:2250/2330 val_loss:5.2015 train_time:85441ms step_avg:37.97ms
step:2251/2330 train_time:85452ms step_avg:37.96ms
step:2252/2330 train_time:85463ms step_avg:37.95ms
step:2253/2330 train_time:85472ms step_avg:37.94ms
step:2254/2330 train_time:85484ms step_avg:37.93ms
step:2255/2330 train_time:85519ms step_avg:37.92ms
step:2256/2330 train_time:85559ms step_avg:37.93ms
step:2257/2330 train_time:85594ms step_avg:37.92ms
step:2258/2330 train_time:85634ms step_avg:37.92ms
step:2259/2330 train_time:85669ms step_avg:37.92ms
step:2260/2330 train_time:85710ms step_avg:37.92ms
step:2261/2330 train_time:85744ms step_avg:37.92ms
step:2262/2330 train_time:85786ms step_avg:37.92ms
step:2263/2330 train_time:85822ms step_avg:37.92ms
step:2264/2330 train_time:85863ms step_avg:37.93ms
step:2265/2330 train_time:85898ms step_avg:37.92ms
step:2266/2330 train_time:85938ms step_avg:37.93ms
step:2267/2330 train_time:85973ms step_avg:37.92ms
step:2268/2330 train_time:86014ms step_avg:37.93ms
step:2269/2330 train_time:86050ms step_avg:37.92ms
step:2270/2330 train_time:86090ms step_avg:37.93ms
step:2271/2330 train_time:86126ms step_avg:37.92ms
step:2272/2330 train_time:86166ms step_avg:37.93ms
step:2273/2330 train_time:86201ms step_avg:37.92ms
step:2274/2330 train_time:86241ms step_avg:37.92ms
step:2275/2330 train_time:86276ms step_avg:37.92ms
step:2276/2330 train_time:86316ms step_avg:37.92ms
step:2277/2330 train_time:86350ms step_avg:37.92ms
step:2278/2330 train_time:86391ms step_avg:37.92ms
step:2279/2330 train_time:86427ms step_avg:37.92ms
step:2280/2330 train_time:86468ms step_avg:37.92ms
step:2281/2330 train_time:86503ms step_avg:37.92ms
step:2282/2330 train_time:86544ms step_avg:37.92ms
step:2283/2330 train_time:86579ms step_avg:37.92ms
step:2284/2330 train_time:86619ms step_avg:37.92ms
step:2285/2330 train_time:86654ms step_avg:37.92ms
step:2286/2330 train_time:86695ms step_avg:37.92ms
step:2287/2330 train_time:86730ms step_avg:37.92ms
step:2288/2330 train_time:86770ms step_avg:37.92ms
step:2289/2330 train_time:86807ms step_avg:37.92ms
step:2290/2330 train_time:86849ms step_avg:37.93ms
step:2291/2330 train_time:86884ms step_avg:37.92ms
step:2292/2330 train_time:86925ms step_avg:37.93ms
step:2293/2330 train_time:86959ms step_avg:37.92ms
step:2294/2330 train_time:87000ms step_avg:37.92ms
step:2295/2330 train_time:87035ms step_avg:37.92ms
step:2296/2330 train_time:87076ms step_avg:37.92ms
step:2297/2330 train_time:87110ms step_avg:37.92ms
step:2298/2330 train_time:87151ms step_avg:37.92ms
step:2299/2330 train_time:87186ms step_avg:37.92ms
step:2300/2330 train_time:87227ms step_avg:37.92ms
step:2301/2330 train_time:87262ms step_avg:37.92ms
step:2302/2330 train_time:87302ms step_avg:37.92ms
step:2303/2330 train_time:87337ms step_avg:37.92ms
step:2304/2330 train_time:87378ms step_avg:37.92ms
step:2305/2330 train_time:87412ms step_avg:37.92ms
step:2306/2330 train_time:87453ms step_avg:37.92ms
step:2307/2330 train_time:87489ms step_avg:37.92ms
step:2308/2330 train_time:87529ms step_avg:37.92ms
step:2309/2330 train_time:87564ms step_avg:37.92ms
step:2310/2330 train_time:87604ms step_avg:37.92ms
step:2311/2330 train_time:87639ms step_avg:37.92ms
step:2312/2330 train_time:87679ms step_avg:37.92ms
step:2313/2330 train_time:87714ms step_avg:37.92ms
step:2314/2330 train_time:87755ms step_avg:37.92ms
step:2315/2330 train_time:87790ms step_avg:37.92ms
step:2316/2330 train_time:87831ms step_avg:37.92ms
step:2317/2330 train_time:87866ms step_avg:37.92ms
step:2318/2330 train_time:87907ms step_avg:37.92ms
step:2319/2330 train_time:87943ms step_avg:37.92ms
step:2320/2330 train_time:87984ms step_avg:37.92ms
step:2321/2330 train_time:88019ms step_avg:37.92ms
step:2322/2330 train_time:88060ms step_avg:37.92ms
step:2323/2330 train_time:88095ms step_avg:37.92ms
step:2324/2330 train_time:88136ms step_avg:37.92ms
step:2325/2330 train_time:88171ms step_avg:37.92ms
step:2326/2330 train_time:88211ms step_avg:37.92ms
step:2327/2330 train_time:88247ms step_avg:37.92ms
step:2328/2330 train_time:88288ms step_avg:37.92ms
step:2329/2330 train_time:88321ms step_avg:37.92ms
step:2330/2330 train_time:88362ms step_avg:37.92ms
step:2330/2330 val_loss:5.1943 train_time:88472ms step_avg:37.97ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
