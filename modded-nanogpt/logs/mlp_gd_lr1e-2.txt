import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr1e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:23:43 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2330 train_time:82ms step_avg:82.31ms
step:2/2330 train_time:146ms step_avg:73.02ms
step:3/2330 train_time:157ms step_avg:52.49ms
step:4/2330 train_time:169ms step_avg:42.33ms
step:5/2330 train_time:180ms step_avg:36.00ms
step:6/2330 train_time:206ms step_avg:34.36ms
step:7/2330 train_time:227ms step_avg:32.47ms
step:8/2330 train_time:282ms step_avg:35.27ms
step:9/2330 train_time:304ms step_avg:33.81ms
step:10/2330 train_time:360ms step_avg:35.99ms
step:11/2330 train_time:382ms step_avg:34.73ms
step:12/2330 train_time:437ms step_avg:36.43ms
step:13/2330 train_time:459ms step_avg:35.29ms
step:14/2330 train_time:514ms step_avg:36.75ms
step:15/2330 train_time:536ms step_avg:35.74ms
step:16/2330 train_time:592ms step_avg:36.98ms
step:17/2330 train_time:613ms step_avg:36.09ms
step:18/2330 train_time:669ms step_avg:37.16ms
step:19/2330 train_time:691ms step_avg:36.38ms
step:20/2330 train_time:747ms step_avg:37.33ms
step:21/2330 train_time:769ms step_avg:36.64ms
step:22/2330 train_time:825ms step_avg:37.52ms
step:23/2330 train_time:848ms step_avg:36.86ms
step:24/2330 train_time:903ms step_avg:37.65ms
step:25/2330 train_time:926ms step_avg:37.03ms
step:26/2330 train_time:986ms step_avg:37.92ms
step:27/2330 train_time:1013ms step_avg:37.52ms
step:28/2330 train_time:1075ms step_avg:38.38ms
step:29/2330 train_time:1101ms step_avg:37.95ms
step:30/2330 train_time:1159ms step_avg:38.63ms
step:31/2330 train_time:1181ms step_avg:38.11ms
step:32/2330 train_time:1238ms step_avg:38.69ms
step:33/2330 train_time:1260ms step_avg:38.19ms
step:34/2330 train_time:1317ms step_avg:38.74ms
step:35/2330 train_time:1339ms step_avg:38.26ms
step:36/2330 train_time:1395ms step_avg:38.75ms
step:37/2330 train_time:1417ms step_avg:38.30ms
step:38/2330 train_time:1473ms step_avg:38.76ms
step:39/2330 train_time:1495ms step_avg:38.33ms
step:40/2330 train_time:1550ms step_avg:38.76ms
step:41/2330 train_time:1572ms step_avg:38.35ms
step:42/2330 train_time:1628ms step_avg:38.76ms
step:43/2330 train_time:1651ms step_avg:38.39ms
step:44/2330 train_time:1707ms step_avg:38.79ms
step:45/2330 train_time:1729ms step_avg:38.42ms
step:46/2330 train_time:1785ms step_avg:38.80ms
step:47/2330 train_time:1807ms step_avg:38.45ms
step:48/2330 train_time:1863ms step_avg:38.82ms
step:49/2330 train_time:1886ms step_avg:38.49ms
step:50/2330 train_time:1944ms step_avg:38.89ms
step:51/2330 train_time:1968ms step_avg:38.60ms
step:52/2330 train_time:2028ms step_avg:39.00ms
step:53/2330 train_time:2053ms step_avg:38.73ms
step:54/2330 train_time:2111ms step_avg:39.09ms
step:55/2330 train_time:2136ms step_avg:38.83ms
step:56/2330 train_time:2194ms step_avg:39.18ms
step:57/2330 train_time:2219ms step_avg:38.92ms
step:58/2330 train_time:2275ms step_avg:39.23ms
step:59/2330 train_time:2298ms step_avg:38.95ms
step:60/2330 train_time:2354ms step_avg:39.24ms
step:61/2330 train_time:2377ms step_avg:38.96ms
step:62/2330 train_time:2433ms step_avg:39.25ms
step:63/2330 train_time:2456ms step_avg:38.98ms
step:64/2330 train_time:2512ms step_avg:39.24ms
step:65/2330 train_time:2534ms step_avg:38.99ms
step:66/2330 train_time:2590ms step_avg:39.25ms
step:67/2330 train_time:2613ms step_avg:39.00ms
step:68/2330 train_time:2670ms step_avg:39.26ms
step:69/2330 train_time:2693ms step_avg:39.02ms
step:70/2330 train_time:2749ms step_avg:39.27ms
step:71/2330 train_time:2772ms step_avg:39.04ms
step:72/2330 train_time:2828ms step_avg:39.28ms
step:73/2330 train_time:2851ms step_avg:39.06ms
step:74/2330 train_time:2909ms step_avg:39.31ms
step:75/2330 train_time:2933ms step_avg:39.10ms
step:76/2330 train_time:2989ms step_avg:39.33ms
step:77/2330 train_time:3013ms step_avg:39.14ms
step:78/2330 train_time:3071ms step_avg:39.38ms
step:79/2330 train_time:3096ms step_avg:39.19ms
step:80/2330 train_time:3153ms step_avg:39.42ms
step:81/2330 train_time:3178ms step_avg:39.23ms
step:82/2330 train_time:3235ms step_avg:39.45ms
step:83/2330 train_time:3258ms step_avg:39.26ms
step:84/2330 train_time:3316ms step_avg:39.48ms
step:85/2330 train_time:3338ms step_avg:39.27ms
step:86/2330 train_time:3395ms step_avg:39.47ms
step:87/2330 train_time:3417ms step_avg:39.27ms
step:88/2330 train_time:3474ms step_avg:39.47ms
step:89/2330 train_time:3496ms step_avg:39.28ms
step:90/2330 train_time:3553ms step_avg:39.47ms
step:91/2330 train_time:3575ms step_avg:39.29ms
step:92/2330 train_time:3631ms step_avg:39.47ms
step:93/2330 train_time:3654ms step_avg:39.29ms
step:94/2330 train_time:3710ms step_avg:39.47ms
step:95/2330 train_time:3733ms step_avg:39.30ms
step:96/2330 train_time:3790ms step_avg:39.48ms
step:97/2330 train_time:3814ms step_avg:39.31ms
step:98/2330 train_time:3871ms step_avg:39.50ms
step:99/2330 train_time:3893ms step_avg:39.32ms
step:100/2330 train_time:3950ms step_avg:39.50ms
step:101/2330 train_time:3974ms step_avg:39.34ms
step:102/2330 train_time:4031ms step_avg:39.52ms
step:103/2330 train_time:4056ms step_avg:39.38ms
step:104/2330 train_time:4113ms step_avg:39.54ms
step:105/2330 train_time:4136ms step_avg:39.39ms
step:106/2330 train_time:4194ms step_avg:39.56ms
step:107/2330 train_time:4217ms step_avg:39.41ms
step:108/2330 train_time:4274ms step_avg:39.57ms
step:109/2330 train_time:4297ms step_avg:39.42ms
step:110/2330 train_time:4354ms step_avg:39.58ms
step:111/2330 train_time:4376ms step_avg:39.42ms
step:112/2330 train_time:4432ms step_avg:39.57ms
step:113/2330 train_time:4455ms step_avg:39.42ms
step:114/2330 train_time:4511ms step_avg:39.57ms
step:115/2330 train_time:4534ms step_avg:39.43ms
step:116/2330 train_time:4590ms step_avg:39.57ms
step:117/2330 train_time:4614ms step_avg:39.43ms
step:118/2330 train_time:4670ms step_avg:39.58ms
step:119/2330 train_time:4693ms step_avg:39.44ms
step:120/2330 train_time:4750ms step_avg:39.59ms
step:121/2330 train_time:4774ms step_avg:39.46ms
step:122/2330 train_time:4831ms step_avg:39.60ms
step:123/2330 train_time:4855ms step_avg:39.47ms
step:124/2330 train_time:4912ms step_avg:39.61ms
step:125/2330 train_time:4934ms step_avg:39.48ms
step:126/2330 train_time:4991ms step_avg:39.61ms
step:127/2330 train_time:5015ms step_avg:39.49ms
step:128/2330 train_time:5073ms step_avg:39.63ms
step:129/2330 train_time:5097ms step_avg:39.51ms
step:130/2330 train_time:5154ms step_avg:39.65ms
step:131/2330 train_time:5177ms step_avg:39.52ms
step:132/2330 train_time:5234ms step_avg:39.65ms
step:133/2330 train_time:5256ms step_avg:39.52ms
step:134/2330 train_time:5312ms step_avg:39.64ms
step:135/2330 train_time:5335ms step_avg:39.52ms
step:136/2330 train_time:5392ms step_avg:39.65ms
step:137/2330 train_time:5415ms step_avg:39.53ms
step:138/2330 train_time:5472ms step_avg:39.65ms
step:139/2330 train_time:5494ms step_avg:39.53ms
step:140/2330 train_time:5551ms step_avg:39.65ms
step:141/2330 train_time:5573ms step_avg:39.53ms
step:142/2330 train_time:5630ms step_avg:39.65ms
step:143/2330 train_time:5653ms step_avg:39.53ms
step:144/2330 train_time:5709ms step_avg:39.65ms
step:145/2330 train_time:5733ms step_avg:39.54ms
step:146/2330 train_time:5789ms step_avg:39.65ms
step:147/2330 train_time:5813ms step_avg:39.54ms
step:148/2330 train_time:5870ms step_avg:39.66ms
step:149/2330 train_time:5893ms step_avg:39.55ms
step:150/2330 train_time:5950ms step_avg:39.66ms
step:151/2330 train_time:5973ms step_avg:39.55ms
step:152/2330 train_time:6029ms step_avg:39.67ms
step:153/2330 train_time:6053ms step_avg:39.56ms
step:154/2330 train_time:6110ms step_avg:39.67ms
step:155/2330 train_time:6133ms step_avg:39.57ms
step:156/2330 train_time:6190ms step_avg:39.68ms
step:157/2330 train_time:6214ms step_avg:39.58ms
step:158/2330 train_time:6271ms step_avg:39.69ms
step:159/2330 train_time:6294ms step_avg:39.59ms
step:160/2330 train_time:6350ms step_avg:39.69ms
step:161/2330 train_time:6374ms step_avg:39.59ms
step:162/2330 train_time:6431ms step_avg:39.70ms
step:163/2330 train_time:6454ms step_avg:39.60ms
step:164/2330 train_time:6510ms step_avg:39.70ms
step:165/2330 train_time:6534ms step_avg:39.60ms
step:166/2330 train_time:6590ms step_avg:39.70ms
step:167/2330 train_time:6613ms step_avg:39.60ms
step:168/2330 train_time:6669ms step_avg:39.70ms
step:169/2330 train_time:6693ms step_avg:39.60ms
step:170/2330 train_time:6749ms step_avg:39.70ms
step:171/2330 train_time:6772ms step_avg:39.60ms
step:172/2330 train_time:6829ms step_avg:39.70ms
step:173/2330 train_time:6853ms step_avg:39.61ms
step:174/2330 train_time:6909ms step_avg:39.71ms
step:175/2330 train_time:6932ms step_avg:39.61ms
step:176/2330 train_time:6989ms step_avg:39.71ms
step:177/2330 train_time:7013ms step_avg:39.62ms
step:178/2330 train_time:7070ms step_avg:39.72ms
step:179/2330 train_time:7093ms step_avg:39.62ms
step:180/2330 train_time:7149ms step_avg:39.72ms
step:181/2330 train_time:7173ms step_avg:39.63ms
step:182/2330 train_time:7230ms step_avg:39.72ms
step:183/2330 train_time:7253ms step_avg:39.63ms
step:184/2330 train_time:7310ms step_avg:39.73ms
step:185/2330 train_time:7333ms step_avg:39.64ms
step:186/2330 train_time:7390ms step_avg:39.73ms
step:187/2330 train_time:7413ms step_avg:39.64ms
step:188/2330 train_time:7470ms step_avg:39.73ms
step:189/2330 train_time:7493ms step_avg:39.65ms
step:190/2330 train_time:7549ms step_avg:39.73ms
step:191/2330 train_time:7573ms step_avg:39.65ms
step:192/2330 train_time:7629ms step_avg:39.74ms
step:193/2330 train_time:7654ms step_avg:39.66ms
step:194/2330 train_time:7710ms step_avg:39.74ms
step:195/2330 train_time:7733ms step_avg:39.66ms
step:196/2330 train_time:7789ms step_avg:39.74ms
step:197/2330 train_time:7813ms step_avg:39.66ms
step:198/2330 train_time:7870ms step_avg:39.75ms
step:199/2330 train_time:7893ms step_avg:39.67ms
step:200/2330 train_time:7950ms step_avg:39.75ms
step:201/2330 train_time:7974ms step_avg:39.67ms
step:202/2330 train_time:8031ms step_avg:39.76ms
step:203/2330 train_time:8055ms step_avg:39.68ms
step:204/2330 train_time:8112ms step_avg:39.76ms
step:205/2330 train_time:8136ms step_avg:39.69ms
step:206/2330 train_time:8193ms step_avg:39.77ms
step:207/2330 train_time:8216ms step_avg:39.69ms
step:208/2330 train_time:8272ms step_avg:39.77ms
step:209/2330 train_time:8296ms step_avg:39.69ms
step:210/2330 train_time:8353ms step_avg:39.78ms
step:211/2330 train_time:8376ms step_avg:39.70ms
step:212/2330 train_time:8433ms step_avg:39.78ms
step:213/2330 train_time:8456ms step_avg:39.70ms
step:214/2330 train_time:8513ms step_avg:39.78ms
step:215/2330 train_time:8536ms step_avg:39.70ms
step:216/2330 train_time:8593ms step_avg:39.78ms
step:217/2330 train_time:8616ms step_avg:39.70ms
step:218/2330 train_time:8673ms step_avg:39.78ms
step:219/2330 train_time:8696ms step_avg:39.71ms
step:220/2330 train_time:8752ms step_avg:39.78ms
step:221/2330 train_time:8776ms step_avg:39.71ms
step:222/2330 train_time:8832ms step_avg:39.78ms
step:223/2330 train_time:8855ms step_avg:39.71ms
step:224/2330 train_time:8912ms step_avg:39.79ms
step:225/2330 train_time:8935ms step_avg:39.71ms
step:226/2330 train_time:8992ms step_avg:39.79ms
step:227/2330 train_time:9016ms step_avg:39.72ms
step:228/2330 train_time:9073ms step_avg:39.79ms
step:229/2330 train_time:9096ms step_avg:39.72ms
step:230/2330 train_time:9154ms step_avg:39.80ms
step:231/2330 train_time:9178ms step_avg:39.73ms
step:232/2330 train_time:9234ms step_avg:39.80ms
step:233/2330 train_time:9256ms step_avg:39.73ms
step:234/2330 train_time:9314ms step_avg:39.80ms
step:235/2330 train_time:9337ms step_avg:39.73ms
step:236/2330 train_time:9394ms step_avg:39.80ms
step:237/2330 train_time:9416ms step_avg:39.73ms
step:238/2330 train_time:9473ms step_avg:39.80ms
step:239/2330 train_time:9496ms step_avg:39.73ms
step:240/2330 train_time:9553ms step_avg:39.81ms
step:241/2330 train_time:9576ms step_avg:39.74ms
step:242/2330 train_time:9634ms step_avg:39.81ms
step:243/2330 train_time:9656ms step_avg:39.74ms
step:244/2330 train_time:9713ms step_avg:39.81ms
step:245/2330 train_time:9737ms step_avg:39.74ms
step:246/2330 train_time:9794ms step_avg:39.81ms
step:247/2330 train_time:9817ms step_avg:39.74ms
step:248/2330 train_time:9874ms step_avg:39.81ms
step:249/2330 train_time:9896ms step_avg:39.74ms
step:250/2330 train_time:9953ms step_avg:39.81ms
step:250/2330 val_loss:5.6125 train_time:10051ms step_avg:40.20ms
step:251/2330 train_time:10062ms step_avg:40.09ms
step:252/2330 train_time:10074ms step_avg:39.98ms
step:253/2330 train_time:10084ms step_avg:39.86ms
step:254/2330 train_time:10114ms step_avg:39.82ms
step:255/2330 train_time:10136ms step_avg:39.75ms
step:256/2330 train_time:10192ms step_avg:39.81ms
step:257/2330 train_time:10214ms step_avg:39.74ms
step:258/2330 train_time:10269ms step_avg:39.80ms
step:259/2330 train_time:10291ms step_avg:39.74ms
step:260/2330 train_time:10348ms step_avg:39.80ms
step:261/2330 train_time:10375ms step_avg:39.75ms
step:262/2330 train_time:10437ms step_avg:39.83ms
step:263/2330 train_time:10460ms step_avg:39.77ms
step:264/2330 train_time:10518ms step_avg:39.84ms
step:265/2330 train_time:10541ms step_avg:39.78ms
step:266/2330 train_time:10598ms step_avg:39.84ms
step:267/2330 train_time:10620ms step_avg:39.78ms
step:268/2330 train_time:10677ms step_avg:39.84ms
step:269/2330 train_time:10699ms step_avg:39.77ms
step:270/2330 train_time:10755ms step_avg:39.83ms
step:271/2330 train_time:10778ms step_avg:39.77ms
step:272/2330 train_time:10834ms step_avg:39.83ms
step:273/2330 train_time:10856ms step_avg:39.77ms
step:274/2330 train_time:10913ms step_avg:39.83ms
step:275/2330 train_time:10936ms step_avg:39.77ms
step:276/2330 train_time:10995ms step_avg:39.84ms
step:277/2330 train_time:11018ms step_avg:39.77ms
step:278/2330 train_time:11075ms step_avg:39.84ms
step:279/2330 train_time:11097ms step_avg:39.77ms
step:280/2330 train_time:11154ms step_avg:39.84ms
step:281/2330 train_time:11176ms step_avg:39.77ms
step:282/2330 train_time:11233ms step_avg:39.84ms
step:283/2330 train_time:11256ms step_avg:39.77ms
step:284/2330 train_time:11314ms step_avg:39.84ms
step:285/2330 train_time:11338ms step_avg:39.78ms
step:286/2330 train_time:11396ms step_avg:39.85ms
step:287/2330 train_time:11420ms step_avg:39.79ms
step:288/2330 train_time:11478ms step_avg:39.86ms
step:289/2330 train_time:11501ms step_avg:39.80ms
step:290/2330 train_time:11557ms step_avg:39.85ms
step:291/2330 train_time:11581ms step_avg:39.80ms
step:292/2330 train_time:11638ms step_avg:39.86ms
step:293/2330 train_time:11662ms step_avg:39.80ms
step:294/2330 train_time:11719ms step_avg:39.86ms
step:295/2330 train_time:11741ms step_avg:39.80ms
step:296/2330 train_time:11798ms step_avg:39.86ms
step:297/2330 train_time:11822ms step_avg:39.80ms
step:298/2330 train_time:11878ms step_avg:39.86ms
step:299/2330 train_time:11901ms step_avg:39.80ms
step:300/2330 train_time:11958ms step_avg:39.86ms
step:301/2330 train_time:11982ms step_avg:39.81ms
step:302/2330 train_time:12039ms step_avg:39.87ms
step:303/2330 train_time:12063ms step_avg:39.81ms
step:304/2330 train_time:12120ms step_avg:39.87ms
step:305/2330 train_time:12143ms step_avg:39.81ms
step:306/2330 train_time:12200ms step_avg:39.87ms
step:307/2330 train_time:12223ms step_avg:39.82ms
step:308/2330 train_time:12282ms step_avg:39.88ms
step:309/2330 train_time:12305ms step_avg:39.82ms
step:310/2330 train_time:12362ms step_avg:39.88ms
step:311/2330 train_time:12385ms step_avg:39.82ms
step:312/2330 train_time:12442ms step_avg:39.88ms
step:313/2330 train_time:12465ms step_avg:39.83ms
step:314/2330 train_time:12522ms step_avg:39.88ms
step:315/2330 train_time:12546ms step_avg:39.83ms
step:316/2330 train_time:12603ms step_avg:39.88ms
step:317/2330 train_time:12627ms step_avg:39.83ms
step:318/2330 train_time:12684ms step_avg:39.89ms
step:319/2330 train_time:12707ms step_avg:39.83ms
step:320/2330 train_time:12764ms step_avg:39.89ms
step:321/2330 train_time:12788ms step_avg:39.84ms
step:322/2330 train_time:12845ms step_avg:39.89ms
step:323/2330 train_time:12869ms step_avg:39.84ms
step:324/2330 train_time:12926ms step_avg:39.89ms
step:325/2330 train_time:12949ms step_avg:39.84ms
step:326/2330 train_time:13005ms step_avg:39.89ms
step:327/2330 train_time:13030ms step_avg:39.85ms
step:328/2330 train_time:13086ms step_avg:39.90ms
step:329/2330 train_time:13110ms step_avg:39.85ms
step:330/2330 train_time:13166ms step_avg:39.90ms
step:331/2330 train_time:13190ms step_avg:39.85ms
step:332/2330 train_time:13247ms step_avg:39.90ms
step:333/2330 train_time:13270ms step_avg:39.85ms
step:334/2330 train_time:13327ms step_avg:39.90ms
step:335/2330 train_time:13351ms step_avg:39.85ms
step:336/2330 train_time:13407ms step_avg:39.90ms
step:337/2330 train_time:13432ms step_avg:39.86ms
step:338/2330 train_time:13489ms step_avg:39.91ms
step:339/2330 train_time:13513ms step_avg:39.86ms
step:340/2330 train_time:13570ms step_avg:39.91ms
step:341/2330 train_time:13594ms step_avg:39.87ms
step:342/2330 train_time:13651ms step_avg:39.92ms
step:343/2330 train_time:13675ms step_avg:39.87ms
step:344/2330 train_time:13732ms step_avg:39.92ms
step:345/2330 train_time:13755ms step_avg:39.87ms
step:346/2330 train_time:13812ms step_avg:39.92ms
step:347/2330 train_time:13835ms step_avg:39.87ms
step:348/2330 train_time:13894ms step_avg:39.92ms
step:349/2330 train_time:13916ms step_avg:39.87ms
step:350/2330 train_time:13974ms step_avg:39.93ms
step:351/2330 train_time:13996ms step_avg:39.88ms
step:352/2330 train_time:14053ms step_avg:39.92ms
step:353/2330 train_time:14076ms step_avg:39.87ms
step:354/2330 train_time:14133ms step_avg:39.92ms
step:355/2330 train_time:14155ms step_avg:39.87ms
step:356/2330 train_time:14212ms step_avg:39.92ms
step:357/2330 train_time:14235ms step_avg:39.87ms
step:358/2330 train_time:14293ms step_avg:39.92ms
step:359/2330 train_time:14315ms step_avg:39.88ms
step:360/2330 train_time:14374ms step_avg:39.93ms
step:361/2330 train_time:14396ms step_avg:39.88ms
step:362/2330 train_time:14453ms step_avg:39.93ms
step:363/2330 train_time:14476ms step_avg:39.88ms
step:364/2330 train_time:14533ms step_avg:39.93ms
step:365/2330 train_time:14556ms step_avg:39.88ms
step:366/2330 train_time:14613ms step_avg:39.93ms
step:367/2330 train_time:14635ms step_avg:39.88ms
step:368/2330 train_time:14693ms step_avg:39.93ms
step:369/2330 train_time:14715ms step_avg:39.88ms
step:370/2330 train_time:14772ms step_avg:39.93ms
step:371/2330 train_time:14795ms step_avg:39.88ms
step:372/2330 train_time:14852ms step_avg:39.92ms
step:373/2330 train_time:14875ms step_avg:39.88ms
step:374/2330 train_time:14932ms step_avg:39.93ms
step:375/2330 train_time:14954ms step_avg:39.88ms
step:376/2330 train_time:15012ms step_avg:39.93ms
step:377/2330 train_time:15034ms step_avg:39.88ms
step:378/2330 train_time:15092ms step_avg:39.93ms
step:379/2330 train_time:15115ms step_avg:39.88ms
step:380/2330 train_time:15172ms step_avg:39.93ms
step:381/2330 train_time:15195ms step_avg:39.88ms
step:382/2330 train_time:15252ms step_avg:39.93ms
step:383/2330 train_time:15275ms step_avg:39.88ms
step:384/2330 train_time:15334ms step_avg:39.93ms
step:385/2330 train_time:15356ms step_avg:39.89ms
step:386/2330 train_time:15414ms step_avg:39.93ms
step:387/2330 train_time:15436ms step_avg:39.89ms
step:388/2330 train_time:15494ms step_avg:39.93ms
step:389/2330 train_time:15516ms step_avg:39.89ms
step:390/2330 train_time:15573ms step_avg:39.93ms
step:391/2330 train_time:15595ms step_avg:39.89ms
step:392/2330 train_time:15653ms step_avg:39.93ms
step:393/2330 train_time:15675ms step_avg:39.89ms
step:394/2330 train_time:15732ms step_avg:39.93ms
step:395/2330 train_time:15754ms step_avg:39.88ms
step:396/2330 train_time:15811ms step_avg:39.93ms
step:397/2330 train_time:15834ms step_avg:39.88ms
step:398/2330 train_time:15892ms step_avg:39.93ms
step:399/2330 train_time:15914ms step_avg:39.89ms
step:400/2330 train_time:15972ms step_avg:39.93ms
step:401/2330 train_time:15994ms step_avg:39.88ms
step:402/2330 train_time:16051ms step_avg:39.93ms
step:403/2330 train_time:16073ms step_avg:39.88ms
step:404/2330 train_time:16130ms step_avg:39.93ms
step:405/2330 train_time:16153ms step_avg:39.88ms
step:406/2330 train_time:16210ms step_avg:39.93ms
step:407/2330 train_time:16233ms step_avg:39.88ms
step:408/2330 train_time:16290ms step_avg:39.93ms
step:409/2330 train_time:16314ms step_avg:39.89ms
step:410/2330 train_time:16372ms step_avg:39.93ms
step:411/2330 train_time:16394ms step_avg:39.89ms
step:412/2330 train_time:16452ms step_avg:39.93ms
step:413/2330 train_time:16475ms step_avg:39.89ms
step:414/2330 train_time:16533ms step_avg:39.93ms
step:415/2330 train_time:16555ms step_avg:39.89ms
step:416/2330 train_time:16612ms step_avg:39.93ms
step:417/2330 train_time:16635ms step_avg:39.89ms
step:418/2330 train_time:16692ms step_avg:39.93ms
step:419/2330 train_time:16715ms step_avg:39.89ms
step:420/2330 train_time:16772ms step_avg:39.93ms
step:421/2330 train_time:16795ms step_avg:39.89ms
step:422/2330 train_time:16852ms step_avg:39.93ms
step:423/2330 train_time:16874ms step_avg:39.89ms
step:424/2330 train_time:16932ms step_avg:39.93ms
step:425/2330 train_time:16954ms step_avg:39.89ms
step:426/2330 train_time:17012ms step_avg:39.93ms
step:427/2330 train_time:17034ms step_avg:39.89ms
step:428/2330 train_time:17093ms step_avg:39.94ms
step:429/2330 train_time:17115ms step_avg:39.90ms
step:430/2330 train_time:17173ms step_avg:39.94ms
step:431/2330 train_time:17195ms step_avg:39.89ms
step:432/2330 train_time:17251ms step_avg:39.93ms
step:433/2330 train_time:17274ms step_avg:39.89ms
step:434/2330 train_time:17332ms step_avg:39.94ms
step:435/2330 train_time:17355ms step_avg:39.90ms
step:436/2330 train_time:17413ms step_avg:39.94ms
step:437/2330 train_time:17435ms step_avg:39.90ms
step:438/2330 train_time:17493ms step_avg:39.94ms
step:439/2330 train_time:17516ms step_avg:39.90ms
step:440/2330 train_time:17574ms step_avg:39.94ms
step:441/2330 train_time:17596ms step_avg:39.90ms
step:442/2330 train_time:17653ms step_avg:39.94ms
step:443/2330 train_time:17675ms step_avg:39.90ms
step:444/2330 train_time:17732ms step_avg:39.94ms
step:445/2330 train_time:17755ms step_avg:39.90ms
step:446/2330 train_time:17812ms step_avg:39.94ms
step:447/2330 train_time:17834ms step_avg:39.90ms
step:448/2330 train_time:17892ms step_avg:39.94ms
step:449/2330 train_time:17914ms step_avg:39.90ms
step:450/2330 train_time:17972ms step_avg:39.94ms
step:451/2330 train_time:17994ms step_avg:39.90ms
step:452/2330 train_time:18052ms step_avg:39.94ms
step:453/2330 train_time:18075ms step_avg:39.90ms
step:454/2330 train_time:18132ms step_avg:39.94ms
step:455/2330 train_time:18154ms step_avg:39.90ms
step:456/2330 train_time:18212ms step_avg:39.94ms
step:457/2330 train_time:18235ms step_avg:39.90ms
step:458/2330 train_time:18293ms step_avg:39.94ms
step:459/2330 train_time:18315ms step_avg:39.90ms
step:460/2330 train_time:18372ms step_avg:39.94ms
step:461/2330 train_time:18395ms step_avg:39.90ms
step:462/2330 train_time:18452ms step_avg:39.94ms
step:463/2330 train_time:18475ms step_avg:39.90ms
step:464/2330 train_time:18533ms step_avg:39.94ms
step:465/2330 train_time:18555ms step_avg:39.90ms
step:466/2330 train_time:18613ms step_avg:39.94ms
step:467/2330 train_time:18635ms step_avg:39.90ms
step:468/2330 train_time:18692ms step_avg:39.94ms
step:469/2330 train_time:18715ms step_avg:39.90ms
step:470/2330 train_time:18772ms step_avg:39.94ms
step:471/2330 train_time:18795ms step_avg:39.90ms
step:472/2330 train_time:18852ms step_avg:39.94ms
step:473/2330 train_time:18875ms step_avg:39.90ms
step:474/2330 train_time:18932ms step_avg:39.94ms
step:475/2330 train_time:18954ms step_avg:39.90ms
step:476/2330 train_time:19011ms step_avg:39.94ms
step:477/2330 train_time:19034ms step_avg:39.90ms
step:478/2330 train_time:19091ms step_avg:39.94ms
step:479/2330 train_time:19115ms step_avg:39.91ms
step:480/2330 train_time:19172ms step_avg:39.94ms
step:481/2330 train_time:19194ms step_avg:39.90ms
step:482/2330 train_time:19252ms step_avg:39.94ms
step:483/2330 train_time:19275ms step_avg:39.91ms
step:484/2330 train_time:19333ms step_avg:39.94ms
step:485/2330 train_time:19355ms step_avg:39.91ms
step:486/2330 train_time:19413ms step_avg:39.94ms
step:487/2330 train_time:19436ms step_avg:39.91ms
step:488/2330 train_time:19494ms step_avg:39.95ms
step:489/2330 train_time:19516ms step_avg:39.91ms
step:490/2330 train_time:19573ms step_avg:39.95ms
step:491/2330 train_time:19596ms step_avg:39.91ms
step:492/2330 train_time:19653ms step_avg:39.94ms
step:493/2330 train_time:19676ms step_avg:39.91ms
step:494/2330 train_time:19733ms step_avg:39.95ms
step:495/2330 train_time:19755ms step_avg:39.91ms
step:496/2330 train_time:19813ms step_avg:39.95ms
step:497/2330 train_time:19835ms step_avg:39.91ms
step:498/2330 train_time:19893ms step_avg:39.94ms
step:499/2330 train_time:19915ms step_avg:39.91ms
step:500/2330 train_time:19972ms step_avg:39.94ms
step:500/2330 val_loss:5.4285 train_time:20069ms step_avg:40.14ms
step:501/2330 train_time:20081ms step_avg:40.08ms
step:502/2330 train_time:20092ms step_avg:40.02ms
step:503/2330 train_time:20102ms step_avg:39.96ms
step:504/2330 train_time:20132ms step_avg:39.94ms
step:505/2330 train_time:20154ms step_avg:39.91ms
step:506/2330 train_time:20210ms step_avg:39.94ms
step:507/2330 train_time:20233ms step_avg:39.91ms
step:508/2330 train_time:20289ms step_avg:39.94ms
step:509/2330 train_time:20311ms step_avg:39.90ms
step:510/2330 train_time:20371ms step_avg:39.94ms
step:511/2330 train_time:20400ms step_avg:39.92ms
step:512/2330 train_time:20460ms step_avg:39.96ms
step:513/2330 train_time:20484ms step_avg:39.93ms
step:514/2330 train_time:20542ms step_avg:39.97ms
step:515/2330 train_time:20565ms step_avg:39.93ms
step:516/2330 train_time:20622ms step_avg:39.96ms
step:517/2330 train_time:20645ms step_avg:39.93ms
step:518/2330 train_time:20701ms step_avg:39.96ms
step:519/2330 train_time:20723ms step_avg:39.93ms
step:520/2330 train_time:20780ms step_avg:39.96ms
step:521/2330 train_time:20802ms step_avg:39.93ms
step:522/2330 train_time:20858ms step_avg:39.96ms
step:523/2330 train_time:20881ms step_avg:39.92ms
step:524/2330 train_time:20937ms step_avg:39.96ms
step:525/2330 train_time:20961ms step_avg:39.93ms
step:526/2330 train_time:21020ms step_avg:39.96ms
step:527/2330 train_time:21045ms step_avg:39.93ms
step:528/2330 train_time:21103ms step_avg:39.97ms
step:529/2330 train_time:21125ms step_avg:39.93ms
step:530/2330 train_time:21182ms step_avg:39.97ms
step:531/2330 train_time:21205ms step_avg:39.93ms
step:532/2330 train_time:21261ms step_avg:39.96ms
step:533/2330 train_time:21284ms step_avg:39.93ms
step:534/2330 train_time:21342ms step_avg:39.97ms
step:535/2330 train_time:21365ms step_avg:39.93ms
step:536/2330 train_time:21424ms step_avg:39.97ms
step:537/2330 train_time:21446ms step_avg:39.94ms
step:538/2330 train_time:21504ms step_avg:39.97ms
step:539/2330 train_time:21526ms step_avg:39.94ms
step:540/2330 train_time:21583ms step_avg:39.97ms
step:541/2330 train_time:21606ms step_avg:39.94ms
step:542/2330 train_time:21663ms step_avg:39.97ms
step:543/2330 train_time:21685ms step_avg:39.94ms
step:544/2330 train_time:21742ms step_avg:39.97ms
step:545/2330 train_time:21764ms step_avg:39.93ms
step:546/2330 train_time:21821ms step_avg:39.96ms
step:547/2330 train_time:21843ms step_avg:39.93ms
step:548/2330 train_time:21901ms step_avg:39.97ms
step:549/2330 train_time:21923ms step_avg:39.93ms
step:550/2330 train_time:21981ms step_avg:39.97ms
step:551/2330 train_time:22003ms step_avg:39.93ms
step:552/2330 train_time:22061ms step_avg:39.97ms
step:553/2330 train_time:22083ms step_avg:39.93ms
step:554/2330 train_time:22141ms step_avg:39.97ms
step:555/2330 train_time:22163ms step_avg:39.93ms
step:556/2330 train_time:22221ms step_avg:39.97ms
step:557/2330 train_time:22243ms step_avg:39.93ms
step:558/2330 train_time:22301ms step_avg:39.97ms
step:559/2330 train_time:22324ms step_avg:39.94ms
step:560/2330 train_time:22381ms step_avg:39.97ms
step:561/2330 train_time:22404ms step_avg:39.94ms
step:562/2330 train_time:22462ms step_avg:39.97ms
step:563/2330 train_time:22485ms step_avg:39.94ms
step:564/2330 train_time:22542ms step_avg:39.97ms
step:565/2330 train_time:22565ms step_avg:39.94ms
step:566/2330 train_time:22622ms step_avg:39.97ms
step:567/2330 train_time:22644ms step_avg:39.94ms
step:568/2330 train_time:22701ms step_avg:39.97ms
step:569/2330 train_time:22724ms step_avg:39.94ms
step:570/2330 train_time:22781ms step_avg:39.97ms
step:571/2330 train_time:22803ms step_avg:39.94ms
step:572/2330 train_time:22860ms step_avg:39.97ms
step:573/2330 train_time:22883ms step_avg:39.93ms
step:574/2330 train_time:22940ms step_avg:39.97ms
step:575/2330 train_time:22962ms step_avg:39.93ms
step:576/2330 train_time:23019ms step_avg:39.96ms
step:577/2330 train_time:23042ms step_avg:39.93ms
step:578/2330 train_time:23100ms step_avg:39.96ms
step:579/2330 train_time:23122ms step_avg:39.93ms
step:580/2330 train_time:23180ms step_avg:39.97ms
step:581/2330 train_time:23203ms step_avg:39.94ms
step:582/2330 train_time:23261ms step_avg:39.97ms
step:583/2330 train_time:23283ms step_avg:39.94ms
step:584/2330 train_time:23341ms step_avg:39.97ms
step:585/2330 train_time:23364ms step_avg:39.94ms
step:586/2330 train_time:23421ms step_avg:39.97ms
step:587/2330 train_time:23444ms step_avg:39.94ms
step:588/2330 train_time:23501ms step_avg:39.97ms
step:589/2330 train_time:23524ms step_avg:39.94ms
step:590/2330 train_time:23581ms step_avg:39.97ms
step:591/2330 train_time:23604ms step_avg:39.94ms
step:592/2330 train_time:23661ms step_avg:39.97ms
step:593/2330 train_time:23683ms step_avg:39.94ms
step:594/2330 train_time:23741ms step_avg:39.97ms
step:595/2330 train_time:23763ms step_avg:39.94ms
step:596/2330 train_time:23820ms step_avg:39.97ms
step:597/2330 train_time:23844ms step_avg:39.94ms
step:598/2330 train_time:23901ms step_avg:39.97ms
step:599/2330 train_time:23924ms step_avg:39.94ms
step:600/2330 train_time:23982ms step_avg:39.97ms
step:601/2330 train_time:24004ms step_avg:39.94ms
step:602/2330 train_time:24062ms step_avg:39.97ms
step:603/2330 train_time:24085ms step_avg:39.94ms
step:604/2330 train_time:24142ms step_avg:39.97ms
step:605/2330 train_time:24165ms step_avg:39.94ms
step:606/2330 train_time:24224ms step_avg:39.97ms
step:607/2330 train_time:24246ms step_avg:39.94ms
step:608/2330 train_time:24303ms step_avg:39.97ms
step:609/2330 train_time:24326ms step_avg:39.94ms
step:610/2330 train_time:24384ms step_avg:39.97ms
step:611/2330 train_time:24406ms step_avg:39.94ms
step:612/2330 train_time:24464ms step_avg:39.97ms
step:613/2330 train_time:24486ms step_avg:39.95ms
step:614/2330 train_time:24544ms step_avg:39.97ms
step:615/2330 train_time:24566ms step_avg:39.94ms
step:616/2330 train_time:24624ms step_avg:39.97ms
step:617/2330 train_time:24646ms step_avg:39.94ms
step:618/2330 train_time:24703ms step_avg:39.97ms
step:619/2330 train_time:24726ms step_avg:39.95ms
step:620/2330 train_time:24783ms step_avg:39.97ms
step:621/2330 train_time:24806ms step_avg:39.95ms
step:622/2330 train_time:24864ms step_avg:39.97ms
step:623/2330 train_time:24886ms step_avg:39.95ms
step:624/2330 train_time:24945ms step_avg:39.98ms
step:625/2330 train_time:24967ms step_avg:39.95ms
step:626/2330 train_time:25024ms step_avg:39.98ms
step:627/2330 train_time:25047ms step_avg:39.95ms
step:628/2330 train_time:25104ms step_avg:39.97ms
step:629/2330 train_time:25127ms step_avg:39.95ms
step:630/2330 train_time:25183ms step_avg:39.97ms
step:631/2330 train_time:25206ms step_avg:39.95ms
step:632/2330 train_time:25264ms step_avg:39.97ms
step:633/2330 train_time:25286ms step_avg:39.95ms
step:634/2330 train_time:25344ms step_avg:39.97ms
step:635/2330 train_time:25367ms step_avg:39.95ms
step:636/2330 train_time:25424ms step_avg:39.98ms
step:637/2330 train_time:25447ms step_avg:39.95ms
step:638/2330 train_time:25505ms step_avg:39.98ms
step:639/2330 train_time:25527ms step_avg:39.95ms
step:640/2330 train_time:25584ms step_avg:39.98ms
step:641/2330 train_time:25607ms step_avg:39.95ms
step:642/2330 train_time:25665ms step_avg:39.98ms
step:643/2330 train_time:25687ms step_avg:39.95ms
step:644/2330 train_time:25744ms step_avg:39.98ms
step:645/2330 train_time:25767ms step_avg:39.95ms
step:646/2330 train_time:25826ms step_avg:39.98ms
step:647/2330 train_time:25849ms step_avg:39.95ms
step:648/2330 train_time:25906ms step_avg:39.98ms
step:649/2330 train_time:25929ms step_avg:39.95ms
step:650/2330 train_time:25987ms step_avg:39.98ms
step:651/2330 train_time:26010ms step_avg:39.95ms
step:652/2330 train_time:26067ms step_avg:39.98ms
step:653/2330 train_time:26091ms step_avg:39.96ms
step:654/2330 train_time:26148ms step_avg:39.98ms
step:655/2330 train_time:26171ms step_avg:39.96ms
step:656/2330 train_time:26229ms step_avg:39.98ms
step:657/2330 train_time:26251ms step_avg:39.96ms
step:658/2330 train_time:26310ms step_avg:39.98ms
step:659/2330 train_time:26333ms step_avg:39.96ms
step:660/2330 train_time:26390ms step_avg:39.99ms
step:661/2330 train_time:26413ms step_avg:39.96ms
step:662/2330 train_time:26470ms step_avg:39.99ms
step:663/2330 train_time:26494ms step_avg:39.96ms
step:664/2330 train_time:26552ms step_avg:39.99ms
step:665/2330 train_time:26576ms step_avg:39.96ms
step:666/2330 train_time:26633ms step_avg:39.99ms
step:667/2330 train_time:26656ms step_avg:39.96ms
step:668/2330 train_time:26713ms step_avg:39.99ms
step:669/2330 train_time:26737ms step_avg:39.97ms
step:670/2330 train_time:26794ms step_avg:39.99ms
step:671/2330 train_time:26817ms step_avg:39.97ms
step:672/2330 train_time:26873ms step_avg:39.99ms
step:673/2330 train_time:26897ms step_avg:39.97ms
step:674/2330 train_time:26953ms step_avg:39.99ms
step:675/2330 train_time:26977ms step_avg:39.97ms
step:676/2330 train_time:27034ms step_avg:39.99ms
step:677/2330 train_time:27057ms step_avg:39.97ms
step:678/2330 train_time:27114ms step_avg:39.99ms
step:679/2330 train_time:27137ms step_avg:39.97ms
step:680/2330 train_time:27194ms step_avg:39.99ms
step:681/2330 train_time:27218ms step_avg:39.97ms
step:682/2330 train_time:27274ms step_avg:39.99ms
step:683/2330 train_time:27298ms step_avg:39.97ms
step:684/2330 train_time:27355ms step_avg:39.99ms
step:685/2330 train_time:27378ms step_avg:39.97ms
step:686/2330 train_time:27435ms step_avg:39.99ms
step:687/2330 train_time:27459ms step_avg:39.97ms
step:688/2330 train_time:27515ms step_avg:39.99ms
step:689/2330 train_time:27539ms step_avg:39.97ms
step:690/2330 train_time:27595ms step_avg:39.99ms
step:691/2330 train_time:27619ms step_avg:39.97ms
step:692/2330 train_time:27676ms step_avg:39.99ms
step:693/2330 train_time:27700ms step_avg:39.97ms
step:694/2330 train_time:27757ms step_avg:40.00ms
step:695/2330 train_time:27781ms step_avg:39.97ms
step:696/2330 train_time:27838ms step_avg:40.00ms
step:697/2330 train_time:27861ms step_avg:39.97ms
step:698/2330 train_time:27918ms step_avg:40.00ms
step:699/2330 train_time:27942ms step_avg:39.97ms
step:700/2330 train_time:28000ms step_avg:40.00ms
step:701/2330 train_time:28024ms step_avg:39.98ms
step:702/2330 train_time:28081ms step_avg:40.00ms
step:703/2330 train_time:28103ms step_avg:39.98ms
step:704/2330 train_time:28160ms step_avg:40.00ms
step:705/2330 train_time:28183ms step_avg:39.98ms
step:706/2330 train_time:28240ms step_avg:40.00ms
step:707/2330 train_time:28263ms step_avg:39.98ms
step:708/2330 train_time:28321ms step_avg:40.00ms
step:709/2330 train_time:28343ms step_avg:39.98ms
step:710/2330 train_time:28401ms step_avg:40.00ms
step:711/2330 train_time:28423ms step_avg:39.98ms
step:712/2330 train_time:28480ms step_avg:40.00ms
step:713/2330 train_time:28503ms step_avg:39.98ms
step:714/2330 train_time:28560ms step_avg:40.00ms
step:715/2330 train_time:28583ms step_avg:39.98ms
step:716/2330 train_time:28640ms step_avg:40.00ms
step:717/2330 train_time:28662ms step_avg:39.98ms
step:718/2330 train_time:28720ms step_avg:40.00ms
step:719/2330 train_time:28742ms step_avg:39.98ms
step:720/2330 train_time:28799ms step_avg:40.00ms
step:721/2330 train_time:28822ms step_avg:39.97ms
step:722/2330 train_time:28879ms step_avg:40.00ms
step:723/2330 train_time:28902ms step_avg:39.97ms
step:724/2330 train_time:28960ms step_avg:40.00ms
step:725/2330 train_time:28982ms step_avg:39.98ms
step:726/2330 train_time:29040ms step_avg:40.00ms
step:727/2330 train_time:29063ms step_avg:39.98ms
step:728/2330 train_time:29120ms step_avg:40.00ms
step:729/2330 train_time:29143ms step_avg:39.98ms
step:730/2330 train_time:29200ms step_avg:40.00ms
step:731/2330 train_time:29223ms step_avg:39.98ms
step:732/2330 train_time:29280ms step_avg:40.00ms
step:733/2330 train_time:29303ms step_avg:39.98ms
step:734/2330 train_time:29359ms step_avg:40.00ms
step:735/2330 train_time:29382ms step_avg:39.98ms
step:736/2330 train_time:29439ms step_avg:40.00ms
step:737/2330 train_time:29462ms step_avg:39.98ms
step:738/2330 train_time:29520ms step_avg:40.00ms
step:739/2330 train_time:29542ms step_avg:39.98ms
step:740/2330 train_time:29599ms step_avg:40.00ms
step:741/2330 train_time:29622ms step_avg:39.98ms
step:742/2330 train_time:29680ms step_avg:40.00ms
step:743/2330 train_time:29702ms step_avg:39.98ms
step:744/2330 train_time:29759ms step_avg:40.00ms
step:745/2330 train_time:29782ms step_avg:39.98ms
step:746/2330 train_time:29839ms step_avg:40.00ms
step:747/2330 train_time:29861ms step_avg:39.98ms
step:748/2330 train_time:29919ms step_avg:40.00ms
step:749/2330 train_time:29942ms step_avg:39.98ms
step:750/2330 train_time:29999ms step_avg:40.00ms
step:750/2330 val_loss:5.3807 train_time:30096ms step_avg:40.13ms
step:751/2330 train_time:30110ms step_avg:40.09ms
step:752/2330 train_time:30121ms step_avg:40.05ms
step:753/2330 train_time:30131ms step_avg:40.02ms
step:754/2330 train_time:30159ms step_avg:40.00ms
step:755/2330 train_time:30181ms step_avg:39.98ms
step:756/2330 train_time:30238ms step_avg:40.00ms
step:757/2330 train_time:30260ms step_avg:39.97ms
step:758/2330 train_time:30316ms step_avg:39.99ms
step:759/2330 train_time:30338ms step_avg:39.97ms
step:760/2330 train_time:30396ms step_avg:39.99ms
step:761/2330 train_time:30422ms step_avg:39.98ms
step:762/2330 train_time:30484ms step_avg:40.00ms
step:763/2330 train_time:30510ms step_avg:39.99ms
step:764/2330 train_time:30568ms step_avg:40.01ms
step:765/2330 train_time:30591ms step_avg:39.99ms
step:766/2330 train_time:30648ms step_avg:40.01ms
step:767/2330 train_time:30670ms step_avg:39.99ms
step:768/2330 train_time:30727ms step_avg:40.01ms
step:769/2330 train_time:30749ms step_avg:39.99ms
step:770/2330 train_time:30806ms step_avg:40.01ms
step:771/2330 train_time:30828ms step_avg:39.98ms
step:772/2330 train_time:30884ms step_avg:40.01ms
step:773/2330 train_time:30907ms step_avg:39.98ms
step:774/2330 train_time:30963ms step_avg:40.00ms
step:775/2330 train_time:30988ms step_avg:39.98ms
step:776/2330 train_time:31048ms step_avg:40.01ms
step:777/2330 train_time:31074ms step_avg:39.99ms
step:778/2330 train_time:31132ms step_avg:40.01ms
step:779/2330 train_time:31154ms step_avg:39.99ms
step:780/2330 train_time:31211ms step_avg:40.01ms
step:781/2330 train_time:31233ms step_avg:39.99ms
step:782/2330 train_time:31289ms step_avg:40.01ms
step:783/2330 train_time:31312ms step_avg:39.99ms
step:784/2330 train_time:31371ms step_avg:40.01ms
step:785/2330 train_time:31394ms step_avg:39.99ms
step:786/2330 train_time:31453ms step_avg:40.02ms
step:787/2330 train_time:31475ms step_avg:39.99ms
step:788/2330 train_time:31533ms step_avg:40.02ms
step:789/2330 train_time:31555ms step_avg:39.99ms
step:790/2330 train_time:31614ms step_avg:40.02ms
step:791/2330 train_time:31637ms step_avg:40.00ms
step:792/2330 train_time:31695ms step_avg:40.02ms
step:793/2330 train_time:31718ms step_avg:40.00ms
step:794/2330 train_time:31775ms step_avg:40.02ms
step:795/2330 train_time:31798ms step_avg:40.00ms
step:796/2330 train_time:31855ms step_avg:40.02ms
step:797/2330 train_time:31877ms step_avg:40.00ms
step:798/2330 train_time:31934ms step_avg:40.02ms
step:799/2330 train_time:31959ms step_avg:40.00ms
step:800/2330 train_time:32016ms step_avg:40.02ms
step:801/2330 train_time:32040ms step_avg:40.00ms
step:802/2330 train_time:32096ms step_avg:40.02ms
step:803/2330 train_time:32120ms step_avg:40.00ms
step:804/2330 train_time:32177ms step_avg:40.02ms
step:805/2330 train_time:32201ms step_avg:40.00ms
step:806/2330 train_time:32258ms step_avg:40.02ms
step:807/2330 train_time:32282ms step_avg:40.00ms
step:808/2330 train_time:32341ms step_avg:40.03ms
step:809/2330 train_time:32365ms step_avg:40.01ms
step:810/2330 train_time:32422ms step_avg:40.03ms
step:811/2330 train_time:32445ms step_avg:40.01ms
step:812/2330 train_time:32501ms step_avg:40.03ms
step:813/2330 train_time:32525ms step_avg:40.01ms
step:814/2330 train_time:32582ms step_avg:40.03ms
step:815/2330 train_time:32605ms step_avg:40.01ms
step:816/2330 train_time:32663ms step_avg:40.03ms
step:817/2330 train_time:32687ms step_avg:40.01ms
step:818/2330 train_time:32744ms step_avg:40.03ms
step:819/2330 train_time:32768ms step_avg:40.01ms
step:820/2330 train_time:32824ms step_avg:40.03ms
step:821/2330 train_time:32848ms step_avg:40.01ms
step:822/2330 train_time:32905ms step_avg:40.03ms
step:823/2330 train_time:32929ms step_avg:40.01ms
step:824/2330 train_time:32987ms step_avg:40.03ms
step:825/2330 train_time:33010ms step_avg:40.01ms
step:826/2330 train_time:33067ms step_avg:40.03ms
step:827/2330 train_time:33090ms step_avg:40.01ms
step:828/2330 train_time:33147ms step_avg:40.03ms
step:829/2330 train_time:33170ms step_avg:40.01ms
step:830/2330 train_time:33228ms step_avg:40.03ms
step:831/2330 train_time:33250ms step_avg:40.01ms
step:832/2330 train_time:33309ms step_avg:40.03ms
step:833/2330 train_time:33332ms step_avg:40.01ms
step:834/2330 train_time:33390ms step_avg:40.04ms
step:835/2330 train_time:33412ms step_avg:40.01ms
step:836/2330 train_time:33469ms step_avg:40.04ms
step:837/2330 train_time:33492ms step_avg:40.01ms
step:838/2330 train_time:33550ms step_avg:40.04ms
step:839/2330 train_time:33573ms step_avg:40.02ms
step:840/2330 train_time:33631ms step_avg:40.04ms
step:841/2330 train_time:33653ms step_avg:40.02ms
step:842/2330 train_time:33711ms step_avg:40.04ms
step:843/2330 train_time:33733ms step_avg:40.02ms
step:844/2330 train_time:33791ms step_avg:40.04ms
step:845/2330 train_time:33813ms step_avg:40.02ms
step:846/2330 train_time:33870ms step_avg:40.04ms
step:847/2330 train_time:33892ms step_avg:40.01ms
step:848/2330 train_time:33950ms step_avg:40.03ms
step:849/2330 train_time:33972ms step_avg:40.01ms
step:850/2330 train_time:34030ms step_avg:40.03ms
step:851/2330 train_time:34052ms step_avg:40.01ms
step:852/2330 train_time:34110ms step_avg:40.04ms
step:853/2330 train_time:34133ms step_avg:40.02ms
step:854/2330 train_time:34191ms step_avg:40.04ms
step:855/2330 train_time:34213ms step_avg:40.02ms
step:856/2330 train_time:34271ms step_avg:40.04ms
step:857/2330 train_time:34293ms step_avg:40.02ms
step:858/2330 train_time:34351ms step_avg:40.04ms
step:859/2330 train_time:34374ms step_avg:40.02ms
step:860/2330 train_time:34432ms step_avg:40.04ms
step:861/2330 train_time:34454ms step_avg:40.02ms
step:862/2330 train_time:34513ms step_avg:40.04ms
step:863/2330 train_time:34536ms step_avg:40.02ms
step:864/2330 train_time:34593ms step_avg:40.04ms
step:865/2330 train_time:34616ms step_avg:40.02ms
step:866/2330 train_time:34674ms step_avg:40.04ms
step:867/2330 train_time:34696ms step_avg:40.02ms
step:868/2330 train_time:34753ms step_avg:40.04ms
step:869/2330 train_time:34776ms step_avg:40.02ms
step:870/2330 train_time:34834ms step_avg:40.04ms
step:871/2330 train_time:34858ms step_avg:40.02ms
step:872/2330 train_time:34915ms step_avg:40.04ms
step:873/2330 train_time:34938ms step_avg:40.02ms
step:874/2330 train_time:34996ms step_avg:40.04ms
step:875/2330 train_time:35020ms step_avg:40.02ms
step:876/2330 train_time:35078ms step_avg:40.04ms
step:877/2330 train_time:35101ms step_avg:40.02ms
step:878/2330 train_time:35158ms step_avg:40.04ms
step:879/2330 train_time:35181ms step_avg:40.02ms
step:880/2330 train_time:35239ms step_avg:40.04ms
step:881/2330 train_time:35262ms step_avg:40.03ms
step:882/2330 train_time:35320ms step_avg:40.05ms
step:883/2330 train_time:35344ms step_avg:40.03ms
step:884/2330 train_time:35401ms step_avg:40.05ms
step:885/2330 train_time:35424ms step_avg:40.03ms
step:886/2330 train_time:35481ms step_avg:40.05ms
step:887/2330 train_time:35505ms step_avg:40.03ms
step:888/2330 train_time:35562ms step_avg:40.05ms
step:889/2330 train_time:35586ms step_avg:40.03ms
step:890/2330 train_time:35642ms step_avg:40.05ms
step:891/2330 train_time:35666ms step_avg:40.03ms
step:892/2330 train_time:35722ms step_avg:40.05ms
step:893/2330 train_time:35745ms step_avg:40.03ms
step:894/2330 train_time:35802ms step_avg:40.05ms
step:895/2330 train_time:35826ms step_avg:40.03ms
step:896/2330 train_time:35883ms step_avg:40.05ms
step:897/2330 train_time:35907ms step_avg:40.03ms
step:898/2330 train_time:35964ms step_avg:40.05ms
step:899/2330 train_time:35988ms step_avg:40.03ms
step:900/2330 train_time:36044ms step_avg:40.05ms
step:901/2330 train_time:36068ms step_avg:40.03ms
step:902/2330 train_time:36126ms step_avg:40.05ms
step:903/2330 train_time:36149ms step_avg:40.03ms
step:904/2330 train_time:36207ms step_avg:40.05ms
step:905/2330 train_time:36229ms step_avg:40.03ms
step:906/2330 train_time:36286ms step_avg:40.05ms
step:907/2330 train_time:36309ms step_avg:40.03ms
step:908/2330 train_time:36366ms step_avg:40.05ms
step:909/2330 train_time:36389ms step_avg:40.03ms
step:910/2330 train_time:36447ms step_avg:40.05ms
step:911/2330 train_time:36469ms step_avg:40.03ms
step:912/2330 train_time:36527ms step_avg:40.05ms
step:913/2330 train_time:36549ms step_avg:40.03ms
step:914/2330 train_time:36607ms step_avg:40.05ms
step:915/2330 train_time:36629ms step_avg:40.03ms
step:916/2330 train_time:36688ms step_avg:40.05ms
step:917/2330 train_time:36710ms step_avg:40.03ms
step:918/2330 train_time:36769ms step_avg:40.05ms
step:919/2330 train_time:36791ms step_avg:40.03ms
step:920/2330 train_time:36849ms step_avg:40.05ms
step:921/2330 train_time:36872ms step_avg:40.04ms
step:922/2330 train_time:36929ms step_avg:40.05ms
step:923/2330 train_time:36952ms step_avg:40.03ms
step:924/2330 train_time:37009ms step_avg:40.05ms
step:925/2330 train_time:37032ms step_avg:40.03ms
step:926/2330 train_time:37089ms step_avg:40.05ms
step:927/2330 train_time:37111ms step_avg:40.03ms
step:928/2330 train_time:37169ms step_avg:40.05ms
step:929/2330 train_time:37191ms step_avg:40.03ms
step:930/2330 train_time:37248ms step_avg:40.05ms
step:931/2330 train_time:37270ms step_avg:40.03ms
step:932/2330 train_time:37328ms step_avg:40.05ms
step:933/2330 train_time:37352ms step_avg:40.03ms
step:934/2330 train_time:37409ms step_avg:40.05ms
step:935/2330 train_time:37431ms step_avg:40.03ms
step:936/2330 train_time:37489ms step_avg:40.05ms
step:937/2330 train_time:37512ms step_avg:40.03ms
step:938/2330 train_time:37570ms step_avg:40.05ms
step:939/2330 train_time:37593ms step_avg:40.03ms
step:940/2330 train_time:37651ms step_avg:40.05ms
step:941/2330 train_time:37673ms step_avg:40.03ms
step:942/2330 train_time:37730ms step_avg:40.05ms
step:943/2330 train_time:37753ms step_avg:40.03ms
step:944/2330 train_time:37811ms step_avg:40.05ms
step:945/2330 train_time:37833ms step_avg:40.04ms
step:946/2330 train_time:37892ms step_avg:40.05ms
step:947/2330 train_time:37914ms step_avg:40.04ms
step:948/2330 train_time:37971ms step_avg:40.05ms
step:949/2330 train_time:37994ms step_avg:40.04ms
step:950/2330 train_time:38051ms step_avg:40.05ms
step:951/2330 train_time:38075ms step_avg:40.04ms
step:952/2330 train_time:38133ms step_avg:40.06ms
step:953/2330 train_time:38155ms step_avg:40.04ms
step:954/2330 train_time:38212ms step_avg:40.05ms
step:955/2330 train_time:38235ms step_avg:40.04ms
step:956/2330 train_time:38292ms step_avg:40.05ms
step:957/2330 train_time:38316ms step_avg:40.04ms
step:958/2330 train_time:38373ms step_avg:40.06ms
step:959/2330 train_time:38396ms step_avg:40.04ms
step:960/2330 train_time:38453ms step_avg:40.06ms
step:961/2330 train_time:38476ms step_avg:40.04ms
step:962/2330 train_time:38535ms step_avg:40.06ms
step:963/2330 train_time:38558ms step_avg:40.04ms
step:964/2330 train_time:38616ms step_avg:40.06ms
step:965/2330 train_time:38639ms step_avg:40.04ms
step:966/2330 train_time:38696ms step_avg:40.06ms
step:967/2330 train_time:38720ms step_avg:40.04ms
step:968/2330 train_time:38778ms step_avg:40.06ms
step:969/2330 train_time:38802ms step_avg:40.04ms
step:970/2330 train_time:38859ms step_avg:40.06ms
step:971/2330 train_time:38883ms step_avg:40.04ms
step:972/2330 train_time:38940ms step_avg:40.06ms
step:973/2330 train_time:38963ms step_avg:40.04ms
step:974/2330 train_time:39020ms step_avg:40.06ms
step:975/2330 train_time:39043ms step_avg:40.04ms
step:976/2330 train_time:39099ms step_avg:40.06ms
step:977/2330 train_time:39124ms step_avg:40.04ms
step:978/2330 train_time:39180ms step_avg:40.06ms
step:979/2330 train_time:39204ms step_avg:40.05ms
step:980/2330 train_time:39261ms step_avg:40.06ms
step:981/2330 train_time:39284ms step_avg:40.05ms
step:982/2330 train_time:39342ms step_avg:40.06ms
step:983/2330 train_time:39365ms step_avg:40.05ms
step:984/2330 train_time:39422ms step_avg:40.06ms
step:985/2330 train_time:39445ms step_avg:40.05ms
step:986/2330 train_time:39502ms step_avg:40.06ms
step:987/2330 train_time:39526ms step_avg:40.05ms
step:988/2330 train_time:39583ms step_avg:40.06ms
step:989/2330 train_time:39607ms step_avg:40.05ms
step:990/2330 train_time:39664ms step_avg:40.07ms
step:991/2330 train_time:39688ms step_avg:40.05ms
step:992/2330 train_time:39745ms step_avg:40.07ms
step:993/2330 train_time:39769ms step_avg:40.05ms
step:994/2330 train_time:39826ms step_avg:40.07ms
step:995/2330 train_time:39849ms step_avg:40.05ms
step:996/2330 train_time:39906ms step_avg:40.07ms
step:997/2330 train_time:39929ms step_avg:40.05ms
step:998/2330 train_time:39987ms step_avg:40.07ms
step:999/2330 train_time:40009ms step_avg:40.05ms
step:1000/2330 train_time:40067ms step_avg:40.07ms
step:1000/2330 val_loss:5.3222 train_time:40164ms step_avg:40.16ms
step:1001/2330 train_time:40177ms step_avg:40.14ms
step:1002/2330 train_time:40189ms step_avg:40.11ms
step:1003/2330 train_time:40201ms step_avg:40.08ms
step:1004/2330 train_time:40226ms step_avg:40.07ms
step:1005/2330 train_time:40248ms step_avg:40.05ms
step:1006/2330 train_time:40304ms step_avg:40.06ms
step:1007/2330 train_time:40327ms step_avg:40.05ms
step:1008/2330 train_time:40383ms step_avg:40.06ms
step:1009/2330 train_time:40405ms step_avg:40.04ms
step:1010/2330 train_time:40463ms step_avg:40.06ms
step:1011/2330 train_time:40490ms step_avg:40.05ms
step:1012/2330 train_time:40551ms step_avg:40.07ms
step:1013/2330 train_time:40577ms step_avg:40.06ms
step:1014/2330 train_time:40634ms step_avg:40.07ms
step:1015/2330 train_time:40659ms step_avg:40.06ms
step:1016/2330 train_time:40717ms step_avg:40.08ms
step:1017/2330 train_time:40739ms step_avg:40.06ms
step:1018/2330 train_time:40796ms step_avg:40.07ms
step:1019/2330 train_time:40818ms step_avg:40.06ms
step:1020/2330 train_time:40875ms step_avg:40.07ms
step:1021/2330 train_time:40897ms step_avg:40.06ms
step:1022/2330 train_time:40954ms step_avg:40.07ms
step:1023/2330 train_time:40976ms step_avg:40.05ms
step:1024/2330 train_time:41032ms step_avg:40.07ms
step:1025/2330 train_time:41055ms step_avg:40.05ms
step:1026/2330 train_time:41113ms step_avg:40.07ms
step:1027/2330 train_time:41138ms step_avg:40.06ms
step:1028/2330 train_time:41196ms step_avg:40.07ms
step:1029/2330 train_time:41219ms step_avg:40.06ms
step:1030/2330 train_time:41277ms step_avg:40.07ms
step:1031/2330 train_time:41299ms step_avg:40.06ms
step:1032/2330 train_time:41355ms step_avg:40.07ms
step:1033/2330 train_time:41378ms step_avg:40.06ms
step:1034/2330 train_time:41436ms step_avg:40.07ms
step:1035/2330 train_time:41459ms step_avg:40.06ms
step:1036/2330 train_time:41518ms step_avg:40.07ms
step:1037/2330 train_time:41540ms step_avg:40.06ms
step:1038/2330 train_time:41598ms step_avg:40.08ms
step:1039/2330 train_time:41621ms step_avg:40.06ms
step:1040/2330 train_time:41679ms step_avg:40.08ms
step:1041/2330 train_time:41702ms step_avg:40.06ms
step:1042/2330 train_time:41759ms step_avg:40.08ms
step:1043/2330 train_time:41782ms step_avg:40.06ms
step:1044/2330 train_time:41839ms step_avg:40.08ms
step:1045/2330 train_time:41861ms step_avg:40.06ms
step:1046/2330 train_time:41917ms step_avg:40.07ms
step:1047/2330 train_time:41939ms step_avg:40.06ms
step:1048/2330 train_time:41995ms step_avg:40.07ms
step:1049/2330 train_time:42019ms step_avg:40.06ms
step:1050/2330 train_time:42076ms step_avg:40.07ms
step:1051/2330 train_time:42100ms step_avg:40.06ms
step:1052/2330 train_time:42158ms step_avg:40.07ms
step:1053/2330 train_time:42181ms step_avg:40.06ms
step:1054/2330 train_time:42238ms step_avg:40.07ms
step:1055/2330 train_time:42261ms step_avg:40.06ms
step:1056/2330 train_time:42318ms step_avg:40.07ms
step:1057/2330 train_time:42341ms step_avg:40.06ms
step:1058/2330 train_time:42399ms step_avg:40.08ms
step:1059/2330 train_time:42422ms step_avg:40.06ms
step:1060/2330 train_time:42480ms step_avg:40.08ms
step:1061/2330 train_time:42502ms step_avg:40.06ms
step:1062/2330 train_time:42560ms step_avg:40.08ms
step:1063/2330 train_time:42582ms step_avg:40.06ms
step:1064/2330 train_time:42640ms step_avg:40.07ms
step:1065/2330 train_time:42662ms step_avg:40.06ms
step:1066/2330 train_time:42720ms step_avg:40.07ms
step:1067/2330 train_time:42742ms step_avg:40.06ms
step:1068/2330 train_time:42799ms step_avg:40.07ms
step:1069/2330 train_time:42822ms step_avg:40.06ms
step:1070/2330 train_time:42880ms step_avg:40.07ms
step:1071/2330 train_time:42902ms step_avg:40.06ms
step:1072/2330 train_time:42960ms step_avg:40.07ms
step:1073/2330 train_time:42982ms step_avg:40.06ms
step:1074/2330 train_time:43039ms step_avg:40.07ms
step:1075/2330 train_time:43062ms step_avg:40.06ms
step:1076/2330 train_time:43119ms step_avg:40.07ms
step:1077/2330 train_time:43142ms step_avg:40.06ms
step:1078/2330 train_time:43199ms step_avg:40.07ms
step:1079/2330 train_time:43222ms step_avg:40.06ms
step:1080/2330 train_time:43279ms step_avg:40.07ms
step:1081/2330 train_time:43301ms step_avg:40.06ms
step:1082/2330 train_time:43359ms step_avg:40.07ms
step:1083/2330 train_time:43382ms step_avg:40.06ms
step:1084/2330 train_time:43440ms step_avg:40.07ms
step:1085/2330 train_time:43463ms step_avg:40.06ms
step:1086/2330 train_time:43520ms step_avg:40.07ms
step:1087/2330 train_time:43543ms step_avg:40.06ms
step:1088/2330 train_time:43600ms step_avg:40.07ms
step:1089/2330 train_time:43623ms step_avg:40.06ms
step:1090/2330 train_time:43681ms step_avg:40.07ms
step:1091/2330 train_time:43703ms step_avg:40.06ms
step:1092/2330 train_time:43760ms step_avg:40.07ms
step:1093/2330 train_time:43782ms step_avg:40.06ms
step:1094/2330 train_time:43840ms step_avg:40.07ms
step:1095/2330 train_time:43863ms step_avg:40.06ms
step:1096/2330 train_time:43921ms step_avg:40.07ms
step:1097/2330 train_time:43944ms step_avg:40.06ms
step:1098/2330 train_time:44001ms step_avg:40.07ms
step:1099/2330 train_time:44025ms step_avg:40.06ms
step:1100/2330 train_time:44084ms step_avg:40.08ms
step:1101/2330 train_time:44106ms step_avg:40.06ms
step:1102/2330 train_time:44165ms step_avg:40.08ms
step:1103/2330 train_time:44188ms step_avg:40.06ms
step:1104/2330 train_time:44246ms step_avg:40.08ms
step:1105/2330 train_time:44269ms step_avg:40.06ms
step:1106/2330 train_time:44327ms step_avg:40.08ms
step:1107/2330 train_time:44351ms step_avg:40.06ms
step:1108/2330 train_time:44408ms step_avg:40.08ms
step:1109/2330 train_time:44431ms step_avg:40.06ms
step:1110/2330 train_time:44488ms step_avg:40.08ms
step:1111/2330 train_time:44512ms step_avg:40.06ms
step:1112/2330 train_time:44569ms step_avg:40.08ms
step:1113/2330 train_time:44592ms step_avg:40.06ms
step:1114/2330 train_time:44649ms step_avg:40.08ms
step:1115/2330 train_time:44672ms step_avg:40.06ms
step:1116/2330 train_time:44729ms step_avg:40.08ms
step:1117/2330 train_time:44753ms step_avg:40.07ms
step:1118/2330 train_time:44810ms step_avg:40.08ms
step:1119/2330 train_time:44834ms step_avg:40.07ms
step:1120/2330 train_time:44891ms step_avg:40.08ms
step:1121/2330 train_time:44914ms step_avg:40.07ms
step:1122/2330 train_time:44972ms step_avg:40.08ms
step:1123/2330 train_time:44996ms step_avg:40.07ms
step:1124/2330 train_time:45052ms step_avg:40.08ms
step:1125/2330 train_time:45076ms step_avg:40.07ms
step:1126/2330 train_time:45132ms step_avg:40.08ms
step:1127/2330 train_time:45155ms step_avg:40.07ms
step:1128/2330 train_time:45213ms step_avg:40.08ms
step:1129/2330 train_time:45236ms step_avg:40.07ms
step:1130/2330 train_time:45294ms step_avg:40.08ms
step:1131/2330 train_time:45317ms step_avg:40.07ms
step:1132/2330 train_time:45375ms step_avg:40.08ms
step:1133/2330 train_time:45398ms step_avg:40.07ms
step:1134/2330 train_time:45455ms step_avg:40.08ms
step:1135/2330 train_time:45478ms step_avg:40.07ms
step:1136/2330 train_time:45535ms step_avg:40.08ms
step:1137/2330 train_time:45558ms step_avg:40.07ms
step:1138/2330 train_time:45615ms step_avg:40.08ms
step:1139/2330 train_time:45638ms step_avg:40.07ms
step:1140/2330 train_time:45697ms step_avg:40.08ms
step:1141/2330 train_time:45719ms step_avg:40.07ms
step:1142/2330 train_time:45776ms step_avg:40.08ms
step:1143/2330 train_time:45799ms step_avg:40.07ms
step:1144/2330 train_time:45857ms step_avg:40.08ms
step:1145/2330 train_time:45879ms step_avg:40.07ms
step:1146/2330 train_time:45936ms step_avg:40.08ms
step:1147/2330 train_time:45959ms step_avg:40.07ms
step:1148/2330 train_time:46017ms step_avg:40.08ms
step:1149/2330 train_time:46039ms step_avg:40.07ms
step:1150/2330 train_time:46096ms step_avg:40.08ms
step:1151/2330 train_time:46119ms step_avg:40.07ms
step:1152/2330 train_time:46176ms step_avg:40.08ms
step:1153/2330 train_time:46198ms step_avg:40.07ms
step:1154/2330 train_time:46256ms step_avg:40.08ms
step:1155/2330 train_time:46278ms step_avg:40.07ms
step:1156/2330 train_time:46335ms step_avg:40.08ms
step:1157/2330 train_time:46358ms step_avg:40.07ms
step:1158/2330 train_time:46416ms step_avg:40.08ms
step:1159/2330 train_time:46439ms step_avg:40.07ms
step:1160/2330 train_time:46496ms step_avg:40.08ms
step:1161/2330 train_time:46519ms step_avg:40.07ms
step:1162/2330 train_time:46576ms step_avg:40.08ms
step:1163/2330 train_time:46598ms step_avg:40.07ms
step:1164/2330 train_time:46655ms step_avg:40.08ms
step:1165/2330 train_time:46678ms step_avg:40.07ms
step:1166/2330 train_time:46735ms step_avg:40.08ms
step:1167/2330 train_time:46758ms step_avg:40.07ms
step:1168/2330 train_time:46815ms step_avg:40.08ms
step:1169/2330 train_time:46838ms step_avg:40.07ms
step:1170/2330 train_time:46895ms step_avg:40.08ms
step:1171/2330 train_time:46918ms step_avg:40.07ms
step:1172/2330 train_time:46975ms step_avg:40.08ms
step:1173/2330 train_time:46998ms step_avg:40.07ms
step:1174/2330 train_time:47056ms step_avg:40.08ms
step:1175/2330 train_time:47078ms step_avg:40.07ms
step:1176/2330 train_time:47136ms step_avg:40.08ms
step:1177/2330 train_time:47158ms step_avg:40.07ms
step:1178/2330 train_time:47215ms step_avg:40.08ms
step:1179/2330 train_time:47237ms step_avg:40.07ms
step:1180/2330 train_time:47295ms step_avg:40.08ms
step:1181/2330 train_time:47317ms step_avg:40.07ms
step:1182/2330 train_time:47374ms step_avg:40.08ms
step:1183/2330 train_time:47397ms step_avg:40.06ms
step:1184/2330 train_time:47454ms step_avg:40.08ms
step:1185/2330 train_time:47477ms step_avg:40.06ms
step:1186/2330 train_time:47535ms step_avg:40.08ms
step:1187/2330 train_time:47557ms step_avg:40.06ms
step:1188/2330 train_time:47614ms step_avg:40.08ms
step:1189/2330 train_time:47637ms step_avg:40.06ms
step:1190/2330 train_time:47695ms step_avg:40.08ms
step:1191/2330 train_time:47718ms step_avg:40.07ms
step:1192/2330 train_time:47776ms step_avg:40.08ms
step:1193/2330 train_time:47798ms step_avg:40.07ms
step:1194/2330 train_time:47855ms step_avg:40.08ms
step:1195/2330 train_time:47878ms step_avg:40.07ms
step:1196/2330 train_time:47936ms step_avg:40.08ms
step:1197/2330 train_time:47958ms step_avg:40.07ms
step:1198/2330 train_time:48016ms step_avg:40.08ms
step:1199/2330 train_time:48038ms step_avg:40.07ms
step:1200/2330 train_time:48095ms step_avg:40.08ms
step:1201/2330 train_time:48118ms step_avg:40.06ms
step:1202/2330 train_time:48175ms step_avg:40.08ms
step:1203/2330 train_time:48198ms step_avg:40.06ms
step:1204/2330 train_time:48255ms step_avg:40.08ms
step:1205/2330 train_time:48277ms step_avg:40.06ms
step:1206/2330 train_time:48335ms step_avg:40.08ms
step:1207/2330 train_time:48357ms step_avg:40.06ms
step:1208/2330 train_time:48415ms step_avg:40.08ms
step:1209/2330 train_time:48437ms step_avg:40.06ms
step:1210/2330 train_time:48496ms step_avg:40.08ms
step:1211/2330 train_time:48519ms step_avg:40.06ms
step:1212/2330 train_time:48577ms step_avg:40.08ms
step:1213/2330 train_time:48599ms step_avg:40.07ms
step:1214/2330 train_time:48657ms step_avg:40.08ms
step:1215/2330 train_time:48679ms step_avg:40.07ms
step:1216/2330 train_time:48737ms step_avg:40.08ms
step:1217/2330 train_time:48759ms step_avg:40.06ms
step:1218/2330 train_time:48816ms step_avg:40.08ms
step:1219/2330 train_time:48838ms step_avg:40.06ms
step:1220/2330 train_time:48896ms step_avg:40.08ms
step:1221/2330 train_time:48920ms step_avg:40.07ms
step:1222/2330 train_time:48977ms step_avg:40.08ms
step:1223/2330 train_time:49000ms step_avg:40.07ms
step:1224/2330 train_time:49057ms step_avg:40.08ms
step:1225/2330 train_time:49079ms step_avg:40.06ms
step:1226/2330 train_time:49136ms step_avg:40.08ms
step:1227/2330 train_time:49159ms step_avg:40.06ms
step:1228/2330 train_time:49215ms step_avg:40.08ms
step:1229/2330 train_time:49237ms step_avg:40.06ms
step:1230/2330 train_time:49295ms step_avg:40.08ms
step:1231/2330 train_time:49317ms step_avg:40.06ms
step:1232/2330 train_time:49374ms step_avg:40.08ms
step:1233/2330 train_time:49397ms step_avg:40.06ms
step:1234/2330 train_time:49455ms step_avg:40.08ms
step:1235/2330 train_time:49477ms step_avg:40.06ms
step:1236/2330 train_time:49535ms step_avg:40.08ms
step:1237/2330 train_time:49558ms step_avg:40.06ms
step:1238/2330 train_time:49615ms step_avg:40.08ms
step:1239/2330 train_time:49637ms step_avg:40.06ms
step:1240/2330 train_time:49695ms step_avg:40.08ms
step:1241/2330 train_time:49717ms step_avg:40.06ms
step:1242/2330 train_time:49774ms step_avg:40.08ms
step:1243/2330 train_time:49796ms step_avg:40.06ms
step:1244/2330 train_time:49854ms step_avg:40.08ms
step:1245/2330 train_time:49877ms step_avg:40.06ms
step:1246/2330 train_time:49934ms step_avg:40.08ms
step:1247/2330 train_time:49957ms step_avg:40.06ms
step:1248/2330 train_time:50015ms step_avg:40.08ms
step:1249/2330 train_time:50038ms step_avg:40.06ms
step:1250/2330 train_time:50095ms step_avg:40.08ms
step:1250/2330 val_loss:5.2655 train_time:50192ms step_avg:40.15ms
step:1251/2330 train_time:50206ms step_avg:40.13ms
step:1252/2330 train_time:50220ms step_avg:40.11ms
step:1253/2330 train_time:50230ms step_avg:40.09ms
step:1254/2330 train_time:50256ms step_avg:40.08ms
step:1255/2330 train_time:50278ms step_avg:40.06ms
step:1256/2330 train_time:50334ms step_avg:40.07ms
step:1257/2330 train_time:50356ms step_avg:40.06ms
step:1258/2330 train_time:50412ms step_avg:40.07ms
step:1259/2330 train_time:50436ms step_avg:40.06ms
step:1260/2330 train_time:50493ms step_avg:40.07ms
step:1261/2330 train_time:50519ms step_avg:40.06ms
step:1262/2330 train_time:50578ms step_avg:40.08ms
step:1263/2330 train_time:50604ms step_avg:40.07ms
step:1264/2330 train_time:50662ms step_avg:40.08ms
step:1265/2330 train_time:50686ms step_avg:40.07ms
step:1266/2330 train_time:50743ms step_avg:40.08ms
step:1267/2330 train_time:50765ms step_avg:40.07ms
step:1268/2330 train_time:50822ms step_avg:40.08ms
step:1269/2330 train_time:50843ms step_avg:40.07ms
step:1270/2330 train_time:50900ms step_avg:40.08ms
step:1271/2330 train_time:50922ms step_avg:40.06ms
step:1272/2330 train_time:50979ms step_avg:40.08ms
step:1273/2330 train_time:51000ms step_avg:40.06ms
step:1274/2330 train_time:51057ms step_avg:40.08ms
step:1275/2330 train_time:51080ms step_avg:40.06ms
step:1276/2330 train_time:51138ms step_avg:40.08ms
step:1277/2330 train_time:51162ms step_avg:40.06ms
step:1278/2330 train_time:51220ms step_avg:40.08ms
step:1279/2330 train_time:51244ms step_avg:40.07ms
step:1280/2330 train_time:51301ms step_avg:40.08ms
step:1281/2330 train_time:51324ms step_avg:40.07ms
step:1282/2330 train_time:51382ms step_avg:40.08ms
step:1283/2330 train_time:51404ms step_avg:40.07ms
step:1284/2330 train_time:51463ms step_avg:40.08ms
step:1285/2330 train_time:51487ms step_avg:40.07ms
step:1286/2330 train_time:51544ms step_avg:40.08ms
step:1287/2330 train_time:51567ms step_avg:40.07ms
step:1288/2330 train_time:51625ms step_avg:40.08ms
step:1289/2330 train_time:51648ms step_avg:40.07ms
step:1290/2330 train_time:51705ms step_avg:40.08ms
step:1291/2330 train_time:51727ms step_avg:40.07ms
step:1292/2330 train_time:51785ms step_avg:40.08ms
step:1293/2330 train_time:51807ms step_avg:40.07ms
step:1294/2330 train_time:51864ms step_avg:40.08ms
step:1295/2330 train_time:51886ms step_avg:40.07ms
step:1296/2330 train_time:51943ms step_avg:40.08ms
step:1297/2330 train_time:51965ms step_avg:40.07ms
step:1298/2330 train_time:52023ms step_avg:40.08ms
step:1299/2330 train_time:52046ms step_avg:40.07ms
step:1300/2330 train_time:52104ms step_avg:40.08ms
step:1301/2330 train_time:52126ms step_avg:40.07ms
step:1302/2330 train_time:52184ms step_avg:40.08ms
step:1303/2330 train_time:52206ms step_avg:40.07ms
step:1304/2330 train_time:52264ms step_avg:40.08ms
step:1305/2330 train_time:52286ms step_avg:40.07ms
step:1306/2330 train_time:52344ms step_avg:40.08ms
step:1307/2330 train_time:52366ms step_avg:40.07ms
step:1308/2330 train_time:52424ms step_avg:40.08ms
step:1309/2330 train_time:52446ms step_avg:40.07ms
step:1310/2330 train_time:52504ms step_avg:40.08ms
step:1311/2330 train_time:52527ms step_avg:40.07ms
step:1312/2330 train_time:52584ms step_avg:40.08ms
step:1313/2330 train_time:52607ms step_avg:40.07ms
step:1314/2330 train_time:52665ms step_avg:40.08ms
step:1315/2330 train_time:52687ms step_avg:40.07ms
step:1316/2330 train_time:52744ms step_avg:40.08ms
step:1317/2330 train_time:52767ms step_avg:40.07ms
step:1318/2330 train_time:52824ms step_avg:40.08ms
step:1319/2330 train_time:52847ms step_avg:40.07ms
step:1320/2330 train_time:52904ms step_avg:40.08ms
step:1321/2330 train_time:52926ms step_avg:40.07ms
step:1322/2330 train_time:52984ms step_avg:40.08ms
step:1323/2330 train_time:53007ms step_avg:40.07ms
step:1324/2330 train_time:53065ms step_avg:40.08ms
step:1325/2330 train_time:53087ms step_avg:40.07ms
step:1326/2330 train_time:53146ms step_avg:40.08ms
step:1327/2330 train_time:53168ms step_avg:40.07ms
step:1328/2330 train_time:53225ms step_avg:40.08ms
step:1329/2330 train_time:53248ms step_avg:40.07ms
step:1330/2330 train_time:53306ms step_avg:40.08ms
step:1331/2330 train_time:53329ms step_avg:40.07ms
step:1332/2330 train_time:53386ms step_avg:40.08ms
step:1333/2330 train_time:53408ms step_avg:40.07ms
step:1334/2330 train_time:53466ms step_avg:40.08ms
step:1335/2330 train_time:53489ms step_avg:40.07ms
step:1336/2330 train_time:53547ms step_avg:40.08ms
step:1337/2330 train_time:53569ms step_avg:40.07ms
step:1338/2330 train_time:53626ms step_avg:40.08ms
step:1339/2330 train_time:53649ms step_avg:40.07ms
step:1340/2330 train_time:53707ms step_avg:40.08ms
step:1341/2330 train_time:53730ms step_avg:40.07ms
step:1342/2330 train_time:53787ms step_avg:40.08ms
step:1343/2330 train_time:53810ms step_avg:40.07ms
step:1344/2330 train_time:53867ms step_avg:40.08ms
step:1345/2330 train_time:53890ms step_avg:40.07ms
step:1346/2330 train_time:53947ms step_avg:40.08ms
step:1347/2330 train_time:53971ms step_avg:40.07ms
step:1348/2330 train_time:54028ms step_avg:40.08ms
step:1349/2330 train_time:54051ms step_avg:40.07ms
step:1350/2330 train_time:54108ms step_avg:40.08ms
step:1351/2330 train_time:54131ms step_avg:40.07ms
step:1352/2330 train_time:54188ms step_avg:40.08ms
step:1353/2330 train_time:54211ms step_avg:40.07ms
step:1354/2330 train_time:54268ms step_avg:40.08ms
step:1355/2330 train_time:54292ms step_avg:40.07ms
step:1356/2330 train_time:54349ms step_avg:40.08ms
step:1357/2330 train_time:54372ms step_avg:40.07ms
step:1358/2330 train_time:54429ms step_avg:40.08ms
step:1359/2330 train_time:54452ms step_avg:40.07ms
step:1360/2330 train_time:54510ms step_avg:40.08ms
step:1361/2330 train_time:54533ms step_avg:40.07ms
step:1362/2330 train_time:54590ms step_avg:40.08ms
step:1363/2330 train_time:54614ms step_avg:40.07ms
step:1364/2330 train_time:54671ms step_avg:40.08ms
step:1365/2330 train_time:54694ms step_avg:40.07ms
step:1366/2330 train_time:54752ms step_avg:40.08ms
step:1367/2330 train_time:54775ms step_avg:40.07ms
step:1368/2330 train_time:54832ms step_avg:40.08ms
step:1369/2330 train_time:54856ms step_avg:40.07ms
step:1370/2330 train_time:54913ms step_avg:40.08ms
step:1371/2330 train_time:54937ms step_avg:40.07ms
step:1372/2330 train_time:54994ms step_avg:40.08ms
step:1373/2330 train_time:55017ms step_avg:40.07ms
step:1374/2330 train_time:55074ms step_avg:40.08ms
step:1375/2330 train_time:55097ms step_avg:40.07ms
step:1376/2330 train_time:55154ms step_avg:40.08ms
step:1377/2330 train_time:55177ms step_avg:40.07ms
step:1378/2330 train_time:55235ms step_avg:40.08ms
step:1379/2330 train_time:55258ms step_avg:40.07ms
step:1380/2330 train_time:55314ms step_avg:40.08ms
step:1381/2330 train_time:55338ms step_avg:40.07ms
step:1382/2330 train_time:55395ms step_avg:40.08ms
step:1383/2330 train_time:55418ms step_avg:40.07ms
step:1384/2330 train_time:55475ms step_avg:40.08ms
step:1385/2330 train_time:55498ms step_avg:40.07ms
step:1386/2330 train_time:55555ms step_avg:40.08ms
step:1387/2330 train_time:55578ms step_avg:40.07ms
step:1388/2330 train_time:55635ms step_avg:40.08ms
step:1389/2330 train_time:55659ms step_avg:40.07ms
step:1390/2330 train_time:55715ms step_avg:40.08ms
step:1391/2330 train_time:55739ms step_avg:40.07ms
step:1392/2330 train_time:55795ms step_avg:40.08ms
step:1393/2330 train_time:55818ms step_avg:40.07ms
step:1394/2330 train_time:55875ms step_avg:40.08ms
step:1395/2330 train_time:55899ms step_avg:40.07ms
step:1396/2330 train_time:55955ms step_avg:40.08ms
step:1397/2330 train_time:55979ms step_avg:40.07ms
step:1398/2330 train_time:56035ms step_avg:40.08ms
step:1399/2330 train_time:56058ms step_avg:40.07ms
step:1400/2330 train_time:56116ms step_avg:40.08ms
step:1401/2330 train_time:56140ms step_avg:40.07ms
step:1402/2330 train_time:56196ms step_avg:40.08ms
step:1403/2330 train_time:56220ms step_avg:40.07ms
step:1404/2330 train_time:56277ms step_avg:40.08ms
step:1405/2330 train_time:56301ms step_avg:40.07ms
step:1406/2330 train_time:56358ms step_avg:40.08ms
step:1407/2330 train_time:56382ms step_avg:40.07ms
step:1408/2330 train_time:56439ms step_avg:40.08ms
step:1409/2330 train_time:56462ms step_avg:40.07ms
step:1410/2330 train_time:56519ms step_avg:40.08ms
step:1411/2330 train_time:56542ms step_avg:40.07ms
step:1412/2330 train_time:56600ms step_avg:40.08ms
step:1413/2330 train_time:56622ms step_avg:40.07ms
step:1414/2330 train_time:56680ms step_avg:40.08ms
step:1415/2330 train_time:56702ms step_avg:40.07ms
step:1416/2330 train_time:56759ms step_avg:40.08ms
step:1417/2330 train_time:56782ms step_avg:40.07ms
step:1418/2330 train_time:56840ms step_avg:40.08ms
step:1419/2330 train_time:56863ms step_avg:40.07ms
step:1420/2330 train_time:56920ms step_avg:40.08ms
step:1421/2330 train_time:56943ms step_avg:40.07ms
step:1422/2330 train_time:57000ms step_avg:40.08ms
step:1423/2330 train_time:57023ms step_avg:40.07ms
step:1424/2330 train_time:57081ms step_avg:40.08ms
step:1425/2330 train_time:57103ms step_avg:40.07ms
step:1426/2330 train_time:57161ms step_avg:40.08ms
step:1427/2330 train_time:57183ms step_avg:40.07ms
step:1428/2330 train_time:57240ms step_avg:40.08ms
step:1429/2330 train_time:57263ms step_avg:40.07ms
step:1430/2330 train_time:57320ms step_avg:40.08ms
step:1431/2330 train_time:57342ms step_avg:40.07ms
step:1432/2330 train_time:57399ms step_avg:40.08ms
step:1433/2330 train_time:57422ms step_avg:40.07ms
step:1434/2330 train_time:57479ms step_avg:40.08ms
step:1435/2330 train_time:57502ms step_avg:40.07ms
step:1436/2330 train_time:57559ms step_avg:40.08ms
step:1437/2330 train_time:57582ms step_avg:40.07ms
step:1438/2330 train_time:57640ms step_avg:40.08ms
step:1439/2330 train_time:57662ms step_avg:40.07ms
step:1440/2330 train_time:57720ms step_avg:40.08ms
step:1441/2330 train_time:57742ms step_avg:40.07ms
step:1442/2330 train_time:57800ms step_avg:40.08ms
step:1443/2330 train_time:57822ms step_avg:40.07ms
step:1444/2330 train_time:57879ms step_avg:40.08ms
step:1445/2330 train_time:57902ms step_avg:40.07ms
step:1446/2330 train_time:57959ms step_avg:40.08ms
step:1447/2330 train_time:57982ms step_avg:40.07ms
step:1448/2330 train_time:58039ms step_avg:40.08ms
step:1449/2330 train_time:58062ms step_avg:40.07ms
step:1450/2330 train_time:58119ms step_avg:40.08ms
step:1451/2330 train_time:58142ms step_avg:40.07ms
step:1452/2330 train_time:58200ms step_avg:40.08ms
step:1453/2330 train_time:58222ms step_avg:40.07ms
step:1454/2330 train_time:58279ms step_avg:40.08ms
step:1455/2330 train_time:58301ms step_avg:40.07ms
step:1456/2330 train_time:58358ms step_avg:40.08ms
step:1457/2330 train_time:58381ms step_avg:40.07ms
step:1458/2330 train_time:58438ms step_avg:40.08ms
step:1459/2330 train_time:58461ms step_avg:40.07ms
step:1460/2330 train_time:58517ms step_avg:40.08ms
step:1461/2330 train_time:58540ms step_avg:40.07ms
step:1462/2330 train_time:58597ms step_avg:40.08ms
step:1463/2330 train_time:58620ms step_avg:40.07ms
step:1464/2330 train_time:58677ms step_avg:40.08ms
step:1465/2330 train_time:58701ms step_avg:40.07ms
step:1466/2330 train_time:58757ms step_avg:40.08ms
step:1467/2330 train_time:58781ms step_avg:40.07ms
step:1468/2330 train_time:58838ms step_avg:40.08ms
step:1469/2330 train_time:58861ms step_avg:40.07ms
step:1470/2330 train_time:58918ms step_avg:40.08ms
step:1471/2330 train_time:58941ms step_avg:40.07ms
step:1472/2330 train_time:58999ms step_avg:40.08ms
step:1473/2330 train_time:59022ms step_avg:40.07ms
step:1474/2330 train_time:59080ms step_avg:40.08ms
step:1475/2330 train_time:59102ms step_avg:40.07ms
step:1476/2330 train_time:59160ms step_avg:40.08ms
step:1477/2330 train_time:59183ms step_avg:40.07ms
step:1478/2330 train_time:59241ms step_avg:40.08ms
step:1479/2330 train_time:59264ms step_avg:40.07ms
step:1480/2330 train_time:59321ms step_avg:40.08ms
step:1481/2330 train_time:59343ms step_avg:40.07ms
step:1482/2330 train_time:59400ms step_avg:40.08ms
step:1483/2330 train_time:59423ms step_avg:40.07ms
step:1484/2330 train_time:59480ms step_avg:40.08ms
step:1485/2330 train_time:59503ms step_avg:40.07ms
step:1486/2330 train_time:59560ms step_avg:40.08ms
step:1487/2330 train_time:59583ms step_avg:40.07ms
step:1488/2330 train_time:59640ms step_avg:40.08ms
step:1489/2330 train_time:59663ms step_avg:40.07ms
step:1490/2330 train_time:59720ms step_avg:40.08ms
step:1491/2330 train_time:59742ms step_avg:40.07ms
step:1492/2330 train_time:59799ms step_avg:40.08ms
step:1493/2330 train_time:59822ms step_avg:40.07ms
step:1494/2330 train_time:59879ms step_avg:40.08ms
step:1495/2330 train_time:59902ms step_avg:40.07ms
step:1496/2330 train_time:59959ms step_avg:40.08ms
step:1497/2330 train_time:59981ms step_avg:40.07ms
step:1498/2330 train_time:60038ms step_avg:40.08ms
step:1499/2330 train_time:60061ms step_avg:40.07ms
step:1500/2330 train_time:60118ms step_avg:40.08ms
step:1500/2330 val_loss:5.2090 train_time:60216ms step_avg:40.14ms
step:1501/2330 train_time:60228ms step_avg:40.13ms
step:1502/2330 train_time:60239ms step_avg:40.11ms
step:1503/2330 train_time:60248ms step_avg:40.09ms
step:1504/2330 train_time:60279ms step_avg:40.08ms
step:1505/2330 train_time:60301ms step_avg:40.07ms
step:1506/2330 train_time:60356ms step_avg:40.08ms
step:1507/2330 train_time:60379ms step_avg:40.07ms
step:1508/2330 train_time:60435ms step_avg:40.08ms
step:1509/2330 train_time:60458ms step_avg:40.06ms
step:1510/2330 train_time:60515ms step_avg:40.08ms
step:1511/2330 train_time:60542ms step_avg:40.07ms
step:1512/2330 train_time:60601ms step_avg:40.08ms
step:1513/2330 train_time:60625ms step_avg:40.07ms
step:1514/2330 train_time:60683ms step_avg:40.08ms
step:1515/2330 train_time:60705ms step_avg:40.07ms
step:1516/2330 train_time:60762ms step_avg:40.08ms
step:1517/2330 train_time:60785ms step_avg:40.07ms
step:1518/2330 train_time:60841ms step_avg:40.08ms
step:1519/2330 train_time:60864ms step_avg:40.07ms
step:1520/2330 train_time:60920ms step_avg:40.08ms
step:1521/2330 train_time:60943ms step_avg:40.07ms
step:1522/2330 train_time:60999ms step_avg:40.08ms
step:1523/2330 train_time:61022ms step_avg:40.07ms
step:1524/2330 train_time:61078ms step_avg:40.08ms
step:1525/2330 train_time:61101ms step_avg:40.07ms
step:1526/2330 train_time:61160ms step_avg:40.08ms
step:1527/2330 train_time:61184ms step_avg:40.07ms
step:1528/2330 train_time:61242ms step_avg:40.08ms
step:1529/2330 train_time:61265ms step_avg:40.07ms
step:1530/2330 train_time:61323ms step_avg:40.08ms
step:1531/2330 train_time:61343ms step_avg:40.07ms
step:1532/2330 train_time:61400ms step_avg:40.08ms
step:1533/2330 train_time:61423ms step_avg:40.07ms
step:1534/2330 train_time:61482ms step_avg:40.08ms
step:1535/2330 train_time:61505ms step_avg:40.07ms
step:1536/2330 train_time:61564ms step_avg:40.08ms
step:1537/2330 train_time:61587ms step_avg:40.07ms
step:1538/2330 train_time:61645ms step_avg:40.08ms
step:1539/2330 train_time:61667ms step_avg:40.07ms
step:1540/2330 train_time:61723ms step_avg:40.08ms
step:1541/2330 train_time:61747ms step_avg:40.07ms
step:1542/2330 train_time:61803ms step_avg:40.08ms
step:1543/2330 train_time:61826ms step_avg:40.07ms
step:1544/2330 train_time:61882ms step_avg:40.08ms
step:1545/2330 train_time:61904ms step_avg:40.07ms
step:1546/2330 train_time:61962ms step_avg:40.08ms
step:1547/2330 train_time:61984ms step_avg:40.07ms
step:1548/2330 train_time:62041ms step_avg:40.08ms
step:1549/2330 train_time:62063ms step_avg:40.07ms
step:1550/2330 train_time:62121ms step_avg:40.08ms
step:1551/2330 train_time:62144ms step_avg:40.07ms
step:1552/2330 train_time:62201ms step_avg:40.08ms
step:1553/2330 train_time:62224ms step_avg:40.07ms
step:1554/2330 train_time:62281ms step_avg:40.08ms
step:1555/2330 train_time:62304ms step_avg:40.07ms
step:1556/2330 train_time:62361ms step_avg:40.08ms
step:1557/2330 train_time:62384ms step_avg:40.07ms
step:1558/2330 train_time:62441ms step_avg:40.08ms
step:1559/2330 train_time:62464ms step_avg:40.07ms
step:1560/2330 train_time:62523ms step_avg:40.08ms
step:1561/2330 train_time:62546ms step_avg:40.07ms
step:1562/2330 train_time:62603ms step_avg:40.08ms
step:1563/2330 train_time:62626ms step_avg:40.07ms
step:1564/2330 train_time:62684ms step_avg:40.08ms
step:1565/2330 train_time:62707ms step_avg:40.07ms
step:1566/2330 train_time:62764ms step_avg:40.08ms
step:1567/2330 train_time:62787ms step_avg:40.07ms
step:1568/2330 train_time:62844ms step_avg:40.08ms
step:1569/2330 train_time:62866ms step_avg:40.07ms
step:1570/2330 train_time:62923ms step_avg:40.08ms
step:1571/2330 train_time:62946ms step_avg:40.07ms
step:1572/2330 train_time:63003ms step_avg:40.08ms
step:1573/2330 train_time:63025ms step_avg:40.07ms
step:1574/2330 train_time:63082ms step_avg:40.08ms
step:1575/2330 train_time:63105ms step_avg:40.07ms
step:1576/2330 train_time:63163ms step_avg:40.08ms
step:1577/2330 train_time:63185ms step_avg:40.07ms
step:1578/2330 train_time:63242ms step_avg:40.08ms
step:1579/2330 train_time:63265ms step_avg:40.07ms
step:1580/2330 train_time:63322ms step_avg:40.08ms
step:1581/2330 train_time:63345ms step_avg:40.07ms
step:1582/2330 train_time:63402ms step_avg:40.08ms
step:1583/2330 train_time:63425ms step_avg:40.07ms
step:1584/2330 train_time:63483ms step_avg:40.08ms
step:1585/2330 train_time:63507ms step_avg:40.07ms
step:1586/2330 train_time:63564ms step_avg:40.08ms
step:1587/2330 train_time:63587ms step_avg:40.07ms
step:1588/2330 train_time:63644ms step_avg:40.08ms
step:1589/2330 train_time:63667ms step_avg:40.07ms
step:1590/2330 train_time:63724ms step_avg:40.08ms
step:1591/2330 train_time:63746ms step_avg:40.07ms
step:1592/2330 train_time:63803ms step_avg:40.08ms
step:1593/2330 train_time:63825ms step_avg:40.07ms
step:1594/2330 train_time:63883ms step_avg:40.08ms
step:1595/2330 train_time:63905ms step_avg:40.07ms
step:1596/2330 train_time:63963ms step_avg:40.08ms
step:1597/2330 train_time:63985ms step_avg:40.07ms
step:1598/2330 train_time:64043ms step_avg:40.08ms
step:1599/2330 train_time:64065ms step_avg:40.07ms
step:1600/2330 train_time:64122ms step_avg:40.08ms
step:1601/2330 train_time:64145ms step_avg:40.07ms
step:1602/2330 train_time:64202ms step_avg:40.08ms
step:1603/2330 train_time:64224ms step_avg:40.07ms
step:1604/2330 train_time:64281ms step_avg:40.08ms
step:1605/2330 train_time:64303ms step_avg:40.06ms
step:1606/2330 train_time:64360ms step_avg:40.07ms
step:1607/2330 train_time:64383ms step_avg:40.06ms
step:1608/2330 train_time:64441ms step_avg:40.08ms
step:1609/2330 train_time:64464ms step_avg:40.06ms
step:1610/2330 train_time:64522ms step_avg:40.08ms
step:1611/2330 train_time:64545ms step_avg:40.07ms
step:1612/2330 train_time:64604ms step_avg:40.08ms
step:1613/2330 train_time:64627ms step_avg:40.07ms
step:1614/2330 train_time:64684ms step_avg:40.08ms
step:1615/2330 train_time:64706ms step_avg:40.07ms
step:1616/2330 train_time:64763ms step_avg:40.08ms
step:1617/2330 train_time:64785ms step_avg:40.07ms
step:1618/2330 train_time:64843ms step_avg:40.08ms
step:1619/2330 train_time:64865ms step_avg:40.07ms
step:1620/2330 train_time:64922ms step_avg:40.08ms
step:1621/2330 train_time:64945ms step_avg:40.06ms
step:1622/2330 train_time:65002ms step_avg:40.08ms
step:1623/2330 train_time:65024ms step_avg:40.06ms
step:1624/2330 train_time:65082ms step_avg:40.08ms
step:1625/2330 train_time:65104ms step_avg:40.06ms
step:1626/2330 train_time:65162ms step_avg:40.07ms
step:1627/2330 train_time:65185ms step_avg:40.06ms
step:1628/2330 train_time:65242ms step_avg:40.08ms
step:1629/2330 train_time:65264ms step_avg:40.06ms
step:1630/2330 train_time:65321ms step_avg:40.07ms
step:1631/2330 train_time:65344ms step_avg:40.06ms
step:1632/2330 train_time:65401ms step_avg:40.07ms
step:1633/2330 train_time:65424ms step_avg:40.06ms
step:1634/2330 train_time:65480ms step_avg:40.07ms
step:1635/2330 train_time:65503ms step_avg:40.06ms
step:1636/2330 train_time:65562ms step_avg:40.07ms
step:1637/2330 train_time:65585ms step_avg:40.06ms
step:1638/2330 train_time:65643ms step_avg:40.08ms
step:1639/2330 train_time:65666ms step_avg:40.06ms
step:1640/2330 train_time:65723ms step_avg:40.08ms
step:1641/2330 train_time:65745ms step_avg:40.06ms
step:1642/2330 train_time:65803ms step_avg:40.07ms
step:1643/2330 train_time:65825ms step_avg:40.06ms
step:1644/2330 train_time:65882ms step_avg:40.07ms
step:1645/2330 train_time:65905ms step_avg:40.06ms
step:1646/2330 train_time:65962ms step_avg:40.07ms
step:1647/2330 train_time:65984ms step_avg:40.06ms
step:1648/2330 train_time:66041ms step_avg:40.07ms
step:1649/2330 train_time:66064ms step_avg:40.06ms
step:1650/2330 train_time:66121ms step_avg:40.07ms
step:1651/2330 train_time:66143ms step_avg:40.06ms
step:1652/2330 train_time:66200ms step_avg:40.07ms
step:1653/2330 train_time:66223ms step_avg:40.06ms
step:1654/2330 train_time:66280ms step_avg:40.07ms
step:1655/2330 train_time:66303ms step_avg:40.06ms
step:1656/2330 train_time:66360ms step_avg:40.07ms
step:1657/2330 train_time:66384ms step_avg:40.06ms
step:1658/2330 train_time:66441ms step_avg:40.07ms
step:1659/2330 train_time:66465ms step_avg:40.06ms
step:1660/2330 train_time:66522ms step_avg:40.07ms
step:1661/2330 train_time:66545ms step_avg:40.06ms
step:1662/2330 train_time:66603ms step_avg:40.07ms
step:1663/2330 train_time:66625ms step_avg:40.06ms
step:1664/2330 train_time:66682ms step_avg:40.07ms
step:1665/2330 train_time:66704ms step_avg:40.06ms
step:1666/2330 train_time:66761ms step_avg:40.07ms
step:1667/2330 train_time:66784ms step_avg:40.06ms
step:1668/2330 train_time:66841ms step_avg:40.07ms
step:1669/2330 train_time:66864ms step_avg:40.06ms
step:1670/2330 train_time:66921ms step_avg:40.07ms
step:1671/2330 train_time:66944ms step_avg:40.06ms
step:1672/2330 train_time:67001ms step_avg:40.07ms
step:1673/2330 train_time:67024ms step_avg:40.06ms
step:1674/2330 train_time:67082ms step_avg:40.07ms
step:1675/2330 train_time:67104ms step_avg:40.06ms
step:1676/2330 train_time:67161ms step_avg:40.07ms
step:1677/2330 train_time:67184ms step_avg:40.06ms
step:1678/2330 train_time:67241ms step_avg:40.07ms
step:1679/2330 train_time:67264ms step_avg:40.06ms
step:1680/2330 train_time:67321ms step_avg:40.07ms
step:1681/2330 train_time:67344ms step_avg:40.06ms
step:1682/2330 train_time:67402ms step_avg:40.07ms
step:1683/2330 train_time:67425ms step_avg:40.06ms
step:1684/2330 train_time:67482ms step_avg:40.07ms
step:1685/2330 train_time:67504ms step_avg:40.06ms
step:1686/2330 train_time:67562ms step_avg:40.07ms
step:1687/2330 train_time:67584ms step_avg:40.06ms
step:1688/2330 train_time:67642ms step_avg:40.07ms
step:1689/2330 train_time:67665ms step_avg:40.06ms
step:1690/2330 train_time:67721ms step_avg:40.07ms
step:1691/2330 train_time:67744ms step_avg:40.06ms
step:1692/2330 train_time:67801ms step_avg:40.07ms
step:1693/2330 train_time:67824ms step_avg:40.06ms
step:1694/2330 train_time:67881ms step_avg:40.07ms
step:1695/2330 train_time:67904ms step_avg:40.06ms
step:1696/2330 train_time:67961ms step_avg:40.07ms
step:1697/2330 train_time:67983ms step_avg:40.06ms
step:1698/2330 train_time:68041ms step_avg:40.07ms
step:1699/2330 train_time:68063ms step_avg:40.06ms
step:1700/2330 train_time:68119ms step_avg:40.07ms
step:1701/2330 train_time:68143ms step_avg:40.06ms
step:1702/2330 train_time:68200ms step_avg:40.07ms
step:1703/2330 train_time:68223ms step_avg:40.06ms
step:1704/2330 train_time:68280ms step_avg:40.07ms
step:1705/2330 train_time:68303ms step_avg:40.06ms
step:1706/2330 train_time:68361ms step_avg:40.07ms
step:1707/2330 train_time:68383ms step_avg:40.06ms
step:1708/2330 train_time:68440ms step_avg:40.07ms
step:1709/2330 train_time:68464ms step_avg:40.06ms
step:1710/2330 train_time:68520ms step_avg:40.07ms
step:1711/2330 train_time:68543ms step_avg:40.06ms
step:1712/2330 train_time:68601ms step_avg:40.07ms
step:1713/2330 train_time:68624ms step_avg:40.06ms
step:1714/2330 train_time:68681ms step_avg:40.07ms
step:1715/2330 train_time:68703ms step_avg:40.06ms
step:1716/2330 train_time:68761ms step_avg:40.07ms
step:1717/2330 train_time:68783ms step_avg:40.06ms
step:1718/2330 train_time:68840ms step_avg:40.07ms
step:1719/2330 train_time:68863ms step_avg:40.06ms
step:1720/2330 train_time:68920ms step_avg:40.07ms
step:1721/2330 train_time:68944ms step_avg:40.06ms
step:1722/2330 train_time:69001ms step_avg:40.07ms
step:1723/2330 train_time:69023ms step_avg:40.06ms
step:1724/2330 train_time:69081ms step_avg:40.07ms
step:1725/2330 train_time:69103ms step_avg:40.06ms
step:1726/2330 train_time:69161ms step_avg:40.07ms
step:1727/2330 train_time:69183ms step_avg:40.06ms
step:1728/2330 train_time:69240ms step_avg:40.07ms
step:1729/2330 train_time:69263ms step_avg:40.06ms
step:1730/2330 train_time:69320ms step_avg:40.07ms
step:1731/2330 train_time:69343ms step_avg:40.06ms
step:1732/2330 train_time:69400ms step_avg:40.07ms
step:1733/2330 train_time:69423ms step_avg:40.06ms
step:1734/2330 train_time:69481ms step_avg:40.07ms
step:1735/2330 train_time:69504ms step_avg:40.06ms
step:1736/2330 train_time:69562ms step_avg:40.07ms
step:1737/2330 train_time:69585ms step_avg:40.06ms
step:1738/2330 train_time:69642ms step_avg:40.07ms
step:1739/2330 train_time:69665ms step_avg:40.06ms
step:1740/2330 train_time:69722ms step_avg:40.07ms
step:1741/2330 train_time:69744ms step_avg:40.06ms
step:1742/2330 train_time:69802ms step_avg:40.07ms
step:1743/2330 train_time:69824ms step_avg:40.06ms
step:1744/2330 train_time:69882ms step_avg:40.07ms
step:1745/2330 train_time:69905ms step_avg:40.06ms
step:1746/2330 train_time:69962ms step_avg:40.07ms
step:1747/2330 train_time:69984ms step_avg:40.06ms
step:1748/2330 train_time:70042ms step_avg:40.07ms
step:1749/2330 train_time:70065ms step_avg:40.06ms
step:1750/2330 train_time:70123ms step_avg:40.07ms
step:1750/2330 val_loss:5.1896 train_time:70220ms step_avg:40.13ms
step:1751/2330 train_time:70233ms step_avg:40.11ms
step:1752/2330 train_time:70246ms step_avg:40.09ms
step:1753/2330 train_time:70256ms step_avg:40.08ms
step:1754/2330 train_time:70282ms step_avg:40.07ms
step:1755/2330 train_time:70304ms step_avg:40.06ms
step:1756/2330 train_time:70360ms step_avg:40.07ms
step:1757/2330 train_time:70382ms step_avg:40.06ms
step:1758/2330 train_time:70437ms step_avg:40.07ms
step:1759/2330 train_time:70459ms step_avg:40.06ms
step:1760/2330 train_time:70518ms step_avg:40.07ms
step:1761/2330 train_time:70545ms step_avg:40.06ms
step:1762/2330 train_time:70606ms step_avg:40.07ms
step:1763/2330 train_time:70631ms step_avg:40.06ms
step:1764/2330 train_time:70688ms step_avg:40.07ms
step:1765/2330 train_time:70712ms step_avg:40.06ms
step:1766/2330 train_time:70768ms step_avg:40.07ms
step:1767/2330 train_time:70791ms step_avg:40.06ms
step:1768/2330 train_time:70848ms step_avg:40.07ms
step:1769/2330 train_time:70870ms step_avg:40.06ms
step:1770/2330 train_time:70927ms step_avg:40.07ms
step:1771/2330 train_time:70949ms step_avg:40.06ms
step:1772/2330 train_time:71005ms step_avg:40.07ms
step:1773/2330 train_time:71028ms step_avg:40.06ms
step:1774/2330 train_time:71084ms step_avg:40.07ms
step:1775/2330 train_time:71108ms step_avg:40.06ms
step:1776/2330 train_time:71166ms step_avg:40.07ms
step:1777/2330 train_time:71189ms step_avg:40.06ms
step:1778/2330 train_time:71246ms step_avg:40.07ms
step:1779/2330 train_time:71269ms step_avg:40.06ms
step:1780/2330 train_time:71325ms step_avg:40.07ms
step:1781/2330 train_time:71349ms step_avg:40.06ms
step:1782/2330 train_time:71405ms step_avg:40.07ms
step:1783/2330 train_time:71428ms step_avg:40.06ms
step:1784/2330 train_time:71486ms step_avg:40.07ms
step:1785/2330 train_time:71510ms step_avg:40.06ms
step:1786/2330 train_time:71568ms step_avg:40.07ms
step:1787/2330 train_time:71593ms step_avg:40.06ms
step:1788/2330 train_time:71651ms step_avg:40.07ms
step:1789/2330 train_time:71673ms step_avg:40.06ms
step:1790/2330 train_time:71731ms step_avg:40.07ms
step:1791/2330 train_time:71753ms step_avg:40.06ms
step:1792/2330 train_time:71810ms step_avg:40.07ms
step:1793/2330 train_time:71832ms step_avg:40.06ms
step:1794/2330 train_time:71888ms step_avg:40.07ms
step:1795/2330 train_time:71911ms step_avg:40.06ms
step:1796/2330 train_time:71968ms step_avg:40.07ms
step:1797/2330 train_time:71990ms step_avg:40.06ms
step:1798/2330 train_time:72047ms step_avg:40.07ms
step:1799/2330 train_time:72070ms step_avg:40.06ms
step:1800/2330 train_time:72127ms step_avg:40.07ms
step:1801/2330 train_time:72150ms step_avg:40.06ms
step:1802/2330 train_time:72207ms step_avg:40.07ms
step:1803/2330 train_time:72231ms step_avg:40.06ms
step:1804/2330 train_time:72288ms step_avg:40.07ms
step:1805/2330 train_time:72310ms step_avg:40.06ms
step:1806/2330 train_time:72368ms step_avg:40.07ms
step:1807/2330 train_time:72391ms step_avg:40.06ms
step:1808/2330 train_time:72448ms step_avg:40.07ms
step:1809/2330 train_time:72472ms step_avg:40.06ms
step:1810/2330 train_time:72530ms step_avg:40.07ms
step:1811/2330 train_time:72553ms step_avg:40.06ms
step:1812/2330 train_time:72611ms step_avg:40.07ms
step:1813/2330 train_time:72633ms step_avg:40.06ms
step:1814/2330 train_time:72691ms step_avg:40.07ms
step:1815/2330 train_time:72713ms step_avg:40.06ms
step:1816/2330 train_time:72770ms step_avg:40.07ms
step:1817/2330 train_time:72793ms step_avg:40.06ms
step:1818/2330 train_time:72849ms step_avg:40.07ms
step:1819/2330 train_time:72872ms step_avg:40.06ms
step:1820/2330 train_time:72929ms step_avg:40.07ms
step:1821/2330 train_time:72951ms step_avg:40.06ms
step:1822/2330 train_time:73009ms step_avg:40.07ms
step:1823/2330 train_time:73031ms step_avg:40.06ms
step:1824/2330 train_time:73088ms step_avg:40.07ms
step:1825/2330 train_time:73110ms step_avg:40.06ms
step:1826/2330 train_time:73168ms step_avg:40.07ms
step:1827/2330 train_time:73191ms step_avg:40.06ms
step:1828/2330 train_time:73248ms step_avg:40.07ms
step:1829/2330 train_time:73271ms step_avg:40.06ms
step:1830/2330 train_time:73329ms step_avg:40.07ms
step:1831/2330 train_time:73352ms step_avg:40.06ms
step:1832/2330 train_time:73409ms step_avg:40.07ms
step:1833/2330 train_time:73432ms step_avg:40.06ms
step:1834/2330 train_time:73490ms step_avg:40.07ms
step:1835/2330 train_time:73513ms step_avg:40.06ms
step:1836/2330 train_time:73570ms step_avg:40.07ms
step:1837/2330 train_time:73593ms step_avg:40.06ms
step:1838/2330 train_time:73650ms step_avg:40.07ms
step:1839/2330 train_time:73673ms step_avg:40.06ms
step:1840/2330 train_time:73730ms step_avg:40.07ms
step:1841/2330 train_time:73753ms step_avg:40.06ms
step:1842/2330 train_time:73810ms step_avg:40.07ms
step:1843/2330 train_time:73833ms step_avg:40.06ms
step:1844/2330 train_time:73891ms step_avg:40.07ms
step:1845/2330 train_time:73913ms step_avg:40.06ms
step:1846/2330 train_time:73970ms step_avg:40.07ms
step:1847/2330 train_time:73992ms step_avg:40.06ms
step:1848/2330 train_time:74050ms step_avg:40.07ms
step:1849/2330 train_time:74072ms step_avg:40.06ms
step:1850/2330 train_time:74130ms step_avg:40.07ms
step:1851/2330 train_time:74152ms step_avg:40.06ms
step:1852/2330 train_time:74210ms step_avg:40.07ms
step:1853/2330 train_time:74232ms step_avg:40.06ms
step:1854/2330 train_time:74289ms step_avg:40.07ms
step:1855/2330 train_time:74311ms step_avg:40.06ms
step:1856/2330 train_time:74369ms step_avg:40.07ms
step:1857/2330 train_time:74392ms step_avg:40.06ms
step:1858/2330 train_time:74449ms step_avg:40.07ms
step:1859/2330 train_time:74472ms step_avg:40.06ms
step:1860/2330 train_time:74529ms step_avg:40.07ms
step:1861/2330 train_time:74552ms step_avg:40.06ms
step:1862/2330 train_time:74609ms step_avg:40.07ms
step:1863/2330 train_time:74632ms step_avg:40.06ms
step:1864/2330 train_time:74690ms step_avg:40.07ms
step:1865/2330 train_time:74713ms step_avg:40.06ms
step:1866/2330 train_time:74770ms step_avg:40.07ms
step:1867/2330 train_time:74793ms step_avg:40.06ms
step:1868/2330 train_time:74850ms step_avg:40.07ms
step:1869/2330 train_time:74872ms step_avg:40.06ms
step:1870/2330 train_time:74930ms step_avg:40.07ms
step:1871/2330 train_time:74952ms step_avg:40.06ms
step:1872/2330 train_time:75010ms step_avg:40.07ms
step:1873/2330 train_time:75032ms step_avg:40.06ms
step:1874/2330 train_time:75089ms step_avg:40.07ms
step:1875/2330 train_time:75111ms step_avg:40.06ms
step:1876/2330 train_time:75168ms step_avg:40.07ms
step:1877/2330 train_time:75192ms step_avg:40.06ms
step:1878/2330 train_time:75249ms step_avg:40.07ms
step:1879/2330 train_time:75271ms step_avg:40.06ms
step:1880/2330 train_time:75329ms step_avg:40.07ms
step:1881/2330 train_time:75352ms step_avg:40.06ms
step:1882/2330 train_time:75409ms step_avg:40.07ms
step:1883/2330 train_time:75431ms step_avg:40.06ms
step:1884/2330 train_time:75489ms step_avg:40.07ms
step:1885/2330 train_time:75512ms step_avg:40.06ms
step:1886/2330 train_time:75570ms step_avg:40.07ms
step:1887/2330 train_time:75593ms step_avg:40.06ms
step:1888/2330 train_time:75651ms step_avg:40.07ms
step:1889/2330 train_time:75673ms step_avg:40.06ms
step:1890/2330 train_time:75730ms step_avg:40.07ms
step:1891/2330 train_time:75753ms step_avg:40.06ms
step:1892/2330 train_time:75811ms step_avg:40.07ms
step:1893/2330 train_time:75833ms step_avg:40.06ms
step:1894/2330 train_time:75890ms step_avg:40.07ms
step:1895/2330 train_time:75912ms step_avg:40.06ms
step:1896/2330 train_time:75969ms step_avg:40.07ms
step:1897/2330 train_time:75991ms step_avg:40.06ms
step:1898/2330 train_time:76048ms step_avg:40.07ms
step:1899/2330 train_time:76071ms step_avg:40.06ms
step:1900/2330 train_time:76128ms step_avg:40.07ms
step:1901/2330 train_time:76151ms step_avg:40.06ms
step:1902/2330 train_time:76209ms step_avg:40.07ms
step:1903/2330 train_time:76231ms step_avg:40.06ms
step:1904/2330 train_time:76289ms step_avg:40.07ms
step:1905/2330 train_time:76312ms step_avg:40.06ms
step:1906/2330 train_time:76369ms step_avg:40.07ms
step:1907/2330 train_time:76391ms step_avg:40.06ms
step:1908/2330 train_time:76448ms step_avg:40.07ms
step:1909/2330 train_time:76471ms step_avg:40.06ms
step:1910/2330 train_time:76529ms step_avg:40.07ms
step:1911/2330 train_time:76552ms step_avg:40.06ms
step:1912/2330 train_time:76610ms step_avg:40.07ms
step:1913/2330 train_time:76632ms step_avg:40.06ms
step:1914/2330 train_time:76690ms step_avg:40.07ms
step:1915/2330 train_time:76713ms step_avg:40.06ms
step:1916/2330 train_time:76770ms step_avg:40.07ms
step:1917/2330 train_time:76793ms step_avg:40.06ms
step:1918/2330 train_time:76851ms step_avg:40.07ms
step:1919/2330 train_time:76873ms step_avg:40.06ms
step:1920/2330 train_time:76929ms step_avg:40.07ms
step:1921/2330 train_time:76952ms step_avg:40.06ms
step:1922/2330 train_time:77009ms step_avg:40.07ms
step:1923/2330 train_time:77031ms step_avg:40.06ms
step:1924/2330 train_time:77089ms step_avg:40.07ms
step:1925/2330 train_time:77111ms step_avg:40.06ms
step:1926/2330 train_time:77168ms step_avg:40.07ms
step:1927/2330 train_time:77191ms step_avg:40.06ms
step:1928/2330 train_time:77247ms step_avg:40.07ms
step:1929/2330 train_time:77270ms step_avg:40.06ms
step:1930/2330 train_time:77327ms step_avg:40.07ms
step:1931/2330 train_time:77350ms step_avg:40.06ms
step:1932/2330 train_time:77407ms step_avg:40.07ms
step:1933/2330 train_time:77430ms step_avg:40.06ms
step:1934/2330 train_time:77487ms step_avg:40.07ms
step:1935/2330 train_time:77511ms step_avg:40.06ms
step:1936/2330 train_time:77568ms step_avg:40.07ms
step:1937/2330 train_time:77592ms step_avg:40.06ms
step:1938/2330 train_time:77650ms step_avg:40.07ms
step:1939/2330 train_time:77673ms step_avg:40.06ms
step:1940/2330 train_time:77730ms step_avg:40.07ms
step:1941/2330 train_time:77753ms step_avg:40.06ms
step:1942/2330 train_time:77811ms step_avg:40.07ms
step:1943/2330 train_time:77833ms step_avg:40.06ms
step:1944/2330 train_time:77890ms step_avg:40.07ms
step:1945/2330 train_time:77913ms step_avg:40.06ms
step:1946/2330 train_time:77970ms step_avg:40.07ms
step:1947/2330 train_time:77993ms step_avg:40.06ms
step:1948/2330 train_time:78050ms step_avg:40.07ms
step:1949/2330 train_time:78072ms step_avg:40.06ms
step:1950/2330 train_time:78130ms step_avg:40.07ms
step:1951/2330 train_time:78152ms step_avg:40.06ms
step:1952/2330 train_time:78209ms step_avg:40.07ms
step:1953/2330 train_time:78232ms step_avg:40.06ms
step:1954/2330 train_time:78289ms step_avg:40.07ms
step:1955/2330 train_time:78312ms step_avg:40.06ms
step:1956/2330 train_time:78369ms step_avg:40.07ms
step:1957/2330 train_time:78391ms step_avg:40.06ms
step:1958/2330 train_time:78448ms step_avg:40.07ms
step:1959/2330 train_time:78471ms step_avg:40.06ms
step:1960/2330 train_time:78528ms step_avg:40.07ms
step:1961/2330 train_time:78551ms step_avg:40.06ms
step:1962/2330 train_time:78608ms step_avg:40.07ms
step:1963/2330 train_time:78632ms step_avg:40.06ms
step:1964/2330 train_time:78689ms step_avg:40.07ms
step:1965/2330 train_time:78712ms step_avg:40.06ms
step:1966/2330 train_time:78769ms step_avg:40.07ms
step:1967/2330 train_time:78792ms step_avg:40.06ms
step:1968/2330 train_time:78850ms step_avg:40.07ms
step:1969/2330 train_time:78872ms step_avg:40.06ms
step:1970/2330 train_time:78929ms step_avg:40.07ms
step:1971/2330 train_time:78952ms step_avg:40.06ms
step:1972/2330 train_time:79009ms step_avg:40.07ms
step:1973/2330 train_time:79031ms step_avg:40.06ms
step:1974/2330 train_time:79089ms step_avg:40.07ms
step:1975/2330 train_time:79111ms step_avg:40.06ms
step:1976/2330 train_time:79169ms step_avg:40.07ms
step:1977/2330 train_time:79191ms step_avg:40.06ms
step:1978/2330 train_time:79248ms step_avg:40.06ms
step:1979/2330 train_time:79271ms step_avg:40.06ms
step:1980/2330 train_time:79328ms step_avg:40.06ms
step:1981/2330 train_time:79351ms step_avg:40.06ms
step:1982/2330 train_time:79409ms step_avg:40.06ms
step:1983/2330 train_time:79432ms step_avg:40.06ms
step:1984/2330 train_time:79488ms step_avg:40.06ms
step:1985/2330 train_time:79511ms step_avg:40.06ms
step:1986/2330 train_time:79568ms step_avg:40.06ms
step:1987/2330 train_time:79591ms step_avg:40.06ms
step:1988/2330 train_time:79648ms step_avg:40.06ms
step:1989/2330 train_time:79671ms step_avg:40.06ms
step:1990/2330 train_time:79729ms step_avg:40.06ms
step:1991/2330 train_time:79751ms step_avg:40.06ms
step:1992/2330 train_time:79808ms step_avg:40.06ms
step:1993/2330 train_time:79831ms step_avg:40.06ms
step:1994/2330 train_time:79887ms step_avg:40.06ms
step:1995/2330 train_time:79910ms step_avg:40.06ms
step:1996/2330 train_time:79967ms step_avg:40.06ms
step:1997/2330 train_time:79990ms step_avg:40.05ms
step:1998/2330 train_time:80046ms step_avg:40.06ms
step:1999/2330 train_time:80069ms step_avg:40.05ms
step:2000/2330 train_time:80126ms step_avg:40.06ms
step:2000/2330 val_loss:5.1391 train_time:80224ms step_avg:40.11ms
step:2001/2330 train_time:80237ms step_avg:40.10ms
step:2002/2330 train_time:80251ms step_avg:40.09ms
step:2003/2330 train_time:80262ms step_avg:40.07ms
step:2004/2330 train_time:80287ms step_avg:40.06ms
step:2005/2330 train_time:80308ms step_avg:40.05ms
step:2006/2330 train_time:80364ms step_avg:40.06ms
step:2007/2330 train_time:80386ms step_avg:40.05ms
step:2008/2330 train_time:80442ms step_avg:40.06ms
step:2009/2330 train_time:80464ms step_avg:40.05ms
step:2010/2330 train_time:80522ms step_avg:40.06ms
step:2011/2330 train_time:80548ms step_avg:40.05ms
step:2012/2330 train_time:80608ms step_avg:40.06ms
step:2013/2330 train_time:80632ms step_avg:40.06ms
step:2014/2330 train_time:80689ms step_avg:40.06ms
step:2015/2330 train_time:80712ms step_avg:40.06ms
step:2016/2330 train_time:80769ms step_avg:40.06ms
step:2017/2330 train_time:80791ms step_avg:40.06ms
step:2018/2330 train_time:80848ms step_avg:40.06ms
step:2019/2330 train_time:80870ms step_avg:40.05ms
step:2020/2330 train_time:80926ms step_avg:40.06ms
step:2021/2330 train_time:80949ms step_avg:40.05ms
step:2022/2330 train_time:81006ms step_avg:40.06ms
step:2023/2330 train_time:81028ms step_avg:40.05ms
step:2024/2330 train_time:81084ms step_avg:40.06ms
step:2025/2330 train_time:81107ms step_avg:40.05ms
step:2026/2330 train_time:81167ms step_avg:40.06ms
step:2027/2330 train_time:81191ms step_avg:40.05ms
step:2028/2330 train_time:81249ms step_avg:40.06ms
step:2029/2330 train_time:81271ms step_avg:40.05ms
step:2030/2330 train_time:81328ms step_avg:40.06ms
step:2031/2330 train_time:81350ms step_avg:40.05ms
step:2032/2330 train_time:81409ms step_avg:40.06ms
step:2033/2330 train_time:81432ms step_avg:40.05ms
step:2034/2330 train_time:81491ms step_avg:40.06ms
step:2035/2330 train_time:81514ms step_avg:40.06ms
step:2036/2330 train_time:81572ms step_avg:40.06ms
step:2037/2330 train_time:81594ms step_avg:40.06ms
step:2038/2330 train_time:81652ms step_avg:40.06ms
step:2039/2330 train_time:81675ms step_avg:40.06ms
step:2040/2330 train_time:81732ms step_avg:40.06ms
step:2041/2330 train_time:81754ms step_avg:40.06ms
step:2042/2330 train_time:81811ms step_avg:40.06ms
step:2043/2330 train_time:81833ms step_avg:40.06ms
step:2044/2330 train_time:81890ms step_avg:40.06ms
step:2045/2330 train_time:81912ms step_avg:40.05ms
step:2046/2330 train_time:81969ms step_avg:40.06ms
step:2047/2330 train_time:81992ms step_avg:40.05ms
step:2048/2330 train_time:82049ms step_avg:40.06ms
step:2049/2330 train_time:82071ms step_avg:40.05ms
step:2050/2330 train_time:82128ms step_avg:40.06ms
step:2051/2330 train_time:82151ms step_avg:40.05ms
step:2052/2330 train_time:82208ms step_avg:40.06ms
step:2053/2330 train_time:82230ms step_avg:40.05ms
step:2054/2330 train_time:82288ms step_avg:40.06ms
step:2055/2330 train_time:82310ms step_avg:40.05ms
step:2056/2330 train_time:82367ms step_avg:40.06ms
step:2057/2330 train_time:82390ms step_avg:40.05ms
step:2058/2330 train_time:82448ms step_avg:40.06ms
step:2059/2330 train_time:82470ms step_avg:40.05ms
step:2060/2330 train_time:82528ms step_avg:40.06ms
step:2061/2330 train_time:82551ms step_avg:40.05ms
step:2062/2330 train_time:82608ms step_avg:40.06ms
step:2063/2330 train_time:82631ms step_avg:40.05ms
step:2064/2330 train_time:82688ms step_avg:40.06ms
step:2065/2330 train_time:82710ms step_avg:40.05ms
step:2066/2330 train_time:82768ms step_avg:40.06ms
step:2067/2330 train_time:82790ms step_avg:40.05ms
step:2068/2330 train_time:82847ms step_avg:40.06ms
step:2069/2330 train_time:82869ms step_avg:40.05ms
step:2070/2330 train_time:82926ms step_avg:40.06ms
step:2071/2330 train_time:82949ms step_avg:40.05ms
step:2072/2330 train_time:83007ms step_avg:40.06ms
step:2073/2330 train_time:83029ms step_avg:40.05ms
step:2074/2330 train_time:83086ms step_avg:40.06ms
step:2075/2330 train_time:83109ms step_avg:40.05ms
step:2076/2330 train_time:83165ms step_avg:40.06ms
step:2077/2330 train_time:83188ms step_avg:40.05ms
step:2078/2330 train_time:83245ms step_avg:40.06ms
step:2079/2330 train_time:83267ms step_avg:40.05ms
step:2080/2330 train_time:83325ms step_avg:40.06ms
step:2081/2330 train_time:83348ms step_avg:40.05ms
step:2082/2330 train_time:83405ms step_avg:40.06ms
step:2083/2330 train_time:83428ms step_avg:40.05ms
step:2084/2330 train_time:83485ms step_avg:40.06ms
step:2085/2330 train_time:83508ms step_avg:40.05ms
step:2086/2330 train_time:83566ms step_avg:40.06ms
step:2087/2330 train_time:83589ms step_avg:40.05ms
step:2088/2330 train_time:83647ms step_avg:40.06ms
step:2089/2330 train_time:83670ms step_avg:40.05ms
step:2090/2330 train_time:83726ms step_avg:40.06ms
step:2091/2330 train_time:83749ms step_avg:40.05ms
step:2092/2330 train_time:83807ms step_avg:40.06ms
step:2093/2330 train_time:83830ms step_avg:40.05ms
step:2094/2330 train_time:83887ms step_avg:40.06ms
step:2095/2330 train_time:83909ms step_avg:40.05ms
step:2096/2330 train_time:83967ms step_avg:40.06ms
step:2097/2330 train_time:83988ms step_avg:40.05ms
step:2098/2330 train_time:84046ms step_avg:40.06ms
step:2099/2330 train_time:84068ms step_avg:40.05ms
step:2100/2330 train_time:84126ms step_avg:40.06ms
step:2101/2330 train_time:84149ms step_avg:40.05ms
step:2102/2330 train_time:84206ms step_avg:40.06ms
step:2103/2330 train_time:84228ms step_avg:40.05ms
step:2104/2330 train_time:84285ms step_avg:40.06ms
step:2105/2330 train_time:84308ms step_avg:40.05ms
step:2106/2330 train_time:84365ms step_avg:40.06ms
step:2107/2330 train_time:84389ms step_avg:40.05ms
step:2108/2330 train_time:84446ms step_avg:40.06ms
step:2109/2330 train_time:84469ms step_avg:40.05ms
step:2110/2330 train_time:84526ms step_avg:40.06ms
step:2111/2330 train_time:84549ms step_avg:40.05ms
step:2112/2330 train_time:84606ms step_avg:40.06ms
step:2113/2330 train_time:84629ms step_avg:40.05ms
step:2114/2330 train_time:84686ms step_avg:40.06ms
step:2115/2330 train_time:84709ms step_avg:40.05ms
step:2116/2330 train_time:84766ms step_avg:40.06ms
step:2117/2330 train_time:84788ms step_avg:40.05ms
step:2118/2330 train_time:84846ms step_avg:40.06ms
step:2119/2330 train_time:84868ms step_avg:40.05ms
step:2120/2330 train_time:84924ms step_avg:40.06ms
step:2121/2330 train_time:84948ms step_avg:40.05ms
step:2122/2330 train_time:85005ms step_avg:40.06ms
step:2123/2330 train_time:85028ms step_avg:40.05ms
step:2124/2330 train_time:85085ms step_avg:40.06ms
step:2125/2330 train_time:85109ms step_avg:40.05ms
step:2126/2330 train_time:85166ms step_avg:40.06ms
step:2127/2330 train_time:85188ms step_avg:40.05ms
step:2128/2330 train_time:85245ms step_avg:40.06ms
step:2129/2330 train_time:85267ms step_avg:40.05ms
step:2130/2330 train_time:85324ms step_avg:40.06ms
step:2131/2330 train_time:85347ms step_avg:40.05ms
step:2132/2330 train_time:85404ms step_avg:40.06ms
step:2133/2330 train_time:85427ms step_avg:40.05ms
step:2134/2330 train_time:85484ms step_avg:40.06ms
step:2135/2330 train_time:85507ms step_avg:40.05ms
step:2136/2330 train_time:85563ms step_avg:40.06ms
step:2137/2330 train_time:85586ms step_avg:40.05ms
step:2138/2330 train_time:85643ms step_avg:40.06ms
step:2139/2330 train_time:85666ms step_avg:40.05ms
step:2140/2330 train_time:85723ms step_avg:40.06ms
step:2141/2330 train_time:85746ms step_avg:40.05ms
step:2142/2330 train_time:85803ms step_avg:40.06ms
step:2143/2330 train_time:85826ms step_avg:40.05ms
step:2144/2330 train_time:85883ms step_avg:40.06ms
step:2145/2330 train_time:85906ms step_avg:40.05ms
step:2146/2330 train_time:85962ms step_avg:40.06ms
step:2147/2330 train_time:85986ms step_avg:40.05ms
step:2148/2330 train_time:86042ms step_avg:40.06ms
step:2149/2330 train_time:86066ms step_avg:40.05ms
step:2150/2330 train_time:86122ms step_avg:40.06ms
step:2151/2330 train_time:86146ms step_avg:40.05ms
step:2152/2330 train_time:86202ms step_avg:40.06ms
step:2153/2330 train_time:86225ms step_avg:40.05ms
step:2154/2330 train_time:86281ms step_avg:40.06ms
step:2155/2330 train_time:86304ms step_avg:40.05ms
step:2156/2330 train_time:86361ms step_avg:40.06ms
step:2157/2330 train_time:86384ms step_avg:40.05ms
step:2158/2330 train_time:86441ms step_avg:40.06ms
step:2159/2330 train_time:86465ms step_avg:40.05ms
step:2160/2330 train_time:86522ms step_avg:40.06ms
step:2161/2330 train_time:86545ms step_avg:40.05ms
step:2162/2330 train_time:86601ms step_avg:40.06ms
step:2163/2330 train_time:86625ms step_avg:40.05ms
step:2164/2330 train_time:86681ms step_avg:40.06ms
step:2165/2330 train_time:86705ms step_avg:40.05ms
step:2166/2330 train_time:86761ms step_avg:40.06ms
step:2167/2330 train_time:86785ms step_avg:40.05ms
step:2168/2330 train_time:86841ms step_avg:40.06ms
step:2169/2330 train_time:86865ms step_avg:40.05ms
step:2170/2330 train_time:86922ms step_avg:40.06ms
step:2171/2330 train_time:86945ms step_avg:40.05ms
step:2172/2330 train_time:87002ms step_avg:40.06ms
step:2173/2330 train_time:87025ms step_avg:40.05ms
step:2174/2330 train_time:87082ms step_avg:40.06ms
step:2175/2330 train_time:87106ms step_avg:40.05ms
step:2176/2330 train_time:87162ms step_avg:40.06ms
step:2177/2330 train_time:87185ms step_avg:40.05ms
step:2178/2330 train_time:87241ms step_avg:40.06ms
step:2179/2330 train_time:87265ms step_avg:40.05ms
step:2180/2330 train_time:87322ms step_avg:40.06ms
step:2181/2330 train_time:87346ms step_avg:40.05ms
step:2182/2330 train_time:87402ms step_avg:40.06ms
step:2183/2330 train_time:87425ms step_avg:40.05ms
step:2184/2330 train_time:87482ms step_avg:40.06ms
step:2185/2330 train_time:87505ms step_avg:40.05ms
step:2186/2330 train_time:87561ms step_avg:40.06ms
step:2187/2330 train_time:87584ms step_avg:40.05ms
step:2188/2330 train_time:87641ms step_avg:40.06ms
step:2189/2330 train_time:87665ms step_avg:40.05ms
step:2190/2330 train_time:87722ms step_avg:40.06ms
step:2191/2330 train_time:87745ms step_avg:40.05ms
step:2192/2330 train_time:87802ms step_avg:40.06ms
step:2193/2330 train_time:87825ms step_avg:40.05ms
step:2194/2330 train_time:87882ms step_avg:40.06ms
step:2195/2330 train_time:87905ms step_avg:40.05ms
step:2196/2330 train_time:87962ms step_avg:40.06ms
step:2197/2330 train_time:87985ms step_avg:40.05ms
step:2198/2330 train_time:88041ms step_avg:40.06ms
step:2199/2330 train_time:88065ms step_avg:40.05ms
step:2200/2330 train_time:88121ms step_avg:40.06ms
step:2201/2330 train_time:88145ms step_avg:40.05ms
step:2202/2330 train_time:88201ms step_avg:40.05ms
step:2203/2330 train_time:88224ms step_avg:40.05ms
step:2204/2330 train_time:88281ms step_avg:40.05ms
step:2205/2330 train_time:88305ms step_avg:40.05ms
step:2206/2330 train_time:88361ms step_avg:40.05ms
step:2207/2330 train_time:88385ms step_avg:40.05ms
step:2208/2330 train_time:88441ms step_avg:40.05ms
step:2209/2330 train_time:88465ms step_avg:40.05ms
step:2210/2330 train_time:88521ms step_avg:40.05ms
step:2211/2330 train_time:88545ms step_avg:40.05ms
step:2212/2330 train_time:88601ms step_avg:40.05ms
step:2213/2330 train_time:88624ms step_avg:40.05ms
step:2214/2330 train_time:88681ms step_avg:40.05ms
step:2215/2330 train_time:88705ms step_avg:40.05ms
step:2216/2330 train_time:88761ms step_avg:40.05ms
step:2217/2330 train_time:88784ms step_avg:40.05ms
step:2218/2330 train_time:88841ms step_avg:40.05ms
step:2219/2330 train_time:88865ms step_avg:40.05ms
step:2220/2330 train_time:88921ms step_avg:40.05ms
step:2221/2330 train_time:88945ms step_avg:40.05ms
step:2222/2330 train_time:89001ms step_avg:40.05ms
step:2223/2330 train_time:89025ms step_avg:40.05ms
step:2224/2330 train_time:89081ms step_avg:40.05ms
step:2225/2330 train_time:89105ms step_avg:40.05ms
step:2226/2330 train_time:89161ms step_avg:40.05ms
step:2227/2330 train_time:89185ms step_avg:40.05ms
step:2228/2330 train_time:89241ms step_avg:40.05ms
step:2229/2330 train_time:89264ms step_avg:40.05ms
step:2230/2330 train_time:89321ms step_avg:40.05ms
step:2231/2330 train_time:89344ms step_avg:40.05ms
step:2232/2330 train_time:89401ms step_avg:40.05ms
step:2233/2330 train_time:89424ms step_avg:40.05ms
step:2234/2330 train_time:89480ms step_avg:40.05ms
step:2235/2330 train_time:89503ms step_avg:40.05ms
step:2236/2330 train_time:89560ms step_avg:40.05ms
step:2237/2330 train_time:89583ms step_avg:40.05ms
step:2238/2330 train_time:89639ms step_avg:40.05ms
step:2239/2330 train_time:89663ms step_avg:40.05ms
step:2240/2330 train_time:89720ms step_avg:40.05ms
step:2241/2330 train_time:89743ms step_avg:40.05ms
step:2242/2330 train_time:89800ms step_avg:40.05ms
step:2243/2330 train_time:89824ms step_avg:40.05ms
step:2244/2330 train_time:89881ms step_avg:40.05ms
step:2245/2330 train_time:89904ms step_avg:40.05ms
step:2246/2330 train_time:89961ms step_avg:40.05ms
step:2247/2330 train_time:89984ms step_avg:40.05ms
step:2248/2330 train_time:90041ms step_avg:40.05ms
step:2249/2330 train_time:90064ms step_avg:40.05ms
step:2250/2330 train_time:90121ms step_avg:40.05ms
step:2250/2330 val_loss:5.1113 train_time:90219ms step_avg:40.10ms
step:2251/2330 train_time:90232ms step_avg:40.09ms
step:2252/2330 train_time:90244ms step_avg:40.07ms
step:2253/2330 train_time:90253ms step_avg:40.06ms
step:2254/2330 train_time:90283ms step_avg:40.05ms
step:2255/2330 train_time:90305ms step_avg:40.05ms
step:2256/2330 train_time:90361ms step_avg:40.05ms
step:2257/2330 train_time:90384ms step_avg:40.05ms
step:2258/2330 train_time:90441ms step_avg:40.05ms
step:2259/2330 train_time:90464ms step_avg:40.05ms
step:2260/2330 train_time:90525ms step_avg:40.06ms
step:2261/2330 train_time:90552ms step_avg:40.05ms
step:2262/2330 train_time:90613ms step_avg:40.06ms
step:2263/2330 train_time:90635ms step_avg:40.05ms
step:2264/2330 train_time:90693ms step_avg:40.06ms
step:2265/2330 train_time:90716ms step_avg:40.05ms
step:2266/2330 train_time:90773ms step_avg:40.06ms
step:2267/2330 train_time:90795ms step_avg:40.05ms
step:2268/2330 train_time:90852ms step_avg:40.06ms
step:2269/2330 train_time:90874ms step_avg:40.05ms
step:2270/2330 train_time:90931ms step_avg:40.06ms
step:2271/2330 train_time:90953ms step_avg:40.05ms
step:2272/2330 train_time:91010ms step_avg:40.06ms
step:2273/2330 train_time:91032ms step_avg:40.05ms
step:2274/2330 train_time:91088ms step_avg:40.06ms
step:2275/2330 train_time:91111ms step_avg:40.05ms
step:2276/2330 train_time:91169ms step_avg:40.06ms
step:2277/2330 train_time:91193ms step_avg:40.05ms
step:2278/2330 train_time:91250ms step_avg:40.06ms
step:2279/2330 train_time:91272ms step_avg:40.05ms
step:2280/2330 train_time:91330ms step_avg:40.06ms
step:2281/2330 train_time:91352ms step_avg:40.05ms
step:2282/2330 train_time:91410ms step_avg:40.06ms
step:2283/2330 train_time:91433ms step_avg:40.05ms
step:2284/2330 train_time:91492ms step_avg:40.06ms
step:2285/2330 train_time:91515ms step_avg:40.05ms
step:2286/2330 train_time:91572ms step_avg:40.06ms
step:2287/2330 train_time:91595ms step_avg:40.05ms
step:2288/2330 train_time:91652ms step_avg:40.06ms
step:2289/2330 train_time:91675ms step_avg:40.05ms
step:2290/2330 train_time:91733ms step_avg:40.06ms
step:2291/2330 train_time:91756ms step_avg:40.05ms
step:2292/2330 train_time:91813ms step_avg:40.06ms
step:2293/2330 train_time:91835ms step_avg:40.05ms
step:2294/2330 train_time:91892ms step_avg:40.06ms
step:2295/2330 train_time:91915ms step_avg:40.05ms
step:2296/2330 train_time:91971ms step_avg:40.06ms
step:2297/2330 train_time:91993ms step_avg:40.05ms
step:2298/2330 train_time:92051ms step_avg:40.06ms
step:2299/2330 train_time:92073ms step_avg:40.05ms
step:2300/2330 train_time:92130ms step_avg:40.06ms
step:2301/2330 train_time:92152ms step_avg:40.05ms
step:2302/2330 train_time:92210ms step_avg:40.06ms
step:2303/2330 train_time:92231ms step_avg:40.05ms
step:2304/2330 train_time:92289ms step_avg:40.06ms
step:2305/2330 train_time:92311ms step_avg:40.05ms
step:2306/2330 train_time:92369ms step_avg:40.06ms
step:2307/2330 train_time:92392ms step_avg:40.05ms
step:2308/2330 train_time:92449ms step_avg:40.06ms
step:2309/2330 train_time:92472ms step_avg:40.05ms
step:2310/2330 train_time:92531ms step_avg:40.06ms
step:2311/2330 train_time:92554ms step_avg:40.05ms
step:2312/2330 train_time:92612ms step_avg:40.06ms
step:2313/2330 train_time:92635ms step_avg:40.05ms
step:2314/2330 train_time:92692ms step_avg:40.06ms
step:2315/2330 train_time:92714ms step_avg:40.05ms
step:2316/2330 train_time:92772ms step_avg:40.06ms
step:2317/2330 train_time:92794ms step_avg:40.05ms
step:2318/2330 train_time:92851ms step_avg:40.06ms
step:2319/2330 train_time:92873ms step_avg:40.05ms
step:2320/2330 train_time:92929ms step_avg:40.06ms
step:2321/2330 train_time:92951ms step_avg:40.05ms
step:2322/2330 train_time:93009ms step_avg:40.06ms
step:2323/2330 train_time:93031ms step_avg:40.05ms
step:2324/2330 train_time:93089ms step_avg:40.06ms
step:2325/2330 train_time:93111ms step_avg:40.05ms
step:2326/2330 train_time:93168ms step_avg:40.05ms
step:2327/2330 train_time:93190ms step_avg:40.05ms
step:2328/2330 train_time:93247ms step_avg:40.05ms
step:2329/2330 train_time:93270ms step_avg:40.05ms
step:2330/2330 train_time:93327ms step_avg:40.05ms
step:2330/2330 val_loss:5.1040 train_time:93426ms step_avg:40.10ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
