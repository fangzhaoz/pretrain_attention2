import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_3e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:04:24 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:96ms step_avg:96.48ms
step:2/2330 train_time:189ms step_avg:94.35ms
step:3/2330 train_time:211ms step_avg:70.29ms
step:4/2330 train_time:238ms step_avg:59.56ms
step:5/2330 train_time:295ms step_avg:58.94ms
step:6/2330 train_time:355ms step_avg:59.17ms
step:7/2330 train_time:413ms step_avg:58.94ms
step:8/2330 train_time:473ms step_avg:59.19ms
step:9/2330 train_time:532ms step_avg:59.08ms
step:10/2330 train_time:592ms step_avg:59.22ms
step:11/2330 train_time:650ms step_avg:59.08ms
step:12/2330 train_time:710ms step_avg:59.21ms
step:13/2330 train_time:768ms step_avg:59.10ms
step:14/2330 train_time:829ms step_avg:59.25ms
step:15/2330 train_time:888ms step_avg:59.17ms
step:16/2330 train_time:949ms step_avg:59.32ms
step:17/2330 train_time:1007ms step_avg:59.26ms
step:18/2330 train_time:1073ms step_avg:59.61ms
step:19/2330 train_time:1138ms step_avg:59.87ms
step:20/2330 train_time:1201ms step_avg:60.03ms
step:21/2330 train_time:1260ms step_avg:59.99ms
step:22/2330 train_time:1323ms step_avg:60.12ms
step:23/2330 train_time:1381ms step_avg:60.04ms
step:24/2330 train_time:1443ms step_avg:60.11ms
step:25/2330 train_time:1501ms step_avg:60.06ms
step:26/2330 train_time:1563ms step_avg:60.12ms
step:27/2330 train_time:1622ms step_avg:60.07ms
step:28/2330 train_time:1683ms step_avg:60.11ms
step:29/2330 train_time:1742ms step_avg:60.09ms
step:30/2330 train_time:1804ms step_avg:60.13ms
step:31/2330 train_time:1862ms step_avg:60.07ms
step:32/2330 train_time:1924ms step_avg:60.12ms
step:33/2330 train_time:1983ms step_avg:60.10ms
step:34/2330 train_time:2046ms step_avg:60.19ms
step:35/2330 train_time:2106ms step_avg:60.17ms
step:36/2330 train_time:2169ms step_avg:60.24ms
step:37/2330 train_time:2228ms step_avg:60.23ms
step:38/2330 train_time:2291ms step_avg:60.29ms
step:39/2330 train_time:2349ms step_avg:60.24ms
step:40/2330 train_time:2412ms step_avg:60.29ms
step:41/2330 train_time:2471ms step_avg:60.26ms
step:42/2330 train_time:2533ms step_avg:60.31ms
step:43/2330 train_time:2593ms step_avg:60.30ms
step:44/2330 train_time:2655ms step_avg:60.35ms
step:45/2330 train_time:2715ms step_avg:60.33ms
step:46/2330 train_time:2777ms step_avg:60.38ms
step:47/2330 train_time:2838ms step_avg:60.38ms
step:48/2330 train_time:2900ms step_avg:60.42ms
step:49/2330 train_time:2960ms step_avg:60.41ms
step:50/2330 train_time:3022ms step_avg:60.43ms
step:51/2330 train_time:3080ms step_avg:60.39ms
step:52/2330 train_time:3142ms step_avg:60.42ms
step:53/2330 train_time:3201ms step_avg:60.40ms
step:54/2330 train_time:3263ms step_avg:60.43ms
step:55/2330 train_time:3322ms step_avg:60.40ms
step:56/2330 train_time:3385ms step_avg:60.45ms
step:57/2330 train_time:3444ms step_avg:60.42ms
step:58/2330 train_time:3507ms step_avg:60.47ms
step:59/2330 train_time:3567ms step_avg:60.45ms
step:60/2330 train_time:3628ms step_avg:60.47ms
step:61/2330 train_time:3686ms step_avg:60.43ms
step:62/2330 train_time:3749ms step_avg:60.46ms
step:63/2330 train_time:3807ms step_avg:60.44ms
step:64/2330 train_time:3870ms step_avg:60.47ms
step:65/2330 train_time:3929ms step_avg:60.45ms
step:66/2330 train_time:3992ms step_avg:60.49ms
step:67/2330 train_time:4052ms step_avg:60.47ms
step:68/2330 train_time:4114ms step_avg:60.50ms
step:69/2330 train_time:4173ms step_avg:60.48ms
step:70/2330 train_time:4236ms step_avg:60.52ms
step:71/2330 train_time:4296ms step_avg:60.50ms
step:72/2330 train_time:4359ms step_avg:60.54ms
step:73/2330 train_time:4418ms step_avg:60.52ms
step:74/2330 train_time:4479ms step_avg:60.53ms
step:75/2330 train_time:4539ms step_avg:60.52ms
step:76/2330 train_time:4600ms step_avg:60.53ms
step:77/2330 train_time:4660ms step_avg:60.51ms
step:78/2330 train_time:4721ms step_avg:60.53ms
step:79/2330 train_time:4780ms step_avg:60.51ms
step:80/2330 train_time:4842ms step_avg:60.53ms
step:81/2330 train_time:4901ms step_avg:60.51ms
step:82/2330 train_time:4963ms step_avg:60.53ms
step:83/2330 train_time:5023ms step_avg:60.51ms
step:84/2330 train_time:5086ms step_avg:60.55ms
step:85/2330 train_time:5144ms step_avg:60.52ms
step:86/2330 train_time:5207ms step_avg:60.55ms
step:87/2330 train_time:5266ms step_avg:60.53ms
step:88/2330 train_time:5328ms step_avg:60.54ms
step:89/2330 train_time:5387ms step_avg:60.53ms
step:90/2330 train_time:5450ms step_avg:60.55ms
step:91/2330 train_time:5508ms step_avg:60.53ms
step:92/2330 train_time:5570ms step_avg:60.55ms
step:93/2330 train_time:5630ms step_avg:60.54ms
step:94/2330 train_time:5692ms step_avg:60.55ms
step:95/2330 train_time:5751ms step_avg:60.53ms
step:96/2330 train_time:5813ms step_avg:60.56ms
step:97/2330 train_time:5873ms step_avg:60.55ms
step:98/2330 train_time:5936ms step_avg:60.57ms
step:99/2330 train_time:5995ms step_avg:60.56ms
step:100/2330 train_time:6058ms step_avg:60.58ms
step:101/2330 train_time:6116ms step_avg:60.56ms
step:102/2330 train_time:6179ms step_avg:60.58ms
step:103/2330 train_time:6239ms step_avg:60.57ms
step:104/2330 train_time:6300ms step_avg:60.58ms
step:105/2330 train_time:6359ms step_avg:60.56ms
step:106/2330 train_time:6421ms step_avg:60.58ms
step:107/2330 train_time:6480ms step_avg:60.56ms
step:108/2330 train_time:6543ms step_avg:60.59ms
step:109/2330 train_time:6602ms step_avg:60.57ms
step:110/2330 train_time:6664ms step_avg:60.58ms
step:111/2330 train_time:6723ms step_avg:60.57ms
step:112/2330 train_time:6785ms step_avg:60.58ms
step:113/2330 train_time:6844ms step_avg:60.57ms
step:114/2330 train_time:6907ms step_avg:60.59ms
step:115/2330 train_time:6966ms step_avg:60.57ms
step:116/2330 train_time:7029ms step_avg:60.59ms
step:117/2330 train_time:7088ms step_avg:60.58ms
step:118/2330 train_time:7150ms step_avg:60.59ms
step:119/2330 train_time:7208ms step_avg:60.57ms
step:120/2330 train_time:7271ms step_avg:60.59ms
step:121/2330 train_time:7331ms step_avg:60.59ms
step:122/2330 train_time:7394ms step_avg:60.60ms
step:123/2330 train_time:7454ms step_avg:60.60ms
step:124/2330 train_time:7517ms step_avg:60.62ms
step:125/2330 train_time:7576ms step_avg:60.61ms
step:126/2330 train_time:7638ms step_avg:60.62ms
step:127/2330 train_time:7698ms step_avg:60.61ms
step:128/2330 train_time:7760ms step_avg:60.62ms
step:129/2330 train_time:7819ms step_avg:60.61ms
step:130/2330 train_time:7882ms step_avg:60.63ms
step:131/2330 train_time:7941ms step_avg:60.62ms
step:132/2330 train_time:8003ms step_avg:60.63ms
step:133/2330 train_time:8062ms step_avg:60.62ms
step:134/2330 train_time:8124ms step_avg:60.63ms
step:135/2330 train_time:8183ms step_avg:60.62ms
step:136/2330 train_time:8246ms step_avg:60.63ms
step:137/2330 train_time:8305ms step_avg:60.62ms
step:138/2330 train_time:8368ms step_avg:60.64ms
step:139/2330 train_time:8427ms step_avg:60.63ms
step:140/2330 train_time:8489ms step_avg:60.64ms
step:141/2330 train_time:8548ms step_avg:60.63ms
step:142/2330 train_time:8610ms step_avg:60.63ms
step:143/2330 train_time:8670ms step_avg:60.63ms
step:144/2330 train_time:8733ms step_avg:60.65ms
step:145/2330 train_time:8792ms step_avg:60.64ms
step:146/2330 train_time:8855ms step_avg:60.65ms
step:147/2330 train_time:8915ms step_avg:60.65ms
step:148/2330 train_time:8978ms step_avg:60.66ms
step:149/2330 train_time:9038ms step_avg:60.66ms
step:150/2330 train_time:9100ms step_avg:60.67ms
step:151/2330 train_time:9159ms step_avg:60.66ms
step:152/2330 train_time:9221ms step_avg:60.66ms
step:153/2330 train_time:9280ms step_avg:60.66ms
step:154/2330 train_time:9342ms step_avg:60.66ms
step:155/2330 train_time:9401ms step_avg:60.65ms
step:156/2330 train_time:9464ms step_avg:60.67ms
step:157/2330 train_time:9524ms step_avg:60.66ms
step:158/2330 train_time:9586ms step_avg:60.67ms
step:159/2330 train_time:9645ms step_avg:60.66ms
step:160/2330 train_time:9708ms step_avg:60.68ms
step:161/2330 train_time:9766ms step_avg:60.66ms
step:162/2330 train_time:9829ms step_avg:60.67ms
step:163/2330 train_time:9887ms step_avg:60.66ms
step:164/2330 train_time:9950ms step_avg:60.67ms
step:165/2330 train_time:10009ms step_avg:60.66ms
step:166/2330 train_time:10073ms step_avg:60.68ms
step:167/2330 train_time:10134ms step_avg:60.68ms
step:168/2330 train_time:10197ms step_avg:60.70ms
step:169/2330 train_time:10256ms step_avg:60.69ms
step:170/2330 train_time:10318ms step_avg:60.70ms
step:171/2330 train_time:10378ms step_avg:60.69ms
step:172/2330 train_time:10441ms step_avg:60.70ms
step:173/2330 train_time:10500ms step_avg:60.69ms
step:174/2330 train_time:10561ms step_avg:60.70ms
step:175/2330 train_time:10620ms step_avg:60.69ms
step:176/2330 train_time:10682ms step_avg:60.69ms
step:177/2330 train_time:10741ms step_avg:60.69ms
step:178/2330 train_time:10803ms step_avg:60.69ms
step:179/2330 train_time:10863ms step_avg:60.69ms
step:180/2330 train_time:10926ms step_avg:60.70ms
step:181/2330 train_time:10986ms step_avg:60.69ms
step:182/2330 train_time:11049ms step_avg:60.71ms
step:183/2330 train_time:11107ms step_avg:60.69ms
step:184/2330 train_time:11169ms step_avg:60.70ms
step:185/2330 train_time:11228ms step_avg:60.69ms
step:186/2330 train_time:11290ms step_avg:60.70ms
step:187/2330 train_time:11350ms step_avg:60.69ms
step:188/2330 train_time:11412ms step_avg:60.70ms
step:189/2330 train_time:11472ms step_avg:60.70ms
step:190/2330 train_time:11536ms step_avg:60.71ms
step:191/2330 train_time:11596ms step_avg:60.71ms
step:192/2330 train_time:11659ms step_avg:60.72ms
step:193/2330 train_time:11718ms step_avg:60.71ms
step:194/2330 train_time:11780ms step_avg:60.72ms
step:195/2330 train_time:11840ms step_avg:60.72ms
step:196/2330 train_time:11901ms step_avg:60.72ms
step:197/2330 train_time:11961ms step_avg:60.71ms
step:198/2330 train_time:12023ms step_avg:60.72ms
step:199/2330 train_time:12082ms step_avg:60.71ms
step:200/2330 train_time:12145ms step_avg:60.72ms
step:201/2330 train_time:12203ms step_avg:60.71ms
step:202/2330 train_time:12266ms step_avg:60.72ms
step:203/2330 train_time:12325ms step_avg:60.72ms
step:204/2330 train_time:12389ms step_avg:60.73ms
step:205/2330 train_time:12447ms step_avg:60.72ms
step:206/2330 train_time:12510ms step_avg:60.73ms
step:207/2330 train_time:12569ms step_avg:60.72ms
step:208/2330 train_time:12631ms step_avg:60.73ms
step:209/2330 train_time:12691ms step_avg:60.72ms
step:210/2330 train_time:12754ms step_avg:60.73ms
step:211/2330 train_time:12814ms step_avg:60.73ms
step:212/2330 train_time:12877ms step_avg:60.74ms
step:213/2330 train_time:12938ms step_avg:60.74ms
step:214/2330 train_time:13000ms step_avg:60.75ms
step:215/2330 train_time:13059ms step_avg:60.74ms
step:216/2330 train_time:13120ms step_avg:60.74ms
step:217/2330 train_time:13179ms step_avg:60.73ms
step:218/2330 train_time:13241ms step_avg:60.74ms
step:219/2330 train_time:13300ms step_avg:60.73ms
step:220/2330 train_time:13363ms step_avg:60.74ms
step:221/2330 train_time:13424ms step_avg:60.74ms
step:222/2330 train_time:13486ms step_avg:60.75ms
step:223/2330 train_time:13546ms step_avg:60.75ms
step:224/2330 train_time:13608ms step_avg:60.75ms
step:225/2330 train_time:13667ms step_avg:60.74ms
step:226/2330 train_time:13729ms step_avg:60.75ms
step:227/2330 train_time:13788ms step_avg:60.74ms
step:228/2330 train_time:13852ms step_avg:60.75ms
step:229/2330 train_time:13911ms step_avg:60.75ms
step:230/2330 train_time:13974ms step_avg:60.75ms
step:231/2330 train_time:14034ms step_avg:60.75ms
step:232/2330 train_time:14096ms step_avg:60.76ms
step:233/2330 train_time:14156ms step_avg:60.75ms
step:234/2330 train_time:14218ms step_avg:60.76ms
step:235/2330 train_time:14277ms step_avg:60.75ms
step:236/2330 train_time:14340ms step_avg:60.76ms
step:237/2330 train_time:14399ms step_avg:60.75ms
step:238/2330 train_time:14461ms step_avg:60.76ms
step:239/2330 train_time:14521ms step_avg:60.76ms
step:240/2330 train_time:14582ms step_avg:60.76ms
step:241/2330 train_time:14641ms step_avg:60.75ms
step:242/2330 train_time:14703ms step_avg:60.76ms
step:243/2330 train_time:14763ms step_avg:60.75ms
step:244/2330 train_time:14826ms step_avg:60.76ms
step:245/2330 train_time:14885ms step_avg:60.76ms
step:246/2330 train_time:14949ms step_avg:60.77ms
step:247/2330 train_time:15008ms step_avg:60.76ms
step:248/2330 train_time:15070ms step_avg:60.77ms
step:249/2330 train_time:15129ms step_avg:60.76ms
step:250/2330 train_time:15192ms step_avg:60.77ms
step:250/2330 val_loss:5.2416 train_time:15265ms step_avg:61.06ms
step:251/2330 train_time:15286ms step_avg:60.90ms
step:252/2330 train_time:15317ms step_avg:60.78ms
step:253/2330 train_time:15376ms step_avg:60.77ms
step:254/2330 train_time:15444ms step_avg:60.80ms
step:255/2330 train_time:15510ms step_avg:60.82ms
step:256/2330 train_time:15573ms step_avg:60.83ms
step:257/2330 train_time:15632ms step_avg:60.83ms
step:258/2330 train_time:15695ms step_avg:60.83ms
step:259/2330 train_time:15754ms step_avg:60.82ms
step:260/2330 train_time:15816ms step_avg:60.83ms
step:261/2330 train_time:15875ms step_avg:60.82ms
step:262/2330 train_time:15936ms step_avg:60.82ms
step:263/2330 train_time:15995ms step_avg:60.82ms
step:264/2330 train_time:16056ms step_avg:60.82ms
step:265/2330 train_time:16115ms step_avg:60.81ms
step:266/2330 train_time:16177ms step_avg:60.81ms
step:267/2330 train_time:16236ms step_avg:60.81ms
step:268/2330 train_time:16298ms step_avg:60.81ms
step:269/2330 train_time:16359ms step_avg:60.81ms
step:270/2330 train_time:16423ms step_avg:60.83ms
step:271/2330 train_time:16484ms step_avg:60.83ms
step:272/2330 train_time:16546ms step_avg:60.83ms
step:273/2330 train_time:16605ms step_avg:60.83ms
step:274/2330 train_time:16667ms step_avg:60.83ms
step:275/2330 train_time:16726ms step_avg:60.82ms
step:276/2330 train_time:16788ms step_avg:60.82ms
step:277/2330 train_time:16847ms step_avg:60.82ms
step:278/2330 train_time:16909ms step_avg:60.82ms
step:279/2330 train_time:16968ms step_avg:60.82ms
step:280/2330 train_time:17030ms step_avg:60.82ms
step:281/2330 train_time:17089ms step_avg:60.81ms
step:282/2330 train_time:17151ms step_avg:60.82ms
step:283/2330 train_time:17211ms step_avg:60.82ms
step:284/2330 train_time:17273ms step_avg:60.82ms
step:285/2330 train_time:17333ms step_avg:60.82ms
step:286/2330 train_time:17396ms step_avg:60.82ms
step:287/2330 train_time:17456ms step_avg:60.82ms
step:288/2330 train_time:17519ms step_avg:60.83ms
step:289/2330 train_time:17579ms step_avg:60.83ms
step:290/2330 train_time:17643ms step_avg:60.84ms
step:291/2330 train_time:17702ms step_avg:60.83ms
step:292/2330 train_time:17764ms step_avg:60.84ms
step:293/2330 train_time:17823ms step_avg:60.83ms
step:294/2330 train_time:17885ms step_avg:60.83ms
step:295/2330 train_time:17944ms step_avg:60.83ms
step:296/2330 train_time:18007ms step_avg:60.83ms
step:297/2330 train_time:18065ms step_avg:60.83ms
step:298/2330 train_time:18127ms step_avg:60.83ms
step:299/2330 train_time:18186ms step_avg:60.82ms
step:300/2330 train_time:18248ms step_avg:60.83ms
step:301/2330 train_time:18309ms step_avg:60.83ms
step:302/2330 train_time:18372ms step_avg:60.83ms
step:303/2330 train_time:18431ms step_avg:60.83ms
step:304/2330 train_time:18494ms step_avg:60.84ms
step:305/2330 train_time:18553ms step_avg:60.83ms
step:306/2330 train_time:18616ms step_avg:60.84ms
step:307/2330 train_time:18675ms step_avg:60.83ms
step:308/2330 train_time:18737ms step_avg:60.84ms
step:309/2330 train_time:18797ms step_avg:60.83ms
step:310/2330 train_time:18860ms step_avg:60.84ms
step:311/2330 train_time:18919ms step_avg:60.83ms
step:312/2330 train_time:18982ms step_avg:60.84ms
step:313/2330 train_time:19042ms step_avg:60.84ms
step:314/2330 train_time:19104ms step_avg:60.84ms
step:315/2330 train_time:19163ms step_avg:60.84ms
step:316/2330 train_time:19225ms step_avg:60.84ms
step:317/2330 train_time:19285ms step_avg:60.84ms
step:318/2330 train_time:19347ms step_avg:60.84ms
step:319/2330 train_time:19408ms step_avg:60.84ms
step:320/2330 train_time:19470ms step_avg:60.84ms
step:321/2330 train_time:19530ms step_avg:60.84ms
step:322/2330 train_time:19593ms step_avg:60.85ms
step:323/2330 train_time:19652ms step_avg:60.84ms
step:324/2330 train_time:19715ms step_avg:60.85ms
step:325/2330 train_time:19773ms step_avg:60.84ms
step:326/2330 train_time:19836ms step_avg:60.85ms
step:327/2330 train_time:19895ms step_avg:60.84ms
step:328/2330 train_time:19957ms step_avg:60.85ms
step:329/2330 train_time:20016ms step_avg:60.84ms
step:330/2330 train_time:20079ms step_avg:60.84ms
step:331/2330 train_time:20140ms step_avg:60.85ms
step:332/2330 train_time:20203ms step_avg:60.85ms
step:333/2330 train_time:20262ms step_avg:60.85ms
step:334/2330 train_time:20325ms step_avg:60.85ms
step:335/2330 train_time:20383ms step_avg:60.85ms
step:336/2330 train_time:20446ms step_avg:60.85ms
step:337/2330 train_time:20506ms step_avg:60.85ms
step:338/2330 train_time:20567ms step_avg:60.85ms
step:339/2330 train_time:20627ms step_avg:60.85ms
step:340/2330 train_time:20691ms step_avg:60.85ms
step:341/2330 train_time:20749ms step_avg:60.85ms
step:342/2330 train_time:20813ms step_avg:60.86ms
step:343/2330 train_time:20872ms step_avg:60.85ms
step:344/2330 train_time:20934ms step_avg:60.86ms
step:345/2330 train_time:20993ms step_avg:60.85ms
step:346/2330 train_time:21055ms step_avg:60.85ms
step:347/2330 train_time:21114ms step_avg:60.85ms
step:348/2330 train_time:21176ms step_avg:60.85ms
step:349/2330 train_time:21237ms step_avg:60.85ms
step:350/2330 train_time:21300ms step_avg:60.86ms
step:351/2330 train_time:21359ms step_avg:60.85ms
step:352/2330 train_time:21423ms step_avg:60.86ms
step:353/2330 train_time:21482ms step_avg:60.86ms
step:354/2330 train_time:21545ms step_avg:60.86ms
step:355/2330 train_time:21605ms step_avg:60.86ms
step:356/2330 train_time:21666ms step_avg:60.86ms
step:357/2330 train_time:21726ms step_avg:60.86ms
step:358/2330 train_time:21788ms step_avg:60.86ms
step:359/2330 train_time:21847ms step_avg:60.86ms
step:360/2330 train_time:21910ms step_avg:60.86ms
step:361/2330 train_time:21970ms step_avg:60.86ms
step:362/2330 train_time:22033ms step_avg:60.86ms
step:363/2330 train_time:22092ms step_avg:60.86ms
step:364/2330 train_time:22154ms step_avg:60.86ms
step:365/2330 train_time:22213ms step_avg:60.86ms
step:366/2330 train_time:22275ms step_avg:60.86ms
step:367/2330 train_time:22335ms step_avg:60.86ms
step:368/2330 train_time:22398ms step_avg:60.86ms
step:369/2330 train_time:22458ms step_avg:60.86ms
step:370/2330 train_time:22522ms step_avg:60.87ms
step:371/2330 train_time:22582ms step_avg:60.87ms
step:372/2330 train_time:22645ms step_avg:60.87ms
step:373/2330 train_time:22704ms step_avg:60.87ms
step:374/2330 train_time:22766ms step_avg:60.87ms
step:375/2330 train_time:22825ms step_avg:60.87ms
step:376/2330 train_time:22887ms step_avg:60.87ms
step:377/2330 train_time:22947ms step_avg:60.87ms
step:378/2330 train_time:23009ms step_avg:60.87ms
step:379/2330 train_time:23068ms step_avg:60.86ms
step:380/2330 train_time:23130ms step_avg:60.87ms
step:381/2330 train_time:23190ms step_avg:60.87ms
step:382/2330 train_time:23252ms step_avg:60.87ms
step:383/2330 train_time:23312ms step_avg:60.87ms
step:384/2330 train_time:23374ms step_avg:60.87ms
step:385/2330 train_time:23433ms step_avg:60.86ms
step:386/2330 train_time:23495ms step_avg:60.87ms
step:387/2330 train_time:23555ms step_avg:60.87ms
step:388/2330 train_time:23618ms step_avg:60.87ms
step:389/2330 train_time:23677ms step_avg:60.87ms
step:390/2330 train_time:23741ms step_avg:60.87ms
step:391/2330 train_time:23800ms step_avg:60.87ms
step:392/2330 train_time:23863ms step_avg:60.88ms
step:393/2330 train_time:23922ms step_avg:60.87ms
step:394/2330 train_time:23984ms step_avg:60.87ms
step:395/2330 train_time:24043ms step_avg:60.87ms
step:396/2330 train_time:24106ms step_avg:60.87ms
step:397/2330 train_time:24165ms step_avg:60.87ms
step:398/2330 train_time:24227ms step_avg:60.87ms
step:399/2330 train_time:24286ms step_avg:60.87ms
step:400/2330 train_time:24350ms step_avg:60.87ms
step:401/2330 train_time:24410ms step_avg:60.87ms
step:402/2330 train_time:24473ms step_avg:60.88ms
step:403/2330 train_time:24532ms step_avg:60.87ms
step:404/2330 train_time:24594ms step_avg:60.88ms
step:405/2330 train_time:24653ms step_avg:60.87ms
step:406/2330 train_time:24716ms step_avg:60.88ms
step:407/2330 train_time:24775ms step_avg:60.87ms
step:408/2330 train_time:24837ms step_avg:60.88ms
step:409/2330 train_time:24898ms step_avg:60.87ms
step:410/2330 train_time:24960ms step_avg:60.88ms
step:411/2330 train_time:25020ms step_avg:60.88ms
step:412/2330 train_time:25084ms step_avg:60.88ms
step:413/2330 train_time:25142ms step_avg:60.88ms
step:414/2330 train_time:25207ms step_avg:60.89ms
step:415/2330 train_time:25265ms step_avg:60.88ms
step:416/2330 train_time:25328ms step_avg:60.88ms
step:417/2330 train_time:25387ms step_avg:60.88ms
step:418/2330 train_time:25450ms step_avg:60.88ms
step:419/2330 train_time:25509ms step_avg:60.88ms
step:420/2330 train_time:25572ms step_avg:60.89ms
step:421/2330 train_time:25631ms step_avg:60.88ms
step:422/2330 train_time:25693ms step_avg:60.88ms
step:423/2330 train_time:25752ms step_avg:60.88ms
step:424/2330 train_time:25815ms step_avg:60.88ms
step:425/2330 train_time:25873ms step_avg:60.88ms
step:426/2330 train_time:25936ms step_avg:60.88ms
step:427/2330 train_time:25996ms step_avg:60.88ms
step:428/2330 train_time:26060ms step_avg:60.89ms
step:429/2330 train_time:26120ms step_avg:60.89ms
step:430/2330 train_time:26184ms step_avg:60.89ms
step:431/2330 train_time:26243ms step_avg:60.89ms
step:432/2330 train_time:26305ms step_avg:60.89ms
step:433/2330 train_time:26364ms step_avg:60.89ms
step:434/2330 train_time:26426ms step_avg:60.89ms
step:435/2330 train_time:26485ms step_avg:60.88ms
step:436/2330 train_time:26548ms step_avg:60.89ms
step:437/2330 train_time:26609ms step_avg:60.89ms
step:438/2330 train_time:26671ms step_avg:60.89ms
step:439/2330 train_time:26731ms step_avg:60.89ms
step:440/2330 train_time:26793ms step_avg:60.89ms
step:441/2330 train_time:26852ms step_avg:60.89ms
step:442/2330 train_time:26915ms step_avg:60.89ms
step:443/2330 train_time:26974ms step_avg:60.89ms
step:444/2330 train_time:27036ms step_avg:60.89ms
step:445/2330 train_time:27097ms step_avg:60.89ms
step:446/2330 train_time:27159ms step_avg:60.90ms
step:447/2330 train_time:27220ms step_avg:60.89ms
step:448/2330 train_time:27283ms step_avg:60.90ms
step:449/2330 train_time:27342ms step_avg:60.90ms
step:450/2330 train_time:27406ms step_avg:60.90ms
step:451/2330 train_time:27464ms step_avg:60.90ms
step:452/2330 train_time:27527ms step_avg:60.90ms
step:453/2330 train_time:27587ms step_avg:60.90ms
step:454/2330 train_time:27648ms step_avg:60.90ms
step:455/2330 train_time:27708ms step_avg:60.90ms
step:456/2330 train_time:27770ms step_avg:60.90ms
step:457/2330 train_time:27830ms step_avg:60.90ms
step:458/2330 train_time:27893ms step_avg:60.90ms
step:459/2330 train_time:27953ms step_avg:60.90ms
step:460/2330 train_time:28015ms step_avg:60.90ms
step:461/2330 train_time:28073ms step_avg:60.90ms
step:462/2330 train_time:28136ms step_avg:60.90ms
step:463/2330 train_time:28196ms step_avg:60.90ms
step:464/2330 train_time:28259ms step_avg:60.90ms
step:465/2330 train_time:28319ms step_avg:60.90ms
step:466/2330 train_time:28383ms step_avg:60.91ms
step:467/2330 train_time:28442ms step_avg:60.90ms
step:468/2330 train_time:28505ms step_avg:60.91ms
step:469/2330 train_time:28564ms step_avg:60.90ms
step:470/2330 train_time:28627ms step_avg:60.91ms
step:471/2330 train_time:28686ms step_avg:60.90ms
step:472/2330 train_time:28748ms step_avg:60.91ms
step:473/2330 train_time:28809ms step_avg:60.91ms
step:474/2330 train_time:28871ms step_avg:60.91ms
step:475/2330 train_time:28930ms step_avg:60.91ms
step:476/2330 train_time:28993ms step_avg:60.91ms
step:477/2330 train_time:29052ms step_avg:60.91ms
step:478/2330 train_time:29115ms step_avg:60.91ms
step:479/2330 train_time:29174ms step_avg:60.91ms
step:480/2330 train_time:29237ms step_avg:60.91ms
step:481/2330 train_time:29297ms step_avg:60.91ms
step:482/2330 train_time:29359ms step_avg:60.91ms
step:483/2330 train_time:29419ms step_avg:60.91ms
step:484/2330 train_time:29481ms step_avg:60.91ms
step:485/2330 train_time:29541ms step_avg:60.91ms
step:486/2330 train_time:29604ms step_avg:60.91ms
step:487/2330 train_time:29663ms step_avg:60.91ms
step:488/2330 train_time:29726ms step_avg:60.91ms
step:489/2330 train_time:29785ms step_avg:60.91ms
step:490/2330 train_time:29848ms step_avg:60.91ms
step:491/2330 train_time:29908ms step_avg:60.91ms
step:492/2330 train_time:29969ms step_avg:60.91ms
step:493/2330 train_time:30030ms step_avg:60.91ms
step:494/2330 train_time:30093ms step_avg:60.92ms
step:495/2330 train_time:30152ms step_avg:60.91ms
step:496/2330 train_time:30215ms step_avg:60.92ms
step:497/2330 train_time:30274ms step_avg:60.91ms
step:498/2330 train_time:30336ms step_avg:60.92ms
step:499/2330 train_time:30396ms step_avg:60.91ms
step:500/2330 train_time:30458ms step_avg:60.92ms
step:500/2330 val_loss:4.8202 train_time:30531ms step_avg:61.06ms
step:501/2330 train_time:30553ms step_avg:60.98ms
step:502/2330 train_time:30583ms step_avg:60.92ms
step:503/2330 train_time:30644ms step_avg:60.92ms
step:504/2330 train_time:30711ms step_avg:60.93ms
step:505/2330 train_time:30772ms step_avg:60.93ms
step:506/2330 train_time:30835ms step_avg:60.94ms
step:507/2330 train_time:30894ms step_avg:60.93ms
step:508/2330 train_time:30956ms step_avg:60.94ms
step:509/2330 train_time:31015ms step_avg:60.93ms
step:510/2330 train_time:31077ms step_avg:60.94ms
step:511/2330 train_time:31137ms step_avg:60.93ms
step:512/2330 train_time:31198ms step_avg:60.93ms
step:513/2330 train_time:31257ms step_avg:60.93ms
step:514/2330 train_time:31319ms step_avg:60.93ms
step:515/2330 train_time:31378ms step_avg:60.93ms
step:516/2330 train_time:31441ms step_avg:60.93ms
step:517/2330 train_time:31499ms step_avg:60.93ms
step:518/2330 train_time:31563ms step_avg:60.93ms
step:519/2330 train_time:31623ms step_avg:60.93ms
step:520/2330 train_time:31685ms step_avg:60.93ms
step:521/2330 train_time:31746ms step_avg:60.93ms
step:522/2330 train_time:31809ms step_avg:60.94ms
step:523/2330 train_time:31868ms step_avg:60.93ms
step:524/2330 train_time:31932ms step_avg:60.94ms
step:525/2330 train_time:31990ms step_avg:60.93ms
step:526/2330 train_time:32053ms step_avg:60.94ms
step:527/2330 train_time:32111ms step_avg:60.93ms
step:528/2330 train_time:32173ms step_avg:60.93ms
step:529/2330 train_time:32232ms step_avg:60.93ms
step:530/2330 train_time:32294ms step_avg:60.93ms
step:531/2330 train_time:32354ms step_avg:60.93ms
step:532/2330 train_time:32416ms step_avg:60.93ms
step:533/2330 train_time:32476ms step_avg:60.93ms
step:534/2330 train_time:32541ms step_avg:60.94ms
step:535/2330 train_time:32600ms step_avg:60.94ms
step:536/2330 train_time:32664ms step_avg:60.94ms
step:537/2330 train_time:32724ms step_avg:60.94ms
step:538/2330 train_time:32786ms step_avg:60.94ms
step:539/2330 train_time:32846ms step_avg:60.94ms
step:540/2330 train_time:32907ms step_avg:60.94ms
step:541/2330 train_time:32966ms step_avg:60.94ms
step:542/2330 train_time:33029ms step_avg:60.94ms
step:543/2330 train_time:33088ms step_avg:60.94ms
step:544/2330 train_time:33152ms step_avg:60.94ms
step:545/2330 train_time:33211ms step_avg:60.94ms
step:546/2330 train_time:33273ms step_avg:60.94ms
step:547/2330 train_time:33332ms step_avg:60.94ms
step:548/2330 train_time:33394ms step_avg:60.94ms
step:549/2330 train_time:33454ms step_avg:60.94ms
step:550/2330 train_time:33516ms step_avg:60.94ms
step:551/2330 train_time:33577ms step_avg:60.94ms
step:552/2330 train_time:33642ms step_avg:60.95ms
step:553/2330 train_time:33702ms step_avg:60.94ms
step:554/2330 train_time:33764ms step_avg:60.95ms
step:555/2330 train_time:33825ms step_avg:60.95ms
step:556/2330 train_time:33886ms step_avg:60.95ms
step:557/2330 train_time:33946ms step_avg:60.94ms
step:558/2330 train_time:34008ms step_avg:60.95ms
step:559/2330 train_time:34067ms step_avg:60.94ms
step:560/2330 train_time:34129ms step_avg:60.94ms
step:561/2330 train_time:34188ms step_avg:60.94ms
step:562/2330 train_time:34251ms step_avg:60.94ms
step:563/2330 train_time:34310ms step_avg:60.94ms
step:564/2330 train_time:34372ms step_avg:60.94ms
step:565/2330 train_time:34431ms step_avg:60.94ms
step:566/2330 train_time:34494ms step_avg:60.94ms
step:567/2330 train_time:34553ms step_avg:60.94ms
step:568/2330 train_time:34616ms step_avg:60.94ms
step:569/2330 train_time:34677ms step_avg:60.94ms
step:570/2330 train_time:34742ms step_avg:60.95ms
step:571/2330 train_time:34801ms step_avg:60.95ms
step:572/2330 train_time:34863ms step_avg:60.95ms
step:573/2330 train_time:34922ms step_avg:60.95ms
step:574/2330 train_time:34984ms step_avg:60.95ms
step:575/2330 train_time:35044ms step_avg:60.95ms
step:576/2330 train_time:35106ms step_avg:60.95ms
step:577/2330 train_time:35166ms step_avg:60.95ms
step:578/2330 train_time:35228ms step_avg:60.95ms
step:579/2330 train_time:35288ms step_avg:60.95ms
step:580/2330 train_time:35351ms step_avg:60.95ms
step:581/2330 train_time:35410ms step_avg:60.95ms
step:582/2330 train_time:35473ms step_avg:60.95ms
step:583/2330 train_time:35532ms step_avg:60.95ms
step:584/2330 train_time:35595ms step_avg:60.95ms
step:585/2330 train_time:35655ms step_avg:60.95ms
step:586/2330 train_time:35718ms step_avg:60.95ms
step:587/2330 train_time:35779ms step_avg:60.95ms
step:588/2330 train_time:35842ms step_avg:60.96ms
step:589/2330 train_time:35900ms step_avg:60.95ms
step:590/2330 train_time:35962ms step_avg:60.95ms
step:591/2330 train_time:36022ms step_avg:60.95ms
step:592/2330 train_time:36084ms step_avg:60.95ms
step:593/2330 train_time:36143ms step_avg:60.95ms
step:594/2330 train_time:36205ms step_avg:60.95ms
step:595/2330 train_time:36265ms step_avg:60.95ms
step:596/2330 train_time:36327ms step_avg:60.95ms
step:597/2330 train_time:36387ms step_avg:60.95ms
step:598/2330 train_time:36450ms step_avg:60.95ms
step:599/2330 train_time:36510ms step_avg:60.95ms
step:600/2330 train_time:36572ms step_avg:60.95ms
step:601/2330 train_time:36631ms step_avg:60.95ms
step:602/2330 train_time:36694ms step_avg:60.95ms
step:603/2330 train_time:36753ms step_avg:60.95ms
step:604/2330 train_time:36816ms step_avg:60.95ms
step:605/2330 train_time:36877ms step_avg:60.95ms
step:606/2330 train_time:36940ms step_avg:60.96ms
step:607/2330 train_time:37000ms step_avg:60.96ms
step:608/2330 train_time:37062ms step_avg:60.96ms
step:609/2330 train_time:37121ms step_avg:60.95ms
step:610/2330 train_time:37183ms step_avg:60.96ms
step:611/2330 train_time:37243ms step_avg:60.95ms
step:612/2330 train_time:37304ms step_avg:60.95ms
step:613/2330 train_time:37364ms step_avg:60.95ms
step:614/2330 train_time:37427ms step_avg:60.96ms
step:615/2330 train_time:37486ms step_avg:60.95ms
step:616/2330 train_time:37549ms step_avg:60.96ms
step:617/2330 train_time:37609ms step_avg:60.95ms
step:618/2330 train_time:37671ms step_avg:60.96ms
step:619/2330 train_time:37730ms step_avg:60.95ms
step:620/2330 train_time:37793ms step_avg:60.96ms
step:621/2330 train_time:37852ms step_avg:60.95ms
step:622/2330 train_time:37915ms step_avg:60.96ms
step:623/2330 train_time:37975ms step_avg:60.95ms
step:624/2330 train_time:38038ms step_avg:60.96ms
step:625/2330 train_time:38098ms step_avg:60.96ms
step:626/2330 train_time:38161ms step_avg:60.96ms
step:627/2330 train_time:38220ms step_avg:60.96ms
step:628/2330 train_time:38283ms step_avg:60.96ms
step:629/2330 train_time:38342ms step_avg:60.96ms
step:630/2330 train_time:38404ms step_avg:60.96ms
step:631/2330 train_time:38464ms step_avg:60.96ms
step:632/2330 train_time:38526ms step_avg:60.96ms
step:633/2330 train_time:38585ms step_avg:60.96ms
step:634/2330 train_time:38648ms step_avg:60.96ms
step:635/2330 train_time:38708ms step_avg:60.96ms
step:636/2330 train_time:38771ms step_avg:60.96ms
step:637/2330 train_time:38830ms step_avg:60.96ms
step:638/2330 train_time:38893ms step_avg:60.96ms
step:639/2330 train_time:38951ms step_avg:60.96ms
step:640/2330 train_time:39014ms step_avg:60.96ms
step:641/2330 train_time:39073ms step_avg:60.96ms
step:642/2330 train_time:39137ms step_avg:60.96ms
step:643/2330 train_time:39197ms step_avg:60.96ms
step:644/2330 train_time:39261ms step_avg:60.96ms
step:645/2330 train_time:39321ms step_avg:60.96ms
step:646/2330 train_time:39384ms step_avg:60.97ms
step:647/2330 train_time:39443ms step_avg:60.96ms
step:648/2330 train_time:39505ms step_avg:60.96ms
step:649/2330 train_time:39565ms step_avg:60.96ms
step:650/2330 train_time:39627ms step_avg:60.96ms
step:651/2330 train_time:39686ms step_avg:60.96ms
step:652/2330 train_time:39750ms step_avg:60.97ms
step:653/2330 train_time:39810ms step_avg:60.96ms
step:654/2330 train_time:39872ms step_avg:60.97ms
step:655/2330 train_time:39930ms step_avg:60.96ms
step:656/2330 train_time:39992ms step_avg:60.96ms
step:657/2330 train_time:40051ms step_avg:60.96ms
step:658/2330 train_time:40114ms step_avg:60.96ms
step:659/2330 train_time:40174ms step_avg:60.96ms
step:660/2330 train_time:40238ms step_avg:60.97ms
step:661/2330 train_time:40298ms step_avg:60.97ms
step:662/2330 train_time:40362ms step_avg:60.97ms
step:663/2330 train_time:40421ms step_avg:60.97ms
step:664/2330 train_time:40484ms step_avg:60.97ms
step:665/2330 train_time:40543ms step_avg:60.97ms
step:666/2330 train_time:40605ms step_avg:60.97ms
step:667/2330 train_time:40665ms step_avg:60.97ms
step:668/2330 train_time:40727ms step_avg:60.97ms
step:669/2330 train_time:40786ms step_avg:60.97ms
step:670/2330 train_time:40850ms step_avg:60.97ms
step:671/2330 train_time:40909ms step_avg:60.97ms
step:672/2330 train_time:40973ms step_avg:60.97ms
step:673/2330 train_time:41032ms step_avg:60.97ms
step:674/2330 train_time:41095ms step_avg:60.97ms
step:675/2330 train_time:41154ms step_avg:60.97ms
step:676/2330 train_time:41217ms step_avg:60.97ms
step:677/2330 train_time:41276ms step_avg:60.97ms
step:678/2330 train_time:41340ms step_avg:60.97ms
step:679/2330 train_time:41400ms step_avg:60.97ms
step:680/2330 train_time:41463ms step_avg:60.97ms
step:681/2330 train_time:41522ms step_avg:60.97ms
step:682/2330 train_time:41585ms step_avg:60.97ms
step:683/2330 train_time:41645ms step_avg:60.97ms
step:684/2330 train_time:41706ms step_avg:60.97ms
step:685/2330 train_time:41766ms step_avg:60.97ms
step:686/2330 train_time:41828ms step_avg:60.97ms
step:687/2330 train_time:41888ms step_avg:60.97ms
step:688/2330 train_time:41951ms step_avg:60.97ms
step:689/2330 train_time:42009ms step_avg:60.97ms
step:690/2330 train_time:42071ms step_avg:60.97ms
step:691/2330 train_time:42130ms step_avg:60.97ms
step:692/2330 train_time:42194ms step_avg:60.97ms
step:693/2330 train_time:42253ms step_avg:60.97ms
step:694/2330 train_time:42316ms step_avg:60.97ms
step:695/2330 train_time:42377ms step_avg:60.97ms
step:696/2330 train_time:42441ms step_avg:60.98ms
step:697/2330 train_time:42501ms step_avg:60.98ms
step:698/2330 train_time:42564ms step_avg:60.98ms
step:699/2330 train_time:42624ms step_avg:60.98ms
step:700/2330 train_time:42686ms step_avg:60.98ms
step:701/2330 train_time:42746ms step_avg:60.98ms
step:702/2330 train_time:42808ms step_avg:60.98ms
step:703/2330 train_time:42867ms step_avg:60.98ms
step:704/2330 train_time:42930ms step_avg:60.98ms
step:705/2330 train_time:42990ms step_avg:60.98ms
step:706/2330 train_time:43053ms step_avg:60.98ms
step:707/2330 train_time:43113ms step_avg:60.98ms
step:708/2330 train_time:43176ms step_avg:60.98ms
step:709/2330 train_time:43235ms step_avg:60.98ms
step:710/2330 train_time:43297ms step_avg:60.98ms
step:711/2330 train_time:43357ms step_avg:60.98ms
step:712/2330 train_time:43420ms step_avg:60.98ms
step:713/2330 train_time:43479ms step_avg:60.98ms
step:714/2330 train_time:43542ms step_avg:60.98ms
step:715/2330 train_time:43601ms step_avg:60.98ms
step:716/2330 train_time:43663ms step_avg:60.98ms
step:717/2330 train_time:43724ms step_avg:60.98ms
step:718/2330 train_time:43785ms step_avg:60.98ms
step:719/2330 train_time:43846ms step_avg:60.98ms
step:720/2330 train_time:43907ms step_avg:60.98ms
step:721/2330 train_time:43967ms step_avg:60.98ms
step:722/2330 train_time:44030ms step_avg:60.98ms
step:723/2330 train_time:44089ms step_avg:60.98ms
step:724/2330 train_time:44152ms step_avg:60.98ms
step:725/2330 train_time:44211ms step_avg:60.98ms
step:726/2330 train_time:44273ms step_avg:60.98ms
step:727/2330 train_time:44333ms step_avg:60.98ms
step:728/2330 train_time:44396ms step_avg:60.98ms
step:729/2330 train_time:44456ms step_avg:60.98ms
step:730/2330 train_time:44520ms step_avg:60.99ms
step:731/2330 train_time:44581ms step_avg:60.99ms
step:732/2330 train_time:44643ms step_avg:60.99ms
step:733/2330 train_time:44702ms step_avg:60.99ms
step:734/2330 train_time:44764ms step_avg:60.99ms
step:735/2330 train_time:44824ms step_avg:60.98ms
step:736/2330 train_time:44885ms step_avg:60.99ms
step:737/2330 train_time:44945ms step_avg:60.98ms
step:738/2330 train_time:45007ms step_avg:60.99ms
step:739/2330 train_time:45068ms step_avg:60.99ms
step:740/2330 train_time:45131ms step_avg:60.99ms
step:741/2330 train_time:45191ms step_avg:60.99ms
step:742/2330 train_time:45253ms step_avg:60.99ms
step:743/2330 train_time:45313ms step_avg:60.99ms
step:744/2330 train_time:45375ms step_avg:60.99ms
step:745/2330 train_time:45435ms step_avg:60.99ms
step:746/2330 train_time:45499ms step_avg:60.99ms
step:747/2330 train_time:45558ms step_avg:60.99ms
step:748/2330 train_time:45620ms step_avg:60.99ms
step:749/2330 train_time:45680ms step_avg:60.99ms
step:750/2330 train_time:45743ms step_avg:60.99ms
step:750/2330 val_loss:4.5249 train_time:45815ms step_avg:61.09ms
step:751/2330 train_time:45834ms step_avg:61.03ms
step:752/2330 train_time:45866ms step_avg:60.99ms
step:753/2330 train_time:45927ms step_avg:60.99ms
step:754/2330 train_time:45996ms step_avg:61.00ms
step:755/2330 train_time:46057ms step_avg:61.00ms
step:756/2330 train_time:46120ms step_avg:61.00ms
step:757/2330 train_time:46179ms step_avg:61.00ms
step:758/2330 train_time:46239ms step_avg:61.00ms
step:759/2330 train_time:46299ms step_avg:61.00ms
step:760/2330 train_time:46360ms step_avg:61.00ms
step:761/2330 train_time:46419ms step_avg:61.00ms
step:762/2330 train_time:46482ms step_avg:61.00ms
step:763/2330 train_time:46540ms step_avg:61.00ms
step:764/2330 train_time:46602ms step_avg:61.00ms
step:765/2330 train_time:46662ms step_avg:61.00ms
step:766/2330 train_time:46723ms step_avg:61.00ms
step:767/2330 train_time:46782ms step_avg:60.99ms
step:768/2330 train_time:46847ms step_avg:61.00ms
step:769/2330 train_time:46907ms step_avg:61.00ms
step:770/2330 train_time:46972ms step_avg:61.00ms
step:771/2330 train_time:47033ms step_avg:61.00ms
step:772/2330 train_time:47097ms step_avg:61.01ms
step:773/2330 train_time:47157ms step_avg:61.01ms
step:774/2330 train_time:47219ms step_avg:61.01ms
step:775/2330 train_time:47280ms step_avg:61.01ms
step:776/2330 train_time:47341ms step_avg:61.01ms
step:777/2330 train_time:47401ms step_avg:61.01ms
step:778/2330 train_time:47464ms step_avg:61.01ms
step:779/2330 train_time:47523ms step_avg:61.01ms
step:780/2330 train_time:47586ms step_avg:61.01ms
step:781/2330 train_time:47645ms step_avg:61.01ms
step:782/2330 train_time:47708ms step_avg:61.01ms
step:783/2330 train_time:47768ms step_avg:61.01ms
step:784/2330 train_time:47831ms step_avg:61.01ms
step:785/2330 train_time:47892ms step_avg:61.01ms
step:786/2330 train_time:47956ms step_avg:61.01ms
step:787/2330 train_time:48016ms step_avg:61.01ms
step:788/2330 train_time:48079ms step_avg:61.01ms
step:789/2330 train_time:48138ms step_avg:61.01ms
step:790/2330 train_time:48202ms step_avg:61.01ms
step:791/2330 train_time:48261ms step_avg:61.01ms
step:792/2330 train_time:48324ms step_avg:61.02ms
step:793/2330 train_time:48385ms step_avg:61.01ms
step:794/2330 train_time:48448ms step_avg:61.02ms
step:795/2330 train_time:48507ms step_avg:61.02ms
step:796/2330 train_time:48570ms step_avg:61.02ms
step:797/2330 train_time:48630ms step_avg:61.02ms
step:798/2330 train_time:48694ms step_avg:61.02ms
step:799/2330 train_time:48753ms step_avg:61.02ms
step:800/2330 train_time:48816ms step_avg:61.02ms
step:801/2330 train_time:48876ms step_avg:61.02ms
step:802/2330 train_time:48939ms step_avg:61.02ms
step:803/2330 train_time:48999ms step_avg:61.02ms
step:804/2330 train_time:49063ms step_avg:61.02ms
step:805/2330 train_time:49124ms step_avg:61.02ms
step:806/2330 train_time:49187ms step_avg:61.03ms
step:807/2330 train_time:49246ms step_avg:61.02ms
step:808/2330 train_time:49310ms step_avg:61.03ms
step:809/2330 train_time:49371ms step_avg:61.03ms
step:810/2330 train_time:49434ms step_avg:61.03ms
step:811/2330 train_time:49494ms step_avg:61.03ms
step:812/2330 train_time:49557ms step_avg:61.03ms
step:813/2330 train_time:49616ms step_avg:61.03ms
step:814/2330 train_time:49679ms step_avg:61.03ms
step:815/2330 train_time:49738ms step_avg:61.03ms
step:816/2330 train_time:49801ms step_avg:61.03ms
step:817/2330 train_time:49861ms step_avg:61.03ms
step:818/2330 train_time:49923ms step_avg:61.03ms
step:819/2330 train_time:49984ms step_avg:61.03ms
step:820/2330 train_time:50047ms step_avg:61.03ms
step:821/2330 train_time:50107ms step_avg:61.03ms
step:822/2330 train_time:50170ms step_avg:61.03ms
step:823/2330 train_time:50230ms step_avg:61.03ms
step:824/2330 train_time:50294ms step_avg:61.04ms
step:825/2330 train_time:50354ms step_avg:61.04ms
step:826/2330 train_time:50418ms step_avg:61.04ms
step:827/2330 train_time:50478ms step_avg:61.04ms
step:828/2330 train_time:50541ms step_avg:61.04ms
step:829/2330 train_time:50600ms step_avg:61.04ms
step:830/2330 train_time:50662ms step_avg:61.04ms
step:831/2330 train_time:50722ms step_avg:61.04ms
step:832/2330 train_time:50785ms step_avg:61.04ms
step:833/2330 train_time:50845ms step_avg:61.04ms
step:834/2330 train_time:50908ms step_avg:61.04ms
step:835/2330 train_time:50968ms step_avg:61.04ms
step:836/2330 train_time:51031ms step_avg:61.04ms
step:837/2330 train_time:51091ms step_avg:61.04ms
step:838/2330 train_time:51155ms step_avg:61.04ms
step:839/2330 train_time:51215ms step_avg:61.04ms
step:840/2330 train_time:51278ms step_avg:61.05ms
step:841/2330 train_time:51337ms step_avg:61.04ms
step:842/2330 train_time:51400ms step_avg:61.05ms
step:843/2330 train_time:51460ms step_avg:61.04ms
step:844/2330 train_time:51523ms step_avg:61.05ms
step:845/2330 train_time:51583ms step_avg:61.04ms
step:846/2330 train_time:51645ms step_avg:61.05ms
step:847/2330 train_time:51706ms step_avg:61.05ms
step:848/2330 train_time:51769ms step_avg:61.05ms
step:849/2330 train_time:51829ms step_avg:61.05ms
step:850/2330 train_time:51892ms step_avg:61.05ms
step:851/2330 train_time:51951ms step_avg:61.05ms
step:852/2330 train_time:52014ms step_avg:61.05ms
step:853/2330 train_time:52074ms step_avg:61.05ms
step:854/2330 train_time:52136ms step_avg:61.05ms
step:855/2330 train_time:52196ms step_avg:61.05ms
step:856/2330 train_time:52259ms step_avg:61.05ms
step:857/2330 train_time:52319ms step_avg:61.05ms
step:858/2330 train_time:52383ms step_avg:61.05ms
step:859/2330 train_time:52443ms step_avg:61.05ms
step:860/2330 train_time:52505ms step_avg:61.05ms
step:861/2330 train_time:52565ms step_avg:61.05ms
step:862/2330 train_time:52627ms step_avg:61.05ms
step:863/2330 train_time:52687ms step_avg:61.05ms
step:864/2330 train_time:52750ms step_avg:61.05ms
step:865/2330 train_time:52810ms step_avg:61.05ms
step:866/2330 train_time:52874ms step_avg:61.06ms
step:867/2330 train_time:52933ms step_avg:61.05ms
step:868/2330 train_time:52996ms step_avg:61.06ms
step:869/2330 train_time:53057ms step_avg:61.06ms
step:870/2330 train_time:53120ms step_avg:61.06ms
step:871/2330 train_time:53179ms step_avg:61.06ms
step:872/2330 train_time:53241ms step_avg:61.06ms
step:873/2330 train_time:53302ms step_avg:61.06ms
step:874/2330 train_time:53366ms step_avg:61.06ms
step:875/2330 train_time:53425ms step_avg:61.06ms
step:876/2330 train_time:53489ms step_avg:61.06ms
step:877/2330 train_time:53549ms step_avg:61.06ms
step:878/2330 train_time:53612ms step_avg:61.06ms
step:879/2330 train_time:53671ms step_avg:61.06ms
step:880/2330 train_time:53734ms step_avg:61.06ms
step:881/2330 train_time:53794ms step_avg:61.06ms
step:882/2330 train_time:53857ms step_avg:61.06ms
step:883/2330 train_time:53917ms step_avg:61.06ms
step:884/2330 train_time:53979ms step_avg:61.06ms
step:885/2330 train_time:54039ms step_avg:61.06ms
step:886/2330 train_time:54102ms step_avg:61.06ms
step:887/2330 train_time:54161ms step_avg:61.06ms
step:888/2330 train_time:54225ms step_avg:61.06ms
step:889/2330 train_time:54285ms step_avg:61.06ms
step:890/2330 train_time:54348ms step_avg:61.06ms
step:891/2330 train_time:54408ms step_avg:61.06ms
step:892/2330 train_time:54470ms step_avg:61.07ms
step:893/2330 train_time:54531ms step_avg:61.06ms
step:894/2330 train_time:54594ms step_avg:61.07ms
step:895/2330 train_time:54654ms step_avg:61.07ms
step:896/2330 train_time:54717ms step_avg:61.07ms
step:897/2330 train_time:54777ms step_avg:61.07ms
step:898/2330 train_time:54840ms step_avg:61.07ms
step:899/2330 train_time:54900ms step_avg:61.07ms
step:900/2330 train_time:54963ms step_avg:61.07ms
step:901/2330 train_time:55022ms step_avg:61.07ms
step:902/2330 train_time:55086ms step_avg:61.07ms
step:903/2330 train_time:55145ms step_avg:61.07ms
step:904/2330 train_time:55208ms step_avg:61.07ms
step:905/2330 train_time:55268ms step_avg:61.07ms
step:906/2330 train_time:55332ms step_avg:61.07ms
step:907/2330 train_time:55392ms step_avg:61.07ms
step:908/2330 train_time:55455ms step_avg:61.07ms
step:909/2330 train_time:55515ms step_avg:61.07ms
step:910/2330 train_time:55578ms step_avg:61.07ms
step:911/2330 train_time:55637ms step_avg:61.07ms
step:912/2330 train_time:55700ms step_avg:61.07ms
step:913/2330 train_time:55759ms step_avg:61.07ms
step:914/2330 train_time:55822ms step_avg:61.07ms
step:915/2330 train_time:55882ms step_avg:61.07ms
step:916/2330 train_time:55944ms step_avg:61.07ms
step:917/2330 train_time:56005ms step_avg:61.07ms
step:918/2330 train_time:56067ms step_avg:61.08ms
step:919/2330 train_time:56127ms step_avg:61.07ms
step:920/2330 train_time:56190ms step_avg:61.08ms
step:921/2330 train_time:56250ms step_avg:61.07ms
step:922/2330 train_time:56313ms step_avg:61.08ms
step:923/2330 train_time:56374ms step_avg:61.08ms
step:924/2330 train_time:56437ms step_avg:61.08ms
step:925/2330 train_time:56497ms step_avg:61.08ms
step:926/2330 train_time:56560ms step_avg:61.08ms
step:927/2330 train_time:56619ms step_avg:61.08ms
step:928/2330 train_time:56682ms step_avg:61.08ms
step:929/2330 train_time:56742ms step_avg:61.08ms
step:930/2330 train_time:56804ms step_avg:61.08ms
step:931/2330 train_time:56865ms step_avg:61.08ms
step:932/2330 train_time:56928ms step_avg:61.08ms
step:933/2330 train_time:56987ms step_avg:61.08ms
step:934/2330 train_time:57050ms step_avg:61.08ms
step:935/2330 train_time:57111ms step_avg:61.08ms
step:936/2330 train_time:57175ms step_avg:61.08ms
step:937/2330 train_time:57234ms step_avg:61.08ms
step:938/2330 train_time:57297ms step_avg:61.08ms
step:939/2330 train_time:57357ms step_avg:61.08ms
step:940/2330 train_time:57419ms step_avg:61.08ms
step:941/2330 train_time:57479ms step_avg:61.08ms
step:942/2330 train_time:57541ms step_avg:61.08ms
step:943/2330 train_time:57601ms step_avg:61.08ms
step:944/2330 train_time:57665ms step_avg:61.09ms
step:945/2330 train_time:57724ms step_avg:61.08ms
step:946/2330 train_time:57788ms step_avg:61.09ms
step:947/2330 train_time:57848ms step_avg:61.09ms
step:948/2330 train_time:57910ms step_avg:61.09ms
step:949/2330 train_time:57971ms step_avg:61.09ms
step:950/2330 train_time:58033ms step_avg:61.09ms
step:951/2330 train_time:58093ms step_avg:61.09ms
step:952/2330 train_time:58156ms step_avg:61.09ms
step:953/2330 train_time:58216ms step_avg:61.09ms
step:954/2330 train_time:58279ms step_avg:61.09ms
step:955/2330 train_time:58338ms step_avg:61.09ms
step:956/2330 train_time:58401ms step_avg:61.09ms
step:957/2330 train_time:58461ms step_avg:61.09ms
step:958/2330 train_time:58525ms step_avg:61.09ms
step:959/2330 train_time:58585ms step_avg:61.09ms
step:960/2330 train_time:58648ms step_avg:61.09ms
step:961/2330 train_time:58707ms step_avg:61.09ms
step:962/2330 train_time:58770ms step_avg:61.09ms
step:963/2330 train_time:58831ms step_avg:61.09ms
step:964/2330 train_time:58894ms step_avg:61.09ms
step:965/2330 train_time:58953ms step_avg:61.09ms
step:966/2330 train_time:59016ms step_avg:61.09ms
step:967/2330 train_time:59076ms step_avg:61.09ms
step:968/2330 train_time:59138ms step_avg:61.09ms
step:969/2330 train_time:59198ms step_avg:61.09ms
step:970/2330 train_time:59261ms step_avg:61.09ms
step:971/2330 train_time:59321ms step_avg:61.09ms
step:972/2330 train_time:59384ms step_avg:61.09ms
step:973/2330 train_time:59444ms step_avg:61.09ms
step:974/2330 train_time:59508ms step_avg:61.10ms
step:975/2330 train_time:59568ms step_avg:61.10ms
step:976/2330 train_time:59631ms step_avg:61.10ms
step:977/2330 train_time:59691ms step_avg:61.10ms
step:978/2330 train_time:59754ms step_avg:61.10ms
step:979/2330 train_time:59814ms step_avg:61.10ms
step:980/2330 train_time:59878ms step_avg:61.10ms
step:981/2330 train_time:59937ms step_avg:61.10ms
step:982/2330 train_time:60000ms step_avg:61.10ms
step:983/2330 train_time:60059ms step_avg:61.10ms
step:984/2330 train_time:60123ms step_avg:61.10ms
step:985/2330 train_time:60184ms step_avg:61.10ms
step:986/2330 train_time:60245ms step_avg:61.10ms
step:987/2330 train_time:60306ms step_avg:61.10ms
step:988/2330 train_time:60369ms step_avg:61.10ms
step:989/2330 train_time:60430ms step_avg:61.10ms
step:990/2330 train_time:60493ms step_avg:61.10ms
step:991/2330 train_time:60553ms step_avg:61.10ms
step:992/2330 train_time:60616ms step_avg:61.10ms
step:993/2330 train_time:60677ms step_avg:61.10ms
step:994/2330 train_time:60739ms step_avg:61.11ms
step:995/2330 train_time:60800ms step_avg:61.11ms
step:996/2330 train_time:60863ms step_avg:61.11ms
step:997/2330 train_time:60922ms step_avg:61.11ms
step:998/2330 train_time:60986ms step_avg:61.11ms
step:999/2330 train_time:61046ms step_avg:61.11ms
step:1000/2330 train_time:61110ms step_avg:61.11ms
step:1000/2330 val_loss:4.2426 train_time:61183ms step_avg:61.18ms
step:1001/2330 train_time:61203ms step_avg:61.14ms
step:1002/2330 train_time:61233ms step_avg:61.11ms
step:1003/2330 train_time:61295ms step_avg:61.11ms
step:1004/2330 train_time:61366ms step_avg:61.12ms
step:1005/2330 train_time:61427ms step_avg:61.12ms
step:1006/2330 train_time:61491ms step_avg:61.12ms
step:1007/2330 train_time:61550ms step_avg:61.12ms
step:1008/2330 train_time:61613ms step_avg:61.12ms
step:1009/2330 train_time:61674ms step_avg:61.12ms
step:1010/2330 train_time:61737ms step_avg:61.13ms
step:1011/2330 train_time:61796ms step_avg:61.12ms
step:1012/2330 train_time:61859ms step_avg:61.13ms
step:1013/2330 train_time:61918ms step_avg:61.12ms
step:1014/2330 train_time:61980ms step_avg:61.12ms
step:1015/2330 train_time:62040ms step_avg:61.12ms
step:1016/2330 train_time:62103ms step_avg:61.13ms
step:1017/2330 train_time:62165ms step_avg:61.13ms
step:1018/2330 train_time:62228ms step_avg:61.13ms
step:1019/2330 train_time:62289ms step_avg:61.13ms
step:1020/2330 train_time:62354ms step_avg:61.13ms
step:1021/2330 train_time:62417ms step_avg:61.13ms
step:1022/2330 train_time:62480ms step_avg:61.13ms
step:1023/2330 train_time:62540ms step_avg:61.13ms
step:1024/2330 train_time:62602ms step_avg:61.13ms
step:1025/2330 train_time:62662ms step_avg:61.13ms
step:1026/2330 train_time:62724ms step_avg:61.13ms
step:1027/2330 train_time:62784ms step_avg:61.13ms
step:1028/2330 train_time:62847ms step_avg:61.13ms
step:1029/2330 train_time:62906ms step_avg:61.13ms
step:1030/2330 train_time:62970ms step_avg:61.14ms
step:1031/2330 train_time:63029ms step_avg:61.13ms
step:1032/2330 train_time:63092ms step_avg:61.14ms
step:1033/2330 train_time:63151ms step_avg:61.13ms
step:1034/2330 train_time:63215ms step_avg:61.14ms
step:1035/2330 train_time:63278ms step_avg:61.14ms
step:1036/2330 train_time:63342ms step_avg:61.14ms
step:1037/2330 train_time:63403ms step_avg:61.14ms
step:1038/2330 train_time:63467ms step_avg:61.14ms
step:1039/2330 train_time:63527ms step_avg:61.14ms
step:1040/2330 train_time:63590ms step_avg:61.14ms
step:1041/2330 train_time:63650ms step_avg:61.14ms
step:1042/2330 train_time:63713ms step_avg:61.15ms
step:1043/2330 train_time:63774ms step_avg:61.14ms
step:1044/2330 train_time:63838ms step_avg:61.15ms
step:1045/2330 train_time:63898ms step_avg:61.15ms
step:1046/2330 train_time:63962ms step_avg:61.15ms
step:1047/2330 train_time:64021ms step_avg:61.15ms
step:1048/2330 train_time:64084ms step_avg:61.15ms
step:1049/2330 train_time:64143ms step_avg:61.15ms
step:1050/2330 train_time:64206ms step_avg:61.15ms
step:1051/2330 train_time:64267ms step_avg:61.15ms
step:1052/2330 train_time:64330ms step_avg:61.15ms
step:1053/2330 train_time:64390ms step_avg:61.15ms
step:1054/2330 train_time:64454ms step_avg:61.15ms
step:1055/2330 train_time:64515ms step_avg:61.15ms
step:1056/2330 train_time:64579ms step_avg:61.15ms
step:1057/2330 train_time:64639ms step_avg:61.15ms
step:1058/2330 train_time:64702ms step_avg:61.15ms
step:1059/2330 train_time:64763ms step_avg:61.15ms
step:1060/2330 train_time:64825ms step_avg:61.16ms
step:1061/2330 train_time:64885ms step_avg:61.15ms
step:1062/2330 train_time:64948ms step_avg:61.16ms
step:1063/2330 train_time:65007ms step_avg:61.15ms
step:1064/2330 train_time:65071ms step_avg:61.16ms
step:1065/2330 train_time:65131ms step_avg:61.16ms
step:1066/2330 train_time:65194ms step_avg:61.16ms
step:1067/2330 train_time:65254ms step_avg:61.16ms
step:1068/2330 train_time:65318ms step_avg:61.16ms
step:1069/2330 train_time:65379ms step_avg:61.16ms
step:1070/2330 train_time:65443ms step_avg:61.16ms
step:1071/2330 train_time:65503ms step_avg:61.16ms
step:1072/2330 train_time:65566ms step_avg:61.16ms
step:1073/2330 train_time:65626ms step_avg:61.16ms
step:1074/2330 train_time:65689ms step_avg:61.16ms
step:1075/2330 train_time:65750ms step_avg:61.16ms
step:1076/2330 train_time:65813ms step_avg:61.16ms
step:1077/2330 train_time:65874ms step_avg:61.16ms
step:1078/2330 train_time:65938ms step_avg:61.17ms
step:1079/2330 train_time:65999ms step_avg:61.17ms
step:1080/2330 train_time:66063ms step_avg:61.17ms
step:1081/2330 train_time:66123ms step_avg:61.17ms
step:1082/2330 train_time:66185ms step_avg:61.17ms
step:1083/2330 train_time:66245ms step_avg:61.17ms
step:1084/2330 train_time:66308ms step_avg:61.17ms
step:1085/2330 train_time:66368ms step_avg:61.17ms
step:1086/2330 train_time:66431ms step_avg:61.17ms
step:1087/2330 train_time:66491ms step_avg:61.17ms
step:1088/2330 train_time:66555ms step_avg:61.17ms
step:1089/2330 train_time:66616ms step_avg:61.17ms
step:1090/2330 train_time:66679ms step_avg:61.17ms
step:1091/2330 train_time:66739ms step_avg:61.17ms
step:1092/2330 train_time:66802ms step_avg:61.17ms
step:1093/2330 train_time:66862ms step_avg:61.17ms
step:1094/2330 train_time:66924ms step_avg:61.17ms
step:1095/2330 train_time:66985ms step_avg:61.17ms
step:1096/2330 train_time:67048ms step_avg:61.18ms
step:1097/2330 train_time:67108ms step_avg:61.17ms
step:1098/2330 train_time:67171ms step_avg:61.18ms
step:1099/2330 train_time:67231ms step_avg:61.18ms
step:1100/2330 train_time:67295ms step_avg:61.18ms
step:1101/2330 train_time:67357ms step_avg:61.18ms
step:1102/2330 train_time:67421ms step_avg:61.18ms
step:1103/2330 train_time:67480ms step_avg:61.18ms
step:1104/2330 train_time:67543ms step_avg:61.18ms
step:1105/2330 train_time:67603ms step_avg:61.18ms
step:1106/2330 train_time:67666ms step_avg:61.18ms
step:1107/2330 train_time:67725ms step_avg:61.18ms
step:1108/2330 train_time:67788ms step_avg:61.18ms
step:1109/2330 train_time:67848ms step_avg:61.18ms
step:1110/2330 train_time:67912ms step_avg:61.18ms
step:1111/2330 train_time:67972ms step_avg:61.18ms
step:1112/2330 train_time:68035ms step_avg:61.18ms
step:1113/2330 train_time:68097ms step_avg:61.18ms
step:1114/2330 train_time:68161ms step_avg:61.19ms
step:1115/2330 train_time:68221ms step_avg:61.18ms
step:1116/2330 train_time:68284ms step_avg:61.19ms
step:1117/2330 train_time:68343ms step_avg:61.18ms
step:1118/2330 train_time:68406ms step_avg:61.19ms
step:1119/2330 train_time:68467ms step_avg:61.19ms
step:1120/2330 train_time:68530ms step_avg:61.19ms
step:1121/2330 train_time:68590ms step_avg:61.19ms
step:1122/2330 train_time:68654ms step_avg:61.19ms
step:1123/2330 train_time:68714ms step_avg:61.19ms
step:1124/2330 train_time:68778ms step_avg:61.19ms
step:1125/2330 train_time:68839ms step_avg:61.19ms
step:1126/2330 train_time:68903ms step_avg:61.19ms
step:1127/2330 train_time:68963ms step_avg:61.19ms
step:1128/2330 train_time:69025ms step_avg:61.19ms
step:1129/2330 train_time:69086ms step_avg:61.19ms
step:1130/2330 train_time:69148ms step_avg:61.19ms
step:1131/2330 train_time:69209ms step_avg:61.19ms
step:1132/2330 train_time:69272ms step_avg:61.19ms
step:1133/2330 train_time:69331ms step_avg:61.19ms
step:1134/2330 train_time:69395ms step_avg:61.19ms
step:1135/2330 train_time:69457ms step_avg:61.20ms
step:1136/2330 train_time:69520ms step_avg:61.20ms
step:1137/2330 train_time:69580ms step_avg:61.20ms
step:1138/2330 train_time:69643ms step_avg:61.20ms
step:1139/2330 train_time:69703ms step_avg:61.20ms
step:1140/2330 train_time:69767ms step_avg:61.20ms
step:1141/2330 train_time:69825ms step_avg:61.20ms
step:1142/2330 train_time:69889ms step_avg:61.20ms
step:1143/2330 train_time:69949ms step_avg:61.20ms
step:1144/2330 train_time:70012ms step_avg:61.20ms
step:1145/2330 train_time:70073ms step_avg:61.20ms
step:1146/2330 train_time:70137ms step_avg:61.20ms
step:1147/2330 train_time:70198ms step_avg:61.20ms
step:1148/2330 train_time:70264ms step_avg:61.21ms
step:1149/2330 train_time:70322ms step_avg:61.20ms
step:1150/2330 train_time:70385ms step_avg:61.20ms
step:1151/2330 train_time:70445ms step_avg:61.20ms
step:1152/2330 train_time:70508ms step_avg:61.21ms
step:1153/2330 train_time:70568ms step_avg:61.20ms
step:1154/2330 train_time:70631ms step_avg:61.21ms
step:1155/2330 train_time:70691ms step_avg:61.20ms
step:1156/2330 train_time:70756ms step_avg:61.21ms
step:1157/2330 train_time:70817ms step_avg:61.21ms
step:1158/2330 train_time:70879ms step_avg:61.21ms
step:1159/2330 train_time:70939ms step_avg:61.21ms
step:1160/2330 train_time:71002ms step_avg:61.21ms
step:1161/2330 train_time:71062ms step_avg:61.21ms
step:1162/2330 train_time:71125ms step_avg:61.21ms
step:1163/2330 train_time:71185ms step_avg:61.21ms
step:1164/2330 train_time:71248ms step_avg:61.21ms
step:1165/2330 train_time:71309ms step_avg:61.21ms
step:1166/2330 train_time:71371ms step_avg:61.21ms
step:1167/2330 train_time:71431ms step_avg:61.21ms
step:1168/2330 train_time:71495ms step_avg:61.21ms
step:1169/2330 train_time:71556ms step_avg:61.21ms
step:1170/2330 train_time:71619ms step_avg:61.21ms
step:1171/2330 train_time:71679ms step_avg:61.21ms
step:1172/2330 train_time:71742ms step_avg:61.21ms
step:1173/2330 train_time:71801ms step_avg:61.21ms
step:1174/2330 train_time:71865ms step_avg:61.21ms
step:1175/2330 train_time:71925ms step_avg:61.21ms
step:1176/2330 train_time:71988ms step_avg:61.21ms
step:1177/2330 train_time:72048ms step_avg:61.21ms
step:1178/2330 train_time:72110ms step_avg:61.21ms
step:1179/2330 train_time:72170ms step_avg:61.21ms
step:1180/2330 train_time:72233ms step_avg:61.21ms
step:1181/2330 train_time:72295ms step_avg:61.21ms
step:1182/2330 train_time:72358ms step_avg:61.22ms
step:1183/2330 train_time:72418ms step_avg:61.22ms
step:1184/2330 train_time:72481ms step_avg:61.22ms
step:1185/2330 train_time:72541ms step_avg:61.22ms
step:1186/2330 train_time:72603ms step_avg:61.22ms
step:1187/2330 train_time:72664ms step_avg:61.22ms
step:1188/2330 train_time:72727ms step_avg:61.22ms
step:1189/2330 train_time:72786ms step_avg:61.22ms
step:1190/2330 train_time:72849ms step_avg:61.22ms
step:1191/2330 train_time:72909ms step_avg:61.22ms
step:1192/2330 train_time:72973ms step_avg:61.22ms
step:1193/2330 train_time:73034ms step_avg:61.22ms
step:1194/2330 train_time:73098ms step_avg:61.22ms
step:1195/2330 train_time:73158ms step_avg:61.22ms
step:1196/2330 train_time:73221ms step_avg:61.22ms
step:1197/2330 train_time:73281ms step_avg:61.22ms
step:1198/2330 train_time:73344ms step_avg:61.22ms
step:1199/2330 train_time:73404ms step_avg:61.22ms
step:1200/2330 train_time:73468ms step_avg:61.22ms
step:1201/2330 train_time:73527ms step_avg:61.22ms
step:1202/2330 train_time:73590ms step_avg:61.22ms
step:1203/2330 train_time:73650ms step_avg:61.22ms
step:1204/2330 train_time:73713ms step_avg:61.22ms
step:1205/2330 train_time:73774ms step_avg:61.22ms
step:1206/2330 train_time:73839ms step_avg:61.23ms
step:1207/2330 train_time:73899ms step_avg:61.23ms
step:1208/2330 train_time:73963ms step_avg:61.23ms
step:1209/2330 train_time:74022ms step_avg:61.23ms
step:1210/2330 train_time:74085ms step_avg:61.23ms
step:1211/2330 train_time:74145ms step_avg:61.23ms
step:1212/2330 train_time:74207ms step_avg:61.23ms
step:1213/2330 train_time:74267ms step_avg:61.23ms
step:1214/2330 train_time:74331ms step_avg:61.23ms
step:1215/2330 train_time:74390ms step_avg:61.23ms
step:1216/2330 train_time:74453ms step_avg:61.23ms
step:1217/2330 train_time:74514ms step_avg:61.23ms
step:1218/2330 train_time:74577ms step_avg:61.23ms
step:1219/2330 train_time:74638ms step_avg:61.23ms
step:1220/2330 train_time:74701ms step_avg:61.23ms
step:1221/2330 train_time:74761ms step_avg:61.23ms
step:1222/2330 train_time:74824ms step_avg:61.23ms
step:1223/2330 train_time:74884ms step_avg:61.23ms
step:1224/2330 train_time:74947ms step_avg:61.23ms
step:1225/2330 train_time:75006ms step_avg:61.23ms
step:1226/2330 train_time:75070ms step_avg:61.23ms
step:1227/2330 train_time:75129ms step_avg:61.23ms
step:1228/2330 train_time:75193ms step_avg:61.23ms
step:1229/2330 train_time:75254ms step_avg:61.23ms
step:1230/2330 train_time:75317ms step_avg:61.23ms
step:1231/2330 train_time:75377ms step_avg:61.23ms
step:1232/2330 train_time:75441ms step_avg:61.23ms
step:1233/2330 train_time:75500ms step_avg:61.23ms
step:1234/2330 train_time:75563ms step_avg:61.23ms
step:1235/2330 train_time:75622ms step_avg:61.23ms
step:1236/2330 train_time:75685ms step_avg:61.23ms
step:1237/2330 train_time:75745ms step_avg:61.23ms
step:1238/2330 train_time:75809ms step_avg:61.23ms
step:1239/2330 train_time:75869ms step_avg:61.23ms
step:1240/2330 train_time:75932ms step_avg:61.24ms
step:1241/2330 train_time:75992ms step_avg:61.23ms
step:1242/2330 train_time:76056ms step_avg:61.24ms
step:1243/2330 train_time:76117ms step_avg:61.24ms
step:1244/2330 train_time:76180ms step_avg:61.24ms
step:1245/2330 train_time:76240ms step_avg:61.24ms
step:1246/2330 train_time:76304ms step_avg:61.24ms
step:1247/2330 train_time:76364ms step_avg:61.24ms
step:1248/2330 train_time:76426ms step_avg:61.24ms
step:1249/2330 train_time:76486ms step_avg:61.24ms
step:1250/2330 train_time:76550ms step_avg:61.24ms
step:1250/2330 val_loss:4.1272 train_time:76623ms step_avg:61.30ms
step:1251/2330 train_time:76644ms step_avg:61.27ms
step:1252/2330 train_time:76675ms step_avg:61.24ms
step:1253/2330 train_time:76739ms step_avg:61.24ms
step:1254/2330 train_time:76808ms step_avg:61.25ms
step:1255/2330 train_time:76868ms step_avg:61.25ms
step:1256/2330 train_time:76931ms step_avg:61.25ms
step:1257/2330 train_time:76991ms step_avg:61.25ms
step:1258/2330 train_time:77053ms step_avg:61.25ms
step:1259/2330 train_time:77112ms step_avg:61.25ms
step:1260/2330 train_time:77174ms step_avg:61.25ms
step:1261/2330 train_time:77233ms step_avg:61.25ms
step:1262/2330 train_time:77296ms step_avg:61.25ms
step:1263/2330 train_time:77355ms step_avg:61.25ms
step:1264/2330 train_time:77418ms step_avg:61.25ms
step:1265/2330 train_time:77478ms step_avg:61.25ms
step:1266/2330 train_time:77539ms step_avg:61.25ms
step:1267/2330 train_time:77600ms step_avg:61.25ms
step:1268/2330 train_time:77664ms step_avg:61.25ms
step:1269/2330 train_time:77726ms step_avg:61.25ms
step:1270/2330 train_time:77790ms step_avg:61.25ms
step:1271/2330 train_time:77851ms step_avg:61.25ms
step:1272/2330 train_time:77914ms step_avg:61.25ms
step:1273/2330 train_time:77974ms step_avg:61.25ms
step:1274/2330 train_time:78037ms step_avg:61.25ms
step:1275/2330 train_time:78097ms step_avg:61.25ms
step:1276/2330 train_time:78160ms step_avg:61.25ms
step:1277/2330 train_time:78219ms step_avg:61.25ms
step:1278/2330 train_time:78282ms step_avg:61.25ms
step:1279/2330 train_time:78340ms step_avg:61.25ms
step:1280/2330 train_time:78403ms step_avg:61.25ms
step:1281/2330 train_time:78462ms step_avg:61.25ms
step:1282/2330 train_time:78525ms step_avg:61.25ms
step:1283/2330 train_time:78585ms step_avg:61.25ms
step:1284/2330 train_time:78649ms step_avg:61.25ms
step:1285/2330 train_time:78709ms step_avg:61.25ms
step:1286/2330 train_time:78773ms step_avg:61.25ms
step:1287/2330 train_time:78833ms step_avg:61.25ms
step:1288/2330 train_time:78898ms step_avg:61.26ms
step:1289/2330 train_time:78958ms step_avg:61.25ms
step:1290/2330 train_time:79021ms step_avg:61.26ms
step:1291/2330 train_time:79081ms step_avg:61.26ms
step:1292/2330 train_time:79143ms step_avg:61.26ms
step:1293/2330 train_time:79202ms step_avg:61.25ms
step:1294/2330 train_time:79265ms step_avg:61.26ms
step:1295/2330 train_time:79325ms step_avg:61.25ms
step:1296/2330 train_time:79388ms step_avg:61.26ms
step:1297/2330 train_time:79447ms step_avg:61.25ms
step:1298/2330 train_time:79509ms step_avg:61.26ms
step:1299/2330 train_time:79569ms step_avg:61.25ms
step:1300/2330 train_time:79633ms step_avg:61.26ms
step:1301/2330 train_time:79694ms step_avg:61.26ms
step:1302/2330 train_time:79757ms step_avg:61.26ms
step:1303/2330 train_time:79818ms step_avg:61.26ms
step:1304/2330 train_time:79883ms step_avg:61.26ms
step:1305/2330 train_time:79942ms step_avg:61.26ms
step:1306/2330 train_time:80006ms step_avg:61.26ms
step:1307/2330 train_time:80065ms step_avg:61.26ms
step:1308/2330 train_time:80128ms step_avg:61.26ms
step:1309/2330 train_time:80188ms step_avg:61.26ms
step:1310/2330 train_time:80250ms step_avg:61.26ms
step:1311/2330 train_time:80309ms step_avg:61.26ms
step:1312/2330 train_time:80373ms step_avg:61.26ms
step:1313/2330 train_time:80432ms step_avg:61.26ms
step:1314/2330 train_time:80496ms step_avg:61.26ms
step:1315/2330 train_time:80556ms step_avg:61.26ms
step:1316/2330 train_time:80620ms step_avg:61.26ms
step:1317/2330 train_time:80680ms step_avg:61.26ms
step:1318/2330 train_time:80744ms step_avg:61.26ms
step:1319/2330 train_time:80804ms step_avg:61.26ms
step:1320/2330 train_time:80867ms step_avg:61.26ms
step:1321/2330 train_time:80927ms step_avg:61.26ms
step:1322/2330 train_time:80991ms step_avg:61.26ms
step:1323/2330 train_time:81051ms step_avg:61.26ms
step:1324/2330 train_time:81114ms step_avg:61.26ms
step:1325/2330 train_time:81174ms step_avg:61.26ms
step:1326/2330 train_time:81237ms step_avg:61.26ms
step:1327/2330 train_time:81297ms step_avg:61.26ms
step:1328/2330 train_time:81360ms step_avg:61.27ms
step:1329/2330 train_time:81420ms step_avg:61.26ms
step:1330/2330 train_time:81483ms step_avg:61.27ms
step:1331/2330 train_time:81543ms step_avg:61.26ms
step:1332/2330 train_time:81605ms step_avg:61.27ms
step:1333/2330 train_time:81665ms step_avg:61.26ms
step:1334/2330 train_time:81728ms step_avg:61.27ms
step:1335/2330 train_time:81789ms step_avg:61.27ms
step:1336/2330 train_time:81853ms step_avg:61.27ms
step:1337/2330 train_time:81912ms step_avg:61.27ms
step:1338/2330 train_time:81976ms step_avg:61.27ms
step:1339/2330 train_time:82035ms step_avg:61.27ms
step:1340/2330 train_time:82099ms step_avg:61.27ms
step:1341/2330 train_time:82159ms step_avg:61.27ms
step:1342/2330 train_time:82222ms step_avg:61.27ms
step:1343/2330 train_time:82282ms step_avg:61.27ms
step:1344/2330 train_time:82344ms step_avg:61.27ms
step:1345/2330 train_time:82404ms step_avg:61.27ms
step:1346/2330 train_time:82466ms step_avg:61.27ms
step:1347/2330 train_time:82526ms step_avg:61.27ms
step:1348/2330 train_time:82588ms step_avg:61.27ms
step:1349/2330 train_time:82649ms step_avg:61.27ms
step:1350/2330 train_time:82712ms step_avg:61.27ms
step:1351/2330 train_time:82771ms step_avg:61.27ms
step:1352/2330 train_time:82835ms step_avg:61.27ms
step:1353/2330 train_time:82895ms step_avg:61.27ms
step:1354/2330 train_time:82958ms step_avg:61.27ms
step:1355/2330 train_time:83018ms step_avg:61.27ms
step:1356/2330 train_time:83082ms step_avg:61.27ms
step:1357/2330 train_time:83142ms step_avg:61.27ms
step:1358/2330 train_time:83204ms step_avg:61.27ms
step:1359/2330 train_time:83264ms step_avg:61.27ms
step:1360/2330 train_time:83327ms step_avg:61.27ms
step:1361/2330 train_time:83386ms step_avg:61.27ms
step:1362/2330 train_time:83450ms step_avg:61.27ms
step:1363/2330 train_time:83509ms step_avg:61.27ms
step:1364/2330 train_time:83572ms step_avg:61.27ms
step:1365/2330 train_time:83631ms step_avg:61.27ms
step:1366/2330 train_time:83694ms step_avg:61.27ms
step:1367/2330 train_time:83754ms step_avg:61.27ms
step:1368/2330 train_time:83817ms step_avg:61.27ms
step:1369/2330 train_time:83877ms step_avg:61.27ms
step:1370/2330 train_time:83941ms step_avg:61.27ms
step:1371/2330 train_time:84001ms step_avg:61.27ms
step:1372/2330 train_time:84065ms step_avg:61.27ms
step:1373/2330 train_time:84124ms step_avg:61.27ms
step:1374/2330 train_time:84188ms step_avg:61.27ms
step:1375/2330 train_time:84247ms step_avg:61.27ms
step:1376/2330 train_time:84310ms step_avg:61.27ms
step:1377/2330 train_time:84370ms step_avg:61.27ms
step:1378/2330 train_time:84433ms step_avg:61.27ms
step:1379/2330 train_time:84492ms step_avg:61.27ms
step:1380/2330 train_time:84555ms step_avg:61.27ms
step:1381/2330 train_time:84615ms step_avg:61.27ms
step:1382/2330 train_time:84678ms step_avg:61.27ms
step:1383/2330 train_time:84739ms step_avg:61.27ms
step:1384/2330 train_time:84802ms step_avg:61.27ms
step:1385/2330 train_time:84862ms step_avg:61.27ms
step:1386/2330 train_time:84925ms step_avg:61.27ms
step:1387/2330 train_time:84985ms step_avg:61.27ms
step:1388/2330 train_time:85048ms step_avg:61.27ms
step:1389/2330 train_time:85108ms step_avg:61.27ms
step:1390/2330 train_time:85171ms step_avg:61.27ms
step:1391/2330 train_time:85232ms step_avg:61.27ms
step:1392/2330 train_time:85295ms step_avg:61.28ms
step:1393/2330 train_time:85355ms step_avg:61.27ms
step:1394/2330 train_time:85418ms step_avg:61.28ms
step:1395/2330 train_time:85478ms step_avg:61.27ms
step:1396/2330 train_time:85542ms step_avg:61.28ms
step:1397/2330 train_time:85601ms step_avg:61.28ms
step:1398/2330 train_time:85664ms step_avg:61.28ms
step:1399/2330 train_time:85725ms step_avg:61.28ms
step:1400/2330 train_time:85787ms step_avg:61.28ms
step:1401/2330 train_time:85846ms step_avg:61.28ms
step:1402/2330 train_time:85909ms step_avg:61.28ms
step:1403/2330 train_time:85971ms step_avg:61.28ms
step:1404/2330 train_time:86034ms step_avg:61.28ms
step:1405/2330 train_time:86094ms step_avg:61.28ms
step:1406/2330 train_time:86157ms step_avg:61.28ms
step:1407/2330 train_time:86217ms step_avg:61.28ms
step:1408/2330 train_time:86282ms step_avg:61.28ms
step:1409/2330 train_time:86341ms step_avg:61.28ms
step:1410/2330 train_time:86404ms step_avg:61.28ms
step:1411/2330 train_time:86464ms step_avg:61.28ms
step:1412/2330 train_time:86527ms step_avg:61.28ms
step:1413/2330 train_time:86586ms step_avg:61.28ms
step:1414/2330 train_time:86649ms step_avg:61.28ms
step:1415/2330 train_time:86708ms step_avg:61.28ms
step:1416/2330 train_time:86771ms step_avg:61.28ms
step:1417/2330 train_time:86831ms step_avg:61.28ms
step:1418/2330 train_time:86895ms step_avg:61.28ms
step:1419/2330 train_time:86954ms step_avg:61.28ms
step:1420/2330 train_time:87017ms step_avg:61.28ms
step:1421/2330 train_time:87078ms step_avg:61.28ms
step:1422/2330 train_time:87141ms step_avg:61.28ms
step:1423/2330 train_time:87201ms step_avg:61.28ms
step:1424/2330 train_time:87264ms step_avg:61.28ms
step:1425/2330 train_time:87323ms step_avg:61.28ms
step:1426/2330 train_time:87386ms step_avg:61.28ms
step:1427/2330 train_time:87446ms step_avg:61.28ms
step:1428/2330 train_time:87510ms step_avg:61.28ms
step:1429/2330 train_time:87570ms step_avg:61.28ms
step:1430/2330 train_time:87633ms step_avg:61.28ms
step:1431/2330 train_time:87693ms step_avg:61.28ms
step:1432/2330 train_time:87755ms step_avg:61.28ms
step:1433/2330 train_time:87816ms step_avg:61.28ms
step:1434/2330 train_time:87879ms step_avg:61.28ms
step:1435/2330 train_time:87939ms step_avg:61.28ms
step:1436/2330 train_time:88002ms step_avg:61.28ms
step:1437/2330 train_time:88062ms step_avg:61.28ms
step:1438/2330 train_time:88125ms step_avg:61.28ms
step:1439/2330 train_time:88185ms step_avg:61.28ms
step:1440/2330 train_time:88248ms step_avg:61.28ms
step:1441/2330 train_time:88308ms step_avg:61.28ms
step:1442/2330 train_time:88370ms step_avg:61.28ms
step:1443/2330 train_time:88431ms step_avg:61.28ms
step:1444/2330 train_time:88494ms step_avg:61.28ms
step:1445/2330 train_time:88554ms step_avg:61.28ms
step:1446/2330 train_time:88617ms step_avg:61.28ms
step:1447/2330 train_time:88679ms step_avg:61.28ms
step:1448/2330 train_time:88741ms step_avg:61.29ms
step:1449/2330 train_time:88801ms step_avg:61.28ms
step:1450/2330 train_time:88864ms step_avg:61.29ms
step:1451/2330 train_time:88924ms step_avg:61.28ms
step:1452/2330 train_time:88987ms step_avg:61.29ms
step:1453/2330 train_time:89047ms step_avg:61.29ms
step:1454/2330 train_time:89110ms step_avg:61.29ms
step:1455/2330 train_time:89170ms step_avg:61.29ms
step:1456/2330 train_time:89234ms step_avg:61.29ms
step:1457/2330 train_time:89293ms step_avg:61.29ms
step:1458/2330 train_time:89357ms step_avg:61.29ms
step:1459/2330 train_time:89417ms step_avg:61.29ms
step:1460/2330 train_time:89482ms step_avg:61.29ms
step:1461/2330 train_time:89540ms step_avg:61.29ms
step:1462/2330 train_time:89604ms step_avg:61.29ms
step:1463/2330 train_time:89664ms step_avg:61.29ms
step:1464/2330 train_time:89726ms step_avg:61.29ms
step:1465/2330 train_time:89787ms step_avg:61.29ms
step:1466/2330 train_time:89850ms step_avg:61.29ms
step:1467/2330 train_time:89910ms step_avg:61.29ms
step:1468/2330 train_time:89973ms step_avg:61.29ms
step:1469/2330 train_time:90032ms step_avg:61.29ms
step:1470/2330 train_time:90096ms step_avg:61.29ms
step:1471/2330 train_time:90157ms step_avg:61.29ms
step:1472/2330 train_time:90220ms step_avg:61.29ms
step:1473/2330 train_time:90281ms step_avg:61.29ms
step:1474/2330 train_time:90344ms step_avg:61.29ms
step:1475/2330 train_time:90404ms step_avg:61.29ms
step:1476/2330 train_time:90467ms step_avg:61.29ms
step:1477/2330 train_time:90526ms step_avg:61.29ms
step:1478/2330 train_time:90590ms step_avg:61.29ms
step:1479/2330 train_time:90649ms step_avg:61.29ms
step:1480/2330 train_time:90712ms step_avg:61.29ms
step:1481/2330 train_time:90772ms step_avg:61.29ms
step:1482/2330 train_time:90834ms step_avg:61.29ms
step:1483/2330 train_time:90894ms step_avg:61.29ms
step:1484/2330 train_time:90958ms step_avg:61.29ms
step:1485/2330 train_time:91018ms step_avg:61.29ms
step:1486/2330 train_time:91082ms step_avg:61.29ms
step:1487/2330 train_time:91141ms step_avg:61.29ms
step:1488/2330 train_time:91204ms step_avg:61.29ms
step:1489/2330 train_time:91264ms step_avg:61.29ms
step:1490/2330 train_time:91327ms step_avg:61.29ms
step:1491/2330 train_time:91387ms step_avg:61.29ms
step:1492/2330 train_time:91449ms step_avg:61.29ms
step:1493/2330 train_time:91510ms step_avg:61.29ms
step:1494/2330 train_time:91573ms step_avg:61.29ms
step:1495/2330 train_time:91633ms step_avg:61.29ms
step:1496/2330 train_time:91697ms step_avg:61.29ms
step:1497/2330 train_time:91756ms step_avg:61.29ms
step:1498/2330 train_time:91820ms step_avg:61.29ms
step:1499/2330 train_time:91880ms step_avg:61.29ms
step:1500/2330 train_time:91943ms step_avg:61.30ms
step:1500/2330 val_loss:4.0812 train_time:92015ms step_avg:61.34ms
step:1501/2330 train_time:92036ms step_avg:61.32ms
step:1502/2330 train_time:92068ms step_avg:61.30ms
step:1503/2330 train_time:92132ms step_avg:61.30ms
step:1504/2330 train_time:92198ms step_avg:61.30ms
step:1505/2330 train_time:92258ms step_avg:61.30ms
step:1506/2330 train_time:92321ms step_avg:61.30ms
step:1507/2330 train_time:92381ms step_avg:61.30ms
step:1508/2330 train_time:92444ms step_avg:61.30ms
step:1509/2330 train_time:92503ms step_avg:61.30ms
step:1510/2330 train_time:92566ms step_avg:61.30ms
step:1511/2330 train_time:92625ms step_avg:61.30ms
step:1512/2330 train_time:92688ms step_avg:61.30ms
step:1513/2330 train_time:92746ms step_avg:61.30ms
step:1514/2330 train_time:92809ms step_avg:61.30ms
step:1515/2330 train_time:92869ms step_avg:61.30ms
step:1516/2330 train_time:92933ms step_avg:61.30ms
step:1517/2330 train_time:92996ms step_avg:61.30ms
step:1518/2330 train_time:93060ms step_avg:61.30ms
step:1519/2330 train_time:93121ms step_avg:61.30ms
step:1520/2330 train_time:93186ms step_avg:61.31ms
step:1521/2330 train_time:93247ms step_avg:61.31ms
step:1522/2330 train_time:93311ms step_avg:61.31ms
step:1523/2330 train_time:93373ms step_avg:61.31ms
step:1524/2330 train_time:93437ms step_avg:61.31ms
step:1525/2330 train_time:93497ms step_avg:61.31ms
step:1526/2330 train_time:93559ms step_avg:61.31ms
step:1527/2330 train_time:93619ms step_avg:61.31ms
step:1528/2330 train_time:93681ms step_avg:61.31ms
step:1529/2330 train_time:93741ms step_avg:61.31ms
step:1530/2330 train_time:93804ms step_avg:61.31ms
step:1531/2330 train_time:93864ms step_avg:61.31ms
step:1532/2330 train_time:93929ms step_avg:61.31ms
step:1533/2330 train_time:93989ms step_avg:61.31ms
step:1534/2330 train_time:94054ms step_avg:61.31ms
step:1535/2330 train_time:94116ms step_avg:61.31ms
step:1536/2330 train_time:94180ms step_avg:61.31ms
step:1537/2330 train_time:94240ms step_avg:61.31ms
step:1538/2330 train_time:94306ms step_avg:61.32ms
step:1539/2330 train_time:94366ms step_avg:61.32ms
step:1540/2330 train_time:94430ms step_avg:61.32ms
step:1541/2330 train_time:94491ms step_avg:61.32ms
step:1542/2330 train_time:94556ms step_avg:61.32ms
step:1543/2330 train_time:94617ms step_avg:61.32ms
step:1544/2330 train_time:94679ms step_avg:61.32ms
step:1545/2330 train_time:94739ms step_avg:61.32ms
step:1546/2330 train_time:94802ms step_avg:61.32ms
step:1547/2330 train_time:94863ms step_avg:61.32ms
step:1548/2330 train_time:94926ms step_avg:61.32ms
step:1549/2330 train_time:94986ms step_avg:61.32ms
step:1550/2330 train_time:95050ms step_avg:61.32ms
step:1551/2330 train_time:95113ms step_avg:61.32ms
step:1552/2330 train_time:95178ms step_avg:61.33ms
step:1553/2330 train_time:95239ms step_avg:61.33ms
step:1554/2330 train_time:95302ms step_avg:61.33ms
step:1555/2330 train_time:95363ms step_avg:61.33ms
step:1556/2330 train_time:95427ms step_avg:61.33ms
step:1557/2330 train_time:95487ms step_avg:61.33ms
step:1558/2330 train_time:95551ms step_avg:61.33ms
step:1559/2330 train_time:95614ms step_avg:61.33ms
step:1560/2330 train_time:95678ms step_avg:61.33ms
step:1561/2330 train_time:95738ms step_avg:61.33ms
step:1562/2330 train_time:95801ms step_avg:61.33ms
step:1563/2330 train_time:95862ms step_avg:61.33ms
step:1564/2330 train_time:95925ms step_avg:61.33ms
step:1565/2330 train_time:95985ms step_avg:61.33ms
step:1566/2330 train_time:96049ms step_avg:61.33ms
step:1567/2330 train_time:96111ms step_avg:61.33ms
step:1568/2330 train_time:96176ms step_avg:61.34ms
step:1569/2330 train_time:96237ms step_avg:61.34ms
step:1570/2330 train_time:96301ms step_avg:61.34ms
step:1571/2330 train_time:96362ms step_avg:61.34ms
step:1572/2330 train_time:96425ms step_avg:61.34ms
step:1573/2330 train_time:96485ms step_avg:61.34ms
step:1574/2330 train_time:96549ms step_avg:61.34ms
step:1575/2330 train_time:96610ms step_avg:61.34ms
step:1576/2330 train_time:96675ms step_avg:61.34ms
step:1577/2330 train_time:96736ms step_avg:61.34ms
step:1578/2330 train_time:96801ms step_avg:61.34ms
step:1579/2330 train_time:96860ms step_avg:61.34ms
step:1580/2330 train_time:96925ms step_avg:61.34ms
step:1581/2330 train_time:96985ms step_avg:61.34ms
step:1582/2330 train_time:97048ms step_avg:61.34ms
step:1583/2330 train_time:97109ms step_avg:61.34ms
step:1584/2330 train_time:97174ms step_avg:61.35ms
step:1585/2330 train_time:97235ms step_avg:61.35ms
step:1586/2330 train_time:97299ms step_avg:61.35ms
step:1587/2330 train_time:97360ms step_avg:61.35ms
step:1588/2330 train_time:97423ms step_avg:61.35ms
step:1589/2330 train_time:97485ms step_avg:61.35ms
step:1590/2330 train_time:97549ms step_avg:61.35ms
step:1591/2330 train_time:97610ms step_avg:61.35ms
step:1592/2330 train_time:97675ms step_avg:61.35ms
step:1593/2330 train_time:97736ms step_avg:61.35ms
step:1594/2330 train_time:97800ms step_avg:61.36ms
step:1595/2330 train_time:97861ms step_avg:61.35ms
step:1596/2330 train_time:97925ms step_avg:61.36ms
step:1597/2330 train_time:97983ms step_avg:61.35ms
step:1598/2330 train_time:98047ms step_avg:61.36ms
step:1599/2330 train_time:98108ms step_avg:61.36ms
step:1600/2330 train_time:98173ms step_avg:61.36ms
step:1601/2330 train_time:98235ms step_avg:61.36ms
step:1602/2330 train_time:98299ms step_avg:61.36ms
step:1603/2330 train_time:98359ms step_avg:61.36ms
step:1604/2330 train_time:98423ms step_avg:61.36ms
step:1605/2330 train_time:98483ms step_avg:61.36ms
step:1606/2330 train_time:98547ms step_avg:61.36ms
step:1607/2330 train_time:98607ms step_avg:61.36ms
step:1608/2330 train_time:98671ms step_avg:61.36ms
step:1609/2330 train_time:98733ms step_avg:61.36ms
step:1610/2330 train_time:98798ms step_avg:61.37ms
step:1611/2330 train_time:98859ms step_avg:61.36ms
step:1612/2330 train_time:98923ms step_avg:61.37ms
step:1613/2330 train_time:98983ms step_avg:61.37ms
step:1614/2330 train_time:99046ms step_avg:61.37ms
step:1615/2330 train_time:99106ms step_avg:61.37ms
step:1616/2330 train_time:99170ms step_avg:61.37ms
step:1617/2330 train_time:99232ms step_avg:61.37ms
step:1618/2330 train_time:99297ms step_avg:61.37ms
step:1619/2330 train_time:99358ms step_avg:61.37ms
step:1620/2330 train_time:99422ms step_avg:61.37ms
step:1621/2330 train_time:99481ms step_avg:61.37ms
step:1622/2330 train_time:99545ms step_avg:61.37ms
step:1623/2330 train_time:99605ms step_avg:61.37ms
step:1624/2330 train_time:99670ms step_avg:61.37ms
step:1625/2330 train_time:99731ms step_avg:61.37ms
step:1626/2330 train_time:99796ms step_avg:61.38ms
step:1627/2330 train_time:99857ms step_avg:61.38ms
step:1628/2330 train_time:99921ms step_avg:61.38ms
step:1629/2330 train_time:99981ms step_avg:61.38ms
step:1630/2330 train_time:100045ms step_avg:61.38ms
step:1631/2330 train_time:100105ms step_avg:61.38ms
step:1632/2330 train_time:100169ms step_avg:61.38ms
step:1633/2330 train_time:100230ms step_avg:61.38ms
step:1634/2330 train_time:100294ms step_avg:61.38ms
step:1635/2330 train_time:100356ms step_avg:61.38ms
step:1636/2330 train_time:100421ms step_avg:61.38ms
step:1637/2330 train_time:100480ms step_avg:61.38ms
step:1638/2330 train_time:100543ms step_avg:61.38ms
step:1639/2330 train_time:100604ms step_avg:61.38ms
step:1640/2330 train_time:100669ms step_avg:61.38ms
step:1641/2330 train_time:100729ms step_avg:61.38ms
step:1642/2330 train_time:100792ms step_avg:61.38ms
step:1643/2330 train_time:100855ms step_avg:61.38ms
step:1644/2330 train_time:100920ms step_avg:61.39ms
step:1645/2330 train_time:100980ms step_avg:61.39ms
step:1646/2330 train_time:101043ms step_avg:61.39ms
step:1647/2330 train_time:101104ms step_avg:61.39ms
step:1648/2330 train_time:101168ms step_avg:61.39ms
step:1649/2330 train_time:101229ms step_avg:61.39ms
step:1650/2330 train_time:101293ms step_avg:61.39ms
step:1651/2330 train_time:101355ms step_avg:61.39ms
step:1652/2330 train_time:101420ms step_avg:61.39ms
step:1653/2330 train_time:101479ms step_avg:61.39ms
step:1654/2330 train_time:101543ms step_avg:61.39ms
step:1655/2330 train_time:101604ms step_avg:61.39ms
step:1656/2330 train_time:101667ms step_avg:61.39ms
step:1657/2330 train_time:101728ms step_avg:61.39ms
step:1658/2330 train_time:101792ms step_avg:61.39ms
step:1659/2330 train_time:101854ms step_avg:61.39ms
step:1660/2330 train_time:101919ms step_avg:61.40ms
step:1661/2330 train_time:101979ms step_avg:61.40ms
step:1662/2330 train_time:102042ms step_avg:61.40ms
step:1663/2330 train_time:102103ms step_avg:61.40ms
step:1664/2330 train_time:102166ms step_avg:61.40ms
step:1665/2330 train_time:102227ms step_avg:61.40ms
step:1666/2330 train_time:102291ms step_avg:61.40ms
step:1667/2330 train_time:102354ms step_avg:61.40ms
step:1668/2330 train_time:102419ms step_avg:61.40ms
step:1669/2330 train_time:102478ms step_avg:61.40ms
step:1670/2330 train_time:102542ms step_avg:61.40ms
step:1671/2330 train_time:102602ms step_avg:61.40ms
step:1672/2330 train_time:102666ms step_avg:61.40ms
step:1673/2330 train_time:102726ms step_avg:61.40ms
step:1674/2330 train_time:102790ms step_avg:61.40ms
step:1675/2330 train_time:102853ms step_avg:61.40ms
step:1676/2330 train_time:102918ms step_avg:61.41ms
step:1677/2330 train_time:102978ms step_avg:61.41ms
step:1678/2330 train_time:103042ms step_avg:61.41ms
step:1679/2330 train_time:103102ms step_avg:61.41ms
step:1680/2330 train_time:103165ms step_avg:61.41ms
step:1681/2330 train_time:103225ms step_avg:61.41ms
step:1682/2330 train_time:103289ms step_avg:61.41ms
step:1683/2330 train_time:103351ms step_avg:61.41ms
step:1684/2330 train_time:103415ms step_avg:61.41ms
step:1685/2330 train_time:103477ms step_avg:61.41ms
step:1686/2330 train_time:103540ms step_avg:61.41ms
step:1687/2330 train_time:103600ms step_avg:61.41ms
step:1688/2330 train_time:103665ms step_avg:61.41ms
step:1689/2330 train_time:103725ms step_avg:61.41ms
step:1690/2330 train_time:103789ms step_avg:61.41ms
step:1691/2330 train_time:103851ms step_avg:61.41ms
step:1692/2330 train_time:103915ms step_avg:61.42ms
step:1693/2330 train_time:103976ms step_avg:61.42ms
step:1694/2330 train_time:104040ms step_avg:61.42ms
step:1695/2330 train_time:104100ms step_avg:61.42ms
step:1696/2330 train_time:104163ms step_avg:61.42ms
step:1697/2330 train_time:104224ms step_avg:61.42ms
step:1698/2330 train_time:104288ms step_avg:61.42ms
step:1699/2330 train_time:104348ms step_avg:61.42ms
step:1700/2330 train_time:104412ms step_avg:61.42ms
step:1701/2330 train_time:104473ms step_avg:61.42ms
step:1702/2330 train_time:104538ms step_avg:61.42ms
step:1703/2330 train_time:104598ms step_avg:61.42ms
step:1704/2330 train_time:104660ms step_avg:61.42ms
step:1705/2330 train_time:104721ms step_avg:61.42ms
step:1706/2330 train_time:104785ms step_avg:61.42ms
step:1707/2330 train_time:104846ms step_avg:61.42ms
step:1708/2330 train_time:104910ms step_avg:61.42ms
step:1709/2330 train_time:104971ms step_avg:61.42ms
step:1710/2330 train_time:105036ms step_avg:61.42ms
step:1711/2330 train_time:105097ms step_avg:61.42ms
step:1712/2330 train_time:105161ms step_avg:61.43ms
step:1713/2330 train_time:105221ms step_avg:61.43ms
step:1714/2330 train_time:105284ms step_avg:61.43ms
step:1715/2330 train_time:105345ms step_avg:61.43ms
step:1716/2330 train_time:105409ms step_avg:61.43ms
step:1717/2330 train_time:105469ms step_avg:61.43ms
step:1718/2330 train_time:105533ms step_avg:61.43ms
step:1719/2330 train_time:105594ms step_avg:61.43ms
step:1720/2330 train_time:105658ms step_avg:61.43ms
step:1721/2330 train_time:105719ms step_avg:61.43ms
step:1722/2330 train_time:105781ms step_avg:61.43ms
step:1723/2330 train_time:105842ms step_avg:61.43ms
step:1724/2330 train_time:105906ms step_avg:61.43ms
step:1725/2330 train_time:105967ms step_avg:61.43ms
step:1726/2330 train_time:106031ms step_avg:61.43ms
step:1727/2330 train_time:106092ms step_avg:61.43ms
step:1728/2330 train_time:106157ms step_avg:61.43ms
step:1729/2330 train_time:106219ms step_avg:61.43ms
step:1730/2330 train_time:106281ms step_avg:61.43ms
step:1731/2330 train_time:106342ms step_avg:61.43ms
step:1732/2330 train_time:106406ms step_avg:61.44ms
step:1733/2330 train_time:106467ms step_avg:61.44ms
step:1734/2330 train_time:106531ms step_avg:61.44ms
step:1735/2330 train_time:106593ms step_avg:61.44ms
step:1736/2330 train_time:106657ms step_avg:61.44ms
step:1737/2330 train_time:106719ms step_avg:61.44ms
step:1738/2330 train_time:106781ms step_avg:61.44ms
step:1739/2330 train_time:106842ms step_avg:61.44ms
step:1740/2330 train_time:106906ms step_avg:61.44ms
step:1741/2330 train_time:106967ms step_avg:61.44ms
step:1742/2330 train_time:107031ms step_avg:61.44ms
step:1743/2330 train_time:107093ms step_avg:61.44ms
step:1744/2330 train_time:107157ms step_avg:61.44ms
step:1745/2330 train_time:107219ms step_avg:61.44ms
step:1746/2330 train_time:107282ms step_avg:61.44ms
step:1747/2330 train_time:107342ms step_avg:61.44ms
step:1748/2330 train_time:107405ms step_avg:61.44ms
step:1749/2330 train_time:107465ms step_avg:61.44ms
step:1750/2330 train_time:107530ms step_avg:61.45ms
step:1750/2330 val_loss:3.9324 train_time:107605ms step_avg:61.49ms
step:1751/2330 train_time:107627ms step_avg:61.47ms
step:1752/2330 train_time:107657ms step_avg:61.45ms
step:1753/2330 train_time:107721ms step_avg:61.45ms
step:1754/2330 train_time:107789ms step_avg:61.45ms
step:1755/2330 train_time:107850ms step_avg:61.45ms
step:1756/2330 train_time:107913ms step_avg:61.45ms
step:1757/2330 train_time:107973ms step_avg:61.45ms
step:1758/2330 train_time:108035ms step_avg:61.45ms
step:1759/2330 train_time:108096ms step_avg:61.45ms
step:1760/2330 train_time:108158ms step_avg:61.45ms
step:1761/2330 train_time:108217ms step_avg:61.45ms
step:1762/2330 train_time:108280ms step_avg:61.45ms
step:1763/2330 train_time:108340ms step_avg:61.45ms
step:1764/2330 train_time:108403ms step_avg:61.45ms
step:1765/2330 train_time:108462ms step_avg:61.45ms
step:1766/2330 train_time:108526ms step_avg:61.45ms
step:1767/2330 train_time:108590ms step_avg:61.45ms
step:1768/2330 train_time:108656ms step_avg:61.46ms
step:1769/2330 train_time:108719ms step_avg:61.46ms
step:1770/2330 train_time:108783ms step_avg:61.46ms
step:1771/2330 train_time:108843ms step_avg:61.46ms
step:1772/2330 train_time:108908ms step_avg:61.46ms
step:1773/2330 train_time:108967ms step_avg:61.46ms
step:1774/2330 train_time:109030ms step_avg:61.46ms
step:1775/2330 train_time:109090ms step_avg:61.46ms
step:1776/2330 train_time:109153ms step_avg:61.46ms
step:1777/2330 train_time:109213ms step_avg:61.46ms
step:1778/2330 train_time:109276ms step_avg:61.46ms
step:1779/2330 train_time:109336ms step_avg:61.46ms
step:1780/2330 train_time:109399ms step_avg:61.46ms
step:1781/2330 train_time:109460ms step_avg:61.46ms
step:1782/2330 train_time:109522ms step_avg:61.46ms
step:1783/2330 train_time:109584ms step_avg:61.46ms
step:1784/2330 train_time:109649ms step_avg:61.46ms
step:1785/2330 train_time:109711ms step_avg:61.46ms
step:1786/2330 train_time:109775ms step_avg:61.46ms
step:1787/2330 train_time:109838ms step_avg:61.46ms
step:1788/2330 train_time:109902ms step_avg:61.47ms
step:1789/2330 train_time:109962ms step_avg:61.47ms
step:1790/2330 train_time:110025ms step_avg:61.47ms
step:1791/2330 train_time:110085ms step_avg:61.47ms
step:1792/2330 train_time:110149ms step_avg:61.47ms
step:1793/2330 train_time:110210ms step_avg:61.47ms
step:1794/2330 train_time:110273ms step_avg:61.47ms
step:1795/2330 train_time:110333ms step_avg:61.47ms
step:1796/2330 train_time:110396ms step_avg:61.47ms
step:1797/2330 train_time:110457ms step_avg:61.47ms
step:1798/2330 train_time:110521ms step_avg:61.47ms
step:1799/2330 train_time:110581ms step_avg:61.47ms
step:1800/2330 train_time:110646ms step_avg:61.47ms
step:1801/2330 train_time:110709ms step_avg:61.47ms
step:1802/2330 train_time:110773ms step_avg:61.47ms
step:1803/2330 train_time:110834ms step_avg:61.47ms
step:1804/2330 train_time:110899ms step_avg:61.47ms
step:1805/2330 train_time:110960ms step_avg:61.47ms
step:1806/2330 train_time:111024ms step_avg:61.47ms
step:1807/2330 train_time:111084ms step_avg:61.47ms
step:1808/2330 train_time:111148ms step_avg:61.48ms
step:1809/2330 train_time:111208ms step_avg:61.47ms
step:1810/2330 train_time:111272ms step_avg:61.48ms
step:1811/2330 train_time:111333ms step_avg:61.48ms
step:1812/2330 train_time:111396ms step_avg:61.48ms
step:1813/2330 train_time:111456ms step_avg:61.48ms
step:1814/2330 train_time:111520ms step_avg:61.48ms
step:1815/2330 train_time:111581ms step_avg:61.48ms
step:1816/2330 train_time:111645ms step_avg:61.48ms
step:1817/2330 train_time:111707ms step_avg:61.48ms
step:1818/2330 train_time:111770ms step_avg:61.48ms
step:1819/2330 train_time:111831ms step_avg:61.48ms
step:1820/2330 train_time:111895ms step_avg:61.48ms
step:1821/2330 train_time:111956ms step_avg:61.48ms
step:1822/2330 train_time:112020ms step_avg:61.48ms
step:1823/2330 train_time:112081ms step_avg:61.48ms
step:1824/2330 train_time:112145ms step_avg:61.48ms
step:1825/2330 train_time:112204ms step_avg:61.48ms
step:1826/2330 train_time:112267ms step_avg:61.48ms
step:1827/2330 train_time:112328ms step_avg:61.48ms
step:1828/2330 train_time:112392ms step_avg:61.48ms
step:1829/2330 train_time:112452ms step_avg:61.48ms
step:1830/2330 train_time:112515ms step_avg:61.48ms
step:1831/2330 train_time:112576ms step_avg:61.48ms
step:1832/2330 train_time:112641ms step_avg:61.49ms
step:1833/2330 train_time:112702ms step_avg:61.49ms
step:1834/2330 train_time:112765ms step_avg:61.49ms
step:1835/2330 train_time:112824ms step_avg:61.48ms
step:1836/2330 train_time:112889ms step_avg:61.49ms
step:1837/2330 train_time:112950ms step_avg:61.49ms
step:1838/2330 train_time:113014ms step_avg:61.49ms
step:1839/2330 train_time:113075ms step_avg:61.49ms
step:1840/2330 train_time:113140ms step_avg:61.49ms
step:1841/2330 train_time:113202ms step_avg:61.49ms
step:1842/2330 train_time:113265ms step_avg:61.49ms
step:1843/2330 train_time:113324ms step_avg:61.49ms
step:1844/2330 train_time:113389ms step_avg:61.49ms
step:1845/2330 train_time:113449ms step_avg:61.49ms
step:1846/2330 train_time:113513ms step_avg:61.49ms
step:1847/2330 train_time:113573ms step_avg:61.49ms
step:1848/2330 train_time:113637ms step_avg:61.49ms
step:1849/2330 train_time:113700ms step_avg:61.49ms
step:1850/2330 train_time:113763ms step_avg:61.49ms
step:1851/2330 train_time:113823ms step_avg:61.49ms
step:1852/2330 train_time:113886ms step_avg:61.49ms
step:1853/2330 train_time:113948ms step_avg:61.49ms
step:1854/2330 train_time:114012ms step_avg:61.50ms
step:1855/2330 train_time:114073ms step_avg:61.49ms
step:1856/2330 train_time:114137ms step_avg:61.50ms
step:1857/2330 train_time:114198ms step_avg:61.50ms
step:1858/2330 train_time:114262ms step_avg:61.50ms
step:1859/2330 train_time:114322ms step_avg:61.50ms
step:1860/2330 train_time:114386ms step_avg:61.50ms
step:1861/2330 train_time:114447ms step_avg:61.50ms
step:1862/2330 train_time:114511ms step_avg:61.50ms
step:1863/2330 train_time:114572ms step_avg:61.50ms
step:1864/2330 train_time:114636ms step_avg:61.50ms
step:1865/2330 train_time:114697ms step_avg:61.50ms
step:1866/2330 train_time:114761ms step_avg:61.50ms
step:1867/2330 train_time:114821ms step_avg:61.50ms
step:1868/2330 train_time:114885ms step_avg:61.50ms
step:1869/2330 train_time:114946ms step_avg:61.50ms
step:1870/2330 train_time:115011ms step_avg:61.50ms
step:1871/2330 train_time:115071ms step_avg:61.50ms
step:1872/2330 train_time:115135ms step_avg:61.50ms
step:1873/2330 train_time:115198ms step_avg:61.50ms
step:1874/2330 train_time:115261ms step_avg:61.51ms
step:1875/2330 train_time:115322ms step_avg:61.50ms
step:1876/2330 train_time:115386ms step_avg:61.51ms
step:1877/2330 train_time:115446ms step_avg:61.51ms
step:1878/2330 train_time:115510ms step_avg:61.51ms
step:1879/2330 train_time:115571ms step_avg:61.51ms
step:1880/2330 train_time:115633ms step_avg:61.51ms
step:1881/2330 train_time:115695ms step_avg:61.51ms
step:1882/2330 train_time:115759ms step_avg:61.51ms
step:1883/2330 train_time:115819ms step_avg:61.51ms
step:1884/2330 train_time:115883ms step_avg:61.51ms
step:1885/2330 train_time:115944ms step_avg:61.51ms
step:1886/2330 train_time:116007ms step_avg:61.51ms
step:1887/2330 train_time:116068ms step_avg:61.51ms
step:1888/2330 train_time:116132ms step_avg:61.51ms
step:1889/2330 train_time:116192ms step_avg:61.51ms
step:1890/2330 train_time:116256ms step_avg:61.51ms
step:1891/2330 train_time:116318ms step_avg:61.51ms
step:1892/2330 train_time:116383ms step_avg:61.51ms
step:1893/2330 train_time:116443ms step_avg:61.51ms
step:1894/2330 train_time:116507ms step_avg:61.51ms
step:1895/2330 train_time:116566ms step_avg:61.51ms
step:1896/2330 train_time:116630ms step_avg:61.51ms
step:1897/2330 train_time:116689ms step_avg:61.51ms
step:1898/2330 train_time:116753ms step_avg:61.51ms
step:1899/2330 train_time:116814ms step_avg:61.51ms
step:1900/2330 train_time:116879ms step_avg:61.52ms
step:1901/2330 train_time:116940ms step_avg:61.52ms
step:1902/2330 train_time:117003ms step_avg:61.52ms
step:1903/2330 train_time:117063ms step_avg:61.52ms
step:1904/2330 train_time:117127ms step_avg:61.52ms
step:1905/2330 train_time:117188ms step_avg:61.52ms
step:1906/2330 train_time:117252ms step_avg:61.52ms
step:1907/2330 train_time:117313ms step_avg:61.52ms
step:1908/2330 train_time:117377ms step_avg:61.52ms
step:1909/2330 train_time:117437ms step_avg:61.52ms
step:1910/2330 train_time:117503ms step_avg:61.52ms
step:1911/2330 train_time:117563ms step_avg:61.52ms
step:1912/2330 train_time:117625ms step_avg:61.52ms
step:1913/2330 train_time:117686ms step_avg:61.52ms
step:1914/2330 train_time:117750ms step_avg:61.52ms
step:1915/2330 train_time:117810ms step_avg:61.52ms
step:1916/2330 train_time:117873ms step_avg:61.52ms
step:1917/2330 train_time:117935ms step_avg:61.52ms
step:1918/2330 train_time:118000ms step_avg:61.52ms
step:1919/2330 train_time:118060ms step_avg:61.52ms
step:1920/2330 train_time:118123ms step_avg:61.52ms
step:1921/2330 train_time:118184ms step_avg:61.52ms
step:1922/2330 train_time:118248ms step_avg:61.52ms
step:1923/2330 train_time:118309ms step_avg:61.52ms
step:1924/2330 train_time:118373ms step_avg:61.52ms
step:1925/2330 train_time:118434ms step_avg:61.52ms
step:1926/2330 train_time:118499ms step_avg:61.53ms
step:1927/2330 train_time:118560ms step_avg:61.53ms
step:1928/2330 train_time:118622ms step_avg:61.53ms
step:1929/2330 train_time:118684ms step_avg:61.53ms
step:1930/2330 train_time:118747ms step_avg:61.53ms
step:1931/2330 train_time:118808ms step_avg:61.53ms
step:1932/2330 train_time:118872ms step_avg:61.53ms
step:1933/2330 train_time:118932ms step_avg:61.53ms
step:1934/2330 train_time:118997ms step_avg:61.53ms
step:1935/2330 train_time:119058ms step_avg:61.53ms
step:1936/2330 train_time:119122ms step_avg:61.53ms
step:1937/2330 train_time:119182ms step_avg:61.53ms
step:1938/2330 train_time:119244ms step_avg:61.53ms
step:1939/2330 train_time:119306ms step_avg:61.53ms
step:1940/2330 train_time:119370ms step_avg:61.53ms
step:1941/2330 train_time:119430ms step_avg:61.53ms
step:1942/2330 train_time:119494ms step_avg:61.53ms
step:1943/2330 train_time:119555ms step_avg:61.53ms
step:1944/2330 train_time:119619ms step_avg:61.53ms
step:1945/2330 train_time:119679ms step_avg:61.53ms
step:1946/2330 train_time:119743ms step_avg:61.53ms
step:1947/2330 train_time:119804ms step_avg:61.53ms
step:1948/2330 train_time:119866ms step_avg:61.53ms
step:1949/2330 train_time:119927ms step_avg:61.53ms
step:1950/2330 train_time:119991ms step_avg:61.53ms
step:1951/2330 train_time:120052ms step_avg:61.53ms
step:1952/2330 train_time:120116ms step_avg:61.53ms
step:1953/2330 train_time:120177ms step_avg:61.53ms
step:1954/2330 train_time:120241ms step_avg:61.54ms
step:1955/2330 train_time:120302ms step_avg:61.54ms
step:1956/2330 train_time:120364ms step_avg:61.54ms
step:1957/2330 train_time:120424ms step_avg:61.53ms
step:1958/2330 train_time:120487ms step_avg:61.54ms
step:1959/2330 train_time:120549ms step_avg:61.54ms
step:1960/2330 train_time:120613ms step_avg:61.54ms
step:1961/2330 train_time:120673ms step_avg:61.54ms
step:1962/2330 train_time:120738ms step_avg:61.54ms
step:1963/2330 train_time:120799ms step_avg:61.54ms
step:1964/2330 train_time:120862ms step_avg:61.54ms
step:1965/2330 train_time:120922ms step_avg:61.54ms
step:1966/2330 train_time:120985ms step_avg:61.54ms
step:1967/2330 train_time:121046ms step_avg:61.54ms
step:1968/2330 train_time:121110ms step_avg:61.54ms
step:1969/2330 train_time:121170ms step_avg:61.54ms
step:1970/2330 train_time:121234ms step_avg:61.54ms
step:1971/2330 train_time:121295ms step_avg:61.54ms
step:1972/2330 train_time:121359ms step_avg:61.54ms
step:1973/2330 train_time:121420ms step_avg:61.54ms
step:1974/2330 train_time:121484ms step_avg:61.54ms
step:1975/2330 train_time:121544ms step_avg:61.54ms
step:1976/2330 train_time:121608ms step_avg:61.54ms
step:1977/2330 train_time:121669ms step_avg:61.54ms
step:1978/2330 train_time:121732ms step_avg:61.54ms
step:1979/2330 train_time:121793ms step_avg:61.54ms
step:1980/2330 train_time:121858ms step_avg:61.54ms
step:1981/2330 train_time:121919ms step_avg:61.54ms
step:1982/2330 train_time:121983ms step_avg:61.55ms
step:1983/2330 train_time:122043ms step_avg:61.54ms
step:1984/2330 train_time:122107ms step_avg:61.55ms
step:1985/2330 train_time:122167ms step_avg:61.54ms
step:1986/2330 train_time:122231ms step_avg:61.55ms
step:1987/2330 train_time:122291ms step_avg:61.55ms
step:1988/2330 train_time:122354ms step_avg:61.55ms
step:1989/2330 train_time:122416ms step_avg:61.55ms
step:1990/2330 train_time:122480ms step_avg:61.55ms
step:1991/2330 train_time:122540ms step_avg:61.55ms
step:1992/2330 train_time:122605ms step_avg:61.55ms
step:1993/2330 train_time:122665ms step_avg:61.55ms
step:1994/2330 train_time:122728ms step_avg:61.55ms
step:1995/2330 train_time:122788ms step_avg:61.55ms
step:1996/2330 train_time:122852ms step_avg:61.55ms
step:1997/2330 train_time:122913ms step_avg:61.55ms
step:1998/2330 train_time:122977ms step_avg:61.55ms
step:1999/2330 train_time:123039ms step_avg:61.55ms
step:2000/2330 train_time:123103ms step_avg:61.55ms
step:2000/2330 val_loss:3.8744 train_time:123176ms step_avg:61.59ms
step:2001/2330 train_time:123196ms step_avg:61.57ms
step:2002/2330 train_time:123228ms step_avg:61.55ms
step:2003/2330 train_time:123295ms step_avg:61.56ms
step:2004/2330 train_time:123361ms step_avg:61.56ms
step:2005/2330 train_time:123423ms step_avg:61.56ms
step:2006/2330 train_time:123486ms step_avg:61.56ms
step:2007/2330 train_time:123548ms step_avg:61.56ms
step:2008/2330 train_time:123611ms step_avg:61.56ms
step:2009/2330 train_time:123671ms step_avg:61.56ms
step:2010/2330 train_time:123734ms step_avg:61.56ms
step:2011/2330 train_time:123794ms step_avg:61.56ms
step:2012/2330 train_time:123856ms step_avg:61.56ms
step:2013/2330 train_time:123916ms step_avg:61.56ms
step:2014/2330 train_time:123979ms step_avg:61.56ms
step:2015/2330 train_time:124038ms step_avg:61.56ms
step:2016/2330 train_time:124102ms step_avg:61.56ms
step:2017/2330 train_time:124162ms step_avg:61.56ms
step:2018/2330 train_time:124228ms step_avg:61.56ms
step:2019/2330 train_time:124292ms step_avg:61.56ms
step:2020/2330 train_time:124355ms step_avg:61.56ms
step:2021/2330 train_time:124417ms step_avg:61.56ms
step:2022/2330 train_time:124480ms step_avg:61.56ms
step:2023/2330 train_time:124541ms step_avg:61.56ms
step:2024/2330 train_time:124606ms step_avg:61.56ms
step:2025/2330 train_time:124666ms step_avg:61.56ms
step:2026/2330 train_time:124730ms step_avg:61.56ms
step:2027/2330 train_time:124790ms step_avg:61.56ms
step:2028/2330 train_time:124854ms step_avg:61.57ms
step:2029/2330 train_time:124914ms step_avg:61.56ms
step:2030/2330 train_time:124977ms step_avg:61.56ms
step:2031/2330 train_time:125037ms step_avg:61.56ms
step:2032/2330 train_time:125100ms step_avg:61.56ms
step:2033/2330 train_time:125160ms step_avg:61.56ms
step:2034/2330 train_time:125225ms step_avg:61.57ms
step:2035/2330 train_time:125285ms step_avg:61.57ms
step:2036/2330 train_time:125349ms step_avg:61.57ms
step:2037/2330 train_time:125413ms step_avg:61.57ms
step:2038/2330 train_time:125476ms step_avg:61.57ms
step:2039/2330 train_time:125536ms step_avg:61.57ms
step:2040/2330 train_time:125600ms step_avg:61.57ms
step:2041/2330 train_time:125660ms step_avg:61.57ms
step:2042/2330 train_time:125725ms step_avg:61.57ms
step:2043/2330 train_time:125785ms step_avg:61.57ms
step:2044/2330 train_time:125849ms step_avg:61.57ms
step:2045/2330 train_time:125910ms step_avg:61.57ms
step:2046/2330 train_time:125973ms step_avg:61.57ms
step:2047/2330 train_time:126034ms step_avg:61.57ms
step:2048/2330 train_time:126097ms step_avg:61.57ms
step:2049/2330 train_time:126158ms step_avg:61.57ms
step:2050/2330 train_time:126221ms step_avg:61.57ms
step:2051/2330 train_time:126283ms step_avg:61.57ms
step:2052/2330 train_time:126347ms step_avg:61.57ms
step:2053/2330 train_time:126407ms step_avg:61.57ms
step:2054/2330 train_time:126472ms step_avg:61.57ms
step:2055/2330 train_time:126533ms step_avg:61.57ms
step:2056/2330 train_time:126598ms step_avg:61.57ms
step:2057/2330 train_time:126658ms step_avg:61.57ms
step:2058/2330 train_time:126723ms step_avg:61.58ms
step:2059/2330 train_time:126784ms step_avg:61.58ms
step:2060/2330 train_time:126846ms step_avg:61.58ms
step:2061/2330 train_time:126907ms step_avg:61.58ms
step:2062/2330 train_time:126970ms step_avg:61.58ms
step:2063/2330 train_time:127031ms step_avg:61.58ms
step:2064/2330 train_time:127094ms step_avg:61.58ms
step:2065/2330 train_time:127156ms step_avg:61.58ms
step:2066/2330 train_time:127219ms step_avg:61.58ms
step:2067/2330 train_time:127279ms step_avg:61.58ms
step:2068/2330 train_time:127342ms step_avg:61.58ms
step:2069/2330 train_time:127404ms step_avg:61.58ms
step:2070/2330 train_time:127466ms step_avg:61.58ms
step:2071/2330 train_time:127529ms step_avg:61.58ms
step:2072/2330 train_time:127594ms step_avg:61.58ms
step:2073/2330 train_time:127655ms step_avg:61.58ms
step:2074/2330 train_time:127718ms step_avg:61.58ms
step:2075/2330 train_time:127778ms step_avg:61.58ms
step:2076/2330 train_time:127841ms step_avg:61.58ms
step:2077/2330 train_time:127901ms step_avg:61.58ms
step:2078/2330 train_time:127965ms step_avg:61.58ms
step:2079/2330 train_time:128026ms step_avg:61.58ms
step:2080/2330 train_time:128089ms step_avg:61.58ms
step:2081/2330 train_time:128151ms step_avg:61.58ms
step:2082/2330 train_time:128217ms step_avg:61.58ms
step:2083/2330 train_time:128276ms step_avg:61.58ms
step:2084/2330 train_time:128340ms step_avg:61.58ms
step:2085/2330 train_time:128401ms step_avg:61.58ms
step:2086/2330 train_time:128465ms step_avg:61.58ms
step:2087/2330 train_time:128526ms step_avg:61.58ms
step:2088/2330 train_time:128589ms step_avg:61.58ms
step:2089/2330 train_time:128651ms step_avg:61.58ms
step:2090/2330 train_time:128715ms step_avg:61.59ms
step:2091/2330 train_time:128775ms step_avg:61.59ms
step:2092/2330 train_time:128838ms step_avg:61.59ms
step:2093/2330 train_time:128899ms step_avg:61.59ms
step:2094/2330 train_time:128963ms step_avg:61.59ms
step:2095/2330 train_time:129024ms step_avg:61.59ms
step:2096/2330 train_time:129087ms step_avg:61.59ms
step:2097/2330 train_time:129148ms step_avg:61.59ms
step:2098/2330 train_time:129213ms step_avg:61.59ms
step:2099/2330 train_time:129274ms step_avg:61.59ms
step:2100/2330 train_time:129337ms step_avg:61.59ms
step:2101/2330 train_time:129397ms step_avg:61.59ms
step:2102/2330 train_time:129461ms step_avg:61.59ms
step:2103/2330 train_time:129521ms step_avg:61.59ms
step:2104/2330 train_time:129585ms step_avg:61.59ms
step:2105/2330 train_time:129646ms step_avg:61.59ms
step:2106/2330 train_time:129710ms step_avg:61.59ms
step:2107/2330 train_time:129771ms step_avg:61.59ms
step:2108/2330 train_time:129836ms step_avg:61.59ms
step:2109/2330 train_time:129897ms step_avg:61.59ms
step:2110/2330 train_time:129960ms step_avg:61.59ms
step:2111/2330 train_time:130022ms step_avg:61.59ms
step:2112/2330 train_time:130085ms step_avg:61.59ms
step:2113/2330 train_time:130145ms step_avg:61.59ms
step:2114/2330 train_time:130208ms step_avg:61.59ms
step:2115/2330 train_time:130270ms step_avg:61.59ms
step:2116/2330 train_time:130333ms step_avg:61.59ms
step:2117/2330 train_time:130394ms step_avg:61.59ms
step:2118/2330 train_time:130457ms step_avg:61.59ms
step:2119/2330 train_time:130517ms step_avg:61.59ms
step:2120/2330 train_time:130581ms step_avg:61.59ms
step:2121/2330 train_time:130641ms step_avg:61.59ms
step:2122/2330 train_time:130705ms step_avg:61.60ms
step:2123/2330 train_time:130766ms step_avg:61.59ms
step:2124/2330 train_time:130831ms step_avg:61.60ms
step:2125/2330 train_time:130893ms step_avg:61.60ms
step:2126/2330 train_time:130956ms step_avg:61.60ms
step:2127/2330 train_time:131017ms step_avg:61.60ms
step:2128/2330 train_time:131081ms step_avg:61.60ms
step:2129/2330 train_time:131141ms step_avg:61.60ms
step:2130/2330 train_time:131204ms step_avg:61.60ms
step:2131/2330 train_time:131264ms step_avg:61.60ms
step:2132/2330 train_time:131328ms step_avg:61.60ms
step:2133/2330 train_time:131389ms step_avg:61.60ms
step:2134/2330 train_time:131454ms step_avg:61.60ms
step:2135/2330 train_time:131514ms step_avg:61.60ms
step:2136/2330 train_time:131578ms step_avg:61.60ms
step:2137/2330 train_time:131638ms step_avg:61.60ms
step:2138/2330 train_time:131701ms step_avg:61.60ms
step:2139/2330 train_time:131763ms step_avg:61.60ms
step:2140/2330 train_time:131827ms step_avg:61.60ms
step:2141/2330 train_time:131887ms step_avg:61.60ms
step:2142/2330 train_time:131953ms step_avg:61.60ms
step:2143/2330 train_time:132014ms step_avg:61.60ms
step:2144/2330 train_time:132077ms step_avg:61.60ms
step:2145/2330 train_time:132137ms step_avg:61.60ms
step:2146/2330 train_time:132201ms step_avg:61.60ms
step:2147/2330 train_time:132261ms step_avg:61.60ms
step:2148/2330 train_time:132325ms step_avg:61.60ms
step:2149/2330 train_time:132385ms step_avg:61.60ms
step:2150/2330 train_time:132449ms step_avg:61.60ms
step:2151/2330 train_time:132511ms step_avg:61.60ms
step:2152/2330 train_time:132576ms step_avg:61.61ms
step:2153/2330 train_time:132636ms step_avg:61.61ms
step:2154/2330 train_time:132698ms step_avg:61.61ms
step:2155/2330 train_time:132759ms step_avg:61.61ms
step:2156/2330 train_time:132823ms step_avg:61.61ms
step:2157/2330 train_time:132885ms step_avg:61.61ms
step:2158/2330 train_time:132949ms step_avg:61.61ms
step:2159/2330 train_time:133011ms step_avg:61.61ms
step:2160/2330 train_time:133076ms step_avg:61.61ms
step:2161/2330 train_time:133136ms step_avg:61.61ms
step:2162/2330 train_time:133199ms step_avg:61.61ms
step:2163/2330 train_time:133260ms step_avg:61.61ms
step:2164/2330 train_time:133325ms step_avg:61.61ms
step:2165/2330 train_time:133384ms step_avg:61.61ms
step:2166/2330 train_time:133447ms step_avg:61.61ms
step:2167/2330 train_time:133509ms step_avg:61.61ms
step:2168/2330 train_time:133573ms step_avg:61.61ms
step:2169/2330 train_time:133633ms step_avg:61.61ms
step:2170/2330 train_time:133698ms step_avg:61.61ms
step:2171/2330 train_time:133759ms step_avg:61.61ms
step:2172/2330 train_time:133823ms step_avg:61.61ms
step:2173/2330 train_time:133883ms step_avg:61.61ms
step:2174/2330 train_time:133947ms step_avg:61.61ms
step:2175/2330 train_time:134008ms step_avg:61.61ms
step:2176/2330 train_time:134073ms step_avg:61.61ms
step:2177/2330 train_time:134133ms step_avg:61.61ms
step:2178/2330 train_time:134198ms step_avg:61.62ms
step:2179/2330 train_time:134258ms step_avg:61.61ms
step:2180/2330 train_time:134323ms step_avg:61.62ms
step:2181/2330 train_time:134384ms step_avg:61.62ms
step:2182/2330 train_time:134448ms step_avg:61.62ms
step:2183/2330 train_time:134508ms step_avg:61.62ms
step:2184/2330 train_time:134572ms step_avg:61.62ms
step:2185/2330 train_time:134633ms step_avg:61.62ms
step:2186/2330 train_time:134697ms step_avg:61.62ms
step:2187/2330 train_time:134757ms step_avg:61.62ms
step:2188/2330 train_time:134822ms step_avg:61.62ms
step:2189/2330 train_time:134882ms step_avg:61.62ms
step:2190/2330 train_time:134946ms step_avg:61.62ms
step:2191/2330 train_time:135007ms step_avg:61.62ms
step:2192/2330 train_time:135072ms step_avg:61.62ms
step:2193/2330 train_time:135132ms step_avg:61.62ms
step:2194/2330 train_time:135197ms step_avg:61.62ms
step:2195/2330 train_time:135256ms step_avg:61.62ms
step:2196/2330 train_time:135321ms step_avg:61.62ms
step:2197/2330 train_time:135381ms step_avg:61.62ms
step:2198/2330 train_time:135446ms step_avg:61.62ms
step:2199/2330 train_time:135506ms step_avg:61.62ms
step:2200/2330 train_time:135570ms step_avg:61.62ms
step:2201/2330 train_time:135631ms step_avg:61.62ms
step:2202/2330 train_time:135695ms step_avg:61.62ms
step:2203/2330 train_time:135756ms step_avg:61.62ms
step:2204/2330 train_time:135821ms step_avg:61.62ms
step:2205/2330 train_time:135881ms step_avg:61.62ms
step:2206/2330 train_time:135945ms step_avg:61.62ms
step:2207/2330 train_time:136005ms step_avg:61.62ms
step:2208/2330 train_time:136069ms step_avg:61.63ms
step:2209/2330 train_time:136131ms step_avg:61.63ms
step:2210/2330 train_time:136196ms step_avg:61.63ms
step:2211/2330 train_time:136256ms step_avg:61.63ms
step:2212/2330 train_time:136321ms step_avg:61.63ms
step:2213/2330 train_time:136381ms step_avg:61.63ms
step:2214/2330 train_time:136444ms step_avg:61.63ms
step:2215/2330 train_time:136504ms step_avg:61.63ms
step:2216/2330 train_time:136569ms step_avg:61.63ms
step:2217/2330 train_time:136630ms step_avg:61.63ms
step:2218/2330 train_time:136694ms step_avg:61.63ms
step:2219/2330 train_time:136754ms step_avg:61.63ms
step:2220/2330 train_time:136819ms step_avg:61.63ms
step:2221/2330 train_time:136878ms step_avg:61.63ms
step:2222/2330 train_time:136943ms step_avg:61.63ms
step:2223/2330 train_time:137003ms step_avg:61.63ms
step:2224/2330 train_time:137067ms step_avg:61.63ms
step:2225/2330 train_time:137127ms step_avg:61.63ms
step:2226/2330 train_time:137192ms step_avg:61.63ms
step:2227/2330 train_time:137254ms step_avg:61.63ms
step:2228/2330 train_time:137319ms step_avg:61.63ms
step:2229/2330 train_time:137377ms step_avg:61.63ms
step:2230/2330 train_time:137441ms step_avg:61.63ms
step:2231/2330 train_time:137502ms step_avg:61.63ms
step:2232/2330 train_time:137567ms step_avg:61.63ms
step:2233/2330 train_time:137628ms step_avg:61.63ms
step:2234/2330 train_time:137692ms step_avg:61.63ms
step:2235/2330 train_time:137753ms step_avg:61.63ms
step:2236/2330 train_time:137818ms step_avg:61.64ms
step:2237/2330 train_time:137877ms step_avg:61.63ms
step:2238/2330 train_time:137941ms step_avg:61.64ms
step:2239/2330 train_time:138001ms step_avg:61.64ms
step:2240/2330 train_time:138065ms step_avg:61.64ms
step:2241/2330 train_time:138125ms step_avg:61.64ms
step:2242/2330 train_time:138188ms step_avg:61.64ms
step:2243/2330 train_time:138250ms step_avg:61.64ms
step:2244/2330 train_time:138315ms step_avg:61.64ms
step:2245/2330 train_time:138376ms step_avg:61.64ms
step:2246/2330 train_time:138439ms step_avg:61.64ms
step:2247/2330 train_time:138499ms step_avg:61.64ms
step:2248/2330 train_time:138564ms step_avg:61.64ms
step:2249/2330 train_time:138624ms step_avg:61.64ms
step:2250/2330 train_time:138688ms step_avg:61.64ms
step:2250/2330 val_loss:3.8251 train_time:138763ms step_avg:61.67ms
step:2251/2330 train_time:138784ms step_avg:61.65ms
step:2252/2330 train_time:138816ms step_avg:61.64ms
step:2253/2330 train_time:138877ms step_avg:61.64ms
step:2254/2330 train_time:138943ms step_avg:61.64ms
step:2255/2330 train_time:139005ms step_avg:61.64ms
step:2256/2330 train_time:139068ms step_avg:61.64ms
step:2257/2330 train_time:139129ms step_avg:61.64ms
step:2258/2330 train_time:139192ms step_avg:61.64ms
step:2259/2330 train_time:139251ms step_avg:61.64ms
step:2260/2330 train_time:139314ms step_avg:61.64ms
step:2261/2330 train_time:139373ms step_avg:61.64ms
step:2262/2330 train_time:139436ms step_avg:61.64ms
step:2263/2330 train_time:139496ms step_avg:61.64ms
step:2264/2330 train_time:139558ms step_avg:61.64ms
step:2265/2330 train_time:139619ms step_avg:61.64ms
step:2266/2330 train_time:139683ms step_avg:61.64ms
step:2267/2330 train_time:139744ms step_avg:61.64ms
step:2268/2330 train_time:139810ms step_avg:61.64ms
step:2269/2330 train_time:139871ms step_avg:61.64ms
step:2270/2330 train_time:139936ms step_avg:61.65ms
step:2271/2330 train_time:139997ms step_avg:61.65ms
step:2272/2330 train_time:140062ms step_avg:61.65ms
step:2273/2330 train_time:140123ms step_avg:61.65ms
step:2274/2330 train_time:140186ms step_avg:61.65ms
step:2275/2330 train_time:140246ms step_avg:61.65ms
step:2276/2330 train_time:140309ms step_avg:61.65ms
step:2277/2330 train_time:140369ms step_avg:61.65ms
step:2278/2330 train_time:140432ms step_avg:61.65ms
step:2279/2330 train_time:140492ms step_avg:61.65ms
step:2280/2330 train_time:140556ms step_avg:61.65ms
step:2281/2330 train_time:140616ms step_avg:61.65ms
step:2282/2330 train_time:140680ms step_avg:61.65ms
step:2283/2330 train_time:140741ms step_avg:61.65ms
step:2284/2330 train_time:140806ms step_avg:61.65ms
step:2285/2330 train_time:140866ms step_avg:61.65ms
step:2286/2330 train_time:140930ms step_avg:61.65ms
step:2287/2330 train_time:140991ms step_avg:61.65ms
step:2288/2330 train_time:141056ms step_avg:61.65ms
step:2289/2330 train_time:141116ms step_avg:61.65ms
step:2290/2330 train_time:141181ms step_avg:61.65ms
step:2291/2330 train_time:141243ms step_avg:61.65ms
step:2292/2330 train_time:141306ms step_avg:61.65ms
step:2293/2330 train_time:141366ms step_avg:61.65ms
step:2294/2330 train_time:141428ms step_avg:61.65ms
step:2295/2330 train_time:141488ms step_avg:61.65ms
step:2296/2330 train_time:141552ms step_avg:61.65ms
step:2297/2330 train_time:141612ms step_avg:61.65ms
step:2298/2330 train_time:141677ms step_avg:61.65ms
step:2299/2330 train_time:141738ms step_avg:61.65ms
step:2300/2330 train_time:141802ms step_avg:61.65ms
step:2301/2330 train_time:141863ms step_avg:61.65ms
step:2302/2330 train_time:141928ms step_avg:61.65ms
step:2303/2330 train_time:141987ms step_avg:61.65ms
step:2304/2330 train_time:142051ms step_avg:61.65ms
step:2305/2330 train_time:142112ms step_avg:61.65ms
step:2306/2330 train_time:142176ms step_avg:61.66ms
step:2307/2330 train_time:142237ms step_avg:61.65ms
step:2308/2330 train_time:142303ms step_avg:61.66ms
step:2309/2330 train_time:142364ms step_avg:61.66ms
step:2310/2330 train_time:142428ms step_avg:61.66ms
step:2311/2330 train_time:142488ms step_avg:61.66ms
step:2312/2330 train_time:142550ms step_avg:61.66ms
step:2313/2330 train_time:142611ms step_avg:61.66ms
step:2314/2330 train_time:142675ms step_avg:61.66ms
step:2315/2330 train_time:142736ms step_avg:61.66ms
step:2316/2330 train_time:142800ms step_avg:61.66ms
step:2317/2330 train_time:142862ms step_avg:61.66ms
step:2318/2330 train_time:142925ms step_avg:61.66ms
step:2319/2330 train_time:142986ms step_avg:61.66ms
step:2320/2330 train_time:143049ms step_avg:61.66ms
step:2321/2330 train_time:143110ms step_avg:61.66ms
step:2322/2330 train_time:143175ms step_avg:61.66ms
step:2323/2330 train_time:143235ms step_avg:61.66ms
step:2324/2330 train_time:143300ms step_avg:61.66ms
step:2325/2330 train_time:143361ms step_avg:61.66ms
step:2326/2330 train_time:143426ms step_avg:61.66ms
step:2327/2330 train_time:143486ms step_avg:61.66ms
step:2328/2330 train_time:143549ms step_avg:61.66ms
step:2329/2330 train_time:143610ms step_avg:61.66ms
step:2330/2330 train_time:143674ms step_avg:61.66ms
step:2330/2330 val_loss:3.8113 train_time:143750ms step_avg:61.70ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
