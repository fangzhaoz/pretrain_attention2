import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr1e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=1e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:19:03 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:83ms step_avg:83.05ms
step:2/2330 train_time:181ms step_avg:90.61ms
step:3/2330 train_time:200ms step_avg:66.74ms
step:4/2330 train_time:222ms step_avg:55.58ms
step:5/2330 train_time:278ms step_avg:55.52ms
step:6/2330 train_time:337ms step_avg:56.17ms
step:7/2330 train_time:392ms step_avg:56.07ms
step:8/2330 train_time:453ms step_avg:56.64ms
step:9/2330 train_time:509ms step_avg:56.53ms
step:10/2330 train_time:569ms step_avg:56.91ms
step:11/2330 train_time:625ms step_avg:56.79ms
step:12/2330 train_time:684ms step_avg:57.02ms
step:13/2330 train_time:740ms step_avg:56.92ms
step:14/2330 train_time:800ms step_avg:57.11ms
step:15/2330 train_time:855ms step_avg:57.02ms
step:16/2330 train_time:915ms step_avg:57.19ms
step:17/2330 train_time:971ms step_avg:57.11ms
step:18/2330 train_time:1031ms step_avg:57.29ms
step:19/2330 train_time:1087ms step_avg:57.24ms
step:20/2330 train_time:1152ms step_avg:57.60ms
step:21/2330 train_time:1210ms step_avg:57.61ms
step:22/2330 train_time:1273ms step_avg:57.86ms
step:23/2330 train_time:1329ms step_avg:57.79ms
step:24/2330 train_time:1391ms step_avg:57.94ms
step:25/2330 train_time:1447ms step_avg:57.88ms
step:26/2330 train_time:1509ms step_avg:58.04ms
step:27/2330 train_time:1565ms step_avg:57.96ms
step:28/2330 train_time:1625ms step_avg:58.04ms
step:29/2330 train_time:1681ms step_avg:57.97ms
step:30/2330 train_time:1742ms step_avg:58.06ms
step:31/2330 train_time:1798ms step_avg:58.01ms
step:32/2330 train_time:1858ms step_avg:58.06ms
step:33/2330 train_time:1914ms step_avg:57.99ms
step:34/2330 train_time:1974ms step_avg:58.06ms
step:35/2330 train_time:2030ms step_avg:58.01ms
step:36/2330 train_time:2091ms step_avg:58.08ms
step:37/2330 train_time:2147ms step_avg:58.04ms
step:38/2330 train_time:2211ms step_avg:58.17ms
step:39/2330 train_time:2268ms step_avg:58.15ms
step:40/2330 train_time:2329ms step_avg:58.22ms
step:41/2330 train_time:2386ms step_avg:58.18ms
step:42/2330 train_time:2447ms step_avg:58.25ms
step:43/2330 train_time:2504ms step_avg:58.23ms
step:44/2330 train_time:2564ms step_avg:58.27ms
step:45/2330 train_time:2620ms step_avg:58.23ms
step:46/2330 train_time:2680ms step_avg:58.27ms
step:47/2330 train_time:2736ms step_avg:58.21ms
step:48/2330 train_time:2797ms step_avg:58.27ms
step:49/2330 train_time:2853ms step_avg:58.23ms
step:50/2330 train_time:2913ms step_avg:58.26ms
step:51/2330 train_time:2970ms step_avg:58.23ms
step:52/2330 train_time:3030ms step_avg:58.26ms
step:53/2330 train_time:3086ms step_avg:58.23ms
step:54/2330 train_time:3148ms step_avg:58.31ms
step:55/2330 train_time:3206ms step_avg:58.30ms
step:56/2330 train_time:3267ms step_avg:58.33ms
step:57/2330 train_time:3324ms step_avg:58.31ms
step:58/2330 train_time:3384ms step_avg:58.35ms
step:59/2330 train_time:3441ms step_avg:58.33ms
step:60/2330 train_time:3502ms step_avg:58.37ms
step:61/2330 train_time:3558ms step_avg:58.33ms
step:62/2330 train_time:3620ms step_avg:58.38ms
step:63/2330 train_time:3676ms step_avg:58.34ms
step:64/2330 train_time:3736ms step_avg:58.37ms
step:65/2330 train_time:3792ms step_avg:58.34ms
step:66/2330 train_time:3853ms step_avg:58.38ms
step:67/2330 train_time:3909ms step_avg:58.35ms
step:68/2330 train_time:3969ms step_avg:58.37ms
step:69/2330 train_time:4025ms step_avg:58.34ms
step:70/2330 train_time:4088ms step_avg:58.39ms
step:71/2330 train_time:4144ms step_avg:58.36ms
step:72/2330 train_time:4206ms step_avg:58.41ms
step:73/2330 train_time:4262ms step_avg:58.38ms
step:74/2330 train_time:4324ms step_avg:58.43ms
step:75/2330 train_time:4380ms step_avg:58.40ms
step:76/2330 train_time:4441ms step_avg:58.44ms
step:77/2330 train_time:4497ms step_avg:58.41ms
step:78/2330 train_time:4558ms step_avg:58.43ms
step:79/2330 train_time:4614ms step_avg:58.41ms
step:80/2330 train_time:4674ms step_avg:58.43ms
step:81/2330 train_time:4731ms step_avg:58.41ms
step:82/2330 train_time:4792ms step_avg:58.44ms
step:83/2330 train_time:4848ms step_avg:58.41ms
step:84/2330 train_time:4909ms step_avg:58.44ms
step:85/2330 train_time:4966ms step_avg:58.42ms
step:86/2330 train_time:5026ms step_avg:58.44ms
step:87/2330 train_time:5082ms step_avg:58.42ms
step:88/2330 train_time:5143ms step_avg:58.44ms
step:89/2330 train_time:5199ms step_avg:58.42ms
step:90/2330 train_time:5260ms step_avg:58.45ms
step:91/2330 train_time:5316ms step_avg:58.42ms
step:92/2330 train_time:5377ms step_avg:58.45ms
step:93/2330 train_time:5434ms step_avg:58.43ms
step:94/2330 train_time:5495ms step_avg:58.46ms
step:95/2330 train_time:5552ms step_avg:58.44ms
step:96/2330 train_time:5612ms step_avg:58.46ms
step:97/2330 train_time:5668ms step_avg:58.44ms
step:98/2330 train_time:5729ms step_avg:58.46ms
step:99/2330 train_time:5786ms step_avg:58.44ms
step:100/2330 train_time:5847ms step_avg:58.47ms
step:101/2330 train_time:5903ms step_avg:58.45ms
step:102/2330 train_time:5965ms step_avg:58.48ms
step:103/2330 train_time:6022ms step_avg:58.47ms
step:104/2330 train_time:6082ms step_avg:58.48ms
step:105/2330 train_time:6138ms step_avg:58.46ms
step:106/2330 train_time:6199ms step_avg:58.48ms
step:107/2330 train_time:6255ms step_avg:58.46ms
step:108/2330 train_time:6316ms step_avg:58.48ms
step:109/2330 train_time:6372ms step_avg:58.46ms
step:110/2330 train_time:6433ms step_avg:58.48ms
step:111/2330 train_time:6490ms step_avg:58.47ms
step:112/2330 train_time:6551ms step_avg:58.49ms
step:113/2330 train_time:6607ms step_avg:58.47ms
step:114/2330 train_time:6668ms step_avg:58.49ms
step:115/2330 train_time:6725ms step_avg:58.48ms
step:116/2330 train_time:6785ms step_avg:58.49ms
step:117/2330 train_time:6841ms step_avg:58.47ms
step:118/2330 train_time:6901ms step_avg:58.49ms
step:119/2330 train_time:6957ms step_avg:58.47ms
step:120/2330 train_time:7018ms step_avg:58.49ms
step:121/2330 train_time:7074ms step_avg:58.47ms
step:122/2330 train_time:7135ms step_avg:58.48ms
step:123/2330 train_time:7191ms step_avg:58.47ms
step:124/2330 train_time:7252ms step_avg:58.49ms
step:125/2330 train_time:7309ms step_avg:58.47ms
step:126/2330 train_time:7371ms step_avg:58.50ms
step:127/2330 train_time:7427ms step_avg:58.48ms
step:128/2330 train_time:7488ms step_avg:58.50ms
step:129/2330 train_time:7544ms step_avg:58.48ms
step:130/2330 train_time:7605ms step_avg:58.50ms
step:131/2330 train_time:7663ms step_avg:58.49ms
step:132/2330 train_time:7723ms step_avg:58.51ms
step:133/2330 train_time:7779ms step_avg:58.49ms
step:134/2330 train_time:7840ms step_avg:58.51ms
step:135/2330 train_time:7897ms step_avg:58.49ms
step:136/2330 train_time:7957ms step_avg:58.51ms
step:137/2330 train_time:8013ms step_avg:58.49ms
step:138/2330 train_time:8073ms step_avg:58.50ms
step:139/2330 train_time:8129ms step_avg:58.48ms
step:140/2330 train_time:8190ms step_avg:58.50ms
step:141/2330 train_time:8247ms step_avg:58.49ms
step:142/2330 train_time:8308ms step_avg:58.51ms
step:143/2330 train_time:8365ms step_avg:58.50ms
step:144/2330 train_time:8425ms step_avg:58.51ms
step:145/2330 train_time:8481ms step_avg:58.49ms
step:146/2330 train_time:8542ms step_avg:58.51ms
step:147/2330 train_time:8599ms step_avg:58.49ms
step:148/2330 train_time:8659ms step_avg:58.51ms
step:149/2330 train_time:8716ms step_avg:58.49ms
step:150/2330 train_time:8776ms step_avg:58.51ms
step:151/2330 train_time:8832ms step_avg:58.49ms
step:152/2330 train_time:8893ms step_avg:58.51ms
step:153/2330 train_time:8949ms step_avg:58.49ms
step:154/2330 train_time:9010ms step_avg:58.51ms
step:155/2330 train_time:9066ms step_avg:58.49ms
step:156/2330 train_time:9128ms step_avg:58.51ms
step:157/2330 train_time:9184ms step_avg:58.50ms
step:158/2330 train_time:9245ms step_avg:58.51ms
step:159/2330 train_time:9301ms step_avg:58.50ms
step:160/2330 train_time:9363ms step_avg:58.52ms
step:161/2330 train_time:9419ms step_avg:58.50ms
step:162/2330 train_time:9481ms step_avg:58.52ms
step:163/2330 train_time:9537ms step_avg:58.51ms
step:164/2330 train_time:9598ms step_avg:58.52ms
step:165/2330 train_time:9654ms step_avg:58.51ms
step:166/2330 train_time:9715ms step_avg:58.52ms
step:167/2330 train_time:9771ms step_avg:58.51ms
step:168/2330 train_time:9832ms step_avg:58.52ms
step:169/2330 train_time:9888ms step_avg:58.51ms
step:170/2330 train_time:9949ms step_avg:58.52ms
step:171/2330 train_time:10006ms step_avg:58.52ms
step:172/2330 train_time:10066ms step_avg:58.52ms
step:173/2330 train_time:10123ms step_avg:58.51ms
step:174/2330 train_time:10183ms step_avg:58.52ms
step:175/2330 train_time:10239ms step_avg:58.51ms
step:176/2330 train_time:10300ms step_avg:58.52ms
step:177/2330 train_time:10356ms step_avg:58.51ms
step:178/2330 train_time:10416ms step_avg:58.52ms
step:179/2330 train_time:10473ms step_avg:58.51ms
step:180/2330 train_time:10534ms step_avg:58.52ms
step:181/2330 train_time:10590ms step_avg:58.51ms
step:182/2330 train_time:10651ms step_avg:58.52ms
step:183/2330 train_time:10707ms step_avg:58.51ms
step:184/2330 train_time:10769ms step_avg:58.53ms
step:185/2330 train_time:10825ms step_avg:58.52ms
step:186/2330 train_time:10887ms step_avg:58.53ms
step:187/2330 train_time:10943ms step_avg:58.52ms
step:188/2330 train_time:11004ms step_avg:58.53ms
step:189/2330 train_time:11060ms step_avg:58.52ms
step:190/2330 train_time:11121ms step_avg:58.53ms
step:191/2330 train_time:11176ms step_avg:58.51ms
step:192/2330 train_time:11237ms step_avg:58.53ms
step:193/2330 train_time:11294ms step_avg:58.52ms
step:194/2330 train_time:11354ms step_avg:58.53ms
step:195/2330 train_time:11411ms step_avg:58.52ms
step:196/2330 train_time:11471ms step_avg:58.53ms
step:197/2330 train_time:11527ms step_avg:58.51ms
step:198/2330 train_time:11588ms step_avg:58.53ms
step:199/2330 train_time:11645ms step_avg:58.52ms
step:200/2330 train_time:11707ms step_avg:58.53ms
step:201/2330 train_time:11763ms step_avg:58.52ms
step:202/2330 train_time:11823ms step_avg:58.53ms
step:203/2330 train_time:11880ms step_avg:58.52ms
step:204/2330 train_time:11940ms step_avg:58.53ms
step:205/2330 train_time:11997ms step_avg:58.52ms
step:206/2330 train_time:12057ms step_avg:58.53ms
step:207/2330 train_time:12113ms step_avg:58.52ms
step:208/2330 train_time:12173ms step_avg:58.53ms
step:209/2330 train_time:12230ms step_avg:58.51ms
step:210/2330 train_time:12290ms step_avg:58.52ms
step:211/2330 train_time:12346ms step_avg:58.51ms
step:212/2330 train_time:12408ms step_avg:58.53ms
step:213/2330 train_time:12465ms step_avg:58.52ms
step:214/2330 train_time:12526ms step_avg:58.53ms
step:215/2330 train_time:12582ms step_avg:58.52ms
step:216/2330 train_time:12643ms step_avg:58.53ms
step:217/2330 train_time:12699ms step_avg:58.52ms
step:218/2330 train_time:12760ms step_avg:58.53ms
step:219/2330 train_time:12817ms step_avg:58.52ms
step:220/2330 train_time:12877ms step_avg:58.53ms
step:221/2330 train_time:12934ms step_avg:58.52ms
step:222/2330 train_time:12994ms step_avg:58.53ms
step:223/2330 train_time:13051ms step_avg:58.53ms
step:224/2330 train_time:13112ms step_avg:58.54ms
step:225/2330 train_time:13168ms step_avg:58.53ms
step:226/2330 train_time:13230ms step_avg:58.54ms
step:227/2330 train_time:13286ms step_avg:58.53ms
step:228/2330 train_time:13348ms step_avg:58.54ms
step:229/2330 train_time:13404ms step_avg:58.53ms
step:230/2330 train_time:13465ms step_avg:58.54ms
step:231/2330 train_time:13521ms step_avg:58.53ms
step:232/2330 train_time:13583ms step_avg:58.55ms
step:233/2330 train_time:13639ms step_avg:58.53ms
step:234/2330 train_time:13700ms step_avg:58.55ms
step:235/2330 train_time:13756ms step_avg:58.54ms
step:236/2330 train_time:13817ms step_avg:58.54ms
step:237/2330 train_time:13873ms step_avg:58.53ms
step:238/2330 train_time:13933ms step_avg:58.54ms
step:239/2330 train_time:13990ms step_avg:58.54ms
step:240/2330 train_time:14051ms step_avg:58.54ms
step:241/2330 train_time:14107ms step_avg:58.54ms
step:242/2330 train_time:14168ms step_avg:58.54ms
step:243/2330 train_time:14224ms step_avg:58.53ms
step:244/2330 train_time:14285ms step_avg:58.54ms
step:245/2330 train_time:14341ms step_avg:58.54ms
step:246/2330 train_time:14402ms step_avg:58.55ms
step:247/2330 train_time:14458ms step_avg:58.54ms
step:248/2330 train_time:14519ms step_avg:58.55ms
step:249/2330 train_time:14576ms step_avg:58.54ms
step:250/2330 train_time:14636ms step_avg:58.54ms
step:250/2330 val_loss:4.8570 train_time:14713ms step_avg:58.85ms
step:251/2330 train_time:14733ms step_avg:58.70ms
step:252/2330 train_time:14755ms step_avg:58.55ms
step:253/2330 train_time:14811ms step_avg:58.54ms
step:254/2330 train_time:14880ms step_avg:58.58ms
step:255/2330 train_time:14936ms step_avg:58.57ms
step:256/2330 train_time:15004ms step_avg:58.61ms
step:257/2330 train_time:15060ms step_avg:58.60ms
step:258/2330 train_time:15122ms step_avg:58.61ms
step:259/2330 train_time:15178ms step_avg:58.60ms
step:260/2330 train_time:15239ms step_avg:58.61ms
step:261/2330 train_time:15295ms step_avg:58.60ms
step:262/2330 train_time:15355ms step_avg:58.61ms
step:263/2330 train_time:15411ms step_avg:58.60ms
step:264/2330 train_time:15470ms step_avg:58.60ms
step:265/2330 train_time:15526ms step_avg:58.59ms
step:266/2330 train_time:15587ms step_avg:58.60ms
step:267/2330 train_time:15644ms step_avg:58.59ms
step:268/2330 train_time:15704ms step_avg:58.60ms
step:269/2330 train_time:15763ms step_avg:58.60ms
step:270/2330 train_time:15823ms step_avg:58.60ms
step:271/2330 train_time:15880ms step_avg:58.60ms
step:272/2330 train_time:15944ms step_avg:58.62ms
step:273/2330 train_time:16000ms step_avg:58.61ms
step:274/2330 train_time:16063ms step_avg:58.62ms
step:275/2330 train_time:16118ms step_avg:58.61ms
step:276/2330 train_time:16180ms step_avg:58.62ms
step:277/2330 train_time:16236ms step_avg:58.61ms
step:278/2330 train_time:16297ms step_avg:58.62ms
step:279/2330 train_time:16353ms step_avg:58.61ms
step:280/2330 train_time:16413ms step_avg:58.62ms
step:281/2330 train_time:16469ms step_avg:58.61ms
step:282/2330 train_time:16530ms step_avg:58.62ms
step:283/2330 train_time:16586ms step_avg:58.61ms
step:284/2330 train_time:16646ms step_avg:58.61ms
step:285/2330 train_time:16702ms step_avg:58.61ms
step:286/2330 train_time:16763ms step_avg:58.61ms
step:287/2330 train_time:16820ms step_avg:58.61ms
step:288/2330 train_time:16883ms step_avg:58.62ms
step:289/2330 train_time:16940ms step_avg:58.61ms
step:290/2330 train_time:17002ms step_avg:58.63ms
step:291/2330 train_time:17058ms step_avg:58.62ms
step:292/2330 train_time:17120ms step_avg:58.63ms
step:293/2330 train_time:17176ms step_avg:58.62ms
step:294/2330 train_time:17238ms step_avg:58.63ms
step:295/2330 train_time:17295ms step_avg:58.63ms
step:296/2330 train_time:17356ms step_avg:58.63ms
step:297/2330 train_time:17412ms step_avg:58.63ms
step:298/2330 train_time:17473ms step_avg:58.63ms
step:299/2330 train_time:17528ms step_avg:58.62ms
step:300/2330 train_time:17588ms step_avg:58.63ms
step:301/2330 train_time:17644ms step_avg:58.62ms
step:302/2330 train_time:17705ms step_avg:58.63ms
step:303/2330 train_time:17761ms step_avg:58.62ms
step:304/2330 train_time:17823ms step_avg:58.63ms
step:305/2330 train_time:17880ms step_avg:58.62ms
step:306/2330 train_time:17941ms step_avg:58.63ms
step:307/2330 train_time:17997ms step_avg:58.62ms
step:308/2330 train_time:18058ms step_avg:58.63ms
step:309/2330 train_time:18115ms step_avg:58.63ms
step:310/2330 train_time:18176ms step_avg:58.63ms
step:311/2330 train_time:18232ms step_avg:58.62ms
step:312/2330 train_time:18293ms step_avg:58.63ms
step:313/2330 train_time:18349ms step_avg:58.62ms
step:314/2330 train_time:18411ms step_avg:58.63ms
step:315/2330 train_time:18468ms step_avg:58.63ms
step:316/2330 train_time:18527ms step_avg:58.63ms
step:317/2330 train_time:18584ms step_avg:58.62ms
step:318/2330 train_time:18644ms step_avg:58.63ms
step:319/2330 train_time:18700ms step_avg:58.62ms
step:320/2330 train_time:18761ms step_avg:58.63ms
step:321/2330 train_time:18817ms step_avg:58.62ms
step:322/2330 train_time:18879ms step_avg:58.63ms
step:323/2330 train_time:18935ms step_avg:58.62ms
step:324/2330 train_time:18996ms step_avg:58.63ms
step:325/2330 train_time:19053ms step_avg:58.62ms
step:326/2330 train_time:19114ms step_avg:58.63ms
step:327/2330 train_time:19171ms step_avg:58.63ms
step:328/2330 train_time:19232ms step_avg:58.63ms
step:329/2330 train_time:19288ms step_avg:58.63ms
step:330/2330 train_time:19349ms step_avg:58.63ms
step:331/2330 train_time:19405ms step_avg:58.63ms
step:332/2330 train_time:19465ms step_avg:58.63ms
step:333/2330 train_time:19522ms step_avg:58.62ms
step:334/2330 train_time:19583ms step_avg:58.63ms
step:335/2330 train_time:19639ms step_avg:58.62ms
step:336/2330 train_time:19699ms step_avg:58.63ms
step:337/2330 train_time:19755ms step_avg:58.62ms
step:338/2330 train_time:19816ms step_avg:58.63ms
step:339/2330 train_time:19872ms step_avg:58.62ms
step:340/2330 train_time:19934ms step_avg:58.63ms
step:341/2330 train_time:19990ms step_avg:58.62ms
step:342/2330 train_time:20050ms step_avg:58.63ms
step:343/2330 train_time:20106ms step_avg:58.62ms
step:344/2330 train_time:20167ms step_avg:58.63ms
step:345/2330 train_time:20224ms step_avg:58.62ms
step:346/2330 train_time:20285ms step_avg:58.63ms
step:347/2330 train_time:20341ms step_avg:58.62ms
step:348/2330 train_time:20403ms step_avg:58.63ms
step:349/2330 train_time:20459ms step_avg:58.62ms
step:350/2330 train_time:20520ms step_avg:58.63ms
step:351/2330 train_time:20577ms step_avg:58.63ms
step:352/2330 train_time:20637ms step_avg:58.63ms
step:353/2330 train_time:20694ms step_avg:58.62ms
step:354/2330 train_time:20754ms step_avg:58.63ms
step:355/2330 train_time:20811ms step_avg:58.62ms
step:356/2330 train_time:20871ms step_avg:58.63ms
step:357/2330 train_time:20927ms step_avg:58.62ms
step:358/2330 train_time:20988ms step_avg:58.63ms
step:359/2330 train_time:21045ms step_avg:58.62ms
step:360/2330 train_time:21105ms step_avg:58.63ms
step:361/2330 train_time:21162ms step_avg:58.62ms
step:362/2330 train_time:21224ms step_avg:58.63ms
step:363/2330 train_time:21280ms step_avg:58.62ms
step:364/2330 train_time:21342ms step_avg:58.63ms
step:365/2330 train_time:21399ms step_avg:58.63ms
step:366/2330 train_time:21460ms step_avg:58.63ms
step:367/2330 train_time:21516ms step_avg:58.63ms
step:368/2330 train_time:21578ms step_avg:58.64ms
step:369/2330 train_time:21635ms step_avg:58.63ms
step:370/2330 train_time:21695ms step_avg:58.64ms
step:371/2330 train_time:21752ms step_avg:58.63ms
step:372/2330 train_time:21812ms step_avg:58.64ms
step:373/2330 train_time:21869ms step_avg:58.63ms
step:374/2330 train_time:21930ms step_avg:58.64ms
step:375/2330 train_time:21986ms step_avg:58.63ms
step:376/2330 train_time:22046ms step_avg:58.63ms
step:377/2330 train_time:22102ms step_avg:58.63ms
step:378/2330 train_time:22163ms step_avg:58.63ms
step:379/2330 train_time:22220ms step_avg:58.63ms
step:380/2330 train_time:22282ms step_avg:58.64ms
step:381/2330 train_time:22338ms step_avg:58.63ms
step:382/2330 train_time:22400ms step_avg:58.64ms
step:383/2330 train_time:22457ms step_avg:58.64ms
step:384/2330 train_time:22518ms step_avg:58.64ms
step:385/2330 train_time:22574ms step_avg:58.63ms
step:386/2330 train_time:22635ms step_avg:58.64ms
step:387/2330 train_time:22692ms step_avg:58.63ms
step:388/2330 train_time:22753ms step_avg:58.64ms
step:389/2330 train_time:22809ms step_avg:58.64ms
step:390/2330 train_time:22870ms step_avg:58.64ms
step:391/2330 train_time:22927ms step_avg:58.64ms
step:392/2330 train_time:22988ms step_avg:58.64ms
step:393/2330 train_time:23044ms step_avg:58.64ms
step:394/2330 train_time:23105ms step_avg:58.64ms
step:395/2330 train_time:23161ms step_avg:58.64ms
step:396/2330 train_time:23223ms step_avg:58.64ms
step:397/2330 train_time:23279ms step_avg:58.64ms
step:398/2330 train_time:23341ms step_avg:58.64ms
step:399/2330 train_time:23397ms step_avg:58.64ms
step:400/2330 train_time:23460ms step_avg:58.65ms
step:401/2330 train_time:23516ms step_avg:58.64ms
step:402/2330 train_time:23577ms step_avg:58.65ms
step:403/2330 train_time:23634ms step_avg:58.65ms
step:404/2330 train_time:23697ms step_avg:58.65ms
step:405/2330 train_time:23754ms step_avg:58.65ms
step:406/2330 train_time:23815ms step_avg:58.66ms
step:407/2330 train_time:23871ms step_avg:58.65ms
step:408/2330 train_time:23933ms step_avg:58.66ms
step:409/2330 train_time:23990ms step_avg:58.65ms
step:410/2330 train_time:24050ms step_avg:58.66ms
step:411/2330 train_time:24106ms step_avg:58.65ms
step:412/2330 train_time:24166ms step_avg:58.66ms
step:413/2330 train_time:24222ms step_avg:58.65ms
step:414/2330 train_time:24283ms step_avg:58.65ms
step:415/2330 train_time:24340ms step_avg:58.65ms
step:416/2330 train_time:24401ms step_avg:58.66ms
step:417/2330 train_time:24457ms step_avg:58.65ms
step:418/2330 train_time:24518ms step_avg:58.66ms
step:419/2330 train_time:24575ms step_avg:58.65ms
step:420/2330 train_time:24636ms step_avg:58.66ms
step:421/2330 train_time:24693ms step_avg:58.65ms
step:422/2330 train_time:24755ms step_avg:58.66ms
step:423/2330 train_time:24811ms step_avg:58.65ms
step:424/2330 train_time:24872ms step_avg:58.66ms
step:425/2330 train_time:24929ms step_avg:58.66ms
step:426/2330 train_time:24990ms step_avg:58.66ms
step:427/2330 train_time:25047ms step_avg:58.66ms
step:428/2330 train_time:25106ms step_avg:58.66ms
step:429/2330 train_time:25162ms step_avg:58.65ms
step:430/2330 train_time:25223ms step_avg:58.66ms
step:431/2330 train_time:25280ms step_avg:58.65ms
step:432/2330 train_time:25342ms step_avg:58.66ms
step:433/2330 train_time:25398ms step_avg:58.66ms
step:434/2330 train_time:25459ms step_avg:58.66ms
step:435/2330 train_time:25515ms step_avg:58.66ms
step:436/2330 train_time:25576ms step_avg:58.66ms
step:437/2330 train_time:25633ms step_avg:58.66ms
step:438/2330 train_time:25694ms step_avg:58.66ms
step:439/2330 train_time:25751ms step_avg:58.66ms
step:440/2330 train_time:25812ms step_avg:58.66ms
step:441/2330 train_time:25868ms step_avg:58.66ms
step:442/2330 train_time:25930ms step_avg:58.67ms
step:443/2330 train_time:25987ms step_avg:58.66ms
step:444/2330 train_time:26048ms step_avg:58.67ms
step:445/2330 train_time:26104ms step_avg:58.66ms
step:446/2330 train_time:26165ms step_avg:58.67ms
step:447/2330 train_time:26221ms step_avg:58.66ms
step:448/2330 train_time:26282ms step_avg:58.67ms
step:449/2330 train_time:26338ms step_avg:58.66ms
step:450/2330 train_time:26401ms step_avg:58.67ms
step:451/2330 train_time:26457ms step_avg:58.66ms
step:452/2330 train_time:26518ms step_avg:58.67ms
step:453/2330 train_time:26575ms step_avg:58.67ms
step:454/2330 train_time:26636ms step_avg:58.67ms
step:455/2330 train_time:26693ms step_avg:58.67ms
step:456/2330 train_time:26754ms step_avg:58.67ms
step:457/2330 train_time:26811ms step_avg:58.67ms
step:458/2330 train_time:26872ms step_avg:58.67ms
step:459/2330 train_time:26928ms step_avg:58.67ms
step:460/2330 train_time:26990ms step_avg:58.67ms
step:461/2330 train_time:27046ms step_avg:58.67ms
step:462/2330 train_time:27106ms step_avg:58.67ms
step:463/2330 train_time:27163ms step_avg:58.67ms
step:464/2330 train_time:27224ms step_avg:58.67ms
step:465/2330 train_time:27280ms step_avg:58.67ms
step:466/2330 train_time:27341ms step_avg:58.67ms
step:467/2330 train_time:27397ms step_avg:58.67ms
step:468/2330 train_time:27459ms step_avg:58.67ms
step:469/2330 train_time:27515ms step_avg:58.67ms
step:470/2330 train_time:27577ms step_avg:58.67ms
step:471/2330 train_time:27633ms step_avg:58.67ms
step:472/2330 train_time:27695ms step_avg:58.68ms
step:473/2330 train_time:27752ms step_avg:58.67ms
step:474/2330 train_time:27814ms step_avg:58.68ms
step:475/2330 train_time:27871ms step_avg:58.68ms
step:476/2330 train_time:27932ms step_avg:58.68ms
step:477/2330 train_time:27988ms step_avg:58.67ms
step:478/2330 train_time:28049ms step_avg:58.68ms
step:479/2330 train_time:28105ms step_avg:58.67ms
step:480/2330 train_time:28165ms step_avg:58.68ms
step:481/2330 train_time:28221ms step_avg:58.67ms
step:482/2330 train_time:28283ms step_avg:58.68ms
step:483/2330 train_time:28339ms step_avg:58.67ms
step:484/2330 train_time:28400ms step_avg:58.68ms
step:485/2330 train_time:28457ms step_avg:58.68ms
step:486/2330 train_time:28518ms step_avg:58.68ms
step:487/2330 train_time:28574ms step_avg:58.67ms
step:488/2330 train_time:28635ms step_avg:58.68ms
step:489/2330 train_time:28691ms step_avg:58.67ms
step:490/2330 train_time:28753ms step_avg:58.68ms
step:491/2330 train_time:28809ms step_avg:58.67ms
step:492/2330 train_time:28870ms step_avg:58.68ms
step:493/2330 train_time:28926ms step_avg:58.67ms
step:494/2330 train_time:28988ms step_avg:58.68ms
step:495/2330 train_time:29044ms step_avg:58.68ms
step:496/2330 train_time:29105ms step_avg:58.68ms
step:497/2330 train_time:29161ms step_avg:58.67ms
step:498/2330 train_time:29223ms step_avg:58.68ms
step:499/2330 train_time:29279ms step_avg:58.68ms
step:500/2330 train_time:29340ms step_avg:58.68ms
step:500/2330 val_loss:4.4108 train_time:29418ms step_avg:58.84ms
step:501/2330 train_time:29437ms step_avg:58.76ms
step:502/2330 train_time:29460ms step_avg:58.68ms
step:503/2330 train_time:29516ms step_avg:58.68ms
step:504/2330 train_time:29581ms step_avg:58.69ms
step:505/2330 train_time:29637ms step_avg:58.69ms
step:506/2330 train_time:29701ms step_avg:58.70ms
step:507/2330 train_time:29758ms step_avg:58.69ms
step:508/2330 train_time:29818ms step_avg:58.70ms
step:509/2330 train_time:29874ms step_avg:58.69ms
step:510/2330 train_time:29935ms step_avg:58.70ms
step:511/2330 train_time:29991ms step_avg:58.69ms
step:512/2330 train_time:30051ms step_avg:58.69ms
step:513/2330 train_time:30107ms step_avg:58.69ms
step:514/2330 train_time:30167ms step_avg:58.69ms
step:515/2330 train_time:30223ms step_avg:58.69ms
step:516/2330 train_time:30284ms step_avg:58.69ms
step:517/2330 train_time:30340ms step_avg:58.69ms
step:518/2330 train_time:30402ms step_avg:58.69ms
step:519/2330 train_time:30459ms step_avg:58.69ms
step:520/2330 train_time:30522ms step_avg:58.70ms
step:521/2330 train_time:30580ms step_avg:58.69ms
step:522/2330 train_time:30642ms step_avg:58.70ms
step:523/2330 train_time:30699ms step_avg:58.70ms
step:524/2330 train_time:30761ms step_avg:58.70ms
step:525/2330 train_time:30818ms step_avg:58.70ms
step:526/2330 train_time:30878ms step_avg:58.70ms
step:527/2330 train_time:30934ms step_avg:58.70ms
step:528/2330 train_time:30995ms step_avg:58.70ms
step:529/2330 train_time:31052ms step_avg:58.70ms
step:530/2330 train_time:31112ms step_avg:58.70ms
step:531/2330 train_time:31168ms step_avg:58.70ms
step:532/2330 train_time:31228ms step_avg:58.70ms
step:533/2330 train_time:31284ms step_avg:58.69ms
step:534/2330 train_time:31345ms step_avg:58.70ms
step:535/2330 train_time:31402ms step_avg:58.69ms
step:536/2330 train_time:31464ms step_avg:58.70ms
step:537/2330 train_time:31522ms step_avg:58.70ms
step:538/2330 train_time:31583ms step_avg:58.71ms
step:539/2330 train_time:31640ms step_avg:58.70ms
step:540/2330 train_time:31702ms step_avg:58.71ms
step:541/2330 train_time:31759ms step_avg:58.70ms
step:542/2330 train_time:31821ms step_avg:58.71ms
step:543/2330 train_time:31878ms step_avg:58.71ms
step:544/2330 train_time:31939ms step_avg:58.71ms
step:545/2330 train_time:31996ms step_avg:58.71ms
step:546/2330 train_time:32057ms step_avg:58.71ms
step:547/2330 train_time:32113ms step_avg:58.71ms
step:548/2330 train_time:32173ms step_avg:58.71ms
step:549/2330 train_time:32229ms step_avg:58.71ms
step:550/2330 train_time:32290ms step_avg:58.71ms
step:551/2330 train_time:32346ms step_avg:58.70ms
step:552/2330 train_time:32407ms step_avg:58.71ms
step:553/2330 train_time:32463ms step_avg:58.70ms
step:554/2330 train_time:32526ms step_avg:58.71ms
step:555/2330 train_time:32582ms step_avg:58.71ms
step:556/2330 train_time:32644ms step_avg:58.71ms
step:557/2330 train_time:32700ms step_avg:58.71ms
step:558/2330 train_time:32762ms step_avg:58.71ms
step:559/2330 train_time:32820ms step_avg:58.71ms
step:560/2330 train_time:32882ms step_avg:58.72ms
step:561/2330 train_time:32939ms step_avg:58.71ms
step:562/2330 train_time:33001ms step_avg:58.72ms
step:563/2330 train_time:33057ms step_avg:58.72ms
step:564/2330 train_time:33118ms step_avg:58.72ms
step:565/2330 train_time:33175ms step_avg:58.72ms
step:566/2330 train_time:33235ms step_avg:58.72ms
step:567/2330 train_time:33291ms step_avg:58.71ms
step:568/2330 train_time:33352ms step_avg:58.72ms
step:569/2330 train_time:33408ms step_avg:58.71ms
step:570/2330 train_time:33469ms step_avg:58.72ms
step:571/2330 train_time:33525ms step_avg:58.71ms
step:572/2330 train_time:33586ms step_avg:58.72ms
step:573/2330 train_time:33643ms step_avg:58.71ms
step:574/2330 train_time:33704ms step_avg:58.72ms
step:575/2330 train_time:33761ms step_avg:58.71ms
step:576/2330 train_time:33824ms step_avg:58.72ms
step:577/2330 train_time:33881ms step_avg:58.72ms
step:578/2330 train_time:33942ms step_avg:58.72ms
step:579/2330 train_time:33999ms step_avg:58.72ms
step:580/2330 train_time:34061ms step_avg:58.73ms
step:581/2330 train_time:34118ms step_avg:58.72ms
step:582/2330 train_time:34180ms step_avg:58.73ms
step:583/2330 train_time:34237ms step_avg:58.72ms
step:584/2330 train_time:34298ms step_avg:58.73ms
step:585/2330 train_time:34354ms step_avg:58.72ms
step:586/2330 train_time:34415ms step_avg:58.73ms
step:587/2330 train_time:34471ms step_avg:58.72ms
step:588/2330 train_time:34532ms step_avg:58.73ms
step:589/2330 train_time:34589ms step_avg:58.72ms
step:590/2330 train_time:34650ms step_avg:58.73ms
step:591/2330 train_time:34706ms step_avg:58.72ms
step:592/2330 train_time:34768ms step_avg:58.73ms
step:593/2330 train_time:34825ms step_avg:58.73ms
step:594/2330 train_time:34886ms step_avg:58.73ms
step:595/2330 train_time:34943ms step_avg:58.73ms
step:596/2330 train_time:35004ms step_avg:58.73ms
step:597/2330 train_time:35061ms step_avg:58.73ms
step:598/2330 train_time:35125ms step_avg:58.74ms
step:599/2330 train_time:35182ms step_avg:58.73ms
step:600/2330 train_time:35243ms step_avg:58.74ms
step:601/2330 train_time:35300ms step_avg:58.74ms
step:602/2330 train_time:35361ms step_avg:58.74ms
step:603/2330 train_time:35418ms step_avg:58.74ms
step:604/2330 train_time:35479ms step_avg:58.74ms
step:605/2330 train_time:35536ms step_avg:58.74ms
step:606/2330 train_time:35597ms step_avg:58.74ms
step:607/2330 train_time:35653ms step_avg:58.74ms
step:608/2330 train_time:35714ms step_avg:58.74ms
step:609/2330 train_time:35771ms step_avg:58.74ms
step:610/2330 train_time:35832ms step_avg:58.74ms
step:611/2330 train_time:35888ms step_avg:58.74ms
step:612/2330 train_time:35949ms step_avg:58.74ms
step:613/2330 train_time:36005ms step_avg:58.74ms
step:614/2330 train_time:36067ms step_avg:58.74ms
step:615/2330 train_time:36124ms step_avg:58.74ms
step:616/2330 train_time:36186ms step_avg:58.74ms
step:617/2330 train_time:36243ms step_avg:58.74ms
step:618/2330 train_time:36304ms step_avg:58.74ms
step:619/2330 train_time:36361ms step_avg:58.74ms
step:620/2330 train_time:36423ms step_avg:58.75ms
step:621/2330 train_time:36480ms step_avg:58.74ms
step:622/2330 train_time:36543ms step_avg:58.75ms
step:623/2330 train_time:36600ms step_avg:58.75ms
step:624/2330 train_time:36661ms step_avg:58.75ms
step:625/2330 train_time:36718ms step_avg:58.75ms
step:626/2330 train_time:36780ms step_avg:58.75ms
step:627/2330 train_time:36837ms step_avg:58.75ms
step:628/2330 train_time:36898ms step_avg:58.75ms
step:629/2330 train_time:36954ms step_avg:58.75ms
step:630/2330 train_time:37014ms step_avg:58.75ms
step:631/2330 train_time:37071ms step_avg:58.75ms
step:632/2330 train_time:37133ms step_avg:58.75ms
step:633/2330 train_time:37189ms step_avg:58.75ms
step:634/2330 train_time:37251ms step_avg:58.76ms
step:635/2330 train_time:37307ms step_avg:58.75ms
step:636/2330 train_time:37369ms step_avg:58.76ms
step:637/2330 train_time:37425ms step_avg:58.75ms
step:638/2330 train_time:37488ms step_avg:58.76ms
step:639/2330 train_time:37544ms step_avg:58.75ms
step:640/2330 train_time:37606ms step_avg:58.76ms
step:641/2330 train_time:37662ms step_avg:58.76ms
step:642/2330 train_time:37726ms step_avg:58.76ms
step:643/2330 train_time:37782ms step_avg:58.76ms
step:644/2330 train_time:37844ms step_avg:58.76ms
step:645/2330 train_time:37901ms step_avg:58.76ms
step:646/2330 train_time:37963ms step_avg:58.77ms
step:647/2330 train_time:38020ms step_avg:58.76ms
step:648/2330 train_time:38081ms step_avg:58.77ms
step:649/2330 train_time:38138ms step_avg:58.76ms
step:650/2330 train_time:38199ms step_avg:58.77ms
step:651/2330 train_time:38256ms step_avg:58.76ms
step:652/2330 train_time:38316ms step_avg:58.77ms
step:653/2330 train_time:38372ms step_avg:58.76ms
step:654/2330 train_time:38433ms step_avg:58.77ms
step:655/2330 train_time:38489ms step_avg:58.76ms
step:656/2330 train_time:38550ms step_avg:58.77ms
step:657/2330 train_time:38606ms step_avg:58.76ms
step:658/2330 train_time:38669ms step_avg:58.77ms
step:659/2330 train_time:38725ms step_avg:58.76ms
step:660/2330 train_time:38787ms step_avg:58.77ms
step:661/2330 train_time:38843ms step_avg:58.76ms
step:662/2330 train_time:38906ms step_avg:58.77ms
step:663/2330 train_time:38962ms step_avg:58.77ms
step:664/2330 train_time:39024ms step_avg:58.77ms
step:665/2330 train_time:39081ms step_avg:58.77ms
step:666/2330 train_time:39144ms step_avg:58.77ms
step:667/2330 train_time:39200ms step_avg:58.77ms
step:668/2330 train_time:39261ms step_avg:58.77ms
step:669/2330 train_time:39318ms step_avg:58.77ms
step:670/2330 train_time:39380ms step_avg:58.78ms
step:671/2330 train_time:39437ms step_avg:58.77ms
step:672/2330 train_time:39500ms step_avg:58.78ms
step:673/2330 train_time:39556ms step_avg:58.78ms
step:674/2330 train_time:39618ms step_avg:58.78ms
step:675/2330 train_time:39674ms step_avg:58.78ms
step:676/2330 train_time:39736ms step_avg:58.78ms
step:677/2330 train_time:39792ms step_avg:58.78ms
step:678/2330 train_time:39852ms step_avg:58.78ms
step:679/2330 train_time:39909ms step_avg:58.78ms
step:680/2330 train_time:39970ms step_avg:58.78ms
step:681/2330 train_time:40026ms step_avg:58.78ms
step:682/2330 train_time:40088ms step_avg:58.78ms
step:683/2330 train_time:40144ms step_avg:58.78ms
step:684/2330 train_time:40206ms step_avg:58.78ms
step:685/2330 train_time:40263ms step_avg:58.78ms
step:686/2330 train_time:40326ms step_avg:58.78ms
step:687/2330 train_time:40382ms step_avg:58.78ms
step:688/2330 train_time:40444ms step_avg:58.79ms
step:689/2330 train_time:40501ms step_avg:58.78ms
step:690/2330 train_time:40562ms step_avg:58.79ms
step:691/2330 train_time:40620ms step_avg:58.78ms
step:692/2330 train_time:40681ms step_avg:58.79ms
step:693/2330 train_time:40738ms step_avg:58.79ms
step:694/2330 train_time:40799ms step_avg:58.79ms
step:695/2330 train_time:40855ms step_avg:58.78ms
step:696/2330 train_time:40916ms step_avg:58.79ms
step:697/2330 train_time:40972ms step_avg:58.78ms
step:698/2330 train_time:41033ms step_avg:58.79ms
step:699/2330 train_time:41089ms step_avg:58.78ms
step:700/2330 train_time:41149ms step_avg:58.78ms
step:701/2330 train_time:41205ms step_avg:58.78ms
step:702/2330 train_time:41267ms step_avg:58.79ms
step:703/2330 train_time:41324ms step_avg:58.78ms
step:704/2330 train_time:41386ms step_avg:58.79ms
step:705/2330 train_time:41443ms step_avg:58.78ms
step:706/2330 train_time:41504ms step_avg:58.79ms
step:707/2330 train_time:41561ms step_avg:58.79ms
step:708/2330 train_time:41624ms step_avg:58.79ms
step:709/2330 train_time:41681ms step_avg:58.79ms
step:710/2330 train_time:41743ms step_avg:58.79ms
step:711/2330 train_time:41800ms step_avg:58.79ms
step:712/2330 train_time:41862ms step_avg:58.79ms
step:713/2330 train_time:41919ms step_avg:58.79ms
step:714/2330 train_time:41979ms step_avg:58.79ms
step:715/2330 train_time:42036ms step_avg:58.79ms
step:716/2330 train_time:42097ms step_avg:58.79ms
step:717/2330 train_time:42153ms step_avg:58.79ms
step:718/2330 train_time:42213ms step_avg:58.79ms
step:719/2330 train_time:42270ms step_avg:58.79ms
step:720/2330 train_time:42331ms step_avg:58.79ms
step:721/2330 train_time:42387ms step_avg:58.79ms
step:722/2330 train_time:42448ms step_avg:58.79ms
step:723/2330 train_time:42504ms step_avg:58.79ms
step:724/2330 train_time:42566ms step_avg:58.79ms
step:725/2330 train_time:42622ms step_avg:58.79ms
step:726/2330 train_time:42685ms step_avg:58.79ms
step:727/2330 train_time:42741ms step_avg:58.79ms
step:728/2330 train_time:42804ms step_avg:58.80ms
step:729/2330 train_time:42861ms step_avg:58.79ms
step:730/2330 train_time:42922ms step_avg:58.80ms
step:731/2330 train_time:42979ms step_avg:58.79ms
step:732/2330 train_time:43041ms step_avg:58.80ms
step:733/2330 train_time:43098ms step_avg:58.80ms
step:734/2330 train_time:43159ms step_avg:58.80ms
step:735/2330 train_time:43216ms step_avg:58.80ms
step:736/2330 train_time:43277ms step_avg:58.80ms
step:737/2330 train_time:43333ms step_avg:58.80ms
step:738/2330 train_time:43395ms step_avg:58.80ms
step:739/2330 train_time:43451ms step_avg:58.80ms
step:740/2330 train_time:43512ms step_avg:58.80ms
step:741/2330 train_time:43569ms step_avg:58.80ms
step:742/2330 train_time:43630ms step_avg:58.80ms
step:743/2330 train_time:43686ms step_avg:58.80ms
step:744/2330 train_time:43748ms step_avg:58.80ms
step:745/2330 train_time:43805ms step_avg:58.80ms
step:746/2330 train_time:43867ms step_avg:58.80ms
step:747/2330 train_time:43924ms step_avg:58.80ms
step:748/2330 train_time:43986ms step_avg:58.80ms
step:749/2330 train_time:44043ms step_avg:58.80ms
step:750/2330 train_time:44104ms step_avg:58.81ms
step:750/2330 val_loss:4.2126 train_time:44182ms step_avg:58.91ms
step:751/2330 train_time:44203ms step_avg:58.86ms
step:752/2330 train_time:44226ms step_avg:58.81ms
step:753/2330 train_time:44282ms step_avg:58.81ms
step:754/2330 train_time:44347ms step_avg:58.82ms
step:755/2330 train_time:44404ms step_avg:58.81ms
step:756/2330 train_time:44467ms step_avg:58.82ms
step:757/2330 train_time:44523ms step_avg:58.82ms
step:758/2330 train_time:44585ms step_avg:58.82ms
step:759/2330 train_time:44641ms step_avg:58.82ms
step:760/2330 train_time:44701ms step_avg:58.82ms
step:761/2330 train_time:44757ms step_avg:58.81ms
step:762/2330 train_time:44817ms step_avg:58.81ms
step:763/2330 train_time:44873ms step_avg:58.81ms
step:764/2330 train_time:44933ms step_avg:58.81ms
step:765/2330 train_time:44990ms step_avg:58.81ms
step:766/2330 train_time:45051ms step_avg:58.81ms
step:767/2330 train_time:45108ms step_avg:58.81ms
step:768/2330 train_time:45169ms step_avg:58.81ms
step:769/2330 train_time:45228ms step_avg:58.81ms
step:770/2330 train_time:45291ms step_avg:58.82ms
step:771/2330 train_time:45350ms step_avg:58.82ms
step:772/2330 train_time:45413ms step_avg:58.83ms
step:773/2330 train_time:45473ms step_avg:58.83ms
step:774/2330 train_time:45534ms step_avg:58.83ms
step:775/2330 train_time:45591ms step_avg:58.83ms
step:776/2330 train_time:45653ms step_avg:58.83ms
step:777/2330 train_time:45711ms step_avg:58.83ms
step:778/2330 train_time:45773ms step_avg:58.83ms
step:779/2330 train_time:45831ms step_avg:58.83ms
step:780/2330 train_time:45892ms step_avg:58.84ms
step:781/2330 train_time:45949ms step_avg:58.83ms
step:782/2330 train_time:46011ms step_avg:58.84ms
step:783/2330 train_time:46069ms step_avg:58.84ms
step:784/2330 train_time:46130ms step_avg:58.84ms
step:785/2330 train_time:46187ms step_avg:58.84ms
step:786/2330 train_time:46250ms step_avg:58.84ms
step:787/2330 train_time:46308ms step_avg:58.84ms
step:788/2330 train_time:46370ms step_avg:58.85ms
step:789/2330 train_time:46429ms step_avg:58.85ms
step:790/2330 train_time:46491ms step_avg:58.85ms
step:791/2330 train_time:46549ms step_avg:58.85ms
step:792/2330 train_time:46610ms step_avg:58.85ms
step:793/2330 train_time:46669ms step_avg:58.85ms
step:794/2330 train_time:46730ms step_avg:58.85ms
step:795/2330 train_time:46787ms step_avg:58.85ms
step:796/2330 train_time:46848ms step_avg:58.85ms
step:797/2330 train_time:46905ms step_avg:58.85ms
step:798/2330 train_time:46966ms step_avg:58.86ms
step:799/2330 train_time:47024ms step_avg:58.85ms
step:800/2330 train_time:47085ms step_avg:58.86ms
step:801/2330 train_time:47142ms step_avg:58.85ms
step:802/2330 train_time:47202ms step_avg:58.86ms
step:803/2330 train_time:47259ms step_avg:58.85ms
step:804/2330 train_time:47322ms step_avg:58.86ms
step:805/2330 train_time:47379ms step_avg:58.86ms
step:806/2330 train_time:47441ms step_avg:58.86ms
step:807/2330 train_time:47498ms step_avg:58.86ms
step:808/2330 train_time:47561ms step_avg:58.86ms
step:809/2330 train_time:47618ms step_avg:58.86ms
step:810/2330 train_time:47680ms step_avg:58.86ms
step:811/2330 train_time:47737ms step_avg:58.86ms
step:812/2330 train_time:47799ms step_avg:58.87ms
step:813/2330 train_time:47856ms step_avg:58.86ms
step:814/2330 train_time:47918ms step_avg:58.87ms
step:815/2330 train_time:47975ms step_avg:58.87ms
step:816/2330 train_time:48037ms step_avg:58.87ms
step:817/2330 train_time:48094ms step_avg:58.87ms
step:818/2330 train_time:48156ms step_avg:58.87ms
step:819/2330 train_time:48213ms step_avg:58.87ms
step:820/2330 train_time:48277ms step_avg:58.87ms
step:821/2330 train_time:48335ms step_avg:58.87ms
step:822/2330 train_time:48397ms step_avg:58.88ms
step:823/2330 train_time:48455ms step_avg:58.88ms
step:824/2330 train_time:48517ms step_avg:58.88ms
step:825/2330 train_time:48575ms step_avg:58.88ms
step:826/2330 train_time:48637ms step_avg:58.88ms
step:827/2330 train_time:48695ms step_avg:58.88ms
step:828/2330 train_time:48757ms step_avg:58.89ms
step:829/2330 train_time:48815ms step_avg:58.88ms
step:830/2330 train_time:48876ms step_avg:58.89ms
step:831/2330 train_time:48934ms step_avg:58.89ms
step:832/2330 train_time:48995ms step_avg:58.89ms
step:833/2330 train_time:49052ms step_avg:58.89ms
step:834/2330 train_time:49114ms step_avg:58.89ms
step:835/2330 train_time:49171ms step_avg:58.89ms
step:836/2330 train_time:49234ms step_avg:58.89ms
step:837/2330 train_time:49291ms step_avg:58.89ms
step:838/2330 train_time:49353ms step_avg:58.89ms
step:839/2330 train_time:49411ms step_avg:58.89ms
step:840/2330 train_time:49473ms step_avg:58.90ms
step:841/2330 train_time:49531ms step_avg:58.89ms
step:842/2330 train_time:49593ms step_avg:58.90ms
step:843/2330 train_time:49651ms step_avg:58.90ms
step:844/2330 train_time:49713ms step_avg:58.90ms
step:845/2330 train_time:49770ms step_avg:58.90ms
step:846/2330 train_time:49832ms step_avg:58.90ms
step:847/2330 train_time:49890ms step_avg:58.90ms
step:848/2330 train_time:49952ms step_avg:58.91ms
step:849/2330 train_time:50009ms step_avg:58.90ms
step:850/2330 train_time:50072ms step_avg:58.91ms
step:851/2330 train_time:50130ms step_avg:58.91ms
step:852/2330 train_time:50191ms step_avg:58.91ms
step:853/2330 train_time:50249ms step_avg:58.91ms
step:854/2330 train_time:50310ms step_avg:58.91ms
step:855/2330 train_time:50368ms step_avg:58.91ms
step:856/2330 train_time:50430ms step_avg:58.91ms
step:857/2330 train_time:50488ms step_avg:58.91ms
step:858/2330 train_time:50550ms step_avg:58.92ms
step:859/2330 train_time:50608ms step_avg:58.91ms
step:860/2330 train_time:50670ms step_avg:58.92ms
step:861/2330 train_time:50728ms step_avg:58.92ms
step:862/2330 train_time:50790ms step_avg:58.92ms
step:863/2330 train_time:50847ms step_avg:58.92ms
step:864/2330 train_time:50909ms step_avg:58.92ms
step:865/2330 train_time:50966ms step_avg:58.92ms
step:866/2330 train_time:51028ms step_avg:58.92ms
step:867/2330 train_time:51086ms step_avg:58.92ms
step:868/2330 train_time:51147ms step_avg:58.92ms
step:869/2330 train_time:51204ms step_avg:58.92ms
step:870/2330 train_time:51265ms step_avg:58.93ms
step:871/2330 train_time:51323ms step_avg:58.92ms
step:872/2330 train_time:51384ms step_avg:58.93ms
step:873/2330 train_time:51442ms step_avg:58.93ms
step:874/2330 train_time:51502ms step_avg:58.93ms
step:875/2330 train_time:51560ms step_avg:58.93ms
step:876/2330 train_time:51621ms step_avg:58.93ms
step:877/2330 train_time:51678ms step_avg:58.93ms
step:878/2330 train_time:51741ms step_avg:58.93ms
step:879/2330 train_time:51797ms step_avg:58.93ms
step:880/2330 train_time:51860ms step_avg:58.93ms
step:881/2330 train_time:51917ms step_avg:58.93ms
step:882/2330 train_time:51980ms step_avg:58.93ms
step:883/2330 train_time:52036ms step_avg:58.93ms
step:884/2330 train_time:52099ms step_avg:58.94ms
step:885/2330 train_time:52156ms step_avg:58.93ms
step:886/2330 train_time:52218ms step_avg:58.94ms
step:887/2330 train_time:52275ms step_avg:58.93ms
step:888/2330 train_time:52338ms step_avg:58.94ms
step:889/2330 train_time:52395ms step_avg:58.94ms
step:890/2330 train_time:52457ms step_avg:58.94ms
step:891/2330 train_time:52514ms step_avg:58.94ms
step:892/2330 train_time:52577ms step_avg:58.94ms
step:893/2330 train_time:52634ms step_avg:58.94ms
step:894/2330 train_time:52697ms step_avg:58.95ms
step:895/2330 train_time:52755ms step_avg:58.94ms
step:896/2330 train_time:52816ms step_avg:58.95ms
step:897/2330 train_time:52874ms step_avg:58.94ms
step:898/2330 train_time:52936ms step_avg:58.95ms
step:899/2330 train_time:52993ms step_avg:58.95ms
step:900/2330 train_time:53055ms step_avg:58.95ms
step:901/2330 train_time:53112ms step_avg:58.95ms
step:902/2330 train_time:53174ms step_avg:58.95ms
step:903/2330 train_time:53232ms step_avg:58.95ms
step:904/2330 train_time:53293ms step_avg:58.95ms
step:905/2330 train_time:53350ms step_avg:58.95ms
step:906/2330 train_time:53412ms step_avg:58.95ms
step:907/2330 train_time:53470ms step_avg:58.95ms
step:908/2330 train_time:53532ms step_avg:58.96ms
step:909/2330 train_time:53590ms step_avg:58.95ms
step:910/2330 train_time:53652ms step_avg:58.96ms
step:911/2330 train_time:53710ms step_avg:58.96ms
step:912/2330 train_time:53772ms step_avg:58.96ms
step:913/2330 train_time:53829ms step_avg:58.96ms
step:914/2330 train_time:53891ms step_avg:58.96ms
step:915/2330 train_time:53949ms step_avg:58.96ms
step:916/2330 train_time:54009ms step_avg:58.96ms
step:917/2330 train_time:54066ms step_avg:58.96ms
step:918/2330 train_time:54128ms step_avg:58.96ms
step:919/2330 train_time:54186ms step_avg:58.96ms
step:920/2330 train_time:54248ms step_avg:58.97ms
step:921/2330 train_time:54306ms step_avg:58.96ms
step:922/2330 train_time:54367ms step_avg:58.97ms
step:923/2330 train_time:54424ms step_avg:58.96ms
step:924/2330 train_time:54486ms step_avg:58.97ms
step:925/2330 train_time:54543ms step_avg:58.97ms
step:926/2330 train_time:54605ms step_avg:58.97ms
step:927/2330 train_time:54662ms step_avg:58.97ms
step:928/2330 train_time:54724ms step_avg:58.97ms
step:929/2330 train_time:54781ms step_avg:58.97ms
step:930/2330 train_time:54843ms step_avg:58.97ms
step:931/2330 train_time:54900ms step_avg:58.97ms
step:932/2330 train_time:54962ms step_avg:58.97ms
step:933/2330 train_time:55019ms step_avg:58.97ms
step:934/2330 train_time:55081ms step_avg:58.97ms
step:935/2330 train_time:55138ms step_avg:58.97ms
step:936/2330 train_time:55201ms step_avg:58.98ms
step:937/2330 train_time:55258ms step_avg:58.97ms
step:938/2330 train_time:55319ms step_avg:58.98ms
step:939/2330 train_time:55376ms step_avg:58.97ms
step:940/2330 train_time:55439ms step_avg:58.98ms
step:941/2330 train_time:55495ms step_avg:58.97ms
step:942/2330 train_time:55558ms step_avg:58.98ms
step:943/2330 train_time:55615ms step_avg:58.98ms
step:944/2330 train_time:55678ms step_avg:58.98ms
step:945/2330 train_time:55735ms step_avg:58.98ms
step:946/2330 train_time:55797ms step_avg:58.98ms
step:947/2330 train_time:55855ms step_avg:58.98ms
step:948/2330 train_time:55916ms step_avg:58.98ms
step:949/2330 train_time:55974ms step_avg:58.98ms
step:950/2330 train_time:56035ms step_avg:58.98ms
step:951/2330 train_time:56093ms step_avg:58.98ms
step:952/2330 train_time:56154ms step_avg:58.99ms
step:953/2330 train_time:56213ms step_avg:58.98ms
step:954/2330 train_time:56275ms step_avg:58.99ms
step:955/2330 train_time:56333ms step_avg:58.99ms
step:956/2330 train_time:56395ms step_avg:58.99ms
step:957/2330 train_time:56452ms step_avg:58.99ms
step:958/2330 train_time:56515ms step_avg:58.99ms
step:959/2330 train_time:56573ms step_avg:58.99ms
step:960/2330 train_time:56635ms step_avg:59.00ms
step:961/2330 train_time:56693ms step_avg:58.99ms
step:962/2330 train_time:56755ms step_avg:59.00ms
step:963/2330 train_time:56812ms step_avg:58.99ms
step:964/2330 train_time:56875ms step_avg:59.00ms
step:965/2330 train_time:56932ms step_avg:59.00ms
step:966/2330 train_time:56995ms step_avg:59.00ms
step:967/2330 train_time:57052ms step_avg:59.00ms
step:968/2330 train_time:57114ms step_avg:59.00ms
step:969/2330 train_time:57171ms step_avg:59.00ms
step:970/2330 train_time:57234ms step_avg:59.00ms
step:971/2330 train_time:57291ms step_avg:59.00ms
step:972/2330 train_time:57353ms step_avg:59.00ms
step:973/2330 train_time:57410ms step_avg:59.00ms
step:974/2330 train_time:57473ms step_avg:59.01ms
step:975/2330 train_time:57531ms step_avg:59.01ms
step:976/2330 train_time:57593ms step_avg:59.01ms
step:977/2330 train_time:57651ms step_avg:59.01ms
step:978/2330 train_time:57712ms step_avg:59.01ms
step:979/2330 train_time:57770ms step_avg:59.01ms
step:980/2330 train_time:57832ms step_avg:59.01ms
step:981/2330 train_time:57890ms step_avg:59.01ms
step:982/2330 train_time:57952ms step_avg:59.01ms
step:983/2330 train_time:58010ms step_avg:59.01ms
step:984/2330 train_time:58072ms step_avg:59.02ms
step:985/2330 train_time:58129ms step_avg:59.01ms
step:986/2330 train_time:58191ms step_avg:59.02ms
step:987/2330 train_time:58248ms step_avg:59.02ms
step:988/2330 train_time:58310ms step_avg:59.02ms
step:989/2330 train_time:58367ms step_avg:59.02ms
step:990/2330 train_time:58429ms step_avg:59.02ms
step:991/2330 train_time:58487ms step_avg:59.02ms
step:992/2330 train_time:58549ms step_avg:59.02ms
step:993/2330 train_time:58607ms step_avg:59.02ms
step:994/2330 train_time:58670ms step_avg:59.02ms
step:995/2330 train_time:58727ms step_avg:59.02ms
step:996/2330 train_time:58788ms step_avg:59.02ms
step:997/2330 train_time:58847ms step_avg:59.02ms
step:998/2330 train_time:58908ms step_avg:59.03ms
step:999/2330 train_time:58966ms step_avg:59.02ms
step:1000/2330 train_time:59027ms step_avg:59.03ms
step:1000/2330 val_loss:4.0676 train_time:59106ms step_avg:59.11ms
step:1001/2330 train_time:59125ms step_avg:59.07ms
step:1002/2330 train_time:59147ms step_avg:59.03ms
step:1003/2330 train_time:59205ms step_avg:59.03ms
step:1004/2330 train_time:59272ms step_avg:59.04ms
step:1005/2330 train_time:59328ms step_avg:59.03ms
step:1006/2330 train_time:59393ms step_avg:59.04ms
step:1007/2330 train_time:59450ms step_avg:59.04ms
step:1008/2330 train_time:59512ms step_avg:59.04ms
step:1009/2330 train_time:59568ms step_avg:59.04ms
step:1010/2330 train_time:59630ms step_avg:59.04ms
step:1011/2330 train_time:59687ms step_avg:59.04ms
step:1012/2330 train_time:59747ms step_avg:59.04ms
step:1013/2330 train_time:59804ms step_avg:59.04ms
step:1014/2330 train_time:59865ms step_avg:59.04ms
step:1015/2330 train_time:59921ms step_avg:59.04ms
step:1016/2330 train_time:59982ms step_avg:59.04ms
step:1017/2330 train_time:60039ms step_avg:59.04ms
step:1018/2330 train_time:60106ms step_avg:59.04ms
step:1019/2330 train_time:60164ms step_avg:59.04ms
step:1020/2330 train_time:60228ms step_avg:59.05ms
step:1021/2330 train_time:60287ms step_avg:59.05ms
step:1022/2330 train_time:60350ms step_avg:59.05ms
step:1023/2330 train_time:60407ms step_avg:59.05ms
step:1024/2330 train_time:60470ms step_avg:59.05ms
step:1025/2330 train_time:60527ms step_avg:59.05ms
step:1026/2330 train_time:60588ms step_avg:59.05ms
step:1027/2330 train_time:60645ms step_avg:59.05ms
step:1028/2330 train_time:60706ms step_avg:59.05ms
step:1029/2330 train_time:60762ms step_avg:59.05ms
step:1030/2330 train_time:60824ms step_avg:59.05ms
step:1031/2330 train_time:60881ms step_avg:59.05ms
step:1032/2330 train_time:60942ms step_avg:59.05ms
step:1033/2330 train_time:60999ms step_avg:59.05ms
step:1034/2330 train_time:61061ms step_avg:59.05ms
step:1035/2330 train_time:61119ms step_avg:59.05ms
step:1036/2330 train_time:61182ms step_avg:59.06ms
step:1037/2330 train_time:61241ms step_avg:59.06ms
step:1038/2330 train_time:61304ms step_avg:59.06ms
step:1039/2330 train_time:61362ms step_avg:59.06ms
step:1040/2330 train_time:61425ms step_avg:59.06ms
step:1041/2330 train_time:61482ms step_avg:59.06ms
step:1042/2330 train_time:61546ms step_avg:59.06ms
step:1043/2330 train_time:61603ms step_avg:59.06ms
step:1044/2330 train_time:61664ms step_avg:59.07ms
step:1045/2330 train_time:61721ms step_avg:59.06ms
step:1046/2330 train_time:61783ms step_avg:59.07ms
step:1047/2330 train_time:61839ms step_avg:59.06ms
step:1048/2330 train_time:61901ms step_avg:59.07ms
step:1049/2330 train_time:61958ms step_avg:59.06ms
step:1050/2330 train_time:62020ms step_avg:59.07ms
step:1051/2330 train_time:62078ms step_avg:59.07ms
step:1052/2330 train_time:62141ms step_avg:59.07ms
step:1053/2330 train_time:62199ms step_avg:59.07ms
step:1054/2330 train_time:62262ms step_avg:59.07ms
step:1055/2330 train_time:62320ms step_avg:59.07ms
step:1056/2330 train_time:62384ms step_avg:59.08ms
step:1057/2330 train_time:62442ms step_avg:59.07ms
step:1058/2330 train_time:62504ms step_avg:59.08ms
step:1059/2330 train_time:62562ms step_avg:59.08ms
step:1060/2330 train_time:62625ms step_avg:59.08ms
step:1061/2330 train_time:62682ms step_avg:59.08ms
step:1062/2330 train_time:62744ms step_avg:59.08ms
step:1063/2330 train_time:62801ms step_avg:59.08ms
step:1064/2330 train_time:62862ms step_avg:59.08ms
step:1065/2330 train_time:62919ms step_avg:59.08ms
step:1066/2330 train_time:62982ms step_avg:59.08ms
step:1067/2330 train_time:63038ms step_avg:59.08ms
step:1068/2330 train_time:63101ms step_avg:59.08ms
step:1069/2330 train_time:63158ms step_avg:59.08ms
step:1070/2330 train_time:63221ms step_avg:59.08ms
step:1071/2330 train_time:63279ms step_avg:59.08ms
step:1072/2330 train_time:63341ms step_avg:59.09ms
step:1073/2330 train_time:63399ms step_avg:59.09ms
step:1074/2330 train_time:63462ms step_avg:59.09ms
step:1075/2330 train_time:63519ms step_avg:59.09ms
step:1076/2330 train_time:63583ms step_avg:59.09ms
step:1077/2330 train_time:63641ms step_avg:59.09ms
step:1078/2330 train_time:63701ms step_avg:59.09ms
step:1079/2330 train_time:63758ms step_avg:59.09ms
step:1080/2330 train_time:63820ms step_avg:59.09ms
step:1081/2330 train_time:63877ms step_avg:59.09ms
step:1082/2330 train_time:63939ms step_avg:59.09ms
step:1083/2330 train_time:63996ms step_avg:59.09ms
step:1084/2330 train_time:64059ms step_avg:59.09ms
step:1085/2330 train_time:64116ms step_avg:59.09ms
step:1086/2330 train_time:64179ms step_avg:59.10ms
step:1087/2330 train_time:64237ms step_avg:59.10ms
step:1088/2330 train_time:64299ms step_avg:59.10ms
step:1089/2330 train_time:64356ms step_avg:59.10ms
step:1090/2330 train_time:64420ms step_avg:59.10ms
step:1091/2330 train_time:64477ms step_avg:59.10ms
step:1092/2330 train_time:64540ms step_avg:59.10ms
step:1093/2330 train_time:64598ms step_avg:59.10ms
step:1094/2330 train_time:64660ms step_avg:59.10ms
step:1095/2330 train_time:64717ms step_avg:59.10ms
step:1096/2330 train_time:64779ms step_avg:59.10ms
step:1097/2330 train_time:64836ms step_avg:59.10ms
step:1098/2330 train_time:64898ms step_avg:59.11ms
step:1099/2330 train_time:64956ms step_avg:59.10ms
step:1100/2330 train_time:65017ms step_avg:59.11ms
step:1101/2330 train_time:65074ms step_avg:59.10ms
step:1102/2330 train_time:65136ms step_avg:59.11ms
step:1103/2330 train_time:65193ms step_avg:59.11ms
step:1104/2330 train_time:65255ms step_avg:59.11ms
step:1105/2330 train_time:65312ms step_avg:59.11ms
step:1106/2330 train_time:65373ms step_avg:59.11ms
step:1107/2330 train_time:65431ms step_avg:59.11ms
step:1108/2330 train_time:65492ms step_avg:59.11ms
step:1109/2330 train_time:65549ms step_avg:59.11ms
step:1110/2330 train_time:65612ms step_avg:59.11ms
step:1111/2330 train_time:65669ms step_avg:59.11ms
step:1112/2330 train_time:65731ms step_avg:59.11ms
step:1113/2330 train_time:65787ms step_avg:59.11ms
step:1114/2330 train_time:65849ms step_avg:59.11ms
step:1115/2330 train_time:65906ms step_avg:59.11ms
step:1116/2330 train_time:65968ms step_avg:59.11ms
step:1117/2330 train_time:66025ms step_avg:59.11ms
step:1118/2330 train_time:66088ms step_avg:59.11ms
step:1119/2330 train_time:66145ms step_avg:59.11ms
step:1120/2330 train_time:66207ms step_avg:59.11ms
step:1121/2330 train_time:66265ms step_avg:59.11ms
step:1122/2330 train_time:66328ms step_avg:59.12ms
step:1123/2330 train_time:66384ms step_avg:59.11ms
step:1124/2330 train_time:66448ms step_avg:59.12ms
step:1125/2330 train_time:66505ms step_avg:59.12ms
step:1126/2330 train_time:66568ms step_avg:59.12ms
step:1127/2330 train_time:66625ms step_avg:59.12ms
step:1128/2330 train_time:66688ms step_avg:59.12ms
step:1129/2330 train_time:66745ms step_avg:59.12ms
step:1130/2330 train_time:66806ms step_avg:59.12ms
step:1131/2330 train_time:66863ms step_avg:59.12ms
step:1132/2330 train_time:66926ms step_avg:59.12ms
step:1133/2330 train_time:66983ms step_avg:59.12ms
step:1134/2330 train_time:67045ms step_avg:59.12ms
step:1135/2330 train_time:67103ms step_avg:59.12ms
step:1136/2330 train_time:67164ms step_avg:59.12ms
step:1137/2330 train_time:67222ms step_avg:59.12ms
step:1138/2330 train_time:67284ms step_avg:59.12ms
step:1139/2330 train_time:67342ms step_avg:59.12ms
step:1140/2330 train_time:67404ms step_avg:59.13ms
step:1141/2330 train_time:67462ms step_avg:59.13ms
step:1142/2330 train_time:67524ms step_avg:59.13ms
step:1143/2330 train_time:67582ms step_avg:59.13ms
step:1144/2330 train_time:67644ms step_avg:59.13ms
step:1145/2330 train_time:67702ms step_avg:59.13ms
step:1146/2330 train_time:67764ms step_avg:59.13ms
step:1147/2330 train_time:67821ms step_avg:59.13ms
step:1148/2330 train_time:67883ms step_avg:59.13ms
step:1149/2330 train_time:67941ms step_avg:59.13ms
step:1150/2330 train_time:68002ms step_avg:59.13ms
step:1151/2330 train_time:68059ms step_avg:59.13ms
step:1152/2330 train_time:68122ms step_avg:59.13ms
step:1153/2330 train_time:68179ms step_avg:59.13ms
step:1154/2330 train_time:68242ms step_avg:59.14ms
step:1155/2330 train_time:68299ms step_avg:59.13ms
step:1156/2330 train_time:68363ms step_avg:59.14ms
step:1157/2330 train_time:68420ms step_avg:59.14ms
step:1158/2330 train_time:68484ms step_avg:59.14ms
step:1159/2330 train_time:68542ms step_avg:59.14ms
step:1160/2330 train_time:68603ms step_avg:59.14ms
step:1161/2330 train_time:68661ms step_avg:59.14ms
step:1162/2330 train_time:68723ms step_avg:59.14ms
step:1163/2330 train_time:68781ms step_avg:59.14ms
step:1164/2330 train_time:68842ms step_avg:59.14ms
step:1165/2330 train_time:68899ms step_avg:59.14ms
step:1166/2330 train_time:68961ms step_avg:59.14ms
step:1167/2330 train_time:69019ms step_avg:59.14ms
step:1168/2330 train_time:69082ms step_avg:59.15ms
step:1169/2330 train_time:69140ms step_avg:59.14ms
step:1170/2330 train_time:69201ms step_avg:59.15ms
step:1171/2330 train_time:69259ms step_avg:59.15ms
step:1172/2330 train_time:69321ms step_avg:59.15ms
step:1173/2330 train_time:69379ms step_avg:59.15ms
step:1174/2330 train_time:69441ms step_avg:59.15ms
step:1175/2330 train_time:69499ms step_avg:59.15ms
step:1176/2330 train_time:69561ms step_avg:59.15ms
step:1177/2330 train_time:69618ms step_avg:59.15ms
step:1178/2330 train_time:69680ms step_avg:59.15ms
step:1179/2330 train_time:69738ms step_avg:59.15ms
step:1180/2330 train_time:69800ms step_avg:59.15ms
step:1181/2330 train_time:69857ms step_avg:59.15ms
step:1182/2330 train_time:69919ms step_avg:59.15ms
step:1183/2330 train_time:69976ms step_avg:59.15ms
step:1184/2330 train_time:70038ms step_avg:59.15ms
step:1185/2330 train_time:70096ms step_avg:59.15ms
step:1186/2330 train_time:70158ms step_avg:59.16ms
step:1187/2330 train_time:70215ms step_avg:59.15ms
step:1188/2330 train_time:70278ms step_avg:59.16ms
step:1189/2330 train_time:70335ms step_avg:59.16ms
step:1190/2330 train_time:70397ms step_avg:59.16ms
step:1191/2330 train_time:70455ms step_avg:59.16ms
step:1192/2330 train_time:70517ms step_avg:59.16ms
step:1193/2330 train_time:70575ms step_avg:59.16ms
step:1194/2330 train_time:70636ms step_avg:59.16ms
step:1195/2330 train_time:70694ms step_avg:59.16ms
step:1196/2330 train_time:70754ms step_avg:59.16ms
step:1197/2330 train_time:70811ms step_avg:59.16ms
step:1198/2330 train_time:70873ms step_avg:59.16ms
step:1199/2330 train_time:70930ms step_avg:59.16ms
step:1200/2330 train_time:70991ms step_avg:59.16ms
step:1201/2330 train_time:71047ms step_avg:59.16ms
step:1202/2330 train_time:71109ms step_avg:59.16ms
step:1203/2330 train_time:71166ms step_avg:59.16ms
step:1204/2330 train_time:71228ms step_avg:59.16ms
step:1205/2330 train_time:71285ms step_avg:59.16ms
step:1206/2330 train_time:71348ms step_avg:59.16ms
step:1207/2330 train_time:71405ms step_avg:59.16ms
step:1208/2330 train_time:71467ms step_avg:59.16ms
step:1209/2330 train_time:71524ms step_avg:59.16ms
step:1210/2330 train_time:71588ms step_avg:59.16ms
step:1211/2330 train_time:71645ms step_avg:59.16ms
step:1212/2330 train_time:71707ms step_avg:59.16ms
step:1213/2330 train_time:71764ms step_avg:59.16ms
step:1214/2330 train_time:71827ms step_avg:59.17ms
step:1215/2330 train_time:71884ms step_avg:59.16ms
step:1216/2330 train_time:71946ms step_avg:59.17ms
step:1217/2330 train_time:72003ms step_avg:59.16ms
step:1218/2330 train_time:72066ms step_avg:59.17ms
step:1219/2330 train_time:72124ms step_avg:59.17ms
step:1220/2330 train_time:72185ms step_avg:59.17ms
step:1221/2330 train_time:72243ms step_avg:59.17ms
step:1222/2330 train_time:72305ms step_avg:59.17ms
step:1223/2330 train_time:72363ms step_avg:59.17ms
step:1224/2330 train_time:72425ms step_avg:59.17ms
step:1225/2330 train_time:72483ms step_avg:59.17ms
step:1226/2330 train_time:72546ms step_avg:59.17ms
step:1227/2330 train_time:72602ms step_avg:59.17ms
step:1228/2330 train_time:72665ms step_avg:59.17ms
step:1229/2330 train_time:72723ms step_avg:59.17ms
step:1230/2330 train_time:72785ms step_avg:59.17ms
step:1231/2330 train_time:72842ms step_avg:59.17ms
step:1232/2330 train_time:72904ms step_avg:59.18ms
step:1233/2330 train_time:72961ms step_avg:59.17ms
step:1234/2330 train_time:73024ms step_avg:59.18ms
step:1235/2330 train_time:73081ms step_avg:59.18ms
step:1236/2330 train_time:73144ms step_avg:59.18ms
step:1237/2330 train_time:73201ms step_avg:59.18ms
step:1238/2330 train_time:73263ms step_avg:59.18ms
step:1239/2330 train_time:73320ms step_avg:59.18ms
step:1240/2330 train_time:73383ms step_avg:59.18ms
step:1241/2330 train_time:73440ms step_avg:59.18ms
step:1242/2330 train_time:73503ms step_avg:59.18ms
step:1243/2330 train_time:73560ms step_avg:59.18ms
step:1244/2330 train_time:73623ms step_avg:59.18ms
step:1245/2330 train_time:73681ms step_avg:59.18ms
step:1246/2330 train_time:73742ms step_avg:59.18ms
step:1247/2330 train_time:73800ms step_avg:59.18ms
step:1248/2330 train_time:73861ms step_avg:59.18ms
step:1249/2330 train_time:73918ms step_avg:59.18ms
step:1250/2330 train_time:73982ms step_avg:59.19ms
step:1250/2330 val_loss:3.9966 train_time:74061ms step_avg:59.25ms
step:1251/2330 train_time:74080ms step_avg:59.22ms
step:1252/2330 train_time:74104ms step_avg:59.19ms
step:1253/2330 train_time:74164ms step_avg:59.19ms
step:1254/2330 train_time:74228ms step_avg:59.19ms
step:1255/2330 train_time:74285ms step_avg:59.19ms
step:1256/2330 train_time:74349ms step_avg:59.19ms
step:1257/2330 train_time:74405ms step_avg:59.19ms
step:1258/2330 train_time:74468ms step_avg:59.20ms
step:1259/2330 train_time:74525ms step_avg:59.19ms
step:1260/2330 train_time:74587ms step_avg:59.20ms
step:1261/2330 train_time:74643ms step_avg:59.19ms
step:1262/2330 train_time:74704ms step_avg:59.20ms
step:1263/2330 train_time:74761ms step_avg:59.19ms
step:1264/2330 train_time:74822ms step_avg:59.19ms
step:1265/2330 train_time:74879ms step_avg:59.19ms
step:1266/2330 train_time:74940ms step_avg:59.19ms
step:1267/2330 train_time:74997ms step_avg:59.19ms
step:1268/2330 train_time:75059ms step_avg:59.19ms
step:1269/2330 train_time:75117ms step_avg:59.19ms
step:1270/2330 train_time:75181ms step_avg:59.20ms
step:1271/2330 train_time:75239ms step_avg:59.20ms
step:1272/2330 train_time:75303ms step_avg:59.20ms
step:1273/2330 train_time:75361ms step_avg:59.20ms
step:1274/2330 train_time:75424ms step_avg:59.20ms
step:1275/2330 train_time:75481ms step_avg:59.20ms
step:1276/2330 train_time:75543ms step_avg:59.20ms
step:1277/2330 train_time:75600ms step_avg:59.20ms
step:1278/2330 train_time:75662ms step_avg:59.20ms
step:1279/2330 train_time:75719ms step_avg:59.20ms
step:1280/2330 train_time:75781ms step_avg:59.20ms
step:1281/2330 train_time:75838ms step_avg:59.20ms
step:1282/2330 train_time:75900ms step_avg:59.20ms
step:1283/2330 train_time:75957ms step_avg:59.20ms
step:1284/2330 train_time:76018ms step_avg:59.20ms
step:1285/2330 train_time:76075ms step_avg:59.20ms
step:1286/2330 train_time:76139ms step_avg:59.21ms
step:1287/2330 train_time:76197ms step_avg:59.20ms
step:1288/2330 train_time:76261ms step_avg:59.21ms
step:1289/2330 train_time:76319ms step_avg:59.21ms
step:1290/2330 train_time:76382ms step_avg:59.21ms
step:1291/2330 train_time:76439ms step_avg:59.21ms
step:1292/2330 train_time:76501ms step_avg:59.21ms
step:1293/2330 train_time:76559ms step_avg:59.21ms
step:1294/2330 train_time:76621ms step_avg:59.21ms
step:1295/2330 train_time:76679ms step_avg:59.21ms
step:1296/2330 train_time:76740ms step_avg:59.21ms
step:1297/2330 train_time:76797ms step_avg:59.21ms
step:1298/2330 train_time:76859ms step_avg:59.21ms
step:1299/2330 train_time:76916ms step_avg:59.21ms
step:1300/2330 train_time:76978ms step_avg:59.21ms
step:1301/2330 train_time:77035ms step_avg:59.21ms
step:1302/2330 train_time:77097ms step_avg:59.21ms
step:1303/2330 train_time:77154ms step_avg:59.21ms
step:1304/2330 train_time:77218ms step_avg:59.22ms
step:1305/2330 train_time:77277ms step_avg:59.22ms
step:1306/2330 train_time:77341ms step_avg:59.22ms
step:1307/2330 train_time:77399ms step_avg:59.22ms
step:1308/2330 train_time:77460ms step_avg:59.22ms
step:1309/2330 train_time:77518ms step_avg:59.22ms
step:1310/2330 train_time:77580ms step_avg:59.22ms
step:1311/2330 train_time:77638ms step_avg:59.22ms
step:1312/2330 train_time:77699ms step_avg:59.22ms
step:1313/2330 train_time:77756ms step_avg:59.22ms
step:1314/2330 train_time:77818ms step_avg:59.22ms
step:1315/2330 train_time:77876ms step_avg:59.22ms
step:1316/2330 train_time:77938ms step_avg:59.22ms
step:1317/2330 train_time:77994ms step_avg:59.22ms
step:1318/2330 train_time:78056ms step_avg:59.22ms
step:1319/2330 train_time:78114ms step_avg:59.22ms
step:1320/2330 train_time:78176ms step_avg:59.22ms
step:1321/2330 train_time:78234ms step_avg:59.22ms
step:1322/2330 train_time:78297ms step_avg:59.23ms
step:1323/2330 train_time:78355ms step_avg:59.23ms
step:1324/2330 train_time:78418ms step_avg:59.23ms
step:1325/2330 train_time:78475ms step_avg:59.23ms
step:1326/2330 train_time:78539ms step_avg:59.23ms
step:1327/2330 train_time:78597ms step_avg:59.23ms
step:1328/2330 train_time:78659ms step_avg:59.23ms
step:1329/2330 train_time:78716ms step_avg:59.23ms
step:1330/2330 train_time:78778ms step_avg:59.23ms
step:1331/2330 train_time:78835ms step_avg:59.23ms
step:1332/2330 train_time:78897ms step_avg:59.23ms
step:1333/2330 train_time:78954ms step_avg:59.23ms
step:1334/2330 train_time:79015ms step_avg:59.23ms
step:1335/2330 train_time:79072ms step_avg:59.23ms
step:1336/2330 train_time:79135ms step_avg:59.23ms
step:1337/2330 train_time:79193ms step_avg:59.23ms
step:1338/2330 train_time:79256ms step_avg:59.23ms
step:1339/2330 train_time:79314ms step_avg:59.23ms
step:1340/2330 train_time:79377ms step_avg:59.24ms
step:1341/2330 train_time:79434ms step_avg:59.24ms
step:1342/2330 train_time:79497ms step_avg:59.24ms
step:1343/2330 train_time:79556ms step_avg:59.24ms
step:1344/2330 train_time:79617ms step_avg:59.24ms
step:1345/2330 train_time:79675ms step_avg:59.24ms
step:1346/2330 train_time:79736ms step_avg:59.24ms
step:1347/2330 train_time:79794ms step_avg:59.24ms
step:1348/2330 train_time:79855ms step_avg:59.24ms
step:1349/2330 train_time:79913ms step_avg:59.24ms
step:1350/2330 train_time:79975ms step_avg:59.24ms
step:1351/2330 train_time:80032ms step_avg:59.24ms
step:1352/2330 train_time:80093ms step_avg:59.24ms
step:1353/2330 train_time:80150ms step_avg:59.24ms
step:1354/2330 train_time:80213ms step_avg:59.24ms
step:1355/2330 train_time:80270ms step_avg:59.24ms
step:1356/2330 train_time:80333ms step_avg:59.24ms
step:1357/2330 train_time:80390ms step_avg:59.24ms
step:1358/2330 train_time:80453ms step_avg:59.24ms
step:1359/2330 train_time:80510ms step_avg:59.24ms
step:1360/2330 train_time:80573ms step_avg:59.24ms
step:1361/2330 train_time:80631ms step_avg:59.24ms
step:1362/2330 train_time:80692ms step_avg:59.25ms
step:1363/2330 train_time:80749ms step_avg:59.24ms
step:1364/2330 train_time:80810ms step_avg:59.24ms
step:1365/2330 train_time:80867ms step_avg:59.24ms
step:1366/2330 train_time:80929ms step_avg:59.25ms
step:1367/2330 train_time:80986ms step_avg:59.24ms
step:1368/2330 train_time:81047ms step_avg:59.24ms
step:1369/2330 train_time:81104ms step_avg:59.24ms
step:1370/2330 train_time:81166ms step_avg:59.25ms
step:1371/2330 train_time:81223ms step_avg:59.24ms
step:1372/2330 train_time:81286ms step_avg:59.25ms
step:1373/2330 train_time:81342ms step_avg:59.24ms
step:1374/2330 train_time:81405ms step_avg:59.25ms
step:1375/2330 train_time:81462ms step_avg:59.24ms
step:1376/2330 train_time:81525ms step_avg:59.25ms
step:1377/2330 train_time:81582ms step_avg:59.25ms
step:1378/2330 train_time:81644ms step_avg:59.25ms
step:1379/2330 train_time:81701ms step_avg:59.25ms
step:1380/2330 train_time:81764ms step_avg:59.25ms
step:1381/2330 train_time:81821ms step_avg:59.25ms
step:1382/2330 train_time:81883ms step_avg:59.25ms
step:1383/2330 train_time:81940ms step_avg:59.25ms
step:1384/2330 train_time:82002ms step_avg:59.25ms
step:1385/2330 train_time:82059ms step_avg:59.25ms
step:1386/2330 train_time:82121ms step_avg:59.25ms
step:1387/2330 train_time:82178ms step_avg:59.25ms
step:1388/2330 train_time:82241ms step_avg:59.25ms
step:1389/2330 train_time:82298ms step_avg:59.25ms
step:1390/2330 train_time:82361ms step_avg:59.25ms
step:1391/2330 train_time:82419ms step_avg:59.25ms
step:1392/2330 train_time:82481ms step_avg:59.25ms
step:1393/2330 train_time:82538ms step_avg:59.25ms
step:1394/2330 train_time:82601ms step_avg:59.25ms
step:1395/2330 train_time:82658ms step_avg:59.25ms
step:1396/2330 train_time:82722ms step_avg:59.26ms
step:1397/2330 train_time:82779ms step_avg:59.26ms
step:1398/2330 train_time:82841ms step_avg:59.26ms
step:1399/2330 train_time:82898ms step_avg:59.26ms
step:1400/2330 train_time:82960ms step_avg:59.26ms
step:1401/2330 train_time:83018ms step_avg:59.26ms
step:1402/2330 train_time:83080ms step_avg:59.26ms
step:1403/2330 train_time:83137ms step_avg:59.26ms
step:1404/2330 train_time:83198ms step_avg:59.26ms
step:1405/2330 train_time:83256ms step_avg:59.26ms
step:1406/2330 train_time:83319ms step_avg:59.26ms
step:1407/2330 train_time:83377ms step_avg:59.26ms
step:1408/2330 train_time:83439ms step_avg:59.26ms
step:1409/2330 train_time:83497ms step_avg:59.26ms
step:1410/2330 train_time:83559ms step_avg:59.26ms
step:1411/2330 train_time:83616ms step_avg:59.26ms
step:1412/2330 train_time:83679ms step_avg:59.26ms
step:1413/2330 train_time:83736ms step_avg:59.26ms
step:1414/2330 train_time:83798ms step_avg:59.26ms
step:1415/2330 train_time:83856ms step_avg:59.26ms
step:1416/2330 train_time:83918ms step_avg:59.26ms
step:1417/2330 train_time:83976ms step_avg:59.26ms
step:1418/2330 train_time:84038ms step_avg:59.27ms
step:1419/2330 train_time:84095ms step_avg:59.26ms
step:1420/2330 train_time:84157ms step_avg:59.27ms
step:1421/2330 train_time:84215ms step_avg:59.26ms
step:1422/2330 train_time:84277ms step_avg:59.27ms
step:1423/2330 train_time:84335ms step_avg:59.27ms
step:1424/2330 train_time:84397ms step_avg:59.27ms
step:1425/2330 train_time:84455ms step_avg:59.27ms
step:1426/2330 train_time:84518ms step_avg:59.27ms
step:1427/2330 train_time:84575ms step_avg:59.27ms
step:1428/2330 train_time:84637ms step_avg:59.27ms
step:1429/2330 train_time:84695ms step_avg:59.27ms
step:1430/2330 train_time:84757ms step_avg:59.27ms
step:1431/2330 train_time:84815ms step_avg:59.27ms
step:1432/2330 train_time:84877ms step_avg:59.27ms
step:1433/2330 train_time:84934ms step_avg:59.27ms
step:1434/2330 train_time:84997ms step_avg:59.27ms
step:1435/2330 train_time:85054ms step_avg:59.27ms
step:1436/2330 train_time:85117ms step_avg:59.27ms
step:1437/2330 train_time:85174ms step_avg:59.27ms
step:1438/2330 train_time:85237ms step_avg:59.27ms
step:1439/2330 train_time:85294ms step_avg:59.27ms
step:1440/2330 train_time:85357ms step_avg:59.28ms
step:1441/2330 train_time:85415ms step_avg:59.27ms
step:1442/2330 train_time:85477ms step_avg:59.28ms
step:1443/2330 train_time:85535ms step_avg:59.28ms
step:1444/2330 train_time:85597ms step_avg:59.28ms
step:1445/2330 train_time:85655ms step_avg:59.28ms
step:1446/2330 train_time:85717ms step_avg:59.28ms
step:1447/2330 train_time:85775ms step_avg:59.28ms
step:1448/2330 train_time:85837ms step_avg:59.28ms
step:1449/2330 train_time:85895ms step_avg:59.28ms
step:1450/2330 train_time:85958ms step_avg:59.28ms
step:1451/2330 train_time:86015ms step_avg:59.28ms
step:1452/2330 train_time:86077ms step_avg:59.28ms
step:1453/2330 train_time:86135ms step_avg:59.28ms
step:1454/2330 train_time:86197ms step_avg:59.28ms
step:1455/2330 train_time:86254ms step_avg:59.28ms
step:1456/2330 train_time:86316ms step_avg:59.28ms
step:1457/2330 train_time:86374ms step_avg:59.28ms
step:1458/2330 train_time:86436ms step_avg:59.28ms
step:1459/2330 train_time:86494ms step_avg:59.28ms
step:1460/2330 train_time:86556ms step_avg:59.28ms
step:1461/2330 train_time:86614ms step_avg:59.28ms
step:1462/2330 train_time:86676ms step_avg:59.29ms
step:1463/2330 train_time:86734ms step_avg:59.28ms
step:1464/2330 train_time:86795ms step_avg:59.29ms
step:1465/2330 train_time:86853ms step_avg:59.29ms
step:1466/2330 train_time:86915ms step_avg:59.29ms
step:1467/2330 train_time:86973ms step_avg:59.29ms
step:1468/2330 train_time:87035ms step_avg:59.29ms
step:1469/2330 train_time:87093ms step_avg:59.29ms
step:1470/2330 train_time:87154ms step_avg:59.29ms
step:1471/2330 train_time:87212ms step_avg:59.29ms
step:1472/2330 train_time:87273ms step_avg:59.29ms
step:1473/2330 train_time:87331ms step_avg:59.29ms
step:1474/2330 train_time:87392ms step_avg:59.29ms
step:1475/2330 train_time:87450ms step_avg:59.29ms
step:1476/2330 train_time:87512ms step_avg:59.29ms
step:1477/2330 train_time:87570ms step_avg:59.29ms
step:1478/2330 train_time:87632ms step_avg:59.29ms
step:1479/2330 train_time:87689ms step_avg:59.29ms
step:1480/2330 train_time:87750ms step_avg:59.29ms
step:1481/2330 train_time:87807ms step_avg:59.29ms
step:1482/2330 train_time:87868ms step_avg:59.29ms
step:1483/2330 train_time:87925ms step_avg:59.29ms
step:1484/2330 train_time:87988ms step_avg:59.29ms
step:1485/2330 train_time:88045ms step_avg:59.29ms
step:1486/2330 train_time:88107ms step_avg:59.29ms
step:1487/2330 train_time:88164ms step_avg:59.29ms
step:1488/2330 train_time:88227ms step_avg:59.29ms
step:1489/2330 train_time:88283ms step_avg:59.29ms
step:1490/2330 train_time:88345ms step_avg:59.29ms
step:1491/2330 train_time:88402ms step_avg:59.29ms
step:1492/2330 train_time:88465ms step_avg:59.29ms
step:1493/2330 train_time:88522ms step_avg:59.29ms
step:1494/2330 train_time:88584ms step_avg:59.29ms
step:1495/2330 train_time:88641ms step_avg:59.29ms
step:1496/2330 train_time:88703ms step_avg:59.29ms
step:1497/2330 train_time:88760ms step_avg:59.29ms
step:1498/2330 train_time:88823ms step_avg:59.29ms
step:1499/2330 train_time:88880ms step_avg:59.29ms
step:1500/2330 train_time:88942ms step_avg:59.29ms
step:1500/2330 val_loss:3.9077 train_time:89022ms step_avg:59.35ms
step:1501/2330 train_time:89042ms step_avg:59.32ms
step:1502/2330 train_time:89066ms step_avg:59.30ms
step:1503/2330 train_time:89126ms step_avg:59.30ms
step:1504/2330 train_time:89191ms step_avg:59.30ms
step:1505/2330 train_time:89249ms step_avg:59.30ms
step:1506/2330 train_time:89311ms step_avg:59.30ms
step:1507/2330 train_time:89367ms step_avg:59.30ms
step:1508/2330 train_time:89430ms step_avg:59.30ms
step:1509/2330 train_time:89486ms step_avg:59.30ms
step:1510/2330 train_time:89548ms step_avg:59.30ms
step:1511/2330 train_time:89605ms step_avg:59.30ms
step:1512/2330 train_time:89667ms step_avg:59.30ms
step:1513/2330 train_time:89724ms step_avg:59.30ms
step:1514/2330 train_time:89785ms step_avg:59.30ms
step:1515/2330 train_time:89841ms step_avg:59.30ms
step:1516/2330 train_time:89903ms step_avg:59.30ms
step:1517/2330 train_time:89960ms step_avg:59.30ms
step:1518/2330 train_time:90025ms step_avg:59.30ms
step:1519/2330 train_time:90084ms step_avg:59.30ms
step:1520/2330 train_time:90150ms step_avg:59.31ms
step:1521/2330 train_time:90208ms step_avg:59.31ms
step:1522/2330 train_time:90271ms step_avg:59.31ms
step:1523/2330 train_time:90329ms step_avg:59.31ms
step:1524/2330 train_time:90391ms step_avg:59.31ms
step:1525/2330 train_time:90448ms step_avg:59.31ms
step:1526/2330 train_time:90510ms step_avg:59.31ms
step:1527/2330 train_time:90567ms step_avg:59.31ms
step:1528/2330 train_time:90628ms step_avg:59.31ms
step:1529/2330 train_time:90686ms step_avg:59.31ms
step:1530/2330 train_time:90748ms step_avg:59.31ms
step:1531/2330 train_time:90804ms step_avg:59.31ms
step:1532/2330 train_time:90867ms step_avg:59.31ms
step:1533/2330 train_time:90924ms step_avg:59.31ms
step:1534/2330 train_time:90987ms step_avg:59.31ms
step:1535/2330 train_time:91045ms step_avg:59.31ms
step:1536/2330 train_time:91110ms step_avg:59.32ms
step:1537/2330 train_time:91167ms step_avg:59.32ms
step:1538/2330 train_time:91232ms step_avg:59.32ms
step:1539/2330 train_time:91289ms step_avg:59.32ms
step:1540/2330 train_time:91352ms step_avg:59.32ms
step:1541/2330 train_time:91410ms step_avg:59.32ms
step:1542/2330 train_time:91472ms step_avg:59.32ms
step:1543/2330 train_time:91529ms step_avg:59.32ms
step:1544/2330 train_time:91591ms step_avg:59.32ms
step:1545/2330 train_time:91648ms step_avg:59.32ms
step:1546/2330 train_time:91711ms step_avg:59.32ms
step:1547/2330 train_time:91768ms step_avg:59.32ms
step:1548/2330 train_time:91830ms step_avg:59.32ms
step:1549/2330 train_time:91887ms step_avg:59.32ms
step:1550/2330 train_time:91950ms step_avg:59.32ms
step:1551/2330 train_time:92007ms step_avg:59.32ms
step:1552/2330 train_time:92071ms step_avg:59.32ms
step:1553/2330 train_time:92128ms step_avg:59.32ms
step:1554/2330 train_time:92192ms step_avg:59.33ms
step:1555/2330 train_time:92250ms step_avg:59.32ms
step:1556/2330 train_time:92313ms step_avg:59.33ms
step:1557/2330 train_time:92370ms step_avg:59.33ms
step:1558/2330 train_time:92433ms step_avg:59.33ms
step:1559/2330 train_time:92491ms step_avg:59.33ms
step:1560/2330 train_time:92553ms step_avg:59.33ms
step:1561/2330 train_time:92610ms step_avg:59.33ms
step:1562/2330 train_time:92673ms step_avg:59.33ms
step:1563/2330 train_time:92730ms step_avg:59.33ms
step:1564/2330 train_time:92792ms step_avg:59.33ms
step:1565/2330 train_time:92850ms step_avg:59.33ms
step:1566/2330 train_time:92913ms step_avg:59.33ms
step:1567/2330 train_time:92970ms step_avg:59.33ms
step:1568/2330 train_time:93034ms step_avg:59.33ms
step:1569/2330 train_time:93091ms step_avg:59.33ms
step:1570/2330 train_time:93156ms step_avg:59.33ms
step:1571/2330 train_time:93213ms step_avg:59.33ms
step:1572/2330 train_time:93275ms step_avg:59.34ms
step:1573/2330 train_time:93333ms step_avg:59.33ms
step:1574/2330 train_time:93395ms step_avg:59.34ms
step:1575/2330 train_time:93453ms step_avg:59.33ms
step:1576/2330 train_time:93515ms step_avg:59.34ms
step:1577/2330 train_time:93572ms step_avg:59.34ms
step:1578/2330 train_time:93635ms step_avg:59.34ms
step:1579/2330 train_time:93692ms step_avg:59.34ms
step:1580/2330 train_time:93754ms step_avg:59.34ms
step:1581/2330 train_time:93811ms step_avg:59.34ms
step:1582/2330 train_time:93873ms step_avg:59.34ms
step:1583/2330 train_time:93931ms step_avg:59.34ms
step:1584/2330 train_time:93994ms step_avg:59.34ms
step:1585/2330 train_time:94052ms step_avg:59.34ms
step:1586/2330 train_time:94114ms step_avg:59.34ms
step:1587/2330 train_time:94172ms step_avg:59.34ms
step:1588/2330 train_time:94235ms step_avg:59.34ms
step:1589/2330 train_time:94292ms step_avg:59.34ms
step:1590/2330 train_time:94355ms step_avg:59.34ms
step:1591/2330 train_time:94412ms step_avg:59.34ms
step:1592/2330 train_time:94475ms step_avg:59.34ms
step:1593/2330 train_time:94532ms step_avg:59.34ms
step:1594/2330 train_time:94595ms step_avg:59.34ms
step:1595/2330 train_time:94651ms step_avg:59.34ms
step:1596/2330 train_time:94714ms step_avg:59.34ms
step:1597/2330 train_time:94771ms step_avg:59.34ms
step:1598/2330 train_time:94834ms step_avg:59.35ms
step:1599/2330 train_time:94891ms step_avg:59.34ms
step:1600/2330 train_time:94954ms step_avg:59.35ms
step:1601/2330 train_time:95011ms step_avg:59.35ms
step:1602/2330 train_time:95074ms step_avg:59.35ms
step:1603/2330 train_time:95131ms step_avg:59.35ms
step:1604/2330 train_time:95194ms step_avg:59.35ms
step:1605/2330 train_time:95251ms step_avg:59.35ms
step:1606/2330 train_time:95314ms step_avg:59.35ms
step:1607/2330 train_time:95371ms step_avg:59.35ms
step:1608/2330 train_time:95434ms step_avg:59.35ms
step:1609/2330 train_time:95491ms step_avg:59.35ms
step:1610/2330 train_time:95554ms step_avg:59.35ms
step:1611/2330 train_time:95611ms step_avg:59.35ms
step:1612/2330 train_time:95674ms step_avg:59.35ms
step:1613/2330 train_time:95731ms step_avg:59.35ms
step:1614/2330 train_time:95794ms step_avg:59.35ms
step:1615/2330 train_time:95852ms step_avg:59.35ms
step:1616/2330 train_time:95915ms step_avg:59.35ms
step:1617/2330 train_time:95971ms step_avg:59.35ms
step:1618/2330 train_time:96035ms step_avg:59.35ms
step:1619/2330 train_time:96092ms step_avg:59.35ms
step:1620/2330 train_time:96155ms step_avg:59.36ms
step:1621/2330 train_time:96213ms step_avg:59.35ms
step:1622/2330 train_time:96275ms step_avg:59.36ms
step:1623/2330 train_time:96332ms step_avg:59.35ms
step:1624/2330 train_time:96395ms step_avg:59.36ms
step:1625/2330 train_time:96452ms step_avg:59.36ms
step:1626/2330 train_time:96514ms step_avg:59.36ms
step:1627/2330 train_time:96572ms step_avg:59.36ms
step:1628/2330 train_time:96634ms step_avg:59.36ms
step:1629/2330 train_time:96691ms step_avg:59.36ms
step:1630/2330 train_time:96754ms step_avg:59.36ms
step:1631/2330 train_time:96811ms step_avg:59.36ms
step:1632/2330 train_time:96874ms step_avg:59.36ms
step:1633/2330 train_time:96931ms step_avg:59.36ms
step:1634/2330 train_time:96996ms step_avg:59.36ms
step:1635/2330 train_time:97053ms step_avg:59.36ms
step:1636/2330 train_time:97115ms step_avg:59.36ms
step:1637/2330 train_time:97173ms step_avg:59.36ms
step:1638/2330 train_time:97234ms step_avg:59.36ms
step:1639/2330 train_time:97292ms step_avg:59.36ms
step:1640/2330 train_time:97355ms step_avg:59.36ms
step:1641/2330 train_time:97412ms step_avg:59.36ms
step:1642/2330 train_time:97474ms step_avg:59.36ms
step:1643/2330 train_time:97531ms step_avg:59.36ms
step:1644/2330 train_time:97594ms step_avg:59.36ms
step:1645/2330 train_time:97651ms step_avg:59.36ms
step:1646/2330 train_time:97714ms step_avg:59.36ms
step:1647/2330 train_time:97772ms step_avg:59.36ms
step:1648/2330 train_time:97834ms step_avg:59.37ms
step:1649/2330 train_time:97891ms step_avg:59.36ms
step:1650/2330 train_time:97954ms step_avg:59.37ms
step:1651/2330 train_time:98010ms step_avg:59.36ms
step:1652/2330 train_time:98075ms step_avg:59.37ms
step:1653/2330 train_time:98132ms step_avg:59.37ms
step:1654/2330 train_time:98196ms step_avg:59.37ms
step:1655/2330 train_time:98253ms step_avg:59.37ms
step:1656/2330 train_time:98315ms step_avg:59.37ms
step:1657/2330 train_time:98372ms step_avg:59.37ms
step:1658/2330 train_time:98435ms step_avg:59.37ms
step:1659/2330 train_time:98492ms step_avg:59.37ms
step:1660/2330 train_time:98555ms step_avg:59.37ms
step:1661/2330 train_time:98612ms step_avg:59.37ms
step:1662/2330 train_time:98675ms step_avg:59.37ms
step:1663/2330 train_time:98732ms step_avg:59.37ms
step:1664/2330 train_time:98795ms step_avg:59.37ms
step:1665/2330 train_time:98853ms step_avg:59.37ms
step:1666/2330 train_time:98914ms step_avg:59.37ms
step:1667/2330 train_time:98971ms step_avg:59.37ms
step:1668/2330 train_time:99034ms step_avg:59.37ms
step:1669/2330 train_time:99092ms step_avg:59.37ms
step:1670/2330 train_time:99155ms step_avg:59.37ms
step:1671/2330 train_time:99213ms step_avg:59.37ms
step:1672/2330 train_time:99275ms step_avg:59.38ms
step:1673/2330 train_time:99333ms step_avg:59.37ms
step:1674/2330 train_time:99395ms step_avg:59.38ms
step:1675/2330 train_time:99452ms step_avg:59.37ms
step:1676/2330 train_time:99514ms step_avg:59.38ms
step:1677/2330 train_time:99571ms step_avg:59.37ms
step:1678/2330 train_time:99634ms step_avg:59.38ms
step:1679/2330 train_time:99691ms step_avg:59.38ms
step:1680/2330 train_time:99755ms step_avg:59.38ms
step:1681/2330 train_time:99813ms step_avg:59.38ms
step:1682/2330 train_time:99875ms step_avg:59.38ms
step:1683/2330 train_time:99932ms step_avg:59.38ms
step:1684/2330 train_time:99994ms step_avg:59.38ms
step:1685/2330 train_time:100052ms step_avg:59.38ms
step:1686/2330 train_time:100114ms step_avg:59.38ms
step:1687/2330 train_time:100172ms step_avg:59.38ms
step:1688/2330 train_time:100235ms step_avg:59.38ms
step:1689/2330 train_time:100292ms step_avg:59.38ms
step:1690/2330 train_time:100355ms step_avg:59.38ms
step:1691/2330 train_time:100412ms step_avg:59.38ms
step:1692/2330 train_time:100474ms step_avg:59.38ms
step:1693/2330 train_time:100531ms step_avg:59.38ms
step:1694/2330 train_time:100594ms step_avg:59.38ms
step:1695/2330 train_time:100652ms step_avg:59.38ms
step:1696/2330 train_time:100713ms step_avg:59.38ms
step:1697/2330 train_time:100771ms step_avg:59.38ms
step:1698/2330 train_time:100834ms step_avg:59.38ms
step:1699/2330 train_time:100892ms step_avg:59.38ms
step:1700/2330 train_time:100955ms step_avg:59.39ms
step:1701/2330 train_time:101012ms step_avg:59.38ms
step:1702/2330 train_time:101075ms step_avg:59.39ms
step:1703/2330 train_time:101133ms step_avg:59.39ms
step:1704/2330 train_time:101195ms step_avg:59.39ms
step:1705/2330 train_time:101252ms step_avg:59.39ms
step:1706/2330 train_time:101315ms step_avg:59.39ms
step:1707/2330 train_time:101372ms step_avg:59.39ms
step:1708/2330 train_time:101434ms step_avg:59.39ms
step:1709/2330 train_time:101492ms step_avg:59.39ms
step:1710/2330 train_time:101554ms step_avg:59.39ms
step:1711/2330 train_time:101612ms step_avg:59.39ms
step:1712/2330 train_time:101675ms step_avg:59.39ms
step:1713/2330 train_time:101731ms step_avg:59.39ms
step:1714/2330 train_time:101795ms step_avg:59.39ms
step:1715/2330 train_time:101852ms step_avg:59.39ms
step:1716/2330 train_time:101915ms step_avg:59.39ms
step:1717/2330 train_time:101972ms step_avg:59.39ms
step:1718/2330 train_time:102035ms step_avg:59.39ms
step:1719/2330 train_time:102092ms step_avg:59.39ms
step:1720/2330 train_time:102155ms step_avg:59.39ms
step:1721/2330 train_time:102212ms step_avg:59.39ms
step:1722/2330 train_time:102275ms step_avg:59.39ms
step:1723/2330 train_time:102332ms step_avg:59.39ms
step:1724/2330 train_time:102394ms step_avg:59.39ms
step:1725/2330 train_time:102452ms step_avg:59.39ms
step:1726/2330 train_time:102514ms step_avg:59.39ms
step:1727/2330 train_time:102571ms step_avg:59.39ms
step:1728/2330 train_time:102634ms step_avg:59.39ms
step:1729/2330 train_time:102691ms step_avg:59.39ms
step:1730/2330 train_time:102754ms step_avg:59.40ms
step:1731/2330 train_time:102811ms step_avg:59.39ms
step:1732/2330 train_time:102874ms step_avg:59.40ms
step:1733/2330 train_time:102931ms step_avg:59.39ms
step:1734/2330 train_time:102995ms step_avg:59.40ms
step:1735/2330 train_time:103052ms step_avg:59.40ms
step:1736/2330 train_time:103115ms step_avg:59.40ms
step:1737/2330 train_time:103172ms step_avg:59.40ms
step:1738/2330 train_time:103234ms step_avg:59.40ms
step:1739/2330 train_time:103292ms step_avg:59.40ms
step:1740/2330 train_time:103354ms step_avg:59.40ms
step:1741/2330 train_time:103411ms step_avg:59.40ms
step:1742/2330 train_time:103474ms step_avg:59.40ms
step:1743/2330 train_time:103531ms step_avg:59.40ms
step:1744/2330 train_time:103594ms step_avg:59.40ms
step:1745/2330 train_time:103652ms step_avg:59.40ms
step:1746/2330 train_time:103714ms step_avg:59.40ms
step:1747/2330 train_time:103771ms step_avg:59.40ms
step:1748/2330 train_time:103833ms step_avg:59.40ms
step:1749/2330 train_time:103891ms step_avg:59.40ms
step:1750/2330 train_time:103953ms step_avg:59.40ms
step:1750/2330 val_loss:3.8249 train_time:104033ms step_avg:59.45ms
step:1751/2330 train_time:104053ms step_avg:59.42ms
step:1752/2330 train_time:104076ms step_avg:59.40ms
step:1753/2330 train_time:104138ms step_avg:59.41ms
step:1754/2330 train_time:104203ms step_avg:59.41ms
step:1755/2330 train_time:104262ms step_avg:59.41ms
step:1756/2330 train_time:104324ms step_avg:59.41ms
step:1757/2330 train_time:104383ms step_avg:59.41ms
step:1758/2330 train_time:104444ms step_avg:59.41ms
step:1759/2330 train_time:104501ms step_avg:59.41ms
step:1760/2330 train_time:104562ms step_avg:59.41ms
step:1761/2330 train_time:104620ms step_avg:59.41ms
step:1762/2330 train_time:104681ms step_avg:59.41ms
step:1763/2330 train_time:104738ms step_avg:59.41ms
step:1764/2330 train_time:104800ms step_avg:59.41ms
step:1765/2330 train_time:104857ms step_avg:59.41ms
step:1766/2330 train_time:104918ms step_avg:59.41ms
step:1767/2330 train_time:104975ms step_avg:59.41ms
step:1768/2330 train_time:105042ms step_avg:59.41ms
step:1769/2330 train_time:105100ms step_avg:59.41ms
step:1770/2330 train_time:105163ms step_avg:59.41ms
step:1771/2330 train_time:105222ms step_avg:59.41ms
step:1772/2330 train_time:105285ms step_avg:59.42ms
step:1773/2330 train_time:105343ms step_avg:59.42ms
step:1774/2330 train_time:105406ms step_avg:59.42ms
step:1775/2330 train_time:105463ms step_avg:59.42ms
step:1776/2330 train_time:105526ms step_avg:59.42ms
step:1777/2330 train_time:105584ms step_avg:59.42ms
step:1778/2330 train_time:105646ms step_avg:59.42ms
step:1779/2330 train_time:105704ms step_avg:59.42ms
step:1780/2330 train_time:105765ms step_avg:59.42ms
step:1781/2330 train_time:105824ms step_avg:59.42ms
step:1782/2330 train_time:105886ms step_avg:59.42ms
step:1783/2330 train_time:105944ms step_avg:59.42ms
step:1784/2330 train_time:106006ms step_avg:59.42ms
step:1785/2330 train_time:106064ms step_avg:59.42ms
step:1786/2330 train_time:106129ms step_avg:59.42ms
step:1787/2330 train_time:106187ms step_avg:59.42ms
step:1788/2330 train_time:106249ms step_avg:59.42ms
step:1789/2330 train_time:106307ms step_avg:59.42ms
step:1790/2330 train_time:106371ms step_avg:59.43ms
step:1791/2330 train_time:106429ms step_avg:59.42ms
step:1792/2330 train_time:106491ms step_avg:59.43ms
step:1793/2330 train_time:106549ms step_avg:59.42ms
step:1794/2330 train_time:106611ms step_avg:59.43ms
step:1795/2330 train_time:106668ms step_avg:59.43ms
step:1796/2330 train_time:106730ms step_avg:59.43ms
step:1797/2330 train_time:106788ms step_avg:59.43ms
step:1798/2330 train_time:106850ms step_avg:59.43ms
step:1799/2330 train_time:106908ms step_avg:59.43ms
step:1800/2330 train_time:106970ms step_avg:59.43ms
step:1801/2330 train_time:107029ms step_avg:59.43ms
step:1802/2330 train_time:107092ms step_avg:59.43ms
step:1803/2330 train_time:107150ms step_avg:59.43ms
step:1804/2330 train_time:107212ms step_avg:59.43ms
step:1805/2330 train_time:107270ms step_avg:59.43ms
step:1806/2330 train_time:107333ms step_avg:59.43ms
step:1807/2330 train_time:107391ms step_avg:59.43ms
step:1808/2330 train_time:107454ms step_avg:59.43ms
step:1809/2330 train_time:107511ms step_avg:59.43ms
step:1810/2330 train_time:107573ms step_avg:59.43ms
step:1811/2330 train_time:107631ms step_avg:59.43ms
step:1812/2330 train_time:107694ms step_avg:59.43ms
step:1813/2330 train_time:107751ms step_avg:59.43ms
step:1814/2330 train_time:107814ms step_avg:59.43ms
step:1815/2330 train_time:107871ms step_avg:59.43ms
step:1816/2330 train_time:107934ms step_avg:59.44ms
step:1817/2330 train_time:107992ms step_avg:59.43ms
step:1818/2330 train_time:108054ms step_avg:59.44ms
step:1819/2330 train_time:108112ms step_avg:59.43ms
step:1820/2330 train_time:108175ms step_avg:59.44ms
step:1821/2330 train_time:108234ms step_avg:59.44ms
step:1822/2330 train_time:108295ms step_avg:59.44ms
step:1823/2330 train_time:108352ms step_avg:59.44ms
step:1824/2330 train_time:108414ms step_avg:59.44ms
step:1825/2330 train_time:108472ms step_avg:59.44ms
step:1826/2330 train_time:108534ms step_avg:59.44ms
step:1827/2330 train_time:108592ms step_avg:59.44ms
step:1828/2330 train_time:108654ms step_avg:59.44ms
step:1829/2330 train_time:108711ms step_avg:59.44ms
step:1830/2330 train_time:108774ms step_avg:59.44ms
step:1831/2330 train_time:108832ms step_avg:59.44ms
step:1832/2330 train_time:108895ms step_avg:59.44ms
step:1833/2330 train_time:108952ms step_avg:59.44ms
step:1834/2330 train_time:109015ms step_avg:59.44ms
step:1835/2330 train_time:109072ms step_avg:59.44ms
step:1836/2330 train_time:109135ms step_avg:59.44ms
step:1837/2330 train_time:109194ms step_avg:59.44ms
step:1838/2330 train_time:109256ms step_avg:59.44ms
step:1839/2330 train_time:109313ms step_avg:59.44ms
step:1840/2330 train_time:109375ms step_avg:59.44ms
step:1841/2330 train_time:109433ms step_avg:59.44ms
step:1842/2330 train_time:109495ms step_avg:59.44ms
step:1843/2330 train_time:109553ms step_avg:59.44ms
step:1844/2330 train_time:109615ms step_avg:59.44ms
step:1845/2330 train_time:109672ms step_avg:59.44ms
step:1846/2330 train_time:109734ms step_avg:59.44ms
step:1847/2330 train_time:109792ms step_avg:59.44ms
step:1848/2330 train_time:109854ms step_avg:59.44ms
step:1849/2330 train_time:109912ms step_avg:59.44ms
step:1850/2330 train_time:109974ms step_avg:59.45ms
step:1851/2330 train_time:110032ms step_avg:59.44ms
step:1852/2330 train_time:110094ms step_avg:59.45ms
step:1853/2330 train_time:110151ms step_avg:59.44ms
step:1854/2330 train_time:110215ms step_avg:59.45ms
step:1855/2330 train_time:110273ms step_avg:59.45ms
step:1856/2330 train_time:110335ms step_avg:59.45ms
step:1857/2330 train_time:110393ms step_avg:59.45ms
step:1858/2330 train_time:110455ms step_avg:59.45ms
step:1859/2330 train_time:110512ms step_avg:59.45ms
step:1860/2330 train_time:110575ms step_avg:59.45ms
step:1861/2330 train_time:110633ms step_avg:59.45ms
step:1862/2330 train_time:110695ms step_avg:59.45ms
step:1863/2330 train_time:110752ms step_avg:59.45ms
step:1864/2330 train_time:110814ms step_avg:59.45ms
step:1865/2330 train_time:110871ms step_avg:59.45ms
step:1866/2330 train_time:110934ms step_avg:59.45ms
step:1867/2330 train_time:110991ms step_avg:59.45ms
step:1868/2330 train_time:111054ms step_avg:59.45ms
step:1869/2330 train_time:111112ms step_avg:59.45ms
step:1870/2330 train_time:111174ms step_avg:59.45ms
step:1871/2330 train_time:111232ms step_avg:59.45ms
step:1872/2330 train_time:111294ms step_avg:59.45ms
step:1873/2330 train_time:111351ms step_avg:59.45ms
step:1874/2330 train_time:111414ms step_avg:59.45ms
step:1875/2330 train_time:111472ms step_avg:59.45ms
step:1876/2330 train_time:111534ms step_avg:59.45ms
step:1877/2330 train_time:111591ms step_avg:59.45ms
step:1878/2330 train_time:111654ms step_avg:59.45ms
step:1879/2330 train_time:111711ms step_avg:59.45ms
step:1880/2330 train_time:111774ms step_avg:59.45ms
step:1881/2330 train_time:111832ms step_avg:59.45ms
step:1882/2330 train_time:111894ms step_avg:59.46ms
step:1883/2330 train_time:111951ms step_avg:59.45ms
step:1884/2330 train_time:112014ms step_avg:59.46ms
step:1885/2330 train_time:112072ms step_avg:59.45ms
step:1886/2330 train_time:112134ms step_avg:59.46ms
step:1887/2330 train_time:112191ms step_avg:59.45ms
step:1888/2330 train_time:112254ms step_avg:59.46ms
step:1889/2330 train_time:112311ms step_avg:59.46ms
step:1890/2330 train_time:112374ms step_avg:59.46ms
step:1891/2330 train_time:112432ms step_avg:59.46ms
step:1892/2330 train_time:112495ms step_avg:59.46ms
step:1893/2330 train_time:112552ms step_avg:59.46ms
step:1894/2330 train_time:112614ms step_avg:59.46ms
step:1895/2330 train_time:112672ms step_avg:59.46ms
step:1896/2330 train_time:112734ms step_avg:59.46ms
step:1897/2330 train_time:112792ms step_avg:59.46ms
step:1898/2330 train_time:112854ms step_avg:59.46ms
step:1899/2330 train_time:112912ms step_avg:59.46ms
step:1900/2330 train_time:112974ms step_avg:59.46ms
step:1901/2330 train_time:113032ms step_avg:59.46ms
step:1902/2330 train_time:113095ms step_avg:59.46ms
step:1903/2330 train_time:113152ms step_avg:59.46ms
step:1904/2330 train_time:113215ms step_avg:59.46ms
step:1905/2330 train_time:113273ms step_avg:59.46ms
step:1906/2330 train_time:113336ms step_avg:59.46ms
step:1907/2330 train_time:113393ms step_avg:59.46ms
step:1908/2330 train_time:113456ms step_avg:59.46ms
step:1909/2330 train_time:113513ms step_avg:59.46ms
step:1910/2330 train_time:113576ms step_avg:59.46ms
step:1911/2330 train_time:113633ms step_avg:59.46ms
step:1912/2330 train_time:113695ms step_avg:59.46ms
step:1913/2330 train_time:113752ms step_avg:59.46ms
step:1914/2330 train_time:113815ms step_avg:59.46ms
step:1915/2330 train_time:113872ms step_avg:59.46ms
step:1916/2330 train_time:113935ms step_avg:59.46ms
step:1917/2330 train_time:113993ms step_avg:59.46ms
step:1918/2330 train_time:114054ms step_avg:59.47ms
step:1919/2330 train_time:114112ms step_avg:59.46ms
step:1920/2330 train_time:114175ms step_avg:59.47ms
step:1921/2330 train_time:114233ms step_avg:59.47ms
step:1922/2330 train_time:114296ms step_avg:59.47ms
step:1923/2330 train_time:114353ms step_avg:59.47ms
step:1924/2330 train_time:114417ms step_avg:59.47ms
step:1925/2330 train_time:114474ms step_avg:59.47ms
step:1926/2330 train_time:114536ms step_avg:59.47ms
step:1927/2330 train_time:114593ms step_avg:59.47ms
step:1928/2330 train_time:114656ms step_avg:59.47ms
step:1929/2330 train_time:114714ms step_avg:59.47ms
step:1930/2330 train_time:114776ms step_avg:59.47ms
step:1931/2330 train_time:114833ms step_avg:59.47ms
step:1932/2330 train_time:114895ms step_avg:59.47ms
step:1933/2330 train_time:114952ms step_avg:59.47ms
step:1934/2330 train_time:115015ms step_avg:59.47ms
step:1935/2330 train_time:115073ms step_avg:59.47ms
step:1936/2330 train_time:115135ms step_avg:59.47ms
step:1937/2330 train_time:115193ms step_avg:59.47ms
step:1938/2330 train_time:115256ms step_avg:59.47ms
step:1939/2330 train_time:115313ms step_avg:59.47ms
step:1940/2330 train_time:115375ms step_avg:59.47ms
step:1941/2330 train_time:115433ms step_avg:59.47ms
step:1942/2330 train_time:115495ms step_avg:59.47ms
step:1943/2330 train_time:115552ms step_avg:59.47ms
step:1944/2330 train_time:115615ms step_avg:59.47ms
step:1945/2330 train_time:115672ms step_avg:59.47ms
step:1946/2330 train_time:115735ms step_avg:59.47ms
step:1947/2330 train_time:115793ms step_avg:59.47ms
step:1948/2330 train_time:115855ms step_avg:59.47ms
step:1949/2330 train_time:115913ms step_avg:59.47ms
step:1950/2330 train_time:115976ms step_avg:59.47ms
step:1951/2330 train_time:116033ms step_avg:59.47ms
step:1952/2330 train_time:116096ms step_avg:59.48ms
step:1953/2330 train_time:116153ms step_avg:59.47ms
step:1954/2330 train_time:116216ms step_avg:59.48ms
step:1955/2330 train_time:116273ms step_avg:59.47ms
step:1956/2330 train_time:116335ms step_avg:59.48ms
step:1957/2330 train_time:116392ms step_avg:59.47ms
step:1958/2330 train_time:116455ms step_avg:59.48ms
step:1959/2330 train_time:116513ms step_avg:59.48ms
step:1960/2330 train_time:116575ms step_avg:59.48ms
step:1961/2330 train_time:116633ms step_avg:59.48ms
step:1962/2330 train_time:116694ms step_avg:59.48ms
step:1963/2330 train_time:116751ms step_avg:59.48ms
step:1964/2330 train_time:116814ms step_avg:59.48ms
step:1965/2330 train_time:116872ms step_avg:59.48ms
step:1966/2330 train_time:116934ms step_avg:59.48ms
step:1967/2330 train_time:116992ms step_avg:59.48ms
step:1968/2330 train_time:117055ms step_avg:59.48ms
step:1969/2330 train_time:117112ms step_avg:59.48ms
step:1970/2330 train_time:117175ms step_avg:59.48ms
step:1971/2330 train_time:117233ms step_avg:59.48ms
step:1972/2330 train_time:117296ms step_avg:59.48ms
step:1973/2330 train_time:117353ms step_avg:59.48ms
step:1974/2330 train_time:117415ms step_avg:59.48ms
step:1975/2330 train_time:117472ms step_avg:59.48ms
step:1976/2330 train_time:117534ms step_avg:59.48ms
step:1977/2330 train_time:117592ms step_avg:59.48ms
step:1978/2330 train_time:117655ms step_avg:59.48ms
step:1979/2330 train_time:117712ms step_avg:59.48ms
step:1980/2330 train_time:117775ms step_avg:59.48ms
step:1981/2330 train_time:117832ms step_avg:59.48ms
step:1982/2330 train_time:117895ms step_avg:59.48ms
step:1983/2330 train_time:117952ms step_avg:59.48ms
step:1984/2330 train_time:118015ms step_avg:59.48ms
step:1985/2330 train_time:118072ms step_avg:59.48ms
step:1986/2330 train_time:118135ms step_avg:59.48ms
step:1987/2330 train_time:118193ms step_avg:59.48ms
step:1988/2330 train_time:118255ms step_avg:59.48ms
step:1989/2330 train_time:118312ms step_avg:59.48ms
step:1990/2330 train_time:118375ms step_avg:59.48ms
step:1991/2330 train_time:118432ms step_avg:59.48ms
step:1992/2330 train_time:118495ms step_avg:59.49ms
step:1993/2330 train_time:118552ms step_avg:59.48ms
step:1994/2330 train_time:118615ms step_avg:59.49ms
step:1995/2330 train_time:118672ms step_avg:59.48ms
step:1996/2330 train_time:118735ms step_avg:59.49ms
step:1997/2330 train_time:118792ms step_avg:59.49ms
step:1998/2330 train_time:118855ms step_avg:59.49ms
step:1999/2330 train_time:118912ms step_avg:59.49ms
step:2000/2330 train_time:118975ms step_avg:59.49ms
step:2000/2330 val_loss:3.7611 train_time:119056ms step_avg:59.53ms
step:2001/2330 train_time:119075ms step_avg:59.51ms
step:2002/2330 train_time:119099ms step_avg:59.49ms
step:2003/2330 train_time:119158ms step_avg:59.49ms
step:2004/2330 train_time:119227ms step_avg:59.49ms
step:2005/2330 train_time:119284ms step_avg:59.49ms
step:2006/2330 train_time:119347ms step_avg:59.50ms
step:2007/2330 train_time:119405ms step_avg:59.49ms
step:2008/2330 train_time:119467ms step_avg:59.50ms
step:2009/2330 train_time:119524ms step_avg:59.49ms
step:2010/2330 train_time:119586ms step_avg:59.50ms
step:2011/2330 train_time:119643ms step_avg:59.49ms
step:2012/2330 train_time:119704ms step_avg:59.50ms
step:2013/2330 train_time:119761ms step_avg:59.49ms
step:2014/2330 train_time:119824ms step_avg:59.50ms
step:2015/2330 train_time:119881ms step_avg:59.49ms
step:2016/2330 train_time:119942ms step_avg:59.50ms
step:2017/2330 train_time:119999ms step_avg:59.49ms
step:2018/2330 train_time:120064ms step_avg:59.50ms
step:2019/2330 train_time:120121ms step_avg:59.50ms
step:2020/2330 train_time:120189ms step_avg:59.50ms
step:2021/2330 train_time:120246ms step_avg:59.50ms
step:2022/2330 train_time:120310ms step_avg:59.50ms
step:2023/2330 train_time:120367ms step_avg:59.50ms
step:2024/2330 train_time:120431ms step_avg:59.50ms
step:2025/2330 train_time:120489ms step_avg:59.50ms
step:2026/2330 train_time:120550ms step_avg:59.50ms
step:2027/2330 train_time:120608ms step_avg:59.50ms
step:2028/2330 train_time:120669ms step_avg:59.50ms
step:2029/2330 train_time:120726ms step_avg:59.50ms
step:2030/2330 train_time:120787ms step_avg:59.50ms
step:2031/2330 train_time:120844ms step_avg:59.50ms
step:2032/2330 train_time:120906ms step_avg:59.50ms
step:2033/2330 train_time:120963ms step_avg:59.50ms
step:2034/2330 train_time:121026ms step_avg:59.50ms
step:2035/2330 train_time:121083ms step_avg:59.50ms
step:2036/2330 train_time:121147ms step_avg:59.50ms
step:2037/2330 train_time:121205ms step_avg:59.50ms
step:2038/2330 train_time:121269ms step_avg:59.50ms
step:2039/2330 train_time:121327ms step_avg:59.50ms
step:2040/2330 train_time:121390ms step_avg:59.50ms
step:2041/2330 train_time:121447ms step_avg:59.50ms
step:2042/2330 train_time:121509ms step_avg:59.50ms
step:2043/2330 train_time:121566ms step_avg:59.50ms
step:2044/2330 train_time:121629ms step_avg:59.51ms
step:2045/2330 train_time:121686ms step_avg:59.50ms
step:2046/2330 train_time:121748ms step_avg:59.51ms
step:2047/2330 train_time:121806ms step_avg:59.50ms
step:2048/2330 train_time:121868ms step_avg:59.51ms
step:2049/2330 train_time:121925ms step_avg:59.50ms
step:2050/2330 train_time:121987ms step_avg:59.51ms
step:2051/2330 train_time:122046ms step_avg:59.51ms
step:2052/2330 train_time:122108ms step_avg:59.51ms
step:2053/2330 train_time:122166ms step_avg:59.51ms
step:2054/2330 train_time:122228ms step_avg:59.51ms
step:2055/2330 train_time:122285ms step_avg:59.51ms
step:2056/2330 train_time:122348ms step_avg:59.51ms
step:2057/2330 train_time:122406ms step_avg:59.51ms
step:2058/2330 train_time:122468ms step_avg:59.51ms
step:2059/2330 train_time:122525ms step_avg:59.51ms
step:2060/2330 train_time:122589ms step_avg:59.51ms
step:2061/2330 train_time:122645ms step_avg:59.51ms
step:2062/2330 train_time:122708ms step_avg:59.51ms
step:2063/2330 train_time:122766ms step_avg:59.51ms
step:2064/2330 train_time:122828ms step_avg:59.51ms
step:2065/2330 train_time:122885ms step_avg:59.51ms
step:2066/2330 train_time:122947ms step_avg:59.51ms
step:2067/2330 train_time:123004ms step_avg:59.51ms
step:2068/2330 train_time:123068ms step_avg:59.51ms
step:2069/2330 train_time:123126ms step_avg:59.51ms
step:2070/2330 train_time:123188ms step_avg:59.51ms
step:2071/2330 train_time:123246ms step_avg:59.51ms
step:2072/2330 train_time:123309ms step_avg:59.51ms
step:2073/2330 train_time:123367ms step_avg:59.51ms
step:2074/2330 train_time:123428ms step_avg:59.51ms
step:2075/2330 train_time:123486ms step_avg:59.51ms
step:2076/2330 train_time:123548ms step_avg:59.51ms
step:2077/2330 train_time:123605ms step_avg:59.51ms
step:2078/2330 train_time:123668ms step_avg:59.51ms
step:2079/2330 train_time:123725ms step_avg:59.51ms
step:2080/2330 train_time:123787ms step_avg:59.51ms
step:2081/2330 train_time:123844ms step_avg:59.51ms
step:2082/2330 train_time:123907ms step_avg:59.51ms
step:2083/2330 train_time:123965ms step_avg:59.51ms
step:2084/2330 train_time:124028ms step_avg:59.51ms
step:2085/2330 train_time:124084ms step_avg:59.51ms
step:2086/2330 train_time:124147ms step_avg:59.51ms
step:2087/2330 train_time:124205ms step_avg:59.51ms
step:2088/2330 train_time:124268ms step_avg:59.52ms
step:2089/2330 train_time:124325ms step_avg:59.51ms
step:2090/2330 train_time:124387ms step_avg:59.52ms
step:2091/2330 train_time:124444ms step_avg:59.51ms
step:2092/2330 train_time:124507ms step_avg:59.52ms
step:2093/2330 train_time:124565ms step_avg:59.52ms
step:2094/2330 train_time:124627ms step_avg:59.52ms
step:2095/2330 train_time:124684ms step_avg:59.52ms
step:2096/2330 train_time:124746ms step_avg:59.52ms
step:2097/2330 train_time:124804ms step_avg:59.52ms
step:2098/2330 train_time:124866ms step_avg:59.52ms
step:2099/2330 train_time:124924ms step_avg:59.52ms
step:2100/2330 train_time:124987ms step_avg:59.52ms
step:2101/2330 train_time:125044ms step_avg:59.52ms
step:2102/2330 train_time:125107ms step_avg:59.52ms
step:2103/2330 train_time:125165ms step_avg:59.52ms
step:2104/2330 train_time:125227ms step_avg:59.52ms
step:2105/2330 train_time:125285ms step_avg:59.52ms
step:2106/2330 train_time:125347ms step_avg:59.52ms
step:2107/2330 train_time:125404ms step_avg:59.52ms
step:2108/2330 train_time:125468ms step_avg:59.52ms
step:2109/2330 train_time:125525ms step_avg:59.52ms
step:2110/2330 train_time:125587ms step_avg:59.52ms
step:2111/2330 train_time:125644ms step_avg:59.52ms
step:2112/2330 train_time:125707ms step_avg:59.52ms
step:2113/2330 train_time:125764ms step_avg:59.52ms
step:2114/2330 train_time:125828ms step_avg:59.52ms
step:2115/2330 train_time:125885ms step_avg:59.52ms
step:2116/2330 train_time:125947ms step_avg:59.52ms
step:2117/2330 train_time:126004ms step_avg:59.52ms
step:2118/2330 train_time:126067ms step_avg:59.52ms
step:2119/2330 train_time:126124ms step_avg:59.52ms
step:2120/2330 train_time:126188ms step_avg:59.52ms
step:2121/2330 train_time:126245ms step_avg:59.52ms
step:2122/2330 train_time:126307ms step_avg:59.52ms
step:2123/2330 train_time:126365ms step_avg:59.52ms
step:2124/2330 train_time:126428ms step_avg:59.52ms
step:2125/2330 train_time:126485ms step_avg:59.52ms
step:2126/2330 train_time:126547ms step_avg:59.52ms
step:2127/2330 train_time:126604ms step_avg:59.52ms
step:2128/2330 train_time:126667ms step_avg:59.52ms
step:2129/2330 train_time:126724ms step_avg:59.52ms
step:2130/2330 train_time:126787ms step_avg:59.52ms
step:2131/2330 train_time:126844ms step_avg:59.52ms
step:2132/2330 train_time:126906ms step_avg:59.52ms
step:2133/2330 train_time:126964ms step_avg:59.52ms
step:2134/2330 train_time:127026ms step_avg:59.53ms
step:2135/2330 train_time:127083ms step_avg:59.52ms
step:2136/2330 train_time:127146ms step_avg:59.53ms
step:2137/2330 train_time:127203ms step_avg:59.52ms
step:2138/2330 train_time:127267ms step_avg:59.53ms
step:2139/2330 train_time:127324ms step_avg:59.53ms
step:2140/2330 train_time:127387ms step_avg:59.53ms
step:2141/2330 train_time:127444ms step_avg:59.53ms
step:2142/2330 train_time:127507ms step_avg:59.53ms
step:2143/2330 train_time:127564ms step_avg:59.53ms
step:2144/2330 train_time:127627ms step_avg:59.53ms
step:2145/2330 train_time:127684ms step_avg:59.53ms
step:2146/2330 train_time:127746ms step_avg:59.53ms
step:2147/2330 train_time:127804ms step_avg:59.53ms
step:2148/2330 train_time:127867ms step_avg:59.53ms
step:2149/2330 train_time:127924ms step_avg:59.53ms
step:2150/2330 train_time:127987ms step_avg:59.53ms
step:2151/2330 train_time:128045ms step_avg:59.53ms
step:2152/2330 train_time:128107ms step_avg:59.53ms
step:2153/2330 train_time:128165ms step_avg:59.53ms
step:2154/2330 train_time:128227ms step_avg:59.53ms
step:2155/2330 train_time:128284ms step_avg:59.53ms
step:2156/2330 train_time:128347ms step_avg:59.53ms
step:2157/2330 train_time:128404ms step_avg:59.53ms
step:2158/2330 train_time:128468ms step_avg:59.53ms
step:2159/2330 train_time:128525ms step_avg:59.53ms
step:2160/2330 train_time:128589ms step_avg:59.53ms
step:2161/2330 train_time:128646ms step_avg:59.53ms
step:2162/2330 train_time:128708ms step_avg:59.53ms
step:2163/2330 train_time:128766ms step_avg:59.53ms
step:2164/2330 train_time:128828ms step_avg:59.53ms
step:2165/2330 train_time:128885ms step_avg:59.53ms
step:2166/2330 train_time:128948ms step_avg:59.53ms
step:2167/2330 train_time:129005ms step_avg:59.53ms
step:2168/2330 train_time:129068ms step_avg:59.53ms
step:2169/2330 train_time:129124ms step_avg:59.53ms
step:2170/2330 train_time:129188ms step_avg:59.53ms
step:2171/2330 train_time:129244ms step_avg:59.53ms
step:2172/2330 train_time:129307ms step_avg:59.53ms
step:2173/2330 train_time:129364ms step_avg:59.53ms
step:2174/2330 train_time:129427ms step_avg:59.53ms
step:2175/2330 train_time:129485ms step_avg:59.53ms
step:2176/2330 train_time:129547ms step_avg:59.53ms
step:2177/2330 train_time:129604ms step_avg:59.53ms
step:2178/2330 train_time:129667ms step_avg:59.53ms
step:2179/2330 train_time:129725ms step_avg:59.53ms
step:2180/2330 train_time:129788ms step_avg:59.54ms
step:2181/2330 train_time:129845ms step_avg:59.53ms
step:2182/2330 train_time:129908ms step_avg:59.54ms
step:2183/2330 train_time:129965ms step_avg:59.54ms
step:2184/2330 train_time:130028ms step_avg:59.54ms
step:2185/2330 train_time:130085ms step_avg:59.54ms
step:2186/2330 train_time:130148ms step_avg:59.54ms
step:2187/2330 train_time:130205ms step_avg:59.54ms
step:2188/2330 train_time:130268ms step_avg:59.54ms
step:2189/2330 train_time:130325ms step_avg:59.54ms
step:2190/2330 train_time:130387ms step_avg:59.54ms
step:2191/2330 train_time:130445ms step_avg:59.54ms
step:2192/2330 train_time:130508ms step_avg:59.54ms
step:2193/2330 train_time:130565ms step_avg:59.54ms
step:2194/2330 train_time:130628ms step_avg:59.54ms
step:2195/2330 train_time:130684ms step_avg:59.54ms
step:2196/2330 train_time:130747ms step_avg:59.54ms
step:2197/2330 train_time:130804ms step_avg:59.54ms
step:2198/2330 train_time:130867ms step_avg:59.54ms
step:2199/2330 train_time:130925ms step_avg:59.54ms
step:2200/2330 train_time:130988ms step_avg:59.54ms
step:2201/2330 train_time:131045ms step_avg:59.54ms
step:2202/2330 train_time:131108ms step_avg:59.54ms
step:2203/2330 train_time:131165ms step_avg:59.54ms
step:2204/2330 train_time:131228ms step_avg:59.54ms
step:2205/2330 train_time:131286ms step_avg:59.54ms
step:2206/2330 train_time:131348ms step_avg:59.54ms
step:2207/2330 train_time:131405ms step_avg:59.54ms
step:2208/2330 train_time:131468ms step_avg:59.54ms
step:2209/2330 train_time:131525ms step_avg:59.54ms
step:2210/2330 train_time:131587ms step_avg:59.54ms
step:2211/2330 train_time:131645ms step_avg:59.54ms
step:2212/2330 train_time:131707ms step_avg:59.54ms
step:2213/2330 train_time:131765ms step_avg:59.54ms
step:2214/2330 train_time:131828ms step_avg:59.54ms
step:2215/2330 train_time:131885ms step_avg:59.54ms
step:2216/2330 train_time:131948ms step_avg:59.54ms
step:2217/2330 train_time:132006ms step_avg:59.54ms
step:2218/2330 train_time:132067ms step_avg:59.54ms
step:2219/2330 train_time:132125ms step_avg:59.54ms
step:2220/2330 train_time:132188ms step_avg:59.54ms
step:2221/2330 train_time:132246ms step_avg:59.54ms
step:2222/2330 train_time:132308ms step_avg:59.54ms
step:2223/2330 train_time:132365ms step_avg:59.54ms
step:2224/2330 train_time:132428ms step_avg:59.55ms
step:2225/2330 train_time:132486ms step_avg:59.54ms
step:2226/2330 train_time:132547ms step_avg:59.54ms
step:2227/2330 train_time:132605ms step_avg:59.54ms
step:2228/2330 train_time:132667ms step_avg:59.55ms
step:2229/2330 train_time:132724ms step_avg:59.54ms
step:2230/2330 train_time:132787ms step_avg:59.55ms
step:2231/2330 train_time:132844ms step_avg:59.54ms
step:2232/2330 train_time:132908ms step_avg:59.55ms
step:2233/2330 train_time:132965ms step_avg:59.55ms
step:2234/2330 train_time:133028ms step_avg:59.55ms
step:2235/2330 train_time:133084ms step_avg:59.55ms
step:2236/2330 train_time:133148ms step_avg:59.55ms
step:2237/2330 train_time:133205ms step_avg:59.55ms
step:2238/2330 train_time:133268ms step_avg:59.55ms
step:2239/2330 train_time:133326ms step_avg:59.55ms
step:2240/2330 train_time:133388ms step_avg:59.55ms
step:2241/2330 train_time:133445ms step_avg:59.55ms
step:2242/2330 train_time:133507ms step_avg:59.55ms
step:2243/2330 train_time:133564ms step_avg:59.55ms
step:2244/2330 train_time:133627ms step_avg:59.55ms
step:2245/2330 train_time:133684ms step_avg:59.55ms
step:2246/2330 train_time:133746ms step_avg:59.55ms
step:2247/2330 train_time:133804ms step_avg:59.55ms
step:2248/2330 train_time:133867ms step_avg:59.55ms
step:2249/2330 train_time:133925ms step_avg:59.55ms
step:2250/2330 train_time:133987ms step_avg:59.55ms
step:2250/2330 val_loss:3.7112 train_time:134067ms step_avg:59.59ms
step:2251/2330 train_time:134087ms step_avg:59.57ms
step:2252/2330 train_time:134110ms step_avg:59.55ms
step:2253/2330 train_time:134172ms step_avg:59.55ms
step:2254/2330 train_time:134238ms step_avg:59.56ms
step:2255/2330 train_time:134296ms step_avg:59.55ms
step:2256/2330 train_time:134358ms step_avg:59.56ms
step:2257/2330 train_time:134415ms step_avg:59.55ms
step:2258/2330 train_time:134478ms step_avg:59.56ms
step:2259/2330 train_time:134535ms step_avg:59.56ms
step:2260/2330 train_time:134597ms step_avg:59.56ms
step:2261/2330 train_time:134654ms step_avg:59.56ms
step:2262/2330 train_time:134715ms step_avg:59.56ms
step:2263/2330 train_time:134773ms step_avg:59.55ms
step:2264/2330 train_time:134834ms step_avg:59.56ms
step:2265/2330 train_time:134891ms step_avg:59.55ms
step:2266/2330 train_time:134953ms step_avg:59.56ms
step:2267/2330 train_time:135010ms step_avg:59.55ms
step:2268/2330 train_time:135075ms step_avg:59.56ms
step:2269/2330 train_time:135133ms step_avg:59.56ms
step:2270/2330 train_time:135199ms step_avg:59.56ms
step:2271/2330 train_time:135256ms step_avg:59.56ms
step:2272/2330 train_time:135321ms step_avg:59.56ms
step:2273/2330 train_time:135379ms step_avg:59.56ms
step:2274/2330 train_time:135440ms step_avg:59.56ms
step:2275/2330 train_time:135497ms step_avg:59.56ms
step:2276/2330 train_time:135559ms step_avg:59.56ms
step:2277/2330 train_time:135615ms step_avg:59.56ms
step:2278/2330 train_time:135678ms step_avg:59.56ms
step:2279/2330 train_time:135735ms step_avg:59.56ms
step:2280/2330 train_time:135798ms step_avg:59.56ms
step:2281/2330 train_time:135854ms step_avg:59.56ms
step:2282/2330 train_time:135916ms step_avg:59.56ms
step:2283/2330 train_time:135974ms step_avg:59.56ms
step:2284/2330 train_time:136037ms step_avg:59.56ms
step:2285/2330 train_time:136094ms step_avg:59.56ms
step:2286/2330 train_time:136159ms step_avg:59.56ms
step:2287/2330 train_time:136216ms step_avg:59.56ms
step:2288/2330 train_time:136281ms step_avg:59.56ms
step:2289/2330 train_time:136338ms step_avg:59.56ms
step:2290/2330 train_time:136401ms step_avg:59.56ms
step:2291/2330 train_time:136458ms step_avg:59.56ms
step:2292/2330 train_time:136521ms step_avg:59.56ms
step:2293/2330 train_time:136578ms step_avg:59.56ms
step:2294/2330 train_time:136640ms step_avg:59.56ms
step:2295/2330 train_time:136698ms step_avg:59.56ms
step:2296/2330 train_time:136759ms step_avg:59.56ms
step:2297/2330 train_time:136816ms step_avg:59.56ms
step:2298/2330 train_time:136879ms step_avg:59.56ms
step:2299/2330 train_time:136936ms step_avg:59.56ms
step:2300/2330 train_time:136999ms step_avg:59.56ms
step:2301/2330 train_time:137057ms step_avg:59.56ms
step:2302/2330 train_time:137120ms step_avg:59.57ms
step:2303/2330 train_time:137178ms step_avg:59.56ms
step:2304/2330 train_time:137242ms step_avg:59.57ms
step:2305/2330 train_time:137299ms step_avg:59.57ms
step:2306/2330 train_time:137362ms step_avg:59.57ms
step:2307/2330 train_time:137419ms step_avg:59.57ms
step:2308/2330 train_time:137482ms step_avg:59.57ms
step:2309/2330 train_time:137540ms step_avg:59.57ms
step:2310/2330 train_time:137602ms step_avg:59.57ms
step:2311/2330 train_time:137659ms step_avg:59.57ms
step:2312/2330 train_time:137722ms step_avg:59.57ms
step:2313/2330 train_time:137779ms step_avg:59.57ms
step:2314/2330 train_time:137841ms step_avg:59.57ms
step:2315/2330 train_time:137898ms step_avg:59.57ms
step:2316/2330 train_time:137960ms step_avg:59.57ms
step:2317/2330 train_time:138018ms step_avg:59.57ms
step:2318/2330 train_time:138081ms step_avg:59.57ms
step:2319/2330 train_time:138138ms step_avg:59.57ms
step:2320/2330 train_time:138202ms step_avg:59.57ms
step:2321/2330 train_time:138259ms step_avg:59.57ms
step:2322/2330 train_time:138322ms step_avg:59.57ms
step:2323/2330 train_time:138379ms step_avg:59.57ms
step:2324/2330 train_time:138442ms step_avg:59.57ms
step:2325/2330 train_time:138499ms step_avg:59.57ms
step:2326/2330 train_time:138561ms step_avg:59.57ms
step:2327/2330 train_time:138618ms step_avg:59.57ms
step:2328/2330 train_time:138682ms step_avg:59.57ms
step:2329/2330 train_time:138739ms step_avg:59.57ms
step:2330/2330 train_time:138802ms step_avg:59.57ms
step:2330/2330 val_loss:3.6959 train_time:138880ms step_avg:59.61ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
