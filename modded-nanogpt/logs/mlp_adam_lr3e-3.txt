import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr3e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=3e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:39:08 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:62ms step_avg:62.35ms
step:2/2330 train_time:144ms step_avg:72.09ms
step:3/2330 train_time:211ms step_avg:70.30ms
step:4/2330 train_time:222ms step_avg:55.56ms
step:5/2330 train_time:232ms step_avg:46.40ms
step:6/2330 train_time:242ms step_avg:40.40ms
step:7/2330 train_time:264ms step_avg:37.72ms
step:8/2330 train_time:304ms step_avg:37.98ms
step:9/2330 train_time:337ms step_avg:37.47ms
step:10/2330 train_time:377ms step_avg:37.74ms
step:11/2330 train_time:411ms step_avg:37.36ms
step:12/2330 train_time:451ms step_avg:37.57ms
step:13/2330 train_time:485ms step_avg:37.27ms
step:14/2330 train_time:525ms step_avg:37.47ms
step:15/2330 train_time:558ms step_avg:37.20ms
step:16/2330 train_time:598ms step_avg:37.38ms
step:17/2330 train_time:632ms step_avg:37.18ms
step:18/2330 train_time:672ms step_avg:37.33ms
step:19/2330 train_time:706ms step_avg:37.15ms
step:20/2330 train_time:746ms step_avg:37.29ms
step:21/2330 train_time:780ms step_avg:37.13ms
step:22/2330 train_time:820ms step_avg:37.28ms
step:23/2330 train_time:854ms step_avg:37.12ms
step:24/2330 train_time:894ms step_avg:37.26ms
step:25/2330 train_time:928ms step_avg:37.12ms
step:26/2330 train_time:968ms step_avg:37.23ms
step:27/2330 train_time:1002ms step_avg:37.10ms
step:28/2330 train_time:1042ms step_avg:37.22ms
step:29/2330 train_time:1076ms step_avg:37.11ms
step:30/2330 train_time:1118ms step_avg:37.27ms
step:31/2330 train_time:1156ms step_avg:37.30ms
step:32/2330 train_time:1198ms step_avg:37.43ms
step:33/2330 train_time:1235ms step_avg:37.43ms
step:34/2330 train_time:1276ms step_avg:37.52ms
step:35/2330 train_time:1312ms step_avg:37.49ms
step:36/2330 train_time:1352ms step_avg:37.57ms
step:37/2330 train_time:1388ms step_avg:37.50ms
step:38/2330 train_time:1428ms step_avg:37.58ms
step:39/2330 train_time:1463ms step_avg:37.51ms
step:40/2330 train_time:1503ms step_avg:37.58ms
step:41/2330 train_time:1538ms step_avg:37.51ms
step:42/2330 train_time:1578ms step_avg:37.57ms
step:43/2330 train_time:1613ms step_avg:37.50ms
step:44/2330 train_time:1653ms step_avg:37.56ms
step:45/2330 train_time:1687ms step_avg:37.49ms
step:46/2330 train_time:1727ms step_avg:37.55ms
step:47/2330 train_time:1762ms step_avg:37.49ms
step:48/2330 train_time:1802ms step_avg:37.54ms
step:49/2330 train_time:1836ms step_avg:37.48ms
step:50/2330 train_time:1877ms step_avg:37.53ms
step:51/2330 train_time:1911ms step_avg:37.47ms
step:52/2330 train_time:1951ms step_avg:37.52ms
step:53/2330 train_time:1986ms step_avg:37.46ms
step:54/2330 train_time:2026ms step_avg:37.51ms
step:55/2330 train_time:2060ms step_avg:37.46ms
step:56/2330 train_time:2102ms step_avg:37.53ms
step:57/2330 train_time:2137ms step_avg:37.49ms
step:58/2330 train_time:2178ms step_avg:37.56ms
step:59/2330 train_time:2215ms step_avg:37.53ms
step:60/2330 train_time:2255ms step_avg:37.59ms
step:61/2330 train_time:2292ms step_avg:37.57ms
step:62/2330 train_time:2332ms step_avg:37.62ms
step:63/2330 train_time:2368ms step_avg:37.59ms
step:64/2330 train_time:2409ms step_avg:37.64ms
step:65/2330 train_time:2445ms step_avg:37.62ms
step:66/2330 train_time:2486ms step_avg:37.66ms
step:67/2330 train_time:2521ms step_avg:37.63ms
step:68/2330 train_time:2562ms step_avg:37.67ms
step:69/2330 train_time:2596ms step_avg:37.62ms
step:70/2330 train_time:2636ms step_avg:37.66ms
step:71/2330 train_time:2671ms step_avg:37.61ms
step:72/2330 train_time:2711ms step_avg:37.66ms
step:73/2330 train_time:2746ms step_avg:37.61ms
step:74/2330 train_time:2786ms step_avg:37.65ms
step:75/2330 train_time:2820ms step_avg:37.60ms
step:76/2330 train_time:2861ms step_avg:37.64ms
step:77/2330 train_time:2895ms step_avg:37.60ms
step:78/2330 train_time:2935ms step_avg:37.63ms
step:79/2330 train_time:2970ms step_avg:37.59ms
step:80/2330 train_time:3010ms step_avg:37.63ms
step:81/2330 train_time:3045ms step_avg:37.59ms
step:82/2330 train_time:3085ms step_avg:37.63ms
step:83/2330 train_time:3121ms step_avg:37.60ms
step:84/2330 train_time:3162ms step_avg:37.64ms
step:85/2330 train_time:3197ms step_avg:37.61ms
step:86/2330 train_time:3237ms step_avg:37.64ms
step:87/2330 train_time:3273ms step_avg:37.62ms
step:88/2330 train_time:3313ms step_avg:37.65ms
step:89/2330 train_time:3349ms step_avg:37.63ms
step:90/2330 train_time:3390ms step_avg:37.67ms
step:91/2330 train_time:3426ms step_avg:37.65ms
step:92/2330 train_time:3467ms step_avg:37.68ms
step:93/2330 train_time:3503ms step_avg:37.66ms
step:94/2330 train_time:3543ms step_avg:37.69ms
step:95/2330 train_time:3578ms step_avg:37.67ms
step:96/2330 train_time:3619ms step_avg:37.70ms
step:97/2330 train_time:3654ms step_avg:37.67ms
step:98/2330 train_time:3694ms step_avg:37.70ms
step:99/2330 train_time:3729ms step_avg:37.67ms
step:100/2330 train_time:3769ms step_avg:37.69ms
step:101/2330 train_time:3804ms step_avg:37.67ms
step:102/2330 train_time:3845ms step_avg:37.69ms
step:103/2330 train_time:3879ms step_avg:37.66ms
step:104/2330 train_time:3920ms step_avg:37.69ms
step:105/2330 train_time:3954ms step_avg:37.66ms
step:106/2330 train_time:3995ms step_avg:37.69ms
step:107/2330 train_time:4030ms step_avg:37.66ms
step:108/2330 train_time:4070ms step_avg:37.69ms
step:109/2330 train_time:4105ms step_avg:37.66ms
step:110/2330 train_time:4145ms step_avg:37.68ms
step:111/2330 train_time:4181ms step_avg:37.67ms
step:112/2330 train_time:4221ms step_avg:37.69ms
step:113/2330 train_time:4257ms step_avg:37.67ms
step:114/2330 train_time:4298ms step_avg:37.70ms
step:115/2330 train_time:4333ms step_avg:37.68ms
step:116/2330 train_time:4373ms step_avg:37.70ms
step:117/2330 train_time:4410ms step_avg:37.69ms
step:118/2330 train_time:4451ms step_avg:37.72ms
step:119/2330 train_time:4486ms step_avg:37.70ms
step:120/2330 train_time:4527ms step_avg:37.72ms
step:121/2330 train_time:4562ms step_avg:37.70ms
step:122/2330 train_time:4603ms step_avg:37.73ms
step:123/2330 train_time:4637ms step_avg:37.70ms
step:124/2330 train_time:4678ms step_avg:37.72ms
step:125/2330 train_time:4713ms step_avg:37.70ms
step:126/2330 train_time:4754ms step_avg:37.73ms
step:127/2330 train_time:4789ms step_avg:37.71ms
step:128/2330 train_time:4830ms step_avg:37.73ms
step:129/2330 train_time:4864ms step_avg:37.71ms
step:130/2330 train_time:4905ms step_avg:37.73ms
step:131/2330 train_time:4939ms step_avg:37.70ms
step:132/2330 train_time:4980ms step_avg:37.73ms
step:133/2330 train_time:5014ms step_avg:37.70ms
step:134/2330 train_time:5055ms step_avg:37.72ms
step:135/2330 train_time:5090ms step_avg:37.70ms
step:136/2330 train_time:5131ms step_avg:37.73ms
step:137/2330 train_time:5166ms step_avg:37.71ms
step:138/2330 train_time:5206ms step_avg:37.73ms
step:139/2330 train_time:5241ms step_avg:37.71ms
step:140/2330 train_time:5282ms step_avg:37.73ms
step:141/2330 train_time:5317ms step_avg:37.71ms
step:142/2330 train_time:5358ms step_avg:37.73ms
step:143/2330 train_time:5393ms step_avg:37.71ms
step:144/2330 train_time:5434ms step_avg:37.74ms
step:145/2330 train_time:5470ms step_avg:37.72ms
step:146/2330 train_time:5511ms step_avg:37.75ms
step:147/2330 train_time:5546ms step_avg:37.73ms
step:148/2330 train_time:5586ms step_avg:37.74ms
step:149/2330 train_time:5621ms step_avg:37.73ms
step:150/2330 train_time:5662ms step_avg:37.75ms
step:151/2330 train_time:5697ms step_avg:37.73ms
step:152/2330 train_time:5737ms step_avg:37.75ms
step:153/2330 train_time:5772ms step_avg:37.72ms
step:154/2330 train_time:5812ms step_avg:37.74ms
step:155/2330 train_time:5848ms step_avg:37.73ms
step:156/2330 train_time:5888ms step_avg:37.74ms
step:157/2330 train_time:5923ms step_avg:37.73ms
step:158/2330 train_time:5964ms step_avg:37.74ms
step:159/2330 train_time:5999ms step_avg:37.73ms
step:160/2330 train_time:6040ms step_avg:37.75ms
step:161/2330 train_time:6074ms step_avg:37.73ms
step:162/2330 train_time:6115ms step_avg:37.75ms
step:163/2330 train_time:6149ms step_avg:37.73ms
step:164/2330 train_time:6190ms step_avg:37.74ms
step:165/2330 train_time:6225ms step_avg:37.73ms
step:166/2330 train_time:6265ms step_avg:37.74ms
step:167/2330 train_time:6301ms step_avg:37.73ms
step:168/2330 train_time:6341ms step_avg:37.74ms
step:169/2330 train_time:6377ms step_avg:37.73ms
step:170/2330 train_time:6418ms step_avg:37.75ms
step:171/2330 train_time:6453ms step_avg:37.74ms
step:172/2330 train_time:6494ms step_avg:37.75ms
step:173/2330 train_time:6528ms step_avg:37.74ms
step:174/2330 train_time:6569ms step_avg:37.75ms
step:175/2330 train_time:6605ms step_avg:37.74ms
step:176/2330 train_time:6646ms step_avg:37.76ms
step:177/2330 train_time:6681ms step_avg:37.74ms
step:178/2330 train_time:6722ms step_avg:37.76ms
step:179/2330 train_time:6756ms step_avg:37.74ms
step:180/2330 train_time:6797ms step_avg:37.76ms
step:181/2330 train_time:6832ms step_avg:37.74ms
step:182/2330 train_time:6872ms step_avg:37.76ms
step:183/2330 train_time:6908ms step_avg:37.75ms
step:184/2330 train_time:6949ms step_avg:37.76ms
step:185/2330 train_time:6985ms step_avg:37.76ms
step:186/2330 train_time:7025ms step_avg:37.77ms
step:187/2330 train_time:7061ms step_avg:37.76ms
step:188/2330 train_time:7102ms step_avg:37.78ms
step:189/2330 train_time:7136ms step_avg:37.76ms
step:190/2330 train_time:7177ms step_avg:37.77ms
step:191/2330 train_time:7212ms step_avg:37.76ms
step:192/2330 train_time:7253ms step_avg:37.77ms
step:193/2330 train_time:7288ms step_avg:37.76ms
step:194/2330 train_time:7329ms step_avg:37.78ms
step:195/2330 train_time:7363ms step_avg:37.76ms
step:196/2330 train_time:7404ms step_avg:37.77ms
step:197/2330 train_time:7438ms step_avg:37.76ms
step:198/2330 train_time:7479ms step_avg:37.77ms
step:199/2330 train_time:7514ms step_avg:37.76ms
step:200/2330 train_time:7555ms step_avg:37.77ms
step:201/2330 train_time:7591ms step_avg:37.77ms
step:202/2330 train_time:7632ms step_avg:37.78ms
step:203/2330 train_time:7667ms step_avg:37.77ms
step:204/2330 train_time:7708ms step_avg:37.78ms
step:205/2330 train_time:7743ms step_avg:37.77ms
step:206/2330 train_time:7783ms step_avg:37.78ms
step:207/2330 train_time:7818ms step_avg:37.77ms
step:208/2330 train_time:7859ms step_avg:37.78ms
step:209/2330 train_time:7895ms step_avg:37.77ms
step:210/2330 train_time:7935ms step_avg:37.79ms
step:211/2330 train_time:7971ms step_avg:37.78ms
step:212/2330 train_time:8012ms step_avg:37.79ms
step:213/2330 train_time:8047ms step_avg:37.78ms
step:214/2330 train_time:8088ms step_avg:37.79ms
step:215/2330 train_time:8122ms step_avg:37.78ms
step:216/2330 train_time:8162ms step_avg:37.79ms
step:217/2330 train_time:8197ms step_avg:37.77ms
step:218/2330 train_time:8238ms step_avg:37.79ms
step:219/2330 train_time:8273ms step_avg:37.78ms
step:220/2330 train_time:8314ms step_avg:37.79ms
step:221/2330 train_time:8350ms step_avg:37.78ms
step:222/2330 train_time:8391ms step_avg:37.80ms
step:223/2330 train_time:8426ms step_avg:37.78ms
step:224/2330 train_time:8466ms step_avg:37.80ms
step:225/2330 train_time:8502ms step_avg:37.79ms
step:226/2330 train_time:8542ms step_avg:37.80ms
step:227/2330 train_time:8577ms step_avg:37.78ms
step:228/2330 train_time:8618ms step_avg:37.80ms
step:229/2330 train_time:8654ms step_avg:37.79ms
step:230/2330 train_time:8694ms step_avg:37.80ms
step:231/2330 train_time:8730ms step_avg:37.79ms
step:232/2330 train_time:8771ms step_avg:37.81ms
step:233/2330 train_time:8806ms step_avg:37.79ms
step:234/2330 train_time:8846ms step_avg:37.80ms
step:235/2330 train_time:8881ms step_avg:37.79ms
step:236/2330 train_time:8922ms step_avg:37.80ms
step:237/2330 train_time:8956ms step_avg:37.79ms
step:238/2330 train_time:8997ms step_avg:37.80ms
step:239/2330 train_time:9032ms step_avg:37.79ms
step:240/2330 train_time:9073ms step_avg:37.80ms
step:241/2330 train_time:9108ms step_avg:37.79ms
step:242/2330 train_time:9149ms step_avg:37.81ms
step:243/2330 train_time:9183ms step_avg:37.79ms
step:244/2330 train_time:9224ms step_avg:37.80ms
step:245/2330 train_time:9258ms step_avg:37.79ms
step:246/2330 train_time:9299ms step_avg:37.80ms
step:247/2330 train_time:9334ms step_avg:37.79ms
step:248/2330 train_time:9375ms step_avg:37.80ms
step:249/2330 train_time:9410ms step_avg:37.79ms
step:250/2330 train_time:9451ms step_avg:37.81ms
step:250/2330 val_loss:5.5657 train_time:9564ms step_avg:38.25ms
step:251/2330 train_time:9574ms step_avg:38.14ms
step:252/2330 train_time:9586ms step_avg:38.04ms
step:253/2330 train_time:9595ms step_avg:37.92ms
step:254/2330 train_time:9607ms step_avg:37.82ms
step:255/2330 train_time:9641ms step_avg:37.81ms
step:256/2330 train_time:9681ms step_avg:37.82ms
step:257/2330 train_time:9716ms step_avg:37.80ms
step:258/2330 train_time:9756ms step_avg:37.81ms
step:259/2330 train_time:9790ms step_avg:37.80ms
step:260/2330 train_time:9831ms step_avg:37.81ms
step:261/2330 train_time:9866ms step_avg:37.80ms
step:262/2330 train_time:9907ms step_avg:37.81ms
step:263/2330 train_time:9949ms step_avg:37.83ms
step:264/2330 train_time:9990ms step_avg:37.84ms
step:265/2330 train_time:10026ms step_avg:37.84ms
step:266/2330 train_time:10067ms step_avg:37.85ms
step:267/2330 train_time:10103ms step_avg:37.84ms
step:268/2330 train_time:10143ms step_avg:37.85ms
step:269/2330 train_time:10180ms step_avg:37.84ms
step:270/2330 train_time:10220ms step_avg:37.85ms
step:271/2330 train_time:10255ms step_avg:37.84ms
step:272/2330 train_time:10295ms step_avg:37.85ms
step:273/2330 train_time:10330ms step_avg:37.84ms
step:274/2330 train_time:10370ms step_avg:37.85ms
step:275/2330 train_time:10405ms step_avg:37.83ms
step:276/2330 train_time:10445ms step_avg:37.84ms
step:277/2330 train_time:10480ms step_avg:37.84ms
step:278/2330 train_time:10521ms step_avg:37.85ms
step:279/2330 train_time:10557ms step_avg:37.84ms
step:280/2330 train_time:10597ms step_avg:37.85ms
step:281/2330 train_time:10633ms step_avg:37.84ms
step:282/2330 train_time:10674ms step_avg:37.85ms
step:283/2330 train_time:10709ms step_avg:37.84ms
step:284/2330 train_time:10750ms step_avg:37.85ms
step:285/2330 train_time:10784ms step_avg:37.84ms
step:286/2330 train_time:10824ms step_avg:37.85ms
step:287/2330 train_time:10860ms step_avg:37.84ms
step:288/2330 train_time:10901ms step_avg:37.85ms
step:289/2330 train_time:10938ms step_avg:37.85ms
step:290/2330 train_time:10978ms step_avg:37.86ms
step:291/2330 train_time:11015ms step_avg:37.85ms
step:292/2330 train_time:11056ms step_avg:37.86ms
step:293/2330 train_time:11090ms step_avg:37.85ms
step:294/2330 train_time:11131ms step_avg:37.86ms
step:295/2330 train_time:11166ms step_avg:37.85ms
step:296/2330 train_time:11207ms step_avg:37.86ms
step:297/2330 train_time:11242ms step_avg:37.85ms
step:298/2330 train_time:11282ms step_avg:37.86ms
step:299/2330 train_time:11318ms step_avg:37.85ms
step:300/2330 train_time:11358ms step_avg:37.86ms
step:301/2330 train_time:11393ms step_avg:37.85ms
step:302/2330 train_time:11434ms step_avg:37.86ms
step:303/2330 train_time:11469ms step_avg:37.85ms
step:304/2330 train_time:11511ms step_avg:37.86ms
step:305/2330 train_time:11545ms step_avg:37.85ms
step:306/2330 train_time:11587ms step_avg:37.86ms
step:307/2330 train_time:11621ms step_avg:37.85ms
step:308/2330 train_time:11662ms step_avg:37.86ms
step:309/2330 train_time:11697ms step_avg:37.85ms
step:310/2330 train_time:11737ms step_avg:37.86ms
step:311/2330 train_time:11772ms step_avg:37.85ms
step:312/2330 train_time:11813ms step_avg:37.86ms
step:313/2330 train_time:11848ms step_avg:37.85ms
step:314/2330 train_time:11889ms step_avg:37.86ms
step:315/2330 train_time:11925ms step_avg:37.86ms
step:316/2330 train_time:11967ms step_avg:37.87ms
step:317/2330 train_time:12002ms step_avg:37.86ms
step:318/2330 train_time:12043ms step_avg:37.87ms
step:319/2330 train_time:12079ms step_avg:37.86ms
step:320/2330 train_time:12119ms step_avg:37.87ms
step:321/2330 train_time:12155ms step_avg:37.87ms
step:322/2330 train_time:12196ms step_avg:37.87ms
step:323/2330 train_time:12230ms step_avg:37.86ms
step:324/2330 train_time:12271ms step_avg:37.87ms
step:325/2330 train_time:12306ms step_avg:37.86ms
step:326/2330 train_time:12346ms step_avg:37.87ms
step:327/2330 train_time:12382ms step_avg:37.87ms
step:328/2330 train_time:12423ms step_avg:37.87ms
step:329/2330 train_time:12458ms step_avg:37.87ms
step:330/2330 train_time:12499ms step_avg:37.88ms
step:331/2330 train_time:12533ms step_avg:37.87ms
step:332/2330 train_time:12574ms step_avg:37.87ms
step:333/2330 train_time:12609ms step_avg:37.87ms
step:334/2330 train_time:12650ms step_avg:37.88ms
step:335/2330 train_time:12685ms step_avg:37.87ms
step:336/2330 train_time:12726ms step_avg:37.87ms
step:337/2330 train_time:12761ms step_avg:37.87ms
step:338/2330 train_time:12802ms step_avg:37.87ms
step:339/2330 train_time:12838ms step_avg:37.87ms
step:340/2330 train_time:12878ms step_avg:37.88ms
step:341/2330 train_time:12915ms step_avg:37.87ms
step:342/2330 train_time:12955ms step_avg:37.88ms
step:343/2330 train_time:12990ms step_avg:37.87ms
step:344/2330 train_time:13031ms step_avg:37.88ms
step:345/2330 train_time:13066ms step_avg:37.87ms
step:346/2330 train_time:13107ms step_avg:37.88ms
step:347/2330 train_time:13142ms step_avg:37.87ms
step:348/2330 train_time:13183ms step_avg:37.88ms
step:349/2330 train_time:13220ms step_avg:37.88ms
step:350/2330 train_time:13260ms step_avg:37.89ms
step:351/2330 train_time:13295ms step_avg:37.88ms
step:352/2330 train_time:13336ms step_avg:37.89ms
step:353/2330 train_time:13372ms step_avg:37.88ms
step:354/2330 train_time:13412ms step_avg:37.89ms
step:355/2330 train_time:13447ms step_avg:37.88ms
step:356/2330 train_time:13488ms step_avg:37.89ms
step:357/2330 train_time:13524ms step_avg:37.88ms
step:358/2330 train_time:13565ms step_avg:37.89ms
step:359/2330 train_time:13600ms step_avg:37.88ms
step:360/2330 train_time:13641ms step_avg:37.89ms
step:361/2330 train_time:13675ms step_avg:37.88ms
step:362/2330 train_time:13716ms step_avg:37.89ms
step:363/2330 train_time:13750ms step_avg:37.88ms
step:364/2330 train_time:13792ms step_avg:37.89ms
step:365/2330 train_time:13827ms step_avg:37.88ms
step:366/2330 train_time:13868ms step_avg:37.89ms
step:367/2330 train_time:13904ms step_avg:37.88ms
step:368/2330 train_time:13945ms step_avg:37.89ms
step:369/2330 train_time:13980ms step_avg:37.89ms
step:370/2330 train_time:14020ms step_avg:37.89ms
step:371/2330 train_time:14055ms step_avg:37.88ms
step:372/2330 train_time:14096ms step_avg:37.89ms
step:373/2330 train_time:14131ms step_avg:37.88ms
step:374/2330 train_time:14172ms step_avg:37.89ms
step:375/2330 train_time:14207ms step_avg:37.88ms
step:376/2330 train_time:14248ms step_avg:37.89ms
step:377/2330 train_time:14284ms step_avg:37.89ms
step:378/2330 train_time:14325ms step_avg:37.90ms
step:379/2330 train_time:14360ms step_avg:37.89ms
step:380/2330 train_time:14400ms step_avg:37.90ms
step:381/2330 train_time:14436ms step_avg:37.89ms
step:382/2330 train_time:14476ms step_avg:37.90ms
step:383/2330 train_time:14511ms step_avg:37.89ms
step:384/2330 train_time:14552ms step_avg:37.90ms
step:385/2330 train_time:14587ms step_avg:37.89ms
step:386/2330 train_time:14628ms step_avg:37.90ms
step:387/2330 train_time:14662ms step_avg:37.89ms
step:388/2330 train_time:14703ms step_avg:37.89ms
step:389/2330 train_time:14738ms step_avg:37.89ms
step:390/2330 train_time:14778ms step_avg:37.89ms
step:391/2330 train_time:14813ms step_avg:37.89ms
step:392/2330 train_time:14854ms step_avg:37.89ms
step:393/2330 train_time:14889ms step_avg:37.89ms
step:394/2330 train_time:14930ms step_avg:37.89ms
step:395/2330 train_time:14966ms step_avg:37.89ms
step:396/2330 train_time:15007ms step_avg:37.90ms
step:397/2330 train_time:15042ms step_avg:37.89ms
step:398/2330 train_time:15084ms step_avg:37.90ms
step:399/2330 train_time:15119ms step_avg:37.89ms
step:400/2330 train_time:15160ms step_avg:37.90ms
step:401/2330 train_time:15196ms step_avg:37.90ms
step:402/2330 train_time:15236ms step_avg:37.90ms
step:403/2330 train_time:15272ms step_avg:37.90ms
step:404/2330 train_time:15313ms step_avg:37.90ms
step:405/2330 train_time:15348ms step_avg:37.90ms
step:406/2330 train_time:15388ms step_avg:37.90ms
step:407/2330 train_time:15424ms step_avg:37.90ms
step:408/2330 train_time:15465ms step_avg:37.90ms
step:409/2330 train_time:15501ms step_avg:37.90ms
step:410/2330 train_time:15542ms step_avg:37.91ms
step:411/2330 train_time:15578ms step_avg:37.90ms
step:412/2330 train_time:15618ms step_avg:37.91ms
step:413/2330 train_time:15653ms step_avg:37.90ms
step:414/2330 train_time:15694ms step_avg:37.91ms
step:415/2330 train_time:15728ms step_avg:37.90ms
step:416/2330 train_time:15769ms step_avg:37.91ms
step:417/2330 train_time:15805ms step_avg:37.90ms
step:418/2330 train_time:15846ms step_avg:37.91ms
step:419/2330 train_time:15881ms step_avg:37.90ms
step:420/2330 train_time:15922ms step_avg:37.91ms
step:421/2330 train_time:15958ms step_avg:37.91ms
step:422/2330 train_time:15999ms step_avg:37.91ms
step:423/2330 train_time:16035ms step_avg:37.91ms
step:424/2330 train_time:16075ms step_avg:37.91ms
step:425/2330 train_time:16110ms step_avg:37.91ms
step:426/2330 train_time:16151ms step_avg:37.91ms
step:427/2330 train_time:16188ms step_avg:37.91ms
step:428/2330 train_time:16228ms step_avg:37.92ms
step:429/2330 train_time:16263ms step_avg:37.91ms
step:430/2330 train_time:16304ms step_avg:37.92ms
step:431/2330 train_time:16339ms step_avg:37.91ms
step:432/2330 train_time:16379ms step_avg:37.91ms
step:433/2330 train_time:16415ms step_avg:37.91ms
step:434/2330 train_time:16455ms step_avg:37.92ms
step:435/2330 train_time:16492ms step_avg:37.91ms
step:436/2330 train_time:16533ms step_avg:37.92ms
step:437/2330 train_time:16569ms step_avg:37.91ms
step:438/2330 train_time:16609ms step_avg:37.92ms
step:439/2330 train_time:16645ms step_avg:37.92ms
step:440/2330 train_time:16686ms step_avg:37.92ms
step:441/2330 train_time:16721ms step_avg:37.92ms
step:442/2330 train_time:16762ms step_avg:37.92ms
step:443/2330 train_time:16797ms step_avg:37.92ms
step:444/2330 train_time:16838ms step_avg:37.92ms
step:445/2330 train_time:16873ms step_avg:37.92ms
step:446/2330 train_time:16914ms step_avg:37.92ms
step:447/2330 train_time:16949ms step_avg:37.92ms
step:448/2330 train_time:16990ms step_avg:37.93ms
step:449/2330 train_time:17026ms step_avg:37.92ms
step:450/2330 train_time:17066ms step_avg:37.92ms
step:451/2330 train_time:17102ms step_avg:37.92ms
step:452/2330 train_time:17143ms step_avg:37.93ms
step:453/2330 train_time:17178ms step_avg:37.92ms
step:454/2330 train_time:17219ms step_avg:37.93ms
step:455/2330 train_time:17254ms step_avg:37.92ms
step:456/2330 train_time:17295ms step_avg:37.93ms
step:457/2330 train_time:17330ms step_avg:37.92ms
step:458/2330 train_time:17370ms step_avg:37.93ms
step:459/2330 train_time:17406ms step_avg:37.92ms
step:460/2330 train_time:17447ms step_avg:37.93ms
step:461/2330 train_time:17482ms step_avg:37.92ms
step:462/2330 train_time:17524ms step_avg:37.93ms
step:463/2330 train_time:17558ms step_avg:37.92ms
step:464/2330 train_time:17599ms step_avg:37.93ms
step:465/2330 train_time:17634ms step_avg:37.92ms
step:466/2330 train_time:17674ms step_avg:37.93ms
step:467/2330 train_time:17710ms step_avg:37.92ms
step:468/2330 train_time:17750ms step_avg:37.93ms
step:469/2330 train_time:17787ms step_avg:37.92ms
step:470/2330 train_time:17827ms step_avg:37.93ms
step:471/2330 train_time:17863ms step_avg:37.93ms
step:472/2330 train_time:17904ms step_avg:37.93ms
step:473/2330 train_time:17940ms step_avg:37.93ms
step:474/2330 train_time:17980ms step_avg:37.93ms
step:475/2330 train_time:18016ms step_avg:37.93ms
step:476/2330 train_time:18056ms step_avg:37.93ms
step:477/2330 train_time:18091ms step_avg:37.93ms
step:478/2330 train_time:18132ms step_avg:37.93ms
step:479/2330 train_time:18167ms step_avg:37.93ms
step:480/2330 train_time:18207ms step_avg:37.93ms
step:481/2330 train_time:18243ms step_avg:37.93ms
step:482/2330 train_time:18284ms step_avg:37.93ms
step:483/2330 train_time:18320ms step_avg:37.93ms
step:484/2330 train_time:18361ms step_avg:37.94ms
step:485/2330 train_time:18397ms step_avg:37.93ms
step:486/2330 train_time:18437ms step_avg:37.94ms
step:487/2330 train_time:18473ms step_avg:37.93ms
step:488/2330 train_time:18514ms step_avg:37.94ms
step:489/2330 train_time:18548ms step_avg:37.93ms
step:490/2330 train_time:18589ms step_avg:37.94ms
step:491/2330 train_time:18625ms step_avg:37.93ms
step:492/2330 train_time:18666ms step_avg:37.94ms
step:493/2330 train_time:18701ms step_avg:37.93ms
step:494/2330 train_time:18742ms step_avg:37.94ms
step:495/2330 train_time:18777ms step_avg:37.93ms
step:496/2330 train_time:18818ms step_avg:37.94ms
step:497/2330 train_time:18853ms step_avg:37.93ms
step:498/2330 train_time:18895ms step_avg:37.94ms
step:499/2330 train_time:18929ms step_avg:37.93ms
step:500/2330 train_time:18971ms step_avg:37.94ms
step:500/2330 val_loss:5.4296 train_time:19083ms step_avg:38.17ms
step:501/2330 train_time:19094ms step_avg:38.11ms
step:502/2330 train_time:19106ms step_avg:38.06ms
step:503/2330 train_time:19116ms step_avg:38.00ms
step:504/2330 train_time:19126ms step_avg:37.95ms
step:505/2330 train_time:19160ms step_avg:37.94ms
step:506/2330 train_time:19201ms step_avg:37.95ms
step:507/2330 train_time:19235ms step_avg:37.94ms
step:508/2330 train_time:19275ms step_avg:37.94ms
step:509/2330 train_time:19310ms step_avg:37.94ms
step:510/2330 train_time:19350ms step_avg:37.94ms
step:511/2330 train_time:19386ms step_avg:37.94ms
step:512/2330 train_time:19431ms step_avg:37.95ms
step:513/2330 train_time:19468ms step_avg:37.95ms
step:514/2330 train_time:19510ms step_avg:37.96ms
step:515/2330 train_time:19545ms step_avg:37.95ms
step:516/2330 train_time:19586ms step_avg:37.96ms
step:517/2330 train_time:19620ms step_avg:37.95ms
step:518/2330 train_time:19661ms step_avg:37.96ms
step:519/2330 train_time:19696ms step_avg:37.95ms
step:520/2330 train_time:19737ms step_avg:37.96ms
step:521/2330 train_time:19772ms step_avg:37.95ms
step:522/2330 train_time:19812ms step_avg:37.95ms
step:523/2330 train_time:19847ms step_avg:37.95ms
step:524/2330 train_time:19888ms step_avg:37.95ms
step:525/2330 train_time:19924ms step_avg:37.95ms
step:526/2330 train_time:19964ms step_avg:37.96ms
step:527/2330 train_time:19999ms step_avg:37.95ms
step:528/2330 train_time:20040ms step_avg:37.96ms
step:529/2330 train_time:20075ms step_avg:37.95ms
step:530/2330 train_time:20116ms step_avg:37.95ms
step:531/2330 train_time:20150ms step_avg:37.95ms
step:532/2330 train_time:20191ms step_avg:37.95ms
step:533/2330 train_time:20226ms step_avg:37.95ms
step:534/2330 train_time:20267ms step_avg:37.95ms
step:535/2330 train_time:20302ms step_avg:37.95ms
step:536/2330 train_time:20343ms step_avg:37.95ms
step:537/2330 train_time:20379ms step_avg:37.95ms
step:538/2330 train_time:20420ms step_avg:37.96ms
step:539/2330 train_time:20456ms step_avg:37.95ms
step:540/2330 train_time:20498ms step_avg:37.96ms
step:541/2330 train_time:20534ms step_avg:37.95ms
step:542/2330 train_time:20574ms step_avg:37.96ms
step:543/2330 train_time:20610ms step_avg:37.96ms
step:544/2330 train_time:20651ms step_avg:37.96ms
step:545/2330 train_time:20685ms step_avg:37.95ms
step:546/2330 train_time:20726ms step_avg:37.96ms
step:547/2330 train_time:20761ms step_avg:37.95ms
step:548/2330 train_time:20802ms step_avg:37.96ms
step:549/2330 train_time:20836ms step_avg:37.95ms
step:550/2330 train_time:20877ms step_avg:37.96ms
step:551/2330 train_time:20912ms step_avg:37.95ms
step:552/2330 train_time:20953ms step_avg:37.96ms
step:553/2330 train_time:20988ms step_avg:37.95ms
step:554/2330 train_time:21029ms step_avg:37.96ms
step:555/2330 train_time:21064ms step_avg:37.95ms
step:556/2330 train_time:21105ms step_avg:37.96ms
step:557/2330 train_time:21140ms step_avg:37.95ms
step:558/2330 train_time:21180ms step_avg:37.96ms
step:559/2330 train_time:21215ms step_avg:37.95ms
step:560/2330 train_time:21256ms step_avg:37.96ms
step:561/2330 train_time:21291ms step_avg:37.95ms
step:562/2330 train_time:21332ms step_avg:37.96ms
step:563/2330 train_time:21367ms step_avg:37.95ms
step:564/2330 train_time:21409ms step_avg:37.96ms
step:565/2330 train_time:21444ms step_avg:37.95ms
step:566/2330 train_time:21486ms step_avg:37.96ms
step:567/2330 train_time:21522ms step_avg:37.96ms
step:568/2330 train_time:21563ms step_avg:37.96ms
step:569/2330 train_time:21599ms step_avg:37.96ms
step:570/2330 train_time:21640ms step_avg:37.96ms
step:571/2330 train_time:21675ms step_avg:37.96ms
step:572/2330 train_time:21715ms step_avg:37.96ms
step:573/2330 train_time:21751ms step_avg:37.96ms
step:574/2330 train_time:21792ms step_avg:37.96ms
step:575/2330 train_time:21827ms step_avg:37.96ms
step:576/2330 train_time:21869ms step_avg:37.97ms
step:577/2330 train_time:21904ms step_avg:37.96ms
step:578/2330 train_time:21945ms step_avg:37.97ms
step:579/2330 train_time:21980ms step_avg:37.96ms
step:580/2330 train_time:22021ms step_avg:37.97ms
step:581/2330 train_time:22056ms step_avg:37.96ms
step:582/2330 train_time:22097ms step_avg:37.97ms
step:583/2330 train_time:22132ms step_avg:37.96ms
step:584/2330 train_time:22172ms step_avg:37.97ms
step:585/2330 train_time:22207ms step_avg:37.96ms
step:586/2330 train_time:22248ms step_avg:37.97ms
step:587/2330 train_time:22283ms step_avg:37.96ms
step:588/2330 train_time:22324ms step_avg:37.97ms
step:589/2330 train_time:22359ms step_avg:37.96ms
step:590/2330 train_time:22400ms step_avg:37.97ms
step:591/2330 train_time:22436ms step_avg:37.96ms
step:592/2330 train_time:22477ms step_avg:37.97ms
step:593/2330 train_time:22512ms step_avg:37.96ms
step:594/2330 train_time:22553ms step_avg:37.97ms
step:595/2330 train_time:22589ms step_avg:37.96ms
step:596/2330 train_time:22630ms step_avg:37.97ms
step:597/2330 train_time:22665ms step_avg:37.96ms
step:598/2330 train_time:22706ms step_avg:37.97ms
step:599/2330 train_time:22741ms step_avg:37.97ms
step:600/2330 train_time:22783ms step_avg:37.97ms
step:601/2330 train_time:22818ms step_avg:37.97ms
step:602/2330 train_time:22860ms step_avg:37.97ms
step:603/2330 train_time:22894ms step_avg:37.97ms
step:604/2330 train_time:22935ms step_avg:37.97ms
step:605/2330 train_time:22969ms step_avg:37.97ms
step:606/2330 train_time:23010ms step_avg:37.97ms
step:607/2330 train_time:23045ms step_avg:37.96ms
step:608/2330 train_time:23086ms step_avg:37.97ms
step:609/2330 train_time:23121ms step_avg:37.97ms
step:610/2330 train_time:23162ms step_avg:37.97ms
step:611/2330 train_time:23197ms step_avg:37.97ms
step:612/2330 train_time:23238ms step_avg:37.97ms
step:613/2330 train_time:23272ms step_avg:37.96ms
step:614/2330 train_time:23313ms step_avg:37.97ms
step:615/2330 train_time:23348ms step_avg:37.96ms
step:616/2330 train_time:23390ms step_avg:37.97ms
step:617/2330 train_time:23425ms step_avg:37.97ms
step:618/2330 train_time:23466ms step_avg:37.97ms
step:619/2330 train_time:23501ms step_avg:37.97ms
step:620/2330 train_time:23543ms step_avg:37.97ms
step:621/2330 train_time:23579ms step_avg:37.97ms
step:622/2330 train_time:23620ms step_avg:37.97ms
step:623/2330 train_time:23657ms step_avg:37.97ms
step:624/2330 train_time:23698ms step_avg:37.98ms
step:625/2330 train_time:23734ms step_avg:37.98ms
step:626/2330 train_time:23775ms step_avg:37.98ms
step:627/2330 train_time:23810ms step_avg:37.97ms
step:628/2330 train_time:23851ms step_avg:37.98ms
step:629/2330 train_time:23886ms step_avg:37.98ms
step:630/2330 train_time:23927ms step_avg:37.98ms
step:631/2330 train_time:23963ms step_avg:37.98ms
step:632/2330 train_time:24004ms step_avg:37.98ms
step:633/2330 train_time:24040ms step_avg:37.98ms
step:634/2330 train_time:24081ms step_avg:37.98ms
step:635/2330 train_time:24115ms step_avg:37.98ms
step:636/2330 train_time:24157ms step_avg:37.98ms
step:637/2330 train_time:24191ms step_avg:37.98ms
step:638/2330 train_time:24232ms step_avg:37.98ms
step:639/2330 train_time:24267ms step_avg:37.98ms
step:640/2330 train_time:24308ms step_avg:37.98ms
step:641/2330 train_time:24342ms step_avg:37.98ms
step:642/2330 train_time:24383ms step_avg:37.98ms
step:643/2330 train_time:24420ms step_avg:37.98ms
step:644/2330 train_time:24460ms step_avg:37.98ms
step:645/2330 train_time:24496ms step_avg:37.98ms
step:646/2330 train_time:24538ms step_avg:37.98ms
step:647/2330 train_time:24573ms step_avg:37.98ms
step:648/2330 train_time:24614ms step_avg:37.98ms
step:649/2330 train_time:24649ms step_avg:37.98ms
step:650/2330 train_time:24690ms step_avg:37.99ms
step:651/2330 train_time:24726ms step_avg:37.98ms
step:652/2330 train_time:24767ms step_avg:37.99ms
step:653/2330 train_time:24802ms step_avg:37.98ms
step:654/2330 train_time:24844ms step_avg:37.99ms
step:655/2330 train_time:24880ms step_avg:37.98ms
step:656/2330 train_time:24921ms step_avg:37.99ms
step:657/2330 train_time:24955ms step_avg:37.98ms
step:658/2330 train_time:24996ms step_avg:37.99ms
step:659/2330 train_time:25031ms step_avg:37.98ms
step:660/2330 train_time:25072ms step_avg:37.99ms
step:661/2330 train_time:25107ms step_avg:37.98ms
step:662/2330 train_time:25148ms step_avg:37.99ms
step:663/2330 train_time:25183ms step_avg:37.98ms
step:664/2330 train_time:25223ms step_avg:37.99ms
step:665/2330 train_time:25259ms step_avg:37.98ms
step:666/2330 train_time:25300ms step_avg:37.99ms
step:667/2330 train_time:25336ms step_avg:37.98ms
step:668/2330 train_time:25376ms step_avg:37.99ms
step:669/2330 train_time:25413ms step_avg:37.99ms
step:670/2330 train_time:25453ms step_avg:37.99ms
step:671/2330 train_time:25489ms step_avg:37.99ms
step:672/2330 train_time:25530ms step_avg:37.99ms
step:673/2330 train_time:25565ms step_avg:37.99ms
step:674/2330 train_time:25606ms step_avg:37.99ms
step:675/2330 train_time:25641ms step_avg:37.99ms
step:676/2330 train_time:25682ms step_avg:37.99ms
step:677/2330 train_time:25718ms step_avg:37.99ms
step:678/2330 train_time:25759ms step_avg:37.99ms
step:679/2330 train_time:25796ms step_avg:37.99ms
step:680/2330 train_time:25836ms step_avg:37.99ms
step:681/2330 train_time:25872ms step_avg:37.99ms
step:682/2330 train_time:25913ms step_avg:38.00ms
step:683/2330 train_time:25949ms step_avg:37.99ms
step:684/2330 train_time:25990ms step_avg:38.00ms
step:685/2330 train_time:26025ms step_avg:37.99ms
step:686/2330 train_time:26066ms step_avg:38.00ms
step:687/2330 train_time:26101ms step_avg:37.99ms
step:688/2330 train_time:26142ms step_avg:38.00ms
step:689/2330 train_time:26178ms step_avg:37.99ms
step:690/2330 train_time:26219ms step_avg:38.00ms
step:691/2330 train_time:26255ms step_avg:38.00ms
step:692/2330 train_time:26295ms step_avg:38.00ms
step:693/2330 train_time:26331ms step_avg:38.00ms
step:694/2330 train_time:26372ms step_avg:38.00ms
step:695/2330 train_time:26407ms step_avg:38.00ms
step:696/2330 train_time:26447ms step_avg:38.00ms
step:697/2330 train_time:26484ms step_avg:38.00ms
step:698/2330 train_time:26525ms step_avg:38.00ms
step:699/2330 train_time:26561ms step_avg:38.00ms
step:700/2330 train_time:26601ms step_avg:38.00ms
step:701/2330 train_time:26636ms step_avg:38.00ms
step:702/2330 train_time:26677ms step_avg:38.00ms
step:703/2330 train_time:26713ms step_avg:38.00ms
step:704/2330 train_time:26753ms step_avg:38.00ms
step:705/2330 train_time:26789ms step_avg:38.00ms
step:706/2330 train_time:26831ms step_avg:38.00ms
step:707/2330 train_time:26867ms step_avg:38.00ms
step:708/2330 train_time:26908ms step_avg:38.01ms
step:709/2330 train_time:26943ms step_avg:38.00ms
step:710/2330 train_time:26984ms step_avg:38.01ms
step:711/2330 train_time:27019ms step_avg:38.00ms
step:712/2330 train_time:27060ms step_avg:38.01ms
step:713/2330 train_time:27095ms step_avg:38.00ms
step:714/2330 train_time:27136ms step_avg:38.01ms
step:715/2330 train_time:27171ms step_avg:38.00ms
step:716/2330 train_time:27212ms step_avg:38.01ms
step:717/2330 train_time:27247ms step_avg:38.00ms
step:718/2330 train_time:27289ms step_avg:38.01ms
step:719/2330 train_time:27323ms step_avg:38.00ms
step:720/2330 train_time:27364ms step_avg:38.01ms
step:721/2330 train_time:27400ms step_avg:38.00ms
step:722/2330 train_time:27441ms step_avg:38.01ms
step:723/2330 train_time:27477ms step_avg:38.00ms
step:724/2330 train_time:27517ms step_avg:38.01ms
step:725/2330 train_time:27554ms step_avg:38.01ms
step:726/2330 train_time:27595ms step_avg:38.01ms
step:727/2330 train_time:27630ms step_avg:38.00ms
step:728/2330 train_time:27671ms step_avg:38.01ms
step:729/2330 train_time:27706ms step_avg:38.00ms
step:730/2330 train_time:27746ms step_avg:38.01ms
step:731/2330 train_time:27782ms step_avg:38.01ms
step:732/2330 train_time:27824ms step_avg:38.01ms
step:733/2330 train_time:27860ms step_avg:38.01ms
step:734/2330 train_time:27901ms step_avg:38.01ms
step:735/2330 train_time:27936ms step_avg:38.01ms
step:736/2330 train_time:27977ms step_avg:38.01ms
step:737/2330 train_time:28012ms step_avg:38.01ms
step:738/2330 train_time:28053ms step_avg:38.01ms
step:739/2330 train_time:28088ms step_avg:38.01ms
step:740/2330 train_time:28129ms step_avg:38.01ms
step:741/2330 train_time:28165ms step_avg:38.01ms
step:742/2330 train_time:28205ms step_avg:38.01ms
step:743/2330 train_time:28241ms step_avg:38.01ms
step:744/2330 train_time:28282ms step_avg:38.01ms
step:745/2330 train_time:28317ms step_avg:38.01ms
step:746/2330 train_time:28358ms step_avg:38.01ms
step:747/2330 train_time:28393ms step_avg:38.01ms
step:748/2330 train_time:28434ms step_avg:38.01ms
step:749/2330 train_time:28469ms step_avg:38.01ms
step:750/2330 train_time:28510ms step_avg:38.01ms
step:750/2330 val_loss:5.3634 train_time:28623ms step_avg:38.16ms
step:751/2330 train_time:28634ms step_avg:38.13ms
step:752/2330 train_time:28646ms step_avg:38.09ms
step:753/2330 train_time:28655ms step_avg:38.05ms
step:754/2330 train_time:28665ms step_avg:38.02ms
step:755/2330 train_time:28701ms step_avg:38.01ms
step:756/2330 train_time:28742ms step_avg:38.02ms
step:757/2330 train_time:28776ms step_avg:38.01ms
step:758/2330 train_time:28816ms step_avg:38.02ms
step:759/2330 train_time:28851ms step_avg:38.01ms
step:760/2330 train_time:28891ms step_avg:38.01ms
step:761/2330 train_time:28927ms step_avg:38.01ms
step:762/2330 train_time:28968ms step_avg:38.02ms
step:763/2330 train_time:29012ms step_avg:38.02ms
step:764/2330 train_time:29053ms step_avg:38.03ms
step:765/2330 train_time:29088ms step_avg:38.02ms
step:766/2330 train_time:29129ms step_avg:38.03ms
step:767/2330 train_time:29163ms step_avg:38.02ms
step:768/2330 train_time:29204ms step_avg:38.03ms
step:769/2330 train_time:29239ms step_avg:38.02ms
step:770/2330 train_time:29280ms step_avg:38.03ms
step:771/2330 train_time:29315ms step_avg:38.02ms
step:772/2330 train_time:29356ms step_avg:38.03ms
step:773/2330 train_time:29390ms step_avg:38.02ms
step:774/2330 train_time:29431ms step_avg:38.02ms
step:775/2330 train_time:29466ms step_avg:38.02ms
step:776/2330 train_time:29507ms step_avg:38.02ms
step:777/2330 train_time:29542ms step_avg:38.02ms
step:778/2330 train_time:29583ms step_avg:38.02ms
step:779/2330 train_time:29618ms step_avg:38.02ms
step:780/2330 train_time:29659ms step_avg:38.02ms
step:781/2330 train_time:29694ms step_avg:38.02ms
step:782/2330 train_time:29735ms step_avg:38.02ms
step:783/2330 train_time:29769ms step_avg:38.02ms
step:784/2330 train_time:29810ms step_avg:38.02ms
step:785/2330 train_time:29845ms step_avg:38.02ms
step:786/2330 train_time:29886ms step_avg:38.02ms
step:787/2330 train_time:29922ms step_avg:38.02ms
step:788/2330 train_time:29963ms step_avg:38.02ms
step:789/2330 train_time:30000ms step_avg:38.02ms
step:790/2330 train_time:30042ms step_avg:38.03ms
step:791/2330 train_time:30077ms step_avg:38.02ms
step:792/2330 train_time:30118ms step_avg:38.03ms
step:793/2330 train_time:30153ms step_avg:38.02ms
step:794/2330 train_time:30194ms step_avg:38.03ms
step:795/2330 train_time:30230ms step_avg:38.03ms
step:796/2330 train_time:30271ms step_avg:38.03ms
step:797/2330 train_time:30306ms step_avg:38.02ms
step:798/2330 train_time:30347ms step_avg:38.03ms
step:799/2330 train_time:30381ms step_avg:38.02ms
step:800/2330 train_time:30422ms step_avg:38.03ms
step:801/2330 train_time:30458ms step_avg:38.03ms
step:802/2330 train_time:30499ms step_avg:38.03ms
step:803/2330 train_time:30535ms step_avg:38.03ms
step:804/2330 train_time:30576ms step_avg:38.03ms
step:805/2330 train_time:30611ms step_avg:38.03ms
step:806/2330 train_time:30651ms step_avg:38.03ms
step:807/2330 train_time:30686ms step_avg:38.03ms
step:808/2330 train_time:30727ms step_avg:38.03ms
step:809/2330 train_time:30762ms step_avg:38.02ms
step:810/2330 train_time:30802ms step_avg:38.03ms
step:811/2330 train_time:30838ms step_avg:38.02ms
step:812/2330 train_time:30879ms step_avg:38.03ms
step:813/2330 train_time:30914ms step_avg:38.02ms
step:814/2330 train_time:30956ms step_avg:38.03ms
step:815/2330 train_time:30991ms step_avg:38.03ms
step:816/2330 train_time:31032ms step_avg:38.03ms
step:817/2330 train_time:31067ms step_avg:38.03ms
step:818/2330 train_time:31108ms step_avg:38.03ms
step:819/2330 train_time:31143ms step_avg:38.03ms
step:820/2330 train_time:31184ms step_avg:38.03ms
step:821/2330 train_time:31220ms step_avg:38.03ms
step:822/2330 train_time:31261ms step_avg:38.03ms
step:823/2330 train_time:31297ms step_avg:38.03ms
step:824/2330 train_time:31338ms step_avg:38.03ms
step:825/2330 train_time:31373ms step_avg:38.03ms
step:826/2330 train_time:31413ms step_avg:38.03ms
step:827/2330 train_time:31448ms step_avg:38.03ms
step:828/2330 train_time:31489ms step_avg:38.03ms
step:829/2330 train_time:31524ms step_avg:38.03ms
step:830/2330 train_time:31565ms step_avg:38.03ms
step:831/2330 train_time:31600ms step_avg:38.03ms
step:832/2330 train_time:31640ms step_avg:38.03ms
step:833/2330 train_time:31677ms step_avg:38.03ms
step:834/2330 train_time:31717ms step_avg:38.03ms
step:835/2330 train_time:31752ms step_avg:38.03ms
step:836/2330 train_time:31793ms step_avg:38.03ms
step:837/2330 train_time:31829ms step_avg:38.03ms
step:838/2330 train_time:31869ms step_avg:38.03ms
step:839/2330 train_time:31904ms step_avg:38.03ms
step:840/2330 train_time:31945ms step_avg:38.03ms
step:841/2330 train_time:31981ms step_avg:38.03ms
step:842/2330 train_time:32023ms step_avg:38.03ms
step:843/2330 train_time:32058ms step_avg:38.03ms
step:844/2330 train_time:32099ms step_avg:38.03ms
step:845/2330 train_time:32134ms step_avg:38.03ms
step:846/2330 train_time:32176ms step_avg:38.03ms
step:847/2330 train_time:32211ms step_avg:38.03ms
step:848/2330 train_time:32251ms step_avg:38.03ms
step:849/2330 train_time:32286ms step_avg:38.03ms
step:850/2330 train_time:32327ms step_avg:38.03ms
step:851/2330 train_time:32362ms step_avg:38.03ms
step:852/2330 train_time:32403ms step_avg:38.03ms
step:853/2330 train_time:32438ms step_avg:38.03ms
step:854/2330 train_time:32479ms step_avg:38.03ms
step:855/2330 train_time:32514ms step_avg:38.03ms
step:856/2330 train_time:32555ms step_avg:38.03ms
step:857/2330 train_time:32589ms step_avg:38.03ms
step:858/2330 train_time:32630ms step_avg:38.03ms
step:859/2330 train_time:32665ms step_avg:38.03ms
step:860/2330 train_time:32707ms step_avg:38.03ms
step:861/2330 train_time:32741ms step_avg:38.03ms
step:862/2330 train_time:32783ms step_avg:38.03ms
step:863/2330 train_time:32817ms step_avg:38.03ms
step:864/2330 train_time:32859ms step_avg:38.03ms
step:865/2330 train_time:32893ms step_avg:38.03ms
step:866/2330 train_time:32935ms step_avg:38.03ms
step:867/2330 train_time:32970ms step_avg:38.03ms
step:868/2330 train_time:33011ms step_avg:38.03ms
step:869/2330 train_time:33047ms step_avg:38.03ms
step:870/2330 train_time:33088ms step_avg:38.03ms
step:871/2330 train_time:33123ms step_avg:38.03ms
step:872/2330 train_time:33164ms step_avg:38.03ms
step:873/2330 train_time:33199ms step_avg:38.03ms
step:874/2330 train_time:33241ms step_avg:38.03ms
step:875/2330 train_time:33276ms step_avg:38.03ms
step:876/2330 train_time:33317ms step_avg:38.03ms
step:877/2330 train_time:33352ms step_avg:38.03ms
step:878/2330 train_time:33393ms step_avg:38.03ms
step:879/2330 train_time:33427ms step_avg:38.03ms
step:880/2330 train_time:33469ms step_avg:38.03ms
step:881/2330 train_time:33503ms step_avg:38.03ms
step:882/2330 train_time:33545ms step_avg:38.03ms
step:883/2330 train_time:33580ms step_avg:38.03ms
step:884/2330 train_time:33621ms step_avg:38.03ms
step:885/2330 train_time:33656ms step_avg:38.03ms
step:886/2330 train_time:33697ms step_avg:38.03ms
step:887/2330 train_time:33731ms step_avg:38.03ms
step:888/2330 train_time:33772ms step_avg:38.03ms
step:889/2330 train_time:33806ms step_avg:38.03ms
step:890/2330 train_time:33848ms step_avg:38.03ms
step:891/2330 train_time:33883ms step_avg:38.03ms
step:892/2330 train_time:33924ms step_avg:38.03ms
step:893/2330 train_time:33959ms step_avg:38.03ms
step:894/2330 train_time:34001ms step_avg:38.03ms
step:895/2330 train_time:34036ms step_avg:38.03ms
step:896/2330 train_time:34078ms step_avg:38.03ms
step:897/2330 train_time:34113ms step_avg:38.03ms
step:898/2330 train_time:34154ms step_avg:38.03ms
step:899/2330 train_time:34190ms step_avg:38.03ms
step:900/2330 train_time:34230ms step_avg:38.03ms
step:901/2330 train_time:34265ms step_avg:38.03ms
step:902/2330 train_time:34306ms step_avg:38.03ms
step:903/2330 train_time:34341ms step_avg:38.03ms
step:904/2330 train_time:34382ms step_avg:38.03ms
step:905/2330 train_time:34417ms step_avg:38.03ms
step:906/2330 train_time:34458ms step_avg:38.03ms
step:907/2330 train_time:34493ms step_avg:38.03ms
step:908/2330 train_time:34534ms step_avg:38.03ms
step:909/2330 train_time:34568ms step_avg:38.03ms
step:910/2330 train_time:34609ms step_avg:38.03ms
step:911/2330 train_time:34644ms step_avg:38.03ms
step:912/2330 train_time:34685ms step_avg:38.03ms
step:913/2330 train_time:34719ms step_avg:38.03ms
step:914/2330 train_time:34761ms step_avg:38.03ms
step:915/2330 train_time:34796ms step_avg:38.03ms
step:916/2330 train_time:34838ms step_avg:38.03ms
step:917/2330 train_time:34872ms step_avg:38.03ms
step:918/2330 train_time:34914ms step_avg:38.03ms
step:919/2330 train_time:34948ms step_avg:38.03ms
step:920/2330 train_time:34989ms step_avg:38.03ms
step:921/2330 train_time:35024ms step_avg:38.03ms
step:922/2330 train_time:35065ms step_avg:38.03ms
step:923/2330 train_time:35101ms step_avg:38.03ms
step:924/2330 train_time:35142ms step_avg:38.03ms
step:925/2330 train_time:35177ms step_avg:38.03ms
step:926/2330 train_time:35218ms step_avg:38.03ms
step:927/2330 train_time:35254ms step_avg:38.03ms
step:928/2330 train_time:35295ms step_avg:38.03ms
step:929/2330 train_time:35330ms step_avg:38.03ms
step:930/2330 train_time:35371ms step_avg:38.03ms
step:931/2330 train_time:35406ms step_avg:38.03ms
step:932/2330 train_time:35448ms step_avg:38.03ms
step:933/2330 train_time:35482ms step_avg:38.03ms
step:934/2330 train_time:35523ms step_avg:38.03ms
step:935/2330 train_time:35558ms step_avg:38.03ms
step:936/2330 train_time:35600ms step_avg:38.03ms
step:937/2330 train_time:35635ms step_avg:38.03ms
step:938/2330 train_time:35676ms step_avg:38.03ms
step:939/2330 train_time:35711ms step_avg:38.03ms
step:940/2330 train_time:35752ms step_avg:38.03ms
step:941/2330 train_time:35787ms step_avg:38.03ms
step:942/2330 train_time:35828ms step_avg:38.03ms
step:943/2330 train_time:35862ms step_avg:38.03ms
step:944/2330 train_time:35904ms step_avg:38.03ms
step:945/2330 train_time:35938ms step_avg:38.03ms
step:946/2330 train_time:35980ms step_avg:38.03ms
step:947/2330 train_time:36014ms step_avg:38.03ms
step:948/2330 train_time:36056ms step_avg:38.03ms
step:949/2330 train_time:36091ms step_avg:38.03ms
step:950/2330 train_time:36132ms step_avg:38.03ms
step:951/2330 train_time:36167ms step_avg:38.03ms
step:952/2330 train_time:36209ms step_avg:38.03ms
step:953/2330 train_time:36244ms step_avg:38.03ms
step:954/2330 train_time:36285ms step_avg:38.03ms
step:955/2330 train_time:36320ms step_avg:38.03ms
step:956/2330 train_time:36361ms step_avg:38.03ms
step:957/2330 train_time:36398ms step_avg:38.03ms
step:958/2330 train_time:36439ms step_avg:38.04ms
step:959/2330 train_time:36473ms step_avg:38.03ms
step:960/2330 train_time:36514ms step_avg:38.04ms
step:961/2330 train_time:36549ms step_avg:38.03ms
step:962/2330 train_time:36590ms step_avg:38.03ms
step:963/2330 train_time:36625ms step_avg:38.03ms
step:964/2330 train_time:36666ms step_avg:38.03ms
step:965/2330 train_time:36701ms step_avg:38.03ms
step:966/2330 train_time:36742ms step_avg:38.04ms
step:967/2330 train_time:36777ms step_avg:38.03ms
step:968/2330 train_time:36819ms step_avg:38.04ms
step:969/2330 train_time:36853ms step_avg:38.03ms
step:970/2330 train_time:36895ms step_avg:38.04ms
step:971/2330 train_time:36929ms step_avg:38.03ms
step:972/2330 train_time:36970ms step_avg:38.03ms
step:973/2330 train_time:37005ms step_avg:38.03ms
step:974/2330 train_time:37047ms step_avg:38.04ms
step:975/2330 train_time:37082ms step_avg:38.03ms
step:976/2330 train_time:37123ms step_avg:38.04ms
step:977/2330 train_time:37159ms step_avg:38.03ms
step:978/2330 train_time:37200ms step_avg:38.04ms
step:979/2330 train_time:37235ms step_avg:38.03ms
step:980/2330 train_time:37276ms step_avg:38.04ms
step:981/2330 train_time:37312ms step_avg:38.04ms
step:982/2330 train_time:37353ms step_avg:38.04ms
step:983/2330 train_time:37389ms step_avg:38.04ms
step:984/2330 train_time:37429ms step_avg:38.04ms
step:985/2330 train_time:37464ms step_avg:38.03ms
step:986/2330 train_time:37506ms step_avg:38.04ms
step:987/2330 train_time:37540ms step_avg:38.03ms
step:988/2330 train_time:37581ms step_avg:38.04ms
step:989/2330 train_time:37616ms step_avg:38.03ms
step:990/2330 train_time:37657ms step_avg:38.04ms
step:991/2330 train_time:37691ms step_avg:38.03ms
step:992/2330 train_time:37732ms step_avg:38.04ms
step:993/2330 train_time:37767ms step_avg:38.03ms
step:994/2330 train_time:37808ms step_avg:38.04ms
step:995/2330 train_time:37843ms step_avg:38.03ms
step:996/2330 train_time:37884ms step_avg:38.04ms
step:997/2330 train_time:37919ms step_avg:38.03ms
step:998/2330 train_time:37960ms step_avg:38.04ms
step:999/2330 train_time:37995ms step_avg:38.03ms
step:1000/2330 train_time:38036ms step_avg:38.04ms
step:1000/2330 val_loss:5.3247 train_time:38147ms step_avg:38.15ms
step:1001/2330 train_time:38157ms step_avg:38.12ms
step:1002/2330 train_time:38168ms step_avg:38.09ms
step:1003/2330 train_time:38177ms step_avg:38.06ms
step:1004/2330 train_time:38189ms step_avg:38.04ms
step:1005/2330 train_time:38224ms step_avg:38.03ms
step:1006/2330 train_time:38264ms step_avg:38.04ms
step:1007/2330 train_time:38299ms step_avg:38.03ms
step:1008/2330 train_time:38339ms step_avg:38.04ms
step:1009/2330 train_time:38374ms step_avg:38.03ms
step:1010/2330 train_time:38414ms step_avg:38.03ms
step:1011/2330 train_time:38450ms step_avg:38.03ms
step:1012/2330 train_time:38492ms step_avg:38.04ms
step:1013/2330 train_time:38531ms step_avg:38.04ms
step:1014/2330 train_time:38572ms step_avg:38.04ms
step:1015/2330 train_time:38608ms step_avg:38.04ms
step:1016/2330 train_time:38649ms step_avg:38.04ms
step:1017/2330 train_time:38686ms step_avg:38.04ms
step:1018/2330 train_time:38726ms step_avg:38.04ms
step:1019/2330 train_time:38762ms step_avg:38.04ms
step:1020/2330 train_time:38803ms step_avg:38.04ms
step:1021/2330 train_time:38838ms step_avg:38.04ms
step:1022/2330 train_time:38878ms step_avg:38.04ms
step:1023/2330 train_time:38913ms step_avg:38.04ms
step:1024/2330 train_time:38954ms step_avg:38.04ms
step:1025/2330 train_time:38989ms step_avg:38.04ms
step:1026/2330 train_time:39030ms step_avg:38.04ms
step:1027/2330 train_time:39065ms step_avg:38.04ms
step:1028/2330 train_time:39106ms step_avg:38.04ms
step:1029/2330 train_time:39141ms step_avg:38.04ms
step:1030/2330 train_time:39182ms step_avg:38.04ms
step:1031/2330 train_time:39217ms step_avg:38.04ms
step:1032/2330 train_time:39258ms step_avg:38.04ms
step:1033/2330 train_time:39293ms step_avg:38.04ms
step:1034/2330 train_time:39334ms step_avg:38.04ms
step:1035/2330 train_time:39369ms step_avg:38.04ms
step:1036/2330 train_time:39410ms step_avg:38.04ms
step:1037/2330 train_time:39446ms step_avg:38.04ms
step:1038/2330 train_time:39486ms step_avg:38.04ms
step:1039/2330 train_time:39523ms step_avg:38.04ms
step:1040/2330 train_time:39563ms step_avg:38.04ms
step:1041/2330 train_time:39599ms step_avg:38.04ms
step:1042/2330 train_time:39641ms step_avg:38.04ms
step:1043/2330 train_time:39675ms step_avg:38.04ms
step:1044/2330 train_time:39716ms step_avg:38.04ms
step:1045/2330 train_time:39752ms step_avg:38.04ms
step:1046/2330 train_time:39793ms step_avg:38.04ms
step:1047/2330 train_time:39828ms step_avg:38.04ms
step:1048/2330 train_time:39869ms step_avg:38.04ms
step:1049/2330 train_time:39904ms step_avg:38.04ms
step:1050/2330 train_time:39945ms step_avg:38.04ms
step:1051/2330 train_time:39980ms step_avg:38.04ms
step:1052/2330 train_time:40021ms step_avg:38.04ms
step:1053/2330 train_time:40056ms step_avg:38.04ms
step:1054/2330 train_time:40097ms step_avg:38.04ms
step:1055/2330 train_time:40133ms step_avg:38.04ms
step:1056/2330 train_time:40174ms step_avg:38.04ms
step:1057/2330 train_time:40209ms step_avg:38.04ms
step:1058/2330 train_time:40251ms step_avg:38.04ms
step:1059/2330 train_time:40285ms step_avg:38.04ms
step:1060/2330 train_time:40326ms step_avg:38.04ms
step:1061/2330 train_time:40361ms step_avg:38.04ms
step:1062/2330 train_time:40402ms step_avg:38.04ms
step:1063/2330 train_time:40437ms step_avg:38.04ms
step:1064/2330 train_time:40478ms step_avg:38.04ms
step:1065/2330 train_time:40514ms step_avg:38.04ms
step:1066/2330 train_time:40556ms step_avg:38.04ms
step:1067/2330 train_time:40591ms step_avg:38.04ms
step:1068/2330 train_time:40633ms step_avg:38.05ms
step:1069/2330 train_time:40667ms step_avg:38.04ms
step:1070/2330 train_time:40708ms step_avg:38.05ms
step:1071/2330 train_time:40743ms step_avg:38.04ms
step:1072/2330 train_time:40783ms step_avg:38.04ms
step:1073/2330 train_time:40819ms step_avg:38.04ms
step:1074/2330 train_time:40860ms step_avg:38.04ms
step:1075/2330 train_time:40895ms step_avg:38.04ms
step:1076/2330 train_time:40936ms step_avg:38.05ms
step:1077/2330 train_time:40971ms step_avg:38.04ms
step:1078/2330 train_time:41012ms step_avg:38.04ms
step:1079/2330 train_time:41048ms step_avg:38.04ms
step:1080/2330 train_time:41089ms step_avg:38.05ms
step:1081/2330 train_time:41125ms step_avg:38.04ms
step:1082/2330 train_time:41165ms step_avg:38.05ms
step:1083/2330 train_time:41201ms step_avg:38.04ms
step:1084/2330 train_time:41243ms step_avg:38.05ms
step:1085/2330 train_time:41277ms step_avg:38.04ms
step:1086/2330 train_time:41319ms step_avg:38.05ms
step:1087/2330 train_time:41353ms step_avg:38.04ms
step:1088/2330 train_time:41394ms step_avg:38.05ms
step:1089/2330 train_time:41430ms step_avg:38.04ms
step:1090/2330 train_time:41471ms step_avg:38.05ms
step:1091/2330 train_time:41506ms step_avg:38.04ms
step:1092/2330 train_time:41547ms step_avg:38.05ms
step:1093/2330 train_time:41583ms step_avg:38.04ms
step:1094/2330 train_time:41623ms step_avg:38.05ms
step:1095/2330 train_time:41659ms step_avg:38.04ms
step:1096/2330 train_time:41700ms step_avg:38.05ms
step:1097/2330 train_time:41734ms step_avg:38.04ms
step:1098/2330 train_time:41776ms step_avg:38.05ms
step:1099/2330 train_time:41811ms step_avg:38.04ms
step:1100/2330 train_time:41853ms step_avg:38.05ms
step:1101/2330 train_time:41888ms step_avg:38.05ms
step:1102/2330 train_time:41929ms step_avg:38.05ms
step:1103/2330 train_time:41964ms step_avg:38.05ms
step:1104/2330 train_time:42005ms step_avg:38.05ms
step:1105/2330 train_time:42041ms step_avg:38.05ms
step:1106/2330 train_time:42082ms step_avg:38.05ms
step:1107/2330 train_time:42117ms step_avg:38.05ms
step:1108/2330 train_time:42159ms step_avg:38.05ms
step:1109/2330 train_time:42194ms step_avg:38.05ms
step:1110/2330 train_time:42235ms step_avg:38.05ms
step:1111/2330 train_time:42270ms step_avg:38.05ms
step:1112/2330 train_time:42312ms step_avg:38.05ms
step:1113/2330 train_time:42347ms step_avg:38.05ms
step:1114/2330 train_time:42388ms step_avg:38.05ms
step:1115/2330 train_time:42424ms step_avg:38.05ms
step:1116/2330 train_time:42465ms step_avg:38.05ms
step:1117/2330 train_time:42501ms step_avg:38.05ms
step:1118/2330 train_time:42542ms step_avg:38.05ms
step:1119/2330 train_time:42577ms step_avg:38.05ms
step:1120/2330 train_time:42618ms step_avg:38.05ms
step:1121/2330 train_time:42653ms step_avg:38.05ms
step:1122/2330 train_time:42695ms step_avg:38.05ms
step:1123/2330 train_time:42731ms step_avg:38.05ms
step:1124/2330 train_time:42771ms step_avg:38.05ms
step:1125/2330 train_time:42807ms step_avg:38.05ms
step:1126/2330 train_time:42848ms step_avg:38.05ms
step:1127/2330 train_time:42883ms step_avg:38.05ms
step:1128/2330 train_time:42923ms step_avg:38.05ms
step:1129/2330 train_time:42958ms step_avg:38.05ms
step:1130/2330 train_time:43000ms step_avg:38.05ms
step:1131/2330 train_time:43034ms step_avg:38.05ms
step:1132/2330 train_time:43076ms step_avg:38.05ms
step:1133/2330 train_time:43110ms step_avg:38.05ms
step:1134/2330 train_time:43152ms step_avg:38.05ms
step:1135/2330 train_time:43187ms step_avg:38.05ms
step:1136/2330 train_time:43228ms step_avg:38.05ms
step:1137/2330 train_time:43262ms step_avg:38.05ms
step:1138/2330 train_time:43303ms step_avg:38.05ms
step:1139/2330 train_time:43339ms step_avg:38.05ms
step:1140/2330 train_time:43380ms step_avg:38.05ms
step:1141/2330 train_time:43416ms step_avg:38.05ms
step:1142/2330 train_time:43457ms step_avg:38.05ms
step:1143/2330 train_time:43492ms step_avg:38.05ms
step:1144/2330 train_time:43533ms step_avg:38.05ms
step:1145/2330 train_time:43569ms step_avg:38.05ms
step:1146/2330 train_time:43610ms step_avg:38.05ms
step:1147/2330 train_time:43646ms step_avg:38.05ms
step:1148/2330 train_time:43687ms step_avg:38.05ms
step:1149/2330 train_time:43723ms step_avg:38.05ms
step:1150/2330 train_time:43764ms step_avg:38.06ms
step:1151/2330 train_time:43799ms step_avg:38.05ms
step:1152/2330 train_time:43840ms step_avg:38.06ms
step:1153/2330 train_time:43875ms step_avg:38.05ms
step:1154/2330 train_time:43916ms step_avg:38.06ms
step:1155/2330 train_time:43952ms step_avg:38.05ms
step:1156/2330 train_time:43993ms step_avg:38.06ms
step:1157/2330 train_time:44028ms step_avg:38.05ms
step:1158/2330 train_time:44069ms step_avg:38.06ms
step:1159/2330 train_time:44105ms step_avg:38.05ms
step:1160/2330 train_time:44146ms step_avg:38.06ms
step:1161/2330 train_time:44181ms step_avg:38.05ms
step:1162/2330 train_time:44222ms step_avg:38.06ms
step:1163/2330 train_time:44257ms step_avg:38.05ms
step:1164/2330 train_time:44298ms step_avg:38.06ms
step:1165/2330 train_time:44333ms step_avg:38.05ms
step:1166/2330 train_time:44374ms step_avg:38.06ms
step:1167/2330 train_time:44410ms step_avg:38.05ms
step:1168/2330 train_time:44451ms step_avg:38.06ms
step:1169/2330 train_time:44487ms step_avg:38.06ms
step:1170/2330 train_time:44528ms step_avg:38.06ms
step:1171/2330 train_time:44562ms step_avg:38.06ms
step:1172/2330 train_time:44604ms step_avg:38.06ms
step:1173/2330 train_time:44639ms step_avg:38.06ms
step:1174/2330 train_time:44680ms step_avg:38.06ms
step:1175/2330 train_time:44715ms step_avg:38.06ms
step:1176/2330 train_time:44756ms step_avg:38.06ms
step:1177/2330 train_time:44792ms step_avg:38.06ms
step:1178/2330 train_time:44833ms step_avg:38.06ms
step:1179/2330 train_time:44868ms step_avg:38.06ms
step:1180/2330 train_time:44909ms step_avg:38.06ms
step:1181/2330 train_time:44944ms step_avg:38.06ms
step:1182/2330 train_time:44985ms step_avg:38.06ms
step:1183/2330 train_time:45020ms step_avg:38.06ms
step:1184/2330 train_time:45061ms step_avg:38.06ms
step:1185/2330 train_time:45096ms step_avg:38.06ms
step:1186/2330 train_time:45137ms step_avg:38.06ms
step:1187/2330 train_time:45172ms step_avg:38.06ms
step:1188/2330 train_time:45213ms step_avg:38.06ms
step:1189/2330 train_time:45248ms step_avg:38.06ms
step:1190/2330 train_time:45290ms step_avg:38.06ms
step:1191/2330 train_time:45325ms step_avg:38.06ms
step:1192/2330 train_time:45366ms step_avg:38.06ms
step:1193/2330 train_time:45401ms step_avg:38.06ms
step:1194/2330 train_time:45442ms step_avg:38.06ms
step:1195/2330 train_time:45477ms step_avg:38.06ms
step:1196/2330 train_time:45518ms step_avg:38.06ms
step:1197/2330 train_time:45555ms step_avg:38.06ms
step:1198/2330 train_time:45596ms step_avg:38.06ms
step:1199/2330 train_time:45631ms step_avg:38.06ms
step:1200/2330 train_time:45672ms step_avg:38.06ms
step:1201/2330 train_time:45708ms step_avg:38.06ms
step:1202/2330 train_time:45749ms step_avg:38.06ms
step:1203/2330 train_time:45784ms step_avg:38.06ms
step:1204/2330 train_time:45825ms step_avg:38.06ms
step:1205/2330 train_time:45860ms step_avg:38.06ms
step:1206/2330 train_time:45901ms step_avg:38.06ms
step:1207/2330 train_time:45936ms step_avg:38.06ms
step:1208/2330 train_time:45977ms step_avg:38.06ms
step:1209/2330 train_time:46012ms step_avg:38.06ms
step:1210/2330 train_time:46053ms step_avg:38.06ms
step:1211/2330 train_time:46089ms step_avg:38.06ms
step:1212/2330 train_time:46130ms step_avg:38.06ms
step:1213/2330 train_time:46165ms step_avg:38.06ms
step:1214/2330 train_time:46206ms step_avg:38.06ms
step:1215/2330 train_time:46241ms step_avg:38.06ms
step:1216/2330 train_time:46282ms step_avg:38.06ms
step:1217/2330 train_time:46317ms step_avg:38.06ms
step:1218/2330 train_time:46358ms step_avg:38.06ms
step:1219/2330 train_time:46394ms step_avg:38.06ms
step:1220/2330 train_time:46434ms step_avg:38.06ms
step:1221/2330 train_time:46470ms step_avg:38.06ms
step:1222/2330 train_time:46511ms step_avg:38.06ms
step:1223/2330 train_time:46547ms step_avg:38.06ms
step:1224/2330 train_time:46588ms step_avg:38.06ms
step:1225/2330 train_time:46623ms step_avg:38.06ms
step:1226/2330 train_time:46663ms step_avg:38.06ms
step:1227/2330 train_time:46698ms step_avg:38.06ms
step:1228/2330 train_time:46739ms step_avg:38.06ms
step:1229/2330 train_time:46775ms step_avg:38.06ms
step:1230/2330 train_time:46816ms step_avg:38.06ms
step:1231/2330 train_time:46852ms step_avg:38.06ms
step:1232/2330 train_time:46893ms step_avg:38.06ms
step:1233/2330 train_time:46928ms step_avg:38.06ms
step:1234/2330 train_time:46969ms step_avg:38.06ms
step:1235/2330 train_time:47005ms step_avg:38.06ms
step:1236/2330 train_time:47045ms step_avg:38.06ms
step:1237/2330 train_time:47080ms step_avg:38.06ms
step:1238/2330 train_time:47121ms step_avg:38.06ms
step:1239/2330 train_time:47157ms step_avg:38.06ms
step:1240/2330 train_time:47198ms step_avg:38.06ms
step:1241/2330 train_time:47233ms step_avg:38.06ms
step:1242/2330 train_time:47274ms step_avg:38.06ms
step:1243/2330 train_time:47310ms step_avg:38.06ms
step:1244/2330 train_time:47351ms step_avg:38.06ms
step:1245/2330 train_time:47387ms step_avg:38.06ms
step:1246/2330 train_time:47427ms step_avg:38.06ms
step:1247/2330 train_time:47463ms step_avg:38.06ms
step:1248/2330 train_time:47503ms step_avg:38.06ms
step:1249/2330 train_time:47538ms step_avg:38.06ms
step:1250/2330 train_time:47579ms step_avg:38.06ms
step:1250/2330 val_loss:5.2927 train_time:47692ms step_avg:38.15ms
step:1251/2330 train_time:47703ms step_avg:38.13ms
step:1252/2330 train_time:47714ms step_avg:38.11ms
step:1253/2330 train_time:47723ms step_avg:38.09ms
step:1254/2330 train_time:47734ms step_avg:38.07ms
step:1255/2330 train_time:47770ms step_avg:38.06ms
step:1256/2330 train_time:47810ms step_avg:38.07ms
step:1257/2330 train_time:47844ms step_avg:38.06ms
step:1258/2330 train_time:47885ms step_avg:38.06ms
step:1259/2330 train_time:47919ms step_avg:38.06ms
step:1260/2330 train_time:47960ms step_avg:38.06ms
step:1261/2330 train_time:47996ms step_avg:38.06ms
step:1262/2330 train_time:48037ms step_avg:38.06ms
step:1263/2330 train_time:48076ms step_avg:38.06ms
step:1264/2330 train_time:48117ms step_avg:38.07ms
step:1265/2330 train_time:48153ms step_avg:38.07ms
step:1266/2330 train_time:48193ms step_avg:38.07ms
step:1267/2330 train_time:48229ms step_avg:38.07ms
step:1268/2330 train_time:48270ms step_avg:38.07ms
step:1269/2330 train_time:48305ms step_avg:38.07ms
step:1270/2330 train_time:48346ms step_avg:38.07ms
step:1271/2330 train_time:48380ms step_avg:38.06ms
step:1272/2330 train_time:48421ms step_avg:38.07ms
step:1273/2330 train_time:48455ms step_avg:38.06ms
step:1274/2330 train_time:48496ms step_avg:38.07ms
step:1275/2330 train_time:48531ms step_avg:38.06ms
step:1276/2330 train_time:48572ms step_avg:38.07ms
step:1277/2330 train_time:48608ms step_avg:38.06ms
step:1278/2330 train_time:48649ms step_avg:38.07ms
step:1279/2330 train_time:48685ms step_avg:38.06ms
step:1280/2330 train_time:48725ms step_avg:38.07ms
step:1281/2330 train_time:48760ms step_avg:38.06ms
step:1282/2330 train_time:48801ms step_avg:38.07ms
step:1283/2330 train_time:48836ms step_avg:38.06ms
step:1284/2330 train_time:48876ms step_avg:38.07ms
step:1285/2330 train_time:48912ms step_avg:38.06ms
step:1286/2330 train_time:48952ms step_avg:38.07ms
step:1287/2330 train_time:48988ms step_avg:38.06ms
step:1288/2330 train_time:49030ms step_avg:38.07ms
step:1289/2330 train_time:49066ms step_avg:38.07ms
step:1290/2330 train_time:49108ms step_avg:38.07ms
step:1291/2330 train_time:49143ms step_avg:38.07ms
step:1292/2330 train_time:49185ms step_avg:38.07ms
step:1293/2330 train_time:49219ms step_avg:38.07ms
step:1294/2330 train_time:49260ms step_avg:38.07ms
step:1295/2330 train_time:49295ms step_avg:38.07ms
step:1296/2330 train_time:49336ms step_avg:38.07ms
step:1297/2330 train_time:49372ms step_avg:38.07ms
step:1298/2330 train_time:49413ms step_avg:38.07ms
step:1299/2330 train_time:49447ms step_avg:38.07ms
step:1300/2330 train_time:49488ms step_avg:38.07ms
step:1301/2330 train_time:49523ms step_avg:38.07ms
step:1302/2330 train_time:49564ms step_avg:38.07ms
step:1303/2330 train_time:49600ms step_avg:38.07ms
step:1304/2330 train_time:49640ms step_avg:38.07ms
step:1305/2330 train_time:49676ms step_avg:38.07ms
step:1306/2330 train_time:49717ms step_avg:38.07ms
step:1307/2330 train_time:49751ms step_avg:38.07ms
step:1308/2330 train_time:49792ms step_avg:38.07ms
step:1309/2330 train_time:49828ms step_avg:38.07ms
step:1310/2330 train_time:49869ms step_avg:38.07ms
step:1311/2330 train_time:49905ms step_avg:38.07ms
step:1312/2330 train_time:49946ms step_avg:38.07ms
step:1313/2330 train_time:49981ms step_avg:38.07ms
step:1314/2330 train_time:50021ms step_avg:38.07ms
step:1315/2330 train_time:50058ms step_avg:38.07ms
step:1316/2330 train_time:50099ms step_avg:38.07ms
step:1317/2330 train_time:50135ms step_avg:38.07ms
step:1318/2330 train_time:50176ms step_avg:38.07ms
step:1319/2330 train_time:50211ms step_avg:38.07ms
step:1320/2330 train_time:50252ms step_avg:38.07ms
step:1321/2330 train_time:50287ms step_avg:38.07ms
step:1322/2330 train_time:50328ms step_avg:38.07ms
step:1323/2330 train_time:50364ms step_avg:38.07ms
step:1324/2330 train_time:50405ms step_avg:38.07ms
step:1325/2330 train_time:50440ms step_avg:38.07ms
step:1326/2330 train_time:50481ms step_avg:38.07ms
step:1327/2330 train_time:50516ms step_avg:38.07ms
step:1328/2330 train_time:50556ms step_avg:38.07ms
step:1329/2330 train_time:50593ms step_avg:38.07ms
step:1330/2330 train_time:50634ms step_avg:38.07ms
step:1331/2330 train_time:50669ms step_avg:38.07ms
step:1332/2330 train_time:50710ms step_avg:38.07ms
step:1333/2330 train_time:50745ms step_avg:38.07ms
step:1334/2330 train_time:50786ms step_avg:38.07ms
step:1335/2330 train_time:50821ms step_avg:38.07ms
step:1336/2330 train_time:50861ms step_avg:38.07ms
step:1337/2330 train_time:50897ms step_avg:38.07ms
step:1338/2330 train_time:50938ms step_avg:38.07ms
step:1339/2330 train_time:50973ms step_avg:38.07ms
step:1340/2330 train_time:51014ms step_avg:38.07ms
step:1341/2330 train_time:51050ms step_avg:38.07ms
step:1342/2330 train_time:51091ms step_avg:38.07ms
step:1343/2330 train_time:51127ms step_avg:38.07ms
step:1344/2330 train_time:51168ms step_avg:38.07ms
step:1345/2330 train_time:51203ms step_avg:38.07ms
step:1346/2330 train_time:51244ms step_avg:38.07ms
step:1347/2330 train_time:51279ms step_avg:38.07ms
step:1348/2330 train_time:51319ms step_avg:38.07ms
step:1349/2330 train_time:51355ms step_avg:38.07ms
step:1350/2330 train_time:51395ms step_avg:38.07ms
step:1351/2330 train_time:51431ms step_avg:38.07ms
step:1352/2330 train_time:51472ms step_avg:38.07ms
step:1353/2330 train_time:51508ms step_avg:38.07ms
step:1354/2330 train_time:51549ms step_avg:38.07ms
step:1355/2330 train_time:51584ms step_avg:38.07ms
step:1356/2330 train_time:51625ms step_avg:38.07ms
step:1357/2330 train_time:51660ms step_avg:38.07ms
step:1358/2330 train_time:51700ms step_avg:38.07ms
step:1359/2330 train_time:51735ms step_avg:38.07ms
step:1360/2330 train_time:51776ms step_avg:38.07ms
step:1361/2330 train_time:51811ms step_avg:38.07ms
step:1362/2330 train_time:51851ms step_avg:38.07ms
step:1363/2330 train_time:51888ms step_avg:38.07ms
step:1364/2330 train_time:51929ms step_avg:38.07ms
step:1365/2330 train_time:51965ms step_avg:38.07ms
step:1366/2330 train_time:52006ms step_avg:38.07ms
step:1367/2330 train_time:52041ms step_avg:38.07ms
step:1368/2330 train_time:52081ms step_avg:38.07ms
step:1369/2330 train_time:52117ms step_avg:38.07ms
step:1370/2330 train_time:52158ms step_avg:38.07ms
step:1371/2330 train_time:52193ms step_avg:38.07ms
step:1372/2330 train_time:52234ms step_avg:38.07ms
step:1373/2330 train_time:52269ms step_avg:38.07ms
step:1374/2330 train_time:52310ms step_avg:38.07ms
step:1375/2330 train_time:52346ms step_avg:38.07ms
step:1376/2330 train_time:52387ms step_avg:38.07ms
step:1377/2330 train_time:52422ms step_avg:38.07ms
step:1378/2330 train_time:52463ms step_avg:38.07ms
step:1379/2330 train_time:52498ms step_avg:38.07ms
step:1380/2330 train_time:52539ms step_avg:38.07ms
step:1381/2330 train_time:52574ms step_avg:38.07ms
step:1382/2330 train_time:52615ms step_avg:38.07ms
step:1383/2330 train_time:52650ms step_avg:38.07ms
step:1384/2330 train_time:52691ms step_avg:38.07ms
step:1385/2330 train_time:52727ms step_avg:38.07ms
step:1386/2330 train_time:52768ms step_avg:38.07ms
step:1387/2330 train_time:52804ms step_avg:38.07ms
step:1388/2330 train_time:52845ms step_avg:38.07ms
step:1389/2330 train_time:52881ms step_avg:38.07ms
step:1390/2330 train_time:52921ms step_avg:38.07ms
step:1391/2330 train_time:52957ms step_avg:38.07ms
step:1392/2330 train_time:52998ms step_avg:38.07ms
step:1393/2330 train_time:53034ms step_avg:38.07ms
step:1394/2330 train_time:53075ms step_avg:38.07ms
step:1395/2330 train_time:53110ms step_avg:38.07ms
step:1396/2330 train_time:53151ms step_avg:38.07ms
step:1397/2330 train_time:53187ms step_avg:38.07ms
step:1398/2330 train_time:53227ms step_avg:38.07ms
step:1399/2330 train_time:53263ms step_avg:38.07ms
step:1400/2330 train_time:53304ms step_avg:38.07ms
step:1401/2330 train_time:53339ms step_avg:38.07ms
step:1402/2330 train_time:53380ms step_avg:38.07ms
step:1403/2330 train_time:53415ms step_avg:38.07ms
step:1404/2330 train_time:53456ms step_avg:38.07ms
step:1405/2330 train_time:53491ms step_avg:38.07ms
step:1406/2330 train_time:53532ms step_avg:38.07ms
step:1407/2330 train_time:53568ms step_avg:38.07ms
step:1408/2330 train_time:53609ms step_avg:38.07ms
step:1409/2330 train_time:53644ms step_avg:38.07ms
step:1410/2330 train_time:53685ms step_avg:38.07ms
step:1411/2330 train_time:53720ms step_avg:38.07ms
step:1412/2330 train_time:53760ms step_avg:38.07ms
step:1413/2330 train_time:53796ms step_avg:38.07ms
step:1414/2330 train_time:53837ms step_avg:38.07ms
step:1415/2330 train_time:53872ms step_avg:38.07ms
step:1416/2330 train_time:53913ms step_avg:38.07ms
step:1417/2330 train_time:53949ms step_avg:38.07ms
step:1418/2330 train_time:53990ms step_avg:38.07ms
step:1419/2330 train_time:54026ms step_avg:38.07ms
step:1420/2330 train_time:54067ms step_avg:38.08ms
step:1421/2330 train_time:54103ms step_avg:38.07ms
step:1422/2330 train_time:54143ms step_avg:38.08ms
step:1423/2330 train_time:54178ms step_avg:38.07ms
step:1424/2330 train_time:54219ms step_avg:38.08ms
step:1425/2330 train_time:54254ms step_avg:38.07ms
step:1426/2330 train_time:54295ms step_avg:38.08ms
step:1427/2330 train_time:54331ms step_avg:38.07ms
step:1428/2330 train_time:54372ms step_avg:38.08ms
step:1429/2330 train_time:54407ms step_avg:38.07ms
step:1430/2330 train_time:54448ms step_avg:38.08ms
step:1431/2330 train_time:54485ms step_avg:38.07ms
step:1432/2330 train_time:54525ms step_avg:38.08ms
step:1433/2330 train_time:54561ms step_avg:38.07ms
step:1434/2330 train_time:54601ms step_avg:38.08ms
step:1435/2330 train_time:54637ms step_avg:38.07ms
step:1436/2330 train_time:54678ms step_avg:38.08ms
step:1437/2330 train_time:54713ms step_avg:38.07ms
step:1438/2330 train_time:54754ms step_avg:38.08ms
step:1439/2330 train_time:54789ms step_avg:38.07ms
step:1440/2330 train_time:54830ms step_avg:38.08ms
step:1441/2330 train_time:54866ms step_avg:38.07ms
step:1442/2330 train_time:54907ms step_avg:38.08ms
step:1443/2330 train_time:54942ms step_avg:38.07ms
step:1444/2330 train_time:54982ms step_avg:38.08ms
step:1445/2330 train_time:55018ms step_avg:38.07ms
step:1446/2330 train_time:55059ms step_avg:38.08ms
step:1447/2330 train_time:55094ms step_avg:38.07ms
step:1448/2330 train_time:55135ms step_avg:38.08ms
step:1449/2330 train_time:55170ms step_avg:38.07ms
step:1450/2330 train_time:55211ms step_avg:38.08ms
step:1451/2330 train_time:55246ms step_avg:38.07ms
step:1452/2330 train_time:55287ms step_avg:38.08ms
step:1453/2330 train_time:55322ms step_avg:38.07ms
step:1454/2330 train_time:55363ms step_avg:38.08ms
step:1455/2330 train_time:55398ms step_avg:38.07ms
step:1456/2330 train_time:55439ms step_avg:38.08ms
step:1457/2330 train_time:55475ms step_avg:38.07ms
step:1458/2330 train_time:55516ms step_avg:38.08ms
step:1459/2330 train_time:55551ms step_avg:38.07ms
step:1460/2330 train_time:55592ms step_avg:38.08ms
step:1461/2330 train_time:55627ms step_avg:38.07ms
step:1462/2330 train_time:55668ms step_avg:38.08ms
step:1463/2330 train_time:55703ms step_avg:38.07ms
step:1464/2330 train_time:55744ms step_avg:38.08ms
step:1465/2330 train_time:55779ms step_avg:38.07ms
step:1466/2330 train_time:55820ms step_avg:38.08ms
step:1467/2330 train_time:55854ms step_avg:38.07ms
step:1468/2330 train_time:55896ms step_avg:38.08ms
step:1469/2330 train_time:55931ms step_avg:38.07ms
step:1470/2330 train_time:55972ms step_avg:38.08ms
step:1471/2330 train_time:56008ms step_avg:38.07ms
step:1472/2330 train_time:56049ms step_avg:38.08ms
step:1473/2330 train_time:56084ms step_avg:38.07ms
step:1474/2330 train_time:56125ms step_avg:38.08ms
step:1475/2330 train_time:56161ms step_avg:38.08ms
step:1476/2330 train_time:56202ms step_avg:38.08ms
step:1477/2330 train_time:56237ms step_avg:38.08ms
step:1478/2330 train_time:56278ms step_avg:38.08ms
step:1479/2330 train_time:56312ms step_avg:38.07ms
step:1480/2330 train_time:56353ms step_avg:38.08ms
step:1481/2330 train_time:56390ms step_avg:38.08ms
step:1482/2330 train_time:56431ms step_avg:38.08ms
step:1483/2330 train_time:56466ms step_avg:38.08ms
step:1484/2330 train_time:56507ms step_avg:38.08ms
step:1485/2330 train_time:56543ms step_avg:38.08ms
step:1486/2330 train_time:56584ms step_avg:38.08ms
step:1487/2330 train_time:56620ms step_avg:38.08ms
step:1488/2330 train_time:56660ms step_avg:38.08ms
step:1489/2330 train_time:56696ms step_avg:38.08ms
step:1490/2330 train_time:56737ms step_avg:38.08ms
step:1491/2330 train_time:56772ms step_avg:38.08ms
step:1492/2330 train_time:56814ms step_avg:38.08ms
step:1493/2330 train_time:56848ms step_avg:38.08ms
step:1494/2330 train_time:56889ms step_avg:38.08ms
step:1495/2330 train_time:56924ms step_avg:38.08ms
step:1496/2330 train_time:56966ms step_avg:38.08ms
step:1497/2330 train_time:57000ms step_avg:38.08ms
step:1498/2330 train_time:57040ms step_avg:38.08ms
step:1499/2330 train_time:57076ms step_avg:38.08ms
step:1500/2330 train_time:57117ms step_avg:38.08ms
step:1500/2330 val_loss:5.2565 train_time:57229ms step_avg:38.15ms
step:1501/2330 train_time:57241ms step_avg:38.14ms
step:1502/2330 train_time:57253ms step_avg:38.12ms
step:1503/2330 train_time:57263ms step_avg:38.10ms
step:1504/2330 train_time:57273ms step_avg:38.08ms
step:1505/2330 train_time:57310ms step_avg:38.08ms
step:1506/2330 train_time:57351ms step_avg:38.08ms
step:1507/2330 train_time:57384ms step_avg:38.08ms
step:1508/2330 train_time:57425ms step_avg:38.08ms
step:1509/2330 train_time:57460ms step_avg:38.08ms
step:1510/2330 train_time:57500ms step_avg:38.08ms
step:1511/2330 train_time:57536ms step_avg:38.08ms
step:1512/2330 train_time:57577ms step_avg:38.08ms
step:1513/2330 train_time:57615ms step_avg:38.08ms
step:1514/2330 train_time:57656ms step_avg:38.08ms
step:1515/2330 train_time:57692ms step_avg:38.08ms
step:1516/2330 train_time:57733ms step_avg:38.08ms
step:1517/2330 train_time:57768ms step_avg:38.08ms
step:1518/2330 train_time:57809ms step_avg:38.08ms
step:1519/2330 train_time:57844ms step_avg:38.08ms
step:1520/2330 train_time:57884ms step_avg:38.08ms
step:1521/2330 train_time:57919ms step_avg:38.08ms
step:1522/2330 train_time:57960ms step_avg:38.08ms
step:1523/2330 train_time:57994ms step_avg:38.08ms
step:1524/2330 train_time:58035ms step_avg:38.08ms
step:1525/2330 train_time:58069ms step_avg:38.08ms
step:1526/2330 train_time:58110ms step_avg:38.08ms
step:1527/2330 train_time:58145ms step_avg:38.08ms
step:1528/2330 train_time:58188ms step_avg:38.08ms
step:1529/2330 train_time:58223ms step_avg:38.08ms
step:1530/2330 train_time:58266ms step_avg:38.08ms
step:1531/2330 train_time:58300ms step_avg:38.08ms
step:1532/2330 train_time:58341ms step_avg:38.08ms
step:1533/2330 train_time:58375ms step_avg:38.08ms
step:1534/2330 train_time:58416ms step_avg:38.08ms
step:1535/2330 train_time:58451ms step_avg:38.08ms
step:1536/2330 train_time:58492ms step_avg:38.08ms
step:1537/2330 train_time:58529ms step_avg:38.08ms
step:1538/2330 train_time:58570ms step_avg:38.08ms
step:1539/2330 train_time:58607ms step_avg:38.08ms
step:1540/2330 train_time:58648ms step_avg:38.08ms
step:1541/2330 train_time:58683ms step_avg:38.08ms
step:1542/2330 train_time:58725ms step_avg:38.08ms
step:1543/2330 train_time:58760ms step_avg:38.08ms
step:1544/2330 train_time:58802ms step_avg:38.08ms
step:1545/2330 train_time:58837ms step_avg:38.08ms
step:1546/2330 train_time:58878ms step_avg:38.08ms
step:1547/2330 train_time:58913ms step_avg:38.08ms
step:1548/2330 train_time:58953ms step_avg:38.08ms
step:1549/2330 train_time:58988ms step_avg:38.08ms
step:1550/2330 train_time:59029ms step_avg:38.08ms
step:1551/2330 train_time:59064ms step_avg:38.08ms
step:1552/2330 train_time:59105ms step_avg:38.08ms
step:1553/2330 train_time:59141ms step_avg:38.08ms
step:1554/2330 train_time:59182ms step_avg:38.08ms
step:1555/2330 train_time:59217ms step_avg:38.08ms
step:1556/2330 train_time:59258ms step_avg:38.08ms
step:1557/2330 train_time:59294ms step_avg:38.08ms
step:1558/2330 train_time:59334ms step_avg:38.08ms
step:1559/2330 train_time:59369ms step_avg:38.08ms
step:1560/2330 train_time:59411ms step_avg:38.08ms
step:1561/2330 train_time:59445ms step_avg:38.08ms
step:1562/2330 train_time:59486ms step_avg:38.08ms
step:1563/2330 train_time:59523ms step_avg:38.08ms
step:1564/2330 train_time:59564ms step_avg:38.08ms
step:1565/2330 train_time:59600ms step_avg:38.08ms
step:1566/2330 train_time:59641ms step_avg:38.08ms
step:1567/2330 train_time:59676ms step_avg:38.08ms
step:1568/2330 train_time:59717ms step_avg:38.08ms
step:1569/2330 train_time:59752ms step_avg:38.08ms
step:1570/2330 train_time:59793ms step_avg:38.08ms
step:1571/2330 train_time:59828ms step_avg:38.08ms
step:1572/2330 train_time:59870ms step_avg:38.08ms
step:1573/2330 train_time:59905ms step_avg:38.08ms
step:1574/2330 train_time:59946ms step_avg:38.08ms
step:1575/2330 train_time:59981ms step_avg:38.08ms
step:1576/2330 train_time:60021ms step_avg:38.08ms
step:1577/2330 train_time:60057ms step_avg:38.08ms
step:1578/2330 train_time:60097ms step_avg:38.08ms
step:1579/2330 train_time:60132ms step_avg:38.08ms
step:1580/2330 train_time:60173ms step_avg:38.08ms
step:1581/2330 train_time:60209ms step_avg:38.08ms
step:1582/2330 train_time:60250ms step_avg:38.08ms
step:1583/2330 train_time:60285ms step_avg:38.08ms
step:1584/2330 train_time:60327ms step_avg:38.09ms
step:1585/2330 train_time:60362ms step_avg:38.08ms
step:1586/2330 train_time:60403ms step_avg:38.09ms
step:1587/2330 train_time:60438ms step_avg:38.08ms
step:1588/2330 train_time:60479ms step_avg:38.09ms
step:1589/2330 train_time:60513ms step_avg:38.08ms
step:1590/2330 train_time:60555ms step_avg:38.08ms
step:1591/2330 train_time:60590ms step_avg:38.08ms
step:1592/2330 train_time:60631ms step_avg:38.08ms
step:1593/2330 train_time:60666ms step_avg:38.08ms
step:1594/2330 train_time:60708ms step_avg:38.09ms
step:1595/2330 train_time:60743ms step_avg:38.08ms
step:1596/2330 train_time:60785ms step_avg:38.09ms
step:1597/2330 train_time:60820ms step_avg:38.08ms
step:1598/2330 train_time:60861ms step_avg:38.09ms
step:1599/2330 train_time:60896ms step_avg:38.08ms
step:1600/2330 train_time:60936ms step_avg:38.09ms
step:1601/2330 train_time:60971ms step_avg:38.08ms
step:1602/2330 train_time:61012ms step_avg:38.09ms
step:1603/2330 train_time:61047ms step_avg:38.08ms
step:1604/2330 train_time:61088ms step_avg:38.08ms
step:1605/2330 train_time:61125ms step_avg:38.08ms
step:1606/2330 train_time:61166ms step_avg:38.09ms
step:1607/2330 train_time:61201ms step_avg:38.08ms
step:1608/2330 train_time:61242ms step_avg:38.09ms
step:1609/2330 train_time:61278ms step_avg:38.08ms
step:1610/2330 train_time:61318ms step_avg:38.09ms
step:1611/2330 train_time:61354ms step_avg:38.08ms
step:1612/2330 train_time:61394ms step_avg:38.09ms
step:1613/2330 train_time:61429ms step_avg:38.08ms
step:1614/2330 train_time:61471ms step_avg:38.09ms
step:1615/2330 train_time:61506ms step_avg:38.08ms
step:1616/2330 train_time:61547ms step_avg:38.09ms
step:1617/2330 train_time:61582ms step_avg:38.08ms
step:1618/2330 train_time:61624ms step_avg:38.09ms
step:1619/2330 train_time:61658ms step_avg:38.08ms
step:1620/2330 train_time:61699ms step_avg:38.09ms
step:1621/2330 train_time:61735ms step_avg:38.08ms
step:1622/2330 train_time:61776ms step_avg:38.09ms
step:1623/2330 train_time:61811ms step_avg:38.08ms
step:1624/2330 train_time:61852ms step_avg:38.09ms
step:1625/2330 train_time:61888ms step_avg:38.08ms
step:1626/2330 train_time:61928ms step_avg:38.09ms
step:1627/2330 train_time:61964ms step_avg:38.08ms
step:1628/2330 train_time:62004ms step_avg:38.09ms
step:1629/2330 train_time:62040ms step_avg:38.08ms
step:1630/2330 train_time:62081ms step_avg:38.09ms
step:1631/2330 train_time:62117ms step_avg:38.09ms
step:1632/2330 train_time:62157ms step_avg:38.09ms
step:1633/2330 train_time:62193ms step_avg:38.09ms
step:1634/2330 train_time:62234ms step_avg:38.09ms
step:1635/2330 train_time:62269ms step_avg:38.08ms
step:1636/2330 train_time:62310ms step_avg:38.09ms
step:1637/2330 train_time:62346ms step_avg:38.09ms
step:1638/2330 train_time:62387ms step_avg:38.09ms
step:1639/2330 train_time:62422ms step_avg:38.09ms
step:1640/2330 train_time:62463ms step_avg:38.09ms
step:1641/2330 train_time:62499ms step_avg:38.09ms
step:1642/2330 train_time:62540ms step_avg:38.09ms
step:1643/2330 train_time:62574ms step_avg:38.09ms
step:1644/2330 train_time:62615ms step_avg:38.09ms
step:1645/2330 train_time:62651ms step_avg:38.09ms
step:1646/2330 train_time:62692ms step_avg:38.09ms
step:1647/2330 train_time:62727ms step_avg:38.09ms
step:1648/2330 train_time:62769ms step_avg:38.09ms
step:1649/2330 train_time:62804ms step_avg:38.09ms
step:1650/2330 train_time:62846ms step_avg:38.09ms
step:1651/2330 train_time:62880ms step_avg:38.09ms
step:1652/2330 train_time:62922ms step_avg:38.09ms
step:1653/2330 train_time:62956ms step_avg:38.09ms
step:1654/2330 train_time:62998ms step_avg:38.09ms
step:1655/2330 train_time:63032ms step_avg:38.09ms
step:1656/2330 train_time:63073ms step_avg:38.09ms
step:1657/2330 train_time:63108ms step_avg:38.09ms
step:1658/2330 train_time:63150ms step_avg:38.09ms
step:1659/2330 train_time:63184ms step_avg:38.09ms
step:1660/2330 train_time:63225ms step_avg:38.09ms
step:1661/2330 train_time:63261ms step_avg:38.09ms
step:1662/2330 train_time:63302ms step_avg:38.09ms
step:1663/2330 train_time:63338ms step_avg:38.09ms
step:1664/2330 train_time:63378ms step_avg:38.09ms
step:1665/2330 train_time:63414ms step_avg:38.09ms
step:1666/2330 train_time:63455ms step_avg:38.09ms
step:1667/2330 train_time:63489ms step_avg:38.09ms
step:1668/2330 train_time:63530ms step_avg:38.09ms
step:1669/2330 train_time:63567ms step_avg:38.09ms
step:1670/2330 train_time:63608ms step_avg:38.09ms
step:1671/2330 train_time:63644ms step_avg:38.09ms
step:1672/2330 train_time:63685ms step_avg:38.09ms
step:1673/2330 train_time:63720ms step_avg:38.09ms
step:1674/2330 train_time:63762ms step_avg:38.09ms
step:1675/2330 train_time:63796ms step_avg:38.09ms
step:1676/2330 train_time:63837ms step_avg:38.09ms
step:1677/2330 train_time:63872ms step_avg:38.09ms
step:1678/2330 train_time:63913ms step_avg:38.09ms
step:1679/2330 train_time:63948ms step_avg:38.09ms
step:1680/2330 train_time:63990ms step_avg:38.09ms
step:1681/2330 train_time:64025ms step_avg:38.09ms
step:1682/2330 train_time:64066ms step_avg:38.09ms
step:1683/2330 train_time:64102ms step_avg:38.09ms
step:1684/2330 train_time:64144ms step_avg:38.09ms
step:1685/2330 train_time:64178ms step_avg:38.09ms
step:1686/2330 train_time:64219ms step_avg:38.09ms
step:1687/2330 train_time:64254ms step_avg:38.09ms
step:1688/2330 train_time:64295ms step_avg:38.09ms
step:1689/2330 train_time:64330ms step_avg:38.09ms
step:1690/2330 train_time:64371ms step_avg:38.09ms
step:1691/2330 train_time:64407ms step_avg:38.09ms
step:1692/2330 train_time:64448ms step_avg:38.09ms
step:1693/2330 train_time:64484ms step_avg:38.09ms
step:1694/2330 train_time:64525ms step_avg:38.09ms
step:1695/2330 train_time:64561ms step_avg:38.09ms
step:1696/2330 train_time:64602ms step_avg:38.09ms
step:1697/2330 train_time:64637ms step_avg:38.09ms
step:1698/2330 train_time:64678ms step_avg:38.09ms
step:1699/2330 train_time:64713ms step_avg:38.09ms
step:1700/2330 train_time:64754ms step_avg:38.09ms
step:1701/2330 train_time:64789ms step_avg:38.09ms
step:1702/2330 train_time:64830ms step_avg:38.09ms
step:1703/2330 train_time:64867ms step_avg:38.09ms
step:1704/2330 train_time:64908ms step_avg:38.09ms
step:1705/2330 train_time:64944ms step_avg:38.09ms
step:1706/2330 train_time:64984ms step_avg:38.09ms
step:1707/2330 train_time:65020ms step_avg:38.09ms
step:1708/2330 train_time:65062ms step_avg:38.09ms
step:1709/2330 train_time:65096ms step_avg:38.09ms
step:1710/2330 train_time:65137ms step_avg:38.09ms
step:1711/2330 train_time:65172ms step_avg:38.09ms
step:1712/2330 train_time:65213ms step_avg:38.09ms
step:1713/2330 train_time:65249ms step_avg:38.09ms
step:1714/2330 train_time:65290ms step_avg:38.09ms
step:1715/2330 train_time:65324ms step_avg:38.09ms
step:1716/2330 train_time:65365ms step_avg:38.09ms
step:1717/2330 train_time:65401ms step_avg:38.09ms
step:1718/2330 train_time:65442ms step_avg:38.09ms
step:1719/2330 train_time:65478ms step_avg:38.09ms
step:1720/2330 train_time:65519ms step_avg:38.09ms
step:1721/2330 train_time:65555ms step_avg:38.09ms
step:1722/2330 train_time:65595ms step_avg:38.09ms
step:1723/2330 train_time:65630ms step_avg:38.09ms
step:1724/2330 train_time:65672ms step_avg:38.09ms
step:1725/2330 train_time:65706ms step_avg:38.09ms
step:1726/2330 train_time:65748ms step_avg:38.09ms
step:1727/2330 train_time:65783ms step_avg:38.09ms
step:1728/2330 train_time:65824ms step_avg:38.09ms
step:1729/2330 train_time:65859ms step_avg:38.09ms
step:1730/2330 train_time:65900ms step_avg:38.09ms
step:1731/2330 train_time:65936ms step_avg:38.09ms
step:1732/2330 train_time:65976ms step_avg:38.09ms
step:1733/2330 train_time:66012ms step_avg:38.09ms
step:1734/2330 train_time:66053ms step_avg:38.09ms
step:1735/2330 train_time:66088ms step_avg:38.09ms
step:1736/2330 train_time:66129ms step_avg:38.09ms
step:1737/2330 train_time:66165ms step_avg:38.09ms
step:1738/2330 train_time:66206ms step_avg:38.09ms
step:1739/2330 train_time:66242ms step_avg:38.09ms
step:1740/2330 train_time:66283ms step_avg:38.09ms
step:1741/2330 train_time:66318ms step_avg:38.09ms
step:1742/2330 train_time:66359ms step_avg:38.09ms
step:1743/2330 train_time:66394ms step_avg:38.09ms
step:1744/2330 train_time:66435ms step_avg:38.09ms
step:1745/2330 train_time:66470ms step_avg:38.09ms
step:1746/2330 train_time:66512ms step_avg:38.09ms
step:1747/2330 train_time:66546ms step_avg:38.09ms
step:1748/2330 train_time:66587ms step_avg:38.09ms
step:1749/2330 train_time:66623ms step_avg:38.09ms
step:1750/2330 train_time:66664ms step_avg:38.09ms
step:1750/2330 val_loss:5.2193 train_time:66777ms step_avg:38.16ms
step:1751/2330 train_time:66788ms step_avg:38.14ms
step:1752/2330 train_time:66799ms step_avg:38.13ms
step:1753/2330 train_time:66809ms step_avg:38.11ms
step:1754/2330 train_time:66819ms step_avg:38.10ms
step:1755/2330 train_time:66854ms step_avg:38.09ms
step:1756/2330 train_time:66894ms step_avg:38.09ms
step:1757/2330 train_time:66929ms step_avg:38.09ms
step:1758/2330 train_time:66969ms step_avg:38.09ms
step:1759/2330 train_time:67004ms step_avg:38.09ms
step:1760/2330 train_time:67044ms step_avg:38.09ms
step:1761/2330 train_time:67079ms step_avg:38.09ms
step:1762/2330 train_time:67120ms step_avg:38.09ms
step:1763/2330 train_time:67155ms step_avg:38.09ms
step:1764/2330 train_time:67196ms step_avg:38.09ms
step:1765/2330 train_time:67231ms step_avg:38.09ms
step:1766/2330 train_time:67271ms step_avg:38.09ms
step:1767/2330 train_time:67306ms step_avg:38.09ms
step:1768/2330 train_time:67347ms step_avg:38.09ms
step:1769/2330 train_time:67382ms step_avg:38.09ms
step:1770/2330 train_time:67422ms step_avg:38.09ms
step:1771/2330 train_time:67456ms step_avg:38.09ms
step:1772/2330 train_time:67497ms step_avg:38.09ms
step:1773/2330 train_time:67532ms step_avg:38.09ms
step:1774/2330 train_time:67572ms step_avg:38.09ms
step:1775/2330 train_time:67607ms step_avg:38.09ms
step:1776/2330 train_time:67648ms step_avg:38.09ms
step:1777/2330 train_time:67686ms step_avg:38.09ms
step:1778/2330 train_time:67728ms step_avg:38.09ms
step:1779/2330 train_time:67764ms step_avg:38.09ms
step:1780/2330 train_time:67806ms step_avg:38.09ms
step:1781/2330 train_time:67842ms step_avg:38.09ms
step:1782/2330 train_time:67883ms step_avg:38.09ms
step:1783/2330 train_time:67918ms step_avg:38.09ms
step:1784/2330 train_time:67959ms step_avg:38.09ms
step:1785/2330 train_time:67994ms step_avg:38.09ms
step:1786/2330 train_time:68035ms step_avg:38.09ms
step:1787/2330 train_time:68070ms step_avg:38.09ms
step:1788/2330 train_time:68110ms step_avg:38.09ms
step:1789/2330 train_time:68145ms step_avg:38.09ms
step:1790/2330 train_time:68186ms step_avg:38.09ms
step:1791/2330 train_time:68221ms step_avg:38.09ms
step:1792/2330 train_time:68261ms step_avg:38.09ms
step:1793/2330 train_time:68297ms step_avg:38.09ms
step:1794/2330 train_time:68337ms step_avg:38.09ms
step:1795/2330 train_time:68372ms step_avg:38.09ms
step:1796/2330 train_time:68413ms step_avg:38.09ms
step:1797/2330 train_time:68448ms step_avg:38.09ms
step:1798/2330 train_time:68489ms step_avg:38.09ms
step:1799/2330 train_time:68524ms step_avg:38.09ms
step:1800/2330 train_time:68564ms step_avg:38.09ms
step:1801/2330 train_time:68599ms step_avg:38.09ms
step:1802/2330 train_time:68640ms step_avg:38.09ms
step:1803/2330 train_time:68676ms step_avg:38.09ms
step:1804/2330 train_time:68718ms step_avg:38.09ms
step:1805/2330 train_time:68753ms step_avg:38.09ms
step:1806/2330 train_time:68795ms step_avg:38.09ms
step:1807/2330 train_time:68831ms step_avg:38.09ms
step:1808/2330 train_time:68872ms step_avg:38.09ms
step:1809/2330 train_time:68907ms step_avg:38.09ms
step:1810/2330 train_time:68948ms step_avg:38.09ms
step:1811/2330 train_time:68983ms step_avg:38.09ms
step:1812/2330 train_time:69024ms step_avg:38.09ms
step:1813/2330 train_time:69058ms step_avg:38.09ms
step:1814/2330 train_time:69100ms step_avg:38.09ms
step:1815/2330 train_time:69134ms step_avg:38.09ms
step:1816/2330 train_time:69175ms step_avg:38.09ms
step:1817/2330 train_time:69210ms step_avg:38.09ms
step:1818/2330 train_time:69251ms step_avg:38.09ms
step:1819/2330 train_time:69287ms step_avg:38.09ms
step:1820/2330 train_time:69328ms step_avg:38.09ms
step:1821/2330 train_time:69363ms step_avg:38.09ms
step:1822/2330 train_time:69404ms step_avg:38.09ms
step:1823/2330 train_time:69439ms step_avg:38.09ms
step:1824/2330 train_time:69480ms step_avg:38.09ms
step:1825/2330 train_time:69514ms step_avg:38.09ms
step:1826/2330 train_time:69555ms step_avg:38.09ms
step:1827/2330 train_time:69591ms step_avg:38.09ms
step:1828/2330 train_time:69631ms step_avg:38.09ms
step:1829/2330 train_time:69667ms step_avg:38.09ms
step:1830/2330 train_time:69708ms step_avg:38.09ms
step:1831/2330 train_time:69745ms step_avg:38.09ms
step:1832/2330 train_time:69786ms step_avg:38.09ms
step:1833/2330 train_time:69822ms step_avg:38.09ms
step:1834/2330 train_time:69863ms step_avg:38.09ms
step:1835/2330 train_time:69898ms step_avg:38.09ms
step:1836/2330 train_time:69939ms step_avg:38.09ms
step:1837/2330 train_time:69975ms step_avg:38.09ms
step:1838/2330 train_time:70016ms step_avg:38.09ms
step:1839/2330 train_time:70052ms step_avg:38.09ms
step:1840/2330 train_time:70092ms step_avg:38.09ms
step:1841/2330 train_time:70128ms step_avg:38.09ms
step:1842/2330 train_time:70169ms step_avg:38.09ms
step:1843/2330 train_time:70204ms step_avg:38.09ms
step:1844/2330 train_time:70245ms step_avg:38.09ms
step:1845/2330 train_time:70279ms step_avg:38.09ms
step:1846/2330 train_time:70320ms step_avg:38.09ms
step:1847/2330 train_time:70356ms step_avg:38.09ms
step:1848/2330 train_time:70397ms step_avg:38.09ms
step:1849/2330 train_time:70432ms step_avg:38.09ms
step:1850/2330 train_time:70473ms step_avg:38.09ms
step:1851/2330 train_time:70509ms step_avg:38.09ms
step:1852/2330 train_time:70549ms step_avg:38.09ms
step:1853/2330 train_time:70585ms step_avg:38.09ms
step:1854/2330 train_time:70626ms step_avg:38.09ms
step:1855/2330 train_time:70661ms step_avg:38.09ms
step:1856/2330 train_time:70702ms step_avg:38.09ms
step:1857/2330 train_time:70737ms step_avg:38.09ms
step:1858/2330 train_time:70778ms step_avg:38.09ms
step:1859/2330 train_time:70814ms step_avg:38.09ms
step:1860/2330 train_time:70854ms step_avg:38.09ms
step:1861/2330 train_time:70891ms step_avg:38.09ms
step:1862/2330 train_time:70931ms step_avg:38.09ms
step:1863/2330 train_time:70966ms step_avg:38.09ms
step:1864/2330 train_time:71007ms step_avg:38.09ms
step:1865/2330 train_time:71043ms step_avg:38.09ms
step:1866/2330 train_time:71084ms step_avg:38.09ms
step:1867/2330 train_time:71119ms step_avg:38.09ms
step:1868/2330 train_time:71160ms step_avg:38.09ms
step:1869/2330 train_time:71195ms step_avg:38.09ms
step:1870/2330 train_time:71236ms step_avg:38.09ms
step:1871/2330 train_time:71271ms step_avg:38.09ms
step:1872/2330 train_time:71312ms step_avg:38.09ms
step:1873/2330 train_time:71348ms step_avg:38.09ms
step:1874/2330 train_time:71389ms step_avg:38.09ms
step:1875/2330 train_time:71424ms step_avg:38.09ms
step:1876/2330 train_time:71464ms step_avg:38.09ms
step:1877/2330 train_time:71500ms step_avg:38.09ms
step:1878/2330 train_time:71540ms step_avg:38.09ms
step:1879/2330 train_time:71576ms step_avg:38.09ms
step:1880/2330 train_time:71617ms step_avg:38.09ms
step:1881/2330 train_time:71652ms step_avg:38.09ms
step:1882/2330 train_time:71694ms step_avg:38.09ms
step:1883/2330 train_time:71729ms step_avg:38.09ms
step:1884/2330 train_time:71770ms step_avg:38.09ms
step:1885/2330 train_time:71807ms step_avg:38.09ms
step:1886/2330 train_time:71848ms step_avg:38.10ms
step:1887/2330 train_time:71884ms step_avg:38.09ms
step:1888/2330 train_time:71924ms step_avg:38.10ms
step:1889/2330 train_time:71960ms step_avg:38.09ms
step:1890/2330 train_time:72001ms step_avg:38.10ms
step:1891/2330 train_time:72037ms step_avg:38.09ms
step:1892/2330 train_time:72077ms step_avg:38.10ms
step:1893/2330 train_time:72113ms step_avg:38.09ms
step:1894/2330 train_time:72154ms step_avg:38.10ms
step:1895/2330 train_time:72190ms step_avg:38.10ms
step:1896/2330 train_time:72232ms step_avg:38.10ms
step:1897/2330 train_time:72267ms step_avg:38.10ms
step:1898/2330 train_time:72308ms step_avg:38.10ms
step:1899/2330 train_time:72342ms step_avg:38.10ms
step:1900/2330 train_time:72383ms step_avg:38.10ms
step:1901/2330 train_time:72418ms step_avg:38.09ms
step:1902/2330 train_time:72459ms step_avg:38.10ms
step:1903/2330 train_time:72494ms step_avg:38.09ms
step:1904/2330 train_time:72535ms step_avg:38.10ms
step:1905/2330 train_time:72570ms step_avg:38.09ms
step:1906/2330 train_time:72611ms step_avg:38.10ms
step:1907/2330 train_time:72646ms step_avg:38.09ms
step:1908/2330 train_time:72687ms step_avg:38.10ms
step:1909/2330 train_time:72723ms step_avg:38.09ms
step:1910/2330 train_time:72764ms step_avg:38.10ms
step:1911/2330 train_time:72800ms step_avg:38.10ms
step:1912/2330 train_time:72841ms step_avg:38.10ms
step:1913/2330 train_time:72876ms step_avg:38.10ms
step:1914/2330 train_time:72917ms step_avg:38.10ms
step:1915/2330 train_time:72953ms step_avg:38.10ms
step:1916/2330 train_time:72994ms step_avg:38.10ms
step:1917/2330 train_time:73029ms step_avg:38.10ms
step:1918/2330 train_time:73070ms step_avg:38.10ms
step:1919/2330 train_time:73105ms step_avg:38.10ms
step:1920/2330 train_time:73146ms step_avg:38.10ms
step:1921/2330 train_time:73182ms step_avg:38.10ms
step:1922/2330 train_time:73223ms step_avg:38.10ms
step:1923/2330 train_time:73258ms step_avg:38.10ms
step:1924/2330 train_time:73298ms step_avg:38.10ms
step:1925/2330 train_time:73333ms step_avg:38.10ms
step:1926/2330 train_time:73374ms step_avg:38.10ms
step:1927/2330 train_time:73410ms step_avg:38.10ms
step:1928/2330 train_time:73451ms step_avg:38.10ms
step:1929/2330 train_time:73486ms step_avg:38.10ms
step:1930/2330 train_time:73527ms step_avg:38.10ms
step:1931/2330 train_time:73563ms step_avg:38.10ms
step:1932/2330 train_time:73603ms step_avg:38.10ms
step:1933/2330 train_time:73639ms step_avg:38.10ms
step:1934/2330 train_time:73679ms step_avg:38.10ms
step:1935/2330 train_time:73715ms step_avg:38.10ms
step:1936/2330 train_time:73756ms step_avg:38.10ms
step:1937/2330 train_time:73792ms step_avg:38.10ms
step:1938/2330 train_time:73833ms step_avg:38.10ms
step:1939/2330 train_time:73868ms step_avg:38.10ms
step:1940/2330 train_time:73909ms step_avg:38.10ms
step:1941/2330 train_time:73944ms step_avg:38.10ms
step:1942/2330 train_time:73985ms step_avg:38.10ms
step:1943/2330 train_time:74019ms step_avg:38.10ms
step:1944/2330 train_time:74060ms step_avg:38.10ms
step:1945/2330 train_time:74097ms step_avg:38.10ms
step:1946/2330 train_time:74138ms step_avg:38.10ms
step:1947/2330 train_time:74173ms step_avg:38.10ms
step:1948/2330 train_time:74214ms step_avg:38.10ms
step:1949/2330 train_time:74250ms step_avg:38.10ms
step:1950/2330 train_time:74291ms step_avg:38.10ms
step:1951/2330 train_time:74326ms step_avg:38.10ms
step:1952/2330 train_time:74367ms step_avg:38.10ms
step:1953/2330 train_time:74402ms step_avg:38.10ms
step:1954/2330 train_time:74443ms step_avg:38.10ms
step:1955/2330 train_time:74479ms step_avg:38.10ms
step:1956/2330 train_time:74520ms step_avg:38.10ms
step:1957/2330 train_time:74555ms step_avg:38.10ms
step:1958/2330 train_time:74596ms step_avg:38.10ms
step:1959/2330 train_time:74631ms step_avg:38.10ms
step:1960/2330 train_time:74672ms step_avg:38.10ms
step:1961/2330 train_time:74708ms step_avg:38.10ms
step:1962/2330 train_time:74749ms step_avg:38.10ms
step:1963/2330 train_time:74784ms step_avg:38.10ms
step:1964/2330 train_time:74825ms step_avg:38.10ms
step:1965/2330 train_time:74859ms step_avg:38.10ms
step:1966/2330 train_time:74900ms step_avg:38.10ms
step:1967/2330 train_time:74935ms step_avg:38.10ms
step:1968/2330 train_time:74976ms step_avg:38.10ms
step:1969/2330 train_time:75012ms step_avg:38.10ms
step:1970/2330 train_time:75053ms step_avg:38.10ms
step:1971/2330 train_time:75089ms step_avg:38.10ms
step:1972/2330 train_time:75130ms step_avg:38.10ms
step:1973/2330 train_time:75165ms step_avg:38.10ms
step:1974/2330 train_time:75206ms step_avg:38.10ms
step:1975/2330 train_time:75241ms step_avg:38.10ms
step:1976/2330 train_time:75281ms step_avg:38.10ms
step:1977/2330 train_time:75317ms step_avg:38.10ms
step:1978/2330 train_time:75358ms step_avg:38.10ms
step:1979/2330 train_time:75392ms step_avg:38.10ms
step:1980/2330 train_time:75434ms step_avg:38.10ms
step:1981/2330 train_time:75469ms step_avg:38.10ms
step:1982/2330 train_time:75510ms step_avg:38.10ms
step:1983/2330 train_time:75545ms step_avg:38.10ms
step:1984/2330 train_time:75586ms step_avg:38.10ms
step:1985/2330 train_time:75621ms step_avg:38.10ms
step:1986/2330 train_time:75661ms step_avg:38.10ms
step:1987/2330 train_time:75697ms step_avg:38.10ms
step:1988/2330 train_time:75738ms step_avg:38.10ms
step:1989/2330 train_time:75773ms step_avg:38.10ms
step:1990/2330 train_time:75814ms step_avg:38.10ms
step:1991/2330 train_time:75850ms step_avg:38.10ms
step:1992/2330 train_time:75891ms step_avg:38.10ms
step:1993/2330 train_time:75926ms step_avg:38.10ms
step:1994/2330 train_time:75968ms step_avg:38.10ms
step:1995/2330 train_time:76003ms step_avg:38.10ms
step:1996/2330 train_time:76044ms step_avg:38.10ms
step:1997/2330 train_time:76080ms step_avg:38.10ms
step:1998/2330 train_time:76121ms step_avg:38.10ms
step:1999/2330 train_time:76157ms step_avg:38.10ms
step:2000/2330 train_time:76197ms step_avg:38.10ms
step:2000/2330 val_loss:5.1879 train_time:76310ms step_avg:38.16ms
step:2001/2330 train_time:76320ms step_avg:38.14ms
step:2002/2330 train_time:76331ms step_avg:38.13ms
step:2003/2330 train_time:76339ms step_avg:38.11ms
step:2004/2330 train_time:76352ms step_avg:38.10ms
step:2005/2330 train_time:76386ms step_avg:38.10ms
step:2006/2330 train_time:76427ms step_avg:38.10ms
step:2007/2330 train_time:76461ms step_avg:38.10ms
step:2008/2330 train_time:76502ms step_avg:38.10ms
step:2009/2330 train_time:76536ms step_avg:38.10ms
step:2010/2330 train_time:76577ms step_avg:38.10ms
step:2011/2330 train_time:76611ms step_avg:38.10ms
step:2012/2330 train_time:76653ms step_avg:38.10ms
step:2013/2330 train_time:76694ms step_avg:38.10ms
step:2014/2330 train_time:76735ms step_avg:38.10ms
step:2015/2330 train_time:76772ms step_avg:38.10ms
step:2016/2330 train_time:76812ms step_avg:38.10ms
step:2017/2330 train_time:76848ms step_avg:38.10ms
step:2018/2330 train_time:76888ms step_avg:38.10ms
step:2019/2330 train_time:76923ms step_avg:38.10ms
step:2020/2330 train_time:76964ms step_avg:38.10ms
step:2021/2330 train_time:76998ms step_avg:38.10ms
step:2022/2330 train_time:77039ms step_avg:38.10ms
step:2023/2330 train_time:77074ms step_avg:38.10ms
step:2024/2330 train_time:77114ms step_avg:38.10ms
step:2025/2330 train_time:77149ms step_avg:38.10ms
step:2026/2330 train_time:77190ms step_avg:38.10ms
step:2027/2330 train_time:77225ms step_avg:38.10ms
step:2028/2330 train_time:77268ms step_avg:38.10ms
step:2029/2330 train_time:77302ms step_avg:38.10ms
step:2030/2330 train_time:77343ms step_avg:38.10ms
step:2031/2330 train_time:77379ms step_avg:38.10ms
step:2032/2330 train_time:77420ms step_avg:38.10ms
step:2033/2330 train_time:77454ms step_avg:38.10ms
step:2034/2330 train_time:77495ms step_avg:38.10ms
step:2035/2330 train_time:77530ms step_avg:38.10ms
step:2036/2330 train_time:77570ms step_avg:38.10ms
step:2037/2330 train_time:77606ms step_avg:38.10ms
step:2038/2330 train_time:77647ms step_avg:38.10ms
step:2039/2330 train_time:77685ms step_avg:38.10ms
step:2040/2330 train_time:77726ms step_avg:38.10ms
step:2041/2330 train_time:77762ms step_avg:38.10ms
step:2042/2330 train_time:77803ms step_avg:38.10ms
step:2043/2330 train_time:77838ms step_avg:38.10ms
step:2044/2330 train_time:77879ms step_avg:38.10ms
step:2045/2330 train_time:77915ms step_avg:38.10ms
step:2046/2330 train_time:77956ms step_avg:38.10ms
step:2047/2330 train_time:77990ms step_avg:38.10ms
step:2048/2330 train_time:78031ms step_avg:38.10ms
step:2049/2330 train_time:78066ms step_avg:38.10ms
step:2050/2330 train_time:78107ms step_avg:38.10ms
step:2051/2330 train_time:78141ms step_avg:38.10ms
step:2052/2330 train_time:78182ms step_avg:38.10ms
step:2053/2330 train_time:78218ms step_avg:38.10ms
step:2054/2330 train_time:78259ms step_avg:38.10ms
step:2055/2330 train_time:78294ms step_avg:38.10ms
step:2056/2330 train_time:78336ms step_avg:38.10ms
step:2057/2330 train_time:78370ms step_avg:38.10ms
step:2058/2330 train_time:78411ms step_avg:38.10ms
step:2059/2330 train_time:78445ms step_avg:38.10ms
step:2060/2330 train_time:78486ms step_avg:38.10ms
step:2061/2330 train_time:78521ms step_avg:38.10ms
step:2062/2330 train_time:78563ms step_avg:38.10ms
step:2063/2330 train_time:78598ms step_avg:38.10ms
step:2064/2330 train_time:78639ms step_avg:38.10ms
step:2065/2330 train_time:78675ms step_avg:38.10ms
step:2066/2330 train_time:78716ms step_avg:38.10ms
step:2067/2330 train_time:78752ms step_avg:38.10ms
step:2068/2330 train_time:78792ms step_avg:38.10ms
step:2069/2330 train_time:78828ms step_avg:38.10ms
step:2070/2330 train_time:78869ms step_avg:38.10ms
step:2071/2330 train_time:78904ms step_avg:38.10ms
step:2072/2330 train_time:78945ms step_avg:38.10ms
step:2073/2330 train_time:78980ms step_avg:38.10ms
step:2074/2330 train_time:79021ms step_avg:38.10ms
step:2075/2330 train_time:79057ms step_avg:38.10ms
step:2076/2330 train_time:79098ms step_avg:38.10ms
step:2077/2330 train_time:79133ms step_avg:38.10ms
step:2078/2330 train_time:79174ms step_avg:38.10ms
step:2079/2330 train_time:79209ms step_avg:38.10ms
step:2080/2330 train_time:79249ms step_avg:38.10ms
step:2081/2330 train_time:79284ms step_avg:38.10ms
step:2082/2330 train_time:79325ms step_avg:38.10ms
step:2083/2330 train_time:79360ms step_avg:38.10ms
step:2084/2330 train_time:79402ms step_avg:38.10ms
step:2085/2330 train_time:79436ms step_avg:38.10ms
step:2086/2330 train_time:79477ms step_avg:38.10ms
step:2087/2330 train_time:79512ms step_avg:38.10ms
step:2088/2330 train_time:79553ms step_avg:38.10ms
step:2089/2330 train_time:79588ms step_avg:38.10ms
step:2090/2330 train_time:79629ms step_avg:38.10ms
step:2091/2330 train_time:79665ms step_avg:38.10ms
step:2092/2330 train_time:79706ms step_avg:38.10ms
step:2093/2330 train_time:79741ms step_avg:38.10ms
step:2094/2330 train_time:79782ms step_avg:38.10ms
step:2095/2330 train_time:79818ms step_avg:38.10ms
step:2096/2330 train_time:79859ms step_avg:38.10ms
step:2097/2330 train_time:79894ms step_avg:38.10ms
step:2098/2330 train_time:79935ms step_avg:38.10ms
step:2099/2330 train_time:79971ms step_avg:38.10ms
step:2100/2330 train_time:80012ms step_avg:38.10ms
step:2101/2330 train_time:80047ms step_avg:38.10ms
step:2102/2330 train_time:80088ms step_avg:38.10ms
step:2103/2330 train_time:80123ms step_avg:38.10ms
step:2104/2330 train_time:80164ms step_avg:38.10ms
step:2105/2330 train_time:80199ms step_avg:38.10ms
step:2106/2330 train_time:80239ms step_avg:38.10ms
step:2107/2330 train_time:80276ms step_avg:38.10ms
step:2108/2330 train_time:80317ms step_avg:38.10ms
step:2109/2330 train_time:80352ms step_avg:38.10ms
step:2110/2330 train_time:80393ms step_avg:38.10ms
step:2111/2330 train_time:80427ms step_avg:38.10ms
step:2112/2330 train_time:80468ms step_avg:38.10ms
step:2113/2330 train_time:80503ms step_avg:38.10ms
step:2114/2330 train_time:80545ms step_avg:38.10ms
step:2115/2330 train_time:80580ms step_avg:38.10ms
step:2116/2330 train_time:80621ms step_avg:38.10ms
step:2117/2330 train_time:80656ms step_avg:38.10ms
step:2118/2330 train_time:80697ms step_avg:38.10ms
step:2119/2330 train_time:80733ms step_avg:38.10ms
step:2120/2330 train_time:80774ms step_avg:38.10ms
step:2121/2330 train_time:80810ms step_avg:38.10ms
step:2122/2330 train_time:80850ms step_avg:38.10ms
step:2123/2330 train_time:80886ms step_avg:38.10ms
step:2124/2330 train_time:80927ms step_avg:38.10ms
step:2125/2330 train_time:80962ms step_avg:38.10ms
step:2126/2330 train_time:81004ms step_avg:38.10ms
step:2127/2330 train_time:81038ms step_avg:38.10ms
step:2128/2330 train_time:81080ms step_avg:38.10ms
step:2129/2330 train_time:81114ms step_avg:38.10ms
step:2130/2330 train_time:81155ms step_avg:38.10ms
step:2131/2330 train_time:81190ms step_avg:38.10ms
step:2132/2330 train_time:81231ms step_avg:38.10ms
step:2133/2330 train_time:81266ms step_avg:38.10ms
step:2134/2330 train_time:81307ms step_avg:38.10ms
step:2135/2330 train_time:81342ms step_avg:38.10ms
step:2136/2330 train_time:81383ms step_avg:38.10ms
step:2137/2330 train_time:81418ms step_avg:38.10ms
step:2138/2330 train_time:81459ms step_avg:38.10ms
step:2139/2330 train_time:81495ms step_avg:38.10ms
step:2140/2330 train_time:81536ms step_avg:38.10ms
step:2141/2330 train_time:81572ms step_avg:38.10ms
step:2142/2330 train_time:81612ms step_avg:38.10ms
step:2143/2330 train_time:81647ms step_avg:38.10ms
step:2144/2330 train_time:81688ms step_avg:38.10ms
step:2145/2330 train_time:81723ms step_avg:38.10ms
step:2146/2330 train_time:81765ms step_avg:38.10ms
step:2147/2330 train_time:81800ms step_avg:38.10ms
step:2148/2330 train_time:81841ms step_avg:38.10ms
step:2149/2330 train_time:81877ms step_avg:38.10ms
step:2150/2330 train_time:81918ms step_avg:38.10ms
step:2151/2330 train_time:81954ms step_avg:38.10ms
step:2152/2330 train_time:81995ms step_avg:38.10ms
step:2153/2330 train_time:82030ms step_avg:38.10ms
step:2154/2330 train_time:82071ms step_avg:38.10ms
step:2155/2330 train_time:82106ms step_avg:38.10ms
step:2156/2330 train_time:82147ms step_avg:38.10ms
step:2157/2330 train_time:82182ms step_avg:38.10ms
step:2158/2330 train_time:82223ms step_avg:38.10ms
step:2159/2330 train_time:82258ms step_avg:38.10ms
step:2160/2330 train_time:82299ms step_avg:38.10ms
step:2161/2330 train_time:82335ms step_avg:38.10ms
step:2162/2330 train_time:82376ms step_avg:38.10ms
step:2163/2330 train_time:82410ms step_avg:38.10ms
step:2164/2330 train_time:82451ms step_avg:38.10ms
step:2165/2330 train_time:82486ms step_avg:38.10ms
step:2166/2330 train_time:82527ms step_avg:38.10ms
step:2167/2330 train_time:82562ms step_avg:38.10ms
step:2168/2330 train_time:82603ms step_avg:38.10ms
step:2169/2330 train_time:82638ms step_avg:38.10ms
step:2170/2330 train_time:82679ms step_avg:38.10ms
step:2171/2330 train_time:82714ms step_avg:38.10ms
step:2172/2330 train_time:82755ms step_avg:38.10ms
step:2173/2330 train_time:82790ms step_avg:38.10ms
step:2174/2330 train_time:82831ms step_avg:38.10ms
step:2175/2330 train_time:82866ms step_avg:38.10ms
step:2176/2330 train_time:82907ms step_avg:38.10ms
step:2177/2330 train_time:82942ms step_avg:38.10ms
step:2178/2330 train_time:82983ms step_avg:38.10ms
step:2179/2330 train_time:83020ms step_avg:38.10ms
step:2180/2330 train_time:83061ms step_avg:38.10ms
step:2181/2330 train_time:83096ms step_avg:38.10ms
step:2182/2330 train_time:83137ms step_avg:38.10ms
step:2183/2330 train_time:83172ms step_avg:38.10ms
step:2184/2330 train_time:83212ms step_avg:38.10ms
step:2185/2330 train_time:83249ms step_avg:38.10ms
step:2186/2330 train_time:83289ms step_avg:38.10ms
step:2187/2330 train_time:83325ms step_avg:38.10ms
step:2188/2330 train_time:83366ms step_avg:38.10ms
step:2189/2330 train_time:83401ms step_avg:38.10ms
step:2190/2330 train_time:83442ms step_avg:38.10ms
step:2191/2330 train_time:83477ms step_avg:38.10ms
step:2192/2330 train_time:83518ms step_avg:38.10ms
step:2193/2330 train_time:83553ms step_avg:38.10ms
step:2194/2330 train_time:83594ms step_avg:38.10ms
step:2195/2330 train_time:83629ms step_avg:38.10ms
step:2196/2330 train_time:83670ms step_avg:38.10ms
step:2197/2330 train_time:83705ms step_avg:38.10ms
step:2198/2330 train_time:83746ms step_avg:38.10ms
step:2199/2330 train_time:83781ms step_avg:38.10ms
step:2200/2330 train_time:83822ms step_avg:38.10ms
step:2201/2330 train_time:83858ms step_avg:38.10ms
step:2202/2330 train_time:83898ms step_avg:38.10ms
step:2203/2330 train_time:83934ms step_avg:38.10ms
step:2204/2330 train_time:83975ms step_avg:38.10ms
step:2205/2330 train_time:84010ms step_avg:38.10ms
step:2206/2330 train_time:84051ms step_avg:38.10ms
step:2207/2330 train_time:84086ms step_avg:38.10ms
step:2208/2330 train_time:84127ms step_avg:38.10ms
step:2209/2330 train_time:84162ms step_avg:38.10ms
step:2210/2330 train_time:84204ms step_avg:38.10ms
step:2211/2330 train_time:84239ms step_avg:38.10ms
step:2212/2330 train_time:84280ms step_avg:38.10ms
step:2213/2330 train_time:84317ms step_avg:38.10ms
step:2214/2330 train_time:84357ms step_avg:38.10ms
step:2215/2330 train_time:84393ms step_avg:38.10ms
step:2216/2330 train_time:84433ms step_avg:38.10ms
step:2217/2330 train_time:84470ms step_avg:38.10ms
step:2218/2330 train_time:84510ms step_avg:38.10ms
step:2219/2330 train_time:84546ms step_avg:38.10ms
step:2220/2330 train_time:84587ms step_avg:38.10ms
step:2221/2330 train_time:84622ms step_avg:38.10ms
step:2222/2330 train_time:84663ms step_avg:38.10ms
step:2223/2330 train_time:84698ms step_avg:38.10ms
step:2224/2330 train_time:84738ms step_avg:38.10ms
step:2225/2330 train_time:84776ms step_avg:38.10ms
step:2226/2330 train_time:84816ms step_avg:38.10ms
step:2227/2330 train_time:84852ms step_avg:38.10ms
step:2228/2330 train_time:84893ms step_avg:38.10ms
step:2229/2330 train_time:84929ms step_avg:38.10ms
step:2230/2330 train_time:84970ms step_avg:38.10ms
step:2231/2330 train_time:85005ms step_avg:38.10ms
step:2232/2330 train_time:85046ms step_avg:38.10ms
step:2233/2330 train_time:85081ms step_avg:38.10ms
step:2234/2330 train_time:85122ms step_avg:38.10ms
step:2235/2330 train_time:85157ms step_avg:38.10ms
step:2236/2330 train_time:85198ms step_avg:38.10ms
step:2237/2330 train_time:85233ms step_avg:38.10ms
step:2238/2330 train_time:85274ms step_avg:38.10ms
step:2239/2330 train_time:85308ms step_avg:38.10ms
step:2240/2330 train_time:85349ms step_avg:38.10ms
step:2241/2330 train_time:85384ms step_avg:38.10ms
step:2242/2330 train_time:85426ms step_avg:38.10ms
step:2243/2330 train_time:85460ms step_avg:38.10ms
step:2244/2330 train_time:85502ms step_avg:38.10ms
step:2245/2330 train_time:85536ms step_avg:38.10ms
step:2246/2330 train_time:85577ms step_avg:38.10ms
step:2247/2330 train_time:85612ms step_avg:38.10ms
step:2248/2330 train_time:85653ms step_avg:38.10ms
step:2249/2330 train_time:85689ms step_avg:38.10ms
step:2250/2330 train_time:85730ms step_avg:38.10ms
step:2250/2330 val_loss:5.1612 train_time:85842ms step_avg:38.15ms
step:2251/2330 train_time:85853ms step_avg:38.14ms
step:2252/2330 train_time:85864ms step_avg:38.13ms
step:2253/2330 train_time:85873ms step_avg:38.12ms
step:2254/2330 train_time:85884ms step_avg:38.10ms
step:2255/2330 train_time:85919ms step_avg:38.10ms
step:2256/2330 train_time:85960ms step_avg:38.10ms
step:2257/2330 train_time:85995ms step_avg:38.10ms
step:2258/2330 train_time:86035ms step_avg:38.10ms
step:2259/2330 train_time:86070ms step_avg:38.10ms
step:2260/2330 train_time:86110ms step_avg:38.10ms
step:2261/2330 train_time:86145ms step_avg:38.10ms
step:2262/2330 train_time:86186ms step_avg:38.10ms
step:2263/2330 train_time:86226ms step_avg:38.10ms
step:2264/2330 train_time:86267ms step_avg:38.10ms
step:2265/2330 train_time:86303ms step_avg:38.10ms
step:2266/2330 train_time:86344ms step_avg:38.10ms
step:2267/2330 train_time:86379ms step_avg:38.10ms
step:2268/2330 train_time:86420ms step_avg:38.10ms
step:2269/2330 train_time:86455ms step_avg:38.10ms
step:2270/2330 train_time:86496ms step_avg:38.10ms
step:2271/2330 train_time:86530ms step_avg:38.10ms
step:2272/2330 train_time:86571ms step_avg:38.10ms
step:2273/2330 train_time:86605ms step_avg:38.10ms
step:2274/2330 train_time:86646ms step_avg:38.10ms
step:2275/2330 train_time:86680ms step_avg:38.10ms
step:2276/2330 train_time:86721ms step_avg:38.10ms
step:2277/2330 train_time:86757ms step_avg:38.10ms
step:2278/2330 train_time:86798ms step_avg:38.10ms
step:2279/2330 train_time:86836ms step_avg:38.10ms
step:2280/2330 train_time:86877ms step_avg:38.10ms
step:2281/2330 train_time:86912ms step_avg:38.10ms
step:2282/2330 train_time:86953ms step_avg:38.10ms
step:2283/2330 train_time:86988ms step_avg:38.10ms
step:2284/2330 train_time:87028ms step_avg:38.10ms
step:2285/2330 train_time:87063ms step_avg:38.10ms
step:2286/2330 train_time:87104ms step_avg:38.10ms
step:2287/2330 train_time:87140ms step_avg:38.10ms
step:2288/2330 train_time:87182ms step_avg:38.10ms
step:2289/2330 train_time:87217ms step_avg:38.10ms
step:2290/2330 train_time:87259ms step_avg:38.10ms
step:2291/2330 train_time:87295ms step_avg:38.10ms
step:2292/2330 train_time:87336ms step_avg:38.10ms
step:2293/2330 train_time:87371ms step_avg:38.10ms
step:2294/2330 train_time:87412ms step_avg:38.10ms
step:2295/2330 train_time:87447ms step_avg:38.10ms
step:2296/2330 train_time:87487ms step_avg:38.10ms
step:2297/2330 train_time:87522ms step_avg:38.10ms
step:2298/2330 train_time:87563ms step_avg:38.10ms
step:2299/2330 train_time:87599ms step_avg:38.10ms
step:2300/2330 train_time:87640ms step_avg:38.10ms
step:2301/2330 train_time:87674ms step_avg:38.10ms
step:2302/2330 train_time:87715ms step_avg:38.10ms
step:2303/2330 train_time:87750ms step_avg:38.10ms
step:2304/2330 train_time:87791ms step_avg:38.10ms
step:2305/2330 train_time:87826ms step_avg:38.10ms
step:2306/2330 train_time:87867ms step_avg:38.10ms
step:2307/2330 train_time:87901ms step_avg:38.10ms
step:2308/2330 train_time:87943ms step_avg:38.10ms
step:2309/2330 train_time:87977ms step_avg:38.10ms
step:2310/2330 train_time:88019ms step_avg:38.10ms
step:2311/2330 train_time:88053ms step_avg:38.10ms
step:2312/2330 train_time:88094ms step_avg:38.10ms
step:2313/2330 train_time:88131ms step_avg:38.10ms
step:2314/2330 train_time:88172ms step_avg:38.10ms
step:2315/2330 train_time:88207ms step_avg:38.10ms
step:2316/2330 train_time:88248ms step_avg:38.10ms
step:2317/2330 train_time:88283ms step_avg:38.10ms
step:2318/2330 train_time:88324ms step_avg:38.10ms
step:2319/2330 train_time:88359ms step_avg:38.10ms
step:2320/2330 train_time:88401ms step_avg:38.10ms
step:2321/2330 train_time:88436ms step_avg:38.10ms
step:2322/2330 train_time:88478ms step_avg:38.10ms
step:2323/2330 train_time:88513ms step_avg:38.10ms
step:2324/2330 train_time:88554ms step_avg:38.10ms
step:2325/2330 train_time:88589ms step_avg:38.10ms
step:2326/2330 train_time:88630ms step_avg:38.10ms
step:2327/2330 train_time:88664ms step_avg:38.10ms
step:2328/2330 train_time:88705ms step_avg:38.10ms
step:2329/2330 train_time:88740ms step_avg:38.10ms
step:2330/2330 train_time:88782ms step_avg:38.10ms
step:2330/2330 val_loss:5.1544 train_time:88894ms step_avg:38.15ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
