import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_5e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 02:51:37 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:83.41ms
step:2/2330 train_time:176ms step_avg:87.79ms
step:3/2330 train_time:196ms step_avg:65.50ms
step:4/2330 train_time:225ms step_avg:56.28ms
step:5/2330 train_time:282ms step_avg:56.31ms
step:6/2330 train_time:341ms step_avg:56.88ms
step:7/2330 train_time:399ms step_avg:56.96ms
step:8/2330 train_time:460ms step_avg:57.46ms
step:9/2330 train_time:518ms step_avg:57.50ms
step:10/2330 train_time:578ms step_avg:57.83ms
step:11/2330 train_time:636ms step_avg:57.86ms
step:12/2330 train_time:698ms step_avg:58.16ms
step:13/2330 train_time:755ms step_avg:58.10ms
step:14/2330 train_time:817ms step_avg:58.35ms
step:15/2330 train_time:875ms step_avg:58.31ms
step:16/2330 train_time:935ms step_avg:58.45ms
step:17/2330 train_time:993ms step_avg:58.42ms
step:18/2330 train_time:1055ms step_avg:58.63ms
step:19/2330 train_time:1119ms step_avg:58.89ms
step:20/2330 train_time:1181ms step_avg:59.06ms
step:21/2330 train_time:1242ms step_avg:59.12ms
step:22/2330 train_time:1304ms step_avg:59.28ms
step:23/2330 train_time:1363ms step_avg:59.28ms
step:24/2330 train_time:1425ms step_avg:59.38ms
step:25/2330 train_time:1483ms step_avg:59.33ms
step:26/2330 train_time:1545ms step_avg:59.41ms
step:27/2330 train_time:1603ms step_avg:59.38ms
step:28/2330 train_time:1666ms step_avg:59.49ms
step:29/2330 train_time:1724ms step_avg:59.46ms
step:30/2330 train_time:1786ms step_avg:59.53ms
step:31/2330 train_time:1844ms step_avg:59.49ms
step:32/2330 train_time:1906ms step_avg:59.56ms
step:33/2330 train_time:1965ms step_avg:59.53ms
step:34/2330 train_time:2027ms step_avg:59.61ms
step:35/2330 train_time:2086ms step_avg:59.60ms
step:36/2330 train_time:2149ms step_avg:59.71ms
step:37/2330 train_time:2210ms step_avg:59.72ms
step:38/2330 train_time:2272ms step_avg:59.80ms
step:39/2330 train_time:2332ms step_avg:59.78ms
step:40/2330 train_time:2393ms step_avg:59.84ms
step:41/2330 train_time:2452ms step_avg:59.81ms
step:42/2330 train_time:2514ms step_avg:59.86ms
step:43/2330 train_time:2573ms step_avg:59.83ms
step:44/2330 train_time:2635ms step_avg:59.88ms
step:45/2330 train_time:2694ms step_avg:59.87ms
step:46/2330 train_time:2756ms step_avg:59.91ms
step:47/2330 train_time:2815ms step_avg:59.89ms
step:48/2330 train_time:2876ms step_avg:59.92ms
step:49/2330 train_time:2936ms step_avg:59.92ms
step:50/2330 train_time:2998ms step_avg:59.96ms
step:51/2330 train_time:3057ms step_avg:59.94ms
step:52/2330 train_time:3119ms step_avg:59.98ms
step:53/2330 train_time:3179ms step_avg:59.98ms
step:54/2330 train_time:3242ms step_avg:60.03ms
step:55/2330 train_time:3300ms step_avg:60.00ms
step:56/2330 train_time:3362ms step_avg:60.03ms
step:57/2330 train_time:3421ms step_avg:60.01ms
step:58/2330 train_time:3482ms step_avg:60.04ms
step:59/2330 train_time:3541ms step_avg:60.01ms
step:60/2330 train_time:3602ms step_avg:60.04ms
step:61/2330 train_time:3661ms step_avg:60.01ms
step:62/2330 train_time:3723ms step_avg:60.04ms
step:63/2330 train_time:3781ms step_avg:60.02ms
step:64/2330 train_time:3842ms step_avg:60.04ms
step:65/2330 train_time:3901ms step_avg:60.02ms
step:66/2330 train_time:3964ms step_avg:60.06ms
step:67/2330 train_time:4023ms step_avg:60.04ms
step:68/2330 train_time:4085ms step_avg:60.08ms
step:69/2330 train_time:4144ms step_avg:60.06ms
step:70/2330 train_time:4207ms step_avg:60.10ms
step:71/2330 train_time:4266ms step_avg:60.09ms
step:72/2330 train_time:4329ms step_avg:60.13ms
step:73/2330 train_time:4388ms step_avg:60.11ms
step:74/2330 train_time:4451ms step_avg:60.15ms
step:75/2330 train_time:4511ms step_avg:60.15ms
step:76/2330 train_time:4573ms step_avg:60.17ms
step:77/2330 train_time:4631ms step_avg:60.15ms
step:78/2330 train_time:4694ms step_avg:60.17ms
step:79/2330 train_time:4753ms step_avg:60.16ms
step:80/2330 train_time:4816ms step_avg:60.20ms
step:81/2330 train_time:4875ms step_avg:60.18ms
step:82/2330 train_time:4937ms step_avg:60.20ms
step:83/2330 train_time:4996ms step_avg:60.19ms
step:84/2330 train_time:5059ms step_avg:60.22ms
step:85/2330 train_time:5118ms step_avg:60.22ms
step:86/2330 train_time:5180ms step_avg:60.24ms
step:87/2330 train_time:5240ms step_avg:60.23ms
step:88/2330 train_time:5302ms step_avg:60.25ms
step:89/2330 train_time:5361ms step_avg:60.24ms
step:90/2330 train_time:5424ms step_avg:60.27ms
step:91/2330 train_time:5484ms step_avg:60.26ms
step:92/2330 train_time:5546ms step_avg:60.28ms
step:93/2330 train_time:5604ms step_avg:60.26ms
step:94/2330 train_time:5666ms step_avg:60.28ms
step:95/2330 train_time:5725ms step_avg:60.26ms
step:96/2330 train_time:5787ms step_avg:60.28ms
step:97/2330 train_time:5846ms step_avg:60.27ms
step:98/2330 train_time:5909ms step_avg:60.30ms
step:99/2330 train_time:5969ms step_avg:60.29ms
step:100/2330 train_time:6031ms step_avg:60.31ms
step:101/2330 train_time:6091ms step_avg:60.31ms
step:102/2330 train_time:6153ms step_avg:60.33ms
step:103/2330 train_time:6213ms step_avg:60.32ms
step:104/2330 train_time:6275ms step_avg:60.34ms
step:105/2330 train_time:6334ms step_avg:60.32ms
step:106/2330 train_time:6396ms step_avg:60.34ms
step:107/2330 train_time:6455ms step_avg:60.33ms
step:108/2330 train_time:6518ms step_avg:60.35ms
step:109/2330 train_time:6577ms step_avg:60.34ms
step:110/2330 train_time:6639ms step_avg:60.35ms
step:111/2330 train_time:6699ms step_avg:60.35ms
step:112/2330 train_time:6760ms step_avg:60.36ms
step:113/2330 train_time:6820ms step_avg:60.35ms
step:114/2330 train_time:6881ms step_avg:60.36ms
step:115/2330 train_time:6941ms step_avg:60.35ms
step:116/2330 train_time:7003ms step_avg:60.37ms
step:117/2330 train_time:7062ms step_avg:60.36ms
step:118/2330 train_time:7123ms step_avg:60.37ms
step:119/2330 train_time:7182ms step_avg:60.36ms
step:120/2330 train_time:7244ms step_avg:60.37ms
step:121/2330 train_time:7303ms step_avg:60.35ms
step:122/2330 train_time:7365ms step_avg:60.37ms
step:123/2330 train_time:7424ms step_avg:60.36ms
step:124/2330 train_time:7486ms step_avg:60.37ms
step:125/2330 train_time:7546ms step_avg:60.36ms
step:126/2330 train_time:7608ms step_avg:60.38ms
step:127/2330 train_time:7668ms step_avg:60.38ms
step:128/2330 train_time:7730ms step_avg:60.39ms
step:129/2330 train_time:7790ms step_avg:60.39ms
step:130/2330 train_time:7853ms step_avg:60.41ms
step:131/2330 train_time:7913ms step_avg:60.41ms
step:132/2330 train_time:7975ms step_avg:60.42ms
step:133/2330 train_time:8033ms step_avg:60.40ms
step:134/2330 train_time:8095ms step_avg:60.41ms
step:135/2330 train_time:8154ms step_avg:60.40ms
step:136/2330 train_time:8217ms step_avg:60.42ms
step:137/2330 train_time:8276ms step_avg:60.41ms
step:138/2330 train_time:8338ms step_avg:60.42ms
step:139/2330 train_time:8397ms step_avg:60.41ms
step:140/2330 train_time:8460ms step_avg:60.43ms
step:141/2330 train_time:8519ms step_avg:60.42ms
step:142/2330 train_time:8582ms step_avg:60.43ms
step:143/2330 train_time:8640ms step_avg:60.42ms
step:144/2330 train_time:8703ms step_avg:60.44ms
step:145/2330 train_time:8762ms step_avg:60.43ms
step:146/2330 train_time:8825ms step_avg:60.45ms
step:147/2330 train_time:8885ms step_avg:60.45ms
step:148/2330 train_time:8948ms step_avg:60.46ms
step:149/2330 train_time:9007ms step_avg:60.45ms
step:150/2330 train_time:9069ms step_avg:60.46ms
step:151/2330 train_time:9129ms step_avg:60.45ms
step:152/2330 train_time:9191ms step_avg:60.46ms
step:153/2330 train_time:9250ms step_avg:60.46ms
step:154/2330 train_time:9313ms step_avg:60.48ms
step:155/2330 train_time:9373ms step_avg:60.47ms
step:156/2330 train_time:9435ms step_avg:60.48ms
step:157/2330 train_time:9494ms step_avg:60.47ms
step:158/2330 train_time:9557ms step_avg:60.48ms
step:159/2330 train_time:9616ms step_avg:60.48ms
step:160/2330 train_time:9678ms step_avg:60.49ms
step:161/2330 train_time:9737ms step_avg:60.48ms
step:162/2330 train_time:9799ms step_avg:60.49ms
step:163/2330 train_time:9859ms step_avg:60.48ms
step:164/2330 train_time:9922ms step_avg:60.50ms
step:165/2330 train_time:9982ms step_avg:60.49ms
step:166/2330 train_time:10044ms step_avg:60.51ms
step:167/2330 train_time:10104ms step_avg:60.50ms
step:168/2330 train_time:10167ms step_avg:60.52ms
step:169/2330 train_time:10225ms step_avg:60.50ms
step:170/2330 train_time:10288ms step_avg:60.52ms
step:171/2330 train_time:10348ms step_avg:60.51ms
step:172/2330 train_time:10412ms step_avg:60.53ms
step:173/2330 train_time:10472ms step_avg:60.53ms
step:174/2330 train_time:10533ms step_avg:60.54ms
step:175/2330 train_time:10592ms step_avg:60.53ms
step:176/2330 train_time:10655ms step_avg:60.54ms
step:177/2330 train_time:10716ms step_avg:60.54ms
step:178/2330 train_time:10777ms step_avg:60.55ms
step:179/2330 train_time:10837ms step_avg:60.54ms
step:180/2330 train_time:10900ms step_avg:60.55ms
step:181/2330 train_time:10959ms step_avg:60.55ms
step:182/2330 train_time:11022ms step_avg:60.56ms
step:183/2330 train_time:11081ms step_avg:60.55ms
step:184/2330 train_time:11144ms step_avg:60.56ms
step:185/2330 train_time:11202ms step_avg:60.55ms
step:186/2330 train_time:11265ms step_avg:60.57ms
step:187/2330 train_time:11325ms step_avg:60.56ms
step:188/2330 train_time:11386ms step_avg:60.57ms
step:189/2330 train_time:11445ms step_avg:60.56ms
step:190/2330 train_time:11507ms step_avg:60.56ms
step:191/2330 train_time:11567ms step_avg:60.56ms
step:192/2330 train_time:11630ms step_avg:60.57ms
step:193/2330 train_time:11688ms step_avg:60.56ms
step:194/2330 train_time:11751ms step_avg:60.57ms
step:195/2330 train_time:11812ms step_avg:60.57ms
step:196/2330 train_time:11875ms step_avg:60.59ms
step:197/2330 train_time:11934ms step_avg:60.58ms
step:198/2330 train_time:11997ms step_avg:60.59ms
step:199/2330 train_time:12056ms step_avg:60.58ms
step:200/2330 train_time:12119ms step_avg:60.60ms
step:201/2330 train_time:12178ms step_avg:60.59ms
step:202/2330 train_time:12241ms step_avg:60.60ms
step:203/2330 train_time:12299ms step_avg:60.59ms
step:204/2330 train_time:12362ms step_avg:60.60ms
step:205/2330 train_time:12422ms step_avg:60.59ms
step:206/2330 train_time:12483ms step_avg:60.60ms
step:207/2330 train_time:12542ms step_avg:60.59ms
step:208/2330 train_time:12605ms step_avg:60.60ms
step:209/2330 train_time:12664ms step_avg:60.59ms
step:210/2330 train_time:12727ms step_avg:60.61ms
step:211/2330 train_time:12786ms step_avg:60.60ms
step:212/2330 train_time:12849ms step_avg:60.61ms
step:213/2330 train_time:12909ms step_avg:60.60ms
step:214/2330 train_time:12971ms step_avg:60.61ms
step:215/2330 train_time:13032ms step_avg:60.61ms
step:216/2330 train_time:13094ms step_avg:60.62ms
step:217/2330 train_time:13153ms step_avg:60.61ms
step:218/2330 train_time:13215ms step_avg:60.62ms
step:219/2330 train_time:13275ms step_avg:60.62ms
step:220/2330 train_time:13337ms step_avg:60.62ms
step:221/2330 train_time:13396ms step_avg:60.61ms
step:222/2330 train_time:13458ms step_avg:60.62ms
step:223/2330 train_time:13518ms step_avg:60.62ms
step:224/2330 train_time:13580ms step_avg:60.63ms
step:225/2330 train_time:13640ms step_avg:60.62ms
step:226/2330 train_time:13702ms step_avg:60.63ms
step:227/2330 train_time:13762ms step_avg:60.63ms
step:228/2330 train_time:13826ms step_avg:60.64ms
step:229/2330 train_time:13885ms step_avg:60.63ms
step:230/2330 train_time:13947ms step_avg:60.64ms
step:231/2330 train_time:14006ms step_avg:60.63ms
step:232/2330 train_time:14069ms step_avg:60.64ms
step:233/2330 train_time:14128ms step_avg:60.64ms
step:234/2330 train_time:14191ms step_avg:60.64ms
step:235/2330 train_time:14250ms step_avg:60.64ms
step:236/2330 train_time:14313ms step_avg:60.65ms
step:237/2330 train_time:14372ms step_avg:60.64ms
step:238/2330 train_time:14435ms step_avg:60.65ms
step:239/2330 train_time:14495ms step_avg:60.65ms
step:240/2330 train_time:14557ms step_avg:60.66ms
step:241/2330 train_time:14617ms step_avg:60.65ms
step:242/2330 train_time:14680ms step_avg:60.66ms
step:243/2330 train_time:14739ms step_avg:60.65ms
step:244/2330 train_time:14802ms step_avg:60.66ms
step:245/2330 train_time:14860ms step_avg:60.65ms
step:246/2330 train_time:14924ms step_avg:60.66ms
step:247/2330 train_time:14984ms step_avg:60.66ms
step:248/2330 train_time:15046ms step_avg:60.67ms
step:249/2330 train_time:15105ms step_avg:60.66ms
step:250/2330 train_time:15166ms step_avg:60.67ms
step:250/2330 val_loss:5.6610 train_time:15239ms step_avg:60.95ms
step:251/2330 train_time:15260ms step_avg:60.80ms
step:252/2330 train_time:15289ms step_avg:60.67ms
step:253/2330 train_time:15352ms step_avg:60.68ms
step:254/2330 train_time:15417ms step_avg:60.70ms
step:255/2330 train_time:15478ms step_avg:60.70ms
step:256/2330 train_time:15543ms step_avg:60.71ms
step:257/2330 train_time:15604ms step_avg:60.71ms
step:258/2330 train_time:15667ms step_avg:60.72ms
step:259/2330 train_time:15727ms step_avg:60.72ms
step:260/2330 train_time:15789ms step_avg:60.73ms
step:261/2330 train_time:15848ms step_avg:60.72ms
step:262/2330 train_time:15908ms step_avg:60.72ms
step:263/2330 train_time:15967ms step_avg:60.71ms
step:264/2330 train_time:16029ms step_avg:60.72ms
step:265/2330 train_time:16088ms step_avg:60.71ms
step:266/2330 train_time:16150ms step_avg:60.71ms
step:267/2330 train_time:16209ms step_avg:60.71ms
step:268/2330 train_time:16271ms step_avg:60.71ms
step:269/2330 train_time:16333ms step_avg:60.72ms
step:270/2330 train_time:16397ms step_avg:60.73ms
step:271/2330 train_time:16457ms step_avg:60.73ms
step:272/2330 train_time:16519ms step_avg:60.73ms
step:273/2330 train_time:16579ms step_avg:60.73ms
step:274/2330 train_time:16642ms step_avg:60.74ms
step:275/2330 train_time:16701ms step_avg:60.73ms
step:276/2330 train_time:16763ms step_avg:60.74ms
step:277/2330 train_time:16822ms step_avg:60.73ms
step:278/2330 train_time:16884ms step_avg:60.73ms
step:279/2330 train_time:16943ms step_avg:60.73ms
step:280/2330 train_time:17005ms step_avg:60.73ms
step:281/2330 train_time:17064ms step_avg:60.73ms
step:282/2330 train_time:17125ms step_avg:60.73ms
step:283/2330 train_time:17184ms step_avg:60.72ms
step:284/2330 train_time:17246ms step_avg:60.73ms
step:285/2330 train_time:17306ms step_avg:60.72ms
step:286/2330 train_time:17369ms step_avg:60.73ms
step:287/2330 train_time:17430ms step_avg:60.73ms
step:288/2330 train_time:17493ms step_avg:60.74ms
step:289/2330 train_time:17553ms step_avg:60.74ms
step:290/2330 train_time:17616ms step_avg:60.74ms
step:291/2330 train_time:17675ms step_avg:60.74ms
step:292/2330 train_time:17737ms step_avg:60.74ms
step:293/2330 train_time:17796ms step_avg:60.74ms
step:294/2330 train_time:17859ms step_avg:60.74ms
step:295/2330 train_time:17918ms step_avg:60.74ms
step:296/2330 train_time:17981ms step_avg:60.74ms
step:297/2330 train_time:18040ms step_avg:60.74ms
step:298/2330 train_time:18103ms step_avg:60.75ms
step:299/2330 train_time:18162ms step_avg:60.74ms
step:300/2330 train_time:18224ms step_avg:60.75ms
step:301/2330 train_time:18284ms step_avg:60.74ms
step:302/2330 train_time:18347ms step_avg:60.75ms
step:303/2330 train_time:18407ms step_avg:60.75ms
step:304/2330 train_time:18470ms step_avg:60.76ms
step:305/2330 train_time:18530ms step_avg:60.75ms
step:306/2330 train_time:18594ms step_avg:60.76ms
step:307/2330 train_time:18653ms step_avg:60.76ms
step:308/2330 train_time:18715ms step_avg:60.76ms
step:309/2330 train_time:18775ms step_avg:60.76ms
step:310/2330 train_time:18837ms step_avg:60.76ms
step:311/2330 train_time:18897ms step_avg:60.76ms
step:312/2330 train_time:18959ms step_avg:60.77ms
step:313/2330 train_time:19018ms step_avg:60.76ms
step:314/2330 train_time:19080ms step_avg:60.77ms
step:315/2330 train_time:19139ms step_avg:60.76ms
step:316/2330 train_time:19202ms step_avg:60.77ms
step:317/2330 train_time:19262ms step_avg:60.76ms
step:318/2330 train_time:19324ms step_avg:60.77ms
step:319/2330 train_time:19384ms step_avg:60.76ms
step:320/2330 train_time:19447ms step_avg:60.77ms
step:321/2330 train_time:19507ms step_avg:60.77ms
step:322/2330 train_time:19570ms step_avg:60.78ms
step:323/2330 train_time:19629ms step_avg:60.77ms
step:324/2330 train_time:19693ms step_avg:60.78ms
step:325/2330 train_time:19751ms step_avg:60.77ms
step:326/2330 train_time:19814ms step_avg:60.78ms
step:327/2330 train_time:19874ms step_avg:60.78ms
step:328/2330 train_time:19936ms step_avg:60.78ms
step:329/2330 train_time:19996ms step_avg:60.78ms
step:330/2330 train_time:20057ms step_avg:60.78ms
step:331/2330 train_time:20118ms step_avg:60.78ms
step:332/2330 train_time:20179ms step_avg:60.78ms
step:333/2330 train_time:20239ms step_avg:60.78ms
step:334/2330 train_time:20301ms step_avg:60.78ms
step:335/2330 train_time:20360ms step_avg:60.78ms
step:336/2330 train_time:20424ms step_avg:60.79ms
step:337/2330 train_time:20483ms step_avg:60.78ms
step:338/2330 train_time:20547ms step_avg:60.79ms
step:339/2330 train_time:20607ms step_avg:60.79ms
step:340/2330 train_time:20669ms step_avg:60.79ms
step:341/2330 train_time:20728ms step_avg:60.79ms
step:342/2330 train_time:20791ms step_avg:60.79ms
step:343/2330 train_time:20850ms step_avg:60.79ms
step:344/2330 train_time:20912ms step_avg:60.79ms
step:345/2330 train_time:20972ms step_avg:60.79ms
step:346/2330 train_time:21035ms step_avg:60.79ms
step:347/2330 train_time:21094ms step_avg:60.79ms
step:348/2330 train_time:21156ms step_avg:60.79ms
step:349/2330 train_time:21216ms step_avg:60.79ms
step:350/2330 train_time:21278ms step_avg:60.79ms
step:351/2330 train_time:21338ms step_avg:60.79ms
step:352/2330 train_time:21401ms step_avg:60.80ms
step:353/2330 train_time:21460ms step_avg:60.79ms
step:354/2330 train_time:21523ms step_avg:60.80ms
step:355/2330 train_time:21581ms step_avg:60.79ms
step:356/2330 train_time:21644ms step_avg:60.80ms
step:357/2330 train_time:21704ms step_avg:60.79ms
step:358/2330 train_time:21767ms step_avg:60.80ms
step:359/2330 train_time:21826ms step_avg:60.80ms
step:360/2330 train_time:21889ms step_avg:60.80ms
step:361/2330 train_time:21949ms step_avg:60.80ms
step:362/2330 train_time:22011ms step_avg:60.80ms
step:363/2330 train_time:22070ms step_avg:60.80ms
step:364/2330 train_time:22132ms step_avg:60.80ms
step:365/2330 train_time:22193ms step_avg:60.80ms
step:366/2330 train_time:22255ms step_avg:60.81ms
step:367/2330 train_time:22315ms step_avg:60.80ms
step:368/2330 train_time:22378ms step_avg:60.81ms
step:369/2330 train_time:22437ms step_avg:60.81ms
step:370/2330 train_time:22499ms step_avg:60.81ms
step:371/2330 train_time:22559ms step_avg:60.81ms
step:372/2330 train_time:22622ms step_avg:60.81ms
step:373/2330 train_time:22681ms step_avg:60.81ms
step:374/2330 train_time:22743ms step_avg:60.81ms
step:375/2330 train_time:22803ms step_avg:60.81ms
step:376/2330 train_time:22865ms step_avg:60.81ms
step:377/2330 train_time:22925ms step_avg:60.81ms
step:378/2330 train_time:22989ms step_avg:60.82ms
step:379/2330 train_time:23049ms step_avg:60.81ms
step:380/2330 train_time:23111ms step_avg:60.82ms
step:381/2330 train_time:23172ms step_avg:60.82ms
step:382/2330 train_time:23234ms step_avg:60.82ms
step:383/2330 train_time:23293ms step_avg:60.82ms
step:384/2330 train_time:23355ms step_avg:60.82ms
step:385/2330 train_time:23415ms step_avg:60.82ms
step:386/2330 train_time:23477ms step_avg:60.82ms
step:387/2330 train_time:23536ms step_avg:60.82ms
step:388/2330 train_time:23599ms step_avg:60.82ms
step:389/2330 train_time:23659ms step_avg:60.82ms
step:390/2330 train_time:23721ms step_avg:60.82ms
step:391/2330 train_time:23780ms step_avg:60.82ms
step:392/2330 train_time:23842ms step_avg:60.82ms
step:393/2330 train_time:23902ms step_avg:60.82ms
step:394/2330 train_time:23966ms step_avg:60.83ms
step:395/2330 train_time:24025ms step_avg:60.82ms
step:396/2330 train_time:24088ms step_avg:60.83ms
step:397/2330 train_time:24147ms step_avg:60.82ms
step:398/2330 train_time:24210ms step_avg:60.83ms
step:399/2330 train_time:24271ms step_avg:60.83ms
step:400/2330 train_time:24333ms step_avg:60.83ms
step:401/2330 train_time:24394ms step_avg:60.83ms
step:402/2330 train_time:24455ms step_avg:60.83ms
step:403/2330 train_time:24515ms step_avg:60.83ms
step:404/2330 train_time:24577ms step_avg:60.83ms
step:405/2330 train_time:24636ms step_avg:60.83ms
step:406/2330 train_time:24700ms step_avg:60.84ms
step:407/2330 train_time:24760ms step_avg:60.83ms
step:408/2330 train_time:24822ms step_avg:60.84ms
step:409/2330 train_time:24881ms step_avg:60.83ms
step:410/2330 train_time:24943ms step_avg:60.84ms
step:411/2330 train_time:25003ms step_avg:60.84ms
step:412/2330 train_time:25066ms step_avg:60.84ms
step:413/2330 train_time:25126ms step_avg:60.84ms
step:414/2330 train_time:25190ms step_avg:60.84ms
step:415/2330 train_time:25249ms step_avg:60.84ms
step:416/2330 train_time:25311ms step_avg:60.84ms
step:417/2330 train_time:25371ms step_avg:60.84ms
step:418/2330 train_time:25434ms step_avg:60.85ms
step:419/2330 train_time:25494ms step_avg:60.85ms
step:420/2330 train_time:25556ms step_avg:60.85ms
step:421/2330 train_time:25615ms step_avg:60.84ms
step:422/2330 train_time:25677ms step_avg:60.85ms
step:423/2330 train_time:25736ms step_avg:60.84ms
step:424/2330 train_time:25799ms step_avg:60.85ms
step:425/2330 train_time:25859ms step_avg:60.84ms
step:426/2330 train_time:25921ms step_avg:60.85ms
step:427/2330 train_time:25980ms step_avg:60.84ms
step:428/2330 train_time:26043ms step_avg:60.85ms
step:429/2330 train_time:26103ms step_avg:60.85ms
step:430/2330 train_time:26165ms step_avg:60.85ms
step:431/2330 train_time:26224ms step_avg:60.85ms
step:432/2330 train_time:26287ms step_avg:60.85ms
step:433/2330 train_time:26346ms step_avg:60.85ms
step:434/2330 train_time:26409ms step_avg:60.85ms
step:435/2330 train_time:26469ms step_avg:60.85ms
step:436/2330 train_time:26532ms step_avg:60.85ms
step:437/2330 train_time:26593ms step_avg:60.85ms
step:438/2330 train_time:26655ms step_avg:60.86ms
step:439/2330 train_time:26715ms step_avg:60.85ms
step:440/2330 train_time:26778ms step_avg:60.86ms
step:441/2330 train_time:26838ms step_avg:60.86ms
step:442/2330 train_time:26900ms step_avg:60.86ms
step:443/2330 train_time:26959ms step_avg:60.86ms
step:444/2330 train_time:27021ms step_avg:60.86ms
step:445/2330 train_time:27081ms step_avg:60.86ms
step:446/2330 train_time:27143ms step_avg:60.86ms
step:447/2330 train_time:27203ms step_avg:60.86ms
step:448/2330 train_time:27265ms step_avg:60.86ms
step:449/2330 train_time:27324ms step_avg:60.86ms
step:450/2330 train_time:27388ms step_avg:60.86ms
step:451/2330 train_time:27448ms step_avg:60.86ms
step:452/2330 train_time:27510ms step_avg:60.86ms
step:453/2330 train_time:27570ms step_avg:60.86ms
step:454/2330 train_time:27634ms step_avg:60.87ms
step:455/2330 train_time:27694ms step_avg:60.87ms
step:456/2330 train_time:27756ms step_avg:60.87ms
step:457/2330 train_time:27814ms step_avg:60.86ms
step:458/2330 train_time:27877ms step_avg:60.87ms
step:459/2330 train_time:27937ms step_avg:60.86ms
step:460/2330 train_time:28000ms step_avg:60.87ms
step:461/2330 train_time:28060ms step_avg:60.87ms
step:462/2330 train_time:28122ms step_avg:60.87ms
step:463/2330 train_time:28182ms step_avg:60.87ms
step:464/2330 train_time:28244ms step_avg:60.87ms
step:465/2330 train_time:28303ms step_avg:60.87ms
step:466/2330 train_time:28366ms step_avg:60.87ms
step:467/2330 train_time:28426ms step_avg:60.87ms
step:468/2330 train_time:28490ms step_avg:60.88ms
step:469/2330 train_time:28549ms step_avg:60.87ms
step:470/2330 train_time:28611ms step_avg:60.87ms
step:471/2330 train_time:28670ms step_avg:60.87ms
step:472/2330 train_time:28734ms step_avg:60.88ms
step:473/2330 train_time:28794ms step_avg:60.87ms
step:474/2330 train_time:28855ms step_avg:60.88ms
step:475/2330 train_time:28915ms step_avg:60.87ms
step:476/2330 train_time:28977ms step_avg:60.88ms
step:477/2330 train_time:29037ms step_avg:60.87ms
step:478/2330 train_time:29101ms step_avg:60.88ms
step:479/2330 train_time:29160ms step_avg:60.88ms
step:480/2330 train_time:29223ms step_avg:60.88ms
step:481/2330 train_time:29281ms step_avg:60.88ms
step:482/2330 train_time:29343ms step_avg:60.88ms
step:483/2330 train_time:29403ms step_avg:60.87ms
step:484/2330 train_time:29465ms step_avg:60.88ms
step:485/2330 train_time:29525ms step_avg:60.88ms
step:486/2330 train_time:29588ms step_avg:60.88ms
step:487/2330 train_time:29648ms step_avg:60.88ms
step:488/2330 train_time:29711ms step_avg:60.88ms
step:489/2330 train_time:29771ms step_avg:60.88ms
step:490/2330 train_time:29834ms step_avg:60.89ms
step:491/2330 train_time:29895ms step_avg:60.88ms
step:492/2330 train_time:29956ms step_avg:60.89ms
step:493/2330 train_time:30016ms step_avg:60.88ms
step:494/2330 train_time:30078ms step_avg:60.89ms
step:495/2330 train_time:30137ms step_avg:60.88ms
step:496/2330 train_time:30200ms step_avg:60.89ms
step:497/2330 train_time:30260ms step_avg:60.89ms
step:498/2330 train_time:30322ms step_avg:60.89ms
step:499/2330 train_time:30381ms step_avg:60.88ms
step:500/2330 train_time:30444ms step_avg:60.89ms
step:500/2330 val_loss:5.1851 train_time:30517ms step_avg:61.03ms
step:501/2330 train_time:30538ms step_avg:60.95ms
step:502/2330 train_time:30568ms step_avg:60.89ms
step:503/2330 train_time:30630ms step_avg:60.90ms
step:504/2330 train_time:30698ms step_avg:60.91ms
step:505/2330 train_time:30759ms step_avg:60.91ms
step:506/2330 train_time:30822ms step_avg:60.91ms
step:507/2330 train_time:30882ms step_avg:60.91ms
step:508/2330 train_time:30944ms step_avg:60.91ms
step:509/2330 train_time:31003ms step_avg:60.91ms
step:510/2330 train_time:31065ms step_avg:60.91ms
step:511/2330 train_time:31125ms step_avg:60.91ms
step:512/2330 train_time:31186ms step_avg:60.91ms
step:513/2330 train_time:31245ms step_avg:60.91ms
step:514/2330 train_time:31307ms step_avg:60.91ms
step:515/2330 train_time:31366ms step_avg:60.90ms
step:516/2330 train_time:31430ms step_avg:60.91ms
step:517/2330 train_time:31489ms step_avg:60.91ms
step:518/2330 train_time:31551ms step_avg:60.91ms
step:519/2330 train_time:31613ms step_avg:60.91ms
step:520/2330 train_time:31678ms step_avg:60.92ms
step:521/2330 train_time:31739ms step_avg:60.92ms
step:522/2330 train_time:31802ms step_avg:60.92ms
step:523/2330 train_time:31861ms step_avg:60.92ms
step:524/2330 train_time:31924ms step_avg:60.92ms
step:525/2330 train_time:31983ms step_avg:60.92ms
step:526/2330 train_time:32044ms step_avg:60.92ms
step:527/2330 train_time:32104ms step_avg:60.92ms
step:528/2330 train_time:32165ms step_avg:60.92ms
step:529/2330 train_time:32224ms step_avg:60.92ms
step:530/2330 train_time:32286ms step_avg:60.92ms
step:531/2330 train_time:32346ms step_avg:60.91ms
step:532/2330 train_time:32408ms step_avg:60.92ms
step:533/2330 train_time:32467ms step_avg:60.91ms
step:534/2330 train_time:32530ms step_avg:60.92ms
step:535/2330 train_time:32590ms step_avg:60.92ms
step:536/2330 train_time:32653ms step_avg:60.92ms
step:537/2330 train_time:32713ms step_avg:60.92ms
step:538/2330 train_time:32776ms step_avg:60.92ms
step:539/2330 train_time:32836ms step_avg:60.92ms
step:540/2330 train_time:32899ms step_avg:60.92ms
step:541/2330 train_time:32958ms step_avg:60.92ms
step:542/2330 train_time:33021ms step_avg:60.93ms
step:543/2330 train_time:33080ms step_avg:60.92ms
step:544/2330 train_time:33143ms step_avg:60.92ms
step:545/2330 train_time:33202ms step_avg:60.92ms
step:546/2330 train_time:33263ms step_avg:60.92ms
step:547/2330 train_time:33323ms step_avg:60.92ms
step:548/2330 train_time:33385ms step_avg:60.92ms
step:549/2330 train_time:33444ms step_avg:60.92ms
step:550/2330 train_time:33508ms step_avg:60.92ms
step:551/2330 train_time:33568ms step_avg:60.92ms
step:552/2330 train_time:33631ms step_avg:60.93ms
step:553/2330 train_time:33691ms step_avg:60.92ms
step:554/2330 train_time:33754ms step_avg:60.93ms
step:555/2330 train_time:33813ms step_avg:60.92ms
step:556/2330 train_time:33875ms step_avg:60.93ms
step:557/2330 train_time:33934ms step_avg:60.92ms
step:558/2330 train_time:33996ms step_avg:60.93ms
step:559/2330 train_time:34056ms step_avg:60.92ms
step:560/2330 train_time:34119ms step_avg:60.93ms
step:561/2330 train_time:34178ms step_avg:60.92ms
step:562/2330 train_time:34241ms step_avg:60.93ms
step:563/2330 train_time:34301ms step_avg:60.93ms
step:564/2330 train_time:34363ms step_avg:60.93ms
step:565/2330 train_time:34424ms step_avg:60.93ms
step:566/2330 train_time:34485ms step_avg:60.93ms
step:567/2330 train_time:34545ms step_avg:60.93ms
step:568/2330 train_time:34608ms step_avg:60.93ms
step:569/2330 train_time:34668ms step_avg:60.93ms
step:570/2330 train_time:34731ms step_avg:60.93ms
step:571/2330 train_time:34790ms step_avg:60.93ms
step:572/2330 train_time:34852ms step_avg:60.93ms
step:573/2330 train_time:34912ms step_avg:60.93ms
step:574/2330 train_time:34975ms step_avg:60.93ms
step:575/2330 train_time:35035ms step_avg:60.93ms
step:576/2330 train_time:35096ms step_avg:60.93ms
step:577/2330 train_time:35155ms step_avg:60.93ms
step:578/2330 train_time:35218ms step_avg:60.93ms
step:579/2330 train_time:35278ms step_avg:60.93ms
step:580/2330 train_time:35342ms step_avg:60.93ms
step:581/2330 train_time:35401ms step_avg:60.93ms
step:582/2330 train_time:35464ms step_avg:60.93ms
step:583/2330 train_time:35525ms step_avg:60.93ms
step:584/2330 train_time:35587ms step_avg:60.94ms
step:585/2330 train_time:35647ms step_avg:60.94ms
step:586/2330 train_time:35709ms step_avg:60.94ms
step:587/2330 train_time:35769ms step_avg:60.94ms
step:588/2330 train_time:35831ms step_avg:60.94ms
step:589/2330 train_time:35891ms step_avg:60.93ms
step:590/2330 train_time:35954ms step_avg:60.94ms
step:591/2330 train_time:36013ms step_avg:60.94ms
step:592/2330 train_time:36075ms step_avg:60.94ms
step:593/2330 train_time:36134ms step_avg:60.93ms
step:594/2330 train_time:36196ms step_avg:60.94ms
step:595/2330 train_time:36255ms step_avg:60.93ms
step:596/2330 train_time:36319ms step_avg:60.94ms
step:597/2330 train_time:36379ms step_avg:60.94ms
step:598/2330 train_time:36443ms step_avg:60.94ms
step:599/2330 train_time:36502ms step_avg:60.94ms
step:600/2330 train_time:36564ms step_avg:60.94ms
step:601/2330 train_time:36624ms step_avg:60.94ms
step:602/2330 train_time:36687ms step_avg:60.94ms
step:603/2330 train_time:36746ms step_avg:60.94ms
step:604/2330 train_time:36809ms step_avg:60.94ms
step:605/2330 train_time:36868ms step_avg:60.94ms
step:606/2330 train_time:36932ms step_avg:60.94ms
step:607/2330 train_time:36992ms step_avg:60.94ms
step:608/2330 train_time:37055ms step_avg:60.95ms
step:609/2330 train_time:37114ms step_avg:60.94ms
step:610/2330 train_time:37176ms step_avg:60.94ms
step:611/2330 train_time:37235ms step_avg:60.94ms
step:612/2330 train_time:37299ms step_avg:60.95ms
step:613/2330 train_time:37358ms step_avg:60.94ms
step:614/2330 train_time:37421ms step_avg:60.95ms
step:615/2330 train_time:37481ms step_avg:60.94ms
step:616/2330 train_time:37543ms step_avg:60.95ms
step:617/2330 train_time:37604ms step_avg:60.95ms
step:618/2330 train_time:37665ms step_avg:60.95ms
step:619/2330 train_time:37725ms step_avg:60.95ms
step:620/2330 train_time:37788ms step_avg:60.95ms
step:621/2330 train_time:37847ms step_avg:60.95ms
step:622/2330 train_time:37910ms step_avg:60.95ms
step:623/2330 train_time:37969ms step_avg:60.95ms
step:624/2330 train_time:38032ms step_avg:60.95ms
step:625/2330 train_time:38091ms step_avg:60.95ms
step:626/2330 train_time:38154ms step_avg:60.95ms
step:627/2330 train_time:38213ms step_avg:60.95ms
step:628/2330 train_time:38275ms step_avg:60.95ms
step:629/2330 train_time:38335ms step_avg:60.95ms
step:630/2330 train_time:38397ms step_avg:60.95ms
step:631/2330 train_time:38457ms step_avg:60.95ms
step:632/2330 train_time:38521ms step_avg:60.95ms
step:633/2330 train_time:38581ms step_avg:60.95ms
step:634/2330 train_time:38644ms step_avg:60.95ms
step:635/2330 train_time:38704ms step_avg:60.95ms
step:636/2330 train_time:38767ms step_avg:60.95ms
step:637/2330 train_time:38827ms step_avg:60.95ms
step:638/2330 train_time:38889ms step_avg:60.95ms
step:639/2330 train_time:38948ms step_avg:60.95ms
step:640/2330 train_time:39010ms step_avg:60.95ms
step:641/2330 train_time:39070ms step_avg:60.95ms
step:642/2330 train_time:39133ms step_avg:60.95ms
step:643/2330 train_time:39193ms step_avg:60.95ms
step:644/2330 train_time:39255ms step_avg:60.96ms
step:645/2330 train_time:39314ms step_avg:60.95ms
step:646/2330 train_time:39377ms step_avg:60.95ms
step:647/2330 train_time:39436ms step_avg:60.95ms
step:648/2330 train_time:39499ms step_avg:60.95ms
step:649/2330 train_time:39559ms step_avg:60.95ms
step:650/2330 train_time:39623ms step_avg:60.96ms
step:651/2330 train_time:39683ms step_avg:60.96ms
step:652/2330 train_time:39744ms step_avg:60.96ms
step:653/2330 train_time:39804ms step_avg:60.96ms
step:654/2330 train_time:39867ms step_avg:60.96ms
step:655/2330 train_time:39927ms step_avg:60.96ms
step:656/2330 train_time:39989ms step_avg:60.96ms
step:657/2330 train_time:40049ms step_avg:60.96ms
step:658/2330 train_time:40110ms step_avg:60.96ms
step:659/2330 train_time:40171ms step_avg:60.96ms
step:660/2330 train_time:40234ms step_avg:60.96ms
step:661/2330 train_time:40294ms step_avg:60.96ms
step:662/2330 train_time:40356ms step_avg:60.96ms
step:663/2330 train_time:40415ms step_avg:60.96ms
step:664/2330 train_time:40478ms step_avg:60.96ms
step:665/2330 train_time:40538ms step_avg:60.96ms
step:666/2330 train_time:40601ms step_avg:60.96ms
step:667/2330 train_time:40661ms step_avg:60.96ms
step:668/2330 train_time:40724ms step_avg:60.96ms
step:669/2330 train_time:40782ms step_avg:60.96ms
step:670/2330 train_time:40846ms step_avg:60.96ms
step:671/2330 train_time:40905ms step_avg:60.96ms
step:672/2330 train_time:40968ms step_avg:60.96ms
step:673/2330 train_time:41027ms step_avg:60.96ms
step:674/2330 train_time:41090ms step_avg:60.96ms
step:675/2330 train_time:41150ms step_avg:60.96ms
step:676/2330 train_time:41212ms step_avg:60.96ms
step:677/2330 train_time:41272ms step_avg:60.96ms
step:678/2330 train_time:41335ms step_avg:60.97ms
step:679/2330 train_time:41394ms step_avg:60.96ms
step:680/2330 train_time:41457ms step_avg:60.97ms
step:681/2330 train_time:41516ms step_avg:60.96ms
step:682/2330 train_time:41579ms step_avg:60.97ms
step:683/2330 train_time:41639ms step_avg:60.96ms
step:684/2330 train_time:41702ms step_avg:60.97ms
step:685/2330 train_time:41761ms step_avg:60.97ms
step:686/2330 train_time:41824ms step_avg:60.97ms
step:687/2330 train_time:41884ms step_avg:60.97ms
step:688/2330 train_time:41946ms step_avg:60.97ms
step:689/2330 train_time:42005ms step_avg:60.97ms
step:690/2330 train_time:42068ms step_avg:60.97ms
step:691/2330 train_time:42128ms step_avg:60.97ms
step:692/2330 train_time:42190ms step_avg:60.97ms
step:693/2330 train_time:42249ms step_avg:60.97ms
step:694/2330 train_time:42312ms step_avg:60.97ms
step:695/2330 train_time:42371ms step_avg:60.97ms
step:696/2330 train_time:42434ms step_avg:60.97ms
step:697/2330 train_time:42492ms step_avg:60.96ms
step:698/2330 train_time:42555ms step_avg:60.97ms
step:699/2330 train_time:42615ms step_avg:60.97ms
step:700/2330 train_time:42677ms step_avg:60.97ms
step:701/2330 train_time:42738ms step_avg:60.97ms
step:702/2330 train_time:42801ms step_avg:60.97ms
step:703/2330 train_time:42861ms step_avg:60.97ms
step:704/2330 train_time:42924ms step_avg:60.97ms
step:705/2330 train_time:42984ms step_avg:60.97ms
step:706/2330 train_time:43045ms step_avg:60.97ms
step:707/2330 train_time:43105ms step_avg:60.97ms
step:708/2330 train_time:43167ms step_avg:60.97ms
step:709/2330 train_time:43228ms step_avg:60.97ms
step:710/2330 train_time:43289ms step_avg:60.97ms
step:711/2330 train_time:43349ms step_avg:60.97ms
step:712/2330 train_time:43411ms step_avg:60.97ms
step:713/2330 train_time:43471ms step_avg:60.97ms
step:714/2330 train_time:43533ms step_avg:60.97ms
step:715/2330 train_time:43593ms step_avg:60.97ms
step:716/2330 train_time:43656ms step_avg:60.97ms
step:717/2330 train_time:43714ms step_avg:60.97ms
step:718/2330 train_time:43777ms step_avg:60.97ms
step:719/2330 train_time:43836ms step_avg:60.97ms
step:720/2330 train_time:43900ms step_avg:60.97ms
step:721/2330 train_time:43960ms step_avg:60.97ms
step:722/2330 train_time:44024ms step_avg:60.97ms
step:723/2330 train_time:44083ms step_avg:60.97ms
step:724/2330 train_time:44146ms step_avg:60.97ms
step:725/2330 train_time:44205ms step_avg:60.97ms
step:726/2330 train_time:44267ms step_avg:60.97ms
step:727/2330 train_time:44328ms step_avg:60.97ms
step:728/2330 train_time:44390ms step_avg:60.97ms
step:729/2330 train_time:44449ms step_avg:60.97ms
step:730/2330 train_time:44511ms step_avg:60.97ms
step:731/2330 train_time:44571ms step_avg:60.97ms
step:732/2330 train_time:44633ms step_avg:60.97ms
step:733/2330 train_time:44693ms step_avg:60.97ms
step:734/2330 train_time:44755ms step_avg:60.97ms
step:735/2330 train_time:44815ms step_avg:60.97ms
step:736/2330 train_time:44878ms step_avg:60.97ms
step:737/2330 train_time:44938ms step_avg:60.97ms
step:738/2330 train_time:45001ms step_avg:60.98ms
step:739/2330 train_time:45061ms step_avg:60.98ms
step:740/2330 train_time:45124ms step_avg:60.98ms
step:741/2330 train_time:45183ms step_avg:60.98ms
step:742/2330 train_time:45245ms step_avg:60.98ms
step:743/2330 train_time:45305ms step_avg:60.98ms
step:744/2330 train_time:45367ms step_avg:60.98ms
step:745/2330 train_time:45427ms step_avg:60.98ms
step:746/2330 train_time:45490ms step_avg:60.98ms
step:747/2330 train_time:45550ms step_avg:60.98ms
step:748/2330 train_time:45612ms step_avg:60.98ms
step:749/2330 train_time:45672ms step_avg:60.98ms
step:750/2330 train_time:45735ms step_avg:60.98ms
step:750/2330 val_loss:4.9940 train_time:45807ms step_avg:61.08ms
step:751/2330 train_time:45829ms step_avg:61.02ms
step:752/2330 train_time:45859ms step_avg:60.98ms
step:753/2330 train_time:45918ms step_avg:60.98ms
step:754/2330 train_time:45985ms step_avg:60.99ms
step:755/2330 train_time:46048ms step_avg:60.99ms
step:756/2330 train_time:46111ms step_avg:60.99ms
step:757/2330 train_time:46170ms step_avg:60.99ms
step:758/2330 train_time:46232ms step_avg:60.99ms
step:759/2330 train_time:46292ms step_avg:60.99ms
step:760/2330 train_time:46353ms step_avg:60.99ms
step:761/2330 train_time:46411ms step_avg:60.99ms
step:762/2330 train_time:46474ms step_avg:60.99ms
step:763/2330 train_time:46532ms step_avg:60.99ms
step:764/2330 train_time:46593ms step_avg:60.99ms
step:765/2330 train_time:46654ms step_avg:60.99ms
step:766/2330 train_time:46716ms step_avg:60.99ms
step:767/2330 train_time:46777ms step_avg:60.99ms
step:768/2330 train_time:46841ms step_avg:60.99ms
step:769/2330 train_time:46903ms step_avg:60.99ms
step:770/2330 train_time:46969ms step_avg:61.00ms
step:771/2330 train_time:47030ms step_avg:61.00ms
step:772/2330 train_time:47093ms step_avg:61.00ms
step:773/2330 train_time:47154ms step_avg:61.00ms
step:774/2330 train_time:47216ms step_avg:61.00ms
step:775/2330 train_time:47277ms step_avg:61.00ms
step:776/2330 train_time:47339ms step_avg:61.00ms
step:777/2330 train_time:47399ms step_avg:61.00ms
step:778/2330 train_time:47462ms step_avg:61.01ms
step:779/2330 train_time:47521ms step_avg:61.00ms
step:780/2330 train_time:47585ms step_avg:61.01ms
step:781/2330 train_time:47646ms step_avg:61.01ms
step:782/2330 train_time:47710ms step_avg:61.01ms
step:783/2330 train_time:47770ms step_avg:61.01ms
step:784/2330 train_time:47833ms step_avg:61.01ms
step:785/2330 train_time:47893ms step_avg:61.01ms
step:786/2330 train_time:47957ms step_avg:61.01ms
step:787/2330 train_time:48017ms step_avg:61.01ms
step:788/2330 train_time:48080ms step_avg:61.02ms
step:789/2330 train_time:48140ms step_avg:61.01ms
step:790/2330 train_time:48204ms step_avg:61.02ms
step:791/2330 train_time:48263ms step_avg:61.02ms
step:792/2330 train_time:48327ms step_avg:61.02ms
step:793/2330 train_time:48388ms step_avg:61.02ms
step:794/2330 train_time:48453ms step_avg:61.02ms
step:795/2330 train_time:48513ms step_avg:61.02ms
step:796/2330 train_time:48575ms step_avg:61.02ms
step:797/2330 train_time:48635ms step_avg:61.02ms
step:798/2330 train_time:48698ms step_avg:61.02ms
step:799/2330 train_time:48758ms step_avg:61.02ms
step:800/2330 train_time:48822ms step_avg:61.03ms
step:801/2330 train_time:48882ms step_avg:61.03ms
step:802/2330 train_time:48946ms step_avg:61.03ms
step:803/2330 train_time:49007ms step_avg:61.03ms
step:804/2330 train_time:49071ms step_avg:61.03ms
step:805/2330 train_time:49131ms step_avg:61.03ms
step:806/2330 train_time:49194ms step_avg:61.04ms
step:807/2330 train_time:49256ms step_avg:61.04ms
step:808/2330 train_time:49318ms step_avg:61.04ms
step:809/2330 train_time:49378ms step_avg:61.04ms
step:810/2330 train_time:49441ms step_avg:61.04ms
step:811/2330 train_time:49501ms step_avg:61.04ms
step:812/2330 train_time:49565ms step_avg:61.04ms
step:813/2330 train_time:49624ms step_avg:61.04ms
step:814/2330 train_time:49688ms step_avg:61.04ms
step:815/2330 train_time:49748ms step_avg:61.04ms
step:816/2330 train_time:49812ms step_avg:61.04ms
step:817/2330 train_time:49872ms step_avg:61.04ms
step:818/2330 train_time:49934ms step_avg:61.04ms
step:819/2330 train_time:49995ms step_avg:61.04ms
step:820/2330 train_time:50059ms step_avg:61.05ms
step:821/2330 train_time:50118ms step_avg:61.05ms
step:822/2330 train_time:50182ms step_avg:61.05ms
step:823/2330 train_time:50241ms step_avg:61.05ms
step:824/2330 train_time:50306ms step_avg:61.05ms
step:825/2330 train_time:50366ms step_avg:61.05ms
step:826/2330 train_time:50429ms step_avg:61.05ms
step:827/2330 train_time:50490ms step_avg:61.05ms
step:828/2330 train_time:50553ms step_avg:61.05ms
step:829/2330 train_time:50613ms step_avg:61.05ms
step:830/2330 train_time:50676ms step_avg:61.06ms
step:831/2330 train_time:50735ms step_avg:61.05ms
step:832/2330 train_time:50799ms step_avg:61.06ms
step:833/2330 train_time:50859ms step_avg:61.05ms
step:834/2330 train_time:50921ms step_avg:61.06ms
step:835/2330 train_time:50982ms step_avg:61.06ms
step:836/2330 train_time:51046ms step_avg:61.06ms
step:837/2330 train_time:51107ms step_avg:61.06ms
step:838/2330 train_time:51171ms step_avg:61.06ms
step:839/2330 train_time:51231ms step_avg:61.06ms
step:840/2330 train_time:51294ms step_avg:61.06ms
step:841/2330 train_time:51355ms step_avg:61.06ms
step:842/2330 train_time:51418ms step_avg:61.07ms
step:843/2330 train_time:51478ms step_avg:61.07ms
step:844/2330 train_time:51541ms step_avg:61.07ms
step:845/2330 train_time:51601ms step_avg:61.07ms
step:846/2330 train_time:51665ms step_avg:61.07ms
step:847/2330 train_time:51725ms step_avg:61.07ms
step:848/2330 train_time:51790ms step_avg:61.07ms
step:849/2330 train_time:51851ms step_avg:61.07ms
step:850/2330 train_time:51914ms step_avg:61.08ms
step:851/2330 train_time:51973ms step_avg:61.07ms
step:852/2330 train_time:52037ms step_avg:61.08ms
step:853/2330 train_time:52097ms step_avg:61.07ms
step:854/2330 train_time:52161ms step_avg:61.08ms
step:855/2330 train_time:52221ms step_avg:61.08ms
step:856/2330 train_time:52284ms step_avg:61.08ms
step:857/2330 train_time:52345ms step_avg:61.08ms
step:858/2330 train_time:52408ms step_avg:61.08ms
step:859/2330 train_time:52469ms step_avg:61.08ms
step:860/2330 train_time:52532ms step_avg:61.08ms
step:861/2330 train_time:52592ms step_avg:61.08ms
step:862/2330 train_time:52655ms step_avg:61.08ms
step:863/2330 train_time:52715ms step_avg:61.08ms
step:864/2330 train_time:52778ms step_avg:61.09ms
step:865/2330 train_time:52838ms step_avg:61.08ms
step:866/2330 train_time:52901ms step_avg:61.09ms
step:867/2330 train_time:52961ms step_avg:61.09ms
step:868/2330 train_time:53024ms step_avg:61.09ms
step:869/2330 train_time:53085ms step_avg:61.09ms
step:870/2330 train_time:53149ms step_avg:61.09ms
step:871/2330 train_time:53209ms step_avg:61.09ms
step:872/2330 train_time:53273ms step_avg:61.09ms
step:873/2330 train_time:53333ms step_avg:61.09ms
step:874/2330 train_time:53396ms step_avg:61.09ms
step:875/2330 train_time:53457ms step_avg:61.09ms
step:876/2330 train_time:53519ms step_avg:61.09ms
step:877/2330 train_time:53579ms step_avg:61.09ms
step:878/2330 train_time:53642ms step_avg:61.10ms
step:879/2330 train_time:53702ms step_avg:61.09ms
step:880/2330 train_time:53766ms step_avg:61.10ms
step:881/2330 train_time:53825ms step_avg:61.10ms
step:882/2330 train_time:53890ms step_avg:61.10ms
step:883/2330 train_time:53952ms step_avg:61.10ms
step:884/2330 train_time:54014ms step_avg:61.10ms
step:885/2330 train_time:54074ms step_avg:61.10ms
step:886/2330 train_time:54136ms step_avg:61.10ms
step:887/2330 train_time:54197ms step_avg:61.10ms
step:888/2330 train_time:54260ms step_avg:61.10ms
step:889/2330 train_time:54321ms step_avg:61.10ms
step:890/2330 train_time:54384ms step_avg:61.11ms
step:891/2330 train_time:54444ms step_avg:61.10ms
step:892/2330 train_time:54507ms step_avg:61.11ms
step:893/2330 train_time:54569ms step_avg:61.11ms
step:894/2330 train_time:54633ms step_avg:61.11ms
step:895/2330 train_time:54693ms step_avg:61.11ms
step:896/2330 train_time:54757ms step_avg:61.11ms
step:897/2330 train_time:54816ms step_avg:61.11ms
step:898/2330 train_time:54880ms step_avg:61.11ms
step:899/2330 train_time:54940ms step_avg:61.11ms
step:900/2330 train_time:55004ms step_avg:61.12ms
step:901/2330 train_time:55065ms step_avg:61.12ms
step:902/2330 train_time:55128ms step_avg:61.12ms
step:903/2330 train_time:55190ms step_avg:61.12ms
step:904/2330 train_time:55254ms step_avg:61.12ms
step:905/2330 train_time:55314ms step_avg:61.12ms
step:906/2330 train_time:55377ms step_avg:61.12ms
step:907/2330 train_time:55437ms step_avg:61.12ms
step:908/2330 train_time:55500ms step_avg:61.12ms
step:909/2330 train_time:55561ms step_avg:61.12ms
step:910/2330 train_time:55624ms step_avg:61.13ms
step:911/2330 train_time:55684ms step_avg:61.12ms
step:912/2330 train_time:55748ms step_avg:61.13ms
step:913/2330 train_time:55809ms step_avg:61.13ms
step:914/2330 train_time:55872ms step_avg:61.13ms
step:915/2330 train_time:55932ms step_avg:61.13ms
step:916/2330 train_time:55995ms step_avg:61.13ms
step:917/2330 train_time:56055ms step_avg:61.13ms
step:918/2330 train_time:56119ms step_avg:61.13ms
step:919/2330 train_time:56179ms step_avg:61.13ms
step:920/2330 train_time:56243ms step_avg:61.13ms
step:921/2330 train_time:56303ms step_avg:61.13ms
step:922/2330 train_time:56367ms step_avg:61.14ms
step:923/2330 train_time:56427ms step_avg:61.13ms
step:924/2330 train_time:56490ms step_avg:61.14ms
step:925/2330 train_time:56551ms step_avg:61.14ms
step:926/2330 train_time:56614ms step_avg:61.14ms
step:927/2330 train_time:56674ms step_avg:61.14ms
step:928/2330 train_time:56737ms step_avg:61.14ms
step:929/2330 train_time:56796ms step_avg:61.14ms
step:930/2330 train_time:56861ms step_avg:61.14ms
step:931/2330 train_time:56921ms step_avg:61.14ms
step:932/2330 train_time:56984ms step_avg:61.14ms
step:933/2330 train_time:57045ms step_avg:61.14ms
step:934/2330 train_time:57108ms step_avg:61.14ms
step:935/2330 train_time:57169ms step_avg:61.14ms
step:936/2330 train_time:57233ms step_avg:61.15ms
step:937/2330 train_time:57293ms step_avg:61.15ms
step:938/2330 train_time:57356ms step_avg:61.15ms
step:939/2330 train_time:57415ms step_avg:61.15ms
step:940/2330 train_time:57479ms step_avg:61.15ms
step:941/2330 train_time:57538ms step_avg:61.15ms
step:942/2330 train_time:57601ms step_avg:61.15ms
step:943/2330 train_time:57661ms step_avg:61.15ms
step:944/2330 train_time:57725ms step_avg:61.15ms
step:945/2330 train_time:57785ms step_avg:61.15ms
step:946/2330 train_time:57849ms step_avg:61.15ms
step:947/2330 train_time:57909ms step_avg:61.15ms
step:948/2330 train_time:57972ms step_avg:61.15ms
step:949/2330 train_time:58032ms step_avg:61.15ms
step:950/2330 train_time:58095ms step_avg:61.15ms
step:951/2330 train_time:58156ms step_avg:61.15ms
step:952/2330 train_time:58219ms step_avg:61.15ms
step:953/2330 train_time:58279ms step_avg:61.15ms
step:954/2330 train_time:58342ms step_avg:61.15ms
step:955/2330 train_time:58402ms step_avg:61.15ms
step:956/2330 train_time:58466ms step_avg:61.16ms
step:957/2330 train_time:58526ms step_avg:61.16ms
step:958/2330 train_time:58591ms step_avg:61.16ms
step:959/2330 train_time:58652ms step_avg:61.16ms
step:960/2330 train_time:58715ms step_avg:61.16ms
step:961/2330 train_time:58774ms step_avg:61.16ms
step:962/2330 train_time:58838ms step_avg:61.16ms
step:963/2330 train_time:58898ms step_avg:61.16ms
step:964/2330 train_time:58961ms step_avg:61.16ms
step:965/2330 train_time:59021ms step_avg:61.16ms
step:966/2330 train_time:59084ms step_avg:61.16ms
step:967/2330 train_time:59145ms step_avg:61.16ms
step:968/2330 train_time:59208ms step_avg:61.17ms
step:969/2330 train_time:59269ms step_avg:61.17ms
step:970/2330 train_time:59332ms step_avg:61.17ms
step:971/2330 train_time:59393ms step_avg:61.17ms
step:972/2330 train_time:59457ms step_avg:61.17ms
step:973/2330 train_time:59517ms step_avg:61.17ms
step:974/2330 train_time:59579ms step_avg:61.17ms
step:975/2330 train_time:59638ms step_avg:61.17ms
step:976/2330 train_time:59702ms step_avg:61.17ms
step:977/2330 train_time:59762ms step_avg:61.17ms
step:978/2330 train_time:59826ms step_avg:61.17ms
step:979/2330 train_time:59887ms step_avg:61.17ms
step:980/2330 train_time:59951ms step_avg:61.17ms
step:981/2330 train_time:60011ms step_avg:61.17ms
step:982/2330 train_time:60074ms step_avg:61.18ms
step:983/2330 train_time:60134ms step_avg:61.17ms
step:984/2330 train_time:60197ms step_avg:61.18ms
step:985/2330 train_time:60258ms step_avg:61.18ms
step:986/2330 train_time:60322ms step_avg:61.18ms
step:987/2330 train_time:60381ms step_avg:61.18ms
step:988/2330 train_time:60445ms step_avg:61.18ms
step:989/2330 train_time:60506ms step_avg:61.18ms
step:990/2330 train_time:60570ms step_avg:61.18ms
step:991/2330 train_time:60630ms step_avg:61.18ms
step:992/2330 train_time:60694ms step_avg:61.18ms
step:993/2330 train_time:60756ms step_avg:61.18ms
step:994/2330 train_time:60818ms step_avg:61.18ms
step:995/2330 train_time:60877ms step_avg:61.18ms
step:996/2330 train_time:60941ms step_avg:61.19ms
step:997/2330 train_time:61001ms step_avg:61.18ms
step:998/2330 train_time:61064ms step_avg:61.19ms
step:999/2330 train_time:61124ms step_avg:61.18ms
step:1000/2330 train_time:61188ms step_avg:61.19ms
step:1000/2330 val_loss:4.7284 train_time:61263ms step_avg:61.26ms
step:1001/2330 train_time:61285ms step_avg:61.22ms
step:1002/2330 train_time:61315ms step_avg:61.19ms
step:1003/2330 train_time:61381ms step_avg:61.20ms
step:1004/2330 train_time:61448ms step_avg:61.20ms
step:1005/2330 train_time:61510ms step_avg:61.20ms
step:1006/2330 train_time:61573ms step_avg:61.21ms
step:1007/2330 train_time:61632ms step_avg:61.20ms
step:1008/2330 train_time:61694ms step_avg:61.20ms
step:1009/2330 train_time:61754ms step_avg:61.20ms
step:1010/2330 train_time:61816ms step_avg:61.20ms
step:1011/2330 train_time:61875ms step_avg:61.20ms
step:1012/2330 train_time:61937ms step_avg:61.20ms
step:1013/2330 train_time:61996ms step_avg:61.20ms
step:1014/2330 train_time:62058ms step_avg:61.20ms
step:1015/2330 train_time:62116ms step_avg:61.20ms
step:1016/2330 train_time:62179ms step_avg:61.20ms
step:1017/2330 train_time:62243ms step_avg:61.20ms
step:1018/2330 train_time:62309ms step_avg:61.21ms
step:1019/2330 train_time:62371ms step_avg:61.21ms
step:1020/2330 train_time:62437ms step_avg:61.21ms
step:1021/2330 train_time:62498ms step_avg:61.21ms
step:1022/2330 train_time:62562ms step_avg:61.22ms
step:1023/2330 train_time:62621ms step_avg:61.21ms
step:1024/2330 train_time:62684ms step_avg:61.22ms
step:1025/2330 train_time:62746ms step_avg:61.22ms
step:1026/2330 train_time:62809ms step_avg:61.22ms
step:1027/2330 train_time:62868ms step_avg:61.22ms
step:1028/2330 train_time:62931ms step_avg:61.22ms
step:1029/2330 train_time:62990ms step_avg:61.21ms
step:1030/2330 train_time:63053ms step_avg:61.22ms
step:1031/2330 train_time:63113ms step_avg:61.22ms
step:1032/2330 train_time:63176ms step_avg:61.22ms
step:1033/2330 train_time:63236ms step_avg:61.22ms
step:1034/2330 train_time:63299ms step_avg:61.22ms
step:1035/2330 train_time:63361ms step_avg:61.22ms
step:1036/2330 train_time:63425ms step_avg:61.22ms
step:1037/2330 train_time:63487ms step_avg:61.22ms
step:1038/2330 train_time:63551ms step_avg:61.22ms
step:1039/2330 train_time:63610ms step_avg:61.22ms
step:1040/2330 train_time:63674ms step_avg:61.23ms
step:1041/2330 train_time:63735ms step_avg:61.22ms
step:1042/2330 train_time:63799ms step_avg:61.23ms
step:1043/2330 train_time:63858ms step_avg:61.23ms
step:1044/2330 train_time:63921ms step_avg:61.23ms
step:1045/2330 train_time:63981ms step_avg:61.23ms
step:1046/2330 train_time:64044ms step_avg:61.23ms
step:1047/2330 train_time:64105ms step_avg:61.23ms
step:1048/2330 train_time:64168ms step_avg:61.23ms
step:1049/2330 train_time:64229ms step_avg:61.23ms
step:1050/2330 train_time:64293ms step_avg:61.23ms
step:1051/2330 train_time:64352ms step_avg:61.23ms
step:1052/2330 train_time:64416ms step_avg:61.23ms
step:1053/2330 train_time:64476ms step_avg:61.23ms
step:1054/2330 train_time:64539ms step_avg:61.23ms
step:1055/2330 train_time:64600ms step_avg:61.23ms
step:1056/2330 train_time:64664ms step_avg:61.23ms
step:1057/2330 train_time:64724ms step_avg:61.23ms
step:1058/2330 train_time:64787ms step_avg:61.24ms
step:1059/2330 train_time:64849ms step_avg:61.24ms
step:1060/2330 train_time:64911ms step_avg:61.24ms
step:1061/2330 train_time:64971ms step_avg:61.24ms
step:1062/2330 train_time:65034ms step_avg:61.24ms
step:1063/2330 train_time:65094ms step_avg:61.24ms
step:1064/2330 train_time:65158ms step_avg:61.24ms
step:1065/2330 train_time:65218ms step_avg:61.24ms
step:1066/2330 train_time:65281ms step_avg:61.24ms
step:1067/2330 train_time:65342ms step_avg:61.24ms
step:1068/2330 train_time:65406ms step_avg:61.24ms
step:1069/2330 train_time:65467ms step_avg:61.24ms
step:1070/2330 train_time:65530ms step_avg:61.24ms
step:1071/2330 train_time:65591ms step_avg:61.24ms
step:1072/2330 train_time:65654ms step_avg:61.24ms
step:1073/2330 train_time:65714ms step_avg:61.24ms
step:1074/2330 train_time:65776ms step_avg:61.24ms
step:1075/2330 train_time:65836ms step_avg:61.24ms
step:1076/2330 train_time:65899ms step_avg:61.24ms
step:1077/2330 train_time:65959ms step_avg:61.24ms
step:1078/2330 train_time:66022ms step_avg:61.25ms
step:1079/2330 train_time:66082ms step_avg:61.24ms
step:1080/2330 train_time:66145ms step_avg:61.25ms
step:1081/2330 train_time:66207ms step_avg:61.25ms
step:1082/2330 train_time:66270ms step_avg:61.25ms
step:1083/2330 train_time:66330ms step_avg:61.25ms
step:1084/2330 train_time:66394ms step_avg:61.25ms
step:1085/2330 train_time:66455ms step_avg:61.25ms
step:1086/2330 train_time:66517ms step_avg:61.25ms
step:1087/2330 train_time:66578ms step_avg:61.25ms
step:1088/2330 train_time:66642ms step_avg:61.25ms
step:1089/2330 train_time:66702ms step_avg:61.25ms
step:1090/2330 train_time:66766ms step_avg:61.25ms
step:1091/2330 train_time:66826ms step_avg:61.25ms
step:1092/2330 train_time:66890ms step_avg:61.25ms
step:1093/2330 train_time:66950ms step_avg:61.25ms
step:1094/2330 train_time:67014ms step_avg:61.26ms
step:1095/2330 train_time:67073ms step_avg:61.25ms
step:1096/2330 train_time:67136ms step_avg:61.26ms
step:1097/2330 train_time:67196ms step_avg:61.25ms
step:1098/2330 train_time:67260ms step_avg:61.26ms
step:1099/2330 train_time:67320ms step_avg:61.26ms
step:1100/2330 train_time:67384ms step_avg:61.26ms
step:1101/2330 train_time:67445ms step_avg:61.26ms
step:1102/2330 train_time:67509ms step_avg:61.26ms
step:1103/2330 train_time:67570ms step_avg:61.26ms
step:1104/2330 train_time:67632ms step_avg:61.26ms
step:1105/2330 train_time:67693ms step_avg:61.26ms
step:1106/2330 train_time:67757ms step_avg:61.26ms
step:1107/2330 train_time:67817ms step_avg:61.26ms
step:1108/2330 train_time:67880ms step_avg:61.26ms
step:1109/2330 train_time:67941ms step_avg:61.26ms
step:1110/2330 train_time:68004ms step_avg:61.27ms
step:1111/2330 train_time:68065ms step_avg:61.26ms
step:1112/2330 train_time:68129ms step_avg:61.27ms
step:1113/2330 train_time:68189ms step_avg:61.27ms
step:1114/2330 train_time:68252ms step_avg:61.27ms
step:1115/2330 train_time:68312ms step_avg:61.27ms
step:1116/2330 train_time:68374ms step_avg:61.27ms
step:1117/2330 train_time:68434ms step_avg:61.27ms
step:1118/2330 train_time:68497ms step_avg:61.27ms
step:1119/2330 train_time:68559ms step_avg:61.27ms
step:1120/2330 train_time:68621ms step_avg:61.27ms
step:1121/2330 train_time:68682ms step_avg:61.27ms
step:1122/2330 train_time:68746ms step_avg:61.27ms
step:1123/2330 train_time:68807ms step_avg:61.27ms
step:1124/2330 train_time:68870ms step_avg:61.27ms
step:1125/2330 train_time:68930ms step_avg:61.27ms
step:1126/2330 train_time:68993ms step_avg:61.27ms
step:1127/2330 train_time:69053ms step_avg:61.27ms
step:1128/2330 train_time:69116ms step_avg:61.27ms
step:1129/2330 train_time:69177ms step_avg:61.27ms
step:1130/2330 train_time:69240ms step_avg:61.27ms
step:1131/2330 train_time:69300ms step_avg:61.27ms
step:1132/2330 train_time:69364ms step_avg:61.28ms
step:1133/2330 train_time:69424ms step_avg:61.27ms
step:1134/2330 train_time:69487ms step_avg:61.28ms
step:1135/2330 train_time:69549ms step_avg:61.28ms
step:1136/2330 train_time:69612ms step_avg:61.28ms
step:1137/2330 train_time:69671ms step_avg:61.28ms
step:1138/2330 train_time:69734ms step_avg:61.28ms
step:1139/2330 train_time:69794ms step_avg:61.28ms
step:1140/2330 train_time:69858ms step_avg:61.28ms
step:1141/2330 train_time:69918ms step_avg:61.28ms
step:1142/2330 train_time:69981ms step_avg:61.28ms
step:1143/2330 train_time:70042ms step_avg:61.28ms
step:1144/2330 train_time:70105ms step_avg:61.28ms
step:1145/2330 train_time:70166ms step_avg:61.28ms
step:1146/2330 train_time:70228ms step_avg:61.28ms
step:1147/2330 train_time:70290ms step_avg:61.28ms
step:1148/2330 train_time:70354ms step_avg:61.28ms
step:1149/2330 train_time:70413ms step_avg:61.28ms
step:1150/2330 train_time:70476ms step_avg:61.28ms
step:1151/2330 train_time:70535ms step_avg:61.28ms
step:1152/2330 train_time:70599ms step_avg:61.28ms
step:1153/2330 train_time:70660ms step_avg:61.28ms
step:1154/2330 train_time:70724ms step_avg:61.29ms
step:1155/2330 train_time:70785ms step_avg:61.29ms
step:1156/2330 train_time:70849ms step_avg:61.29ms
step:1157/2330 train_time:70910ms step_avg:61.29ms
step:1158/2330 train_time:70973ms step_avg:61.29ms
step:1159/2330 train_time:71033ms step_avg:61.29ms
step:1160/2330 train_time:71095ms step_avg:61.29ms
step:1161/2330 train_time:71155ms step_avg:61.29ms
step:1162/2330 train_time:71219ms step_avg:61.29ms
step:1163/2330 train_time:71278ms step_avg:61.29ms
step:1164/2330 train_time:71343ms step_avg:61.29ms
step:1165/2330 train_time:71403ms step_avg:61.29ms
step:1166/2330 train_time:71467ms step_avg:61.29ms
step:1167/2330 train_time:71528ms step_avg:61.29ms
step:1168/2330 train_time:71591ms step_avg:61.29ms
step:1169/2330 train_time:71652ms step_avg:61.29ms
step:1170/2330 train_time:71714ms step_avg:61.29ms
step:1171/2330 train_time:71774ms step_avg:61.29ms
step:1172/2330 train_time:71838ms step_avg:61.29ms
step:1173/2330 train_time:71898ms step_avg:61.29ms
step:1174/2330 train_time:71961ms step_avg:61.30ms
step:1175/2330 train_time:72022ms step_avg:61.30ms
step:1176/2330 train_time:72086ms step_avg:61.30ms
step:1177/2330 train_time:72147ms step_avg:61.30ms
step:1178/2330 train_time:72210ms step_avg:61.30ms
step:1179/2330 train_time:72270ms step_avg:61.30ms
step:1180/2330 train_time:72332ms step_avg:61.30ms
step:1181/2330 train_time:72393ms step_avg:61.30ms
step:1182/2330 train_time:72456ms step_avg:61.30ms
step:1183/2330 train_time:72516ms step_avg:61.30ms
step:1184/2330 train_time:72580ms step_avg:61.30ms
step:1185/2330 train_time:72640ms step_avg:61.30ms
step:1186/2330 train_time:72704ms step_avg:61.30ms
step:1187/2330 train_time:72764ms step_avg:61.30ms
step:1188/2330 train_time:72829ms step_avg:61.30ms
step:1189/2330 train_time:72889ms step_avg:61.30ms
step:1190/2330 train_time:72953ms step_avg:61.31ms
step:1191/2330 train_time:73012ms step_avg:61.30ms
step:1192/2330 train_time:73076ms step_avg:61.31ms
step:1193/2330 train_time:73136ms step_avg:61.30ms
step:1194/2330 train_time:73198ms step_avg:61.31ms
step:1195/2330 train_time:73258ms step_avg:61.30ms
step:1196/2330 train_time:73322ms step_avg:61.31ms
step:1197/2330 train_time:73382ms step_avg:61.31ms
step:1198/2330 train_time:73447ms step_avg:61.31ms
step:1199/2330 train_time:73507ms step_avg:61.31ms
step:1200/2330 train_time:73570ms step_avg:61.31ms
step:1201/2330 train_time:73631ms step_avg:61.31ms
step:1202/2330 train_time:73693ms step_avg:61.31ms
step:1203/2330 train_time:73754ms step_avg:61.31ms
step:1204/2330 train_time:73816ms step_avg:61.31ms
step:1205/2330 train_time:73877ms step_avg:61.31ms
step:1206/2330 train_time:73940ms step_avg:61.31ms
step:1207/2330 train_time:74000ms step_avg:61.31ms
step:1208/2330 train_time:74064ms step_avg:61.31ms
step:1209/2330 train_time:74125ms step_avg:61.31ms
step:1210/2330 train_time:74189ms step_avg:61.31ms
step:1211/2330 train_time:74250ms step_avg:61.31ms
step:1212/2330 train_time:74312ms step_avg:61.31ms
step:1213/2330 train_time:74372ms step_avg:61.31ms
step:1214/2330 train_time:74435ms step_avg:61.31ms
step:1215/2330 train_time:74495ms step_avg:61.31ms
step:1216/2330 train_time:74559ms step_avg:61.31ms
step:1217/2330 train_time:74619ms step_avg:61.31ms
step:1218/2330 train_time:74683ms step_avg:61.32ms
step:1219/2330 train_time:74744ms step_avg:61.32ms
step:1220/2330 train_time:74808ms step_avg:61.32ms
step:1221/2330 train_time:74868ms step_avg:61.32ms
step:1222/2330 train_time:74931ms step_avg:61.32ms
step:1223/2330 train_time:74991ms step_avg:61.32ms
step:1224/2330 train_time:75055ms step_avg:61.32ms
step:1225/2330 train_time:75114ms step_avg:61.32ms
step:1226/2330 train_time:75178ms step_avg:61.32ms
step:1227/2330 train_time:75238ms step_avg:61.32ms
step:1228/2330 train_time:75302ms step_avg:61.32ms
step:1229/2330 train_time:75362ms step_avg:61.32ms
step:1230/2330 train_time:75425ms step_avg:61.32ms
step:1231/2330 train_time:75487ms step_avg:61.32ms
step:1232/2330 train_time:75550ms step_avg:61.32ms
step:1233/2330 train_time:75610ms step_avg:61.32ms
step:1234/2330 train_time:75673ms step_avg:61.32ms
step:1235/2330 train_time:75733ms step_avg:61.32ms
step:1236/2330 train_time:75797ms step_avg:61.32ms
step:1237/2330 train_time:75858ms step_avg:61.32ms
step:1238/2330 train_time:75921ms step_avg:61.33ms
step:1239/2330 train_time:75981ms step_avg:61.32ms
step:1240/2330 train_time:76045ms step_avg:61.33ms
step:1241/2330 train_time:76105ms step_avg:61.33ms
step:1242/2330 train_time:76170ms step_avg:61.33ms
step:1243/2330 train_time:76232ms step_avg:61.33ms
step:1244/2330 train_time:76294ms step_avg:61.33ms
step:1245/2330 train_time:76354ms step_avg:61.33ms
step:1246/2330 train_time:76417ms step_avg:61.33ms
step:1247/2330 train_time:76477ms step_avg:61.33ms
step:1248/2330 train_time:76540ms step_avg:61.33ms
step:1249/2330 train_time:76601ms step_avg:61.33ms
step:1250/2330 train_time:76665ms step_avg:61.33ms
step:1250/2330 val_loss:4.5630 train_time:76739ms step_avg:61.39ms
step:1251/2330 train_time:76761ms step_avg:61.36ms
step:1252/2330 train_time:76792ms step_avg:61.34ms
step:1253/2330 train_time:76857ms step_avg:61.34ms
step:1254/2330 train_time:76922ms step_avg:61.34ms
step:1255/2330 train_time:76982ms step_avg:61.34ms
step:1256/2330 train_time:77045ms step_avg:61.34ms
step:1257/2330 train_time:77106ms step_avg:61.34ms
step:1258/2330 train_time:77170ms step_avg:61.34ms
step:1259/2330 train_time:77229ms step_avg:61.34ms
step:1260/2330 train_time:77292ms step_avg:61.34ms
step:1261/2330 train_time:77351ms step_avg:61.34ms
step:1262/2330 train_time:77413ms step_avg:61.34ms
step:1263/2330 train_time:77473ms step_avg:61.34ms
step:1264/2330 train_time:77535ms step_avg:61.34ms
step:1265/2330 train_time:77594ms step_avg:61.34ms
step:1266/2330 train_time:77656ms step_avg:61.34ms
step:1267/2330 train_time:77717ms step_avg:61.34ms
step:1268/2330 train_time:77783ms step_avg:61.34ms
step:1269/2330 train_time:77844ms step_avg:61.34ms
step:1270/2330 train_time:77909ms step_avg:61.35ms
step:1271/2330 train_time:77971ms step_avg:61.35ms
step:1272/2330 train_time:78035ms step_avg:61.35ms
step:1273/2330 train_time:78094ms step_avg:61.35ms
step:1274/2330 train_time:78157ms step_avg:61.35ms
step:1275/2330 train_time:78218ms step_avg:61.35ms
step:1276/2330 train_time:78280ms step_avg:61.35ms
step:1277/2330 train_time:78341ms step_avg:61.35ms
step:1278/2330 train_time:78406ms step_avg:61.35ms
step:1279/2330 train_time:78466ms step_avg:61.35ms
step:1280/2330 train_time:78529ms step_avg:61.35ms
step:1281/2330 train_time:78589ms step_avg:61.35ms
step:1282/2330 train_time:78652ms step_avg:61.35ms
step:1283/2330 train_time:78713ms step_avg:61.35ms
step:1284/2330 train_time:78776ms step_avg:61.35ms
step:1285/2330 train_time:78838ms step_avg:61.35ms
step:1286/2330 train_time:78902ms step_avg:61.35ms
step:1287/2330 train_time:78964ms step_avg:61.36ms
step:1288/2330 train_time:79029ms step_avg:61.36ms
step:1289/2330 train_time:79090ms step_avg:61.36ms
step:1290/2330 train_time:79153ms step_avg:61.36ms
step:1291/2330 train_time:79214ms step_avg:61.36ms
step:1292/2330 train_time:79276ms step_avg:61.36ms
step:1293/2330 train_time:79338ms step_avg:61.36ms
step:1294/2330 train_time:79401ms step_avg:61.36ms
step:1295/2330 train_time:79461ms step_avg:61.36ms
step:1296/2330 train_time:79525ms step_avg:61.36ms
step:1297/2330 train_time:79585ms step_avg:61.36ms
step:1298/2330 train_time:79649ms step_avg:61.36ms
step:1299/2330 train_time:79709ms step_avg:61.36ms
step:1300/2330 train_time:79773ms step_avg:61.36ms
step:1301/2330 train_time:79833ms step_avg:61.36ms
step:1302/2330 train_time:79895ms step_avg:61.36ms
step:1303/2330 train_time:79956ms step_avg:61.36ms
step:1304/2330 train_time:80020ms step_avg:61.36ms
step:1305/2330 train_time:80080ms step_avg:61.36ms
step:1306/2330 train_time:80144ms step_avg:61.37ms
step:1307/2330 train_time:80204ms step_avg:61.37ms
step:1308/2330 train_time:80270ms step_avg:61.37ms
step:1309/2330 train_time:80329ms step_avg:61.37ms
step:1310/2330 train_time:80392ms step_avg:61.37ms
step:1311/2330 train_time:80452ms step_avg:61.37ms
step:1312/2330 train_time:80516ms step_avg:61.37ms
step:1313/2330 train_time:80577ms step_avg:61.37ms
step:1314/2330 train_time:80640ms step_avg:61.37ms
step:1315/2330 train_time:80700ms step_avg:61.37ms
step:1316/2330 train_time:80764ms step_avg:61.37ms
step:1317/2330 train_time:80824ms step_avg:61.37ms
step:1318/2330 train_time:80888ms step_avg:61.37ms
step:1319/2330 train_time:80948ms step_avg:61.37ms
step:1320/2330 train_time:81012ms step_avg:61.37ms
step:1321/2330 train_time:81073ms step_avg:61.37ms
step:1322/2330 train_time:81135ms step_avg:61.37ms
step:1323/2330 train_time:81196ms step_avg:61.37ms
step:1324/2330 train_time:81260ms step_avg:61.37ms
step:1325/2330 train_time:81320ms step_avg:61.37ms
step:1326/2330 train_time:81384ms step_avg:61.38ms
step:1327/2330 train_time:81444ms step_avg:61.37ms
step:1328/2330 train_time:81508ms step_avg:61.38ms
step:1329/2330 train_time:81568ms step_avg:61.38ms
step:1330/2330 train_time:81631ms step_avg:61.38ms
step:1331/2330 train_time:81691ms step_avg:61.38ms
step:1332/2330 train_time:81754ms step_avg:61.38ms
step:1333/2330 train_time:81814ms step_avg:61.38ms
step:1334/2330 train_time:81877ms step_avg:61.38ms
step:1335/2330 train_time:81937ms step_avg:61.38ms
step:1336/2330 train_time:82001ms step_avg:61.38ms
step:1337/2330 train_time:82062ms step_avg:61.38ms
step:1338/2330 train_time:82126ms step_avg:61.38ms
step:1339/2330 train_time:82188ms step_avg:61.38ms
step:1340/2330 train_time:82251ms step_avg:61.38ms
step:1341/2330 train_time:82311ms step_avg:61.38ms
step:1342/2330 train_time:82375ms step_avg:61.38ms
step:1343/2330 train_time:82434ms step_avg:61.38ms
step:1344/2330 train_time:82498ms step_avg:61.38ms
step:1345/2330 train_time:82557ms step_avg:61.38ms
step:1346/2330 train_time:82621ms step_avg:61.38ms
step:1347/2330 train_time:82681ms step_avg:61.38ms
step:1348/2330 train_time:82744ms step_avg:61.38ms
step:1349/2330 train_time:82806ms step_avg:61.38ms
step:1350/2330 train_time:82870ms step_avg:61.38ms
step:1351/2330 train_time:82929ms step_avg:61.38ms
step:1352/2330 train_time:82992ms step_avg:61.38ms
step:1353/2330 train_time:83053ms step_avg:61.38ms
step:1354/2330 train_time:83116ms step_avg:61.39ms
step:1355/2330 train_time:83178ms step_avg:61.39ms
step:1356/2330 train_time:83241ms step_avg:61.39ms
step:1357/2330 train_time:83301ms step_avg:61.39ms
step:1358/2330 train_time:83365ms step_avg:61.39ms
step:1359/2330 train_time:83427ms step_avg:61.39ms
step:1360/2330 train_time:83489ms step_avg:61.39ms
step:1361/2330 train_time:83550ms step_avg:61.39ms
step:1362/2330 train_time:83612ms step_avg:61.39ms
step:1363/2330 train_time:83672ms step_avg:61.39ms
step:1364/2330 train_time:83736ms step_avg:61.39ms
step:1365/2330 train_time:83796ms step_avg:61.39ms
step:1366/2330 train_time:83859ms step_avg:61.39ms
step:1367/2330 train_time:83919ms step_avg:61.39ms
step:1368/2330 train_time:83983ms step_avg:61.39ms
step:1369/2330 train_time:84042ms step_avg:61.39ms
step:1370/2330 train_time:84107ms step_avg:61.39ms
step:1371/2330 train_time:84168ms step_avg:61.39ms
step:1372/2330 train_time:84231ms step_avg:61.39ms
step:1373/2330 train_time:84291ms step_avg:61.39ms
step:1374/2330 train_time:84355ms step_avg:61.39ms
step:1375/2330 train_time:84415ms step_avg:61.39ms
step:1376/2330 train_time:84478ms step_avg:61.39ms
step:1377/2330 train_time:84538ms step_avg:61.39ms
step:1378/2330 train_time:84601ms step_avg:61.39ms
step:1379/2330 train_time:84662ms step_avg:61.39ms
step:1380/2330 train_time:84726ms step_avg:61.40ms
step:1381/2330 train_time:84787ms step_avg:61.40ms
step:1382/2330 train_time:84850ms step_avg:61.40ms
step:1383/2330 train_time:84910ms step_avg:61.40ms
step:1384/2330 train_time:84974ms step_avg:61.40ms
step:1385/2330 train_time:85033ms step_avg:61.40ms
step:1386/2330 train_time:85095ms step_avg:61.40ms
step:1387/2330 train_time:85156ms step_avg:61.40ms
step:1388/2330 train_time:85220ms step_avg:61.40ms
step:1389/2330 train_time:85282ms step_avg:61.40ms
step:1390/2330 train_time:85345ms step_avg:61.40ms
step:1391/2330 train_time:85406ms step_avg:61.40ms
step:1392/2330 train_time:85469ms step_avg:61.40ms
step:1393/2330 train_time:85529ms step_avg:61.40ms
step:1394/2330 train_time:85592ms step_avg:61.40ms
step:1395/2330 train_time:85651ms step_avg:61.40ms
step:1396/2330 train_time:85714ms step_avg:61.40ms
step:1397/2330 train_time:85775ms step_avg:61.40ms
step:1398/2330 train_time:85837ms step_avg:61.40ms
step:1399/2330 train_time:85897ms step_avg:61.40ms
step:1400/2330 train_time:85962ms step_avg:61.40ms
step:1401/2330 train_time:86022ms step_avg:61.40ms
step:1402/2330 train_time:86085ms step_avg:61.40ms
step:1403/2330 train_time:86147ms step_avg:61.40ms
step:1404/2330 train_time:86210ms step_avg:61.40ms
step:1405/2330 train_time:86271ms step_avg:61.40ms
step:1406/2330 train_time:86333ms step_avg:61.40ms
step:1407/2330 train_time:86392ms step_avg:61.40ms
step:1408/2330 train_time:86456ms step_avg:61.40ms
step:1409/2330 train_time:86515ms step_avg:61.40ms
step:1410/2330 train_time:86579ms step_avg:61.40ms
step:1411/2330 train_time:86638ms step_avg:61.40ms
step:1412/2330 train_time:86703ms step_avg:61.40ms
step:1413/2330 train_time:86764ms step_avg:61.40ms
step:1414/2330 train_time:86827ms step_avg:61.41ms
step:1415/2330 train_time:86888ms step_avg:61.40ms
step:1416/2330 train_time:86951ms step_avg:61.41ms
step:1417/2330 train_time:87012ms step_avg:61.41ms
step:1418/2330 train_time:87076ms step_avg:61.41ms
step:1419/2330 train_time:87136ms step_avg:61.41ms
step:1420/2330 train_time:87200ms step_avg:61.41ms
step:1421/2330 train_time:87260ms step_avg:61.41ms
step:1422/2330 train_time:87324ms step_avg:61.41ms
step:1423/2330 train_time:87384ms step_avg:61.41ms
step:1424/2330 train_time:87448ms step_avg:61.41ms
step:1425/2330 train_time:87507ms step_avg:61.41ms
step:1426/2330 train_time:87571ms step_avg:61.41ms
step:1427/2330 train_time:87630ms step_avg:61.41ms
step:1428/2330 train_time:87693ms step_avg:61.41ms
step:1429/2330 train_time:87753ms step_avg:61.41ms
step:1430/2330 train_time:87816ms step_avg:61.41ms
step:1431/2330 train_time:87877ms step_avg:61.41ms
step:1432/2330 train_time:87939ms step_avg:61.41ms
step:1433/2330 train_time:88000ms step_avg:61.41ms
step:1434/2330 train_time:88065ms step_avg:61.41ms
step:1435/2330 train_time:88126ms step_avg:61.41ms
step:1436/2330 train_time:88190ms step_avg:61.41ms
step:1437/2330 train_time:88250ms step_avg:61.41ms
step:1438/2330 train_time:88312ms step_avg:61.41ms
step:1439/2330 train_time:88373ms step_avg:61.41ms
step:1440/2330 train_time:88437ms step_avg:61.41ms
step:1441/2330 train_time:88497ms step_avg:61.41ms
step:1442/2330 train_time:88559ms step_avg:61.41ms
step:1443/2330 train_time:88620ms step_avg:61.41ms
step:1444/2330 train_time:88683ms step_avg:61.42ms
step:1445/2330 train_time:88744ms step_avg:61.41ms
step:1446/2330 train_time:88808ms step_avg:61.42ms
step:1447/2330 train_time:88869ms step_avg:61.42ms
step:1448/2330 train_time:88932ms step_avg:61.42ms
step:1449/2330 train_time:88992ms step_avg:61.42ms
step:1450/2330 train_time:89055ms step_avg:61.42ms
step:1451/2330 train_time:89114ms step_avg:61.42ms
step:1452/2330 train_time:89178ms step_avg:61.42ms
step:1453/2330 train_time:89238ms step_avg:61.42ms
step:1454/2330 train_time:89303ms step_avg:61.42ms
step:1455/2330 train_time:89364ms step_avg:61.42ms
step:1456/2330 train_time:89427ms step_avg:61.42ms
step:1457/2330 train_time:89488ms step_avg:61.42ms
step:1458/2330 train_time:89552ms step_avg:61.42ms
step:1459/2330 train_time:89611ms step_avg:61.42ms
step:1460/2330 train_time:89675ms step_avg:61.42ms
step:1461/2330 train_time:89735ms step_avg:61.42ms
step:1462/2330 train_time:89798ms step_avg:61.42ms
step:1463/2330 train_time:89858ms step_avg:61.42ms
step:1464/2330 train_time:89921ms step_avg:61.42ms
step:1465/2330 train_time:89982ms step_avg:61.42ms
step:1466/2330 train_time:90046ms step_avg:61.42ms
step:1467/2330 train_time:90107ms step_avg:61.42ms
step:1468/2330 train_time:90171ms step_avg:61.42ms
step:1469/2330 train_time:90231ms step_avg:61.42ms
step:1470/2330 train_time:90294ms step_avg:61.42ms
step:1471/2330 train_time:90353ms step_avg:61.42ms
step:1472/2330 train_time:90417ms step_avg:61.42ms
step:1473/2330 train_time:90477ms step_avg:61.42ms
step:1474/2330 train_time:90540ms step_avg:61.42ms
step:1475/2330 train_time:90601ms step_avg:61.42ms
step:1476/2330 train_time:90666ms step_avg:61.43ms
step:1477/2330 train_time:90726ms step_avg:61.43ms
step:1478/2330 train_time:90789ms step_avg:61.43ms
step:1479/2330 train_time:90849ms step_avg:61.43ms
step:1480/2330 train_time:90912ms step_avg:61.43ms
step:1481/2330 train_time:90973ms step_avg:61.43ms
step:1482/2330 train_time:91035ms step_avg:61.43ms
step:1483/2330 train_time:91096ms step_avg:61.43ms
step:1484/2330 train_time:91159ms step_avg:61.43ms
step:1485/2330 train_time:91218ms step_avg:61.43ms
step:1486/2330 train_time:91282ms step_avg:61.43ms
step:1487/2330 train_time:91343ms step_avg:61.43ms
step:1488/2330 train_time:91408ms step_avg:61.43ms
step:1489/2330 train_time:91469ms step_avg:61.43ms
step:1490/2330 train_time:91531ms step_avg:61.43ms
step:1491/2330 train_time:91592ms step_avg:61.43ms
step:1492/2330 train_time:91655ms step_avg:61.43ms
step:1493/2330 train_time:91716ms step_avg:61.43ms
step:1494/2330 train_time:91779ms step_avg:61.43ms
step:1495/2330 train_time:91839ms step_avg:61.43ms
step:1496/2330 train_time:91902ms step_avg:61.43ms
step:1497/2330 train_time:91964ms step_avg:61.43ms
step:1498/2330 train_time:92027ms step_avg:61.43ms
step:1499/2330 train_time:92088ms step_avg:61.43ms
step:1500/2330 train_time:92151ms step_avg:61.43ms
step:1500/2330 val_loss:4.4082 train_time:92224ms step_avg:61.48ms
step:1501/2330 train_time:92246ms step_avg:61.46ms
step:1502/2330 train_time:92276ms step_avg:61.44ms
step:1503/2330 train_time:92337ms step_avg:61.44ms
step:1504/2330 train_time:92405ms step_avg:61.44ms
step:1505/2330 train_time:92465ms step_avg:61.44ms
step:1506/2330 train_time:92529ms step_avg:61.44ms
step:1507/2330 train_time:92588ms step_avg:61.44ms
step:1508/2330 train_time:92651ms step_avg:61.44ms
step:1509/2330 train_time:92710ms step_avg:61.44ms
step:1510/2330 train_time:92773ms step_avg:61.44ms
step:1511/2330 train_time:92832ms step_avg:61.44ms
step:1512/2330 train_time:92895ms step_avg:61.44ms
step:1513/2330 train_time:92955ms step_avg:61.44ms
step:1514/2330 train_time:93018ms step_avg:61.44ms
step:1515/2330 train_time:93077ms step_avg:61.44ms
step:1516/2330 train_time:93140ms step_avg:61.44ms
step:1517/2330 train_time:93201ms step_avg:61.44ms
step:1518/2330 train_time:93267ms step_avg:61.44ms
step:1519/2330 train_time:93329ms step_avg:61.44ms
step:1520/2330 train_time:93393ms step_avg:61.44ms
step:1521/2330 train_time:93453ms step_avg:61.44ms
step:1522/2330 train_time:93517ms step_avg:61.44ms
step:1523/2330 train_time:93578ms step_avg:61.44ms
step:1524/2330 train_time:93642ms step_avg:61.44ms
step:1525/2330 train_time:93702ms step_avg:61.44ms
step:1526/2330 train_time:93765ms step_avg:61.45ms
step:1527/2330 train_time:93825ms step_avg:61.44ms
step:1528/2330 train_time:93890ms step_avg:61.45ms
step:1529/2330 train_time:93951ms step_avg:61.45ms
step:1530/2330 train_time:94013ms step_avg:61.45ms
step:1531/2330 train_time:94073ms step_avg:61.45ms
step:1532/2330 train_time:94136ms step_avg:61.45ms
step:1533/2330 train_time:94197ms step_avg:61.45ms
step:1534/2330 train_time:94261ms step_avg:61.45ms
step:1535/2330 train_time:94323ms step_avg:61.45ms
step:1536/2330 train_time:94389ms step_avg:61.45ms
step:1537/2330 train_time:94450ms step_avg:61.45ms
step:1538/2330 train_time:94513ms step_avg:61.45ms
step:1539/2330 train_time:94574ms step_avg:61.45ms
step:1540/2330 train_time:94638ms step_avg:61.45ms
step:1541/2330 train_time:94698ms step_avg:61.45ms
step:1542/2330 train_time:94762ms step_avg:61.45ms
step:1543/2330 train_time:94823ms step_avg:61.45ms
step:1544/2330 train_time:94887ms step_avg:61.46ms
step:1545/2330 train_time:94948ms step_avg:61.46ms
step:1546/2330 train_time:95011ms step_avg:61.46ms
step:1547/2330 train_time:95071ms step_avg:61.46ms
step:1548/2330 train_time:95134ms step_avg:61.46ms
step:1549/2330 train_time:95195ms step_avg:61.46ms
step:1550/2330 train_time:95261ms step_avg:61.46ms
step:1551/2330 train_time:95321ms step_avg:61.46ms
step:1552/2330 train_time:95387ms step_avg:61.46ms
step:1553/2330 train_time:95449ms step_avg:61.46ms
step:1554/2330 train_time:95512ms step_avg:61.46ms
step:1555/2330 train_time:95572ms step_avg:61.46ms
step:1556/2330 train_time:95636ms step_avg:61.46ms
step:1557/2330 train_time:95698ms step_avg:61.46ms
step:1558/2330 train_time:95761ms step_avg:61.46ms
step:1559/2330 train_time:95821ms step_avg:61.46ms
step:1560/2330 train_time:95886ms step_avg:61.47ms
step:1561/2330 train_time:95947ms step_avg:61.46ms
step:1562/2330 train_time:96010ms step_avg:61.47ms
step:1563/2330 train_time:96071ms step_avg:61.47ms
step:1564/2330 train_time:96134ms step_avg:61.47ms
step:1565/2330 train_time:96195ms step_avg:61.47ms
step:1566/2330 train_time:96259ms step_avg:61.47ms
step:1567/2330 train_time:96319ms step_avg:61.47ms
step:1568/2330 train_time:96386ms step_avg:61.47ms
step:1569/2330 train_time:96447ms step_avg:61.47ms
step:1570/2330 train_time:96510ms step_avg:61.47ms
step:1571/2330 train_time:96571ms step_avg:61.47ms
step:1572/2330 train_time:96635ms step_avg:61.47ms
step:1573/2330 train_time:96696ms step_avg:61.47ms
step:1574/2330 train_time:96759ms step_avg:61.47ms
step:1575/2330 train_time:96820ms step_avg:61.47ms
step:1576/2330 train_time:96885ms step_avg:61.48ms
step:1577/2330 train_time:96946ms step_avg:61.48ms
step:1578/2330 train_time:97010ms step_avg:61.48ms
step:1579/2330 train_time:97071ms step_avg:61.48ms
step:1580/2330 train_time:97135ms step_avg:61.48ms
step:1581/2330 train_time:97196ms step_avg:61.48ms
step:1582/2330 train_time:97260ms step_avg:61.48ms
step:1583/2330 train_time:97321ms step_avg:61.48ms
step:1584/2330 train_time:97386ms step_avg:61.48ms
step:1585/2330 train_time:97447ms step_avg:61.48ms
step:1586/2330 train_time:97510ms step_avg:61.48ms
step:1587/2330 train_time:97572ms step_avg:61.48ms
step:1588/2330 train_time:97635ms step_avg:61.48ms
step:1589/2330 train_time:97697ms step_avg:61.48ms
step:1590/2330 train_time:97761ms step_avg:61.49ms
step:1591/2330 train_time:97822ms step_avg:61.48ms
step:1592/2330 train_time:97886ms step_avg:61.49ms
step:1593/2330 train_time:97946ms step_avg:61.49ms
step:1594/2330 train_time:98010ms step_avg:61.49ms
step:1595/2330 train_time:98071ms step_avg:61.49ms
step:1596/2330 train_time:98134ms step_avg:61.49ms
step:1597/2330 train_time:98196ms step_avg:61.49ms
step:1598/2330 train_time:98259ms step_avg:61.49ms
step:1599/2330 train_time:98320ms step_avg:61.49ms
step:1600/2330 train_time:98384ms step_avg:61.49ms
step:1601/2330 train_time:98445ms step_avg:61.49ms
step:1602/2330 train_time:98510ms step_avg:61.49ms
step:1603/2330 train_time:98571ms step_avg:61.49ms
step:1604/2330 train_time:98634ms step_avg:61.49ms
step:1605/2330 train_time:98694ms step_avg:61.49ms
step:1606/2330 train_time:98758ms step_avg:61.49ms
step:1607/2330 train_time:98818ms step_avg:61.49ms
step:1608/2330 train_time:98882ms step_avg:61.49ms
step:1609/2330 train_time:98943ms step_avg:61.49ms
step:1610/2330 train_time:99007ms step_avg:61.49ms
step:1611/2330 train_time:99067ms step_avg:61.49ms
step:1612/2330 train_time:99131ms step_avg:61.50ms
step:1613/2330 train_time:99192ms step_avg:61.50ms
step:1614/2330 train_time:99256ms step_avg:61.50ms
step:1615/2330 train_time:99317ms step_avg:61.50ms
step:1616/2330 train_time:99380ms step_avg:61.50ms
step:1617/2330 train_time:99441ms step_avg:61.50ms
step:1618/2330 train_time:99506ms step_avg:61.50ms
step:1619/2330 train_time:99568ms step_avg:61.50ms
step:1620/2330 train_time:99631ms step_avg:61.50ms
step:1621/2330 train_time:99693ms step_avg:61.50ms
step:1622/2330 train_time:99757ms step_avg:61.50ms
step:1623/2330 train_time:99816ms step_avg:61.50ms
step:1624/2330 train_time:99880ms step_avg:61.50ms
step:1625/2330 train_time:99941ms step_avg:61.50ms
step:1626/2330 train_time:100006ms step_avg:61.50ms
step:1627/2330 train_time:100067ms step_avg:61.50ms
step:1628/2330 train_time:100131ms step_avg:61.51ms
step:1629/2330 train_time:100192ms step_avg:61.51ms
step:1630/2330 train_time:100256ms step_avg:61.51ms
step:1631/2330 train_time:100316ms step_avg:61.51ms
step:1632/2330 train_time:100380ms step_avg:61.51ms
step:1633/2330 train_time:100441ms step_avg:61.51ms
step:1634/2330 train_time:100506ms step_avg:61.51ms
step:1635/2330 train_time:100567ms step_avg:61.51ms
step:1636/2330 train_time:100631ms step_avg:61.51ms
step:1637/2330 train_time:100692ms step_avg:61.51ms
step:1638/2330 train_time:100756ms step_avg:61.51ms
step:1639/2330 train_time:100817ms step_avg:61.51ms
step:1640/2330 train_time:100880ms step_avg:61.51ms
step:1641/2330 train_time:100941ms step_avg:61.51ms
step:1642/2330 train_time:101006ms step_avg:61.51ms
step:1643/2330 train_time:101068ms step_avg:61.51ms
step:1644/2330 train_time:101133ms step_avg:61.52ms
step:1645/2330 train_time:101193ms step_avg:61.52ms
step:1646/2330 train_time:101258ms step_avg:61.52ms
step:1647/2330 train_time:101318ms step_avg:61.52ms
step:1648/2330 train_time:101382ms step_avg:61.52ms
step:1649/2330 train_time:101443ms step_avg:61.52ms
step:1650/2330 train_time:101507ms step_avg:61.52ms
step:1651/2330 train_time:101569ms step_avg:61.52ms
step:1652/2330 train_time:101633ms step_avg:61.52ms
step:1653/2330 train_time:101693ms step_avg:61.52ms
step:1654/2330 train_time:101757ms step_avg:61.52ms
step:1655/2330 train_time:101817ms step_avg:61.52ms
step:1656/2330 train_time:101881ms step_avg:61.52ms
step:1657/2330 train_time:101942ms step_avg:61.52ms
step:1658/2330 train_time:102005ms step_avg:61.52ms
step:1659/2330 train_time:102067ms step_avg:61.52ms
step:1660/2330 train_time:102131ms step_avg:61.52ms
step:1661/2330 train_time:102192ms step_avg:61.52ms
step:1662/2330 train_time:102255ms step_avg:61.53ms
step:1663/2330 train_time:102315ms step_avg:61.52ms
step:1664/2330 train_time:102379ms step_avg:61.53ms
step:1665/2330 train_time:102439ms step_avg:61.53ms
step:1666/2330 train_time:102503ms step_avg:61.53ms
step:1667/2330 train_time:102564ms step_avg:61.53ms
step:1668/2330 train_time:102629ms step_avg:61.53ms
step:1669/2330 train_time:102690ms step_avg:61.53ms
step:1670/2330 train_time:102752ms step_avg:61.53ms
step:1671/2330 train_time:102813ms step_avg:61.53ms
step:1672/2330 train_time:102877ms step_avg:61.53ms
step:1673/2330 train_time:102937ms step_avg:61.53ms
step:1674/2330 train_time:103001ms step_avg:61.53ms
step:1675/2330 train_time:103063ms step_avg:61.53ms
step:1676/2330 train_time:103128ms step_avg:61.53ms
step:1677/2330 train_time:103189ms step_avg:61.53ms
step:1678/2330 train_time:103252ms step_avg:61.53ms
step:1679/2330 train_time:103314ms step_avg:61.53ms
step:1680/2330 train_time:103378ms step_avg:61.53ms
step:1681/2330 train_time:103439ms step_avg:61.53ms
step:1682/2330 train_time:103503ms step_avg:61.54ms
step:1683/2330 train_time:103565ms step_avg:61.54ms
step:1684/2330 train_time:103629ms step_avg:61.54ms
step:1685/2330 train_time:103691ms step_avg:61.54ms
step:1686/2330 train_time:103753ms step_avg:61.54ms
step:1687/2330 train_time:103813ms step_avg:61.54ms
step:1688/2330 train_time:103878ms step_avg:61.54ms
step:1689/2330 train_time:103938ms step_avg:61.54ms
step:1690/2330 train_time:104002ms step_avg:61.54ms
step:1691/2330 train_time:104063ms step_avg:61.54ms
step:1692/2330 train_time:104128ms step_avg:61.54ms
step:1693/2330 train_time:104190ms step_avg:61.54ms
step:1694/2330 train_time:104253ms step_avg:61.54ms
step:1695/2330 train_time:104313ms step_avg:61.54ms
step:1696/2330 train_time:104377ms step_avg:61.54ms
step:1697/2330 train_time:104438ms step_avg:61.54ms
step:1698/2330 train_time:104502ms step_avg:61.54ms
step:1699/2330 train_time:104563ms step_avg:61.54ms
step:1700/2330 train_time:104628ms step_avg:61.55ms
step:1701/2330 train_time:104689ms step_avg:61.55ms
step:1702/2330 train_time:104752ms step_avg:61.55ms
step:1703/2330 train_time:104812ms step_avg:61.55ms
step:1704/2330 train_time:104876ms step_avg:61.55ms
step:1705/2330 train_time:104937ms step_avg:61.55ms
step:1706/2330 train_time:105001ms step_avg:61.55ms
step:1707/2330 train_time:105062ms step_avg:61.55ms
step:1708/2330 train_time:105127ms step_avg:61.55ms
step:1709/2330 train_time:105190ms step_avg:61.55ms
step:1710/2330 train_time:105254ms step_avg:61.55ms
step:1711/2330 train_time:105314ms step_avg:61.55ms
step:1712/2330 train_time:105378ms step_avg:61.55ms
step:1713/2330 train_time:105439ms step_avg:61.55ms
step:1714/2330 train_time:105502ms step_avg:61.55ms
step:1715/2330 train_time:105565ms step_avg:61.55ms
step:1716/2330 train_time:105629ms step_avg:61.56ms
step:1717/2330 train_time:105690ms step_avg:61.55ms
step:1718/2330 train_time:105753ms step_avg:61.56ms
step:1719/2330 train_time:105814ms step_avg:61.56ms
step:1720/2330 train_time:105878ms step_avg:61.56ms
step:1721/2330 train_time:105937ms step_avg:61.56ms
step:1722/2330 train_time:106001ms step_avg:61.56ms
step:1723/2330 train_time:106063ms step_avg:61.56ms
step:1724/2330 train_time:106128ms step_avg:61.56ms
step:1725/2330 train_time:106190ms step_avg:61.56ms
step:1726/2330 train_time:106253ms step_avg:61.56ms
step:1727/2330 train_time:106314ms step_avg:61.56ms
step:1728/2330 train_time:106378ms step_avg:61.56ms
step:1729/2330 train_time:106439ms step_avg:61.56ms
step:1730/2330 train_time:106503ms step_avg:61.56ms
step:1731/2330 train_time:106564ms step_avg:61.56ms
step:1732/2330 train_time:106629ms step_avg:61.56ms
step:1733/2330 train_time:106691ms step_avg:61.56ms
step:1734/2330 train_time:106754ms step_avg:61.57ms
step:1735/2330 train_time:106814ms step_avg:61.56ms
step:1736/2330 train_time:106878ms step_avg:61.57ms
step:1737/2330 train_time:106938ms step_avg:61.57ms
step:1738/2330 train_time:107002ms step_avg:61.57ms
step:1739/2330 train_time:107064ms step_avg:61.57ms
step:1740/2330 train_time:107129ms step_avg:61.57ms
step:1741/2330 train_time:107191ms step_avg:61.57ms
step:1742/2330 train_time:107254ms step_avg:61.57ms
step:1743/2330 train_time:107314ms step_avg:61.57ms
step:1744/2330 train_time:107379ms step_avg:61.57ms
step:1745/2330 train_time:107440ms step_avg:61.57ms
step:1746/2330 train_time:107504ms step_avg:61.57ms
step:1747/2330 train_time:107565ms step_avg:61.57ms
step:1748/2330 train_time:107630ms step_avg:61.57ms
step:1749/2330 train_time:107691ms step_avg:61.57ms
step:1750/2330 train_time:107754ms step_avg:61.57ms
step:1750/2330 val_loss:4.2927 train_time:107827ms step_avg:61.62ms
step:1751/2330 train_time:107850ms step_avg:61.59ms
step:1752/2330 train_time:107879ms step_avg:61.57ms
step:1753/2330 train_time:107942ms step_avg:61.58ms
step:1754/2330 train_time:108012ms step_avg:61.58ms
step:1755/2330 train_time:108074ms step_avg:61.58ms
step:1756/2330 train_time:108137ms step_avg:61.58ms
step:1757/2330 train_time:108197ms step_avg:61.58ms
step:1758/2330 train_time:108260ms step_avg:61.58ms
step:1759/2330 train_time:108320ms step_avg:61.58ms
step:1760/2330 train_time:108383ms step_avg:61.58ms
step:1761/2330 train_time:108443ms step_avg:61.58ms
step:1762/2330 train_time:108507ms step_avg:61.58ms
step:1763/2330 train_time:108568ms step_avg:61.58ms
step:1764/2330 train_time:108631ms step_avg:61.58ms
step:1765/2330 train_time:108691ms step_avg:61.58ms
step:1766/2330 train_time:108755ms step_avg:61.58ms
step:1767/2330 train_time:108816ms step_avg:61.58ms
step:1768/2330 train_time:108882ms step_avg:61.58ms
step:1769/2330 train_time:108943ms step_avg:61.58ms
step:1770/2330 train_time:109008ms step_avg:61.59ms
step:1771/2330 train_time:109071ms step_avg:61.59ms
step:1772/2330 train_time:109134ms step_avg:61.59ms
step:1773/2330 train_time:109196ms step_avg:61.59ms
step:1774/2330 train_time:109258ms step_avg:61.59ms
step:1775/2330 train_time:109319ms step_avg:61.59ms
step:1776/2330 train_time:109382ms step_avg:61.59ms
step:1777/2330 train_time:109442ms step_avg:61.59ms
step:1778/2330 train_time:109505ms step_avg:61.59ms
step:1779/2330 train_time:109566ms step_avg:61.59ms
step:1780/2330 train_time:109630ms step_avg:61.59ms
step:1781/2330 train_time:109691ms step_avg:61.59ms
step:1782/2330 train_time:109754ms step_avg:61.59ms
step:1783/2330 train_time:109815ms step_avg:61.59ms
step:1784/2330 train_time:109879ms step_avg:61.59ms
step:1785/2330 train_time:109940ms step_avg:61.59ms
step:1786/2330 train_time:110005ms step_avg:61.59ms
step:1787/2330 train_time:110068ms step_avg:61.59ms
step:1788/2330 train_time:110132ms step_avg:61.60ms
step:1789/2330 train_time:110195ms step_avg:61.60ms
step:1790/2330 train_time:110257ms step_avg:61.60ms
step:1791/2330 train_time:110317ms step_avg:61.60ms
step:1792/2330 train_time:110380ms step_avg:61.60ms
step:1793/2330 train_time:110441ms step_avg:61.60ms
step:1794/2330 train_time:110504ms step_avg:61.60ms
step:1795/2330 train_time:110563ms step_avg:61.59ms
step:1796/2330 train_time:110627ms step_avg:61.60ms
step:1797/2330 train_time:110688ms step_avg:61.60ms
step:1798/2330 train_time:110752ms step_avg:61.60ms
step:1799/2330 train_time:110812ms step_avg:61.60ms
step:1800/2330 train_time:110876ms step_avg:61.60ms
step:1801/2330 train_time:110938ms step_avg:61.60ms
step:1802/2330 train_time:111002ms step_avg:61.60ms
step:1803/2330 train_time:111062ms step_avg:61.60ms
step:1804/2330 train_time:111126ms step_avg:61.60ms
step:1805/2330 train_time:111188ms step_avg:61.60ms
step:1806/2330 train_time:111252ms step_avg:61.60ms
step:1807/2330 train_time:111313ms step_avg:61.60ms
step:1808/2330 train_time:111377ms step_avg:61.60ms
step:1809/2330 train_time:111438ms step_avg:61.60ms
step:1810/2330 train_time:111502ms step_avg:61.60ms
step:1811/2330 train_time:111561ms step_avg:61.60ms
step:1812/2330 train_time:111625ms step_avg:61.60ms
step:1813/2330 train_time:111685ms step_avg:61.60ms
step:1814/2330 train_time:111749ms step_avg:61.60ms
step:1815/2330 train_time:111810ms step_avg:61.60ms
step:1816/2330 train_time:111874ms step_avg:61.60ms
step:1817/2330 train_time:111936ms step_avg:61.60ms
step:1818/2330 train_time:111999ms step_avg:61.61ms
step:1819/2330 train_time:112060ms step_avg:61.61ms
step:1820/2330 train_time:112124ms step_avg:61.61ms
step:1821/2330 train_time:112186ms step_avg:61.61ms
step:1822/2330 train_time:112250ms step_avg:61.61ms
step:1823/2330 train_time:112312ms step_avg:61.61ms
step:1824/2330 train_time:112375ms step_avg:61.61ms
step:1825/2330 train_time:112436ms step_avg:61.61ms
step:1826/2330 train_time:112499ms step_avg:61.61ms
step:1827/2330 train_time:112559ms step_avg:61.61ms
step:1828/2330 train_time:112623ms step_avg:61.61ms
step:1829/2330 train_time:112682ms step_avg:61.61ms
step:1830/2330 train_time:112746ms step_avg:61.61ms
step:1831/2330 train_time:112808ms step_avg:61.61ms
step:1832/2330 train_time:112872ms step_avg:61.61ms
step:1833/2330 train_time:112933ms step_avg:61.61ms
step:1834/2330 train_time:112996ms step_avg:61.61ms
step:1835/2330 train_time:113057ms step_avg:61.61ms
step:1836/2330 train_time:113121ms step_avg:61.61ms
step:1837/2330 train_time:113181ms step_avg:61.61ms
step:1838/2330 train_time:113245ms step_avg:61.61ms
step:1839/2330 train_time:113306ms step_avg:61.61ms
step:1840/2330 train_time:113371ms step_avg:61.61ms
step:1841/2330 train_time:113433ms step_avg:61.61ms
step:1842/2330 train_time:113497ms step_avg:61.62ms
step:1843/2330 train_time:113557ms step_avg:61.62ms
step:1844/2330 train_time:113621ms step_avg:61.62ms
step:1845/2330 train_time:113681ms step_avg:61.62ms
step:1846/2330 train_time:113745ms step_avg:61.62ms
step:1847/2330 train_time:113806ms step_avg:61.62ms
step:1848/2330 train_time:113870ms step_avg:61.62ms
step:1849/2330 train_time:113932ms step_avg:61.62ms
step:1850/2330 train_time:113997ms step_avg:61.62ms
step:1851/2330 train_time:114057ms step_avg:61.62ms
step:1852/2330 train_time:114121ms step_avg:61.62ms
step:1853/2330 train_time:114182ms step_avg:61.62ms
step:1854/2330 train_time:114247ms step_avg:61.62ms
step:1855/2330 train_time:114308ms step_avg:61.62ms
step:1856/2330 train_time:114374ms step_avg:61.62ms
step:1857/2330 train_time:114435ms step_avg:61.62ms
step:1858/2330 train_time:114500ms step_avg:61.63ms
step:1859/2330 train_time:114559ms step_avg:61.62ms
step:1860/2330 train_time:114623ms step_avg:61.63ms
step:1861/2330 train_time:114683ms step_avg:61.62ms
step:1862/2330 train_time:114746ms step_avg:61.63ms
step:1863/2330 train_time:114807ms step_avg:61.62ms
step:1864/2330 train_time:114871ms step_avg:61.63ms
step:1865/2330 train_time:114933ms step_avg:61.63ms
step:1866/2330 train_time:114998ms step_avg:61.63ms
step:1867/2330 train_time:115058ms step_avg:61.63ms
step:1868/2330 train_time:115121ms step_avg:61.63ms
step:1869/2330 train_time:115181ms step_avg:61.63ms
step:1870/2330 train_time:115245ms step_avg:61.63ms
step:1871/2330 train_time:115306ms step_avg:61.63ms
step:1872/2330 train_time:115371ms step_avg:61.63ms
step:1873/2330 train_time:115433ms step_avg:61.63ms
step:1874/2330 train_time:115497ms step_avg:61.63ms
step:1875/2330 train_time:115557ms step_avg:61.63ms
step:1876/2330 train_time:115620ms step_avg:61.63ms
step:1877/2330 train_time:115681ms step_avg:61.63ms
step:1878/2330 train_time:115744ms step_avg:61.63ms
step:1879/2330 train_time:115806ms step_avg:61.63ms
step:1880/2330 train_time:115869ms step_avg:61.63ms
step:1881/2330 train_time:115931ms step_avg:61.63ms
step:1882/2330 train_time:115997ms step_avg:61.64ms
step:1883/2330 train_time:116057ms step_avg:61.63ms
step:1884/2330 train_time:116120ms step_avg:61.63ms
step:1885/2330 train_time:116181ms step_avg:61.63ms
step:1886/2330 train_time:116245ms step_avg:61.64ms
step:1887/2330 train_time:116306ms step_avg:61.64ms
step:1888/2330 train_time:116371ms step_avg:61.64ms
step:1889/2330 train_time:116433ms step_avg:61.64ms
step:1890/2330 train_time:116498ms step_avg:61.64ms
step:1891/2330 train_time:116557ms step_avg:61.64ms
step:1892/2330 train_time:116621ms step_avg:61.64ms
step:1893/2330 train_time:116681ms step_avg:61.64ms
step:1894/2330 train_time:116744ms step_avg:61.64ms
step:1895/2330 train_time:116806ms step_avg:61.64ms
step:1896/2330 train_time:116870ms step_avg:61.64ms
step:1897/2330 train_time:116931ms step_avg:61.64ms
step:1898/2330 train_time:116996ms step_avg:61.64ms
step:1899/2330 train_time:117055ms step_avg:61.64ms
step:1900/2330 train_time:117119ms step_avg:61.64ms
step:1901/2330 train_time:117179ms step_avg:61.64ms
step:1902/2330 train_time:117243ms step_avg:61.64ms
step:1903/2330 train_time:117304ms step_avg:61.64ms
step:1904/2330 train_time:117367ms step_avg:61.64ms
step:1905/2330 train_time:117430ms step_avg:61.64ms
step:1906/2330 train_time:117495ms step_avg:61.64ms
step:1907/2330 train_time:117555ms step_avg:61.64ms
step:1908/2330 train_time:117618ms step_avg:61.64ms
step:1909/2330 train_time:117679ms step_avg:61.64ms
step:1910/2330 train_time:117743ms step_avg:61.65ms
step:1911/2330 train_time:117803ms step_avg:61.64ms
step:1912/2330 train_time:117867ms step_avg:61.65ms
step:1913/2330 train_time:117929ms step_avg:61.65ms
step:1914/2330 train_time:117994ms step_avg:61.65ms
step:1915/2330 train_time:118054ms step_avg:61.65ms
step:1916/2330 train_time:118117ms step_avg:61.65ms
step:1917/2330 train_time:118178ms step_avg:61.65ms
step:1918/2330 train_time:118241ms step_avg:61.65ms
step:1919/2330 train_time:118301ms step_avg:61.65ms
step:1920/2330 train_time:118366ms step_avg:61.65ms
step:1921/2330 train_time:118428ms step_avg:61.65ms
step:1922/2330 train_time:118492ms step_avg:61.65ms
step:1923/2330 train_time:118554ms step_avg:61.65ms
step:1924/2330 train_time:118617ms step_avg:61.65ms
step:1925/2330 train_time:118677ms step_avg:61.65ms
step:1926/2330 train_time:118741ms step_avg:61.65ms
step:1927/2330 train_time:118802ms step_avg:61.65ms
step:1928/2330 train_time:118866ms step_avg:61.65ms
step:1929/2330 train_time:118928ms step_avg:61.65ms
step:1930/2330 train_time:118993ms step_avg:61.65ms
step:1931/2330 train_time:119053ms step_avg:61.65ms
step:1932/2330 train_time:119116ms step_avg:61.65ms
step:1933/2330 train_time:119177ms step_avg:61.65ms
step:1934/2330 train_time:119240ms step_avg:61.65ms
step:1935/2330 train_time:119301ms step_avg:61.65ms
step:1936/2330 train_time:119365ms step_avg:61.66ms
step:1937/2330 train_time:119427ms step_avg:61.66ms
step:1938/2330 train_time:119491ms step_avg:61.66ms
step:1939/2330 train_time:119552ms step_avg:61.66ms
step:1940/2330 train_time:119616ms step_avg:61.66ms
step:1941/2330 train_time:119677ms step_avg:61.66ms
step:1942/2330 train_time:119740ms step_avg:61.66ms
step:1943/2330 train_time:119801ms step_avg:61.66ms
step:1944/2330 train_time:119864ms step_avg:61.66ms
step:1945/2330 train_time:119926ms step_avg:61.66ms
step:1946/2330 train_time:119990ms step_avg:61.66ms
step:1947/2330 train_time:120050ms step_avg:61.66ms
step:1948/2330 train_time:120114ms step_avg:61.66ms
step:1949/2330 train_time:120175ms step_avg:61.66ms
step:1950/2330 train_time:120240ms step_avg:61.66ms
step:1951/2330 train_time:120300ms step_avg:61.66ms
step:1952/2330 train_time:120364ms step_avg:61.66ms
step:1953/2330 train_time:120425ms step_avg:61.66ms
step:1954/2330 train_time:120490ms step_avg:61.66ms
step:1955/2330 train_time:120550ms step_avg:61.66ms
step:1956/2330 train_time:120614ms step_avg:61.66ms
step:1957/2330 train_time:120675ms step_avg:61.66ms
step:1958/2330 train_time:120739ms step_avg:61.66ms
step:1959/2330 train_time:120800ms step_avg:61.66ms
step:1960/2330 train_time:120864ms step_avg:61.67ms
step:1961/2330 train_time:120924ms step_avg:61.66ms
step:1962/2330 train_time:120988ms step_avg:61.67ms
step:1963/2330 train_time:121049ms step_avg:61.67ms
step:1964/2330 train_time:121113ms step_avg:61.67ms
step:1965/2330 train_time:121175ms step_avg:61.67ms
step:1966/2330 train_time:121239ms step_avg:61.67ms
step:1967/2330 train_time:121300ms step_avg:61.67ms
step:1968/2330 train_time:121364ms step_avg:61.67ms
step:1969/2330 train_time:121424ms step_avg:61.67ms
step:1970/2330 train_time:121489ms step_avg:61.67ms
step:1971/2330 train_time:121550ms step_avg:61.67ms
step:1972/2330 train_time:121613ms step_avg:61.67ms
step:1973/2330 train_time:121675ms step_avg:61.67ms
step:1974/2330 train_time:121738ms step_avg:61.67ms
step:1975/2330 train_time:121799ms step_avg:61.67ms
step:1976/2330 train_time:121862ms step_avg:61.67ms
step:1977/2330 train_time:121922ms step_avg:61.67ms
step:1978/2330 train_time:121986ms step_avg:61.67ms
step:1979/2330 train_time:122048ms step_avg:61.67ms
step:1980/2330 train_time:122112ms step_avg:61.67ms
step:1981/2330 train_time:122174ms step_avg:61.67ms
step:1982/2330 train_time:122238ms step_avg:61.67ms
step:1983/2330 train_time:122299ms step_avg:61.67ms
step:1984/2330 train_time:122361ms step_avg:61.67ms
step:1985/2330 train_time:122421ms step_avg:61.67ms
step:1986/2330 train_time:122486ms step_avg:61.67ms
step:1987/2330 train_time:122547ms step_avg:61.67ms
step:1988/2330 train_time:122612ms step_avg:61.68ms
step:1989/2330 train_time:122673ms step_avg:61.68ms
step:1990/2330 train_time:122737ms step_avg:61.68ms
step:1991/2330 train_time:122798ms step_avg:61.68ms
step:1992/2330 train_time:122863ms step_avg:61.68ms
step:1993/2330 train_time:122923ms step_avg:61.68ms
step:1994/2330 train_time:122987ms step_avg:61.68ms
step:1995/2330 train_time:123049ms step_avg:61.68ms
step:1996/2330 train_time:123114ms step_avg:61.68ms
step:1997/2330 train_time:123174ms step_avg:61.68ms
step:1998/2330 train_time:123237ms step_avg:61.68ms
step:1999/2330 train_time:123298ms step_avg:61.68ms
step:2000/2330 train_time:123363ms step_avg:61.68ms
step:2000/2330 val_loss:4.2160 train_time:123436ms step_avg:61.72ms
step:2001/2330 train_time:123459ms step_avg:61.70ms
step:2002/2330 train_time:123489ms step_avg:61.68ms
step:2003/2330 train_time:123553ms step_avg:61.68ms
step:2004/2330 train_time:123621ms step_avg:61.69ms
step:2005/2330 train_time:123681ms step_avg:61.69ms
step:2006/2330 train_time:123745ms step_avg:61.69ms
step:2007/2330 train_time:123807ms step_avg:61.69ms
step:2008/2330 train_time:123870ms step_avg:61.69ms
step:2009/2330 train_time:123930ms step_avg:61.69ms
step:2010/2330 train_time:123993ms step_avg:61.69ms
step:2011/2330 train_time:124052ms step_avg:61.69ms
step:2012/2330 train_time:124115ms step_avg:61.69ms
step:2013/2330 train_time:124175ms step_avg:61.69ms
step:2014/2330 train_time:124237ms step_avg:61.69ms
step:2015/2330 train_time:124296ms step_avg:61.69ms
step:2016/2330 train_time:124360ms step_avg:61.69ms
step:2017/2330 train_time:124422ms step_avg:61.69ms
step:2018/2330 train_time:124488ms step_avg:61.69ms
step:2019/2330 train_time:124549ms step_avg:61.69ms
step:2020/2330 train_time:124614ms step_avg:61.69ms
step:2021/2330 train_time:124675ms step_avg:61.69ms
step:2022/2330 train_time:124740ms step_avg:61.69ms
step:2023/2330 train_time:124799ms step_avg:61.69ms
step:2024/2330 train_time:124864ms step_avg:61.69ms
step:2025/2330 train_time:124925ms step_avg:61.69ms
step:2026/2330 train_time:124988ms step_avg:61.69ms
step:2027/2330 train_time:125049ms step_avg:61.69ms
step:2028/2330 train_time:125113ms step_avg:61.69ms
step:2029/2330 train_time:125172ms step_avg:61.69ms
step:2030/2330 train_time:125234ms step_avg:61.69ms
step:2031/2330 train_time:125294ms step_avg:61.69ms
step:2032/2330 train_time:125358ms step_avg:61.69ms
step:2033/2330 train_time:125419ms step_avg:61.69ms
step:2034/2330 train_time:125483ms step_avg:61.69ms
step:2035/2330 train_time:125544ms step_avg:61.69ms
step:2036/2330 train_time:125609ms step_avg:61.69ms
step:2037/2330 train_time:125671ms step_avg:61.69ms
step:2038/2330 train_time:125735ms step_avg:61.70ms
step:2039/2330 train_time:125796ms step_avg:61.69ms
step:2040/2330 train_time:125859ms step_avg:61.70ms
step:2041/2330 train_time:125920ms step_avg:61.70ms
step:2042/2330 train_time:125984ms step_avg:61.70ms
step:2043/2330 train_time:126044ms step_avg:61.70ms
step:2044/2330 train_time:126108ms step_avg:61.70ms
step:2045/2330 train_time:126169ms step_avg:61.70ms
step:2046/2330 train_time:126232ms step_avg:61.70ms
step:2047/2330 train_time:126293ms step_avg:61.70ms
step:2048/2330 train_time:126356ms step_avg:61.70ms
step:2049/2330 train_time:126417ms step_avg:61.70ms
step:2050/2330 train_time:126479ms step_avg:61.70ms
step:2051/2330 train_time:126541ms step_avg:61.70ms
step:2052/2330 train_time:126604ms step_avg:61.70ms
step:2053/2330 train_time:126665ms step_avg:61.70ms
step:2054/2330 train_time:126729ms step_avg:61.70ms
step:2055/2330 train_time:126791ms step_avg:61.70ms
step:2056/2330 train_time:126856ms step_avg:61.70ms
step:2057/2330 train_time:126918ms step_avg:61.70ms
step:2058/2330 train_time:126980ms step_avg:61.70ms
step:2059/2330 train_time:127040ms step_avg:61.70ms
step:2060/2330 train_time:127103ms step_avg:61.70ms
step:2061/2330 train_time:127164ms step_avg:61.70ms
step:2062/2330 train_time:127227ms step_avg:61.70ms
step:2063/2330 train_time:127289ms step_avg:61.70ms
step:2064/2330 train_time:127353ms step_avg:61.70ms
step:2065/2330 train_time:127414ms step_avg:61.70ms
step:2066/2330 train_time:127477ms step_avg:61.70ms
step:2067/2330 train_time:127537ms step_avg:61.70ms
step:2068/2330 train_time:127600ms step_avg:61.70ms
step:2069/2330 train_time:127662ms step_avg:61.70ms
step:2070/2330 train_time:127725ms step_avg:61.70ms
step:2071/2330 train_time:127786ms step_avg:61.70ms
step:2072/2330 train_time:127852ms step_avg:61.70ms
step:2073/2330 train_time:127914ms step_avg:61.70ms
step:2074/2330 train_time:127977ms step_avg:61.71ms
step:2075/2330 train_time:128038ms step_avg:61.71ms
step:2076/2330 train_time:128101ms step_avg:61.71ms
step:2077/2330 train_time:128161ms step_avg:61.70ms
step:2078/2330 train_time:128225ms step_avg:61.71ms
step:2079/2330 train_time:128285ms step_avg:61.71ms
step:2080/2330 train_time:128349ms step_avg:61.71ms
step:2081/2330 train_time:128411ms step_avg:61.71ms
step:2082/2330 train_time:128475ms step_avg:61.71ms
step:2083/2330 train_time:128535ms step_avg:61.71ms
step:2084/2330 train_time:128598ms step_avg:61.71ms
step:2085/2330 train_time:128659ms step_avg:61.71ms
step:2086/2330 train_time:128724ms step_avg:61.71ms
step:2087/2330 train_time:128784ms step_avg:61.71ms
step:2088/2330 train_time:128848ms step_avg:61.71ms
step:2089/2330 train_time:128909ms step_avg:61.71ms
step:2090/2330 train_time:128975ms step_avg:61.71ms
step:2091/2330 train_time:129036ms step_avg:61.71ms
step:2092/2330 train_time:129099ms step_avg:61.71ms
step:2093/2330 train_time:129159ms step_avg:61.71ms
step:2094/2330 train_time:129225ms step_avg:61.71ms
step:2095/2330 train_time:129285ms step_avg:61.71ms
step:2096/2330 train_time:129348ms step_avg:61.71ms
step:2097/2330 train_time:129409ms step_avg:61.71ms
step:2098/2330 train_time:129474ms step_avg:61.71ms
step:2099/2330 train_time:129535ms step_avg:61.71ms
step:2100/2330 train_time:129598ms step_avg:61.71ms
step:2101/2330 train_time:129658ms step_avg:61.71ms
step:2102/2330 train_time:129722ms step_avg:61.71ms
step:2103/2330 train_time:129782ms step_avg:61.71ms
step:2104/2330 train_time:129846ms step_avg:61.71ms
step:2105/2330 train_time:129908ms step_avg:61.71ms
step:2106/2330 train_time:129972ms step_avg:61.72ms
step:2107/2330 train_time:130033ms step_avg:61.71ms
step:2108/2330 train_time:130098ms step_avg:61.72ms
step:2109/2330 train_time:130158ms step_avg:61.72ms
step:2110/2330 train_time:130222ms step_avg:61.72ms
step:2111/2330 train_time:130282ms step_avg:61.72ms
step:2112/2330 train_time:130345ms step_avg:61.72ms
step:2113/2330 train_time:130405ms step_avg:61.72ms
step:2114/2330 train_time:130469ms step_avg:61.72ms
step:2115/2330 train_time:130531ms step_avg:61.72ms
step:2116/2330 train_time:130594ms step_avg:61.72ms
step:2117/2330 train_time:130655ms step_avg:61.72ms
step:2118/2330 train_time:130719ms step_avg:61.72ms
step:2119/2330 train_time:130778ms step_avg:61.72ms
step:2120/2330 train_time:130842ms step_avg:61.72ms
step:2121/2330 train_time:130902ms step_avg:61.72ms
step:2122/2330 train_time:130966ms step_avg:61.72ms
step:2123/2330 train_time:131028ms step_avg:61.72ms
step:2124/2330 train_time:131092ms step_avg:61.72ms
step:2125/2330 train_time:131154ms step_avg:61.72ms
step:2126/2330 train_time:131218ms step_avg:61.72ms
step:2127/2330 train_time:131277ms step_avg:61.72ms
step:2128/2330 train_time:131340ms step_avg:61.72ms
step:2129/2330 train_time:131401ms step_avg:61.72ms
step:2130/2330 train_time:131465ms step_avg:61.72ms
step:2131/2330 train_time:131526ms step_avg:61.72ms
step:2132/2330 train_time:131591ms step_avg:61.72ms
step:2133/2330 train_time:131652ms step_avg:61.72ms
step:2134/2330 train_time:131715ms step_avg:61.72ms
step:2135/2330 train_time:131775ms step_avg:61.72ms
step:2136/2330 train_time:131840ms step_avg:61.72ms
step:2137/2330 train_time:131899ms step_avg:61.72ms
step:2138/2330 train_time:131963ms step_avg:61.72ms
step:2139/2330 train_time:132025ms step_avg:61.72ms
step:2140/2330 train_time:132089ms step_avg:61.72ms
step:2141/2330 train_time:132151ms step_avg:61.72ms
step:2142/2330 train_time:132215ms step_avg:61.73ms
step:2143/2330 train_time:132275ms step_avg:61.72ms
step:2144/2330 train_time:132337ms step_avg:61.72ms
step:2145/2330 train_time:132398ms step_avg:61.72ms
step:2146/2330 train_time:132462ms step_avg:61.73ms
step:2147/2330 train_time:132523ms step_avg:61.72ms
step:2148/2330 train_time:132586ms step_avg:61.73ms
step:2149/2330 train_time:132647ms step_avg:61.73ms
step:2150/2330 train_time:132713ms step_avg:61.73ms
step:2151/2330 train_time:132773ms step_avg:61.73ms
step:2152/2330 train_time:132837ms step_avg:61.73ms
step:2153/2330 train_time:132898ms step_avg:61.73ms
step:2154/2330 train_time:132962ms step_avg:61.73ms
step:2155/2330 train_time:133022ms step_avg:61.73ms
step:2156/2330 train_time:133086ms step_avg:61.73ms
step:2157/2330 train_time:133147ms step_avg:61.73ms
step:2158/2330 train_time:133212ms step_avg:61.73ms
step:2159/2330 train_time:133273ms step_avg:61.73ms
step:2160/2330 train_time:133336ms step_avg:61.73ms
step:2161/2330 train_time:133397ms step_avg:61.73ms
step:2162/2330 train_time:133461ms step_avg:61.73ms
step:2163/2330 train_time:133522ms step_avg:61.73ms
step:2164/2330 train_time:133585ms step_avg:61.73ms
step:2165/2330 train_time:133646ms step_avg:61.73ms
step:2166/2330 train_time:133711ms step_avg:61.73ms
step:2167/2330 train_time:133771ms step_avg:61.73ms
step:2168/2330 train_time:133835ms step_avg:61.73ms
step:2169/2330 train_time:133895ms step_avg:61.73ms
step:2170/2330 train_time:133958ms step_avg:61.73ms
step:2171/2330 train_time:134019ms step_avg:61.73ms
step:2172/2330 train_time:134084ms step_avg:61.73ms
step:2173/2330 train_time:134144ms step_avg:61.73ms
step:2174/2330 train_time:134207ms step_avg:61.73ms
step:2175/2330 train_time:134269ms step_avg:61.73ms
step:2176/2330 train_time:134333ms step_avg:61.73ms
step:2177/2330 train_time:134395ms step_avg:61.73ms
step:2178/2330 train_time:134458ms step_avg:61.73ms
step:2179/2330 train_time:134519ms step_avg:61.73ms
step:2180/2330 train_time:134583ms step_avg:61.74ms
step:2181/2330 train_time:134644ms step_avg:61.73ms
step:2182/2330 train_time:134709ms step_avg:61.74ms
step:2183/2330 train_time:134769ms step_avg:61.74ms
step:2184/2330 train_time:134834ms step_avg:61.74ms
step:2185/2330 train_time:134894ms step_avg:61.74ms
step:2186/2330 train_time:134957ms step_avg:61.74ms
step:2187/2330 train_time:135019ms step_avg:61.74ms
step:2188/2330 train_time:135083ms step_avg:61.74ms
step:2189/2330 train_time:135142ms step_avg:61.74ms
step:2190/2330 train_time:135206ms step_avg:61.74ms
step:2191/2330 train_time:135267ms step_avg:61.74ms
step:2192/2330 train_time:135330ms step_avg:61.74ms
step:2193/2330 train_time:135391ms step_avg:61.74ms
step:2194/2330 train_time:135456ms step_avg:61.74ms
step:2195/2330 train_time:135516ms step_avg:61.74ms
step:2196/2330 train_time:135579ms step_avg:61.74ms
step:2197/2330 train_time:135640ms step_avg:61.74ms
step:2198/2330 train_time:135703ms step_avg:61.74ms
step:2199/2330 train_time:135764ms step_avg:61.74ms
step:2200/2330 train_time:135828ms step_avg:61.74ms
step:2201/2330 train_time:135891ms step_avg:61.74ms
step:2202/2330 train_time:135955ms step_avg:61.74ms
step:2203/2330 train_time:136016ms step_avg:61.74ms
step:2204/2330 train_time:136080ms step_avg:61.74ms
step:2205/2330 train_time:136140ms step_avg:61.74ms
step:2206/2330 train_time:136204ms step_avg:61.74ms
step:2207/2330 train_time:136265ms step_avg:61.74ms
step:2208/2330 train_time:136329ms step_avg:61.74ms
step:2209/2330 train_time:136390ms step_avg:61.74ms
step:2210/2330 train_time:136455ms step_avg:61.74ms
step:2211/2330 train_time:136516ms step_avg:61.74ms
step:2212/2330 train_time:136579ms step_avg:61.74ms
step:2213/2330 train_time:136639ms step_avg:61.74ms
step:2214/2330 train_time:136703ms step_avg:61.74ms
step:2215/2330 train_time:136764ms step_avg:61.74ms
step:2216/2330 train_time:136828ms step_avg:61.75ms
step:2217/2330 train_time:136890ms step_avg:61.75ms
step:2218/2330 train_time:136955ms step_avg:61.75ms
step:2219/2330 train_time:137016ms step_avg:61.75ms
step:2220/2330 train_time:137078ms step_avg:61.75ms
step:2221/2330 train_time:137139ms step_avg:61.75ms
step:2222/2330 train_time:137203ms step_avg:61.75ms
step:2223/2330 train_time:137264ms step_avg:61.75ms
step:2224/2330 train_time:137328ms step_avg:61.75ms
step:2225/2330 train_time:137389ms step_avg:61.75ms
step:2226/2330 train_time:137454ms step_avg:61.75ms
step:2227/2330 train_time:137515ms step_avg:61.75ms
step:2228/2330 train_time:137578ms step_avg:61.75ms
step:2229/2330 train_time:137639ms step_avg:61.75ms
step:2230/2330 train_time:137702ms step_avg:61.75ms
step:2231/2330 train_time:137763ms step_avg:61.75ms
step:2232/2330 train_time:137827ms step_avg:61.75ms
step:2233/2330 train_time:137888ms step_avg:61.75ms
step:2234/2330 train_time:137952ms step_avg:61.75ms
step:2235/2330 train_time:138014ms step_avg:61.75ms
step:2236/2330 train_time:138077ms step_avg:61.75ms
step:2237/2330 train_time:138138ms step_avg:61.75ms
step:2238/2330 train_time:138202ms step_avg:61.75ms
step:2239/2330 train_time:138262ms step_avg:61.75ms
step:2240/2330 train_time:138326ms step_avg:61.75ms
step:2241/2330 train_time:138387ms step_avg:61.75ms
step:2242/2330 train_time:138452ms step_avg:61.75ms
step:2243/2330 train_time:138514ms step_avg:61.75ms
step:2244/2330 train_time:138577ms step_avg:61.75ms
step:2245/2330 train_time:138637ms step_avg:61.75ms
step:2246/2330 train_time:138701ms step_avg:61.75ms
step:2247/2330 train_time:138762ms step_avg:61.75ms
step:2248/2330 train_time:138825ms step_avg:61.75ms
step:2249/2330 train_time:138885ms step_avg:61.75ms
step:2250/2330 train_time:138950ms step_avg:61.76ms
step:2250/2330 val_loss:4.1599 train_time:139024ms step_avg:61.79ms
step:2251/2330 train_time:139046ms step_avg:61.77ms
step:2252/2330 train_time:139076ms step_avg:61.76ms
step:2253/2330 train_time:139142ms step_avg:61.76ms
step:2254/2330 train_time:139207ms step_avg:61.76ms
step:2255/2330 train_time:139269ms step_avg:61.76ms
step:2256/2330 train_time:139332ms step_avg:61.76ms
step:2257/2330 train_time:139392ms step_avg:61.76ms
step:2258/2330 train_time:139457ms step_avg:61.76ms
step:2259/2330 train_time:139517ms step_avg:61.76ms
step:2260/2330 train_time:139580ms step_avg:61.76ms
step:2261/2330 train_time:139640ms step_avg:61.76ms
step:2262/2330 train_time:139701ms step_avg:61.76ms
step:2263/2330 train_time:139762ms step_avg:61.76ms
step:2264/2330 train_time:139825ms step_avg:61.76ms
step:2265/2330 train_time:139886ms step_avg:61.76ms
step:2266/2330 train_time:139948ms step_avg:61.76ms
step:2267/2330 train_time:140009ms step_avg:61.76ms
step:2268/2330 train_time:140075ms step_avg:61.76ms
step:2269/2330 train_time:140137ms step_avg:61.76ms
step:2270/2330 train_time:140202ms step_avg:61.76ms
step:2271/2330 train_time:140264ms step_avg:61.76ms
step:2272/2330 train_time:140328ms step_avg:61.76ms
step:2273/2330 train_time:140387ms step_avg:61.76ms
step:2274/2330 train_time:140452ms step_avg:61.76ms
step:2275/2330 train_time:140513ms step_avg:61.76ms
step:2276/2330 train_time:140576ms step_avg:61.76ms
step:2277/2330 train_time:140636ms step_avg:61.76ms
step:2278/2330 train_time:140700ms step_avg:61.76ms
step:2279/2330 train_time:140759ms step_avg:61.76ms
step:2280/2330 train_time:140823ms step_avg:61.76ms
step:2281/2330 train_time:140883ms step_avg:61.76ms
step:2282/2330 train_time:140947ms step_avg:61.76ms
step:2283/2330 train_time:141007ms step_avg:61.76ms
step:2284/2330 train_time:141072ms step_avg:61.77ms
step:2285/2330 train_time:141134ms step_avg:61.77ms
step:2286/2330 train_time:141199ms step_avg:61.77ms
step:2287/2330 train_time:141259ms step_avg:61.77ms
step:2288/2330 train_time:141325ms step_avg:61.77ms
step:2289/2330 train_time:141385ms step_avg:61.77ms
step:2290/2330 train_time:141448ms step_avg:61.77ms
step:2291/2330 train_time:141508ms step_avg:61.77ms
step:2292/2330 train_time:141572ms step_avg:61.77ms
step:2293/2330 train_time:141633ms step_avg:61.77ms
step:2294/2330 train_time:141697ms step_avg:61.77ms
step:2295/2330 train_time:141758ms step_avg:61.77ms
step:2296/2330 train_time:141822ms step_avg:61.77ms
step:2297/2330 train_time:141881ms step_avg:61.77ms
step:2298/2330 train_time:141945ms step_avg:61.77ms
step:2299/2330 train_time:142006ms step_avg:61.77ms
step:2300/2330 train_time:142070ms step_avg:61.77ms
step:2301/2330 train_time:142131ms step_avg:61.77ms
step:2302/2330 train_time:142196ms step_avg:61.77ms
step:2303/2330 train_time:142257ms step_avg:61.77ms
step:2304/2330 train_time:142321ms step_avg:61.77ms
step:2305/2330 train_time:142381ms step_avg:61.77ms
step:2306/2330 train_time:142446ms step_avg:61.77ms
step:2307/2330 train_time:142507ms step_avg:61.77ms
step:2308/2330 train_time:142571ms step_avg:61.77ms
step:2309/2330 train_time:142632ms step_avg:61.77ms
step:2310/2330 train_time:142696ms step_avg:61.77ms
step:2311/2330 train_time:142757ms step_avg:61.77ms
step:2312/2330 train_time:142822ms step_avg:61.77ms
step:2313/2330 train_time:142881ms step_avg:61.77ms
step:2314/2330 train_time:142944ms step_avg:61.77ms
step:2315/2330 train_time:143004ms step_avg:61.77ms
step:2316/2330 train_time:143068ms step_avg:61.77ms
step:2317/2330 train_time:143129ms step_avg:61.77ms
step:2318/2330 train_time:143192ms step_avg:61.77ms
step:2319/2330 train_time:143254ms step_avg:61.77ms
step:2320/2330 train_time:143318ms step_avg:61.78ms
step:2321/2330 train_time:143379ms step_avg:61.77ms
step:2322/2330 train_time:143442ms step_avg:61.78ms
step:2323/2330 train_time:143503ms step_avg:61.77ms
step:2324/2330 train_time:143567ms step_avg:61.78ms
step:2325/2330 train_time:143628ms step_avg:61.78ms
step:2326/2330 train_time:143691ms step_avg:61.78ms
step:2327/2330 train_time:143752ms step_avg:61.78ms
step:2328/2330 train_time:143816ms step_avg:61.78ms
step:2329/2330 train_time:143877ms step_avg:61.78ms
step:2330/2330 train_time:143939ms step_avg:61.78ms
step:2330/2330 val_loss:4.1496 train_time:144014ms step_avg:61.81ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
