import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 22:33:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             114W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   34C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:91ms step_avg:90.58ms
step:2/2330 train_time:185ms step_avg:92.54ms
step:3/2330 train_time:207ms step_avg:68.97ms
step:4/2330 train_time:242ms step_avg:60.40ms
step:5/2330 train_time:299ms step_avg:59.79ms
step:6/2330 train_time:360ms step_avg:59.93ms
step:7/2330 train_time:419ms step_avg:59.79ms
step:8/2330 train_time:479ms step_avg:59.89ms
step:9/2330 train_time:537ms step_avg:59.71ms
step:10/2330 train_time:598ms step_avg:59.83ms
step:11/2330 train_time:658ms step_avg:59.79ms
step:12/2330 train_time:719ms step_avg:59.90ms
step:13/2330 train_time:777ms step_avg:59.80ms
step:14/2330 train_time:838ms step_avg:59.88ms
step:15/2330 train_time:897ms step_avg:59.81ms
step:16/2330 train_time:958ms step_avg:59.90ms
step:17/2330 train_time:1021ms step_avg:60.07ms
step:18/2330 train_time:1087ms step_avg:60.37ms
step:19/2330 train_time:1149ms step_avg:60.47ms
step:20/2330 train_time:1212ms step_avg:60.60ms
step:21/2330 train_time:1272ms step_avg:60.55ms
step:22/2330 train_time:1333ms step_avg:60.61ms
step:23/2330 train_time:1392ms step_avg:60.54ms
step:24/2330 train_time:1453ms step_avg:60.56ms
step:25/2330 train_time:1513ms step_avg:60.51ms
step:26/2330 train_time:1574ms step_avg:60.53ms
step:27/2330 train_time:1633ms step_avg:60.48ms
step:28/2330 train_time:1694ms step_avg:60.50ms
step:29/2330 train_time:1753ms step_avg:60.44ms
step:30/2330 train_time:1814ms step_avg:60.48ms
step:31/2330 train_time:1873ms step_avg:60.42ms
step:32/2330 train_time:1935ms step_avg:60.47ms
step:33/2330 train_time:1995ms step_avg:60.46ms
step:34/2330 train_time:2058ms step_avg:60.54ms
step:35/2330 train_time:2121ms step_avg:60.59ms
step:36/2330 train_time:2183ms step_avg:60.65ms
step:37/2330 train_time:2244ms step_avg:60.65ms
step:38/2330 train_time:2307ms step_avg:60.70ms
step:39/2330 train_time:2366ms step_avg:60.66ms
step:40/2330 train_time:2428ms step_avg:60.70ms
step:41/2330 train_time:2488ms step_avg:60.68ms
step:42/2330 train_time:2549ms step_avg:60.70ms
step:43/2330 train_time:2609ms step_avg:60.69ms
step:44/2330 train_time:2671ms step_avg:60.69ms
step:45/2330 train_time:2730ms step_avg:60.66ms
step:46/2330 train_time:2791ms step_avg:60.68ms
step:47/2330 train_time:2851ms step_avg:60.67ms
step:48/2330 train_time:2913ms step_avg:60.69ms
step:49/2330 train_time:2973ms step_avg:60.67ms
step:50/2330 train_time:3036ms step_avg:60.72ms
step:51/2330 train_time:3096ms step_avg:60.71ms
step:52/2330 train_time:3158ms step_avg:60.73ms
step:53/2330 train_time:3218ms step_avg:60.72ms
step:54/2330 train_time:3280ms step_avg:60.75ms
step:55/2330 train_time:3340ms step_avg:60.73ms
step:56/2330 train_time:3403ms step_avg:60.76ms
step:57/2330 train_time:3462ms step_avg:60.74ms
step:58/2330 train_time:3525ms step_avg:60.77ms
step:59/2330 train_time:3585ms step_avg:60.76ms
step:60/2330 train_time:3646ms step_avg:60.77ms
step:61/2330 train_time:3706ms step_avg:60.76ms
step:62/2330 train_time:3768ms step_avg:60.77ms
step:63/2330 train_time:3828ms step_avg:60.76ms
step:64/2330 train_time:3890ms step_avg:60.78ms
step:65/2330 train_time:3951ms step_avg:60.78ms
step:66/2330 train_time:4012ms step_avg:60.79ms
step:67/2330 train_time:4072ms step_avg:60.78ms
step:68/2330 train_time:4134ms step_avg:60.79ms
step:69/2330 train_time:4193ms step_avg:60.77ms
step:70/2330 train_time:4255ms step_avg:60.79ms
step:71/2330 train_time:4315ms step_avg:60.77ms
step:72/2330 train_time:4376ms step_avg:60.78ms
step:73/2330 train_time:4436ms step_avg:60.77ms
step:74/2330 train_time:4498ms step_avg:60.79ms
step:75/2330 train_time:4558ms step_avg:60.78ms
step:76/2330 train_time:4620ms step_avg:60.79ms
step:77/2330 train_time:4680ms step_avg:60.78ms
step:78/2330 train_time:4743ms step_avg:60.80ms
step:79/2330 train_time:4803ms step_avg:60.79ms
step:80/2330 train_time:4864ms step_avg:60.80ms
step:81/2330 train_time:4924ms step_avg:60.79ms
step:82/2330 train_time:4986ms step_avg:60.81ms
step:83/2330 train_time:5046ms step_avg:60.80ms
step:84/2330 train_time:5109ms step_avg:60.82ms
step:85/2330 train_time:5169ms step_avg:60.82ms
step:86/2330 train_time:5231ms step_avg:60.82ms
step:87/2330 train_time:5290ms step_avg:60.80ms
step:88/2330 train_time:5351ms step_avg:60.81ms
step:89/2330 train_time:5411ms step_avg:60.79ms
step:90/2330 train_time:5471ms step_avg:60.79ms
step:91/2330 train_time:5531ms step_avg:60.78ms
step:92/2330 train_time:5593ms step_avg:60.79ms
step:93/2330 train_time:5653ms step_avg:60.78ms
step:94/2330 train_time:5714ms step_avg:60.79ms
step:95/2330 train_time:5773ms step_avg:60.77ms
step:96/2330 train_time:5835ms step_avg:60.78ms
step:97/2330 train_time:5895ms step_avg:60.77ms
step:98/2330 train_time:5957ms step_avg:60.79ms
step:99/2330 train_time:6017ms step_avg:60.78ms
step:100/2330 train_time:6080ms step_avg:60.80ms
step:101/2330 train_time:6140ms step_avg:60.79ms
step:102/2330 train_time:6202ms step_avg:60.80ms
step:103/2330 train_time:6261ms step_avg:60.79ms
step:104/2330 train_time:6324ms step_avg:60.81ms
step:105/2330 train_time:6384ms step_avg:60.80ms
step:106/2330 train_time:6446ms step_avg:60.81ms
step:107/2330 train_time:6506ms step_avg:60.80ms
step:108/2330 train_time:6568ms step_avg:60.81ms
step:109/2330 train_time:6628ms step_avg:60.81ms
step:110/2330 train_time:6690ms step_avg:60.81ms
step:111/2330 train_time:6750ms step_avg:60.81ms
step:112/2330 train_time:6811ms step_avg:60.81ms
step:113/2330 train_time:6870ms step_avg:60.80ms
step:114/2330 train_time:6932ms step_avg:60.81ms
step:115/2330 train_time:6992ms step_avg:60.80ms
step:116/2330 train_time:7054ms step_avg:60.81ms
step:117/2330 train_time:7114ms step_avg:60.80ms
step:118/2330 train_time:7175ms step_avg:60.81ms
step:119/2330 train_time:7235ms step_avg:60.80ms
step:120/2330 train_time:7297ms step_avg:60.81ms
step:121/2330 train_time:7357ms step_avg:60.80ms
step:122/2330 train_time:7419ms step_avg:60.81ms
step:123/2330 train_time:7479ms step_avg:60.80ms
step:124/2330 train_time:7541ms step_avg:60.81ms
step:125/2330 train_time:7600ms step_avg:60.80ms
step:126/2330 train_time:7662ms step_avg:60.81ms
step:127/2330 train_time:7722ms step_avg:60.81ms
step:128/2330 train_time:7784ms step_avg:60.81ms
step:129/2330 train_time:7844ms step_avg:60.80ms
step:130/2330 train_time:7906ms step_avg:60.82ms
step:131/2330 train_time:7967ms step_avg:60.81ms
step:132/2330 train_time:8029ms step_avg:60.83ms
step:133/2330 train_time:8089ms step_avg:60.82ms
step:134/2330 train_time:8150ms step_avg:60.82ms
step:135/2330 train_time:8210ms step_avg:60.81ms
step:136/2330 train_time:8271ms step_avg:60.82ms
step:137/2330 train_time:8331ms step_avg:60.81ms
step:138/2330 train_time:8392ms step_avg:60.81ms
step:139/2330 train_time:8452ms step_avg:60.80ms
step:140/2330 train_time:8514ms step_avg:60.81ms
step:141/2330 train_time:8574ms step_avg:60.81ms
step:142/2330 train_time:8636ms step_avg:60.81ms
step:143/2330 train_time:8695ms step_avg:60.81ms
step:144/2330 train_time:8757ms step_avg:60.81ms
step:145/2330 train_time:8817ms step_avg:60.81ms
step:146/2330 train_time:8880ms step_avg:60.82ms
step:147/2330 train_time:8940ms step_avg:60.82ms
step:148/2330 train_time:9003ms step_avg:60.83ms
step:149/2330 train_time:9063ms step_avg:60.83ms
step:150/2330 train_time:9125ms step_avg:60.83ms
step:151/2330 train_time:9185ms step_avg:60.83ms
step:152/2330 train_time:9247ms step_avg:60.84ms
step:153/2330 train_time:9307ms step_avg:60.83ms
step:154/2330 train_time:9369ms step_avg:60.84ms
step:155/2330 train_time:9429ms step_avg:60.83ms
step:156/2330 train_time:9491ms step_avg:60.84ms
step:157/2330 train_time:9551ms step_avg:60.83ms
step:158/2330 train_time:9613ms step_avg:60.84ms
step:159/2330 train_time:9672ms step_avg:60.83ms
step:160/2330 train_time:9733ms step_avg:60.83ms
step:161/2330 train_time:9793ms step_avg:60.83ms
step:162/2330 train_time:9855ms step_avg:60.83ms
step:163/2330 train_time:9916ms step_avg:60.83ms
step:164/2330 train_time:9977ms step_avg:60.84ms
step:165/2330 train_time:10037ms step_avg:60.83ms
step:166/2330 train_time:10098ms step_avg:60.83ms
step:167/2330 train_time:10159ms step_avg:60.83ms
step:168/2330 train_time:10222ms step_avg:60.84ms
step:169/2330 train_time:10282ms step_avg:60.84ms
step:170/2330 train_time:10344ms step_avg:60.85ms
step:171/2330 train_time:10404ms step_avg:60.84ms
step:172/2330 train_time:10466ms step_avg:60.85ms
step:173/2330 train_time:10527ms step_avg:60.85ms
step:174/2330 train_time:10588ms step_avg:60.85ms
step:175/2330 train_time:10649ms step_avg:60.85ms
step:176/2330 train_time:10711ms step_avg:60.86ms
step:177/2330 train_time:10771ms step_avg:60.85ms
step:178/2330 train_time:10833ms step_avg:60.86ms
step:179/2330 train_time:10893ms step_avg:60.86ms
step:180/2330 train_time:10955ms step_avg:60.86ms
step:181/2330 train_time:11014ms step_avg:60.85ms
step:182/2330 train_time:11075ms step_avg:60.85ms
step:183/2330 train_time:11135ms step_avg:60.85ms
step:184/2330 train_time:11197ms step_avg:60.85ms
step:185/2330 train_time:11258ms step_avg:60.85ms
step:186/2330 train_time:11320ms step_avg:60.86ms
step:187/2330 train_time:11380ms step_avg:60.86ms
step:188/2330 train_time:11442ms step_avg:60.86ms
step:189/2330 train_time:11502ms step_avg:60.86ms
step:190/2330 train_time:11564ms step_avg:60.86ms
step:191/2330 train_time:11625ms step_avg:60.86ms
step:192/2330 train_time:11687ms step_avg:60.87ms
step:193/2330 train_time:11746ms step_avg:60.86ms
step:194/2330 train_time:11809ms step_avg:60.87ms
step:195/2330 train_time:11870ms step_avg:60.87ms
step:196/2330 train_time:11931ms step_avg:60.87ms
step:197/2330 train_time:11992ms step_avg:60.87ms
step:198/2330 train_time:12053ms step_avg:60.87ms
step:199/2330 train_time:12112ms step_avg:60.87ms
step:200/2330 train_time:12173ms step_avg:60.87ms
step:201/2330 train_time:12233ms step_avg:60.86ms
step:202/2330 train_time:12294ms step_avg:60.86ms
step:203/2330 train_time:12354ms step_avg:60.86ms
step:204/2330 train_time:12417ms step_avg:60.87ms
step:205/2330 train_time:12477ms step_avg:60.86ms
step:206/2330 train_time:12539ms step_avg:60.87ms
step:207/2330 train_time:12600ms step_avg:60.87ms
step:208/2330 train_time:12663ms step_avg:60.88ms
step:209/2330 train_time:12724ms step_avg:60.88ms
step:210/2330 train_time:12785ms step_avg:60.88ms
step:211/2330 train_time:12846ms step_avg:60.88ms
step:212/2330 train_time:12909ms step_avg:60.89ms
step:213/2330 train_time:12970ms step_avg:60.89ms
step:214/2330 train_time:13032ms step_avg:60.90ms
step:215/2330 train_time:13091ms step_avg:60.89ms
step:216/2330 train_time:13153ms step_avg:60.89ms
step:217/2330 train_time:13213ms step_avg:60.89ms
step:218/2330 train_time:13274ms step_avg:60.89ms
step:219/2330 train_time:13334ms step_avg:60.89ms
step:220/2330 train_time:13396ms step_avg:60.89ms
step:221/2330 train_time:13455ms step_avg:60.88ms
step:222/2330 train_time:13518ms step_avg:60.89ms
step:223/2330 train_time:13578ms step_avg:60.89ms
step:224/2330 train_time:13640ms step_avg:60.89ms
step:225/2330 train_time:13701ms step_avg:60.89ms
step:226/2330 train_time:13763ms step_avg:60.90ms
step:227/2330 train_time:13824ms step_avg:60.90ms
step:228/2330 train_time:13885ms step_avg:60.90ms
step:229/2330 train_time:13945ms step_avg:60.90ms
step:230/2330 train_time:14008ms step_avg:60.90ms
step:231/2330 train_time:14067ms step_avg:60.90ms
step:232/2330 train_time:14130ms step_avg:60.90ms
step:233/2330 train_time:14190ms step_avg:60.90ms
step:234/2330 train_time:14251ms step_avg:60.90ms
step:235/2330 train_time:14311ms step_avg:60.90ms
step:236/2330 train_time:14372ms step_avg:60.90ms
step:237/2330 train_time:14432ms step_avg:60.89ms
step:238/2330 train_time:14493ms step_avg:60.90ms
step:239/2330 train_time:14553ms step_avg:60.89ms
step:240/2330 train_time:14615ms step_avg:60.90ms
step:241/2330 train_time:14676ms step_avg:60.90ms
step:242/2330 train_time:14738ms step_avg:60.90ms
step:243/2330 train_time:14798ms step_avg:60.90ms
step:244/2330 train_time:14861ms step_avg:60.90ms
step:245/2330 train_time:14921ms step_avg:60.90ms
step:246/2330 train_time:14984ms step_avg:60.91ms
step:247/2330 train_time:15044ms step_avg:60.91ms
step:248/2330 train_time:15106ms step_avg:60.91ms
step:249/2330 train_time:15167ms step_avg:60.91ms
step:250/2330 train_time:15229ms step_avg:60.92ms
step:250/2330 val_loss:4.3021 train_time:15293ms step_avg:61.17ms
step:251/2330 train_time:15315ms step_avg:61.01ms
step:252/2330 train_time:15353ms step_avg:60.92ms
step:253/2330 train_time:15418ms step_avg:60.94ms
step:254/2330 train_time:15483ms step_avg:60.96ms
step:255/2330 train_time:15545ms step_avg:60.96ms
step:256/2330 train_time:15608ms step_avg:60.97ms
step:257/2330 train_time:15668ms step_avg:60.96ms
step:258/2330 train_time:15730ms step_avg:60.97ms
step:259/2330 train_time:15790ms step_avg:60.96ms
step:260/2330 train_time:15851ms step_avg:60.96ms
step:261/2330 train_time:15910ms step_avg:60.96ms
step:262/2330 train_time:15971ms step_avg:60.96ms
step:263/2330 train_time:16030ms step_avg:60.95ms
step:264/2330 train_time:16091ms step_avg:60.95ms
step:265/2330 train_time:16150ms step_avg:60.94ms
step:266/2330 train_time:16212ms step_avg:60.95ms
step:267/2330 train_time:16271ms step_avg:60.94ms
step:268/2330 train_time:16334ms step_avg:60.95ms
step:269/2330 train_time:16394ms step_avg:60.94ms
step:270/2330 train_time:16456ms step_avg:60.95ms
step:271/2330 train_time:16517ms step_avg:60.95ms
step:272/2330 train_time:16579ms step_avg:60.95ms
step:273/2330 train_time:16640ms step_avg:60.95ms
step:274/2330 train_time:16703ms step_avg:60.96ms
step:275/2330 train_time:16763ms step_avg:60.96ms
step:276/2330 train_time:16825ms step_avg:60.96ms
step:277/2330 train_time:16884ms step_avg:60.95ms
step:278/2330 train_time:16946ms step_avg:60.96ms
step:279/2330 train_time:17005ms step_avg:60.95ms
step:280/2330 train_time:17067ms step_avg:60.96ms
step:281/2330 train_time:17128ms step_avg:60.95ms
step:282/2330 train_time:17190ms step_avg:60.96ms
step:283/2330 train_time:17249ms step_avg:60.95ms
step:284/2330 train_time:17312ms step_avg:60.96ms
step:285/2330 train_time:17372ms step_avg:60.95ms
step:286/2330 train_time:17434ms step_avg:60.96ms
step:287/2330 train_time:17494ms step_avg:60.95ms
step:288/2330 train_time:17556ms step_avg:60.96ms
step:289/2330 train_time:17616ms step_avg:60.96ms
step:290/2330 train_time:17678ms step_avg:60.96ms
step:291/2330 train_time:17738ms step_avg:60.96ms
step:292/2330 train_time:17801ms step_avg:60.96ms
step:293/2330 train_time:17861ms step_avg:60.96ms
step:294/2330 train_time:17923ms step_avg:60.96ms
step:295/2330 train_time:17983ms step_avg:60.96ms
step:296/2330 train_time:18045ms step_avg:60.96ms
step:297/2330 train_time:18105ms step_avg:60.96ms
step:298/2330 train_time:18167ms step_avg:60.96ms
step:299/2330 train_time:18226ms step_avg:60.96ms
step:300/2330 train_time:18288ms step_avg:60.96ms
step:301/2330 train_time:18350ms step_avg:60.96ms
step:302/2330 train_time:18412ms step_avg:60.97ms
step:303/2330 train_time:18472ms step_avg:60.97ms
step:304/2330 train_time:18534ms step_avg:60.97ms
step:305/2330 train_time:18595ms step_avg:60.97ms
step:306/2330 train_time:18656ms step_avg:60.97ms
step:307/2330 train_time:18716ms step_avg:60.96ms
step:308/2330 train_time:18777ms step_avg:60.96ms
step:309/2330 train_time:18837ms step_avg:60.96ms
step:310/2330 train_time:18899ms step_avg:60.96ms
step:311/2330 train_time:18959ms step_avg:60.96ms
step:312/2330 train_time:19021ms step_avg:60.96ms
step:313/2330 train_time:19081ms step_avg:60.96ms
step:314/2330 train_time:19144ms step_avg:60.97ms
step:315/2330 train_time:19205ms step_avg:60.97ms
step:316/2330 train_time:19266ms step_avg:60.97ms
step:317/2330 train_time:19328ms step_avg:60.97ms
step:318/2330 train_time:19390ms step_avg:60.97ms
step:319/2330 train_time:19450ms step_avg:60.97ms
step:320/2330 train_time:19512ms step_avg:60.98ms
step:321/2330 train_time:19573ms step_avg:60.97ms
step:322/2330 train_time:19635ms step_avg:60.98ms
step:323/2330 train_time:19694ms step_avg:60.97ms
step:324/2330 train_time:19755ms step_avg:60.97ms
step:325/2330 train_time:19815ms step_avg:60.97ms
step:326/2330 train_time:19876ms step_avg:60.97ms
step:327/2330 train_time:19935ms step_avg:60.96ms
step:328/2330 train_time:19997ms step_avg:60.97ms
step:329/2330 train_time:20057ms step_avg:60.96ms
step:330/2330 train_time:20119ms step_avg:60.97ms
step:331/2330 train_time:20178ms step_avg:60.96ms
step:332/2330 train_time:20241ms step_avg:60.97ms
step:333/2330 train_time:20302ms step_avg:60.97ms
step:334/2330 train_time:20364ms step_avg:60.97ms
step:335/2330 train_time:20425ms step_avg:60.97ms
step:336/2330 train_time:20487ms step_avg:60.97ms
step:337/2330 train_time:20547ms step_avg:60.97ms
step:338/2330 train_time:20609ms step_avg:60.97ms
step:339/2330 train_time:20669ms step_avg:60.97ms
step:340/2330 train_time:20732ms step_avg:60.98ms
step:341/2330 train_time:20792ms step_avg:60.97ms
step:342/2330 train_time:20853ms step_avg:60.97ms
step:343/2330 train_time:20913ms step_avg:60.97ms
step:344/2330 train_time:20975ms step_avg:60.97ms
step:345/2330 train_time:21035ms step_avg:60.97ms
step:346/2330 train_time:21097ms step_avg:60.97ms
step:347/2330 train_time:21156ms step_avg:60.97ms
step:348/2330 train_time:21218ms step_avg:60.97ms
step:349/2330 train_time:21278ms step_avg:60.97ms
step:350/2330 train_time:21341ms step_avg:60.97ms
step:351/2330 train_time:21401ms step_avg:60.97ms
step:352/2330 train_time:21463ms step_avg:60.98ms
step:353/2330 train_time:21525ms step_avg:60.98ms
step:354/2330 train_time:21587ms step_avg:60.98ms
step:355/2330 train_time:21647ms step_avg:60.98ms
step:356/2330 train_time:21710ms step_avg:60.98ms
step:357/2330 train_time:21769ms step_avg:60.98ms
step:358/2330 train_time:21831ms step_avg:60.98ms
step:359/2330 train_time:21892ms step_avg:60.98ms
step:360/2330 train_time:21953ms step_avg:60.98ms
step:361/2330 train_time:22013ms step_avg:60.98ms
step:362/2330 train_time:22074ms step_avg:60.98ms
step:363/2330 train_time:22134ms step_avg:60.98ms
step:364/2330 train_time:22196ms step_avg:60.98ms
step:365/2330 train_time:22256ms step_avg:60.98ms
step:366/2330 train_time:22318ms step_avg:60.98ms
step:367/2330 train_time:22378ms step_avg:60.98ms
step:368/2330 train_time:22440ms step_avg:60.98ms
step:369/2330 train_time:22501ms step_avg:60.98ms
step:370/2330 train_time:22564ms step_avg:60.98ms
step:371/2330 train_time:22624ms step_avg:60.98ms
step:372/2330 train_time:22686ms step_avg:60.98ms
step:373/2330 train_time:22746ms step_avg:60.98ms
step:374/2330 train_time:22809ms step_avg:60.99ms
step:375/2330 train_time:22870ms step_avg:60.99ms
step:376/2330 train_time:22932ms step_avg:60.99ms
step:377/2330 train_time:22993ms step_avg:60.99ms
step:378/2330 train_time:23054ms step_avg:60.99ms
step:379/2330 train_time:23113ms step_avg:60.98ms
step:380/2330 train_time:23175ms step_avg:60.99ms
step:381/2330 train_time:23234ms step_avg:60.98ms
step:382/2330 train_time:23296ms step_avg:60.98ms
step:383/2330 train_time:23355ms step_avg:60.98ms
step:384/2330 train_time:23418ms step_avg:60.98ms
step:385/2330 train_time:23477ms step_avg:60.98ms
step:386/2330 train_time:23539ms step_avg:60.98ms
step:387/2330 train_time:23600ms step_avg:60.98ms
step:388/2330 train_time:23662ms step_avg:60.98ms
step:389/2330 train_time:23723ms step_avg:60.98ms
step:390/2330 train_time:23786ms step_avg:60.99ms
step:391/2330 train_time:23846ms step_avg:60.99ms
step:392/2330 train_time:23909ms step_avg:60.99ms
step:393/2330 train_time:23969ms step_avg:60.99ms
step:394/2330 train_time:24031ms step_avg:60.99ms
step:395/2330 train_time:24092ms step_avg:60.99ms
step:396/2330 train_time:24153ms step_avg:60.99ms
step:397/2330 train_time:24213ms step_avg:60.99ms
step:398/2330 train_time:24274ms step_avg:60.99ms
step:399/2330 train_time:24333ms step_avg:60.99ms
step:400/2330 train_time:24395ms step_avg:60.99ms
step:401/2330 train_time:24454ms step_avg:60.98ms
step:402/2330 train_time:24516ms step_avg:60.98ms
step:403/2330 train_time:24576ms step_avg:60.98ms
step:404/2330 train_time:24638ms step_avg:60.99ms
step:405/2330 train_time:24698ms step_avg:60.98ms
step:406/2330 train_time:24760ms step_avg:60.99ms
step:407/2330 train_time:24821ms step_avg:60.98ms
step:408/2330 train_time:24883ms step_avg:60.99ms
step:409/2330 train_time:24943ms step_avg:60.99ms
step:410/2330 train_time:25007ms step_avg:60.99ms
step:411/2330 train_time:25067ms step_avg:60.99ms
step:412/2330 train_time:25129ms step_avg:60.99ms
step:413/2330 train_time:25190ms step_avg:60.99ms
step:414/2330 train_time:25252ms step_avg:60.99ms
step:415/2330 train_time:25312ms step_avg:60.99ms
step:416/2330 train_time:25374ms step_avg:61.00ms
step:417/2330 train_time:25434ms step_avg:60.99ms
step:418/2330 train_time:25496ms step_avg:61.00ms
step:419/2330 train_time:25555ms step_avg:60.99ms
step:420/2330 train_time:25618ms step_avg:60.99ms
step:421/2330 train_time:25677ms step_avg:60.99ms
step:422/2330 train_time:25739ms step_avg:60.99ms
step:423/2330 train_time:25799ms step_avg:60.99ms
step:424/2330 train_time:25861ms step_avg:60.99ms
step:425/2330 train_time:25922ms step_avg:60.99ms
step:426/2330 train_time:25985ms step_avg:61.00ms
step:427/2330 train_time:26046ms step_avg:61.00ms
step:428/2330 train_time:26108ms step_avg:61.00ms
step:429/2330 train_time:26169ms step_avg:61.00ms
step:430/2330 train_time:26231ms step_avg:61.00ms
step:431/2330 train_time:26291ms step_avg:61.00ms
step:432/2330 train_time:26353ms step_avg:61.00ms
step:433/2330 train_time:26413ms step_avg:61.00ms
step:434/2330 train_time:26474ms step_avg:61.00ms
step:435/2330 train_time:26534ms step_avg:61.00ms
step:436/2330 train_time:26596ms step_avg:61.00ms
step:437/2330 train_time:26655ms step_avg:61.00ms
step:438/2330 train_time:26717ms step_avg:61.00ms
step:439/2330 train_time:26777ms step_avg:60.99ms
step:440/2330 train_time:26838ms step_avg:61.00ms
step:441/2330 train_time:26898ms step_avg:60.99ms
step:442/2330 train_time:26961ms step_avg:61.00ms
step:443/2330 train_time:27022ms step_avg:61.00ms
step:444/2330 train_time:27085ms step_avg:61.00ms
step:445/2330 train_time:27145ms step_avg:61.00ms
step:446/2330 train_time:27208ms step_avg:61.00ms
step:447/2330 train_time:27268ms step_avg:61.00ms
step:448/2330 train_time:27330ms step_avg:61.01ms
step:449/2330 train_time:27391ms step_avg:61.00ms
step:450/2330 train_time:27452ms step_avg:61.01ms
step:451/2330 train_time:27512ms step_avg:61.00ms
step:452/2330 train_time:27574ms step_avg:61.00ms
step:453/2330 train_time:27634ms step_avg:61.00ms
step:454/2330 train_time:27695ms step_avg:61.00ms
step:455/2330 train_time:27755ms step_avg:61.00ms
step:456/2330 train_time:27816ms step_avg:61.00ms
step:457/2330 train_time:27876ms step_avg:61.00ms
step:458/2330 train_time:27938ms step_avg:61.00ms
step:459/2330 train_time:27999ms step_avg:61.00ms
step:460/2330 train_time:28062ms step_avg:61.00ms
step:461/2330 train_time:28123ms step_avg:61.00ms
step:462/2330 train_time:28185ms step_avg:61.01ms
step:463/2330 train_time:28246ms step_avg:61.01ms
step:464/2330 train_time:28308ms step_avg:61.01ms
step:465/2330 train_time:28368ms step_avg:61.01ms
step:466/2330 train_time:28430ms step_avg:61.01ms
step:467/2330 train_time:28490ms step_avg:61.01ms
step:468/2330 train_time:28552ms step_avg:61.01ms
step:469/2330 train_time:28612ms step_avg:61.01ms
step:470/2330 train_time:28674ms step_avg:61.01ms
step:471/2330 train_time:28734ms step_avg:61.01ms
step:472/2330 train_time:28796ms step_avg:61.01ms
step:473/2330 train_time:28856ms step_avg:61.01ms
step:474/2330 train_time:28918ms step_avg:61.01ms
step:475/2330 train_time:28978ms step_avg:61.01ms
step:476/2330 train_time:29040ms step_avg:61.01ms
step:477/2330 train_time:29099ms step_avg:61.01ms
step:478/2330 train_time:29163ms step_avg:61.01ms
step:479/2330 train_time:29223ms step_avg:61.01ms
step:480/2330 train_time:29286ms step_avg:61.01ms
step:481/2330 train_time:29346ms step_avg:61.01ms
step:482/2330 train_time:29408ms step_avg:61.01ms
step:483/2330 train_time:29469ms step_avg:61.01ms
step:484/2330 train_time:29530ms step_avg:61.01ms
step:485/2330 train_time:29590ms step_avg:61.01ms
step:486/2330 train_time:29652ms step_avg:61.01ms
step:487/2330 train_time:29713ms step_avg:61.01ms
step:488/2330 train_time:29774ms step_avg:61.01ms
step:489/2330 train_time:29834ms step_avg:61.01ms
step:490/2330 train_time:29895ms step_avg:61.01ms
step:491/2330 train_time:29955ms step_avg:61.01ms
step:492/2330 train_time:30017ms step_avg:61.01ms
step:493/2330 train_time:30077ms step_avg:61.01ms
step:494/2330 train_time:30139ms step_avg:61.01ms
step:495/2330 train_time:30199ms step_avg:61.01ms
step:496/2330 train_time:30262ms step_avg:61.01ms
step:497/2330 train_time:30322ms step_avg:61.01ms
step:498/2330 train_time:30385ms step_avg:61.01ms
step:499/2330 train_time:30446ms step_avg:61.01ms
step:500/2330 train_time:30508ms step_avg:61.02ms
step:500/2330 val_loss:3.9339 train_time:30572ms step_avg:61.14ms
step:501/2330 train_time:30594ms step_avg:61.06ms
step:502/2330 train_time:30632ms step_avg:61.02ms
step:503/2330 train_time:30696ms step_avg:61.03ms
step:504/2330 train_time:30762ms step_avg:61.04ms
step:505/2330 train_time:30822ms step_avg:61.03ms
step:506/2330 train_time:30885ms step_avg:61.04ms
step:507/2330 train_time:30944ms step_avg:61.03ms
step:508/2330 train_time:31006ms step_avg:61.03ms
step:509/2330 train_time:31065ms step_avg:61.03ms
step:510/2330 train_time:31126ms step_avg:61.03ms
step:511/2330 train_time:31185ms step_avg:61.03ms
step:512/2330 train_time:31247ms step_avg:61.03ms
step:513/2330 train_time:31305ms step_avg:61.02ms
step:514/2330 train_time:31367ms step_avg:61.02ms
step:515/2330 train_time:31426ms step_avg:61.02ms
step:516/2330 train_time:31488ms step_avg:61.02ms
step:517/2330 train_time:31549ms step_avg:61.02ms
step:518/2330 train_time:31612ms step_avg:61.03ms
step:519/2330 train_time:31673ms step_avg:61.03ms
step:520/2330 train_time:31737ms step_avg:61.03ms
step:521/2330 train_time:31797ms step_avg:61.03ms
step:522/2330 train_time:31860ms step_avg:61.04ms
step:523/2330 train_time:31921ms step_avg:61.03ms
step:524/2330 train_time:31982ms step_avg:61.03ms
step:525/2330 train_time:32043ms step_avg:61.03ms
step:526/2330 train_time:32104ms step_avg:61.03ms
step:527/2330 train_time:32165ms step_avg:61.03ms
step:528/2330 train_time:32226ms step_avg:61.03ms
step:529/2330 train_time:32285ms step_avg:61.03ms
step:530/2330 train_time:32347ms step_avg:61.03ms
step:531/2330 train_time:32406ms step_avg:61.03ms
step:532/2330 train_time:32467ms step_avg:61.03ms
step:533/2330 train_time:32527ms step_avg:61.03ms
step:534/2330 train_time:32589ms step_avg:61.03ms
step:535/2330 train_time:32649ms step_avg:61.03ms
step:536/2330 train_time:32712ms step_avg:61.03ms
step:537/2330 train_time:32772ms step_avg:61.03ms
step:538/2330 train_time:32834ms step_avg:61.03ms
step:539/2330 train_time:32895ms step_avg:61.03ms
step:540/2330 train_time:32957ms step_avg:61.03ms
step:541/2330 train_time:33017ms step_avg:61.03ms
step:542/2330 train_time:33080ms step_avg:61.03ms
step:543/2330 train_time:33140ms step_avg:61.03ms
step:544/2330 train_time:33202ms step_avg:61.03ms
step:545/2330 train_time:33261ms step_avg:61.03ms
step:546/2330 train_time:33323ms step_avg:61.03ms
step:547/2330 train_time:33383ms step_avg:61.03ms
step:548/2330 train_time:33445ms step_avg:61.03ms
step:549/2330 train_time:33506ms step_avg:61.03ms
step:550/2330 train_time:33568ms step_avg:61.03ms
step:551/2330 train_time:33628ms step_avg:61.03ms
step:552/2330 train_time:33690ms step_avg:61.03ms
step:553/2330 train_time:33751ms step_avg:61.03ms
step:554/2330 train_time:33813ms step_avg:61.03ms
step:555/2330 train_time:33873ms step_avg:61.03ms
step:556/2330 train_time:33935ms step_avg:61.03ms
step:557/2330 train_time:33995ms step_avg:61.03ms
step:558/2330 train_time:34057ms step_avg:61.03ms
step:559/2330 train_time:34118ms step_avg:61.03ms
step:560/2330 train_time:34181ms step_avg:61.04ms
step:561/2330 train_time:34241ms step_avg:61.04ms
step:562/2330 train_time:34302ms step_avg:61.04ms
step:563/2330 train_time:34362ms step_avg:61.03ms
step:564/2330 train_time:34424ms step_avg:61.04ms
step:565/2330 train_time:34484ms step_avg:61.03ms
step:566/2330 train_time:34546ms step_avg:61.03ms
step:567/2330 train_time:34606ms step_avg:61.03ms
step:568/2330 train_time:34668ms step_avg:61.04ms
step:569/2330 train_time:34728ms step_avg:61.03ms
step:570/2330 train_time:34790ms step_avg:61.03ms
step:571/2330 train_time:34849ms step_avg:61.03ms
step:572/2330 train_time:34911ms step_avg:61.03ms
step:573/2330 train_time:34971ms step_avg:61.03ms
step:574/2330 train_time:35034ms step_avg:61.03ms
step:575/2330 train_time:35094ms step_avg:61.03ms
step:576/2330 train_time:35156ms step_avg:61.04ms
step:577/2330 train_time:35216ms step_avg:61.03ms
step:578/2330 train_time:35279ms step_avg:61.04ms
step:579/2330 train_time:35340ms step_avg:61.04ms
step:580/2330 train_time:35402ms step_avg:61.04ms
step:581/2330 train_time:35462ms step_avg:61.04ms
step:582/2330 train_time:35524ms step_avg:61.04ms
step:583/2330 train_time:35584ms step_avg:61.04ms
step:584/2330 train_time:35646ms step_avg:61.04ms
step:585/2330 train_time:35706ms step_avg:61.04ms
step:586/2330 train_time:35768ms step_avg:61.04ms
step:587/2330 train_time:35829ms step_avg:61.04ms
step:588/2330 train_time:35890ms step_avg:61.04ms
step:589/2330 train_time:35950ms step_avg:61.04ms
step:590/2330 train_time:36012ms step_avg:61.04ms
step:591/2330 train_time:36072ms step_avg:61.04ms
step:592/2330 train_time:36134ms step_avg:61.04ms
step:593/2330 train_time:36194ms step_avg:61.04ms
step:594/2330 train_time:36256ms step_avg:61.04ms
step:595/2330 train_time:36316ms step_avg:61.04ms
step:596/2330 train_time:36379ms step_avg:61.04ms
step:597/2330 train_time:36440ms step_avg:61.04ms
step:598/2330 train_time:36502ms step_avg:61.04ms
step:599/2330 train_time:36562ms step_avg:61.04ms
step:600/2330 train_time:36624ms step_avg:61.04ms
step:601/2330 train_time:36684ms step_avg:61.04ms
step:602/2330 train_time:36745ms step_avg:61.04ms
step:603/2330 train_time:36806ms step_avg:61.04ms
step:604/2330 train_time:36868ms step_avg:61.04ms
step:605/2330 train_time:36929ms step_avg:61.04ms
step:606/2330 train_time:36990ms step_avg:61.04ms
step:607/2330 train_time:37050ms step_avg:61.04ms
step:608/2330 train_time:37112ms step_avg:61.04ms
step:609/2330 train_time:37172ms step_avg:61.04ms
step:610/2330 train_time:37234ms step_avg:61.04ms
step:611/2330 train_time:37294ms step_avg:61.04ms
step:612/2330 train_time:37356ms step_avg:61.04ms
step:613/2330 train_time:37416ms step_avg:61.04ms
step:614/2330 train_time:37479ms step_avg:61.04ms
step:615/2330 train_time:37540ms step_avg:61.04ms
step:616/2330 train_time:37602ms step_avg:61.04ms
step:617/2330 train_time:37663ms step_avg:61.04ms
step:618/2330 train_time:37725ms step_avg:61.04ms
step:619/2330 train_time:37786ms step_avg:61.04ms
step:620/2330 train_time:37848ms step_avg:61.04ms
step:621/2330 train_time:37907ms step_avg:61.04ms
step:622/2330 train_time:37969ms step_avg:61.04ms
step:623/2330 train_time:38029ms step_avg:61.04ms
step:624/2330 train_time:38090ms step_avg:61.04ms
step:625/2330 train_time:38150ms step_avg:61.04ms
step:626/2330 train_time:38212ms step_avg:61.04ms
step:627/2330 train_time:38272ms step_avg:61.04ms
step:628/2330 train_time:38334ms step_avg:61.04ms
step:629/2330 train_time:38395ms step_avg:61.04ms
step:630/2330 train_time:38457ms step_avg:61.04ms
step:631/2330 train_time:38518ms step_avg:61.04ms
step:632/2330 train_time:38580ms step_avg:61.04ms
step:633/2330 train_time:38641ms step_avg:61.04ms
step:634/2330 train_time:38703ms step_avg:61.05ms
step:635/2330 train_time:38764ms step_avg:61.05ms
step:636/2330 train_time:38826ms step_avg:61.05ms
step:637/2330 train_time:38887ms step_avg:61.05ms
step:638/2330 train_time:38949ms step_avg:61.05ms
step:639/2330 train_time:39008ms step_avg:61.05ms
step:640/2330 train_time:39069ms step_avg:61.05ms
step:641/2330 train_time:39130ms step_avg:61.05ms
step:642/2330 train_time:39191ms step_avg:61.05ms
step:643/2330 train_time:39251ms step_avg:61.04ms
step:644/2330 train_time:39313ms step_avg:61.04ms
step:645/2330 train_time:39373ms step_avg:61.04ms
step:646/2330 train_time:39435ms step_avg:61.04ms
step:647/2330 train_time:39495ms step_avg:61.04ms
step:648/2330 train_time:39558ms step_avg:61.05ms
step:649/2330 train_time:39619ms step_avg:61.05ms
step:650/2330 train_time:39681ms step_avg:61.05ms
step:651/2330 train_time:39742ms step_avg:61.05ms
step:652/2330 train_time:39804ms step_avg:61.05ms
step:653/2330 train_time:39864ms step_avg:61.05ms
step:654/2330 train_time:39927ms step_avg:61.05ms
step:655/2330 train_time:39987ms step_avg:61.05ms
step:656/2330 train_time:40049ms step_avg:61.05ms
step:657/2330 train_time:40109ms step_avg:61.05ms
step:658/2330 train_time:40171ms step_avg:61.05ms
step:659/2330 train_time:40230ms step_avg:61.05ms
step:660/2330 train_time:40292ms step_avg:61.05ms
step:661/2330 train_time:40352ms step_avg:61.05ms
step:662/2330 train_time:40413ms step_avg:61.05ms
step:663/2330 train_time:40473ms step_avg:61.05ms
step:664/2330 train_time:40535ms step_avg:61.05ms
step:665/2330 train_time:40596ms step_avg:61.05ms
step:666/2330 train_time:40659ms step_avg:61.05ms
step:667/2330 train_time:40720ms step_avg:61.05ms
step:668/2330 train_time:40782ms step_avg:61.05ms
step:669/2330 train_time:40842ms step_avg:61.05ms
step:670/2330 train_time:40904ms step_avg:61.05ms
step:671/2330 train_time:40964ms step_avg:61.05ms
step:672/2330 train_time:41026ms step_avg:61.05ms
step:673/2330 train_time:41086ms step_avg:61.05ms
step:674/2330 train_time:41148ms step_avg:61.05ms
step:675/2330 train_time:41208ms step_avg:61.05ms
step:676/2330 train_time:41270ms step_avg:61.05ms
step:677/2330 train_time:41331ms step_avg:61.05ms
step:678/2330 train_time:41393ms step_avg:61.05ms
step:679/2330 train_time:41453ms step_avg:61.05ms
step:680/2330 train_time:41514ms step_avg:61.05ms
step:681/2330 train_time:41574ms step_avg:61.05ms
step:682/2330 train_time:41637ms step_avg:61.05ms
step:683/2330 train_time:41698ms step_avg:61.05ms
step:684/2330 train_time:41761ms step_avg:61.05ms
step:685/2330 train_time:41821ms step_avg:61.05ms
step:686/2330 train_time:41883ms step_avg:61.05ms
step:687/2330 train_time:41943ms step_avg:61.05ms
step:688/2330 train_time:42005ms step_avg:61.05ms
step:689/2330 train_time:42065ms step_avg:61.05ms
step:690/2330 train_time:42127ms step_avg:61.05ms
step:691/2330 train_time:42187ms step_avg:61.05ms
step:692/2330 train_time:42249ms step_avg:61.05ms
step:693/2330 train_time:42311ms step_avg:61.05ms
step:694/2330 train_time:42373ms step_avg:61.06ms
step:695/2330 train_time:42433ms step_avg:61.05ms
step:696/2330 train_time:42495ms step_avg:61.06ms
step:697/2330 train_time:42554ms step_avg:61.05ms
step:698/2330 train_time:42616ms step_avg:61.05ms
step:699/2330 train_time:42676ms step_avg:61.05ms
step:700/2330 train_time:42739ms step_avg:61.06ms
step:701/2330 train_time:42800ms step_avg:61.06ms
step:702/2330 train_time:42863ms step_avg:61.06ms
step:703/2330 train_time:42922ms step_avg:61.06ms
step:704/2330 train_time:42984ms step_avg:61.06ms
step:705/2330 train_time:43044ms step_avg:61.06ms
step:706/2330 train_time:43106ms step_avg:61.06ms
step:707/2330 train_time:43167ms step_avg:61.06ms
step:708/2330 train_time:43230ms step_avg:61.06ms
step:709/2330 train_time:43290ms step_avg:61.06ms
step:710/2330 train_time:43351ms step_avg:61.06ms
step:711/2330 train_time:43411ms step_avg:61.06ms
step:712/2330 train_time:43473ms step_avg:61.06ms
step:713/2330 train_time:43532ms step_avg:61.06ms
step:714/2330 train_time:43594ms step_avg:61.06ms
step:715/2330 train_time:43654ms step_avg:61.05ms
step:716/2330 train_time:43717ms step_avg:61.06ms
step:717/2330 train_time:43778ms step_avg:61.06ms
step:718/2330 train_time:43841ms step_avg:61.06ms
step:719/2330 train_time:43902ms step_avg:61.06ms
step:720/2330 train_time:43964ms step_avg:61.06ms
step:721/2330 train_time:44024ms step_avg:61.06ms
step:722/2330 train_time:44086ms step_avg:61.06ms
step:723/2330 train_time:44146ms step_avg:61.06ms
step:724/2330 train_time:44208ms step_avg:61.06ms
step:725/2330 train_time:44267ms step_avg:61.06ms
step:726/2330 train_time:44329ms step_avg:61.06ms
step:727/2330 train_time:44389ms step_avg:61.06ms
step:728/2330 train_time:44451ms step_avg:61.06ms
step:729/2330 train_time:44511ms step_avg:61.06ms
step:730/2330 train_time:44572ms step_avg:61.06ms
step:731/2330 train_time:44632ms step_avg:61.06ms
step:732/2330 train_time:44694ms step_avg:61.06ms
step:733/2330 train_time:44754ms step_avg:61.06ms
step:734/2330 train_time:44816ms step_avg:61.06ms
step:735/2330 train_time:44877ms step_avg:61.06ms
step:736/2330 train_time:44941ms step_avg:61.06ms
step:737/2330 train_time:45001ms step_avg:61.06ms
step:738/2330 train_time:45063ms step_avg:61.06ms
step:739/2330 train_time:45123ms step_avg:61.06ms
step:740/2330 train_time:45186ms step_avg:61.06ms
step:741/2330 train_time:45246ms step_avg:61.06ms
step:742/2330 train_time:45308ms step_avg:61.06ms
step:743/2330 train_time:45367ms step_avg:61.06ms
step:744/2330 train_time:45429ms step_avg:61.06ms
step:745/2330 train_time:45489ms step_avg:61.06ms
step:746/2330 train_time:45551ms step_avg:61.06ms
step:747/2330 train_time:45611ms step_avg:61.06ms
step:748/2330 train_time:45672ms step_avg:61.06ms
step:749/2330 train_time:45732ms step_avg:61.06ms
step:750/2330 train_time:45794ms step_avg:61.06ms
step:750/2330 val_loss:3.7941 train_time:45859ms step_avg:61.14ms
step:751/2330 train_time:45881ms step_avg:61.09ms
step:752/2330 train_time:45918ms step_avg:61.06ms
step:753/2330 train_time:45983ms step_avg:61.07ms
step:754/2330 train_time:46050ms step_avg:61.07ms
step:755/2330 train_time:46112ms step_avg:61.08ms
step:756/2330 train_time:46174ms step_avg:61.08ms
step:757/2330 train_time:46233ms step_avg:61.07ms
step:758/2330 train_time:46295ms step_avg:61.08ms
step:759/2330 train_time:46355ms step_avg:61.07ms
step:760/2330 train_time:46416ms step_avg:61.07ms
step:761/2330 train_time:46475ms step_avg:61.07ms
step:762/2330 train_time:46536ms step_avg:61.07ms
step:763/2330 train_time:46595ms step_avg:61.07ms
step:764/2330 train_time:46656ms step_avg:61.07ms
step:765/2330 train_time:46715ms step_avg:61.07ms
step:766/2330 train_time:46778ms step_avg:61.07ms
step:767/2330 train_time:46838ms step_avg:61.07ms
step:768/2330 train_time:46902ms step_avg:61.07ms
step:769/2330 train_time:46965ms step_avg:61.07ms
step:770/2330 train_time:47028ms step_avg:61.08ms
step:771/2330 train_time:47090ms step_avg:61.08ms
step:772/2330 train_time:47154ms step_avg:61.08ms
step:773/2330 train_time:47215ms step_avg:61.08ms
step:774/2330 train_time:47278ms step_avg:61.08ms
step:775/2330 train_time:47338ms step_avg:61.08ms
step:776/2330 train_time:47400ms step_avg:61.08ms
step:777/2330 train_time:47460ms step_avg:61.08ms
step:778/2330 train_time:47523ms step_avg:61.08ms
step:779/2330 train_time:47583ms step_avg:61.08ms
step:780/2330 train_time:47645ms step_avg:61.08ms
step:781/2330 train_time:47706ms step_avg:61.08ms
step:782/2330 train_time:47769ms step_avg:61.09ms
step:783/2330 train_time:47830ms step_avg:61.09ms
step:784/2330 train_time:47892ms step_avg:61.09ms
step:785/2330 train_time:47954ms step_avg:61.09ms
step:786/2330 train_time:48017ms step_avg:61.09ms
step:787/2330 train_time:48078ms step_avg:61.09ms
step:788/2330 train_time:48141ms step_avg:61.09ms
step:789/2330 train_time:48202ms step_avg:61.09ms
step:790/2330 train_time:48265ms step_avg:61.09ms
step:791/2330 train_time:48326ms step_avg:61.10ms
step:792/2330 train_time:48389ms step_avg:61.10ms
step:793/2330 train_time:48450ms step_avg:61.10ms
step:794/2330 train_time:48512ms step_avg:61.10ms
step:795/2330 train_time:48572ms step_avg:61.10ms
step:796/2330 train_time:48635ms step_avg:61.10ms
step:797/2330 train_time:48695ms step_avg:61.10ms
step:798/2330 train_time:48757ms step_avg:61.10ms
step:799/2330 train_time:48817ms step_avg:61.10ms
step:800/2330 train_time:48880ms step_avg:61.10ms
step:801/2330 train_time:48940ms step_avg:61.10ms
step:802/2330 train_time:49003ms step_avg:61.10ms
step:803/2330 train_time:49064ms step_avg:61.10ms
step:804/2330 train_time:49127ms step_avg:61.10ms
step:805/2330 train_time:49188ms step_avg:61.10ms
step:806/2330 train_time:49251ms step_avg:61.10ms
step:807/2330 train_time:49312ms step_avg:61.10ms
step:808/2330 train_time:49375ms step_avg:61.11ms
step:809/2330 train_time:49435ms step_avg:61.11ms
step:810/2330 train_time:49498ms step_avg:61.11ms
step:811/2330 train_time:49558ms step_avg:61.11ms
step:812/2330 train_time:49620ms step_avg:61.11ms
step:813/2330 train_time:49680ms step_avg:61.11ms
step:814/2330 train_time:49742ms step_avg:61.11ms
step:815/2330 train_time:49803ms step_avg:61.11ms
step:816/2330 train_time:49866ms step_avg:61.11ms
step:817/2330 train_time:49927ms step_avg:61.11ms
step:818/2330 train_time:49989ms step_avg:61.11ms
step:819/2330 train_time:50050ms step_avg:61.11ms
step:820/2330 train_time:50113ms step_avg:61.11ms
step:821/2330 train_time:50174ms step_avg:61.11ms
step:822/2330 train_time:50237ms step_avg:61.12ms
step:823/2330 train_time:50298ms step_avg:61.12ms
step:824/2330 train_time:50361ms step_avg:61.12ms
step:825/2330 train_time:50421ms step_avg:61.12ms
step:826/2330 train_time:50484ms step_avg:61.12ms
step:827/2330 train_time:50545ms step_avg:61.12ms
step:828/2330 train_time:50608ms step_avg:61.12ms
step:829/2330 train_time:50669ms step_avg:61.12ms
step:830/2330 train_time:50732ms step_avg:61.12ms
step:831/2330 train_time:50793ms step_avg:61.12ms
step:832/2330 train_time:50856ms step_avg:61.13ms
step:833/2330 train_time:50917ms step_avg:61.12ms
step:834/2330 train_time:50979ms step_avg:61.13ms
step:835/2330 train_time:51039ms step_avg:61.13ms
step:836/2330 train_time:51102ms step_avg:61.13ms
step:837/2330 train_time:51163ms step_avg:61.13ms
step:838/2330 train_time:51226ms step_avg:61.13ms
step:839/2330 train_time:51287ms step_avg:61.13ms
step:840/2330 train_time:51350ms step_avg:61.13ms
step:841/2330 train_time:51411ms step_avg:61.13ms
step:842/2330 train_time:51474ms step_avg:61.13ms
step:843/2330 train_time:51534ms step_avg:61.13ms
step:844/2330 train_time:51597ms step_avg:61.13ms
step:845/2330 train_time:51657ms step_avg:61.13ms
step:846/2330 train_time:51719ms step_avg:61.13ms
step:847/2330 train_time:51780ms step_avg:61.13ms
step:848/2330 train_time:51842ms step_avg:61.13ms
step:849/2330 train_time:51903ms step_avg:61.13ms
step:850/2330 train_time:51966ms step_avg:61.14ms
step:851/2330 train_time:52026ms step_avg:61.14ms
step:852/2330 train_time:52089ms step_avg:61.14ms
step:853/2330 train_time:52149ms step_avg:61.14ms
step:854/2330 train_time:52212ms step_avg:61.14ms
step:855/2330 train_time:52273ms step_avg:61.14ms
step:856/2330 train_time:52336ms step_avg:61.14ms
step:857/2330 train_time:52396ms step_avg:61.14ms
step:858/2330 train_time:52459ms step_avg:61.14ms
step:859/2330 train_time:52520ms step_avg:61.14ms
step:860/2330 train_time:52584ms step_avg:61.14ms
step:861/2330 train_time:52644ms step_avg:61.14ms
step:862/2330 train_time:52707ms step_avg:61.15ms
step:863/2330 train_time:52768ms step_avg:61.14ms
step:864/2330 train_time:52831ms step_avg:61.15ms
step:865/2330 train_time:52891ms step_avg:61.15ms
step:866/2330 train_time:52954ms step_avg:61.15ms
step:867/2330 train_time:53015ms step_avg:61.15ms
step:868/2330 train_time:53078ms step_avg:61.15ms
step:869/2330 train_time:53137ms step_avg:61.15ms
step:870/2330 train_time:53200ms step_avg:61.15ms
step:871/2330 train_time:53260ms step_avg:61.15ms
step:872/2330 train_time:53323ms step_avg:61.15ms
step:873/2330 train_time:53384ms step_avg:61.15ms
step:874/2330 train_time:53448ms step_avg:61.15ms
step:875/2330 train_time:53509ms step_avg:61.15ms
step:876/2330 train_time:53572ms step_avg:61.16ms
step:877/2330 train_time:53632ms step_avg:61.15ms
step:878/2330 train_time:53695ms step_avg:61.16ms
step:879/2330 train_time:53756ms step_avg:61.16ms
step:880/2330 train_time:53818ms step_avg:61.16ms
step:881/2330 train_time:53878ms step_avg:61.16ms
step:882/2330 train_time:53940ms step_avg:61.16ms
step:883/2330 train_time:54000ms step_avg:61.16ms
step:884/2330 train_time:54062ms step_avg:61.16ms
step:885/2330 train_time:54123ms step_avg:61.16ms
step:886/2330 train_time:54186ms step_avg:61.16ms
step:887/2330 train_time:54247ms step_avg:61.16ms
step:888/2330 train_time:54310ms step_avg:61.16ms
step:889/2330 train_time:54371ms step_avg:61.16ms
step:890/2330 train_time:54434ms step_avg:61.16ms
step:891/2330 train_time:54494ms step_avg:61.16ms
step:892/2330 train_time:54557ms step_avg:61.16ms
step:893/2330 train_time:54618ms step_avg:61.16ms
step:894/2330 train_time:54681ms step_avg:61.16ms
step:895/2330 train_time:54741ms step_avg:61.16ms
step:896/2330 train_time:54804ms step_avg:61.17ms
step:897/2330 train_time:54865ms step_avg:61.17ms
step:898/2330 train_time:54928ms step_avg:61.17ms
step:899/2330 train_time:54989ms step_avg:61.17ms
step:900/2330 train_time:55051ms step_avg:61.17ms
step:901/2330 train_time:55112ms step_avg:61.17ms
step:902/2330 train_time:55175ms step_avg:61.17ms
step:903/2330 train_time:55235ms step_avg:61.17ms
step:904/2330 train_time:55299ms step_avg:61.17ms
step:905/2330 train_time:55359ms step_avg:61.17ms
step:906/2330 train_time:55421ms step_avg:61.17ms
step:907/2330 train_time:55481ms step_avg:61.17ms
step:908/2330 train_time:55544ms step_avg:61.17ms
step:909/2330 train_time:55605ms step_avg:61.17ms
step:910/2330 train_time:55669ms step_avg:61.17ms
step:911/2330 train_time:55729ms step_avg:61.17ms
step:912/2330 train_time:55792ms step_avg:61.17ms
step:913/2330 train_time:55852ms step_avg:61.17ms
step:914/2330 train_time:55915ms step_avg:61.18ms
step:915/2330 train_time:55976ms step_avg:61.18ms
step:916/2330 train_time:56038ms step_avg:61.18ms
step:917/2330 train_time:56098ms step_avg:61.18ms
step:918/2330 train_time:56161ms step_avg:61.18ms
step:919/2330 train_time:56221ms step_avg:61.18ms
step:920/2330 train_time:56283ms step_avg:61.18ms
step:921/2330 train_time:56344ms step_avg:61.18ms
step:922/2330 train_time:56407ms step_avg:61.18ms
step:923/2330 train_time:56468ms step_avg:61.18ms
step:924/2330 train_time:56531ms step_avg:61.18ms
step:925/2330 train_time:56592ms step_avg:61.18ms
step:926/2330 train_time:56654ms step_avg:61.18ms
step:927/2330 train_time:56715ms step_avg:61.18ms
step:928/2330 train_time:56778ms step_avg:61.18ms
step:929/2330 train_time:56839ms step_avg:61.18ms
step:930/2330 train_time:56902ms step_avg:61.18ms
step:931/2330 train_time:56962ms step_avg:61.18ms
step:932/2330 train_time:57024ms step_avg:61.18ms
step:933/2330 train_time:57085ms step_avg:61.18ms
step:934/2330 train_time:57148ms step_avg:61.19ms
step:935/2330 train_time:57208ms step_avg:61.19ms
step:936/2330 train_time:57271ms step_avg:61.19ms
step:937/2330 train_time:57332ms step_avg:61.19ms
step:938/2330 train_time:57394ms step_avg:61.19ms
step:939/2330 train_time:57455ms step_avg:61.19ms
step:940/2330 train_time:57517ms step_avg:61.19ms
step:941/2330 train_time:57578ms step_avg:61.19ms
step:942/2330 train_time:57640ms step_avg:61.19ms
step:943/2330 train_time:57700ms step_avg:61.19ms
step:944/2330 train_time:57763ms step_avg:61.19ms
step:945/2330 train_time:57824ms step_avg:61.19ms
step:946/2330 train_time:57888ms step_avg:61.19ms
step:947/2330 train_time:57949ms step_avg:61.19ms
step:948/2330 train_time:58012ms step_avg:61.19ms
step:949/2330 train_time:58072ms step_avg:61.19ms
step:950/2330 train_time:58134ms step_avg:61.19ms
step:951/2330 train_time:58195ms step_avg:61.19ms
step:952/2330 train_time:58258ms step_avg:61.20ms
step:953/2330 train_time:58318ms step_avg:61.19ms
step:954/2330 train_time:58381ms step_avg:61.20ms
step:955/2330 train_time:58440ms step_avg:61.19ms
step:956/2330 train_time:58504ms step_avg:61.20ms
step:957/2330 train_time:58565ms step_avg:61.20ms
step:958/2330 train_time:58628ms step_avg:61.20ms
step:959/2330 train_time:58689ms step_avg:61.20ms
step:960/2330 train_time:58752ms step_avg:61.20ms
step:961/2330 train_time:58813ms step_avg:61.20ms
step:962/2330 train_time:58876ms step_avg:61.20ms
step:963/2330 train_time:58936ms step_avg:61.20ms
step:964/2330 train_time:58998ms step_avg:61.20ms
step:965/2330 train_time:59059ms step_avg:61.20ms
step:966/2330 train_time:59121ms step_avg:61.20ms
step:967/2330 train_time:59181ms step_avg:61.20ms
step:968/2330 train_time:59244ms step_avg:61.20ms
step:969/2330 train_time:59304ms step_avg:61.20ms
step:970/2330 train_time:59368ms step_avg:61.20ms
step:971/2330 train_time:59428ms step_avg:61.20ms
step:972/2330 train_time:59491ms step_avg:61.20ms
step:973/2330 train_time:59552ms step_avg:61.20ms
step:974/2330 train_time:59614ms step_avg:61.21ms
step:975/2330 train_time:59675ms step_avg:61.21ms
step:976/2330 train_time:59737ms step_avg:61.21ms
step:977/2330 train_time:59798ms step_avg:61.21ms
step:978/2330 train_time:59861ms step_avg:61.21ms
step:979/2330 train_time:59921ms step_avg:61.21ms
step:980/2330 train_time:59985ms step_avg:61.21ms
step:981/2330 train_time:60047ms step_avg:61.21ms
step:982/2330 train_time:60110ms step_avg:61.21ms
step:983/2330 train_time:60171ms step_avg:61.21ms
step:984/2330 train_time:60233ms step_avg:61.21ms
step:985/2330 train_time:60294ms step_avg:61.21ms
step:986/2330 train_time:60356ms step_avg:61.21ms
step:987/2330 train_time:60417ms step_avg:61.21ms
step:988/2330 train_time:60479ms step_avg:61.21ms
step:989/2330 train_time:60540ms step_avg:61.21ms
step:990/2330 train_time:60602ms step_avg:61.21ms
step:991/2330 train_time:60663ms step_avg:61.21ms
step:992/2330 train_time:60725ms step_avg:61.22ms
step:993/2330 train_time:60787ms step_avg:61.22ms
step:994/2330 train_time:60850ms step_avg:61.22ms
step:995/2330 train_time:60911ms step_avg:61.22ms
step:996/2330 train_time:60974ms step_avg:61.22ms
step:997/2330 train_time:61034ms step_avg:61.22ms
step:998/2330 train_time:61097ms step_avg:61.22ms
step:999/2330 train_time:61157ms step_avg:61.22ms
step:1000/2330 train_time:61219ms step_avg:61.22ms
step:1000/2330 val_loss:3.6798 train_time:61284ms step_avg:61.28ms
step:1001/2330 train_time:61306ms step_avg:61.25ms
step:1002/2330 train_time:61345ms step_avg:61.22ms
step:1003/2330 train_time:61410ms step_avg:61.23ms
step:1004/2330 train_time:61475ms step_avg:61.23ms
step:1005/2330 train_time:61536ms step_avg:61.23ms
step:1006/2330 train_time:61599ms step_avg:61.23ms
step:1007/2330 train_time:61659ms step_avg:61.23ms
step:1008/2330 train_time:61721ms step_avg:61.23ms
step:1009/2330 train_time:61781ms step_avg:61.23ms
step:1010/2330 train_time:61843ms step_avg:61.23ms
step:1011/2330 train_time:61903ms step_avg:61.23ms
step:1012/2330 train_time:61965ms step_avg:61.23ms
step:1013/2330 train_time:62025ms step_avg:61.23ms
step:1014/2330 train_time:62087ms step_avg:61.23ms
step:1015/2330 train_time:62147ms step_avg:61.23ms
step:1016/2330 train_time:62212ms step_avg:61.23ms
step:1017/2330 train_time:62274ms step_avg:61.23ms
step:1018/2330 train_time:62338ms step_avg:61.24ms
step:1019/2330 train_time:62401ms step_avg:61.24ms
step:1020/2330 train_time:62465ms step_avg:61.24ms
step:1021/2330 train_time:62526ms step_avg:61.24ms
step:1022/2330 train_time:62589ms step_avg:61.24ms
step:1023/2330 train_time:62649ms step_avg:61.24ms
step:1024/2330 train_time:62711ms step_avg:61.24ms
step:1025/2330 train_time:62771ms step_avg:61.24ms
step:1026/2330 train_time:62834ms step_avg:61.24ms
step:1027/2330 train_time:62894ms step_avg:61.24ms
step:1028/2330 train_time:62957ms step_avg:61.24ms
step:1029/2330 train_time:63017ms step_avg:61.24ms
step:1030/2330 train_time:63080ms step_avg:61.24ms
step:1031/2330 train_time:63140ms step_avg:61.24ms
step:1032/2330 train_time:63204ms step_avg:61.24ms
step:1033/2330 train_time:63265ms step_avg:61.24ms
step:1034/2330 train_time:63328ms step_avg:61.25ms
step:1035/2330 train_time:63389ms step_avg:61.25ms
step:1036/2330 train_time:63452ms step_avg:61.25ms
step:1037/2330 train_time:63513ms step_avg:61.25ms
step:1038/2330 train_time:63576ms step_avg:61.25ms
step:1039/2330 train_time:63636ms step_avg:61.25ms
step:1040/2330 train_time:63699ms step_avg:61.25ms
step:1041/2330 train_time:63760ms step_avg:61.25ms
step:1042/2330 train_time:63822ms step_avg:61.25ms
step:1043/2330 train_time:63883ms step_avg:61.25ms
step:1044/2330 train_time:63945ms step_avg:61.25ms
step:1045/2330 train_time:64005ms step_avg:61.25ms
step:1046/2330 train_time:64067ms step_avg:61.25ms
step:1047/2330 train_time:64127ms step_avg:61.25ms
step:1048/2330 train_time:64190ms step_avg:61.25ms
step:1049/2330 train_time:64250ms step_avg:61.25ms
step:1050/2330 train_time:64314ms step_avg:61.25ms
step:1051/2330 train_time:64375ms step_avg:61.25ms
step:1052/2330 train_time:64438ms step_avg:61.25ms
step:1053/2330 train_time:64500ms step_avg:61.25ms
step:1054/2330 train_time:64563ms step_avg:61.26ms
step:1055/2330 train_time:64624ms step_avg:61.25ms
step:1056/2330 train_time:64687ms step_avg:61.26ms
step:1057/2330 train_time:64747ms step_avg:61.26ms
step:1058/2330 train_time:64809ms step_avg:61.26ms
step:1059/2330 train_time:64870ms step_avg:61.26ms
step:1060/2330 train_time:64932ms step_avg:61.26ms
step:1061/2330 train_time:64992ms step_avg:61.26ms
step:1062/2330 train_time:65055ms step_avg:61.26ms
step:1063/2330 train_time:65116ms step_avg:61.26ms
step:1064/2330 train_time:65179ms step_avg:61.26ms
step:1065/2330 train_time:65240ms step_avg:61.26ms
step:1066/2330 train_time:65303ms step_avg:61.26ms
step:1067/2330 train_time:65364ms step_avg:61.26ms
step:1068/2330 train_time:65427ms step_avg:61.26ms
step:1069/2330 train_time:65487ms step_avg:61.26ms
step:1070/2330 train_time:65550ms step_avg:61.26ms
step:1071/2330 train_time:65610ms step_avg:61.26ms
step:1072/2330 train_time:65672ms step_avg:61.26ms
step:1073/2330 train_time:65733ms step_avg:61.26ms
step:1074/2330 train_time:65797ms step_avg:61.26ms
step:1075/2330 train_time:65857ms step_avg:61.26ms
step:1076/2330 train_time:65920ms step_avg:61.26ms
step:1077/2330 train_time:65981ms step_avg:61.26ms
step:1078/2330 train_time:66043ms step_avg:61.26ms
step:1079/2330 train_time:66104ms step_avg:61.26ms
step:1080/2330 train_time:66166ms step_avg:61.27ms
step:1081/2330 train_time:66226ms step_avg:61.26ms
step:1082/2330 train_time:66289ms step_avg:61.27ms
step:1083/2330 train_time:66350ms step_avg:61.26ms
step:1084/2330 train_time:66413ms step_avg:61.27ms
step:1085/2330 train_time:66475ms step_avg:61.27ms
step:1086/2330 train_time:66538ms step_avg:61.27ms
step:1087/2330 train_time:66599ms step_avg:61.27ms
step:1088/2330 train_time:66661ms step_avg:61.27ms
step:1089/2330 train_time:66722ms step_avg:61.27ms
step:1090/2330 train_time:66785ms step_avg:61.27ms
step:1091/2330 train_time:66846ms step_avg:61.27ms
step:1092/2330 train_time:66909ms step_avg:61.27ms
step:1093/2330 train_time:66969ms step_avg:61.27ms
step:1094/2330 train_time:67032ms step_avg:61.27ms
step:1095/2330 train_time:67093ms step_avg:61.27ms
step:1096/2330 train_time:67156ms step_avg:61.27ms
step:1097/2330 train_time:67217ms step_avg:61.27ms
step:1098/2330 train_time:67280ms step_avg:61.28ms
step:1099/2330 train_time:67341ms step_avg:61.27ms
step:1100/2330 train_time:67404ms step_avg:61.28ms
step:1101/2330 train_time:67465ms step_avg:61.28ms
step:1102/2330 train_time:67527ms step_avg:61.28ms
step:1103/2330 train_time:67587ms step_avg:61.28ms
step:1104/2330 train_time:67650ms step_avg:61.28ms
step:1105/2330 train_time:67710ms step_avg:61.28ms
step:1106/2330 train_time:67772ms step_avg:61.28ms
step:1107/2330 train_time:67833ms step_avg:61.28ms
step:1108/2330 train_time:67897ms step_avg:61.28ms
step:1109/2330 train_time:67957ms step_avg:61.28ms
step:1110/2330 train_time:68020ms step_avg:61.28ms
step:1111/2330 train_time:68081ms step_avg:61.28ms
step:1112/2330 train_time:68144ms step_avg:61.28ms
step:1113/2330 train_time:68205ms step_avg:61.28ms
step:1114/2330 train_time:68267ms step_avg:61.28ms
step:1115/2330 train_time:68328ms step_avg:61.28ms
step:1116/2330 train_time:68390ms step_avg:61.28ms
step:1117/2330 train_time:68450ms step_avg:61.28ms
step:1118/2330 train_time:68513ms step_avg:61.28ms
step:1119/2330 train_time:68574ms step_avg:61.28ms
step:1120/2330 train_time:68637ms step_avg:61.28ms
step:1121/2330 train_time:68698ms step_avg:61.28ms
step:1122/2330 train_time:68761ms step_avg:61.28ms
step:1123/2330 train_time:68821ms step_avg:61.28ms
step:1124/2330 train_time:68885ms step_avg:61.29ms
step:1125/2330 train_time:68945ms step_avg:61.28ms
step:1126/2330 train_time:69008ms step_avg:61.29ms
step:1127/2330 train_time:69068ms step_avg:61.29ms
step:1128/2330 train_time:69130ms step_avg:61.29ms
step:1129/2330 train_time:69191ms step_avg:61.29ms
step:1130/2330 train_time:69254ms step_avg:61.29ms
step:1131/2330 train_time:69316ms step_avg:61.29ms
step:1132/2330 train_time:69379ms step_avg:61.29ms
step:1133/2330 train_time:69440ms step_avg:61.29ms
step:1134/2330 train_time:69503ms step_avg:61.29ms
step:1135/2330 train_time:69564ms step_avg:61.29ms
step:1136/2330 train_time:69627ms step_avg:61.29ms
step:1137/2330 train_time:69687ms step_avg:61.29ms
step:1138/2330 train_time:69749ms step_avg:61.29ms
step:1139/2330 train_time:69809ms step_avg:61.29ms
step:1140/2330 train_time:69872ms step_avg:61.29ms
step:1141/2330 train_time:69932ms step_avg:61.29ms
step:1142/2330 train_time:69995ms step_avg:61.29ms
step:1143/2330 train_time:70056ms step_avg:61.29ms
step:1144/2330 train_time:70120ms step_avg:61.29ms
step:1145/2330 train_time:70181ms step_avg:61.29ms
step:1146/2330 train_time:70244ms step_avg:61.30ms
step:1147/2330 train_time:70305ms step_avg:61.29ms
step:1148/2330 train_time:70368ms step_avg:61.30ms
step:1149/2330 train_time:70428ms step_avg:61.30ms
step:1150/2330 train_time:70491ms step_avg:61.30ms
step:1151/2330 train_time:70551ms step_avg:61.30ms
step:1152/2330 train_time:70614ms step_avg:61.30ms
step:1153/2330 train_time:70675ms step_avg:61.30ms
step:1154/2330 train_time:70738ms step_avg:61.30ms
step:1155/2330 train_time:70799ms step_avg:61.30ms
step:1156/2330 train_time:70861ms step_avg:61.30ms
step:1157/2330 train_time:70922ms step_avg:61.30ms
step:1158/2330 train_time:70986ms step_avg:61.30ms
step:1159/2330 train_time:71046ms step_avg:61.30ms
step:1160/2330 train_time:71109ms step_avg:61.30ms
step:1161/2330 train_time:71169ms step_avg:61.30ms
step:1162/2330 train_time:71231ms step_avg:61.30ms
step:1163/2330 train_time:71292ms step_avg:61.30ms
step:1164/2330 train_time:71355ms step_avg:61.30ms
step:1165/2330 train_time:71417ms step_avg:61.30ms
step:1166/2330 train_time:71480ms step_avg:61.30ms
step:1167/2330 train_time:71541ms step_avg:61.30ms
step:1168/2330 train_time:71604ms step_avg:61.30ms
step:1169/2330 train_time:71665ms step_avg:61.30ms
step:1170/2330 train_time:71727ms step_avg:61.31ms
step:1171/2330 train_time:71788ms step_avg:61.31ms
step:1172/2330 train_time:71850ms step_avg:61.31ms
step:1173/2330 train_time:71911ms step_avg:61.30ms
step:1174/2330 train_time:71973ms step_avg:61.31ms
step:1175/2330 train_time:72034ms step_avg:61.31ms
step:1176/2330 train_time:72098ms step_avg:61.31ms
step:1177/2330 train_time:72158ms step_avg:61.31ms
step:1178/2330 train_time:72221ms step_avg:61.31ms
step:1179/2330 train_time:72282ms step_avg:61.31ms
step:1180/2330 train_time:72345ms step_avg:61.31ms
step:1181/2330 train_time:72405ms step_avg:61.31ms
step:1182/2330 train_time:72468ms step_avg:61.31ms
step:1183/2330 train_time:72528ms step_avg:61.31ms
step:1184/2330 train_time:72590ms step_avg:61.31ms
step:1185/2330 train_time:72652ms step_avg:61.31ms
step:1186/2330 train_time:72714ms step_avg:61.31ms
step:1187/2330 train_time:72775ms step_avg:61.31ms
step:1188/2330 train_time:72838ms step_avg:61.31ms
step:1189/2330 train_time:72899ms step_avg:61.31ms
step:1190/2330 train_time:72961ms step_avg:61.31ms
step:1191/2330 train_time:73022ms step_avg:61.31ms
step:1192/2330 train_time:73085ms step_avg:61.31ms
step:1193/2330 train_time:73146ms step_avg:61.31ms
step:1194/2330 train_time:73209ms step_avg:61.31ms
step:1195/2330 train_time:73269ms step_avg:61.31ms
step:1196/2330 train_time:73332ms step_avg:61.31ms
step:1197/2330 train_time:73393ms step_avg:61.31ms
step:1198/2330 train_time:73456ms step_avg:61.32ms
step:1199/2330 train_time:73517ms step_avg:61.32ms
step:1200/2330 train_time:73580ms step_avg:61.32ms
step:1201/2330 train_time:73642ms step_avg:61.32ms
step:1202/2330 train_time:73705ms step_avg:61.32ms
step:1203/2330 train_time:73765ms step_avg:61.32ms
step:1204/2330 train_time:73827ms step_avg:61.32ms
step:1205/2330 train_time:73887ms step_avg:61.32ms
step:1206/2330 train_time:73949ms step_avg:61.32ms
step:1207/2330 train_time:74010ms step_avg:61.32ms
step:1208/2330 train_time:74073ms step_avg:61.32ms
step:1209/2330 train_time:74134ms step_avg:61.32ms
step:1210/2330 train_time:74197ms step_avg:61.32ms
step:1211/2330 train_time:74258ms step_avg:61.32ms
step:1212/2330 train_time:74321ms step_avg:61.32ms
step:1213/2330 train_time:74382ms step_avg:61.32ms
step:1214/2330 train_time:74445ms step_avg:61.32ms
step:1215/2330 train_time:74506ms step_avg:61.32ms
step:1216/2330 train_time:74568ms step_avg:61.32ms
step:1217/2330 train_time:74628ms step_avg:61.32ms
step:1218/2330 train_time:74691ms step_avg:61.32ms
step:1219/2330 train_time:74751ms step_avg:61.32ms
step:1220/2330 train_time:74814ms step_avg:61.32ms
step:1221/2330 train_time:74875ms step_avg:61.32ms
step:1222/2330 train_time:74939ms step_avg:61.32ms
step:1223/2330 train_time:75000ms step_avg:61.32ms
step:1224/2330 train_time:75063ms step_avg:61.33ms
step:1225/2330 train_time:75123ms step_avg:61.33ms
step:1226/2330 train_time:75186ms step_avg:61.33ms
step:1227/2330 train_time:75247ms step_avg:61.33ms
step:1228/2330 train_time:75309ms step_avg:61.33ms
step:1229/2330 train_time:75370ms step_avg:61.33ms
step:1230/2330 train_time:75432ms step_avg:61.33ms
step:1231/2330 train_time:75493ms step_avg:61.33ms
step:1232/2330 train_time:75556ms step_avg:61.33ms
step:1233/2330 train_time:75617ms step_avg:61.33ms
step:1234/2330 train_time:75680ms step_avg:61.33ms
step:1235/2330 train_time:75740ms step_avg:61.33ms
step:1236/2330 train_time:75804ms step_avg:61.33ms
step:1237/2330 train_time:75865ms step_avg:61.33ms
step:1238/2330 train_time:75927ms step_avg:61.33ms
step:1239/2330 train_time:75988ms step_avg:61.33ms
step:1240/2330 train_time:76050ms step_avg:61.33ms
step:1241/2330 train_time:76113ms step_avg:61.33ms
step:1242/2330 train_time:76176ms step_avg:61.33ms
step:1243/2330 train_time:76236ms step_avg:61.33ms
step:1244/2330 train_time:76299ms step_avg:61.33ms
step:1245/2330 train_time:76361ms step_avg:61.33ms
step:1246/2330 train_time:76423ms step_avg:61.33ms
step:1247/2330 train_time:76483ms step_avg:61.33ms
step:1248/2330 train_time:76546ms step_avg:61.33ms
step:1249/2330 train_time:76607ms step_avg:61.33ms
step:1250/2330 train_time:76669ms step_avg:61.33ms
step:1250/2330 val_loss:3.5938 train_time:76734ms step_avg:61.39ms
step:1251/2330 train_time:76755ms step_avg:61.35ms
step:1252/2330 train_time:76794ms step_avg:61.34ms
step:1253/2330 train_time:76858ms step_avg:61.34ms
step:1254/2330 train_time:76923ms step_avg:61.34ms
step:1255/2330 train_time:76984ms step_avg:61.34ms
step:1256/2330 train_time:77047ms step_avg:61.34ms
step:1257/2330 train_time:77107ms step_avg:61.34ms
step:1258/2330 train_time:77169ms step_avg:61.34ms
step:1259/2330 train_time:77229ms step_avg:61.34ms
step:1260/2330 train_time:77291ms step_avg:61.34ms
step:1261/2330 train_time:77350ms step_avg:61.34ms
step:1262/2330 train_time:77412ms step_avg:61.34ms
step:1263/2330 train_time:77472ms step_avg:61.34ms
step:1264/2330 train_time:77535ms step_avg:61.34ms
step:1265/2330 train_time:77594ms step_avg:61.34ms
step:1266/2330 train_time:77656ms step_avg:61.34ms
step:1267/2330 train_time:77716ms step_avg:61.34ms
step:1268/2330 train_time:77780ms step_avg:61.34ms
step:1269/2330 train_time:77842ms step_avg:61.34ms
step:1270/2330 train_time:77905ms step_avg:61.34ms
step:1271/2330 train_time:77967ms step_avg:61.34ms
step:1272/2330 train_time:78029ms step_avg:61.34ms
step:1273/2330 train_time:78090ms step_avg:61.34ms
step:1274/2330 train_time:78152ms step_avg:61.34ms
step:1275/2330 train_time:78212ms step_avg:61.34ms
step:1276/2330 train_time:78274ms step_avg:61.34ms
step:1277/2330 train_time:78334ms step_avg:61.34ms
step:1278/2330 train_time:78396ms step_avg:61.34ms
step:1279/2330 train_time:78455ms step_avg:61.34ms
step:1280/2330 train_time:78518ms step_avg:61.34ms
step:1281/2330 train_time:78578ms step_avg:61.34ms
step:1282/2330 train_time:78640ms step_avg:61.34ms
step:1283/2330 train_time:78700ms step_avg:61.34ms
step:1284/2330 train_time:78764ms step_avg:61.34ms
step:1285/2330 train_time:78826ms step_avg:61.34ms
step:1286/2330 train_time:78888ms step_avg:61.34ms
step:1287/2330 train_time:78950ms step_avg:61.34ms
step:1288/2330 train_time:79013ms step_avg:61.35ms
step:1289/2330 train_time:79073ms step_avg:61.34ms
step:1290/2330 train_time:79136ms step_avg:61.35ms
step:1291/2330 train_time:79196ms step_avg:61.34ms
step:1292/2330 train_time:79258ms step_avg:61.35ms
step:1293/2330 train_time:79318ms step_avg:61.34ms
step:1294/2330 train_time:79380ms step_avg:61.34ms
step:1295/2330 train_time:79440ms step_avg:61.34ms
step:1296/2330 train_time:79503ms step_avg:61.34ms
step:1297/2330 train_time:79563ms step_avg:61.34ms
step:1298/2330 train_time:79625ms step_avg:61.34ms
step:1299/2330 train_time:79686ms step_avg:61.34ms
step:1300/2330 train_time:79749ms step_avg:61.35ms
step:1301/2330 train_time:79810ms step_avg:61.34ms
step:1302/2330 train_time:79872ms step_avg:61.35ms
step:1303/2330 train_time:79934ms step_avg:61.35ms
step:1304/2330 train_time:79997ms step_avg:61.35ms
step:1305/2330 train_time:80057ms step_avg:61.35ms
step:1306/2330 train_time:80120ms step_avg:61.35ms
step:1307/2330 train_time:80181ms step_avg:61.35ms
step:1308/2330 train_time:80244ms step_avg:61.35ms
step:1309/2330 train_time:80305ms step_avg:61.35ms
step:1310/2330 train_time:80367ms step_avg:61.35ms
step:1311/2330 train_time:80428ms step_avg:61.35ms
step:1312/2330 train_time:80490ms step_avg:61.35ms
step:1313/2330 train_time:80551ms step_avg:61.35ms
step:1314/2330 train_time:80614ms step_avg:61.35ms
step:1315/2330 train_time:80674ms step_avg:61.35ms
step:1316/2330 train_time:80737ms step_avg:61.35ms
step:1317/2330 train_time:80796ms step_avg:61.35ms
step:1318/2330 train_time:80859ms step_avg:61.35ms
step:1319/2330 train_time:80919ms step_avg:61.35ms
step:1320/2330 train_time:80981ms step_avg:61.35ms
step:1321/2330 train_time:81042ms step_avg:61.35ms
step:1322/2330 train_time:81106ms step_avg:61.35ms
step:1323/2330 train_time:81167ms step_avg:61.35ms
step:1324/2330 train_time:81229ms step_avg:61.35ms
step:1325/2330 train_time:81290ms step_avg:61.35ms
step:1326/2330 train_time:81352ms step_avg:61.35ms
step:1327/2330 train_time:81414ms step_avg:61.35ms
step:1328/2330 train_time:81476ms step_avg:61.35ms
step:1329/2330 train_time:81535ms step_avg:61.35ms
step:1330/2330 train_time:81597ms step_avg:61.35ms
step:1331/2330 train_time:81657ms step_avg:61.35ms
step:1332/2330 train_time:81719ms step_avg:61.35ms
step:1333/2330 train_time:81780ms step_avg:61.35ms
step:1334/2330 train_time:81842ms step_avg:61.35ms
step:1335/2330 train_time:81903ms step_avg:61.35ms
step:1336/2330 train_time:81965ms step_avg:61.35ms
step:1337/2330 train_time:82026ms step_avg:61.35ms
step:1338/2330 train_time:82089ms step_avg:61.35ms
step:1339/2330 train_time:82150ms step_avg:61.35ms
step:1340/2330 train_time:82213ms step_avg:61.35ms
step:1341/2330 train_time:82274ms step_avg:61.35ms
step:1342/2330 train_time:82336ms step_avg:61.35ms
step:1343/2330 train_time:82396ms step_avg:61.35ms
step:1344/2330 train_time:82458ms step_avg:61.35ms
step:1345/2330 train_time:82519ms step_avg:61.35ms
step:1346/2330 train_time:82581ms step_avg:61.35ms
step:1347/2330 train_time:82641ms step_avg:61.35ms
step:1348/2330 train_time:82703ms step_avg:61.35ms
step:1349/2330 train_time:82764ms step_avg:61.35ms
step:1350/2330 train_time:82828ms step_avg:61.35ms
step:1351/2330 train_time:82888ms step_avg:61.35ms
step:1352/2330 train_time:82951ms step_avg:61.35ms
step:1353/2330 train_time:83012ms step_avg:61.35ms
step:1354/2330 train_time:83074ms step_avg:61.35ms
step:1355/2330 train_time:83134ms step_avg:61.35ms
step:1356/2330 train_time:83197ms step_avg:61.35ms
step:1357/2330 train_time:83257ms step_avg:61.35ms
step:1358/2330 train_time:83320ms step_avg:61.35ms
step:1359/2330 train_time:83381ms step_avg:61.35ms
step:1360/2330 train_time:83444ms step_avg:61.36ms
step:1361/2330 train_time:83504ms step_avg:61.36ms
step:1362/2330 train_time:83567ms step_avg:61.36ms
step:1363/2330 train_time:83628ms step_avg:61.36ms
step:1364/2330 train_time:83690ms step_avg:61.36ms
step:1365/2330 train_time:83751ms step_avg:61.36ms
step:1366/2330 train_time:83813ms step_avg:61.36ms
step:1367/2330 train_time:83874ms step_avg:61.36ms
step:1368/2330 train_time:83937ms step_avg:61.36ms
step:1369/2330 train_time:83997ms step_avg:61.36ms
step:1370/2330 train_time:84059ms step_avg:61.36ms
step:1371/2330 train_time:84119ms step_avg:61.36ms
step:1372/2330 train_time:84182ms step_avg:61.36ms
step:1373/2330 train_time:84242ms step_avg:61.36ms
step:1374/2330 train_time:84304ms step_avg:61.36ms
step:1375/2330 train_time:84365ms step_avg:61.36ms
step:1376/2330 train_time:84429ms step_avg:61.36ms
step:1377/2330 train_time:84489ms step_avg:61.36ms
step:1378/2330 train_time:84552ms step_avg:61.36ms
step:1379/2330 train_time:84612ms step_avg:61.36ms
step:1380/2330 train_time:84674ms step_avg:61.36ms
step:1381/2330 train_time:84734ms step_avg:61.36ms
step:1382/2330 train_time:84797ms step_avg:61.36ms
step:1383/2330 train_time:84857ms step_avg:61.36ms
step:1384/2330 train_time:84920ms step_avg:61.36ms
step:1385/2330 train_time:84980ms step_avg:61.36ms
step:1386/2330 train_time:85042ms step_avg:61.36ms
step:1387/2330 train_time:85102ms step_avg:61.36ms
step:1388/2330 train_time:85165ms step_avg:61.36ms
step:1389/2330 train_time:85225ms step_avg:61.36ms
step:1390/2330 train_time:85288ms step_avg:61.36ms
step:1391/2330 train_time:85349ms step_avg:61.36ms
step:1392/2330 train_time:85412ms step_avg:61.36ms
step:1393/2330 train_time:85473ms step_avg:61.36ms
step:1394/2330 train_time:85535ms step_avg:61.36ms
step:1395/2330 train_time:85596ms step_avg:61.36ms
step:1396/2330 train_time:85658ms step_avg:61.36ms
step:1397/2330 train_time:85718ms step_avg:61.36ms
step:1398/2330 train_time:85781ms step_avg:61.36ms
step:1399/2330 train_time:85842ms step_avg:61.36ms
step:1400/2330 train_time:85904ms step_avg:61.36ms
step:1401/2330 train_time:85965ms step_avg:61.36ms
step:1402/2330 train_time:86028ms step_avg:61.36ms
step:1403/2330 train_time:86088ms step_avg:61.36ms
step:1404/2330 train_time:86151ms step_avg:61.36ms
step:1405/2330 train_time:86211ms step_avg:61.36ms
step:1406/2330 train_time:86273ms step_avg:61.36ms
step:1407/2330 train_time:86334ms step_avg:61.36ms
step:1408/2330 train_time:86396ms step_avg:61.36ms
step:1409/2330 train_time:86457ms step_avg:61.36ms
step:1410/2330 train_time:86519ms step_avg:61.36ms
step:1411/2330 train_time:86580ms step_avg:61.36ms
step:1412/2330 train_time:86643ms step_avg:61.36ms
step:1413/2330 train_time:86704ms step_avg:61.36ms
step:1414/2330 train_time:86767ms step_avg:61.36ms
step:1415/2330 train_time:86827ms step_avg:61.36ms
step:1416/2330 train_time:86890ms step_avg:61.36ms
step:1417/2330 train_time:86951ms step_avg:61.36ms
step:1418/2330 train_time:87014ms step_avg:61.36ms
step:1419/2330 train_time:87074ms step_avg:61.36ms
step:1420/2330 train_time:87137ms step_avg:61.36ms
step:1421/2330 train_time:87196ms step_avg:61.36ms
step:1422/2330 train_time:87258ms step_avg:61.36ms
step:1423/2330 train_time:87319ms step_avg:61.36ms
step:1424/2330 train_time:87382ms step_avg:61.36ms
step:1425/2330 train_time:87442ms step_avg:61.36ms
step:1426/2330 train_time:87505ms step_avg:61.36ms
step:1427/2330 train_time:87566ms step_avg:61.36ms
step:1428/2330 train_time:87629ms step_avg:61.36ms
step:1429/2330 train_time:87689ms step_avg:61.36ms
step:1430/2330 train_time:87752ms step_avg:61.36ms
step:1431/2330 train_time:87812ms step_avg:61.36ms
step:1432/2330 train_time:87875ms step_avg:61.37ms
step:1433/2330 train_time:87936ms step_avg:61.36ms
step:1434/2330 train_time:87998ms step_avg:61.37ms
step:1435/2330 train_time:88058ms step_avg:61.36ms
step:1436/2330 train_time:88120ms step_avg:61.36ms
step:1437/2330 train_time:88181ms step_avg:61.36ms
step:1438/2330 train_time:88243ms step_avg:61.37ms
step:1439/2330 train_time:88304ms step_avg:61.36ms
step:1440/2330 train_time:88366ms step_avg:61.37ms
step:1441/2330 train_time:88427ms step_avg:61.36ms
step:1442/2330 train_time:88489ms step_avg:61.37ms
step:1443/2330 train_time:88550ms step_avg:61.37ms
step:1444/2330 train_time:88613ms step_avg:61.37ms
step:1445/2330 train_time:88673ms step_avg:61.37ms
step:1446/2330 train_time:88735ms step_avg:61.37ms
step:1447/2330 train_time:88795ms step_avg:61.36ms
step:1448/2330 train_time:88857ms step_avg:61.37ms
step:1449/2330 train_time:88917ms step_avg:61.36ms
step:1450/2330 train_time:88980ms step_avg:61.37ms
step:1451/2330 train_time:89040ms step_avg:61.36ms
step:1452/2330 train_time:89103ms step_avg:61.37ms
step:1453/2330 train_time:89164ms step_avg:61.37ms
step:1454/2330 train_time:89227ms step_avg:61.37ms
step:1455/2330 train_time:89288ms step_avg:61.37ms
step:1456/2330 train_time:89350ms step_avg:61.37ms
step:1457/2330 train_time:89410ms step_avg:61.37ms
step:1458/2330 train_time:89473ms step_avg:61.37ms
step:1459/2330 train_time:89534ms step_avg:61.37ms
step:1460/2330 train_time:89597ms step_avg:61.37ms
step:1461/2330 train_time:89657ms step_avg:61.37ms
step:1462/2330 train_time:89719ms step_avg:61.37ms
step:1463/2330 train_time:89780ms step_avg:61.37ms
step:1464/2330 train_time:89842ms step_avg:61.37ms
step:1465/2330 train_time:89903ms step_avg:61.37ms
step:1466/2330 train_time:89965ms step_avg:61.37ms
step:1467/2330 train_time:90026ms step_avg:61.37ms
step:1468/2330 train_time:90089ms step_avg:61.37ms
step:1469/2330 train_time:90149ms step_avg:61.37ms
step:1470/2330 train_time:90212ms step_avg:61.37ms
step:1471/2330 train_time:90272ms step_avg:61.37ms
step:1472/2330 train_time:90335ms step_avg:61.37ms
step:1473/2330 train_time:90395ms step_avg:61.37ms
step:1474/2330 train_time:90457ms step_avg:61.37ms
step:1475/2330 train_time:90517ms step_avg:61.37ms
step:1476/2330 train_time:90579ms step_avg:61.37ms
step:1477/2330 train_time:90640ms step_avg:61.37ms
step:1478/2330 train_time:90702ms step_avg:61.37ms
step:1479/2330 train_time:90763ms step_avg:61.37ms
step:1480/2330 train_time:90826ms step_avg:61.37ms
step:1481/2330 train_time:90887ms step_avg:61.37ms
step:1482/2330 train_time:90950ms step_avg:61.37ms
step:1483/2330 train_time:91011ms step_avg:61.37ms
step:1484/2330 train_time:91073ms step_avg:61.37ms
step:1485/2330 train_time:91134ms step_avg:61.37ms
step:1486/2330 train_time:91196ms step_avg:61.37ms
step:1487/2330 train_time:91256ms step_avg:61.37ms
step:1488/2330 train_time:91318ms step_avg:61.37ms
step:1489/2330 train_time:91379ms step_avg:61.37ms
step:1490/2330 train_time:91441ms step_avg:61.37ms
step:1491/2330 train_time:91502ms step_avg:61.37ms
step:1492/2330 train_time:91565ms step_avg:61.37ms
step:1493/2330 train_time:91627ms step_avg:61.37ms
step:1494/2330 train_time:91689ms step_avg:61.37ms
step:1495/2330 train_time:91750ms step_avg:61.37ms
step:1496/2330 train_time:91813ms step_avg:61.37ms
step:1497/2330 train_time:91874ms step_avg:61.37ms
step:1498/2330 train_time:91936ms step_avg:61.37ms
step:1499/2330 train_time:91997ms step_avg:61.37ms
step:1500/2330 train_time:92059ms step_avg:61.37ms
step:1500/2330 val_loss:3.5253 train_time:92124ms step_avg:61.42ms
step:1501/2330 train_time:92147ms step_avg:61.39ms
step:1502/2330 train_time:92185ms step_avg:61.37ms
step:1503/2330 train_time:92249ms step_avg:61.38ms
step:1504/2330 train_time:92315ms step_avg:61.38ms
step:1505/2330 train_time:92375ms step_avg:61.38ms
step:1506/2330 train_time:92437ms step_avg:61.38ms
step:1507/2330 train_time:92497ms step_avg:61.38ms
step:1508/2330 train_time:92559ms step_avg:61.38ms
step:1509/2330 train_time:92618ms step_avg:61.38ms
step:1510/2330 train_time:92680ms step_avg:61.38ms
step:1511/2330 train_time:92740ms step_avg:61.38ms
step:1512/2330 train_time:92802ms step_avg:61.38ms
step:1513/2330 train_time:92862ms step_avg:61.38ms
step:1514/2330 train_time:92924ms step_avg:61.38ms
step:1515/2330 train_time:92984ms step_avg:61.38ms
step:1516/2330 train_time:93048ms step_avg:61.38ms
step:1517/2330 train_time:93110ms step_avg:61.38ms
step:1518/2330 train_time:93174ms step_avg:61.38ms
step:1519/2330 train_time:93238ms step_avg:61.38ms
step:1520/2330 train_time:93300ms step_avg:61.38ms
step:1521/2330 train_time:93362ms step_avg:61.38ms
step:1522/2330 train_time:93425ms step_avg:61.38ms
step:1523/2330 train_time:93486ms step_avg:61.38ms
step:1524/2330 train_time:93549ms step_avg:61.38ms
step:1525/2330 train_time:93609ms step_avg:61.38ms
step:1526/2330 train_time:93671ms step_avg:61.38ms
step:1527/2330 train_time:93732ms step_avg:61.38ms
step:1528/2330 train_time:93794ms step_avg:61.38ms
step:1529/2330 train_time:93855ms step_avg:61.38ms
step:1530/2330 train_time:93918ms step_avg:61.38ms
step:1531/2330 train_time:93979ms step_avg:61.38ms
step:1532/2330 train_time:94042ms step_avg:61.39ms
step:1533/2330 train_time:94103ms step_avg:61.39ms
step:1534/2330 train_time:94167ms step_avg:61.39ms
step:1535/2330 train_time:94229ms step_avg:61.39ms
step:1536/2330 train_time:94293ms step_avg:61.39ms
step:1537/2330 train_time:94354ms step_avg:61.39ms
step:1538/2330 train_time:94417ms step_avg:61.39ms
step:1539/2330 train_time:94478ms step_avg:61.39ms
step:1540/2330 train_time:94541ms step_avg:61.39ms
step:1541/2330 train_time:94602ms step_avg:61.39ms
step:1542/2330 train_time:94665ms step_avg:61.39ms
step:1543/2330 train_time:94726ms step_avg:61.39ms
step:1544/2330 train_time:94789ms step_avg:61.39ms
step:1545/2330 train_time:94851ms step_avg:61.39ms
step:1546/2330 train_time:94914ms step_avg:61.39ms
step:1547/2330 train_time:94976ms step_avg:61.39ms
step:1548/2330 train_time:95039ms step_avg:61.39ms
step:1549/2330 train_time:95100ms step_avg:61.39ms
step:1550/2330 train_time:95162ms step_avg:61.40ms
step:1551/2330 train_time:95223ms step_avg:61.39ms
step:1552/2330 train_time:95287ms step_avg:61.40ms
step:1553/2330 train_time:95348ms step_avg:61.40ms
step:1554/2330 train_time:95411ms step_avg:61.40ms
step:1555/2330 train_time:95473ms step_avg:61.40ms
step:1556/2330 train_time:95536ms step_avg:61.40ms
step:1557/2330 train_time:95597ms step_avg:61.40ms
step:1558/2330 train_time:95660ms step_avg:61.40ms
step:1559/2330 train_time:95722ms step_avg:61.40ms
step:1560/2330 train_time:95785ms step_avg:61.40ms
step:1561/2330 train_time:95846ms step_avg:61.40ms
step:1562/2330 train_time:95909ms step_avg:61.40ms
step:1563/2330 train_time:95970ms step_avg:61.40ms
step:1564/2330 train_time:96035ms step_avg:61.40ms
step:1565/2330 train_time:96096ms step_avg:61.40ms
step:1566/2330 train_time:96159ms step_avg:61.40ms
step:1567/2330 train_time:96219ms step_avg:61.40ms
step:1568/2330 train_time:96282ms step_avg:61.40ms
step:1569/2330 train_time:96344ms step_avg:61.41ms
step:1570/2330 train_time:96407ms step_avg:61.41ms
step:1571/2330 train_time:96469ms step_avg:61.41ms
step:1572/2330 train_time:96533ms step_avg:61.41ms
step:1573/2330 train_time:96594ms step_avg:61.41ms
step:1574/2330 train_time:96658ms step_avg:61.41ms
step:1575/2330 train_time:96719ms step_avg:61.41ms
step:1576/2330 train_time:96782ms step_avg:61.41ms
step:1577/2330 train_time:96843ms step_avg:61.41ms
step:1578/2330 train_time:96907ms step_avg:61.41ms
step:1579/2330 train_time:96969ms step_avg:61.41ms
step:1580/2330 train_time:97032ms step_avg:61.41ms
step:1581/2330 train_time:97093ms step_avg:61.41ms
step:1582/2330 train_time:97156ms step_avg:61.41ms
step:1583/2330 train_time:97218ms step_avg:61.41ms
step:1584/2330 train_time:97281ms step_avg:61.41ms
step:1585/2330 train_time:97342ms step_avg:61.41ms
step:1586/2330 train_time:97406ms step_avg:61.42ms
step:1587/2330 train_time:97467ms step_avg:61.42ms
step:1588/2330 train_time:97531ms step_avg:61.42ms
step:1589/2330 train_time:97592ms step_avg:61.42ms
step:1590/2330 train_time:97657ms step_avg:61.42ms
step:1591/2330 train_time:97718ms step_avg:61.42ms
step:1592/2330 train_time:97781ms step_avg:61.42ms
step:1593/2330 train_time:97841ms step_avg:61.42ms
step:1594/2330 train_time:97905ms step_avg:61.42ms
step:1595/2330 train_time:97966ms step_avg:61.42ms
step:1596/2330 train_time:98029ms step_avg:61.42ms
step:1597/2330 train_time:98090ms step_avg:61.42ms
step:1598/2330 train_time:98153ms step_avg:61.42ms
step:1599/2330 train_time:98215ms step_avg:61.42ms
step:1600/2330 train_time:98278ms step_avg:61.42ms
step:1601/2330 train_time:98338ms step_avg:61.42ms
step:1602/2330 train_time:98401ms step_avg:61.42ms
step:1603/2330 train_time:98462ms step_avg:61.42ms
step:1604/2330 train_time:98526ms step_avg:61.43ms
step:1605/2330 train_time:98588ms step_avg:61.43ms
step:1606/2330 train_time:98651ms step_avg:61.43ms
step:1607/2330 train_time:98712ms step_avg:61.43ms
step:1608/2330 train_time:98776ms step_avg:61.43ms
step:1609/2330 train_time:98838ms step_avg:61.43ms
step:1610/2330 train_time:98901ms step_avg:61.43ms
step:1611/2330 train_time:98963ms step_avg:61.43ms
step:1612/2330 train_time:99025ms step_avg:61.43ms
step:1613/2330 train_time:99085ms step_avg:61.43ms
step:1614/2330 train_time:99149ms step_avg:61.43ms
step:1615/2330 train_time:99210ms step_avg:61.43ms
step:1616/2330 train_time:99273ms step_avg:61.43ms
step:1617/2330 train_time:99335ms step_avg:61.43ms
step:1618/2330 train_time:99398ms step_avg:61.43ms
step:1619/2330 train_time:99459ms step_avg:61.43ms
step:1620/2330 train_time:99522ms step_avg:61.43ms
step:1621/2330 train_time:99583ms step_avg:61.43ms
step:1622/2330 train_time:99647ms step_avg:61.43ms
step:1623/2330 train_time:99709ms step_avg:61.43ms
step:1624/2330 train_time:99772ms step_avg:61.44ms
step:1625/2330 train_time:99833ms step_avg:61.44ms
step:1626/2330 train_time:99897ms step_avg:61.44ms
step:1627/2330 train_time:99958ms step_avg:61.44ms
step:1628/2330 train_time:100021ms step_avg:61.44ms
step:1629/2330 train_time:100082ms step_avg:61.44ms
step:1630/2330 train_time:100145ms step_avg:61.44ms
step:1631/2330 train_time:100207ms step_avg:61.44ms
step:1632/2330 train_time:100270ms step_avg:61.44ms
step:1633/2330 train_time:100332ms step_avg:61.44ms
step:1634/2330 train_time:100395ms step_avg:61.44ms
step:1635/2330 train_time:100457ms step_avg:61.44ms
step:1636/2330 train_time:100519ms step_avg:61.44ms
step:1637/2330 train_time:100581ms step_avg:61.44ms
step:1638/2330 train_time:100644ms step_avg:61.44ms
step:1639/2330 train_time:100705ms step_avg:61.44ms
step:1640/2330 train_time:100769ms step_avg:61.44ms
step:1641/2330 train_time:100830ms step_avg:61.44ms
step:1642/2330 train_time:100895ms step_avg:61.45ms
step:1643/2330 train_time:100956ms step_avg:61.45ms
step:1644/2330 train_time:101020ms step_avg:61.45ms
step:1645/2330 train_time:101081ms step_avg:61.45ms
step:1646/2330 train_time:101143ms step_avg:61.45ms
step:1647/2330 train_time:101204ms step_avg:61.45ms
step:1648/2330 train_time:101266ms step_avg:61.45ms
step:1649/2330 train_time:101327ms step_avg:61.45ms
step:1650/2330 train_time:101392ms step_avg:61.45ms
step:1651/2330 train_time:101453ms step_avg:61.45ms
step:1652/2330 train_time:101517ms step_avg:61.45ms
step:1653/2330 train_time:101578ms step_avg:61.45ms
step:1654/2330 train_time:101641ms step_avg:61.45ms
step:1655/2330 train_time:101701ms step_avg:61.45ms
step:1656/2330 train_time:101764ms step_avg:61.45ms
step:1657/2330 train_time:101826ms step_avg:61.45ms
step:1658/2330 train_time:101890ms step_avg:61.45ms
step:1659/2330 train_time:101951ms step_avg:61.45ms
step:1660/2330 train_time:102014ms step_avg:61.45ms
step:1661/2330 train_time:102076ms step_avg:61.45ms
step:1662/2330 train_time:102139ms step_avg:61.46ms
step:1663/2330 train_time:102199ms step_avg:61.45ms
step:1664/2330 train_time:102263ms step_avg:61.46ms
step:1665/2330 train_time:102323ms step_avg:61.46ms
step:1666/2330 train_time:102387ms step_avg:61.46ms
step:1667/2330 train_time:102448ms step_avg:61.46ms
step:1668/2330 train_time:102511ms step_avg:61.46ms
step:1669/2330 train_time:102573ms step_avg:61.46ms
step:1670/2330 train_time:102636ms step_avg:61.46ms
step:1671/2330 train_time:102697ms step_avg:61.46ms
step:1672/2330 train_time:102760ms step_avg:61.46ms
step:1673/2330 train_time:102820ms step_avg:61.46ms
step:1674/2330 train_time:102884ms step_avg:61.46ms
step:1675/2330 train_time:102946ms step_avg:61.46ms
step:1676/2330 train_time:103009ms step_avg:61.46ms
step:1677/2330 train_time:103071ms step_avg:61.46ms
step:1678/2330 train_time:103134ms step_avg:61.46ms
step:1679/2330 train_time:103195ms step_avg:61.46ms
step:1680/2330 train_time:103259ms step_avg:61.46ms
step:1681/2330 train_time:103319ms step_avg:61.46ms
step:1682/2330 train_time:103382ms step_avg:61.46ms
step:1683/2330 train_time:103443ms step_avg:61.46ms
step:1684/2330 train_time:103506ms step_avg:61.46ms
step:1685/2330 train_time:103568ms step_avg:61.46ms
step:1686/2330 train_time:103631ms step_avg:61.47ms
step:1687/2330 train_time:103692ms step_avg:61.47ms
step:1688/2330 train_time:103755ms step_avg:61.47ms
step:1689/2330 train_time:103816ms step_avg:61.47ms
step:1690/2330 train_time:103880ms step_avg:61.47ms
step:1691/2330 train_time:103941ms step_avg:61.47ms
step:1692/2330 train_time:104004ms step_avg:61.47ms
step:1693/2330 train_time:104064ms step_avg:61.47ms
step:1694/2330 train_time:104128ms step_avg:61.47ms
step:1695/2330 train_time:104189ms step_avg:61.47ms
step:1696/2330 train_time:104252ms step_avg:61.47ms
step:1697/2330 train_time:104314ms step_avg:61.47ms
step:1698/2330 train_time:104377ms step_avg:61.47ms
step:1699/2330 train_time:104438ms step_avg:61.47ms
step:1700/2330 train_time:104501ms step_avg:61.47ms
step:1701/2330 train_time:104561ms step_avg:61.47ms
step:1702/2330 train_time:104624ms step_avg:61.47ms
step:1703/2330 train_time:104685ms step_avg:61.47ms
step:1704/2330 train_time:104749ms step_avg:61.47ms
step:1705/2330 train_time:104810ms step_avg:61.47ms
step:1706/2330 train_time:104874ms step_avg:61.47ms
step:1707/2330 train_time:104936ms step_avg:61.47ms
step:1708/2330 train_time:104999ms step_avg:61.47ms
step:1709/2330 train_time:105059ms step_avg:61.47ms
step:1710/2330 train_time:105122ms step_avg:61.47ms
step:1711/2330 train_time:105184ms step_avg:61.47ms
step:1712/2330 train_time:105247ms step_avg:61.48ms
step:1713/2330 train_time:105307ms step_avg:61.48ms
step:1714/2330 train_time:105371ms step_avg:61.48ms
step:1715/2330 train_time:105432ms step_avg:61.48ms
step:1716/2330 train_time:105495ms step_avg:61.48ms
step:1717/2330 train_time:105556ms step_avg:61.48ms
step:1718/2330 train_time:105619ms step_avg:61.48ms
step:1719/2330 train_time:105680ms step_avg:61.48ms
step:1720/2330 train_time:105743ms step_avg:61.48ms
step:1721/2330 train_time:105804ms step_avg:61.48ms
step:1722/2330 train_time:105867ms step_avg:61.48ms
step:1723/2330 train_time:105929ms step_avg:61.48ms
step:1724/2330 train_time:105992ms step_avg:61.48ms
step:1725/2330 train_time:106053ms step_avg:61.48ms
step:1726/2330 train_time:106117ms step_avg:61.48ms
step:1727/2330 train_time:106177ms step_avg:61.48ms
step:1728/2330 train_time:106240ms step_avg:61.48ms
step:1729/2330 train_time:106301ms step_avg:61.48ms
step:1730/2330 train_time:106364ms step_avg:61.48ms
step:1731/2330 train_time:106425ms step_avg:61.48ms
step:1732/2330 train_time:106488ms step_avg:61.48ms
step:1733/2330 train_time:106550ms step_avg:61.48ms
step:1734/2330 train_time:106613ms step_avg:61.48ms
step:1735/2330 train_time:106674ms step_avg:61.48ms
step:1736/2330 train_time:106737ms step_avg:61.48ms
step:1737/2330 train_time:106798ms step_avg:61.48ms
step:1738/2330 train_time:106860ms step_avg:61.48ms
step:1739/2330 train_time:106921ms step_avg:61.48ms
step:1740/2330 train_time:106985ms step_avg:61.49ms
step:1741/2330 train_time:107047ms step_avg:61.49ms
step:1742/2330 train_time:107110ms step_avg:61.49ms
step:1743/2330 train_time:107171ms step_avg:61.49ms
step:1744/2330 train_time:107234ms step_avg:61.49ms
step:1745/2330 train_time:107295ms step_avg:61.49ms
step:1746/2330 train_time:107358ms step_avg:61.49ms
step:1747/2330 train_time:107419ms step_avg:61.49ms
step:1748/2330 train_time:107481ms step_avg:61.49ms
step:1749/2330 train_time:107542ms step_avg:61.49ms
step:1750/2330 train_time:107605ms step_avg:61.49ms
step:1750/2330 val_loss:3.4567 train_time:107671ms step_avg:61.53ms
step:1751/2330 train_time:107693ms step_avg:61.50ms
step:1752/2330 train_time:107734ms step_avg:61.49ms
step:1753/2330 train_time:107802ms step_avg:61.50ms
step:1754/2330 train_time:107866ms step_avg:61.50ms
step:1755/2330 train_time:107926ms step_avg:61.50ms
step:1756/2330 train_time:107989ms step_avg:61.50ms
step:1757/2330 train_time:108049ms step_avg:61.50ms
step:1758/2330 train_time:108111ms step_avg:61.50ms
step:1759/2330 train_time:108171ms step_avg:61.50ms
step:1760/2330 train_time:108233ms step_avg:61.50ms
step:1761/2330 train_time:108294ms step_avg:61.50ms
step:1762/2330 train_time:108357ms step_avg:61.50ms
step:1763/2330 train_time:108417ms step_avg:61.50ms
step:1764/2330 train_time:108479ms step_avg:61.50ms
step:1765/2330 train_time:108539ms step_avg:61.50ms
step:1766/2330 train_time:108604ms step_avg:61.50ms
step:1767/2330 train_time:108666ms step_avg:61.50ms
step:1768/2330 train_time:108731ms step_avg:61.50ms
step:1769/2330 train_time:108793ms step_avg:61.50ms
step:1770/2330 train_time:108857ms step_avg:61.50ms
step:1771/2330 train_time:108918ms step_avg:61.50ms
step:1772/2330 train_time:108981ms step_avg:61.50ms
step:1773/2330 train_time:109042ms step_avg:61.50ms
step:1774/2330 train_time:109105ms step_avg:61.50ms
step:1775/2330 train_time:109165ms step_avg:61.50ms
step:1776/2330 train_time:109228ms step_avg:61.50ms
step:1777/2330 train_time:109289ms step_avg:61.50ms
step:1778/2330 train_time:109351ms step_avg:61.50ms
step:1779/2330 train_time:109411ms step_avg:61.50ms
step:1780/2330 train_time:109474ms step_avg:61.50ms
step:1781/2330 train_time:109535ms step_avg:61.50ms
step:1782/2330 train_time:109598ms step_avg:61.50ms
step:1783/2330 train_time:109661ms step_avg:61.50ms
step:1784/2330 train_time:109725ms step_avg:61.50ms
step:1785/2330 train_time:109787ms step_avg:61.51ms
step:1786/2330 train_time:109850ms step_avg:61.51ms
step:1787/2330 train_time:109911ms step_avg:61.51ms
step:1788/2330 train_time:109976ms step_avg:61.51ms
step:1789/2330 train_time:110037ms step_avg:61.51ms
step:1790/2330 train_time:110101ms step_avg:61.51ms
step:1791/2330 train_time:110163ms step_avg:61.51ms
step:1792/2330 train_time:110225ms step_avg:61.51ms
step:1793/2330 train_time:110286ms step_avg:61.51ms
step:1794/2330 train_time:110348ms step_avg:61.51ms
step:1795/2330 train_time:110408ms step_avg:61.51ms
step:1796/2330 train_time:110471ms step_avg:61.51ms
step:1797/2330 train_time:110531ms step_avg:61.51ms
step:1798/2330 train_time:110594ms step_avg:61.51ms
step:1799/2330 train_time:110656ms step_avg:61.51ms
step:1800/2330 train_time:110720ms step_avg:61.51ms
step:1801/2330 train_time:110781ms step_avg:61.51ms
step:1802/2330 train_time:110845ms step_avg:61.51ms
step:1803/2330 train_time:110905ms step_avg:61.51ms
step:1804/2330 train_time:110969ms step_avg:61.51ms
step:1805/2330 train_time:111031ms step_avg:61.51ms
step:1806/2330 train_time:111095ms step_avg:61.51ms
step:1807/2330 train_time:111156ms step_avg:61.51ms
step:1808/2330 train_time:111220ms step_avg:61.52ms
step:1809/2330 train_time:111281ms step_avg:61.52ms
step:1810/2330 train_time:111345ms step_avg:61.52ms
step:1811/2330 train_time:111406ms step_avg:61.52ms
step:1812/2330 train_time:111469ms step_avg:61.52ms
step:1813/2330 train_time:111529ms step_avg:61.52ms
step:1814/2330 train_time:111593ms step_avg:61.52ms
step:1815/2330 train_time:111654ms step_avg:61.52ms
step:1816/2330 train_time:111718ms step_avg:61.52ms
step:1817/2330 train_time:111778ms step_avg:61.52ms
step:1818/2330 train_time:111842ms step_avg:61.52ms
step:1819/2330 train_time:111903ms step_avg:61.52ms
step:1820/2330 train_time:111968ms step_avg:61.52ms
step:1821/2330 train_time:112029ms step_avg:61.52ms
step:1822/2330 train_time:112093ms step_avg:61.52ms
step:1823/2330 train_time:112155ms step_avg:61.52ms
step:1824/2330 train_time:112218ms step_avg:61.52ms
step:1825/2330 train_time:112279ms step_avg:61.52ms
step:1826/2330 train_time:112343ms step_avg:61.52ms
step:1827/2330 train_time:112404ms step_avg:61.52ms
step:1828/2330 train_time:112466ms step_avg:61.52ms
step:1829/2330 train_time:112527ms step_avg:61.52ms
step:1830/2330 train_time:112590ms step_avg:61.52ms
step:1831/2330 train_time:112651ms step_avg:61.52ms
step:1832/2330 train_time:112714ms step_avg:61.52ms
step:1833/2330 train_time:112776ms step_avg:61.53ms
step:1834/2330 train_time:112839ms step_avg:61.53ms
step:1835/2330 train_time:112901ms step_avg:61.53ms
step:1836/2330 train_time:112964ms step_avg:61.53ms
step:1837/2330 train_time:113025ms step_avg:61.53ms
step:1838/2330 train_time:113087ms step_avg:61.53ms
step:1839/2330 train_time:113148ms step_avg:61.53ms
step:1840/2330 train_time:113211ms step_avg:61.53ms
step:1841/2330 train_time:113273ms step_avg:61.53ms
step:1842/2330 train_time:113336ms step_avg:61.53ms
step:1843/2330 train_time:113397ms step_avg:61.53ms
step:1844/2330 train_time:113460ms step_avg:61.53ms
step:1845/2330 train_time:113522ms step_avg:61.53ms
step:1846/2330 train_time:113585ms step_avg:61.53ms
step:1847/2330 train_time:113645ms step_avg:61.53ms
step:1848/2330 train_time:113707ms step_avg:61.53ms
step:1849/2330 train_time:113769ms step_avg:61.53ms
step:1850/2330 train_time:113832ms step_avg:61.53ms
step:1851/2330 train_time:113893ms step_avg:61.53ms
step:1852/2330 train_time:113957ms step_avg:61.53ms
step:1853/2330 train_time:114018ms step_avg:61.53ms
step:1854/2330 train_time:114082ms step_avg:61.53ms
step:1855/2330 train_time:114144ms step_avg:61.53ms
step:1856/2330 train_time:114206ms step_avg:61.53ms
step:1857/2330 train_time:114267ms step_avg:61.53ms
step:1858/2330 train_time:114330ms step_avg:61.53ms
step:1859/2330 train_time:114390ms step_avg:61.53ms
step:1860/2330 train_time:114454ms step_avg:61.53ms
step:1861/2330 train_time:114516ms step_avg:61.53ms
step:1862/2330 train_time:114580ms step_avg:61.54ms
step:1863/2330 train_time:114641ms step_avg:61.54ms
step:1864/2330 train_time:114705ms step_avg:61.54ms
step:1865/2330 train_time:114766ms step_avg:61.54ms
step:1866/2330 train_time:114829ms step_avg:61.54ms
step:1867/2330 train_time:114890ms step_avg:61.54ms
step:1868/2330 train_time:114953ms step_avg:61.54ms
step:1869/2330 train_time:115014ms step_avg:61.54ms
step:1870/2330 train_time:115078ms step_avg:61.54ms
step:1871/2330 train_time:115139ms step_avg:61.54ms
step:1872/2330 train_time:115202ms step_avg:61.54ms
step:1873/2330 train_time:115263ms step_avg:61.54ms
step:1874/2330 train_time:115326ms step_avg:61.54ms
step:1875/2330 train_time:115387ms step_avg:61.54ms
step:1876/2330 train_time:115450ms step_avg:61.54ms
step:1877/2330 train_time:115511ms step_avg:61.54ms
step:1878/2330 train_time:115576ms step_avg:61.54ms
step:1879/2330 train_time:115637ms step_avg:61.54ms
step:1880/2330 train_time:115700ms step_avg:61.54ms
step:1881/2330 train_time:115762ms step_avg:61.54ms
step:1882/2330 train_time:115826ms step_avg:61.54ms
step:1883/2330 train_time:115886ms step_avg:61.54ms
step:1884/2330 train_time:115948ms step_avg:61.54ms
step:1885/2330 train_time:116009ms step_avg:61.54ms
step:1886/2330 train_time:116073ms step_avg:61.54ms
step:1887/2330 train_time:116136ms step_avg:61.55ms
step:1888/2330 train_time:116199ms step_avg:61.55ms
step:1889/2330 train_time:116260ms step_avg:61.55ms
step:1890/2330 train_time:116323ms step_avg:61.55ms
step:1891/2330 train_time:116383ms step_avg:61.55ms
step:1892/2330 train_time:116446ms step_avg:61.55ms
step:1893/2330 train_time:116507ms step_avg:61.55ms
step:1894/2330 train_time:116571ms step_avg:61.55ms
step:1895/2330 train_time:116632ms step_avg:61.55ms
step:1896/2330 train_time:116695ms step_avg:61.55ms
step:1897/2330 train_time:116756ms step_avg:61.55ms
step:1898/2330 train_time:116820ms step_avg:61.55ms
step:1899/2330 train_time:116881ms step_avg:61.55ms
step:1900/2330 train_time:116945ms step_avg:61.55ms
step:1901/2330 train_time:117006ms step_avg:61.55ms
step:1902/2330 train_time:117068ms step_avg:61.55ms
step:1903/2330 train_time:117129ms step_avg:61.55ms
step:1904/2330 train_time:117192ms step_avg:61.55ms
step:1905/2330 train_time:117253ms step_avg:61.55ms
step:1906/2330 train_time:117317ms step_avg:61.55ms
step:1907/2330 train_time:117378ms step_avg:61.55ms
step:1908/2330 train_time:117441ms step_avg:61.55ms
step:1909/2330 train_time:117503ms step_avg:61.55ms
step:1910/2330 train_time:117567ms step_avg:61.55ms
step:1911/2330 train_time:117627ms step_avg:61.55ms
step:1912/2330 train_time:117690ms step_avg:61.55ms
step:1913/2330 train_time:117751ms step_avg:61.55ms
step:1914/2330 train_time:117814ms step_avg:61.55ms
step:1915/2330 train_time:117876ms step_avg:61.55ms
step:1916/2330 train_time:117939ms step_avg:61.55ms
step:1917/2330 train_time:118000ms step_avg:61.55ms
step:1918/2330 train_time:118063ms step_avg:61.56ms
step:1919/2330 train_time:118123ms step_avg:61.55ms
step:1920/2330 train_time:118186ms step_avg:61.56ms
step:1921/2330 train_time:118247ms step_avg:61.56ms
step:1922/2330 train_time:118311ms step_avg:61.56ms
step:1923/2330 train_time:118372ms step_avg:61.56ms
step:1924/2330 train_time:118435ms step_avg:61.56ms
step:1925/2330 train_time:118496ms step_avg:61.56ms
step:1926/2330 train_time:118561ms step_avg:61.56ms
step:1927/2330 train_time:118622ms step_avg:61.56ms
step:1928/2330 train_time:118685ms step_avg:61.56ms
step:1929/2330 train_time:118746ms step_avg:61.56ms
step:1930/2330 train_time:118809ms step_avg:61.56ms
step:1931/2330 train_time:118870ms step_avg:61.56ms
step:1932/2330 train_time:118933ms step_avg:61.56ms
step:1933/2330 train_time:118995ms step_avg:61.56ms
step:1934/2330 train_time:119057ms step_avg:61.56ms
step:1935/2330 train_time:119118ms step_avg:61.56ms
step:1936/2330 train_time:119182ms step_avg:61.56ms
step:1937/2330 train_time:119244ms step_avg:61.56ms
step:1938/2330 train_time:119307ms step_avg:61.56ms
step:1939/2330 train_time:119368ms step_avg:61.56ms
step:1940/2330 train_time:119431ms step_avg:61.56ms
step:1941/2330 train_time:119492ms step_avg:61.56ms
step:1942/2330 train_time:119556ms step_avg:61.56ms
step:1943/2330 train_time:119618ms step_avg:61.56ms
step:1944/2330 train_time:119681ms step_avg:61.56ms
step:1945/2330 train_time:119742ms step_avg:61.56ms
step:1946/2330 train_time:119805ms step_avg:61.56ms
step:1947/2330 train_time:119866ms step_avg:61.56ms
step:1948/2330 train_time:119928ms step_avg:61.56ms
step:1949/2330 train_time:119989ms step_avg:61.56ms
step:1950/2330 train_time:120052ms step_avg:61.57ms
step:1951/2330 train_time:120114ms step_avg:61.57ms
step:1952/2330 train_time:120178ms step_avg:61.57ms
step:1953/2330 train_time:120239ms step_avg:61.57ms
step:1954/2330 train_time:120302ms step_avg:61.57ms
step:1955/2330 train_time:120363ms step_avg:61.57ms
step:1956/2330 train_time:120427ms step_avg:61.57ms
step:1957/2330 train_time:120488ms step_avg:61.57ms
step:1958/2330 train_time:120551ms step_avg:61.57ms
step:1959/2330 train_time:120612ms step_avg:61.57ms
step:1960/2330 train_time:120676ms step_avg:61.57ms
step:1961/2330 train_time:120737ms step_avg:61.57ms
step:1962/2330 train_time:120800ms step_avg:61.57ms
step:1963/2330 train_time:120862ms step_avg:61.57ms
step:1964/2330 train_time:120925ms step_avg:61.57ms
step:1965/2330 train_time:120985ms step_avg:61.57ms
step:1966/2330 train_time:121048ms step_avg:61.57ms
step:1967/2330 train_time:121109ms step_avg:61.57ms
step:1968/2330 train_time:121173ms step_avg:61.57ms
step:1969/2330 train_time:121234ms step_avg:61.57ms
step:1970/2330 train_time:121297ms step_avg:61.57ms
step:1971/2330 train_time:121359ms step_avg:61.57ms
step:1972/2330 train_time:121422ms step_avg:61.57ms
step:1973/2330 train_time:121483ms step_avg:61.57ms
step:1974/2330 train_time:121547ms step_avg:61.57ms
step:1975/2330 train_time:121607ms step_avg:61.57ms
step:1976/2330 train_time:121670ms step_avg:61.57ms
step:1977/2330 train_time:121732ms step_avg:61.57ms
step:1978/2330 train_time:121795ms step_avg:61.57ms
step:1979/2330 train_time:121857ms step_avg:61.57ms
step:1980/2330 train_time:121920ms step_avg:61.58ms
step:1981/2330 train_time:121981ms step_avg:61.58ms
step:1982/2330 train_time:122043ms step_avg:61.58ms
step:1983/2330 train_time:122104ms step_avg:61.58ms
step:1984/2330 train_time:122166ms step_avg:61.58ms
step:1985/2330 train_time:122228ms step_avg:61.58ms
step:1986/2330 train_time:122292ms step_avg:61.58ms
step:1987/2330 train_time:122353ms step_avg:61.58ms
step:1988/2330 train_time:122416ms step_avg:61.58ms
step:1989/2330 train_time:122477ms step_avg:61.58ms
step:1990/2330 train_time:122541ms step_avg:61.58ms
step:1991/2330 train_time:122602ms step_avg:61.58ms
step:1992/2330 train_time:122665ms step_avg:61.58ms
step:1993/2330 train_time:122725ms step_avg:61.58ms
step:1994/2330 train_time:122789ms step_avg:61.58ms
step:1995/2330 train_time:122850ms step_avg:61.58ms
step:1996/2330 train_time:122913ms step_avg:61.58ms
step:1997/2330 train_time:122975ms step_avg:61.58ms
step:1998/2330 train_time:123038ms step_avg:61.58ms
step:1999/2330 train_time:123100ms step_avg:61.58ms
step:2000/2330 train_time:123164ms step_avg:61.58ms
step:2000/2330 val_loss:3.4079 train_time:123229ms step_avg:61.61ms
step:2001/2330 train_time:123251ms step_avg:61.59ms
step:2002/2330 train_time:123293ms step_avg:61.58ms
step:2003/2330 train_time:123358ms step_avg:61.59ms
step:2004/2330 train_time:123422ms step_avg:61.59ms
step:2005/2330 train_time:123484ms step_avg:61.59ms
step:2006/2330 train_time:123547ms step_avg:61.59ms
step:2007/2330 train_time:123608ms step_avg:61.59ms
step:2008/2330 train_time:123671ms step_avg:61.59ms
step:2009/2330 train_time:123731ms step_avg:61.59ms
step:2010/2330 train_time:123793ms step_avg:61.59ms
step:2011/2330 train_time:123854ms step_avg:61.59ms
step:2012/2330 train_time:123916ms step_avg:61.59ms
step:2013/2330 train_time:123976ms step_avg:61.59ms
step:2014/2330 train_time:124038ms step_avg:61.59ms
step:2015/2330 train_time:124098ms step_avg:61.59ms
step:2016/2330 train_time:124162ms step_avg:61.59ms
step:2017/2330 train_time:124224ms step_avg:61.59ms
step:2018/2330 train_time:124289ms step_avg:61.59ms
step:2019/2330 train_time:124352ms step_avg:61.59ms
step:2020/2330 train_time:124417ms step_avg:61.59ms
step:2021/2330 train_time:124479ms step_avg:61.59ms
step:2022/2330 train_time:124541ms step_avg:61.59ms
step:2023/2330 train_time:124602ms step_avg:61.59ms
step:2024/2330 train_time:124666ms step_avg:61.59ms
step:2025/2330 train_time:124726ms step_avg:61.59ms
step:2026/2330 train_time:124790ms step_avg:61.59ms
step:2027/2330 train_time:124851ms step_avg:61.59ms
step:2028/2330 train_time:124913ms step_avg:61.59ms
step:2029/2330 train_time:124974ms step_avg:61.59ms
step:2030/2330 train_time:125036ms step_avg:61.59ms
step:2031/2330 train_time:125097ms step_avg:61.59ms
step:2032/2330 train_time:125161ms step_avg:61.59ms
step:2033/2330 train_time:125223ms step_avg:61.60ms
step:2034/2330 train_time:125288ms step_avg:61.60ms
step:2035/2330 train_time:125351ms step_avg:61.60ms
step:2036/2330 train_time:125415ms step_avg:61.60ms
step:2037/2330 train_time:125477ms step_avg:61.60ms
step:2038/2330 train_time:125539ms step_avg:61.60ms
step:2039/2330 train_time:125601ms step_avg:61.60ms
step:2040/2330 train_time:125663ms step_avg:61.60ms
step:2041/2330 train_time:125724ms step_avg:61.60ms
step:2042/2330 train_time:125787ms step_avg:61.60ms
step:2043/2330 train_time:125849ms step_avg:61.60ms
step:2044/2330 train_time:125912ms step_avg:61.60ms
step:2045/2330 train_time:125973ms step_avg:61.60ms
step:2046/2330 train_time:126035ms step_avg:61.60ms
step:2047/2330 train_time:126095ms step_avg:61.60ms
step:2048/2330 train_time:126158ms step_avg:61.60ms
step:2049/2330 train_time:126220ms step_avg:61.60ms
step:2050/2330 train_time:126283ms step_avg:61.60ms
step:2051/2330 train_time:126345ms step_avg:61.60ms
step:2052/2330 train_time:126409ms step_avg:61.60ms
step:2053/2330 train_time:126471ms step_avg:61.60ms
step:2054/2330 train_time:126534ms step_avg:61.60ms
step:2055/2330 train_time:126596ms step_avg:61.60ms
step:2056/2330 train_time:126659ms step_avg:61.60ms
step:2057/2330 train_time:126720ms step_avg:61.60ms
step:2058/2330 train_time:126783ms step_avg:61.60ms
step:2059/2330 train_time:126843ms step_avg:61.60ms
step:2060/2330 train_time:126907ms step_avg:61.61ms
step:2061/2330 train_time:126968ms step_avg:61.61ms
step:2062/2330 train_time:127031ms step_avg:61.61ms
step:2063/2330 train_time:127093ms step_avg:61.61ms
step:2064/2330 train_time:127155ms step_avg:61.61ms
step:2065/2330 train_time:127217ms step_avg:61.61ms
step:2066/2330 train_time:127281ms step_avg:61.61ms
step:2067/2330 train_time:127342ms step_avg:61.61ms
step:2068/2330 train_time:127406ms step_avg:61.61ms
step:2069/2330 train_time:127467ms step_avg:61.61ms
step:2070/2330 train_time:127531ms step_avg:61.61ms
step:2071/2330 train_time:127593ms step_avg:61.61ms
step:2072/2330 train_time:127657ms step_avg:61.61ms
step:2073/2330 train_time:127719ms step_avg:61.61ms
step:2074/2330 train_time:127782ms step_avg:61.61ms
step:2075/2330 train_time:127842ms step_avg:61.61ms
step:2076/2330 train_time:127905ms step_avg:61.61ms
step:2077/2330 train_time:127966ms step_avg:61.61ms
step:2078/2330 train_time:128030ms step_avg:61.61ms
step:2079/2330 train_time:128091ms step_avg:61.61ms
step:2080/2330 train_time:128154ms step_avg:61.61ms
step:2081/2330 train_time:128215ms step_avg:61.61ms
step:2082/2330 train_time:128278ms step_avg:61.61ms
step:2083/2330 train_time:128339ms step_avg:61.61ms
step:2084/2330 train_time:128403ms step_avg:61.61ms
step:2085/2330 train_time:128464ms step_avg:61.61ms
step:2086/2330 train_time:128527ms step_avg:61.61ms
step:2087/2330 train_time:128589ms step_avg:61.61ms
step:2088/2330 train_time:128653ms step_avg:61.62ms
step:2089/2330 train_time:128715ms step_avg:61.62ms
step:2090/2330 train_time:128778ms step_avg:61.62ms
step:2091/2330 train_time:128839ms step_avg:61.62ms
step:2092/2330 train_time:128903ms step_avg:61.62ms
step:2093/2330 train_time:128963ms step_avg:61.62ms
step:2094/2330 train_time:129027ms step_avg:61.62ms
step:2095/2330 train_time:129089ms step_avg:61.62ms
step:2096/2330 train_time:129152ms step_avg:61.62ms
step:2097/2330 train_time:129213ms step_avg:61.62ms
step:2098/2330 train_time:129277ms step_avg:61.62ms
step:2099/2330 train_time:129338ms step_avg:61.62ms
step:2100/2330 train_time:129402ms step_avg:61.62ms
step:2101/2330 train_time:129463ms step_avg:61.62ms
step:2102/2330 train_time:129526ms step_avg:61.62ms
step:2103/2330 train_time:129589ms step_avg:61.62ms
step:2104/2330 train_time:129652ms step_avg:61.62ms
step:2105/2330 train_time:129714ms step_avg:61.62ms
step:2106/2330 train_time:129777ms step_avg:61.62ms
step:2107/2330 train_time:129838ms step_avg:61.62ms
step:2108/2330 train_time:129900ms step_avg:61.62ms
step:2109/2330 train_time:129961ms step_avg:61.62ms
step:2110/2330 train_time:130024ms step_avg:61.62ms
step:2111/2330 train_time:130085ms step_avg:61.62ms
step:2112/2330 train_time:130148ms step_avg:61.62ms
step:2113/2330 train_time:130209ms step_avg:61.62ms
step:2114/2330 train_time:130272ms step_avg:61.62ms
step:2115/2330 train_time:130334ms step_avg:61.62ms
step:2116/2330 train_time:130397ms step_avg:61.62ms
step:2117/2330 train_time:130458ms step_avg:61.62ms
step:2118/2330 train_time:130521ms step_avg:61.62ms
step:2119/2330 train_time:130582ms step_avg:61.62ms
step:2120/2330 train_time:130646ms step_avg:61.63ms
step:2121/2330 train_time:130707ms step_avg:61.63ms
step:2122/2330 train_time:130771ms step_avg:61.63ms
step:2123/2330 train_time:130832ms step_avg:61.63ms
step:2124/2330 train_time:130896ms step_avg:61.63ms
step:2125/2330 train_time:130957ms step_avg:61.63ms
step:2126/2330 train_time:131021ms step_avg:61.63ms
step:2127/2330 train_time:131082ms step_avg:61.63ms
step:2128/2330 train_time:131144ms step_avg:61.63ms
step:2129/2330 train_time:131205ms step_avg:61.63ms
step:2130/2330 train_time:131268ms step_avg:61.63ms
step:2131/2330 train_time:131329ms step_avg:61.63ms
step:2132/2330 train_time:131393ms step_avg:61.63ms
step:2133/2330 train_time:131454ms step_avg:61.63ms
step:2134/2330 train_time:131517ms step_avg:61.63ms
step:2135/2330 train_time:131578ms step_avg:61.63ms
step:2136/2330 train_time:131642ms step_avg:61.63ms
step:2137/2330 train_time:131703ms step_avg:61.63ms
step:2138/2330 train_time:131766ms step_avg:61.63ms
step:2139/2330 train_time:131827ms step_avg:61.63ms
step:2140/2330 train_time:131891ms step_avg:61.63ms
step:2141/2330 train_time:131952ms step_avg:61.63ms
step:2142/2330 train_time:132016ms step_avg:61.63ms
step:2143/2330 train_time:132078ms step_avg:61.63ms
step:2144/2330 train_time:132141ms step_avg:61.63ms
step:2145/2330 train_time:132201ms step_avg:61.63ms
step:2146/2330 train_time:132263ms step_avg:61.63ms
step:2147/2330 train_time:132325ms step_avg:61.63ms
step:2148/2330 train_time:132388ms step_avg:61.63ms
step:2149/2330 train_time:132449ms step_avg:61.63ms
step:2150/2330 train_time:132513ms step_avg:61.63ms
step:2151/2330 train_time:132575ms step_avg:61.63ms
step:2152/2330 train_time:132638ms step_avg:61.63ms
step:2153/2330 train_time:132699ms step_avg:61.63ms
step:2154/2330 train_time:132762ms step_avg:61.64ms
step:2155/2330 train_time:132823ms step_avg:61.63ms
step:2156/2330 train_time:132886ms step_avg:61.64ms
step:2157/2330 train_time:132947ms step_avg:61.64ms
step:2158/2330 train_time:133010ms step_avg:61.64ms
step:2159/2330 train_time:133072ms step_avg:61.64ms
step:2160/2330 train_time:133135ms step_avg:61.64ms
step:2161/2330 train_time:133196ms step_avg:61.64ms
step:2162/2330 train_time:133260ms step_avg:61.64ms
step:2163/2330 train_time:133321ms step_avg:61.64ms
step:2164/2330 train_time:133384ms step_avg:61.64ms
step:2165/2330 train_time:133445ms step_avg:61.64ms
step:2166/2330 train_time:133508ms step_avg:61.64ms
step:2167/2330 train_time:133569ms step_avg:61.64ms
step:2168/2330 train_time:133633ms step_avg:61.64ms
step:2169/2330 train_time:133694ms step_avg:61.64ms
step:2170/2330 train_time:133758ms step_avg:61.64ms
step:2171/2330 train_time:133819ms step_avg:61.64ms
step:2172/2330 train_time:133882ms step_avg:61.64ms
step:2173/2330 train_time:133943ms step_avg:61.64ms
step:2174/2330 train_time:134006ms step_avg:61.64ms
step:2175/2330 train_time:134067ms step_avg:61.64ms
step:2176/2330 train_time:134130ms step_avg:61.64ms
step:2177/2330 train_time:134192ms step_avg:61.64ms
step:2178/2330 train_time:134256ms step_avg:61.64ms
step:2179/2330 train_time:134317ms step_avg:61.64ms
step:2180/2330 train_time:134380ms step_avg:61.64ms
step:2181/2330 train_time:134440ms step_avg:61.64ms
step:2182/2330 train_time:134503ms step_avg:61.64ms
step:2183/2330 train_time:134564ms step_avg:61.64ms
step:2184/2330 train_time:134627ms step_avg:61.64ms
step:2185/2330 train_time:134690ms step_avg:61.64ms
step:2186/2330 train_time:134754ms step_avg:61.64ms
step:2187/2330 train_time:134815ms step_avg:61.64ms
step:2188/2330 train_time:134879ms step_avg:61.64ms
step:2189/2330 train_time:134939ms step_avg:61.64ms
step:2190/2330 train_time:135003ms step_avg:61.65ms
step:2191/2330 train_time:135064ms step_avg:61.64ms
step:2192/2330 train_time:135128ms step_avg:61.65ms
step:2193/2330 train_time:135190ms step_avg:61.65ms
step:2194/2330 train_time:135253ms step_avg:61.65ms
step:2195/2330 train_time:135314ms step_avg:61.65ms
step:2196/2330 train_time:135378ms step_avg:61.65ms
step:2197/2330 train_time:135438ms step_avg:61.65ms
step:2198/2330 train_time:135501ms step_avg:61.65ms
step:2199/2330 train_time:135562ms step_avg:61.65ms
step:2200/2330 train_time:135625ms step_avg:61.65ms
step:2201/2330 train_time:135687ms step_avg:61.65ms
step:2202/2330 train_time:135750ms step_avg:61.65ms
step:2203/2330 train_time:135812ms step_avg:61.65ms
step:2204/2330 train_time:135876ms step_avg:61.65ms
step:2205/2330 train_time:135936ms step_avg:61.65ms
step:2206/2330 train_time:135999ms step_avg:61.65ms
step:2207/2330 train_time:136060ms step_avg:61.65ms
step:2208/2330 train_time:136124ms step_avg:61.65ms
step:2209/2330 train_time:136185ms step_avg:61.65ms
step:2210/2330 train_time:136249ms step_avg:61.65ms
step:2211/2330 train_time:136311ms step_avg:61.65ms
step:2212/2330 train_time:136374ms step_avg:61.65ms
step:2213/2330 train_time:136436ms step_avg:61.65ms
step:2214/2330 train_time:136500ms step_avg:61.65ms
step:2215/2330 train_time:136561ms step_avg:61.65ms
step:2216/2330 train_time:136623ms step_avg:61.65ms
step:2217/2330 train_time:136684ms step_avg:61.65ms
step:2218/2330 train_time:136748ms step_avg:61.65ms
step:2219/2330 train_time:136809ms step_avg:61.65ms
step:2220/2330 train_time:136873ms step_avg:61.65ms
step:2221/2330 train_time:136934ms step_avg:61.65ms
step:2222/2330 train_time:136997ms step_avg:61.65ms
step:2223/2330 train_time:137058ms step_avg:61.65ms
step:2224/2330 train_time:137120ms step_avg:61.65ms
step:2225/2330 train_time:137181ms step_avg:61.65ms
step:2226/2330 train_time:137245ms step_avg:61.66ms
step:2227/2330 train_time:137306ms step_avg:61.66ms
step:2228/2330 train_time:137370ms step_avg:61.66ms
step:2229/2330 train_time:137432ms step_avg:61.66ms
step:2230/2330 train_time:137496ms step_avg:61.66ms
step:2231/2330 train_time:137557ms step_avg:61.66ms
step:2232/2330 train_time:137620ms step_avg:61.66ms
step:2233/2330 train_time:137681ms step_avg:61.66ms
step:2234/2330 train_time:137743ms step_avg:61.66ms
step:2235/2330 train_time:137805ms step_avg:61.66ms
step:2236/2330 train_time:137869ms step_avg:61.66ms
step:2237/2330 train_time:137931ms step_avg:61.66ms
step:2238/2330 train_time:137994ms step_avg:61.66ms
step:2239/2330 train_time:138055ms step_avg:61.66ms
step:2240/2330 train_time:138119ms step_avg:61.66ms
step:2241/2330 train_time:138179ms step_avg:61.66ms
step:2242/2330 train_time:138242ms step_avg:61.66ms
step:2243/2330 train_time:138303ms step_avg:61.66ms
step:2244/2330 train_time:138367ms step_avg:61.66ms
step:2245/2330 train_time:138429ms step_avg:61.66ms
step:2246/2330 train_time:138492ms step_avg:61.66ms
step:2247/2330 train_time:138554ms step_avg:61.66ms
step:2248/2330 train_time:138617ms step_avg:61.66ms
step:2249/2330 train_time:138679ms step_avg:61.66ms
step:2250/2330 train_time:138742ms step_avg:61.66ms
step:2250/2330 val_loss:3.3697 train_time:138807ms step_avg:61.69ms
step:2251/2330 train_time:138829ms step_avg:61.67ms
step:2252/2330 train_time:138869ms step_avg:61.66ms
step:2253/2330 train_time:138936ms step_avg:61.67ms
step:2254/2330 train_time:139001ms step_avg:61.67ms
step:2255/2330 train_time:139062ms step_avg:61.67ms
step:2256/2330 train_time:139126ms step_avg:61.67ms
step:2257/2330 train_time:139186ms step_avg:61.67ms
step:2258/2330 train_time:139250ms step_avg:61.67ms
step:2259/2330 train_time:139310ms step_avg:61.67ms
step:2260/2330 train_time:139372ms step_avg:61.67ms
step:2261/2330 train_time:139432ms step_avg:61.67ms
step:2262/2330 train_time:139494ms step_avg:61.67ms
step:2263/2330 train_time:139554ms step_avg:61.67ms
step:2264/2330 train_time:139617ms step_avg:61.67ms
step:2265/2330 train_time:139677ms step_avg:61.67ms
step:2266/2330 train_time:139740ms step_avg:61.67ms
step:2267/2330 train_time:139803ms step_avg:61.67ms
step:2268/2330 train_time:139868ms step_avg:61.67ms
step:2269/2330 train_time:139931ms step_avg:61.67ms
step:2270/2330 train_time:139996ms step_avg:61.67ms
step:2271/2330 train_time:140057ms step_avg:61.67ms
step:2272/2330 train_time:140121ms step_avg:61.67ms
step:2273/2330 train_time:140182ms step_avg:61.67ms
step:2274/2330 train_time:140245ms step_avg:61.67ms
step:2275/2330 train_time:140306ms step_avg:61.67ms
step:2276/2330 train_time:140369ms step_avg:61.67ms
step:2277/2330 train_time:140429ms step_avg:61.67ms
step:2278/2330 train_time:140492ms step_avg:61.67ms
step:2279/2330 train_time:140552ms step_avg:61.67ms
step:2280/2330 train_time:140615ms step_avg:61.67ms
step:2281/2330 train_time:140675ms step_avg:61.67ms
step:2282/2330 train_time:140737ms step_avg:61.67ms
step:2283/2330 train_time:140799ms step_avg:61.67ms
step:2284/2330 train_time:140864ms step_avg:61.67ms
step:2285/2330 train_time:140925ms step_avg:61.67ms
step:2286/2330 train_time:140989ms step_avg:61.68ms
step:2287/2330 train_time:141051ms step_avg:61.67ms
step:2288/2330 train_time:141114ms step_avg:61.68ms
step:2289/2330 train_time:141174ms step_avg:61.68ms
step:2290/2330 train_time:141237ms step_avg:61.68ms
step:2291/2330 train_time:141298ms step_avg:61.68ms
step:2292/2330 train_time:141362ms step_avg:61.68ms
step:2293/2330 train_time:141422ms step_avg:61.68ms
step:2294/2330 train_time:141486ms step_avg:61.68ms
step:2295/2330 train_time:141546ms step_avg:61.68ms
step:2296/2330 train_time:141610ms step_avg:61.68ms
step:2297/2330 train_time:141671ms step_avg:61.68ms
step:2298/2330 train_time:141733ms step_avg:61.68ms
step:2299/2330 train_time:141794ms step_avg:61.68ms
step:2300/2330 train_time:141857ms step_avg:61.68ms
step:2301/2330 train_time:141918ms step_avg:61.68ms
step:2302/2330 train_time:141982ms step_avg:61.68ms
step:2303/2330 train_time:142043ms step_avg:61.68ms
step:2304/2330 train_time:142107ms step_avg:61.68ms
step:2305/2330 train_time:142168ms step_avg:61.68ms
step:2306/2330 train_time:142232ms step_avg:61.68ms
step:2307/2330 train_time:142293ms step_avg:61.68ms
step:2308/2330 train_time:142356ms step_avg:61.68ms
step:2309/2330 train_time:142417ms step_avg:61.68ms
step:2310/2330 train_time:142479ms step_avg:61.68ms
step:2311/2330 train_time:142541ms step_avg:61.68ms
step:2312/2330 train_time:142604ms step_avg:61.68ms
step:2313/2330 train_time:142664ms step_avg:61.68ms
step:2314/2330 train_time:142727ms step_avg:61.68ms
step:2315/2330 train_time:142788ms step_avg:61.68ms
step:2316/2330 train_time:142851ms step_avg:61.68ms
step:2317/2330 train_time:142912ms step_avg:61.68ms
step:2318/2330 train_time:142976ms step_avg:61.68ms
step:2319/2330 train_time:143036ms step_avg:61.68ms
step:2320/2330 train_time:143100ms step_avg:61.68ms
step:2321/2330 train_time:143162ms step_avg:61.68ms
step:2322/2330 train_time:143225ms step_avg:61.68ms
step:2323/2330 train_time:143286ms step_avg:61.68ms
step:2324/2330 train_time:143350ms step_avg:61.68ms
step:2325/2330 train_time:143411ms step_avg:61.68ms
step:2326/2330 train_time:143473ms step_avg:61.68ms
step:2327/2330 train_time:143534ms step_avg:61.68ms
step:2328/2330 train_time:143598ms step_avg:61.68ms
step:2329/2330 train_time:143659ms step_avg:61.68ms
step:2330/2330 train_time:143722ms step_avg:61.68ms
step:2330/2330 val_loss:3.3562 train_time:143788ms step_avg:61.71ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
