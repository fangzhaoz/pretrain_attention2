import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr5e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=5e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:41:56 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:101ms step_avg:100.96ms
step:2/2330 train_time:159ms step_avg:79.66ms
step:3/2330 train_time:171ms step_avg:56.86ms
step:4/2330 train_time:183ms step_avg:45.75ms
step:5/2330 train_time:193ms step_avg:38.63ms
step:6/2330 train_time:203ms step_avg:33.89ms
step:7/2330 train_time:235ms step_avg:33.50ms
step:8/2330 train_time:274ms step_avg:34.29ms
step:9/2330 train_time:308ms step_avg:34.18ms
step:10/2330 train_time:347ms step_avg:34.74ms
step:11/2330 train_time:381ms step_avg:34.62ms
step:12/2330 train_time:421ms step_avg:35.05ms
step:13/2330 train_time:454ms step_avg:34.93ms
step:14/2330 train_time:494ms step_avg:35.29ms
step:15/2330 train_time:527ms step_avg:35.16ms
step:16/2330 train_time:567ms step_avg:35.46ms
step:17/2330 train_time:601ms step_avg:35.33ms
step:18/2330 train_time:641ms step_avg:35.60ms
step:19/2330 train_time:674ms step_avg:35.49ms
step:20/2330 train_time:714ms step_avg:35.72ms
step:21/2330 train_time:748ms step_avg:35.62ms
step:22/2330 train_time:788ms step_avg:35.82ms
step:23/2330 train_time:822ms step_avg:35.72ms
step:24/2330 train_time:862ms step_avg:35.90ms
step:25/2330 train_time:895ms step_avg:35.80ms
step:26/2330 train_time:935ms step_avg:35.97ms
step:27/2330 train_time:969ms step_avg:35.88ms
step:28/2330 train_time:1009ms step_avg:36.02ms
step:29/2330 train_time:1043ms step_avg:35.98ms
step:30/2330 train_time:1084ms step_avg:36.12ms
step:31/2330 train_time:1120ms step_avg:36.13ms
step:32/2330 train_time:1161ms step_avg:36.27ms
step:33/2330 train_time:1195ms step_avg:36.22ms
step:34/2330 train_time:1236ms step_avg:36.34ms
step:35/2330 train_time:1270ms step_avg:36.29ms
step:36/2330 train_time:1310ms step_avg:36.39ms
step:37/2330 train_time:1345ms step_avg:36.35ms
step:38/2330 train_time:1385ms step_avg:36.45ms
step:39/2330 train_time:1419ms step_avg:36.39ms
step:40/2330 train_time:1459ms step_avg:36.48ms
step:41/2330 train_time:1493ms step_avg:36.42ms
step:42/2330 train_time:1533ms step_avg:36.51ms
step:43/2330 train_time:1568ms step_avg:36.46ms
step:44/2330 train_time:1608ms step_avg:36.54ms
step:45/2330 train_time:1642ms step_avg:36.48ms
step:46/2330 train_time:1682ms step_avg:36.56ms
step:47/2330 train_time:1715ms step_avg:36.50ms
step:48/2330 train_time:1756ms step_avg:36.58ms
step:49/2330 train_time:1789ms step_avg:36.52ms
step:50/2330 train_time:1829ms step_avg:36.59ms
step:51/2330 train_time:1863ms step_avg:36.53ms
step:52/2330 train_time:1903ms step_avg:36.60ms
step:53/2330 train_time:1937ms step_avg:36.55ms
step:54/2330 train_time:1977ms step_avg:36.62ms
step:55/2330 train_time:2011ms step_avg:36.57ms
step:56/2330 train_time:2051ms step_avg:36.63ms
step:57/2330 train_time:2087ms step_avg:36.62ms
step:58/2330 train_time:2127ms step_avg:36.68ms
step:59/2330 train_time:2162ms step_avg:36.65ms
step:60/2330 train_time:2203ms step_avg:36.71ms
step:61/2330 train_time:2237ms step_avg:36.67ms
step:62/2330 train_time:2277ms step_avg:36.73ms
step:63/2330 train_time:2311ms step_avg:36.69ms
step:64/2330 train_time:2352ms step_avg:36.74ms
step:65/2330 train_time:2385ms step_avg:36.70ms
step:66/2330 train_time:2425ms step_avg:36.75ms
step:67/2330 train_time:2459ms step_avg:36.70ms
step:68/2330 train_time:2499ms step_avg:36.75ms
step:69/2330 train_time:2534ms step_avg:36.72ms
step:70/2330 train_time:2574ms step_avg:36.77ms
step:71/2330 train_time:2608ms step_avg:36.74ms
step:72/2330 train_time:2649ms step_avg:36.79ms
step:73/2330 train_time:2682ms step_avg:36.75ms
step:74/2330 train_time:2723ms step_avg:36.79ms
step:75/2330 train_time:2757ms step_avg:36.75ms
step:76/2330 train_time:2797ms step_avg:36.80ms
step:77/2330 train_time:2831ms step_avg:36.76ms
step:78/2330 train_time:2871ms step_avg:36.80ms
step:79/2330 train_time:2905ms step_avg:36.78ms
step:80/2330 train_time:2945ms step_avg:36.82ms
step:81/2330 train_time:2980ms step_avg:36.79ms
step:82/2330 train_time:3020ms step_avg:36.83ms
step:83/2330 train_time:3055ms step_avg:36.81ms
step:84/2330 train_time:3095ms step_avg:36.85ms
step:85/2330 train_time:3130ms step_avg:36.83ms
step:86/2330 train_time:3170ms step_avg:36.86ms
step:87/2330 train_time:3205ms step_avg:36.84ms
step:88/2330 train_time:3245ms step_avg:36.88ms
step:89/2330 train_time:3280ms step_avg:36.85ms
step:90/2330 train_time:3320ms step_avg:36.89ms
step:91/2330 train_time:3354ms step_avg:36.86ms
step:92/2330 train_time:3395ms step_avg:36.90ms
step:93/2330 train_time:3430ms step_avg:36.88ms
step:94/2330 train_time:3470ms step_avg:36.91ms
step:95/2330 train_time:3505ms step_avg:36.89ms
step:96/2330 train_time:3545ms step_avg:36.92ms
step:97/2330 train_time:3579ms step_avg:36.89ms
step:98/2330 train_time:3619ms step_avg:36.93ms
step:99/2330 train_time:3653ms step_avg:36.89ms
step:100/2330 train_time:3693ms step_avg:36.93ms
step:101/2330 train_time:3727ms step_avg:36.90ms
step:102/2330 train_time:3767ms step_avg:36.93ms
step:103/2330 train_time:3801ms step_avg:36.90ms
step:104/2330 train_time:3841ms step_avg:36.93ms
step:105/2330 train_time:3875ms step_avg:36.91ms
step:106/2330 train_time:3916ms step_avg:36.94ms
step:107/2330 train_time:3950ms step_avg:36.92ms
step:108/2330 train_time:3990ms step_avg:36.95ms
step:109/2330 train_time:4025ms step_avg:36.93ms
step:110/2330 train_time:4065ms step_avg:36.96ms
step:111/2330 train_time:4100ms step_avg:36.94ms
step:112/2330 train_time:4140ms step_avg:36.97ms
step:113/2330 train_time:4175ms step_avg:36.95ms
step:114/2330 train_time:4215ms step_avg:36.98ms
step:115/2330 train_time:4251ms step_avg:36.96ms
step:116/2330 train_time:4291ms step_avg:36.99ms
step:117/2330 train_time:4325ms step_avg:36.97ms
step:118/2330 train_time:4365ms step_avg:36.99ms
step:119/2330 train_time:4400ms step_avg:36.98ms
step:120/2330 train_time:4440ms step_avg:37.00ms
step:121/2330 train_time:4475ms step_avg:36.99ms
step:122/2330 train_time:4516ms step_avg:37.01ms
step:123/2330 train_time:4550ms step_avg:36.99ms
step:124/2330 train_time:4590ms step_avg:37.02ms
step:125/2330 train_time:4625ms step_avg:37.00ms
step:126/2330 train_time:4665ms step_avg:37.02ms
step:127/2330 train_time:4700ms step_avg:37.01ms
step:128/2330 train_time:4740ms step_avg:37.03ms
step:129/2330 train_time:4774ms step_avg:37.01ms
step:130/2330 train_time:4815ms step_avg:37.04ms
step:131/2330 train_time:4849ms step_avg:37.02ms
step:132/2330 train_time:4889ms step_avg:37.04ms
step:133/2330 train_time:4924ms step_avg:37.02ms
step:134/2330 train_time:4964ms step_avg:37.04ms
step:135/2330 train_time:4999ms step_avg:37.03ms
step:136/2330 train_time:5039ms step_avg:37.05ms
step:137/2330 train_time:5074ms step_avg:37.03ms
step:138/2330 train_time:5114ms step_avg:37.06ms
step:139/2330 train_time:5149ms step_avg:37.04ms
step:140/2330 train_time:5189ms step_avg:37.06ms
step:141/2330 train_time:5224ms step_avg:37.05ms
step:142/2330 train_time:5264ms step_avg:37.07ms
step:143/2330 train_time:5298ms step_avg:37.05ms
step:144/2330 train_time:5339ms step_avg:37.08ms
step:145/2330 train_time:5374ms step_avg:37.06ms
step:146/2330 train_time:5414ms step_avg:37.08ms
step:147/2330 train_time:5449ms step_avg:37.07ms
step:148/2330 train_time:5489ms step_avg:37.09ms
step:149/2330 train_time:5524ms step_avg:37.07ms
step:150/2330 train_time:5564ms step_avg:37.09ms
step:151/2330 train_time:5598ms step_avg:37.07ms
step:152/2330 train_time:5638ms step_avg:37.09ms
step:153/2330 train_time:5674ms step_avg:37.08ms
step:154/2330 train_time:5714ms step_avg:37.10ms
step:155/2330 train_time:5748ms step_avg:37.08ms
step:156/2330 train_time:5788ms step_avg:37.10ms
step:157/2330 train_time:5823ms step_avg:37.09ms
step:158/2330 train_time:5863ms step_avg:37.10ms
step:159/2330 train_time:5897ms step_avg:37.09ms
step:160/2330 train_time:5937ms step_avg:37.11ms
step:161/2330 train_time:5972ms step_avg:37.09ms
step:162/2330 train_time:6012ms step_avg:37.11ms
step:163/2330 train_time:6047ms step_avg:37.10ms
step:164/2330 train_time:6088ms step_avg:37.12ms
step:165/2330 train_time:6122ms step_avg:37.10ms
step:166/2330 train_time:6162ms step_avg:37.12ms
step:167/2330 train_time:6197ms step_avg:37.11ms
step:168/2330 train_time:6238ms step_avg:37.13ms
step:169/2330 train_time:6272ms step_avg:37.11ms
step:170/2330 train_time:6313ms step_avg:37.13ms
step:171/2330 train_time:6347ms step_avg:37.12ms
step:172/2330 train_time:6387ms step_avg:37.14ms
step:173/2330 train_time:6422ms step_avg:37.12ms
step:174/2330 train_time:6462ms step_avg:37.14ms
step:175/2330 train_time:6497ms step_avg:37.12ms
step:176/2330 train_time:6537ms step_avg:37.14ms
step:177/2330 train_time:6572ms step_avg:37.13ms
step:178/2330 train_time:6612ms step_avg:37.15ms
step:179/2330 train_time:6647ms step_avg:37.14ms
step:180/2330 train_time:6688ms step_avg:37.15ms
step:181/2330 train_time:6722ms step_avg:37.14ms
step:182/2330 train_time:6762ms step_avg:37.15ms
step:183/2330 train_time:6796ms step_avg:37.14ms
step:184/2330 train_time:6837ms step_avg:37.16ms
step:185/2330 train_time:6872ms step_avg:37.14ms
step:186/2330 train_time:6912ms step_avg:37.16ms
step:187/2330 train_time:6948ms step_avg:37.15ms
step:188/2330 train_time:6988ms step_avg:37.17ms
step:189/2330 train_time:7022ms step_avg:37.16ms
step:190/2330 train_time:7063ms step_avg:37.17ms
step:191/2330 train_time:7098ms step_avg:37.16ms
step:192/2330 train_time:7138ms step_avg:37.18ms
step:193/2330 train_time:7173ms step_avg:37.17ms
step:194/2330 train_time:7213ms step_avg:37.18ms
step:195/2330 train_time:7249ms step_avg:37.17ms
step:196/2330 train_time:7289ms step_avg:37.19ms
step:197/2330 train_time:7324ms step_avg:37.18ms
step:198/2330 train_time:7365ms step_avg:37.19ms
step:199/2330 train_time:7398ms step_avg:37.18ms
step:200/2330 train_time:7439ms step_avg:37.19ms
step:201/2330 train_time:7474ms step_avg:37.18ms
step:202/2330 train_time:7515ms step_avg:37.20ms
step:203/2330 train_time:7550ms step_avg:37.19ms
step:204/2330 train_time:7590ms step_avg:37.21ms
step:205/2330 train_time:7625ms step_avg:37.19ms
step:206/2330 train_time:7665ms step_avg:37.21ms
step:207/2330 train_time:7699ms step_avg:37.20ms
step:208/2330 train_time:7740ms step_avg:37.21ms
step:209/2330 train_time:7775ms step_avg:37.20ms
step:210/2330 train_time:7815ms step_avg:37.22ms
step:211/2330 train_time:7850ms step_avg:37.21ms
step:212/2330 train_time:7890ms step_avg:37.22ms
step:213/2330 train_time:7925ms step_avg:37.21ms
step:214/2330 train_time:7965ms step_avg:37.22ms
step:215/2330 train_time:8000ms step_avg:37.21ms
step:216/2330 train_time:8040ms step_avg:37.22ms
step:217/2330 train_time:8075ms step_avg:37.21ms
step:218/2330 train_time:8116ms step_avg:37.23ms
step:219/2330 train_time:8151ms step_avg:37.22ms
step:220/2330 train_time:8191ms step_avg:37.23ms
step:221/2330 train_time:8226ms step_avg:37.22ms
step:222/2330 train_time:8266ms step_avg:37.23ms
step:223/2330 train_time:8301ms step_avg:37.22ms
step:224/2330 train_time:8342ms step_avg:37.24ms
step:225/2330 train_time:8377ms step_avg:37.23ms
step:226/2330 train_time:8418ms step_avg:37.25ms
step:227/2330 train_time:8453ms step_avg:37.24ms
step:228/2330 train_time:8493ms step_avg:37.25ms
step:229/2330 train_time:8528ms step_avg:37.24ms
step:230/2330 train_time:8569ms step_avg:37.26ms
step:231/2330 train_time:8603ms step_avg:37.24ms
step:232/2330 train_time:8644ms step_avg:37.26ms
step:233/2330 train_time:8678ms step_avg:37.24ms
step:234/2330 train_time:8719ms step_avg:37.26ms
step:235/2330 train_time:8753ms step_avg:37.25ms
step:236/2330 train_time:8794ms step_avg:37.26ms
step:237/2330 train_time:8828ms step_avg:37.25ms
step:238/2330 train_time:8869ms step_avg:37.26ms
step:239/2330 train_time:8903ms step_avg:37.25ms
step:240/2330 train_time:8944ms step_avg:37.27ms
step:241/2330 train_time:8978ms step_avg:37.25ms
step:242/2330 train_time:9019ms step_avg:37.27ms
step:243/2330 train_time:9054ms step_avg:37.26ms
step:244/2330 train_time:9095ms step_avg:37.27ms
step:245/2330 train_time:9130ms step_avg:37.26ms
step:246/2330 train_time:9170ms step_avg:37.28ms
step:247/2330 train_time:9206ms step_avg:37.27ms
step:248/2330 train_time:9246ms step_avg:37.28ms
step:249/2330 train_time:9281ms step_avg:37.27ms
step:250/2330 train_time:9321ms step_avg:37.28ms
step:250/2330 val_loss:5.5896 train_time:9431ms step_avg:37.72ms
step:251/2330 train_time:9442ms step_avg:37.62ms
step:252/2330 train_time:9453ms step_avg:37.51ms
step:253/2330 train_time:9463ms step_avg:37.40ms
step:254/2330 train_time:9473ms step_avg:37.30ms
step:255/2330 train_time:9508ms step_avg:37.28ms
step:256/2330 train_time:9548ms step_avg:37.30ms
step:257/2330 train_time:9582ms step_avg:37.28ms
step:258/2330 train_time:9622ms step_avg:37.29ms
step:259/2330 train_time:9656ms step_avg:37.28ms
step:260/2330 train_time:9696ms step_avg:37.29ms
step:261/2330 train_time:9731ms step_avg:37.29ms
step:262/2330 train_time:9772ms step_avg:37.30ms
step:263/2330 train_time:9812ms step_avg:37.31ms
step:264/2330 train_time:9852ms step_avg:37.32ms
step:265/2330 train_time:9889ms step_avg:37.32ms
step:266/2330 train_time:9929ms step_avg:37.33ms
step:267/2330 train_time:9965ms step_avg:37.32ms
step:268/2330 train_time:10005ms step_avg:37.33ms
step:269/2330 train_time:10040ms step_avg:37.32ms
step:270/2330 train_time:10080ms step_avg:37.33ms
step:271/2330 train_time:10115ms step_avg:37.32ms
step:272/2330 train_time:10155ms step_avg:37.33ms
step:273/2330 train_time:10189ms step_avg:37.32ms
step:274/2330 train_time:10230ms step_avg:37.33ms
step:275/2330 train_time:10265ms step_avg:37.33ms
step:276/2330 train_time:10305ms step_avg:37.34ms
step:277/2330 train_time:10340ms step_avg:37.33ms
step:278/2330 train_time:10380ms step_avg:37.34ms
step:279/2330 train_time:10416ms step_avg:37.33ms
step:280/2330 train_time:10456ms step_avg:37.34ms
step:281/2330 train_time:10491ms step_avg:37.33ms
step:282/2330 train_time:10531ms step_avg:37.34ms
step:283/2330 train_time:10565ms step_avg:37.33ms
step:284/2330 train_time:10606ms step_avg:37.34ms
step:285/2330 train_time:10641ms step_avg:37.34ms
step:286/2330 train_time:10681ms step_avg:37.35ms
step:287/2330 train_time:10716ms step_avg:37.34ms
step:288/2330 train_time:10757ms step_avg:37.35ms
step:289/2330 train_time:10793ms step_avg:37.35ms
step:290/2330 train_time:10834ms step_avg:37.36ms
step:291/2330 train_time:10869ms step_avg:37.35ms
step:292/2330 train_time:10910ms step_avg:37.36ms
step:293/2330 train_time:10945ms step_avg:37.36ms
step:294/2330 train_time:10985ms step_avg:37.37ms
step:295/2330 train_time:11020ms step_avg:37.36ms
step:296/2330 train_time:11061ms step_avg:37.37ms
step:297/2330 train_time:11096ms step_avg:37.36ms
step:298/2330 train_time:11136ms step_avg:37.37ms
step:299/2330 train_time:11170ms step_avg:37.36ms
step:300/2330 train_time:11211ms step_avg:37.37ms
step:301/2330 train_time:11245ms step_avg:37.36ms
step:302/2330 train_time:11286ms step_avg:37.37ms
step:303/2330 train_time:11320ms step_avg:37.36ms
step:304/2330 train_time:11362ms step_avg:37.37ms
step:305/2330 train_time:11396ms step_avg:37.36ms
step:306/2330 train_time:11436ms step_avg:37.37ms
step:307/2330 train_time:11471ms step_avg:37.36ms
step:308/2330 train_time:11511ms step_avg:37.37ms
step:309/2330 train_time:11546ms step_avg:37.36ms
step:310/2330 train_time:11586ms step_avg:37.38ms
step:311/2330 train_time:11621ms step_avg:37.37ms
step:312/2330 train_time:11661ms step_avg:37.38ms
step:313/2330 train_time:11697ms step_avg:37.37ms
step:314/2330 train_time:11738ms step_avg:37.38ms
step:315/2330 train_time:11774ms step_avg:37.38ms
step:316/2330 train_time:11815ms step_avg:37.39ms
step:317/2330 train_time:11850ms step_avg:37.38ms
step:318/2330 train_time:11891ms step_avg:37.39ms
step:319/2330 train_time:11925ms step_avg:37.38ms
step:320/2330 train_time:11966ms step_avg:37.39ms
step:321/2330 train_time:12001ms step_avg:37.39ms
step:322/2330 train_time:12042ms step_avg:37.40ms
step:323/2330 train_time:12077ms step_avg:37.39ms
step:324/2330 train_time:12117ms step_avg:37.40ms
step:325/2330 train_time:12152ms step_avg:37.39ms
step:326/2330 train_time:12192ms step_avg:37.40ms
step:327/2330 train_time:12227ms step_avg:37.39ms
step:328/2330 train_time:12267ms step_avg:37.40ms
step:329/2330 train_time:12302ms step_avg:37.39ms
step:330/2330 train_time:12343ms step_avg:37.40ms
step:331/2330 train_time:12377ms step_avg:37.39ms
step:332/2330 train_time:12417ms step_avg:37.40ms
step:333/2330 train_time:12452ms step_avg:37.39ms
step:334/2330 train_time:12492ms step_avg:37.40ms
step:335/2330 train_time:12526ms step_avg:37.39ms
step:336/2330 train_time:12567ms step_avg:37.40ms
step:337/2330 train_time:12602ms step_avg:37.39ms
step:338/2330 train_time:12642ms step_avg:37.40ms
step:339/2330 train_time:12677ms step_avg:37.40ms
step:340/2330 train_time:12718ms step_avg:37.41ms
step:341/2330 train_time:12752ms step_avg:37.40ms
step:342/2330 train_time:12793ms step_avg:37.41ms
step:343/2330 train_time:12828ms step_avg:37.40ms
step:344/2330 train_time:12869ms step_avg:37.41ms
step:345/2330 train_time:12904ms step_avg:37.40ms
step:346/2330 train_time:12946ms step_avg:37.42ms
step:347/2330 train_time:12980ms step_avg:37.41ms
step:348/2330 train_time:13020ms step_avg:37.41ms
step:349/2330 train_time:13056ms step_avg:37.41ms
step:350/2330 train_time:13096ms step_avg:37.42ms
step:351/2330 train_time:13132ms step_avg:37.41ms
step:352/2330 train_time:13173ms step_avg:37.42ms
step:353/2330 train_time:13209ms step_avg:37.42ms
step:354/2330 train_time:13249ms step_avg:37.43ms
step:355/2330 train_time:13284ms step_avg:37.42ms
step:356/2330 train_time:13325ms step_avg:37.43ms
step:357/2330 train_time:13359ms step_avg:37.42ms
step:358/2330 train_time:13400ms step_avg:37.43ms
step:359/2330 train_time:13434ms step_avg:37.42ms
step:360/2330 train_time:13475ms step_avg:37.43ms
step:361/2330 train_time:13509ms step_avg:37.42ms
step:362/2330 train_time:13550ms step_avg:37.43ms
step:363/2330 train_time:13584ms step_avg:37.42ms
step:364/2330 train_time:13625ms step_avg:37.43ms
step:365/2330 train_time:13660ms step_avg:37.42ms
step:366/2330 train_time:13700ms step_avg:37.43ms
step:367/2330 train_time:13736ms step_avg:37.43ms
step:368/2330 train_time:13776ms step_avg:37.44ms
step:369/2330 train_time:13812ms step_avg:37.43ms
step:370/2330 train_time:13852ms step_avg:37.44ms
step:371/2330 train_time:13888ms step_avg:37.43ms
step:372/2330 train_time:13928ms step_avg:37.44ms
step:373/2330 train_time:13963ms step_avg:37.43ms
step:374/2330 train_time:14004ms step_avg:37.44ms
step:375/2330 train_time:14038ms step_avg:37.44ms
step:376/2330 train_time:14079ms step_avg:37.44ms
step:377/2330 train_time:14114ms step_avg:37.44ms
step:378/2330 train_time:14155ms step_avg:37.45ms
step:379/2330 train_time:14190ms step_avg:37.44ms
step:380/2330 train_time:14230ms step_avg:37.45ms
step:381/2330 train_time:14265ms step_avg:37.44ms
step:382/2330 train_time:14305ms step_avg:37.45ms
step:383/2330 train_time:14340ms step_avg:37.44ms
step:384/2330 train_time:14380ms step_avg:37.45ms
step:385/2330 train_time:14415ms step_avg:37.44ms
step:386/2330 train_time:14456ms step_avg:37.45ms
step:387/2330 train_time:14490ms step_avg:37.44ms
step:388/2330 train_time:14531ms step_avg:37.45ms
step:389/2330 train_time:14566ms step_avg:37.45ms
step:390/2330 train_time:14607ms step_avg:37.45ms
step:391/2330 train_time:14642ms step_avg:37.45ms
step:392/2330 train_time:14683ms step_avg:37.46ms
step:393/2330 train_time:14717ms step_avg:37.45ms
step:394/2330 train_time:14758ms step_avg:37.46ms
step:395/2330 train_time:14792ms step_avg:37.45ms
step:396/2330 train_time:14833ms step_avg:37.46ms
step:397/2330 train_time:14868ms step_avg:37.45ms
step:398/2330 train_time:14908ms step_avg:37.46ms
step:399/2330 train_time:14943ms step_avg:37.45ms
step:400/2330 train_time:14983ms step_avg:37.46ms
step:401/2330 train_time:15018ms step_avg:37.45ms
step:402/2330 train_time:15059ms step_avg:37.46ms
step:403/2330 train_time:15093ms step_avg:37.45ms
step:404/2330 train_time:15134ms step_avg:37.46ms
step:405/2330 train_time:15168ms step_avg:37.45ms
step:406/2330 train_time:15209ms step_avg:37.46ms
step:407/2330 train_time:15244ms step_avg:37.45ms
step:408/2330 train_time:15285ms step_avg:37.46ms
step:409/2330 train_time:15319ms step_avg:37.46ms
step:410/2330 train_time:15360ms step_avg:37.46ms
step:411/2330 train_time:15395ms step_avg:37.46ms
step:412/2330 train_time:15436ms step_avg:37.47ms
step:413/2330 train_time:15470ms step_avg:37.46ms
step:414/2330 train_time:15511ms step_avg:37.47ms
step:415/2330 train_time:15546ms step_avg:37.46ms
step:416/2330 train_time:15586ms step_avg:37.47ms
step:417/2330 train_time:15621ms step_avg:37.46ms
step:418/2330 train_time:15662ms step_avg:37.47ms
step:419/2330 train_time:15697ms step_avg:37.46ms
step:420/2330 train_time:15737ms step_avg:37.47ms
step:421/2330 train_time:15772ms step_avg:37.46ms
step:422/2330 train_time:15813ms step_avg:37.47ms
step:423/2330 train_time:15848ms step_avg:37.46ms
step:424/2330 train_time:15888ms step_avg:37.47ms
step:425/2330 train_time:15923ms step_avg:37.47ms
step:426/2330 train_time:15964ms step_avg:37.47ms
step:427/2330 train_time:15998ms step_avg:37.47ms
step:428/2330 train_time:16039ms step_avg:37.47ms
step:429/2330 train_time:16074ms step_avg:37.47ms
step:430/2330 train_time:16115ms step_avg:37.48ms
step:431/2330 train_time:16149ms step_avg:37.47ms
step:432/2330 train_time:16190ms step_avg:37.48ms
step:433/2330 train_time:16224ms step_avg:37.47ms
step:434/2330 train_time:16265ms step_avg:37.48ms
step:435/2330 train_time:16300ms step_avg:37.47ms
step:436/2330 train_time:16341ms step_avg:37.48ms
step:437/2330 train_time:16376ms step_avg:37.47ms
step:438/2330 train_time:16417ms step_avg:37.48ms
step:439/2330 train_time:16451ms step_avg:37.47ms
step:440/2330 train_time:16492ms step_avg:37.48ms
step:441/2330 train_time:16528ms step_avg:37.48ms
step:442/2330 train_time:16568ms step_avg:37.48ms
step:443/2330 train_time:16603ms step_avg:37.48ms
step:444/2330 train_time:16644ms step_avg:37.49ms
step:445/2330 train_time:16678ms step_avg:37.48ms
step:446/2330 train_time:16720ms step_avg:37.49ms
step:447/2330 train_time:16754ms step_avg:37.48ms
step:448/2330 train_time:16796ms step_avg:37.49ms
step:449/2330 train_time:16830ms step_avg:37.48ms
step:450/2330 train_time:16871ms step_avg:37.49ms
step:451/2330 train_time:16906ms step_avg:37.48ms
step:452/2330 train_time:16946ms step_avg:37.49ms
step:453/2330 train_time:16981ms step_avg:37.49ms
step:454/2330 train_time:17022ms step_avg:37.49ms
step:455/2330 train_time:17056ms step_avg:37.49ms
step:456/2330 train_time:17097ms step_avg:37.49ms
step:457/2330 train_time:17132ms step_avg:37.49ms
step:458/2330 train_time:17172ms step_avg:37.49ms
step:459/2330 train_time:17207ms step_avg:37.49ms
step:460/2330 train_time:17248ms step_avg:37.49ms
step:461/2330 train_time:17282ms step_avg:37.49ms
step:462/2330 train_time:17323ms step_avg:37.50ms
step:463/2330 train_time:17358ms step_avg:37.49ms
step:464/2330 train_time:17399ms step_avg:37.50ms
step:465/2330 train_time:17434ms step_avg:37.49ms
step:466/2330 train_time:17475ms step_avg:37.50ms
step:467/2330 train_time:17510ms step_avg:37.49ms
step:468/2330 train_time:17550ms step_avg:37.50ms
step:469/2330 train_time:17586ms step_avg:37.50ms
step:470/2330 train_time:17626ms step_avg:37.50ms
step:471/2330 train_time:17661ms step_avg:37.50ms
step:472/2330 train_time:17702ms step_avg:37.50ms
step:473/2330 train_time:17737ms step_avg:37.50ms
step:474/2330 train_time:17778ms step_avg:37.51ms
step:475/2330 train_time:17814ms step_avg:37.50ms
step:476/2330 train_time:17854ms step_avg:37.51ms
step:477/2330 train_time:17889ms step_avg:37.50ms
step:478/2330 train_time:17930ms step_avg:37.51ms
step:479/2330 train_time:17965ms step_avg:37.51ms
step:480/2330 train_time:18006ms step_avg:37.51ms
step:481/2330 train_time:18040ms step_avg:37.51ms
step:482/2330 train_time:18081ms step_avg:37.51ms
step:483/2330 train_time:18116ms step_avg:37.51ms
step:484/2330 train_time:18157ms step_avg:37.51ms
step:485/2330 train_time:18192ms step_avg:37.51ms
step:486/2330 train_time:18232ms step_avg:37.52ms
step:487/2330 train_time:18267ms step_avg:37.51ms
step:488/2330 train_time:18307ms step_avg:37.52ms
step:489/2330 train_time:18342ms step_avg:37.51ms
step:490/2330 train_time:18383ms step_avg:37.52ms
step:491/2330 train_time:18418ms step_avg:37.51ms
step:492/2330 train_time:18458ms step_avg:37.52ms
step:493/2330 train_time:18493ms step_avg:37.51ms
step:494/2330 train_time:18534ms step_avg:37.52ms
step:495/2330 train_time:18569ms step_avg:37.51ms
step:496/2330 train_time:18609ms step_avg:37.52ms
step:497/2330 train_time:18645ms step_avg:37.51ms
step:498/2330 train_time:18686ms step_avg:37.52ms
step:499/2330 train_time:18720ms step_avg:37.52ms
step:500/2330 train_time:18761ms step_avg:37.52ms
step:500/2330 val_loss:5.4515 train_time:18872ms step_avg:37.74ms
step:501/2330 train_time:18884ms step_avg:37.69ms
step:502/2330 train_time:18894ms step_avg:37.64ms
step:503/2330 train_time:18903ms step_avg:37.58ms
step:504/2330 train_time:18915ms step_avg:37.53ms
step:505/2330 train_time:18949ms step_avg:37.52ms
step:506/2330 train_time:18989ms step_avg:37.53ms
step:507/2330 train_time:19024ms step_avg:37.52ms
step:508/2330 train_time:19064ms step_avg:37.53ms
step:509/2330 train_time:19099ms step_avg:37.52ms
step:510/2330 train_time:19139ms step_avg:37.53ms
step:511/2330 train_time:19175ms step_avg:37.53ms
step:512/2330 train_time:19217ms step_avg:37.53ms
step:513/2330 train_time:19257ms step_avg:37.54ms
step:514/2330 train_time:19298ms step_avg:37.55ms
step:515/2330 train_time:19334ms step_avg:37.54ms
step:516/2330 train_time:19375ms step_avg:37.55ms
step:517/2330 train_time:19410ms step_avg:37.54ms
step:518/2330 train_time:19451ms step_avg:37.55ms
step:519/2330 train_time:19485ms step_avg:37.54ms
step:520/2330 train_time:19526ms step_avg:37.55ms
step:521/2330 train_time:19561ms step_avg:37.54ms
step:522/2330 train_time:19601ms step_avg:37.55ms
step:523/2330 train_time:19636ms step_avg:37.54ms
step:524/2330 train_time:19676ms step_avg:37.55ms
step:525/2330 train_time:19710ms step_avg:37.54ms
step:526/2330 train_time:19751ms step_avg:37.55ms
step:527/2330 train_time:19786ms step_avg:37.54ms
step:528/2330 train_time:19827ms step_avg:37.55ms
step:529/2330 train_time:19862ms step_avg:37.55ms
step:530/2330 train_time:19903ms step_avg:37.55ms
step:531/2330 train_time:19938ms step_avg:37.55ms
step:532/2330 train_time:19978ms step_avg:37.55ms
step:533/2330 train_time:20015ms step_avg:37.55ms
step:534/2330 train_time:20055ms step_avg:37.56ms
step:535/2330 train_time:20090ms step_avg:37.55ms
step:536/2330 train_time:20131ms step_avg:37.56ms
step:537/2330 train_time:20166ms step_avg:37.55ms
step:538/2330 train_time:20207ms step_avg:37.56ms
step:539/2330 train_time:20242ms step_avg:37.56ms
step:540/2330 train_time:20283ms step_avg:37.56ms
step:541/2330 train_time:20319ms step_avg:37.56ms
step:542/2330 train_time:20360ms step_avg:37.56ms
step:543/2330 train_time:20395ms step_avg:37.56ms
step:544/2330 train_time:20436ms step_avg:37.57ms
step:545/2330 train_time:20471ms step_avg:37.56ms
step:546/2330 train_time:20512ms step_avg:37.57ms
step:547/2330 train_time:20547ms step_avg:37.56ms
step:548/2330 train_time:20587ms step_avg:37.57ms
step:549/2330 train_time:20623ms step_avg:37.56ms
step:550/2330 train_time:20663ms step_avg:37.57ms
step:551/2330 train_time:20698ms step_avg:37.56ms
step:552/2330 train_time:20738ms step_avg:37.57ms
step:553/2330 train_time:20774ms step_avg:37.57ms
step:554/2330 train_time:20814ms step_avg:37.57ms
step:555/2330 train_time:20851ms step_avg:37.57ms
step:556/2330 train_time:20891ms step_avg:37.57ms
step:557/2330 train_time:20927ms step_avg:37.57ms
step:558/2330 train_time:20968ms step_avg:37.58ms
step:559/2330 train_time:21002ms step_avg:37.57ms
step:560/2330 train_time:21043ms step_avg:37.58ms
step:561/2330 train_time:21078ms step_avg:37.57ms
step:562/2330 train_time:21119ms step_avg:37.58ms
step:563/2330 train_time:21154ms step_avg:37.57ms
step:564/2330 train_time:21195ms step_avg:37.58ms
step:565/2330 train_time:21231ms step_avg:37.58ms
step:566/2330 train_time:21272ms step_avg:37.58ms
step:567/2330 train_time:21308ms step_avg:37.58ms
step:568/2330 train_time:21348ms step_avg:37.58ms
step:569/2330 train_time:21383ms step_avg:37.58ms
step:570/2330 train_time:21424ms step_avg:37.59ms
step:571/2330 train_time:21459ms step_avg:37.58ms
step:572/2330 train_time:21499ms step_avg:37.59ms
step:573/2330 train_time:21535ms step_avg:37.58ms
step:574/2330 train_time:21575ms step_avg:37.59ms
step:575/2330 train_time:21610ms step_avg:37.58ms
step:576/2330 train_time:21651ms step_avg:37.59ms
step:577/2330 train_time:21686ms step_avg:37.58ms
step:578/2330 train_time:21727ms step_avg:37.59ms
step:579/2330 train_time:21761ms step_avg:37.58ms
step:580/2330 train_time:21802ms step_avg:37.59ms
step:581/2330 train_time:21837ms step_avg:37.59ms
step:582/2330 train_time:21878ms step_avg:37.59ms
step:583/2330 train_time:21914ms step_avg:37.59ms
step:584/2330 train_time:21954ms step_avg:37.59ms
step:585/2330 train_time:21990ms step_avg:37.59ms
step:586/2330 train_time:22031ms step_avg:37.60ms
step:587/2330 train_time:22066ms step_avg:37.59ms
step:588/2330 train_time:22106ms step_avg:37.60ms
step:589/2330 train_time:22141ms step_avg:37.59ms
step:590/2330 train_time:22182ms step_avg:37.60ms
step:591/2330 train_time:22218ms step_avg:37.59ms
step:592/2330 train_time:22259ms step_avg:37.60ms
step:593/2330 train_time:22294ms step_avg:37.60ms
step:594/2330 train_time:22335ms step_avg:37.60ms
step:595/2330 train_time:22371ms step_avg:37.60ms
step:596/2330 train_time:22412ms step_avg:37.60ms
step:597/2330 train_time:22447ms step_avg:37.60ms
step:598/2330 train_time:22487ms step_avg:37.60ms
step:599/2330 train_time:22522ms step_avg:37.60ms
step:600/2330 train_time:22563ms step_avg:37.60ms
step:601/2330 train_time:22598ms step_avg:37.60ms
step:602/2330 train_time:22638ms step_avg:37.61ms
step:603/2330 train_time:22675ms step_avg:37.60ms
step:604/2330 train_time:22716ms step_avg:37.61ms
step:605/2330 train_time:22751ms step_avg:37.60ms
step:606/2330 train_time:22791ms step_avg:37.61ms
step:607/2330 train_time:22826ms step_avg:37.61ms
step:608/2330 train_time:22867ms step_avg:37.61ms
step:609/2330 train_time:22901ms step_avg:37.60ms
step:610/2330 train_time:22942ms step_avg:37.61ms
step:611/2330 train_time:22977ms step_avg:37.61ms
step:612/2330 train_time:23018ms step_avg:37.61ms
step:613/2330 train_time:23054ms step_avg:37.61ms
step:614/2330 train_time:23094ms step_avg:37.61ms
step:615/2330 train_time:23129ms step_avg:37.61ms
step:616/2330 train_time:23170ms step_avg:37.61ms
step:617/2330 train_time:23205ms step_avg:37.61ms
step:618/2330 train_time:23245ms step_avg:37.61ms
step:619/2330 train_time:23281ms step_avg:37.61ms
step:620/2330 train_time:23322ms step_avg:37.62ms
step:621/2330 train_time:23358ms step_avg:37.61ms
step:622/2330 train_time:23398ms step_avg:37.62ms
step:623/2330 train_time:23434ms step_avg:37.62ms
step:624/2330 train_time:23475ms step_avg:37.62ms
step:625/2330 train_time:23510ms step_avg:37.62ms
step:626/2330 train_time:23550ms step_avg:37.62ms
step:627/2330 train_time:23585ms step_avg:37.62ms
step:628/2330 train_time:23626ms step_avg:37.62ms
step:629/2330 train_time:23660ms step_avg:37.62ms
step:630/2330 train_time:23701ms step_avg:37.62ms
step:631/2330 train_time:23737ms step_avg:37.62ms
step:632/2330 train_time:23778ms step_avg:37.62ms
step:633/2330 train_time:23814ms step_avg:37.62ms
step:634/2330 train_time:23854ms step_avg:37.62ms
step:635/2330 train_time:23889ms step_avg:37.62ms
step:636/2330 train_time:23930ms step_avg:37.63ms
step:637/2330 train_time:23965ms step_avg:37.62ms
step:638/2330 train_time:24006ms step_avg:37.63ms
step:639/2330 train_time:24041ms step_avg:37.62ms
step:640/2330 train_time:24082ms step_avg:37.63ms
step:641/2330 train_time:24117ms step_avg:37.62ms
step:642/2330 train_time:24158ms step_avg:37.63ms
step:643/2330 train_time:24193ms step_avg:37.62ms
step:644/2330 train_time:24233ms step_avg:37.63ms
step:645/2330 train_time:24268ms step_avg:37.62ms
step:646/2330 train_time:24309ms step_avg:37.63ms
step:647/2330 train_time:24343ms step_avg:37.63ms
step:648/2330 train_time:24384ms step_avg:37.63ms
step:649/2330 train_time:24419ms step_avg:37.63ms
step:650/2330 train_time:24460ms step_avg:37.63ms
step:651/2330 train_time:24494ms step_avg:37.63ms
step:652/2330 train_time:24535ms step_avg:37.63ms
step:653/2330 train_time:24570ms step_avg:37.63ms
step:654/2330 train_time:24611ms step_avg:37.63ms
step:655/2330 train_time:24645ms step_avg:37.63ms
step:656/2330 train_time:24686ms step_avg:37.63ms
step:657/2330 train_time:24721ms step_avg:37.63ms
step:658/2330 train_time:24762ms step_avg:37.63ms
step:659/2330 train_time:24797ms step_avg:37.63ms
step:660/2330 train_time:24838ms step_avg:37.63ms
step:661/2330 train_time:24873ms step_avg:37.63ms
step:662/2330 train_time:24914ms step_avg:37.63ms
step:663/2330 train_time:24949ms step_avg:37.63ms
step:664/2330 train_time:24989ms step_avg:37.63ms
step:665/2330 train_time:25025ms step_avg:37.63ms
step:666/2330 train_time:25066ms step_avg:37.64ms
step:667/2330 train_time:25100ms step_avg:37.63ms
step:668/2330 train_time:25141ms step_avg:37.64ms
step:669/2330 train_time:25175ms step_avg:37.63ms
step:670/2330 train_time:25216ms step_avg:37.64ms
step:671/2330 train_time:25252ms step_avg:37.63ms
step:672/2330 train_time:25293ms step_avg:37.64ms
step:673/2330 train_time:25329ms step_avg:37.64ms
step:674/2330 train_time:25369ms step_avg:37.64ms
step:675/2330 train_time:25405ms step_avg:37.64ms
step:676/2330 train_time:25446ms step_avg:37.64ms
step:677/2330 train_time:25480ms step_avg:37.64ms
step:678/2330 train_time:25521ms step_avg:37.64ms
step:679/2330 train_time:25556ms step_avg:37.64ms
step:680/2330 train_time:25596ms step_avg:37.64ms
step:681/2330 train_time:25631ms step_avg:37.64ms
step:682/2330 train_time:25672ms step_avg:37.64ms
step:683/2330 train_time:25706ms step_avg:37.64ms
step:684/2330 train_time:25747ms step_avg:37.64ms
step:685/2330 train_time:25782ms step_avg:37.64ms
step:686/2330 train_time:25823ms step_avg:37.64ms
step:687/2330 train_time:25857ms step_avg:37.64ms
step:688/2330 train_time:25897ms step_avg:37.64ms
step:689/2330 train_time:25933ms step_avg:37.64ms
step:690/2330 train_time:25974ms step_avg:37.64ms
step:691/2330 train_time:26010ms step_avg:37.64ms
step:692/2330 train_time:26050ms step_avg:37.65ms
step:693/2330 train_time:26087ms step_avg:37.64ms
step:694/2330 train_time:26127ms step_avg:37.65ms
step:695/2330 train_time:26162ms step_avg:37.64ms
step:696/2330 train_time:26203ms step_avg:37.65ms
step:697/2330 train_time:26238ms step_avg:37.64ms
step:698/2330 train_time:26279ms step_avg:37.65ms
step:699/2330 train_time:26314ms step_avg:37.64ms
step:700/2330 train_time:26354ms step_avg:37.65ms
step:701/2330 train_time:26389ms step_avg:37.64ms
step:702/2330 train_time:26429ms step_avg:37.65ms
step:703/2330 train_time:26465ms step_avg:37.65ms
step:704/2330 train_time:26505ms step_avg:37.65ms
step:705/2330 train_time:26540ms step_avg:37.65ms
step:706/2330 train_time:26581ms step_avg:37.65ms
step:707/2330 train_time:26616ms step_avg:37.65ms
step:708/2330 train_time:26657ms step_avg:37.65ms
step:709/2330 train_time:26693ms step_avg:37.65ms
step:710/2330 train_time:26733ms step_avg:37.65ms
step:711/2330 train_time:26770ms step_avg:37.65ms
step:712/2330 train_time:26810ms step_avg:37.65ms
step:713/2330 train_time:26845ms step_avg:37.65ms
step:714/2330 train_time:26886ms step_avg:37.66ms
step:715/2330 train_time:26921ms step_avg:37.65ms
step:716/2330 train_time:26962ms step_avg:37.66ms
step:717/2330 train_time:26998ms step_avg:37.65ms
step:718/2330 train_time:27039ms step_avg:37.66ms
step:719/2330 train_time:27074ms step_avg:37.65ms
step:720/2330 train_time:27115ms step_avg:37.66ms
step:721/2330 train_time:27149ms step_avg:37.66ms
step:722/2330 train_time:27190ms step_avg:37.66ms
step:723/2330 train_time:27225ms step_avg:37.66ms
step:724/2330 train_time:27266ms step_avg:37.66ms
step:725/2330 train_time:27300ms step_avg:37.66ms
step:726/2330 train_time:27341ms step_avg:37.66ms
step:727/2330 train_time:27376ms step_avg:37.66ms
step:728/2330 train_time:27417ms step_avg:37.66ms
step:729/2330 train_time:27453ms step_avg:37.66ms
step:730/2330 train_time:27494ms step_avg:37.66ms
step:731/2330 train_time:27529ms step_avg:37.66ms
step:732/2330 train_time:27570ms step_avg:37.66ms
step:733/2330 train_time:27605ms step_avg:37.66ms
step:734/2330 train_time:27645ms step_avg:37.66ms
step:735/2330 train_time:27680ms step_avg:37.66ms
step:736/2330 train_time:27720ms step_avg:37.66ms
step:737/2330 train_time:27755ms step_avg:37.66ms
step:738/2330 train_time:27795ms step_avg:37.66ms
step:739/2330 train_time:27831ms step_avg:37.66ms
step:740/2330 train_time:27871ms step_avg:37.66ms
step:741/2330 train_time:27907ms step_avg:37.66ms
step:742/2330 train_time:27948ms step_avg:37.67ms
step:743/2330 train_time:27983ms step_avg:37.66ms
step:744/2330 train_time:28024ms step_avg:37.67ms
step:745/2330 train_time:28059ms step_avg:37.66ms
step:746/2330 train_time:28099ms step_avg:37.67ms
step:747/2330 train_time:28135ms step_avg:37.66ms
step:748/2330 train_time:28176ms step_avg:37.67ms
step:749/2330 train_time:28212ms step_avg:37.67ms
step:750/2330 train_time:28253ms step_avg:37.67ms
step:750/2330 val_loss:5.3885 train_time:28365ms step_avg:37.82ms
step:751/2330 train_time:28376ms step_avg:37.78ms
step:752/2330 train_time:28386ms step_avg:37.75ms
step:753/2330 train_time:28395ms step_avg:37.71ms
step:754/2330 train_time:28408ms step_avg:37.68ms
step:755/2330 train_time:28441ms step_avg:37.67ms
step:756/2330 train_time:28482ms step_avg:37.67ms
step:757/2330 train_time:28516ms step_avg:37.67ms
step:758/2330 train_time:28557ms step_avg:37.67ms
step:759/2330 train_time:28591ms step_avg:37.67ms
step:760/2330 train_time:28631ms step_avg:37.67ms
step:761/2330 train_time:28665ms step_avg:37.67ms
step:762/2330 train_time:28707ms step_avg:37.67ms
step:763/2330 train_time:28744ms step_avg:37.67ms
step:764/2330 train_time:28784ms step_avg:37.68ms
step:765/2330 train_time:28822ms step_avg:37.68ms
step:766/2330 train_time:28862ms step_avg:37.68ms
step:767/2330 train_time:28897ms step_avg:37.68ms
step:768/2330 train_time:28937ms step_avg:37.68ms
step:769/2330 train_time:28973ms step_avg:37.68ms
step:770/2330 train_time:29013ms step_avg:37.68ms
step:771/2330 train_time:29049ms step_avg:37.68ms
step:772/2330 train_time:29089ms step_avg:37.68ms
step:773/2330 train_time:29124ms step_avg:37.68ms
step:774/2330 train_time:29165ms step_avg:37.68ms
step:775/2330 train_time:29199ms step_avg:37.68ms
step:776/2330 train_time:29240ms step_avg:37.68ms
step:777/2330 train_time:29275ms step_avg:37.68ms
step:778/2330 train_time:29315ms step_avg:37.68ms
step:779/2330 train_time:29351ms step_avg:37.68ms
step:780/2330 train_time:29392ms step_avg:37.68ms
step:781/2330 train_time:29427ms step_avg:37.68ms
step:782/2330 train_time:29467ms step_avg:37.68ms
step:783/2330 train_time:29502ms step_avg:37.68ms
step:784/2330 train_time:29543ms step_avg:37.68ms
step:785/2330 train_time:29577ms step_avg:37.68ms
step:786/2330 train_time:29618ms step_avg:37.68ms
step:787/2330 train_time:29653ms step_avg:37.68ms
step:788/2330 train_time:29694ms step_avg:37.68ms
step:789/2330 train_time:29730ms step_avg:37.68ms
step:790/2330 train_time:29771ms step_avg:37.68ms
step:791/2330 train_time:29808ms step_avg:37.68ms
step:792/2330 train_time:29848ms step_avg:37.69ms
step:793/2330 train_time:29883ms step_avg:37.68ms
step:794/2330 train_time:29924ms step_avg:37.69ms
step:795/2330 train_time:29959ms step_avg:37.68ms
step:796/2330 train_time:30000ms step_avg:37.69ms
step:797/2330 train_time:30034ms step_avg:37.68ms
step:798/2330 train_time:30075ms step_avg:37.69ms
step:799/2330 train_time:30110ms step_avg:37.68ms
step:800/2330 train_time:30150ms step_avg:37.69ms
step:801/2330 train_time:30185ms step_avg:37.68ms
step:802/2330 train_time:30225ms step_avg:37.69ms
step:803/2330 train_time:30260ms step_avg:37.68ms
step:804/2330 train_time:30300ms step_avg:37.69ms
step:805/2330 train_time:30335ms step_avg:37.68ms
step:806/2330 train_time:30376ms step_avg:37.69ms
step:807/2330 train_time:30411ms step_avg:37.68ms
step:808/2330 train_time:30452ms step_avg:37.69ms
step:809/2330 train_time:30487ms step_avg:37.68ms
step:810/2330 train_time:30527ms step_avg:37.69ms
step:811/2330 train_time:30561ms step_avg:37.68ms
step:812/2330 train_time:30602ms step_avg:37.69ms
step:813/2330 train_time:30637ms step_avg:37.68ms
step:814/2330 train_time:30677ms step_avg:37.69ms
step:815/2330 train_time:30712ms step_avg:37.68ms
step:816/2330 train_time:30753ms step_avg:37.69ms
step:817/2330 train_time:30789ms step_avg:37.69ms
step:818/2330 train_time:30830ms step_avg:37.69ms
step:819/2330 train_time:30866ms step_avg:37.69ms
step:820/2330 train_time:30907ms step_avg:37.69ms
step:821/2330 train_time:30943ms step_avg:37.69ms
step:822/2330 train_time:30983ms step_avg:37.69ms
step:823/2330 train_time:31019ms step_avg:37.69ms
step:824/2330 train_time:31059ms step_avg:37.69ms
step:825/2330 train_time:31094ms step_avg:37.69ms
step:826/2330 train_time:31134ms step_avg:37.69ms
step:827/2330 train_time:31169ms step_avg:37.69ms
step:828/2330 train_time:31209ms step_avg:37.69ms
step:829/2330 train_time:31244ms step_avg:37.69ms
step:830/2330 train_time:31285ms step_avg:37.69ms
step:831/2330 train_time:31319ms step_avg:37.69ms
step:832/2330 train_time:31359ms step_avg:37.69ms
step:833/2330 train_time:31395ms step_avg:37.69ms
step:834/2330 train_time:31435ms step_avg:37.69ms
step:835/2330 train_time:31470ms step_avg:37.69ms
step:836/2330 train_time:31511ms step_avg:37.69ms
step:837/2330 train_time:31546ms step_avg:37.69ms
step:838/2330 train_time:31586ms step_avg:37.69ms
step:839/2330 train_time:31621ms step_avg:37.69ms
step:840/2330 train_time:31661ms step_avg:37.69ms
step:841/2330 train_time:31697ms step_avg:37.69ms
step:842/2330 train_time:31738ms step_avg:37.69ms
step:843/2330 train_time:31772ms step_avg:37.69ms
step:844/2330 train_time:31813ms step_avg:37.69ms
step:845/2330 train_time:31848ms step_avg:37.69ms
step:846/2330 train_time:31889ms step_avg:37.69ms
step:847/2330 train_time:31924ms step_avg:37.69ms
step:848/2330 train_time:31965ms step_avg:37.69ms
step:849/2330 train_time:32000ms step_avg:37.69ms
step:850/2330 train_time:32041ms step_avg:37.69ms
step:851/2330 train_time:32076ms step_avg:37.69ms
step:852/2330 train_time:32117ms step_avg:37.70ms
step:853/2330 train_time:32151ms step_avg:37.69ms
step:854/2330 train_time:32192ms step_avg:37.70ms
step:855/2330 train_time:32227ms step_avg:37.69ms
step:856/2330 train_time:32267ms step_avg:37.70ms
step:857/2330 train_time:32302ms step_avg:37.69ms
step:858/2330 train_time:32342ms step_avg:37.69ms
step:859/2330 train_time:32377ms step_avg:37.69ms
step:860/2330 train_time:32418ms step_avg:37.70ms
step:861/2330 train_time:32453ms step_avg:37.69ms
step:862/2330 train_time:32493ms step_avg:37.70ms
step:863/2330 train_time:32529ms step_avg:37.69ms
step:864/2330 train_time:32569ms step_avg:37.70ms
step:865/2330 train_time:32605ms step_avg:37.69ms
step:866/2330 train_time:32645ms step_avg:37.70ms
step:867/2330 train_time:32680ms step_avg:37.69ms
step:868/2330 train_time:32720ms step_avg:37.70ms
step:869/2330 train_time:32756ms step_avg:37.69ms
step:870/2330 train_time:32796ms step_avg:37.70ms
step:871/2330 train_time:32833ms step_avg:37.70ms
step:872/2330 train_time:32873ms step_avg:37.70ms
step:873/2330 train_time:32910ms step_avg:37.70ms
step:874/2330 train_time:32951ms step_avg:37.70ms
step:875/2330 train_time:32987ms step_avg:37.70ms
step:876/2330 train_time:33027ms step_avg:37.70ms
step:877/2330 train_time:33063ms step_avg:37.70ms
step:878/2330 train_time:33103ms step_avg:37.70ms
step:879/2330 train_time:33138ms step_avg:37.70ms
step:880/2330 train_time:33179ms step_avg:37.70ms
step:881/2330 train_time:33213ms step_avg:37.70ms
step:882/2330 train_time:33254ms step_avg:37.70ms
step:883/2330 train_time:33289ms step_avg:37.70ms
step:884/2330 train_time:33330ms step_avg:37.70ms
step:885/2330 train_time:33364ms step_avg:37.70ms
step:886/2330 train_time:33405ms step_avg:37.70ms
step:887/2330 train_time:33439ms step_avg:37.70ms
step:888/2330 train_time:33480ms step_avg:37.70ms
step:889/2330 train_time:33515ms step_avg:37.70ms
step:890/2330 train_time:33556ms step_avg:37.70ms
step:891/2330 train_time:33592ms step_avg:37.70ms
step:892/2330 train_time:33632ms step_avg:37.70ms
step:893/2330 train_time:33668ms step_avg:37.70ms
step:894/2330 train_time:33709ms step_avg:37.71ms
step:895/2330 train_time:33744ms step_avg:37.70ms
step:896/2330 train_time:33785ms step_avg:37.71ms
step:897/2330 train_time:33820ms step_avg:37.70ms
step:898/2330 train_time:33860ms step_avg:37.71ms
step:899/2330 train_time:33895ms step_avg:37.70ms
step:900/2330 train_time:33936ms step_avg:37.71ms
step:901/2330 train_time:33971ms step_avg:37.70ms
step:902/2330 train_time:34012ms step_avg:37.71ms
step:903/2330 train_time:34048ms step_avg:37.71ms
step:904/2330 train_time:34089ms step_avg:37.71ms
step:905/2330 train_time:34125ms step_avg:37.71ms
step:906/2330 train_time:34166ms step_avg:37.71ms
step:907/2330 train_time:34201ms step_avg:37.71ms
step:908/2330 train_time:34242ms step_avg:37.71ms
step:909/2330 train_time:34277ms step_avg:37.71ms
step:910/2330 train_time:34318ms step_avg:37.71ms
step:911/2330 train_time:34352ms step_avg:37.71ms
step:912/2330 train_time:34393ms step_avg:37.71ms
step:913/2330 train_time:34427ms step_avg:37.71ms
step:914/2330 train_time:34467ms step_avg:37.71ms
step:915/2330 train_time:34502ms step_avg:37.71ms
step:916/2330 train_time:34542ms step_avg:37.71ms
step:917/2330 train_time:34577ms step_avg:37.71ms
step:918/2330 train_time:34618ms step_avg:37.71ms
step:919/2330 train_time:34653ms step_avg:37.71ms
step:920/2330 train_time:34694ms step_avg:37.71ms
step:921/2330 train_time:34728ms step_avg:37.71ms
step:922/2330 train_time:34769ms step_avg:37.71ms
step:923/2330 train_time:34803ms step_avg:37.71ms
step:924/2330 train_time:34844ms step_avg:37.71ms
step:925/2330 train_time:34879ms step_avg:37.71ms
step:926/2330 train_time:34919ms step_avg:37.71ms
step:927/2330 train_time:34954ms step_avg:37.71ms
step:928/2330 train_time:34995ms step_avg:37.71ms
step:929/2330 train_time:35030ms step_avg:37.71ms
step:930/2330 train_time:35071ms step_avg:37.71ms
step:931/2330 train_time:35106ms step_avg:37.71ms
step:932/2330 train_time:35147ms step_avg:37.71ms
step:933/2330 train_time:35182ms step_avg:37.71ms
step:934/2330 train_time:35222ms step_avg:37.71ms
step:935/2330 train_time:35258ms step_avg:37.71ms
step:936/2330 train_time:35299ms step_avg:37.71ms
step:937/2330 train_time:35333ms step_avg:37.71ms
step:938/2330 train_time:35373ms step_avg:37.71ms
step:939/2330 train_time:35408ms step_avg:37.71ms
step:940/2330 train_time:35449ms step_avg:37.71ms
step:941/2330 train_time:35484ms step_avg:37.71ms
step:942/2330 train_time:35525ms step_avg:37.71ms
step:943/2330 train_time:35559ms step_avg:37.71ms
step:944/2330 train_time:35600ms step_avg:37.71ms
step:945/2330 train_time:35635ms step_avg:37.71ms
step:946/2330 train_time:35676ms step_avg:37.71ms
step:947/2330 train_time:35711ms step_avg:37.71ms
step:948/2330 train_time:35752ms step_avg:37.71ms
step:949/2330 train_time:35787ms step_avg:37.71ms
step:950/2330 train_time:35827ms step_avg:37.71ms
step:951/2330 train_time:35862ms step_avg:37.71ms
step:952/2330 train_time:35902ms step_avg:37.71ms
step:953/2330 train_time:35938ms step_avg:37.71ms
step:954/2330 train_time:35978ms step_avg:37.71ms
step:955/2330 train_time:36014ms step_avg:37.71ms
step:956/2330 train_time:36054ms step_avg:37.71ms
step:957/2330 train_time:36090ms step_avg:37.71ms
step:958/2330 train_time:36131ms step_avg:37.71ms
step:959/2330 train_time:36166ms step_avg:37.71ms
step:960/2330 train_time:36207ms step_avg:37.72ms
step:961/2330 train_time:36242ms step_avg:37.71ms
step:962/2330 train_time:36282ms step_avg:37.71ms
step:963/2330 train_time:36317ms step_avg:37.71ms
step:964/2330 train_time:36358ms step_avg:37.72ms
step:965/2330 train_time:36393ms step_avg:37.71ms
step:966/2330 train_time:36433ms step_avg:37.72ms
step:967/2330 train_time:36468ms step_avg:37.71ms
step:968/2330 train_time:36509ms step_avg:37.72ms
step:969/2330 train_time:36544ms step_avg:37.71ms
step:970/2330 train_time:36584ms step_avg:37.72ms
step:971/2330 train_time:36619ms step_avg:37.71ms
step:972/2330 train_time:36660ms step_avg:37.72ms
step:973/2330 train_time:36695ms step_avg:37.71ms
step:974/2330 train_time:36736ms step_avg:37.72ms
step:975/2330 train_time:36770ms step_avg:37.71ms
step:976/2330 train_time:36811ms step_avg:37.72ms
step:977/2330 train_time:36846ms step_avg:37.71ms
step:978/2330 train_time:36887ms step_avg:37.72ms
step:979/2330 train_time:36921ms step_avg:37.71ms
step:980/2330 train_time:36962ms step_avg:37.72ms
step:981/2330 train_time:36997ms step_avg:37.71ms
step:982/2330 train_time:37038ms step_avg:37.72ms
step:983/2330 train_time:37074ms step_avg:37.72ms
step:984/2330 train_time:37115ms step_avg:37.72ms
step:985/2330 train_time:37150ms step_avg:37.72ms
step:986/2330 train_time:37190ms step_avg:37.72ms
step:987/2330 train_time:37226ms step_avg:37.72ms
step:988/2330 train_time:37266ms step_avg:37.72ms
step:989/2330 train_time:37302ms step_avg:37.72ms
step:990/2330 train_time:37342ms step_avg:37.72ms
step:991/2330 train_time:37377ms step_avg:37.72ms
step:992/2330 train_time:37418ms step_avg:37.72ms
step:993/2330 train_time:37454ms step_avg:37.72ms
step:994/2330 train_time:37494ms step_avg:37.72ms
step:995/2330 train_time:37530ms step_avg:37.72ms
step:996/2330 train_time:37570ms step_avg:37.72ms
step:997/2330 train_time:37606ms step_avg:37.72ms
step:998/2330 train_time:37647ms step_avg:37.72ms
step:999/2330 train_time:37682ms step_avg:37.72ms
step:1000/2330 train_time:37722ms step_avg:37.72ms
step:1000/2330 val_loss:5.3478 train_time:37835ms step_avg:37.83ms
step:1001/2330 train_time:37846ms step_avg:37.81ms
step:1002/2330 train_time:37856ms step_avg:37.78ms
step:1003/2330 train_time:37866ms step_avg:37.75ms
step:1004/2330 train_time:37876ms step_avg:37.73ms
step:1005/2330 train_time:37912ms step_avg:37.72ms
step:1006/2330 train_time:37952ms step_avg:37.73ms
step:1007/2330 train_time:37986ms step_avg:37.72ms
step:1008/2330 train_time:38027ms step_avg:37.73ms
step:1009/2330 train_time:38062ms step_avg:37.72ms
step:1010/2330 train_time:38102ms step_avg:37.72ms
step:1011/2330 train_time:38138ms step_avg:37.72ms
step:1012/2330 train_time:38178ms step_avg:37.73ms
step:1013/2330 train_time:38216ms step_avg:37.73ms
step:1014/2330 train_time:38256ms step_avg:37.73ms
step:1015/2330 train_time:38292ms step_avg:37.73ms
step:1016/2330 train_time:38333ms step_avg:37.73ms
step:1017/2330 train_time:38368ms step_avg:37.73ms
step:1018/2330 train_time:38408ms step_avg:37.73ms
step:1019/2330 train_time:38444ms step_avg:37.73ms
step:1020/2330 train_time:38484ms step_avg:37.73ms
step:1021/2330 train_time:38518ms step_avg:37.73ms
step:1022/2330 train_time:38559ms step_avg:37.73ms
step:1023/2330 train_time:38593ms step_avg:37.73ms
step:1024/2330 train_time:38634ms step_avg:37.73ms
step:1025/2330 train_time:38668ms step_avg:37.73ms
step:1026/2330 train_time:38709ms step_avg:37.73ms
step:1027/2330 train_time:38745ms step_avg:37.73ms
step:1028/2330 train_time:38787ms step_avg:37.73ms
step:1029/2330 train_time:38822ms step_avg:37.73ms
step:1030/2330 train_time:38863ms step_avg:37.73ms
step:1031/2330 train_time:38899ms step_avg:37.73ms
step:1032/2330 train_time:38939ms step_avg:37.73ms
step:1033/2330 train_time:38975ms step_avg:37.73ms
step:1034/2330 train_time:39015ms step_avg:37.73ms
step:1035/2330 train_time:39051ms step_avg:37.73ms
step:1036/2330 train_time:39091ms step_avg:37.73ms
step:1037/2330 train_time:39127ms step_avg:37.73ms
step:1038/2330 train_time:39167ms step_avg:37.73ms
step:1039/2330 train_time:39203ms step_avg:37.73ms
step:1040/2330 train_time:39243ms step_avg:37.73ms
step:1041/2330 train_time:39278ms step_avg:37.73ms
step:1042/2330 train_time:39319ms step_avg:37.73ms
step:1043/2330 train_time:39354ms step_avg:37.73ms
step:1044/2330 train_time:39395ms step_avg:37.73ms
step:1045/2330 train_time:39429ms step_avg:37.73ms
step:1046/2330 train_time:39469ms step_avg:37.73ms
step:1047/2330 train_time:39504ms step_avg:37.73ms
step:1048/2330 train_time:39545ms step_avg:37.73ms
step:1049/2330 train_time:39579ms step_avg:37.73ms
step:1050/2330 train_time:39620ms step_avg:37.73ms
step:1051/2330 train_time:39655ms step_avg:37.73ms
step:1052/2330 train_time:39695ms step_avg:37.73ms
step:1053/2330 train_time:39730ms step_avg:37.73ms
step:1054/2330 train_time:39771ms step_avg:37.73ms
step:1055/2330 train_time:39806ms step_avg:37.73ms
step:1056/2330 train_time:39846ms step_avg:37.73ms
step:1057/2330 train_time:39882ms step_avg:37.73ms
step:1058/2330 train_time:39923ms step_avg:37.73ms
step:1059/2330 train_time:39958ms step_avg:37.73ms
step:1060/2330 train_time:39999ms step_avg:37.73ms
step:1061/2330 train_time:40035ms step_avg:37.73ms
step:1062/2330 train_time:40076ms step_avg:37.74ms
step:1063/2330 train_time:40110ms step_avg:37.73ms
step:1064/2330 train_time:40151ms step_avg:37.74ms
step:1065/2330 train_time:40186ms step_avg:37.73ms
step:1066/2330 train_time:40226ms step_avg:37.74ms
step:1067/2330 train_time:40261ms step_avg:37.73ms
step:1068/2330 train_time:40302ms step_avg:37.74ms
step:1069/2330 train_time:40338ms step_avg:37.73ms
step:1070/2330 train_time:40378ms step_avg:37.74ms
step:1071/2330 train_time:40414ms step_avg:37.73ms
step:1072/2330 train_time:40455ms step_avg:37.74ms
step:1073/2330 train_time:40488ms step_avg:37.73ms
step:1074/2330 train_time:40529ms step_avg:37.74ms
step:1075/2330 train_time:40564ms step_avg:37.73ms
step:1076/2330 train_time:40604ms step_avg:37.74ms
step:1077/2330 train_time:40641ms step_avg:37.74ms
step:1078/2330 train_time:40682ms step_avg:37.74ms
step:1079/2330 train_time:40717ms step_avg:37.74ms
step:1080/2330 train_time:40758ms step_avg:37.74ms
step:1081/2330 train_time:40792ms step_avg:37.74ms
step:1082/2330 train_time:40833ms step_avg:37.74ms
step:1083/2330 train_time:40869ms step_avg:37.74ms
step:1084/2330 train_time:40910ms step_avg:37.74ms
step:1085/2330 train_time:40946ms step_avg:37.74ms
step:1086/2330 train_time:40986ms step_avg:37.74ms
step:1087/2330 train_time:41021ms step_avg:37.74ms
step:1088/2330 train_time:41062ms step_avg:37.74ms
step:1089/2330 train_time:41097ms step_avg:37.74ms
step:1090/2330 train_time:41137ms step_avg:37.74ms
step:1091/2330 train_time:41172ms step_avg:37.74ms
step:1092/2330 train_time:41213ms step_avg:37.74ms
step:1093/2330 train_time:41248ms step_avg:37.74ms
step:1094/2330 train_time:41289ms step_avg:37.74ms
step:1095/2330 train_time:41324ms step_avg:37.74ms
step:1096/2330 train_time:41365ms step_avg:37.74ms
step:1097/2330 train_time:41399ms step_avg:37.74ms
step:1098/2330 train_time:41440ms step_avg:37.74ms
step:1099/2330 train_time:41475ms step_avg:37.74ms
step:1100/2330 train_time:41515ms step_avg:37.74ms
step:1101/2330 train_time:41550ms step_avg:37.74ms
step:1102/2330 train_time:41590ms step_avg:37.74ms
step:1103/2330 train_time:41626ms step_avg:37.74ms
step:1104/2330 train_time:41667ms step_avg:37.74ms
step:1105/2330 train_time:41703ms step_avg:37.74ms
step:1106/2330 train_time:41744ms step_avg:37.74ms
step:1107/2330 train_time:41779ms step_avg:37.74ms
step:1108/2330 train_time:41819ms step_avg:37.74ms
step:1109/2330 train_time:41854ms step_avg:37.74ms
step:1110/2330 train_time:41895ms step_avg:37.74ms
step:1111/2330 train_time:41930ms step_avg:37.74ms
step:1112/2330 train_time:41971ms step_avg:37.74ms
step:1113/2330 train_time:42006ms step_avg:37.74ms
step:1114/2330 train_time:42047ms step_avg:37.74ms
step:1115/2330 train_time:42083ms step_avg:37.74ms
step:1116/2330 train_time:42124ms step_avg:37.75ms
step:1117/2330 train_time:42159ms step_avg:37.74ms
step:1118/2330 train_time:42200ms step_avg:37.75ms
step:1119/2330 train_time:42235ms step_avg:37.74ms
step:1120/2330 train_time:42276ms step_avg:37.75ms
step:1121/2330 train_time:42311ms step_avg:37.74ms
step:1122/2330 train_time:42351ms step_avg:37.75ms
step:1123/2330 train_time:42386ms step_avg:37.74ms
step:1124/2330 train_time:42427ms step_avg:37.75ms
step:1125/2330 train_time:42462ms step_avg:37.74ms
step:1126/2330 train_time:42503ms step_avg:37.75ms
step:1127/2330 train_time:42539ms step_avg:37.75ms
step:1128/2330 train_time:42579ms step_avg:37.75ms
step:1129/2330 train_time:42615ms step_avg:37.75ms
step:1130/2330 train_time:42656ms step_avg:37.75ms
step:1131/2330 train_time:42690ms step_avg:37.75ms
step:1132/2330 train_time:42731ms step_avg:37.75ms
step:1133/2330 train_time:42767ms step_avg:37.75ms
step:1134/2330 train_time:42808ms step_avg:37.75ms
step:1135/2330 train_time:42843ms step_avg:37.75ms
step:1136/2330 train_time:42884ms step_avg:37.75ms
step:1137/2330 train_time:42919ms step_avg:37.75ms
step:1138/2330 train_time:42959ms step_avg:37.75ms
step:1139/2330 train_time:42995ms step_avg:37.75ms
step:1140/2330 train_time:43036ms step_avg:37.75ms
step:1141/2330 train_time:43071ms step_avg:37.75ms
step:1142/2330 train_time:43112ms step_avg:37.75ms
step:1143/2330 train_time:43147ms step_avg:37.75ms
step:1144/2330 train_time:43188ms step_avg:37.75ms
step:1145/2330 train_time:43224ms step_avg:37.75ms
step:1146/2330 train_time:43265ms step_avg:37.75ms
step:1147/2330 train_time:43301ms step_avg:37.75ms
step:1148/2330 train_time:43341ms step_avg:37.75ms
step:1149/2330 train_time:43376ms step_avg:37.75ms
step:1150/2330 train_time:43417ms step_avg:37.75ms
step:1151/2330 train_time:43452ms step_avg:37.75ms
step:1152/2330 train_time:43493ms step_avg:37.75ms
step:1153/2330 train_time:43528ms step_avg:37.75ms
step:1154/2330 train_time:43568ms step_avg:37.75ms
step:1155/2330 train_time:43604ms step_avg:37.75ms
step:1156/2330 train_time:43645ms step_avg:37.76ms
step:1157/2330 train_time:43679ms step_avg:37.75ms
step:1158/2330 train_time:43720ms step_avg:37.75ms
step:1159/2330 train_time:43755ms step_avg:37.75ms
step:1160/2330 train_time:43796ms step_avg:37.75ms
step:1161/2330 train_time:43831ms step_avg:37.75ms
step:1162/2330 train_time:43871ms step_avg:37.75ms
step:1163/2330 train_time:43906ms step_avg:37.75ms
step:1164/2330 train_time:43947ms step_avg:37.76ms
step:1165/2330 train_time:43982ms step_avg:37.75ms
step:1166/2330 train_time:44023ms step_avg:37.76ms
step:1167/2330 train_time:44058ms step_avg:37.75ms
step:1168/2330 train_time:44098ms step_avg:37.76ms
step:1169/2330 train_time:44134ms step_avg:37.75ms
step:1170/2330 train_time:44174ms step_avg:37.76ms
step:1171/2330 train_time:44209ms step_avg:37.75ms
step:1172/2330 train_time:44250ms step_avg:37.76ms
step:1173/2330 train_time:44285ms step_avg:37.75ms
step:1174/2330 train_time:44326ms step_avg:37.76ms
step:1175/2330 train_time:44361ms step_avg:37.75ms
step:1176/2330 train_time:44402ms step_avg:37.76ms
step:1177/2330 train_time:44437ms step_avg:37.75ms
step:1178/2330 train_time:44477ms step_avg:37.76ms
step:1179/2330 train_time:44513ms step_avg:37.75ms
step:1180/2330 train_time:44553ms step_avg:37.76ms
step:1181/2330 train_time:44588ms step_avg:37.75ms
step:1182/2330 train_time:44628ms step_avg:37.76ms
step:1183/2330 train_time:44665ms step_avg:37.76ms
step:1184/2330 train_time:44705ms step_avg:37.76ms
step:1185/2330 train_time:44741ms step_avg:37.76ms
step:1186/2330 train_time:44781ms step_avg:37.76ms
step:1187/2330 train_time:44817ms step_avg:37.76ms
step:1188/2330 train_time:44857ms step_avg:37.76ms
step:1189/2330 train_time:44893ms step_avg:37.76ms
step:1190/2330 train_time:44933ms step_avg:37.76ms
step:1191/2330 train_time:44969ms step_avg:37.76ms
step:1192/2330 train_time:45009ms step_avg:37.76ms
step:1193/2330 train_time:45046ms step_avg:37.76ms
step:1194/2330 train_time:45086ms step_avg:37.76ms
step:1195/2330 train_time:45122ms step_avg:37.76ms
step:1196/2330 train_time:45163ms step_avg:37.76ms
step:1197/2330 train_time:45199ms step_avg:37.76ms
step:1198/2330 train_time:45239ms step_avg:37.76ms
step:1199/2330 train_time:45275ms step_avg:37.76ms
step:1200/2330 train_time:45316ms step_avg:37.76ms
step:1201/2330 train_time:45350ms step_avg:37.76ms
step:1202/2330 train_time:45391ms step_avg:37.76ms
step:1203/2330 train_time:45426ms step_avg:37.76ms
step:1204/2330 train_time:45466ms step_avg:37.76ms
step:1205/2330 train_time:45502ms step_avg:37.76ms
step:1206/2330 train_time:45542ms step_avg:37.76ms
step:1207/2330 train_time:45579ms step_avg:37.76ms
step:1208/2330 train_time:45619ms step_avg:37.76ms
step:1209/2330 train_time:45655ms step_avg:37.76ms
step:1210/2330 train_time:45696ms step_avg:37.77ms
step:1211/2330 train_time:45731ms step_avg:37.76ms
step:1212/2330 train_time:45772ms step_avg:37.77ms
step:1213/2330 train_time:45807ms step_avg:37.76ms
step:1214/2330 train_time:45848ms step_avg:37.77ms
step:1215/2330 train_time:45884ms step_avg:37.76ms
step:1216/2330 train_time:45924ms step_avg:37.77ms
step:1217/2330 train_time:45959ms step_avg:37.76ms
step:1218/2330 train_time:46000ms step_avg:37.77ms
step:1219/2330 train_time:46035ms step_avg:37.76ms
step:1220/2330 train_time:46076ms step_avg:37.77ms
step:1221/2330 train_time:46111ms step_avg:37.77ms
step:1222/2330 train_time:46152ms step_avg:37.77ms
step:1223/2330 train_time:46187ms step_avg:37.77ms
step:1224/2330 train_time:46228ms step_avg:37.77ms
step:1225/2330 train_time:46263ms step_avg:37.77ms
step:1226/2330 train_time:46304ms step_avg:37.77ms
step:1227/2330 train_time:46340ms step_avg:37.77ms
step:1228/2330 train_time:46381ms step_avg:37.77ms
step:1229/2330 train_time:46416ms step_avg:37.77ms
step:1230/2330 train_time:46457ms step_avg:37.77ms
step:1231/2330 train_time:46492ms step_avg:37.77ms
step:1232/2330 train_time:46533ms step_avg:37.77ms
step:1233/2330 train_time:46568ms step_avg:37.77ms
step:1234/2330 train_time:46609ms step_avg:37.77ms
step:1235/2330 train_time:46646ms step_avg:37.77ms
step:1236/2330 train_time:46687ms step_avg:37.77ms
step:1237/2330 train_time:46722ms step_avg:37.77ms
step:1238/2330 train_time:46763ms step_avg:37.77ms
step:1239/2330 train_time:46798ms step_avg:37.77ms
step:1240/2330 train_time:46838ms step_avg:37.77ms
step:1241/2330 train_time:46873ms step_avg:37.77ms
step:1242/2330 train_time:46914ms step_avg:37.77ms
step:1243/2330 train_time:46949ms step_avg:37.77ms
step:1244/2330 train_time:46989ms step_avg:37.77ms
step:1245/2330 train_time:47025ms step_avg:37.77ms
step:1246/2330 train_time:47066ms step_avg:37.77ms
step:1247/2330 train_time:47101ms step_avg:37.77ms
step:1248/2330 train_time:47142ms step_avg:37.77ms
step:1249/2330 train_time:47177ms step_avg:37.77ms
step:1250/2330 train_time:47217ms step_avg:37.77ms
step:1250/2330 val_loss:5.3146 train_time:47330ms step_avg:37.86ms
step:1251/2330 train_time:47342ms step_avg:37.84ms
step:1252/2330 train_time:47353ms step_avg:37.82ms
step:1253/2330 train_time:47362ms step_avg:37.80ms
step:1254/2330 train_time:47374ms step_avg:37.78ms
step:1255/2330 train_time:47407ms step_avg:37.77ms
step:1256/2330 train_time:47448ms step_avg:37.78ms
step:1257/2330 train_time:47482ms step_avg:37.77ms
step:1258/2330 train_time:47522ms step_avg:37.78ms
step:1259/2330 train_time:47557ms step_avg:37.77ms
step:1260/2330 train_time:47597ms step_avg:37.78ms
step:1261/2330 train_time:47632ms step_avg:37.77ms
step:1262/2330 train_time:47674ms step_avg:37.78ms
step:1263/2330 train_time:47712ms step_avg:37.78ms
step:1264/2330 train_time:47752ms step_avg:37.78ms
step:1265/2330 train_time:47788ms step_avg:37.78ms
step:1266/2330 train_time:47829ms step_avg:37.78ms
step:1267/2330 train_time:47865ms step_avg:37.78ms
step:1268/2330 train_time:47905ms step_avg:37.78ms
step:1269/2330 train_time:47942ms step_avg:37.78ms
step:1270/2330 train_time:47982ms step_avg:37.78ms
step:1271/2330 train_time:48017ms step_avg:37.78ms
step:1272/2330 train_time:48057ms step_avg:37.78ms
step:1273/2330 train_time:48091ms step_avg:37.78ms
step:1274/2330 train_time:48132ms step_avg:37.78ms
step:1275/2330 train_time:48167ms step_avg:37.78ms
step:1276/2330 train_time:48207ms step_avg:37.78ms
step:1277/2330 train_time:48242ms step_avg:37.78ms
step:1278/2330 train_time:48284ms step_avg:37.78ms
step:1279/2330 train_time:48319ms step_avg:37.78ms
step:1280/2330 train_time:48359ms step_avg:37.78ms
step:1281/2330 train_time:48394ms step_avg:37.78ms
step:1282/2330 train_time:48435ms step_avg:37.78ms
step:1283/2330 train_time:48470ms step_avg:37.78ms
step:1284/2330 train_time:48510ms step_avg:37.78ms
step:1285/2330 train_time:48545ms step_avg:37.78ms
step:1286/2330 train_time:48586ms step_avg:37.78ms
step:1287/2330 train_time:48622ms step_avg:37.78ms
step:1288/2330 train_time:48663ms step_avg:37.78ms
step:1289/2330 train_time:48699ms step_avg:37.78ms
step:1290/2330 train_time:48740ms step_avg:37.78ms
step:1291/2330 train_time:48776ms step_avg:37.78ms
step:1292/2330 train_time:48816ms step_avg:37.78ms
step:1293/2330 train_time:48852ms step_avg:37.78ms
step:1294/2330 train_time:48893ms step_avg:37.78ms
step:1295/2330 train_time:48928ms step_avg:37.78ms
step:1296/2330 train_time:48969ms step_avg:37.78ms
step:1297/2330 train_time:49004ms step_avg:37.78ms
step:1298/2330 train_time:49044ms step_avg:37.78ms
step:1299/2330 train_time:49079ms step_avg:37.78ms
step:1300/2330 train_time:49120ms step_avg:37.78ms
step:1301/2330 train_time:49154ms step_avg:37.78ms
step:1302/2330 train_time:49195ms step_avg:37.78ms
step:1303/2330 train_time:49230ms step_avg:37.78ms
step:1304/2330 train_time:49271ms step_avg:37.78ms
step:1305/2330 train_time:49305ms step_avg:37.78ms
step:1306/2330 train_time:49346ms step_avg:37.78ms
step:1307/2330 train_time:49381ms step_avg:37.78ms
step:1308/2330 train_time:49422ms step_avg:37.78ms
step:1309/2330 train_time:49456ms step_avg:37.78ms
step:1310/2330 train_time:49497ms step_avg:37.78ms
step:1311/2330 train_time:49532ms step_avg:37.78ms
step:1312/2330 train_time:49573ms step_avg:37.78ms
step:1313/2330 train_time:49608ms step_avg:37.78ms
step:1314/2330 train_time:49649ms step_avg:37.78ms
step:1315/2330 train_time:49685ms step_avg:37.78ms
step:1316/2330 train_time:49726ms step_avg:37.79ms
step:1317/2330 train_time:49762ms step_avg:37.78ms
step:1318/2330 train_time:49803ms step_avg:37.79ms
step:1319/2330 train_time:49838ms step_avg:37.78ms
step:1320/2330 train_time:49878ms step_avg:37.79ms
step:1321/2330 train_time:49914ms step_avg:37.79ms
step:1322/2330 train_time:49955ms step_avg:37.79ms
step:1323/2330 train_time:49989ms step_avg:37.78ms
step:1324/2330 train_time:50030ms step_avg:37.79ms
step:1325/2330 train_time:50065ms step_avg:37.78ms
step:1326/2330 train_time:50105ms step_avg:37.79ms
step:1327/2330 train_time:50141ms step_avg:37.79ms
step:1328/2330 train_time:50182ms step_avg:37.79ms
step:1329/2330 train_time:50218ms step_avg:37.79ms
step:1330/2330 train_time:50258ms step_avg:37.79ms
step:1331/2330 train_time:50294ms step_avg:37.79ms
step:1332/2330 train_time:50335ms step_avg:37.79ms
step:1333/2330 train_time:50369ms step_avg:37.79ms
step:1334/2330 train_time:50410ms step_avg:37.79ms
step:1335/2330 train_time:50445ms step_avg:37.79ms
step:1336/2330 train_time:50486ms step_avg:37.79ms
step:1337/2330 train_time:50522ms step_avg:37.79ms
step:1338/2330 train_time:50562ms step_avg:37.79ms
step:1339/2330 train_time:50598ms step_avg:37.79ms
step:1340/2330 train_time:50639ms step_avg:37.79ms
step:1341/2330 train_time:50675ms step_avg:37.79ms
step:1342/2330 train_time:50715ms step_avg:37.79ms
step:1343/2330 train_time:50751ms step_avg:37.79ms
step:1344/2330 train_time:50792ms step_avg:37.79ms
step:1345/2330 train_time:50828ms step_avg:37.79ms
step:1346/2330 train_time:50869ms step_avg:37.79ms
step:1347/2330 train_time:50904ms step_avg:37.79ms
step:1348/2330 train_time:50945ms step_avg:37.79ms
step:1349/2330 train_time:50982ms step_avg:37.79ms
step:1350/2330 train_time:51022ms step_avg:37.79ms
step:1351/2330 train_time:51057ms step_avg:37.79ms
step:1352/2330 train_time:51097ms step_avg:37.79ms
step:1353/2330 train_time:51133ms step_avg:37.79ms
step:1354/2330 train_time:51174ms step_avg:37.79ms
step:1355/2330 train_time:51209ms step_avg:37.79ms
step:1356/2330 train_time:51250ms step_avg:37.79ms
step:1357/2330 train_time:51286ms step_avg:37.79ms
step:1358/2330 train_time:51326ms step_avg:37.80ms
step:1359/2330 train_time:51362ms step_avg:37.79ms
step:1360/2330 train_time:51402ms step_avg:37.80ms
step:1361/2330 train_time:51438ms step_avg:37.79ms
step:1362/2330 train_time:51478ms step_avg:37.80ms
step:1363/2330 train_time:51513ms step_avg:37.79ms
step:1364/2330 train_time:51554ms step_avg:37.80ms
step:1365/2330 train_time:51589ms step_avg:37.79ms
step:1366/2330 train_time:51630ms step_avg:37.80ms
step:1367/2330 train_time:51666ms step_avg:37.79ms
step:1368/2330 train_time:51706ms step_avg:37.80ms
step:1369/2330 train_time:51742ms step_avg:37.80ms
step:1370/2330 train_time:51783ms step_avg:37.80ms
step:1371/2330 train_time:51819ms step_avg:37.80ms
step:1372/2330 train_time:51859ms step_avg:37.80ms
step:1373/2330 train_time:51894ms step_avg:37.80ms
step:1374/2330 train_time:51935ms step_avg:37.80ms
step:1375/2330 train_time:51970ms step_avg:37.80ms
step:1376/2330 train_time:52010ms step_avg:37.80ms
step:1377/2330 train_time:52046ms step_avg:37.80ms
step:1378/2330 train_time:52088ms step_avg:37.80ms
step:1379/2330 train_time:52123ms step_avg:37.80ms
step:1380/2330 train_time:52164ms step_avg:37.80ms
step:1381/2330 train_time:52199ms step_avg:37.80ms
step:1382/2330 train_time:52240ms step_avg:37.80ms
step:1383/2330 train_time:52275ms step_avg:37.80ms
step:1384/2330 train_time:52316ms step_avg:37.80ms
step:1385/2330 train_time:52351ms step_avg:37.80ms
step:1386/2330 train_time:52392ms step_avg:37.80ms
step:1387/2330 train_time:52426ms step_avg:37.80ms
step:1388/2330 train_time:52467ms step_avg:37.80ms
step:1389/2330 train_time:52502ms step_avg:37.80ms
step:1390/2330 train_time:52542ms step_avg:37.80ms
step:1391/2330 train_time:52577ms step_avg:37.80ms
step:1392/2330 train_time:52617ms step_avg:37.80ms
step:1393/2330 train_time:52653ms step_avg:37.80ms
step:1394/2330 train_time:52694ms step_avg:37.80ms
step:1395/2330 train_time:52729ms step_avg:37.80ms
step:1396/2330 train_time:52770ms step_avg:37.80ms
step:1397/2330 train_time:52805ms step_avg:37.80ms
step:1398/2330 train_time:52846ms step_avg:37.80ms
step:1399/2330 train_time:52882ms step_avg:37.80ms
step:1400/2330 train_time:52922ms step_avg:37.80ms
step:1401/2330 train_time:52958ms step_avg:37.80ms
step:1402/2330 train_time:52999ms step_avg:37.80ms
step:1403/2330 train_time:53034ms step_avg:37.80ms
step:1404/2330 train_time:53075ms step_avg:37.80ms
step:1405/2330 train_time:53109ms step_avg:37.80ms
step:1406/2330 train_time:53150ms step_avg:37.80ms
step:1407/2330 train_time:53186ms step_avg:37.80ms
step:1408/2330 train_time:53226ms step_avg:37.80ms
step:1409/2330 train_time:53263ms step_avg:37.80ms
step:1410/2330 train_time:53303ms step_avg:37.80ms
step:1411/2330 train_time:53339ms step_avg:37.80ms
step:1412/2330 train_time:53380ms step_avg:37.80ms
step:1413/2330 train_time:53416ms step_avg:37.80ms
step:1414/2330 train_time:53456ms step_avg:37.80ms
step:1415/2330 train_time:53491ms step_avg:37.80ms
step:1416/2330 train_time:53533ms step_avg:37.81ms
step:1417/2330 train_time:53567ms step_avg:37.80ms
step:1418/2330 train_time:53608ms step_avg:37.81ms
step:1419/2330 train_time:53644ms step_avg:37.80ms
step:1420/2330 train_time:53684ms step_avg:37.81ms
step:1421/2330 train_time:53721ms step_avg:37.80ms
step:1422/2330 train_time:53762ms step_avg:37.81ms
step:1423/2330 train_time:53797ms step_avg:37.81ms
step:1424/2330 train_time:53837ms step_avg:37.81ms
step:1425/2330 train_time:53872ms step_avg:37.80ms
step:1426/2330 train_time:53913ms step_avg:37.81ms
step:1427/2330 train_time:53948ms step_avg:37.81ms
step:1428/2330 train_time:53988ms step_avg:37.81ms
step:1429/2330 train_time:54024ms step_avg:37.81ms
step:1430/2330 train_time:54064ms step_avg:37.81ms
step:1431/2330 train_time:54099ms step_avg:37.81ms
step:1432/2330 train_time:54140ms step_avg:37.81ms
step:1433/2330 train_time:54175ms step_avg:37.81ms
step:1434/2330 train_time:54215ms step_avg:37.81ms
step:1435/2330 train_time:54252ms step_avg:37.81ms
step:1436/2330 train_time:54292ms step_avg:37.81ms
step:1437/2330 train_time:54327ms step_avg:37.81ms
step:1438/2330 train_time:54368ms step_avg:37.81ms
step:1439/2330 train_time:54403ms step_avg:37.81ms
step:1440/2330 train_time:54443ms step_avg:37.81ms
step:1441/2330 train_time:54479ms step_avg:37.81ms
step:1442/2330 train_time:54520ms step_avg:37.81ms
step:1443/2330 train_time:54555ms step_avg:37.81ms
step:1444/2330 train_time:54596ms step_avg:37.81ms
step:1445/2330 train_time:54630ms step_avg:37.81ms
step:1446/2330 train_time:54671ms step_avg:37.81ms
step:1447/2330 train_time:54706ms step_avg:37.81ms
step:1448/2330 train_time:54746ms step_avg:37.81ms
step:1449/2330 train_time:54783ms step_avg:37.81ms
step:1450/2330 train_time:54823ms step_avg:37.81ms
step:1451/2330 train_time:54858ms step_avg:37.81ms
step:1452/2330 train_time:54899ms step_avg:37.81ms
step:1453/2330 train_time:54934ms step_avg:37.81ms
step:1454/2330 train_time:54975ms step_avg:37.81ms
step:1455/2330 train_time:55009ms step_avg:37.81ms
step:1456/2330 train_time:55050ms step_avg:37.81ms
step:1457/2330 train_time:55086ms step_avg:37.81ms
step:1458/2330 train_time:55126ms step_avg:37.81ms
step:1459/2330 train_time:55163ms step_avg:37.81ms
step:1460/2330 train_time:55203ms step_avg:37.81ms
step:1461/2330 train_time:55239ms step_avg:37.81ms
step:1462/2330 train_time:55280ms step_avg:37.81ms
step:1463/2330 train_time:55316ms step_avg:37.81ms
step:1464/2330 train_time:55356ms step_avg:37.81ms
step:1465/2330 train_time:55392ms step_avg:37.81ms
step:1466/2330 train_time:55433ms step_avg:37.81ms
step:1467/2330 train_time:55467ms step_avg:37.81ms
step:1468/2330 train_time:55508ms step_avg:37.81ms
step:1469/2330 train_time:55543ms step_avg:37.81ms
step:1470/2330 train_time:55584ms step_avg:37.81ms
step:1471/2330 train_time:55619ms step_avg:37.81ms
step:1472/2330 train_time:55660ms step_avg:37.81ms
step:1473/2330 train_time:55695ms step_avg:37.81ms
step:1474/2330 train_time:55736ms step_avg:37.81ms
step:1475/2330 train_time:55771ms step_avg:37.81ms
step:1476/2330 train_time:55812ms step_avg:37.81ms
step:1477/2330 train_time:55848ms step_avg:37.81ms
step:1478/2330 train_time:55889ms step_avg:37.81ms
step:1479/2330 train_time:55924ms step_avg:37.81ms
step:1480/2330 train_time:55965ms step_avg:37.81ms
step:1481/2330 train_time:56001ms step_avg:37.81ms
step:1482/2330 train_time:56041ms step_avg:37.81ms
step:1483/2330 train_time:56077ms step_avg:37.81ms
step:1484/2330 train_time:56117ms step_avg:37.81ms
step:1485/2330 train_time:56152ms step_avg:37.81ms
step:1486/2330 train_time:56193ms step_avg:37.81ms
step:1487/2330 train_time:56229ms step_avg:37.81ms
step:1488/2330 train_time:56269ms step_avg:37.82ms
step:1489/2330 train_time:56305ms step_avg:37.81ms
step:1490/2330 train_time:56346ms step_avg:37.82ms
step:1491/2330 train_time:56381ms step_avg:37.81ms
step:1492/2330 train_time:56421ms step_avg:37.82ms
step:1493/2330 train_time:56457ms step_avg:37.81ms
step:1494/2330 train_time:56497ms step_avg:37.82ms
step:1495/2330 train_time:56532ms step_avg:37.81ms
step:1496/2330 train_time:56573ms step_avg:37.82ms
step:1497/2330 train_time:56608ms step_avg:37.81ms
step:1498/2330 train_time:56649ms step_avg:37.82ms
step:1499/2330 train_time:56684ms step_avg:37.81ms
step:1500/2330 train_time:56724ms step_avg:37.82ms
step:1500/2330 val_loss:5.2778 train_time:56836ms step_avg:37.89ms
step:1501/2330 train_time:56847ms step_avg:37.87ms
step:1502/2330 train_time:56858ms step_avg:37.85ms
step:1503/2330 train_time:56868ms step_avg:37.84ms
step:1504/2330 train_time:56879ms step_avg:37.82ms
step:1505/2330 train_time:56912ms step_avg:37.82ms
step:1506/2330 train_time:56953ms step_avg:37.82ms
step:1507/2330 train_time:56988ms step_avg:37.82ms
step:1508/2330 train_time:57028ms step_avg:37.82ms
step:1509/2330 train_time:57063ms step_avg:37.81ms
step:1510/2330 train_time:57103ms step_avg:37.82ms
step:1511/2330 train_time:57140ms step_avg:37.82ms
step:1512/2330 train_time:57180ms step_avg:37.82ms
step:1513/2330 train_time:57219ms step_avg:37.82ms
step:1514/2330 train_time:57260ms step_avg:37.82ms
step:1515/2330 train_time:57296ms step_avg:37.82ms
step:1516/2330 train_time:57336ms step_avg:37.82ms
step:1517/2330 train_time:57372ms step_avg:37.82ms
step:1518/2330 train_time:57413ms step_avg:37.82ms
step:1519/2330 train_time:57447ms step_avg:37.82ms
step:1520/2330 train_time:57488ms step_avg:37.82ms
step:1521/2330 train_time:57523ms step_avg:37.82ms
step:1522/2330 train_time:57563ms step_avg:37.82ms
step:1523/2330 train_time:57599ms step_avg:37.82ms
step:1524/2330 train_time:57639ms step_avg:37.82ms
step:1525/2330 train_time:57674ms step_avg:37.82ms
step:1526/2330 train_time:57714ms step_avg:37.82ms
step:1527/2330 train_time:57750ms step_avg:37.82ms
step:1528/2330 train_time:57790ms step_avg:37.82ms
step:1529/2330 train_time:57827ms step_avg:37.82ms
step:1530/2330 train_time:57868ms step_avg:37.82ms
step:1531/2330 train_time:57903ms step_avg:37.82ms
step:1532/2330 train_time:57944ms step_avg:37.82ms
step:1533/2330 train_time:57979ms step_avg:37.82ms
step:1534/2330 train_time:58019ms step_avg:37.82ms
step:1535/2330 train_time:58054ms step_avg:37.82ms
step:1536/2330 train_time:58094ms step_avg:37.82ms
step:1537/2330 train_time:58129ms step_avg:37.82ms
step:1538/2330 train_time:58171ms step_avg:37.82ms
step:1539/2330 train_time:58206ms step_avg:37.82ms
step:1540/2330 train_time:58248ms step_avg:37.82ms
step:1541/2330 train_time:58284ms step_avg:37.82ms
step:1542/2330 train_time:58325ms step_avg:37.82ms
step:1543/2330 train_time:58360ms step_avg:37.82ms
step:1544/2330 train_time:58401ms step_avg:37.82ms
step:1545/2330 train_time:58436ms step_avg:37.82ms
step:1546/2330 train_time:58476ms step_avg:37.82ms
step:1547/2330 train_time:58511ms step_avg:37.82ms
step:1548/2330 train_time:58552ms step_avg:37.82ms
step:1549/2330 train_time:58587ms step_avg:37.82ms
step:1550/2330 train_time:58627ms step_avg:37.82ms
step:1551/2330 train_time:58662ms step_avg:37.82ms
step:1552/2330 train_time:58702ms step_avg:37.82ms
step:1553/2330 train_time:58737ms step_avg:37.82ms
step:1554/2330 train_time:58778ms step_avg:37.82ms
step:1555/2330 train_time:58813ms step_avg:37.82ms
step:1556/2330 train_time:58853ms step_avg:37.82ms
step:1557/2330 train_time:58888ms step_avg:37.82ms
step:1558/2330 train_time:58929ms step_avg:37.82ms
step:1559/2330 train_time:58964ms step_avg:37.82ms
step:1560/2330 train_time:59005ms step_avg:37.82ms
step:1561/2330 train_time:59040ms step_avg:37.82ms
step:1562/2330 train_time:59081ms step_avg:37.82ms
step:1563/2330 train_time:59117ms step_avg:37.82ms
step:1564/2330 train_time:59157ms step_avg:37.82ms
step:1565/2330 train_time:59194ms step_avg:37.82ms
step:1566/2330 train_time:59234ms step_avg:37.83ms
step:1567/2330 train_time:59270ms step_avg:37.82ms
step:1568/2330 train_time:59310ms step_avg:37.83ms
step:1569/2330 train_time:59346ms step_avg:37.82ms
step:1570/2330 train_time:59386ms step_avg:37.83ms
step:1571/2330 train_time:59422ms step_avg:37.82ms
step:1572/2330 train_time:59463ms step_avg:37.83ms
step:1573/2330 train_time:59498ms step_avg:37.82ms
step:1574/2330 train_time:59539ms step_avg:37.83ms
step:1575/2330 train_time:59574ms step_avg:37.82ms
step:1576/2330 train_time:59614ms step_avg:37.83ms
step:1577/2330 train_time:59650ms step_avg:37.82ms
step:1578/2330 train_time:59690ms step_avg:37.83ms
step:1579/2330 train_time:59725ms step_avg:37.82ms
step:1580/2330 train_time:59766ms step_avg:37.83ms
step:1581/2330 train_time:59801ms step_avg:37.82ms
step:1582/2330 train_time:59842ms step_avg:37.83ms
step:1583/2330 train_time:59877ms step_avg:37.82ms
step:1584/2330 train_time:59917ms step_avg:37.83ms
step:1585/2330 train_time:59952ms step_avg:37.82ms
step:1586/2330 train_time:59993ms step_avg:37.83ms
step:1587/2330 train_time:60027ms step_avg:37.82ms
step:1588/2330 train_time:60068ms step_avg:37.83ms
step:1589/2330 train_time:60103ms step_avg:37.82ms
step:1590/2330 train_time:60144ms step_avg:37.83ms
step:1591/2330 train_time:60181ms step_avg:37.83ms
step:1592/2330 train_time:60222ms step_avg:37.83ms
step:1593/2330 train_time:60258ms step_avg:37.83ms
step:1594/2330 train_time:60298ms step_avg:37.83ms
step:1595/2330 train_time:60334ms step_avg:37.83ms
step:1596/2330 train_time:60375ms step_avg:37.83ms
step:1597/2330 train_time:60410ms step_avg:37.83ms
step:1598/2330 train_time:60451ms step_avg:37.83ms
step:1599/2330 train_time:60487ms step_avg:37.83ms
step:1600/2330 train_time:60527ms step_avg:37.83ms
step:1601/2330 train_time:60562ms step_avg:37.83ms
step:1602/2330 train_time:60603ms step_avg:37.83ms
step:1603/2330 train_time:60637ms step_avg:37.83ms
step:1604/2330 train_time:60678ms step_avg:37.83ms
step:1605/2330 train_time:60713ms step_avg:37.83ms
step:1606/2330 train_time:60754ms step_avg:37.83ms
step:1607/2330 train_time:60788ms step_avg:37.83ms
step:1608/2330 train_time:60830ms step_avg:37.83ms
step:1609/2330 train_time:60864ms step_avg:37.83ms
step:1610/2330 train_time:60905ms step_avg:37.83ms
step:1611/2330 train_time:60941ms step_avg:37.83ms
step:1612/2330 train_time:60982ms step_avg:37.83ms
step:1613/2330 train_time:61017ms step_avg:37.83ms
step:1614/2330 train_time:61058ms step_avg:37.83ms
step:1615/2330 train_time:61093ms step_avg:37.83ms
step:1616/2330 train_time:61134ms step_avg:37.83ms
step:1617/2330 train_time:61169ms step_avg:37.83ms
step:1618/2330 train_time:61210ms step_avg:37.83ms
step:1619/2330 train_time:61246ms step_avg:37.83ms
step:1620/2330 train_time:61286ms step_avg:37.83ms
step:1621/2330 train_time:61323ms step_avg:37.83ms
step:1622/2330 train_time:61363ms step_avg:37.83ms
step:1623/2330 train_time:61399ms step_avg:37.83ms
step:1624/2330 train_time:61440ms step_avg:37.83ms
step:1625/2330 train_time:61475ms step_avg:37.83ms
step:1626/2330 train_time:61515ms step_avg:37.83ms
step:1627/2330 train_time:61551ms step_avg:37.83ms
step:1628/2330 train_time:61591ms step_avg:37.83ms
step:1629/2330 train_time:61626ms step_avg:37.83ms
step:1630/2330 train_time:61667ms step_avg:37.83ms
step:1631/2330 train_time:61702ms step_avg:37.83ms
step:1632/2330 train_time:61742ms step_avg:37.83ms
step:1633/2330 train_time:61778ms step_avg:37.83ms
step:1634/2330 train_time:61818ms step_avg:37.83ms
step:1635/2330 train_time:61853ms step_avg:37.83ms
step:1636/2330 train_time:61894ms step_avg:37.83ms
step:1637/2330 train_time:61928ms step_avg:37.83ms
step:1638/2330 train_time:61969ms step_avg:37.83ms
step:1639/2330 train_time:62004ms step_avg:37.83ms
step:1640/2330 train_time:62046ms step_avg:37.83ms
step:1641/2330 train_time:62081ms step_avg:37.83ms
step:1642/2330 train_time:62122ms step_avg:37.83ms
step:1643/2330 train_time:62158ms step_avg:37.83ms
step:1644/2330 train_time:62199ms step_avg:37.83ms
step:1645/2330 train_time:62235ms step_avg:37.83ms
step:1646/2330 train_time:62275ms step_avg:37.83ms
step:1647/2330 train_time:62311ms step_avg:37.83ms
step:1648/2330 train_time:62352ms step_avg:37.83ms
step:1649/2330 train_time:62388ms step_avg:37.83ms
step:1650/2330 train_time:62429ms step_avg:37.84ms
step:1651/2330 train_time:62464ms step_avg:37.83ms
step:1652/2330 train_time:62505ms step_avg:37.84ms
step:1653/2330 train_time:62540ms step_avg:37.83ms
step:1654/2330 train_time:62581ms step_avg:37.84ms
step:1655/2330 train_time:62617ms step_avg:37.83ms
step:1656/2330 train_time:62657ms step_avg:37.84ms
step:1657/2330 train_time:62692ms step_avg:37.83ms
step:1658/2330 train_time:62732ms step_avg:37.84ms
step:1659/2330 train_time:62767ms step_avg:37.83ms
step:1660/2330 train_time:62808ms step_avg:37.84ms
step:1661/2330 train_time:62843ms step_avg:37.83ms
step:1662/2330 train_time:62883ms step_avg:37.84ms
step:1663/2330 train_time:62918ms step_avg:37.83ms
step:1664/2330 train_time:62958ms step_avg:37.84ms
step:1665/2330 train_time:62994ms step_avg:37.83ms
step:1666/2330 train_time:63034ms step_avg:37.84ms
step:1667/2330 train_time:63069ms step_avg:37.83ms
step:1668/2330 train_time:63111ms step_avg:37.84ms
step:1669/2330 train_time:63146ms step_avg:37.83ms
step:1670/2330 train_time:63187ms step_avg:37.84ms
step:1671/2330 train_time:63222ms step_avg:37.83ms
step:1672/2330 train_time:63263ms step_avg:37.84ms
step:1673/2330 train_time:63299ms step_avg:37.84ms
step:1674/2330 train_time:63340ms step_avg:37.84ms
step:1675/2330 train_time:63376ms step_avg:37.84ms
step:1676/2330 train_time:63416ms step_avg:37.84ms
step:1677/2330 train_time:63451ms step_avg:37.84ms
step:1678/2330 train_time:63492ms step_avg:37.84ms
step:1679/2330 train_time:63527ms step_avg:37.84ms
step:1680/2330 train_time:63567ms step_avg:37.84ms
step:1681/2330 train_time:63603ms step_avg:37.84ms
step:1682/2330 train_time:63644ms step_avg:37.84ms
step:1683/2330 train_time:63680ms step_avg:37.84ms
step:1684/2330 train_time:63720ms step_avg:37.84ms
step:1685/2330 train_time:63756ms step_avg:37.84ms
step:1686/2330 train_time:63796ms step_avg:37.84ms
step:1687/2330 train_time:63832ms step_avg:37.84ms
step:1688/2330 train_time:63872ms step_avg:37.84ms
step:1689/2330 train_time:63908ms step_avg:37.84ms
step:1690/2330 train_time:63948ms step_avg:37.84ms
step:1691/2330 train_time:63984ms step_avg:37.84ms
step:1692/2330 train_time:64024ms step_avg:37.84ms
step:1693/2330 train_time:64059ms step_avg:37.84ms
step:1694/2330 train_time:64100ms step_avg:37.84ms
step:1695/2330 train_time:64135ms step_avg:37.84ms
step:1696/2330 train_time:64175ms step_avg:37.84ms
step:1697/2330 train_time:64211ms step_avg:37.84ms
step:1698/2330 train_time:64252ms step_avg:37.84ms
step:1699/2330 train_time:64288ms step_avg:37.84ms
step:1700/2330 train_time:64329ms step_avg:37.84ms
step:1701/2330 train_time:64364ms step_avg:37.84ms
step:1702/2330 train_time:64405ms step_avg:37.84ms
step:1703/2330 train_time:64441ms step_avg:37.84ms
step:1704/2330 train_time:64481ms step_avg:37.84ms
step:1705/2330 train_time:64518ms step_avg:37.84ms
step:1706/2330 train_time:64558ms step_avg:37.84ms
step:1707/2330 train_time:64594ms step_avg:37.84ms
step:1708/2330 train_time:64635ms step_avg:37.84ms
step:1709/2330 train_time:64670ms step_avg:37.84ms
step:1710/2330 train_time:64711ms step_avg:37.84ms
step:1711/2330 train_time:64746ms step_avg:37.84ms
step:1712/2330 train_time:64787ms step_avg:37.84ms
step:1713/2330 train_time:64823ms step_avg:37.84ms
step:1714/2330 train_time:64863ms step_avg:37.84ms
step:1715/2330 train_time:64900ms step_avg:37.84ms
step:1716/2330 train_time:64940ms step_avg:37.84ms
step:1717/2330 train_time:64976ms step_avg:37.84ms
step:1718/2330 train_time:65016ms step_avg:37.84ms
step:1719/2330 train_time:65052ms step_avg:37.84ms
step:1720/2330 train_time:65092ms step_avg:37.84ms
step:1721/2330 train_time:65127ms step_avg:37.84ms
step:1722/2330 train_time:65167ms step_avg:37.84ms
step:1723/2330 train_time:65204ms step_avg:37.84ms
step:1724/2330 train_time:65244ms step_avg:37.84ms
step:1725/2330 train_time:65280ms step_avg:37.84ms
step:1726/2330 train_time:65320ms step_avg:37.84ms
step:1727/2330 train_time:65356ms step_avg:37.84ms
step:1728/2330 train_time:65396ms step_avg:37.84ms
step:1729/2330 train_time:65431ms step_avg:37.84ms
step:1730/2330 train_time:65472ms step_avg:37.84ms
step:1731/2330 train_time:65508ms step_avg:37.84ms
step:1732/2330 train_time:65548ms step_avg:37.85ms
step:1733/2330 train_time:65585ms step_avg:37.84ms
step:1734/2330 train_time:65626ms step_avg:37.85ms
step:1735/2330 train_time:65661ms step_avg:37.84ms
step:1736/2330 train_time:65702ms step_avg:37.85ms
step:1737/2330 train_time:65737ms step_avg:37.84ms
step:1738/2330 train_time:65777ms step_avg:37.85ms
step:1739/2330 train_time:65812ms step_avg:37.84ms
step:1740/2330 train_time:65853ms step_avg:37.85ms
step:1741/2330 train_time:65888ms step_avg:37.85ms
step:1742/2330 train_time:65929ms step_avg:37.85ms
step:1743/2330 train_time:65964ms step_avg:37.85ms
step:1744/2330 train_time:66005ms step_avg:37.85ms
step:1745/2330 train_time:66041ms step_avg:37.85ms
step:1746/2330 train_time:66081ms step_avg:37.85ms
step:1747/2330 train_time:66118ms step_avg:37.85ms
step:1748/2330 train_time:66158ms step_avg:37.85ms
step:1749/2330 train_time:66194ms step_avg:37.85ms
step:1750/2330 train_time:66234ms step_avg:37.85ms
step:1750/2330 val_loss:5.2405 train_time:66347ms step_avg:37.91ms
step:1751/2330 train_time:66358ms step_avg:37.90ms
step:1752/2330 train_time:66368ms step_avg:37.88ms
step:1753/2330 train_time:66377ms step_avg:37.86ms
step:1754/2330 train_time:66388ms step_avg:37.85ms
step:1755/2330 train_time:66425ms step_avg:37.85ms
step:1756/2330 train_time:66465ms step_avg:37.85ms
step:1757/2330 train_time:66500ms step_avg:37.85ms
step:1758/2330 train_time:66540ms step_avg:37.85ms
step:1759/2330 train_time:66575ms step_avg:37.85ms
step:1760/2330 train_time:66615ms step_avg:37.85ms
step:1761/2330 train_time:66651ms step_avg:37.85ms
step:1762/2330 train_time:66692ms step_avg:37.85ms
step:1763/2330 train_time:66732ms step_avg:37.85ms
step:1764/2330 train_time:66773ms step_avg:37.85ms
step:1765/2330 train_time:66809ms step_avg:37.85ms
step:1766/2330 train_time:66850ms step_avg:37.85ms
step:1767/2330 train_time:66885ms step_avg:37.85ms
step:1768/2330 train_time:66925ms step_avg:37.85ms
step:1769/2330 train_time:66961ms step_avg:37.85ms
step:1770/2330 train_time:67001ms step_avg:37.85ms
step:1771/2330 train_time:67037ms step_avg:37.85ms
step:1772/2330 train_time:67077ms step_avg:37.85ms
step:1773/2330 train_time:67112ms step_avg:37.85ms
step:1774/2330 train_time:67153ms step_avg:37.85ms
step:1775/2330 train_time:67187ms step_avg:37.85ms
step:1776/2330 train_time:67228ms step_avg:37.85ms
step:1777/2330 train_time:67264ms step_avg:37.85ms
step:1778/2330 train_time:67305ms step_avg:37.85ms
step:1779/2330 train_time:67340ms step_avg:37.85ms
step:1780/2330 train_time:67382ms step_avg:37.85ms
step:1781/2330 train_time:67417ms step_avg:37.85ms
step:1782/2330 train_time:67457ms step_avg:37.85ms
step:1783/2330 train_time:67492ms step_avg:37.85ms
step:1784/2330 train_time:67533ms step_avg:37.85ms
step:1785/2330 train_time:67567ms step_avg:37.85ms
step:1786/2330 train_time:67608ms step_avg:37.85ms
step:1787/2330 train_time:67645ms step_avg:37.85ms
step:1788/2330 train_time:67686ms step_avg:37.86ms
step:1789/2330 train_time:67723ms step_avg:37.86ms
step:1790/2330 train_time:67764ms step_avg:37.86ms
step:1791/2330 train_time:67800ms step_avg:37.86ms
step:1792/2330 train_time:67840ms step_avg:37.86ms
step:1793/2330 train_time:67877ms step_avg:37.86ms
step:1794/2330 train_time:67917ms step_avg:37.86ms
step:1795/2330 train_time:67952ms step_avg:37.86ms
step:1796/2330 train_time:67993ms step_avg:37.86ms
step:1797/2330 train_time:68028ms step_avg:37.86ms
step:1798/2330 train_time:68068ms step_avg:37.86ms
step:1799/2330 train_time:68103ms step_avg:37.86ms
step:1800/2330 train_time:68143ms step_avg:37.86ms
step:1801/2330 train_time:68178ms step_avg:37.86ms
step:1802/2330 train_time:68218ms step_avg:37.86ms
step:1803/2330 train_time:68254ms step_avg:37.86ms
step:1804/2330 train_time:68294ms step_avg:37.86ms
step:1805/2330 train_time:68330ms step_avg:37.86ms
step:1806/2330 train_time:68371ms step_avg:37.86ms
step:1807/2330 train_time:68405ms step_avg:37.86ms
step:1808/2330 train_time:68446ms step_avg:37.86ms
step:1809/2330 train_time:68481ms step_avg:37.86ms
step:1810/2330 train_time:68522ms step_avg:37.86ms
step:1811/2330 train_time:68556ms step_avg:37.86ms
step:1812/2330 train_time:68597ms step_avg:37.86ms
step:1813/2330 train_time:68632ms step_avg:37.86ms
step:1814/2330 train_time:68673ms step_avg:37.86ms
step:1815/2330 train_time:68708ms step_avg:37.86ms
step:1816/2330 train_time:68749ms step_avg:37.86ms
step:1817/2330 train_time:68784ms step_avg:37.86ms
step:1818/2330 train_time:68825ms step_avg:37.86ms
step:1819/2330 train_time:68861ms step_avg:37.86ms
step:1820/2330 train_time:68902ms step_avg:37.86ms
step:1821/2330 train_time:68937ms step_avg:37.86ms
step:1822/2330 train_time:68978ms step_avg:37.86ms
step:1823/2330 train_time:69013ms step_avg:37.86ms
step:1824/2330 train_time:69053ms step_avg:37.86ms
step:1825/2330 train_time:69089ms step_avg:37.86ms
step:1826/2330 train_time:69129ms step_avg:37.86ms
step:1827/2330 train_time:69164ms step_avg:37.86ms
step:1828/2330 train_time:69205ms step_avg:37.86ms
step:1829/2330 train_time:69240ms step_avg:37.86ms
step:1830/2330 train_time:69281ms step_avg:37.86ms
step:1831/2330 train_time:69316ms step_avg:37.86ms
step:1832/2330 train_time:69356ms step_avg:37.86ms
step:1833/2330 train_time:69392ms step_avg:37.86ms
step:1834/2330 train_time:69433ms step_avg:37.86ms
step:1835/2330 train_time:69467ms step_avg:37.86ms
step:1836/2330 train_time:69508ms step_avg:37.86ms
step:1837/2330 train_time:69543ms step_avg:37.86ms
step:1838/2330 train_time:69584ms step_avg:37.86ms
step:1839/2330 train_time:69620ms step_avg:37.86ms
step:1840/2330 train_time:69661ms step_avg:37.86ms
step:1841/2330 train_time:69697ms step_avg:37.86ms
step:1842/2330 train_time:69737ms step_avg:37.86ms
step:1843/2330 train_time:69774ms step_avg:37.86ms
step:1844/2330 train_time:69814ms step_avg:37.86ms
step:1845/2330 train_time:69850ms step_avg:37.86ms
step:1846/2330 train_time:69890ms step_avg:37.86ms
step:1847/2330 train_time:69926ms step_avg:37.86ms
step:1848/2330 train_time:69967ms step_avg:37.86ms
step:1849/2330 train_time:70003ms step_avg:37.86ms
step:1850/2330 train_time:70043ms step_avg:37.86ms
step:1851/2330 train_time:70080ms step_avg:37.86ms
step:1852/2330 train_time:70120ms step_avg:37.86ms
step:1853/2330 train_time:70156ms step_avg:37.86ms
step:1854/2330 train_time:70196ms step_avg:37.86ms
step:1855/2330 train_time:70232ms step_avg:37.86ms
step:1856/2330 train_time:70273ms step_avg:37.86ms
step:1857/2330 train_time:70308ms step_avg:37.86ms
step:1858/2330 train_time:70348ms step_avg:37.86ms
step:1859/2330 train_time:70384ms step_avg:37.86ms
step:1860/2330 train_time:70424ms step_avg:37.86ms
step:1861/2330 train_time:70460ms step_avg:37.86ms
step:1862/2330 train_time:70500ms step_avg:37.86ms
step:1863/2330 train_time:70536ms step_avg:37.86ms
step:1864/2330 train_time:70576ms step_avg:37.86ms
step:1865/2330 train_time:70612ms step_avg:37.86ms
step:1866/2330 train_time:70653ms step_avg:37.86ms
step:1867/2330 train_time:70687ms step_avg:37.86ms
step:1868/2330 train_time:70728ms step_avg:37.86ms
step:1869/2330 train_time:70764ms step_avg:37.86ms
step:1870/2330 train_time:70804ms step_avg:37.86ms
step:1871/2330 train_time:70840ms step_avg:37.86ms
step:1872/2330 train_time:70881ms step_avg:37.86ms
step:1873/2330 train_time:70917ms step_avg:37.86ms
step:1874/2330 train_time:70957ms step_avg:37.86ms
step:1875/2330 train_time:70993ms step_avg:37.86ms
step:1876/2330 train_time:71033ms step_avg:37.86ms
step:1877/2330 train_time:71068ms step_avg:37.86ms
step:1878/2330 train_time:71109ms step_avg:37.86ms
step:1879/2330 train_time:71145ms step_avg:37.86ms
step:1880/2330 train_time:71185ms step_avg:37.86ms
step:1881/2330 train_time:71221ms step_avg:37.86ms
step:1882/2330 train_time:71262ms step_avg:37.86ms
step:1883/2330 train_time:71297ms step_avg:37.86ms
step:1884/2330 train_time:71337ms step_avg:37.86ms
step:1885/2330 train_time:71373ms step_avg:37.86ms
step:1886/2330 train_time:71413ms step_avg:37.87ms
step:1887/2330 train_time:71449ms step_avg:37.86ms
step:1888/2330 train_time:71490ms step_avg:37.87ms
step:1889/2330 train_time:71525ms step_avg:37.86ms
step:1890/2330 train_time:71565ms step_avg:37.87ms
step:1891/2330 train_time:71602ms step_avg:37.86ms
step:1892/2330 train_time:71642ms step_avg:37.87ms
step:1893/2330 train_time:71679ms step_avg:37.87ms
step:1894/2330 train_time:71719ms step_avg:37.87ms
step:1895/2330 train_time:71755ms step_avg:37.87ms
step:1896/2330 train_time:71795ms step_avg:37.87ms
step:1897/2330 train_time:71831ms step_avg:37.87ms
step:1898/2330 train_time:71872ms step_avg:37.87ms
step:1899/2330 train_time:71907ms step_avg:37.87ms
step:1900/2330 train_time:71948ms step_avg:37.87ms
step:1901/2330 train_time:71983ms step_avg:37.87ms
step:1902/2330 train_time:72024ms step_avg:37.87ms
step:1903/2330 train_time:72059ms step_avg:37.87ms
step:1904/2330 train_time:72100ms step_avg:37.87ms
step:1905/2330 train_time:72136ms step_avg:37.87ms
step:1906/2330 train_time:72177ms step_avg:37.87ms
step:1907/2330 train_time:72212ms step_avg:37.87ms
step:1908/2330 train_time:72253ms step_avg:37.87ms
step:1909/2330 train_time:72288ms step_avg:37.87ms
step:1910/2330 train_time:72328ms step_avg:37.87ms
step:1911/2330 train_time:72364ms step_avg:37.87ms
step:1912/2330 train_time:72405ms step_avg:37.87ms
step:1913/2330 train_time:72441ms step_avg:37.87ms
step:1914/2330 train_time:72482ms step_avg:37.87ms
step:1915/2330 train_time:72518ms step_avg:37.87ms
step:1916/2330 train_time:72558ms step_avg:37.87ms
step:1917/2330 train_time:72594ms step_avg:37.87ms
step:1918/2330 train_time:72634ms step_avg:37.87ms
step:1919/2330 train_time:72670ms step_avg:37.87ms
step:1920/2330 train_time:72710ms step_avg:37.87ms
step:1921/2330 train_time:72745ms step_avg:37.87ms
step:1922/2330 train_time:72786ms step_avg:37.87ms
step:1923/2330 train_time:72822ms step_avg:37.87ms
step:1924/2330 train_time:72863ms step_avg:37.87ms
step:1925/2330 train_time:72899ms step_avg:37.87ms
step:1926/2330 train_time:72940ms step_avg:37.87ms
step:1927/2330 train_time:72975ms step_avg:37.87ms
step:1928/2330 train_time:73016ms step_avg:37.87ms
step:1929/2330 train_time:73051ms step_avg:37.87ms
step:1930/2330 train_time:73091ms step_avg:37.87ms
step:1931/2330 train_time:73127ms step_avg:37.87ms
step:1932/2330 train_time:73167ms step_avg:37.87ms
step:1933/2330 train_time:73203ms step_avg:37.87ms
step:1934/2330 train_time:73243ms step_avg:37.87ms
step:1935/2330 train_time:73279ms step_avg:37.87ms
step:1936/2330 train_time:73320ms step_avg:37.87ms
step:1937/2330 train_time:73356ms step_avg:37.87ms
step:1938/2330 train_time:73396ms step_avg:37.87ms
step:1939/2330 train_time:73432ms step_avg:37.87ms
step:1940/2330 train_time:73473ms step_avg:37.87ms
step:1941/2330 train_time:73508ms step_avg:37.87ms
step:1942/2330 train_time:73549ms step_avg:37.87ms
step:1943/2330 train_time:73585ms step_avg:37.87ms
step:1944/2330 train_time:73626ms step_avg:37.87ms
step:1945/2330 train_time:73661ms step_avg:37.87ms
step:1946/2330 train_time:73701ms step_avg:37.87ms
step:1947/2330 train_time:73737ms step_avg:37.87ms
step:1948/2330 train_time:73777ms step_avg:37.87ms
step:1949/2330 train_time:73813ms step_avg:37.87ms
step:1950/2330 train_time:73853ms step_avg:37.87ms
step:1951/2330 train_time:73889ms step_avg:37.87ms
step:1952/2330 train_time:73930ms step_avg:37.87ms
step:1953/2330 train_time:73964ms step_avg:37.87ms
step:1954/2330 train_time:74006ms step_avg:37.87ms
step:1955/2330 train_time:74040ms step_avg:37.87ms
step:1956/2330 train_time:74081ms step_avg:37.87ms
step:1957/2330 train_time:74116ms step_avg:37.87ms
step:1958/2330 train_time:74156ms step_avg:37.87ms
step:1959/2330 train_time:74192ms step_avg:37.87ms
step:1960/2330 train_time:74233ms step_avg:37.87ms
step:1961/2330 train_time:74267ms step_avg:37.87ms
step:1962/2330 train_time:74307ms step_avg:37.87ms
step:1963/2330 train_time:74343ms step_avg:37.87ms
step:1964/2330 train_time:74384ms step_avg:37.87ms
step:1965/2330 train_time:74420ms step_avg:37.87ms
step:1966/2330 train_time:74461ms step_avg:37.87ms
step:1967/2330 train_time:74496ms step_avg:37.87ms
step:1968/2330 train_time:74536ms step_avg:37.87ms
step:1969/2330 train_time:74572ms step_avg:37.87ms
step:1970/2330 train_time:74613ms step_avg:37.87ms
step:1971/2330 train_time:74648ms step_avg:37.87ms
step:1972/2330 train_time:74688ms step_avg:37.87ms
step:1973/2330 train_time:74724ms step_avg:37.87ms
step:1974/2330 train_time:74764ms step_avg:37.87ms
step:1975/2330 train_time:74800ms step_avg:37.87ms
step:1976/2330 train_time:74841ms step_avg:37.88ms
step:1977/2330 train_time:74876ms step_avg:37.87ms
step:1978/2330 train_time:74917ms step_avg:37.88ms
step:1979/2330 train_time:74952ms step_avg:37.87ms
step:1980/2330 train_time:74992ms step_avg:37.87ms
step:1981/2330 train_time:75027ms step_avg:37.87ms
step:1982/2330 train_time:75068ms step_avg:37.87ms
step:1983/2330 train_time:75103ms step_avg:37.87ms
step:1984/2330 train_time:75144ms step_avg:37.88ms
step:1985/2330 train_time:75180ms step_avg:37.87ms
step:1986/2330 train_time:75221ms step_avg:37.88ms
step:1987/2330 train_time:75256ms step_avg:37.87ms
step:1988/2330 train_time:75296ms step_avg:37.88ms
step:1989/2330 train_time:75332ms step_avg:37.87ms
step:1990/2330 train_time:75372ms step_avg:37.88ms
step:1991/2330 train_time:75408ms step_avg:37.87ms
step:1992/2330 train_time:75448ms step_avg:37.88ms
step:1993/2330 train_time:75485ms step_avg:37.87ms
step:1994/2330 train_time:75525ms step_avg:37.88ms
step:1995/2330 train_time:75561ms step_avg:37.88ms
step:1996/2330 train_time:75602ms step_avg:37.88ms
step:1997/2330 train_time:75637ms step_avg:37.88ms
step:1998/2330 train_time:75677ms step_avg:37.88ms
step:1999/2330 train_time:75712ms step_avg:37.88ms
step:2000/2330 train_time:75753ms step_avg:37.88ms
step:2000/2330 val_loss:5.2085 train_time:75864ms step_avg:37.93ms
step:2001/2330 train_time:75876ms step_avg:37.92ms
step:2002/2330 train_time:75886ms step_avg:37.91ms
step:2003/2330 train_time:75895ms step_avg:37.89ms
step:2004/2330 train_time:75907ms step_avg:37.88ms
step:2005/2330 train_time:75942ms step_avg:37.88ms
step:2006/2330 train_time:75982ms step_avg:37.88ms
step:2007/2330 train_time:76016ms step_avg:37.88ms
step:2008/2330 train_time:76056ms step_avg:37.88ms
step:2009/2330 train_time:76091ms step_avg:37.88ms
step:2010/2330 train_time:76132ms step_avg:37.88ms
step:2011/2330 train_time:76166ms step_avg:37.87ms
step:2012/2330 train_time:76210ms step_avg:37.88ms
step:2013/2330 train_time:76248ms step_avg:37.88ms
step:2014/2330 train_time:76289ms step_avg:37.88ms
step:2015/2330 train_time:76325ms step_avg:37.88ms
step:2016/2330 train_time:76366ms step_avg:37.88ms
step:2017/2330 train_time:76401ms step_avg:37.88ms
step:2018/2330 train_time:76442ms step_avg:37.88ms
step:2019/2330 train_time:76478ms step_avg:37.88ms
step:2020/2330 train_time:76518ms step_avg:37.88ms
step:2021/2330 train_time:76553ms step_avg:37.88ms
step:2022/2330 train_time:76593ms step_avg:37.88ms
step:2023/2330 train_time:76628ms step_avg:37.88ms
step:2024/2330 train_time:76669ms step_avg:37.88ms
step:2025/2330 train_time:76703ms step_avg:37.88ms
step:2026/2330 train_time:76743ms step_avg:37.88ms
step:2027/2330 train_time:76778ms step_avg:37.88ms
step:2028/2330 train_time:76819ms step_avg:37.88ms
step:2029/2330 train_time:76854ms step_avg:37.88ms
step:2030/2330 train_time:76894ms step_avg:37.88ms
step:2031/2330 train_time:76930ms step_avg:37.88ms
step:2032/2330 train_time:76971ms step_avg:37.88ms
step:2033/2330 train_time:77006ms step_avg:37.88ms
step:2034/2330 train_time:77047ms step_avg:37.88ms
step:2035/2330 train_time:77082ms step_avg:37.88ms
step:2036/2330 train_time:77123ms step_avg:37.88ms
step:2037/2330 train_time:77158ms step_avg:37.88ms
step:2038/2330 train_time:77199ms step_avg:37.88ms
step:2039/2330 train_time:77235ms step_avg:37.88ms
step:2040/2330 train_time:77276ms step_avg:37.88ms
step:2041/2330 train_time:77312ms step_avg:37.88ms
step:2042/2330 train_time:77353ms step_avg:37.88ms
step:2043/2330 train_time:77388ms step_avg:37.88ms
step:2044/2330 train_time:77429ms step_avg:37.88ms
step:2045/2330 train_time:77464ms step_avg:37.88ms
step:2046/2330 train_time:77504ms step_avg:37.88ms
step:2047/2330 train_time:77539ms step_avg:37.88ms
step:2048/2330 train_time:77580ms step_avg:37.88ms
step:2049/2330 train_time:77615ms step_avg:37.88ms
step:2050/2330 train_time:77655ms step_avg:37.88ms
step:2051/2330 train_time:77690ms step_avg:37.88ms
step:2052/2330 train_time:77731ms step_avg:37.88ms
step:2053/2330 train_time:77767ms step_avg:37.88ms
step:2054/2330 train_time:77807ms step_avg:37.88ms
step:2055/2330 train_time:77842ms step_avg:37.88ms
step:2056/2330 train_time:77883ms step_avg:37.88ms
step:2057/2330 train_time:77918ms step_avg:37.88ms
step:2058/2330 train_time:77958ms step_avg:37.88ms
step:2059/2330 train_time:77993ms step_avg:37.88ms
step:2060/2330 train_time:78033ms step_avg:37.88ms
step:2061/2330 train_time:78069ms step_avg:37.88ms
step:2062/2330 train_time:78110ms step_avg:37.88ms
step:2063/2330 train_time:78145ms step_avg:37.88ms
step:2064/2330 train_time:78186ms step_avg:37.88ms
step:2065/2330 train_time:78222ms step_avg:37.88ms
step:2066/2330 train_time:78263ms step_avg:37.88ms
step:2067/2330 train_time:78299ms step_avg:37.88ms
step:2068/2330 train_time:78339ms step_avg:37.88ms
step:2069/2330 train_time:78375ms step_avg:37.88ms
step:2070/2330 train_time:78416ms step_avg:37.88ms
step:2071/2330 train_time:78452ms step_avg:37.88ms
step:2072/2330 train_time:78492ms step_avg:37.88ms
step:2073/2330 train_time:78527ms step_avg:37.88ms
step:2074/2330 train_time:78568ms step_avg:37.88ms
step:2075/2330 train_time:78603ms step_avg:37.88ms
step:2076/2330 train_time:78644ms step_avg:37.88ms
step:2077/2330 train_time:78679ms step_avg:37.88ms
step:2078/2330 train_time:78719ms step_avg:37.88ms
step:2079/2330 train_time:78755ms step_avg:37.88ms
step:2080/2330 train_time:78795ms step_avg:37.88ms
step:2081/2330 train_time:78830ms step_avg:37.88ms
step:2082/2330 train_time:78871ms step_avg:37.88ms
step:2083/2330 train_time:78906ms step_avg:37.88ms
step:2084/2330 train_time:78946ms step_avg:37.88ms
step:2085/2330 train_time:78980ms step_avg:37.88ms
step:2086/2330 train_time:79021ms step_avg:37.88ms
step:2087/2330 train_time:79056ms step_avg:37.88ms
step:2088/2330 train_time:79096ms step_avg:37.88ms
step:2089/2330 train_time:79132ms step_avg:37.88ms
step:2090/2330 train_time:79172ms step_avg:37.88ms
step:2091/2330 train_time:79208ms step_avg:37.88ms
step:2092/2330 train_time:79248ms step_avg:37.88ms
step:2093/2330 train_time:79284ms step_avg:37.88ms
step:2094/2330 train_time:79326ms step_avg:37.88ms
step:2095/2330 train_time:79361ms step_avg:37.88ms
step:2096/2330 train_time:79402ms step_avg:37.88ms
step:2097/2330 train_time:79437ms step_avg:37.88ms
step:2098/2330 train_time:79477ms step_avg:37.88ms
step:2099/2330 train_time:79512ms step_avg:37.88ms
step:2100/2330 train_time:79553ms step_avg:37.88ms
step:2101/2330 train_time:79588ms step_avg:37.88ms
step:2102/2330 train_time:79629ms step_avg:37.88ms
step:2103/2330 train_time:79665ms step_avg:37.88ms
step:2104/2330 train_time:79706ms step_avg:37.88ms
step:2105/2330 train_time:79742ms step_avg:37.88ms
step:2106/2330 train_time:79782ms step_avg:37.88ms
step:2107/2330 train_time:79818ms step_avg:37.88ms
step:2108/2330 train_time:79858ms step_avg:37.88ms
step:2109/2330 train_time:79893ms step_avg:37.88ms
step:2110/2330 train_time:79933ms step_avg:37.88ms
step:2111/2330 train_time:79969ms step_avg:37.88ms
step:2112/2330 train_time:80009ms step_avg:37.88ms
step:2113/2330 train_time:80044ms step_avg:37.88ms
step:2114/2330 train_time:80085ms step_avg:37.88ms
step:2115/2330 train_time:80120ms step_avg:37.88ms
step:2116/2330 train_time:80161ms step_avg:37.88ms
step:2117/2330 train_time:80196ms step_avg:37.88ms
step:2118/2330 train_time:80236ms step_avg:37.88ms
step:2119/2330 train_time:80271ms step_avg:37.88ms
step:2120/2330 train_time:80312ms step_avg:37.88ms
step:2121/2330 train_time:80348ms step_avg:37.88ms
step:2122/2330 train_time:80389ms step_avg:37.88ms
step:2123/2330 train_time:80424ms step_avg:37.88ms
step:2124/2330 train_time:80464ms step_avg:37.88ms
step:2125/2330 train_time:80500ms step_avg:37.88ms
step:2126/2330 train_time:80540ms step_avg:37.88ms
step:2127/2330 train_time:80576ms step_avg:37.88ms
step:2128/2330 train_time:80617ms step_avg:37.88ms
step:2129/2330 train_time:80652ms step_avg:37.88ms
step:2130/2330 train_time:80692ms step_avg:37.88ms
step:2131/2330 train_time:80728ms step_avg:37.88ms
step:2132/2330 train_time:80768ms step_avg:37.88ms
step:2133/2330 train_time:80803ms step_avg:37.88ms
step:2134/2330 train_time:80844ms step_avg:37.88ms
step:2135/2330 train_time:80879ms step_avg:37.88ms
step:2136/2330 train_time:80919ms step_avg:37.88ms
step:2137/2330 train_time:80955ms step_avg:37.88ms
step:2138/2330 train_time:80995ms step_avg:37.88ms
step:2139/2330 train_time:81031ms step_avg:37.88ms
step:2140/2330 train_time:81071ms step_avg:37.88ms
step:2141/2330 train_time:81105ms step_avg:37.88ms
step:2142/2330 train_time:81146ms step_avg:37.88ms
step:2143/2330 train_time:81181ms step_avg:37.88ms
step:2144/2330 train_time:81222ms step_avg:37.88ms
step:2145/2330 train_time:81257ms step_avg:37.88ms
step:2146/2330 train_time:81297ms step_avg:37.88ms
step:2147/2330 train_time:81333ms step_avg:37.88ms
step:2148/2330 train_time:81373ms step_avg:37.88ms
step:2149/2330 train_time:81409ms step_avg:37.88ms
step:2150/2330 train_time:81450ms step_avg:37.88ms
step:2151/2330 train_time:81485ms step_avg:37.88ms
step:2152/2330 train_time:81526ms step_avg:37.88ms
step:2153/2330 train_time:81561ms step_avg:37.88ms
step:2154/2330 train_time:81601ms step_avg:37.88ms
step:2155/2330 train_time:81637ms step_avg:37.88ms
step:2156/2330 train_time:81677ms step_avg:37.88ms
step:2157/2330 train_time:81713ms step_avg:37.88ms
step:2158/2330 train_time:81753ms step_avg:37.88ms
step:2159/2330 train_time:81789ms step_avg:37.88ms
step:2160/2330 train_time:81830ms step_avg:37.88ms
step:2161/2330 train_time:81866ms step_avg:37.88ms
step:2162/2330 train_time:81906ms step_avg:37.88ms
step:2163/2330 train_time:81942ms step_avg:37.88ms
step:2164/2330 train_time:81983ms step_avg:37.88ms
step:2165/2330 train_time:82017ms step_avg:37.88ms
step:2166/2330 train_time:82058ms step_avg:37.88ms
step:2167/2330 train_time:82092ms step_avg:37.88ms
step:2168/2330 train_time:82133ms step_avg:37.88ms
step:2169/2330 train_time:82168ms step_avg:37.88ms
step:2170/2330 train_time:82209ms step_avg:37.88ms
step:2171/2330 train_time:82244ms step_avg:37.88ms
step:2172/2330 train_time:82285ms step_avg:37.88ms
step:2173/2330 train_time:82320ms step_avg:37.88ms
step:2174/2330 train_time:82360ms step_avg:37.88ms
step:2175/2330 train_time:82396ms step_avg:37.88ms
step:2176/2330 train_time:82436ms step_avg:37.88ms
step:2177/2330 train_time:82472ms step_avg:37.88ms
step:2178/2330 train_time:82513ms step_avg:37.88ms
step:2179/2330 train_time:82547ms step_avg:37.88ms
step:2180/2330 train_time:82588ms step_avg:37.88ms
step:2181/2330 train_time:82624ms step_avg:37.88ms
step:2182/2330 train_time:82665ms step_avg:37.88ms
step:2183/2330 train_time:82701ms step_avg:37.88ms
step:2184/2330 train_time:82741ms step_avg:37.89ms
step:2185/2330 train_time:82777ms step_avg:37.88ms
step:2186/2330 train_time:82817ms step_avg:37.89ms
step:2187/2330 train_time:82853ms step_avg:37.88ms
step:2188/2330 train_time:82893ms step_avg:37.89ms
step:2189/2330 train_time:82928ms step_avg:37.88ms
step:2190/2330 train_time:82969ms step_avg:37.89ms
step:2191/2330 train_time:83006ms step_avg:37.88ms
step:2192/2330 train_time:83046ms step_avg:37.89ms
step:2193/2330 train_time:83082ms step_avg:37.88ms
step:2194/2330 train_time:83122ms step_avg:37.89ms
step:2195/2330 train_time:83157ms step_avg:37.88ms
step:2196/2330 train_time:83197ms step_avg:37.89ms
step:2197/2330 train_time:83232ms step_avg:37.88ms
step:2198/2330 train_time:83272ms step_avg:37.89ms
step:2199/2330 train_time:83308ms step_avg:37.88ms
step:2200/2330 train_time:83349ms step_avg:37.89ms
step:2201/2330 train_time:83384ms step_avg:37.88ms
step:2202/2330 train_time:83425ms step_avg:37.89ms
step:2203/2330 train_time:83460ms step_avg:37.88ms
step:2204/2330 train_time:83502ms step_avg:37.89ms
step:2205/2330 train_time:83536ms step_avg:37.88ms
step:2206/2330 train_time:83576ms step_avg:37.89ms
step:2207/2330 train_time:83611ms step_avg:37.88ms
step:2208/2330 train_time:83652ms step_avg:37.89ms
step:2209/2330 train_time:83688ms step_avg:37.88ms
step:2210/2330 train_time:83729ms step_avg:37.89ms
step:2211/2330 train_time:83764ms step_avg:37.89ms
step:2212/2330 train_time:83805ms step_avg:37.89ms
step:2213/2330 train_time:83841ms step_avg:37.89ms
step:2214/2330 train_time:83881ms step_avg:37.89ms
step:2215/2330 train_time:83916ms step_avg:37.89ms
step:2216/2330 train_time:83957ms step_avg:37.89ms
step:2217/2330 train_time:83992ms step_avg:37.89ms
step:2218/2330 train_time:84033ms step_avg:37.89ms
step:2219/2330 train_time:84068ms step_avg:37.89ms
step:2220/2330 train_time:84109ms step_avg:37.89ms
step:2221/2330 train_time:84143ms step_avg:37.89ms
step:2222/2330 train_time:84184ms step_avg:37.89ms
step:2223/2330 train_time:84219ms step_avg:37.89ms
step:2224/2330 train_time:84260ms step_avg:37.89ms
step:2225/2330 train_time:84294ms step_avg:37.89ms
step:2226/2330 train_time:84335ms step_avg:37.89ms
step:2227/2330 train_time:84370ms step_avg:37.88ms
step:2228/2330 train_time:84411ms step_avg:37.89ms
step:2229/2330 train_time:84445ms step_avg:37.88ms
step:2230/2330 train_time:84486ms step_avg:37.89ms
step:2231/2330 train_time:84521ms step_avg:37.88ms
step:2232/2330 train_time:84562ms step_avg:37.89ms
step:2233/2330 train_time:84597ms step_avg:37.88ms
step:2234/2330 train_time:84638ms step_avg:37.89ms
step:2235/2330 train_time:84673ms step_avg:37.88ms
step:2236/2330 train_time:84713ms step_avg:37.89ms
step:2237/2330 train_time:84749ms step_avg:37.89ms
step:2238/2330 train_time:84789ms step_avg:37.89ms
step:2239/2330 train_time:84826ms step_avg:37.89ms
step:2240/2330 train_time:84866ms step_avg:37.89ms
step:2241/2330 train_time:84903ms step_avg:37.89ms
step:2242/2330 train_time:84943ms step_avg:37.89ms
step:2243/2330 train_time:84978ms step_avg:37.89ms
step:2244/2330 train_time:85019ms step_avg:37.89ms
step:2245/2330 train_time:85054ms step_avg:37.89ms
step:2246/2330 train_time:85094ms step_avg:37.89ms
step:2247/2330 train_time:85129ms step_avg:37.89ms
step:2248/2330 train_time:85170ms step_avg:37.89ms
step:2249/2330 train_time:85205ms step_avg:37.89ms
step:2250/2330 train_time:85245ms step_avg:37.89ms
step:2250/2330 val_loss:5.1808 train_time:85357ms step_avg:37.94ms
step:2251/2330 train_time:85369ms step_avg:37.92ms
step:2252/2330 train_time:85379ms step_avg:37.91ms
step:2253/2330 train_time:85388ms step_avg:37.90ms
step:2254/2330 train_time:85399ms step_avg:37.89ms
step:2255/2330 train_time:85434ms step_avg:37.89ms
step:2256/2330 train_time:85475ms step_avg:37.89ms
step:2257/2330 train_time:85509ms step_avg:37.89ms
step:2258/2330 train_time:85549ms step_avg:37.89ms
step:2259/2330 train_time:85585ms step_avg:37.89ms
step:2260/2330 train_time:85625ms step_avg:37.89ms
step:2261/2330 train_time:85660ms step_avg:37.89ms
step:2262/2330 train_time:85702ms step_avg:37.89ms
step:2263/2330 train_time:85741ms step_avg:37.89ms
step:2264/2330 train_time:85781ms step_avg:37.89ms
step:2265/2330 train_time:85818ms step_avg:37.89ms
step:2266/2330 train_time:85859ms step_avg:37.89ms
step:2267/2330 train_time:85894ms step_avg:37.89ms
step:2268/2330 train_time:85935ms step_avg:37.89ms
step:2269/2330 train_time:85970ms step_avg:37.89ms
step:2270/2330 train_time:86010ms step_avg:37.89ms
step:2271/2330 train_time:86045ms step_avg:37.89ms
step:2272/2330 train_time:86085ms step_avg:37.89ms
step:2273/2330 train_time:86120ms step_avg:37.89ms
step:2274/2330 train_time:86160ms step_avg:37.89ms
step:2275/2330 train_time:86194ms step_avg:37.89ms
step:2276/2330 train_time:86235ms step_avg:37.89ms
step:2277/2330 train_time:86270ms step_avg:37.89ms
step:2278/2330 train_time:86311ms step_avg:37.89ms
step:2279/2330 train_time:86346ms step_avg:37.89ms
step:2280/2330 train_time:86387ms step_avg:37.89ms
step:2281/2330 train_time:86422ms step_avg:37.89ms
step:2282/2330 train_time:86463ms step_avg:37.89ms
step:2283/2330 train_time:86498ms step_avg:37.89ms
step:2284/2330 train_time:86538ms step_avg:37.89ms
step:2285/2330 train_time:86573ms step_avg:37.89ms
step:2286/2330 train_time:86614ms step_avg:37.89ms
step:2287/2330 train_time:86650ms step_avg:37.89ms
step:2288/2330 train_time:86691ms step_avg:37.89ms
step:2289/2330 train_time:86728ms step_avg:37.89ms
step:2290/2330 train_time:86769ms step_avg:37.89ms
step:2291/2330 train_time:86805ms step_avg:37.89ms
step:2292/2330 train_time:86845ms step_avg:37.89ms
step:2293/2330 train_time:86882ms step_avg:37.89ms
step:2294/2330 train_time:86922ms step_avg:37.89ms
step:2295/2330 train_time:86958ms step_avg:37.89ms
step:2296/2330 train_time:86998ms step_avg:37.89ms
step:2297/2330 train_time:87033ms step_avg:37.89ms
step:2298/2330 train_time:87074ms step_avg:37.89ms
step:2299/2330 train_time:87108ms step_avg:37.89ms
step:2300/2330 train_time:87149ms step_avg:37.89ms
step:2301/2330 train_time:87183ms step_avg:37.89ms
step:2302/2330 train_time:87224ms step_avg:37.89ms
step:2303/2330 train_time:87259ms step_avg:37.89ms
step:2304/2330 train_time:87299ms step_avg:37.89ms
step:2305/2330 train_time:87334ms step_avg:37.89ms
step:2306/2330 train_time:87375ms step_avg:37.89ms
step:2307/2330 train_time:87410ms step_avg:37.89ms
step:2308/2330 train_time:87450ms step_avg:37.89ms
step:2309/2330 train_time:87486ms step_avg:37.89ms
step:2310/2330 train_time:87527ms step_avg:37.89ms
step:2311/2330 train_time:87562ms step_avg:37.89ms
step:2312/2330 train_time:87603ms step_avg:37.89ms
step:2313/2330 train_time:87639ms step_avg:37.89ms
step:2314/2330 train_time:87680ms step_avg:37.89ms
step:2315/2330 train_time:87716ms step_avg:37.89ms
step:2316/2330 train_time:87756ms step_avg:37.89ms
step:2317/2330 train_time:87792ms step_avg:37.89ms
step:2318/2330 train_time:87832ms step_avg:37.89ms
step:2319/2330 train_time:87869ms step_avg:37.89ms
step:2320/2330 train_time:87909ms step_avg:37.89ms
step:2321/2330 train_time:87944ms step_avg:37.89ms
step:2322/2330 train_time:87985ms step_avg:37.89ms
step:2323/2330 train_time:88021ms step_avg:37.89ms
step:2324/2330 train_time:88061ms step_avg:37.89ms
step:2325/2330 train_time:88097ms step_avg:37.89ms
step:2326/2330 train_time:88138ms step_avg:37.89ms
step:2327/2330 train_time:88173ms step_avg:37.89ms
step:2328/2330 train_time:88213ms step_avg:37.89ms
step:2329/2330 train_time:88248ms step_avg:37.89ms
step:2330/2330 train_time:88289ms step_avg:37.89ms
step:2330/2330 val_loss:5.1735 train_time:88401ms step_avg:37.94ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
