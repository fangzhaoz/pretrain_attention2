import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-3, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 23:09:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   26C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             107W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:107ms step_avg:107.35ms
step:2/2330 train_time:193ms step_avg:96.56ms
step:3/2330 train_time:214ms step_avg:71.48ms
step:4/2330 train_time:250ms step_avg:62.54ms
step:5/2330 train_time:308ms step_avg:61.50ms
step:6/2330 train_time:368ms step_avg:61.34ms
step:7/2330 train_time:427ms step_avg:60.96ms
step:8/2330 train_time:487ms step_avg:60.93ms
step:9/2330 train_time:546ms step_avg:60.62ms
step:10/2330 train_time:606ms step_avg:60.64ms
step:11/2330 train_time:665ms step_avg:60.43ms
step:12/2330 train_time:726ms step_avg:60.47ms
step:13/2330 train_time:784ms step_avg:60.30ms
step:14/2330 train_time:845ms step_avg:60.34ms
step:15/2330 train_time:904ms step_avg:60.24ms
step:16/2330 train_time:965ms step_avg:60.30ms
step:17/2330 train_time:1026ms step_avg:60.38ms
step:18/2330 train_time:1094ms step_avg:60.79ms
step:19/2330 train_time:1157ms step_avg:60.92ms
step:20/2330 train_time:1220ms step_avg:60.99ms
step:21/2330 train_time:1280ms step_avg:60.93ms
step:22/2330 train_time:1341ms step_avg:60.95ms
step:23/2330 train_time:1400ms step_avg:60.87ms
step:24/2330 train_time:1461ms step_avg:60.88ms
step:25/2330 train_time:1520ms step_avg:60.79ms
step:26/2330 train_time:1581ms step_avg:60.80ms
step:27/2330 train_time:1640ms step_avg:60.74ms
step:28/2330 train_time:1701ms step_avg:60.76ms
step:29/2330 train_time:1760ms step_avg:60.69ms
step:30/2330 train_time:1821ms step_avg:60.70ms
step:31/2330 train_time:1880ms step_avg:60.64ms
step:32/2330 train_time:1941ms step_avg:60.66ms
step:33/2330 train_time:2001ms step_avg:60.64ms
step:34/2330 train_time:2063ms step_avg:60.69ms
step:35/2330 train_time:2124ms step_avg:60.70ms
step:36/2330 train_time:2187ms step_avg:60.75ms
step:37/2330 train_time:2247ms step_avg:60.72ms
step:38/2330 train_time:2308ms step_avg:60.74ms
step:39/2330 train_time:2368ms step_avg:60.71ms
step:40/2330 train_time:2430ms step_avg:60.75ms
step:41/2330 train_time:2489ms step_avg:60.71ms
step:42/2330 train_time:2551ms step_avg:60.74ms
step:43/2330 train_time:2610ms step_avg:60.71ms
step:44/2330 train_time:2672ms step_avg:60.73ms
step:45/2330 train_time:2731ms step_avg:60.69ms
step:46/2330 train_time:2793ms step_avg:60.72ms
step:47/2330 train_time:2852ms step_avg:60.69ms
step:48/2330 train_time:2915ms step_avg:60.72ms
step:49/2330 train_time:2975ms step_avg:60.71ms
step:50/2330 train_time:3037ms step_avg:60.73ms
step:51/2330 train_time:3097ms step_avg:60.72ms
step:52/2330 train_time:3159ms step_avg:60.75ms
step:53/2330 train_time:3220ms step_avg:60.75ms
step:54/2330 train_time:3281ms step_avg:60.75ms
step:55/2330 train_time:3340ms step_avg:60.73ms
step:56/2330 train_time:3401ms step_avg:60.73ms
step:57/2330 train_time:3462ms step_avg:60.73ms
step:58/2330 train_time:3523ms step_avg:60.74ms
step:59/2330 train_time:3583ms step_avg:60.73ms
step:60/2330 train_time:3645ms step_avg:60.74ms
step:61/2330 train_time:3704ms step_avg:60.72ms
step:62/2330 train_time:3765ms step_avg:60.73ms
step:63/2330 train_time:3824ms step_avg:60.71ms
step:64/2330 train_time:3887ms step_avg:60.73ms
step:65/2330 train_time:3946ms step_avg:60.71ms
step:66/2330 train_time:4008ms step_avg:60.73ms
step:67/2330 train_time:4068ms step_avg:60.72ms
step:68/2330 train_time:4131ms step_avg:60.75ms
step:69/2330 train_time:4191ms step_avg:60.74ms
step:70/2330 train_time:4253ms step_avg:60.75ms
step:71/2330 train_time:4313ms step_avg:60.74ms
step:72/2330 train_time:4374ms step_avg:60.76ms
step:73/2330 train_time:4434ms step_avg:60.74ms
step:74/2330 train_time:4496ms step_avg:60.75ms
step:75/2330 train_time:4555ms step_avg:60.73ms
step:76/2330 train_time:4618ms step_avg:60.76ms
step:77/2330 train_time:4679ms step_avg:60.76ms
step:78/2330 train_time:4740ms step_avg:60.77ms
step:79/2330 train_time:4800ms step_avg:60.76ms
step:80/2330 train_time:4862ms step_avg:60.78ms
step:81/2330 train_time:4921ms step_avg:60.75ms
step:82/2330 train_time:4982ms step_avg:60.76ms
step:83/2330 train_time:5041ms step_avg:60.73ms
step:84/2330 train_time:5103ms step_avg:60.75ms
step:85/2330 train_time:5163ms step_avg:60.74ms
step:86/2330 train_time:5224ms step_avg:60.75ms
step:87/2330 train_time:5285ms step_avg:60.74ms
step:88/2330 train_time:5346ms step_avg:60.75ms
step:89/2330 train_time:5407ms step_avg:60.75ms
step:90/2330 train_time:5469ms step_avg:60.76ms
step:91/2330 train_time:5528ms step_avg:60.75ms
step:92/2330 train_time:5590ms step_avg:60.76ms
step:93/2330 train_time:5650ms step_avg:60.75ms
step:94/2330 train_time:5712ms step_avg:60.77ms
step:95/2330 train_time:5772ms step_avg:60.76ms
step:96/2330 train_time:5834ms step_avg:60.77ms
step:97/2330 train_time:5894ms step_avg:60.77ms
step:98/2330 train_time:5957ms step_avg:60.79ms
step:99/2330 train_time:6017ms step_avg:60.78ms
step:100/2330 train_time:6079ms step_avg:60.79ms
step:101/2330 train_time:6139ms step_avg:60.78ms
step:102/2330 train_time:6200ms step_avg:60.78ms
step:103/2330 train_time:6261ms step_avg:60.79ms
step:104/2330 train_time:6322ms step_avg:60.79ms
step:105/2330 train_time:6382ms step_avg:60.78ms
step:106/2330 train_time:6444ms step_avg:60.79ms
step:107/2330 train_time:6504ms step_avg:60.78ms
step:108/2330 train_time:6565ms step_avg:60.79ms
step:109/2330 train_time:6624ms step_avg:60.78ms
step:110/2330 train_time:6686ms step_avg:60.78ms
step:111/2330 train_time:6746ms step_avg:60.77ms
step:112/2330 train_time:6808ms step_avg:60.79ms
step:113/2330 train_time:6868ms step_avg:60.78ms
step:114/2330 train_time:6930ms step_avg:60.79ms
step:115/2330 train_time:6990ms step_avg:60.78ms
step:116/2330 train_time:7053ms step_avg:60.80ms
step:117/2330 train_time:7113ms step_avg:60.79ms
step:118/2330 train_time:7174ms step_avg:60.80ms
step:119/2330 train_time:7234ms step_avg:60.79ms
step:120/2330 train_time:7296ms step_avg:60.80ms
step:121/2330 train_time:7355ms step_avg:60.79ms
step:122/2330 train_time:7417ms step_avg:60.80ms
step:123/2330 train_time:7477ms step_avg:60.79ms
step:124/2330 train_time:7539ms step_avg:60.79ms
step:125/2330 train_time:7598ms step_avg:60.79ms
step:126/2330 train_time:7660ms step_avg:60.79ms
step:127/2330 train_time:7719ms step_avg:60.78ms
step:128/2330 train_time:7781ms step_avg:60.79ms
step:129/2330 train_time:7840ms step_avg:60.78ms
step:130/2330 train_time:7902ms step_avg:60.78ms
step:131/2330 train_time:7961ms step_avg:60.77ms
step:132/2330 train_time:8023ms step_avg:60.78ms
step:133/2330 train_time:8083ms step_avg:60.77ms
step:134/2330 train_time:8144ms step_avg:60.78ms
step:135/2330 train_time:8204ms step_avg:60.77ms
step:136/2330 train_time:8266ms step_avg:60.78ms
step:137/2330 train_time:8326ms step_avg:60.77ms
step:138/2330 train_time:8388ms step_avg:60.78ms
step:139/2330 train_time:8447ms step_avg:60.77ms
step:140/2330 train_time:8509ms step_avg:60.78ms
step:141/2330 train_time:8568ms step_avg:60.77ms
step:142/2330 train_time:8630ms step_avg:60.78ms
step:143/2330 train_time:8691ms step_avg:60.78ms
step:144/2330 train_time:8753ms step_avg:60.78ms
step:145/2330 train_time:8813ms step_avg:60.78ms
step:146/2330 train_time:8875ms step_avg:60.79ms
step:147/2330 train_time:8934ms step_avg:60.78ms
step:148/2330 train_time:8996ms step_avg:60.79ms
step:149/2330 train_time:9057ms step_avg:60.78ms
step:150/2330 train_time:9118ms step_avg:60.79ms
step:151/2330 train_time:9178ms step_avg:60.78ms
step:152/2330 train_time:9240ms step_avg:60.79ms
step:153/2330 train_time:9299ms step_avg:60.78ms
step:154/2330 train_time:9361ms step_avg:60.78ms
step:155/2330 train_time:9421ms step_avg:60.78ms
step:156/2330 train_time:9482ms step_avg:60.78ms
step:157/2330 train_time:9542ms step_avg:60.78ms
step:158/2330 train_time:9603ms step_avg:60.78ms
step:159/2330 train_time:9663ms step_avg:60.77ms
step:160/2330 train_time:9724ms step_avg:60.78ms
step:161/2330 train_time:9784ms step_avg:60.77ms
step:162/2330 train_time:9845ms step_avg:60.77ms
step:163/2330 train_time:9905ms step_avg:60.77ms
step:164/2330 train_time:9967ms step_avg:60.78ms
step:165/2330 train_time:10027ms step_avg:60.77ms
step:166/2330 train_time:10089ms step_avg:60.78ms
step:167/2330 train_time:10150ms step_avg:60.78ms
step:168/2330 train_time:10212ms step_avg:60.78ms
step:169/2330 train_time:10272ms step_avg:60.78ms
step:170/2330 train_time:10333ms step_avg:60.78ms
step:171/2330 train_time:10393ms step_avg:60.78ms
step:172/2330 train_time:10455ms step_avg:60.79ms
step:173/2330 train_time:10515ms step_avg:60.78ms
step:174/2330 train_time:10577ms step_avg:60.79ms
step:175/2330 train_time:10637ms step_avg:60.78ms
step:176/2330 train_time:10699ms step_avg:60.79ms
step:177/2330 train_time:10759ms step_avg:60.78ms
step:178/2330 train_time:10820ms step_avg:60.79ms
step:179/2330 train_time:10879ms step_avg:60.78ms
step:180/2330 train_time:10941ms step_avg:60.78ms
step:181/2330 train_time:11000ms step_avg:60.77ms
step:182/2330 train_time:11061ms step_avg:60.78ms
step:183/2330 train_time:11121ms step_avg:60.77ms
step:184/2330 train_time:11183ms step_avg:60.78ms
step:185/2330 train_time:11243ms step_avg:60.77ms
step:186/2330 train_time:11305ms step_avg:60.78ms
step:187/2330 train_time:11365ms step_avg:60.77ms
step:188/2330 train_time:11427ms step_avg:60.78ms
step:189/2330 train_time:11487ms step_avg:60.78ms
step:190/2330 train_time:11548ms step_avg:60.78ms
step:191/2330 train_time:11608ms step_avg:60.77ms
step:192/2330 train_time:11671ms step_avg:60.78ms
step:193/2330 train_time:11731ms step_avg:60.78ms
step:194/2330 train_time:11792ms step_avg:60.78ms
step:195/2330 train_time:11852ms step_avg:60.78ms
step:196/2330 train_time:11913ms step_avg:60.78ms
step:197/2330 train_time:11973ms step_avg:60.78ms
step:198/2330 train_time:12035ms step_avg:60.78ms
step:199/2330 train_time:12095ms step_avg:60.78ms
step:200/2330 train_time:12157ms step_avg:60.78ms
step:201/2330 train_time:12217ms step_avg:60.78ms
step:202/2330 train_time:12279ms step_avg:60.79ms
step:203/2330 train_time:12339ms step_avg:60.78ms
step:204/2330 train_time:12400ms step_avg:60.78ms
step:205/2330 train_time:12459ms step_avg:60.78ms
step:206/2330 train_time:12521ms step_avg:60.78ms
step:207/2330 train_time:12581ms step_avg:60.78ms
step:208/2330 train_time:12643ms step_avg:60.78ms
step:209/2330 train_time:12703ms step_avg:60.78ms
step:210/2330 train_time:12765ms step_avg:60.78ms
step:211/2330 train_time:12825ms step_avg:60.78ms
step:212/2330 train_time:12886ms step_avg:60.79ms
step:213/2330 train_time:12946ms step_avg:60.78ms
step:214/2330 train_time:13009ms step_avg:60.79ms
step:215/2330 train_time:13069ms step_avg:60.79ms
step:216/2330 train_time:13131ms step_avg:60.79ms
step:217/2330 train_time:13191ms step_avg:60.79ms
step:218/2330 train_time:13253ms step_avg:60.79ms
step:219/2330 train_time:13313ms step_avg:60.79ms
step:220/2330 train_time:13376ms step_avg:60.80ms
step:221/2330 train_time:13436ms step_avg:60.80ms
step:222/2330 train_time:13497ms step_avg:60.80ms
step:223/2330 train_time:13557ms step_avg:60.79ms
step:224/2330 train_time:13619ms step_avg:60.80ms
step:225/2330 train_time:13679ms step_avg:60.80ms
step:226/2330 train_time:13740ms step_avg:60.80ms
step:227/2330 train_time:13800ms step_avg:60.79ms
step:228/2330 train_time:13862ms step_avg:60.80ms
step:229/2330 train_time:13921ms step_avg:60.79ms
step:230/2330 train_time:13983ms step_avg:60.80ms
step:231/2330 train_time:14042ms step_avg:60.79ms
step:232/2330 train_time:14104ms step_avg:60.79ms
step:233/2330 train_time:14164ms step_avg:60.79ms
step:234/2330 train_time:14226ms step_avg:60.79ms
step:235/2330 train_time:14286ms step_avg:60.79ms
step:236/2330 train_time:14348ms step_avg:60.80ms
step:237/2330 train_time:14408ms step_avg:60.80ms
step:238/2330 train_time:14471ms step_avg:60.80ms
step:239/2330 train_time:14532ms step_avg:60.80ms
step:240/2330 train_time:14593ms step_avg:60.81ms
step:241/2330 train_time:14654ms step_avg:60.80ms
step:242/2330 train_time:14715ms step_avg:60.81ms
step:243/2330 train_time:14775ms step_avg:60.80ms
step:244/2330 train_time:14838ms step_avg:60.81ms
step:245/2330 train_time:14898ms step_avg:60.81ms
step:246/2330 train_time:14960ms step_avg:60.81ms
step:247/2330 train_time:15019ms step_avg:60.81ms
step:248/2330 train_time:15081ms step_avg:60.81ms
step:249/2330 train_time:15141ms step_avg:60.81ms
step:250/2330 train_time:15202ms step_avg:60.81ms
step:250/2330 val_loss:4.2186 train_time:15266ms step_avg:61.07ms
step:251/2330 train_time:15289ms step_avg:60.91ms
step:252/2330 train_time:15326ms step_avg:60.82ms
step:253/2330 train_time:15392ms step_avg:60.84ms
step:254/2330 train_time:15458ms step_avg:60.86ms
step:255/2330 train_time:15519ms step_avg:60.86ms
step:256/2330 train_time:15581ms step_avg:60.86ms
step:257/2330 train_time:15640ms step_avg:60.86ms
step:258/2330 train_time:15701ms step_avg:60.86ms
step:259/2330 train_time:15760ms step_avg:60.85ms
step:260/2330 train_time:15821ms step_avg:60.85ms
step:261/2330 train_time:15880ms step_avg:60.84ms
step:262/2330 train_time:15941ms step_avg:60.84ms
step:263/2330 train_time:16000ms step_avg:60.84ms
step:264/2330 train_time:16061ms step_avg:60.84ms
step:265/2330 train_time:16120ms step_avg:60.83ms
step:266/2330 train_time:16181ms step_avg:60.83ms
step:267/2330 train_time:16240ms step_avg:60.83ms
step:268/2330 train_time:16304ms step_avg:60.84ms
step:269/2330 train_time:16366ms step_avg:60.84ms
step:270/2330 train_time:16428ms step_avg:60.85ms
step:271/2330 train_time:16489ms step_avg:60.84ms
step:272/2330 train_time:16552ms step_avg:60.85ms
step:273/2330 train_time:16611ms step_avg:60.85ms
step:274/2330 train_time:16674ms step_avg:60.85ms
step:275/2330 train_time:16733ms step_avg:60.85ms
step:276/2330 train_time:16795ms step_avg:60.85ms
step:277/2330 train_time:16854ms step_avg:60.85ms
step:278/2330 train_time:16916ms step_avg:60.85ms
step:279/2330 train_time:16975ms step_avg:60.84ms
step:280/2330 train_time:17037ms step_avg:60.84ms
step:281/2330 train_time:17096ms step_avg:60.84ms
step:282/2330 train_time:17158ms step_avg:60.84ms
step:283/2330 train_time:17218ms step_avg:60.84ms
step:284/2330 train_time:17280ms step_avg:60.85ms
step:285/2330 train_time:17341ms step_avg:60.85ms
step:286/2330 train_time:17403ms step_avg:60.85ms
step:287/2330 train_time:17463ms step_avg:60.85ms
step:288/2330 train_time:17525ms step_avg:60.85ms
step:289/2330 train_time:17586ms step_avg:60.85ms
step:290/2330 train_time:17648ms step_avg:60.85ms
step:291/2330 train_time:17708ms step_avg:60.85ms
step:292/2330 train_time:17769ms step_avg:60.85ms
step:293/2330 train_time:17829ms step_avg:60.85ms
step:294/2330 train_time:17890ms step_avg:60.85ms
step:295/2330 train_time:17950ms step_avg:60.85ms
step:296/2330 train_time:18012ms step_avg:60.85ms
step:297/2330 train_time:18073ms step_avg:60.85ms
step:298/2330 train_time:18135ms step_avg:60.86ms
step:299/2330 train_time:18195ms step_avg:60.85ms
step:300/2330 train_time:18257ms step_avg:60.86ms
step:301/2330 train_time:18317ms step_avg:60.85ms
step:302/2330 train_time:18379ms step_avg:60.86ms
step:303/2330 train_time:18439ms step_avg:60.86ms
step:304/2330 train_time:18502ms step_avg:60.86ms
step:305/2330 train_time:18562ms step_avg:60.86ms
step:306/2330 train_time:18624ms step_avg:60.86ms
step:307/2330 train_time:18683ms step_avg:60.86ms
step:308/2330 train_time:18745ms step_avg:60.86ms
step:309/2330 train_time:18805ms step_avg:60.86ms
step:310/2330 train_time:18867ms step_avg:60.86ms
step:311/2330 train_time:18927ms step_avg:60.86ms
step:312/2330 train_time:18989ms step_avg:60.86ms
step:313/2330 train_time:19048ms step_avg:60.86ms
step:314/2330 train_time:19110ms step_avg:60.86ms
step:315/2330 train_time:19170ms step_avg:60.86ms
step:316/2330 train_time:19232ms step_avg:60.86ms
step:317/2330 train_time:19293ms step_avg:60.86ms
step:318/2330 train_time:19355ms step_avg:60.87ms
step:319/2330 train_time:19415ms step_avg:60.86ms
step:320/2330 train_time:19476ms step_avg:60.86ms
step:321/2330 train_time:19537ms step_avg:60.86ms
step:322/2330 train_time:19599ms step_avg:60.87ms
step:323/2330 train_time:19660ms step_avg:60.87ms
step:324/2330 train_time:19722ms step_avg:60.87ms
step:325/2330 train_time:19781ms step_avg:60.87ms
step:326/2330 train_time:19842ms step_avg:60.87ms
step:327/2330 train_time:19902ms step_avg:60.86ms
step:328/2330 train_time:19965ms step_avg:60.87ms
step:329/2330 train_time:20025ms step_avg:60.87ms
step:330/2330 train_time:20086ms step_avg:60.87ms
step:331/2330 train_time:20145ms step_avg:60.86ms
step:332/2330 train_time:20208ms step_avg:60.87ms
step:333/2330 train_time:20268ms step_avg:60.86ms
step:334/2330 train_time:20330ms step_avg:60.87ms
step:335/2330 train_time:20391ms step_avg:60.87ms
step:336/2330 train_time:20453ms step_avg:60.87ms
step:337/2330 train_time:20514ms step_avg:60.87ms
step:338/2330 train_time:20576ms step_avg:60.88ms
step:339/2330 train_time:20637ms step_avg:60.88ms
step:340/2330 train_time:20699ms step_avg:60.88ms
step:341/2330 train_time:20759ms step_avg:60.88ms
step:342/2330 train_time:20821ms step_avg:60.88ms
step:343/2330 train_time:20881ms step_avg:60.88ms
step:344/2330 train_time:20943ms step_avg:60.88ms
step:345/2330 train_time:21003ms step_avg:60.88ms
step:346/2330 train_time:21065ms step_avg:60.88ms
step:347/2330 train_time:21124ms step_avg:60.88ms
step:348/2330 train_time:21186ms step_avg:60.88ms
step:349/2330 train_time:21246ms step_avg:60.88ms
step:350/2330 train_time:21307ms step_avg:60.88ms
step:351/2330 train_time:21368ms step_avg:60.88ms
step:352/2330 train_time:21430ms step_avg:60.88ms
step:353/2330 train_time:21490ms step_avg:60.88ms
step:354/2330 train_time:21553ms step_avg:60.89ms
step:355/2330 train_time:21614ms step_avg:60.88ms
step:356/2330 train_time:21676ms step_avg:60.89ms
step:357/2330 train_time:21737ms step_avg:60.89ms
step:358/2330 train_time:21799ms step_avg:60.89ms
step:359/2330 train_time:21860ms step_avg:60.89ms
step:360/2330 train_time:21921ms step_avg:60.89ms
step:361/2330 train_time:21981ms step_avg:60.89ms
step:362/2330 train_time:22043ms step_avg:60.89ms
step:363/2330 train_time:22102ms step_avg:60.89ms
step:364/2330 train_time:22164ms step_avg:60.89ms
step:365/2330 train_time:22224ms step_avg:60.89ms
step:366/2330 train_time:22285ms step_avg:60.89ms
step:367/2330 train_time:22345ms step_avg:60.89ms
step:368/2330 train_time:22407ms step_avg:60.89ms
step:369/2330 train_time:22468ms step_avg:60.89ms
step:370/2330 train_time:22529ms step_avg:60.89ms
step:371/2330 train_time:22590ms step_avg:60.89ms
step:372/2330 train_time:22652ms step_avg:60.89ms
step:373/2330 train_time:22713ms step_avg:60.89ms
step:374/2330 train_time:22775ms step_avg:60.90ms
step:375/2330 train_time:22835ms step_avg:60.89ms
step:376/2330 train_time:22897ms step_avg:60.90ms
step:377/2330 train_time:22956ms step_avg:60.89ms
step:378/2330 train_time:23018ms step_avg:60.89ms
step:379/2330 train_time:23077ms step_avg:60.89ms
step:380/2330 train_time:23140ms step_avg:60.89ms
step:381/2330 train_time:23200ms step_avg:60.89ms
step:382/2330 train_time:23261ms step_avg:60.89ms
step:383/2330 train_time:23320ms step_avg:60.89ms
step:384/2330 train_time:23383ms step_avg:60.89ms
step:385/2330 train_time:23443ms step_avg:60.89ms
step:386/2330 train_time:23505ms step_avg:60.89ms
step:387/2330 train_time:23565ms step_avg:60.89ms
step:388/2330 train_time:23627ms step_avg:60.89ms
step:389/2330 train_time:23687ms step_avg:60.89ms
step:390/2330 train_time:23749ms step_avg:60.89ms
step:391/2330 train_time:23809ms step_avg:60.89ms
step:392/2330 train_time:23871ms step_avg:60.90ms
step:393/2330 train_time:23932ms step_avg:60.90ms
step:394/2330 train_time:23994ms step_avg:60.90ms
step:395/2330 train_time:24054ms step_avg:60.90ms
step:396/2330 train_time:24116ms step_avg:60.90ms
step:397/2330 train_time:24177ms step_avg:60.90ms
step:398/2330 train_time:24239ms step_avg:60.90ms
step:399/2330 train_time:24300ms step_avg:60.90ms
step:400/2330 train_time:24362ms step_avg:60.90ms
step:401/2330 train_time:24421ms step_avg:60.90ms
step:402/2330 train_time:24483ms step_avg:60.90ms
step:403/2330 train_time:24543ms step_avg:60.90ms
step:404/2330 train_time:24605ms step_avg:60.90ms
step:405/2330 train_time:24664ms step_avg:60.90ms
step:406/2330 train_time:24726ms step_avg:60.90ms
step:407/2330 train_time:24785ms step_avg:60.90ms
step:408/2330 train_time:24847ms step_avg:60.90ms
step:409/2330 train_time:24907ms step_avg:60.90ms
step:410/2330 train_time:24969ms step_avg:60.90ms
step:411/2330 train_time:25029ms step_avg:60.90ms
step:412/2330 train_time:25092ms step_avg:60.90ms
step:413/2330 train_time:25153ms step_avg:60.90ms
step:414/2330 train_time:25215ms step_avg:60.91ms
step:415/2330 train_time:25275ms step_avg:60.90ms
step:416/2330 train_time:25337ms step_avg:60.91ms
step:417/2330 train_time:25398ms step_avg:60.91ms
step:418/2330 train_time:25459ms step_avg:60.91ms
step:419/2330 train_time:25519ms step_avg:60.91ms
step:420/2330 train_time:25581ms step_avg:60.91ms
step:421/2330 train_time:25641ms step_avg:60.90ms
step:422/2330 train_time:25702ms step_avg:60.91ms
step:423/2330 train_time:25762ms step_avg:60.90ms
step:424/2330 train_time:25823ms step_avg:60.90ms
step:425/2330 train_time:25882ms step_avg:60.90ms
step:426/2330 train_time:25944ms step_avg:60.90ms
step:427/2330 train_time:26004ms step_avg:60.90ms
step:428/2330 train_time:26067ms step_avg:60.91ms
step:429/2330 train_time:26127ms step_avg:60.90ms
step:430/2330 train_time:26190ms step_avg:60.91ms
step:431/2330 train_time:26250ms step_avg:60.90ms
step:432/2330 train_time:26313ms step_avg:60.91ms
step:433/2330 train_time:26373ms step_avg:60.91ms
step:434/2330 train_time:26434ms step_avg:60.91ms
step:435/2330 train_time:26494ms step_avg:60.91ms
step:436/2330 train_time:26556ms step_avg:60.91ms
step:437/2330 train_time:26616ms step_avg:60.91ms
step:438/2330 train_time:26678ms step_avg:60.91ms
step:439/2330 train_time:26739ms step_avg:60.91ms
step:440/2330 train_time:26801ms step_avg:60.91ms
step:441/2330 train_time:26860ms step_avg:60.91ms
step:442/2330 train_time:26922ms step_avg:60.91ms
step:443/2330 train_time:26982ms step_avg:60.91ms
step:444/2330 train_time:27044ms step_avg:60.91ms
step:445/2330 train_time:27103ms step_avg:60.90ms
step:446/2330 train_time:27165ms step_avg:60.91ms
step:447/2330 train_time:27226ms step_avg:60.91ms
step:448/2330 train_time:27287ms step_avg:60.91ms
step:449/2330 train_time:27347ms step_avg:60.91ms
step:450/2330 train_time:27410ms step_avg:60.91ms
step:451/2330 train_time:27470ms step_avg:60.91ms
step:452/2330 train_time:27532ms step_avg:60.91ms
step:453/2330 train_time:27591ms step_avg:60.91ms
step:454/2330 train_time:27654ms step_avg:60.91ms
step:455/2330 train_time:27714ms step_avg:60.91ms
step:456/2330 train_time:27776ms step_avg:60.91ms
step:457/2330 train_time:27836ms step_avg:60.91ms
step:458/2330 train_time:27898ms step_avg:60.91ms
step:459/2330 train_time:27959ms step_avg:60.91ms
step:460/2330 train_time:28020ms step_avg:60.91ms
step:461/2330 train_time:28081ms step_avg:60.91ms
step:462/2330 train_time:28143ms step_avg:60.92ms
step:463/2330 train_time:28202ms step_avg:60.91ms
step:464/2330 train_time:28264ms step_avg:60.91ms
step:465/2330 train_time:28323ms step_avg:60.91ms
step:466/2330 train_time:28385ms step_avg:60.91ms
step:467/2330 train_time:28444ms step_avg:60.91ms
step:468/2330 train_time:28506ms step_avg:60.91ms
step:469/2330 train_time:28566ms step_avg:60.91ms
step:470/2330 train_time:28629ms step_avg:60.91ms
step:471/2330 train_time:28689ms step_avg:60.91ms
step:472/2330 train_time:28751ms step_avg:60.91ms
step:473/2330 train_time:28811ms step_avg:60.91ms
step:474/2330 train_time:28874ms step_avg:60.92ms
step:475/2330 train_time:28933ms step_avg:60.91ms
step:476/2330 train_time:28995ms step_avg:60.91ms
step:477/2330 train_time:29055ms step_avg:60.91ms
step:478/2330 train_time:29117ms step_avg:60.91ms
step:479/2330 train_time:29178ms step_avg:60.91ms
step:480/2330 train_time:29241ms step_avg:60.92ms
step:481/2330 train_time:29300ms step_avg:60.92ms
step:482/2330 train_time:29362ms step_avg:60.92ms
step:483/2330 train_time:29422ms step_avg:60.91ms
step:484/2330 train_time:29483ms step_avg:60.91ms
step:485/2330 train_time:29542ms step_avg:60.91ms
step:486/2330 train_time:29604ms step_avg:60.91ms
step:487/2330 train_time:29663ms step_avg:60.91ms
step:488/2330 train_time:29725ms step_avg:60.91ms
step:489/2330 train_time:29785ms step_avg:60.91ms
step:490/2330 train_time:29847ms step_avg:60.91ms
step:491/2330 train_time:29909ms step_avg:60.91ms
step:492/2330 train_time:29970ms step_avg:60.92ms
step:493/2330 train_time:30030ms step_avg:60.91ms
step:494/2330 train_time:30092ms step_avg:60.92ms
step:495/2330 train_time:30153ms step_avg:60.91ms
step:496/2330 train_time:30214ms step_avg:60.92ms
step:497/2330 train_time:30274ms step_avg:60.91ms
step:498/2330 train_time:30336ms step_avg:60.92ms
step:499/2330 train_time:30396ms step_avg:60.91ms
step:500/2330 train_time:30458ms step_avg:60.92ms
step:500/2330 val_loss:3.8847 train_time:30522ms step_avg:61.04ms
step:501/2330 train_time:30544ms step_avg:60.97ms
step:502/2330 train_time:30584ms step_avg:60.92ms
step:503/2330 train_time:30647ms step_avg:60.93ms
step:504/2330 train_time:30712ms step_avg:60.94ms
step:505/2330 train_time:30773ms step_avg:60.94ms
step:506/2330 train_time:30835ms step_avg:60.94ms
step:507/2330 train_time:30894ms step_avg:60.94ms
step:508/2330 train_time:30956ms step_avg:60.94ms
step:509/2330 train_time:31016ms step_avg:60.93ms
step:510/2330 train_time:31077ms step_avg:60.93ms
step:511/2330 train_time:31136ms step_avg:60.93ms
step:512/2330 train_time:31197ms step_avg:60.93ms
step:513/2330 train_time:31256ms step_avg:60.93ms
step:514/2330 train_time:31318ms step_avg:60.93ms
step:515/2330 train_time:31377ms step_avg:60.93ms
step:516/2330 train_time:31440ms step_avg:60.93ms
step:517/2330 train_time:31501ms step_avg:60.93ms
step:518/2330 train_time:31564ms step_avg:60.94ms
step:519/2330 train_time:31627ms step_avg:60.94ms
step:520/2330 train_time:31690ms step_avg:60.94ms
step:521/2330 train_time:31750ms step_avg:60.94ms
step:522/2330 train_time:31812ms step_avg:60.94ms
step:523/2330 train_time:31872ms step_avg:60.94ms
step:524/2330 train_time:31933ms step_avg:60.94ms
step:525/2330 train_time:31993ms step_avg:60.94ms
step:526/2330 train_time:32055ms step_avg:60.94ms
step:527/2330 train_time:32115ms step_avg:60.94ms
step:528/2330 train_time:32177ms step_avg:60.94ms
step:529/2330 train_time:32237ms step_avg:60.94ms
step:530/2330 train_time:32298ms step_avg:60.94ms
step:531/2330 train_time:32357ms step_avg:60.94ms
step:532/2330 train_time:32419ms step_avg:60.94ms
step:533/2330 train_time:32479ms step_avg:60.94ms
step:534/2330 train_time:32541ms step_avg:60.94ms
step:535/2330 train_time:32602ms step_avg:60.94ms
step:536/2330 train_time:32665ms step_avg:60.94ms
step:537/2330 train_time:32726ms step_avg:60.94ms
step:538/2330 train_time:32788ms step_avg:60.94ms
step:539/2330 train_time:32847ms step_avg:60.94ms
step:540/2330 train_time:32909ms step_avg:60.94ms
step:541/2330 train_time:32968ms step_avg:60.94ms
step:542/2330 train_time:33030ms step_avg:60.94ms
step:543/2330 train_time:33089ms step_avg:60.94ms
step:544/2330 train_time:33150ms step_avg:60.94ms
step:545/2330 train_time:33210ms step_avg:60.94ms
step:546/2330 train_time:33272ms step_avg:60.94ms
step:547/2330 train_time:33332ms step_avg:60.94ms
step:548/2330 train_time:33395ms step_avg:60.94ms
step:549/2330 train_time:33455ms step_avg:60.94ms
step:550/2330 train_time:33517ms step_avg:60.94ms
step:551/2330 train_time:33577ms step_avg:60.94ms
step:552/2330 train_time:33639ms step_avg:60.94ms
step:553/2330 train_time:33700ms step_avg:60.94ms
step:554/2330 train_time:33762ms step_avg:60.94ms
step:555/2330 train_time:33822ms step_avg:60.94ms
step:556/2330 train_time:33884ms step_avg:60.94ms
step:557/2330 train_time:33945ms step_avg:60.94ms
step:558/2330 train_time:34007ms step_avg:60.94ms
step:559/2330 train_time:34066ms step_avg:60.94ms
step:560/2330 train_time:34128ms step_avg:60.94ms
step:561/2330 train_time:34187ms step_avg:60.94ms
step:562/2330 train_time:34249ms step_avg:60.94ms
step:563/2330 train_time:34308ms step_avg:60.94ms
step:564/2330 train_time:34369ms step_avg:60.94ms
step:565/2330 train_time:34429ms step_avg:60.94ms
step:566/2330 train_time:34491ms step_avg:60.94ms
step:567/2330 train_time:34551ms step_avg:60.94ms
step:568/2330 train_time:34613ms step_avg:60.94ms
step:569/2330 train_time:34674ms step_avg:60.94ms
step:570/2330 train_time:34736ms step_avg:60.94ms
step:571/2330 train_time:34796ms step_avg:60.94ms
step:572/2330 train_time:34859ms step_avg:60.94ms
step:573/2330 train_time:34918ms step_avg:60.94ms
step:574/2330 train_time:34981ms step_avg:60.94ms
step:575/2330 train_time:35041ms step_avg:60.94ms
step:576/2330 train_time:35102ms step_avg:60.94ms
step:577/2330 train_time:35162ms step_avg:60.94ms
step:578/2330 train_time:35224ms step_avg:60.94ms
step:579/2330 train_time:35284ms step_avg:60.94ms
step:580/2330 train_time:35345ms step_avg:60.94ms
step:581/2330 train_time:35405ms step_avg:60.94ms
step:582/2330 train_time:35466ms step_avg:60.94ms
step:583/2330 train_time:35525ms step_avg:60.94ms
step:584/2330 train_time:35587ms step_avg:60.94ms
step:585/2330 train_time:35647ms step_avg:60.93ms
step:586/2330 train_time:35709ms step_avg:60.94ms
step:587/2330 train_time:35768ms step_avg:60.93ms
step:588/2330 train_time:35830ms step_avg:60.94ms
step:589/2330 train_time:35890ms step_avg:60.93ms
step:590/2330 train_time:35953ms step_avg:60.94ms
step:591/2330 train_time:36013ms step_avg:60.94ms
step:592/2330 train_time:36075ms step_avg:60.94ms
step:593/2330 train_time:36135ms step_avg:60.94ms
step:594/2330 train_time:36198ms step_avg:60.94ms
step:595/2330 train_time:36257ms step_avg:60.94ms
step:596/2330 train_time:36320ms step_avg:60.94ms
step:597/2330 train_time:36379ms step_avg:60.94ms
step:598/2330 train_time:36442ms step_avg:60.94ms
step:599/2330 train_time:36502ms step_avg:60.94ms
step:600/2330 train_time:36564ms step_avg:60.94ms
step:601/2330 train_time:36624ms step_avg:60.94ms
step:602/2330 train_time:36686ms step_avg:60.94ms
step:603/2330 train_time:36746ms step_avg:60.94ms
step:604/2330 train_time:36807ms step_avg:60.94ms
step:605/2330 train_time:36866ms step_avg:60.94ms
step:606/2330 train_time:36928ms step_avg:60.94ms
step:607/2330 train_time:36988ms step_avg:60.94ms
step:608/2330 train_time:37049ms step_avg:60.94ms
step:609/2330 train_time:37109ms step_avg:60.93ms
step:610/2330 train_time:37171ms step_avg:60.94ms
step:611/2330 train_time:37230ms step_avg:60.93ms
step:612/2330 train_time:37293ms step_avg:60.94ms
step:613/2330 train_time:37353ms step_avg:60.93ms
step:614/2330 train_time:37416ms step_avg:60.94ms
step:615/2330 train_time:37477ms step_avg:60.94ms
step:616/2330 train_time:37538ms step_avg:60.94ms
step:617/2330 train_time:37598ms step_avg:60.94ms
step:618/2330 train_time:37660ms step_avg:60.94ms
step:619/2330 train_time:37720ms step_avg:60.94ms
step:620/2330 train_time:37783ms step_avg:60.94ms
step:621/2330 train_time:37844ms step_avg:60.94ms
step:622/2330 train_time:37905ms step_avg:60.94ms
step:623/2330 train_time:37965ms step_avg:60.94ms
step:624/2330 train_time:38027ms step_avg:60.94ms
step:625/2330 train_time:38087ms step_avg:60.94ms
step:626/2330 train_time:38148ms step_avg:60.94ms
step:627/2330 train_time:38208ms step_avg:60.94ms
step:628/2330 train_time:38269ms step_avg:60.94ms
step:629/2330 train_time:38330ms step_avg:60.94ms
step:630/2330 train_time:38392ms step_avg:60.94ms
step:631/2330 train_time:38452ms step_avg:60.94ms
step:632/2330 train_time:38515ms step_avg:60.94ms
step:633/2330 train_time:38575ms step_avg:60.94ms
step:634/2330 train_time:38638ms step_avg:60.94ms
step:635/2330 train_time:38698ms step_avg:60.94ms
step:636/2330 train_time:38760ms step_avg:60.94ms
step:637/2330 train_time:38820ms step_avg:60.94ms
step:638/2330 train_time:38882ms step_avg:60.94ms
step:639/2330 train_time:38943ms step_avg:60.94ms
step:640/2330 train_time:39005ms step_avg:60.95ms
step:641/2330 train_time:39065ms step_avg:60.94ms
step:642/2330 train_time:39126ms step_avg:60.94ms
step:643/2330 train_time:39187ms step_avg:60.94ms
step:644/2330 train_time:39247ms step_avg:60.94ms
step:645/2330 train_time:39307ms step_avg:60.94ms
step:646/2330 train_time:39369ms step_avg:60.94ms
step:647/2330 train_time:39428ms step_avg:60.94ms
step:648/2330 train_time:39490ms step_avg:60.94ms
step:649/2330 train_time:39550ms step_avg:60.94ms
step:650/2330 train_time:39612ms step_avg:60.94ms
step:651/2330 train_time:39673ms step_avg:60.94ms
step:652/2330 train_time:39735ms step_avg:60.94ms
step:653/2330 train_time:39795ms step_avg:60.94ms
step:654/2330 train_time:39858ms step_avg:60.94ms
step:655/2330 train_time:39919ms step_avg:60.94ms
step:656/2330 train_time:39981ms step_avg:60.95ms
step:657/2330 train_time:40040ms step_avg:60.94ms
step:658/2330 train_time:40102ms step_avg:60.95ms
step:659/2330 train_time:40162ms step_avg:60.94ms
step:660/2330 train_time:40224ms step_avg:60.94ms
step:661/2330 train_time:40284ms step_avg:60.94ms
step:662/2330 train_time:40346ms step_avg:60.95ms
step:663/2330 train_time:40406ms step_avg:60.94ms
step:664/2330 train_time:40468ms step_avg:60.95ms
step:665/2330 train_time:40528ms step_avg:60.94ms
step:666/2330 train_time:40589ms step_avg:60.94ms
step:667/2330 train_time:40649ms step_avg:60.94ms
step:668/2330 train_time:40711ms step_avg:60.94ms
step:669/2330 train_time:40771ms step_avg:60.94ms
step:670/2330 train_time:40833ms step_avg:60.94ms
step:671/2330 train_time:40894ms step_avg:60.94ms
step:672/2330 train_time:40956ms step_avg:60.95ms
step:673/2330 train_time:41017ms step_avg:60.95ms
step:674/2330 train_time:41079ms step_avg:60.95ms
step:675/2330 train_time:41139ms step_avg:60.95ms
step:676/2330 train_time:41200ms step_avg:60.95ms
step:677/2330 train_time:41260ms step_avg:60.95ms
step:678/2330 train_time:41322ms step_avg:60.95ms
step:679/2330 train_time:41382ms step_avg:60.95ms
step:680/2330 train_time:41444ms step_avg:60.95ms
step:681/2330 train_time:41505ms step_avg:60.95ms
step:682/2330 train_time:41567ms step_avg:60.95ms
step:683/2330 train_time:41626ms step_avg:60.95ms
step:684/2330 train_time:41688ms step_avg:60.95ms
step:685/2330 train_time:41746ms step_avg:60.94ms
step:686/2330 train_time:41808ms step_avg:60.94ms
step:687/2330 train_time:41867ms step_avg:60.94ms
step:688/2330 train_time:41930ms step_avg:60.94ms
step:689/2330 train_time:41990ms step_avg:60.94ms
step:690/2330 train_time:42052ms step_avg:60.94ms
step:691/2330 train_time:42112ms step_avg:60.94ms
step:692/2330 train_time:42174ms step_avg:60.95ms
step:693/2330 train_time:42234ms step_avg:60.94ms
step:694/2330 train_time:42298ms step_avg:60.95ms
step:695/2330 train_time:42359ms step_avg:60.95ms
step:696/2330 train_time:42420ms step_avg:60.95ms
step:697/2330 train_time:42480ms step_avg:60.95ms
step:698/2330 train_time:42542ms step_avg:60.95ms
step:699/2330 train_time:42602ms step_avg:60.95ms
step:700/2330 train_time:42665ms step_avg:60.95ms
step:701/2330 train_time:42724ms step_avg:60.95ms
step:702/2330 train_time:42786ms step_avg:60.95ms
step:703/2330 train_time:42846ms step_avg:60.95ms
step:704/2330 train_time:42907ms step_avg:60.95ms
step:705/2330 train_time:42967ms step_avg:60.95ms
step:706/2330 train_time:43029ms step_avg:60.95ms
step:707/2330 train_time:43088ms step_avg:60.95ms
step:708/2330 train_time:43150ms step_avg:60.95ms
step:709/2330 train_time:43210ms step_avg:60.95ms
step:710/2330 train_time:43273ms step_avg:60.95ms
step:711/2330 train_time:43333ms step_avg:60.95ms
step:712/2330 train_time:43395ms step_avg:60.95ms
step:713/2330 train_time:43456ms step_avg:60.95ms
step:714/2330 train_time:43517ms step_avg:60.95ms
step:715/2330 train_time:43578ms step_avg:60.95ms
step:716/2330 train_time:43640ms step_avg:60.95ms
step:717/2330 train_time:43700ms step_avg:60.95ms
step:718/2330 train_time:43762ms step_avg:60.95ms
step:719/2330 train_time:43822ms step_avg:60.95ms
step:720/2330 train_time:43884ms step_avg:60.95ms
step:721/2330 train_time:43944ms step_avg:60.95ms
step:722/2330 train_time:44006ms step_avg:60.95ms
step:723/2330 train_time:44067ms step_avg:60.95ms
step:724/2330 train_time:44128ms step_avg:60.95ms
step:725/2330 train_time:44187ms step_avg:60.95ms
step:726/2330 train_time:44250ms step_avg:60.95ms
step:727/2330 train_time:44309ms step_avg:60.95ms
step:728/2330 train_time:44370ms step_avg:60.95ms
step:729/2330 train_time:44430ms step_avg:60.95ms
step:730/2330 train_time:44493ms step_avg:60.95ms
step:731/2330 train_time:44553ms step_avg:60.95ms
step:732/2330 train_time:44616ms step_avg:60.95ms
step:733/2330 train_time:44676ms step_avg:60.95ms
step:734/2330 train_time:44738ms step_avg:60.95ms
step:735/2330 train_time:44799ms step_avg:60.95ms
step:736/2330 train_time:44860ms step_avg:60.95ms
step:737/2330 train_time:44920ms step_avg:60.95ms
step:738/2330 train_time:44983ms step_avg:60.95ms
step:739/2330 train_time:45043ms step_avg:60.95ms
step:740/2330 train_time:45105ms step_avg:60.95ms
step:741/2330 train_time:45165ms step_avg:60.95ms
step:742/2330 train_time:45226ms step_avg:60.95ms
step:743/2330 train_time:45286ms step_avg:60.95ms
step:744/2330 train_time:45347ms step_avg:60.95ms
step:745/2330 train_time:45407ms step_avg:60.95ms
step:746/2330 train_time:45469ms step_avg:60.95ms
step:747/2330 train_time:45529ms step_avg:60.95ms
step:748/2330 train_time:45591ms step_avg:60.95ms
step:749/2330 train_time:45650ms step_avg:60.95ms
step:750/2330 train_time:45713ms step_avg:60.95ms
step:750/2330 val_loss:3.7434 train_time:45777ms step_avg:61.04ms
step:751/2330 train_time:45798ms step_avg:60.98ms
step:752/2330 train_time:45836ms step_avg:60.95ms
step:753/2330 train_time:45902ms step_avg:60.96ms
step:754/2330 train_time:45965ms step_avg:60.96ms
step:755/2330 train_time:46025ms step_avg:60.96ms
step:756/2330 train_time:46086ms step_avg:60.96ms
step:757/2330 train_time:46145ms step_avg:60.96ms
step:758/2330 train_time:46206ms step_avg:60.96ms
step:759/2330 train_time:46266ms step_avg:60.96ms
step:760/2330 train_time:46327ms step_avg:60.96ms
step:761/2330 train_time:46385ms step_avg:60.95ms
step:762/2330 train_time:46446ms step_avg:60.95ms
step:763/2330 train_time:46505ms step_avg:60.95ms
step:764/2330 train_time:46566ms step_avg:60.95ms
step:765/2330 train_time:46625ms step_avg:60.95ms
step:766/2330 train_time:46688ms step_avg:60.95ms
step:767/2330 train_time:46750ms step_avg:60.95ms
step:768/2330 train_time:46814ms step_avg:60.96ms
step:769/2330 train_time:46876ms step_avg:60.96ms
step:770/2330 train_time:46940ms step_avg:60.96ms
step:771/2330 train_time:47001ms step_avg:60.96ms
step:772/2330 train_time:47064ms step_avg:60.96ms
step:773/2330 train_time:47125ms step_avg:60.96ms
step:774/2330 train_time:47187ms step_avg:60.97ms
step:775/2330 train_time:47247ms step_avg:60.96ms
step:776/2330 train_time:47309ms step_avg:60.96ms
step:777/2330 train_time:47369ms step_avg:60.96ms
step:778/2330 train_time:47431ms step_avg:60.97ms
step:779/2330 train_time:47491ms step_avg:60.96ms
step:780/2330 train_time:47554ms step_avg:60.97ms
step:781/2330 train_time:47614ms step_avg:60.97ms
step:782/2330 train_time:47676ms step_avg:60.97ms
step:783/2330 train_time:47737ms step_avg:60.97ms
step:784/2330 train_time:47800ms step_avg:60.97ms
step:785/2330 train_time:47861ms step_avg:60.97ms
step:786/2330 train_time:47924ms step_avg:60.97ms
step:787/2330 train_time:47985ms step_avg:60.97ms
step:788/2330 train_time:48048ms step_avg:60.97ms
step:789/2330 train_time:48108ms step_avg:60.97ms
step:790/2330 train_time:48170ms step_avg:60.98ms
step:791/2330 train_time:48231ms step_avg:60.97ms
step:792/2330 train_time:48293ms step_avg:60.98ms
step:793/2330 train_time:48354ms step_avg:60.98ms
step:794/2330 train_time:48417ms step_avg:60.98ms
step:795/2330 train_time:48478ms step_avg:60.98ms
step:796/2330 train_time:48541ms step_avg:60.98ms
step:797/2330 train_time:48601ms step_avg:60.98ms
step:798/2330 train_time:48664ms step_avg:60.98ms
step:799/2330 train_time:48725ms step_avg:60.98ms
step:800/2330 train_time:48787ms step_avg:60.98ms
step:801/2330 train_time:48847ms step_avg:60.98ms
step:802/2330 train_time:48909ms step_avg:60.98ms
step:803/2330 train_time:48970ms step_avg:60.98ms
step:804/2330 train_time:49033ms step_avg:60.99ms
step:805/2330 train_time:49094ms step_avg:60.99ms
step:806/2330 train_time:49157ms step_avg:60.99ms
step:807/2330 train_time:49218ms step_avg:60.99ms
step:808/2330 train_time:49280ms step_avg:60.99ms
step:809/2330 train_time:49341ms step_avg:60.99ms
step:810/2330 train_time:49404ms step_avg:60.99ms
step:811/2330 train_time:49465ms step_avg:60.99ms
step:812/2330 train_time:49527ms step_avg:60.99ms
step:813/2330 train_time:49587ms step_avg:60.99ms
step:814/2330 train_time:49649ms step_avg:60.99ms
step:815/2330 train_time:49709ms step_avg:60.99ms
step:816/2330 train_time:49771ms step_avg:60.99ms
step:817/2330 train_time:49832ms step_avg:60.99ms
step:818/2330 train_time:49894ms step_avg:61.00ms
step:819/2330 train_time:49955ms step_avg:61.00ms
step:820/2330 train_time:50018ms step_avg:61.00ms
step:821/2330 train_time:50079ms step_avg:61.00ms
step:822/2330 train_time:50142ms step_avg:61.00ms
step:823/2330 train_time:50202ms step_avg:61.00ms
step:824/2330 train_time:50265ms step_avg:61.00ms
step:825/2330 train_time:50326ms step_avg:61.00ms
step:826/2330 train_time:50388ms step_avg:61.00ms
step:827/2330 train_time:50448ms step_avg:61.00ms
step:828/2330 train_time:50510ms step_avg:61.00ms
step:829/2330 train_time:50570ms step_avg:61.00ms
step:830/2330 train_time:50632ms step_avg:61.00ms
step:831/2330 train_time:50693ms step_avg:61.00ms
step:832/2330 train_time:50755ms step_avg:61.00ms
step:833/2330 train_time:50816ms step_avg:61.00ms
step:834/2330 train_time:50879ms step_avg:61.01ms
step:835/2330 train_time:50940ms step_avg:61.01ms
step:836/2330 train_time:51002ms step_avg:61.01ms
step:837/2330 train_time:51063ms step_avg:61.01ms
step:838/2330 train_time:51125ms step_avg:61.01ms
step:839/2330 train_time:51185ms step_avg:61.01ms
step:840/2330 train_time:51248ms step_avg:61.01ms
step:841/2330 train_time:51308ms step_avg:61.01ms
step:842/2330 train_time:51371ms step_avg:61.01ms
step:843/2330 train_time:51431ms step_avg:61.01ms
step:844/2330 train_time:51493ms step_avg:61.01ms
step:845/2330 train_time:51554ms step_avg:61.01ms
step:846/2330 train_time:51617ms step_avg:61.01ms
step:847/2330 train_time:51679ms step_avg:61.01ms
step:848/2330 train_time:51741ms step_avg:61.02ms
step:849/2330 train_time:51802ms step_avg:61.01ms
step:850/2330 train_time:51864ms step_avg:61.02ms
step:851/2330 train_time:51924ms step_avg:61.02ms
step:852/2330 train_time:51986ms step_avg:61.02ms
step:853/2330 train_time:52047ms step_avg:61.02ms
step:854/2330 train_time:52109ms step_avg:61.02ms
step:855/2330 train_time:52169ms step_avg:61.02ms
step:856/2330 train_time:52232ms step_avg:61.02ms
step:857/2330 train_time:52292ms step_avg:61.02ms
step:858/2330 train_time:52355ms step_avg:61.02ms
step:859/2330 train_time:52416ms step_avg:61.02ms
step:860/2330 train_time:52478ms step_avg:61.02ms
step:861/2330 train_time:52538ms step_avg:61.02ms
step:862/2330 train_time:52602ms step_avg:61.02ms
step:863/2330 train_time:52662ms step_avg:61.02ms
step:864/2330 train_time:52724ms step_avg:61.02ms
step:865/2330 train_time:52784ms step_avg:61.02ms
step:866/2330 train_time:52847ms step_avg:61.02ms
step:867/2330 train_time:52907ms step_avg:61.02ms
step:868/2330 train_time:52969ms step_avg:61.02ms
step:869/2330 train_time:53030ms step_avg:61.02ms
step:870/2330 train_time:53092ms step_avg:61.03ms
step:871/2330 train_time:53152ms step_avg:61.02ms
step:872/2330 train_time:53215ms step_avg:61.03ms
step:873/2330 train_time:53275ms step_avg:61.03ms
step:874/2330 train_time:53338ms step_avg:61.03ms
step:875/2330 train_time:53399ms step_avg:61.03ms
step:876/2330 train_time:53461ms step_avg:61.03ms
step:877/2330 train_time:53522ms step_avg:61.03ms
step:878/2330 train_time:53584ms step_avg:61.03ms
step:879/2330 train_time:53645ms step_avg:61.03ms
step:880/2330 train_time:53708ms step_avg:61.03ms
step:881/2330 train_time:53768ms step_avg:61.03ms
step:882/2330 train_time:53830ms step_avg:61.03ms
step:883/2330 train_time:53890ms step_avg:61.03ms
step:884/2330 train_time:53953ms step_avg:61.03ms
step:885/2330 train_time:54013ms step_avg:61.03ms
step:886/2330 train_time:54076ms step_avg:61.03ms
step:887/2330 train_time:54137ms step_avg:61.03ms
step:888/2330 train_time:54200ms step_avg:61.04ms
step:889/2330 train_time:54261ms step_avg:61.04ms
step:890/2330 train_time:54323ms step_avg:61.04ms
step:891/2330 train_time:54383ms step_avg:61.04ms
step:892/2330 train_time:54446ms step_avg:61.04ms
step:893/2330 train_time:54507ms step_avg:61.04ms
step:894/2330 train_time:54569ms step_avg:61.04ms
step:895/2330 train_time:54630ms step_avg:61.04ms
step:896/2330 train_time:54692ms step_avg:61.04ms
step:897/2330 train_time:54752ms step_avg:61.04ms
step:898/2330 train_time:54815ms step_avg:61.04ms
step:899/2330 train_time:54876ms step_avg:61.04ms
step:900/2330 train_time:54938ms step_avg:61.04ms
step:901/2330 train_time:54998ms step_avg:61.04ms
step:902/2330 train_time:55061ms step_avg:61.04ms
step:903/2330 train_time:55122ms step_avg:61.04ms
step:904/2330 train_time:55184ms step_avg:61.04ms
step:905/2330 train_time:55245ms step_avg:61.04ms
step:906/2330 train_time:55307ms step_avg:61.05ms
step:907/2330 train_time:55368ms step_avg:61.04ms
step:908/2330 train_time:55430ms step_avg:61.05ms
step:909/2330 train_time:55490ms step_avg:61.05ms
step:910/2330 train_time:55553ms step_avg:61.05ms
step:911/2330 train_time:55613ms step_avg:61.05ms
step:912/2330 train_time:55675ms step_avg:61.05ms
step:913/2330 train_time:55736ms step_avg:61.05ms
step:914/2330 train_time:55799ms step_avg:61.05ms
step:915/2330 train_time:55860ms step_avg:61.05ms
step:916/2330 train_time:55922ms step_avg:61.05ms
step:917/2330 train_time:55983ms step_avg:61.05ms
step:918/2330 train_time:56045ms step_avg:61.05ms
step:919/2330 train_time:56106ms step_avg:61.05ms
step:920/2330 train_time:56168ms step_avg:61.05ms
step:921/2330 train_time:56229ms step_avg:61.05ms
step:922/2330 train_time:56291ms step_avg:61.05ms
step:923/2330 train_time:56351ms step_avg:61.05ms
step:924/2330 train_time:56413ms step_avg:61.05ms
step:925/2330 train_time:56474ms step_avg:61.05ms
step:926/2330 train_time:56537ms step_avg:61.05ms
step:927/2330 train_time:56597ms step_avg:61.05ms
step:928/2330 train_time:56660ms step_avg:61.06ms
step:929/2330 train_time:56720ms step_avg:61.06ms
step:930/2330 train_time:56782ms step_avg:61.06ms
step:931/2330 train_time:56843ms step_avg:61.06ms
step:932/2330 train_time:56905ms step_avg:61.06ms
step:933/2330 train_time:56965ms step_avg:61.06ms
step:934/2330 train_time:57028ms step_avg:61.06ms
step:935/2330 train_time:57087ms step_avg:61.06ms
step:936/2330 train_time:57150ms step_avg:61.06ms
step:937/2330 train_time:57210ms step_avg:61.06ms
step:938/2330 train_time:57272ms step_avg:61.06ms
step:939/2330 train_time:57333ms step_avg:61.06ms
step:940/2330 train_time:57395ms step_avg:61.06ms
step:941/2330 train_time:57456ms step_avg:61.06ms
step:942/2330 train_time:57518ms step_avg:61.06ms
step:943/2330 train_time:57578ms step_avg:61.06ms
step:944/2330 train_time:57640ms step_avg:61.06ms
step:945/2330 train_time:57701ms step_avg:61.06ms
step:946/2330 train_time:57764ms step_avg:61.06ms
step:947/2330 train_time:57824ms step_avg:61.06ms
step:948/2330 train_time:57886ms step_avg:61.06ms
step:949/2330 train_time:57946ms step_avg:61.06ms
step:950/2330 train_time:58009ms step_avg:61.06ms
step:951/2330 train_time:58069ms step_avg:61.06ms
step:952/2330 train_time:58131ms step_avg:61.06ms
step:953/2330 train_time:58192ms step_avg:61.06ms
step:954/2330 train_time:58254ms step_avg:61.06ms
step:955/2330 train_time:58314ms step_avg:61.06ms
step:956/2330 train_time:58377ms step_avg:61.06ms
step:957/2330 train_time:58438ms step_avg:61.06ms
step:958/2330 train_time:58501ms step_avg:61.07ms
step:959/2330 train_time:58561ms step_avg:61.06ms
step:960/2330 train_time:58623ms step_avg:61.07ms
step:961/2330 train_time:58683ms step_avg:61.06ms
step:962/2330 train_time:58745ms step_avg:61.07ms
step:963/2330 train_time:58805ms step_avg:61.06ms
step:964/2330 train_time:58867ms step_avg:61.07ms
step:965/2330 train_time:58928ms step_avg:61.07ms
step:966/2330 train_time:58990ms step_avg:61.07ms
step:967/2330 train_time:59051ms step_avg:61.07ms
step:968/2330 train_time:59113ms step_avg:61.07ms
step:969/2330 train_time:59173ms step_avg:61.07ms
step:970/2330 train_time:59236ms step_avg:61.07ms
step:971/2330 train_time:59296ms step_avg:61.07ms
step:972/2330 train_time:59359ms step_avg:61.07ms
step:973/2330 train_time:59420ms step_avg:61.07ms
step:974/2330 train_time:59483ms step_avg:61.07ms
step:975/2330 train_time:59544ms step_avg:61.07ms
step:976/2330 train_time:59606ms step_avg:61.07ms
step:977/2330 train_time:59666ms step_avg:61.07ms
step:978/2330 train_time:59729ms step_avg:61.07ms
step:979/2330 train_time:59788ms step_avg:61.07ms
step:980/2330 train_time:59851ms step_avg:61.07ms
step:981/2330 train_time:59912ms step_avg:61.07ms
step:982/2330 train_time:59975ms step_avg:61.07ms
step:983/2330 train_time:60035ms step_avg:61.07ms
step:984/2330 train_time:60097ms step_avg:61.07ms
step:985/2330 train_time:60158ms step_avg:61.07ms
step:986/2330 train_time:60221ms step_avg:61.08ms
step:987/2330 train_time:60281ms step_avg:61.08ms
step:988/2330 train_time:60344ms step_avg:61.08ms
step:989/2330 train_time:60405ms step_avg:61.08ms
step:990/2330 train_time:60468ms step_avg:61.08ms
step:991/2330 train_time:60528ms step_avg:61.08ms
step:992/2330 train_time:60590ms step_avg:61.08ms
step:993/2330 train_time:60650ms step_avg:61.08ms
step:994/2330 train_time:60713ms step_avg:61.08ms
step:995/2330 train_time:60774ms step_avg:61.08ms
step:996/2330 train_time:60836ms step_avg:61.08ms
step:997/2330 train_time:60896ms step_avg:61.08ms
step:998/2330 train_time:60958ms step_avg:61.08ms
step:999/2330 train_time:61019ms step_avg:61.08ms
step:1000/2330 train_time:61081ms step_avg:61.08ms
step:1000/2330 val_loss:3.6400 train_time:61146ms step_avg:61.15ms
step:1001/2330 train_time:61168ms step_avg:61.11ms
step:1002/2330 train_time:61207ms step_avg:61.08ms
step:1003/2330 train_time:61272ms step_avg:61.09ms
step:1004/2330 train_time:61336ms step_avg:61.09ms
step:1005/2330 train_time:61396ms step_avg:61.09ms
step:1006/2330 train_time:61459ms step_avg:61.09ms
step:1007/2330 train_time:61518ms step_avg:61.09ms
step:1008/2330 train_time:61580ms step_avg:61.09ms
step:1009/2330 train_time:61639ms step_avg:61.09ms
step:1010/2330 train_time:61701ms step_avg:61.09ms
step:1011/2330 train_time:61760ms step_avg:61.09ms
step:1012/2330 train_time:61822ms step_avg:61.09ms
step:1013/2330 train_time:61881ms step_avg:61.09ms
step:1014/2330 train_time:61944ms step_avg:61.09ms
step:1015/2330 train_time:62004ms step_avg:61.09ms
step:1016/2330 train_time:62069ms step_avg:61.09ms
step:1017/2330 train_time:62131ms step_avg:61.09ms
step:1018/2330 train_time:62195ms step_avg:61.10ms
step:1019/2330 train_time:62258ms step_avg:61.10ms
step:1020/2330 train_time:62321ms step_avg:61.10ms
step:1021/2330 train_time:62382ms step_avg:61.10ms
step:1022/2330 train_time:62445ms step_avg:61.10ms
step:1023/2330 train_time:62506ms step_avg:61.10ms
step:1024/2330 train_time:62568ms step_avg:61.10ms
step:1025/2330 train_time:62627ms step_avg:61.10ms
step:1026/2330 train_time:62689ms step_avg:61.10ms
step:1027/2330 train_time:62749ms step_avg:61.10ms
step:1028/2330 train_time:62811ms step_avg:61.10ms
step:1029/2330 train_time:62871ms step_avg:61.10ms
step:1030/2330 train_time:62934ms step_avg:61.10ms
step:1031/2330 train_time:62994ms step_avg:61.10ms
step:1032/2330 train_time:63057ms step_avg:61.10ms
step:1033/2330 train_time:63117ms step_avg:61.10ms
step:1034/2330 train_time:63181ms step_avg:61.10ms
step:1035/2330 train_time:63243ms step_avg:61.10ms
step:1036/2330 train_time:63307ms step_avg:61.11ms
step:1037/2330 train_time:63368ms step_avg:61.11ms
step:1038/2330 train_time:63430ms step_avg:61.11ms
step:1039/2330 train_time:63491ms step_avg:61.11ms
step:1040/2330 train_time:63553ms step_avg:61.11ms
step:1041/2330 train_time:63614ms step_avg:61.11ms
step:1042/2330 train_time:63675ms step_avg:61.11ms
step:1043/2330 train_time:63736ms step_avg:61.11ms
step:1044/2330 train_time:63798ms step_avg:61.11ms
step:1045/2330 train_time:63858ms step_avg:61.11ms
step:1046/2330 train_time:63919ms step_avg:61.11ms
step:1047/2330 train_time:63979ms step_avg:61.11ms
step:1048/2330 train_time:64041ms step_avg:61.11ms
step:1049/2330 train_time:64103ms step_avg:61.11ms
step:1050/2330 train_time:64166ms step_avg:61.11ms
step:1051/2330 train_time:64228ms step_avg:61.11ms
step:1052/2330 train_time:64291ms step_avg:61.11ms
step:1053/2330 train_time:64351ms step_avg:61.11ms
step:1054/2330 train_time:64414ms step_avg:61.11ms
step:1055/2330 train_time:64475ms step_avg:61.11ms
step:1056/2330 train_time:64537ms step_avg:61.11ms
step:1057/2330 train_time:64597ms step_avg:61.11ms
step:1058/2330 train_time:64660ms step_avg:61.12ms
step:1059/2330 train_time:64720ms step_avg:61.11ms
step:1060/2330 train_time:64781ms step_avg:61.11ms
step:1061/2330 train_time:64842ms step_avg:61.11ms
step:1062/2330 train_time:64904ms step_avg:61.11ms
step:1063/2330 train_time:64964ms step_avg:61.11ms
step:1064/2330 train_time:65027ms step_avg:61.12ms
step:1065/2330 train_time:65088ms step_avg:61.12ms
step:1066/2330 train_time:65151ms step_avg:61.12ms
step:1067/2330 train_time:65213ms step_avg:61.12ms
step:1068/2330 train_time:65275ms step_avg:61.12ms
step:1069/2330 train_time:65336ms step_avg:61.12ms
step:1070/2330 train_time:65398ms step_avg:61.12ms
step:1071/2330 train_time:65459ms step_avg:61.12ms
step:1072/2330 train_time:65521ms step_avg:61.12ms
step:1073/2330 train_time:65582ms step_avg:61.12ms
step:1074/2330 train_time:65645ms step_avg:61.12ms
step:1075/2330 train_time:65705ms step_avg:61.12ms
step:1076/2330 train_time:65768ms step_avg:61.12ms
step:1077/2330 train_time:65828ms step_avg:61.12ms
step:1078/2330 train_time:65890ms step_avg:61.12ms
step:1079/2330 train_time:65951ms step_avg:61.12ms
step:1080/2330 train_time:66013ms step_avg:61.12ms
step:1081/2330 train_time:66074ms step_avg:61.12ms
step:1082/2330 train_time:66137ms step_avg:61.12ms
step:1083/2330 train_time:66197ms step_avg:61.12ms
step:1084/2330 train_time:66260ms step_avg:61.13ms
step:1085/2330 train_time:66320ms step_avg:61.12ms
step:1086/2330 train_time:66384ms step_avg:61.13ms
step:1087/2330 train_time:66445ms step_avg:61.13ms
step:1088/2330 train_time:66508ms step_avg:61.13ms
step:1089/2330 train_time:66569ms step_avg:61.13ms
step:1090/2330 train_time:66632ms step_avg:61.13ms
step:1091/2330 train_time:66692ms step_avg:61.13ms
step:1092/2330 train_time:66754ms step_avg:61.13ms
step:1093/2330 train_time:66814ms step_avg:61.13ms
step:1094/2330 train_time:66876ms step_avg:61.13ms
step:1095/2330 train_time:66936ms step_avg:61.13ms
step:1096/2330 train_time:66998ms step_avg:61.13ms
step:1097/2330 train_time:67059ms step_avg:61.13ms
step:1098/2330 train_time:67121ms step_avg:61.13ms
step:1099/2330 train_time:67181ms step_avg:61.13ms
step:1100/2330 train_time:67245ms step_avg:61.13ms
step:1101/2330 train_time:67305ms step_avg:61.13ms
step:1102/2330 train_time:67368ms step_avg:61.13ms
step:1103/2330 train_time:67430ms step_avg:61.13ms
step:1104/2330 train_time:67493ms step_avg:61.14ms
step:1105/2330 train_time:67554ms step_avg:61.13ms
step:1106/2330 train_time:67616ms step_avg:61.14ms
step:1107/2330 train_time:67676ms step_avg:61.13ms
step:1108/2330 train_time:67738ms step_avg:61.14ms
step:1109/2330 train_time:67798ms step_avg:61.13ms
step:1110/2330 train_time:67861ms step_avg:61.14ms
step:1111/2330 train_time:67921ms step_avg:61.13ms
step:1112/2330 train_time:67983ms step_avg:61.14ms
step:1113/2330 train_time:68044ms step_avg:61.14ms
step:1114/2330 train_time:68106ms step_avg:61.14ms
step:1115/2330 train_time:68166ms step_avg:61.14ms
step:1116/2330 train_time:68228ms step_avg:61.14ms
step:1117/2330 train_time:68289ms step_avg:61.14ms
step:1118/2330 train_time:68352ms step_avg:61.14ms
step:1119/2330 train_time:68413ms step_avg:61.14ms
step:1120/2330 train_time:68476ms step_avg:61.14ms
step:1121/2330 train_time:68536ms step_avg:61.14ms
step:1122/2330 train_time:68598ms step_avg:61.14ms
step:1123/2330 train_time:68658ms step_avg:61.14ms
step:1124/2330 train_time:68720ms step_avg:61.14ms
step:1125/2330 train_time:68781ms step_avg:61.14ms
step:1126/2330 train_time:68845ms step_avg:61.14ms
step:1127/2330 train_time:68906ms step_avg:61.14ms
step:1128/2330 train_time:68968ms step_avg:61.14ms
step:1129/2330 train_time:69028ms step_avg:61.14ms
step:1130/2330 train_time:69090ms step_avg:61.14ms
step:1131/2330 train_time:69150ms step_avg:61.14ms
step:1132/2330 train_time:69212ms step_avg:61.14ms
step:1133/2330 train_time:69273ms step_avg:61.14ms
step:1134/2330 train_time:69336ms step_avg:61.14ms
step:1135/2330 train_time:69396ms step_avg:61.14ms
step:1136/2330 train_time:69459ms step_avg:61.14ms
step:1137/2330 train_time:69520ms step_avg:61.14ms
step:1138/2330 train_time:69582ms step_avg:61.14ms
step:1139/2330 train_time:69642ms step_avg:61.14ms
step:1140/2330 train_time:69705ms step_avg:61.15ms
step:1141/2330 train_time:69766ms step_avg:61.14ms
step:1142/2330 train_time:69829ms step_avg:61.15ms
step:1143/2330 train_time:69889ms step_avg:61.15ms
step:1144/2330 train_time:69951ms step_avg:61.15ms
step:1145/2330 train_time:70011ms step_avg:61.15ms
step:1146/2330 train_time:70074ms step_avg:61.15ms
step:1147/2330 train_time:70134ms step_avg:61.15ms
step:1148/2330 train_time:70196ms step_avg:61.15ms
step:1149/2330 train_time:70256ms step_avg:61.15ms
step:1150/2330 train_time:70319ms step_avg:61.15ms
step:1151/2330 train_time:70379ms step_avg:61.15ms
step:1152/2330 train_time:70441ms step_avg:61.15ms
step:1153/2330 train_time:70502ms step_avg:61.15ms
step:1154/2330 train_time:70565ms step_avg:61.15ms
step:1155/2330 train_time:70625ms step_avg:61.15ms
step:1156/2330 train_time:70688ms step_avg:61.15ms
step:1157/2330 train_time:70749ms step_avg:61.15ms
step:1158/2330 train_time:70812ms step_avg:61.15ms
step:1159/2330 train_time:70872ms step_avg:61.15ms
step:1160/2330 train_time:70934ms step_avg:61.15ms
step:1161/2330 train_time:70995ms step_avg:61.15ms
step:1162/2330 train_time:71056ms step_avg:61.15ms
step:1163/2330 train_time:71117ms step_avg:61.15ms
step:1164/2330 train_time:71179ms step_avg:61.15ms
step:1165/2330 train_time:71239ms step_avg:61.15ms
step:1166/2330 train_time:71302ms step_avg:61.15ms
step:1167/2330 train_time:71363ms step_avg:61.15ms
step:1168/2330 train_time:71425ms step_avg:61.15ms
step:1169/2330 train_time:71486ms step_avg:61.15ms
step:1170/2330 train_time:71549ms step_avg:61.15ms
step:1171/2330 train_time:71609ms step_avg:61.15ms
step:1172/2330 train_time:71672ms step_avg:61.15ms
step:1173/2330 train_time:71733ms step_avg:61.15ms
step:1174/2330 train_time:71796ms step_avg:61.16ms
step:1175/2330 train_time:71857ms step_avg:61.15ms
step:1176/2330 train_time:71919ms step_avg:61.16ms
step:1177/2330 train_time:71979ms step_avg:61.15ms
step:1178/2330 train_time:72041ms step_avg:61.16ms
step:1179/2330 train_time:72102ms step_avg:61.16ms
step:1180/2330 train_time:72165ms step_avg:61.16ms
step:1181/2330 train_time:72225ms step_avg:61.16ms
step:1182/2330 train_time:72288ms step_avg:61.16ms
step:1183/2330 train_time:72349ms step_avg:61.16ms
step:1184/2330 train_time:72411ms step_avg:61.16ms
step:1185/2330 train_time:72472ms step_avg:61.16ms
step:1186/2330 train_time:72534ms step_avg:61.16ms
step:1187/2330 train_time:72595ms step_avg:61.16ms
step:1188/2330 train_time:72657ms step_avg:61.16ms
step:1189/2330 train_time:72717ms step_avg:61.16ms
step:1190/2330 train_time:72780ms step_avg:61.16ms
step:1191/2330 train_time:72841ms step_avg:61.16ms
step:1192/2330 train_time:72904ms step_avg:61.16ms
step:1193/2330 train_time:72964ms step_avg:61.16ms
step:1194/2330 train_time:73026ms step_avg:61.16ms
step:1195/2330 train_time:73087ms step_avg:61.16ms
step:1196/2330 train_time:73149ms step_avg:61.16ms
step:1197/2330 train_time:73210ms step_avg:61.16ms
step:1198/2330 train_time:73273ms step_avg:61.16ms
step:1199/2330 train_time:73333ms step_avg:61.16ms
step:1200/2330 train_time:73395ms step_avg:61.16ms
step:1201/2330 train_time:73455ms step_avg:61.16ms
step:1202/2330 train_time:73517ms step_avg:61.16ms
step:1203/2330 train_time:73578ms step_avg:61.16ms
step:1204/2330 train_time:73640ms step_avg:61.16ms
step:1205/2330 train_time:73700ms step_avg:61.16ms
step:1206/2330 train_time:73762ms step_avg:61.16ms
step:1207/2330 train_time:73823ms step_avg:61.16ms
step:1208/2330 train_time:73886ms step_avg:61.16ms
step:1209/2330 train_time:73946ms step_avg:61.16ms
step:1210/2330 train_time:74010ms step_avg:61.17ms
step:1211/2330 train_time:74070ms step_avg:61.16ms
step:1212/2330 train_time:74133ms step_avg:61.17ms
step:1213/2330 train_time:74193ms step_avg:61.16ms
step:1214/2330 train_time:74256ms step_avg:61.17ms
step:1215/2330 train_time:74316ms step_avg:61.17ms
step:1216/2330 train_time:74377ms step_avg:61.17ms
step:1217/2330 train_time:74438ms step_avg:61.16ms
step:1218/2330 train_time:74500ms step_avg:61.17ms
step:1219/2330 train_time:74561ms step_avg:61.17ms
step:1220/2330 train_time:74624ms step_avg:61.17ms
step:1221/2330 train_time:74684ms step_avg:61.17ms
step:1222/2330 train_time:74747ms step_avg:61.17ms
step:1223/2330 train_time:74808ms step_avg:61.17ms
step:1224/2330 train_time:74871ms step_avg:61.17ms
step:1225/2330 train_time:74931ms step_avg:61.17ms
step:1226/2330 train_time:74993ms step_avg:61.17ms
step:1227/2330 train_time:75053ms step_avg:61.17ms
step:1228/2330 train_time:75115ms step_avg:61.17ms
step:1229/2330 train_time:75176ms step_avg:61.17ms
step:1230/2330 train_time:75238ms step_avg:61.17ms
step:1231/2330 train_time:75299ms step_avg:61.17ms
step:1232/2330 train_time:75362ms step_avg:61.17ms
step:1233/2330 train_time:75422ms step_avg:61.17ms
step:1234/2330 train_time:75485ms step_avg:61.17ms
step:1235/2330 train_time:75546ms step_avg:61.17ms
step:1236/2330 train_time:75609ms step_avg:61.17ms
step:1237/2330 train_time:75669ms step_avg:61.17ms
step:1238/2330 train_time:75733ms step_avg:61.17ms
step:1239/2330 train_time:75793ms step_avg:61.17ms
step:1240/2330 train_time:75855ms step_avg:61.17ms
step:1241/2330 train_time:75916ms step_avg:61.17ms
step:1242/2330 train_time:75977ms step_avg:61.17ms
step:1243/2330 train_time:76038ms step_avg:61.17ms
step:1244/2330 train_time:76101ms step_avg:61.17ms
step:1245/2330 train_time:76162ms step_avg:61.17ms
step:1246/2330 train_time:76225ms step_avg:61.18ms
step:1247/2330 train_time:76286ms step_avg:61.18ms
step:1248/2330 train_time:76349ms step_avg:61.18ms
step:1249/2330 train_time:76410ms step_avg:61.18ms
step:1250/2330 train_time:76472ms step_avg:61.18ms
step:1250/2330 val_loss:3.5584 train_time:76537ms step_avg:61.23ms
step:1251/2330 train_time:76559ms step_avg:61.20ms
step:1252/2330 train_time:76599ms step_avg:61.18ms
step:1253/2330 train_time:76662ms step_avg:61.18ms
step:1254/2330 train_time:76724ms step_avg:61.18ms
step:1255/2330 train_time:76784ms step_avg:61.18ms
step:1256/2330 train_time:76847ms step_avg:61.18ms
step:1257/2330 train_time:76906ms step_avg:61.18ms
step:1258/2330 train_time:76968ms step_avg:61.18ms
step:1259/2330 train_time:77029ms step_avg:61.18ms
step:1260/2330 train_time:77091ms step_avg:61.18ms
step:1261/2330 train_time:77152ms step_avg:61.18ms
step:1262/2330 train_time:77214ms step_avg:61.18ms
step:1263/2330 train_time:77274ms step_avg:61.18ms
step:1264/2330 train_time:77336ms step_avg:61.18ms
step:1265/2330 train_time:77396ms step_avg:61.18ms
step:1266/2330 train_time:77459ms step_avg:61.18ms
step:1267/2330 train_time:77520ms step_avg:61.18ms
step:1268/2330 train_time:77584ms step_avg:61.19ms
step:1269/2330 train_time:77645ms step_avg:61.19ms
step:1270/2330 train_time:77708ms step_avg:61.19ms
step:1271/2330 train_time:77769ms step_avg:61.19ms
step:1272/2330 train_time:77832ms step_avg:61.19ms
step:1273/2330 train_time:77893ms step_avg:61.19ms
step:1274/2330 train_time:77955ms step_avg:61.19ms
step:1275/2330 train_time:78015ms step_avg:61.19ms
step:1276/2330 train_time:78078ms step_avg:61.19ms
step:1277/2330 train_time:78138ms step_avg:61.19ms
step:1278/2330 train_time:78199ms step_avg:61.19ms
step:1279/2330 train_time:78259ms step_avg:61.19ms
step:1280/2330 train_time:78321ms step_avg:61.19ms
step:1281/2330 train_time:78382ms step_avg:61.19ms
step:1282/2330 train_time:78444ms step_avg:61.19ms
step:1283/2330 train_time:78505ms step_avg:61.19ms
step:1284/2330 train_time:78568ms step_avg:61.19ms
step:1285/2330 train_time:78628ms step_avg:61.19ms
step:1286/2330 train_time:78692ms step_avg:61.19ms
step:1287/2330 train_time:78753ms step_avg:61.19ms
step:1288/2330 train_time:78815ms step_avg:61.19ms
step:1289/2330 train_time:78875ms step_avg:61.19ms
step:1290/2330 train_time:78937ms step_avg:61.19ms
step:1291/2330 train_time:78997ms step_avg:61.19ms
step:1292/2330 train_time:79059ms step_avg:61.19ms
step:1293/2330 train_time:79119ms step_avg:61.19ms
step:1294/2330 train_time:79182ms step_avg:61.19ms
step:1295/2330 train_time:79241ms step_avg:61.19ms
step:1296/2330 train_time:79303ms step_avg:61.19ms
step:1297/2330 train_time:79363ms step_avg:61.19ms
step:1298/2330 train_time:79425ms step_avg:61.19ms
step:1299/2330 train_time:79486ms step_avg:61.19ms
step:1300/2330 train_time:79549ms step_avg:61.19ms
step:1301/2330 train_time:79610ms step_avg:61.19ms
step:1302/2330 train_time:79674ms step_avg:61.19ms
step:1303/2330 train_time:79734ms step_avg:61.19ms
step:1304/2330 train_time:79796ms step_avg:61.19ms
step:1305/2330 train_time:79857ms step_avg:61.19ms
step:1306/2330 train_time:79919ms step_avg:61.19ms
step:1307/2330 train_time:79979ms step_avg:61.19ms
step:1308/2330 train_time:80041ms step_avg:61.19ms
step:1309/2330 train_time:80102ms step_avg:61.19ms
step:1310/2330 train_time:80163ms step_avg:61.19ms
step:1311/2330 train_time:80223ms step_avg:61.19ms
step:1312/2330 train_time:80286ms step_avg:61.19ms
step:1313/2330 train_time:80346ms step_avg:61.19ms
step:1314/2330 train_time:80408ms step_avg:61.19ms
step:1315/2330 train_time:80469ms step_avg:61.19ms
step:1316/2330 train_time:80532ms step_avg:61.19ms
step:1317/2330 train_time:80592ms step_avg:61.19ms
step:1318/2330 train_time:80655ms step_avg:61.20ms
step:1319/2330 train_time:80716ms step_avg:61.19ms
step:1320/2330 train_time:80779ms step_avg:61.20ms
step:1321/2330 train_time:80839ms step_avg:61.20ms
step:1322/2330 train_time:80901ms step_avg:61.20ms
step:1323/2330 train_time:80962ms step_avg:61.20ms
step:1324/2330 train_time:81024ms step_avg:61.20ms
step:1325/2330 train_time:81084ms step_avg:61.20ms
step:1326/2330 train_time:81146ms step_avg:61.20ms
step:1327/2330 train_time:81206ms step_avg:61.19ms
step:1328/2330 train_time:81268ms step_avg:61.20ms
step:1329/2330 train_time:81329ms step_avg:61.20ms
step:1330/2330 train_time:81392ms step_avg:61.20ms
step:1331/2330 train_time:81452ms step_avg:61.20ms
step:1332/2330 train_time:81515ms step_avg:61.20ms
step:1333/2330 train_time:81576ms step_avg:61.20ms
step:1334/2330 train_time:81638ms step_avg:61.20ms
step:1335/2330 train_time:81699ms step_avg:61.20ms
step:1336/2330 train_time:81762ms step_avg:61.20ms
step:1337/2330 train_time:81822ms step_avg:61.20ms
step:1338/2330 train_time:81885ms step_avg:61.20ms
step:1339/2330 train_time:81946ms step_avg:61.20ms
step:1340/2330 train_time:82008ms step_avg:61.20ms
step:1341/2330 train_time:82068ms step_avg:61.20ms
step:1342/2330 train_time:82131ms step_avg:61.20ms
step:1343/2330 train_time:82191ms step_avg:61.20ms
step:1344/2330 train_time:82253ms step_avg:61.20ms
step:1345/2330 train_time:82313ms step_avg:61.20ms
step:1346/2330 train_time:82376ms step_avg:61.20ms
step:1347/2330 train_time:82437ms step_avg:61.20ms
step:1348/2330 train_time:82499ms step_avg:61.20ms
step:1349/2330 train_time:82559ms step_avg:61.20ms
step:1350/2330 train_time:82622ms step_avg:61.20ms
step:1351/2330 train_time:82682ms step_avg:61.20ms
step:1352/2330 train_time:82744ms step_avg:61.20ms
step:1353/2330 train_time:82804ms step_avg:61.20ms
step:1354/2330 train_time:82866ms step_avg:61.20ms
step:1355/2330 train_time:82927ms step_avg:61.20ms
step:1356/2330 train_time:82991ms step_avg:61.20ms
step:1357/2330 train_time:83051ms step_avg:61.20ms
step:1358/2330 train_time:83114ms step_avg:61.20ms
step:1359/2330 train_time:83174ms step_avg:61.20ms
step:1360/2330 train_time:83236ms step_avg:61.20ms
step:1361/2330 train_time:83297ms step_avg:61.20ms
step:1362/2330 train_time:83359ms step_avg:61.20ms
step:1363/2330 train_time:83420ms step_avg:61.20ms
step:1364/2330 train_time:83482ms step_avg:61.20ms
step:1365/2330 train_time:83542ms step_avg:61.20ms
step:1366/2330 train_time:83604ms step_avg:61.20ms
step:1367/2330 train_time:83664ms step_avg:61.20ms
step:1368/2330 train_time:83726ms step_avg:61.20ms
step:1369/2330 train_time:83787ms step_avg:61.20ms
step:1370/2330 train_time:83849ms step_avg:61.20ms
step:1371/2330 train_time:83910ms step_avg:61.20ms
step:1372/2330 train_time:83973ms step_avg:61.21ms
step:1373/2330 train_time:84034ms step_avg:61.20ms
step:1374/2330 train_time:84096ms step_avg:61.21ms
step:1375/2330 train_time:84156ms step_avg:61.20ms
step:1376/2330 train_time:84218ms step_avg:61.21ms
step:1377/2330 train_time:84278ms step_avg:61.20ms
step:1378/2330 train_time:84340ms step_avg:61.20ms
step:1379/2330 train_time:84402ms step_avg:61.20ms
step:1380/2330 train_time:84464ms step_avg:61.21ms
step:1381/2330 train_time:84524ms step_avg:61.20ms
step:1382/2330 train_time:84587ms step_avg:61.21ms
step:1383/2330 train_time:84648ms step_avg:61.21ms
step:1384/2330 train_time:84711ms step_avg:61.21ms
step:1385/2330 train_time:84772ms step_avg:61.21ms
step:1386/2330 train_time:84834ms step_avg:61.21ms
step:1387/2330 train_time:84895ms step_avg:61.21ms
step:1388/2330 train_time:84957ms step_avg:61.21ms
step:1389/2330 train_time:85017ms step_avg:61.21ms
step:1390/2330 train_time:85080ms step_avg:61.21ms
step:1391/2330 train_time:85140ms step_avg:61.21ms
step:1392/2330 train_time:85202ms step_avg:61.21ms
step:1393/2330 train_time:85262ms step_avg:61.21ms
step:1394/2330 train_time:85324ms step_avg:61.21ms
step:1395/2330 train_time:85385ms step_avg:61.21ms
step:1396/2330 train_time:85447ms step_avg:61.21ms
step:1397/2330 train_time:85508ms step_avg:61.21ms
step:1398/2330 train_time:85570ms step_avg:61.21ms
step:1399/2330 train_time:85631ms step_avg:61.21ms
step:1400/2330 train_time:85694ms step_avg:61.21ms
step:1401/2330 train_time:85754ms step_avg:61.21ms
step:1402/2330 train_time:85817ms step_avg:61.21ms
step:1403/2330 train_time:85877ms step_avg:61.21ms
step:1404/2330 train_time:85939ms step_avg:61.21ms
step:1405/2330 train_time:85999ms step_avg:61.21ms
step:1406/2330 train_time:86062ms step_avg:61.21ms
step:1407/2330 train_time:86122ms step_avg:61.21ms
step:1408/2330 train_time:86184ms step_avg:61.21ms
step:1409/2330 train_time:86244ms step_avg:61.21ms
step:1410/2330 train_time:86306ms step_avg:61.21ms
step:1411/2330 train_time:86367ms step_avg:61.21ms
step:1412/2330 train_time:86429ms step_avg:61.21ms
step:1413/2330 train_time:86489ms step_avg:61.21ms
step:1414/2330 train_time:86552ms step_avg:61.21ms
step:1415/2330 train_time:86613ms step_avg:61.21ms
step:1416/2330 train_time:86676ms step_avg:61.21ms
step:1417/2330 train_time:86736ms step_avg:61.21ms
step:1418/2330 train_time:86798ms step_avg:61.21ms
step:1419/2330 train_time:86859ms step_avg:61.21ms
step:1420/2330 train_time:86921ms step_avg:61.21ms
step:1421/2330 train_time:86981ms step_avg:61.21ms
step:1422/2330 train_time:87044ms step_avg:61.21ms
step:1423/2330 train_time:87104ms step_avg:61.21ms
step:1424/2330 train_time:87166ms step_avg:61.21ms
step:1425/2330 train_time:87226ms step_avg:61.21ms
step:1426/2330 train_time:87288ms step_avg:61.21ms
step:1427/2330 train_time:87348ms step_avg:61.21ms
step:1428/2330 train_time:87411ms step_avg:61.21ms
step:1429/2330 train_time:87471ms step_avg:61.21ms
step:1430/2330 train_time:87533ms step_avg:61.21ms
step:1431/2330 train_time:87595ms step_avg:61.21ms
step:1432/2330 train_time:87657ms step_avg:61.21ms
step:1433/2330 train_time:87718ms step_avg:61.21ms
step:1434/2330 train_time:87780ms step_avg:61.21ms
step:1435/2330 train_time:87840ms step_avg:61.21ms
step:1436/2330 train_time:87902ms step_avg:61.21ms
step:1437/2330 train_time:87963ms step_avg:61.21ms
step:1438/2330 train_time:88025ms step_avg:61.21ms
step:1439/2330 train_time:88085ms step_avg:61.21ms
step:1440/2330 train_time:88147ms step_avg:61.21ms
step:1441/2330 train_time:88208ms step_avg:61.21ms
step:1442/2330 train_time:88271ms step_avg:61.21ms
step:1443/2330 train_time:88331ms step_avg:61.21ms
step:1444/2330 train_time:88393ms step_avg:61.21ms
step:1445/2330 train_time:88453ms step_avg:61.21ms
step:1446/2330 train_time:88516ms step_avg:61.21ms
step:1447/2330 train_time:88577ms step_avg:61.21ms
step:1448/2330 train_time:88639ms step_avg:61.21ms
step:1449/2330 train_time:88699ms step_avg:61.21ms
step:1450/2330 train_time:88761ms step_avg:61.21ms
step:1451/2330 train_time:88822ms step_avg:61.21ms
step:1452/2330 train_time:88885ms step_avg:61.22ms
step:1453/2330 train_time:88945ms step_avg:61.21ms
step:1454/2330 train_time:89006ms step_avg:61.21ms
step:1455/2330 train_time:89067ms step_avg:61.21ms
step:1456/2330 train_time:89129ms step_avg:61.21ms
step:1457/2330 train_time:89190ms step_avg:61.21ms
step:1458/2330 train_time:89252ms step_avg:61.22ms
step:1459/2330 train_time:89313ms step_avg:61.22ms
step:1460/2330 train_time:89376ms step_avg:61.22ms
step:1461/2330 train_time:89437ms step_avg:61.22ms
step:1462/2330 train_time:89499ms step_avg:61.22ms
step:1463/2330 train_time:89559ms step_avg:61.22ms
step:1464/2330 train_time:89622ms step_avg:61.22ms
step:1465/2330 train_time:89682ms step_avg:61.22ms
step:1466/2330 train_time:89744ms step_avg:61.22ms
step:1467/2330 train_time:89805ms step_avg:61.22ms
step:1468/2330 train_time:89867ms step_avg:61.22ms
step:1469/2330 train_time:89928ms step_avg:61.22ms
step:1470/2330 train_time:89990ms step_avg:61.22ms
step:1471/2330 train_time:90050ms step_avg:61.22ms
step:1472/2330 train_time:90113ms step_avg:61.22ms
step:1473/2330 train_time:90173ms step_avg:61.22ms
step:1474/2330 train_time:90236ms step_avg:61.22ms
step:1475/2330 train_time:90297ms step_avg:61.22ms
step:1476/2330 train_time:90360ms step_avg:61.22ms
step:1477/2330 train_time:90420ms step_avg:61.22ms
step:1478/2330 train_time:90483ms step_avg:61.22ms
step:1479/2330 train_time:90543ms step_avg:61.22ms
step:1480/2330 train_time:90605ms step_avg:61.22ms
step:1481/2330 train_time:90664ms step_avg:61.22ms
step:1482/2330 train_time:90726ms step_avg:61.22ms
step:1483/2330 train_time:90787ms step_avg:61.22ms
step:1484/2330 train_time:90849ms step_avg:61.22ms
step:1485/2330 train_time:90910ms step_avg:61.22ms
step:1486/2330 train_time:90973ms step_avg:61.22ms
step:1487/2330 train_time:91033ms step_avg:61.22ms
step:1488/2330 train_time:91095ms step_avg:61.22ms
step:1489/2330 train_time:91156ms step_avg:61.22ms
step:1490/2330 train_time:91218ms step_avg:61.22ms
step:1491/2330 train_time:91279ms step_avg:61.22ms
step:1492/2330 train_time:91342ms step_avg:61.22ms
step:1493/2330 train_time:91402ms step_avg:61.22ms
step:1494/2330 train_time:91464ms step_avg:61.22ms
step:1495/2330 train_time:91524ms step_avg:61.22ms
step:1496/2330 train_time:91587ms step_avg:61.22ms
step:1497/2330 train_time:91647ms step_avg:61.22ms
step:1498/2330 train_time:91709ms step_avg:61.22ms
step:1499/2330 train_time:91770ms step_avg:61.22ms
step:1500/2330 train_time:91833ms step_avg:61.22ms
step:1500/2330 val_loss:3.4935 train_time:91897ms step_avg:61.26ms
step:1501/2330 train_time:91920ms step_avg:61.24ms
step:1502/2330 train_time:91958ms step_avg:61.22ms
step:1503/2330 train_time:92023ms step_avg:61.23ms
step:1504/2330 train_time:92088ms step_avg:61.23ms
step:1505/2330 train_time:92148ms step_avg:61.23ms
step:1506/2330 train_time:92211ms step_avg:61.23ms
step:1507/2330 train_time:92271ms step_avg:61.23ms
step:1508/2330 train_time:92332ms step_avg:61.23ms
step:1509/2330 train_time:92391ms step_avg:61.23ms
step:1510/2330 train_time:92453ms step_avg:61.23ms
step:1511/2330 train_time:92513ms step_avg:61.23ms
step:1512/2330 train_time:92575ms step_avg:61.23ms
step:1513/2330 train_time:92635ms step_avg:61.23ms
step:1514/2330 train_time:92697ms step_avg:61.23ms
step:1515/2330 train_time:92756ms step_avg:61.22ms
step:1516/2330 train_time:92819ms step_avg:61.23ms
step:1517/2330 train_time:92880ms step_avg:61.23ms
step:1518/2330 train_time:92945ms step_avg:61.23ms
step:1519/2330 train_time:93007ms step_avg:61.23ms
step:1520/2330 train_time:93070ms step_avg:61.23ms
step:1521/2330 train_time:93131ms step_avg:61.23ms
step:1522/2330 train_time:93193ms step_avg:61.23ms
step:1523/2330 train_time:93254ms step_avg:61.23ms
step:1524/2330 train_time:93316ms step_avg:61.23ms
step:1525/2330 train_time:93375ms step_avg:61.23ms
step:1526/2330 train_time:93437ms step_avg:61.23ms
step:1527/2330 train_time:93497ms step_avg:61.23ms
step:1528/2330 train_time:93559ms step_avg:61.23ms
step:1529/2330 train_time:93619ms step_avg:61.23ms
step:1530/2330 train_time:93682ms step_avg:61.23ms
step:1531/2330 train_time:93743ms step_avg:61.23ms
step:1532/2330 train_time:93806ms step_avg:61.23ms
step:1533/2330 train_time:93867ms step_avg:61.23ms
step:1534/2330 train_time:93930ms step_avg:61.23ms
step:1535/2330 train_time:93993ms step_avg:61.23ms
step:1536/2330 train_time:94056ms step_avg:61.23ms
step:1537/2330 train_time:94116ms step_avg:61.23ms
step:1538/2330 train_time:94179ms step_avg:61.23ms
step:1539/2330 train_time:94241ms step_avg:61.23ms
step:1540/2330 train_time:94305ms step_avg:61.24ms
step:1541/2330 train_time:94366ms step_avg:61.24ms
step:1542/2330 train_time:94429ms step_avg:61.24ms
step:1543/2330 train_time:94489ms step_avg:61.24ms
step:1544/2330 train_time:94552ms step_avg:61.24ms
step:1545/2330 train_time:94612ms step_avg:61.24ms
step:1546/2330 train_time:94675ms step_avg:61.24ms
step:1547/2330 train_time:94736ms step_avg:61.24ms
step:1548/2330 train_time:94798ms step_avg:61.24ms
step:1549/2330 train_time:94859ms step_avg:61.24ms
step:1550/2330 train_time:94922ms step_avg:61.24ms
step:1551/2330 train_time:94983ms step_avg:61.24ms
step:1552/2330 train_time:95047ms step_avg:61.24ms
step:1553/2330 train_time:95109ms step_avg:61.24ms
step:1554/2330 train_time:95172ms step_avg:61.24ms
step:1555/2330 train_time:95233ms step_avg:61.24ms
step:1556/2330 train_time:95296ms step_avg:61.24ms
step:1557/2330 train_time:95357ms step_avg:61.24ms
step:1558/2330 train_time:95420ms step_avg:61.25ms
step:1559/2330 train_time:95481ms step_avg:61.24ms
step:1560/2330 train_time:95544ms step_avg:61.25ms
step:1561/2330 train_time:95605ms step_avg:61.25ms
step:1562/2330 train_time:95668ms step_avg:61.25ms
step:1563/2330 train_time:95729ms step_avg:61.25ms
step:1564/2330 train_time:95792ms step_avg:61.25ms
step:1565/2330 train_time:95853ms step_avg:61.25ms
step:1566/2330 train_time:95916ms step_avg:61.25ms
step:1567/2330 train_time:95976ms step_avg:61.25ms
step:1568/2330 train_time:96040ms step_avg:61.25ms
step:1569/2330 train_time:96101ms step_avg:61.25ms
step:1570/2330 train_time:96165ms step_avg:61.25ms
step:1571/2330 train_time:96226ms step_avg:61.25ms
step:1572/2330 train_time:96289ms step_avg:61.25ms
step:1573/2330 train_time:96350ms step_avg:61.25ms
step:1574/2330 train_time:96413ms step_avg:61.25ms
step:1575/2330 train_time:96474ms step_avg:61.25ms
step:1576/2330 train_time:96536ms step_avg:61.25ms
step:1577/2330 train_time:96596ms step_avg:61.25ms
step:1578/2330 train_time:96660ms step_avg:61.25ms
step:1579/2330 train_time:96721ms step_avg:61.25ms
step:1580/2330 train_time:96785ms step_avg:61.26ms
step:1581/2330 train_time:96847ms step_avg:61.26ms
step:1582/2330 train_time:96911ms step_avg:61.26ms
step:1583/2330 train_time:96971ms step_avg:61.26ms
step:1584/2330 train_time:97035ms step_avg:61.26ms
step:1585/2330 train_time:97095ms step_avg:61.26ms
step:1586/2330 train_time:97158ms step_avg:61.26ms
step:1587/2330 train_time:97219ms step_avg:61.26ms
step:1588/2330 train_time:97282ms step_avg:61.26ms
step:1589/2330 train_time:97344ms step_avg:61.26ms
step:1590/2330 train_time:97408ms step_avg:61.26ms
step:1591/2330 train_time:97469ms step_avg:61.26ms
step:1592/2330 train_time:97532ms step_avg:61.26ms
step:1593/2330 train_time:97593ms step_avg:61.26ms
step:1594/2330 train_time:97656ms step_avg:61.26ms
step:1595/2330 train_time:97717ms step_avg:61.26ms
step:1596/2330 train_time:97780ms step_avg:61.27ms
step:1597/2330 train_time:97841ms step_avg:61.27ms
step:1598/2330 train_time:97905ms step_avg:61.27ms
step:1599/2330 train_time:97967ms step_avg:61.27ms
step:1600/2330 train_time:98030ms step_avg:61.27ms
step:1601/2330 train_time:98091ms step_avg:61.27ms
step:1602/2330 train_time:98154ms step_avg:61.27ms
step:1603/2330 train_time:98214ms step_avg:61.27ms
step:1604/2330 train_time:98277ms step_avg:61.27ms
step:1605/2330 train_time:98337ms step_avg:61.27ms
step:1606/2330 train_time:98400ms step_avg:61.27ms
step:1607/2330 train_time:98461ms step_avg:61.27ms
step:1608/2330 train_time:98524ms step_avg:61.27ms
step:1609/2330 train_time:98585ms step_avg:61.27ms
step:1610/2330 train_time:98648ms step_avg:61.27ms
step:1611/2330 train_time:98709ms step_avg:61.27ms
step:1612/2330 train_time:98773ms step_avg:61.27ms
step:1613/2330 train_time:98833ms step_avg:61.27ms
step:1614/2330 train_time:98896ms step_avg:61.27ms
step:1615/2330 train_time:98957ms step_avg:61.27ms
step:1616/2330 train_time:99020ms step_avg:61.27ms
step:1617/2330 train_time:99081ms step_avg:61.27ms
step:1618/2330 train_time:99145ms step_avg:61.28ms
step:1619/2330 train_time:99206ms step_avg:61.28ms
step:1620/2330 train_time:99268ms step_avg:61.28ms
step:1621/2330 train_time:99329ms step_avg:61.28ms
step:1622/2330 train_time:99393ms step_avg:61.28ms
step:1623/2330 train_time:99453ms step_avg:61.28ms
step:1624/2330 train_time:99516ms step_avg:61.28ms
step:1625/2330 train_time:99576ms step_avg:61.28ms
step:1626/2330 train_time:99639ms step_avg:61.28ms
step:1627/2330 train_time:99701ms step_avg:61.28ms
step:1628/2330 train_time:99763ms step_avg:61.28ms
step:1629/2330 train_time:99824ms step_avg:61.28ms
step:1630/2330 train_time:99887ms step_avg:61.28ms
step:1631/2330 train_time:99949ms step_avg:61.28ms
step:1632/2330 train_time:100013ms step_avg:61.28ms
step:1633/2330 train_time:100074ms step_avg:61.28ms
step:1634/2330 train_time:100136ms step_avg:61.28ms
step:1635/2330 train_time:100197ms step_avg:61.28ms
step:1636/2330 train_time:100259ms step_avg:61.28ms
step:1637/2330 train_time:100320ms step_avg:61.28ms
step:1638/2330 train_time:100383ms step_avg:61.28ms
step:1639/2330 train_time:100444ms step_avg:61.28ms
step:1640/2330 train_time:100508ms step_avg:61.29ms
step:1641/2330 train_time:100569ms step_avg:61.28ms
step:1642/2330 train_time:100632ms step_avg:61.29ms
step:1643/2330 train_time:100694ms step_avg:61.29ms
step:1644/2330 train_time:100756ms step_avg:61.29ms
step:1645/2330 train_time:100818ms step_avg:61.29ms
step:1646/2330 train_time:100881ms step_avg:61.29ms
step:1647/2330 train_time:100942ms step_avg:61.29ms
step:1648/2330 train_time:101006ms step_avg:61.29ms
step:1649/2330 train_time:101066ms step_avg:61.29ms
step:1650/2330 train_time:101129ms step_avg:61.29ms
step:1651/2330 train_time:101191ms step_avg:61.29ms
step:1652/2330 train_time:101254ms step_avg:61.29ms
step:1653/2330 train_time:101315ms step_avg:61.29ms
step:1654/2330 train_time:101377ms step_avg:61.29ms
step:1655/2330 train_time:101438ms step_avg:61.29ms
step:1656/2330 train_time:101501ms step_avg:61.29ms
step:1657/2330 train_time:101563ms step_avg:61.29ms
step:1658/2330 train_time:101625ms step_avg:61.29ms
step:1659/2330 train_time:101687ms step_avg:61.29ms
step:1660/2330 train_time:101750ms step_avg:61.30ms
step:1661/2330 train_time:101811ms step_avg:61.30ms
step:1662/2330 train_time:101874ms step_avg:61.30ms
step:1663/2330 train_time:101935ms step_avg:61.30ms
step:1664/2330 train_time:101996ms step_avg:61.30ms
step:1665/2330 train_time:102057ms step_avg:61.30ms
step:1666/2330 train_time:102120ms step_avg:61.30ms
step:1667/2330 train_time:102182ms step_avg:61.30ms
step:1668/2330 train_time:102245ms step_avg:61.30ms
step:1669/2330 train_time:102307ms step_avg:61.30ms
step:1670/2330 train_time:102370ms step_avg:61.30ms
step:1671/2330 train_time:102430ms step_avg:61.30ms
step:1672/2330 train_time:102493ms step_avg:61.30ms
step:1673/2330 train_time:102554ms step_avg:61.30ms
step:1674/2330 train_time:102617ms step_avg:61.30ms
step:1675/2330 train_time:102677ms step_avg:61.30ms
step:1676/2330 train_time:102740ms step_avg:61.30ms
step:1677/2330 train_time:102800ms step_avg:61.30ms
step:1678/2330 train_time:102864ms step_avg:61.30ms
step:1679/2330 train_time:102924ms step_avg:61.30ms
step:1680/2330 train_time:102988ms step_avg:61.30ms
step:1681/2330 train_time:103049ms step_avg:61.30ms
step:1682/2330 train_time:103112ms step_avg:61.30ms
step:1683/2330 train_time:103172ms step_avg:61.30ms
step:1684/2330 train_time:103235ms step_avg:61.30ms
step:1685/2330 train_time:103296ms step_avg:61.30ms
step:1686/2330 train_time:103358ms step_avg:61.30ms
step:1687/2330 train_time:103419ms step_avg:61.30ms
step:1688/2330 train_time:103482ms step_avg:61.30ms
step:1689/2330 train_time:103544ms step_avg:61.31ms
step:1690/2330 train_time:103608ms step_avg:61.31ms
step:1691/2330 train_time:103669ms step_avg:61.31ms
step:1692/2330 train_time:103732ms step_avg:61.31ms
step:1693/2330 train_time:103793ms step_avg:61.31ms
step:1694/2330 train_time:103856ms step_avg:61.31ms
step:1695/2330 train_time:103916ms step_avg:61.31ms
step:1696/2330 train_time:103979ms step_avg:61.31ms
step:1697/2330 train_time:104041ms step_avg:61.31ms
step:1698/2330 train_time:104105ms step_avg:61.31ms
step:1699/2330 train_time:104166ms step_avg:61.31ms
step:1700/2330 train_time:104230ms step_avg:61.31ms
step:1701/2330 train_time:104291ms step_avg:61.31ms
step:1702/2330 train_time:104353ms step_avg:61.31ms
step:1703/2330 train_time:104414ms step_avg:61.31ms
step:1704/2330 train_time:104477ms step_avg:61.31ms
step:1705/2330 train_time:104538ms step_avg:61.31ms
step:1706/2330 train_time:104601ms step_avg:61.31ms
step:1707/2330 train_time:104662ms step_avg:61.31ms
step:1708/2330 train_time:104725ms step_avg:61.31ms
step:1709/2330 train_time:104786ms step_avg:61.31ms
step:1710/2330 train_time:104849ms step_avg:61.32ms
step:1711/2330 train_time:104910ms step_avg:61.32ms
step:1712/2330 train_time:104974ms step_avg:61.32ms
step:1713/2330 train_time:105034ms step_avg:61.32ms
step:1714/2330 train_time:105098ms step_avg:61.32ms
step:1715/2330 train_time:105158ms step_avg:61.32ms
step:1716/2330 train_time:105222ms step_avg:61.32ms
step:1717/2330 train_time:105283ms step_avg:61.32ms
step:1718/2330 train_time:105346ms step_avg:61.32ms
step:1719/2330 train_time:105407ms step_avg:61.32ms
step:1720/2330 train_time:105470ms step_avg:61.32ms
step:1721/2330 train_time:105531ms step_avg:61.32ms
step:1722/2330 train_time:105594ms step_avg:61.32ms
step:1723/2330 train_time:105655ms step_avg:61.32ms
step:1724/2330 train_time:105718ms step_avg:61.32ms
step:1725/2330 train_time:105778ms step_avg:61.32ms
step:1726/2330 train_time:105842ms step_avg:61.32ms
step:1727/2330 train_time:105903ms step_avg:61.32ms
step:1728/2330 train_time:105966ms step_avg:61.32ms
step:1729/2330 train_time:106027ms step_avg:61.32ms
step:1730/2330 train_time:106090ms step_avg:61.32ms
step:1731/2330 train_time:106151ms step_avg:61.32ms
step:1732/2330 train_time:106216ms step_avg:61.33ms
step:1733/2330 train_time:106275ms step_avg:61.32ms
step:1734/2330 train_time:106339ms step_avg:61.33ms
step:1735/2330 train_time:106399ms step_avg:61.33ms
step:1736/2330 train_time:106462ms step_avg:61.33ms
step:1737/2330 train_time:106525ms step_avg:61.33ms
step:1738/2330 train_time:106588ms step_avg:61.33ms
step:1739/2330 train_time:106649ms step_avg:61.33ms
step:1740/2330 train_time:106713ms step_avg:61.33ms
step:1741/2330 train_time:106774ms step_avg:61.33ms
step:1742/2330 train_time:106836ms step_avg:61.33ms
step:1743/2330 train_time:106896ms step_avg:61.33ms
step:1744/2330 train_time:106959ms step_avg:61.33ms
step:1745/2330 train_time:107020ms step_avg:61.33ms
step:1746/2330 train_time:107084ms step_avg:61.33ms
step:1747/2330 train_time:107146ms step_avg:61.33ms
step:1748/2330 train_time:107209ms step_avg:61.33ms
step:1749/2330 train_time:107269ms step_avg:61.33ms
step:1750/2330 train_time:107332ms step_avg:61.33ms
step:1750/2330 val_loss:3.4235 train_time:107397ms step_avg:61.37ms
step:1751/2330 train_time:107420ms step_avg:61.35ms
step:1752/2330 train_time:107458ms step_avg:61.33ms
step:1753/2330 train_time:107522ms step_avg:61.34ms
step:1754/2330 train_time:107588ms step_avg:61.34ms
step:1755/2330 train_time:107649ms step_avg:61.34ms
step:1756/2330 train_time:107713ms step_avg:61.34ms
step:1757/2330 train_time:107772ms step_avg:61.34ms
step:1758/2330 train_time:107834ms step_avg:61.34ms
step:1759/2330 train_time:107894ms step_avg:61.34ms
step:1760/2330 train_time:107956ms step_avg:61.34ms
step:1761/2330 train_time:108015ms step_avg:61.34ms
step:1762/2330 train_time:108077ms step_avg:61.34ms
step:1763/2330 train_time:108137ms step_avg:61.34ms
step:1764/2330 train_time:108199ms step_avg:61.34ms
step:1765/2330 train_time:108260ms step_avg:61.34ms
step:1766/2330 train_time:108327ms step_avg:61.34ms
step:1767/2330 train_time:108391ms step_avg:61.34ms
step:1768/2330 train_time:108456ms step_avg:61.34ms
step:1769/2330 train_time:108519ms step_avg:61.34ms
step:1770/2330 train_time:108582ms step_avg:61.35ms
step:1771/2330 train_time:108643ms step_avg:61.35ms
step:1772/2330 train_time:108706ms step_avg:61.35ms
step:1773/2330 train_time:108767ms step_avg:61.35ms
step:1774/2330 train_time:108829ms step_avg:61.35ms
step:1775/2330 train_time:108890ms step_avg:61.35ms
step:1776/2330 train_time:108952ms step_avg:61.35ms
step:1777/2330 train_time:109013ms step_avg:61.35ms
step:1778/2330 train_time:109075ms step_avg:61.35ms
step:1779/2330 train_time:109135ms step_avg:61.35ms
step:1780/2330 train_time:109198ms step_avg:61.35ms
step:1781/2330 train_time:109259ms step_avg:61.35ms
step:1782/2330 train_time:109323ms step_avg:61.35ms
step:1783/2330 train_time:109385ms step_avg:61.35ms
step:1784/2330 train_time:109450ms step_avg:61.35ms
step:1785/2330 train_time:109513ms step_avg:61.35ms
step:1786/2330 train_time:109577ms step_avg:61.35ms
step:1787/2330 train_time:109638ms step_avg:61.35ms
step:1788/2330 train_time:109702ms step_avg:61.35ms
step:1789/2330 train_time:109762ms step_avg:61.35ms
step:1790/2330 train_time:109824ms step_avg:61.35ms
step:1791/2330 train_time:109885ms step_avg:61.35ms
step:1792/2330 train_time:109947ms step_avg:61.35ms
step:1793/2330 train_time:110008ms step_avg:61.35ms
step:1794/2330 train_time:110070ms step_avg:61.35ms
step:1795/2330 train_time:110131ms step_avg:61.35ms
step:1796/2330 train_time:110194ms step_avg:61.36ms
step:1797/2330 train_time:110255ms step_avg:61.36ms
step:1798/2330 train_time:110318ms step_avg:61.36ms
step:1799/2330 train_time:110380ms step_avg:61.36ms
step:1800/2330 train_time:110444ms step_avg:61.36ms
step:1801/2330 train_time:110505ms step_avg:61.36ms
step:1802/2330 train_time:110569ms step_avg:61.36ms
step:1803/2330 train_time:110631ms step_avg:61.36ms
step:1804/2330 train_time:110694ms step_avg:61.36ms
step:1805/2330 train_time:110754ms step_avg:61.36ms
step:1806/2330 train_time:110817ms step_avg:61.36ms
step:1807/2330 train_time:110878ms step_avg:61.36ms
step:1808/2330 train_time:110940ms step_avg:61.36ms
step:1809/2330 train_time:111001ms step_avg:61.36ms
step:1810/2330 train_time:111064ms step_avg:61.36ms
step:1811/2330 train_time:111125ms step_avg:61.36ms
step:1812/2330 train_time:111188ms step_avg:61.36ms
step:1813/2330 train_time:111249ms step_avg:61.36ms
step:1814/2330 train_time:111313ms step_avg:61.36ms
step:1815/2330 train_time:111374ms step_avg:61.36ms
step:1816/2330 train_time:111438ms step_avg:61.36ms
step:1817/2330 train_time:111499ms step_avg:61.36ms
step:1818/2330 train_time:111562ms step_avg:61.37ms
step:1819/2330 train_time:111623ms step_avg:61.37ms
step:1820/2330 train_time:111686ms step_avg:61.37ms
step:1821/2330 train_time:111748ms step_avg:61.37ms
step:1822/2330 train_time:111811ms step_avg:61.37ms
step:1823/2330 train_time:111872ms step_avg:61.37ms
step:1824/2330 train_time:111935ms step_avg:61.37ms
step:1825/2330 train_time:111997ms step_avg:61.37ms
step:1826/2330 train_time:112059ms step_avg:61.37ms
step:1827/2330 train_time:112120ms step_avg:61.37ms
step:1828/2330 train_time:112182ms step_avg:61.37ms
step:1829/2330 train_time:112243ms step_avg:61.37ms
step:1830/2330 train_time:112305ms step_avg:61.37ms
step:1831/2330 train_time:112367ms step_avg:61.37ms
step:1832/2330 train_time:112431ms step_avg:61.37ms
step:1833/2330 train_time:112492ms step_avg:61.37ms
step:1834/2330 train_time:112555ms step_avg:61.37ms
step:1835/2330 train_time:112616ms step_avg:61.37ms
step:1836/2330 train_time:112679ms step_avg:61.37ms
step:1837/2330 train_time:112739ms step_avg:61.37ms
step:1838/2330 train_time:112802ms step_avg:61.37ms
step:1839/2330 train_time:112863ms step_avg:61.37ms
step:1840/2330 train_time:112927ms step_avg:61.37ms
step:1841/2330 train_time:112987ms step_avg:61.37ms
step:1842/2330 train_time:113050ms step_avg:61.37ms
step:1843/2330 train_time:113111ms step_avg:61.37ms
step:1844/2330 train_time:113174ms step_avg:61.37ms
step:1845/2330 train_time:113235ms step_avg:61.37ms
step:1846/2330 train_time:113298ms step_avg:61.37ms
step:1847/2330 train_time:113359ms step_avg:61.37ms
step:1848/2330 train_time:113422ms step_avg:61.38ms
step:1849/2330 train_time:113483ms step_avg:61.38ms
step:1850/2330 train_time:113547ms step_avg:61.38ms
step:1851/2330 train_time:113608ms step_avg:61.38ms
step:1852/2330 train_time:113671ms step_avg:61.38ms
step:1853/2330 train_time:113732ms step_avg:61.38ms
step:1854/2330 train_time:113795ms step_avg:61.38ms
step:1855/2330 train_time:113857ms step_avg:61.38ms
step:1856/2330 train_time:113920ms step_avg:61.38ms
step:1857/2330 train_time:113980ms step_avg:61.38ms
step:1858/2330 train_time:114043ms step_avg:61.38ms
step:1859/2330 train_time:114104ms step_avg:61.38ms
step:1860/2330 train_time:114167ms step_avg:61.38ms
step:1861/2330 train_time:114228ms step_avg:61.38ms
step:1862/2330 train_time:114291ms step_avg:61.38ms
step:1863/2330 train_time:114352ms step_avg:61.38ms
step:1864/2330 train_time:114416ms step_avg:61.38ms
step:1865/2330 train_time:114478ms step_avg:61.38ms
step:1866/2330 train_time:114541ms step_avg:61.38ms
step:1867/2330 train_time:114602ms step_avg:61.38ms
step:1868/2330 train_time:114665ms step_avg:61.38ms
step:1869/2330 train_time:114727ms step_avg:61.38ms
step:1870/2330 train_time:114790ms step_avg:61.39ms
step:1871/2330 train_time:114852ms step_avg:61.39ms
step:1872/2330 train_time:114915ms step_avg:61.39ms
step:1873/2330 train_time:114976ms step_avg:61.39ms
step:1874/2330 train_time:115039ms step_avg:61.39ms
step:1875/2330 train_time:115100ms step_avg:61.39ms
step:1876/2330 train_time:115162ms step_avg:61.39ms
step:1877/2330 train_time:115223ms step_avg:61.39ms
step:1878/2330 train_time:115286ms step_avg:61.39ms
step:1879/2330 train_time:115347ms step_avg:61.39ms
step:1880/2330 train_time:115411ms step_avg:61.39ms
step:1881/2330 train_time:115473ms step_avg:61.39ms
step:1882/2330 train_time:115537ms step_avg:61.39ms
step:1883/2330 train_time:115598ms step_avg:61.39ms
step:1884/2330 train_time:115661ms step_avg:61.39ms
step:1885/2330 train_time:115721ms step_avg:61.39ms
step:1886/2330 train_time:115784ms step_avg:61.39ms
step:1887/2330 train_time:115845ms step_avg:61.39ms
step:1888/2330 train_time:115909ms step_avg:61.39ms
step:1889/2330 train_time:115970ms step_avg:61.39ms
step:1890/2330 train_time:116033ms step_avg:61.39ms
step:1891/2330 train_time:116095ms step_avg:61.39ms
step:1892/2330 train_time:116158ms step_avg:61.39ms
step:1893/2330 train_time:116219ms step_avg:61.39ms
step:1894/2330 train_time:116281ms step_avg:61.39ms
step:1895/2330 train_time:116342ms step_avg:61.39ms
step:1896/2330 train_time:116405ms step_avg:61.39ms
step:1897/2330 train_time:116467ms step_avg:61.40ms
step:1898/2330 train_time:116530ms step_avg:61.40ms
step:1899/2330 train_time:116591ms step_avg:61.40ms
step:1900/2330 train_time:116655ms step_avg:61.40ms
step:1901/2330 train_time:116716ms step_avg:61.40ms
step:1902/2330 train_time:116778ms step_avg:61.40ms
step:1903/2330 train_time:116839ms step_avg:61.40ms
step:1904/2330 train_time:116902ms step_avg:61.40ms
step:1905/2330 train_time:116963ms step_avg:61.40ms
step:1906/2330 train_time:117026ms step_avg:61.40ms
step:1907/2330 train_time:117087ms step_avg:61.40ms
step:1908/2330 train_time:117150ms step_avg:61.40ms
step:1909/2330 train_time:117211ms step_avg:61.40ms
step:1910/2330 train_time:117274ms step_avg:61.40ms
step:1911/2330 train_time:117336ms step_avg:61.40ms
step:1912/2330 train_time:117399ms step_avg:61.40ms
step:1913/2330 train_time:117459ms step_avg:61.40ms
step:1914/2330 train_time:117522ms step_avg:61.40ms
step:1915/2330 train_time:117583ms step_avg:61.40ms
step:1916/2330 train_time:117646ms step_avg:61.40ms
step:1917/2330 train_time:117707ms step_avg:61.40ms
step:1918/2330 train_time:117770ms step_avg:61.40ms
step:1919/2330 train_time:117832ms step_avg:61.40ms
step:1920/2330 train_time:117895ms step_avg:61.40ms
step:1921/2330 train_time:117956ms step_avg:61.40ms
step:1922/2330 train_time:118019ms step_avg:61.40ms
step:1923/2330 train_time:118080ms step_avg:61.40ms
step:1924/2330 train_time:118143ms step_avg:61.40ms
step:1925/2330 train_time:118204ms step_avg:61.40ms
step:1926/2330 train_time:118266ms step_avg:61.41ms
step:1927/2330 train_time:118327ms step_avg:61.40ms
step:1928/2330 train_time:118391ms step_avg:61.41ms
step:1929/2330 train_time:118452ms step_avg:61.41ms
step:1930/2330 train_time:118515ms step_avg:61.41ms
step:1931/2330 train_time:118576ms step_avg:61.41ms
step:1932/2330 train_time:118639ms step_avg:61.41ms
step:1933/2330 train_time:118700ms step_avg:61.41ms
step:1934/2330 train_time:118763ms step_avg:61.41ms
step:1935/2330 train_time:118824ms step_avg:61.41ms
step:1936/2330 train_time:118888ms step_avg:61.41ms
step:1937/2330 train_time:118948ms step_avg:61.41ms
step:1938/2330 train_time:119012ms step_avg:61.41ms
step:1939/2330 train_time:119073ms step_avg:61.41ms
step:1940/2330 train_time:119136ms step_avg:61.41ms
step:1941/2330 train_time:119198ms step_avg:61.41ms
step:1942/2330 train_time:119261ms step_avg:61.41ms
step:1943/2330 train_time:119321ms step_avg:61.41ms
step:1944/2330 train_time:119384ms step_avg:61.41ms
step:1945/2330 train_time:119446ms step_avg:61.41ms
step:1946/2330 train_time:119510ms step_avg:61.41ms
step:1947/2330 train_time:119571ms step_avg:61.41ms
step:1948/2330 train_time:119634ms step_avg:61.41ms
step:1949/2330 train_time:119695ms step_avg:61.41ms
step:1950/2330 train_time:119758ms step_avg:61.41ms
step:1951/2330 train_time:119820ms step_avg:61.41ms
step:1952/2330 train_time:119883ms step_avg:61.42ms
step:1953/2330 train_time:119943ms step_avg:61.41ms
step:1954/2330 train_time:120006ms step_avg:61.42ms
step:1955/2330 train_time:120068ms step_avg:61.42ms
step:1956/2330 train_time:120132ms step_avg:61.42ms
step:1957/2330 train_time:120194ms step_avg:61.42ms
step:1958/2330 train_time:120257ms step_avg:61.42ms
step:1959/2330 train_time:120318ms step_avg:61.42ms
step:1960/2330 train_time:120382ms step_avg:61.42ms
step:1961/2330 train_time:120442ms step_avg:61.42ms
step:1962/2330 train_time:120505ms step_avg:61.42ms
step:1963/2330 train_time:120566ms step_avg:61.42ms
step:1964/2330 train_time:120629ms step_avg:61.42ms
step:1965/2330 train_time:120691ms step_avg:61.42ms
step:1966/2330 train_time:120754ms step_avg:61.42ms
step:1967/2330 train_time:120816ms step_avg:61.42ms
step:1968/2330 train_time:120879ms step_avg:61.42ms
step:1969/2330 train_time:120939ms step_avg:61.42ms
step:1970/2330 train_time:121002ms step_avg:61.42ms
step:1971/2330 train_time:121063ms step_avg:61.42ms
step:1972/2330 train_time:121126ms step_avg:61.42ms
step:1973/2330 train_time:121188ms step_avg:61.42ms
step:1974/2330 train_time:121252ms step_avg:61.42ms
step:1975/2330 train_time:121313ms step_avg:61.42ms
step:1976/2330 train_time:121376ms step_avg:61.43ms
step:1977/2330 train_time:121437ms step_avg:61.42ms
step:1978/2330 train_time:121501ms step_avg:61.43ms
step:1979/2330 train_time:121562ms step_avg:61.43ms
step:1980/2330 train_time:121626ms step_avg:61.43ms
step:1981/2330 train_time:121686ms step_avg:61.43ms
step:1982/2330 train_time:121749ms step_avg:61.43ms
step:1983/2330 train_time:121811ms step_avg:61.43ms
step:1984/2330 train_time:121873ms step_avg:61.43ms
step:1985/2330 train_time:121934ms step_avg:61.43ms
step:1986/2330 train_time:121998ms step_avg:61.43ms
step:1987/2330 train_time:122059ms step_avg:61.43ms
step:1988/2330 train_time:122122ms step_avg:61.43ms
step:1989/2330 train_time:122182ms step_avg:61.43ms
step:1990/2330 train_time:122245ms step_avg:61.43ms
step:1991/2330 train_time:122306ms step_avg:61.43ms
step:1992/2330 train_time:122370ms step_avg:61.43ms
step:1993/2330 train_time:122432ms step_avg:61.43ms
step:1994/2330 train_time:122495ms step_avg:61.43ms
step:1995/2330 train_time:122557ms step_avg:61.43ms
step:1996/2330 train_time:122620ms step_avg:61.43ms
step:1997/2330 train_time:122681ms step_avg:61.43ms
step:1998/2330 train_time:122743ms step_avg:61.43ms
step:1999/2330 train_time:122804ms step_avg:61.43ms
step:2000/2330 train_time:122868ms step_avg:61.43ms
step:2000/2330 val_loss:3.3747 train_time:122933ms step_avg:61.47ms
step:2001/2330 train_time:122955ms step_avg:61.45ms
step:2002/2330 train_time:122994ms step_avg:61.44ms
step:2003/2330 train_time:123063ms step_avg:61.44ms
step:2004/2330 train_time:123130ms step_avg:61.44ms
step:2005/2330 train_time:123192ms step_avg:61.44ms
step:2006/2330 train_time:123255ms step_avg:61.44ms
step:2007/2330 train_time:123316ms step_avg:61.44ms
step:2008/2330 train_time:123378ms step_avg:61.44ms
step:2009/2330 train_time:123439ms step_avg:61.44ms
step:2010/2330 train_time:123501ms step_avg:61.44ms
step:2011/2330 train_time:123561ms step_avg:61.44ms
step:2012/2330 train_time:123623ms step_avg:61.44ms
step:2013/2330 train_time:123684ms step_avg:61.44ms
step:2014/2330 train_time:123745ms step_avg:61.44ms
step:2015/2330 train_time:123805ms step_avg:61.44ms
step:2016/2330 train_time:123868ms step_avg:61.44ms
step:2017/2330 train_time:123930ms step_avg:61.44ms
step:2018/2330 train_time:123994ms step_avg:61.44ms
step:2019/2330 train_time:124056ms step_avg:61.44ms
step:2020/2330 train_time:124121ms step_avg:61.45ms
step:2021/2330 train_time:124183ms step_avg:61.45ms
step:2022/2330 train_time:124246ms step_avg:61.45ms
step:2023/2330 train_time:124307ms step_avg:61.45ms
step:2024/2330 train_time:124370ms step_avg:61.45ms
step:2025/2330 train_time:124431ms step_avg:61.45ms
step:2026/2330 train_time:124493ms step_avg:61.45ms
step:2027/2330 train_time:124554ms step_avg:61.45ms
step:2028/2330 train_time:124617ms step_avg:61.45ms
step:2029/2330 train_time:124677ms step_avg:61.45ms
step:2030/2330 train_time:124739ms step_avg:61.45ms
step:2031/2330 train_time:124800ms step_avg:61.45ms
step:2032/2330 train_time:124864ms step_avg:61.45ms
step:2033/2330 train_time:124925ms step_avg:61.45ms
step:2034/2330 train_time:124988ms step_avg:61.45ms
step:2035/2330 train_time:125050ms step_avg:61.45ms
step:2036/2330 train_time:125114ms step_avg:61.45ms
step:2037/2330 train_time:125175ms step_avg:61.45ms
step:2038/2330 train_time:125239ms step_avg:61.45ms
step:2039/2330 train_time:125300ms step_avg:61.45ms
step:2040/2330 train_time:125363ms step_avg:61.45ms
step:2041/2330 train_time:125424ms step_avg:61.45ms
step:2042/2330 train_time:125486ms step_avg:61.45ms
step:2043/2330 train_time:125546ms step_avg:61.45ms
step:2044/2330 train_time:125609ms step_avg:61.45ms
step:2045/2330 train_time:125670ms step_avg:61.45ms
step:2046/2330 train_time:125733ms step_avg:61.45ms
step:2047/2330 train_time:125794ms step_avg:61.45ms
step:2048/2330 train_time:125857ms step_avg:61.45ms
step:2049/2330 train_time:125918ms step_avg:61.45ms
step:2050/2330 train_time:125982ms step_avg:61.45ms
step:2051/2330 train_time:126045ms step_avg:61.46ms
step:2052/2330 train_time:126108ms step_avg:61.46ms
step:2053/2330 train_time:126170ms step_avg:61.46ms
step:2054/2330 train_time:126233ms step_avg:61.46ms
step:2055/2330 train_time:126293ms step_avg:61.46ms
step:2056/2330 train_time:126357ms step_avg:61.46ms
step:2057/2330 train_time:126419ms step_avg:61.46ms
step:2058/2330 train_time:126482ms step_avg:61.46ms
step:2059/2330 train_time:126544ms step_avg:61.46ms
step:2060/2330 train_time:126607ms step_avg:61.46ms
step:2061/2330 train_time:126668ms step_avg:61.46ms
step:2062/2330 train_time:126730ms step_avg:61.46ms
step:2063/2330 train_time:126791ms step_avg:61.46ms
step:2064/2330 train_time:126854ms step_avg:61.46ms
step:2065/2330 train_time:126915ms step_avg:61.46ms
step:2066/2330 train_time:126978ms step_avg:61.46ms
step:2067/2330 train_time:127040ms step_avg:61.46ms
step:2068/2330 train_time:127104ms step_avg:61.46ms
step:2069/2330 train_time:127165ms step_avg:61.46ms
step:2070/2330 train_time:127229ms step_avg:61.46ms
step:2071/2330 train_time:127289ms step_avg:61.46ms
step:2072/2330 train_time:127353ms step_avg:61.46ms
step:2073/2330 train_time:127413ms step_avg:61.46ms
step:2074/2330 train_time:127477ms step_avg:61.46ms
step:2075/2330 train_time:127538ms step_avg:61.46ms
step:2076/2330 train_time:127601ms step_avg:61.46ms
step:2077/2330 train_time:127662ms step_avg:61.46ms
step:2078/2330 train_time:127725ms step_avg:61.47ms
step:2079/2330 train_time:127785ms step_avg:61.46ms
step:2080/2330 train_time:127847ms step_avg:61.46ms
step:2081/2330 train_time:127908ms step_avg:61.46ms
step:2082/2330 train_time:127971ms step_avg:61.47ms
step:2083/2330 train_time:128032ms step_avg:61.47ms
step:2084/2330 train_time:128095ms step_avg:61.47ms
step:2085/2330 train_time:128157ms step_avg:61.47ms
step:2086/2330 train_time:128221ms step_avg:61.47ms
step:2087/2330 train_time:128283ms step_avg:61.47ms
step:2088/2330 train_time:128346ms step_avg:61.47ms
step:2089/2330 train_time:128407ms step_avg:61.47ms
step:2090/2330 train_time:128470ms step_avg:61.47ms
step:2091/2330 train_time:128532ms step_avg:61.47ms
step:2092/2330 train_time:128596ms step_avg:61.47ms
step:2093/2330 train_time:128657ms step_avg:61.47ms
step:2094/2330 train_time:128720ms step_avg:61.47ms
step:2095/2330 train_time:128781ms step_avg:61.47ms
step:2096/2330 train_time:128844ms step_avg:61.47ms
step:2097/2330 train_time:128904ms step_avg:61.47ms
step:2098/2330 train_time:128967ms step_avg:61.47ms
step:2099/2330 train_time:129028ms step_avg:61.47ms
step:2100/2330 train_time:129091ms step_avg:61.47ms
step:2101/2330 train_time:129154ms step_avg:61.47ms
step:2102/2330 train_time:129218ms step_avg:61.47ms
step:2103/2330 train_time:129279ms step_avg:61.47ms
step:2104/2330 train_time:129342ms step_avg:61.47ms
step:2105/2330 train_time:129404ms step_avg:61.47ms
step:2106/2330 train_time:129466ms step_avg:61.47ms
step:2107/2330 train_time:129528ms step_avg:61.47ms
step:2108/2330 train_time:129591ms step_avg:61.48ms
step:2109/2330 train_time:129652ms step_avg:61.48ms
step:2110/2330 train_time:129716ms step_avg:61.48ms
step:2111/2330 train_time:129777ms step_avg:61.48ms
step:2112/2330 train_time:129839ms step_avg:61.48ms
step:2113/2330 train_time:129900ms step_avg:61.48ms
step:2114/2330 train_time:129963ms step_avg:61.48ms
step:2115/2330 train_time:130024ms step_avg:61.48ms
step:2116/2330 train_time:130087ms step_avg:61.48ms
step:2117/2330 train_time:130147ms step_avg:61.48ms
step:2118/2330 train_time:130210ms step_avg:61.48ms
step:2119/2330 train_time:130272ms step_avg:61.48ms
step:2120/2330 train_time:130335ms step_avg:61.48ms
step:2121/2330 train_time:130397ms step_avg:61.48ms
step:2122/2330 train_time:130461ms step_avg:61.48ms
step:2123/2330 train_time:130522ms step_avg:61.48ms
step:2124/2330 train_time:130585ms step_avg:61.48ms
step:2125/2330 train_time:130645ms step_avg:61.48ms
step:2126/2330 train_time:130708ms step_avg:61.48ms
step:2127/2330 train_time:130769ms step_avg:61.48ms
step:2128/2330 train_time:130833ms step_avg:61.48ms
step:2129/2330 train_time:130894ms step_avg:61.48ms
step:2130/2330 train_time:130957ms step_avg:61.48ms
step:2131/2330 train_time:131019ms step_avg:61.48ms
step:2132/2330 train_time:131082ms step_avg:61.48ms
step:2133/2330 train_time:131144ms step_avg:61.48ms
step:2134/2330 train_time:131207ms step_avg:61.48ms
step:2135/2330 train_time:131267ms step_avg:61.48ms
step:2136/2330 train_time:131331ms step_avg:61.48ms
step:2137/2330 train_time:131391ms step_avg:61.48ms
step:2138/2330 train_time:131455ms step_avg:61.48ms
step:2139/2330 train_time:131517ms step_avg:61.49ms
step:2140/2330 train_time:131580ms step_avg:61.49ms
step:2141/2330 train_time:131640ms step_avg:61.49ms
step:2142/2330 train_time:131704ms step_avg:61.49ms
step:2143/2330 train_time:131764ms step_avg:61.49ms
step:2144/2330 train_time:131827ms step_avg:61.49ms
step:2145/2330 train_time:131888ms step_avg:61.49ms
step:2146/2330 train_time:131951ms step_avg:61.49ms
step:2147/2330 train_time:132013ms step_avg:61.49ms
step:2148/2330 train_time:132076ms step_avg:61.49ms
step:2149/2330 train_time:132138ms step_avg:61.49ms
step:2150/2330 train_time:132202ms step_avg:61.49ms
step:2151/2330 train_time:132263ms step_avg:61.49ms
step:2152/2330 train_time:132326ms step_avg:61.49ms
step:2153/2330 train_time:132386ms step_avg:61.49ms
step:2154/2330 train_time:132448ms step_avg:61.49ms
step:2155/2330 train_time:132509ms step_avg:61.49ms
step:2156/2330 train_time:132573ms step_avg:61.49ms
step:2157/2330 train_time:132634ms step_avg:61.49ms
step:2158/2330 train_time:132698ms step_avg:61.49ms
step:2159/2330 train_time:132759ms step_avg:61.49ms
step:2160/2330 train_time:132822ms step_avg:61.49ms
step:2161/2330 train_time:132883ms step_avg:61.49ms
step:2162/2330 train_time:132945ms step_avg:61.49ms
step:2163/2330 train_time:133006ms step_avg:61.49ms
step:2164/2330 train_time:133069ms step_avg:61.49ms
step:2165/2330 train_time:133130ms step_avg:61.49ms
step:2166/2330 train_time:133194ms step_avg:61.49ms
step:2167/2330 train_time:133257ms step_avg:61.49ms
step:2168/2330 train_time:133320ms step_avg:61.49ms
step:2169/2330 train_time:133381ms step_avg:61.49ms
step:2170/2330 train_time:133443ms step_avg:61.49ms
step:2171/2330 train_time:133504ms step_avg:61.49ms
step:2172/2330 train_time:133567ms step_avg:61.49ms
step:2173/2330 train_time:133627ms step_avg:61.49ms
step:2174/2330 train_time:133690ms step_avg:61.50ms
step:2175/2330 train_time:133752ms step_avg:61.50ms
step:2176/2330 train_time:133816ms step_avg:61.50ms
step:2177/2330 train_time:133878ms step_avg:61.50ms
step:2178/2330 train_time:133940ms step_avg:61.50ms
step:2179/2330 train_time:134002ms step_avg:61.50ms
step:2180/2330 train_time:134065ms step_avg:61.50ms
step:2181/2330 train_time:134126ms step_avg:61.50ms
step:2182/2330 train_time:134189ms step_avg:61.50ms
step:2183/2330 train_time:134250ms step_avg:61.50ms
step:2184/2330 train_time:134315ms step_avg:61.50ms
step:2185/2330 train_time:134376ms step_avg:61.50ms
step:2186/2330 train_time:134440ms step_avg:61.50ms
step:2187/2330 train_time:134501ms step_avg:61.50ms
step:2188/2330 train_time:134564ms step_avg:61.50ms
step:2189/2330 train_time:134625ms step_avg:61.50ms
step:2190/2330 train_time:134688ms step_avg:61.50ms
step:2191/2330 train_time:134748ms step_avg:61.50ms
step:2192/2330 train_time:134812ms step_avg:61.50ms
step:2193/2330 train_time:134873ms step_avg:61.50ms
step:2194/2330 train_time:134936ms step_avg:61.50ms
step:2195/2330 train_time:134998ms step_avg:61.50ms
step:2196/2330 train_time:135061ms step_avg:61.50ms
step:2197/2330 train_time:135123ms step_avg:61.50ms
step:2198/2330 train_time:135185ms step_avg:61.50ms
step:2199/2330 train_time:135245ms step_avg:61.50ms
step:2200/2330 train_time:135309ms step_avg:61.50ms
step:2201/2330 train_time:135371ms step_avg:61.50ms
step:2202/2330 train_time:135435ms step_avg:61.51ms
step:2203/2330 train_time:135497ms step_avg:61.51ms
step:2204/2330 train_time:135559ms step_avg:61.51ms
step:2205/2330 train_time:135620ms step_avg:61.51ms
step:2206/2330 train_time:135684ms step_avg:61.51ms
step:2207/2330 train_time:135745ms step_avg:61.51ms
step:2208/2330 train_time:135808ms step_avg:61.51ms
step:2209/2330 train_time:135868ms step_avg:61.51ms
step:2210/2330 train_time:135931ms step_avg:61.51ms
step:2211/2330 train_time:135992ms step_avg:61.51ms
step:2212/2330 train_time:136057ms step_avg:61.51ms
step:2213/2330 train_time:136118ms step_avg:61.51ms
step:2214/2330 train_time:136181ms step_avg:61.51ms
step:2215/2330 train_time:136241ms step_avg:61.51ms
step:2216/2330 train_time:136305ms step_avg:61.51ms
step:2217/2330 train_time:136366ms step_avg:61.51ms
step:2218/2330 train_time:136429ms step_avg:61.51ms
step:2219/2330 train_time:136491ms step_avg:61.51ms
step:2220/2330 train_time:136554ms step_avg:61.51ms
step:2221/2330 train_time:136617ms step_avg:61.51ms
step:2222/2330 train_time:136680ms step_avg:61.51ms
step:2223/2330 train_time:136741ms step_avg:61.51ms
step:2224/2330 train_time:136805ms step_avg:61.51ms
step:2225/2330 train_time:136865ms step_avg:61.51ms
step:2226/2330 train_time:136928ms step_avg:61.51ms
step:2227/2330 train_time:136988ms step_avg:61.51ms
step:2228/2330 train_time:137052ms step_avg:61.51ms
step:2229/2330 train_time:137112ms step_avg:61.51ms
step:2230/2330 train_time:137175ms step_avg:61.51ms
step:2231/2330 train_time:137236ms step_avg:61.51ms
step:2232/2330 train_time:137300ms step_avg:61.51ms
step:2233/2330 train_time:137361ms step_avg:61.51ms
step:2234/2330 train_time:137424ms step_avg:61.51ms
step:2235/2330 train_time:137485ms step_avg:61.51ms
step:2236/2330 train_time:137548ms step_avg:61.52ms
step:2237/2330 train_time:137610ms step_avg:61.52ms
step:2238/2330 train_time:137672ms step_avg:61.52ms
step:2239/2330 train_time:137734ms step_avg:61.52ms
step:2240/2330 train_time:137798ms step_avg:61.52ms
step:2241/2330 train_time:137859ms step_avg:61.52ms
step:2242/2330 train_time:137922ms step_avg:61.52ms
step:2243/2330 train_time:137984ms step_avg:61.52ms
step:2244/2330 train_time:138047ms step_avg:61.52ms
step:2245/2330 train_time:138107ms step_avg:61.52ms
step:2246/2330 train_time:138170ms step_avg:61.52ms
step:2247/2330 train_time:138231ms step_avg:61.52ms
step:2248/2330 train_time:138293ms step_avg:61.52ms
step:2249/2330 train_time:138355ms step_avg:61.52ms
step:2250/2330 train_time:138419ms step_avg:61.52ms
step:2250/2330 val_loss:3.3351 train_time:138485ms step_avg:61.55ms
step:2251/2330 train_time:138507ms step_avg:61.53ms
step:2252/2330 train_time:138547ms step_avg:61.52ms
step:2253/2330 train_time:138612ms step_avg:61.52ms
step:2254/2330 train_time:138675ms step_avg:61.52ms
step:2255/2330 train_time:138737ms step_avg:61.52ms
step:2256/2330 train_time:138799ms step_avg:61.52ms
step:2257/2330 train_time:138859ms step_avg:61.52ms
step:2258/2330 train_time:138923ms step_avg:61.52ms
step:2259/2330 train_time:138983ms step_avg:61.52ms
step:2260/2330 train_time:139045ms step_avg:61.52ms
step:2261/2330 train_time:139105ms step_avg:61.52ms
step:2262/2330 train_time:139167ms step_avg:61.52ms
step:2263/2330 train_time:139227ms step_avg:61.52ms
step:2264/2330 train_time:139290ms step_avg:61.52ms
step:2265/2330 train_time:139350ms step_avg:61.52ms
step:2266/2330 train_time:139415ms step_avg:61.52ms
step:2267/2330 train_time:139478ms step_avg:61.53ms
step:2268/2330 train_time:139543ms step_avg:61.53ms
step:2269/2330 train_time:139604ms step_avg:61.53ms
step:2270/2330 train_time:139668ms step_avg:61.53ms
step:2271/2330 train_time:139729ms step_avg:61.53ms
step:2272/2330 train_time:139793ms step_avg:61.53ms
step:2273/2330 train_time:139853ms step_avg:61.53ms
step:2274/2330 train_time:139916ms step_avg:61.53ms
step:2275/2330 train_time:139977ms step_avg:61.53ms
step:2276/2330 train_time:140039ms step_avg:61.53ms
step:2277/2330 train_time:140100ms step_avg:61.53ms
step:2278/2330 train_time:140162ms step_avg:61.53ms
step:2279/2330 train_time:140223ms step_avg:61.53ms
step:2280/2330 train_time:140285ms step_avg:61.53ms
step:2281/2330 train_time:140346ms step_avg:61.53ms
step:2282/2330 train_time:140409ms step_avg:61.53ms
step:2283/2330 train_time:140471ms step_avg:61.53ms
step:2284/2330 train_time:140536ms step_avg:61.53ms
step:2285/2330 train_time:140597ms step_avg:61.53ms
step:2286/2330 train_time:140661ms step_avg:61.53ms
step:2287/2330 train_time:140723ms step_avg:61.53ms
step:2288/2330 train_time:140786ms step_avg:61.53ms
step:2289/2330 train_time:140847ms step_avg:61.53ms
step:2290/2330 train_time:140909ms step_avg:61.53ms
step:2291/2330 train_time:140970ms step_avg:61.53ms
step:2292/2330 train_time:141033ms step_avg:61.53ms
step:2293/2330 train_time:141094ms step_avg:61.53ms
step:2294/2330 train_time:141156ms step_avg:61.53ms
step:2295/2330 train_time:141218ms step_avg:61.53ms
step:2296/2330 train_time:141281ms step_avg:61.53ms
step:2297/2330 train_time:141341ms step_avg:61.53ms
step:2298/2330 train_time:141404ms step_avg:61.53ms
step:2299/2330 train_time:141465ms step_avg:61.53ms
step:2300/2330 train_time:141528ms step_avg:61.53ms
step:2301/2330 train_time:141590ms step_avg:61.53ms
step:2302/2330 train_time:141653ms step_avg:61.53ms
step:2303/2330 train_time:141714ms step_avg:61.53ms
step:2304/2330 train_time:141778ms step_avg:61.54ms
step:2305/2330 train_time:141839ms step_avg:61.54ms
step:2306/2330 train_time:141902ms step_avg:61.54ms
step:2307/2330 train_time:141964ms step_avg:61.54ms
step:2308/2330 train_time:142027ms step_avg:61.54ms
step:2309/2330 train_time:142088ms step_avg:61.54ms
step:2310/2330 train_time:142151ms step_avg:61.54ms
step:2311/2330 train_time:142211ms step_avg:61.54ms
step:2312/2330 train_time:142274ms step_avg:61.54ms
step:2313/2330 train_time:142335ms step_avg:61.54ms
step:2314/2330 train_time:142398ms step_avg:61.54ms
step:2315/2330 train_time:142459ms step_avg:61.54ms
step:2316/2330 train_time:142522ms step_avg:61.54ms
step:2317/2330 train_time:142584ms step_avg:61.54ms
step:2318/2330 train_time:142647ms step_avg:61.54ms
step:2319/2330 train_time:142707ms step_avg:61.54ms
step:2320/2330 train_time:142770ms step_avg:61.54ms
step:2321/2330 train_time:142832ms step_avg:61.54ms
step:2322/2330 train_time:142895ms step_avg:61.54ms
step:2323/2330 train_time:142956ms step_avg:61.54ms
step:2324/2330 train_time:143020ms step_avg:61.54ms
step:2325/2330 train_time:143081ms step_avg:61.54ms
step:2326/2330 train_time:143144ms step_avg:61.54ms
step:2327/2330 train_time:143205ms step_avg:61.54ms
step:2328/2330 train_time:143268ms step_avg:61.54ms
step:2329/2330 train_time:143328ms step_avg:61.54ms
step:2330/2330 train_time:143391ms step_avg:61.54ms
step:2330/2330 val_loss:3.3214 train_time:143457ms step_avg:61.57ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
