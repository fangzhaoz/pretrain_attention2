import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_3e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-4, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 02:47:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:87ms step_avg:87.04ms
step:2/2330 train_time:200ms step_avg:100.07ms
step:3/2330 train_time:220ms step_avg:73.20ms
step:4/2330 train_time:298ms step_avg:74.44ms
step:5/2330 train_time:356ms step_avg:71.14ms
step:6/2330 train_time:416ms step_avg:69.27ms
step:7/2330 train_time:473ms step_avg:67.55ms
step:8/2330 train_time:534ms step_avg:66.70ms
step:9/2330 train_time:592ms step_avg:65.73ms
step:10/2330 train_time:652ms step_avg:65.22ms
step:11/2330 train_time:710ms step_avg:64.50ms
step:12/2330 train_time:770ms step_avg:64.21ms
step:13/2330 train_time:828ms step_avg:63.69ms
step:14/2330 train_time:889ms step_avg:63.50ms
step:15/2330 train_time:947ms step_avg:63.16ms
step:16/2330 train_time:1007ms step_avg:62.97ms
step:17/2330 train_time:1066ms step_avg:62.72ms
step:18/2330 train_time:1127ms step_avg:62.63ms
step:19/2330 train_time:1188ms step_avg:62.55ms
step:20/2330 train_time:1257ms step_avg:62.86ms
step:21/2330 train_time:1318ms step_avg:62.78ms
step:22/2330 train_time:1380ms step_avg:62.74ms
step:23/2330 train_time:1440ms step_avg:62.61ms
step:24/2330 train_time:1502ms step_avg:62.58ms
step:25/2330 train_time:1561ms step_avg:62.45ms
step:26/2330 train_time:1623ms step_avg:62.43ms
step:27/2330 train_time:1682ms step_avg:62.29ms
step:28/2330 train_time:1744ms step_avg:62.28ms
step:29/2330 train_time:1802ms step_avg:62.15ms
step:30/2330 train_time:1865ms step_avg:62.18ms
step:31/2330 train_time:1924ms step_avg:62.07ms
step:32/2330 train_time:1985ms step_avg:62.02ms
step:33/2330 train_time:2044ms step_avg:61.93ms
step:34/2330 train_time:2105ms step_avg:61.92ms
step:35/2330 train_time:2166ms step_avg:61.88ms
step:36/2330 train_time:2228ms step_avg:61.88ms
step:37/2330 train_time:2288ms step_avg:61.83ms
step:38/2330 train_time:2349ms step_avg:61.82ms
step:39/2330 train_time:2408ms step_avg:61.74ms
step:40/2330 train_time:2470ms step_avg:61.75ms
step:41/2330 train_time:2529ms step_avg:61.69ms
step:42/2330 train_time:2591ms step_avg:61.70ms
step:43/2330 train_time:2650ms step_avg:61.63ms
step:44/2330 train_time:2713ms step_avg:61.65ms
step:45/2330 train_time:2772ms step_avg:61.61ms
step:46/2330 train_time:2835ms step_avg:61.62ms
step:47/2330 train_time:2893ms step_avg:61.56ms
step:48/2330 train_time:2956ms step_avg:61.58ms
step:49/2330 train_time:3014ms step_avg:61.52ms
step:50/2330 train_time:3077ms step_avg:61.53ms
step:51/2330 train_time:3135ms step_avg:61.48ms
step:52/2330 train_time:3199ms step_avg:61.53ms
step:53/2330 train_time:3259ms step_avg:61.49ms
step:54/2330 train_time:3320ms step_avg:61.48ms
step:55/2330 train_time:3379ms step_avg:61.43ms
step:56/2330 train_time:3441ms step_avg:61.45ms
step:57/2330 train_time:3502ms step_avg:61.43ms
step:58/2330 train_time:3564ms step_avg:61.46ms
step:59/2330 train_time:3623ms step_avg:61.41ms
step:60/2330 train_time:3685ms step_avg:61.41ms
step:61/2330 train_time:3744ms step_avg:61.37ms
step:62/2330 train_time:3805ms step_avg:61.38ms
step:63/2330 train_time:3864ms step_avg:61.34ms
step:64/2330 train_time:3926ms step_avg:61.35ms
step:65/2330 train_time:3985ms step_avg:61.31ms
step:66/2330 train_time:4047ms step_avg:61.32ms
step:67/2330 train_time:4106ms step_avg:61.29ms
step:68/2330 train_time:4169ms step_avg:61.32ms
step:69/2330 train_time:4228ms step_avg:61.28ms
step:70/2330 train_time:4290ms step_avg:61.28ms
step:71/2330 train_time:4348ms step_avg:61.24ms
step:72/2330 train_time:4410ms step_avg:61.25ms
step:73/2330 train_time:4469ms step_avg:61.22ms
step:74/2330 train_time:4531ms step_avg:61.23ms
step:75/2330 train_time:4590ms step_avg:61.20ms
step:76/2330 train_time:4653ms step_avg:61.22ms
step:77/2330 train_time:4712ms step_avg:61.20ms
step:78/2330 train_time:4775ms step_avg:61.21ms
step:79/2330 train_time:4834ms step_avg:61.19ms
step:80/2330 train_time:4896ms step_avg:61.20ms
step:81/2330 train_time:4955ms step_avg:61.18ms
step:82/2330 train_time:5017ms step_avg:61.18ms
step:83/2330 train_time:5076ms step_avg:61.16ms
step:84/2330 train_time:5138ms step_avg:61.16ms
step:85/2330 train_time:5197ms step_avg:61.15ms
step:86/2330 train_time:5260ms step_avg:61.16ms
step:87/2330 train_time:5320ms step_avg:61.14ms
step:88/2330 train_time:5381ms step_avg:61.15ms
step:89/2330 train_time:5440ms step_avg:61.12ms
step:90/2330 train_time:5502ms step_avg:61.13ms
step:91/2330 train_time:5562ms step_avg:61.12ms
step:92/2330 train_time:5623ms step_avg:61.12ms
step:93/2330 train_time:5682ms step_avg:61.10ms
step:94/2330 train_time:5745ms step_avg:61.11ms
step:95/2330 train_time:5804ms step_avg:61.09ms
step:96/2330 train_time:5866ms step_avg:61.10ms
step:97/2330 train_time:5924ms step_avg:61.08ms
step:98/2330 train_time:5988ms step_avg:61.10ms
step:99/2330 train_time:6047ms step_avg:61.08ms
step:100/2330 train_time:6110ms step_avg:61.10ms
step:101/2330 train_time:6169ms step_avg:61.08ms
step:102/2330 train_time:6231ms step_avg:61.09ms
step:103/2330 train_time:6289ms step_avg:61.06ms
step:104/2330 train_time:6351ms step_avg:61.07ms
step:105/2330 train_time:6411ms step_avg:61.05ms
step:106/2330 train_time:6473ms step_avg:61.07ms
step:107/2330 train_time:6531ms step_avg:61.04ms
step:108/2330 train_time:6594ms step_avg:61.06ms
step:109/2330 train_time:6653ms step_avg:61.04ms
step:110/2330 train_time:6715ms step_avg:61.05ms
step:111/2330 train_time:6774ms step_avg:61.03ms
step:112/2330 train_time:6836ms step_avg:61.04ms
step:113/2330 train_time:6896ms step_avg:61.03ms
step:114/2330 train_time:6959ms step_avg:61.04ms
step:115/2330 train_time:7018ms step_avg:61.03ms
step:116/2330 train_time:7081ms step_avg:61.04ms
step:117/2330 train_time:7140ms step_avg:61.02ms
step:118/2330 train_time:7202ms step_avg:61.03ms
step:119/2330 train_time:7261ms step_avg:61.02ms
step:120/2330 train_time:7323ms step_avg:61.03ms
step:121/2330 train_time:7382ms step_avg:61.01ms
step:122/2330 train_time:7444ms step_avg:61.01ms
step:123/2330 train_time:7503ms step_avg:61.00ms
step:124/2330 train_time:7566ms step_avg:61.02ms
step:125/2330 train_time:7624ms step_avg:60.99ms
step:126/2330 train_time:7687ms step_avg:61.01ms
step:127/2330 train_time:7747ms step_avg:61.00ms
step:128/2330 train_time:7808ms step_avg:61.00ms
step:129/2330 train_time:7867ms step_avg:60.99ms
step:130/2330 train_time:7930ms step_avg:61.00ms
step:131/2330 train_time:7988ms step_avg:60.98ms
step:132/2330 train_time:8051ms step_avg:60.99ms
step:133/2330 train_time:8110ms step_avg:60.98ms
step:134/2330 train_time:8172ms step_avg:60.98ms
step:135/2330 train_time:8231ms step_avg:60.97ms
step:136/2330 train_time:8293ms step_avg:60.98ms
step:137/2330 train_time:8352ms step_avg:60.96ms
step:138/2330 train_time:8415ms step_avg:60.98ms
step:139/2330 train_time:8474ms step_avg:60.96ms
step:140/2330 train_time:8536ms step_avg:60.97ms
step:141/2330 train_time:8595ms step_avg:60.96ms
step:142/2330 train_time:8658ms step_avg:60.97ms
step:143/2330 train_time:8718ms step_avg:60.96ms
step:144/2330 train_time:8780ms step_avg:60.97ms
step:145/2330 train_time:8839ms step_avg:60.96ms
step:146/2330 train_time:8901ms step_avg:60.96ms
step:147/2330 train_time:8960ms step_avg:60.95ms
step:148/2330 train_time:9022ms step_avg:60.96ms
step:149/2330 train_time:9081ms step_avg:60.95ms
step:150/2330 train_time:9144ms step_avg:60.96ms
step:151/2330 train_time:9203ms step_avg:60.95ms
step:152/2330 train_time:9265ms step_avg:60.95ms
step:153/2330 train_time:9324ms step_avg:60.94ms
step:154/2330 train_time:9386ms step_avg:60.95ms
step:155/2330 train_time:9444ms step_avg:60.93ms
step:156/2330 train_time:9507ms step_avg:60.94ms
step:157/2330 train_time:9565ms step_avg:60.93ms
step:158/2330 train_time:9628ms step_avg:60.94ms
step:159/2330 train_time:9687ms step_avg:60.92ms
step:160/2330 train_time:9749ms step_avg:60.93ms
step:161/2330 train_time:9808ms step_avg:60.92ms
step:162/2330 train_time:9870ms step_avg:60.93ms
step:163/2330 train_time:9929ms step_avg:60.91ms
step:164/2330 train_time:9991ms step_avg:60.92ms
step:165/2330 train_time:10050ms step_avg:60.91ms
step:166/2330 train_time:10112ms step_avg:60.92ms
step:167/2330 train_time:10171ms step_avg:60.91ms
step:168/2330 train_time:10234ms step_avg:60.92ms
step:169/2330 train_time:10292ms step_avg:60.90ms
step:170/2330 train_time:10355ms step_avg:60.91ms
step:171/2330 train_time:10413ms step_avg:60.90ms
step:172/2330 train_time:10476ms step_avg:60.91ms
step:173/2330 train_time:10535ms step_avg:60.90ms
step:174/2330 train_time:10598ms step_avg:60.91ms
step:175/2330 train_time:10657ms step_avg:60.90ms
step:176/2330 train_time:10719ms step_avg:60.90ms
step:177/2330 train_time:10778ms step_avg:60.89ms
step:178/2330 train_time:10839ms step_avg:60.90ms
step:179/2330 train_time:10900ms step_avg:60.89ms
step:180/2330 train_time:10962ms step_avg:60.90ms
step:181/2330 train_time:11022ms step_avg:60.89ms
step:182/2330 train_time:11083ms step_avg:60.90ms
step:183/2330 train_time:11143ms step_avg:60.89ms
step:184/2330 train_time:11205ms step_avg:60.89ms
step:185/2330 train_time:11264ms step_avg:60.88ms
step:186/2330 train_time:11325ms step_avg:60.89ms
step:187/2330 train_time:11384ms step_avg:60.88ms
step:188/2330 train_time:11447ms step_avg:60.89ms
step:189/2330 train_time:11505ms step_avg:60.87ms
step:190/2330 train_time:11568ms step_avg:60.89ms
step:191/2330 train_time:11628ms step_avg:60.88ms
step:192/2330 train_time:11690ms step_avg:60.88ms
step:193/2330 train_time:11749ms step_avg:60.87ms
step:194/2330 train_time:11811ms step_avg:60.88ms
step:195/2330 train_time:11870ms step_avg:60.87ms
step:196/2330 train_time:11932ms step_avg:60.88ms
step:197/2330 train_time:11991ms step_avg:60.87ms
step:198/2330 train_time:12053ms step_avg:60.87ms
step:199/2330 train_time:12112ms step_avg:60.86ms
step:200/2330 train_time:12174ms step_avg:60.87ms
step:201/2330 train_time:12233ms step_avg:60.86ms
step:202/2330 train_time:12296ms step_avg:60.87ms
step:203/2330 train_time:12356ms step_avg:60.87ms
step:204/2330 train_time:12418ms step_avg:60.87ms
step:205/2330 train_time:12477ms step_avg:60.86ms
step:206/2330 train_time:12540ms step_avg:60.88ms
step:207/2330 train_time:12599ms step_avg:60.87ms
step:208/2330 train_time:12662ms step_avg:60.87ms
step:209/2330 train_time:12721ms step_avg:60.87ms
step:210/2330 train_time:12783ms step_avg:60.87ms
step:211/2330 train_time:12842ms step_avg:60.86ms
step:212/2330 train_time:12904ms step_avg:60.87ms
step:213/2330 train_time:12964ms step_avg:60.86ms
step:214/2330 train_time:13025ms step_avg:60.87ms
step:215/2330 train_time:13085ms step_avg:60.86ms
step:216/2330 train_time:13147ms step_avg:60.86ms
step:217/2330 train_time:13206ms step_avg:60.86ms
step:218/2330 train_time:13269ms step_avg:60.87ms
step:219/2330 train_time:13328ms step_avg:60.86ms
step:220/2330 train_time:13391ms step_avg:60.87ms
step:221/2330 train_time:13450ms step_avg:60.86ms
step:222/2330 train_time:13513ms step_avg:60.87ms
step:223/2330 train_time:13572ms step_avg:60.86ms
step:224/2330 train_time:13634ms step_avg:60.86ms
step:225/2330 train_time:13692ms step_avg:60.86ms
step:226/2330 train_time:13754ms step_avg:60.86ms
step:227/2330 train_time:13813ms step_avg:60.85ms
step:228/2330 train_time:13876ms step_avg:60.86ms
step:229/2330 train_time:13935ms step_avg:60.85ms
step:230/2330 train_time:13998ms step_avg:60.86ms
step:231/2330 train_time:14058ms step_avg:60.86ms
step:232/2330 train_time:14120ms step_avg:60.86ms
step:233/2330 train_time:14179ms step_avg:60.86ms
step:234/2330 train_time:14242ms step_avg:60.86ms
step:235/2330 train_time:14301ms step_avg:60.86ms
step:236/2330 train_time:14364ms step_avg:60.87ms
step:237/2330 train_time:14423ms step_avg:60.86ms
step:238/2330 train_time:14486ms step_avg:60.86ms
step:239/2330 train_time:14545ms step_avg:60.86ms
step:240/2330 train_time:14607ms step_avg:60.86ms
step:241/2330 train_time:14666ms step_avg:60.86ms
step:242/2330 train_time:14728ms step_avg:60.86ms
step:243/2330 train_time:14787ms step_avg:60.85ms
step:244/2330 train_time:14850ms step_avg:60.86ms
step:245/2330 train_time:14909ms step_avg:60.85ms
step:246/2330 train_time:14971ms step_avg:60.86ms
step:247/2330 train_time:15030ms step_avg:60.85ms
step:248/2330 train_time:15093ms step_avg:60.86ms
step:249/2330 train_time:15152ms step_avg:60.85ms
step:250/2330 train_time:15215ms step_avg:60.86ms
step:250/2330 val_loss:5.5585 train_time:15288ms step_avg:61.15ms
step:251/2330 train_time:15308ms step_avg:60.99ms
step:252/2330 train_time:15339ms step_avg:60.87ms
step:253/2330 train_time:15402ms step_avg:60.88ms
step:254/2330 train_time:15468ms step_avg:60.90ms
step:255/2330 train_time:15531ms step_avg:60.91ms
step:256/2330 train_time:15594ms step_avg:60.91ms
step:257/2330 train_time:15653ms step_avg:60.91ms
step:258/2330 train_time:15715ms step_avg:60.91ms
step:259/2330 train_time:15774ms step_avg:60.90ms
step:260/2330 train_time:15836ms step_avg:60.91ms
step:261/2330 train_time:15894ms step_avg:60.90ms
step:262/2330 train_time:15955ms step_avg:60.90ms
step:263/2330 train_time:16014ms step_avg:60.89ms
step:264/2330 train_time:16075ms step_avg:60.89ms
step:265/2330 train_time:16134ms step_avg:60.88ms
step:266/2330 train_time:16195ms step_avg:60.88ms
step:267/2330 train_time:16253ms step_avg:60.87ms
step:268/2330 train_time:16316ms step_avg:60.88ms
step:269/2330 train_time:16376ms step_avg:60.88ms
step:270/2330 train_time:16441ms step_avg:60.89ms
step:271/2330 train_time:16502ms step_avg:60.89ms
step:272/2330 train_time:16565ms step_avg:60.90ms
step:273/2330 train_time:16624ms step_avg:60.89ms
step:274/2330 train_time:16686ms step_avg:60.90ms
step:275/2330 train_time:16746ms step_avg:60.89ms
step:276/2330 train_time:16808ms step_avg:60.90ms
step:277/2330 train_time:16867ms step_avg:60.89ms
step:278/2330 train_time:16930ms step_avg:60.90ms
step:279/2330 train_time:16989ms step_avg:60.89ms
step:280/2330 train_time:17050ms step_avg:60.89ms
step:281/2330 train_time:17109ms step_avg:60.88ms
step:282/2330 train_time:17171ms step_avg:60.89ms
step:283/2330 train_time:17230ms step_avg:60.88ms
step:284/2330 train_time:17292ms step_avg:60.89ms
step:285/2330 train_time:17351ms step_avg:60.88ms
step:286/2330 train_time:17414ms step_avg:60.89ms
step:287/2330 train_time:17474ms step_avg:60.88ms
step:288/2330 train_time:17537ms step_avg:60.89ms
step:289/2330 train_time:17596ms step_avg:60.89ms
step:290/2330 train_time:17660ms step_avg:60.90ms
step:291/2330 train_time:17719ms step_avg:60.89ms
step:292/2330 train_time:17782ms step_avg:60.90ms
step:293/2330 train_time:17841ms step_avg:60.89ms
step:294/2330 train_time:17904ms step_avg:60.90ms
step:295/2330 train_time:17962ms step_avg:60.89ms
step:296/2330 train_time:18025ms step_avg:60.89ms
step:297/2330 train_time:18085ms step_avg:60.89ms
step:298/2330 train_time:18146ms step_avg:60.89ms
step:299/2330 train_time:18206ms step_avg:60.89ms
step:300/2330 train_time:18268ms step_avg:60.89ms
step:301/2330 train_time:18328ms step_avg:60.89ms
step:302/2330 train_time:18389ms step_avg:60.89ms
step:303/2330 train_time:18449ms step_avg:60.89ms
step:304/2330 train_time:18511ms step_avg:60.89ms
step:305/2330 train_time:18571ms step_avg:60.89ms
step:306/2330 train_time:18634ms step_avg:60.90ms
step:307/2330 train_time:18693ms step_avg:60.89ms
step:308/2330 train_time:18755ms step_avg:60.89ms
step:309/2330 train_time:18813ms step_avg:60.88ms
step:310/2330 train_time:18876ms step_avg:60.89ms
step:311/2330 train_time:18936ms step_avg:60.89ms
step:312/2330 train_time:18998ms step_avg:60.89ms
step:313/2330 train_time:19057ms step_avg:60.89ms
step:314/2330 train_time:19119ms step_avg:60.89ms
step:315/2330 train_time:19177ms step_avg:60.88ms
step:316/2330 train_time:19240ms step_avg:60.89ms
step:317/2330 train_time:19300ms step_avg:60.88ms
step:318/2330 train_time:19363ms step_avg:60.89ms
step:319/2330 train_time:19422ms step_avg:60.88ms
step:320/2330 train_time:19484ms step_avg:60.89ms
step:321/2330 train_time:19543ms step_avg:60.88ms
step:322/2330 train_time:19605ms step_avg:60.89ms
step:323/2330 train_time:19665ms step_avg:60.88ms
step:324/2330 train_time:19727ms step_avg:60.89ms
step:325/2330 train_time:19786ms step_avg:60.88ms
step:326/2330 train_time:19848ms step_avg:60.88ms
step:327/2330 train_time:19907ms step_avg:60.88ms
step:328/2330 train_time:19969ms step_avg:60.88ms
step:329/2330 train_time:20028ms step_avg:60.88ms
step:330/2330 train_time:20090ms step_avg:60.88ms
step:331/2330 train_time:20149ms step_avg:60.87ms
step:332/2330 train_time:20211ms step_avg:60.88ms
step:333/2330 train_time:20270ms step_avg:60.87ms
step:334/2330 train_time:20333ms step_avg:60.88ms
step:335/2330 train_time:20392ms step_avg:60.87ms
step:336/2330 train_time:20453ms step_avg:60.87ms
step:337/2330 train_time:20513ms step_avg:60.87ms
step:338/2330 train_time:20576ms step_avg:60.87ms
step:339/2330 train_time:20635ms step_avg:60.87ms
step:340/2330 train_time:20697ms step_avg:60.87ms
step:341/2330 train_time:20756ms step_avg:60.87ms
step:342/2330 train_time:20819ms step_avg:60.87ms
step:343/2330 train_time:20879ms step_avg:60.87ms
step:344/2330 train_time:20941ms step_avg:60.88ms
step:345/2330 train_time:21000ms step_avg:60.87ms
step:346/2330 train_time:21062ms step_avg:60.87ms
step:347/2330 train_time:21122ms step_avg:60.87ms
step:348/2330 train_time:21184ms step_avg:60.87ms
step:349/2330 train_time:21243ms step_avg:60.87ms
step:350/2330 train_time:21304ms step_avg:60.87ms
step:351/2330 train_time:21364ms step_avg:60.87ms
step:352/2330 train_time:21427ms step_avg:60.87ms
step:353/2330 train_time:21487ms step_avg:60.87ms
step:354/2330 train_time:21549ms step_avg:60.87ms
step:355/2330 train_time:21609ms step_avg:60.87ms
step:356/2330 train_time:21670ms step_avg:60.87ms
step:357/2330 train_time:21730ms step_avg:60.87ms
step:358/2330 train_time:21791ms step_avg:60.87ms
step:359/2330 train_time:21850ms step_avg:60.86ms
step:360/2330 train_time:21912ms step_avg:60.87ms
step:361/2330 train_time:21971ms step_avg:60.86ms
step:362/2330 train_time:22034ms step_avg:60.87ms
step:363/2330 train_time:22093ms step_avg:60.86ms
step:364/2330 train_time:22155ms step_avg:60.87ms
step:365/2330 train_time:22215ms step_avg:60.86ms
step:366/2330 train_time:22277ms step_avg:60.87ms
step:367/2330 train_time:22336ms step_avg:60.86ms
step:368/2330 train_time:22398ms step_avg:60.87ms
step:369/2330 train_time:22458ms step_avg:60.86ms
step:370/2330 train_time:22521ms step_avg:60.87ms
step:371/2330 train_time:22580ms step_avg:60.86ms
step:372/2330 train_time:22643ms step_avg:60.87ms
step:373/2330 train_time:22702ms step_avg:60.86ms
step:374/2330 train_time:22764ms step_avg:60.87ms
step:375/2330 train_time:22823ms step_avg:60.86ms
step:376/2330 train_time:22885ms step_avg:60.86ms
step:377/2330 train_time:22943ms step_avg:60.86ms
step:378/2330 train_time:23006ms step_avg:60.86ms
step:379/2330 train_time:23066ms step_avg:60.86ms
step:380/2330 train_time:23129ms step_avg:60.86ms
step:381/2330 train_time:23188ms step_avg:60.86ms
step:382/2330 train_time:23249ms step_avg:60.86ms
step:383/2330 train_time:23309ms step_avg:60.86ms
step:384/2330 train_time:23370ms step_avg:60.86ms
step:385/2330 train_time:23430ms step_avg:60.86ms
step:386/2330 train_time:23492ms step_avg:60.86ms
step:387/2330 train_time:23551ms step_avg:60.86ms
step:388/2330 train_time:23613ms step_avg:60.86ms
step:389/2330 train_time:23672ms step_avg:60.85ms
step:390/2330 train_time:23735ms step_avg:60.86ms
step:391/2330 train_time:23793ms step_avg:60.85ms
step:392/2330 train_time:23856ms step_avg:60.86ms
step:393/2330 train_time:23915ms step_avg:60.85ms
step:394/2330 train_time:23978ms step_avg:60.86ms
step:395/2330 train_time:24037ms step_avg:60.85ms
step:396/2330 train_time:24099ms step_avg:60.86ms
step:397/2330 train_time:24159ms step_avg:60.85ms
step:398/2330 train_time:24222ms step_avg:60.86ms
step:399/2330 train_time:24282ms step_avg:60.86ms
step:400/2330 train_time:24344ms step_avg:60.86ms
step:401/2330 train_time:24403ms step_avg:60.86ms
step:402/2330 train_time:24465ms step_avg:60.86ms
step:403/2330 train_time:24524ms step_avg:60.85ms
step:404/2330 train_time:24587ms step_avg:60.86ms
step:405/2330 train_time:24646ms step_avg:60.85ms
step:406/2330 train_time:24708ms step_avg:60.86ms
step:407/2330 train_time:24769ms step_avg:60.86ms
step:408/2330 train_time:24831ms step_avg:60.86ms
step:409/2330 train_time:24889ms step_avg:60.85ms
step:410/2330 train_time:24951ms step_avg:60.86ms
step:411/2330 train_time:25011ms step_avg:60.85ms
step:412/2330 train_time:25072ms step_avg:60.86ms
step:413/2330 train_time:25132ms step_avg:60.85ms
step:414/2330 train_time:25194ms step_avg:60.86ms
step:415/2330 train_time:25253ms step_avg:60.85ms
step:416/2330 train_time:25316ms step_avg:60.85ms
step:417/2330 train_time:25375ms step_avg:60.85ms
step:418/2330 train_time:25438ms step_avg:60.86ms
step:419/2330 train_time:25497ms step_avg:60.85ms
step:420/2330 train_time:25559ms step_avg:60.85ms
step:421/2330 train_time:25619ms step_avg:60.85ms
step:422/2330 train_time:25681ms step_avg:60.86ms
step:423/2330 train_time:25741ms step_avg:60.85ms
step:424/2330 train_time:25803ms step_avg:60.86ms
step:425/2330 train_time:25863ms step_avg:60.85ms
step:426/2330 train_time:25926ms step_avg:60.86ms
step:427/2330 train_time:25985ms step_avg:60.85ms
step:428/2330 train_time:26047ms step_avg:60.86ms
step:429/2330 train_time:26105ms step_avg:60.85ms
step:430/2330 train_time:26169ms step_avg:60.86ms
step:431/2330 train_time:26228ms step_avg:60.85ms
step:432/2330 train_time:26290ms step_avg:60.86ms
step:433/2330 train_time:26349ms step_avg:60.85ms
step:434/2330 train_time:26412ms step_avg:60.86ms
step:435/2330 train_time:26471ms step_avg:60.85ms
step:436/2330 train_time:26534ms step_avg:60.86ms
step:437/2330 train_time:26592ms step_avg:60.85ms
step:438/2330 train_time:26655ms step_avg:60.86ms
step:439/2330 train_time:26714ms step_avg:60.85ms
step:440/2330 train_time:26777ms step_avg:60.86ms
step:441/2330 train_time:26837ms step_avg:60.86ms
step:442/2330 train_time:26899ms step_avg:60.86ms
step:443/2330 train_time:26959ms step_avg:60.86ms
step:444/2330 train_time:27022ms step_avg:60.86ms
step:445/2330 train_time:27081ms step_avg:60.86ms
step:446/2330 train_time:27144ms step_avg:60.86ms
step:447/2330 train_time:27203ms step_avg:60.86ms
step:448/2330 train_time:27266ms step_avg:60.86ms
step:449/2330 train_time:27325ms step_avg:60.86ms
step:450/2330 train_time:27387ms step_avg:60.86ms
step:451/2330 train_time:27447ms step_avg:60.86ms
step:452/2330 train_time:27509ms step_avg:60.86ms
step:453/2330 train_time:27570ms step_avg:60.86ms
step:454/2330 train_time:27632ms step_avg:60.86ms
step:455/2330 train_time:27691ms step_avg:60.86ms
step:456/2330 train_time:27754ms step_avg:60.86ms
step:457/2330 train_time:27812ms step_avg:60.86ms
step:458/2330 train_time:27874ms step_avg:60.86ms
step:459/2330 train_time:27933ms step_avg:60.86ms
step:460/2330 train_time:27996ms step_avg:60.86ms
step:461/2330 train_time:28055ms step_avg:60.86ms
step:462/2330 train_time:28118ms step_avg:60.86ms
step:463/2330 train_time:28178ms step_avg:60.86ms
step:464/2330 train_time:28241ms step_avg:60.86ms
step:465/2330 train_time:28300ms step_avg:60.86ms
step:466/2330 train_time:28363ms step_avg:60.86ms
step:467/2330 train_time:28423ms step_avg:60.86ms
step:468/2330 train_time:28486ms step_avg:60.87ms
step:469/2330 train_time:28545ms step_avg:60.86ms
step:470/2330 train_time:28608ms step_avg:60.87ms
step:471/2330 train_time:28668ms step_avg:60.87ms
step:472/2330 train_time:28731ms step_avg:60.87ms
step:473/2330 train_time:28790ms step_avg:60.87ms
step:474/2330 train_time:28851ms step_avg:60.87ms
step:475/2330 train_time:28911ms step_avg:60.86ms
step:476/2330 train_time:28973ms step_avg:60.87ms
step:477/2330 train_time:29033ms step_avg:60.87ms
step:478/2330 train_time:29095ms step_avg:60.87ms
step:479/2330 train_time:29154ms step_avg:60.86ms
step:480/2330 train_time:29216ms step_avg:60.87ms
step:481/2330 train_time:29275ms step_avg:60.86ms
step:482/2330 train_time:29338ms step_avg:60.87ms
step:483/2330 train_time:29397ms step_avg:60.86ms
step:484/2330 train_time:29459ms step_avg:60.87ms
step:485/2330 train_time:29520ms step_avg:60.86ms
step:486/2330 train_time:29582ms step_avg:60.87ms
step:487/2330 train_time:29642ms step_avg:60.87ms
step:488/2330 train_time:29705ms step_avg:60.87ms
step:489/2330 train_time:29764ms step_avg:60.87ms
step:490/2330 train_time:29828ms step_avg:60.87ms
step:491/2330 train_time:29887ms step_avg:60.87ms
step:492/2330 train_time:29949ms step_avg:60.87ms
step:493/2330 train_time:30008ms step_avg:60.87ms
step:494/2330 train_time:30070ms step_avg:60.87ms
step:495/2330 train_time:30130ms step_avg:60.87ms
step:496/2330 train_time:30192ms step_avg:60.87ms
step:497/2330 train_time:30252ms step_avg:60.87ms
step:498/2330 train_time:30314ms step_avg:60.87ms
step:499/2330 train_time:30373ms step_avg:60.87ms
step:500/2330 train_time:30436ms step_avg:60.87ms
step:500/2330 val_loss:5.1473 train_time:30510ms step_avg:61.02ms
step:501/2330 train_time:30531ms step_avg:60.94ms
step:502/2330 train_time:30561ms step_avg:60.88ms
step:503/2330 train_time:30622ms step_avg:60.88ms
step:504/2330 train_time:30689ms step_avg:60.89ms
step:505/2330 train_time:30750ms step_avg:60.89ms
step:506/2330 train_time:30812ms step_avg:60.89ms
step:507/2330 train_time:30872ms step_avg:60.89ms
step:508/2330 train_time:30934ms step_avg:60.89ms
step:509/2330 train_time:30992ms step_avg:60.89ms
step:510/2330 train_time:31054ms step_avg:60.89ms
step:511/2330 train_time:31113ms step_avg:60.89ms
step:512/2330 train_time:31175ms step_avg:60.89ms
step:513/2330 train_time:31233ms step_avg:60.88ms
step:514/2330 train_time:31294ms step_avg:60.88ms
step:515/2330 train_time:31353ms step_avg:60.88ms
step:516/2330 train_time:31414ms step_avg:60.88ms
step:517/2330 train_time:31474ms step_avg:60.88ms
step:518/2330 train_time:31537ms step_avg:60.88ms
step:519/2330 train_time:31598ms step_avg:60.88ms
step:520/2330 train_time:31663ms step_avg:60.89ms
step:521/2330 train_time:31723ms step_avg:60.89ms
step:522/2330 train_time:31784ms step_avg:60.89ms
step:523/2330 train_time:31843ms step_avg:60.88ms
step:524/2330 train_time:31905ms step_avg:60.89ms
step:525/2330 train_time:31964ms step_avg:60.88ms
step:526/2330 train_time:32026ms step_avg:60.89ms
step:527/2330 train_time:32085ms step_avg:60.88ms
step:528/2330 train_time:32147ms step_avg:60.88ms
step:529/2330 train_time:32206ms step_avg:60.88ms
step:530/2330 train_time:32268ms step_avg:60.88ms
step:531/2330 train_time:32326ms step_avg:60.88ms
step:532/2330 train_time:32389ms step_avg:60.88ms
step:533/2330 train_time:32447ms step_avg:60.88ms
step:534/2330 train_time:32510ms step_avg:60.88ms
step:535/2330 train_time:32571ms step_avg:60.88ms
step:536/2330 train_time:32634ms step_avg:60.88ms
step:537/2330 train_time:32694ms step_avg:60.88ms
step:538/2330 train_time:32757ms step_avg:60.89ms
step:539/2330 train_time:32816ms step_avg:60.88ms
step:540/2330 train_time:32879ms step_avg:60.89ms
step:541/2330 train_time:32938ms step_avg:60.88ms
step:542/2330 train_time:33001ms step_avg:60.89ms
step:543/2330 train_time:33059ms step_avg:60.88ms
step:544/2330 train_time:33122ms step_avg:60.89ms
step:545/2330 train_time:33181ms step_avg:60.88ms
step:546/2330 train_time:33243ms step_avg:60.88ms
step:547/2330 train_time:33302ms step_avg:60.88ms
step:548/2330 train_time:33365ms step_avg:60.88ms
step:549/2330 train_time:33424ms step_avg:60.88ms
step:550/2330 train_time:33486ms step_avg:60.88ms
step:551/2330 train_time:33545ms step_avg:60.88ms
step:552/2330 train_time:33608ms step_avg:60.88ms
step:553/2330 train_time:33668ms step_avg:60.88ms
step:554/2330 train_time:33730ms step_avg:60.88ms
step:555/2330 train_time:33790ms step_avg:60.88ms
step:556/2330 train_time:33853ms step_avg:60.89ms
step:557/2330 train_time:33912ms step_avg:60.88ms
step:558/2330 train_time:33975ms step_avg:60.89ms
step:559/2330 train_time:34034ms step_avg:60.88ms
step:560/2330 train_time:34096ms step_avg:60.89ms
step:561/2330 train_time:34156ms step_avg:60.88ms
step:562/2330 train_time:34218ms step_avg:60.89ms
step:563/2330 train_time:34277ms step_avg:60.88ms
step:564/2330 train_time:34340ms step_avg:60.89ms
step:565/2330 train_time:34399ms step_avg:60.88ms
step:566/2330 train_time:34462ms step_avg:60.89ms
step:567/2330 train_time:34521ms step_avg:60.88ms
step:568/2330 train_time:34584ms step_avg:60.89ms
step:569/2330 train_time:34644ms step_avg:60.89ms
step:570/2330 train_time:34706ms step_avg:60.89ms
step:571/2330 train_time:34766ms step_avg:60.89ms
step:572/2330 train_time:34827ms step_avg:60.89ms
step:573/2330 train_time:34886ms step_avg:60.88ms
step:574/2330 train_time:34949ms step_avg:60.89ms
step:575/2330 train_time:35009ms step_avg:60.88ms
step:576/2330 train_time:35071ms step_avg:60.89ms
step:577/2330 train_time:35131ms step_avg:60.88ms
step:578/2330 train_time:35193ms step_avg:60.89ms
step:579/2330 train_time:35253ms step_avg:60.89ms
step:580/2330 train_time:35316ms step_avg:60.89ms
step:581/2330 train_time:35375ms step_avg:60.89ms
step:582/2330 train_time:35438ms step_avg:60.89ms
step:583/2330 train_time:35497ms step_avg:60.89ms
step:584/2330 train_time:35560ms step_avg:60.89ms
step:585/2330 train_time:35620ms step_avg:60.89ms
step:586/2330 train_time:35683ms step_avg:60.89ms
step:587/2330 train_time:35742ms step_avg:60.89ms
step:588/2330 train_time:35804ms step_avg:60.89ms
step:589/2330 train_time:35863ms step_avg:60.89ms
step:590/2330 train_time:35926ms step_avg:60.89ms
step:591/2330 train_time:35985ms step_avg:60.89ms
step:592/2330 train_time:36048ms step_avg:60.89ms
step:593/2330 train_time:36107ms step_avg:60.89ms
step:594/2330 train_time:36170ms step_avg:60.89ms
step:595/2330 train_time:36230ms step_avg:60.89ms
step:596/2330 train_time:36293ms step_avg:60.89ms
step:597/2330 train_time:36354ms step_avg:60.89ms
step:598/2330 train_time:36417ms step_avg:60.90ms
step:599/2330 train_time:36475ms step_avg:60.89ms
step:600/2330 train_time:36538ms step_avg:60.90ms
step:601/2330 train_time:36598ms step_avg:60.89ms
step:602/2330 train_time:36662ms step_avg:60.90ms
step:603/2330 train_time:36721ms step_avg:60.90ms
step:604/2330 train_time:36783ms step_avg:60.90ms
step:605/2330 train_time:36842ms step_avg:60.90ms
step:606/2330 train_time:36904ms step_avg:60.90ms
step:607/2330 train_time:36965ms step_avg:60.90ms
step:608/2330 train_time:37026ms step_avg:60.90ms
step:609/2330 train_time:37085ms step_avg:60.90ms
step:610/2330 train_time:37148ms step_avg:60.90ms
step:611/2330 train_time:37208ms step_avg:60.90ms
step:612/2330 train_time:37271ms step_avg:60.90ms
step:613/2330 train_time:37330ms step_avg:60.90ms
step:614/2330 train_time:37393ms step_avg:60.90ms
step:615/2330 train_time:37453ms step_avg:60.90ms
step:616/2330 train_time:37516ms step_avg:60.90ms
step:617/2330 train_time:37576ms step_avg:60.90ms
step:618/2330 train_time:37638ms step_avg:60.90ms
step:619/2330 train_time:37697ms step_avg:60.90ms
step:620/2330 train_time:37760ms step_avg:60.90ms
step:621/2330 train_time:37819ms step_avg:60.90ms
step:622/2330 train_time:37882ms step_avg:60.90ms
step:623/2330 train_time:37941ms step_avg:60.90ms
step:624/2330 train_time:38004ms step_avg:60.90ms
step:625/2330 train_time:38064ms step_avg:60.90ms
step:626/2330 train_time:38126ms step_avg:60.90ms
step:627/2330 train_time:38185ms step_avg:60.90ms
step:628/2330 train_time:38249ms step_avg:60.91ms
step:629/2330 train_time:38307ms step_avg:60.90ms
step:630/2330 train_time:38370ms step_avg:60.91ms
step:631/2330 train_time:38429ms step_avg:60.90ms
step:632/2330 train_time:38492ms step_avg:60.91ms
step:633/2330 train_time:38552ms step_avg:60.90ms
step:634/2330 train_time:38615ms step_avg:60.91ms
step:635/2330 train_time:38675ms step_avg:60.91ms
step:636/2330 train_time:38737ms step_avg:60.91ms
step:637/2330 train_time:38796ms step_avg:60.90ms
step:638/2330 train_time:38858ms step_avg:60.91ms
step:639/2330 train_time:38917ms step_avg:60.90ms
step:640/2330 train_time:38980ms step_avg:60.91ms
step:641/2330 train_time:39039ms step_avg:60.90ms
step:642/2330 train_time:39101ms step_avg:60.91ms
step:643/2330 train_time:39162ms step_avg:60.91ms
step:644/2330 train_time:39225ms step_avg:60.91ms
step:645/2330 train_time:39285ms step_avg:60.91ms
step:646/2330 train_time:39348ms step_avg:60.91ms
step:647/2330 train_time:39407ms step_avg:60.91ms
step:648/2330 train_time:39470ms step_avg:60.91ms
step:649/2330 train_time:39529ms step_avg:60.91ms
step:650/2330 train_time:39593ms step_avg:60.91ms
step:651/2330 train_time:39654ms step_avg:60.91ms
step:652/2330 train_time:39716ms step_avg:60.91ms
step:653/2330 train_time:39776ms step_avg:60.91ms
step:654/2330 train_time:39839ms step_avg:60.92ms
step:655/2330 train_time:39898ms step_avg:60.91ms
step:656/2330 train_time:39961ms step_avg:60.92ms
step:657/2330 train_time:40019ms step_avg:60.91ms
step:658/2330 train_time:40081ms step_avg:60.91ms
step:659/2330 train_time:40141ms step_avg:60.91ms
step:660/2330 train_time:40203ms step_avg:60.91ms
step:661/2330 train_time:40263ms step_avg:60.91ms
step:662/2330 train_time:40326ms step_avg:60.91ms
step:663/2330 train_time:40385ms step_avg:60.91ms
step:664/2330 train_time:40449ms step_avg:60.92ms
step:665/2330 train_time:40507ms step_avg:60.91ms
step:666/2330 train_time:40571ms step_avg:60.92ms
step:667/2330 train_time:40630ms step_avg:60.91ms
step:668/2330 train_time:40693ms step_avg:60.92ms
step:669/2330 train_time:40753ms step_avg:60.92ms
step:670/2330 train_time:40816ms step_avg:60.92ms
step:671/2330 train_time:40875ms step_avg:60.92ms
step:672/2330 train_time:40939ms step_avg:60.92ms
step:673/2330 train_time:40999ms step_avg:60.92ms
step:674/2330 train_time:41061ms step_avg:60.92ms
step:675/2330 train_time:41119ms step_avg:60.92ms
step:676/2330 train_time:41181ms step_avg:60.92ms
step:677/2330 train_time:41241ms step_avg:60.92ms
step:678/2330 train_time:41304ms step_avg:60.92ms
step:679/2330 train_time:41363ms step_avg:60.92ms
step:680/2330 train_time:41426ms step_avg:60.92ms
step:681/2330 train_time:41485ms step_avg:60.92ms
step:682/2330 train_time:41548ms step_avg:60.92ms
step:683/2330 train_time:41607ms step_avg:60.92ms
step:684/2330 train_time:41670ms step_avg:60.92ms
step:685/2330 train_time:41729ms step_avg:60.92ms
step:686/2330 train_time:41791ms step_avg:60.92ms
step:687/2330 train_time:41852ms step_avg:60.92ms
step:688/2330 train_time:41914ms step_avg:60.92ms
step:689/2330 train_time:41974ms step_avg:60.92ms
step:690/2330 train_time:42036ms step_avg:60.92ms
step:691/2330 train_time:42095ms step_avg:60.92ms
step:692/2330 train_time:42158ms step_avg:60.92ms
step:693/2330 train_time:42218ms step_avg:60.92ms
step:694/2330 train_time:42279ms step_avg:60.92ms
step:695/2330 train_time:42339ms step_avg:60.92ms
step:696/2330 train_time:42402ms step_avg:60.92ms
step:697/2330 train_time:42463ms step_avg:60.92ms
step:698/2330 train_time:42524ms step_avg:60.92ms
step:699/2330 train_time:42584ms step_avg:60.92ms
step:700/2330 train_time:42646ms step_avg:60.92ms
step:701/2330 train_time:42706ms step_avg:60.92ms
step:702/2330 train_time:42769ms step_avg:60.92ms
step:703/2330 train_time:42828ms step_avg:60.92ms
step:704/2330 train_time:42891ms step_avg:60.92ms
step:705/2330 train_time:42951ms step_avg:60.92ms
step:706/2330 train_time:43014ms step_avg:60.93ms
step:707/2330 train_time:43075ms step_avg:60.93ms
step:708/2330 train_time:43137ms step_avg:60.93ms
step:709/2330 train_time:43196ms step_avg:60.92ms
step:710/2330 train_time:43258ms step_avg:60.93ms
step:711/2330 train_time:43317ms step_avg:60.92ms
step:712/2330 train_time:43379ms step_avg:60.93ms
step:713/2330 train_time:43439ms step_avg:60.92ms
step:714/2330 train_time:43503ms step_avg:60.93ms
step:715/2330 train_time:43562ms step_avg:60.93ms
step:716/2330 train_time:43624ms step_avg:60.93ms
step:717/2330 train_time:43683ms step_avg:60.92ms
step:718/2330 train_time:43746ms step_avg:60.93ms
step:719/2330 train_time:43806ms step_avg:60.93ms
step:720/2330 train_time:43869ms step_avg:60.93ms
step:721/2330 train_time:43929ms step_avg:60.93ms
step:722/2330 train_time:43992ms step_avg:60.93ms
step:723/2330 train_time:44051ms step_avg:60.93ms
step:724/2330 train_time:44114ms step_avg:60.93ms
step:725/2330 train_time:44173ms step_avg:60.93ms
step:726/2330 train_time:44235ms step_avg:60.93ms
step:727/2330 train_time:44294ms step_avg:60.93ms
step:728/2330 train_time:44357ms step_avg:60.93ms
step:729/2330 train_time:44416ms step_avg:60.93ms
step:730/2330 train_time:44478ms step_avg:60.93ms
step:731/2330 train_time:44537ms step_avg:60.93ms
step:732/2330 train_time:44600ms step_avg:60.93ms
step:733/2330 train_time:44660ms step_avg:60.93ms
step:734/2330 train_time:44723ms step_avg:60.93ms
step:735/2330 train_time:44782ms step_avg:60.93ms
step:736/2330 train_time:44845ms step_avg:60.93ms
step:737/2330 train_time:44906ms step_avg:60.93ms
step:738/2330 train_time:44970ms step_avg:60.93ms
step:739/2330 train_time:45028ms step_avg:60.93ms
step:740/2330 train_time:45089ms step_avg:60.93ms
step:741/2330 train_time:45149ms step_avg:60.93ms
step:742/2330 train_time:45212ms step_avg:60.93ms
step:743/2330 train_time:45271ms step_avg:60.93ms
step:744/2330 train_time:45334ms step_avg:60.93ms
step:745/2330 train_time:45394ms step_avg:60.93ms
step:746/2330 train_time:45458ms step_avg:60.94ms
step:747/2330 train_time:45517ms step_avg:60.93ms
step:748/2330 train_time:45579ms step_avg:60.93ms
step:749/2330 train_time:45638ms step_avg:60.93ms
step:750/2330 train_time:45700ms step_avg:60.93ms
step:750/2330 val_loss:4.9239 train_time:45773ms step_avg:61.03ms
step:751/2330 train_time:45794ms step_avg:60.98ms
step:752/2330 train_time:45825ms step_avg:60.94ms
step:753/2330 train_time:45886ms step_avg:60.94ms
step:754/2330 train_time:45952ms step_avg:60.94ms
step:755/2330 train_time:46013ms step_avg:60.94ms
step:756/2330 train_time:46075ms step_avg:60.95ms
step:757/2330 train_time:46133ms step_avg:60.94ms
step:758/2330 train_time:46195ms step_avg:60.94ms
step:759/2330 train_time:46254ms step_avg:60.94ms
step:760/2330 train_time:46316ms step_avg:60.94ms
step:761/2330 train_time:46375ms step_avg:60.94ms
step:762/2330 train_time:46438ms step_avg:60.94ms
step:763/2330 train_time:46496ms step_avg:60.94ms
step:764/2330 train_time:46558ms step_avg:60.94ms
step:765/2330 train_time:46618ms step_avg:60.94ms
step:766/2330 train_time:46680ms step_avg:60.94ms
step:767/2330 train_time:46740ms step_avg:60.94ms
step:768/2330 train_time:46805ms step_avg:60.94ms
step:769/2330 train_time:46866ms step_avg:60.94ms
step:770/2330 train_time:46930ms step_avg:60.95ms
step:771/2330 train_time:46991ms step_avg:60.95ms
step:772/2330 train_time:47054ms step_avg:60.95ms
step:773/2330 train_time:47114ms step_avg:60.95ms
step:774/2330 train_time:47176ms step_avg:60.95ms
step:775/2330 train_time:47236ms step_avg:60.95ms
step:776/2330 train_time:47299ms step_avg:60.95ms
step:777/2330 train_time:47358ms step_avg:60.95ms
step:778/2330 train_time:47420ms step_avg:60.95ms
step:779/2330 train_time:47480ms step_avg:60.95ms
step:780/2330 train_time:47542ms step_avg:60.95ms
step:781/2330 train_time:47602ms step_avg:60.95ms
step:782/2330 train_time:47665ms step_avg:60.95ms
step:783/2330 train_time:47724ms step_avg:60.95ms
step:784/2330 train_time:47788ms step_avg:60.95ms
step:785/2330 train_time:47847ms step_avg:60.95ms
step:786/2330 train_time:47911ms step_avg:60.96ms
step:787/2330 train_time:47971ms step_avg:60.95ms
step:788/2330 train_time:48035ms step_avg:60.96ms
step:789/2330 train_time:48094ms step_avg:60.96ms
step:790/2330 train_time:48157ms step_avg:60.96ms
step:791/2330 train_time:48216ms step_avg:60.96ms
step:792/2330 train_time:48279ms step_avg:60.96ms
step:793/2330 train_time:48340ms step_avg:60.96ms
step:794/2330 train_time:48403ms step_avg:60.96ms
step:795/2330 train_time:48463ms step_avg:60.96ms
step:796/2330 train_time:48525ms step_avg:60.96ms
step:797/2330 train_time:48585ms step_avg:60.96ms
step:798/2330 train_time:48647ms step_avg:60.96ms
step:799/2330 train_time:48707ms step_avg:60.96ms
step:800/2330 train_time:48770ms step_avg:60.96ms
step:801/2330 train_time:48830ms step_avg:60.96ms
step:802/2330 train_time:48894ms step_avg:60.97ms
step:803/2330 train_time:48955ms step_avg:60.96ms
step:804/2330 train_time:49017ms step_avg:60.97ms
step:805/2330 train_time:49077ms step_avg:60.97ms
step:806/2330 train_time:49140ms step_avg:60.97ms
step:807/2330 train_time:49201ms step_avg:60.97ms
step:808/2330 train_time:49264ms step_avg:60.97ms
step:809/2330 train_time:49324ms step_avg:60.97ms
step:810/2330 train_time:49387ms step_avg:60.97ms
step:811/2330 train_time:49447ms step_avg:60.97ms
step:812/2330 train_time:49509ms step_avg:60.97ms
step:813/2330 train_time:49570ms step_avg:60.97ms
step:814/2330 train_time:49632ms step_avg:60.97ms
step:815/2330 train_time:49692ms step_avg:60.97ms
step:816/2330 train_time:49756ms step_avg:60.98ms
step:817/2330 train_time:49816ms step_avg:60.97ms
step:818/2330 train_time:49879ms step_avg:60.98ms
step:819/2330 train_time:49939ms step_avg:60.98ms
step:820/2330 train_time:50003ms step_avg:60.98ms
step:821/2330 train_time:50063ms step_avg:60.98ms
step:822/2330 train_time:50126ms step_avg:60.98ms
step:823/2330 train_time:50187ms step_avg:60.98ms
step:824/2330 train_time:50248ms step_avg:60.98ms
step:825/2330 train_time:50308ms step_avg:60.98ms
step:826/2330 train_time:50371ms step_avg:60.98ms
step:827/2330 train_time:50432ms step_avg:60.98ms
step:828/2330 train_time:50494ms step_avg:60.98ms
step:829/2330 train_time:50553ms step_avg:60.98ms
step:830/2330 train_time:50617ms step_avg:60.98ms
step:831/2330 train_time:50677ms step_avg:60.98ms
step:832/2330 train_time:50740ms step_avg:60.99ms
step:833/2330 train_time:50801ms step_avg:60.99ms
step:834/2330 train_time:50863ms step_avg:60.99ms
step:835/2330 train_time:50924ms step_avg:60.99ms
step:836/2330 train_time:50986ms step_avg:60.99ms
step:837/2330 train_time:51046ms step_avg:60.99ms
step:838/2330 train_time:51109ms step_avg:60.99ms
step:839/2330 train_time:51169ms step_avg:60.99ms
step:840/2330 train_time:51232ms step_avg:60.99ms
step:841/2330 train_time:51292ms step_avg:60.99ms
step:842/2330 train_time:51355ms step_avg:60.99ms
step:843/2330 train_time:51413ms step_avg:60.99ms
step:844/2330 train_time:51476ms step_avg:60.99ms
step:845/2330 train_time:51537ms step_avg:60.99ms
step:846/2330 train_time:51600ms step_avg:60.99ms
step:847/2330 train_time:51659ms step_avg:60.99ms
step:848/2330 train_time:51723ms step_avg:60.99ms
step:849/2330 train_time:51783ms step_avg:60.99ms
step:850/2330 train_time:51845ms step_avg:60.99ms
step:851/2330 train_time:51905ms step_avg:60.99ms
step:852/2330 train_time:51968ms step_avg:60.99ms
step:853/2330 train_time:52028ms step_avg:60.99ms
step:854/2330 train_time:52092ms step_avg:61.00ms
step:855/2330 train_time:52153ms step_avg:61.00ms
step:856/2330 train_time:52216ms step_avg:61.00ms
step:857/2330 train_time:52276ms step_avg:61.00ms
step:858/2330 train_time:52338ms step_avg:61.00ms
step:859/2330 train_time:52398ms step_avg:61.00ms
step:860/2330 train_time:52460ms step_avg:61.00ms
step:861/2330 train_time:52520ms step_avg:61.00ms
step:862/2330 train_time:52583ms step_avg:61.00ms
step:863/2330 train_time:52643ms step_avg:61.00ms
step:864/2330 train_time:52705ms step_avg:61.00ms
step:865/2330 train_time:52765ms step_avg:61.00ms
step:866/2330 train_time:52828ms step_avg:61.00ms
step:867/2330 train_time:52888ms step_avg:61.00ms
step:868/2330 train_time:52950ms step_avg:61.00ms
step:869/2330 train_time:53011ms step_avg:61.00ms
step:870/2330 train_time:53074ms step_avg:61.00ms
step:871/2330 train_time:53133ms step_avg:61.00ms
step:872/2330 train_time:53198ms step_avg:61.01ms
step:873/2330 train_time:53257ms step_avg:61.00ms
step:874/2330 train_time:53320ms step_avg:61.01ms
step:875/2330 train_time:53380ms step_avg:61.01ms
step:876/2330 train_time:53443ms step_avg:61.01ms
step:877/2330 train_time:53503ms step_avg:61.01ms
step:878/2330 train_time:53567ms step_avg:61.01ms
step:879/2330 train_time:53626ms step_avg:61.01ms
step:880/2330 train_time:53690ms step_avg:61.01ms
step:881/2330 train_time:53749ms step_avg:61.01ms
step:882/2330 train_time:53812ms step_avg:61.01ms
step:883/2330 train_time:53871ms step_avg:61.01ms
step:884/2330 train_time:53934ms step_avg:61.01ms
step:885/2330 train_time:53994ms step_avg:61.01ms
step:886/2330 train_time:54057ms step_avg:61.01ms
step:887/2330 train_time:54117ms step_avg:61.01ms
step:888/2330 train_time:54181ms step_avg:61.01ms
step:889/2330 train_time:54241ms step_avg:61.01ms
step:890/2330 train_time:54304ms step_avg:61.02ms
step:891/2330 train_time:54364ms step_avg:61.02ms
step:892/2330 train_time:54427ms step_avg:61.02ms
step:893/2330 train_time:54488ms step_avg:61.02ms
step:894/2330 train_time:54550ms step_avg:61.02ms
step:895/2330 train_time:54610ms step_avg:61.02ms
step:896/2330 train_time:54672ms step_avg:61.02ms
step:897/2330 train_time:54732ms step_avg:61.02ms
step:898/2330 train_time:54795ms step_avg:61.02ms
step:899/2330 train_time:54854ms step_avg:61.02ms
step:900/2330 train_time:54918ms step_avg:61.02ms
step:901/2330 train_time:54977ms step_avg:61.02ms
step:902/2330 train_time:55040ms step_avg:61.02ms
step:903/2330 train_time:55101ms step_avg:61.02ms
step:904/2330 train_time:55164ms step_avg:61.02ms
step:905/2330 train_time:55224ms step_avg:61.02ms
step:906/2330 train_time:55288ms step_avg:61.02ms
step:907/2330 train_time:55347ms step_avg:61.02ms
step:908/2330 train_time:55410ms step_avg:61.02ms
step:909/2330 train_time:55469ms step_avg:61.02ms
step:910/2330 train_time:55532ms step_avg:61.02ms
step:911/2330 train_time:55593ms step_avg:61.02ms
step:912/2330 train_time:55656ms step_avg:61.03ms
step:913/2330 train_time:55716ms step_avg:61.03ms
step:914/2330 train_time:55780ms step_avg:61.03ms
step:915/2330 train_time:55839ms step_avg:61.03ms
step:916/2330 train_time:55902ms step_avg:61.03ms
step:917/2330 train_time:55962ms step_avg:61.03ms
step:918/2330 train_time:56025ms step_avg:61.03ms
step:919/2330 train_time:56086ms step_avg:61.03ms
step:920/2330 train_time:56147ms step_avg:61.03ms
step:921/2330 train_time:56208ms step_avg:61.03ms
step:922/2330 train_time:56270ms step_avg:61.03ms
step:923/2330 train_time:56330ms step_avg:61.03ms
step:924/2330 train_time:56393ms step_avg:61.03ms
step:925/2330 train_time:56454ms step_avg:61.03ms
step:926/2330 train_time:56516ms step_avg:61.03ms
step:927/2330 train_time:56576ms step_avg:61.03ms
step:928/2330 train_time:56639ms step_avg:61.03ms
step:929/2330 train_time:56699ms step_avg:61.03ms
step:930/2330 train_time:56762ms step_avg:61.03ms
step:931/2330 train_time:56822ms step_avg:61.03ms
step:932/2330 train_time:56886ms step_avg:61.04ms
step:933/2330 train_time:56946ms step_avg:61.04ms
step:934/2330 train_time:57008ms step_avg:61.04ms
step:935/2330 train_time:57068ms step_avg:61.04ms
step:936/2330 train_time:57131ms step_avg:61.04ms
step:937/2330 train_time:57192ms step_avg:61.04ms
step:938/2330 train_time:57253ms step_avg:61.04ms
step:939/2330 train_time:57314ms step_avg:61.04ms
step:940/2330 train_time:57377ms step_avg:61.04ms
step:941/2330 train_time:57436ms step_avg:61.04ms
step:942/2330 train_time:57499ms step_avg:61.04ms
step:943/2330 train_time:57559ms step_avg:61.04ms
step:944/2330 train_time:57622ms step_avg:61.04ms
step:945/2330 train_time:57683ms step_avg:61.04ms
step:946/2330 train_time:57745ms step_avg:61.04ms
step:947/2330 train_time:57805ms step_avg:61.04ms
step:948/2330 train_time:57868ms step_avg:61.04ms
step:949/2330 train_time:57928ms step_avg:61.04ms
step:950/2330 train_time:57990ms step_avg:61.04ms
step:951/2330 train_time:58050ms step_avg:61.04ms
step:952/2330 train_time:58113ms step_avg:61.04ms
step:953/2330 train_time:58173ms step_avg:61.04ms
step:954/2330 train_time:58236ms step_avg:61.04ms
step:955/2330 train_time:58296ms step_avg:61.04ms
step:956/2330 train_time:58360ms step_avg:61.05ms
step:957/2330 train_time:58419ms step_avg:61.04ms
step:958/2330 train_time:58482ms step_avg:61.05ms
step:959/2330 train_time:58543ms step_avg:61.05ms
step:960/2330 train_time:58605ms step_avg:61.05ms
step:961/2330 train_time:58665ms step_avg:61.05ms
step:962/2330 train_time:58727ms step_avg:61.05ms
step:963/2330 train_time:58788ms step_avg:61.05ms
step:964/2330 train_time:58850ms step_avg:61.05ms
step:965/2330 train_time:58910ms step_avg:61.05ms
step:966/2330 train_time:58972ms step_avg:61.05ms
step:967/2330 train_time:59033ms step_avg:61.05ms
step:968/2330 train_time:59096ms step_avg:61.05ms
step:969/2330 train_time:59156ms step_avg:61.05ms
step:970/2330 train_time:59219ms step_avg:61.05ms
step:971/2330 train_time:59280ms step_avg:61.05ms
step:972/2330 train_time:59343ms step_avg:61.05ms
step:973/2330 train_time:59403ms step_avg:61.05ms
step:974/2330 train_time:59466ms step_avg:61.05ms
step:975/2330 train_time:59526ms step_avg:61.05ms
step:976/2330 train_time:59589ms step_avg:61.05ms
step:977/2330 train_time:59649ms step_avg:61.05ms
step:978/2330 train_time:59712ms step_avg:61.06ms
step:979/2330 train_time:59772ms step_avg:61.05ms
step:980/2330 train_time:59837ms step_avg:61.06ms
step:981/2330 train_time:59895ms step_avg:61.06ms
step:982/2330 train_time:59959ms step_avg:61.06ms
step:983/2330 train_time:60019ms step_avg:61.06ms
step:984/2330 train_time:60082ms step_avg:61.06ms
step:985/2330 train_time:60142ms step_avg:61.06ms
step:986/2330 train_time:60205ms step_avg:61.06ms
step:987/2330 train_time:60265ms step_avg:61.06ms
step:988/2330 train_time:60327ms step_avg:61.06ms
step:989/2330 train_time:60388ms step_avg:61.06ms
step:990/2330 train_time:60451ms step_avg:61.06ms
step:991/2330 train_time:60510ms step_avg:61.06ms
step:992/2330 train_time:60573ms step_avg:61.06ms
step:993/2330 train_time:60632ms step_avg:61.06ms
step:994/2330 train_time:60696ms step_avg:61.06ms
step:995/2330 train_time:60756ms step_avg:61.06ms
step:996/2330 train_time:60819ms step_avg:61.06ms
step:997/2330 train_time:60879ms step_avg:61.06ms
step:998/2330 train_time:60942ms step_avg:61.06ms
step:999/2330 train_time:61002ms step_avg:61.06ms
step:1000/2330 train_time:61065ms step_avg:61.07ms
step:1000/2330 val_loss:4.7439 train_time:61138ms step_avg:61.14ms
step:1001/2330 train_time:61158ms step_avg:61.10ms
step:1002/2330 train_time:61189ms step_avg:61.07ms
step:1003/2330 train_time:61250ms step_avg:61.07ms
step:1004/2330 train_time:61321ms step_avg:61.08ms
step:1005/2330 train_time:61384ms step_avg:61.08ms
step:1006/2330 train_time:61447ms step_avg:61.08ms
step:1007/2330 train_time:61508ms step_avg:61.08ms
step:1008/2330 train_time:61571ms step_avg:61.08ms
step:1009/2330 train_time:61630ms step_avg:61.08ms
step:1010/2330 train_time:61693ms step_avg:61.08ms
step:1011/2330 train_time:61752ms step_avg:61.08ms
step:1012/2330 train_time:61814ms step_avg:61.08ms
step:1013/2330 train_time:61874ms step_avg:61.08ms
step:1014/2330 train_time:61936ms step_avg:61.08ms
step:1015/2330 train_time:61995ms step_avg:61.08ms
step:1016/2330 train_time:62059ms step_avg:61.08ms
step:1017/2330 train_time:62121ms step_avg:61.08ms
step:1018/2330 train_time:62185ms step_avg:61.09ms
step:1019/2330 train_time:62245ms step_avg:61.08ms
step:1020/2330 train_time:62309ms step_avg:61.09ms
step:1021/2330 train_time:62372ms step_avg:61.09ms
step:1022/2330 train_time:62435ms step_avg:61.09ms
step:1023/2330 train_time:62497ms step_avg:61.09ms
step:1024/2330 train_time:62560ms step_avg:61.09ms
step:1025/2330 train_time:62620ms step_avg:61.09ms
step:1026/2330 train_time:62682ms step_avg:61.09ms
step:1027/2330 train_time:62741ms step_avg:61.09ms
step:1028/2330 train_time:62804ms step_avg:61.09ms
step:1029/2330 train_time:62863ms step_avg:61.09ms
step:1030/2330 train_time:62926ms step_avg:61.09ms
step:1031/2330 train_time:62986ms step_avg:61.09ms
step:1032/2330 train_time:63049ms step_avg:61.09ms
step:1033/2330 train_time:63108ms step_avg:61.09ms
step:1034/2330 train_time:63172ms step_avg:61.09ms
step:1035/2330 train_time:63232ms step_avg:61.09ms
step:1036/2330 train_time:63297ms step_avg:61.10ms
step:1037/2330 train_time:63358ms step_avg:61.10ms
step:1038/2330 train_time:63422ms step_avg:61.10ms
step:1039/2330 train_time:63482ms step_avg:61.10ms
step:1040/2330 train_time:63546ms step_avg:61.10ms
step:1041/2330 train_time:63605ms step_avg:61.10ms
step:1042/2330 train_time:63668ms step_avg:61.10ms
step:1043/2330 train_time:63728ms step_avg:61.10ms
step:1044/2330 train_time:63791ms step_avg:61.10ms
step:1045/2330 train_time:63851ms step_avg:61.10ms
step:1046/2330 train_time:63913ms step_avg:61.10ms
step:1047/2330 train_time:63974ms step_avg:61.10ms
step:1048/2330 train_time:64036ms step_avg:61.10ms
step:1049/2330 train_time:64096ms step_avg:61.10ms
step:1050/2330 train_time:64159ms step_avg:61.10ms
step:1051/2330 train_time:64219ms step_avg:61.10ms
step:1052/2330 train_time:64282ms step_avg:61.10ms
step:1053/2330 train_time:64342ms step_avg:61.10ms
step:1054/2330 train_time:64405ms step_avg:61.11ms
step:1055/2330 train_time:64466ms step_avg:61.10ms
step:1056/2330 train_time:64529ms step_avg:61.11ms
step:1057/2330 train_time:64590ms step_avg:61.11ms
step:1058/2330 train_time:64654ms step_avg:61.11ms
step:1059/2330 train_time:64713ms step_avg:61.11ms
step:1060/2330 train_time:64776ms step_avg:61.11ms
step:1061/2330 train_time:64836ms step_avg:61.11ms
step:1062/2330 train_time:64899ms step_avg:61.11ms
step:1063/2330 train_time:64959ms step_avg:61.11ms
step:1064/2330 train_time:65022ms step_avg:61.11ms
step:1065/2330 train_time:65081ms step_avg:61.11ms
step:1066/2330 train_time:65144ms step_avg:61.11ms
step:1067/2330 train_time:65204ms step_avg:61.11ms
step:1068/2330 train_time:65268ms step_avg:61.11ms
step:1069/2330 train_time:65327ms step_avg:61.11ms
step:1070/2330 train_time:65392ms step_avg:61.11ms
step:1071/2330 train_time:65452ms step_avg:61.11ms
step:1072/2330 train_time:65515ms step_avg:61.11ms
step:1073/2330 train_time:65576ms step_avg:61.11ms
step:1074/2330 train_time:65639ms step_avg:61.12ms
step:1075/2330 train_time:65699ms step_avg:61.12ms
step:1076/2330 train_time:65762ms step_avg:61.12ms
step:1077/2330 train_time:65821ms step_avg:61.11ms
step:1078/2330 train_time:65884ms step_avg:61.12ms
step:1079/2330 train_time:65943ms step_avg:61.12ms
step:1080/2330 train_time:66006ms step_avg:61.12ms
step:1081/2330 train_time:66066ms step_avg:61.12ms
step:1082/2330 train_time:66130ms step_avg:61.12ms
step:1083/2330 train_time:66190ms step_avg:61.12ms
step:1084/2330 train_time:66253ms step_avg:61.12ms
step:1085/2330 train_time:66313ms step_avg:61.12ms
step:1086/2330 train_time:66378ms step_avg:61.12ms
step:1087/2330 train_time:66438ms step_avg:61.12ms
step:1088/2330 train_time:66501ms step_avg:61.12ms
step:1089/2330 train_time:66561ms step_avg:61.12ms
step:1090/2330 train_time:66625ms step_avg:61.12ms
step:1091/2330 train_time:66685ms step_avg:61.12ms
step:1092/2330 train_time:66748ms step_avg:61.12ms
step:1093/2330 train_time:66808ms step_avg:61.12ms
step:1094/2330 train_time:66872ms step_avg:61.13ms
step:1095/2330 train_time:66932ms step_avg:61.13ms
step:1096/2330 train_time:66996ms step_avg:61.13ms
step:1097/2330 train_time:67056ms step_avg:61.13ms
step:1098/2330 train_time:67119ms step_avg:61.13ms
step:1099/2330 train_time:67179ms step_avg:61.13ms
step:1100/2330 train_time:67243ms step_avg:61.13ms
step:1101/2330 train_time:67303ms step_avg:61.13ms
step:1102/2330 train_time:67366ms step_avg:61.13ms
step:1103/2330 train_time:67425ms step_avg:61.13ms
step:1104/2330 train_time:67488ms step_avg:61.13ms
step:1105/2330 train_time:67548ms step_avg:61.13ms
step:1106/2330 train_time:67612ms step_avg:61.13ms
step:1107/2330 train_time:67672ms step_avg:61.13ms
step:1108/2330 train_time:67735ms step_avg:61.13ms
step:1109/2330 train_time:67796ms step_avg:61.13ms
step:1110/2330 train_time:67860ms step_avg:61.14ms
step:1111/2330 train_time:67920ms step_avg:61.13ms
step:1112/2330 train_time:67983ms step_avg:61.14ms
step:1113/2330 train_time:68043ms step_avg:61.13ms
step:1114/2330 train_time:68106ms step_avg:61.14ms
step:1115/2330 train_time:68166ms step_avg:61.14ms
step:1116/2330 train_time:68229ms step_avg:61.14ms
step:1117/2330 train_time:68289ms step_avg:61.14ms
step:1118/2330 train_time:68352ms step_avg:61.14ms
step:1119/2330 train_time:68411ms step_avg:61.14ms
step:1120/2330 train_time:68476ms step_avg:61.14ms
step:1121/2330 train_time:68537ms step_avg:61.14ms
step:1122/2330 train_time:68600ms step_avg:61.14ms
step:1123/2330 train_time:68659ms step_avg:61.14ms
step:1124/2330 train_time:68722ms step_avg:61.14ms
step:1125/2330 train_time:68782ms step_avg:61.14ms
step:1126/2330 train_time:68845ms step_avg:61.14ms
step:1127/2330 train_time:68905ms step_avg:61.14ms
step:1128/2330 train_time:68968ms step_avg:61.14ms
step:1129/2330 train_time:69028ms step_avg:61.14ms
step:1130/2330 train_time:69091ms step_avg:61.14ms
step:1131/2330 train_time:69151ms step_avg:61.14ms
step:1132/2330 train_time:69215ms step_avg:61.14ms
step:1133/2330 train_time:69276ms step_avg:61.14ms
step:1134/2330 train_time:69338ms step_avg:61.14ms
step:1135/2330 train_time:69398ms step_avg:61.14ms
step:1136/2330 train_time:69462ms step_avg:61.15ms
step:1137/2330 train_time:69521ms step_avg:61.14ms
step:1138/2330 train_time:69584ms step_avg:61.15ms
step:1139/2330 train_time:69643ms step_avg:61.14ms
step:1140/2330 train_time:69707ms step_avg:61.15ms
step:1141/2330 train_time:69767ms step_avg:61.15ms
step:1142/2330 train_time:69830ms step_avg:61.15ms
step:1143/2330 train_time:69891ms step_avg:61.15ms
step:1144/2330 train_time:69954ms step_avg:61.15ms
step:1145/2330 train_time:70015ms step_avg:61.15ms
step:1146/2330 train_time:70078ms step_avg:61.15ms
step:1147/2330 train_time:70139ms step_avg:61.15ms
step:1148/2330 train_time:70203ms step_avg:61.15ms
step:1149/2330 train_time:70264ms step_avg:61.15ms
step:1150/2330 train_time:70326ms step_avg:61.15ms
step:1151/2330 train_time:70386ms step_avg:61.15ms
step:1152/2330 train_time:70449ms step_avg:61.15ms
step:1153/2330 train_time:70508ms step_avg:61.15ms
step:1154/2330 train_time:70572ms step_avg:61.15ms
step:1155/2330 train_time:70631ms step_avg:61.15ms
step:1156/2330 train_time:70695ms step_avg:61.15ms
step:1157/2330 train_time:70756ms step_avg:61.15ms
step:1158/2330 train_time:70820ms step_avg:61.16ms
step:1159/2330 train_time:70879ms step_avg:61.16ms
step:1160/2330 train_time:70942ms step_avg:61.16ms
step:1161/2330 train_time:71002ms step_avg:61.16ms
step:1162/2330 train_time:71065ms step_avg:61.16ms
step:1163/2330 train_time:71126ms step_avg:61.16ms
step:1164/2330 train_time:71190ms step_avg:61.16ms
step:1165/2330 train_time:71250ms step_avg:61.16ms
step:1166/2330 train_time:71313ms step_avg:61.16ms
step:1167/2330 train_time:71373ms step_avg:61.16ms
step:1168/2330 train_time:71437ms step_avg:61.16ms
step:1169/2330 train_time:71497ms step_avg:61.16ms
step:1170/2330 train_time:71561ms step_avg:61.16ms
step:1171/2330 train_time:71621ms step_avg:61.16ms
step:1172/2330 train_time:71683ms step_avg:61.16ms
step:1173/2330 train_time:71744ms step_avg:61.16ms
step:1174/2330 train_time:71806ms step_avg:61.16ms
step:1175/2330 train_time:71866ms step_avg:61.16ms
step:1176/2330 train_time:71928ms step_avg:61.16ms
step:1177/2330 train_time:71988ms step_avg:61.16ms
step:1178/2330 train_time:72052ms step_avg:61.16ms
step:1179/2330 train_time:72111ms step_avg:61.16ms
step:1180/2330 train_time:72175ms step_avg:61.16ms
step:1181/2330 train_time:72236ms step_avg:61.17ms
step:1182/2330 train_time:72299ms step_avg:61.17ms
step:1183/2330 train_time:72359ms step_avg:61.17ms
step:1184/2330 train_time:72422ms step_avg:61.17ms
step:1185/2330 train_time:72482ms step_avg:61.17ms
step:1186/2330 train_time:72545ms step_avg:61.17ms
step:1187/2330 train_time:72604ms step_avg:61.17ms
step:1188/2330 train_time:72668ms step_avg:61.17ms
step:1189/2330 train_time:72729ms step_avg:61.17ms
step:1190/2330 train_time:72792ms step_avg:61.17ms
step:1191/2330 train_time:72852ms step_avg:61.17ms
step:1192/2330 train_time:72916ms step_avg:61.17ms
step:1193/2330 train_time:72976ms step_avg:61.17ms
step:1194/2330 train_time:73039ms step_avg:61.17ms
step:1195/2330 train_time:73099ms step_avg:61.17ms
step:1196/2330 train_time:73163ms step_avg:61.17ms
step:1197/2330 train_time:73222ms step_avg:61.17ms
step:1198/2330 train_time:73285ms step_avg:61.17ms
step:1199/2330 train_time:73344ms step_avg:61.17ms
step:1200/2330 train_time:73407ms step_avg:61.17ms
step:1201/2330 train_time:73467ms step_avg:61.17ms
step:1202/2330 train_time:73530ms step_avg:61.17ms
step:1203/2330 train_time:73591ms step_avg:61.17ms
step:1204/2330 train_time:73654ms step_avg:61.17ms
step:1205/2330 train_time:73715ms step_avg:61.17ms
step:1206/2330 train_time:73779ms step_avg:61.18ms
step:1207/2330 train_time:73839ms step_avg:61.18ms
step:1208/2330 train_time:73902ms step_avg:61.18ms
step:1209/2330 train_time:73962ms step_avg:61.18ms
step:1210/2330 train_time:74025ms step_avg:61.18ms
step:1211/2330 train_time:74084ms step_avg:61.18ms
step:1212/2330 train_time:74148ms step_avg:61.18ms
step:1213/2330 train_time:74208ms step_avg:61.18ms
step:1214/2330 train_time:74271ms step_avg:61.18ms
step:1215/2330 train_time:74331ms step_avg:61.18ms
step:1216/2330 train_time:74395ms step_avg:61.18ms
step:1217/2330 train_time:74456ms step_avg:61.18ms
step:1218/2330 train_time:74519ms step_avg:61.18ms
step:1219/2330 train_time:74579ms step_avg:61.18ms
step:1220/2330 train_time:74642ms step_avg:61.18ms
step:1221/2330 train_time:74702ms step_avg:61.18ms
step:1222/2330 train_time:74765ms step_avg:61.18ms
step:1223/2330 train_time:74825ms step_avg:61.18ms
step:1224/2330 train_time:74888ms step_avg:61.18ms
step:1225/2330 train_time:74948ms step_avg:61.18ms
step:1226/2330 train_time:75011ms step_avg:61.18ms
step:1227/2330 train_time:75073ms step_avg:61.18ms
step:1228/2330 train_time:75135ms step_avg:61.18ms
step:1229/2330 train_time:75196ms step_avg:61.18ms
step:1230/2330 train_time:75260ms step_avg:61.19ms
step:1231/2330 train_time:75320ms step_avg:61.19ms
step:1232/2330 train_time:75382ms step_avg:61.19ms
step:1233/2330 train_time:75443ms step_avg:61.19ms
step:1234/2330 train_time:75506ms step_avg:61.19ms
step:1235/2330 train_time:75566ms step_avg:61.19ms
step:1236/2330 train_time:75629ms step_avg:61.19ms
step:1237/2330 train_time:75689ms step_avg:61.19ms
step:1238/2330 train_time:75753ms step_avg:61.19ms
step:1239/2330 train_time:75813ms step_avg:61.19ms
step:1240/2330 train_time:75877ms step_avg:61.19ms
step:1241/2330 train_time:75936ms step_avg:61.19ms
step:1242/2330 train_time:76000ms step_avg:61.19ms
step:1243/2330 train_time:76061ms step_avg:61.19ms
step:1244/2330 train_time:76123ms step_avg:61.19ms
step:1245/2330 train_time:76184ms step_avg:61.19ms
step:1246/2330 train_time:76246ms step_avg:61.19ms
step:1247/2330 train_time:76306ms step_avg:61.19ms
step:1248/2330 train_time:76370ms step_avg:61.19ms
step:1249/2330 train_time:76430ms step_avg:61.19ms
step:1250/2330 train_time:76494ms step_avg:61.19ms
step:1250/2330 val_loss:4.6414 train_time:76566ms step_avg:61.25ms
step:1251/2330 train_time:76589ms step_avg:61.22ms
step:1252/2330 train_time:76618ms step_avg:61.20ms
step:1253/2330 train_time:76682ms step_avg:61.20ms
step:1254/2330 train_time:76749ms step_avg:61.20ms
step:1255/2330 train_time:76809ms step_avg:61.20ms
step:1256/2330 train_time:76872ms step_avg:61.20ms
step:1257/2330 train_time:76932ms step_avg:61.20ms
step:1258/2330 train_time:76994ms step_avg:61.20ms
step:1259/2330 train_time:77054ms step_avg:61.20ms
step:1260/2330 train_time:77117ms step_avg:61.20ms
step:1261/2330 train_time:77176ms step_avg:61.20ms
step:1262/2330 train_time:77238ms step_avg:61.20ms
step:1263/2330 train_time:77297ms step_avg:61.20ms
step:1264/2330 train_time:77359ms step_avg:61.20ms
step:1265/2330 train_time:77419ms step_avg:61.20ms
step:1266/2330 train_time:77483ms step_avg:61.20ms
step:1267/2330 train_time:77543ms step_avg:61.20ms
step:1268/2330 train_time:77608ms step_avg:61.20ms
step:1269/2330 train_time:77670ms step_avg:61.21ms
step:1270/2330 train_time:77734ms step_avg:61.21ms
step:1271/2330 train_time:77794ms step_avg:61.21ms
step:1272/2330 train_time:77857ms step_avg:61.21ms
step:1273/2330 train_time:77917ms step_avg:61.21ms
step:1274/2330 train_time:77979ms step_avg:61.21ms
step:1275/2330 train_time:78039ms step_avg:61.21ms
step:1276/2330 train_time:78102ms step_avg:61.21ms
step:1277/2330 train_time:78162ms step_avg:61.21ms
step:1278/2330 train_time:78225ms step_avg:61.21ms
step:1279/2330 train_time:78284ms step_avg:61.21ms
step:1280/2330 train_time:78346ms step_avg:61.21ms
step:1281/2330 train_time:78406ms step_avg:61.21ms
step:1282/2330 train_time:78469ms step_avg:61.21ms
step:1283/2330 train_time:78529ms step_avg:61.21ms
step:1284/2330 train_time:78592ms step_avg:61.21ms
step:1285/2330 train_time:78652ms step_avg:61.21ms
step:1286/2330 train_time:78715ms step_avg:61.21ms
step:1287/2330 train_time:78777ms step_avg:61.21ms
step:1288/2330 train_time:78840ms step_avg:61.21ms
step:1289/2330 train_time:78900ms step_avg:61.21ms
step:1290/2330 train_time:78963ms step_avg:61.21ms
step:1291/2330 train_time:79024ms step_avg:61.21ms
step:1292/2330 train_time:79087ms step_avg:61.21ms
step:1293/2330 train_time:79147ms step_avg:61.21ms
step:1294/2330 train_time:79210ms step_avg:61.21ms
step:1295/2330 train_time:79270ms step_avg:61.21ms
step:1296/2330 train_time:79333ms step_avg:61.21ms
step:1297/2330 train_time:79392ms step_avg:61.21ms
step:1298/2330 train_time:79454ms step_avg:61.21ms
step:1299/2330 train_time:79515ms step_avg:61.21ms
step:1300/2330 train_time:79579ms step_avg:61.21ms
step:1301/2330 train_time:79639ms step_avg:61.21ms
step:1302/2330 train_time:79701ms step_avg:61.21ms
step:1303/2330 train_time:79761ms step_avg:61.21ms
step:1304/2330 train_time:79825ms step_avg:61.22ms
step:1305/2330 train_time:79885ms step_avg:61.21ms
step:1306/2330 train_time:79949ms step_avg:61.22ms
step:1307/2330 train_time:80009ms step_avg:61.22ms
step:1308/2330 train_time:80071ms step_avg:61.22ms
step:1309/2330 train_time:80131ms step_avg:61.22ms
step:1310/2330 train_time:80195ms step_avg:61.22ms
step:1311/2330 train_time:80255ms step_avg:61.22ms
step:1312/2330 train_time:80318ms step_avg:61.22ms
step:1313/2330 train_time:80377ms step_avg:61.22ms
step:1314/2330 train_time:80439ms step_avg:61.22ms
step:1315/2330 train_time:80499ms step_avg:61.22ms
step:1316/2330 train_time:80561ms step_avg:61.22ms
step:1317/2330 train_time:80621ms step_avg:61.22ms
step:1318/2330 train_time:80684ms step_avg:61.22ms
step:1319/2330 train_time:80745ms step_avg:61.22ms
step:1320/2330 train_time:80808ms step_avg:61.22ms
step:1321/2330 train_time:80868ms step_avg:61.22ms
step:1322/2330 train_time:80931ms step_avg:61.22ms
step:1323/2330 train_time:80991ms step_avg:61.22ms
step:1324/2330 train_time:81053ms step_avg:61.22ms
step:1325/2330 train_time:81113ms step_avg:61.22ms
step:1326/2330 train_time:81177ms step_avg:61.22ms
step:1327/2330 train_time:81236ms step_avg:61.22ms
step:1328/2330 train_time:81299ms step_avg:61.22ms
step:1329/2330 train_time:81359ms step_avg:61.22ms
step:1330/2330 train_time:81421ms step_avg:61.22ms
step:1331/2330 train_time:81482ms step_avg:61.22ms
step:1332/2330 train_time:81544ms step_avg:61.22ms
step:1333/2330 train_time:81604ms step_avg:61.22ms
step:1334/2330 train_time:81666ms step_avg:61.22ms
step:1335/2330 train_time:81727ms step_avg:61.22ms
step:1336/2330 train_time:81790ms step_avg:61.22ms
step:1337/2330 train_time:81850ms step_avg:61.22ms
step:1338/2330 train_time:81914ms step_avg:61.22ms
step:1339/2330 train_time:81975ms step_avg:61.22ms
step:1340/2330 train_time:82037ms step_avg:61.22ms
step:1341/2330 train_time:82097ms step_avg:61.22ms
step:1342/2330 train_time:82160ms step_avg:61.22ms
step:1343/2330 train_time:82220ms step_avg:61.22ms
step:1344/2330 train_time:82283ms step_avg:61.22ms
step:1345/2330 train_time:82343ms step_avg:61.22ms
step:1346/2330 train_time:82405ms step_avg:61.22ms
step:1347/2330 train_time:82465ms step_avg:61.22ms
step:1348/2330 train_time:82528ms step_avg:61.22ms
step:1349/2330 train_time:82588ms step_avg:61.22ms
step:1350/2330 train_time:82651ms step_avg:61.22ms
step:1351/2330 train_time:82711ms step_avg:61.22ms
step:1352/2330 train_time:82774ms step_avg:61.22ms
step:1353/2330 train_time:82833ms step_avg:61.22ms
step:1354/2330 train_time:82897ms step_avg:61.22ms
step:1355/2330 train_time:82957ms step_avg:61.22ms
step:1356/2330 train_time:83020ms step_avg:61.22ms
step:1357/2330 train_time:83080ms step_avg:61.22ms
step:1358/2330 train_time:83142ms step_avg:61.22ms
step:1359/2330 train_time:83202ms step_avg:61.22ms
step:1360/2330 train_time:83265ms step_avg:61.22ms
step:1361/2330 train_time:83325ms step_avg:61.22ms
step:1362/2330 train_time:83388ms step_avg:61.22ms
step:1363/2330 train_time:83447ms step_avg:61.22ms
step:1364/2330 train_time:83511ms step_avg:61.22ms
step:1365/2330 train_time:83570ms step_avg:61.22ms
step:1366/2330 train_time:83633ms step_avg:61.22ms
step:1367/2330 train_time:83693ms step_avg:61.22ms
step:1368/2330 train_time:83755ms step_avg:61.22ms
step:1369/2330 train_time:83816ms step_avg:61.22ms
step:1370/2330 train_time:83880ms step_avg:61.23ms
step:1371/2330 train_time:83939ms step_avg:61.22ms
step:1372/2330 train_time:84002ms step_avg:61.23ms
step:1373/2330 train_time:84062ms step_avg:61.23ms
step:1374/2330 train_time:84125ms step_avg:61.23ms
step:1375/2330 train_time:84186ms step_avg:61.23ms
step:1376/2330 train_time:84249ms step_avg:61.23ms
step:1377/2330 train_time:84309ms step_avg:61.23ms
step:1378/2330 train_time:84372ms step_avg:61.23ms
step:1379/2330 train_time:84432ms step_avg:61.23ms
step:1380/2330 train_time:84495ms step_avg:61.23ms
step:1381/2330 train_time:84556ms step_avg:61.23ms
step:1382/2330 train_time:84619ms step_avg:61.23ms
step:1383/2330 train_time:84678ms step_avg:61.23ms
step:1384/2330 train_time:84741ms step_avg:61.23ms
step:1385/2330 train_time:84802ms step_avg:61.23ms
step:1386/2330 train_time:84864ms step_avg:61.23ms
step:1387/2330 train_time:84924ms step_avg:61.23ms
step:1388/2330 train_time:84988ms step_avg:61.23ms
step:1389/2330 train_time:85047ms step_avg:61.23ms
step:1390/2330 train_time:85111ms step_avg:61.23ms
step:1391/2330 train_time:85171ms step_avg:61.23ms
step:1392/2330 train_time:85234ms step_avg:61.23ms
step:1393/2330 train_time:85294ms step_avg:61.23ms
step:1394/2330 train_time:85358ms step_avg:61.23ms
step:1395/2330 train_time:85418ms step_avg:61.23ms
step:1396/2330 train_time:85481ms step_avg:61.23ms
step:1397/2330 train_time:85541ms step_avg:61.23ms
step:1398/2330 train_time:85603ms step_avg:61.23ms
step:1399/2330 train_time:85663ms step_avg:61.23ms
step:1400/2330 train_time:85726ms step_avg:61.23ms
step:1401/2330 train_time:85787ms step_avg:61.23ms
step:1402/2330 train_time:85848ms step_avg:61.23ms
step:1403/2330 train_time:85909ms step_avg:61.23ms
step:1404/2330 train_time:85972ms step_avg:61.23ms
step:1405/2330 train_time:86032ms step_avg:61.23ms
step:1406/2330 train_time:86095ms step_avg:61.23ms
step:1407/2330 train_time:86155ms step_avg:61.23ms
step:1408/2330 train_time:86218ms step_avg:61.23ms
step:1409/2330 train_time:86279ms step_avg:61.23ms
step:1410/2330 train_time:86341ms step_avg:61.24ms
step:1411/2330 train_time:86401ms step_avg:61.23ms
step:1412/2330 train_time:86464ms step_avg:61.24ms
step:1413/2330 train_time:86523ms step_avg:61.23ms
step:1414/2330 train_time:86587ms step_avg:61.24ms
step:1415/2330 train_time:86647ms step_avg:61.23ms
step:1416/2330 train_time:86710ms step_avg:61.24ms
step:1417/2330 train_time:86770ms step_avg:61.24ms
step:1418/2330 train_time:86834ms step_avg:61.24ms
step:1419/2330 train_time:86894ms step_avg:61.24ms
step:1420/2330 train_time:86957ms step_avg:61.24ms
step:1421/2330 train_time:87017ms step_avg:61.24ms
step:1422/2330 train_time:87080ms step_avg:61.24ms
step:1423/2330 train_time:87140ms step_avg:61.24ms
step:1424/2330 train_time:87203ms step_avg:61.24ms
step:1425/2330 train_time:87262ms step_avg:61.24ms
step:1426/2330 train_time:87326ms step_avg:61.24ms
step:1427/2330 train_time:87386ms step_avg:61.24ms
step:1428/2330 train_time:87450ms step_avg:61.24ms
step:1429/2330 train_time:87510ms step_avg:61.24ms
step:1430/2330 train_time:87573ms step_avg:61.24ms
step:1431/2330 train_time:87634ms step_avg:61.24ms
step:1432/2330 train_time:87696ms step_avg:61.24ms
step:1433/2330 train_time:87756ms step_avg:61.24ms
step:1434/2330 train_time:87819ms step_avg:61.24ms
step:1435/2330 train_time:87880ms step_avg:61.24ms
step:1436/2330 train_time:87942ms step_avg:61.24ms
step:1437/2330 train_time:88002ms step_avg:61.24ms
step:1438/2330 train_time:88065ms step_avg:61.24ms
step:1439/2330 train_time:88125ms step_avg:61.24ms
step:1440/2330 train_time:88188ms step_avg:61.24ms
step:1441/2330 train_time:88248ms step_avg:61.24ms
step:1442/2330 train_time:88311ms step_avg:61.24ms
step:1443/2330 train_time:88371ms step_avg:61.24ms
step:1444/2330 train_time:88435ms step_avg:61.24ms
step:1445/2330 train_time:88495ms step_avg:61.24ms
step:1446/2330 train_time:88558ms step_avg:61.24ms
step:1447/2330 train_time:88617ms step_avg:61.24ms
step:1448/2330 train_time:88680ms step_avg:61.24ms
step:1449/2330 train_time:88739ms step_avg:61.24ms
step:1450/2330 train_time:88803ms step_avg:61.24ms
step:1451/2330 train_time:88862ms step_avg:61.24ms
step:1452/2330 train_time:88925ms step_avg:61.24ms
step:1453/2330 train_time:88986ms step_avg:61.24ms
step:1454/2330 train_time:89048ms step_avg:61.24ms
step:1455/2330 train_time:89108ms step_avg:61.24ms
step:1456/2330 train_time:89171ms step_avg:61.24ms
step:1457/2330 train_time:89230ms step_avg:61.24ms
step:1458/2330 train_time:89293ms step_avg:61.24ms
step:1459/2330 train_time:89353ms step_avg:61.24ms
step:1460/2330 train_time:89416ms step_avg:61.24ms
step:1461/2330 train_time:89476ms step_avg:61.24ms
step:1462/2330 train_time:89539ms step_avg:61.24ms
step:1463/2330 train_time:89599ms step_avg:61.24ms
step:1464/2330 train_time:89662ms step_avg:61.24ms
step:1465/2330 train_time:89722ms step_avg:61.24ms
step:1466/2330 train_time:89785ms step_avg:61.25ms
step:1467/2330 train_time:89845ms step_avg:61.24ms
step:1468/2330 train_time:89908ms step_avg:61.25ms
step:1469/2330 train_time:89968ms step_avg:61.24ms
step:1470/2330 train_time:90031ms step_avg:61.25ms
step:1471/2330 train_time:90091ms step_avg:61.24ms
step:1472/2330 train_time:90154ms step_avg:61.25ms
step:1473/2330 train_time:90214ms step_avg:61.25ms
step:1474/2330 train_time:90277ms step_avg:61.25ms
step:1475/2330 train_time:90337ms step_avg:61.25ms
step:1476/2330 train_time:90400ms step_avg:61.25ms
step:1477/2330 train_time:90459ms step_avg:61.25ms
step:1478/2330 train_time:90522ms step_avg:61.25ms
step:1479/2330 train_time:90582ms step_avg:61.25ms
step:1480/2330 train_time:90644ms step_avg:61.25ms
step:1481/2330 train_time:90704ms step_avg:61.25ms
step:1482/2330 train_time:90768ms step_avg:61.25ms
step:1483/2330 train_time:90827ms step_avg:61.25ms
step:1484/2330 train_time:90891ms step_avg:61.25ms
step:1485/2330 train_time:90951ms step_avg:61.25ms
step:1486/2330 train_time:91014ms step_avg:61.25ms
step:1487/2330 train_time:91075ms step_avg:61.25ms
step:1488/2330 train_time:91138ms step_avg:61.25ms
step:1489/2330 train_time:91197ms step_avg:61.25ms
step:1490/2330 train_time:91260ms step_avg:61.25ms
step:1491/2330 train_time:91320ms step_avg:61.25ms
step:1492/2330 train_time:91384ms step_avg:61.25ms
step:1493/2330 train_time:91443ms step_avg:61.25ms
step:1494/2330 train_time:91506ms step_avg:61.25ms
step:1495/2330 train_time:91566ms step_avg:61.25ms
step:1496/2330 train_time:91629ms step_avg:61.25ms
step:1497/2330 train_time:91688ms step_avg:61.25ms
step:1498/2330 train_time:91753ms step_avg:61.25ms
step:1499/2330 train_time:91813ms step_avg:61.25ms
step:1500/2330 train_time:91876ms step_avg:61.25ms
step:1500/2330 val_loss:4.4554 train_time:91949ms step_avg:61.30ms
step:1501/2330 train_time:91969ms step_avg:61.27ms
step:1502/2330 train_time:92001ms step_avg:61.25ms
step:1503/2330 train_time:92065ms step_avg:61.25ms
step:1504/2330 train_time:92131ms step_avg:61.26ms
step:1505/2330 train_time:92192ms step_avg:61.26ms
step:1506/2330 train_time:92255ms step_avg:61.26ms
step:1507/2330 train_time:92315ms step_avg:61.26ms
step:1508/2330 train_time:92377ms step_avg:61.26ms
step:1509/2330 train_time:92437ms step_avg:61.26ms
step:1510/2330 train_time:92500ms step_avg:61.26ms
step:1511/2330 train_time:92558ms step_avg:61.26ms
step:1512/2330 train_time:92620ms step_avg:61.26ms
step:1513/2330 train_time:92680ms step_avg:61.26ms
step:1514/2330 train_time:92742ms step_avg:61.26ms
step:1515/2330 train_time:92802ms step_avg:61.26ms
step:1516/2330 train_time:92864ms step_avg:61.26ms
step:1517/2330 train_time:92924ms step_avg:61.26ms
step:1518/2330 train_time:92989ms step_avg:61.26ms
step:1519/2330 train_time:93051ms step_avg:61.26ms
step:1520/2330 train_time:93114ms step_avg:61.26ms
step:1521/2330 train_time:93175ms step_avg:61.26ms
step:1522/2330 train_time:93238ms step_avg:61.26ms
step:1523/2330 train_time:93298ms step_avg:61.26ms
step:1524/2330 train_time:93361ms step_avg:61.26ms
step:1525/2330 train_time:93420ms step_avg:61.26ms
step:1526/2330 train_time:93483ms step_avg:61.26ms
step:1527/2330 train_time:93542ms step_avg:61.26ms
step:1528/2330 train_time:93604ms step_avg:61.26ms
step:1529/2330 train_time:93665ms step_avg:61.26ms
step:1530/2330 train_time:93726ms step_avg:61.26ms
step:1531/2330 train_time:93786ms step_avg:61.26ms
step:1532/2330 train_time:93849ms step_avg:61.26ms
step:1533/2330 train_time:93910ms step_avg:61.26ms
step:1534/2330 train_time:93973ms step_avg:61.26ms
step:1535/2330 train_time:94035ms step_avg:61.26ms
step:1536/2330 train_time:94099ms step_avg:61.26ms
step:1537/2330 train_time:94159ms step_avg:61.26ms
step:1538/2330 train_time:94223ms step_avg:61.26ms
step:1539/2330 train_time:94284ms step_avg:61.26ms
step:1540/2330 train_time:94347ms step_avg:61.26ms
step:1541/2330 train_time:94407ms step_avg:61.26ms
step:1542/2330 train_time:94472ms step_avg:61.27ms
step:1543/2330 train_time:94532ms step_avg:61.27ms
step:1544/2330 train_time:94596ms step_avg:61.27ms
step:1545/2330 train_time:94656ms step_avg:61.27ms
step:1546/2330 train_time:94720ms step_avg:61.27ms
step:1547/2330 train_time:94781ms step_avg:61.27ms
step:1548/2330 train_time:94844ms step_avg:61.27ms
step:1549/2330 train_time:94905ms step_avg:61.27ms
step:1550/2330 train_time:94968ms step_avg:61.27ms
step:1551/2330 train_time:95028ms step_avg:61.27ms
step:1552/2330 train_time:95092ms step_avg:61.27ms
step:1553/2330 train_time:95152ms step_avg:61.27ms
step:1554/2330 train_time:95216ms step_avg:61.27ms
step:1555/2330 train_time:95279ms step_avg:61.27ms
step:1556/2330 train_time:95342ms step_avg:61.27ms
step:1557/2330 train_time:95403ms step_avg:61.27ms
step:1558/2330 train_time:95465ms step_avg:61.27ms
step:1559/2330 train_time:95525ms step_avg:61.27ms
step:1560/2330 train_time:95588ms step_avg:61.27ms
step:1561/2330 train_time:95648ms step_avg:61.27ms
step:1562/2330 train_time:95712ms step_avg:61.28ms
step:1563/2330 train_time:95772ms step_avg:61.27ms
step:1564/2330 train_time:95835ms step_avg:61.28ms
step:1565/2330 train_time:95897ms step_avg:61.28ms
step:1566/2330 train_time:95961ms step_avg:61.28ms
step:1567/2330 train_time:96022ms step_avg:61.28ms
step:1568/2330 train_time:96085ms step_avg:61.28ms
step:1569/2330 train_time:96144ms step_avg:61.28ms
step:1570/2330 train_time:96208ms step_avg:61.28ms
step:1571/2330 train_time:96269ms step_avg:61.28ms
step:1572/2330 train_time:96333ms step_avg:61.28ms
step:1573/2330 train_time:96394ms step_avg:61.28ms
step:1574/2330 train_time:96457ms step_avg:61.28ms
step:1575/2330 train_time:96518ms step_avg:61.28ms
step:1576/2330 train_time:96582ms step_avg:61.28ms
step:1577/2330 train_time:96642ms step_avg:61.28ms
step:1578/2330 train_time:96706ms step_avg:61.28ms
step:1579/2330 train_time:96766ms step_avg:61.28ms
step:1580/2330 train_time:96830ms step_avg:61.28ms
step:1581/2330 train_time:96890ms step_avg:61.28ms
step:1582/2330 train_time:96953ms step_avg:61.29ms
step:1583/2330 train_time:97015ms step_avg:61.29ms
step:1584/2330 train_time:97080ms step_avg:61.29ms
step:1585/2330 train_time:97140ms step_avg:61.29ms
step:1586/2330 train_time:97204ms step_avg:61.29ms
step:1587/2330 train_time:97263ms step_avg:61.29ms
step:1588/2330 train_time:97326ms step_avg:61.29ms
step:1589/2330 train_time:97387ms step_avg:61.29ms
step:1590/2330 train_time:97451ms step_avg:61.29ms
step:1591/2330 train_time:97511ms step_avg:61.29ms
step:1592/2330 train_time:97574ms step_avg:61.29ms
step:1593/2330 train_time:97636ms step_avg:61.29ms
step:1594/2330 train_time:97701ms step_avg:61.29ms
step:1595/2330 train_time:97762ms step_avg:61.29ms
step:1596/2330 train_time:97825ms step_avg:61.29ms
step:1597/2330 train_time:97884ms step_avg:61.29ms
step:1598/2330 train_time:97948ms step_avg:61.29ms
step:1599/2330 train_time:98008ms step_avg:61.29ms
step:1600/2330 train_time:98072ms step_avg:61.30ms
step:1601/2330 train_time:98133ms step_avg:61.29ms
step:1602/2330 train_time:98198ms step_avg:61.30ms
step:1603/2330 train_time:98257ms step_avg:61.30ms
step:1604/2330 train_time:98321ms step_avg:61.30ms
step:1605/2330 train_time:98381ms step_avg:61.30ms
step:1606/2330 train_time:98444ms step_avg:61.30ms
step:1607/2330 train_time:98504ms step_avg:61.30ms
step:1608/2330 train_time:98569ms step_avg:61.30ms
step:1609/2330 train_time:98629ms step_avg:61.30ms
step:1610/2330 train_time:98693ms step_avg:61.30ms
step:1611/2330 train_time:98754ms step_avg:61.30ms
step:1612/2330 train_time:98818ms step_avg:61.30ms
step:1613/2330 train_time:98879ms step_avg:61.30ms
step:1614/2330 train_time:98943ms step_avg:61.30ms
step:1615/2330 train_time:99003ms step_avg:61.30ms
step:1616/2330 train_time:99065ms step_avg:61.30ms
step:1617/2330 train_time:99125ms step_avg:61.30ms
step:1618/2330 train_time:99189ms step_avg:61.30ms
step:1619/2330 train_time:99250ms step_avg:61.30ms
step:1620/2330 train_time:99314ms step_avg:61.30ms
step:1621/2330 train_time:99374ms step_avg:61.30ms
step:1622/2330 train_time:99439ms step_avg:61.31ms
step:1623/2330 train_time:99499ms step_avg:61.31ms
step:1624/2330 train_time:99562ms step_avg:61.31ms
step:1625/2330 train_time:99622ms step_avg:61.31ms
step:1626/2330 train_time:99686ms step_avg:61.31ms
step:1627/2330 train_time:99746ms step_avg:61.31ms
step:1628/2330 train_time:99810ms step_avg:61.31ms
step:1629/2330 train_time:99870ms step_avg:61.31ms
step:1630/2330 train_time:99935ms step_avg:61.31ms
step:1631/2330 train_time:99996ms step_avg:61.31ms
step:1632/2330 train_time:100059ms step_avg:61.31ms
step:1633/2330 train_time:100121ms step_avg:61.31ms
step:1634/2330 train_time:100184ms step_avg:61.31ms
step:1635/2330 train_time:100244ms step_avg:61.31ms
step:1636/2330 train_time:100308ms step_avg:61.31ms
step:1637/2330 train_time:100368ms step_avg:61.31ms
step:1638/2330 train_time:100432ms step_avg:61.31ms
step:1639/2330 train_time:100493ms step_avg:61.31ms
step:1640/2330 train_time:100558ms step_avg:61.32ms
step:1641/2330 train_time:100619ms step_avg:61.32ms
step:1642/2330 train_time:100683ms step_avg:61.32ms
step:1643/2330 train_time:100743ms step_avg:61.32ms
step:1644/2330 train_time:100807ms step_avg:61.32ms
step:1645/2330 train_time:100868ms step_avg:61.32ms
step:1646/2330 train_time:100931ms step_avg:61.32ms
step:1647/2330 train_time:100991ms step_avg:61.32ms
step:1648/2330 train_time:101054ms step_avg:61.32ms
step:1649/2330 train_time:101116ms step_avg:61.32ms
step:1650/2330 train_time:101181ms step_avg:61.32ms
step:1651/2330 train_time:101242ms step_avg:61.32ms
step:1652/2330 train_time:101305ms step_avg:61.32ms
step:1653/2330 train_time:101366ms step_avg:61.32ms
step:1654/2330 train_time:101430ms step_avg:61.32ms
step:1655/2330 train_time:101490ms step_avg:61.32ms
step:1656/2330 train_time:101553ms step_avg:61.32ms
step:1657/2330 train_time:101615ms step_avg:61.32ms
step:1658/2330 train_time:101679ms step_avg:61.33ms
step:1659/2330 train_time:101740ms step_avg:61.33ms
step:1660/2330 train_time:101803ms step_avg:61.33ms
step:1661/2330 train_time:101864ms step_avg:61.33ms
step:1662/2330 train_time:101927ms step_avg:61.33ms
step:1663/2330 train_time:101987ms step_avg:61.33ms
step:1664/2330 train_time:102050ms step_avg:61.33ms
step:1665/2330 train_time:102110ms step_avg:61.33ms
step:1666/2330 train_time:102174ms step_avg:61.33ms
step:1667/2330 train_time:102236ms step_avg:61.33ms
step:1668/2330 train_time:102299ms step_avg:61.33ms
step:1669/2330 train_time:102359ms step_avg:61.33ms
step:1670/2330 train_time:102423ms step_avg:61.33ms
step:1671/2330 train_time:102483ms step_avg:61.33ms
step:1672/2330 train_time:102547ms step_avg:61.33ms
step:1673/2330 train_time:102608ms step_avg:61.33ms
step:1674/2330 train_time:102671ms step_avg:61.33ms
step:1675/2330 train_time:102731ms step_avg:61.33ms
step:1676/2330 train_time:102794ms step_avg:61.33ms
step:1677/2330 train_time:102855ms step_avg:61.33ms
step:1678/2330 train_time:102919ms step_avg:61.33ms
step:1679/2330 train_time:102980ms step_avg:61.33ms
step:1680/2330 train_time:103042ms step_avg:61.33ms
step:1681/2330 train_time:103103ms step_avg:61.33ms
step:1682/2330 train_time:103167ms step_avg:61.34ms
step:1683/2330 train_time:103227ms step_avg:61.34ms
step:1684/2330 train_time:103291ms step_avg:61.34ms
step:1685/2330 train_time:103351ms step_avg:61.34ms
step:1686/2330 train_time:103415ms step_avg:61.34ms
step:1687/2330 train_time:103476ms step_avg:61.34ms
step:1688/2330 train_time:103540ms step_avg:61.34ms
step:1689/2330 train_time:103600ms step_avg:61.34ms
step:1690/2330 train_time:103663ms step_avg:61.34ms
step:1691/2330 train_time:103724ms step_avg:61.34ms
step:1692/2330 train_time:103788ms step_avg:61.34ms
step:1693/2330 train_time:103847ms step_avg:61.34ms
step:1694/2330 train_time:103911ms step_avg:61.34ms
step:1695/2330 train_time:103972ms step_avg:61.34ms
step:1696/2330 train_time:104036ms step_avg:61.34ms
step:1697/2330 train_time:104098ms step_avg:61.34ms
step:1698/2330 train_time:104162ms step_avg:61.34ms
step:1699/2330 train_time:104221ms step_avg:61.34ms
step:1700/2330 train_time:104284ms step_avg:61.34ms
step:1701/2330 train_time:104344ms step_avg:61.34ms
step:1702/2330 train_time:104409ms step_avg:61.34ms
step:1703/2330 train_time:104469ms step_avg:61.34ms
step:1704/2330 train_time:104532ms step_avg:61.35ms
step:1705/2330 train_time:104594ms step_avg:61.35ms
step:1706/2330 train_time:104658ms step_avg:61.35ms
step:1707/2330 train_time:104720ms step_avg:61.35ms
step:1708/2330 train_time:104783ms step_avg:61.35ms
step:1709/2330 train_time:104844ms step_avg:61.35ms
step:1710/2330 train_time:104907ms step_avg:61.35ms
step:1711/2330 train_time:104967ms step_avg:61.35ms
step:1712/2330 train_time:105031ms step_avg:61.35ms
step:1713/2330 train_time:105092ms step_avg:61.35ms
step:1714/2330 train_time:105155ms step_avg:61.35ms
step:1715/2330 train_time:105217ms step_avg:61.35ms
step:1716/2330 train_time:105280ms step_avg:61.35ms
step:1717/2330 train_time:105341ms step_avg:61.35ms
step:1718/2330 train_time:105404ms step_avg:61.35ms
step:1719/2330 train_time:105465ms step_avg:61.35ms
step:1720/2330 train_time:105529ms step_avg:61.35ms
step:1721/2330 train_time:105589ms step_avg:61.35ms
step:1722/2330 train_time:105653ms step_avg:61.35ms
step:1723/2330 train_time:105714ms step_avg:61.35ms
step:1724/2330 train_time:105779ms step_avg:61.36ms
step:1725/2330 train_time:105839ms step_avg:61.36ms
step:1726/2330 train_time:105904ms step_avg:61.36ms
step:1727/2330 train_time:105963ms step_avg:61.36ms
step:1728/2330 train_time:106027ms step_avg:61.36ms
step:1729/2330 train_time:106087ms step_avg:61.36ms
step:1730/2330 train_time:106151ms step_avg:61.36ms
step:1731/2330 train_time:106212ms step_avg:61.36ms
step:1732/2330 train_time:106276ms step_avg:61.36ms
step:1733/2330 train_time:106338ms step_avg:61.36ms
step:1734/2330 train_time:106401ms step_avg:61.36ms
step:1735/2330 train_time:106462ms step_avg:61.36ms
step:1736/2330 train_time:106526ms step_avg:61.36ms
step:1737/2330 train_time:106585ms step_avg:61.36ms
step:1738/2330 train_time:106649ms step_avg:61.36ms
step:1739/2330 train_time:106709ms step_avg:61.36ms
step:1740/2330 train_time:106773ms step_avg:61.36ms
step:1741/2330 train_time:106834ms step_avg:61.36ms
step:1742/2330 train_time:106900ms step_avg:61.37ms
step:1743/2330 train_time:106959ms step_avg:61.36ms
step:1744/2330 train_time:107023ms step_avg:61.37ms
step:1745/2330 train_time:107082ms step_avg:61.37ms
step:1746/2330 train_time:107145ms step_avg:61.37ms
step:1747/2330 train_time:107206ms step_avg:61.37ms
step:1748/2330 train_time:107270ms step_avg:61.37ms
step:1749/2330 train_time:107330ms step_avg:61.37ms
step:1750/2330 train_time:107394ms step_avg:61.37ms
step:1750/2330 val_loss:4.3385 train_time:107468ms step_avg:61.41ms
step:1751/2330 train_time:107491ms step_avg:61.39ms
step:1752/2330 train_time:107520ms step_avg:61.37ms
step:1753/2330 train_time:107582ms step_avg:61.37ms
step:1754/2330 train_time:107653ms step_avg:61.38ms
step:1755/2330 train_time:107715ms step_avg:61.38ms
step:1756/2330 train_time:107778ms step_avg:61.38ms
step:1757/2330 train_time:107837ms step_avg:61.38ms
step:1758/2330 train_time:107899ms step_avg:61.38ms
step:1759/2330 train_time:107959ms step_avg:61.38ms
step:1760/2330 train_time:108022ms step_avg:61.38ms
step:1761/2330 train_time:108082ms step_avg:61.38ms
step:1762/2330 train_time:108145ms step_avg:61.38ms
step:1763/2330 train_time:108205ms step_avg:61.38ms
step:1764/2330 train_time:108268ms step_avg:61.38ms
step:1765/2330 train_time:108328ms step_avg:61.38ms
step:1766/2330 train_time:108392ms step_avg:61.38ms
step:1767/2330 train_time:108454ms step_avg:61.38ms
step:1768/2330 train_time:108519ms step_avg:61.38ms
step:1769/2330 train_time:108580ms step_avg:61.38ms
step:1770/2330 train_time:108646ms step_avg:61.38ms
step:1771/2330 train_time:108708ms step_avg:61.38ms
step:1772/2330 train_time:108772ms step_avg:61.38ms
step:1773/2330 train_time:108833ms step_avg:61.38ms
step:1774/2330 train_time:108896ms step_avg:61.38ms
step:1775/2330 train_time:108956ms step_avg:61.38ms
step:1776/2330 train_time:109019ms step_avg:61.38ms
step:1777/2330 train_time:109079ms step_avg:61.38ms
step:1778/2330 train_time:109142ms step_avg:61.38ms
step:1779/2330 train_time:109202ms step_avg:61.38ms
step:1780/2330 train_time:109265ms step_avg:61.38ms
step:1781/2330 train_time:109325ms step_avg:61.38ms
step:1782/2330 train_time:109389ms step_avg:61.39ms
step:1783/2330 train_time:109450ms step_avg:61.39ms
step:1784/2330 train_time:109515ms step_avg:61.39ms
step:1785/2330 train_time:109576ms step_avg:61.39ms
step:1786/2330 train_time:109639ms step_avg:61.39ms
step:1787/2330 train_time:109702ms step_avg:61.39ms
step:1788/2330 train_time:109766ms step_avg:61.39ms
step:1789/2330 train_time:109828ms step_avg:61.39ms
step:1790/2330 train_time:109893ms step_avg:61.39ms
step:1791/2330 train_time:109953ms step_avg:61.39ms
step:1792/2330 train_time:110016ms step_avg:61.39ms
step:1793/2330 train_time:110077ms step_avg:61.39ms
step:1794/2330 train_time:110140ms step_avg:61.39ms
step:1795/2330 train_time:110199ms step_avg:61.39ms
step:1796/2330 train_time:110261ms step_avg:61.39ms
step:1797/2330 train_time:110322ms step_avg:61.39ms
step:1798/2330 train_time:110386ms step_avg:61.39ms
step:1799/2330 train_time:110447ms step_avg:61.39ms
step:1800/2330 train_time:110511ms step_avg:61.40ms
step:1801/2330 train_time:110573ms step_avg:61.40ms
step:1802/2330 train_time:110636ms step_avg:61.40ms
step:1803/2330 train_time:110698ms step_avg:61.40ms
step:1804/2330 train_time:110761ms step_avg:61.40ms
step:1805/2330 train_time:110821ms step_avg:61.40ms
step:1806/2330 train_time:110886ms step_avg:61.40ms
step:1807/2330 train_time:110947ms step_avg:61.40ms
step:1808/2330 train_time:111011ms step_avg:61.40ms
step:1809/2330 train_time:111072ms step_avg:61.40ms
step:1810/2330 train_time:111135ms step_avg:61.40ms
step:1811/2330 train_time:111196ms step_avg:61.40ms
step:1812/2330 train_time:111260ms step_avg:61.40ms
step:1813/2330 train_time:111319ms step_avg:61.40ms
step:1814/2330 train_time:111383ms step_avg:61.40ms
step:1815/2330 train_time:111443ms step_avg:61.40ms
step:1816/2330 train_time:111507ms step_avg:61.40ms
step:1817/2330 train_time:111568ms step_avg:61.40ms
step:1818/2330 train_time:111633ms step_avg:61.40ms
step:1819/2330 train_time:111695ms step_avg:61.40ms
step:1820/2330 train_time:111758ms step_avg:61.41ms
step:1821/2330 train_time:111818ms step_avg:61.40ms
step:1822/2330 train_time:111883ms step_avg:61.41ms
step:1823/2330 train_time:111943ms step_avg:61.41ms
step:1824/2330 train_time:112007ms step_avg:61.41ms
step:1825/2330 train_time:112068ms step_avg:61.41ms
step:1826/2330 train_time:112132ms step_avg:61.41ms
step:1827/2330 train_time:112193ms step_avg:61.41ms
step:1828/2330 train_time:112256ms step_avg:61.41ms
step:1829/2330 train_time:112317ms step_avg:61.41ms
step:1830/2330 train_time:112381ms step_avg:61.41ms
step:1831/2330 train_time:112441ms step_avg:61.41ms
step:1832/2330 train_time:112505ms step_avg:61.41ms
step:1833/2330 train_time:112566ms step_avg:61.41ms
step:1834/2330 train_time:112629ms step_avg:61.41ms
step:1835/2330 train_time:112691ms step_avg:61.41ms
step:1836/2330 train_time:112754ms step_avg:61.41ms
step:1837/2330 train_time:112815ms step_avg:61.41ms
step:1838/2330 train_time:112878ms step_avg:61.41ms
step:1839/2330 train_time:112938ms step_avg:61.41ms
step:1840/2330 train_time:113003ms step_avg:61.41ms
step:1841/2330 train_time:113062ms step_avg:61.41ms
step:1842/2330 train_time:113126ms step_avg:61.41ms
step:1843/2330 train_time:113188ms step_avg:61.41ms
step:1844/2330 train_time:113251ms step_avg:61.42ms
step:1845/2330 train_time:113311ms step_avg:61.42ms
step:1846/2330 train_time:113375ms step_avg:61.42ms
step:1847/2330 train_time:113435ms step_avg:61.42ms
step:1848/2330 train_time:113497ms step_avg:61.42ms
step:1849/2330 train_time:113559ms step_avg:61.42ms
step:1850/2330 train_time:113622ms step_avg:61.42ms
step:1851/2330 train_time:113683ms step_avg:61.42ms
step:1852/2330 train_time:113747ms step_avg:61.42ms
step:1853/2330 train_time:113809ms step_avg:61.42ms
step:1854/2330 train_time:113873ms step_avg:61.42ms
step:1855/2330 train_time:113933ms step_avg:61.42ms
step:1856/2330 train_time:113997ms step_avg:61.42ms
step:1857/2330 train_time:114057ms step_avg:61.42ms
step:1858/2330 train_time:114120ms step_avg:61.42ms
step:1859/2330 train_time:114180ms step_avg:61.42ms
step:1860/2330 train_time:114244ms step_avg:61.42ms
step:1861/2330 train_time:114305ms step_avg:61.42ms
step:1862/2330 train_time:114370ms step_avg:61.42ms
step:1863/2330 train_time:114430ms step_avg:61.42ms
step:1864/2330 train_time:114494ms step_avg:61.42ms
step:1865/2330 train_time:114555ms step_avg:61.42ms
step:1866/2330 train_time:114618ms step_avg:61.42ms
step:1867/2330 train_time:114678ms step_avg:61.42ms
step:1868/2330 train_time:114743ms step_avg:61.43ms
step:1869/2330 train_time:114804ms step_avg:61.43ms
step:1870/2330 train_time:114867ms step_avg:61.43ms
step:1871/2330 train_time:114929ms step_avg:61.43ms
step:1872/2330 train_time:114994ms step_avg:61.43ms
step:1873/2330 train_time:115054ms step_avg:61.43ms
step:1874/2330 train_time:115117ms step_avg:61.43ms
step:1875/2330 train_time:115178ms step_avg:61.43ms
step:1876/2330 train_time:115242ms step_avg:61.43ms
step:1877/2330 train_time:115302ms step_avg:61.43ms
step:1878/2330 train_time:115366ms step_avg:61.43ms
step:1879/2330 train_time:115428ms step_avg:61.43ms
step:1880/2330 train_time:115492ms step_avg:61.43ms
step:1881/2330 train_time:115553ms step_avg:61.43ms
step:1882/2330 train_time:115616ms step_avg:61.43ms
step:1883/2330 train_time:115676ms step_avg:61.43ms
step:1884/2330 train_time:115740ms step_avg:61.43ms
step:1885/2330 train_time:115800ms step_avg:61.43ms
step:1886/2330 train_time:115864ms step_avg:61.43ms
step:1887/2330 train_time:115926ms step_avg:61.43ms
step:1888/2330 train_time:115990ms step_avg:61.44ms
step:1889/2330 train_time:116051ms step_avg:61.44ms
step:1890/2330 train_time:116114ms step_avg:61.44ms
step:1891/2330 train_time:116175ms step_avg:61.44ms
step:1892/2330 train_time:116239ms step_avg:61.44ms
step:1893/2330 train_time:116299ms step_avg:61.44ms
step:1894/2330 train_time:116363ms step_avg:61.44ms
step:1895/2330 train_time:116424ms step_avg:61.44ms
step:1896/2330 train_time:116488ms step_avg:61.44ms
step:1897/2330 train_time:116549ms step_avg:61.44ms
step:1898/2330 train_time:116612ms step_avg:61.44ms
step:1899/2330 train_time:116673ms step_avg:61.44ms
step:1900/2330 train_time:116736ms step_avg:61.44ms
step:1901/2330 train_time:116797ms step_avg:61.44ms
step:1902/2330 train_time:116861ms step_avg:61.44ms
step:1903/2330 train_time:116921ms step_avg:61.44ms
step:1904/2330 train_time:116986ms step_avg:61.44ms
step:1905/2330 train_time:117047ms step_avg:61.44ms
step:1906/2330 train_time:117111ms step_avg:61.44ms
step:1907/2330 train_time:117172ms step_avg:61.44ms
step:1908/2330 train_time:117235ms step_avg:61.44ms
step:1909/2330 train_time:117296ms step_avg:61.44ms
step:1910/2330 train_time:117359ms step_avg:61.44ms
step:1911/2330 train_time:117420ms step_avg:61.44ms
step:1912/2330 train_time:117483ms step_avg:61.45ms
step:1913/2330 train_time:117544ms step_avg:61.44ms
step:1914/2330 train_time:117608ms step_avg:61.45ms
step:1915/2330 train_time:117669ms step_avg:61.45ms
step:1916/2330 train_time:117733ms step_avg:61.45ms
step:1917/2330 train_time:117794ms step_avg:61.45ms
step:1918/2330 train_time:117858ms step_avg:61.45ms
step:1919/2330 train_time:117918ms step_avg:61.45ms
step:1920/2330 train_time:117982ms step_avg:61.45ms
step:1921/2330 train_time:118041ms step_avg:61.45ms
step:1922/2330 train_time:118106ms step_avg:61.45ms
step:1923/2330 train_time:118169ms step_avg:61.45ms
step:1924/2330 train_time:118233ms step_avg:61.45ms
step:1925/2330 train_time:118294ms step_avg:61.45ms
step:1926/2330 train_time:118357ms step_avg:61.45ms
step:1927/2330 train_time:118418ms step_avg:61.45ms
step:1928/2330 train_time:118482ms step_avg:61.45ms
step:1929/2330 train_time:118542ms step_avg:61.45ms
step:1930/2330 train_time:118606ms step_avg:61.45ms
step:1931/2330 train_time:118666ms step_avg:61.45ms
step:1932/2330 train_time:118730ms step_avg:61.45ms
step:1933/2330 train_time:118793ms step_avg:61.46ms
step:1934/2330 train_time:118856ms step_avg:61.46ms
step:1935/2330 train_time:118916ms step_avg:61.46ms
step:1936/2330 train_time:118980ms step_avg:61.46ms
step:1937/2330 train_time:119040ms step_avg:61.46ms
step:1938/2330 train_time:119104ms step_avg:61.46ms
step:1939/2330 train_time:119164ms step_avg:61.46ms
step:1940/2330 train_time:119228ms step_avg:61.46ms
step:1941/2330 train_time:119290ms step_avg:61.46ms
step:1942/2330 train_time:119353ms step_avg:61.46ms
step:1943/2330 train_time:119414ms step_avg:61.46ms
step:1944/2330 train_time:119477ms step_avg:61.46ms
step:1945/2330 train_time:119537ms step_avg:61.46ms
step:1946/2330 train_time:119601ms step_avg:61.46ms
step:1947/2330 train_time:119660ms step_avg:61.46ms
step:1948/2330 train_time:119725ms step_avg:61.46ms
step:1949/2330 train_time:119785ms step_avg:61.46ms
step:1950/2330 train_time:119849ms step_avg:61.46ms
step:1951/2330 train_time:119910ms step_avg:61.46ms
step:1952/2330 train_time:119974ms step_avg:61.46ms
step:1953/2330 train_time:120035ms step_avg:61.46ms
step:1954/2330 train_time:120098ms step_avg:61.46ms
step:1955/2330 train_time:120158ms step_avg:61.46ms
step:1956/2330 train_time:120222ms step_avg:61.46ms
step:1957/2330 train_time:120284ms step_avg:61.46ms
step:1958/2330 train_time:120347ms step_avg:61.46ms
step:1959/2330 train_time:120408ms step_avg:61.46ms
step:1960/2330 train_time:120472ms step_avg:61.47ms
step:1961/2330 train_time:120533ms step_avg:61.47ms
step:1962/2330 train_time:120597ms step_avg:61.47ms
step:1963/2330 train_time:120656ms step_avg:61.47ms
step:1964/2330 train_time:120719ms step_avg:61.47ms
step:1965/2330 train_time:120779ms step_avg:61.47ms
step:1966/2330 train_time:120843ms step_avg:61.47ms
step:1967/2330 train_time:120904ms step_avg:61.47ms
step:1968/2330 train_time:120967ms step_avg:61.47ms
step:1969/2330 train_time:121030ms step_avg:61.47ms
step:1970/2330 train_time:121095ms step_avg:61.47ms
step:1971/2330 train_time:121155ms step_avg:61.47ms
step:1972/2330 train_time:121218ms step_avg:61.47ms
step:1973/2330 train_time:121279ms step_avg:61.47ms
step:1974/2330 train_time:121343ms step_avg:61.47ms
step:1975/2330 train_time:121404ms step_avg:61.47ms
step:1976/2330 train_time:121467ms step_avg:61.47ms
step:1977/2330 train_time:121529ms step_avg:61.47ms
step:1978/2330 train_time:121594ms step_avg:61.47ms
step:1979/2330 train_time:121655ms step_avg:61.47ms
step:1980/2330 train_time:121717ms step_avg:61.47ms
step:1981/2330 train_time:121777ms step_avg:61.47ms
step:1982/2330 train_time:121842ms step_avg:61.47ms
step:1983/2330 train_time:121902ms step_avg:61.47ms
step:1984/2330 train_time:121965ms step_avg:61.47ms
step:1985/2330 train_time:122027ms step_avg:61.47ms
step:1986/2330 train_time:122093ms step_avg:61.48ms
step:1987/2330 train_time:122152ms step_avg:61.48ms
step:1988/2330 train_time:122215ms step_avg:61.48ms
step:1989/2330 train_time:122276ms step_avg:61.48ms
step:1990/2330 train_time:122340ms step_avg:61.48ms
step:1991/2330 train_time:122401ms step_avg:61.48ms
step:1992/2330 train_time:122464ms step_avg:61.48ms
step:1993/2330 train_time:122526ms step_avg:61.48ms
step:1994/2330 train_time:122592ms step_avg:61.48ms
step:1995/2330 train_time:122652ms step_avg:61.48ms
step:1996/2330 train_time:122715ms step_avg:61.48ms
step:1997/2330 train_time:122776ms step_avg:61.48ms
step:1998/2330 train_time:122839ms step_avg:61.48ms
step:1999/2330 train_time:122899ms step_avg:61.48ms
step:2000/2330 train_time:122964ms step_avg:61.48ms
step:2000/2330 val_loss:4.2717 train_time:123038ms step_avg:61.52ms
step:2001/2330 train_time:123061ms step_avg:61.50ms
step:2002/2330 train_time:123091ms step_avg:61.48ms
step:2003/2330 train_time:123154ms step_avg:61.48ms
step:2004/2330 train_time:123223ms step_avg:61.49ms
step:2005/2330 train_time:123283ms step_avg:61.49ms
step:2006/2330 train_time:123348ms step_avg:61.49ms
step:2007/2330 train_time:123409ms step_avg:61.49ms
step:2008/2330 train_time:123472ms step_avg:61.49ms
step:2009/2330 train_time:123531ms step_avg:61.49ms
step:2010/2330 train_time:123594ms step_avg:61.49ms
step:2011/2330 train_time:123653ms step_avg:61.49ms
step:2012/2330 train_time:123715ms step_avg:61.49ms
step:2013/2330 train_time:123775ms step_avg:61.49ms
step:2014/2330 train_time:123838ms step_avg:61.49ms
step:2015/2330 train_time:123898ms step_avg:61.49ms
step:2016/2330 train_time:123961ms step_avg:61.49ms
step:2017/2330 train_time:124022ms step_avg:61.49ms
step:2018/2330 train_time:124087ms step_avg:61.49ms
step:2019/2330 train_time:124148ms step_avg:61.49ms
step:2020/2330 train_time:124214ms step_avg:61.49ms
step:2021/2330 train_time:124276ms step_avg:61.49ms
step:2022/2330 train_time:124341ms step_avg:61.49ms
step:2023/2330 train_time:124402ms step_avg:61.49ms
step:2024/2330 train_time:124465ms step_avg:61.49ms
step:2025/2330 train_time:124525ms step_avg:61.49ms
step:2026/2330 train_time:124588ms step_avg:61.49ms
step:2027/2330 train_time:124648ms step_avg:61.49ms
step:2028/2330 train_time:124711ms step_avg:61.49ms
step:2029/2330 train_time:124771ms step_avg:61.49ms
step:2030/2330 train_time:124834ms step_avg:61.49ms
step:2031/2330 train_time:124894ms step_avg:61.49ms
step:2032/2330 train_time:124958ms step_avg:61.49ms
step:2033/2330 train_time:125019ms step_avg:61.49ms
step:2034/2330 train_time:125083ms step_avg:61.50ms
step:2035/2330 train_time:125144ms step_avg:61.50ms
step:2036/2330 train_time:125208ms step_avg:61.50ms
step:2037/2330 train_time:125268ms step_avg:61.50ms
step:2038/2330 train_time:125332ms step_avg:61.50ms
step:2039/2330 train_time:125394ms step_avg:61.50ms
step:2040/2330 train_time:125460ms step_avg:61.50ms
step:2041/2330 train_time:125521ms step_avg:61.50ms
step:2042/2330 train_time:125584ms step_avg:61.50ms
step:2043/2330 train_time:125645ms step_avg:61.50ms
step:2044/2330 train_time:125708ms step_avg:61.50ms
step:2045/2330 train_time:125768ms step_avg:61.50ms
step:2046/2330 train_time:125831ms step_avg:61.50ms
step:2047/2330 train_time:125891ms step_avg:61.50ms
step:2048/2330 train_time:125954ms step_avg:61.50ms
step:2049/2330 train_time:126014ms step_avg:61.50ms
step:2050/2330 train_time:126079ms step_avg:61.50ms
step:2051/2330 train_time:126141ms step_avg:61.50ms
step:2052/2330 train_time:126205ms step_avg:61.50ms
step:2053/2330 train_time:126265ms step_avg:61.50ms
step:2054/2330 train_time:126328ms step_avg:61.50ms
step:2055/2330 train_time:126389ms step_avg:61.50ms
step:2056/2330 train_time:126454ms step_avg:61.50ms
step:2057/2330 train_time:126515ms step_avg:61.50ms
step:2058/2330 train_time:126580ms step_avg:61.51ms
step:2059/2330 train_time:126642ms step_avg:61.51ms
step:2060/2330 train_time:126706ms step_avg:61.51ms
step:2061/2330 train_time:126766ms step_avg:61.51ms
step:2062/2330 train_time:126828ms step_avg:61.51ms
step:2063/2330 train_time:126888ms step_avg:61.51ms
step:2064/2330 train_time:126952ms step_avg:61.51ms
step:2065/2330 train_time:127012ms step_avg:61.51ms
step:2066/2330 train_time:127076ms step_avg:61.51ms
step:2067/2330 train_time:127137ms step_avg:61.51ms
step:2068/2330 train_time:127202ms step_avg:61.51ms
step:2069/2330 train_time:127262ms step_avg:61.51ms
step:2070/2330 train_time:127325ms step_avg:61.51ms
step:2071/2330 train_time:127386ms step_avg:61.51ms
step:2072/2330 train_time:127450ms step_avg:61.51ms
step:2073/2330 train_time:127511ms step_avg:61.51ms
step:2074/2330 train_time:127575ms step_avg:61.51ms
step:2075/2330 train_time:127637ms step_avg:61.51ms
step:2076/2330 train_time:127701ms step_avg:61.51ms
step:2077/2330 train_time:127762ms step_avg:61.51ms
step:2078/2330 train_time:127824ms step_avg:61.51ms
step:2079/2330 train_time:127884ms step_avg:61.51ms
step:2080/2330 train_time:127947ms step_avg:61.51ms
step:2081/2330 train_time:128008ms step_avg:61.51ms
step:2082/2330 train_time:128072ms step_avg:61.51ms
step:2083/2330 train_time:128133ms step_avg:61.51ms
step:2084/2330 train_time:128196ms step_avg:61.51ms
step:2085/2330 train_time:128257ms step_avg:61.51ms
step:2086/2330 train_time:128322ms step_avg:61.52ms
step:2087/2330 train_time:128383ms step_avg:61.52ms
step:2088/2330 train_time:128445ms step_avg:61.52ms
step:2089/2330 train_time:128507ms step_avg:61.52ms
step:2090/2330 train_time:128570ms step_avg:61.52ms
step:2091/2330 train_time:128631ms step_avg:61.52ms
step:2092/2330 train_time:128695ms step_avg:61.52ms
step:2093/2330 train_time:128756ms step_avg:61.52ms
step:2094/2330 train_time:128820ms step_avg:61.52ms
step:2095/2330 train_time:128882ms step_avg:61.52ms
step:2096/2330 train_time:128944ms step_avg:61.52ms
step:2097/2330 train_time:129005ms step_avg:61.52ms
step:2098/2330 train_time:129068ms step_avg:61.52ms
step:2099/2330 train_time:129128ms step_avg:61.52ms
step:2100/2330 train_time:129191ms step_avg:61.52ms
step:2101/2330 train_time:129252ms step_avg:61.52ms
step:2102/2330 train_time:129316ms step_avg:61.52ms
step:2103/2330 train_time:129376ms step_avg:61.52ms
step:2104/2330 train_time:129441ms step_avg:61.52ms
step:2105/2330 train_time:129502ms step_avg:61.52ms
step:2106/2330 train_time:129564ms step_avg:61.52ms
step:2107/2330 train_time:129625ms step_avg:61.52ms
step:2108/2330 train_time:129689ms step_avg:61.52ms
step:2109/2330 train_time:129749ms step_avg:61.52ms
step:2110/2330 train_time:129813ms step_avg:61.52ms
step:2111/2330 train_time:129875ms step_avg:61.52ms
step:2112/2330 train_time:129939ms step_avg:61.52ms
step:2113/2330 train_time:130000ms step_avg:61.52ms
step:2114/2330 train_time:130063ms step_avg:61.52ms
step:2115/2330 train_time:130123ms step_avg:61.52ms
step:2116/2330 train_time:130186ms step_avg:61.52ms
step:2117/2330 train_time:130246ms step_avg:61.52ms
step:2118/2330 train_time:130310ms step_avg:61.53ms
step:2119/2330 train_time:130370ms step_avg:61.52ms
step:2120/2330 train_time:130433ms step_avg:61.53ms
step:2121/2330 train_time:130495ms step_avg:61.53ms
step:2122/2330 train_time:130560ms step_avg:61.53ms
step:2123/2330 train_time:130620ms step_avg:61.53ms
step:2124/2330 train_time:130684ms step_avg:61.53ms
step:2125/2330 train_time:130744ms step_avg:61.53ms
step:2126/2330 train_time:130809ms step_avg:61.53ms
step:2127/2330 train_time:130868ms step_avg:61.53ms
step:2128/2330 train_time:130932ms step_avg:61.53ms
step:2129/2330 train_time:130992ms step_avg:61.53ms
step:2130/2330 train_time:131057ms step_avg:61.53ms
step:2131/2330 train_time:131118ms step_avg:61.53ms
step:2132/2330 train_time:131182ms step_avg:61.53ms
step:2133/2330 train_time:131242ms step_avg:61.53ms
step:2134/2330 train_time:131307ms step_avg:61.53ms
step:2135/2330 train_time:131366ms step_avg:61.53ms
step:2136/2330 train_time:131431ms step_avg:61.53ms
step:2137/2330 train_time:131490ms step_avg:61.53ms
step:2138/2330 train_time:131554ms step_avg:61.53ms
step:2139/2330 train_time:131615ms step_avg:61.53ms
step:2140/2330 train_time:131680ms step_avg:61.53ms
step:2141/2330 train_time:131741ms step_avg:61.53ms
step:2142/2330 train_time:131804ms step_avg:61.53ms
step:2143/2330 train_time:131864ms step_avg:61.53ms
step:2144/2330 train_time:131927ms step_avg:61.53ms
step:2145/2330 train_time:131987ms step_avg:61.53ms
step:2146/2330 train_time:132051ms step_avg:61.53ms
step:2147/2330 train_time:132111ms step_avg:61.53ms
step:2148/2330 train_time:132175ms step_avg:61.53ms
step:2149/2330 train_time:132236ms step_avg:61.53ms
step:2150/2330 train_time:132301ms step_avg:61.54ms
step:2151/2330 train_time:132361ms step_avg:61.53ms
step:2152/2330 train_time:132424ms step_avg:61.54ms
step:2153/2330 train_time:132484ms step_avg:61.53ms
step:2154/2330 train_time:132548ms step_avg:61.54ms
step:2155/2330 train_time:132608ms step_avg:61.54ms
step:2156/2330 train_time:132672ms step_avg:61.54ms
step:2157/2330 train_time:132733ms step_avg:61.54ms
step:2158/2330 train_time:132797ms step_avg:61.54ms
step:2159/2330 train_time:132858ms step_avg:61.54ms
step:2160/2330 train_time:132922ms step_avg:61.54ms
step:2161/2330 train_time:132983ms step_avg:61.54ms
step:2162/2330 train_time:133046ms step_avg:61.54ms
step:2163/2330 train_time:133106ms step_avg:61.54ms
step:2164/2330 train_time:133170ms step_avg:61.54ms
step:2165/2330 train_time:133231ms step_avg:61.54ms
step:2166/2330 train_time:133295ms step_avg:61.54ms
step:2167/2330 train_time:133356ms step_avg:61.54ms
step:2168/2330 train_time:133420ms step_avg:61.54ms
step:2169/2330 train_time:133481ms step_avg:61.54ms
step:2170/2330 train_time:133544ms step_avg:61.54ms
step:2171/2330 train_time:133605ms step_avg:61.54ms
step:2172/2330 train_time:133668ms step_avg:61.54ms
step:2173/2330 train_time:133728ms step_avg:61.54ms
step:2174/2330 train_time:133793ms step_avg:61.54ms
step:2175/2330 train_time:133853ms step_avg:61.54ms
step:2176/2330 train_time:133917ms step_avg:61.54ms
step:2177/2330 train_time:133978ms step_avg:61.54ms
step:2178/2330 train_time:134042ms step_avg:61.54ms
step:2179/2330 train_time:134105ms step_avg:61.54ms
step:2180/2330 train_time:134167ms step_avg:61.54ms
step:2181/2330 train_time:134227ms step_avg:61.54ms
step:2182/2330 train_time:134292ms step_avg:61.55ms
step:2183/2330 train_time:134353ms step_avg:61.55ms
step:2184/2330 train_time:134417ms step_avg:61.55ms
step:2185/2330 train_time:134478ms step_avg:61.55ms
step:2186/2330 train_time:134542ms step_avg:61.55ms
step:2187/2330 train_time:134603ms step_avg:61.55ms
step:2188/2330 train_time:134666ms step_avg:61.55ms
step:2189/2330 train_time:134726ms step_avg:61.55ms
step:2190/2330 train_time:134790ms step_avg:61.55ms
step:2191/2330 train_time:134851ms step_avg:61.55ms
step:2192/2330 train_time:134915ms step_avg:61.55ms
step:2193/2330 train_time:134976ms step_avg:61.55ms
step:2194/2330 train_time:135041ms step_avg:61.55ms
step:2195/2330 train_time:135101ms step_avg:61.55ms
step:2196/2330 train_time:135164ms step_avg:61.55ms
step:2197/2330 train_time:135225ms step_avg:61.55ms
step:2198/2330 train_time:135289ms step_avg:61.55ms
step:2199/2330 train_time:135350ms step_avg:61.55ms
step:2200/2330 train_time:135413ms step_avg:61.55ms
step:2201/2330 train_time:135474ms step_avg:61.55ms
step:2202/2330 train_time:135539ms step_avg:61.55ms
step:2203/2330 train_time:135600ms step_avg:61.55ms
step:2204/2330 train_time:135663ms step_avg:61.55ms
step:2205/2330 train_time:135724ms step_avg:61.55ms
step:2206/2330 train_time:135787ms step_avg:61.55ms
step:2207/2330 train_time:135847ms step_avg:61.55ms
step:2208/2330 train_time:135911ms step_avg:61.55ms
step:2209/2330 train_time:135971ms step_avg:61.55ms
step:2210/2330 train_time:136035ms step_avg:61.55ms
step:2211/2330 train_time:136097ms step_avg:61.55ms
step:2212/2330 train_time:136161ms step_avg:61.56ms
step:2213/2330 train_time:136221ms step_avg:61.56ms
step:2214/2330 train_time:136285ms step_avg:61.56ms
step:2215/2330 train_time:136345ms step_avg:61.56ms
step:2216/2330 train_time:136409ms step_avg:61.56ms
step:2217/2330 train_time:136468ms step_avg:61.56ms
step:2218/2330 train_time:136532ms step_avg:61.56ms
step:2219/2330 train_time:136593ms step_avg:61.56ms
step:2220/2330 train_time:136658ms step_avg:61.56ms
step:2221/2330 train_time:136719ms step_avg:61.56ms
step:2222/2330 train_time:136782ms step_avg:61.56ms
step:2223/2330 train_time:136842ms step_avg:61.56ms
step:2224/2330 train_time:136906ms step_avg:61.56ms
step:2225/2330 train_time:136965ms step_avg:61.56ms
step:2226/2330 train_time:137029ms step_avg:61.56ms
step:2227/2330 train_time:137089ms step_avg:61.56ms
step:2228/2330 train_time:137154ms step_avg:61.56ms
step:2229/2330 train_time:137214ms step_avg:61.56ms
step:2230/2330 train_time:137277ms step_avg:61.56ms
step:2231/2330 train_time:137338ms step_avg:61.56ms
step:2232/2330 train_time:137403ms step_avg:61.56ms
step:2233/2330 train_time:137463ms step_avg:61.56ms
step:2234/2330 train_time:137526ms step_avg:61.56ms
step:2235/2330 train_time:137586ms step_avg:61.56ms
step:2236/2330 train_time:137651ms step_avg:61.56ms
step:2237/2330 train_time:137711ms step_avg:61.56ms
step:2238/2330 train_time:137775ms step_avg:61.56ms
step:2239/2330 train_time:137836ms step_avg:61.56ms
step:2240/2330 train_time:137902ms step_avg:61.56ms
step:2241/2330 train_time:137962ms step_avg:61.56ms
step:2242/2330 train_time:138024ms step_avg:61.56ms
step:2243/2330 train_time:138085ms step_avg:61.56ms
step:2244/2330 train_time:138148ms step_avg:61.56ms
step:2245/2330 train_time:138209ms step_avg:61.56ms
step:2246/2330 train_time:138272ms step_avg:61.56ms
step:2247/2330 train_time:138333ms step_avg:61.56ms
step:2248/2330 train_time:138398ms step_avg:61.56ms
step:2249/2330 train_time:138458ms step_avg:61.56ms
step:2250/2330 train_time:138522ms step_avg:61.57ms
step:2250/2330 val_loss:4.2250 train_time:138595ms step_avg:61.60ms
step:2251/2330 train_time:138618ms step_avg:61.58ms
step:2252/2330 train_time:138648ms step_avg:61.57ms
step:2253/2330 train_time:138712ms step_avg:61.57ms
step:2254/2330 train_time:138780ms step_avg:61.57ms
step:2255/2330 train_time:138841ms step_avg:61.57ms
step:2256/2330 train_time:138905ms step_avg:61.57ms
step:2257/2330 train_time:138965ms step_avg:61.57ms
step:2258/2330 train_time:139029ms step_avg:61.57ms
step:2259/2330 train_time:139087ms step_avg:61.57ms
step:2260/2330 train_time:139151ms step_avg:61.57ms
step:2261/2330 train_time:139212ms step_avg:61.57ms
step:2262/2330 train_time:139274ms step_avg:61.57ms
step:2263/2330 train_time:139333ms step_avg:61.57ms
step:2264/2330 train_time:139396ms step_avg:61.57ms
step:2265/2330 train_time:139456ms step_avg:61.57ms
step:2266/2330 train_time:139519ms step_avg:61.57ms
step:2267/2330 train_time:139579ms step_avg:61.57ms
step:2268/2330 train_time:139645ms step_avg:61.57ms
step:2269/2330 train_time:139706ms step_avg:61.57ms
step:2270/2330 train_time:139771ms step_avg:61.57ms
step:2271/2330 train_time:139834ms step_avg:61.57ms
step:2272/2330 train_time:139898ms step_avg:61.57ms
step:2273/2330 train_time:139959ms step_avg:61.57ms
step:2274/2330 train_time:140023ms step_avg:61.58ms
step:2275/2330 train_time:140083ms step_avg:61.57ms
step:2276/2330 train_time:140146ms step_avg:61.58ms
step:2277/2330 train_time:140206ms step_avg:61.57ms
step:2278/2330 train_time:140269ms step_avg:61.58ms
step:2279/2330 train_time:140329ms step_avg:61.57ms
step:2280/2330 train_time:140391ms step_avg:61.58ms
step:2281/2330 train_time:140452ms step_avg:61.57ms
step:2282/2330 train_time:140516ms step_avg:61.58ms
step:2283/2330 train_time:140576ms step_avg:61.58ms
step:2284/2330 train_time:140640ms step_avg:61.58ms
step:2285/2330 train_time:140700ms step_avg:61.58ms
step:2286/2330 train_time:140764ms step_avg:61.58ms
step:2287/2330 train_time:140825ms step_avg:61.58ms
step:2288/2330 train_time:140889ms step_avg:61.58ms
step:2289/2330 train_time:140952ms step_avg:61.58ms
step:2290/2330 train_time:141017ms step_avg:61.58ms
step:2291/2330 train_time:141078ms step_avg:61.58ms
step:2292/2330 train_time:141141ms step_avg:61.58ms
step:2293/2330 train_time:141202ms step_avg:61.58ms
step:2294/2330 train_time:141265ms step_avg:61.58ms
step:2295/2330 train_time:141325ms step_avg:61.58ms
step:2296/2330 train_time:141388ms step_avg:61.58ms
step:2297/2330 train_time:141448ms step_avg:61.58ms
step:2298/2330 train_time:141513ms step_avg:61.58ms
step:2299/2330 train_time:141574ms step_avg:61.58ms
step:2300/2330 train_time:141637ms step_avg:61.58ms
step:2301/2330 train_time:141699ms step_avg:61.58ms
step:2302/2330 train_time:141762ms step_avg:61.58ms
step:2303/2330 train_time:141822ms step_avg:61.58ms
step:2304/2330 train_time:141886ms step_avg:61.58ms
step:2305/2330 train_time:141946ms step_avg:61.58ms
step:2306/2330 train_time:142011ms step_avg:61.58ms
step:2307/2330 train_time:142072ms step_avg:61.58ms
step:2308/2330 train_time:142136ms step_avg:61.58ms
step:2309/2330 train_time:142197ms step_avg:61.58ms
step:2310/2330 train_time:142260ms step_avg:61.58ms
step:2311/2330 train_time:142320ms step_avg:61.58ms
step:2312/2330 train_time:142384ms step_avg:61.58ms
step:2313/2330 train_time:142444ms step_avg:61.58ms
step:2314/2330 train_time:142507ms step_avg:61.58ms
step:2315/2330 train_time:142567ms step_avg:61.58ms
step:2316/2330 train_time:142631ms step_avg:61.59ms
step:2317/2330 train_time:142694ms step_avg:61.59ms
step:2318/2330 train_time:142757ms step_avg:61.59ms
step:2319/2330 train_time:142817ms step_avg:61.59ms
step:2320/2330 train_time:142881ms step_avg:61.59ms
step:2321/2330 train_time:142941ms step_avg:61.59ms
step:2322/2330 train_time:143006ms step_avg:61.59ms
step:2323/2330 train_time:143066ms step_avg:61.59ms
step:2324/2330 train_time:143131ms step_avg:61.59ms
step:2325/2330 train_time:143192ms step_avg:61.59ms
step:2326/2330 train_time:143257ms step_avg:61.59ms
step:2327/2330 train_time:143318ms step_avg:61.59ms
step:2328/2330 train_time:143381ms step_avg:61.59ms
step:2329/2330 train_time:143442ms step_avg:61.59ms
step:2330/2330 train_time:143505ms step_avg:61.59ms
step:2330/2330 val_loss:4.2135 train_time:143579ms step_avg:61.62ms
peak memory allocated: 29226 MiB reserved: 44076 MiB
