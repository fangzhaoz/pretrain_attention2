import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr3e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-4, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:51:48 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:85ms step_avg:85.49ms
step:2/2330 train_time:148ms step_avg:73.83ms
step:3/2330 train_time:161ms step_avg:53.79ms
step:4/2330 train_time:175ms step_avg:43.77ms
step:5/2330 train_time:187ms step_avg:37.37ms
step:6/2330 train_time:218ms step_avg:36.36ms
step:7/2330 train_time:252ms step_avg:35.98ms
step:8/2330 train_time:295ms step_avg:36.93ms
step:9/2330 train_time:329ms step_avg:36.61ms
step:10/2330 train_time:373ms step_avg:37.34ms
step:11/2330 train_time:408ms step_avg:37.09ms
step:12/2330 train_time:452ms step_avg:37.65ms
step:13/2330 train_time:486ms step_avg:37.40ms
step:14/2330 train_time:530ms step_avg:37.87ms
step:15/2330 train_time:564ms step_avg:37.62ms
step:16/2330 train_time:608ms step_avg:38.01ms
step:17/2330 train_time:642ms step_avg:37.79ms
step:18/2330 train_time:686ms step_avg:38.10ms
step:19/2330 train_time:720ms step_avg:37.90ms
step:20/2330 train_time:764ms step_avg:38.19ms
step:21/2330 train_time:799ms step_avg:38.06ms
step:22/2330 train_time:844ms step_avg:38.35ms
step:23/2330 train_time:878ms step_avg:38.18ms
step:24/2330 train_time:923ms step_avg:38.44ms
step:25/2330 train_time:958ms step_avg:38.30ms
step:26/2330 train_time:1004ms step_avg:38.61ms
step:27/2330 train_time:1043ms step_avg:38.63ms
step:28/2330 train_time:1092ms step_avg:39.01ms
step:29/2330 train_time:1132ms step_avg:39.05ms
step:30/2330 train_time:1179ms step_avg:39.29ms
step:31/2330 train_time:1214ms step_avg:39.17ms
step:32/2330 train_time:1259ms step_avg:39.34ms
step:33/2330 train_time:1294ms step_avg:39.22ms
step:34/2330 train_time:1339ms step_avg:39.37ms
step:35/2330 train_time:1374ms step_avg:39.26ms
step:36/2330 train_time:1419ms step_avg:39.43ms
step:37/2330 train_time:1455ms step_avg:39.31ms
step:38/2330 train_time:1499ms step_avg:39.45ms
step:39/2330 train_time:1535ms step_avg:39.35ms
step:40/2330 train_time:1580ms step_avg:39.49ms
step:41/2330 train_time:1615ms step_avg:39.39ms
step:42/2330 train_time:1660ms step_avg:39.52ms
step:43/2330 train_time:1695ms step_avg:39.41ms
step:44/2330 train_time:1738ms step_avg:39.51ms
step:45/2330 train_time:1773ms step_avg:39.41ms
step:46/2330 train_time:1819ms step_avg:39.54ms
step:47/2330 train_time:1854ms step_avg:39.44ms
step:48/2330 train_time:1899ms step_avg:39.57ms
step:49/2330 train_time:1935ms step_avg:39.48ms
step:50/2330 train_time:1980ms step_avg:39.60ms
step:51/2330 train_time:2018ms step_avg:39.56ms
step:52/2330 train_time:2065ms step_avg:39.70ms
step:53/2330 train_time:2100ms step_avg:39.63ms
step:54/2330 train_time:2145ms step_avg:39.73ms
step:55/2330 train_time:2181ms step_avg:39.66ms
step:56/2330 train_time:2227ms step_avg:39.76ms
step:57/2330 train_time:2264ms step_avg:39.71ms
step:58/2330 train_time:2309ms step_avg:39.80ms
step:59/2330 train_time:2345ms step_avg:39.74ms
step:60/2330 train_time:2390ms step_avg:39.84ms
step:61/2330 train_time:2427ms step_avg:39.78ms
step:62/2330 train_time:2472ms step_avg:39.88ms
step:63/2330 train_time:2509ms step_avg:39.82ms
step:64/2330 train_time:2553ms step_avg:39.90ms
step:65/2330 train_time:2588ms step_avg:39.82ms
step:66/2330 train_time:2633ms step_avg:39.90ms
step:67/2330 train_time:2668ms step_avg:39.82ms
step:68/2330 train_time:2712ms step_avg:39.89ms
step:69/2330 train_time:2747ms step_avg:39.82ms
step:70/2330 train_time:2792ms step_avg:39.89ms
step:71/2330 train_time:2827ms step_avg:39.82ms
step:72/2330 train_time:2873ms step_avg:39.90ms
step:73/2330 train_time:2910ms step_avg:39.86ms
step:74/2330 train_time:2956ms step_avg:39.95ms
step:75/2330 train_time:2991ms step_avg:39.89ms
step:76/2330 train_time:3037ms step_avg:39.96ms
step:77/2330 train_time:3072ms step_avg:39.90ms
step:78/2330 train_time:3118ms step_avg:39.97ms
step:79/2330 train_time:3154ms step_avg:39.92ms
step:80/2330 train_time:3200ms step_avg:39.99ms
step:81/2330 train_time:3237ms step_avg:39.96ms
step:82/2330 train_time:3281ms step_avg:40.02ms
step:83/2330 train_time:3317ms step_avg:39.97ms
step:84/2330 train_time:3362ms step_avg:40.02ms
step:85/2330 train_time:3398ms step_avg:39.97ms
step:86/2330 train_time:3444ms step_avg:40.04ms
step:87/2330 train_time:3480ms step_avg:40.00ms
step:88/2330 train_time:3525ms step_avg:40.05ms
step:89/2330 train_time:3560ms step_avg:40.01ms
step:90/2330 train_time:3605ms step_avg:40.06ms
step:91/2330 train_time:3641ms step_avg:40.01ms
step:92/2330 train_time:3686ms step_avg:40.07ms
step:93/2330 train_time:3721ms step_avg:40.01ms
step:94/2330 train_time:3766ms step_avg:40.06ms
step:95/2330 train_time:3801ms step_avg:40.01ms
step:96/2330 train_time:3846ms step_avg:40.06ms
step:97/2330 train_time:3882ms step_avg:40.02ms
step:98/2330 train_time:3927ms step_avg:40.07ms
step:99/2330 train_time:3964ms step_avg:40.04ms
step:100/2330 train_time:4009ms step_avg:40.09ms
step:101/2330 train_time:4045ms step_avg:40.04ms
step:102/2330 train_time:4091ms step_avg:40.11ms
step:103/2330 train_time:4128ms step_avg:40.07ms
step:104/2330 train_time:4172ms step_avg:40.12ms
step:105/2330 train_time:4208ms step_avg:40.08ms
step:106/2330 train_time:4253ms step_avg:40.12ms
step:107/2330 train_time:4289ms step_avg:40.08ms
step:108/2330 train_time:4334ms step_avg:40.13ms
step:109/2330 train_time:4369ms step_avg:40.08ms
step:110/2330 train_time:4415ms step_avg:40.14ms
step:111/2330 train_time:4451ms step_avg:40.10ms
step:112/2330 train_time:4496ms step_avg:40.15ms
step:113/2330 train_time:4532ms step_avg:40.11ms
step:114/2330 train_time:4578ms step_avg:40.15ms
step:115/2330 train_time:4613ms step_avg:40.11ms
step:116/2330 train_time:4659ms step_avg:40.16ms
step:117/2330 train_time:4693ms step_avg:40.11ms
step:118/2330 train_time:4739ms step_avg:40.16ms
step:119/2330 train_time:4774ms step_avg:40.12ms
step:120/2330 train_time:4819ms step_avg:40.16ms
step:121/2330 train_time:4855ms step_avg:40.13ms
step:122/2330 train_time:4900ms step_avg:40.16ms
step:123/2330 train_time:4935ms step_avg:40.12ms
step:124/2330 train_time:4979ms step_avg:40.16ms
step:125/2330 train_time:5016ms step_avg:40.12ms
step:126/2330 train_time:5061ms step_avg:40.17ms
step:127/2330 train_time:5096ms step_avg:40.12ms
step:128/2330 train_time:5141ms step_avg:40.17ms
step:129/2330 train_time:5177ms step_avg:40.14ms
step:130/2330 train_time:5222ms step_avg:40.17ms
step:131/2330 train_time:5258ms step_avg:40.14ms
step:132/2330 train_time:5304ms step_avg:40.18ms
step:133/2330 train_time:5339ms step_avg:40.14ms
step:134/2330 train_time:5384ms step_avg:40.18ms
step:135/2330 train_time:5419ms step_avg:40.14ms
step:136/2330 train_time:5464ms step_avg:40.18ms
step:137/2330 train_time:5500ms step_avg:40.14ms
step:138/2330 train_time:5545ms step_avg:40.18ms
step:139/2330 train_time:5582ms step_avg:40.16ms
step:140/2330 train_time:5628ms step_avg:40.20ms
step:141/2330 train_time:5663ms step_avg:40.16ms
step:142/2330 train_time:5708ms step_avg:40.20ms
step:143/2330 train_time:5744ms step_avg:40.17ms
step:144/2330 train_time:5789ms step_avg:40.20ms
step:145/2330 train_time:5825ms step_avg:40.18ms
step:146/2330 train_time:5871ms step_avg:40.21ms
step:147/2330 train_time:5908ms step_avg:40.19ms
step:148/2330 train_time:5954ms step_avg:40.23ms
step:149/2330 train_time:5989ms step_avg:40.19ms
step:150/2330 train_time:6034ms step_avg:40.23ms
step:151/2330 train_time:6069ms step_avg:40.19ms
step:152/2330 train_time:6115ms step_avg:40.23ms
step:153/2330 train_time:6149ms step_avg:40.19ms
step:154/2330 train_time:6195ms step_avg:40.23ms
step:155/2330 train_time:6229ms step_avg:40.19ms
step:156/2330 train_time:6275ms step_avg:40.22ms
step:157/2330 train_time:6310ms step_avg:40.19ms
step:158/2330 train_time:6356ms step_avg:40.23ms
step:159/2330 train_time:6392ms step_avg:40.20ms
step:160/2330 train_time:6437ms step_avg:40.23ms
step:161/2330 train_time:6473ms step_avg:40.20ms
step:162/2330 train_time:6518ms step_avg:40.24ms
step:163/2330 train_time:6554ms step_avg:40.21ms
step:164/2330 train_time:6598ms step_avg:40.23ms
step:165/2330 train_time:6634ms step_avg:40.21ms
step:166/2330 train_time:6680ms step_avg:40.24ms
step:167/2330 train_time:6716ms step_avg:40.21ms
step:168/2330 train_time:6761ms step_avg:40.24ms
step:169/2330 train_time:6797ms step_avg:40.22ms
step:170/2330 train_time:6843ms step_avg:40.25ms
step:171/2330 train_time:6879ms step_avg:40.23ms
step:172/2330 train_time:6923ms step_avg:40.25ms
step:173/2330 train_time:6960ms step_avg:40.23ms
step:174/2330 train_time:7005ms step_avg:40.26ms
step:175/2330 train_time:7041ms step_avg:40.23ms
step:176/2330 train_time:7086ms step_avg:40.26ms
step:177/2330 train_time:7121ms step_avg:40.23ms
step:178/2330 train_time:7166ms step_avg:40.26ms
step:179/2330 train_time:7202ms step_avg:40.23ms
step:180/2330 train_time:7247ms step_avg:40.26ms
step:181/2330 train_time:7283ms step_avg:40.24ms
step:182/2330 train_time:7329ms step_avg:40.27ms
step:183/2330 train_time:7365ms step_avg:40.25ms
step:184/2330 train_time:7412ms step_avg:40.28ms
step:185/2330 train_time:7448ms step_avg:40.26ms
step:186/2330 train_time:7493ms step_avg:40.28ms
step:187/2330 train_time:7529ms step_avg:40.26ms
step:188/2330 train_time:7574ms step_avg:40.29ms
step:189/2330 train_time:7611ms step_avg:40.27ms
step:190/2330 train_time:7655ms step_avg:40.29ms
step:191/2330 train_time:7692ms step_avg:40.27ms
step:192/2330 train_time:7737ms step_avg:40.30ms
step:193/2330 train_time:7772ms step_avg:40.27ms
step:194/2330 train_time:7817ms step_avg:40.29ms
step:195/2330 train_time:7854ms step_avg:40.27ms
step:196/2330 train_time:7899ms step_avg:40.30ms
step:197/2330 train_time:7935ms step_avg:40.28ms
step:198/2330 train_time:7980ms step_avg:40.30ms
step:199/2330 train_time:8015ms step_avg:40.28ms
step:200/2330 train_time:8060ms step_avg:40.30ms
step:201/2330 train_time:8096ms step_avg:40.28ms
step:202/2330 train_time:8141ms step_avg:40.30ms
step:203/2330 train_time:8177ms step_avg:40.28ms
step:204/2330 train_time:8222ms step_avg:40.31ms
step:205/2330 train_time:8258ms step_avg:40.28ms
step:206/2330 train_time:8304ms step_avg:40.31ms
step:207/2330 train_time:8339ms step_avg:40.29ms
step:208/2330 train_time:8384ms step_avg:40.31ms
step:209/2330 train_time:8419ms step_avg:40.28ms
step:210/2330 train_time:8465ms step_avg:40.31ms
step:211/2330 train_time:8501ms step_avg:40.29ms
step:212/2330 train_time:8547ms step_avg:40.32ms
step:213/2330 train_time:8583ms step_avg:40.29ms
step:214/2330 train_time:8628ms step_avg:40.32ms
step:215/2330 train_time:8664ms step_avg:40.30ms
step:216/2330 train_time:8710ms step_avg:40.32ms
step:217/2330 train_time:8747ms step_avg:40.31ms
step:218/2330 train_time:8792ms step_avg:40.33ms
step:219/2330 train_time:8828ms step_avg:40.31ms
step:220/2330 train_time:8873ms step_avg:40.33ms
step:221/2330 train_time:8909ms step_avg:40.31ms
step:222/2330 train_time:8955ms step_avg:40.34ms
step:223/2330 train_time:8990ms step_avg:40.32ms
step:224/2330 train_time:9036ms step_avg:40.34ms
step:225/2330 train_time:9072ms step_avg:40.32ms
step:226/2330 train_time:9117ms step_avg:40.34ms
step:227/2330 train_time:9153ms step_avg:40.32ms
step:228/2330 train_time:9198ms step_avg:40.34ms
step:229/2330 train_time:9233ms step_avg:40.32ms
step:230/2330 train_time:9277ms step_avg:40.34ms
step:231/2330 train_time:9313ms step_avg:40.32ms
step:232/2330 train_time:9359ms step_avg:40.34ms
step:233/2330 train_time:9395ms step_avg:40.32ms
step:234/2330 train_time:9441ms step_avg:40.35ms
step:235/2330 train_time:9477ms step_avg:40.33ms
step:236/2330 train_time:9522ms step_avg:40.35ms
step:237/2330 train_time:9559ms step_avg:40.33ms
step:238/2330 train_time:9603ms step_avg:40.35ms
step:239/2330 train_time:9638ms step_avg:40.33ms
step:240/2330 train_time:9684ms step_avg:40.35ms
step:241/2330 train_time:9720ms step_avg:40.33ms
step:242/2330 train_time:9765ms step_avg:40.35ms
step:243/2330 train_time:9800ms step_avg:40.33ms
step:244/2330 train_time:9846ms step_avg:40.35ms
step:245/2330 train_time:9882ms step_avg:40.33ms
step:246/2330 train_time:9927ms step_avg:40.35ms
step:247/2330 train_time:9963ms step_avg:40.34ms
step:248/2330 train_time:10009ms step_avg:40.36ms
step:249/2330 train_time:10046ms step_avg:40.34ms
step:250/2330 train_time:10092ms step_avg:40.37ms
step:250/2330 val_loss:5.7491 train_time:10180ms step_avg:40.72ms
step:251/2330 train_time:10193ms step_avg:40.61ms
step:252/2330 train_time:10208ms step_avg:40.51ms
step:253/2330 train_time:10219ms step_avg:40.39ms
step:254/2330 train_time:10252ms step_avg:40.36ms
step:255/2330 train_time:10286ms step_avg:40.34ms
step:256/2330 train_time:10332ms step_avg:40.36ms
step:257/2330 train_time:10366ms step_avg:40.34ms
step:258/2330 train_time:10411ms step_avg:40.35ms
step:259/2330 train_time:10446ms step_avg:40.33ms
step:260/2330 train_time:10493ms step_avg:40.36ms
step:261/2330 train_time:10531ms step_avg:40.35ms
step:262/2330 train_time:10579ms step_avg:40.38ms
step:263/2330 train_time:10615ms step_avg:40.36ms
step:264/2330 train_time:10662ms step_avg:40.38ms
step:265/2330 train_time:10698ms step_avg:40.37ms
step:266/2330 train_time:10743ms step_avg:40.39ms
step:267/2330 train_time:10779ms step_avg:40.37ms
step:268/2330 train_time:10824ms step_avg:40.39ms
step:269/2330 train_time:10859ms step_avg:40.37ms
step:270/2330 train_time:10904ms step_avg:40.39ms
step:271/2330 train_time:10939ms step_avg:40.37ms
step:272/2330 train_time:10984ms step_avg:40.38ms
step:273/2330 train_time:11019ms step_avg:40.36ms
step:274/2330 train_time:11065ms step_avg:40.38ms
step:275/2330 train_time:11103ms step_avg:40.37ms
step:276/2330 train_time:11150ms step_avg:40.40ms
step:277/2330 train_time:11186ms step_avg:40.38ms
step:278/2330 train_time:11231ms step_avg:40.40ms
step:279/2330 train_time:11267ms step_avg:40.38ms
step:280/2330 train_time:11311ms step_avg:40.40ms
step:281/2330 train_time:11346ms step_avg:40.38ms
step:282/2330 train_time:11391ms step_avg:40.39ms
step:283/2330 train_time:11427ms step_avg:40.38ms
step:284/2330 train_time:11473ms step_avg:40.40ms
step:285/2330 train_time:11509ms step_avg:40.38ms
step:286/2330 train_time:11555ms step_avg:40.40ms
step:287/2330 train_time:11591ms step_avg:40.39ms
step:288/2330 train_time:11637ms step_avg:40.41ms
step:289/2330 train_time:11673ms step_avg:40.39ms
step:290/2330 train_time:11717ms step_avg:40.40ms
step:291/2330 train_time:11753ms step_avg:40.39ms
step:292/2330 train_time:11798ms step_avg:40.40ms
step:293/2330 train_time:11834ms step_avg:40.39ms
step:294/2330 train_time:11879ms step_avg:40.40ms
step:295/2330 train_time:11914ms step_avg:40.39ms
step:296/2330 train_time:11959ms step_avg:40.40ms
step:297/2330 train_time:11994ms step_avg:40.39ms
step:298/2330 train_time:12039ms step_avg:40.40ms
step:299/2330 train_time:12075ms step_avg:40.39ms
step:300/2330 train_time:12121ms step_avg:40.40ms
step:301/2330 train_time:12157ms step_avg:40.39ms
step:302/2330 train_time:12203ms step_avg:40.41ms
step:303/2330 train_time:12239ms step_avg:40.39ms
step:304/2330 train_time:12285ms step_avg:40.41ms
step:305/2330 train_time:12321ms step_avg:40.40ms
step:306/2330 train_time:12367ms step_avg:40.42ms
step:307/2330 train_time:12403ms step_avg:40.40ms
step:308/2330 train_time:12449ms step_avg:40.42ms
step:309/2330 train_time:12485ms step_avg:40.41ms
step:310/2330 train_time:12531ms step_avg:40.42ms
step:311/2330 train_time:12567ms step_avg:40.41ms
step:312/2330 train_time:12612ms step_avg:40.42ms
step:313/2330 train_time:12647ms step_avg:40.41ms
step:314/2330 train_time:12693ms step_avg:40.42ms
step:315/2330 train_time:12728ms step_avg:40.41ms
step:316/2330 train_time:12774ms step_avg:40.42ms
step:317/2330 train_time:12810ms step_avg:40.41ms
step:318/2330 train_time:12856ms step_avg:40.43ms
step:319/2330 train_time:12890ms step_avg:40.41ms
step:320/2330 train_time:12935ms step_avg:40.42ms
step:321/2330 train_time:12970ms step_avg:40.41ms
step:322/2330 train_time:13015ms step_avg:40.42ms
step:323/2330 train_time:13052ms step_avg:40.41ms
step:324/2330 train_time:13098ms step_avg:40.42ms
step:325/2330 train_time:13134ms step_avg:40.41ms
step:326/2330 train_time:13178ms step_avg:40.42ms
step:327/2330 train_time:13213ms step_avg:40.41ms
step:328/2330 train_time:13259ms step_avg:40.42ms
step:329/2330 train_time:13296ms step_avg:40.41ms
step:330/2330 train_time:13340ms step_avg:40.43ms
step:331/2330 train_time:13376ms step_avg:40.41ms
step:332/2330 train_time:13423ms step_avg:40.43ms
step:333/2330 train_time:13460ms step_avg:40.42ms
step:334/2330 train_time:13506ms step_avg:40.44ms
step:335/2330 train_time:13542ms step_avg:40.42ms
step:336/2330 train_time:13588ms step_avg:40.44ms
step:337/2330 train_time:13624ms step_avg:40.43ms
step:338/2330 train_time:13668ms step_avg:40.44ms
step:339/2330 train_time:13704ms step_avg:40.42ms
step:340/2330 train_time:13749ms step_avg:40.44ms
step:341/2330 train_time:13785ms step_avg:40.43ms
step:342/2330 train_time:13831ms step_avg:40.44ms
step:343/2330 train_time:13866ms step_avg:40.43ms
step:344/2330 train_time:13911ms step_avg:40.44ms
step:345/2330 train_time:13947ms step_avg:40.43ms
step:346/2330 train_time:13993ms step_avg:40.44ms
step:347/2330 train_time:14028ms step_avg:40.43ms
step:348/2330 train_time:14074ms step_avg:40.44ms
step:349/2330 train_time:14109ms step_avg:40.43ms
step:350/2330 train_time:14155ms step_avg:40.44ms
step:351/2330 train_time:14191ms step_avg:40.43ms
step:352/2330 train_time:14236ms step_avg:40.44ms
step:353/2330 train_time:14272ms step_avg:40.43ms
step:354/2330 train_time:14317ms step_avg:40.44ms
step:355/2330 train_time:14353ms step_avg:40.43ms
step:356/2330 train_time:14398ms step_avg:40.44ms
step:357/2330 train_time:14434ms step_avg:40.43ms
step:358/2330 train_time:14480ms step_avg:40.45ms
step:359/2330 train_time:14516ms step_avg:40.43ms
step:360/2330 train_time:14560ms step_avg:40.45ms
step:361/2330 train_time:14596ms step_avg:40.43ms
step:362/2330 train_time:14641ms step_avg:40.45ms
step:363/2330 train_time:14677ms step_avg:40.43ms
step:364/2330 train_time:14722ms step_avg:40.45ms
step:365/2330 train_time:14759ms step_avg:40.44ms
step:366/2330 train_time:14806ms step_avg:40.45ms
step:367/2330 train_time:14842ms step_avg:40.44ms
step:368/2330 train_time:14887ms step_avg:40.45ms
step:369/2330 train_time:14923ms step_avg:40.44ms
step:370/2330 train_time:14968ms step_avg:40.45ms
step:371/2330 train_time:15003ms step_avg:40.44ms
step:372/2330 train_time:15048ms step_avg:40.45ms
step:373/2330 train_time:15083ms step_avg:40.44ms
step:374/2330 train_time:15130ms step_avg:40.45ms
step:375/2330 train_time:15166ms step_avg:40.44ms
step:376/2330 train_time:15212ms step_avg:40.46ms
step:377/2330 train_time:15248ms step_avg:40.44ms
step:378/2330 train_time:15293ms step_avg:40.46ms
step:379/2330 train_time:15329ms step_avg:40.45ms
step:380/2330 train_time:15374ms step_avg:40.46ms
step:381/2330 train_time:15411ms step_avg:40.45ms
step:382/2330 train_time:15457ms step_avg:40.46ms
step:383/2330 train_time:15493ms step_avg:40.45ms
step:384/2330 train_time:15538ms step_avg:40.46ms
step:385/2330 train_time:15573ms step_avg:40.45ms
step:386/2330 train_time:15618ms step_avg:40.46ms
step:387/2330 train_time:15653ms step_avg:40.45ms
step:388/2330 train_time:15698ms step_avg:40.46ms
step:389/2330 train_time:15734ms step_avg:40.45ms
step:390/2330 train_time:15780ms step_avg:40.46ms
step:391/2330 train_time:15816ms step_avg:40.45ms
step:392/2330 train_time:15862ms step_avg:40.46ms
step:393/2330 train_time:15897ms step_avg:40.45ms
step:394/2330 train_time:15944ms step_avg:40.47ms
step:395/2330 train_time:15980ms step_avg:40.46ms
step:396/2330 train_time:16026ms step_avg:40.47ms
step:397/2330 train_time:16061ms step_avg:40.46ms
step:398/2330 train_time:16107ms step_avg:40.47ms
step:399/2330 train_time:16143ms step_avg:40.46ms
step:400/2330 train_time:16188ms step_avg:40.47ms
step:401/2330 train_time:16225ms step_avg:40.46ms
step:402/2330 train_time:16270ms step_avg:40.47ms
step:403/2330 train_time:16306ms step_avg:40.46ms
step:404/2330 train_time:16353ms step_avg:40.48ms
step:405/2330 train_time:16388ms step_avg:40.47ms
step:406/2330 train_time:16433ms step_avg:40.48ms
step:407/2330 train_time:16469ms step_avg:40.46ms
step:408/2330 train_time:16514ms step_avg:40.48ms
step:409/2330 train_time:16551ms step_avg:40.47ms
step:410/2330 train_time:16595ms step_avg:40.48ms
step:411/2330 train_time:16630ms step_avg:40.46ms
step:412/2330 train_time:16676ms step_avg:40.48ms
step:413/2330 train_time:16712ms step_avg:40.46ms
step:414/2330 train_time:16757ms step_avg:40.48ms
step:415/2330 train_time:16793ms step_avg:40.47ms
step:416/2330 train_time:16839ms step_avg:40.48ms
step:417/2330 train_time:16875ms step_avg:40.47ms
step:418/2330 train_time:16920ms step_avg:40.48ms
step:419/2330 train_time:16955ms step_avg:40.47ms
step:420/2330 train_time:17001ms step_avg:40.48ms
step:421/2330 train_time:17037ms step_avg:40.47ms
step:422/2330 train_time:17083ms step_avg:40.48ms
step:423/2330 train_time:17120ms step_avg:40.47ms
step:424/2330 train_time:17166ms step_avg:40.49ms
step:425/2330 train_time:17202ms step_avg:40.48ms
step:426/2330 train_time:17248ms step_avg:40.49ms
step:427/2330 train_time:17283ms step_avg:40.48ms
step:428/2330 train_time:17330ms step_avg:40.49ms
step:429/2330 train_time:17365ms step_avg:40.48ms
step:430/2330 train_time:17411ms step_avg:40.49ms
step:431/2330 train_time:17446ms step_avg:40.48ms
step:432/2330 train_time:17492ms step_avg:40.49ms
step:433/2330 train_time:17527ms step_avg:40.48ms
step:434/2330 train_time:17572ms step_avg:40.49ms
step:435/2330 train_time:17608ms step_avg:40.48ms
step:436/2330 train_time:17653ms step_avg:40.49ms
step:437/2330 train_time:17689ms step_avg:40.48ms
step:438/2330 train_time:17734ms step_avg:40.49ms
step:439/2330 train_time:17769ms step_avg:40.48ms
step:440/2330 train_time:17815ms step_avg:40.49ms
step:441/2330 train_time:17851ms step_avg:40.48ms
step:442/2330 train_time:17897ms step_avg:40.49ms
step:443/2330 train_time:17932ms step_avg:40.48ms
step:444/2330 train_time:17978ms step_avg:40.49ms
step:445/2330 train_time:18014ms step_avg:40.48ms
step:446/2330 train_time:18059ms step_avg:40.49ms
step:447/2330 train_time:18095ms step_avg:40.48ms
step:448/2330 train_time:18141ms step_avg:40.49ms
step:449/2330 train_time:18176ms step_avg:40.48ms
step:450/2330 train_time:18221ms step_avg:40.49ms
step:451/2330 train_time:18257ms step_avg:40.48ms
step:452/2330 train_time:18303ms step_avg:40.49ms
step:453/2330 train_time:18339ms step_avg:40.48ms
step:454/2330 train_time:18385ms step_avg:40.50ms
step:455/2330 train_time:18421ms step_avg:40.49ms
step:456/2330 train_time:18467ms step_avg:40.50ms
step:457/2330 train_time:18503ms step_avg:40.49ms
step:458/2330 train_time:18548ms step_avg:40.50ms
step:459/2330 train_time:18584ms step_avg:40.49ms
step:460/2330 train_time:18629ms step_avg:40.50ms
step:461/2330 train_time:18665ms step_avg:40.49ms
step:462/2330 train_time:18710ms step_avg:40.50ms
step:463/2330 train_time:18746ms step_avg:40.49ms
step:464/2330 train_time:18792ms step_avg:40.50ms
step:465/2330 train_time:18827ms step_avg:40.49ms
step:466/2330 train_time:18872ms step_avg:40.50ms
step:467/2330 train_time:18908ms step_avg:40.49ms
step:468/2330 train_time:18953ms step_avg:40.50ms
step:469/2330 train_time:18990ms step_avg:40.49ms
step:470/2330 train_time:19036ms step_avg:40.50ms
step:471/2330 train_time:19072ms step_avg:40.49ms
step:472/2330 train_time:19117ms step_avg:40.50ms
step:473/2330 train_time:19153ms step_avg:40.49ms
step:474/2330 train_time:19198ms step_avg:40.50ms
step:475/2330 train_time:19234ms step_avg:40.49ms
step:476/2330 train_time:19279ms step_avg:40.50ms
step:477/2330 train_time:19314ms step_avg:40.49ms
step:478/2330 train_time:19360ms step_avg:40.50ms
step:479/2330 train_time:19396ms step_avg:40.49ms
step:480/2330 train_time:19442ms step_avg:40.50ms
step:481/2330 train_time:19478ms step_avg:40.49ms
step:482/2330 train_time:19524ms step_avg:40.51ms
step:483/2330 train_time:19561ms step_avg:40.50ms
step:484/2330 train_time:19607ms step_avg:40.51ms
step:485/2330 train_time:19643ms step_avg:40.50ms
step:486/2330 train_time:19688ms step_avg:40.51ms
step:487/2330 train_time:19724ms step_avg:40.50ms
step:488/2330 train_time:19770ms step_avg:40.51ms
step:489/2330 train_time:19805ms step_avg:40.50ms
step:490/2330 train_time:19851ms step_avg:40.51ms
step:491/2330 train_time:19887ms step_avg:40.50ms
step:492/2330 train_time:19933ms step_avg:40.51ms
step:493/2330 train_time:19968ms step_avg:40.50ms
step:494/2330 train_time:20014ms step_avg:40.51ms
step:495/2330 train_time:20049ms step_avg:40.50ms
step:496/2330 train_time:20095ms step_avg:40.51ms
step:497/2330 train_time:20131ms step_avg:40.51ms
step:498/2330 train_time:20177ms step_avg:40.52ms
step:499/2330 train_time:20212ms step_avg:40.50ms
step:500/2330 train_time:20258ms step_avg:40.52ms
step:500/2330 val_loss:5.5872 train_time:20348ms step_avg:40.70ms
step:501/2330 train_time:20361ms step_avg:40.64ms
step:502/2330 train_time:20372ms step_avg:40.58ms
step:503/2330 train_time:20382ms step_avg:40.52ms
step:504/2330 train_time:20421ms step_avg:40.52ms
step:505/2330 train_time:20456ms step_avg:40.51ms
step:506/2330 train_time:20500ms step_avg:40.51ms
step:507/2330 train_time:20535ms step_avg:40.50ms
step:508/2330 train_time:20579ms step_avg:40.51ms
step:509/2330 train_time:20615ms step_avg:40.50ms
step:510/2330 train_time:20665ms step_avg:40.52ms
step:511/2330 train_time:20706ms step_avg:40.52ms
step:512/2330 train_time:20754ms step_avg:40.53ms
step:513/2330 train_time:20789ms step_avg:40.52ms
step:514/2330 train_time:20834ms step_avg:40.53ms
step:515/2330 train_time:20870ms step_avg:40.52ms
step:516/2330 train_time:20916ms step_avg:40.54ms
step:517/2330 train_time:20952ms step_avg:40.53ms
step:518/2330 train_time:20996ms step_avg:40.53ms
step:519/2330 train_time:21031ms step_avg:40.52ms
step:520/2330 train_time:21076ms step_avg:40.53ms
step:521/2330 train_time:21111ms step_avg:40.52ms
step:522/2330 train_time:21156ms step_avg:40.53ms
step:523/2330 train_time:21191ms step_avg:40.52ms
step:524/2330 train_time:21236ms step_avg:40.53ms
step:525/2330 train_time:21272ms step_avg:40.52ms
step:526/2330 train_time:21317ms step_avg:40.53ms
step:527/2330 train_time:21352ms step_avg:40.52ms
step:528/2330 train_time:21397ms step_avg:40.53ms
step:529/2330 train_time:21433ms step_avg:40.52ms
step:530/2330 train_time:21478ms step_avg:40.52ms
step:531/2330 train_time:21513ms step_avg:40.51ms
step:532/2330 train_time:21559ms step_avg:40.52ms
step:533/2330 train_time:21595ms step_avg:40.52ms
step:534/2330 train_time:21642ms step_avg:40.53ms
step:535/2330 train_time:21679ms step_avg:40.52ms
step:536/2330 train_time:21725ms step_avg:40.53ms
step:537/2330 train_time:21762ms step_avg:40.53ms
step:538/2330 train_time:21808ms step_avg:40.54ms
step:539/2330 train_time:21845ms step_avg:40.53ms
step:540/2330 train_time:21891ms step_avg:40.54ms
step:541/2330 train_time:21926ms step_avg:40.53ms
step:542/2330 train_time:21972ms step_avg:40.54ms
step:543/2330 train_time:22007ms step_avg:40.53ms
step:544/2330 train_time:22053ms step_avg:40.54ms
step:545/2330 train_time:22087ms step_avg:40.53ms
step:546/2330 train_time:22131ms step_avg:40.53ms
step:547/2330 train_time:22167ms step_avg:40.52ms
step:548/2330 train_time:22212ms step_avg:40.53ms
step:549/2330 train_time:22247ms step_avg:40.52ms
step:550/2330 train_time:22292ms step_avg:40.53ms
step:551/2330 train_time:22327ms step_avg:40.52ms
step:552/2330 train_time:22373ms step_avg:40.53ms
step:553/2330 train_time:22409ms step_avg:40.52ms
step:554/2330 train_time:22454ms step_avg:40.53ms
step:555/2330 train_time:22490ms step_avg:40.52ms
step:556/2330 train_time:22536ms step_avg:40.53ms
step:557/2330 train_time:22573ms step_avg:40.53ms
step:558/2330 train_time:22619ms step_avg:40.54ms
step:559/2330 train_time:22655ms step_avg:40.53ms
step:560/2330 train_time:22701ms step_avg:40.54ms
step:561/2330 train_time:22738ms step_avg:40.53ms
step:562/2330 train_time:22784ms step_avg:40.54ms
step:563/2330 train_time:22820ms step_avg:40.53ms
step:564/2330 train_time:22865ms step_avg:40.54ms
step:565/2330 train_time:22901ms step_avg:40.53ms
step:566/2330 train_time:22946ms step_avg:40.54ms
step:567/2330 train_time:22982ms step_avg:40.53ms
step:568/2330 train_time:23028ms step_avg:40.54ms
step:569/2330 train_time:23065ms step_avg:40.54ms
step:570/2330 train_time:23110ms step_avg:40.54ms
step:571/2330 train_time:23147ms step_avg:40.54ms
step:572/2330 train_time:23191ms step_avg:40.54ms
step:573/2330 train_time:23226ms step_avg:40.53ms
step:574/2330 train_time:23273ms step_avg:40.54ms
step:575/2330 train_time:23307ms step_avg:40.53ms
step:576/2330 train_time:23353ms step_avg:40.54ms
step:577/2330 train_time:23388ms step_avg:40.53ms
step:578/2330 train_time:23433ms step_avg:40.54ms
step:579/2330 train_time:23470ms step_avg:40.53ms
step:580/2330 train_time:23515ms step_avg:40.54ms
step:581/2330 train_time:23552ms step_avg:40.54ms
step:582/2330 train_time:23598ms step_avg:40.55ms
step:583/2330 train_time:23634ms step_avg:40.54ms
step:584/2330 train_time:23679ms step_avg:40.55ms
step:585/2330 train_time:23717ms step_avg:40.54ms
step:586/2330 train_time:23763ms step_avg:40.55ms
step:587/2330 train_time:23799ms step_avg:40.54ms
step:588/2330 train_time:23844ms step_avg:40.55ms
step:589/2330 train_time:23880ms step_avg:40.54ms
step:590/2330 train_time:23925ms step_avg:40.55ms
step:591/2330 train_time:23961ms step_avg:40.54ms
step:592/2330 train_time:24006ms step_avg:40.55ms
step:593/2330 train_time:24041ms step_avg:40.54ms
step:594/2330 train_time:24086ms step_avg:40.55ms
step:595/2330 train_time:24122ms step_avg:40.54ms
step:596/2330 train_time:24166ms step_avg:40.55ms
step:597/2330 train_time:24202ms step_avg:40.54ms
step:598/2330 train_time:24247ms step_avg:40.55ms
step:599/2330 train_time:24283ms step_avg:40.54ms
step:600/2330 train_time:24328ms step_avg:40.55ms
step:601/2330 train_time:24364ms step_avg:40.54ms
step:602/2330 train_time:24411ms step_avg:40.55ms
step:603/2330 train_time:24447ms step_avg:40.54ms
step:604/2330 train_time:24492ms step_avg:40.55ms
step:605/2330 train_time:24529ms step_avg:40.54ms
step:606/2330 train_time:24574ms step_avg:40.55ms
step:607/2330 train_time:24609ms step_avg:40.54ms
step:608/2330 train_time:24656ms step_avg:40.55ms
step:609/2330 train_time:24692ms step_avg:40.54ms
step:610/2330 train_time:24737ms step_avg:40.55ms
step:611/2330 train_time:24773ms step_avg:40.55ms
step:612/2330 train_time:24819ms step_avg:40.55ms
step:613/2330 train_time:24855ms step_avg:40.55ms
step:614/2330 train_time:24901ms step_avg:40.56ms
step:615/2330 train_time:24936ms step_avg:40.55ms
step:616/2330 train_time:24982ms step_avg:40.56ms
step:617/2330 train_time:25018ms step_avg:40.55ms
step:618/2330 train_time:25064ms step_avg:40.56ms
step:619/2330 train_time:25099ms step_avg:40.55ms
step:620/2330 train_time:25144ms step_avg:40.55ms
step:621/2330 train_time:25179ms step_avg:40.55ms
step:622/2330 train_time:25224ms step_avg:40.55ms
step:623/2330 train_time:25261ms step_avg:40.55ms
step:624/2330 train_time:25307ms step_avg:40.56ms
step:625/2330 train_time:25343ms step_avg:40.55ms
step:626/2330 train_time:25388ms step_avg:40.56ms
step:627/2330 train_time:25424ms step_avg:40.55ms
step:628/2330 train_time:25470ms step_avg:40.56ms
step:629/2330 train_time:25507ms step_avg:40.55ms
step:630/2330 train_time:25551ms step_avg:40.56ms
step:631/2330 train_time:25587ms step_avg:40.55ms
step:632/2330 train_time:25632ms step_avg:40.56ms
step:633/2330 train_time:25668ms step_avg:40.55ms
step:634/2330 train_time:25714ms step_avg:40.56ms
step:635/2330 train_time:25750ms step_avg:40.55ms
step:636/2330 train_time:25797ms step_avg:40.56ms
step:637/2330 train_time:25832ms step_avg:40.55ms
step:638/2330 train_time:25878ms step_avg:40.56ms
step:639/2330 train_time:25914ms step_avg:40.55ms
step:640/2330 train_time:25960ms step_avg:40.56ms
step:641/2330 train_time:25997ms step_avg:40.56ms
step:642/2330 train_time:26042ms step_avg:40.56ms
step:643/2330 train_time:26078ms step_avg:40.56ms
step:644/2330 train_time:26123ms step_avg:40.56ms
step:645/2330 train_time:26158ms step_avg:40.56ms
step:646/2330 train_time:26203ms step_avg:40.56ms
step:647/2330 train_time:26238ms step_avg:40.55ms
step:648/2330 train_time:26283ms step_avg:40.56ms
step:649/2330 train_time:26320ms step_avg:40.55ms
step:650/2330 train_time:26365ms step_avg:40.56ms
step:651/2330 train_time:26400ms step_avg:40.55ms
step:652/2330 train_time:26446ms step_avg:40.56ms
step:653/2330 train_time:26482ms step_avg:40.55ms
step:654/2330 train_time:26527ms step_avg:40.56ms
step:655/2330 train_time:26564ms step_avg:40.56ms
step:656/2330 train_time:26609ms step_avg:40.56ms
step:657/2330 train_time:26645ms step_avg:40.56ms
step:658/2330 train_time:26691ms step_avg:40.56ms
step:659/2330 train_time:26727ms step_avg:40.56ms
step:660/2330 train_time:26773ms step_avg:40.57ms
step:661/2330 train_time:26810ms step_avg:40.56ms
step:662/2330 train_time:26855ms step_avg:40.57ms
step:663/2330 train_time:26891ms step_avg:40.56ms
step:664/2330 train_time:26936ms step_avg:40.57ms
step:665/2330 train_time:26972ms step_avg:40.56ms
step:666/2330 train_time:27017ms step_avg:40.57ms
step:667/2330 train_time:27053ms step_avg:40.56ms
step:668/2330 train_time:27099ms step_avg:40.57ms
step:669/2330 train_time:27135ms step_avg:40.56ms
step:670/2330 train_time:27181ms step_avg:40.57ms
step:671/2330 train_time:27216ms step_avg:40.56ms
step:672/2330 train_time:27263ms step_avg:40.57ms
step:673/2330 train_time:27299ms step_avg:40.56ms
step:674/2330 train_time:27344ms step_avg:40.57ms
step:675/2330 train_time:27380ms step_avg:40.56ms
step:676/2330 train_time:27425ms step_avg:40.57ms
step:677/2330 train_time:27460ms step_avg:40.56ms
step:678/2330 train_time:27506ms step_avg:40.57ms
step:679/2330 train_time:27541ms step_avg:40.56ms
step:680/2330 train_time:27587ms step_avg:40.57ms
step:681/2330 train_time:27623ms step_avg:40.56ms
step:682/2330 train_time:27669ms step_avg:40.57ms
step:683/2330 train_time:27705ms step_avg:40.56ms
step:684/2330 train_time:27751ms step_avg:40.57ms
step:685/2330 train_time:27787ms step_avg:40.56ms
step:686/2330 train_time:27833ms step_avg:40.57ms
step:687/2330 train_time:27870ms step_avg:40.57ms
step:688/2330 train_time:27915ms step_avg:40.57ms
step:689/2330 train_time:27951ms step_avg:40.57ms
step:690/2330 train_time:27996ms step_avg:40.57ms
step:691/2330 train_time:28032ms step_avg:40.57ms
step:692/2330 train_time:28078ms step_avg:40.58ms
step:693/2330 train_time:28114ms step_avg:40.57ms
step:694/2330 train_time:28160ms step_avg:40.58ms
step:695/2330 train_time:28195ms step_avg:40.57ms
step:696/2330 train_time:28241ms step_avg:40.58ms
step:697/2330 train_time:28277ms step_avg:40.57ms
step:698/2330 train_time:28324ms step_avg:40.58ms
step:699/2330 train_time:28359ms step_avg:40.57ms
step:700/2330 train_time:28405ms step_avg:40.58ms
step:701/2330 train_time:28440ms step_avg:40.57ms
step:702/2330 train_time:28485ms step_avg:40.58ms
step:703/2330 train_time:28521ms step_avg:40.57ms
step:704/2330 train_time:28566ms step_avg:40.58ms
step:705/2330 train_time:28601ms step_avg:40.57ms
step:706/2330 train_time:28646ms step_avg:40.58ms
step:707/2330 train_time:28683ms step_avg:40.57ms
step:708/2330 train_time:28728ms step_avg:40.58ms
step:709/2330 train_time:28765ms step_avg:40.57ms
step:710/2330 train_time:28812ms step_avg:40.58ms
step:711/2330 train_time:28848ms step_avg:40.57ms
step:712/2330 train_time:28893ms step_avg:40.58ms
step:713/2330 train_time:28929ms step_avg:40.57ms
step:714/2330 train_time:28975ms step_avg:40.58ms
step:715/2330 train_time:29011ms step_avg:40.57ms
step:716/2330 train_time:29056ms step_avg:40.58ms
step:717/2330 train_time:29092ms step_avg:40.57ms
step:718/2330 train_time:29138ms step_avg:40.58ms
step:719/2330 train_time:29173ms step_avg:40.57ms
step:720/2330 train_time:29219ms step_avg:40.58ms
step:721/2330 train_time:29255ms step_avg:40.58ms
step:722/2330 train_time:29301ms step_avg:40.58ms
step:723/2330 train_time:29336ms step_avg:40.58ms
step:724/2330 train_time:29382ms step_avg:40.58ms
step:725/2330 train_time:29418ms step_avg:40.58ms
step:726/2330 train_time:29464ms step_avg:40.58ms
step:727/2330 train_time:29500ms step_avg:40.58ms
step:728/2330 train_time:29544ms step_avg:40.58ms
step:729/2330 train_time:29580ms step_avg:40.58ms
step:730/2330 train_time:29625ms step_avg:40.58ms
step:731/2330 train_time:29661ms step_avg:40.58ms
step:732/2330 train_time:29707ms step_avg:40.58ms
step:733/2330 train_time:29743ms step_avg:40.58ms
step:734/2330 train_time:29789ms step_avg:40.58ms
step:735/2330 train_time:29826ms step_avg:40.58ms
step:736/2330 train_time:29872ms step_avg:40.59ms
step:737/2330 train_time:29907ms step_avg:40.58ms
step:738/2330 train_time:29952ms step_avg:40.59ms
step:739/2330 train_time:29989ms step_avg:40.58ms
step:740/2330 train_time:30034ms step_avg:40.59ms
step:741/2330 train_time:30070ms step_avg:40.58ms
step:742/2330 train_time:30115ms step_avg:40.59ms
step:743/2330 train_time:30151ms step_avg:40.58ms
step:744/2330 train_time:30196ms step_avg:40.59ms
step:745/2330 train_time:30232ms step_avg:40.58ms
step:746/2330 train_time:30279ms step_avg:40.59ms
step:747/2330 train_time:30314ms step_avg:40.58ms
step:748/2330 train_time:30360ms step_avg:40.59ms
step:749/2330 train_time:30395ms step_avg:40.58ms
step:750/2330 train_time:30441ms step_avg:40.59ms
step:750/2330 val_loss:5.3817 train_time:30530ms step_avg:40.71ms
step:751/2330 train_time:30543ms step_avg:40.67ms
step:752/2330 train_time:30555ms step_avg:40.63ms
step:753/2330 train_time:30566ms step_avg:40.59ms
step:754/2330 train_time:30604ms step_avg:40.59ms
step:755/2330 train_time:30639ms step_avg:40.58ms
step:756/2330 train_time:30683ms step_avg:40.59ms
step:757/2330 train_time:30717ms step_avg:40.58ms
step:758/2330 train_time:30762ms step_avg:40.58ms
step:759/2330 train_time:30797ms step_avg:40.58ms
step:760/2330 train_time:30843ms step_avg:40.58ms
step:761/2330 train_time:30885ms step_avg:40.58ms
step:762/2330 train_time:30936ms step_avg:40.60ms
step:763/2330 train_time:30973ms step_avg:40.59ms
step:764/2330 train_time:31018ms step_avg:40.60ms
step:765/2330 train_time:31055ms step_avg:40.59ms
step:766/2330 train_time:31099ms step_avg:40.60ms
step:767/2330 train_time:31134ms step_avg:40.59ms
step:768/2330 train_time:31179ms step_avg:40.60ms
step:769/2330 train_time:31215ms step_avg:40.59ms
step:770/2330 train_time:31260ms step_avg:40.60ms
step:771/2330 train_time:31295ms step_avg:40.59ms
step:772/2330 train_time:31340ms step_avg:40.60ms
step:773/2330 train_time:31375ms step_avg:40.59ms
step:774/2330 train_time:31420ms step_avg:40.59ms
step:775/2330 train_time:31456ms step_avg:40.59ms
step:776/2330 train_time:31501ms step_avg:40.59ms
step:777/2330 train_time:31537ms step_avg:40.59ms
step:778/2330 train_time:31581ms step_avg:40.59ms
step:779/2330 train_time:31617ms step_avg:40.59ms
step:780/2330 train_time:31661ms step_avg:40.59ms
step:781/2330 train_time:31696ms step_avg:40.58ms
step:782/2330 train_time:31741ms step_avg:40.59ms
step:783/2330 train_time:31777ms step_avg:40.58ms
step:784/2330 train_time:31824ms step_avg:40.59ms
step:785/2330 train_time:31862ms step_avg:40.59ms
step:786/2330 train_time:31911ms step_avg:40.60ms
step:787/2330 train_time:31949ms step_avg:40.60ms
step:788/2330 train_time:31995ms step_avg:40.60ms
step:789/2330 train_time:32031ms step_avg:40.60ms
step:790/2330 train_time:32077ms step_avg:40.60ms
step:791/2330 train_time:32113ms step_avg:40.60ms
step:792/2330 train_time:32158ms step_avg:40.60ms
step:793/2330 train_time:32194ms step_avg:40.60ms
step:794/2330 train_time:32239ms step_avg:40.60ms
step:795/2330 train_time:32274ms step_avg:40.60ms
step:796/2330 train_time:32319ms step_avg:40.60ms
step:797/2330 train_time:32355ms step_avg:40.60ms
step:798/2330 train_time:32400ms step_avg:40.60ms
step:799/2330 train_time:32435ms step_avg:40.59ms
step:800/2330 train_time:32480ms step_avg:40.60ms
step:801/2330 train_time:32516ms step_avg:40.59ms
step:802/2330 train_time:32561ms step_avg:40.60ms
step:803/2330 train_time:32596ms step_avg:40.59ms
step:804/2330 train_time:32640ms step_avg:40.60ms
step:805/2330 train_time:32676ms step_avg:40.59ms
step:806/2330 train_time:32721ms step_avg:40.60ms
step:807/2330 train_time:32757ms step_avg:40.59ms
step:808/2330 train_time:32804ms step_avg:40.60ms
step:809/2330 train_time:32840ms step_avg:40.59ms
step:810/2330 train_time:32887ms step_avg:40.60ms
step:811/2330 train_time:32925ms step_avg:40.60ms
step:812/2330 train_time:32971ms step_avg:40.60ms
step:813/2330 train_time:33007ms step_avg:40.60ms
step:814/2330 train_time:33052ms step_avg:40.60ms
step:815/2330 train_time:33087ms step_avg:40.60ms
step:816/2330 train_time:33133ms step_avg:40.60ms
step:817/2330 train_time:33170ms step_avg:40.60ms
step:818/2330 train_time:33214ms step_avg:40.60ms
step:819/2330 train_time:33250ms step_avg:40.60ms
step:820/2330 train_time:33295ms step_avg:40.60ms
step:821/2330 train_time:33331ms step_avg:40.60ms
step:822/2330 train_time:33376ms step_avg:40.60ms
step:823/2330 train_time:33412ms step_avg:40.60ms
step:824/2330 train_time:33457ms step_avg:40.60ms
step:825/2330 train_time:33493ms step_avg:40.60ms
step:826/2330 train_time:33538ms step_avg:40.60ms
step:827/2330 train_time:33573ms step_avg:40.60ms
step:828/2330 train_time:33618ms step_avg:40.60ms
step:829/2330 train_time:33654ms step_avg:40.60ms
step:830/2330 train_time:33700ms step_avg:40.60ms
step:831/2330 train_time:33735ms step_avg:40.60ms
step:832/2330 train_time:33781ms step_avg:40.60ms
step:833/2330 train_time:33818ms step_avg:40.60ms
step:834/2330 train_time:33863ms step_avg:40.60ms
step:835/2330 train_time:33900ms step_avg:40.60ms
step:836/2330 train_time:33947ms step_avg:40.61ms
step:837/2330 train_time:33983ms step_avg:40.60ms
step:838/2330 train_time:34029ms step_avg:40.61ms
step:839/2330 train_time:34065ms step_avg:40.60ms
step:840/2330 train_time:34111ms step_avg:40.61ms
step:841/2330 train_time:34147ms step_avg:40.60ms
step:842/2330 train_time:34192ms step_avg:40.61ms
step:843/2330 train_time:34228ms step_avg:40.60ms
step:844/2330 train_time:34274ms step_avg:40.61ms
step:845/2330 train_time:34310ms step_avg:40.60ms
step:846/2330 train_time:34355ms step_avg:40.61ms
step:847/2330 train_time:34390ms step_avg:40.60ms
step:848/2330 train_time:34437ms step_avg:40.61ms
step:849/2330 train_time:34472ms step_avg:40.60ms
step:850/2330 train_time:34516ms step_avg:40.61ms
step:851/2330 train_time:34551ms step_avg:40.60ms
step:852/2330 train_time:34597ms step_avg:40.61ms
step:853/2330 train_time:34632ms step_avg:40.60ms
step:854/2330 train_time:34678ms step_avg:40.61ms
step:855/2330 train_time:34714ms step_avg:40.60ms
step:856/2330 train_time:34759ms step_avg:40.61ms
step:857/2330 train_time:34795ms step_avg:40.60ms
step:858/2330 train_time:34841ms step_avg:40.61ms
step:859/2330 train_time:34877ms step_avg:40.60ms
step:860/2330 train_time:34923ms step_avg:40.61ms
step:861/2330 train_time:34959ms step_avg:40.60ms
step:862/2330 train_time:35005ms step_avg:40.61ms
step:863/2330 train_time:35042ms step_avg:40.60ms
step:864/2330 train_time:35088ms step_avg:40.61ms
step:865/2330 train_time:35125ms step_avg:40.61ms
step:866/2330 train_time:35170ms step_avg:40.61ms
step:867/2330 train_time:35207ms step_avg:40.61ms
step:868/2330 train_time:35251ms step_avg:40.61ms
step:869/2330 train_time:35287ms step_avg:40.61ms
step:870/2330 train_time:35333ms step_avg:40.61ms
step:871/2330 train_time:35369ms step_avg:40.61ms
step:872/2330 train_time:35415ms step_avg:40.61ms
step:873/2330 train_time:35450ms step_avg:40.61ms
step:874/2330 train_time:35495ms step_avg:40.61ms
step:875/2330 train_time:35530ms step_avg:40.61ms
step:876/2330 train_time:35576ms step_avg:40.61ms
step:877/2330 train_time:35613ms step_avg:40.61ms
step:878/2330 train_time:35658ms step_avg:40.61ms
step:879/2330 train_time:35694ms step_avg:40.61ms
step:880/2330 train_time:35740ms step_avg:40.61ms
step:881/2330 train_time:35776ms step_avg:40.61ms
step:882/2330 train_time:35822ms step_avg:40.61ms
step:883/2330 train_time:35858ms step_avg:40.61ms
step:884/2330 train_time:35903ms step_avg:40.61ms
step:885/2330 train_time:35938ms step_avg:40.61ms
step:886/2330 train_time:35983ms step_avg:40.61ms
step:887/2330 train_time:36020ms step_avg:40.61ms
step:888/2330 train_time:36067ms step_avg:40.62ms
step:889/2330 train_time:36103ms step_avg:40.61ms
step:890/2330 train_time:36149ms step_avg:40.62ms
step:891/2330 train_time:36184ms step_avg:40.61ms
step:892/2330 train_time:36230ms step_avg:40.62ms
step:893/2330 train_time:36267ms step_avg:40.61ms
step:894/2330 train_time:36312ms step_avg:40.62ms
step:895/2330 train_time:36348ms step_avg:40.61ms
step:896/2330 train_time:36393ms step_avg:40.62ms
step:897/2330 train_time:36429ms step_avg:40.61ms
step:898/2330 train_time:36474ms step_avg:40.62ms
step:899/2330 train_time:36510ms step_avg:40.61ms
step:900/2330 train_time:36555ms step_avg:40.62ms
step:901/2330 train_time:36591ms step_avg:40.61ms
step:902/2330 train_time:36637ms step_avg:40.62ms
step:903/2330 train_time:36672ms step_avg:40.61ms
step:904/2330 train_time:36718ms step_avg:40.62ms
step:905/2330 train_time:36754ms step_avg:40.61ms
step:906/2330 train_time:36800ms step_avg:40.62ms
step:907/2330 train_time:36835ms step_avg:40.61ms
step:908/2330 train_time:36881ms step_avg:40.62ms
step:909/2330 train_time:36917ms step_avg:40.61ms
step:910/2330 train_time:36962ms step_avg:40.62ms
step:911/2330 train_time:36998ms step_avg:40.61ms
step:912/2330 train_time:37044ms step_avg:40.62ms
step:913/2330 train_time:37080ms step_avg:40.61ms
step:914/2330 train_time:37126ms step_avg:40.62ms
step:915/2330 train_time:37162ms step_avg:40.61ms
step:916/2330 train_time:37208ms step_avg:40.62ms
step:917/2330 train_time:37244ms step_avg:40.61ms
step:918/2330 train_time:37289ms step_avg:40.62ms
step:919/2330 train_time:37327ms step_avg:40.62ms
step:920/2330 train_time:37373ms step_avg:40.62ms
step:921/2330 train_time:37409ms step_avg:40.62ms
step:922/2330 train_time:37454ms step_avg:40.62ms
step:923/2330 train_time:37489ms step_avg:40.62ms
step:924/2330 train_time:37535ms step_avg:40.62ms
step:925/2330 train_time:37571ms step_avg:40.62ms
step:926/2330 train_time:37616ms step_avg:40.62ms
step:927/2330 train_time:37652ms step_avg:40.62ms
step:928/2330 train_time:37697ms step_avg:40.62ms
step:929/2330 train_time:37732ms step_avg:40.62ms
step:930/2330 train_time:37778ms step_avg:40.62ms
step:931/2330 train_time:37813ms step_avg:40.62ms
step:932/2330 train_time:37860ms step_avg:40.62ms
step:933/2330 train_time:37896ms step_avg:40.62ms
step:934/2330 train_time:37941ms step_avg:40.62ms
step:935/2330 train_time:37977ms step_avg:40.62ms
step:936/2330 train_time:38022ms step_avg:40.62ms
step:937/2330 train_time:38058ms step_avg:40.62ms
step:938/2330 train_time:38103ms step_avg:40.62ms
step:939/2330 train_time:38140ms step_avg:40.62ms
step:940/2330 train_time:38185ms step_avg:40.62ms
step:941/2330 train_time:38221ms step_avg:40.62ms
step:942/2330 train_time:38267ms step_avg:40.62ms
step:943/2330 train_time:38304ms step_avg:40.62ms
step:944/2330 train_time:38350ms step_avg:40.63ms
step:945/2330 train_time:38386ms step_avg:40.62ms
step:946/2330 train_time:38432ms step_avg:40.63ms
step:947/2330 train_time:38468ms step_avg:40.62ms
step:948/2330 train_time:38514ms step_avg:40.63ms
step:949/2330 train_time:38550ms step_avg:40.62ms
step:950/2330 train_time:38595ms step_avg:40.63ms
step:951/2330 train_time:38630ms step_avg:40.62ms
step:952/2330 train_time:38676ms step_avg:40.63ms
step:953/2330 train_time:38711ms step_avg:40.62ms
step:954/2330 train_time:38755ms step_avg:40.62ms
step:955/2330 train_time:38792ms step_avg:40.62ms
step:956/2330 train_time:38837ms step_avg:40.62ms
step:957/2330 train_time:38873ms step_avg:40.62ms
step:958/2330 train_time:38919ms step_avg:40.63ms
step:959/2330 train_time:38955ms step_avg:40.62ms
step:960/2330 train_time:39001ms step_avg:40.63ms
step:961/2330 train_time:39037ms step_avg:40.62ms
step:962/2330 train_time:39082ms step_avg:40.63ms
step:963/2330 train_time:39118ms step_avg:40.62ms
step:964/2330 train_time:39163ms step_avg:40.63ms
step:965/2330 train_time:39199ms step_avg:40.62ms
step:966/2330 train_time:39245ms step_avg:40.63ms
step:967/2330 train_time:39281ms step_avg:40.62ms
step:968/2330 train_time:39327ms step_avg:40.63ms
step:969/2330 train_time:39364ms step_avg:40.62ms
step:970/2330 train_time:39410ms step_avg:40.63ms
step:971/2330 train_time:39446ms step_avg:40.62ms
step:972/2330 train_time:39491ms step_avg:40.63ms
step:973/2330 train_time:39528ms step_avg:40.62ms
step:974/2330 train_time:39573ms step_avg:40.63ms
step:975/2330 train_time:39608ms step_avg:40.62ms
step:976/2330 train_time:39654ms step_avg:40.63ms
step:977/2330 train_time:39689ms step_avg:40.62ms
step:978/2330 train_time:39734ms step_avg:40.63ms
step:979/2330 train_time:39770ms step_avg:40.62ms
step:980/2330 train_time:39816ms step_avg:40.63ms
step:981/2330 train_time:39851ms step_avg:40.62ms
step:982/2330 train_time:39898ms step_avg:40.63ms
step:983/2330 train_time:39933ms step_avg:40.62ms
step:984/2330 train_time:39979ms step_avg:40.63ms
step:985/2330 train_time:40015ms step_avg:40.62ms
step:986/2330 train_time:40060ms step_avg:40.63ms
step:987/2330 train_time:40096ms step_avg:40.62ms
step:988/2330 train_time:40141ms step_avg:40.63ms
step:989/2330 train_time:40176ms step_avg:40.62ms
step:990/2330 train_time:40221ms step_avg:40.63ms
step:991/2330 train_time:40258ms step_avg:40.62ms
step:992/2330 train_time:40304ms step_avg:40.63ms
step:993/2330 train_time:40341ms step_avg:40.63ms
step:994/2330 train_time:40387ms step_avg:40.63ms
step:995/2330 train_time:40424ms step_avg:40.63ms
step:996/2330 train_time:40470ms step_avg:40.63ms
step:997/2330 train_time:40506ms step_avg:40.63ms
step:998/2330 train_time:40552ms step_avg:40.63ms
step:999/2330 train_time:40588ms step_avg:40.63ms
step:1000/2330 train_time:40633ms step_avg:40.63ms
step:1000/2330 val_loss:5.3187 train_time:40722ms step_avg:40.72ms
step:1001/2330 train_time:40735ms step_avg:40.69ms
step:1002/2330 train_time:40748ms step_avg:40.67ms
step:1003/2330 train_time:40759ms step_avg:40.64ms
step:1004/2330 train_time:40795ms step_avg:40.63ms
step:1005/2330 train_time:40829ms step_avg:40.63ms
step:1006/2330 train_time:40873ms step_avg:40.63ms
step:1007/2330 train_time:40909ms step_avg:40.62ms
step:1008/2330 train_time:40953ms step_avg:40.63ms
step:1009/2330 train_time:40987ms step_avg:40.62ms
step:1010/2330 train_time:41035ms step_avg:40.63ms
step:1011/2330 train_time:41076ms step_avg:40.63ms
step:1012/2330 train_time:41125ms step_avg:40.64ms
step:1013/2330 train_time:41163ms step_avg:40.63ms
step:1014/2330 train_time:41209ms step_avg:40.64ms
step:1015/2330 train_time:41245ms step_avg:40.64ms
step:1016/2330 train_time:41291ms step_avg:40.64ms
step:1017/2330 train_time:41326ms step_avg:40.64ms
step:1018/2330 train_time:41371ms step_avg:40.64ms
step:1019/2330 train_time:41407ms step_avg:40.64ms
step:1020/2330 train_time:41452ms step_avg:40.64ms
step:1021/2330 train_time:41487ms step_avg:40.63ms
step:1022/2330 train_time:41533ms step_avg:40.64ms
step:1023/2330 train_time:41568ms step_avg:40.63ms
step:1024/2330 train_time:41612ms step_avg:40.64ms
step:1025/2330 train_time:41648ms step_avg:40.63ms
step:1026/2330 train_time:41694ms step_avg:40.64ms
step:1027/2330 train_time:41731ms step_avg:40.63ms
step:1028/2330 train_time:41776ms step_avg:40.64ms
step:1029/2330 train_time:41811ms step_avg:40.63ms
step:1030/2330 train_time:41856ms step_avg:40.64ms
step:1031/2330 train_time:41892ms step_avg:40.63ms
step:1032/2330 train_time:41937ms step_avg:40.64ms
step:1033/2330 train_time:41973ms step_avg:40.63ms
step:1034/2330 train_time:42019ms step_avg:40.64ms
step:1035/2330 train_time:42057ms step_avg:40.63ms
step:1036/2330 train_time:42105ms step_avg:40.64ms
step:1037/2330 train_time:42141ms step_avg:40.64ms
step:1038/2330 train_time:42187ms step_avg:40.64ms
step:1039/2330 train_time:42222ms step_avg:40.64ms
step:1040/2330 train_time:42268ms step_avg:40.64ms
step:1041/2330 train_time:42304ms step_avg:40.64ms
step:1042/2330 train_time:42350ms step_avg:40.64ms
step:1043/2330 train_time:42386ms step_avg:40.64ms
step:1044/2330 train_time:42432ms step_avg:40.64ms
step:1045/2330 train_time:42467ms step_avg:40.64ms
step:1046/2330 train_time:42512ms step_avg:40.64ms
step:1047/2330 train_time:42548ms step_avg:40.64ms
step:1048/2330 train_time:42593ms step_avg:40.64ms
step:1049/2330 train_time:42628ms step_avg:40.64ms
step:1050/2330 train_time:42674ms step_avg:40.64ms
step:1051/2330 train_time:42710ms step_avg:40.64ms
step:1052/2330 train_time:42755ms step_avg:40.64ms
step:1053/2330 train_time:42790ms step_avg:40.64ms
step:1054/2330 train_time:42835ms step_avg:40.64ms
step:1055/2330 train_time:42870ms step_avg:40.64ms
step:1056/2330 train_time:42916ms step_avg:40.64ms
step:1057/2330 train_time:42951ms step_avg:40.63ms
step:1058/2330 train_time:42996ms step_avg:40.64ms
step:1059/2330 train_time:43032ms step_avg:40.64ms
step:1060/2330 train_time:43078ms step_avg:40.64ms
step:1061/2330 train_time:43115ms step_avg:40.64ms
step:1062/2330 train_time:43161ms step_avg:40.64ms
step:1063/2330 train_time:43198ms step_avg:40.64ms
step:1064/2330 train_time:43245ms step_avg:40.64ms
step:1065/2330 train_time:43281ms step_avg:40.64ms
step:1066/2330 train_time:43327ms step_avg:40.64ms
step:1067/2330 train_time:43364ms step_avg:40.64ms
step:1068/2330 train_time:43408ms step_avg:40.64ms
step:1069/2330 train_time:43444ms step_avg:40.64ms
step:1070/2330 train_time:43489ms step_avg:40.64ms
step:1071/2330 train_time:43524ms step_avg:40.64ms
step:1072/2330 train_time:43569ms step_avg:40.64ms
step:1073/2330 train_time:43605ms step_avg:40.64ms
step:1074/2330 train_time:43650ms step_avg:40.64ms
step:1075/2330 train_time:43685ms step_avg:40.64ms
step:1076/2330 train_time:43730ms step_avg:40.64ms
step:1077/2330 train_time:43765ms step_avg:40.64ms
step:1078/2330 train_time:43811ms step_avg:40.64ms
step:1079/2330 train_time:43846ms step_avg:40.64ms
step:1080/2330 train_time:43892ms step_avg:40.64ms
step:1081/2330 train_time:43929ms step_avg:40.64ms
step:1082/2330 train_time:43975ms step_avg:40.64ms
step:1083/2330 train_time:44011ms step_avg:40.64ms
step:1084/2330 train_time:44057ms step_avg:40.64ms
step:1085/2330 train_time:44093ms step_avg:40.64ms
step:1086/2330 train_time:44138ms step_avg:40.64ms
step:1087/2330 train_time:44174ms step_avg:40.64ms
step:1088/2330 train_time:44220ms step_avg:40.64ms
step:1089/2330 train_time:44256ms step_avg:40.64ms
step:1090/2330 train_time:44304ms step_avg:40.65ms
step:1091/2330 train_time:44340ms step_avg:40.64ms
step:1092/2330 train_time:44386ms step_avg:40.65ms
step:1093/2330 train_time:44421ms step_avg:40.64ms
step:1094/2330 train_time:44467ms step_avg:40.65ms
step:1095/2330 train_time:44503ms step_avg:40.64ms
step:1096/2330 train_time:44549ms step_avg:40.65ms
step:1097/2330 train_time:44584ms step_avg:40.64ms
step:1098/2330 train_time:44630ms step_avg:40.65ms
step:1099/2330 train_time:44665ms step_avg:40.64ms
step:1100/2330 train_time:44711ms step_avg:40.65ms
step:1101/2330 train_time:44747ms step_avg:40.64ms
step:1102/2330 train_time:44793ms step_avg:40.65ms
step:1103/2330 train_time:44829ms step_avg:40.64ms
step:1104/2330 train_time:44874ms step_avg:40.65ms
step:1105/2330 train_time:44910ms step_avg:40.64ms
step:1106/2330 train_time:44955ms step_avg:40.65ms
step:1107/2330 train_time:44991ms step_avg:40.64ms
step:1108/2330 train_time:45036ms step_avg:40.65ms
step:1109/2330 train_time:45072ms step_avg:40.64ms
step:1110/2330 train_time:45119ms step_avg:40.65ms
step:1111/2330 train_time:45155ms step_avg:40.64ms
step:1112/2330 train_time:45202ms step_avg:40.65ms
step:1113/2330 train_time:45238ms step_avg:40.65ms
step:1114/2330 train_time:45283ms step_avg:40.65ms
step:1115/2330 train_time:45319ms step_avg:40.64ms
step:1116/2330 train_time:45365ms step_avg:40.65ms
step:1117/2330 train_time:45401ms step_avg:40.65ms
step:1118/2330 train_time:45446ms step_avg:40.65ms
step:1119/2330 train_time:45482ms step_avg:40.65ms
step:1120/2330 train_time:45527ms step_avg:40.65ms
step:1121/2330 train_time:45564ms step_avg:40.65ms
step:1122/2330 train_time:45609ms step_avg:40.65ms
step:1123/2330 train_time:45645ms step_avg:40.65ms
step:1124/2330 train_time:45691ms step_avg:40.65ms
step:1125/2330 train_time:45726ms step_avg:40.65ms
step:1126/2330 train_time:45771ms step_avg:40.65ms
step:1127/2330 train_time:45806ms step_avg:40.64ms
step:1128/2330 train_time:45851ms step_avg:40.65ms
step:1129/2330 train_time:45887ms step_avg:40.64ms
step:1130/2330 train_time:45933ms step_avg:40.65ms
step:1131/2330 train_time:45968ms step_avg:40.64ms
step:1132/2330 train_time:46014ms step_avg:40.65ms
step:1133/2330 train_time:46050ms step_avg:40.64ms
step:1134/2330 train_time:46095ms step_avg:40.65ms
step:1135/2330 train_time:46132ms step_avg:40.64ms
step:1136/2330 train_time:46177ms step_avg:40.65ms
step:1137/2330 train_time:46214ms step_avg:40.65ms
step:1138/2330 train_time:46260ms step_avg:40.65ms
step:1139/2330 train_time:46296ms step_avg:40.65ms
step:1140/2330 train_time:46342ms step_avg:40.65ms
step:1141/2330 train_time:46377ms step_avg:40.65ms
step:1142/2330 train_time:46422ms step_avg:40.65ms
step:1143/2330 train_time:46458ms step_avg:40.65ms
step:1144/2330 train_time:46504ms step_avg:40.65ms
step:1145/2330 train_time:46540ms step_avg:40.65ms
step:1146/2330 train_time:46587ms step_avg:40.65ms
step:1147/2330 train_time:46622ms step_avg:40.65ms
step:1148/2330 train_time:46668ms step_avg:40.65ms
step:1149/2330 train_time:46704ms step_avg:40.65ms
step:1150/2330 train_time:46749ms step_avg:40.65ms
step:1151/2330 train_time:46784ms step_avg:40.65ms
step:1152/2330 train_time:46830ms step_avg:40.65ms
step:1153/2330 train_time:46865ms step_avg:40.65ms
step:1154/2330 train_time:46911ms step_avg:40.65ms
step:1155/2330 train_time:46947ms step_avg:40.65ms
step:1156/2330 train_time:46993ms step_avg:40.65ms
step:1157/2330 train_time:47029ms step_avg:40.65ms
step:1158/2330 train_time:47075ms step_avg:40.65ms
step:1159/2330 train_time:47110ms step_avg:40.65ms
step:1160/2330 train_time:47155ms step_avg:40.65ms
step:1161/2330 train_time:47191ms step_avg:40.65ms
step:1162/2330 train_time:47236ms step_avg:40.65ms
step:1163/2330 train_time:47273ms step_avg:40.65ms
step:1164/2330 train_time:47319ms step_avg:40.65ms
step:1165/2330 train_time:47355ms step_avg:40.65ms
step:1166/2330 train_time:47401ms step_avg:40.65ms
step:1167/2330 train_time:47438ms step_avg:40.65ms
step:1168/2330 train_time:47482ms step_avg:40.65ms
step:1169/2330 train_time:47518ms step_avg:40.65ms
step:1170/2330 train_time:47563ms step_avg:40.65ms
step:1171/2330 train_time:47599ms step_avg:40.65ms
step:1172/2330 train_time:47645ms step_avg:40.65ms
step:1173/2330 train_time:47681ms step_avg:40.65ms
step:1174/2330 train_time:47727ms step_avg:40.65ms
step:1175/2330 train_time:47764ms step_avg:40.65ms
step:1176/2330 train_time:47809ms step_avg:40.65ms
step:1177/2330 train_time:47845ms step_avg:40.65ms
step:1178/2330 train_time:47891ms step_avg:40.65ms
step:1179/2330 train_time:47927ms step_avg:40.65ms
step:1180/2330 train_time:47973ms step_avg:40.65ms
step:1181/2330 train_time:48009ms step_avg:40.65ms
step:1182/2330 train_time:48055ms step_avg:40.66ms
step:1183/2330 train_time:48090ms step_avg:40.65ms
step:1184/2330 train_time:48136ms step_avg:40.66ms
step:1185/2330 train_time:48172ms step_avg:40.65ms
step:1186/2330 train_time:48217ms step_avg:40.66ms
step:1187/2330 train_time:48252ms step_avg:40.65ms
step:1188/2330 train_time:48297ms step_avg:40.65ms
step:1189/2330 train_time:48333ms step_avg:40.65ms
step:1190/2330 train_time:48377ms step_avg:40.65ms
step:1191/2330 train_time:48414ms step_avg:40.65ms
step:1192/2330 train_time:48459ms step_avg:40.65ms
step:1193/2330 train_time:48496ms step_avg:40.65ms
step:1194/2330 train_time:48542ms step_avg:40.65ms
step:1195/2330 train_time:48578ms step_avg:40.65ms
step:1196/2330 train_time:48624ms step_avg:40.66ms
step:1197/2330 train_time:48660ms step_avg:40.65ms
step:1198/2330 train_time:48705ms step_avg:40.66ms
step:1199/2330 train_time:48741ms step_avg:40.65ms
step:1200/2330 train_time:48787ms step_avg:40.66ms
step:1201/2330 train_time:48823ms step_avg:40.65ms
step:1202/2330 train_time:48869ms step_avg:40.66ms
step:1203/2330 train_time:48905ms step_avg:40.65ms
step:1204/2330 train_time:48951ms step_avg:40.66ms
step:1205/2330 train_time:48987ms step_avg:40.65ms
step:1206/2330 train_time:49032ms step_avg:40.66ms
step:1207/2330 train_time:49068ms step_avg:40.65ms
step:1208/2330 train_time:49114ms step_avg:40.66ms
step:1209/2330 train_time:49150ms step_avg:40.65ms
step:1210/2330 train_time:49196ms step_avg:40.66ms
step:1211/2330 train_time:49232ms step_avg:40.65ms
step:1212/2330 train_time:49277ms step_avg:40.66ms
step:1213/2330 train_time:49313ms step_avg:40.65ms
step:1214/2330 train_time:49359ms step_avg:40.66ms
step:1215/2330 train_time:49395ms step_avg:40.65ms
step:1216/2330 train_time:49441ms step_avg:40.66ms
step:1217/2330 train_time:49477ms step_avg:40.66ms
step:1218/2330 train_time:49522ms step_avg:40.66ms
step:1219/2330 train_time:49558ms step_avg:40.65ms
step:1220/2330 train_time:49603ms step_avg:40.66ms
step:1221/2330 train_time:49639ms step_avg:40.65ms
step:1222/2330 train_time:49685ms step_avg:40.66ms
step:1223/2330 train_time:49721ms step_avg:40.65ms
step:1224/2330 train_time:49766ms step_avg:40.66ms
step:1225/2330 train_time:49802ms step_avg:40.65ms
step:1226/2330 train_time:49848ms step_avg:40.66ms
step:1227/2330 train_time:49885ms step_avg:40.66ms
step:1228/2330 train_time:49930ms step_avg:40.66ms
step:1229/2330 train_time:49966ms step_avg:40.66ms
step:1230/2330 train_time:50012ms step_avg:40.66ms
step:1231/2330 train_time:50047ms step_avg:40.66ms
step:1232/2330 train_time:50092ms step_avg:40.66ms
step:1233/2330 train_time:50128ms step_avg:40.66ms
step:1234/2330 train_time:50174ms step_avg:40.66ms
step:1235/2330 train_time:50210ms step_avg:40.66ms
step:1236/2330 train_time:50256ms step_avg:40.66ms
step:1237/2330 train_time:50291ms step_avg:40.66ms
step:1238/2330 train_time:50336ms step_avg:40.66ms
step:1239/2330 train_time:50371ms step_avg:40.65ms
step:1240/2330 train_time:50417ms step_avg:40.66ms
step:1241/2330 train_time:50453ms step_avg:40.66ms
step:1242/2330 train_time:50499ms step_avg:40.66ms
step:1243/2330 train_time:50535ms step_avg:40.66ms
step:1244/2330 train_time:50580ms step_avg:40.66ms
step:1245/2330 train_time:50616ms step_avg:40.66ms
step:1246/2330 train_time:50661ms step_avg:40.66ms
step:1247/2330 train_time:50697ms step_avg:40.66ms
step:1248/2330 train_time:50743ms step_avg:40.66ms
step:1249/2330 train_time:50779ms step_avg:40.66ms
step:1250/2330 train_time:50825ms step_avg:40.66ms
step:1250/2330 val_loss:5.2609 train_time:50915ms step_avg:40.73ms
step:1251/2330 train_time:50929ms step_avg:40.71ms
step:1252/2330 train_time:50941ms step_avg:40.69ms
step:1253/2330 train_time:50952ms step_avg:40.66ms
step:1254/2330 train_time:50989ms step_avg:40.66ms
step:1255/2330 train_time:51024ms step_avg:40.66ms
step:1256/2330 train_time:51068ms step_avg:40.66ms
step:1257/2330 train_time:51103ms step_avg:40.66ms
step:1258/2330 train_time:51148ms step_avg:40.66ms
step:1259/2330 train_time:51184ms step_avg:40.65ms
step:1260/2330 train_time:51231ms step_avg:40.66ms
step:1261/2330 train_time:51270ms step_avg:40.66ms
step:1262/2330 train_time:51317ms step_avg:40.66ms
step:1263/2330 train_time:51354ms step_avg:40.66ms
step:1264/2330 train_time:51401ms step_avg:40.66ms
step:1265/2330 train_time:51437ms step_avg:40.66ms
step:1266/2330 train_time:51483ms step_avg:40.67ms
step:1267/2330 train_time:51519ms step_avg:40.66ms
step:1268/2330 train_time:51564ms step_avg:40.67ms
step:1269/2330 train_time:51599ms step_avg:40.66ms
step:1270/2330 train_time:51644ms step_avg:40.66ms
step:1271/2330 train_time:51679ms step_avg:40.66ms
step:1272/2330 train_time:51724ms step_avg:40.66ms
step:1273/2330 train_time:51759ms step_avg:40.66ms
step:1274/2330 train_time:51804ms step_avg:40.66ms
step:1275/2330 train_time:51840ms step_avg:40.66ms
step:1276/2330 train_time:51886ms step_avg:40.66ms
step:1277/2330 train_time:51922ms step_avg:40.66ms
step:1278/2330 train_time:51967ms step_avg:40.66ms
step:1279/2330 train_time:52003ms step_avg:40.66ms
step:1280/2330 train_time:52048ms step_avg:40.66ms
step:1281/2330 train_time:52082ms step_avg:40.66ms
step:1282/2330 train_time:52127ms step_avg:40.66ms
step:1283/2330 train_time:52164ms step_avg:40.66ms
step:1284/2330 train_time:52210ms step_avg:40.66ms
step:1285/2330 train_time:52246ms step_avg:40.66ms
step:1286/2330 train_time:52293ms step_avg:40.66ms
step:1287/2330 train_time:52330ms step_avg:40.66ms
step:1288/2330 train_time:52376ms step_avg:40.66ms
step:1289/2330 train_time:52412ms step_avg:40.66ms
step:1290/2330 train_time:52457ms step_avg:40.66ms
step:1291/2330 train_time:52493ms step_avg:40.66ms
step:1292/2330 train_time:52538ms step_avg:40.66ms
step:1293/2330 train_time:52574ms step_avg:40.66ms
step:1294/2330 train_time:52619ms step_avg:40.66ms
step:1295/2330 train_time:52655ms step_avg:40.66ms
step:1296/2330 train_time:52700ms step_avg:40.66ms
step:1297/2330 train_time:52735ms step_avg:40.66ms
step:1298/2330 train_time:52781ms step_avg:40.66ms
step:1299/2330 train_time:52817ms step_avg:40.66ms
step:1300/2330 train_time:52864ms step_avg:40.66ms
step:1301/2330 train_time:52899ms step_avg:40.66ms
step:1302/2330 train_time:52944ms step_avg:40.66ms
step:1303/2330 train_time:52979ms step_avg:40.66ms
step:1304/2330 train_time:53025ms step_avg:40.66ms
step:1305/2330 train_time:53061ms step_avg:40.66ms
step:1306/2330 train_time:53107ms step_avg:40.66ms
step:1307/2330 train_time:53142ms step_avg:40.66ms
step:1308/2330 train_time:53188ms step_avg:40.66ms
step:1309/2330 train_time:53226ms step_avg:40.66ms
step:1310/2330 train_time:53271ms step_avg:40.66ms
step:1311/2330 train_time:53307ms step_avg:40.66ms
step:1312/2330 train_time:53354ms step_avg:40.67ms
step:1313/2330 train_time:53390ms step_avg:40.66ms
step:1314/2330 train_time:53436ms step_avg:40.67ms
step:1315/2330 train_time:53472ms step_avg:40.66ms
step:1316/2330 train_time:53517ms step_avg:40.67ms
step:1317/2330 train_time:53552ms step_avg:40.66ms
step:1318/2330 train_time:53597ms step_avg:40.67ms
step:1319/2330 train_time:53632ms step_avg:40.66ms
step:1320/2330 train_time:53677ms step_avg:40.66ms
step:1321/2330 train_time:53713ms step_avg:40.66ms
step:1322/2330 train_time:53759ms step_avg:40.66ms
step:1323/2330 train_time:53795ms step_avg:40.66ms
step:1324/2330 train_time:53841ms step_avg:40.67ms
step:1325/2330 train_time:53878ms step_avg:40.66ms
step:1326/2330 train_time:53923ms step_avg:40.67ms
step:1327/2330 train_time:53960ms step_avg:40.66ms
step:1328/2330 train_time:54005ms step_avg:40.67ms
step:1329/2330 train_time:54039ms step_avg:40.66ms
step:1330/2330 train_time:54084ms step_avg:40.66ms
step:1331/2330 train_time:54120ms step_avg:40.66ms
step:1332/2330 train_time:54167ms step_avg:40.67ms
step:1333/2330 train_time:54203ms step_avg:40.66ms
step:1334/2330 train_time:54250ms step_avg:40.67ms
step:1335/2330 train_time:54285ms step_avg:40.66ms
step:1336/2330 train_time:54331ms step_avg:40.67ms
step:1337/2330 train_time:54368ms step_avg:40.66ms
step:1338/2330 train_time:54413ms step_avg:40.67ms
step:1339/2330 train_time:54449ms step_avg:40.66ms
step:1340/2330 train_time:54495ms step_avg:40.67ms
step:1341/2330 train_time:54530ms step_avg:40.66ms
step:1342/2330 train_time:54575ms step_avg:40.67ms
step:1343/2330 train_time:54611ms step_avg:40.66ms
step:1344/2330 train_time:54655ms step_avg:40.67ms
step:1345/2330 train_time:54691ms step_avg:40.66ms
step:1346/2330 train_time:54737ms step_avg:40.67ms
step:1347/2330 train_time:54773ms step_avg:40.66ms
step:1348/2330 train_time:54817ms step_avg:40.67ms
step:1349/2330 train_time:54853ms step_avg:40.66ms
step:1350/2330 train_time:54899ms step_avg:40.67ms
step:1351/2330 train_time:54936ms step_avg:40.66ms
step:1352/2330 train_time:54981ms step_avg:40.67ms
step:1353/2330 train_time:55018ms step_avg:40.66ms
step:1354/2330 train_time:55064ms step_avg:40.67ms
step:1355/2330 train_time:55101ms step_avg:40.66ms
step:1356/2330 train_time:55147ms step_avg:40.67ms
step:1357/2330 train_time:55183ms step_avg:40.67ms
step:1358/2330 train_time:55230ms step_avg:40.67ms
step:1359/2330 train_time:55265ms step_avg:40.67ms
step:1360/2330 train_time:55311ms step_avg:40.67ms
step:1361/2330 train_time:55346ms step_avg:40.67ms
step:1362/2330 train_time:55393ms step_avg:40.67ms
step:1363/2330 train_time:55429ms step_avg:40.67ms
step:1364/2330 train_time:55474ms step_avg:40.67ms
step:1365/2330 train_time:55510ms step_avg:40.67ms
step:1366/2330 train_time:55555ms step_avg:40.67ms
step:1367/2330 train_time:55590ms step_avg:40.67ms
step:1368/2330 train_time:55636ms step_avg:40.67ms
step:1369/2330 train_time:55672ms step_avg:40.67ms
step:1370/2330 train_time:55717ms step_avg:40.67ms
step:1371/2330 train_time:55753ms step_avg:40.67ms
step:1372/2330 train_time:55798ms step_avg:40.67ms
step:1373/2330 train_time:55833ms step_avg:40.67ms
step:1374/2330 train_time:55878ms step_avg:40.67ms
step:1375/2330 train_time:55914ms step_avg:40.66ms
step:1376/2330 train_time:55960ms step_avg:40.67ms
step:1377/2330 train_time:55997ms step_avg:40.67ms
step:1378/2330 train_time:56043ms step_avg:40.67ms
step:1379/2330 train_time:56079ms step_avg:40.67ms
step:1380/2330 train_time:56126ms step_avg:40.67ms
step:1381/2330 train_time:56163ms step_avg:40.67ms
step:1382/2330 train_time:56208ms step_avg:40.67ms
step:1383/2330 train_time:56244ms step_avg:40.67ms
step:1384/2330 train_time:56290ms step_avg:40.67ms
step:1385/2330 train_time:56326ms step_avg:40.67ms
step:1386/2330 train_time:56371ms step_avg:40.67ms
step:1387/2330 train_time:56406ms step_avg:40.67ms
step:1388/2330 train_time:56452ms step_avg:40.67ms
step:1389/2330 train_time:56488ms step_avg:40.67ms
step:1390/2330 train_time:56533ms step_avg:40.67ms
step:1391/2330 train_time:56570ms step_avg:40.67ms
step:1392/2330 train_time:56615ms step_avg:40.67ms
step:1393/2330 train_time:56651ms step_avg:40.67ms
step:1394/2330 train_time:56696ms step_avg:40.67ms
step:1395/2330 train_time:56732ms step_avg:40.67ms
step:1396/2330 train_time:56777ms step_avg:40.67ms
step:1397/2330 train_time:56812ms step_avg:40.67ms
step:1398/2330 train_time:56857ms step_avg:40.67ms
step:1399/2330 train_time:56893ms step_avg:40.67ms
step:1400/2330 train_time:56938ms step_avg:40.67ms
step:1401/2330 train_time:56975ms step_avg:40.67ms
step:1402/2330 train_time:57021ms step_avg:40.67ms
step:1403/2330 train_time:57058ms step_avg:40.67ms
step:1404/2330 train_time:57103ms step_avg:40.67ms
step:1405/2330 train_time:57140ms step_avg:40.67ms
step:1406/2330 train_time:57185ms step_avg:40.67ms
step:1407/2330 train_time:57222ms step_avg:40.67ms
step:1408/2330 train_time:57267ms step_avg:40.67ms
step:1409/2330 train_time:57303ms step_avg:40.67ms
step:1410/2330 train_time:57348ms step_avg:40.67ms
step:1411/2330 train_time:57384ms step_avg:40.67ms
step:1412/2330 train_time:57430ms step_avg:40.67ms
step:1413/2330 train_time:57465ms step_avg:40.67ms
step:1414/2330 train_time:57510ms step_avg:40.67ms
step:1415/2330 train_time:57546ms step_avg:40.67ms
step:1416/2330 train_time:57592ms step_avg:40.67ms
step:1417/2330 train_time:57627ms step_avg:40.67ms
step:1418/2330 train_time:57673ms step_avg:40.67ms
step:1419/2330 train_time:57709ms step_avg:40.67ms
step:1420/2330 train_time:57754ms step_avg:40.67ms
step:1421/2330 train_time:57790ms step_avg:40.67ms
step:1422/2330 train_time:57836ms step_avg:40.67ms
step:1423/2330 train_time:57872ms step_avg:40.67ms
step:1424/2330 train_time:57917ms step_avg:40.67ms
step:1425/2330 train_time:57953ms step_avg:40.67ms
step:1426/2330 train_time:57999ms step_avg:40.67ms
step:1427/2330 train_time:58035ms step_avg:40.67ms
step:1428/2330 train_time:58081ms step_avg:40.67ms
step:1429/2330 train_time:58118ms step_avg:40.67ms
step:1430/2330 train_time:58164ms step_avg:40.67ms
step:1431/2330 train_time:58200ms step_avg:40.67ms
step:1432/2330 train_time:58245ms step_avg:40.67ms
step:1433/2330 train_time:58281ms step_avg:40.67ms
step:1434/2330 train_time:58328ms step_avg:40.67ms
step:1435/2330 train_time:58363ms step_avg:40.67ms
step:1436/2330 train_time:58408ms step_avg:40.67ms
step:1437/2330 train_time:58444ms step_avg:40.67ms
step:1438/2330 train_time:58489ms step_avg:40.67ms
step:1439/2330 train_time:58525ms step_avg:40.67ms
step:1440/2330 train_time:58570ms step_avg:40.67ms
step:1441/2330 train_time:58607ms step_avg:40.67ms
step:1442/2330 train_time:58652ms step_avg:40.67ms
step:1443/2330 train_time:58687ms step_avg:40.67ms
step:1444/2330 train_time:58734ms step_avg:40.67ms
step:1445/2330 train_time:58771ms step_avg:40.67ms
step:1446/2330 train_time:58815ms step_avg:40.67ms
step:1447/2330 train_time:58851ms step_avg:40.67ms
step:1448/2330 train_time:58896ms step_avg:40.67ms
step:1449/2330 train_time:58931ms step_avg:40.67ms
step:1450/2330 train_time:58976ms step_avg:40.67ms
step:1451/2330 train_time:59012ms step_avg:40.67ms
step:1452/2330 train_time:59058ms step_avg:40.67ms
step:1453/2330 train_time:59094ms step_avg:40.67ms
step:1454/2330 train_time:59140ms step_avg:40.67ms
step:1455/2330 train_time:59176ms step_avg:40.67ms
step:1456/2330 train_time:59223ms step_avg:40.68ms
step:1457/2330 train_time:59259ms step_avg:40.67ms
step:1458/2330 train_time:59305ms step_avg:40.68ms
step:1459/2330 train_time:59341ms step_avg:40.67ms
step:1460/2330 train_time:59386ms step_avg:40.68ms
step:1461/2330 train_time:59423ms step_avg:40.67ms
step:1462/2330 train_time:59468ms step_avg:40.68ms
step:1463/2330 train_time:59503ms step_avg:40.67ms
step:1464/2330 train_time:59549ms step_avg:40.68ms
step:1465/2330 train_time:59585ms step_avg:40.67ms
step:1466/2330 train_time:59631ms step_avg:40.68ms
step:1467/2330 train_time:59667ms step_avg:40.67ms
step:1468/2330 train_time:59713ms step_avg:40.68ms
step:1469/2330 train_time:59749ms step_avg:40.67ms
step:1470/2330 train_time:59795ms step_avg:40.68ms
step:1471/2330 train_time:59830ms step_avg:40.67ms
step:1472/2330 train_time:59875ms step_avg:40.68ms
step:1473/2330 train_time:59911ms step_avg:40.67ms
step:1474/2330 train_time:59956ms step_avg:40.68ms
step:1475/2330 train_time:59992ms step_avg:40.67ms
step:1476/2330 train_time:60037ms step_avg:40.68ms
step:1477/2330 train_time:60073ms step_avg:40.67ms
step:1478/2330 train_time:60118ms step_avg:40.68ms
step:1479/2330 train_time:60154ms step_avg:40.67ms
step:1480/2330 train_time:60200ms step_avg:40.68ms
step:1481/2330 train_time:60236ms step_avg:40.67ms
step:1482/2330 train_time:60283ms step_avg:40.68ms
step:1483/2330 train_time:60319ms step_avg:40.67ms
step:1484/2330 train_time:60365ms step_avg:40.68ms
step:1485/2330 train_time:60400ms step_avg:40.67ms
step:1486/2330 train_time:60446ms step_avg:40.68ms
step:1487/2330 train_time:60481ms step_avg:40.67ms
step:1488/2330 train_time:60527ms step_avg:40.68ms
step:1489/2330 train_time:60564ms step_avg:40.67ms
step:1490/2330 train_time:60610ms step_avg:40.68ms
step:1491/2330 train_time:60645ms step_avg:40.67ms
step:1492/2330 train_time:60690ms step_avg:40.68ms
step:1493/2330 train_time:60727ms step_avg:40.67ms
step:1494/2330 train_time:60772ms step_avg:40.68ms
step:1495/2330 train_time:60808ms step_avg:40.67ms
step:1496/2330 train_time:60854ms step_avg:40.68ms
step:1497/2330 train_time:60889ms step_avg:40.67ms
step:1498/2330 train_time:60935ms step_avg:40.68ms
step:1499/2330 train_time:60972ms step_avg:40.68ms
step:1500/2330 train_time:61016ms step_avg:40.68ms
step:1500/2330 val_loss:5.2882 train_time:61106ms step_avg:40.74ms
step:1501/2330 train_time:61119ms step_avg:40.72ms
step:1502/2330 train_time:61132ms step_avg:40.70ms
step:1503/2330 train_time:61143ms step_avg:40.68ms
step:1504/2330 train_time:61180ms step_avg:40.68ms
step:1505/2330 train_time:61214ms step_avg:40.67ms
step:1506/2330 train_time:61258ms step_avg:40.68ms
step:1507/2330 train_time:61293ms step_avg:40.67ms
step:1508/2330 train_time:61337ms step_avg:40.67ms
step:1509/2330 train_time:61372ms step_avg:40.67ms
step:1510/2330 train_time:61420ms step_avg:40.68ms
step:1511/2330 train_time:61459ms step_avg:40.67ms
step:1512/2330 train_time:61509ms step_avg:40.68ms
step:1513/2330 train_time:61546ms step_avg:40.68ms
step:1514/2330 train_time:61593ms step_avg:40.68ms
step:1515/2330 train_time:61629ms step_avg:40.68ms
step:1516/2330 train_time:61674ms step_avg:40.68ms
step:1517/2330 train_time:61710ms step_avg:40.68ms
step:1518/2330 train_time:61755ms step_avg:40.68ms
step:1519/2330 train_time:61790ms step_avg:40.68ms
step:1520/2330 train_time:62030ms step_avg:40.81ms
step:1521/2330 train_time:62041ms step_avg:40.79ms
step:1522/2330 train_time:62053ms step_avg:40.77ms
step:1523/2330 train_time:62079ms step_avg:40.76ms
step:1524/2330 train_time:62122ms step_avg:40.76ms
step:1525/2330 train_time:62156ms step_avg:40.76ms
step:1526/2330 train_time:62200ms step_avg:40.76ms
step:1527/2330 train_time:62234ms step_avg:40.76ms
step:1528/2330 train_time:62279ms step_avg:40.76ms
step:1529/2330 train_time:62315ms step_avg:40.76ms
step:1530/2330 train_time:62360ms step_avg:40.76ms
step:1531/2330 train_time:62513ms step_avg:40.83ms
step:1532/2330 train_time:62556ms step_avg:40.83ms
step:1533/2330 train_time:62591ms step_avg:40.83ms
step:1534/2330 train_time:62635ms step_avg:40.83ms
step:1535/2330 train_time:62670ms step_avg:40.83ms
step:1536/2330 train_time:62715ms step_avg:40.83ms
step:1537/2330 train_time:62750ms step_avg:40.83ms
step:1538/2330 train_time:62795ms step_avg:40.83ms
step:1539/2330 train_time:62830ms step_avg:40.83ms
step:1540/2330 train_time:62874ms step_avg:40.83ms
step:1541/2330 train_time:62909ms step_avg:40.82ms
step:1542/2330 train_time:62953ms step_avg:40.83ms
step:1543/2330 train_time:62989ms step_avg:40.82ms
step:1544/2330 train_time:63033ms step_avg:40.82ms
step:1545/2330 train_time:63068ms step_avg:40.82ms
step:1546/2330 train_time:63112ms step_avg:40.82ms
step:1547/2330 train_time:63148ms step_avg:40.82ms
step:1548/2330 train_time:63193ms step_avg:40.82ms
step:1549/2330 train_time:63228ms step_avg:40.82ms
step:1550/2330 train_time:63272ms step_avg:40.82ms
step:1551/2330 train_time:63308ms step_avg:40.82ms
step:1552/2330 train_time:63355ms step_avg:40.82ms
step:1553/2330 train_time:63397ms step_avg:40.82ms
step:1554/2330 train_time:63448ms step_avg:40.83ms
step:1555/2330 train_time:63488ms step_avg:40.83ms
step:1556/2330 train_time:63535ms step_avg:40.83ms
step:1557/2330 train_time:63571ms step_avg:40.83ms
step:1558/2330 train_time:63617ms step_avg:40.83ms
step:1559/2330 train_time:63652ms step_avg:40.83ms
step:1560/2330 train_time:63697ms step_avg:40.83ms
step:1561/2330 train_time:63732ms step_avg:40.83ms
step:1562/2330 train_time:63777ms step_avg:40.83ms
step:1563/2330 train_time:63814ms step_avg:40.83ms
step:1564/2330 train_time:63859ms step_avg:40.83ms
step:1565/2330 train_time:63894ms step_avg:40.83ms
step:1566/2330 train_time:63939ms step_avg:40.83ms
step:1567/2330 train_time:63975ms step_avg:40.83ms
step:1568/2330 train_time:64020ms step_avg:40.83ms
step:1569/2330 train_time:64055ms step_avg:40.83ms
step:1570/2330 train_time:64100ms step_avg:40.83ms
step:1571/2330 train_time:64136ms step_avg:40.83ms
step:1572/2330 train_time:64182ms step_avg:40.83ms
step:1573/2330 train_time:64218ms step_avg:40.82ms
step:1574/2330 train_time:64263ms step_avg:40.83ms
step:1575/2330 train_time:64300ms step_avg:40.83ms
step:1576/2330 train_time:64345ms step_avg:40.83ms
step:1577/2330 train_time:64384ms step_avg:40.83ms
step:1578/2330 train_time:64431ms step_avg:40.83ms
step:1579/2330 train_time:64468ms step_avg:40.83ms
step:1580/2330 train_time:64516ms step_avg:40.83ms
step:1581/2330 train_time:64553ms step_avg:40.83ms
step:1582/2330 train_time:64599ms step_avg:40.83ms
step:1583/2330 train_time:64635ms step_avg:40.83ms
step:1584/2330 train_time:64680ms step_avg:40.83ms
step:1585/2330 train_time:64717ms step_avg:40.83ms
step:1586/2330 train_time:64763ms step_avg:40.83ms
step:1587/2330 train_time:64799ms step_avg:40.83ms
step:1588/2330 train_time:64844ms step_avg:40.83ms
step:1589/2330 train_time:64880ms step_avg:40.83ms
step:1590/2330 train_time:64926ms step_avg:40.83ms
step:1591/2330 train_time:64961ms step_avg:40.83ms
step:1592/2330 train_time:65006ms step_avg:40.83ms
step:1593/2330 train_time:65041ms step_avg:40.83ms
step:1594/2330 train_time:65086ms step_avg:40.83ms
step:1595/2330 train_time:65120ms step_avg:40.83ms
step:1596/2330 train_time:65166ms step_avg:40.83ms
step:1597/2330 train_time:65201ms step_avg:40.83ms
step:1598/2330 train_time:65246ms step_avg:40.83ms
step:1599/2330 train_time:65282ms step_avg:40.83ms
step:1600/2330 train_time:65328ms step_avg:40.83ms
step:1601/2330 train_time:65365ms step_avg:40.83ms
step:1602/2330 train_time:65411ms step_avg:40.83ms
step:1603/2330 train_time:65447ms step_avg:40.83ms
step:1604/2330 train_time:65494ms step_avg:40.83ms
step:1605/2330 train_time:65530ms step_avg:40.83ms
step:1606/2330 train_time:65576ms step_avg:40.83ms
step:1607/2330 train_time:65612ms step_avg:40.83ms
step:1608/2330 train_time:65658ms step_avg:40.83ms
step:1609/2330 train_time:65693ms step_avg:40.83ms
step:1610/2330 train_time:65739ms step_avg:40.83ms
step:1611/2330 train_time:65774ms step_avg:40.83ms
step:1612/2330 train_time:65819ms step_avg:40.83ms
step:1613/2330 train_time:65855ms step_avg:40.83ms
step:1614/2330 train_time:65900ms step_avg:40.83ms
step:1615/2330 train_time:65936ms step_avg:40.83ms
step:1616/2330 train_time:65982ms step_avg:40.83ms
step:1617/2330 train_time:66018ms step_avg:40.83ms
step:1618/2330 train_time:66063ms step_avg:40.83ms
step:1619/2330 train_time:66100ms step_avg:40.83ms
step:1620/2330 train_time:66144ms step_avg:40.83ms
step:1621/2330 train_time:66181ms step_avg:40.83ms
step:1622/2330 train_time:66226ms step_avg:40.83ms
step:1623/2330 train_time:66263ms step_avg:40.83ms
step:1624/2330 train_time:66308ms step_avg:40.83ms
step:1625/2330 train_time:66344ms step_avg:40.83ms
step:1626/2330 train_time:66391ms step_avg:40.83ms
step:1627/2330 train_time:66427ms step_avg:40.83ms
step:1628/2330 train_time:66473ms step_avg:40.83ms
step:1629/2330 train_time:66509ms step_avg:40.83ms
step:1630/2330 train_time:66556ms step_avg:40.83ms
step:1631/2330 train_time:66591ms step_avg:40.83ms
step:1632/2330 train_time:66638ms step_avg:40.83ms
step:1633/2330 train_time:66674ms step_avg:40.83ms
step:1634/2330 train_time:66719ms step_avg:40.83ms
step:1635/2330 train_time:66755ms step_avg:40.83ms
step:1636/2330 train_time:66800ms step_avg:40.83ms
step:1637/2330 train_time:66836ms step_avg:40.83ms
step:1638/2330 train_time:66881ms step_avg:40.83ms
step:1639/2330 train_time:66917ms step_avg:40.83ms
step:1640/2330 train_time:66963ms step_avg:40.83ms
step:1641/2330 train_time:66999ms step_avg:40.83ms
step:1642/2330 train_time:67045ms step_avg:40.83ms
step:1643/2330 train_time:67081ms step_avg:40.83ms
step:1644/2330 train_time:67126ms step_avg:40.83ms
step:1645/2330 train_time:67162ms step_avg:40.83ms
step:1646/2330 train_time:67208ms step_avg:40.83ms
step:1647/2330 train_time:67243ms step_avg:40.83ms
step:1648/2330 train_time:67288ms step_avg:40.83ms
step:1649/2330 train_time:67324ms step_avg:40.83ms
step:1650/2330 train_time:67370ms step_avg:40.83ms
step:1651/2330 train_time:67407ms step_avg:40.83ms
step:1652/2330 train_time:67453ms step_avg:40.83ms
step:1653/2330 train_time:67489ms step_avg:40.83ms
step:1654/2330 train_time:67535ms step_avg:40.83ms
step:1655/2330 train_time:67571ms step_avg:40.83ms
step:1656/2330 train_time:67616ms step_avg:40.83ms
step:1657/2330 train_time:67651ms step_avg:40.83ms
step:1658/2330 train_time:67697ms step_avg:40.83ms
step:1659/2330 train_time:67733ms step_avg:40.83ms
step:1660/2330 train_time:67778ms step_avg:40.83ms
step:1661/2330 train_time:67814ms step_avg:40.83ms
step:1662/2330 train_time:67859ms step_avg:40.83ms
step:1663/2330 train_time:67894ms step_avg:40.83ms
step:1664/2330 train_time:67940ms step_avg:40.83ms
step:1665/2330 train_time:67976ms step_avg:40.83ms
step:1666/2330 train_time:68021ms step_avg:40.83ms
step:1667/2330 train_time:68056ms step_avg:40.83ms
step:1668/2330 train_time:68103ms step_avg:40.83ms
step:1669/2330 train_time:68138ms step_avg:40.83ms
step:1670/2330 train_time:68184ms step_avg:40.83ms
step:1671/2330 train_time:68221ms step_avg:40.83ms
step:1672/2330 train_time:68266ms step_avg:40.83ms
step:1673/2330 train_time:68302ms step_avg:40.83ms
step:1674/2330 train_time:68348ms step_avg:40.83ms
step:1675/2330 train_time:68384ms step_avg:40.83ms
step:1676/2330 train_time:68430ms step_avg:40.83ms
step:1677/2330 train_time:68466ms step_avg:40.83ms
step:1678/2330 train_time:68512ms step_avg:40.83ms
step:1679/2330 train_time:68547ms step_avg:40.83ms
step:1680/2330 train_time:68593ms step_avg:40.83ms
step:1681/2330 train_time:68628ms step_avg:40.83ms
step:1682/2330 train_time:68674ms step_avg:40.83ms
step:1683/2330 train_time:68710ms step_avg:40.83ms
step:1684/2330 train_time:68756ms step_avg:40.83ms
step:1685/2330 train_time:68792ms step_avg:40.83ms
step:1686/2330 train_time:68837ms step_avg:40.83ms
step:1687/2330 train_time:68872ms step_avg:40.83ms
step:1688/2330 train_time:68918ms step_avg:40.83ms
step:1689/2330 train_time:68955ms step_avg:40.83ms
step:1690/2330 train_time:68999ms step_avg:40.83ms
step:1691/2330 train_time:69036ms step_avg:40.83ms
step:1692/2330 train_time:69081ms step_avg:40.83ms
step:1693/2330 train_time:69118ms step_avg:40.83ms
step:1694/2330 train_time:69164ms step_avg:40.83ms
step:1695/2330 train_time:69200ms step_avg:40.83ms
step:1696/2330 train_time:69245ms step_avg:40.83ms
step:1697/2330 train_time:69281ms step_avg:40.83ms
step:1698/2330 train_time:69327ms step_avg:40.83ms
step:1699/2330 train_time:69362ms step_avg:40.83ms
step:1700/2330 train_time:69407ms step_avg:40.83ms
step:1701/2330 train_time:69444ms step_avg:40.83ms
step:1702/2330 train_time:69490ms step_avg:40.83ms
step:1703/2330 train_time:69528ms step_avg:40.83ms
step:1704/2330 train_time:69572ms step_avg:40.83ms
step:1705/2330 train_time:69609ms step_avg:40.83ms
step:1706/2330 train_time:69654ms step_avg:40.83ms
step:1707/2330 train_time:69689ms step_avg:40.83ms
step:1708/2330 train_time:69735ms step_avg:40.83ms
step:1709/2330 train_time:69771ms step_avg:40.83ms
step:1710/2330 train_time:69816ms step_avg:40.83ms
step:1711/2330 train_time:69851ms step_avg:40.82ms
step:1712/2330 train_time:69897ms step_avg:40.83ms
step:1713/2330 train_time:69933ms step_avg:40.82ms
step:1714/2330 train_time:69978ms step_avg:40.83ms
step:1715/2330 train_time:70014ms step_avg:40.82ms
step:1716/2330 train_time:70059ms step_avg:40.83ms
step:1717/2330 train_time:70095ms step_avg:40.82ms
step:1718/2330 train_time:70141ms step_avg:40.83ms
step:1719/2330 train_time:70177ms step_avg:40.82ms
step:1720/2330 train_time:70224ms step_avg:40.83ms
step:1721/2330 train_time:70260ms step_avg:40.83ms
step:1722/2330 train_time:70306ms step_avg:40.83ms
step:1723/2330 train_time:70343ms step_avg:40.83ms
step:1724/2330 train_time:70388ms step_avg:40.83ms
step:1725/2330 train_time:70424ms step_avg:40.83ms
step:1726/2330 train_time:70470ms step_avg:40.83ms
step:1727/2330 train_time:70506ms step_avg:40.83ms
step:1728/2330 train_time:70552ms step_avg:40.83ms
step:1729/2330 train_time:70587ms step_avg:40.83ms
step:1730/2330 train_time:70632ms step_avg:40.83ms
step:1731/2330 train_time:70667ms step_avg:40.82ms
step:1732/2330 train_time:70713ms step_avg:40.83ms
step:1733/2330 train_time:70749ms step_avg:40.82ms
step:1734/2330 train_time:70795ms step_avg:40.83ms
step:1735/2330 train_time:70831ms step_avg:40.82ms
step:1736/2330 train_time:70877ms step_avg:40.83ms
step:1737/2330 train_time:70912ms step_avg:40.82ms
step:1738/2330 train_time:70957ms step_avg:40.83ms
step:1739/2330 train_time:70993ms step_avg:40.82ms
step:1740/2330 train_time:71038ms step_avg:40.83ms
step:1741/2330 train_time:71074ms step_avg:40.82ms
step:1742/2330 train_time:71120ms step_avg:40.83ms
step:1743/2330 train_time:71156ms step_avg:40.82ms
step:1744/2330 train_time:71202ms step_avg:40.83ms
step:1745/2330 train_time:71238ms step_avg:40.82ms
step:1746/2330 train_time:71284ms step_avg:40.83ms
step:1747/2330 train_time:71321ms step_avg:40.82ms
step:1748/2330 train_time:71367ms step_avg:40.83ms
step:1749/2330 train_time:71402ms step_avg:40.82ms
step:1750/2330 train_time:71448ms step_avg:40.83ms
step:1750/2330 val_loss:5.1746 train_time:71538ms step_avg:40.88ms
step:1751/2330 train_time:71552ms step_avg:40.86ms
step:1752/2330 train_time:71565ms step_avg:40.85ms
step:1753/2330 train_time:71576ms step_avg:40.83ms
step:1754/2330 train_time:71611ms step_avg:40.83ms
step:1755/2330 train_time:71646ms step_avg:40.82ms
step:1756/2330 train_time:71690ms step_avg:40.83ms
step:1757/2330 train_time:71725ms step_avg:40.82ms
step:1758/2330 train_time:71769ms step_avg:40.82ms
step:1759/2330 train_time:71804ms step_avg:40.82ms
step:1760/2330 train_time:71849ms step_avg:40.82ms
step:1761/2330 train_time:71889ms step_avg:40.82ms
step:1762/2330 train_time:71938ms step_avg:40.83ms
step:1763/2330 train_time:71975ms step_avg:40.83ms
step:1764/2330 train_time:72020ms step_avg:40.83ms
step:1765/2330 train_time:72056ms step_avg:40.82ms
step:1766/2330 train_time:72100ms step_avg:40.83ms
step:1767/2330 train_time:72137ms step_avg:40.82ms
step:1768/2330 train_time:72181ms step_avg:40.83ms
step:1769/2330 train_time:72216ms step_avg:40.82ms
step:1770/2330 train_time:72260ms step_avg:40.83ms
step:1771/2330 train_time:72295ms step_avg:40.82ms
step:1772/2330 train_time:72339ms step_avg:40.82ms
step:1773/2330 train_time:72374ms step_avg:40.82ms
step:1774/2330 train_time:72419ms step_avg:40.82ms
step:1775/2330 train_time:72461ms step_avg:40.82ms
step:1776/2330 train_time:72512ms step_avg:40.83ms
step:1777/2330 train_time:72549ms step_avg:40.83ms
step:1778/2330 train_time:72595ms step_avg:40.83ms
step:1779/2330 train_time:72631ms step_avg:40.83ms
step:1780/2330 train_time:72677ms step_avg:40.83ms
step:1781/2330 train_time:72713ms step_avg:40.83ms
step:1782/2330 train_time:72759ms step_avg:40.83ms
step:1783/2330 train_time:72795ms step_avg:40.83ms
step:1784/2330 train_time:72843ms step_avg:40.83ms
step:1785/2330 train_time:72879ms step_avg:40.83ms
step:1786/2330 train_time:72923ms step_avg:40.83ms
step:1787/2330 train_time:72958ms step_avg:40.83ms
step:1788/2330 train_time:73003ms step_avg:40.83ms
step:1789/2330 train_time:73038ms step_avg:40.83ms
step:1790/2330 train_time:73083ms step_avg:40.83ms
step:1791/2330 train_time:73118ms step_avg:40.83ms
step:1792/2330 train_time:73162ms step_avg:40.83ms
step:1793/2330 train_time:73197ms step_avg:40.82ms
step:1794/2330 train_time:73242ms step_avg:40.83ms
step:1795/2330 train_time:73277ms step_avg:40.82ms
step:1796/2330 train_time:73322ms step_avg:40.83ms
step:1797/2330 train_time:73359ms step_avg:40.82ms
step:1798/2330 train_time:73406ms step_avg:40.83ms
step:1799/2330 train_time:73444ms step_avg:40.82ms
step:1800/2330 train_time:73492ms step_avg:40.83ms
step:1801/2330 train_time:73528ms step_avg:40.83ms
step:1802/2330 train_time:73573ms step_avg:40.83ms
step:1803/2330 train_time:73609ms step_avg:40.83ms
step:1804/2330 train_time:73655ms step_avg:40.83ms
step:1805/2330 train_time:73690ms step_avg:40.83ms
step:1806/2330 train_time:73736ms step_avg:40.83ms
step:1807/2330 train_time:73772ms step_avg:40.83ms
step:1808/2330 train_time:73818ms step_avg:40.83ms
step:1809/2330 train_time:73855ms step_avg:40.83ms
step:1810/2330 train_time:73900ms step_avg:40.83ms
step:1811/2330 train_time:73936ms step_avg:40.83ms
step:1812/2330 train_time:73982ms step_avg:40.83ms
step:1813/2330 train_time:74016ms step_avg:40.83ms
step:1814/2330 train_time:74061ms step_avg:40.83ms
step:1815/2330 train_time:74097ms step_avg:40.82ms
step:1816/2330 train_time:74143ms step_avg:40.83ms
step:1817/2330 train_time:74178ms step_avg:40.82ms
step:1818/2330 train_time:74223ms step_avg:40.83ms
step:1819/2330 train_time:74257ms step_avg:40.82ms
step:1820/2330 train_time:74302ms step_avg:40.83ms
step:1821/2330 train_time:74339ms step_avg:40.82ms
step:1822/2330 train_time:74385ms step_avg:40.83ms
step:1823/2330 train_time:74423ms step_avg:40.82ms
step:1824/2330 train_time:74469ms step_avg:40.83ms
step:1825/2330 train_time:74505ms step_avg:40.82ms
step:1826/2330 train_time:74551ms step_avg:40.83ms
step:1827/2330 train_time:74587ms step_avg:40.82ms
step:1828/2330 train_time:74633ms step_avg:40.83ms
step:1829/2330 train_time:74669ms step_avg:40.82ms
step:1830/2330 train_time:74714ms step_avg:40.83ms
step:1831/2330 train_time:74749ms step_avg:40.82ms
step:1832/2330 train_time:74795ms step_avg:40.83ms
step:1833/2330 train_time:74831ms step_avg:40.82ms
step:1834/2330 train_time:74876ms step_avg:40.83ms
step:1835/2330 train_time:74912ms step_avg:40.82ms
step:1836/2330 train_time:74957ms step_avg:40.83ms
step:1837/2330 train_time:74993ms step_avg:40.82ms
step:1838/2330 train_time:75037ms step_avg:40.83ms
step:1839/2330 train_time:75073ms step_avg:40.82ms
step:1840/2330 train_time:75118ms step_avg:40.82ms
step:1841/2330 train_time:75154ms step_avg:40.82ms
step:1842/2330 train_time:75199ms step_avg:40.82ms
step:1843/2330 train_time:75234ms step_avg:40.82ms
step:1844/2330 train_time:75279ms step_avg:40.82ms
step:1845/2330 train_time:75316ms step_avg:40.82ms
step:1846/2330 train_time:75363ms step_avg:40.82ms
step:1847/2330 train_time:75401ms step_avg:40.82ms
step:1848/2330 train_time:75446ms step_avg:40.83ms
step:1849/2330 train_time:75483ms step_avg:40.82ms
step:1850/2330 train_time:75528ms step_avg:40.83ms
step:1851/2330 train_time:75564ms step_avg:40.82ms
step:1852/2330 train_time:75611ms step_avg:40.83ms
step:1853/2330 train_time:75647ms step_avg:40.82ms
step:1854/2330 train_time:75693ms step_avg:40.83ms
step:1855/2330 train_time:75728ms step_avg:40.82ms
step:1856/2330 train_time:75774ms step_avg:40.83ms
step:1857/2330 train_time:75810ms step_avg:40.82ms
step:1858/2330 train_time:75855ms step_avg:40.83ms
step:1859/2330 train_time:75891ms step_avg:40.82ms
step:1860/2330 train_time:75936ms step_avg:40.83ms
step:1861/2330 train_time:75972ms step_avg:40.82ms
step:1862/2330 train_time:76016ms step_avg:40.83ms
step:1863/2330 train_time:76052ms step_avg:40.82ms
step:1864/2330 train_time:76096ms step_avg:40.82ms
step:1865/2330 train_time:76133ms step_avg:40.82ms
step:1866/2330 train_time:76178ms step_avg:40.82ms
step:1867/2330 train_time:76214ms step_avg:40.82ms
step:1868/2330 train_time:76260ms step_avg:40.82ms
step:1869/2330 train_time:76296ms step_avg:40.82ms
step:1870/2330 train_time:76342ms step_avg:40.82ms
step:1871/2330 train_time:76377ms step_avg:40.82ms
step:1872/2330 train_time:76424ms step_avg:40.82ms
step:1873/2330 train_time:76460ms step_avg:40.82ms
step:1874/2330 train_time:76505ms step_avg:40.82ms
step:1875/2330 train_time:76543ms step_avg:40.82ms
step:1876/2330 train_time:76588ms step_avg:40.83ms
step:1877/2330 train_time:76624ms step_avg:40.82ms
step:1878/2330 train_time:76670ms step_avg:40.83ms
step:1879/2330 train_time:76706ms step_avg:40.82ms
step:1880/2330 train_time:76750ms step_avg:40.82ms
step:1881/2330 train_time:76786ms step_avg:40.82ms
step:1882/2330 train_time:76832ms step_avg:40.82ms
step:1883/2330 train_time:76868ms step_avg:40.82ms
step:1884/2330 train_time:76914ms step_avg:40.82ms
step:1885/2330 train_time:76950ms step_avg:40.82ms
step:1886/2330 train_time:76995ms step_avg:40.82ms
step:1887/2330 train_time:77031ms step_avg:40.82ms
step:1888/2330 train_time:77076ms step_avg:40.82ms
step:1889/2330 train_time:77111ms step_avg:40.82ms
step:1890/2330 train_time:77157ms step_avg:40.82ms
step:1891/2330 train_time:77192ms step_avg:40.82ms
step:1892/2330 train_time:77238ms step_avg:40.82ms
step:1893/2330 train_time:77274ms step_avg:40.82ms
step:1894/2330 train_time:77319ms step_avg:40.82ms
step:1895/2330 train_time:77356ms step_avg:40.82ms
step:1896/2330 train_time:77402ms step_avg:40.82ms
step:1897/2330 train_time:77438ms step_avg:40.82ms
step:1898/2330 train_time:77484ms step_avg:40.82ms
step:1899/2330 train_time:77519ms step_avg:40.82ms
step:1900/2330 train_time:77565ms step_avg:40.82ms
step:1901/2330 train_time:77601ms step_avg:40.82ms
step:1902/2330 train_time:77648ms step_avg:40.82ms
step:1903/2330 train_time:77684ms step_avg:40.82ms
step:1904/2330 train_time:77729ms step_avg:40.82ms
step:1905/2330 train_time:77765ms step_avg:40.82ms
step:1906/2330 train_time:77811ms step_avg:40.82ms
step:1907/2330 train_time:77846ms step_avg:40.82ms
step:1908/2330 train_time:77892ms step_avg:40.82ms
step:1909/2330 train_time:77928ms step_avg:40.82ms
step:1910/2330 train_time:77973ms step_avg:40.82ms
step:1911/2330 train_time:78009ms step_avg:40.82ms
step:1912/2330 train_time:78055ms step_avg:40.82ms
step:1913/2330 train_time:78090ms step_avg:40.82ms
step:1914/2330 train_time:78135ms step_avg:40.82ms
step:1915/2330 train_time:78171ms step_avg:40.82ms
step:1916/2330 train_time:78216ms step_avg:40.82ms
step:1917/2330 train_time:78252ms step_avg:40.82ms
step:1918/2330 train_time:78297ms step_avg:40.82ms
step:1919/2330 train_time:78334ms step_avg:40.82ms
step:1920/2330 train_time:78380ms step_avg:40.82ms
step:1921/2330 train_time:78416ms step_avg:40.82ms
step:1922/2330 train_time:78462ms step_avg:40.82ms
step:1923/2330 train_time:78498ms step_avg:40.82ms
step:1924/2330 train_time:78544ms step_avg:40.82ms
step:1925/2330 train_time:78580ms step_avg:40.82ms
step:1926/2330 train_time:78626ms step_avg:40.82ms
step:1927/2330 train_time:78662ms step_avg:40.82ms
step:1928/2330 train_time:78708ms step_avg:40.82ms
step:1929/2330 train_time:78744ms step_avg:40.82ms
step:1930/2330 train_time:78789ms step_avg:40.82ms
step:1931/2330 train_time:78825ms step_avg:40.82ms
step:1932/2330 train_time:78871ms step_avg:40.82ms
step:1933/2330 train_time:78907ms step_avg:40.82ms
step:1934/2330 train_time:78952ms step_avg:40.82ms
step:1935/2330 train_time:78988ms step_avg:40.82ms
step:1936/2330 train_time:79033ms step_avg:40.82ms
step:1937/2330 train_time:79069ms step_avg:40.82ms
step:1938/2330 train_time:79114ms step_avg:40.82ms
step:1939/2330 train_time:79151ms step_avg:40.82ms
step:1940/2330 train_time:79195ms step_avg:40.82ms
step:1941/2330 train_time:79231ms step_avg:40.82ms
step:1942/2330 train_time:79276ms step_avg:40.82ms
step:1943/2330 train_time:79312ms step_avg:40.82ms
step:1944/2330 train_time:79358ms step_avg:40.82ms
step:1945/2330 train_time:79394ms step_avg:40.82ms
step:1946/2330 train_time:79440ms step_avg:40.82ms
step:1947/2330 train_time:79477ms step_avg:40.82ms
step:1948/2330 train_time:79523ms step_avg:40.82ms
step:1949/2330 train_time:79558ms step_avg:40.82ms
step:1950/2330 train_time:79605ms step_avg:40.82ms
step:1951/2330 train_time:79641ms step_avg:40.82ms
step:1952/2330 train_time:79686ms step_avg:40.82ms
step:1953/2330 train_time:79722ms step_avg:40.82ms
step:1954/2330 train_time:79768ms step_avg:40.82ms
step:1955/2330 train_time:79803ms step_avg:40.82ms
step:1956/2330 train_time:79848ms step_avg:40.82ms
step:1957/2330 train_time:79884ms step_avg:40.82ms
step:1958/2330 train_time:79930ms step_avg:40.82ms
step:1959/2330 train_time:79965ms step_avg:40.82ms
step:1960/2330 train_time:80010ms step_avg:40.82ms
step:1961/2330 train_time:80047ms step_avg:40.82ms
step:1962/2330 train_time:80092ms step_avg:40.82ms
step:1963/2330 train_time:80129ms step_avg:40.82ms
step:1964/2330 train_time:80175ms step_avg:40.82ms
step:1965/2330 train_time:80210ms step_avg:40.82ms
step:1966/2330 train_time:80256ms step_avg:40.82ms
step:1967/2330 train_time:80292ms step_avg:40.82ms
step:1968/2330 train_time:80337ms step_avg:40.82ms
step:1969/2330 train_time:80373ms step_avg:40.82ms
step:1970/2330 train_time:80418ms step_avg:40.82ms
step:1971/2330 train_time:80454ms step_avg:40.82ms
step:1972/2330 train_time:80499ms step_avg:40.82ms
step:1973/2330 train_time:80536ms step_avg:40.82ms
step:1974/2330 train_time:80582ms step_avg:40.82ms
step:1975/2330 train_time:80618ms step_avg:40.82ms
step:1976/2330 train_time:80664ms step_avg:40.82ms
step:1977/2330 train_time:80700ms step_avg:40.82ms
step:1978/2330 train_time:80746ms step_avg:40.82ms
step:1979/2330 train_time:80782ms step_avg:40.82ms
step:1980/2330 train_time:80827ms step_avg:40.82ms
step:1981/2330 train_time:80863ms step_avg:40.82ms
step:1982/2330 train_time:80908ms step_avg:40.82ms
step:1983/2330 train_time:80945ms step_avg:40.82ms
step:1984/2330 train_time:80990ms step_avg:40.82ms
step:1985/2330 train_time:81026ms step_avg:40.82ms
step:1986/2330 train_time:81072ms step_avg:40.82ms
step:1987/2330 train_time:81108ms step_avg:40.82ms
step:1988/2330 train_time:81153ms step_avg:40.82ms
step:1989/2330 train_time:81189ms step_avg:40.82ms
step:1990/2330 train_time:81235ms step_avg:40.82ms
step:1991/2330 train_time:81271ms step_avg:40.82ms
step:1992/2330 train_time:81316ms step_avg:40.82ms
step:1993/2330 train_time:81352ms step_avg:40.82ms
step:1994/2330 train_time:81397ms step_avg:40.82ms
step:1995/2330 train_time:81433ms step_avg:40.82ms
step:1996/2330 train_time:81479ms step_avg:40.82ms
step:1997/2330 train_time:81515ms step_avg:40.82ms
step:1998/2330 train_time:81560ms step_avg:40.82ms
step:1999/2330 train_time:81597ms step_avg:40.82ms
step:2000/2330 train_time:81642ms step_avg:40.82ms
step:2000/2330 val_loss:5.1373 train_time:81731ms step_avg:40.87ms
step:2001/2330 train_time:81744ms step_avg:40.85ms
step:2002/2330 train_time:81758ms step_avg:40.84ms
step:2003/2330 train_time:81769ms step_avg:40.82ms
step:2004/2330 train_time:81805ms step_avg:40.82ms
step:2005/2330 train_time:81840ms step_avg:40.82ms
step:2006/2330 train_time:81885ms step_avg:40.82ms
step:2007/2330 train_time:81919ms step_avg:40.82ms
step:2008/2330 train_time:81964ms step_avg:40.82ms
step:2009/2330 train_time:82000ms step_avg:40.82ms
step:2010/2330 train_time:82049ms step_avg:40.82ms
step:2011/2330 train_time:82088ms step_avg:40.82ms
step:2012/2330 train_time:82135ms step_avg:40.82ms
step:2013/2330 train_time:82172ms step_avg:40.82ms
step:2014/2330 train_time:82219ms step_avg:40.82ms
step:2015/2330 train_time:82256ms step_avg:40.82ms
step:2016/2330 train_time:82301ms step_avg:40.82ms
step:2017/2330 train_time:82337ms step_avg:40.82ms
step:2018/2330 train_time:82381ms step_avg:40.82ms
step:2019/2330 train_time:82416ms step_avg:40.82ms
step:2020/2330 train_time:82461ms step_avg:40.82ms
step:2021/2330 train_time:82497ms step_avg:40.82ms
step:2022/2330 train_time:82542ms step_avg:40.82ms
step:2023/2330 train_time:82576ms step_avg:40.82ms
step:2024/2330 train_time:82621ms step_avg:40.82ms
step:2025/2330 train_time:82658ms step_avg:40.82ms
step:2026/2330 train_time:82705ms step_avg:40.82ms
step:2027/2330 train_time:82741ms step_avg:40.82ms
step:2028/2330 train_time:82785ms step_avg:40.82ms
step:2029/2330 train_time:82820ms step_avg:40.82ms
step:2030/2330 train_time:82865ms step_avg:40.82ms
step:2031/2330 train_time:82900ms step_avg:40.82ms
step:2032/2330 train_time:82946ms step_avg:40.82ms
step:2033/2330 train_time:82981ms step_avg:40.82ms
step:2034/2330 train_time:83028ms step_avg:40.82ms
step:2035/2330 train_time:83065ms step_avg:40.82ms
step:2036/2330 train_time:83112ms step_avg:40.82ms
step:2037/2330 train_time:83148ms step_avg:40.82ms
step:2038/2330 train_time:83195ms step_avg:40.82ms
step:2039/2330 train_time:83231ms step_avg:40.82ms
step:2040/2330 train_time:83277ms step_avg:40.82ms
step:2041/2330 train_time:83313ms step_avg:40.82ms
step:2042/2330 train_time:83359ms step_avg:40.82ms
step:2043/2330 train_time:83394ms step_avg:40.82ms
step:2044/2330 train_time:83439ms step_avg:40.82ms
step:2045/2330 train_time:83475ms step_avg:40.82ms
step:2046/2330 train_time:83519ms step_avg:40.82ms
step:2047/2330 train_time:83554ms step_avg:40.82ms
step:2048/2330 train_time:83601ms step_avg:40.82ms
step:2049/2330 train_time:83638ms step_avg:40.82ms
step:2050/2330 train_time:83683ms step_avg:40.82ms
step:2051/2330 train_time:83718ms step_avg:40.82ms
step:2052/2330 train_time:83763ms step_avg:40.82ms
step:2053/2330 train_time:83798ms step_avg:40.82ms
step:2054/2330 train_time:83844ms step_avg:40.82ms
step:2055/2330 train_time:83879ms step_avg:40.82ms
step:2056/2330 train_time:83925ms step_avg:40.82ms
step:2057/2330 train_time:83961ms step_avg:40.82ms
step:2058/2330 train_time:84007ms step_avg:40.82ms
step:2059/2330 train_time:84043ms step_avg:40.82ms
step:2060/2330 train_time:84089ms step_avg:40.82ms
step:2061/2330 train_time:84126ms step_avg:40.82ms
step:2062/2330 train_time:84172ms step_avg:40.82ms
step:2063/2330 train_time:84207ms step_avg:40.82ms
step:2064/2330 train_time:84252ms step_avg:40.82ms
step:2065/2330 train_time:84288ms step_avg:40.82ms
step:2066/2330 train_time:84333ms step_avg:40.82ms
step:2067/2330 train_time:84369ms step_avg:40.82ms
step:2068/2330 train_time:84415ms step_avg:40.82ms
step:2069/2330 train_time:84451ms step_avg:40.82ms
step:2070/2330 train_time:84496ms step_avg:40.82ms
step:2071/2330 train_time:84532ms step_avg:40.82ms
step:2072/2330 train_time:84577ms step_avg:40.82ms
step:2073/2330 train_time:84614ms step_avg:40.82ms
step:2074/2330 train_time:84660ms step_avg:40.82ms
step:2075/2330 train_time:84696ms step_avg:40.82ms
step:2076/2330 train_time:84741ms step_avg:40.82ms
step:2077/2330 train_time:84777ms step_avg:40.82ms
step:2078/2330 train_time:84822ms step_avg:40.82ms
step:2079/2330 train_time:84858ms step_avg:40.82ms
step:2080/2330 train_time:84902ms step_avg:40.82ms
step:2081/2330 train_time:84940ms step_avg:40.82ms
step:2082/2330 train_time:84986ms step_avg:40.82ms
step:2083/2330 train_time:85022ms step_avg:40.82ms
step:2084/2330 train_time:85067ms step_avg:40.82ms
step:2085/2330 train_time:85103ms step_avg:40.82ms
step:2086/2330 train_time:85149ms step_avg:40.82ms
step:2087/2330 train_time:85186ms step_avg:40.82ms
step:2088/2330 train_time:85231ms step_avg:40.82ms
step:2089/2330 train_time:85267ms step_avg:40.82ms
step:2090/2330 train_time:85313ms step_avg:40.82ms
step:2091/2330 train_time:85349ms step_avg:40.82ms
step:2092/2330 train_time:85394ms step_avg:40.82ms
step:2093/2330 train_time:85429ms step_avg:40.82ms
step:2094/2330 train_time:85474ms step_avg:40.82ms
step:2095/2330 train_time:85510ms step_avg:40.82ms
step:2096/2330 train_time:85555ms step_avg:40.82ms
step:2097/2330 train_time:85591ms step_avg:40.82ms
step:2098/2330 train_time:85637ms step_avg:40.82ms
step:2099/2330 train_time:85673ms step_avg:40.82ms
step:2100/2330 train_time:85719ms step_avg:40.82ms
step:2101/2330 train_time:85754ms step_avg:40.82ms
step:2102/2330 train_time:85800ms step_avg:40.82ms
step:2103/2330 train_time:85836ms step_avg:40.82ms
step:2104/2330 train_time:85882ms step_avg:40.82ms
step:2105/2330 train_time:85918ms step_avg:40.82ms
step:2106/2330 train_time:85963ms step_avg:40.82ms
step:2107/2330 train_time:86000ms step_avg:40.82ms
step:2108/2330 train_time:86046ms step_avg:40.82ms
step:2109/2330 train_time:86081ms step_avg:40.82ms
step:2110/2330 train_time:86127ms step_avg:40.82ms
step:2111/2330 train_time:86163ms step_avg:40.82ms
step:2112/2330 train_time:86208ms step_avg:40.82ms
step:2113/2330 train_time:86244ms step_avg:40.82ms
step:2114/2330 train_time:86290ms step_avg:40.82ms
step:2115/2330 train_time:86327ms step_avg:40.82ms
step:2116/2330 train_time:86372ms step_avg:40.82ms
step:2117/2330 train_time:86409ms step_avg:40.82ms
step:2118/2330 train_time:86453ms step_avg:40.82ms
step:2119/2330 train_time:86489ms step_avg:40.82ms
step:2120/2330 train_time:86534ms step_avg:40.82ms
step:2121/2330 train_time:86570ms step_avg:40.82ms
step:2122/2330 train_time:86615ms step_avg:40.82ms
step:2123/2330 train_time:86651ms step_avg:40.82ms
step:2124/2330 train_time:86696ms step_avg:40.82ms
step:2125/2330 train_time:86733ms step_avg:40.82ms
step:2126/2330 train_time:86778ms step_avg:40.82ms
step:2127/2330 train_time:86814ms step_avg:40.82ms
step:2128/2330 train_time:86860ms step_avg:40.82ms
step:2129/2330 train_time:86897ms step_avg:40.82ms
step:2130/2330 train_time:86943ms step_avg:40.82ms
step:2131/2330 train_time:86979ms step_avg:40.82ms
step:2132/2330 train_time:87024ms step_avg:40.82ms
step:2133/2330 train_time:87062ms step_avg:40.82ms
step:2134/2330 train_time:87107ms step_avg:40.82ms
step:2135/2330 train_time:87142ms step_avg:40.82ms
step:2136/2330 train_time:87188ms step_avg:40.82ms
step:2137/2330 train_time:87223ms step_avg:40.82ms
step:2138/2330 train_time:87268ms step_avg:40.82ms
step:2139/2330 train_time:87304ms step_avg:40.82ms
step:2140/2330 train_time:87351ms step_avg:40.82ms
step:2141/2330 train_time:87386ms step_avg:40.82ms
step:2142/2330 train_time:87432ms step_avg:40.82ms
step:2143/2330 train_time:87467ms step_avg:40.82ms
step:2144/2330 train_time:87513ms step_avg:40.82ms
step:2145/2330 train_time:87548ms step_avg:40.81ms
step:2146/2330 train_time:87593ms step_avg:40.82ms
step:2147/2330 train_time:87628ms step_avg:40.81ms
step:2148/2330 train_time:87674ms step_avg:40.82ms
step:2149/2330 train_time:87710ms step_avg:40.81ms
step:2150/2330 train_time:87756ms step_avg:40.82ms
step:2151/2330 train_time:87792ms step_avg:40.81ms
step:2152/2330 train_time:87837ms step_avg:40.82ms
step:2153/2330 train_time:87873ms step_avg:40.81ms
step:2154/2330 train_time:87919ms step_avg:40.82ms
step:2155/2330 train_time:87956ms step_avg:40.81ms
step:2156/2330 train_time:88002ms step_avg:40.82ms
step:2157/2330 train_time:88038ms step_avg:40.81ms
step:2158/2330 train_time:88082ms step_avg:40.82ms
step:2159/2330 train_time:88118ms step_avg:40.81ms
step:2160/2330 train_time:88164ms step_avg:40.82ms
step:2161/2330 train_time:88200ms step_avg:40.81ms
step:2162/2330 train_time:88245ms step_avg:40.82ms
step:2163/2330 train_time:88281ms step_avg:40.81ms
step:2164/2330 train_time:88327ms step_avg:40.82ms
step:2165/2330 train_time:88363ms step_avg:40.81ms
step:2166/2330 train_time:88409ms step_avg:40.82ms
step:2167/2330 train_time:88445ms step_avg:40.81ms
step:2168/2330 train_time:88489ms step_avg:40.82ms
step:2169/2330 train_time:88525ms step_avg:40.81ms
step:2170/2330 train_time:88571ms step_avg:40.82ms
step:2171/2330 train_time:88606ms step_avg:40.81ms
step:2172/2330 train_time:88652ms step_avg:40.82ms
step:2173/2330 train_time:88688ms step_avg:40.81ms
step:2174/2330 train_time:88733ms step_avg:40.82ms
step:2175/2330 train_time:88769ms step_avg:40.81ms
step:2176/2330 train_time:88815ms step_avg:40.82ms
step:2177/2330 train_time:88850ms step_avg:40.81ms
step:2178/2330 train_time:88896ms step_avg:40.82ms
step:2179/2330 train_time:88932ms step_avg:40.81ms
step:2180/2330 train_time:88978ms step_avg:40.82ms
step:2181/2330 train_time:89014ms step_avg:40.81ms
step:2182/2330 train_time:89061ms step_avg:40.82ms
step:2183/2330 train_time:89097ms step_avg:40.81ms
step:2184/2330 train_time:89143ms step_avg:40.82ms
step:2185/2330 train_time:89179ms step_avg:40.81ms
step:2186/2330 train_time:89225ms step_avg:40.82ms
step:2187/2330 train_time:89261ms step_avg:40.81ms
step:2188/2330 train_time:89307ms step_avg:40.82ms
step:2189/2330 train_time:89342ms step_avg:40.81ms
step:2190/2330 train_time:89388ms step_avg:40.82ms
step:2191/2330 train_time:89423ms step_avg:40.81ms
step:2192/2330 train_time:89469ms step_avg:40.82ms
step:2193/2330 train_time:89505ms step_avg:40.81ms
step:2194/2330 train_time:89551ms step_avg:40.82ms
step:2195/2330 train_time:89587ms step_avg:40.81ms
step:2196/2330 train_time:89632ms step_avg:40.82ms
step:2197/2330 train_time:89668ms step_avg:40.81ms
step:2198/2330 train_time:89713ms step_avg:40.82ms
step:2199/2330 train_time:89749ms step_avg:40.81ms
step:2200/2330 train_time:89794ms step_avg:40.82ms
step:2201/2330 train_time:89831ms step_avg:40.81ms
step:2202/2330 train_time:89876ms step_avg:40.82ms
step:2203/2330 train_time:89912ms step_avg:40.81ms
step:2204/2330 train_time:89959ms step_avg:40.82ms
step:2205/2330 train_time:89995ms step_avg:40.81ms
step:2206/2330 train_time:90041ms step_avg:40.82ms
step:2207/2330 train_time:90077ms step_avg:40.81ms
step:2208/2330 train_time:90121ms step_avg:40.82ms
step:2209/2330 train_time:90158ms step_avg:40.81ms
step:2210/2330 train_time:90205ms step_avg:40.82ms
step:2211/2330 train_time:90240ms step_avg:40.81ms
step:2212/2330 train_time:90285ms step_avg:40.82ms
step:2213/2330 train_time:90321ms step_avg:40.81ms
step:2214/2330 train_time:90367ms step_avg:40.82ms
step:2215/2330 train_time:90403ms step_avg:40.81ms
step:2216/2330 train_time:90449ms step_avg:40.82ms
step:2217/2330 train_time:90484ms step_avg:40.81ms
step:2218/2330 train_time:90529ms step_avg:40.82ms
step:2219/2330 train_time:90565ms step_avg:40.81ms
step:2220/2330 train_time:90611ms step_avg:40.82ms
step:2221/2330 train_time:90647ms step_avg:40.81ms
step:2222/2330 train_time:90693ms step_avg:40.82ms
step:2223/2330 train_time:90729ms step_avg:40.81ms
step:2224/2330 train_time:90774ms step_avg:40.82ms
step:2225/2330 train_time:90810ms step_avg:40.81ms
step:2226/2330 train_time:90855ms step_avg:40.82ms
step:2227/2330 train_time:90891ms step_avg:40.81ms
step:2228/2330 train_time:90936ms step_avg:40.82ms
step:2229/2330 train_time:90972ms step_avg:40.81ms
step:2230/2330 train_time:91018ms step_avg:40.82ms
step:2231/2330 train_time:91055ms step_avg:40.81ms
step:2232/2330 train_time:91101ms step_avg:40.82ms
step:2233/2330 train_time:91137ms step_avg:40.81ms
step:2234/2330 train_time:91181ms step_avg:40.82ms
step:2235/2330 train_time:91218ms step_avg:40.81ms
step:2236/2330 train_time:91264ms step_avg:40.82ms
step:2237/2330 train_time:91300ms step_avg:40.81ms
step:2238/2330 train_time:91346ms step_avg:40.82ms
step:2239/2330 train_time:91381ms step_avg:40.81ms
step:2240/2330 train_time:91427ms step_avg:40.82ms
step:2241/2330 train_time:91463ms step_avg:40.81ms
step:2242/2330 train_time:91509ms step_avg:40.82ms
step:2243/2330 train_time:91544ms step_avg:40.81ms
step:2244/2330 train_time:91590ms step_avg:40.82ms
step:2245/2330 train_time:91626ms step_avg:40.81ms
step:2246/2330 train_time:91671ms step_avg:40.82ms
step:2247/2330 train_time:91707ms step_avg:40.81ms
step:2248/2330 train_time:91752ms step_avg:40.81ms
step:2249/2330 train_time:91788ms step_avg:40.81ms
step:2250/2330 train_time:91833ms step_avg:40.81ms
step:2250/2330 val_loss:5.1103 train_time:91922ms step_avg:40.85ms
step:2251/2330 train_time:91937ms step_avg:40.84ms
step:2252/2330 train_time:91950ms step_avg:40.83ms
step:2253/2330 train_time:91961ms step_avg:40.82ms
step:2254/2330 train_time:91996ms step_avg:40.81ms
step:2255/2330 train_time:92031ms step_avg:40.81ms
step:2256/2330 train_time:92076ms step_avg:40.81ms
step:2257/2330 train_time:92111ms step_avg:40.81ms
step:2258/2330 train_time:92155ms step_avg:40.81ms
step:2259/2330 train_time:92190ms step_avg:40.81ms
step:2260/2330 train_time:92239ms step_avg:40.81ms
step:2261/2330 train_time:92276ms step_avg:40.81ms
step:2262/2330 train_time:92324ms step_avg:40.82ms
step:2263/2330 train_time:92363ms step_avg:40.81ms
step:2264/2330 train_time:92409ms step_avg:40.82ms
step:2265/2330 train_time:92446ms step_avg:40.81ms
step:2266/2330 train_time:92492ms step_avg:40.82ms
step:2267/2330 train_time:92527ms step_avg:40.81ms
step:2268/2330 train_time:92571ms step_avg:40.82ms
step:2269/2330 train_time:92606ms step_avg:40.81ms
step:2270/2330 train_time:92652ms step_avg:40.82ms
step:2271/2330 train_time:92687ms step_avg:40.81ms
step:2272/2330 train_time:92732ms step_avg:40.82ms
step:2273/2330 train_time:92767ms step_avg:40.81ms
step:2274/2330 train_time:92814ms step_avg:40.82ms
step:2275/2330 train_time:92853ms step_avg:40.81ms
step:2276/2330 train_time:92899ms step_avg:40.82ms
step:2277/2330 train_time:92935ms step_avg:40.81ms
step:2278/2330 train_time:92979ms step_avg:40.82ms
step:2279/2330 train_time:93014ms step_avg:40.81ms
step:2280/2330 train_time:93059ms step_avg:40.82ms
step:2281/2330 train_time:93093ms step_avg:40.81ms
step:2282/2330 train_time:93139ms step_avg:40.81ms
step:2283/2330 train_time:93175ms step_avg:40.81ms
step:2284/2330 train_time:93222ms step_avg:40.82ms
step:2285/2330 train_time:93259ms step_avg:40.81ms
step:2286/2330 train_time:93305ms step_avg:40.82ms
step:2287/2330 train_time:93341ms step_avg:40.81ms
step:2288/2330 train_time:93388ms step_avg:40.82ms
step:2289/2330 train_time:93424ms step_avg:40.81ms
step:2290/2330 train_time:93469ms step_avg:40.82ms
step:2291/2330 train_time:93505ms step_avg:40.81ms
step:2292/2330 train_time:93551ms step_avg:40.82ms
step:2293/2330 train_time:93586ms step_avg:40.81ms
step:2294/2330 train_time:93630ms step_avg:40.82ms
step:2295/2330 train_time:93666ms step_avg:40.81ms
step:2296/2330 train_time:93711ms step_avg:40.82ms
step:2297/2330 train_time:93747ms step_avg:40.81ms
step:2298/2330 train_time:93794ms step_avg:40.82ms
step:2299/2330 train_time:93830ms step_avg:40.81ms
step:2300/2330 train_time:93876ms step_avg:40.82ms
step:2301/2330 train_time:93912ms step_avg:40.81ms
step:2302/2330 train_time:93957ms step_avg:40.82ms
step:2303/2330 train_time:93992ms step_avg:40.81ms
step:2304/2330 train_time:94037ms step_avg:40.81ms
step:2305/2330 train_time:94073ms step_avg:40.81ms
step:2306/2330 train_time:94119ms step_avg:40.81ms
step:2307/2330 train_time:94155ms step_avg:40.81ms
step:2308/2330 train_time:94200ms step_avg:40.81ms
step:2309/2330 train_time:94235ms step_avg:40.81ms
step:2310/2330 train_time:94282ms step_avg:40.81ms
step:2311/2330 train_time:94317ms step_avg:40.81ms
step:2312/2330 train_time:94364ms step_avg:40.81ms
step:2313/2330 train_time:94400ms step_avg:40.81ms
step:2314/2330 train_time:94445ms step_avg:40.81ms
step:2315/2330 train_time:94481ms step_avg:40.81ms
step:2316/2330 train_time:94526ms step_avg:40.81ms
step:2317/2330 train_time:94562ms step_avg:40.81ms
step:2318/2330 train_time:94607ms step_avg:40.81ms
step:2319/2330 train_time:94642ms step_avg:40.81ms
step:2320/2330 train_time:94688ms step_avg:40.81ms
step:2321/2330 train_time:94724ms step_avg:40.81ms
step:2322/2330 train_time:94770ms step_avg:40.81ms
step:2323/2330 train_time:94808ms step_avg:40.81ms
step:2324/2330 train_time:94854ms step_avg:40.81ms
step:2325/2330 train_time:94890ms step_avg:40.81ms
step:2326/2330 train_time:94935ms step_avg:40.81ms
step:2327/2330 train_time:94970ms step_avg:40.81ms
step:2328/2330 train_time:95016ms step_avg:40.81ms
step:2329/2330 train_time:95052ms step_avg:40.81ms
step:2330/2330 train_time:95099ms step_avg:40.81ms
step:2330/2330 val_loss:5.1034 train_time:95187ms step_avg:40.85ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
